{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textstat import textstat\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset,  Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://arxiv.org/search/?query={CONFERENCE_NAME}&searchtype=all&abstracts=show&order=-announced_date_first&size=200\"\n",
    "output_dir = \"{CONFERENCE_NAME}\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_pdf(pdf_url, output_path):\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"[SUCCESS] Downloaded: {output_path}\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to download: {pdf_url}\")\n",
    "\n",
    "response = requests.get(search_url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    pdf_links = soup.find_all(\"a\", href=lambda href: href and \"/pdf/\" in href)\n",
    "\n",
    "    for link in pdf_links:\n",
    "        pdf_url = f\"https://arxiv.org{link['href']}.pdf\"\n",
    "        pdf_name = os.path.basename(link['href']) + \".pdf\"\n",
    "        pdf_path = os.path.join(output_dir, pdf_name)\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            download_pdf(pdf_url, pdf_path)\n",
    "        else:\n",
    "            print(f\"[INFO] Already downloaded: {pdf_path}\")\n",
    "else:\n",
    "    print(f\"[ERROR] Failed to fetch ArXiv page: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCorruptor:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        self.headers = {\"Authorization\": \"Bearer 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\"}\n",
    "        self._test_api_connection()\n",
    "        self.fallback_phrases = [\n",
    "            \"potato dreams fly upward\", \"singing mountains eat clouds\",\n",
    "            \"blue ideas sleep furiously\", \"yesterday tomorrow today simultaneously\"\n",
    "        ]\n",
    "\n",
    "    def _test_api_connection(self):\n",
    "        try:\n",
    "            response = requests.post(self.api_url, headers=self.headers, json={\"inputs\": \"test\"})\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"API connection failed: {e}\")\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        return ' '.join(text.replace('\\n', ' ').split())\n",
    "\n",
    "    def remove_characters(self, text):\n",
    "        if not text:\n",
    "            return text\n",
    "        chars = list(text)\n",
    "        remove_count = random.randint(len(chars) // 20, len(chars) // 10)\n",
    "        for _ in range(remove_count):\n",
    "            if chars:\n",
    "                idx = random.randint(0, len(chars) - 1)\n",
    "                chars.pop(idx)\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def remove_words(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) <= 1:\n",
    "            return text\n",
    "            \n",
    "        remove_count = random.randint(len(words) // 6, len(words) // 4)\n",
    "        for _ in range(remove_count):\n",
    "            if words:\n",
    "                idx = random.randint(0, len(words) - 1)\n",
    "                words.pop(idx)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def remove_sentences(self, section):\n",
    "        sentences = [s.strip() for s in section.split('.') if s.strip()]\n",
    "        if len(sentences) <= 1:\n",
    "            return section\n",
    "            \n",
    "        remove_count = random.randint(max(1, int(len(sentences) * 0.2)), \n",
    "                                    max(1, int(len(sentences) * 0.4)))\n",
    "        for _ in range(remove_count):\n",
    "            if sentences:\n",
    "                idx = random.randint(0, len(sentences) - 1)\n",
    "                sentences.pop(idx)\n",
    "        return '. '.join(sentences) + '.' if sentences else ''\n",
    "\n",
    "    def remove_paragraphs(self, text):\n",
    "        paragraphs = text.split('. ')\n",
    "        if len(paragraphs) <= 1:\n",
    "            return text\n",
    "            \n",
    "        remove_count = random.randint(len(paragraphs) // 4, len(paragraphs) // 2)\n",
    "        for _ in range(remove_count):\n",
    "            if paragraphs:\n",
    "                idx = random.randint(0, len(paragraphs) - 1)\n",
    "                paragraphs.pop(idx)\n",
    "        return '. '.join(paragraphs)\n",
    "\n",
    "    def generate_nonsense(self):\n",
    "        try:\n",
    "            payload = {\n",
    "                \"inputs\": \"Generate a nonsensical phrase. it should be completely random and should be atleast 5 - 20 words\",\n",
    "                \"parameters\": {\"max_length\": 50, \"temperature\": 0.9}\n",
    "            }\n",
    "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "            text = response.json()[0][\"generated_text\"].split(\":\")[-1].strip('\"\\'').strip()\n",
    "            return text if text and len(text.split()) <= 5 else random.choice(self.fallback_phrases)\n",
    "        except:\n",
    "            return random.choice(self.fallback_phrases)\n",
    "\n",
    "    def add_nonsense(self, section):\n",
    "        words = section.split()\n",
    "        if not words:\n",
    "            return section\n",
    "        num_phrases = random.randint(1, 2)\n",
    "        for _ in range(num_phrases):\n",
    "            if words:\n",
    "                pos = random.randint(0, len(words))\n",
    "                words.insert(pos, self.generate_nonsense())\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def disturb_grammar(self, text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "            \n",
    "        for i in range(len(words)):\n",
    "            if random.random() > 0.8:\n",
    "                if words[i].lower() in {'a', 'an', 'the'}:\n",
    "                    words[i] = ''\n",
    "                elif len(words[i]) > 3:\n",
    "                    if words[i].endswith('ing'):\n",
    "                        words[i] = words[i][:-3] + 'ed'\n",
    "                    elif words[i].endswith('ed'):\n",
    "                        words[i] = words[i][:-2] + 'ing'\n",
    "        return ' '.join(w for w in words if w)\n",
    "\n",
    "    def reorder_text(self, text):\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        if len(sentences) <= 1:\n",
    "            return text\n",
    "        random.shuffle(sentences)\n",
    "        return '. '.join(sentences) + '.'\n",
    "\n",
    "    def corrupt_document(self, data):\n",
    "        cleaned_data = {self.clean_text(k): self.clean_text(v) for k, v in data.items()}\n",
    "        \n",
    "        # More moderate section removal (20-40% of sections)\n",
    "        sections = list(cleaned_data.keys())\n",
    "        if len(sections) > 1:\n",
    "            remove_count = random.randint(\n",
    "                max(1, int(len(sections) * 0.2)),\n",
    "                max(1, int(len(sections) * 0.4))\n",
    "            )\n",
    "            for _ in range(remove_count):\n",
    "                if sections:\n",
    "                    cleaned_data.pop(random.choice(sections))\n",
    "                    sections = list(cleaned_data.keys())\n",
    "        \n",
    "        corrupted = {}\n",
    "        for heading, content in cleaned_data.items():\n",
    "            # Apply removal operations with moderate probabilities\n",
    "            if random.random() < 0.5:\n",
    "                content = self.remove_paragraphs(content)\n",
    "            if random.random() < 0.7:\n",
    "                content = self.remove_sentences(content)\n",
    "            if random.random() < 0.7:\n",
    "                content = self.remove_words(content)\n",
    "            if random.random() < 0.6:\n",
    "                content = self.remove_characters(content)\n",
    "            \n",
    "            # Apply other corruptions\n",
    "            if random.random() < 0.5:\n",
    "                content = self.add_nonsense(content)\n",
    "            if random.random() < 0.6:\n",
    "                content = self.disturb_grammar(content)\n",
    "            if random.random() < 0.5:\n",
    "                content = self.reorder_text(content)\n",
    "            \n",
    "            # 30% chance to corrupt heading\n",
    "            if random.random() < 0.4:\n",
    "                heading = self.remove_words(heading)\n",
    "                if random.random() < 0.4:\n",
    "                    heading = self.remove_characters(heading)\n",
    "            \n",
    "            if content.strip():\n",
    "                corrupted[heading] = content\n",
    "        \n",
    "        items = list(corrupted.items())\n",
    "        random.shuffle(items)\n",
    "        return dict(items)\n",
    "\n",
    "def process_directory(input_dir: str, output_dir: str):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    corruptor = TextCorruptor()\n",
    "    \n",
    "    for json_file in input_path.glob('**/*.json'):\n",
    "        output_file = output_path / json_file.name\n",
    "        \n",
    "        if output_file.exists():\n",
    "            print(f\"Skipping {json_file.name} - already processed\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            corrupted_data = corruptor.corrupt_document(data)\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(corrupted_data, f, indent=4)\n",
    "            print(f\"Processed: {json_file.name} -> {output_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"Dataset/texts/publishable\"\n",
    "    output_dir = \"Dataset/texts/non-publishable\"\n",
    "    process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoraemonPDFParser:\n",
    "    def __init__(self, min_section_length: int = 50):\n",
    "        self.patterns = {\n",
    "            'urls': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "            'citations': r'\\[[0-9,\\s-]+\\]',\n",
    "            'references': r'^(?:References?|REFERENCES?|Bibliography|BIBLIOGRAPHY)(?:\\s|$)',\n",
    "            'appendix': r'^(?:Appendix|APPENDIX)(?:\\s+[A-Z])?(?:\\s|:|$)',\n",
    "            'acknowledgments': r'^(?:Acknowledgments?|ACKNOWLEDGMENTS?)(?:\\s|$)',\n",
    "            'emails': r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+',\n",
    "            'line_numbers': r'^\\d+$',\n",
    "            'page_numbers': r'^\\d+$',\n",
    "            'cross_refs': r'(Fig\\.|Figure|Table|Section)\\s*\\d+',\n",
    "            'figure_captions': r'(Figure|Fig\\.)\\s*\\d+[.:]\\s*.*?(?=\\n|$)',\n",
    "            'table_captions': r'Table\\s*\\d+[.:]\\s*.*?(?=\\n|$)',\n",
    "            'section_number': r'^(?:\\d+\\.)*\\d+(?:\\s+|\\b)|^\\.\\d+(?:\\s+|\\b)'\n",
    "        }\n",
    "        self.heading_font_sizes = []\n",
    "        self.found_references = False\n",
    "        self.found_appendix = False\n",
    "        self.min_section_length = min_section_length\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        log_dir = \"logs\"\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(log_dir, f\"pdf_parser_{timestamp}.log\")\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            if pattern_name not in ['figure_captions', 'table_captions', 'section_number']:\n",
    "                text = re.sub(pattern, '', text)\n",
    "        text = re.sub(r'(?<=[a-z])-\\n(?=[a-z])', '', text)\n",
    "        text = re.sub(r'(?<=[a-z])\\n(?=[a-z])', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def get_font_properties(self, block: Dict) -> Tuple[str, float, bool, str]:\n",
    "        text = \"\"\n",
    "        max_font_size = 0\n",
    "        is_bold = False\n",
    "        font_face = \"\"\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                text += span.get(\"text\", \"\")\n",
    "                current_size = span.get(\"size\", 0)\n",
    "                if current_size > max_font_size:\n",
    "                    max_font_size = current_size\n",
    "                    font_face = span.get(\"font\", \"\")\n",
    "                is_bold = is_bold or (span.get(\"flags\", 0) & 2 ** 2 != 0)\n",
    "        return text, max_font_size, is_bold, font_face\n",
    "\n",
    "    def clean_heading(self, text: str) -> str:\n",
    "        text = re.sub(r'^(?:\\d+\\.)*\\d+(?:\\s+|\\b)|^\\.\\d+(?:\\s+|\\b)', '', text)\n",
    "        text = re.sub(r'^(?:\\d+\\.)*\\d+([A-Z][a-z])', r'\\1', text)\n",
    "        text = re.sub(r'^\\.\\d+([A-Z][a-z])', r'\\1', text)\n",
    "        text = re.sub(r'\\.+\\s*$', '', text)\n",
    "        text = re.sub(r'^\\d+([A-Z])', r'\\1', text)\n",
    "        text = re.sub(r'^\\.\\d+([A-Z])', r'\\1', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def is_heading(self, text: str, font_size: float, is_bold: bool, font_face: str) -> Tuple[bool, str]:\n",
    "        if re.match(self.patterns['references'], text):\n",
    "            self.found_references = True\n",
    "            return False, text\n",
    "        elif re.match(self.patterns['appendix'], text):\n",
    "            self.found_appendix = True\n",
    "            return False, text\n",
    "            \n",
    "        heading_patterns = [\n",
    "            r'^(?:\\d+\\.)*\\d+\\s+[A-Z][A-Za-z\\s]+$',\n",
    "            r'^(?:\\d+\\.)*\\d+\\s+[A-Z][A-Z\\s]+$',\n",
    "            r'^(?:\\d+\\.)*\\d+[A-Z][A-Za-z\\s]+$',\n",
    "            r'^\\.\\d+\\s+[A-Z][A-Za-z\\s]+$',\n",
    "            r'^\\.\\d+[A-Z][A-Za-z\\s]+$',\n",
    "            r'^[A-Z][A-Z\\s]{3,}[A-Z]$',\n",
    "            r'^[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*$'\n",
    "        ]\n",
    "        \n",
    "        if len(self.heading_font_sizes) < 10:\n",
    "            self.heading_font_sizes.append(font_size)\n",
    "        \n",
    "        avg_font_size = sum(self.heading_font_sizes) / len(self.heading_font_sizes) if self.heading_font_sizes else 11\n",
    "        is_larger_font = font_size > avg_font_size + 1\n",
    "        text = text.strip()\n",
    "        \n",
    "        is_pattern_match = any(re.match(pattern, text) for pattern in heading_patterns)\n",
    "        is_short = len(text) < 200\n",
    "        has_heading_properties = (is_larger_font or is_bold) and is_short\n",
    "        \n",
    "        if is_pattern_match or has_heading_properties:\n",
    "            cleaned_heading = self.clean_heading(text)\n",
    "            return True, cleaned_heading\n",
    "        return False, text\n",
    "\n",
    "    def should_include_section(self, heading: str, text: str) -> bool:\n",
    "        if self.found_references or self.found_appendix:\n",
    "            return False\n",
    "            \n",
    "        if any(re.match(self.patterns[pattern], heading.strip()) \n",
    "               for pattern in ['references', 'appendix', 'acknowledgments']):\n",
    "            if re.match(self.patterns['references'], heading.strip()):\n",
    "                self.found_references = True\n",
    "            elif re.match(self.patterns['appendix'], heading.strip()):\n",
    "                self.found_appendix = True\n",
    "            return False\n",
    "            \n",
    "        if len(text.strip().split()) < self.min_section_length:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def parse_pdf(self, pdf_path: str) -> Dict:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            sections = {}\n",
    "            current_heading = ''\n",
    "            current_content = []\n",
    "            self.heading_font_sizes = []\n",
    "            self.found_references = False\n",
    "            self.found_appendix = False\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if block.get(\"type\") == 0:\n",
    "                        text, font_size, is_bold, font_face = self.get_font_properties(block)\n",
    "                        text = self.clean_text(text)\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        is_heading, heading_text = self.is_heading(text, font_size, is_bold, font_face)\n",
    "                        if is_heading:\n",
    "                            if current_heading and current_content:\n",
    "                                section_text = ' '.join(current_content)\n",
    "                                if self.should_include_section(current_heading, section_text):\n",
    "                                    sections[current_heading] = section_text\n",
    "                            current_heading = heading_text\n",
    "                            current_content = []\n",
    "                        else:\n",
    "                            current_content.append(text)\n",
    "            \n",
    "            if current_heading and current_content:\n",
    "                section_text = ' '.join(current_content)\n",
    "                if self.should_include_section(current_heading, section_text):\n",
    "                    sections[current_heading] = section_text\n",
    "            \n",
    "            doc.close()\n",
    "            return sections\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing PDF {pdf_path}: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_json(self, parsed_content: Dict, output_path: str):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(parsed_content, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"Successfully saved parsed content to {output_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving to JSON: {str(e)}\")\n",
    "\n",
    "def process_directory(input_dir: str, output_dir: str, min_section_length: int = 50):\n",
    "    parser = DoraemonPDFParser(min_section_length=min_section_length)\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, input_dir)\n",
    "                output_path = os.path.join(\n",
    "                    output_dir,\n",
    "                    relative_path,\n",
    "                    f\"P{count:04d}.json\"\n",
    "                )\n",
    "                count += 1\n",
    "                #parser.logger.info(f\"Processing: {pdf_path}\")\n",
    "                parsed_content = parser.parse_pdf(pdf_path)\n",
    "                parser.save_to_json(parsed_content, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_input_dir = \"Dataset/pdfs/non-publishable\"\n",
    "    base_output_dir = \"Dataset/texts/non-publishable\"\n",
    "    input_dir = os.path.join(base_input_dir)\n",
    "    output_dir = os.path.join(base_output_dir)\n",
    "    process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoraemonProcessor:\n",
    "    def __init__(self, model_name: str = \"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"[INFO] Model loaded on: {self.device}\")\n",
    "        self.hidden_size = 768\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def get_chunks(self, text: str, max_length: int = 512) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_ids = tokens[\"input_ids\"][0]\n",
    "        attention_mask = tokens[\"attention_mask\"][0]\n",
    "        num_tokens = input_ids.shape[0]\n",
    "        chunk_boundaries = range(0, num_tokens, max_length)\n",
    "        chunks_ids = []\n",
    "        chunks_mask = []\n",
    "\n",
    "        for start in chunk_boundaries:\n",
    "            end = min(start + max_length, num_tokens)\n",
    "            chunk_ids = input_ids[start:end]\n",
    "            chunk_mask = attention_mask[start:end]\n",
    "            if chunk_ids.shape[0] < max_length:\n",
    "                pad_length = max_length - chunk_ids.shape[0]\n",
    "                chunk_ids = torch.nn.functional.pad(chunk_ids, (0, pad_length), value=self.tokenizer.pad_token_id)\n",
    "                chunk_mask = torch.nn.functional.pad(chunk_mask, (0, pad_length), value=0)\n",
    "            chunks_ids.append(chunk_ids)\n",
    "            chunks_mask.append(chunk_mask)\n",
    "\n",
    "        all_chunk_ids = torch.stack(chunks_ids).unsqueeze(0)\n",
    "        all_chunk_masks = torch.stack(chunks_mask).unsqueeze(0)\n",
    "        return all_chunk_ids.to(self.device), all_chunk_masks.to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def process_text(self, chunks_ids: torch.Tensor, chunks_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        all_embeddings = []\n",
    "        all_hidden_states = []\n",
    "\n",
    "        for i in range(chunks_ids.shape[1]):\n",
    "            chunk_ids = chunks_ids[:, i, :]\n",
    "            chunk_mask = chunks_mask[:, i, :]\n",
    "            outputs = self.model(input_ids=chunk_ids, attention_mask=chunk_mask, output_hidden_states=True)\n",
    "            all_embeddings.append(outputs.last_hidden_state)\n",
    "            all_hidden_states.append(torch.stack(outputs.hidden_states[-4:]))\n",
    "\n",
    "        final_embeddings = torch.cat(all_embeddings, dim=1)\n",
    "        final_mask = chunks_mask.squeeze(0).reshape(-1)\n",
    "        final_mask = final_mask[:final_embeddings.shape[1]]\n",
    "        return final_embeddings, final_mask, torch.stack(all_hidden_states)\n",
    "\n",
    "    def aggregate_embeddings(self, embeddings: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "        if embeddings.dim() == 2:\n",
    "            embeddings = embeddings.unsqueeze(0)\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size())\n",
    "        masked_embeddings = embeddings * mask_expanded\n",
    "        \n",
    "        sum_embeddings = torch.sum(masked_embeddings, dim=1)\n",
    "        valid_tokens = torch.sum(attention_mask, dim=1, keepdim=True)\n",
    "        valid_tokens = torch.clamp(valid_tokens, min=1e-9)\n",
    "        mean_pooled = sum_embeddings / valid_tokens\n",
    "        \n",
    "        masked_embeddings_for_max = masked_embeddings.clone()\n",
    "        masked_embeddings_for_max[~mask_expanded.bool()] = float('-inf')\n",
    "        max_pooled = torch.max(masked_embeddings_for_max, dim=1)[0]\n",
    "        \n",
    "        attention_weights = torch.mean(embeddings, dim=-1)\n",
    "        attention_weights = attention_weights.masked_fill(~attention_mask.bool(), float('-inf'))\n",
    "        attention_scores = softmax(attention_weights, dim=1).unsqueeze(-1)\n",
    "        attention_pooled = torch.sum(attention_scores * embeddings, dim=1)\n",
    "\n",
    "        return {\"mean\": mean_pooled, \"max\": max_pooled, \"attention\": attention_pooled}\n",
    "\n",
    "    def _calculate_statistical_features(self, text: str) -> Dict[str, float]:\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        unique_words = len(set(words))\n",
    "        total_words = max(1, len(words))\n",
    "        total_sentences = max(1, len(sentences))\n",
    "\n",
    "        return {\n",
    "            \"word_count\": float(total_words),\n",
    "            \"sentence_count\": float(total_sentences),\n",
    "            \"avg_word_length\": sum(len(word) for word in words) / total_words,\n",
    "            \"avg_sentence_length\": total_words / total_sentences,\n",
    "            \"lexical_diversity\": unique_words / total_words\n",
    "        }\n",
    "\n",
    "    def _calculate_readability_scores(self, text: str) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": float(textstat.flesch_reading_ease(text)),\n",
    "            \"gunning_fog\": float(textstat.gunning_fog(text)),\n",
    "            \"smog_index\": float(textstat.smog_index(text)),\n",
    "            \"automated_readability_index\": float(textstat.automated_readability_index(text)),\n",
    "            \"dale_chall_score\": float(textstat.dale_chall_readability_score(text)),\n",
    "            \"difficult_words\": float(textstat.difficult_words(text)),\n",
    "            \"linsear_write_formula\": float(textstat.linsear_write_formula(text))\n",
    "        }\n",
    "    def create_weight_vectors(self, total_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        component_sizes = [768, 768, 768, 768, 3072, 5, 7, 20] \n",
    "        weight1 = torch.ones(total_size)\n",
    "        weight2 = torch.ones(total_size)\n",
    "        component_weights_v1 = [1.2, 1.2, 1.2, 1.5, 1.0, 1.0, 1.0, 0.8]\n",
    "        component_weights_v2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0]\n",
    "        start_idx = 0\n",
    "        for i, size in enumerate(component_sizes):\n",
    "            weight1[start_idx:start_idx + size] *= component_weights_v1[i]\n",
    "            weight2[start_idx:start_idx + size] *= component_weights_v2[i]\n",
    "            start_idx += size\n",
    "        return weight1, weight2\n",
    "\n",
    "    def _extract_topics(self, text: str, n_topics: int = 1, num_keywords: int = 20) -> Tuple[torch.Tensor, List[Tuple[str, float]]]:\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\", min_df=1)\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "        text_vectorized = vectorizer.fit_transform([text])\n",
    "        lda.fit(text_vectorized)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        keywords = []\n",
    "        for topic in lda.components_:\n",
    "            top_indices = topic.argsort()[:-(num_keywords):][::-1]\n",
    "            topic_keywords = [(feature_names[i], float(topic[i])) for i in top_indices]\n",
    "            keywords.extend(topic_keywords)\n",
    "        \n",
    "        keywords = sorted(keywords, key=lambda x: x[1], reverse=True)[:num_keywords]\n",
    "        \n",
    "        keywords_embeddings = torch.zeros(num_keywords, device=self.device)\n",
    "        for i, (keyword, weight) in enumerate(keywords):\n",
    "            with torch.no_grad():\n",
    "                keyword_tokens = self.tokenizer(\n",
    "                    keyword, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True\n",
    "                )\n",
    "                input_ids = keyword_tokens[\"input_ids\"].clone().to(self.device)\n",
    "                attention_mask = keyword_tokens[\"attention_mask\"].clone().to(self.device)\n",
    "                \n",
    "                keyword_outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                keyword_embedding = keyword_outputs.last_hidden_state.clone().mean()\n",
    "                keywords_embeddings[i] = keyword_embedding * weight\n",
    "        \n",
    "        keywords_embeddings = keywords_embeddings.detach().clone()\n",
    "        return keywords_embeddings, keywords\n",
    "    \n",
    "    def process_document(self, text: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[Tuple[str, float]]]:\n",
    "        chunks_ids, chunks_mask = self.get_chunks(text)\n",
    "        embeddings, attention_mask, hidden_states = self.process_text(chunks_ids, chunks_mask)\n",
    "        \n",
    "        pooled_embeddings = self.aggregate_embeddings(embeddings, attention_mask)\n",
    "        mean_pooled = pooled_embeddings[\"mean\"]\n",
    "        max_pooled = pooled_embeddings[\"max\"]\n",
    "        attention_pooled = pooled_embeddings[\"attention\"]\n",
    "        cls_embeddings = embeddings[:, 0, :]\n",
    "        layer_wise_embeddings = hidden_states[..., 0, :].mean(dim=0).unsqueeze(0)\n",
    "        \n",
    "        statistical_features = torch.tensor(list(self._calculate_statistical_features(text).values())).to(self.device)\n",
    "        readability_scores = torch.tensor(list(self._calculate_readability_scores(text).values())).to(self.device)\n",
    "        keyword_embeddings, keywords = self._extract_topics(text)\n",
    "        \n",
    "        combined_features = torch.cat([\n",
    "            max_pooled.flatten(),\n",
    "            mean_pooled.flatten(),\n",
    "            attention_pooled.flatten(),\n",
    "            cls_embeddings.flatten(),\n",
    "            layer_wise_embeddings.flatten(),\n",
    "            statistical_features,\n",
    "            readability_scores,\n",
    "            keyword_embeddings.flatten()\n",
    "        ])\n",
    "        print(f\"[INFO] Keyword Embeddings Length : {keyword_embeddings.size()}\")\n",
    "        weight1, weight2 = self.create_weight_vectors(combined_features.size(0))\n",
    "        return combined_features, weight1, weight2, keywords\n",
    "\n",
    "    def process_single_file(self, file_data: tuple) -> Tuple[bool, torch.Tensor, torch.Tensor]:\n",
    "        json_file, input_path, vector_output_path, keywords_output_path, first_vector_size = file_data\n",
    "        try:\n",
    "            relative_path = os.path.relpath(json_file.parent, input_path)\n",
    "            vector_dir = vector_output_path / relative_path\n",
    "            keywords_dir = keywords_output_path / relative_path\n",
    "            \n",
    "            with self.lock:\n",
    "                vector_dir.mkdir(parents=True, exist_ok=True)\n",
    "                keywords_dir.mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"\\n[INFO] Processing file: {json_file}\")\n",
    "\n",
    "            with open(json_file, \"r\") as f:\n",
    "                sections = json.load(f)\n",
    "\n",
    "            text = \"\"\n",
    "            for heading, content in sections.items():\n",
    "                text += f\"{heading}\\n{content}\\n\\n\"\n",
    "\n",
    "            with self.lock:\n",
    "                print(f\"[INFO] Extracting features from text (length: {len(text)} chars)\")\n",
    "            \n",
    "            combined_features, weight1, weight2, keywords = self.process_document(text)\n",
    "            print(f\"[INFO] Combined Features Length: {combined_features.size()}\")\n",
    "\n",
    "            current_size = combined_features.size(0)\n",
    "            if first_vector_size is not None:\n",
    "                assert current_size == first_vector_size, f\"Vector size mismatch: {current_size} vs {first_vector_size}\"\n",
    "\n",
    "            vector_file = vector_dir / f\"{json_file.stem}.pt\"\n",
    "            torch.save(combined_features, vector_file)\n",
    "            keywords_file = keywords_dir / f\"{json_file.stem}.txt\"\n",
    "            with open(keywords_file, 'w', encoding='utf-8') as f:\n",
    "                for keyword, weight in keywords:\n",
    "                    f.write(f\"{keyword}\\n\")\n",
    "            \n",
    "            with self.lock:\n",
    "                print(f\"[SUCCESS] Saved vector features to: {vector_file}\")\n",
    "                print(f\"[SUCCESS] Saved keywords to: {keywords_file}\")\n",
    "            \n",
    "            return True, weight1, weight2\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.lock:\n",
    "                print(f\"[ERROR] Failed to process {json_file}: {str(e)}\")\n",
    "            return False, None, None\n",
    "\n",
    "    def process_json_files(self, input_dir: str, output_dir: str, max_workers: int = 4):\n",
    "        print(f\"[INFO] Starting processing from input directory: {input_dir}\")\n",
    "        print(f\"[INFO] Output will be saved to: {output_dir}\")\n",
    "        print(f\"[INFO] Using {max_workers} worker threads\")\n",
    "\n",
    "        processed_count = 0\n",
    "        total_files = 0\n",
    "        first_vector_size = None\n",
    "        weights_saved = False\n",
    "        all_files = []\n",
    "        for category in [\"publishable\", \"non-publishable\"]:\n",
    "            input_path = Path(input_dir) / category\n",
    "            vector_output_path = Path(output_dir) / \"vectors\" / category\n",
    "            keywords_output_path = Path(output_dir) / \"keywords\" / category\n",
    "            \n",
    "            print(f\"\\n[INFO] Collecting files from category: {category}\")\n",
    "            \n",
    "            for root, dirs, files in os.walk(input_path):\n",
    "                json_files = [Path(root) / f for f in files if f.lower().endswith('.json')]\n",
    "                all_files.extend([(f, input_path, vector_output_path, keywords_output_path, first_vector_size) for f in json_files])\n",
    "                total_files += len(json_files)\n",
    "        \n",
    "        # input_path = Path(input_dir) \n",
    "        # vector_output_path = Path(output_dir) / \"vectors\"\n",
    "        # keywords_output_path = Path(output_dir) / \"keywords\" \n",
    "            \n",
    "        # for root, dirs, files in os.walk(input_path):\n",
    "        #     json_files = [Path(root) / f for f in files if f.lower().endswith('.json')]\n",
    "        #     all_files.extend([(f, input_path, vector_output_path, keywords_output_path, first_vector_size) for f in json_files])\n",
    "        #     total_files += len(json_files)\n",
    "\n",
    "        print(f\"\\n[INFO] Found {total_files} files to process\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for file_data in all_files:\n",
    "                future = executor.submit(self.process_single_file, file_data)\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                success, weight1, weight2 = future.result()\n",
    "                if success:\n",
    "                    processed_count += 1\n",
    "                    if not weights_saved and weight1 is not None and weight2 is not None:\n",
    "                        weight_path1 = Path(output_dir) / \"weight1.pt\"\n",
    "                        weight_path2 = Path(output_dir) / \"weight2.pt\"\n",
    "                        torch.save(weight1, weight_path1)\n",
    "                        torch.save(weight2, weight_path2)\n",
    "                        print(f\"[SUCCESS] Saved weight vectors\")\n",
    "                        weights_saved = True\n",
    "\n",
    "                    print(f\"[PROGRESS] Processed {processed_count}/{total_files} files ({(processed_count/total_files)*100:.1f}%)\")\n",
    "\n",
    "        print(f\"\\n[COMPLETE] Processing finished. Total files processed: {processed_count}/{total_files}\")\n",
    "        if first_vector_size is not None:\n",
    "            print(f\"Vector size for all processed files: {first_vector_size}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"Dataset/texts\"\n",
    "    output_dir = \"Dataset\"\n",
    "    processor = DoraemonProcessor()\n",
    "    processor.process_json_files(input_dir, output_dir, max_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoraemonDataset(Dataset):\n",
    "    def __init__(self, root_dir, weights_path):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        print(f\"\\nLoading weights from {weights_path}\")\n",
    "        self.weights = torch.load(weights_path)\n",
    "        \n",
    "        # Load non-publishable data\n",
    "        non_pub_dir = os.path.join(root_dir, \"non-publishable\")\n",
    "        print(f\"\\nLoading non-publishable data from {non_pub_dir}\")\n",
    "        non_pub_files = [f for f in os.listdir(non_pub_dir) if f.endswith(\".pt\")]\n",
    "        for file in tqdm(non_pub_files, desc=\"Loading non-publishable data\"):\n",
    "            tensor = torch.load(os.path.join(non_pub_dir, file))\n",
    "            tensor = tensor * self.weights\n",
    "            self.features.append(tensor)\n",
    "            self.labels.append(0)\n",
    "        \n",
    "        # Load publishable data\n",
    "        pub_dir = os.path.join(root_dir, \"publishable\")\n",
    "        print(f\"\\nLoading publishable data from {pub_dir}\")\n",
    "        pub_count = 0\n",
    "        for subfolder in os.listdir(pub_dir):\n",
    "            subfolder_path = os.path.join(pub_dir, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                files = [f for f in os.listdir(subfolder_path) if f.endswith(\".pt\")]\n",
    "                for file in tqdm(files, desc=f\"Loading {subfolder}\"):\n",
    "                    tensor = torch.load(os.path.join(subfolder_path, file))\n",
    "                    tensor = tensor * self.weights\n",
    "                    self.features.append(tensor)\n",
    "                    self.labels.append(1)\n",
    "                    pub_count += 1\n",
    "        \n",
    "        self.features = torch.stack(self.features)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "        \n",
    "        print(\"\\nDataset Summary:\")\n",
    "        print(f\"Total samples: {len(self.labels)}\")\n",
    "        print(f\"Non-publishable samples: {len(non_pub_files)}\")\n",
    "        print(f\"Publishable samples: {pub_count}\")\n",
    "        print(f\"Feature dimension: {self.features.shape[1]}\")\n",
    "        print(f\"Class distribution: {torch.bincount(self.labels.long()).tolist()}\")\n",
    "        \n",
    "        # Normalize features\n",
    "        print(\"\\nNormalizing features...\")\n",
    "        self.features = (self.features - self.features.mean(dim=0)) / self.features.std(dim=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class DoraemonBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DoraemonBinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Print model architecture\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"\\nModel Architecture:\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    tp = torch.sum((y_true == 1) & (y_pred == 1)).float()\n",
    "    tn = torch.sum((y_true == 0) & (y_pred == 0)).float()\n",
    "    fp = torch.sum((y_true == 0) & (y_pred == 1)).float()\n",
    "    fn = torch.sum((y_true == 1) & (y_pred == 0)).float()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else torch.tensor(0.0)\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else torch.tensor(0.0)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else torch.tensor(0.0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy.item(),\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1': f1.item(),\n",
    "        'tp': tp.item(),\n",
    "        'tn': tn.item(),\n",
    "        'fp': fp.item(),\n",
    "        'fn': fn.item()\n",
    "    }\n",
    "\n",
    "def create_model(input_dim):\n",
    "    print(f\"\\nCreating model with input dimension: {input_dim}\")\n",
    "    model = DoraemonBinaryClassifier(input_dim)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    return model, loss_fn, optimizer, scheduler\n",
    "\n",
    "def train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, epochs=20, device='cpu'):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\nStarting training for {epochs} epochs\")\n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for X_batch, y_batch in train_pbar:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend((outputs >= 0.5).float().cpu())\n",
    "            train_labels.extend(y_batch.cpu())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_pbar:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(X_val).squeeze()\n",
    "                loss = loss_fn(val_outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_predictions.extend((val_outputs >= 0.5).float().cpu())\n",
    "                val_labels.extend(y_val.cpu())\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_predictions = torch.tensor(train_predictions)\n",
    "        train_labels = torch.tensor(train_labels)\n",
    "        val_predictions = torch.tensor(val_predictions)\n",
    "        val_labels = torch.tensor(val_labels)\n",
    "        \n",
    "        train_metrics = calculate_metrics(train_labels, train_predictions)\n",
    "        val_metrics = calculate_metrics(val_labels, val_predictions)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"Training:\")\n",
    "        print(f\"  Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {train_metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"  Confusion Matrix: [TP: {train_metrics['tp']}, TN: {train_metrics['tn']}, FP: {train_metrics['fp']}, FN: {train_metrics['fn']}]\")\n",
    "        \n",
    "        print(f\"Validation:\")\n",
    "        print(f\"  Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {val_metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"  Confusion Matrix: [TP: {val_metrics['tp']}, TN: {val_metrics['tn']}, FP: {val_metrics['fp']}, FN: {val_metrics['fn']}]\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"\\nNew best model found! Saving checkpoint...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "                'metrics': val_metrics,\n",
    "            }, 'doraemon_binary_classifier.pt')\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "\n",
    "def prepare_data(data_dir, weights_path, train_split=0.8, batch_size=32):\n",
    "    print(f\"\\nPreparing data from {data_dir}\")\n",
    "    print(f\"Train split: {train_split}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    dataset = DoraemonDataset(data_dir, weights_path)\n",
    "    labels = dataset.labels\n",
    "    \n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=(1 - train_split), random_state=42)\n",
    "    train_indices, val_indices = next(stratified_split.split(torch.arange(len(labels)), labels))\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, dataset.features.shape[1]\n",
    "\n",
    "def main():\n",
    "    print(\"\\nStarting binary classification training\")\n",
    "    \n",
    "    data_dir = \"Dataset/vectors\"\n",
    "    weights_path = \"Dataset/weight1.pt\"\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    \n",
    "    train_loader, val_loader, input_dim = prepare_data(data_dir, weights_path, batch_size=batch_size)\n",
    "    \n",
    "    model, loss_fn, optimizer, scheduler = create_model(input_dim)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, \n",
    "                epochs=epochs, device=device)\n",
    "    \n",
    "    print(\"\\nTraining completed. Best model saved as 'doraemon_binary_classifier.pt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoraemonConferenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, weights_path, label_map):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        print(f\"\\nLoading weights from {weights_path}\")\n",
    "        self.weights = torch.load(weights_path)\n",
    "        \n",
    "        total_samples = 0\n",
    "        class_counts = {label: 0 for label in label_map.keys()}\n",
    "        \n",
    "        print(\"\\nLoading conference data...\")\n",
    "        for label, subfolder in label_map.items():\n",
    "            subfolder_path = os.path.join(root_dir, \"publishable\", subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                files = [f for f in os.listdir(subfolder_path) if f.endswith(\".pt\")]\n",
    "                for file in tqdm(files, desc=f\"Loading {subfolder}\"):\n",
    "                    tensor = torch.load(os.path.join(subfolder_path, file))\n",
    "                    tensor = tensor * self.weights\n",
    "                    self.features.append(tensor)\n",
    "                    self.labels.append(label)\n",
    "                    class_counts[label] += 1\n",
    "                    total_samples += 1\n",
    "        \n",
    "        self.features = torch.stack(self.features)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        \n",
    "        print(\"\\nDataset Summary:\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        for label, count in class_counts.items():\n",
    "            print(f\"{label_map[label]}: {count} samples\")\n",
    "        print(f\"Feature dimension: {self.features.shape[1]}\")\n",
    "        \n",
    "        # Normalize features\n",
    "        print(\"\\nNormalizing features...\")\n",
    "        self.features = (self.features - self.features.mean(dim=0)) / (self.features.std(dim=0) + 1e-6)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class DoraemonConferenceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(DoraemonConferenceClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Print model architecture\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"\\nModel Architecture:\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def calculate_multiclass_metrics(y_true, y_pred, num_classes):\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    total = y_true.size(0)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_correct = torch.zeros(num_classes)\n",
    "    class_total = torch.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        mask = (y_true == i)\n",
    "        class_correct[i] = ((y_pred == y_true) & mask).sum().item()\n",
    "        class_total[i] = mask.sum().item()\n",
    "    \n",
    "    class_accuracies = class_correct / (class_total + 1e-6)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'class_accuracies': class_accuracies.tolist(),\n",
    "        'class_counts': class_total.tolist()\n",
    "    }\n",
    "\n",
    "def conference_model(input_dim, num_classes):\n",
    "    print(f\"\\nCreating model with input dimension: {input_dim}\")\n",
    "    model = DoraemonConferenceClassifier(input_dim, num_classes)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    return model, loss_fn, optimizer, scheduler\n",
    "\n",
    "def train_multiclass_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, label_map, epochs=20, device='cpu'):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    num_classes = len(label_map)\n",
    "    \n",
    "    print(f\"\\nStarting training for {epochs} epochs\")\n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for X_batch, y_batch in train_pbar:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(torch.argmax(outputs, dim=1).cpu())\n",
    "            train_labels.extend(y_batch.cpu())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_pbar:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(X_val)\n",
    "                loss = loss_fn(val_outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_predictions.extend(torch.argmax(val_outputs, dim=1).cpu())\n",
    "                val_labels.extend(y_val.cpu())\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_predictions = torch.tensor(train_predictions)\n",
    "        train_labels = torch.tensor(train_labels)\n",
    "        val_predictions = torch.tensor(val_predictions)\n",
    "        val_labels = torch.tensor(val_labels)\n",
    "        \n",
    "        train_metrics = calculate_multiclass_metrics(train_labels, train_predictions, num_classes)\n",
    "        val_metrics = calculate_multiclass_metrics(val_labels, val_predictions, num_classes)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"Training:\")\n",
    "        print(f\"  Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Overall Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "        print(\"  Per-class Accuracies:\")\n",
    "        for i, acc in enumerate(train_metrics['class_accuracies']):\n",
    "            print(f\"    {label_map[i]}: {acc:.4f} ({train_metrics['class_counts'][i]} samples)\")\n",
    "        \n",
    "        print(f\"\\nValidation:\")\n",
    "        print(f\"  Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Overall Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(\"  Per-class Accuracies:\")\n",
    "        for i, acc in enumerate(val_metrics['class_accuracies']):\n",
    "            print(f\"    {label_map[i]}: {acc:.4f} ({val_metrics['class_counts'][i]} samples)\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"\\nNew best model found! Saving checkpoint...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "                'metrics': val_metrics,\n",
    "            }, 'doraemon_conference_classifier.pt')\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "\n",
    "def prepare_multiclass_data(data_dir, weights_path, label_map, train_split=0.8, batch_size=32):\n",
    "    print(f\"\\nPreparing data from {data_dir}\")\n",
    "    print(f\"Train split: {train_split}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    dataset = DoraemonConferenceDataset(data_dir, weights_path, label_map)\n",
    "    labels = [label for _, label in dataset]\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(labels)),\n",
    "        test_size=1 - train_split,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, dataset.features.shape[1]\n",
    "\n",
    "def main():\n",
    "    print(\"\\nStarting conference classification training\")\n",
    "    \n",
    "    data_dir = \"Dataset/vectors\"\n",
    "    weights_path = \"Dataset/weight2.pt\"\n",
    "    label_map = {0: \"CVPR\", 1: \"TMLR\", 2: \"KDD\", 3: \"NEURIPS\", 4: \"EMNLP\"}\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    \n",
    "    train_loader, val_loader, input_dim = prepare_multiclass_data(data_dir, weights_path, label_map, batch_size=batch_size)\n",
    "    \n",
    "    model, loss_fn, optimizer, scheduler = conference_model(input_dim, len(label_map))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    train_multiclass_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, \n",
    "                          label_map, epochs=epochs, device=device)\n",
    "    \n",
    "    print(\"\\nTraining completed. Best model saved as 'doraemon_conference_classifier.pt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from Mistral7b_Instruct import Doraemon_justification\n",
    "from Binary_classification import DoraemonBinaryClassifier\n",
    "from Conference_classification import DoraemonConferenceClassifier\n",
    "\n",
    "def load_model(model, checkpoint_path, device):\n",
    "    \"\"\"Helper function to load model with correct state dict structure\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "def process_saved_data(input_dir: Path, output_dir: Path):\n",
    "    print(\"[INFO] Initializing processing of saved data...\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    text_dir = input_dir / \"texts\"\n",
    "    vector_dir = input_dir / \"vectors\"\n",
    "    keywords_dir = input_dir / \"keywords\"\n",
    "\n",
    "    for dir_path in [text_dir, vector_dir, keywords_dir]:\n",
    "        if not dir_path.exists():\n",
    "            raise ValueError(f\"Directory not found: {dir_path}\")\n",
    "        \n",
    "    vector_files = list(vector_dir.glob(\"*.pt\"))\n",
    "    if not vector_files:\n",
    "        raise ValueError(f\"No vector files found in {vector_dir}\")\n",
    "    print(f\"[INFO] Found {len(vector_files)} files to process\")\n",
    "\n",
    "    sample_vector = torch.load(vector_files[0], map_location=device)\n",
    "    input_dim = sample_vector.shape[0]\n",
    "    print(f\"[INFO] Detected input dimension: {input_dim}\")\n",
    "\n",
    "    try:\n",
    "        binary_classifier = DoraemonBinaryClassifier(input_dim=input_dim).to(device)\n",
    "        conference_classifier = DoraemonConferenceClassifier(input_dim=input_dim, num_classes=5).to(device)\n",
    "        \n",
    "        binary_classifier = load_model(binary_classifier, \"doraemon_binary_classifier.pt\", device)\n",
    "        conference_classifier = load_model(conference_classifier, \"doraemon_conference_classifier.pt\", device)\n",
    "        \n",
    "        binary_classifier.eval()\n",
    "        conference_classifier.eval()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading models: {str(e)}\")\n",
    "\n",
    "    label_map = {0: \"CVPR\", 1: \"TMLR\", 2: \"KDD\", 3: \"NEURIPS\", 4: \"EMNLP\"}\n",
    "    \n",
    "    print(\"[INFO] Loading and processing saved data...\")\n",
    "    features_list = []\n",
    "    file_ids = []\n",
    "    \n",
    "    for vector_file in tqdm(vector_files, desc=\"Loading vectors\"):\n",
    "        try:\n",
    "            features = torch.load(vector_file, map_location=device)\n",
    "            features_list.append(features)\n",
    "            file_ids.append(vector_file.stem)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error loading vector {vector_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(\"[INFO] Computing normalization statistics...\")\n",
    "    all_features = torch.stack(features_list)\n",
    "    feature_mean = all_features.mean(dim=0)\n",
    "    feature_std = all_features.std(dim=0) + 1e-6\n",
    "\n",
    "    print(\"[INFO] Processing with normalized features...\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, file_id in enumerate(tqdm(file_ids, desc=\"Processing files\")):\n",
    "        try:\n",
    "            text_file = text_dir / f\"{file_id}.json\"\n",
    "            keywords_file = keywords_dir / f\"{file_id}.txt\"\n",
    "            \n",
    "            with open(text_file, 'r') as f:\n",
    "                parsed_content = json.load(f)\n",
    "            \n",
    "            with open(keywords_file, 'r') as f:\n",
    "                keywords = f.read().splitlines()\n",
    "\n",
    "            abstract = \"\"\n",
    "            conclusion = \"\"\n",
    "            for heading, content in parsed_content.items():\n",
    "                if 'abstract' in heading.lower() or 'introduction' in heading.lower():\n",
    "                    abstract = content\n",
    "                elif 'conclusion' in heading.lower() or 'summary' in heading.lower():\n",
    "                    conclusion = content\n",
    "\n",
    "            if abstract == \"\" or conclusion == \"\":\n",
    "                for heading, content in parsed_content.items():\n",
    "                    if 'abstract' in content.lower() or 'introduction' in content.lower():\n",
    "                        abstract = content\n",
    "                        break\n",
    "                for heading, content in parsed_content.items():\n",
    "                    if 'conclusion' in content.lower() or 'summary' in content.lower():\n",
    "                        conclusion = content\n",
    "                        break\n",
    "\n",
    "            normalized_features = (features_list[idx] - feature_mean) / feature_std\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                binary_pred = binary_classifier(normalized_features.unsqueeze(0).to(device))\n",
    "                is_publishable = binary_pred.item() > 0.5\n",
    "                \n",
    "                conference = \"na\"\n",
    "                justification = \"na\"\n",
    "                \n",
    "                if is_publishable:\n",
    "                    conf_pred = conference_classifier(normalized_features.unsqueeze(0).to(device))\n",
    "                    conference_id = torch.argmax(conf_pred).item()\n",
    "                    conference = label_map[conference_id]\n",
    "                    \n",
    "                    justification = Doraemon_justification(\n",
    "                        abstract=abstract,\n",
    "                        conclusion=conclusion,\n",
    "                        keywords=keywords,\n",
    "                        conference_name=conference\n",
    "                    )\n",
    "            \n",
    "            results.append([file_id, int(is_publishable), conference, justification])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error processing results for {file_id}: {str(e)}\")\n",
    "            results.append([file_id, 0, 'error', f'Error: {str(e)}'])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Paper ID', 'Publishable', 'Conference', 'Rationale'])\n",
    "    df.to_csv(output_dir / \"results.csv\", index=False)\n",
    "    print(f\"[INFO] Results saved to {output_dir / 'results.csv'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = Path(\"Sample\")\n",
    "    output_dir = Path(\"Sample\")\n",
    "    process_saved_data(input_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
