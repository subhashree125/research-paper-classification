{
  "Abstract": "The increasing prevalence of dynamic characteristics in modern deep learning taskshas led to the growing importance of dynamic shape compilers. These compilersare designed to create effective kernels for dynamic shape graphs, which have astable structure but uncertain tensor shapes. However, memory optimization, whichis vital in the era of large models, has not been thoroughly investigated for dynamicshape graphs. The core issue lies in the absence of specific tensor shapes, which aregenerally required by existing methods like operation scheduling and rematerializa-tion. To overcome this issue, we present operation scheduling and rematerializationstrategies that utilize symbolic shapes, implemented in BladeDISC++. Furthermore,given that rematerialization decisions cannot be determined at compile time alonedue to unknown tensor shapes, BladeDISC++ uses a hybrid approach combiningcompilation and runtime to address shape changes effectively. Our findings demon-strate that BladeDISC++ significantly reduces memory consumption for dynamicshape graphs, achieving levels similar to those of optimizations with precise shapes.This advancement facilitates the broader use of dynamic shape compilers.",
  "Introduction": "Dynamic shape compilers are becoming more and more necessary due to their ability to optimizedeep learning tasks that have dynamic attributes. While advancements in kernel generation have beenmade by systems like TorchInductor and Modular, memory optimization remains a less-explored area.Traditional methods like operation scheduling and rematerialization, which encompass recomputationand offloading, depend on precise tensor shapes to evaluate the memory impact of operations orsubgraphs, and consequently make optimization choices during compilation. However, these methodsbecome impractical when shape values are not available. BladeDISC++, which is based on the dynamic shape compiler BladeDISC, uses symbolic shapesto address these challenges. With symbolic shapes, BladeDISC++ is capable of comparing thememory effects of different operation sequences, and identifying the ideal scheduling order. Forrematerialization, symbolic shapes are used to identify the optimal recomputation subgraph at compiletime, and assist in making final rematerialization decisions during runtime. Our experiments reveal that BladeDISC++ can efficiently reduce memory usage during trainingwith dynamic shape graphs when compared to BladeDISC. Furthermore, BladeDISC++ achievesmemory consumption similar to static shape training while eliminating the overhead associated withrecompilation and tensor padding.",
  "Memory optimizations based on symbolic shapes": "As shown in , BladeDISC++ starts with a dynamic shape computation graph, and proceeds byconducting a symbolic shape analysis to construct a global symbolic shape graph. This graph detailsthe mathematical connections between the shape symbols, which will be discussed in section 2.1.Following this, the symbolic shape graph, along with the computation graph, is optimized through",
  "steps that include operation fusion, operation scheduling, and rematerialization. These steps areaimed at memory usage reduction": "As previous work on BladeDISC has addressed operation fusion, this paper focuses on operationscheduling, which will be discussed in section 2.2, and rematerialization, which will be discussedin section 2.3. Using the symbolic shape graph instead of exact tensor shapes, BladeDISC++ canstill compare the memory usage of different operation sequences and determine the benefit ofrecomputation subgraphs. Moreover, because the memory needs of a dynamic shape graph canfluctuate between different runs, it is not practical to base rematerialization decisions, such as howmuch memory to free, solely on compile time. Consequently, BladeDISC++ investigates all possiblerematerialization options, searches for the corresponding regeneration subgraphs, and makes finalrematerialization decisions during runtime.",
  "Symbolic shape graph analysis": "BladeDISC++ systematically analyzes and obtains shape information from the semantics of eachoperation within the dynamic shape computation graph. Following this, it establishes a globalsymbolic shape graph. This graph is designed to show the mathematical relationships between shapedimensions through shape value extraction and input-output shape inference. func . func @main (% arg0 : tensor <? ,[ @S0 ] > , % arg1 : tensor <12 x11008 >) {%1 = broadcast (% arg1 ) -> tensor <4096 x ? , [ @C4096 , @S0 ] >%2 = d yna mi c_r eshape (% arg0 , % new_shape ) -> tensor <? x12 ,[ @S1 , @C12 ] >// The last consumer of %2%3 = dot (%2 , % arg1 ) -> tensor <? x11008 , [ @S1 , @C11008 ] >// The last consumer of %3%4 = reduce (%3) -> tensor <? , [ @S1 ] >%1084 = broadcast (%4) -> tensor <11008 x ? , [ @C11008 , @S1 ] >%1085 = broadcast (% arg0 ) -> tensor <1024 x ? , [ @C1024 , @S0 ] >}",
  "Listing 1: Example of a dynamic shape graph and its symbolic shape graph": "As shown in Listing 1, BladeDISC++ uses a SymbolicDim operation to represent a symbolic value.This value is linked to a dimension of a tensor shape in the dynamic shape graph as an attribute, forexample, tensor<?x?, [@S0, @S1]>. The equation @S0 = 12 * @S1, for instance, is derived from aDynamicReshapeOp. It means the input and output tensors have an equivalent number of elements. The comparison of tensor memory sizes is vital for both operation scheduling and rematerialization.BladeDISC++ uses SymbolicExpr to show mathematical expressions of symbolic dimensions. Thisallows for comparisons using a best-effort approach. For example, the element count of tensors",
  "Operation scheduling": "Operation scheduling aims to discover a memory-efficient sequence of operations from the initialcomputation graph. Existing scheduling algorithms typically traverse the graph and select an operationfrom a ReadySet, which includes operations whose predecessors have been scheduled, at each step.The selection is mainly based on a comparison of the memory impact of the different operations,which is determined by calculating the difference between the memory freed and the memory allocatedafter scheduling a particular operation. BladeDISC++ employs a similar strategy, emphasizing thecalculation and comparison of memory impact among different operations when exact tensor shapesare unavailable in dynamic shape graphs. In BladeDISC++, the memory impact of each operation",
  "Rematerialization": "Traditional rematerialization methods use algorithms to decide which tensors to release early to reducememory pressure, and how to conduct the following regeneration via reloading or recomputation.These methods also search for optimal recomputation subgraphs, evaluating their memory effects.Tensor rematerialization can negatively impact end-to-end performance, so it should only be usedwhen the graphs execution could exceed memory limits. However, dynamic shape graphs, withuncertain tensor shapes, may show varied peak memory use between different runs. Some runs maynot need rematerialization as they remain within memory limits, whereas others may. Therefore, it isimpractical to make decisions solely at compilation time. Also, the absence of exact shapes presentschallenges in evaluating the memory effects of potential recomputation subgraphs. To address these challenges, BladeDISC++ uses a combined compilation-runtime approach based onsymbolic shapes to better manage shape variations during graph runs. At compile time, it explores allpossible rematerialization candidates and identifies the regeneration subgraphs associated with them.These subgraphs are incorporated into the original computation graph as separate execution paths.Final choices regarding which tensor to release and the related regeneration method are made duringruntime. During compilation, as shown in , BladeDISC++ adds a Remat::EvictOp after each operation.This checks if active tensors at that point need to be released to lower memory pressure. Regenerationsubgraphs, including reload and recomputation, are created for each potential tensor. While reloadingonly involves a host-to-device instruction and has no impact on memory, finding recomputationsubgraphs needs thorough evaluation as poor choices can increase peak memory consumption.BladeDISC++ uses a standard search approach, but assesses the memory impact of subgraphs usingSymbolicExpr.",
  "Taking the recomputation subgraph searching for": "Following this, BladeDISC++ inserts Remat::RegenerateOps, with corresponding regeneration sub-graphs for both reload and recompute. These are inserted before each potential tensors subsequentconsumers. The Remat::RegenerateOp checks if a tensor has been released, and which regenerationmethod is being used. During runtime, BladeDISC++ monitors memory usage throughout kernel execution. Whenever anEvictOp is triggered, BladeDISC++ checks the present memory usage. When the memory limit isabout to be exceeded, it performs a real-time analysis of all potential tensors offered by the EvictOp.Final decisions about which tensor needs to be released, and the regeneration method, are determinedby taking memory savings and end-to-end performance into account, following a similar approach asdetailed in. Subsequent Remat::RegenerateOps then check these choices to decide which regenerationsubgraphs to trigger.",
  "Evaluation": "For our evaluation, we performed experiments on the supervised fine-tuning of Llama-2-1b, which isa customized model from the official Llama-2-7b with only the number of hidden layers decreasedfrom 32 to 4. This was done on an Alibaba Cloud instance, with 40GB of GPU RAM. We usedthe CodeAlpaca-20K dataset, which contains text samples with lengths from about 100 to 3000characters. During each training cycle, a fixed amount of randomly selected samples are put into abatch. This leads to variations in batch shapes between cycles.",
  "To evaluate the effectiveness of BladeDISC++, we compared memory usage and end-to-end per-formance of dynamic shape training with BladeDISC++ against both dynamic and static shape": "training with BladeDISC. For static shape training, following common methods, input sequences arepadded to the closest power of 2 in length. This balances redundant computation and compilationoverhead. Additionally, we set the largest bucket size to be equal to the longest sequence length inthe dataset. This was done to investigate whether comparable memory optimization can be achievedusing symbolic shapes instead of exact shapes. The experimental results show that BladeDISC++ is able to reduce peak memory consumptionduring dynamic shape training. BladeDISC++ also demonstrated memory consumption similarto static shape training, while improving end-to-end performance by eliminating the overheads ofrecompilation and input bucketing.",
  "Conclusion": "This study presents our practical experience in optimizing memory for dynamic shape graphs. Wehave introduced operation scheduling and rematerialization strategies that use symbolic shapes,implemented in BladeDISC++. Evaluations demonstrate that BladeDISC++ effectively decreasesmemory usage for dynamic shape training and can match the memory optimization results of staticshape training. To the best of our knowledge, this work is the first attempt in this area. We hopeit will support the compiler community in handling dynamic shape tasks, and increase the use ofdynamic shape compilers."
}