{
  "Abstract": "This paper aims to highlight vision related tasks centered around car, which hasbeen largely neglected by vision community in comparison to other objects. Weshow that there are still many interesting car-related problems and applications,which are not yet well explored and researched. To facilitate future car-relatedresearch, in this paper we present our on-going effort in collecting a large-scaledataset, CompCars, that covers not only different car views, but also their dif-ferent internal and external parts, and rich attributes. Importantly, the dataset isconstructed with a cross-modality nature, containing a surveillance- nature set anda web-nature set. We further demonstrate a few important applications exploitingthe dataset, namely car model classification, car model verification, and attributeprediction. We also discuss specific challenges of the car-related problems andother potential applications that worth further investigations.** Update: This technical report serves as an extension to our earlier work publishedin CVPR 2015. The experiments shown in Sec. 5 gain better performance onall three tasks, i.e. car model classification, attribute prediction, and car modelverification, thanks to more training data and better network structures. Theexperimental results can serve as baselines in any later research works. The settingsand the train/test splits are provided on the project page.** Update 2: This update provides preliminary experiment results for fine-grainedclassification on the surveillance data of CompCars. The train/test splits areprovided in the updated dataset. See details in .",
  "Introduction": "Cars represent a revolution in mobility and convenience, bringing us the flexibility of moving fromplace to place. The societal benefits (and cost) are far-reaching. Cars are now indispensable from ourmodern life as a vehicle for transportation. In many places, the car is also viewed as a tool to helpproject someones economic status, or reflects our economic stratification. In addition, the car hasevolved into a subject of interest amongst many car enthusiasts in the world. In general, the demandon car has shifted over the years to cover not only practicality and reliability, but also high comfortand design. The enormous number of car designs and car model makes car a rich object class, whichcan potentially foster more sophisticated and robust computer vision models and algorithms. Cars present several unique properties that other objects cannot offer, which provides more challengesand facilitates a range of novel research topics in object categorization. Specifically, cars own largequantity of models that most other categories do not have, enabling a more challenging fine-grainedtask. In addition, cars yield large appearance differences in their unconstrained poses, which demandsviewpoint-aware analyses and algorithms (see (b)). Importantly, a unique hierarchy is presentedfor the car category, which is three levels from top to bottom: make, model, and released year.This structure indicates a direction to address the fine-grained task in a hierarchical way, which isonly discussed by limited literature. Apart from the categorization task, cars reveal a number ofinteresting computer vision problems. Firstly, different designing styles are applied by differentcar manufacturers and in different years, which opens the door to fine-grained style analysis and",
  "Related Work": "Most previous car model research focuses on car model classification. propose an evolutionarycomputing framework to fit a wireframe model to the car on an image. Then the wireframe model isemployed for car model recognition. construct 3D space curves using 2D training images, then matchthe 3D curves to 2D image curves using a 3D view-based alignment technique. The car model isfinally determined with the alignment result. optimize 3D model fitting and fine-grained classificationjointly. All these works are restricted to a small number of car models. Recently, propose to extract3D car representation for classifying 196 car models. The experiment is the largest scale that weare aware of. Car model classification is a fine-grained categorization task. In contrast to generalobject classification, fine-grained categorization targets at recognizing the subcategories in one objectclass. Fol- lowing this line of research, many studies have proposed different datasets on a varietyof categories: birds, dogs, cars, flowers, etc. But all these datasets are limited by their scales andsubcategory numbers. To our knowledge, there is no previous attempt on the car model verification task. Closely related tocar model verification, face verification has been a popular topic. The recent deep learning basedalgorithms first train a deep neural network on human identity clas- sification, then train a verification model with the feature extracted from the deep neural network. Joint Bayesian is a widely-usedverification model that models two faces jointly with an appropriate prior on the face representation.We adopt Joint Bayesian as a baseline model in car model verification. Attribute prediction of humans is a popular research topic in recent years. However, a large portionof the labeled attributes in the current attribute datasets, such as long hair and short pants lack strictcriteria, which causes annotation ambiguities. The attributes with ambiguities will potentially harmthe effectiveness of evaluation on related datasets. In contrast, the attributes provided by CompCars(e.g. maximum speed, door number, seat capacity) all have strict criteria since they are set by the carmanufacturers. The dataset is thus advantageous over the current datasets in terms of the attributesvalidity. Other car-related research includes detection, track- ing, joint detection and pose estimation, and 3Dparsing. Fine-grained car models are not explored in these studies. Previous research related to carparts includes car logo recognition and car style analysis based on mid-level features. Similar to CompCars, the Cars dataset also targets at fine-grained tasks on the car category. Apartfrom the larger-scale database, our CompCars dataset offers several significant benefits in comparisonto the Cars dataset. First, our dataset contains car images diversely distributed in all viewpoints(annotated by front, rear, side, front-side, and rear-side), while Cars dataset mostly consists of front-side car images. Second, our dataset contains aligned car part images, which can be utilized for manycomputer vision algorithms that demand precise alignment. Third, our dataset provides rich attributeannotations for each car model, which are absent in the Cars dataset.",
  "Properties of CompCars": "The CompCars dataset contains data from two scenarios, including images from web-nature andsurveillance-nature. The images of the web-nature are collected from car forums, public websites,and search engines. The images of the surveillance-nature are collected by surveillance cameras. Thedata of these two scenarios are widely used in the real-world applications. They open the door forcross-modality analysis of cars. In particular, the web-nature data contains 163 car makes with 1, 716car models, covering most of the commercial car models in the recent ten years. There are a total of136, 727 images capturing the entire cars and 27, 618 images capturing the car parts, where mostof them are labeled with attributes and viewpoints. The surveillance-nature data contains 44, 481car images captured in the front view. Each image in the surveillance-nature partition is annotatedwith bounding box, model, and color of the car. illustrates some examples of surveillanceimages, which are affected by large variations from lightings and haze. Note that the data from thesurveillance-nature are significantly different from the web-nature data in , suggesting the greatchallenges in cross-scenario car analysis. Overall, CompCars dataset offers four unique features incomparison to existing car image databases, namely car hierarchy, car attributes, viewpoints, and carparts. the Car Hierarchy The car models can be organized into a large tree structure, consisting of three layers, namely car make, car model, and year of manufacture, top to bottom as depicted in . Thecomplexity is further compounded by the fact that each car model can be produced in different years,yielding subtle difference in their appearances. For instance, three versions of Audi A4L wereproduced between 2009 to 2011 respectively. from Car Attributes Each car model is labeled with five at- tributes, including maximum speed, displace-ment, number of doors, number of seats, and type of car. These attributes provide rich informationwhile learning the relations or similarities between different car models. For example, we definetwelve types of cars, which are MPV, SUV, hatchback, sedan, minibus, fastback, estate, pickup, sports,crossover, convertible, and hardtop convertible, as shown in . Furthermore, these attributescan be partitioned into two groups: explicit and implicit attributes. The former group contains doornumber, seat number, and car type, which are represented by discrete values, while the latter groupcontains maximum speed and displacement (volume of an engines cylinders), represented by contin-uous values. Humans can easily tell the numbers of doors and seats from a cars proper viewpoint,but hardly recognize its maximum speed and displacement. We conduct interesting experiments topredict these attributes in .2. Viewpoints We also label five viewpoints for each car model, including front (F), rear (R), side (S),front-side (FS), and rear-side (RS). These viewpoints are labeled by several professional annotators.The quantity distribution of the labeled car images is shown in . Note that the numbers ofviewpoint images are not balanced among different car models, because the images of some lesspopular car models are difficult to collect. Car Parts We collect images capturing the eight car parts for each car model, including four exteriorparts (i.e. headlight, taillight, fog light, and air intake) and four interior parts (i.e. console, steeringwheel, dashboard, and gear lever). These images are roughly aligned for the convenience of furtheranalysis. A summary and some examples are given in and respectively.",
  "Applications": "In this section, we study three applications using CompCars, including fine-grained car classification,attribute prediction, and car verification. We select 78, 126 images from the CompCars dataset anddivide them into three subsets without overlaps. The first subset (Part-I) contains 431 car models witha total of 30, 955 images capturing the entire car and 20, 349 images capturing car parts. The secondsubset (Part-II) consists 111 models with 4, 454 images in total. The last subset (Part-III) contains 1,145 car models with 22, 236 images. Fine-grained car classification is conducted using images in thefirst subset. For attribute prediction, the models are trained on the first subset but tested on the secondone. The last subset is utilized for car verification. We investigate the above potential applications using Convolutional Neural Network (CNN), whichachieves great empirical successes in many computer vision prob- lems, such as object classification,detection, face alignment, and face verification. Specifically, we employ the Overfeat model, whichis pretrained on ImageNet classification task, and fine-tuned with the car images for car classificationand attribute prediction. For car model verification, the fine-tuned model is employed as a featureextractor.",
  "Fine-Grained Classification": "We classify the car images into 431 car models. For each car model, the car images produced indifferent years are considered as a single category. One may treat them as different categories, leadingto a more challenging problem because their differences are relatively small. Our experiments havetwo settings, comprising fine-grained classification with the entire car images and the car parts. Forboth settings, we divide the data into half for training and another half for testing. Car model labelsare regarded as training target and logistic loss is used to fine-tune the Overfeat model.",
  "The Entire Car Images": "We compare the recognition performances of the CNN models, which are fine-tuned with car imagesin specific viewpoints and all the viewpoints respectively, denoted as front (F), rear (R), side(S), front-side (FS), rear- side (RS), and All-View. The performances of these six models aresummarized in , where FS and RS achieve better performances than the performancesof the other viewpoint models. Surprisingly, the All- View model yields the best performance,although it did not leverage the information of viewpoints. This result reveals that the CNN model iscapable of learning discriminative representation across different views. To verify this observation,we visualize the car images that trigger high responses with respect to each neuron in the last fully-connected layer. As shown in , these neurons capture car images of specific car models acrossdifferent viewpoints. Several challenging cases are given in , where the images on the left hand side are the testingimages and the images on the right hand side are the examples of the wrong predictions (of theAll-View model). We found that most of the wrong predictions belong to the same car makes as thetest images. We report the top- 1 accuracies of car make classification in the last row of ,where the All-View model obtain reasonable good result, indicating that a coarse-to-fine (i.e. fromcar make to model) classification is possible for fine-grained car recognition. To observe the learned feature space of the All-View model, we project the features extractedfrom the last fully- connected layer to a two-dimensional embedding space using multi-dimensionalscaling. visualizes the projected features of twelve car models, where the images are chosenfrom different viewpoints. We observe that features from different models are separable in the 2Dspace and features of similar models are closer than those of dissimilar models. For instance, thedistances between BWM 5 Series and BWM 7 Series are smaller than those between BWM 5Series and Chevrolet Captiva. We also conduct a cross-modality experiment, where the CNN model fine-tuned by the web-naturedata is evaluated on the surveillance-nature data. illustrates some predictions, suggesting thatthe model may account for data variations in a different modality to a certain extent. This experimentindicates that the features obtained from the web-nature data have potential to be transferred to datain the other scenario. : Fine-grained classification results for the models trained on car images. Top-1 and Top-5denote the top-1 and top-5 accuracy for car model classification, respectively. Make denotes the makelevel classification accuracy.",
  "Car Parts": "Car enthusiasts are able to distinguish car models by examining the car parts. We investigate ifthe CNN model can mimic this strength. We train a CNN model using images from each of theeight car parts. The results are reported in , where taillight demonstrates the best accuracy.We visualize taillight images that have high responses with respect to each neuron in the last fully-connected layer. displays such images with respect to two neurons. Taillight wins amongthe different car parts, mostly likely due to the relatively more distinctive designs, and the modelname printed close to the taillight, which is a very informative feature for the CNN model.",
  "Top-10.4790.6840.3870.4840.5350.5400.5020.3550.808Top-50.6900.8590.5660.6950.7450.7730.7360.5890.927": "provides sufficient information of the door number and car type, but it is hard to infer these attributesfrom the frontal view. The appearance of a car also provides hints on the implicit attributes, suchas the maximum speed and the displacement. For instance, a car model is probably designed forhigh-speed driving, if it has a low under-pan and a streamline body. In this section, we deliberately design a challenging experimental setting for attribute recognition,where the car models presented in the test images are exclusive from the training images. We fine-tunethe CNN with the sum- of-square loss to model the continuous attributes, such as maximum speedand displacement, but a logistic loss to predict the discrete attributes such as door number, seatnumber, and car type. For example, the door number has four states, i.e. 2, 3, 4, 5 doors, whileseat number also has four states, i.e. 2, 4, 5, > 5 seats. The attribute car type has twelve states asdiscussed in Sec. 3. To study the effectiveness of different viewpoints for attribute prediction, we train CNN models fordifferent viewpoints separately. summarizes the results, where the mean guess representsthe errors computed by using the mean of the training set as the prediction. We observe that theperformances of maximum speed and displacement are insensitive to viewpoints. However, forthe explicit attributes, the best accuracy is obtained under side view. We also found that the theimplicit attributes are more difficult to predict then the explicit attributes. Several test images andtheir attribute predictions are provided in . : Attribute prediction results for the five single viewpoint models. For the continuous attributes(maximum speed and displacement), we display the mean difference from the ground truth. For thediscrete attributes (door and seat number, car type), we display the classification accuracy. Meanguess denotes the mean error with a prediction of the mean value on the training set.",
  "Car Verification": "The evaluation pipeline follows .3. We evaluate the three deep models combined with twoverification models: Joint Bayesian and SVM with polynomial kernel. The feature extracted from theCNN models is reduced to 200 by PCA before training and testing in all experiments. The performances of the three networks combined with the two verification models are shown in, where each model is denoted by name of the deep model + name of the verification model.GoogLeNet + Joint Bayesian achieves the best performance in all three settings. For each deep model,Joint Bayesian outperforms SVM consistently. Compared to , Overfeat + Joint Bayesianyields a performance gain of 2 4",
  "P(x1, x2|HE),(4)": "which has closed-form solution. The feature extracted from the CNN model has a dimension of 4,096, which is reduced to 20 by PCA. The compressed features are then utilized to train the JointBayesian model. During the testing stage, each image pair is classified by comparing the likelihoodratio produced by Joint Bayesian with a threshold. This model is denoted as (CNN feature + JointBayesian). The second method combines the CNN features and SVM, denoted as CNN feature + SVM. Here,SVM is a binary classifier using a pair of image features as input. The label 1 represents positivepair, while 0 represents negative pair. We extract 100, 000 pairs of image features from Part-II datafor training. The performances of the two models are shown in and the ROC curves for the hard setare plotted in . We observe that CNN feature + Joint Bayesian outperforms CNN feature+ SVM with large margins, indicating the advantage of Joint Bayesian for this task. However, itsbenefit in car verification is not as effective as in face verification, where CNN and Joint Bayesiannearly saturated the LFW dataset and approached human performance. depicts several pairsof test images as well as their predictions by CNN feature + Joint Bayesian. We observe two majorchallenges. First, for the image pair of the same model but different viewpoints, it is difficult toobtain the correspondences directly from the raw image pixels. Second, the appearances of differentcar models of the same car make are extremely similar. It is difficult to distinguish these car modelsusing the entire images. Part localization or detection is crucial for car verification.",
  "Updated Results: Comparing Different Deep Models": "As an extension to the experiments in , we conduct experiments for fine-grained carclassification, at- tribute prediction, and car verification with the entire dataset and different deepmodels, in order to explore the different capabilities of the models on these tasks. The split of thedataset into the three tasks is similar to , where three subsets contain 431, 111, and 1, 145car models, with 52, 083, 11, 129, and 72, 962 images respectively. The only difference is that weadopt full set of CompCars in order to establish updated baseline experiments and to make use of thedataset to the largest extent. We keep the testing sets of car verification same to those in .3. We evaluate three network structures, namely AlexNet, Overfeat, and GoogLeNet for all three tasks.All networks are pre-trained on the ImageNet classification task, and fine-tuned with the samemini-batch size, epochs, and learning rates for each task. All predictions of the deep models areproduced with a single center crop of the image. We use Caffe as the platform for our experiments.",
  "Attribute Prediction": "We predict attributes from 111 models not existed in the training set. Different from .2where models are trained with cars in single viewpoints, we train with images in all viewpoints tobuild a compact model. summarizes the results for the three networks, where mean guessrepresents the prediction with the mean of the values on the training set. GoogLeNet performs thebest for all attributes and Overfeat is a close running-up."
}