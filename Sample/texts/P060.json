{
  "Abstract": "Recent work on background subtraction has shown developments on two majorfronts. In one, there has been increasing sophistication of probabilistic models,from mixtures of Gaussians at each pixel, to kernel density estimates at eachpixel, and more recently to joint domain-range density estimates that incorporatespatial information. Another line of work has shown the benefits of increasinglycomplex feature representations, including the use of texture information, localbinary patterns, and recently scale-invariant local ternary patterns. In this work, weuse joint domain-range based estimates for background and foreground scores andshow that dynamically choosing kernel variances in our kernel estimates at eachindividual pixel can significantly improve results. We give a heuristic method forselectively applying the adaptive kernel calculations which is nearly as accurate asthe full procedure but runs much faster. We combine these modeling improvementswith recently developed complex features and show significant improvements on astandard backgrounding benchmark.",
  "Introduction": "Background modeling is often an important step in detecting moving objects in video sequences. Acommon approach to background modeling is to define and learn a background distribution overfeature values at each pixel location and then classify each image pixel as belonging to the backgroundprocess or not. The distributions at each pixel may be modeled in a parametric manner using a mixtureof Gaussians or using non-parametric kernel density estimation. More recently, models that allowa pixels spatial neighbors to influence its distribution have been developed by joint domain-rangedensity estimation. These models that allow spatial influence from neighboring pixels have beenshown to perform better than earlier neighbor-independent models. Also, the use of an explicit foreground model along with a background model can be useful. In amanner similar to theirs, we use a kernel estimate to obtain the background and foreground scoresat each pixel location using data samples from a spatial neighborhood around that location fromprevious frames. The background score is computed as a kernel estimate depending on the distancein the joint domain-range space between the estimation point and the samples in the backgroundmodel. A similar estimate is obtained for the foreground score. Each pixel is then assigned a (soft)label based on the ratio of the background and foreground scores. The variance used in the estimation kernel reflects the spatial and appearance uncertainties in thescene. On applying our method to a data set with wide variations across the videos, we found thatchoosing suitable kernel variances during the estimation process is very important. With variousexperiments, we establish that the best kernel variance could vary for different videos and moreimportantly, even within a single video, different regions in the image should be treated with differentvariance values. For example, in a scene with a steady tree trunk and leaves that are waving in thewind, the trunk region can be explained with a small amount of spatial variance. The leaf regionsmay be better explained by a process with a large variance. Interestingly, when there is no wind, theleaf regions may also be explained with a low variance. The optimal variance hence changes for",
  "Background and foreground models": "In a video captured by a static camera, the pixel values are influenced by the background phenomenon,and new or existing foreground objects. We refer to any phenomenon that can affect image pixelvalues as a process. Like , we model the background and foreground processes using data samplesfrom previous frames. The scores for the background and foreground processes at each pixel locationare calculated using contributions from the data samples in each model. One major difference betweenand our model is that we allow soft labeling, i.e. the data samples contribute probabilistically to thebackground score depending on the samples probability of belonging to the background. Let a pixel sample a = [ax, ay, ar, ag, ab], where (ax, ay) are the location of the pixel and (ar, ag, ab)are the red, green, and blue values of the pixel. In each frame of the video, we compute backgroundand foreground scores using pixel samples from the previous frames. The background model consistsof the samples B = bi : i [1 : nB] and foreground samples are F = fi : i [1 : nF ], with nB and nF beingthe number of background and foreground samples respectively, and bi and fi being pixel samplesobtained from previous frames in the video. Under a KDE model, the likelihood of the sample underthe background model is",
  "i=1G(argb birgb; rgbB ) G(axy bixy; dB) P(bg|bi)(3)": "NB is the number of frames from which the background samples have been collected, B d and Brgb are two and three dimensional background covariance matrices in spatial and color dimensionsrespectively. A large spatial covariance allows neighboring pixels to contribute more to the score at agiven pixel location. Color covariance allows for some color appearance changes at a given pixellocation. Use of NB in the denominator compensates for the different lengths of the background andforeground models. The above equation basically sums the contribution from each background sample based on itsdistance in color space, weighted by its distance in spatial dimensions and the probability of thesample belonging to the background. The use of P (bg|bi) in Equation 3 and normalization by the number of frames as opposed to thenumber of samples means that the score does not sum to 1 over all possible values of a. Thus, thescore, although similar to the likelihood in Equation 1, is not a probability distribution.",
  "NF is the number of frames from which the foreground samples have been collected, F d and F rgbare the covariances associated with the foreground process": "However, for the foreground process, to account for emergence of new colors in the scene, we mixin a constant contribution independent of the estimation points and data samples color values. Weassume that each data sample in a pixels spatial neighborhood contributes a constant value u to theforeground score. The constant contribution UF (a) is given by",
  "P(fg|a) = 1 P(bg|a).(8)": "Adding the constant factor U to the foreground score (and hence to the denominator of the Bayes-likeequation) has the interesting property that when either one of the foreground or background scores issignificantly larger than U , U has little effect on the classification. However, if both the backgroundand foreground scores are less than U , then Equation 7 will return a low value as P (bg|a). Hence,an observation that has very low background and foreground scores will be classified as foreground.This is desirable because if a pixel observation is not well explained by either model, it is natural toassume that the pixel is a result of a new object in the scene and is hence foreground. In terms oflikelihoods, adding the constant factor to the foreground likelihood is akin to mixing it with a uniformdistribution.",
  "Model initialization and update": "To initialize the models, it is assumed that the first few frames (typically 50) are all background pixels.The background model is populated using pixel samples from these frames. In order to improveefficiency, we sample 5 frames at equal time intervals from these 50 frames. The foreground model isinitialized to have no samples. The modified foreground score (Equation 6) enables colors that arenot well explained by the background model to be classified as foreground, thus bootstrapping theforeground model. Once the pixel at location (ax, ay) from a new frame is classified using Equation7, the background and foreground models at the location (ax, ay) can then be updated with the newsample a. Background and foreground samples at location (ax, ay) from the oldest frame in themodels are replaced by a. Samples from the previous 5 frames are maintained in memory as theforeground model samples. The label probabilities of the background/foreground from Equation 7are also saved along with the sample values for subsequent use in the Equations 3 and 4. One consequence of the update procedure described above is that when a large foreground objectoccludes a background pixel at (ax, ay) for more than 50 frames, all the background samples in thespatial neighborhood of (ax, ay) are replaced by these foreground samples that have very low P (bg|bi)values. This causes the pixel at (ax, ay) to be misclassified as foreground even when the occludingforeground object has moved away (because the background score will be extremely low due to theinfluence of P (bg|bi) in Equation 3). To avoid this problem, we replace the background sample fromlocation (ax, ay) in the oldest frame in the background model with the new sample a from the currentframe only if P (bg|a) estimated from Equation 7 is greater than 0.5. In our chosen evaluation data set, there are several videos with moving objects in the first 50 frames.The assumption that all these pixels are background is not severely limiting even in these videos.The model update procedure allows us to recover from any errors that are caused by the presence offoreground objects in the initialization frames.",
  "Pixel-wise adaptive kernel variance selection": "Background and foreground kernels. use the same kernel parameters for background and foregroundmodels. Given the different nature of the two processes, it is reasonable to use different kernelparameters. For instance, foreground objects typically move between 5 and 10 pixels per frame in thedata set, whereas background pixels are either stationary or move very little. Hence, it is useful tohave a larger spatial variance for the foreground model than for the background model. Optimal kernel variance for all videos. In the results section, we show that for a data set withlarge variations like , a single value for kernel variance for all videos is not sufficient to capture thevariability in all the videos. Variable kernel variance for a single video. As explained in the introduction, different parts of thescene may have different statistics and hence need different kernel variance values. For example, ina to 1d, having a high spatial dimension kernel variance helps in accurate classification of the water surface pixels, but doing so causes some pixels on the persons leg to become part of thebackground. Ideally, we would have different kernel variances for the water surface pixels and the restof the pixels. Similarly in the second video (e to 1h), having a high kernel variance allowsaccurate classification of some of the fountain pixels as background at the cost of misclassifyingmany foreground pixels. The figure also shows that while the medium kernel variance may be thebest choice for the first video, the low kernel variance may be best for the second video. Optimal kernel variance for classification. Having different variances for the background andforeground models reflects the differences between the expected uncertainty in the two processes.However, having different variances for the two processes could cause erroneous classification ofpixels. shows a 1-dimensional example where using a very wide kernel (high variance)or very narrow kernel for the background process causes misclassification. Assuming that the redpoint (square) is a background sample and the blue point (triangle) is a foreground sample, having avery low variance kernel (dashed red line) or a very high variance (solid red line) for the backgroundprocess makes the background likelihood of the center point x lower than the foreground likelihood.Thus, it is important to pick the optimal kernel variance for each process during classification. In order to address all four issues discussed above, we propose the use of location-specific variances.For each location in the image, a range of kernel variances is tried and the variance which results inthe highest score is chosen for the background and the foreground models separately.",
  "Here, B RB d and B rgb. RB d and RB rgb,ax,ay d,ax,ay rgb represent the set of spatial and colordimension variances from which to choose the optimal variance": "A similar procedure may be followed for the foreground score. However, in practice, it was foundthat the variance selection procedure yielded large improvements when applied to the backgroundmodel and little improvement in the foreground model. Hence, our final implementation uses anadaptive kernel variance procedure for the background model and a fixed kernel variance for theforeground model.",
  "Results": "For comparisons, we use the data set which consists of 9 videos taken using a static camera invarious environments. The data set offers various challenges including dynamic background like treesand waves, gradual and sudden illumination changes, and the presence of multiple moving objects.Ground truth for 20 frames in each video is provided with the data set. The F-measure is used tomeasure accuracy. The effect of choosing various kernel widths for the background and foreground models is shown in. The table shows the F-measure for each of the videos in the data set for various choices ofthe kernel variances. The first 5 columns correspond to using a constant variance for each process atall pixel locations in the video. Having identical kernel variances for the background and foregroundmodels (columns 1, 2) is not as effective as having different variances (all other columns). Comparingcolumns 2 and 3 shows that using a larger spatial variance for the foreground model than for thebackground model is beneficial. Changing the spatial variance from 3 (column 3) to 1 (column 4)helps the overall accuracy in one video (Fountain). Using a selection procedure where the best kernelvariance is chosen from a set of values gives the best results for most videos (column 6) and frames.",
  "Comparison of our selection procedure to a baseline method of using a standard algorithm for varianceselection in KDE (AMISE criterion) shows that the standard algorithm is not as accurate as our": "method (column 7). Our choice for the variance values for spatial dimension reflects no motion (B d= 1/4) and very little motion (B d = 3/4) for the background, and moderate amount of motion (F d= 12/4) for the foreground. For the color dimension, the choice is between little variation (B rgb=5/4), moderate variation (B rgb= 15/4), and high variation (B rgb= 45/4) for the background, andmoderate variation (F rgb= 15/4) for the foreground. These choices are based on our intuition aboutthe processes involved. For videos that differ significantly from the videos we use, it is possible thatthe baseline AMISE method would perform better. We would like to point out that ideally the variance value sets should be learned automatically from aseparate training data set. In absence of suitable training data for these videos in particular and forbackground subtraction research in general, we resort to manually choosing these values. This alsoappears to be the common practice among researchers in this area. Benchmark comparisons are provided for selected existing methods - MOG, the complex foregroundmodel (ACMMM03), and SILTP. To evaluate our results, the posterior probability of the backgroundlabel is thresholded at a value of 0.5 to get the foreground pixels. Following the same procedure as ,any foreground 4-connected components smaller than a size threshold of 15 pixels are ignored. shows qualitative results for the same frames that were reported by . We present results forour kernel method with uniform variances and adaptive variances with RGB features (Uniform-rgband VKS-rgb respectively), and adaptive variances with a hybrid feature space of LAB color andSILTP features (VKS-lab+siltp). Except for the Lobby video, the VKS results are better than othermethods. The Lobby video is an instance where there is a sudden change in illumination in thescene (turning a light switch on and off). Due to use of an explicit foreground model, our kernelmethods misclassify most of the pixels as foreground and take a long time to recover from this error.A possible solution for this case is presented later. Compared to the uniform variance kernel estimates,we see that VKS-rgb has fewer false positive foreground pixels. Quantitative results in compare the F-measure scores for our method against MoG,ACMMM03, and SILTP results as reported by . The table shows that methods that share spa-tial information (uniform kernel and VKS) with RGB features give significantly better results thanmethods that use RGB features without spatial sharing. Comparing the variable kernel method toa uniform kernel method in the same feature space (RGB), we see a significant improvement inperformance for most videos. Scale-invariant local ternary pattern (SILTP) is a recent texture featurethat is robust to soft shadows and lighting changes. We believe SILTP represents the state of the artin background modeling and hence compare our results to this method. Scale-invariant local states isa slight variation in the representation of the SILTP feature. For comparison, we use SILTP resultsfrom because in human judgement was used to vary a size threshold parameter for each video. Webelieve results from the latter fall under a different category of human-assisted backgrounding andhence do not compare to our method where no video-specific hand-tuning of parameters was done. shows that SILTP is very robust to lighting changes and works well across the entire data set.Blue entries in correspond to videos where our method performs better than SILTP. VKSwith RGB features (VKS-rgb) performs well in videos that have few shadows and lighting changes.Use of color features that are more robust to illumination change, like LAB features in place of RGBhelps in successful classification of the shadow regions as background. Texture features are robustto lighting changes but not effective on large texture-less objects. Color features are effective onlarge objects, but not very robust to varying illumination. By combining texture features with LABcolor features, we expect to benefit from the strengths of both feature spaces. Such a combination hasproved useful in earlier work. Augmenting the LAB features with SILTP features (computed at 3resolutions) in the VKS framework (VKS-lab+siltp) results in an improvement in 7 out of 9 videos(last column). The variance values used in our implementation are given in . We also compare our results (VKS-lab+siltp) to the 5 videos that were submitted as supplementarymaterial by . highlights some key frames that highlight the strengths and weaknesses ofour system versus the SILTP results. The common problems with our algorithm are shadows beingclassified as foreground (row e) and initialization errors (row e shows a scene where the desk wasoccluded by people when the background model was initialized. Due to the explicit foregroundmodel, VKS takes some time to recover from the erroneous initialization). A common drawback withSILTP is that large texture-less objects have holes in them (row a). Use of color features helpsavoid these errors. The SILTP system also loses objects that stop moving (rows b, c, d, f). Due to theexplicit modeling of the foreground, VKS is able to detect objects that stop moving. The two videos in the data set where our algorithm performs worse than SILTP are the Escalatorvideo (rows g, h) and the Lobby video (rows i, j). In the Escalator video, our algorithm fails at theescalator steps due to large variation in color in the region. In the Lobby video, at the time of sudden illumination change, many pixels in the image get classifiedas foreground. Due to the foreground model, these pixels continue to be misclassified for a longduration (row j). The problem is more serious for RGB features ( column 2). One method toaddress the situation is to observe the illumination change from one frame to the next. If more thanhalf the pixels in the image change in illumination by a threshold value of TI or more, we throw awayall the background samples at that instance and begin learning a new model from the subsequent 50frames. This method allows us to address the poor performance in the Lobby video with resultingF-measure values of 86.77 for uniform-rgb, 78.46 for VKS-rgb, and 77.76 for VKS-lab+siltp. TI of10 and 2.5 were used for RGB and LAB spaces respectively. The illumination change procedure doesnot affect the performance of VKS on any other video in the data set.",
  "Caching optimal kernel variances from previous frame": "A major drawback with trying multiple variance values at each pixel to select the best variance isthat the amount of computation per pixel increases significantly. In order to reduce the complexitythe algorithm, we use a scheme where the current frames optimal variance values for each pixellocation for both the background and foreground processes is stored (Bcache x,y , Fcache x,y ) foreach location (x, y) in the image. When classifying pixels in the next frame, these cached variancevalues are first tried. If the resulting scores are very far apart, then it is very likely that the pixelhas not changed its label from the previous frame. The expensive variance selection procedure isperformed only at pixels where the resulting scores are close to each other. Algorithm 1 for efficientcomputation results in a reduction in computation in about 80",
  "Discussion": "By applying kernel estimate method to a large data set, we have established, as do , that the useof spatial information is extremely helpful. Some of the important issues pertaining to the choiceof kernel parameters for data sets with wide variations have been addressed. Having a uniformkernel variance for the entire data set and for all pixels in the image results in a poor overall system.Dynamically adapting the variance for each pixel results in a significant increase in accuracy. Using color features in the joint domain-range kernel estimation approach can complement complexbackground model features in settings where the latter are known to be inaccurate. Combining robustcolor features like LAB with texture features like SILTP in a VKS framework yields a highly accuratebackground classification system. For future work, we believe our method could be explained more elegantly in a probabilistic frame-work where the scores are replaced by likelihoods and informative priors are used in the Bayes ruleclassification."
}