{
  "Abstract": "This paper reduces discontinuous parsing to sequence labeling. It first shows thatexisting reductions for constituent parsing as labeling do not support discontinuities.Second, it fills this gap and proposes to encode tree discontinuities as nearly orderedpermutations of the input sequence. Third, it studies whether such discontinuousrepresentations are learnable. The experiments show that despite the architecturalsimplicity, under the right representation, the models are fast and accurate.",
  "Introduction": "Discontinuous constituent parsing studies how to generate phrase-structure trees of sentences comingfrom non-configurational languages, where non-consecutive tokens can be part of the same grammati-cal function (e.g. nonconsecutive terms belonging to the same verb phrase). shows a Germansentence exhibiting this phenomenon. Discontinuities happen in languages that exhibit free wordorder such as German or Guugu Yimidhirr, but also in those with high rigidity, e.g. English, whosegrammar allows certain discontinuous expressions, such as wh-movement or extraposition. Thismakes discontinuous parsing a core computational linguistics problem that affects a wide spectrumof languages. There are different paradigms for discontinuous phrase-structure parsing, such as chart-based parsers,transitionbased algorithms or reductions to a problem of a different nature, such as dependencyparsing. However, many of these approaches come either at a high complexity or low",
  "speed, while others give up significant performance to achieve an acceptable latency": "Related to these research aspects, this work explores the feasibility of discontinuous parsing underthe sequence labeling paradigm, inspired by work on fast and simple continuous constituent parsing.We will focus on tackling the limitations of their encoding functions when it comes to analyzingdiscontinuous structures, and include an empirical comparison against existing parsers. Contribution (i) The first contribution is theoretical: to reduce constituent parsing of free word orderlanguages to a sequence labeling problem. This is done by encoding the order of the sentence as(nearly ordered) permutations. We present various ways of doing so, which can be naturally combinedwith the labels produced by existing reductions for continuous constituent parsing. (ii) The secondcontribution is a practical one: to show how these representations can be learned by neural transducers.We also shed light on whether general-purpose architectures for NLP tasks can effectively parsefree word order languages, and be used as an alternative to adhoc algorithms and architectures fordiscontinuous constituent parsing.",
  "Related work": "Discontinuous phrase-structure trees can be derived by expressive formalisms such as MultipleContext Free Grammmars (MCFGs) or Linear Context-Free Rewriting Systems (LCFRS). MCFGsand LCFRS are essentially an extension of Context-Free Grammars (CFGs) such that non-terminalscan link to non-consecutive spans. Traditionally, chart-based parsers relying on this paradigm commonly suffer from high complexity. Let k be the block degree, i.e. the number of nonconsecutivespans than can be attached to a single non-terminal; the complexity of applying CYK (after binarizingthe grammar) would be O(n3k), which can be improved to O(n2k+2) if the parser is restricted towell-nested LCFRS, and discusses how for a standard discontinuous treebank, k 3 (in contrast tok = 1 in CFGs). Recently, presents a chart-based parser for k = 2 that can run in O(n3), which isequivalent to the running time of a continuous chart parser, while covering 98 Differently, it is possible to rely on the idea that discontinuities are inherently related to the locationof the token in the sentence. In this sense, it is possible to reorder the tokens while still obtaining agrammatical sentence that could be parsed by a continuous algorithm. This is usually achieved withtransition-based parsing algorithms and the swap transition which switches the topmost elements inthe stack. For instance, uses this transition to adapt an easy-first strategy for dependency parsing todiscontinuous constituent parsing. In a similar vein, builds on top of a fast continuous shift-reduceconstituent parser, and incorporates both standard and bundled swap transitions in order to analyzediscontinuous constituents. system produces derivations of up to a length of n2 n + 1 given asentence of length n. More efficiently, presents a transition system which replaces swap with a gaptransition. The intuition is that a reduction does not need to be always applied locally to the twotopmost elements in the stack, and that those two items can be connected, despite the existence of agap between them, using non-local reductions. Their algorithm ensures an upper-bound of n(n1)2transitions. With a different optimization goal, removed the traditional reliance of discontinuousparsers on averaged perceptrons and hand-crafted features for a recursive neural network approachthat guides a swap-based system, with the capacity to generate contextualized representations. replacethe stack used in transition-based systems with a memory set containing the created constituents.This model allows interactions between elements that are not adjacent, without the swap transition, tocreate a new (discontinuous) constituent. Trained on a 2 stacked BiLSTM transducer, the model isguaranteed to build a tree with in 4n-2 transitions, given a sentence of length n. A middle ground between explicit constituent parsing algorithms and this paper is the work based ontransformations. For instance, convert constituent trees into a nonlinguistic dependency representationthat is learned by a transition-based dependency parser, to then map its output back to a constituent tree.A similar approach is taken by, but they proposed a more compact representation that leads to a muchreduced set of output labels. Other authors such as propose a two-step approach that approximatesdiscontinuous structure trees by parsing context-free grammars with generative probabilistic modelsand transforming them to discontinuous ones. cast discontinuous phrase-structure parsing into aframework that jointly performs supertagging and non-projective dependency parsing by a reductionto the Generalized Maximum Spanning Arborescence problem. The recent work by can be alsoframed within this paradigm. They essentially adapt the work by and replace the averaged perceptronclassifier with pointer networks, adressing In this context, the closest work to ours is the reduction proposed by, who cast continuous constituentparsing as sequence labeling. In the next sections we build on top of their work and: (i) analyze whytheir approach cannot handle discontinuous phrases, (ii) extend it to handle such phenomena, and (iii)train functional sequence labeling discontinuous parsers.",
  "Preliminaries": "Let w = [w0, w1, ..., w|w|1] be an input sequence of tokens, and T|w| the set of (continuous)constituent trees for sequences of length |w|; define an encoding function : T|w| L|w| to mapcontinuous constituent trees into a sequence of labels of the same length as the input. Each label, liL, is composed of three components li = (ni, xi, ui): ni encodes the number of levels in the tree in common between a word wi and wi+1. To obtain amanageable output vocabulary space, ni is actually encoded as the difference ni ni1, with n1 = 0. Wedenote by abs(ni) the absolute number of levels represented by ni. i.e. the total levels in commonshared between a word and its next one.",
  "xi represents the lowest non-terminal symbol shared between wi and wi+1 at level abs(ni)": "ui encodes a leaf unary chain, i.e. nonterminals that belong only to the path from the terminal wi tothe root. Note that cannot encode this information in (ni, xi), as these components always representcommon information between wi and wi+1. Incompleteness for discontinuous phrase structures proved that is complete and injective for continu-ous trees. However, it is easy to prove that its validity does not extend to discontinuous trees, by usinga counterexample. shows a minimal discontinuous tree that cannot be correctly decoded. The inability to encode discontinuities lies on the assumption that wi+1 will always be attached to anode belonging to the path from the root to wi (ni is then used to specify the location of that node inthe path). This is always true in continuous trees, but not in discontinuous trees, as can be seen in where c is the child of a constituent that does not lie in the path from S to b.",
  "Encoding nearly ordered permutations": "Next, we fill this gap to address discontinuous parsing as sequence labeling. We will extend theencoding to the set of discontinuous constituent trees, which we will call T|w|. The key to do thisrelies on a well-known property: a discontinuous tree t T|w| can be represented as a continuous oneusing an in-order traversal that keeps track of the original indexes (e.g. the trees at the left and theright in ). We will call this tree the (canonical) continuous arrangement of t, (t) T|w|. Thus, if given an input sentence we can generate the position of every word as a terminal in (t), theexisting encodings to predict continuous trees as sequence labeling could be applied on (t). In essence,this is learning to predict a permutation of w. As introduced in 2, the concept of location of a tokenis not a stranger in transition-based discontinuous parsing, where actions such as swap switch theposition of two elements in order to create a discontinuous phrase. We instead propose to explorehow to handle this problem in end-to-end sequence labeling fashion, without relying on any parsingstructure nor a set of transitions. Todo so, first we denote by : {0, . . . , |w| 1} {0, . . . , |w| 1} the permutation that maps theposition i of a given wi in w into its position as a terminal node in (t). From this, one can derive 1, a function that encodes a permutation of w in such a way that its phrase structure does not havecrossing branches. For continuous trees, and 1 are identity permutations. Then, we extend thetree encoding function to T|w| L|w| where l L is enriched with a fourth component pi suchthat l = (ni, xi, ui, pi), where pi is a discrete symbol such that the sequence of pis encodes thepermutation (typically, each pi will be an encoding of (i), i.e., the position of wi in the continuousarrangement, although this need not be true in all encodings, as will be seen below). The crux of defining a viable encoding for discontinuous parsing is then in how we encode tau asa sequence of values pi, for i = 0 . . . |w| 1. While the naive approach would be the identityencoding (pi = tau(i)), we ideally want an encoding that balances minimizing sparsity (by minimizinginfrequently-used values) and maximizing learnability (by being predictable). To do so, we will lookfor encodings that take advantage of the fact that discontinuities in attested syntactic structures aremild , i.e., in most cases, tau (i + 1) = tau (i) + 1. In other words, permutations tau corresponding toreal syntactic trees tend to be nearly ordered permutations. Based on these principles, we proposebelow a set of concrete encodings, which are also depicted on an example in . All of themhandle multiple gaps (a discontinuity inside a discontinuity) and cover 100 Absolute-position: For every token wi, pi = (i) only if wi = (i). Otherwise, we use a speciallabel INV, which represents that the word is a fixed point in the permutation, i.e., it occupies the sameplace in the sentence and in the continuous arrangement.",
  "Relative-position If i != tau(i), then pi = i tau(i). otherwise, we again use the INV label": "Lehmer code In combinatorics, let n = [0, ..., n 1] be a sorted sequence of objects, a Lehmer codeis a sequence sigma = [sigma0, ...sigman1] that encodes one of the n! permutations of n, namely .The idea is intuitive: let ni+1 be the subsequence of objects from n that remain available after wehave permuted the first i objects to achieve the permutation , then sigmai+1 equals the (zero-based)position in ni+1 of the next object to be selected. For instance, given n = and a validpermutation = , then sigma = . Note that the identity permutation would beencoded as a sequence of zeros. In the context of discontinuous parsing and encoding pi, n can be seen as the input sentence wwhere pi(w) is encoded by sigma. The Lehmer code is particularly suitable for this task in termsof compression, as in most of the cases we expect (nearly) ordered permutations, which translatesinto the majority of elements of sigma being zero. However, this encoding poses some potential",
  ": Main hyper-parameters for the training of the BiLSTMs, both for the gold and predictedsetups": "learnability problems. The root of the problem is that sigmai does not necessarily encode tau(i), buttau(j) where j is the index of the word that occupies the ith position in the continuous arrangement(i.e., j = tau 1(i)). In other words, this encoding is expressed following the order of words in thecontinuous arrangement rather than the input order, causing a non-straightforward mapping betweeninput words and labels. For instance, in the previous example, sigma2 does not encode the location ofthe object n2 = 2 but that of n3 = 3. Lehmer code of the inverse permutation To ensure that each pi encodes tau(i), we instead interpretpi as meaning that should fill the (pi + 1)th currently remaining blank in a sequence sigma that isinitialized as a sequence of blanks, i.e. sigma = [,,...,] .Forinstance, letn = be Pointer-based encoding When encoding tau(i), the previous encodings generate the position for thetarget word, but they do not really take into account the left-to-right order in which sentences arenaturally read, nor they are linguistically inspired. In particular, informally speaking, in human lin- Finally, in we list the number of parameters for each of the transducers trained on the pointer-based encoding. For the rest of the encodings, the models have a similar number of parameters, as theonly change in the architecture is the small part involving the feed-forward output layer that predictsthe label component pi.",
  "Additionally we use a char-based LSTM with a hidden layer of 100/132 dimensions (English/German).For both approaches, a linear layer followed by a softmax is used to predict every label component": "For BERT and DistilBERT we use the default fine-tuning parameters. We use Adam as optimizer andcross entropy as the loss function. The learning rate and other hyper-parameters are left as defaultin the transformers library, except for the number of training epochs (we train them for at most 30epochs), and the batch size, which is adjusted depending on the memory required by the model (e.g. 8for BERT and 32 for DistilBERT). For the BERT-large model, due to the limitations in GPU memory,we have to reduce the training batch size to 1, and use a smaller learning rate of 1e-5.",
  "Experiments": "Setup For English, we use the discontinuous Penn Treebank (DPTB) by. For German, we use TIGERand NEGRA. We use the splits by which in turn follow the splits for the NEGRA treebank, the splitsfor TIGER, and the standard splits for (D)PTB (Sections 2 to 21 for training, 22 for development and23 for testing). See also Appendix A.5 for more detailed statistics. We consider gold and predictedPoS tags. For the latter, the parsers are trained on predicted PoS tags, which are generated by a2stacked BiLSTM, with the hyper-parameters used to train the parsers. The PoS tagging accuracy (",
  "Results": "shows the results on the dev sets for all encodings and transducers. The tendency is clearshowing that the pointer-based encodings obtain the best results. The pointer-based encoding withsimplified PoS tags does not lead however to clear improvements, suggesting that the models can learnthe sparser original PoS tags set. For the rest of encodings we also observe interesting tendencies. Forinstance, when running experiments using stacked BiLSTMs, the relative encoding performs better than the absolute one, which was somehow expected as the encoding is less sparse. However, thetendency is the opposite for the Transformer encoders (including BERT and DistilBERT), especiallyfor the case of discontinuous constituents. We hypothesize this is due to the capacity of Transformersto attend to every other word through multihead attention, which might give an advantage to encodeabsolute positions over BiLSTMs, where the whole left and right context is represented by a singlevector. With respect to the Lehmer and Lehmer of the inverse permutation encodings, the latterperforms better overall, confirming the bigger difficulties for the tested sequence labelers to learnLehmer, which in some cases has a performance even close to the naive absolute-positional encoding(e.g. for TIGER using the vanilla Transformer encoder and BERT). As introduced in 4, wehypothesize this is caused by the non-straightforward mapping between words and labels (in theLehmer code the label generated for a word does not necessarily contain information about theposition of such word in the continuous arrangement). In we compare a selection of our models against previous work using both gold and predictedPoS tags. In particular, we include: (i) models using the pointer-based encoding, since they obtainedthe overall best performance on the dev sets, and (ii) a representative subset of encodings (the absolutepositional one and the Lehmer code of the inverse permutation) trained with the best performingtransducer. Additionally, for the case of the (English) DPTB, we also include experiments using abert-large model, to shed more light on whether the size of the networks is playing a role when itcomes to detect discontinuities. Additionally, we report speeds on CPU and GPU. The experimentsshow that the encodings are learnable, but that the models power makes a difference. For instance, inthe predicted setup BILSTMs and vanilla Transformers perform in line with predeep learning models, DistilBERT already achieves a robust performance, close to models such as and BERT transducerssuffice to achieve results close to some of the strongest approaches, e.g.. Yet, the results lag behindthe state of the art. With respect to the architectures that performed the best the main issue is thatthey are the bottleneck of the pipeline. Thus, the computation of the contextualized word vectorsunder current approaches greatly decreases the importance, when it comes to speed, of the chosenparsing paradigm used to generate the output trees (e.g. chart-based versus sequence labeling).",
  "Finally, details the discontinuous performance of our best performing models": "Discussion on other applications It is worth noting that while we focused on parsing as sequencelabeling, encoding syntactic trees as labels is useful to straightforwardly feed syntactic informationto downstream models, even if the trees themselves come from a non-sequence-labeling parser. Forexample, use the sequence labeling encoding of to provide syntactic information to a semantic rolelabeling model. Apart from providing fast and accurate parsers, our encodings can be used to do thesame with discontinuous syntax.",
  "Conclusion": "We reduced discontinuous parsing to sequence labeling. The key contribution consisted in predictinga continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and definingvarious ways to encode such a rearrangement as a sequence of labels associated to each word, takingadvantage of the fact that in practice they are nearly ordered permutations. We tested whether thoseencodings are learnable by neural models and saw that the choice of permutation encoding is nottrivial, and there are interactions between encodings"
}