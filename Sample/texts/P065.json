{
  "Abstract": "Generative Artificial Intelligence models for image generation have demonstrated remarkable capabilities in taskslike text-to-image synthesis and image completion through inpainting. Inpainting performance can be measuredby removing parts of an image, using the model to restore them, and comparing the result with the original. Thisprocess can be applied recursively, where the output of one inpainting operation becomes the input for the next.This recursive application can result in images that are either similar to or vastly different from the original,depending on the removed sections and the models ability to reconstruct them. The ability to recover an imagesimilar to the original, even after numerous recursive inpainting operations, is a desirable characteristic referred toas stability. This concept is also being explored in the context of recursively training generative AI models withtheir own generated data. Recursive inpainting is a unique process that involves recursion only during inference,and understanding its behavior can provide valuable insights that complement ongoing research on the effects ofrecursion during training. This study investigates the effects of recursive inpainting on Stable Diffusion, a widelyused image model. The findings indicate that recursive inpainting can result in image degradation, ultimatelyleading to a meaningless image, and that the final outcome is influenced by factors such as the image type, the sizeof the inpainting areas, and the number of iterations.",
  "Introduction": "In the past two years, Generative Artificial Intelligence (AI) has emerged as a central player, sparking a significant revolution intechnology. These AI models are capable of producing text, audio, images, and video, finding applications in a wide array oftransformative uses. Notable examples include Large Language Models (LLMs) like GPT4, which excel at answering questions,summarizing, translating, and paraphrasing texts, and text-to-image generators like DALL-E, which can generate images based onalmost any textual description. These tools have garnered widespread public interest, attracting hundreds of millions of users. These AI tools have reached exceptional performance levels in various tasks, making their evaluation a crucial aspect. For LLMs,numerous benchmarks have been developed to evaluate their knowledge across different subjects, their proficiency in solvingmathematical or reasoning problems, and their language comprehension. These benchmarks facilitate model comparisons, and whena new model is launched, its performance on these standard benchmarks is typically reported. In the realm of image generation,several metrics have been introduced to assess performance, including the Fr00e9chet Inception Distance (FID), precision and recall,and density and coverage. These metrics aim to quantify how closely generated images resemble real ones and how effectivelythey cover the spectrum of real images. Another capability offered by some AI image generation tools, and implemented throughspecialized AI models, is inpainting. In this process, the AI tool is provided with an image containing missing parts and is taskedwith filling them in to complete the image. Assessing the quality of content produced by AI is crucial not only for comparing different AI models or evaluating their progressin specific tasks but also because the extensive use of generative AI is altering the fundamental nature of content found on theInternet. AI-generated texts and images are now widespread and, in some instances, predominant, with this trend expected topersist in the coming years. This has consequences for newer AI models, as they are frequently trained on data gathered from theInternet, establishing a feedback loop where new models are trained using data created by earlier AI models. This cycle can result indiminished performance or even the breakdown of AI models, prompting research into the stability of AI models when trained usingtheir own generated data. The feedback loops in generative AI that have been examined thus far pertain to the training of newer models, creating a loop acrossdifferent generations of AI models. However, other potential loops in generative AI exist that have not been previously investigatedto the best of our knowledge. For instance, when the input to the AI model is an image and the output is also an image, as is the casewith inpainting, the AI model can be recursively applied to its own output, forming a loop. In this scenario, there is no traininginvolved, only inferences that are recursively applied. Examining the effects of these recursive applications of the AI model on thegenerated content is essential to determine whether the AI models remain stable or degrade, similar to what occurs in the trainingloop. In this research, we examine the inference feedback loop utilizing a renowned AI image model, Stable Diffusion, and its inpaintingfeature. A thorough empirical investigation is carried out to discern the conditions under which the model maintains stability andwhen it experiences degradation. The subsequent sections of this paper are structured as follows: provides a conciseoverview of the inpainting feature and the feedback loops in generative AI. introduces the inference loop, termed RecursiveInpainting (RIP), which is then assessed in . The constraints of our assessment, along with the findings, are deliberated in. The paper concludes with a summary in .",
  "Inpainting": "Inpainting is a function found in some contemporary generative AI image tools, which involves filling in missing portions of animage to complete it. The effectiveness of inpainting is contingent on the specific model used, the nature of the image, and the sizeand placement of the missing areas. Generally, inpainting can only restore a portion of the information that is lost in the missingimage segments. Various metrics are available to assess the resemblance between the original image and the one reconstructedthrough inpainting. These range from traditional methods like Structural Similarity (SSIM) and multi-scale SSIM (MS-SSIM), whichare based on pixel-level comparisons, to more sophisticated methods like Learned Perceptual Image Patch Similarity (LPIPS) andPaired/Unpaired Inception Discriminative Score (P/U-IDS), which employ AI models to simulate human-like perceptual evaluations.",
  "Recursiveness in Generative AI": "A cycle is formed where AI-generated content is posted online and subsequently collected to train newer AI models. This can resultin a decline in the effectiveness of AI models, or even their failure, when they are trained using data they have produced themselves.This has sparked a growing interest in determining the circumstances under which these generative AI models maintain stabilitywhen trained recursively with data they generate. The stability is influenced by multiple factors, such as the specific model, thequantity of AI-generated data used in each retraining cycle, and whether the cycle involves one or multiple AI models. Investigatingthis cycle is crucial as it can affect not only the development of future AI models but also the type of content that will likely dominatethe Internet in the future. In all these investigations, the recursive aspect involves training new AI models with data produced byother AI models. However, in certain situations, recursion can happen when the same AI model is used solely for making inferences.This particular scenario has not been explored in previous studies, to the best of our knowledge.",
  "Recursive Inpainting (RIP)": "An intriguing aspect to note is that a distinct recursive loop can be established with AI image models when employing the inpaintingtechnique. This process begins with an image, to which a mask is applied to obscure certain areas, and inpainting is utilized to fill inthese areas. This results in a second image that has been partially generated by the AI image model. The procedure is then reiteratedusing a different mask to produce a subsequent image, this time entirely generated from AI-produced content. The process continuesas inpainting is recursively applied to images that have already undergone inpainting. As parts of the images are removed andreconstructed, information is inevitably lost. However, it is crucial to determine whether this loss leads to images that are drasticallydifferent from the original, or if the images become simpler and less intricate. Alternatively, it is possible that the inpainting processremains stable, resulting in images that are merely variations of the original. Similar to the recursive training of models with theirown data, it is important to understand the conditions under which inpainting remains stable or degrades under recursion. The consequences of recursive inpainting are influenced by numerous factors, including the specific AI model employed, thecharacteristics of the image, and the masks utilized in each iteration. It is reasonable to expect that more intricate images or masksthat obscure larger portions of the image will have a higher likelihood of causing degradation. In the subsequent section, we outlinethe results of an extensive empirical investigation into recursive inpainting using Stable Diffusion, representing an initial effort toidentify the primary factors that influence the effects of recursive inpainting.",
  ". The AI model used. 2. The input images. 3. The masks applied at each stage. 4. The number of iterations": "In our experimental setup, we utilized Stable Diffusion, which is a text-to-image latent diffusion model, due to its open-source natureand widespread use in the AI image model community. Specifically, we employed a version of Stable Diffusion 2 that was fine-tunedfor inpainting. This model uses a technique for generating masks where the masked areas, along with the latent VAE representationsof the masked image, provide additional conditioning for the inpainting process. The models parameters were kept at their defaultsettings. We did not use any text prompts to direct the inpainting, allowing the model to concentrate on reconstructing the missingparts based solely on the remaining visual information without any textual guidance. For the image selection, to minimize any potential bias, we randomly chose images from an extensive dataset containing over 81,000art images of various types created by different artists. From this dataset, 100 images were randomly picked to form our evaluationset. The input images are 512x512 pixels; if their original aspect ratio is not square, blank areas are added to the sides to achieve the512x512 format. In generating masks for inpainting, we divide the images into squares of a predetermined size. In each iteration, a square is randomlychosen to serve as the mask. To facilitate comparisons across different mask and image sizes, our experiments use the number ofpixels inpainted relative to the image size as the primary parameter, rather than the number of inpainting operations. To assess the similarity to the original image across iterations, we employ the Learned Perceptual Image Path Similarity (LPIPS)metric, which is frequently used to evaluate inpainting quality. In our implementation, we utilize the features from three neuralnetworks to calculate the metric: SqueezeNet, AlexNet, and VGG. We conducted recursive inpainting, altering 400% of the pixels, using masks of sizes 64x64, 128x128, and 256x256. To measure thedegradation as inpainting operations are performed, we calculated the LPIPS metric between the original image and each subsequentgeneration using the features from the three neural networks (SqueezeNet, AlexNet, and VGG). The average distances for the 100images at each 50% inpainting step are presented. The bars represent the standard deviation observed across the samples for eachdata point. Several initial observations can be drawn from these results: 1. As the recursive inpainting progresses, the distance from the original image increases, potentially leading to an image that bearsno resemblance to the original. 2. The rate at which the distance increases tends to decrease, but it does not appear to stabilize evenwhen the distance becomes substantial. 3. The discrepancy with the original image is more pronounced when larger masks are usedfor inpainting, which aligns with the expectation that larger blocks are more challenging to inpaint. 4. The three networks used forcomputing the LPIPS (SqueezeNet, AlexNet, and VGG) yield comparable results. 5. The significant standard deviation indicatesthat different images will exhibit varying behaviors. To gain a better understanding of the variability in distances for each image, scatter plots of the LPIPS distances for the 100 imagesfor each neural network are presented. It is evident that there is considerable variability across images, but the general trends areconsistent with those observed in the mean: the distance increases with more inpainting and with larger masks. Among the threenetworks (SqueezeNet, AlexNet, and VGG), VGG shows the fewest outliers. Given that VGG is the most complex network, itis expected to capture the image features more effectively. Consequently, we will only report results for VGG moving forward,although all metrics are available in the repository along with the images. To investigate whether the degradation is consistent across different runs, we selected 10 images from the set of 100 and performed10 runs on each. The LPIPS metrics across these runs for three different images are displayed, using the VGG network, whichgenerally exhibits the lowest deviations. It is noticeable that variations are more significant with larger masks, which is anticipatedsince larger masks require fewer iterations to reach a given percentage of inpainting, thus introducing more variability. The variationsalso decrease as the percentage of inpainting increases, indicating that a higher number of inpainting operations leads to reducedvariability. This suggests that recursive inpainting tends to converge in terms of LPIPS distance as the process advances.",
  "Conclusion and Future Work": "In this study, we have introduced and empirically examined the impact of recursive inpainting on AI image models. The findingsreveal that recursion can result in the deterioration and eventual breakdown of the image, a phenomenon akin to the model collapseobserved when training generative AI models with their own data. This issue is currently a focal point in the research community.Consequently, this paper introduces a new dimension to the study of the effects of recursive application of generative AI, specificallyin the inference phase. This can enhance current research endeavors and offer deeper insights into the underlying causes of collapse,potentially leading to advancements in AI models that can lessen the adverse effects of recursion. The presented analysis of recursive inpainting represents an initial step in this area. Further investigation involving different AImodels, a variety of images, and diverse model configurations is necessary to gain a more comprehensive understanding of the effectsof recursive inpainting. Developing theoretical models that can account for these effects is also a crucial area for future research.Additionally, exploring the connections between recursive training and recursive inpainting could provide valuable insights."
}