{
  "Abstract": "Understanding language goes hand in hand with the ability to integrate com-plex contextual information obtained via perception. We present a novel task forgrounded language understanding: disambiguating a sentence given a visual scenewhich depicts one of the possible interpretations of that sentence. To this end, weintroduce a new multimodal corpus containing ambiguous sentences, representinga wide range of syntactic, semantic and discourse ambiguities, coupled with videosthat visualize the different interpretations for each sentence. We address this taskby extending a vision model which determines if a sentence is depicted by a video.We demonstrate how such a model can be adjusted to recognize different interpre-tations of the same underlying sentence, allowing to disambiguate sentences in aunified fashion across the different ambiguity types.",
  "Introduction": "Ambiguity is one of the defining characteristics of human languages, and language understandingcrucially relies on the ability to obtain unambiguous representations of linguistic content. Whilesome ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of manylinguistic constructions requires integration of world knowledge and perceptual information obtainedfrom other modalities. We focus on the problem of grounding language in the visual modality, and introduce a novel taskfor language understanding which requires resolving linguistic ambiguities by utilizing the visualcontext in which the linguistic content is expressed. This type of inference is frequently called for inhuman communication that occurs in a visual environment, and is crucial for language acquisition,when much of the linguistic content refers to the visual surroundings of the child. Our task is also fundamental to the problem of grounding vision in language, by focusing onphenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked whenusing language as a medium for expressing understanding of visual content. Due to such ambiguities,a superficially appropriate description of a visual scene may in fact not be sufficient for demonstratinga correct understanding of the relevant visual content. Our task addresses this issue by introducing adeep validation protocol for visual understanding, requiring not only providing a surface descriptionof a visual activity but also demonstrating structural understanding at the levels of syntax, semanticsand discourse. To enable the systematic study of visually grounded processing of ambiguous language, we createa new corpus, LAVA (Language and Vision Ambiguities). This corpus contains sentences withlinguistic ambiguities that can only be resolved using external information. The sentences are pairedwith short videos that visualize different interpretations of each sentence. Our sentences encompass awide range of syntactic, semantic and dis- course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions,logical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3interpretations per sentence, and an average of 3.37 videos that depict visual variations of eachsentence interpretation, corresponding to a total of 1679 videos. Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentencethat matches the content of a given video. Our approach for tackling this task extends the sentencetracker. The sentence tracker produces a score which determines if a sentence is depicted by avideo. This earlier work had no concept of ambiguities; it assumed that every sentence had a singleinterpretation. We extend this approach to represent multiple interpretations of a sentence, enablingus to pick the interpretation that is most compatible with the video.",
  "Previous work relating ambiguity in language to the visual modality addressed the problem of word": "sense disambiguation. However, this work is limited to context independent interpretation of individ-ual words, and does not consider structure-related ambiguities. Discourse ambiguities were previouslystudied in work on multimodal coreference resolution. Our work expands this line of research, andaddresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the bestof our knowledge our study is the first to present a systematic treatment of syntactic and semanticsentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have beenextensively studied in psycholinguistics and cognitive psychology. A considerable fraction of thiswork focused on the processing of ambiguous language, providing evidence for the importance ofvisual information for linguistic ambiguity resolution by humans. Such information is also vitalduring language acquisition, when much of the linguistic content perceived by the child refers to theirimmediate visual environment. Over time, children develop mechanisms for grounded disambiguationof language, manifested among others by the usage of iconic gestures when communicating ambigu-ous linguistic content. Our study leverages such insights to develop a complementary framework thatenables addressing the challenge of visually grounded disambiguation of language in the realm ofartificial intelligence.",
  "Task": "We provide a concrete framework for the study of language understanding with visual context byintroducing the task of grounded language disambiguation. This task requires to choose the correctlinguistic representation of a sentence given a visual context depicted in a video. Specifically, providedwith a sentence, n candidate interpretations of that sentence and a video that depicts the content ofthe sentence, one needs to choose the interpretation that corresponds to the content of the video.",
  "Approach Overview": "To address the grounded language disambiguation task, we use a compositional approach for determin-ing if a specific interpretation of a sentence is depicted by a video. a sentence and an accompanyinginterpretation encoded in first order logic, give rise to a grounded model that matches a video againstthe provided sentence interpretation. The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words,and trackers which locate objects in video frames. To represent an interpretation of a sentence, wordmodels are combined with trackers through a cross-product which respects the semantic representationof the sentence to create a single model which recognizes that interpretation.",
  "Corpus": "To enable a systematic study of linguistic ambiguities that are grounded in vision, we compileda corpus with ambiguous sentences describing visual actions. The sentences are formulated suchthat the correct linguistic interpretation of each sentence can only be determined using external,non-linguistic, information about the depicted activity. For example, in the sentence Bill held thegreen chair and bag, the correct scope of green can only be determined by integrating additionalinformation about the color of the bag. This information is provided in the accompanying videos,which visualize the possible interpretations of each sentence. presents the syntactic parsesfor this example along with frames from the respective videos. Although our videos contain visualuncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting,and hence a video always corresponds to a single candidate representation of a sentence.",
  "The corpus covers a wide range of well": "known syntactic, semantic and discourse ambiguity classes. While the ambiguities are associatedwith various types, different sentence interpretations always represent distinct sentence meanings,and are hence encoded semantically using first order logic. For syntactic and discourse ambiguitieswe also provide an additional, ambiguity type specific encoding as described below. Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase(VP) attachments, and ambiguities in the interpretation of conjunctions. In addition tological forms, sentences with syntactic ambiguities are also accompanied with Context FreeGrammar (CFG) parses of the candidate interpretations, generated from a deterministic CFGparser. Semantics The corpus addresses several classes of semantic quantification ambiguities, inwhich a syntactically unambiguous sentence may correspond to different logical forms. Foreach such sentence we provide the respective logical forms. Discourse The corpus contains two types of discourse ambiguities, Pronoun Anaphora andEllipsis, offering examples comprising two sentences. In anaphora ambiguity cases, anambiguous pronoun in the second sentence is given its candidate antecedents in the firstsentence, as well as a corresponding logical form for the meaning of the second sentence. Inellipsis cases, a part of the second sentence, which can constitute either the subject and theverb, or the verb and the object, is omitted. We provide both interpretations of the omissionin the form of a single unambiguous sentence, and its logical form, which combines themeanings of the first and the second sentences.",
  "lists examples of the different ambiguity classes, along with the candidate interpretations ofeach example": "The corpus is generated using Part of Speech (POS) tag sequence templates. For each template, thePOS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all thevisually applicable assignments. This generation process yields an overall of 237 sentences, of which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations. presents the corpus templates for each ambiguity class, along with the number of sentencesgenerated from each template. The corpus videos are filmed in an indoor environment containing background objects and pedestrians.To account for the manner of performing actions, videos are shot twice with different actors. Wheneverapplicable, we also filmed the actions from two different directions (e.g. approach from the left,and approach from the right). Finally, all videos were shot with two cameras from two differentview points. Taking these variations into account, the resulting video corpus contains 7.1 videosper sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos. : POS templates for generating the sentences in our corpus. The rightmost column representsthe number of sentences in each category. The sentences are produced by replacing the POS tagswith all the visually applicable assignments of lexical items from the corpus lexicon shown in table 3.",
  "The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage(152434 frames)": "A custom corpus is required for this task because no existing corpus, containing either videos orimages, systematically covers multimodal ambiguities. Datasets aim to control for more aspects ofthe videos than just the main action being performed but they do not provide the range of ambiguitiesdiscussed here. The closest dataset is that of as it controls for object appearance, color, action,and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable forevaluating the work described here.",
  "Model": "To perform the disambiguation task, we extend the sentence recognition model which representssentences as compositions of words. Given a sentence, its first order logic interpretation and avideo, our model produces a score which determines if the sentence is depicted by the video. Itsimultaneously tracks the participants in the events described by the sentence while recognizing theevents themselves. This al- lows it to be flexible in the presence of noise by integrating top-down information from the sentencewith bottom-up information from object and property detectors. Each word in the query sentence isrepresented by an HMM, which recognizes tracks (i.e. paths of detections in a video for a specificobject) that satisfy the semantics of the given word. In essence, this model can be described as havingtwo layers, one in which object tracking occurs and one in which words observe tracks and filtertracks that do not satisfy the word constraints. Given a sentence interpretation, we construct a sentence-specific model which recognizes if a videodepicts the sentence as follows. Each predicate in the first order logic formula has a correspondingHMM, which can recognize if that predicate is true of a video given its arguments. Each variable hasa corresponding tracker which attempts to physically locate the bounding box corresponding to thatvariable in each frame of a video. This creates a bipartite graph: HMMs that represent predicates are connected to trackers thatrepresent variables. The trackers themselves are similar to the HMMs, in that they comprise a latticeof potential bounding boxes in every frame. To construct a joint model for a sentence interpretation,we take the cross product of HMMs and trackers, taking only those cross products dictated by thestructure of the formula corresponding to the desired interpretation. Given a video, we employ anobject detector to generate candidate detections in each frame, construct trackers which select one ofthese detections in each frame, and finally construct the overall model from HMMs and trackers. : An overview of the different ambiguity types, along with examples of ambiguous sentenceswith their linguistic and visual interpretations. Note that similarly to semantic ambiguities, syntacticand discourse ambiguities are also provided with first order logic formulas for the resulting sentenceinterpretations. shows additional examples for each ambiguity type, with frames from samplevideos corresponding to the different interpretations of each sentence.",
  "Syntactic CategoryVisual CategoryWords": "NounsObjects, Peoplechair, bag, telescope, someone, proper namesVerbsActionspick up, put down, hold, move (transitive), look at, approach, leavePrepositionsSpacial Relationswith, left of, right of, onAdjectivesVisual Propertiesyellow, green Provided an interpretation and its corresponding formula composed of P predicates and V variables,along with a collection of object detections, bframeidetection index, in each frame of a video oflength T the model computes the score of the videosentence pair by finding the optimal detectionfor each participant in every frame. This is in essence the Viterbi algorithm, the MAP algorithm forHMMs, applied to finding optimal object detections jframevariable for each participant, and the optimalstate kframepredicate for each predicate HMM, in every frame. Each detection is scored by its confidencefrom the object detector, f and each object track is scored by a motion coherence metric g which",
  "(1)": "p, is scored by the probability of observing a particular detection in a given state hp, and by theprobability of transitioning between states ap. The structure of the formula and the fact that multiplepredicates often refer to the same variables is recorded by , a mapping between predicates and theirarguments. The model computes the MAP estimate as: for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binarypredicates) but is trivially extended to arbitrary arities. provides a visual overview of themodel as a cross-product of tracker models and word models. Our model extends the approach of in several ways. First, we depart from the dependency basedrepresentation used in that work, and recast the model to encode first order logic formulas. Notethat some complex first order logic formulas cannot be directly encoded in the model and requireadditional inference steps. This extension enables us to represent ambiguities in which a givensentence has multiple logical interpretations for the same syntactic parse. Second, we introduce several model components which are not specific to disambiguation, but arerequired to encode linguistic constructions that are present in our corpus and could not be handled bythe model of. These new components are the predicate not equal, disjunction, and conjunction. Thekey addition among these components is support for the new predicate not equal, which enforcesthat two tracks, i.e. objects, are distinct from each other. For example, in the sentence Claire and Billmoved a chair one would want to ensure that the two movers are distinct entities. In earlier work,this was not required because the sentences tested in that work were designed to distinguish objectsbased on constraints rather than identity. In other words, there might have been two different peoplebut they were distinguished in the sentence by their actions or appearance. To faithfully recognizethat two actors are moving the chair in the earlier example, we must ensure that they are disjointfrom each other. In order to do this we create a new HMM for this predicate, which assigns lowprobability to tracks that heavily overlap, forcing the model to fit two different actors in the previousexample. By combining the new first order logic based semantic representation in lieu of a syntacticrepresentation with a more expressive model, we can encode the sentence interpretations required toperform the disambiguation task. (left) shows an example of two different interpretations of the above discussed sentenceClaire and Bill moved a chair. Object trackers, which correspond to variables in the first orderlogic representation of the sentence interpretation, are shown in red. Predicates which constrain thepossible bindings of the trackers, corresponding to predicates in the representation of the sentence, areshown in blue. Links represent the argument structure of the first order logic formula, and determinethe cross products that are taken between the predicate HMMs and tracker lattices in order to formthe joint model which recognizes the entire interpretation in a video. The resulting model provides a single unified formalism for representing all the ambiguities in table2. Moreover, this approach can be tuned to different levels of specificity. We can create models thatare specific to one interpretation of a sentence or that are generic, and accept multiple interpretationsby eliding constraints that are not com-",
  "Experimental Results": "We tested the performance of the model described in the previous section on the LAVA datasetpresented in section 5. Each video in the dataset was pre-processed with object detectors for humans,bags, chairs, and telescopes. We employed a mixture of CNN and DPM detectors, trained on heldout sections of our corpus. For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space. Thescoring function consisted of a sigmoid over the confidence of the detectors trained on the same heldout portion of the training set. As none of the disambiguation examples discussed here rely on thespecific identity of the actors, we did not detect their identity. Instead, any sentence which containsnames was automatically converted to one which contains arbitrary person labels. The sentences in our corpus have either two or three interpretations. Each interpretation has one ormore associated videos where the scene was shot from a different angle, carried out either by differentactors, with different objects, or in different directions of motion. For each sentence-video pair, weperformed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations ofthe corresponding sentence best fits that video. Overall chance performance on our dataset is 49.04%,slightly lower than 50% due to the 1out-of-3 classification examples. The model presented here achieved an accuracy of 75.36% over the entire corpus averaged acrossall error categories. This demonstrates that the model is largely capable of capturing the underlyingtask and that similar compositional crossmodal models may do the same. For each of the 3 majorambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semanticambiguities, and 64.44% for discourse ambiguities.",
  "The most significant source of model failures are poor object detections. Objects are often rotatedand presented at angles that are difficult to recognize. Certain object classes like the telescope": "are much more difficult to recognize due to their small size and the fact that hands tend to largelyocclude them. This accounts for the degraded performance of the semantic ambiguities relative to thesyntactic ambiguities, as many more semantic ambiguities involved the telescope. Object detectorperformance is similarly responsible for the lower performance of the discourse ambiguities whichrelied much more on the accuracy of the person detector as many sentences involve only peopleinteracting with each other without any additional objects. This degrades performance by removing ahelpful constraint for inference, according to which people tend to be close to the objects they aremanipulating. In addition, these sentences introduced more visual uncertainty as they often involvedthree actors. The remaining errors are due to the event models. HMMs can fixate on short sequences of eventswhich seem as if they are part of an action, but in fact are just noise or the prefix of another action.Ideally, one would want an event model which has a global view of the action, if an object went upfrom the beginning to the end of the video while a person was holding it, its likely that the object wasbeing picked up. The event models used here cannot enforce this constraint, they merely assert thatthe object was moving up for some number of frames; an event which can happen due to noise in theobject detectors. Enforcing such local constraints instead of the global constraint of the motion of theobject over the video makes joint tracking and event recognition tractable in the framework presentedhere but can lead to errors. Finding models which strike a better balance between local informationand global constraints while maintaining tractable inference remains an area of future work.",
  "Conclusion": "We present a novel framework for studying ambiguous utterances expressed in a visual context. Inparticular, we formulate a new task for resolving structural ambiguities using visual signal. This is afundamental task for humans, involving complex cognitive processing, and is a key challenge forlanguage acquisition during childhood. We release a multimodal corpus that enables to address thistask, as well as support further investigation of ambiguity related phenomena in visually groundedlanguage processing. Finally, we",
  "present a unified approach for resolving ambiguous descriptions of videos, achieving good perfor-mance on our corpus": "While our current investigation focuses on structural inference, we intend to extend this line of workto learning scenarios, in which the agent has to deduce the meaning of words and sentences fromstructurally ambiguous input. Furthermore, our framework can be beneficial for image and videoretrieval applications in which the query is expressed in natural language. Given an ambiguous query,our approach will enable matching and clustering the retrieved results according to the different queryinterpretations."
}