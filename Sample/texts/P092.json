{
  "Abstract": "This manuscript provides an in-depth explanation of the methodology developedfor a recent image compression challenge. The method primarily incorporates twoinnovative aspects: the application of advanced residual networks for enhancedcompression and the utilization of sub-pixel convolution techniques for efficientup-sampling during decompression. The efficacy of these methodologies, whichachieved a high Multiscale Structural Similarity Index (MS-SSIM) of 0.972 undera strict bit rate constraint of 0.15 bits per pixel (bpp) while maintaining reasonablecomputational demands during the evaluation stage.",
  "Introduction": "Image compression remains a crucial research area within the field of signal processing, aiming tofacilitate more efficient data storage and transfer. Conventional image compression algorithms, likethe various JPEG standards, often employ manually designed encoder/decoder frameworks. However,with the emergence of novel image formats and the proliferation of high-resolution mobile devices,there is a growing recognition that existing standards may not represent the most effective or universalsolutions for image compression. Recently, deep learning-based techniques have shown a surge of progress in the image compressiondomain. Some of these methods employ generative models, trained adversarially, to effectively learnthe underlying distribution of images, resulting in impressive subjective quality even at exceptionallylow bit rates. Other works utilize recurrent neural networks to iteratively compress residual informa-tion, enabling progressive coding which allows for multiple quality levels within a single compressionoperation. Further advancements have been made by focusing on relaxing quantization constraintsand improving entropy modeling, leading to enhanced performance compared to established imagecompression methods. Nevertheless, identifying an optimal network structure presents a formidable challenge across variousmachine learning applications, including image compression. This paper primarily discusses twoimportant aspects of network design for image compression. The first concerns the selection of kernelsize, a parameter that significantly influences compression effectiveness in traditional algorithms.Motivated by its impact in classical methods, this paper presents experiments that use different filtersizes to prove that larger kernel sizes contribute to improved coding efficiency. Building upon this, astrategy is presented that utilizes a deep residual learning approach, allowing for the maintenanceof a broad receptive field while utilizing a reduced number of parameters. This approach not onlydecreases the models overall size but also substantially enhances its performance. Additionally,the architecture of up-sampling operations within the decoder plays a pivotal role in determiningthe quality of reconstructed images and the presence of artifacts. This issue, extensively studied inthe context of super-resolution, involves various implementations for up-sampling layers, such asinterpolation, transposed convolution, and sub-pixel convolution. This work compares two commonlyused up-sampling methods, transposed and sub-pixel convolutions, to demonstrate their relativeperformance in the context of image compression.",
  "J = d(x, x) + R(y)(1)": "where is a parameter that balances the importance of rate and distortion. The secondary autoencoderhandles the encoding of side information, which is used to model the probability distribution of thecompressed data. A Gaussian scale mixture approach is utilized to develop an image-adaptive entropymodel, with scale parameters conditioned on a hyperprior.",
  "From Small Kernel Size to Large Kernel Size": "In traditional image compression techniques, the size of transform filters significantly affects codingefficiency, especially for high-definition videos. Initially, transform sizes were small, but as the fieldprogressed, there was a gradual shift towards larger sizes to better capture spatial correlations andsemantic details. The experiments detailed in this paper, using a standard dataset, explore the impactof different filter sizes in both the main and auxiliary autoencoders. indicates that for theBaseline architecture, larger kernel sizes lead to better rate-distortion outcomes. Similarly, demonstrates comparable improvements for the HyperPrior architectures. reveals thatemploying large kernels in the auxiliary autoencoder does not enhance rate-distortion performanceand may even negatively impact it. This is likely due to the small size of the compressed codes, whichmakes smaller kernels sufficient for effective encoding. An excessive number of trainable parameterscan hinder the learning process.",
  "From Shallow Network to Deep Residual Network": "In terms of receptive field coverage, a sequence of four 3x3 kernels can encompass the same area asa single 9x9 kernel but with a reduced parameter count. Initial attempts to substitute a large kernelwith multiple 3x3 filters encountered convergence issues during training. To address this, shortcutconnections were incorporated between adjacent 3x3 kernels. The resultant deep residual network architecture for image compression is denoted as ResNet-3x3(4), signifying that a stack of four 3x3kernels achieves an equivalent receptive field to a 9x9 kernel. To minimize parameter overhead,GDN/IGDN activation functions are applied only once within each residual unit when the outputdimensions change. For the remaining convolutional layers, parameter-free Leaky ReLU activationsare employed to introduce non-linearity. As indicated in , ResNet-3x3(4) surpasses bothResNet-3x3(3) and Hyperprior-9 in terms of performance.",
  "Upsampling Operations at Decoder Side": "The encoder-decoder structure is characterized by its symmetrical design. While down-sampling atthe encoder is typically achieved using strided convolution filters, up-sampling at the decoder can beimplemented through various methods, such as bicubic interpolation, transposed convolution, andsub-pixel convolution. Considering the importance of rapid end-to-end learning, bicubic interpolationwas excluded, and a comparison was made between the two widely used up-sampling techniques:transposed convolution (TConv) and sub-pixel convolution (SubPixel). To implement sub-pixelconvolution, the channel count is expanded fourfold, followed by the application of a depth-to-spaceoperation. The results presented in demonstrate that sub-pixel convolution filters offer slightimprovements in both PSNR and MS-SSIM compared to transposed convolution filters.",
  "Experiments": "For the training process, 256x256 image patches were extracted from a large-scale image dataset. Abatch size of 8 was employed, and training was conducted for up to 2 million iterations to ensurestable convergence. Optimization was performed using the Adam optimizer, with an initial learningrate of 1 x 10<sup>-4</sup>, reduced to 1 x 10<sup>-5</sup> for the final 80,000 iterations. Two primary strategies were implemented. The first strategy, termed \"Wide Bottleneck,\" involvesincreasing the models capacity by expanding the number of filters. Since increasing filters in largefeature maps significantly increases computational cost (FLOPs), the filter count was only raised inthe encoders final layer, from 128 to 192. This results in a minor FLOPs increase, as detailed in . While Bottleneck192 effectively reduces the bit rate, it also leads to some quality degradationcompared to Bottleneck128.",
  "ResNet-3x3(4)-Bottleneck12826.4980.96220.1700ResNet-3x3(4)-Bottleneck19226.3170.96190.1667": "The second strategy is \"Rate Control.\" For achieving a target bit rate, two models are trained atdistinct bit rates by adjusting the parameter. This allows for adaptive selection during encoding toapproach the target bit rate while maximizing MS-SSIM, as shown in . A single bit is addedto the bitstream to indicate the model used for decoding, without increasing decoder complexity.",
  "EntryDescriptionPSNRMS-SSIMRate": "KattolabHyperPrior-928.9020.96740.134KattolabHyperPrior-9 + Rate Control29.1020.97010.150KattolabResNet-3x3(4)-TConv + Rate Control29.3150.97160.150Kattolabv2ResNet-3x3(4)-SubPixel+ Rate Control29.3000.97200.150KattolabSSIMResNet-3x3(4)-SubPixel + Wide Bottleneck + Rate Control29.2110.97240.150 While deep residual networks enhance coding gain, they also lead to a substantial increase in modelsize. This section analyzes the parameter count and model complexity in terms of floating-pointoperations per second (FLOPs) for various architectures. Specifically, using the HyperPrior-9architecture as an example, provides a layer-wise breakdown of model size. The number ofparameters and FLOPs are calculated as follows:",
  "FLOPs = Para H W (3)": "where h w represents the kernel size, H W denotes the output dimensions, and Cin and Coutare the number of input and output channels, respectively. The +1 term is omitted when no biasis used. Quantization and leaky-ReLU are parameter-free. GDN operates across channels but notspatial positions, resulting in a parameter count of (Cin + 1) Cout. The total FLOPs for GDNand inverse GDN calculations are minimal. This analysis primarily focuses on the backbone ofconvolutional layers, so the FLOPs of GDN, inverse GDN, and factorized prior are not included in thecomparison. presents a comparison of different architectures, with the last column showingthe relative FLOPs using Baseline-5 as the reference. The proposed models achieve improved codingperformance with relatively low computational complexity.",
  "Conclusion": "This manuscript details the proposed deep residual learning framework and sub-pixel convolutiontechnique for image compression, forming the foundation of the submitted entries: Kattolab, Katto-labv2, and KattolabSSIM. The results demonstrate that these approaches achieve a high MS-SSIM of0.972 under a bit rate constraint of 0.15 bpp, while maintaining a moderate level of computationalcomplexity during the validation phase.",
  "LayerKernelChannelOutputParaFLOPshwCinCoutH x W": "conv1993128128 x 128312325.12 x 10<sup>9</sup>conv29912812864 x 6413272325.44 x 10<sup>7</sup>conv39912812832 x 3213272321.36 x 10<sup>7</sup>conv49912812816 x 1613271043.40 x 10<sup>6</sup>GDN/IGDN99072-Hconv13312812816 x 161475843.78 x 10<sup>6</sup>Hconv2551281288 x 84097282.62 x 10<sup>6</sup>Hconv3551281284 x 44097286.56 x 10<sup>5</sup>FactorizedPrior5888-HTconv1551281288 x 84097282.62 x 10<sup>6</sup>HTconv25512819216 x 166145921.57 x 10<sup>7</sup>HTconv33319225616 x 164426241.13 x 10<sup>7</sup>layer125664016 x 161644804.21 x 10<sup>6</sup>layer264051216 x 163281928.40 x 10<sup>6</sup>layer351225616 x 161310723.36 x 10<sup>6</sup>Tconv19912812832 x 3213272321.36 x 10<sup>7</sup>Tconv29912812864 x 6413272325.44 x 10<sup>7</sup>Tconv399128128128 x 12813272322.17 x 10<sup>10</sup>Tconv4991283256 x 256311072.04 x 10<sup>7</sup>"
}