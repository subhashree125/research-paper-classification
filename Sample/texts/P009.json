{
  "Abstract": "Bayesian learning often necessitates online inference, adaptive models, and the combination of multiple distinctmodels. Recent advancements have demonstrated the use of random feature approximations for scalable, onlineaggregation of Gaussian processes, which possess favorable theoretical characteristics and practical uses. A crucialaspect of these methods is the incorporation of a random walk on model parameters, which introduces adaptability.We demonstrate that these methods can be readily extended to any model using basis function expansion and thatemploying alternative basis expansions, such as Hilbert space Gaussian processes, frequently leads to enhancedperformance. To streamline the selection of a specific basis expansion, the versatility of our approach also enablesthe aggregation of several entirely different models, such as a Gaussian process and polynomial regression. Lastly,we introduce an innovative technique for combining both static and dynamic models.",
  "Introduction": "Numerous machine learning applications demand real-time, online data processing, a scenario that frequently requires substantialalterations to conventional techniques. Online adaptations of various methods have been developed, including kernel machines,(kernel) least-squares, and Gaussian processes. The field of online learning has also been thoroughly investigated from an optimizationstandpoint. Online learning can be further complicated when model selection is needed, as the best-performing model is rarely evident at theoutset of the learning process. One solution involves training multiple models concurrently and then combining them. In a Bayesianframework, Bayesian model averaging (BMA) has long been employed to combine online models, functioning by assigning weightsto each \"expert\" model based on its supporting evidence. More recently, it was shown how to adapt BMA to online Gaussian processes (GPs) in a technique called incremental ensemblesof GPs. GPs are a versatile, non-parametric instrument in Bayesian machine learning that possesses universal approximationcapabilities and provides well-founded uncertainty estimations. By employing a random Fourier feature (RFF) approximation forGaussian processes, online learning can be executed, featuring closed-form Bayesian model averaging updates and a manageableregret analysis. Besides an online ensemble of GPs, the advantages of incorporating random walks on model parameters were illustrated, which theyterm dynamic IE-GPs (DIE-GPs). This can significantly enhance performance when the learning task undergoes slight changes overtime.",
  "Related Work": "The concept of combining random feature GPs, as introduced by IE-GPs, has demonstrated adaptability and effectiveness. Extensionsto this framework encompass Gaussian process state-space models, deep Gaussian processes, and graph learning. Along with itsextensions, DIE-GPs have been effectively applied in Bayesian optimization and causal inference. However, the dependence on the RFF approximation implies that IE-GPs also inherit the limitations of random feature GPs.Specifically, the RFF approximation is a direct Monte Carlo approximation of the Wiener-Khinchin integral and thus is significantlyimpacted by the curse of dimensionality. Our findings reveal that on several real-world datasets, (D)IE-GPs exhibit performance thatis comparable to or worse than that of simpler models, such as online Bayesian linear regression and one-layer RBF networks.",
  "Methodology": "In this paper, we present online ensembles of basis expansions (OEBEs), a generalization of IE-GPs that overcomes their dependenceon RFF GPs and enhances performance across multiple real datasets. Our specific contributions are as follows: 1. We observe that the derivation of DIE-GPs does not rely on the RFF approximation, except for its role as a linear basisexpansion. The same derivations and code can be reused to combine arbitrary Bayesian linear models with any design matrix.This allows for the combination of not only models of the same type but also various distinct basis expansions (e.g., B-splines,one-layer RBF networks, etc.). 2. We contend that a GP with a generalized additive model (GAM) structure is often moresuitable when GP regression is the focus. To this end, we employ GAM Hilbert space Gaussian processes (HSGPs), which canbe interpreted as a quadrature rule for the same integral that the RFF approximation addresses through direct Monte Carlo. Apartfrom theoretical considerations, empirical evidence indicates that HSGPs converge to the true approximated GP more rapidly (interms of the number of basis functions) than RFF GPs. We offer a similar empirical evaluation. 3. We introduce a new method forintegrating static and dynamic models, enabling the use of principled posteriors of static methods when appropriate and extendingthe expressiveness of dynamic methods otherwise. We demonstrate the necessity of this method by providing a constructive exampleon real data where the naive approach to combining static and dynamic methods is unsuccessful. 4. We provide Jax/Objax codeat that only requires the user to specify the designmatrix, with several choices already implemented. The remainder of this paper is organized as follows: reviews foundational concepts in linear basis expansions, GP regression,spectral approximations of GPs, and BMA. These concepts are put into practice in , where we present the OEBEs andseveral extensions, including applications to non-Gaussian likelihoods, and provide some concise theoretical observations. We offerfurther practical insights regarding the development of OEBEs, including a discussion on the composition of an ensemble and howto combine static and dynamic models in . The proposed models are empirically evaluated in . Finally, we presentconcluding remarks and suggest future directions in .",
  "Experiments": "We present three distinct experiments in the main text, with supplementary experiments in the appendices. In the first experiment(.1), we assess ensembles of several different basis expansions, demonstrating that the best-performing model variesconsiderably. In the second experiment (.2), we illustrate how model collapse can occur between static and dynamic modelsand how the model introduced in .2 mitigates this issue. Lastly, we demonstrate that E-DOEBE can effectively combinemethods that are both static and dynamic, and of different basis expansions (.3). The metrics we employ are the normalized mean square error (nMSE) and the predictive log-likelihood (PLL). The nMSE is definedas the MSE of yt with the predictive mean, divided by the variance of y1:T. Specifically, at time t, the nMSE is calculated as:",
  "t": "Across all experiments, we utilize several publicly available datasets, varying in both size and the number of features. A summaryof dataset statistics is provided in . Friedman 1 and Friedman 2 are synthetic datasets designed to be highly nonlinear and,notably, are i.i.d. The Elevators dataset pertains to controlling the elevators on an aircraft. The SARCOS dataset uses simulations ofa robotic arm, and Kuka 1 is a similar real dataset derived from physical experiments. CaData comprises California housing data,and the task of CPU Small is to predict a type of CPU usage based on system properties. All hyperparameter optimization was performed on the first 1,000 samples of each dataset; since we already assume access, eachdataset was additionally standardized in both x and y using the statistics of the first 1,000 samples. We follow prior work in setting aweight to 0 when it falls below the threshold of 10-16.",
  "Comparing Different Basis Expansions": "To demonstrate that having a diverse set of basis expansion models available is beneficial, we evaluate several model types on eachdataset listed in . Furthermore, we examine both static and dynamic versions of models to assess their performance. Models used for comparison include an additive HSGP model [(D)OE-HSGP], an RFF GP [(D)OE-RFF], an ensemble of quadratic,cubic, and quartic polynomials with additive structure [(D)OE-Poly], linear regression [(D)OE-Linear], and a one-layer RBF network[(D)OE-RBF]. Apart from additional hyperparameter tuning in an ARD kernel, the (D)OE-RFF model is identical to the (D)IE-GP. For RFF GPs, 50 Fourier features were employed (resulting in F = 2 50), and for HSGPs, 230a100/D230b features were used foreach dimension (resulting in F 2272 100). An SE-ARD kernel was utilized in both cases. For RBF networks, 100 locations wereinitialized using K-means and subsequently optimized with empirical Bayes, along with ARD length scales. For all models exceptRBF networks, ensembles were generated using the process outlined in .1 2014 for RBF networks, the computation of theHessian was too computationally demanding, so parameters were randomly perturbed by white Gaussian noise with variance 10-3instead.",
  "For dynamic models, 03c32 was set to 10-3. The initial values of 03c3203b8 and 03c3203f5 were 1.0 and 0.25, respectively.Optimization was carried out using Adam": "Results of the average nMSE and PLL are presented in and . We observe that the best-performing class of modelsvaries significantly across datasets. Specifically, in terms of both nMSE and PLL, HSGPs, RFF GPs, and RBF networks each achievethe best performance on at least one dataset. This reinforces the notion that combining several different models is advantageous, asno single method consistently outperforms the others. Moreover, as anticipated, dynamic models can substantially outperform static models in specific scenarios (e.g., on SARCOS andKuka 1) but yield a lower PLL on datasets where the data is reasonably i.i.d. (e.g., Friedman 1). As expected, when an additive structure is a reasonable approximation, additive HSGP methods surpass RFF GPs, for instance, onKuka 1 and CaData. The RFF GP approximation rarely exhibits particularly poor performance, making it a consistently \"good\"estimator, and it achieves the highest PLL on Friedman 2, SARCOS, and CPU Small. However, it is also occasionally outperformedby simpler methods, such as the RBF network, highlighting the potential advantages of employing diverse basis expansions. Key Takeaways Key takeaways from this experiment include: (1) neither dynamic nor static methods are strictly superior across allsettings, (2) no single basis expansion is superior across all datasets, and (3) RFF GPs consistently provide good performance, butthis performance can often be improved upon by using other basis expansions.",
  "The Necessity of Ensembles of Dynamic Ensembles": "In this experiment, we demonstrate that the E-DOEBE model introduced in .2 can indeed prevent the premature collapse ofBMA weights. While this premature collapse of BMA weights does not appear to be common in real datasets, it is not difficult toillustrate its possibility, even on real datasets with high-performing methods. As a constructive example, we can create an ensemble of additive HSGPs on the Kuka 1 dataset, where dynamic models performedsignificantly better in .1. Specifically, we created an ensemble of two additive HSGPs, with the first model being dynamic(03c3(1)rw2 = 10-3) and the second model being static (03c3(2) = 0). The ensemble hyperparameters were determined usingempirical Bayes, with initial length scale values set to the vector of ones. Subsequently, the resulting ensemble was trained online asa DOEBE and as an E-DOEBE, with 03b4 = 10-2. Note that in this carefully controlled setting, each basis expansion is entirelydeterministic given the hyperparameters, so the results are purely deterministic and cannot be attributed to poor random seeds. The resulting weights demonstrate that premature collapse of BMA weights can be a problem. Numerically, the log-likelihood of theE-DOEBE model is dramatically better than that of the DOEBE model (), showing this collapse can be catastrophic. This issue can be partially averted by eliminating the threshold of 10-16 when ensembling. Indeed, in this example, the weightsreach a minimum of approximately 10-72. However, with any finite precision arithmetic, there is always the potential for this typeof collapse to occur due to numerical underflow. It is trivial to construct such examples by generating the first N1 samples with03c3(m)rw = 0 until weight collapse occurs, and the rest of the dataset with 03c3(m)rw > 0. Key Takeaway The key takeaway of this experiment is that an ensemble of dynamic and static models can catastrophically collapse2014 even when the discrepancy in performance along the entire dataset is large 2014 and that the E-DOEBE approach proposed in.2 can avoid this collapse.",
  "E-DOEBE Outperforms Other Methods": "The ultimate goal of the E-DOEBE model is to combine static and dynamic models of several different types. To do so, we repeatthe experiments of .1 while comparing to an E-DOEBE model. We restrict our attention to static and dynamic versions ofthe three best-performing families of models in Experiment 1 ((D)OE-HSGP, (D)OE-RFF, and (D)OE-RBF), and an E-DOEBEensemble containing all of them. The E-DOEBE model is created with 03b4 = 10-2, which was not tuned. As desired, the E-DOEBE model can effectively ensemble dynamic and static models of different basis expansions. Across allexperiments, the E-DOEBE model performs the best in terms of PLL, and is the best in terms of NMSE for all but one dataset(Friedman 2).",
  "Conclusion": "In this paper, we demonstrated that recent advancements in online prediction using RFF GPs can be extended to arbitrary linear basisexpansions. This included several basis expansions that surpass RFF GPs on real and synthetic datasets. We show how differentlinear basis expansions can be combined within a simple framework, enhancing ensemble diversity. While several common choicesof basis expansions were employed, it would be worthwhile to expand the tests even further, particularly with splines. We also demonstrated that the premature collapse of BMA weights can be a concern in online combining. We introduced theE-DOEBE model, which mitigates this issue, and demonstrated its effectiveness. However, this meta-combining may be perceived as adding a complex workaround to BMA rather than addressing the underlying problems. Further research could explore theincorporation of other Bayesian combining methods, such as Bayesian (hierarchical) stacking. While we provide guidance on initializing ensembles given a set of basis expansions, determining which basis expansions to use isan important open topic. A naive approach would be to expand on the existing use of the marginal likelihood for model selection,but this may be \"unsafe\" when using different basis expansions and therefore requires caution. We additionally presented severalideas for inference with non-Gaussian likelihoods, for example, for classification tasks. Determining which, if any, of these tasks issuperior to the Laplace approximation is another interesting topic for future study. Finally, it could be beneficial to modify or add new basis expansions in the online setting. Indeed, recent progress in GPs has workedtowards selecting and adapting kernels online to great benefit. If such techniques could be adapted to DOEBE, it could eliminate thepre-training period and allow for adapting the domain of approximations when new data arrives."
}