{
  "Abstract": "Real-world business applications require a trade-off between language modelperformance and size. We propose a new method for model compression that relieson vocabulary transfer. We evaluate the method on various vertical domains anddownstream tasks. Our results indicate that vocabulary transfer can be effectivelyused in combination with other compression techniques, yielding a significantreduction in model size and inference time while marginally compromising onperformance.",
  "Introduction": "In the last few years, many NLP applications have been relying more and more on large pre-trainedLanguage Models (LM). Because larger LMs, on average, exhibit higher accuracy, a common trendhas been to increase the models size. Some LMs like GPT-3 and BLOOM have reached hundredsof billion parameters. However, these models superior performance comes at the cost of a steepincrease in computational footprint, both for development and for inference, ultimately hamperingtheir adoption in real-world business use-cases. Besides models that only a few hi-tech giants canafford, like GPT-3, even smaller LMs with hundreds of million parameters could be too expensiveor infeasible for certain products. For one thing, despite being tremendously cheaper than theirbigger cousins, fine-tuning, deploying and maintaining large numbers of such models (one for eachdownstream task) soon becomes too expensive. Furthermore, latency and/or hardware requirementsmay limit their applicability to specific use-cases. For all these reasons, significant efforts - in bothacademic and industry-driven research - are oriented towards the designing of solutions to drasticallyreduce the costs of LMs. Recently, several attempts have been made to make these models smaller, faster and cheaper, whileretaining most of their original performance. Knowledge Distillation (KD) is a teacher-studentframework, whereby the teacher consists of a pre-trained large model and the student of a smallerone. The teacher-student framework requires that both the teacher and the student estimate the sameprobability distribution. While the outcome is a smaller model, yet, this procedure constrains thestudent to operate with the same vocabulary as the teacher in the context of Language Modeling. In this work, we explore a method for further reducing an LMs size by compressing its vocabularythrough the training of a tokenizer in the downstream task domain. The tokenizer is a crucial partof modern LMs. In particular, moving from word to subword- level, the tokenization solves twoproblems: vocabulary explosion and unknown words. Moreover, the capability to tokenize texteffectively in any domain is key for the massive adoption of pre-trained general-purpose LMs fine-tuned on downstream tasks. Indeed, tokenizers are still able to process out-of-distribution texts at thecost of producing frequent word splits into multiple tokens. However, the language varies significantly in vertical domains or, more generally, in different topics.Hence, ad-hoc tokenizers, trained on the domain statistics, may perform a more efficient tokenization,reducing on average the length of the tokenized sequences. This is important since compact andmeaningful inputs could reduce computational costs, while improving performance. Indeed, memoryand time complexity of attention layers grows quadratically with respect to the sequence length.",
  "Furthermore, a vertical tokenizer may require a smaller vocabulary, which also affects the size of theembedding matrix, hence further reducing the models size": "Following this intuition, we propose a Vocabulary Transfer (VT) technique to adapt LMs to in-domain,smaller tokenizers, in order to further compress and accelerate them. This technique is complementaryto the aforementioned model compression methods and independent of the type of tokenizer. As amatter of fact, we apply it in combination with KD. Our experiments show that VT achieves an inference speed-up between x1.07 and x1.40, dependingon the downstream task, with a limited performance drop, and that a combination of VT with KDyields an overall reduction up to x2.76.",
  "Related Work": "The goal of Model Compression is to shrink and optimize neural architectures, while retaining mostof their initial performance. Research on LM compression has been carried out following a variety ofapproaches like quantization, pruning knowledge distillation, and combinations thereof. A most popular distillation approach in NLP was proposed by Sanh et al. (2019). The obtainedmodel, called DistilBERT, is a smaller version of BERT, with the same architecture but half the layers,trained to imitate the full output distribution of the teacher (a pre-trained BERT model). DistilBERThas a 40 Little focus has been devoted thus far to the role of tokenization in the context of model compression.Even in domain adaptation, the vocabulary was kept the same. Both the versatility of the subword-level tokenization, and the constraints imposed by the teacher- student framework (same outputdistribution), discouraged such investigations. Recently, Samenko et al. (2021) presented an approachfor transferring the vocabulary of an LM into a new vocabulary learned from new domain, with thepurpose of boosting the performance of the fine-tuned model. To the best of our knowledge, we arethe first to study VT in the scope of model compression.",
  "Vocabulary Transfer": "Let us consider a LM, trained on a general-purpose domain Dgen and associated with a vocabularyVgen. Such a vocabulary is used by the LMs tokenizer in order to produce an encoding of the inputstring via an embedding matrix Egen defined on Vgen. More specifically, a tokenizer is a functionthat maps a textual string into a sequence of symbols of a given vocabulary V . Let T be a tokenizerassociated with a vocabulary V and a string s, we have T : s (t1, . . . , tn), ti V, i = 1, . . . , n.Hence, the vocabulary of the tokenizer determines how words in a text are split, whether as words,sub-words, or even characters. These symbols, which define the LMs vocabulary, are statisticallydetermined by training the tokenizer to learn the distribution of a dataset. Now, let us consider a vertical domain Din, also referred as in-domain. For the reasons discussedearlier, a vocabulary Vin specialized on Din itself better fits the language distribution than Vgen.Unfortunately, with a new vocabulary, embedding representations associated with the tokens of Vgenwould be lost. Thus, VT aims to initialize Vin by re-using most of the information learned from theLM pre-trained on Dgen. Once the new tokenizer Tin has been trained on the in-domain dataset Dinusing a given vocabulary size, Tin will be different from the LMs tokenizer Tgen. However, the twotokenizers vocabularies Vgen and Vin may still have a large portion of their symbols in common.Our objective is to transfer most of the information from Vgen into Vin. To this end, we first define amapping between each symbol in Vin and a set of symbols in Vgen. Then, we define an assignmentcriterion, based on the mapping, to obtain the embeddings for the tokens of Tin. One such criterion, called Vocabulary Initialization with Partial Inheritance (VIPI), was defined bySamenko et al. (2021). Whenever a token is in Vin but not in Vgen, VIPI calculates all the partitionsof the new token with tokens from Vgen, then takes the minimal partitions and finally averages themto obtain an embedding for the new token. Differently, we define a simplified implementation of VIPI called FVT for Fast Vocabulary Transfer. Instead of calculating all tokenizations, FVT uses astraightforward assignment mechanism, whereby each token ti Vin is partitioned using Tgen. If tibelongs to both vocabularies, ti Vin Vgen, then Tgen(ti) = ti and the in-domain LM embedding.",
  "Please notice that Equation (2) is a generalization of Equation (1). Indeed, in case ti Vin Vgen,Equation (2) falls back to Equation (1)": "Once embeddings are initialized with FVT, we adjust the models weights by training it with MLMon the in-domain data before fine-tuning it on the downstream task. MLM eases adaptation and hasalready been found to be beneficial in (Samenko et al., 2021). We observed this trend as well duringpreliminary experiments, therefore we kept such a tuning stage in all our experiments. As a baseline model, we also implement a method called Partial Vocabulary Transfer (PVT), wherebyonly the tokens belonging to both vocabularies ti Vin Vgen are initialized with pre-trainedembeddings, while unseen new tokens are randomly initialized.",
  "Distillation": "VT can be combined with other model compression methods like quantization, pruning and KD. Forsome of the methods, the combination is trivial, since they have no impact on the vocabulary. KD,however, requires the vocabularies of the student and teacher to be aligned. Hence, its integrationwith VT is non-trivial. Accordingly, we set up a KD procedure with VT, in order to determine theeffects of applying both VT and KD to an LM. Our distillation consists of two steps. In the first step, we replicate the distillation process used in(Sanh et al., 2019) for DistilBERT, in which the number of layers of the encoder is halved and atriple loss-function is applied: a distillation loss, a MLM loss, and a cosine embedding loss. However,unlike the original setup, we do not remove the token-type embeddings and pooler. after distilling thestudent on Dgen, we further distil the student using Din. However, instead of adapting the teacherbefore the second distillation, we simply distil the student a second time on the in-domain dataset.Finally, we apply VT using either FVT or PVT and fine-tune the student model on the in-domaindatasets. Our choice of applying VT after KD is based on findings by Kim and Hassan (2020), that differentinput embedding spaces will produce different output embedding spaces. This difference in spaces isnot conducive to knowledge transfer during distillation. Hence, if VT were to be applied first to thestudent, its input embedding space would differ greatly from that of the pre-trained teacher duringdistillation.",
  "Experimental Setup": "We consider for all our experiments the pre-trained cased version of BERTbase as our pre-trainedlanguage model. Its tokenizer is composed of 28996 wordpieces. We then define four vocabularysizes for retraining our tokenizers. Specifically, we take the original vocabulary size and define itas a vocabulary size of 100 percent. We subsequently reduce this size to 75percent, 50percent, and25percent, From now on, we will refer to such tokenizers as T100, T75, T50, T25 respectively, whilethe original vocabulary will be called Tgen. Models are fine-tuned for 10 epochs with early stopping on the downstream task. We set the initiallearning rate to 3 105 and batch size to 64 for each task. The sequence length is set to 64 for ADEand CoNLL03 and 128 for LEDGAR. Each configuration is repeated 3 times with different randominitializations. MLM is performed for one epoch.",
  "ADE. The Adverse Drug Events (ADE) corpus is a binary sentence": "classification dataset in the medical domain. This domain is particularly suitable for investigating thebenefits of VT, since documents are characterized by the presence of frequent technical terms, suchas drug and disease names, that are usually rare in common language. Domain-specific words areusually split into multiple tokens, yielding longer sequences and breaking the semantics of a wordinto multiple pieces. An example is shown in . LEDGAR. LEDGAR is a document classification corpus of legal provisions in contracts fromthe US Securities and Exchange Commission (SEC). The dataset is annotated with 100 differentmutually-exclusive labels. It is also part of LexGLUE, a benchmark for legal language understanding. CoNLL03. CoNLL03 is a popular Named Entity Recognition (NER) benchmark. It is made of newsstories from the Reuters corpus. We chose this corpus because, differently from ADE and LEDGAR,the news domain typically uses a more standard language, hence we expect its distribution to differless from the one captured by a general-purpose tokenizers in the web. Statistics in confirmsthis hypothesis. We can observe that the sequence compression gain obtained with domain- specifictokenizers is less significant with respect to LEDGAR and ADE.",
  "We report an extensive evaluation of FVT on different setups and perspectives": "In-domain Tokenization. By retraining the tokenizer on the in-domain dataset, the average number oftokens per sequence decreases since the learned distribution reduces the number of word splits, asshown in . In the medical domain, which is particularly specialized, we notice a remarkable32 : Average sequence length on the three datasets with different tokenizers. Tgen is the generictokenizer (BERT cased), the same in each corpus, while T percent are the tokenizers trained in thevertical domain itself.",
  "ADE3121222326LEDGAR155131131132135CoNLL031917171820": "Vocabulary Transfer. From the results shown in Tables 2 and 3, we note a few interesting findings.First, FVT vectors initialization method consistently outperforms the baseline PVT, which confirmsthe positive contribution of Equation 2. Second, transferring vocabulary with FVT causes limiteddrops in performance, especially in LEDGAR (the largest one), where F1 slightly increases despite a75 : F1 results on the three benchmarks. A pre- trained language model fine-tuned on the task(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)adapted by transferring information with FVT or PVT.",
  "TransferADELEDGARCoNLL03": "Tgen90.8080.9389.43T100 + FVT90.7780.6087.87T75 + FVT90.4080.9387.90T50 + FVT90.0780.9386.87T25 + FVT90.2781.0386.17T100 + PVT82.5780.0784.53T75 + PVT82.4780.3384.63T50 + PVT83.0780.2384.43T25 + PVT83.5780.2083.47 : F1 results on the three benchmarks. A distilled language model fine-tuned on the task(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)adapted by transferring information with FVT or PVT.",
  "ADELEDGARCoNLL03": "Tgen90.4778.3786.90T100 + FVT89.4778.3384.63T75 + FVT88.5778.9084.23T50 + FVT88.4379.3083.80T25 + FVT88.2378.1083.13T100 + PVT79.1376.9781.13T75 + PVT78.8776.9381.40T50 + PVT76.3077.3781.63T25 + PVT77.9077.3379.50 Vocabulary Transfer and Distillation. The results summarized in clearly indicate that KDis complementary to VT: there is no harm in applying them together, in terms of performance onthe downstream task. Crucially, this guarantees a full exploitation of FVT in the scope of languagemodel compression. Compression and Efficiency. After showcasing that VT has limited impact on performance, weanalyze and discuss its effects on efficiency and model compression. reports the relativeF1 drop on the downstream task with respect to the original LM (2206F1), the relative reduction inmodel size (2206Size) and the speedup gained by FVT alone and by FVT combined with KD forvarying vocabulary sizes. Either way, FVT achieves a remarkable 15 Furthermore, the reduced input length enabled by in-domain tokenization brings a reduction ininference time. The more a language is specialized, the higher is the speedup with in-domaintokenizers. This is also confirmed by the experiments, where the major benefits are obtained on themedical domain, with a x1.40 speedup. In CoNLL03 instead where language is much less specialized,speedup reduces and even disappears with T25. Distillation further pushes compression and speedupin any benchmark and setup, up to about 55",
  "Conclusion": "The viability and success of industrial NLP applications often hinges on a delicate trade-off betweencomputational requirements, responsiveness and output quality. Hence, language model compressionmethods are an active area of research whose practical ramifications are self-evident. One of thefactors that greatly contribute to a models inference speed and memory footprint is vocabulary size.VT has been recently proposed for improving performance, but never so far in the scope of model",
  "F12206SizeSpeedup2206F12206SizeSpeedup2206F12206SizeSpeedup": "Tgen90.80433.321.0080.93433.621.0089.43430.981.00T100 + FVT-0.040.001.40-0.410.001.21-1.750.001.07T75 + FVT-0.44-5.141.350.00-5.141.21-1.71-5.171.07T50 + FVT-0.81-10.281.320.00-10.271.10-2.87-10.331.02T25 + FVT-0.59-15.421.200.12-15.411.09-3.65-15.500.99Distil + T100 + FVT-1.47-39.262.76-3.21-39.242.38-5.37-39.482.11Distil + T75 + FVT-2.46-44.402.64-2.51-44.372.38-5.81-44.642.11Distil + T50 + FVT-2.61-49.542.59-2.02-49.512.16-6.30-49.812.01Distil + T25 + FVT-2.83-54.682.37-3.50-54.642.14-7.04-54.981.96 compression. In this work, we run an extensive experimental study on the application of a lightweightmethod for VT, called FVT. An analysis conducted on various downstream tasks, application domains,vocabulary sizes and on its possible combination with knowledge distillation indicates that FVTenables a strategic trade-off between compression rate, inference speed and accuracy, especially, butnot only, in more specialized domains. Importantly, FVT appears to be orthogonal to other modelcompression methods."
}