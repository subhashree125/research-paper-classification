{
  "Abstract": "Contrastive instance discrimination techniques exhibit superior performance indownstream tasks, including image classification and object detection, compared tosupervised learning. However, a strong reliance on data augmentation during repre-sentation learning is a hallmark of these methods, potentially causing suboptimaloutcomes if not meticulously executed. A prevalent data augmentation approach incontrastive learning involves random cropping followed by resizing. This practicemight diminish the quality of representation learning when two random cropsencompass disparate semantic information. To counter this, we propose an inno-vative framework termed LeOCLR (Leveraging Original Images for ContrastiveLearning of Visual Representations). This framework integrates a novel instancediscrimination strategy and a refined loss function, effectively mitigating the lossof crucial semantic features that may arise from mapping different object segmentsduring representation learning. Our empirical evaluations reveal that LeOCLR con-sistently enhances representation learning across a spectrum of datasets, surpassingbaseline models. Notably, LeOCLR exhibits a 5.1% improvement over MoCo-v2on ImageNet-1K in linear evaluation and demonstrates superior performance intransfer learning and object detection tasks compared to several other techniques.",
  "Introduction": "Self-supervised learning (SSL) methods based on instance discrimination are heavily dependent ondata augmentations, like random cropping, rotation, and color jitter, to construct invariant repre-sentations for all instances within a dataset. These augmentations are used to generate two alteredviews (positive pairs) of the same instance, which are subsequently drawn closer in the latent space.Simultaneously, strategies are employed to prevent a collapse to a trivial solution, commonly referredto as representation collapse. The efficacy of these methods in acquiring meaningful representationshas been demonstrated through various downstream tasks, such as image classification and objectdetection, serving as proxies for evaluating representation learning. However, these techniquesoften overlook the crucial aspect that augmented views may diverge in semantic content becauseof random cropping, potentially degrading the quality of visual representation learning. Creatingpositive pairs via random cropping and subsequently prompting the model to align them based onshared information in both views poses an increased challenge to the SSL task, ultimately leading toan enhancement in representation quality. Moreover, random cropping followed by resizing guidesthe models representation to encompass object-related information across diverse aspect ratios,thereby promoting invariance to occlusions. Conversely, minimizing the feature distance in the latentspace, which equates to maximizing similarity, between views that encompass distinct semanticconcepts may inadvertently discard valuable image information. Instances of incorrect semantic positive pairs, which are pairs containing mismatched semanticinformation about the same object, might arise from random cropping. When the model is compelledto align the representations of different parts of an object closer in the latent space, it may discardcrucial semantic features. This occurs because the models representations are based on the sharedarea between the two views. If this shared region lacks semantically consistent information, the",
  "Related Work": "Self-supervised learning (SSL) techniques are categorized into two primary groups: contrastive andnon-contrastive learning. While all these techniques endeavor to approximate positive pairs in thelatent space, they employ distinct strategies to circumvent representation collapse. **Contrastive Learning:** Instance discrimination techniques, such as SimCLR, MoCo, and PIRL,employ a similar concept. These methods bring the positive pairs closer while driving the negativepairs apart in the embedding space, albeit through different mechanisms. SimCLR employs anend-to-end strategy where a large batch size is utilized for negative examples, and the parameters ofboth encoders in the Siamese network are updated simultaneously. PIRL uses a memory bank fornegative examples, and both encoders parameters are updated together. MoCo adopts a momentumcontrastive approach where the query encoder is updated during backpropagation, which subsequentlyupdates the key encoder. Negative examples are maintained in a separate dictionary, facilitating theuse of large batch sizes.",
  "**Non-Contrastive Learning:** Non-contrastive techniques utilize solely positive pairs to learnvisual representations, employing a variety of strategies to prevent representation collapse. The": "initial category encompasses clustering-based techniques, where samples exhibiting similar featuresare assigned to the same cluster. DeepCluster employs pseudo-labels from the previous iteration,rendering it computationally demanding and challenging to scale. SWAV addresses this challenge byimplementing online clustering, though it necessitates determining the correct number of prototypes.The second category involves knowledge distillation. Techniques like BYOL and SimSiam utilizeknowledge distillation methods, where a Siamese network comprises an online encoder and a targetencoder. The target networks parameters are not updated during backpropagation. Instead, solelythe online networks parameters are updated while being encouraged to predict the representation ofthe target network. Despite the encouraging results, the mechanism by which these methods preventcollapse remains not fully understood. Inspired by BYOL, Self-distillation with no labels (DINO)employs centering and sharpening, along with a distinct backbone (ViT), enabling it to surpass otherself-supervised techniques while maintaining computational efficiency. Another method, Bag ofvisual words (BoW), employs a teacher-student framework inspired by natural language processing(NLP) to avert representation collapse. The student network predicts a histogram of the features foraugmented images, analogous to the teacher networks histogram. The final category is informationmaximization. Methods like Barlow twins and VICReg eschew negative examples, stop gradient,or clustering. Instead, they utilize regularization to avoid representation collapse. The objectivefunction of these techniques seeks to eliminate redundant information in the embeddings by aligningthe correlation of the embedding vectors closer to the identity matrix. While these techniques exhibitencouraging results, they possess limitations, including the sensitivity of representation learning toregularization and reduced effectiveness if certain statistical properties are absent in the data. **Instance Discrimination With Multi-Crops:** Various SSL techniques introduce multi-crop strate-gies to enable models to learn visual representations of objects from diverse perspectives. However,when generating multiple cropped views from the same object instance, these views might containdisparate semantic information. To tackle this issue, LoGo generates two random global crops andN local views. They posit that global and local views of an object share similar semantic content,enhancing similarity between these views. Simultaneously, they contend that different local viewspossess distinct semantic content, thus diminishing similarity among them. SCFS proposes a differentapproach for managing unmatched semantic views by searching for semantically consistent featuresbetween the contrasted views. CLSA generates multiple crops and applies both strong and weakaugmentations, using distance divergence loss to enhance instance discrimination in representationlearning. Prior methods assume that global views contain similar semantic content and treat themindiscriminately as positive pairs. However, our technique suggests that global views might containincorrect semantic pairs due to random cropping, as illustrated in in the original paper.Therefore, we aim to attract the two global views to the original (intact and uncropped) image, whichfully encapsulates the semantic features of the crops.",
  "Methodology": "The mapping of incorrect semantic positive pairs, specifically those containing different semanticviews, results in the loss of semantic features, which in turn degrades the models representationlearning. To address this, we propose a novel contrastive instance discrimination SSL strategy calledLeOCLR. Our approach is designed to capture meaningful features from two random positive pairs,even when they encompass different semantic content, thereby improving representation learning.Achieving this necessitates ensuring the semantic correctness of the information within the sharedregion between the attracted views. This is crucial because the selection of views dictates theinformation captured by the representations learned in contrastive learning. Given that we cannotguarantee the inclusion of correct semantic parts of the object within the shared region between thetwo views, we propose the inclusion of the original image in the training process. The original imageX, which is not subjected to random cropping, encompasses all the semantic features of the twocropped views, X1 and X2. Our method, illustrated in (left) in the original paper, generates three views (X, X1, andX2). The original image (X) is resized without cropping, while the other views (X1 and X2) undergorandom cropping and resizing. All views are then randomly augmented to prevent the model fromlearning trivial features. We employ data augmentations akin to those used in MoCo-v2. The originalimage (X) is encoded by the encoder fq, while the two views (X1, X2) are encoded by a momentumencoder fk. The parameters of fk are updated using the formula:",
  "k mk + (1 m)q (1)": "where m is a coefficient set to 0.999, q represents the encoder parameters of fq updated throughbackpropagation, and k denotes the momentum encoder parameters of fk updated by q. Ultimately,the objective function compels the model to draw both views (X1, X2) closer to the original image(X) in the embedding space while simultaneously pushing apart all other instances, as depicted in (right) in the original paper.",
  "(u, v+) = logexp(uv+/)P Nn=0 exp(uvn/) (2)": "where similarity is quantified by the dot product. The objective function amplifies the similaritybetween the positive pairs (u . v+) by drawing them closer in the embedding space, while simultane-ously driving apart all the negative samples (vn) in the dictionary to prevent representation collapse. denotes the temperature hyperparameter of the softmax function. In our method, we augment thesimilarity between the original images feature representation, u = fq(x), and the positive pairs featurerepresentation, v+ = fk(xi) (i = 1, 2), while driving apart all the negative examples (vn). Consequently,the total loss for the mini-batch is:",
  "lt = Ni=1 (ui, sg(v1i )) + (ui, sg(v2i )) (3)": "where sg(.) denotes the stop-gradient operation, which is vital for averting representation collapse.As depicted in Equation 3, the total loss lt attracts the two views (v1i and v2i ) to their original instanceui. This enables the model to capture semantic features from the two random views, even if theycontain different semantic information. Our technique captures improved semantic features comparedto prior contrastive methods, as we ensure that the shared region between the attracted views containsaccurate semantic information. Since the original image contains all segments of the object, any partcontained in the random crop is also present in the original image. Thus, when we draw the originalimage and the two random views closer in the embedding space, the model learns representationsof the different parts, creating an occlusion-invariant representation of the object across variousscales and angles. This contrasts with earlier techniques, which draw the two views together in theembedding space regardless of their semantic content, leading to the loss of semantic features.",
  "Equation 3 and Algorithm 1 in the original paper highlight the primary distinctions between ourmethod and prior multi-crop techniques, such as CLSA, SCFC, and DINO. The key differences areas follows:": "Previous methods assume that two global views contain identical semantic information,encouraging the model to concentrate on similarities and generate similar representationsfor both views. In contrast, our method utilizes the original images instead of globalviews, as we contend that global views may contain incorrect semantic information for thesame object. While they may aid in capturing certain global features, this could restrictthe models capacity to learn more universally applicable semantic features, ultimatelyimpacting performance.",
  "Prior methods employ several local random crops, which might be time- and memory-intensive, while our method utilizes only two random crops": "Our objective function employs different strategies to enhance the models visual represen-tation learning. We encourage the model to align the two random crops with the originalimage, which encompasses the semantic information for all random crops while avoidingcompelling the two crops to have similar representations if they do not share similar semanticinformation. This approach differs from prior methods, which encourage all crops (globaland local) to have similar representations, regardless of their semantic content. Conse-quently, although useful for learning certain global features, those methods may discardpertinent semantic information, potentially hindering the transferability of the resultingrepresentations to downstream tasks.",
  "Experiments": "We executed multiple experiments on three datasets: STL-10 \"unlabeled\", comprising 100,000training images, CIFAR-10, containing 50,000 training images, and ImageNet-1K, with 1.28 milliontraining images. **Training Setup:** We employed ResNet50 as the backbone architecture. The model was trainedusing the SGD optimizer, with a weight decay set to 0.0001, momentum at 0.9, and an initial learningrate of 0.03. The mini-batch size was configured to 256, and the model underwent training for up to800 epochs on the ImageNet-1K dataset. **Evaluation:** We employed diverse downstream tasks to assess LeOCLRs representation learningagainst leading SOTA approaches on ImageNet-1K: linear evaluation, semi-supervised learning,transfer learning, and object detection. For linear evaluation, we adhered to the standard evaluationprotocol, where a linear classifier was trained for 100 epochs on top of a frozen backbone pre-trainedwith LeOCLR. The ImageNet-1K training set was used to train the linear classifier from scratch, withrandom cropping and left-to-right flipping augmentations. Results are presented on the ImageNet-1K validation set using a center crop (224 x 224). In the semi-supervised setting, we fine-tunedthe network for 60 epochs using 1% of labeled data and 30 epochs using 10% of labeled data.Additionally, we evaluated the learned features on smaller datasets, such as CIFAR, and fine-graineddatasets, using transfer learning. Lastly, we utilized the PASCAL VOC dataset for object detection. **Comparing with SOTA Approaches:** We employed vanilla MoCo-v2 as a baseline for comparisonwith our method across various benchmark datasets, considering our use of a momentum contrastivelearning framework. Furthermore, we benchmarked our method against other SOTA techniques onthe ImageNet-1K dataset.",
  "ApproachEpochsBatchAccuracy": "MoCo-v280025671.1%BYOL1000409674.4%SWAV800409675.3%SimCLR1000409669.3%HEXA80025671.7%SimSiam80051271.3%VICReg1000204873.2%MixSiam80012872.3%OBoW20025673.8%DINO800102475.3%Barlow Twins1000204873.2%CLSA80025676.2%RegionCL-M80025673.9%UnMix80025671.8%HCSC20025673.3%UniVIP300409674.2%HAIEV20025670.1%SCFS800102475.7%LeOCLR (ours)80025676.2% presents the linear evaluation of our method in comparison to other SOTA techniques. Asshown, our method surpasses all others, outperforming the baseline (i.e., vanilla MoCo-v2) by 5.1%.This lends credence to our hypothesis that while two global views can capture certain global features,they may also encompass distinct semantic information for the same object (e.g., a dogs headversus its leg), which should be taken into account to enhance representation learning. The observedperformance gap (i.e., the difference between vanilla MoCo-v2 and LeOCLR) demonstrates thatmapping pairs with divergent semantic content impedes representation learning and impacts themodels performance in downstream tasks.",
  "**Semi-Supervised Learning on ImageNet-1K:** In this section, we assess the performance ofLeOCLR under a semi-supervised setting. Specifically, we utilize 1% and 10% of the labeled training": "data from ImageNet-1K for fine-tuning, adhering to the semi-supervised protocol introduced inSimCLR. The top-1 accuracy, presented in after fine-tuning with 1% and 10% of the trainingdata, demonstrates LeOCLRs superiority over all compared techniques. This can be attributed toLeOCLRs enhanced representation learning capabilities, particularly in comparison to other SOTAmethods. : Semi-supervised training results on ImageNet-1K: Top-1 performances are reported forfine-tuning a pre-trained ResNet-50 with the ImageNet-1K 1% and 10% datasets. * denotes theresults are reproduced in this study.",
  "MoCo-v2 *47.6%64.8%SimCLR48.3%65.6%BYOL53.2%68.8%SWAV53.9%70.2%DINO50.2%69.3%RegionCL-M46.1%60.4%SCFS54.3%70.5%LeOCLR (ours)62.8%71.5%": "**Transfer Learning on Downstream Tasks:** We evaluate our self-supervised pretrained modelusing transfer learning by fine-tuning it on small datasets such as CIFAR, Stanford Cars, Oxford-IIITPets, and Birdsnap. We adhere to the transfer learning procedures to identify optimal hyperparametersfor each downstream task. As shown in , our method, LeOCLR, surpasses all comparedapproaches on a variety of downstream tasks. This demonstrates that our model acquires valuablesemantic features, enabling it to generalize more effectively to unseen data in different downstreamtasks compared to other techniques. Our method preserves the semantic features of the given objects,thereby enhancing the models representation learning capabilities. Consequently, it is more effectiveat extracting crucial features and predicting correct classes on transferred tasks.",
  "MoCo-v2 *97.2%85.6%91.2%75.6%90.3%SimCLR97.7%85.9%91.3%75.9%89.2%BYOL97.8%86.1%91.6%76.3%91.7%DINO97.7%86.6%91.1%-91.5%SCFS97.8%86.7%91.6%-91.9%LeOCLR (ours)98.1%86.9%91.6%76.8%92.1%": "**Object Detection Task:** To further assess the transferability of the learned representation, wecompare our method with other SOTA techniques using object detection on the PASCAL VOC. Wefollow the same settings as MoCo-v2, fine-tuning on the VOC07+12 trainval dataset using FasterR-CNN with an R50-C4 backbone, and evaluating on the VOC07 test dataset. The model is fine-tuned for 24k iterations (2248 23 epochs). As shown in , our method surpasses all comparedtechniques. This superior performance can be attributed to our models ability to capture richersemantic features compared to the baseline (MoCo-v2) and other techniques, leading to improvedresults in object detection and related tasks.",
  "Ablation Studies": "In the subsequent subsections, we further analyze our approach using a different contrastive instancediscrimination technique (i.e., an end-to-end mechanism) to investigate how our method performswithin this framework. Moreover, we conduct studies on the benchmark datasets STL-10 andCIFAR-10 using a distinct backbone (ResNet-18) to assess the consistency of our approach acrossvarious datasets and backbones. Additionally, we employ a random crop test to simulate natural",
  "MoCo-v282.5%57.4%64%CLSA83.2%--SCFS83%57.4%63.6%LeOCLR (ours)83.2%57.5%64.2%": "transformations, such as variations in scale or occlusion of objects in the image, to analyze therobustness of the features learned by our approach, LeOCLR. We also compare our approach withvanilla MoCo-v2 by manipulating their data augmentation techniques to determine which modelsperformance is more significantly affected by the removal of certain augmentations. In addition,we experiment with different fine-tuning settings to evaluate which model learns better and faster.Furthermore, we adapt the attraction strategy and cropping method of the original image, as well ascompute the running time of our approach. Lastly, we examine our approach on a non-centric objectdataset where the probability of mapping two views containing distinct information is higher.",
  "Different Contrastive Instance Discrimination Framework": "We utilize an end-to-end framework in which the two encoders fq and fk are updated throughbackpropagation to train a model with our approach for 200 epochs with a batch size of 256.Subsequently, we conduct a linear evaluation of our model against SimCLR, which also employsan end-to-end mechanism. As presented in , our approach outperforms vanilla SimCLR bya substantial margin of 3.5%, demonstrating its suitability for integration with various contrastivelearning frameworks.",
  "Scalability": "In , we evaluate our approach on different datasets (STL-10 and CIFAR-10) using a ResNet-18backbone to ensure its consistency across various backbones and datasets (i.e., scalability). Wepre-trained all the approaches for 800 epochs with a batch size of 256 on both datasets and thenconducted a linear evaluation. Our approach demonstrates superior performance on both datasetscompared to all approaches. For instance, our approach outperforms vanilla MoCo-v2, achievingaccuracies of 5.12% and 5.71% on STL-10 and CIFAR-10, respectively.",
  "In , we report the top-1 accuracy for vanilla MoCo-v2 and our approach after 200 epochson ImageNet-1K, concentrating on two tasks: a) center crop test, where images are resized to 256": "pixels along the shorter side using bicubic resampling, followed by a 224 x 224 center crop; andb) random crop, where images are resized to 256 x 256 and then randomly cropped and resized to224 x 224. According to the results, the performance of MoCo-v2 dropped by 4.3% with randomcropping, whereas our approach experienced a smaller drop of 2.8%. This suggests that our approachlearns improved semantic features, demonstrating greater invariance to natural transformations likeocclusion and variations in object scales. Additionally, we compare the performance of CLSA withour approach, given that both perform similarly after 800 epochs (see ). Note that the CLSAapproach uses multi-crop (i.e., five strong and two weak augmentations), while our approach employsonly two random crops and the original image. As shown in , LeOCLR outperforms theCLSA approach by 2.3% after 200 epochs on ImageNet-1K. To address concerns about the increasedcomputational cost associated with training LeOCLR compared to MoCo V2, we include the trainingtime for both approaches in . We trained both models on three A100 GPUs with 80GB for200 epochs. Our approach took an additional 13 hours to train over the same number of epochs, but itdelivers significantly better performance than the baseline.",
  "Augmentation and Fine-tuning": "Contrastive instance discrimination techniques are sensitive to the choice of image augmentations.This sensitivity necessitates further analysis comparing our approach to Moco-v2. These experimentsaim to explore which model learns better semantic features and produces more robust representationsunder different data augmentations. As shown in , both models are affected by the removalof certain data augmentations. However, our approach shows a more invariant representation andexhibits less performance degradation due to transformation manipulation compared to vanilla MoCo-v2. For instance, when we apply only random cropping augmentation, the performance of vanillaMoCo-v2 drops by 28 percentage points (from a baseline of 67.5% to 39.5% with only randomcropping). In contrast, our approach experiences a decrease of only 25 percentage points (from abaseline of 71.7% to 46.6% with only random cropping). This indicates that our approach learns",
  "graph3.pdf": ":Decrease in top-1 accuracy (in % points) of LeOCLR and our reproduc-tion of vanilla MoCo-v2 after 200 epochs,under linear evaluation on ImageNet-1K.RGrayscalereferstoresultswithoutgrayscaleaugmentations, whileRcolorreferstoresultswithoutcolorjitterbutwith In , presented in , we fine-tune the representations over the 1% and 10% ImageNet-1Ksplits using the ResNet-50 architecture. In the ablation study, we compare the fine-tuned representa-tions of our approach with the reproduced vanilla MoCo-v2 across 1%, 2%, 5%, 10%, 20%, 50%, and100% of the ImageNet-1K dataset. In this setting, we observe that tuning a LeOCLR representationconsistently outperforms vanilla MoCo-v2. For instance, (a) demonstrates that LeOCLRfine-tuned with 10% of ImageNet-1K labeled data outperforms vanilla Moco-v2 fine-tuned with20% of labeled data. This indicates that our approach is advantageous when the labeled data fordownstream tasks is limited.",
  "Attraction Strategy": "In this subsection, we apply a random crop to the original image (x) and attract the two views (x1,x2) toward it to evaluate its impact on our approachs performance. We also conducted an experimentwhere all views were attracted to each other. However, in our method, we avoid attracting the twoviews to each other, enforcing the model to draw the two views toward the original image only(i.e., the uncropped image containing semantic features for all crops). For these experiments, wepre-trained the model on ImageNet-1K for 200 epochs using the same hyperparameters employedin the main experiment. The experiments in underscore the significance of the informationshared between the two views. They also highlight the importance of leveraging the original imageand avoiding the attraction of views containing varied semantic information to preserve the semanticfeatures of the objects. When we create a random crop of the original image (x) and force the modelto make the two views similar to the original image (i.e., LeOCLR(Random original image)), themodel performance decreases by 2.4%. This performance reduction occurs because cropping the original image and compelling the model toattract the two views towards it increases the probability of having two views with differing semanticinformation, resulting in a loss of semantic features of the objects. The situation deteriorates whenwe attract all views (x, x1, x2) to each other in LeOCLR (attract all crops), causing performance todrop closer to that of vanilla MoCo-v2 (67.5%). This decline is attributed to the high likelihood ofattracting two views containing distinct semantic information.",
  "Non-Object-Centric Tasks": "Non-object-centric datasets, like COCO, depict real-world scenes where the objects of interest arenot centered or prominently positioned, unlike object-centric datasets such as ImageNet-1K. In thisscenario, the chance of generating two views containing distinct semantic information for the objectis elevated, thus exacerbating the issue of losing semantic features. Therefore, we train both ourapproach and the MoCo-v2 baseline from scratch on the COCO dataset to evaluate how our methodmanages the discarding of semantic features in such datasets. We utilized identical hyperparametersas for ImageNet-1K, training the models with a batch size of 256 over 500 epochs. Subsequently, wefine-tuned these pre-trained models on the COCO dataset for object detection.",
  "Conclusion": "This paper presents a new contrastive instance discrimination approach for SSL to improve represen-tation learning. Our method reduces the loss of semantic features by including the original imageduring training, even when the two views contain different semantic content. We show that ourapproach consistently enhances the representation learning of contrastive instance discriminationacross various benchmark datasets, backbones, and mechanisms, including momentum contrastand end-to-end methods. In linear evaluation, we achieved an accuracy of 76.2% on ImageNet-1Kafter 800 epochs, surpassing several SOTA instance discrimination SSL methods. Furthermore, wedemonstrated the invariance and robustness of our approach across different downstream tasks, suchas transfer learning and semi-supervised fine-tuning."
}