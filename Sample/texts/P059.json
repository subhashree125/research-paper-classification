{
  "Abstract": "This research presents a counterintuitive discovery: positional encoding, a high-dimensional representation of time indices on input data, improves the learningcapabilities of recurrent neural networks (RNNs). Although positional encoding iswidely recognized for complementing Transformer neural networks by enablingthem to process data order, its application to RNNs seems unnecessary becauseRNNs inherently encode temporal information. However, our analysis using syn-thetic benchmarks shows that combining positional encoding with RNNs offersadvantages, especially when dealing with extensive vocabularies that include low-frequency tokens. Further investigation reveals that these infrequent tokens causeinstability in the gradients of standard RNNs, and positional encoding helps to miti-gate this instability. These findings highlight a new function of positional encodingbeyond its well-known role as a timekeeping mechanism for Transformers.",
  "Introduction": "Since their introduction, Transformer neural networks have become the preferred method for pro-cessing and generating time series data, surpassing traditional recurrent neural networks (RNNs). Asignificant difference between these models is their handling of temporal information, that is, thesequence of data points or tokens. RNNs process temporal information by adjusting their internalstate based on new inputs and their existing state. Conversely, Transformers lack an intrinsic mecha-nism for understanding data sequence order and, therefore, depend on an external system known aspositional encoding to keep track of time. Positional encoding represents time indices in a high-dimensional format. A common methodinvolves using sinusoidal waves of predetermined frequencies. This method marks input tokens byadding or appending these vectors to the input embeddings. Unlike RNNs, positional encodings timerepresentation remains constant regardless of input values until processed by a network. Although positional encoding is often viewed as a way to represent time that can replace RNNs whenused with Transformers, it is not incompatible with RNNs. Inputs to RNNs can be augmented withposition-encoding vectors. Autonomous activities in biological neurons, such as oscillations, arebelieved to be important for time perception and other perceptual processes, as well as motor control. This study, therefore, investigates the effects of adding positional encoding to the inputs of RNNs,using synthetic benchmarks. The results demonstrate that positional encoding helps RNNs manage amore extensive range of discrete inputs, or a larger vocabulary, compared to those without positionalencoding.",
  "Theoretical and Empirical Computational Power of (Vanilla) RNNs": "Mathematically, RNNs are recognized as being Turing-complete, capable of simulating Turingmachines if their weights are infinitely precise and perfectly tuned. In practice, however, RNNweights are limited by finite precision and the need to optimize based on a finite set of observations.These constraints impose practical limitations on the capabilities of RNNs. For instance, empiricalRNNs cannot store an infinite number of observations in their memory, and the memorized informationtends to degrade over time. More recently, research into extending memory retention has explored continuous-time models.Instead of modifying a latent state in discrete-time steps, these models use a linear combinationof orthogonal polynomials in a continuous-time domain to approximate the input history. Thecoefficients of these polynomials provide a finite-dimensional representation of the input sequence,known as the High-Order Polynomial Projection Operator (HiPPO), and the dynamics of thesecoefficients can be described by an ordinary differential equation (ODE). This concept has beenfurther developed into neural state-space models by replacing the fixed state matrix in the ODEwith a learnable one, constrained to a diagonal structure plus a row-rank matrix. With additionalenhancements, the latest state-space models have shown language modeling performance that rivalsTransformer-based models.",
  "Positional Encoding": "Positional encoding serves as a high-dimensional representation of the temporal structures presentin input data. This method is particularly crucial for Transformers, which, unlike RNNs, do notinherently capture the order of inputs. Therefore, input tokens to a Transformer are \"time-stamped\"by adding or concatenating a position-encoding vector. In the initial implementation of the Transformer, token positions were represented using sinusoidalwaves of various predefined frequencies. Although this method is effective for a wide range of tasks,researchers have explored other encoding schemes as well. For instance, the well-known BERTpretraining for natural language processing used learnable embeddings to indicate token positions.Some studies have suggested that combining sinusoidal and learnable encodings can enhance modelperformance. Another approach is to encode the distance between tokens instead of the time elapsedfrom the sequences beginning. Beyond Transformers, positional encoding is used to indicate elapsed time in diffusion processes.Its effectiveness is not limited to temporal information; studies on three-dimensional mesh andpoint-cloud modeling have shown that sinusoidal transformation of spatial data outperforms rawcoordinate representation. Despite its widespread use across various areas of machine learning, the application of positionalencoding to pure RNNs has been largely unexplored. To the authors knowledge, only a few studieshave investigated position-encoded RNNs. The time index in time series data has rarely been directlyused by RNNs, likely due to perceived redundancy alongside RNN functionalities.",
  "Model Architecture": "This studys investigations were based on single-layer gated recurrent units (GRUs), long short-termmemory (LSTM) networks, and a neural state-space model, S4D. Each integer in the input sequenceswas first embedded, concatenated with its positional encoding, and then fed into the RNN or S4D.After processing the entire input sequence, the network received a command to produce the output,represented by a time-invariant learnable vector. The outputs from the RNN or S4D module werelinearly projected into classification logits, and the cross-entropy loss against the target sequence wasused to optimize the entire network. Model predictions during testing were determined by the argmaxof these logits for each time step. The canonical sinusoidal positional encoding used for Transformers was adopted in this study.Specifically, each time step t was encoded by a Dpos-dimensional vector, (PEt,1, ..., PEt,Dpos)T ,defined as follows:",
  "Implementation Details": "Across the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. Theembedding of the input integers and the memory cell of the LSTM also had the same dimensionalityof 512. Similarly, the hidden dimensionality of S4D was set to 512, while its state size (or the orderof the Legendre polynomials) was maintained at the default value of 64. The models were trained for 300,000 iterations using the Adam optimizer with parameters (1, 2) :=(0.9, 0.999) and no weight decay. The learning rate was linearly warmed up from 0.0 to 0.001 for thefirst 1,000 iterations, and then annealed according to the cosine schedule. The batch size was 512.",
  "Key Findings": "Positional encoding improved the ability of RNNs to handle a larger vocabulary in the reverse-orderingtask. The position-encoded GRU and LSTM successfully reversed input sequences of 64 integersdrawn uniformly at random from vocabularies of size 32-256 and 256-16,384, respectively, achievingtoken-wise accuracy above 95%. In contrast, the performance of the vanilla models without positionalencoding degraded as the vocabulary size increased. Similarly, positional encoding enhanced thecapacity of S4D to handle large vocabularies. These improvements are also evident in the reducedsequence-wise reconstruction errors, measured by the Damerau-Levenshtein distance. Neither extratraining iterations nor greater batch sizes improved the performance of the vanilla models.",
  "Frequency Matters": "The most apparent consequence of the increased vocabulary size was the reduced chance of observingindividual vocabulary items. Accordingly, additional experiments were conducted with non-uniformlydistributed tokens to investigate the relation between their frequency and RNN performance. Specif-ically, the input vocabulary was evenly divided into Frequent and Rare groups, and the Frequenttokens had three times the probability of the Rare tokens. The training data consisted of 64 independent samples from this dual-frequency vocabulary. Bycontrast, the test data were systematically constructed so that each sequence included a single\"target\" token (Frequent/Rare) whose retrieval was evaluated for accuracy assessment, along with63 \"disturbants\" that were either all Frequent or all Rare. The experiment revealed that it was thedisturbant tokens whose frequency significantly impacted the performance of the vanilla RNNs andS4D. On the one hand, the Rare targets were successfully retrieved as long as they were surroundedby the Frequent disturbants. On the other hand, the vanilla GRU struggled to recover the Frequenttargets when the other input tokens were filled with the Rare disturbants. The LSTM performance wasalso degraded, especially when the targets were positioned in the first quarter of the input sequence (1 t 16). Similarly, the Rare disturbants were detrimental to the S4D; unlike the RNNs, however,the accuracy was worst when the targets were located in the middle of the input sequences (17 t 32). In contrast, the position-encoded RNNs exhibited robustness to the frequency of the target anddisturbant tokens. They achieved nearly perfect accuracies in most cases, except when the GRUprocessed the fully Rare data whose target was located in the first half of the sequence (1 t 32). Likewise, positional encoding enhanced the resilience of the S4D against the influence of Raredisturbants.",
  "Analysis of Gradient Stability": "To delve deeper into the influence of token frequency on RNN performance, the gradients of theRNN latent states were scrutinized. In the analysis, pairs of input sequences were processed by theRNNs trained on the dual-frequency vocabulary (comprising Frequent and Rare items). Each pairof sequences shared the same initial token (t = 1; \"target\") but varied in the subsequent tokens (2 t L; \"disturbants\"). Then, gradients were computed for the distant mapping between the firstand last updated states (i.e., at time t = 1 and 2L) of the RNNs using backpropagation through time.The stability of RNN learning was assessed by measuring the dot-product similarity of the gradientsbetween the paired input sequences (after normalization over output dimensions). Formally, the paired input sequences, denoted as A and B, established two distinct, but ideally similarmappings, f (A) and f (B), from the first to the last latent state of the RNNs (h(s)2L = f (s)(z1), wheres {A, B}). The gradient stability of the RNNs was defined by the dot-product similarities betweenthe normalized gradients of these paired mappings:",
  "(2)": "Monitoring the gradients at training checkpoints revealed that Rare disturbants destabilize the learningof vanilla RNNs. The similarity of the paired gradients decreased gradually (GRU) or rapidly (LSTM)when the networks were exposed to the Rare disturbants. Positional encoding endowed the RNNswith robustness to these RARE disturbants. Both the GRU and LSTM maintained the high similarityof the paired gradients across the different target/disturbant conditions. By contrast, the impact ofpositional encoding on the gradient stability of the S4D was marginal; unlike the RNNs, the vanillaS4D was highly stable by itself against Rare disturbants throughout the training, even though there",
  "Difficulties in Handling a Large Vocabulary": "This study introduces a novel challenge in training (vanilla) RNNs: managing large vocabularies.While the manageable vocabulary size of RNNs is a pertinent research area, crucial for empiricalapplications like natural language processing, previous studies have primarily focused on evaluatingand improving the memory duration of RNNs, typically with small vocabulary sizes. This research examined RNN gradients and identified their destabilization when processing low-frequency tokens, which are necessarily included in a large vocabulary. Specifically, inputs that donot contribute to gradient-based optimization at a target time step were found to be detrimental. In general time series processing, data points carrying crucial information for specific time stepsbecome irrelevant otherwise. Consequently, each token exhibits a dual natureboth crucial andnoisythroughout the task. Processing rare tokens is particularly challenging, presumably becausethey are irrelevant most of the time while making a large impact on learning due to their greater loss,compensating for fewer learning opportunities. Dealing with such \"unignorable noise\" presents apervasive challenge for RNNs.",
  "Functionality of Positional Encoding beyond the Timekeeper for Transformers": "Although low-frequency tokens destabilize the gradient-based learning of RNNs, this study alsodiscovered that positional encoding can alleviate this issue. This enhancement of RNNs via positionalencoding is noteworthy because RNNs were specifically designed to process time series data ontheir own. Unlike Transformers, they are presumed to function without relying on an \"externalclock\". Consequently, position-encoded RNNs have remained largely unexplored. The findings ofthe present studynamely, the improvement in the manageable vocabulary size due to enhancedgradient stabilitybroaden the currently limited understanding of the impact of positional encodingon RNNs. Additionally, the results of this study shed new light on the utility of positional encoding. Whilepositional encoding has been viewed as nothing more than input timestamps for Transformers, thepresent study demonstrated its efficacy in stabilizing the gradients of RNNs against disruption bylow-frequency tokens. This novel functionality of positional encoding would not have been visible inTransformer studies, as the model can dynamically adjust the relevance of input tokens through theirattention mechanism, thus inherently mitigating the impact of disturbant tokens.",
  "Limitations and Future Directions": "A primary unresolved question in this study pertains to the mechanism behind the gradient stabilizationby positional encoding. All the findings here are based on experimental investigations, lackingrigorous mathematical explanations for how and why the gradients of RNNs are destabilized byinfrequent tokens and stabilized by positional encoding. Moreover, the present study primarily focusedon the canonical implementation of sinusoidal positional encoding designed for Transformers, leavingopen which parameters of the sinusoidal waves (i.e., frequencies and phases) are critical for gradientstabilization. Future research may broaden its scope to encompass more general forms of positionalencoding, such as wavelets and non-periodic signals. Moreover, the analysis of gradient stability did not fully address the enhanced performance ofthe position-encoded state-space model (S4D). In terms of accuracy, the positioned-encoded S4Dexhibited greater robustness to infrequent tokens compared to the vanilla model, resembling thebehavior observed in RNNs. However, the gradients of the vanilla S4D were too stable to account forthis decline in performance. This leaves open the question of how positional encoding influencesgradient-based learning of state-space models. Additionally, future studies may investigate a broaderrange of state-space models to achieve a comprehensive understanding of the interplay betweenpositional encoding and these models. In addition to these scientifically oriented questions, future studies could also address practicalapplications of position-encoded RNNs and neural state-space models. Although positional encodingenhanced model performance across different synthetic tasks, the extent of this enhancement is task-dependent. Thus, positional encoding is not a panacea for arbitrary tasks, and further investigationsare necessary to determine when it is effective.",
  "Reverse-Ordering + Delayed-Addition": "This section reports the performance of position-encoded RNNs on a more complicated, combinatorialtask than the reverse ordering of input sequences. Extending the reverse-ordering task, the modelsreceived additional random input integers during the output phase, and added each of them to thecorresponding token in the reverse-ordered input sequence (modulo the vocabulary size, so that theoutput range was bounded). This task was too challenging to GRUseven after reducing the inputlength to L = 16so only the results from LSTMs are reported below. Also, the network was trainedfor 600,000 iterations (i.e., twice longer than the other tasks) for ensuring the convergence. The otherconditions/hyperparameters were the same as reported in the main text. Consequently, positionalencoding improved the model performance as the vocabulary size grew from 896 to 1088.",
  "Sorting": "In the reverse ordering task, the order of input integers was important information for accomplishingthe task. Thus, positional encoding may play its originally intended role in encoding the temporalinformation. This section reports the effectiveness of positional encoding for a task in which the order of inputobservations was completely irrelevant; the learning objective was to simply sort the input integers intheir inherent ascending order (e.g. 8, 29, 2, 11 -> 2, 8, 11, 29). The input integers were uniformlyrandomly sampled with replacement, allowing for ties in the sorting process.",
  "Predecessor Query": "Finally, this section presents benchmark results for the predecessor-query task. The network firstreceived a sequence of non-repeating random integers, x1, ..., xL. Subsequently, one of the non-initialinput integers, xtquery (2 tquery L), was randomly selected and reintroduced to the networkat time t = L + 1. The learning objective is to return the predecessor of the reviewed integer (=xtquery1). The predecessor-query task evaluates the capacity of RNNs to integrate informationregarding both the order and content of input sequences. As in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 dueto the complexity of the task, and the experiment focused on the LSTM. The number of trainingiterations was maintained at 300,000. Similar to the other benchmarks, positional encoding improvedthe LSTMs capacity to manage the larger vocabularies.",
  "Robustness to Variations in Input Length": "So far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder ifpositional encoding is exceptionally effective under this setting, informing RNNs with the exacttiming when each input token should be returned as the output. Thus, it remains unclear whetheror not position-encoded RNNs can also handle a larger vocabulary even when the input length isvariable and, thus, the exact timing of the output emission is not identifiable from the positionalencoding attached to the inputs. To assess the robustness to variations in the input length, an additional experiment was conducted onthe LSTM, with the input length varied between 32 and 64. In this setup, the maximum input length(= 64) covers the entirety of the shortest input sequence plus its reversed reconstruction (= 32 + 32).Consequently, the positional encoding per se cannot even distinguish the input vs. output phases at t= 33, ..., 64. The vocabulary size was set to 16,384. As a result, the positional encoding still improved the LSTMs performance on the reverse-orderingtask against the perturbations in the input length. This result suggests that the effectiveness of thepositional encoding for RNNs is not limited to strictly scheduled tasks.",
  "Effects of Additional Parameters in Position-Encoded RNNs": "The concatenation of positional encoding with input embeddings inflates the number of learnableparameters in the input-to-hidden projection weights. This additional parameterization per se doesnot influence the learning of the input embeddings, and therefore does not elucidate the enhancedperformance of position-encoded RNNs. This section substantiates this argument by equalizing thenumber of learnable parameters between the vanilla and position-encoded models. Specifically, the equalization was achieved by concatenating two identical copies of the inputembeddings and feeding them to the LSTM. This configurationhenceforth termed \"doublevanilla\"effectively doubled the size of the input- to-hidden weight for each gate in the LSTM,aligning it with that of the position-encoded LSTM, while maintaining all other parameters, includingthe dimensionality of the (non-repeated) input embeddings. As illustrated, the double vanilla LSTM did not yield any improvements in the reverse-ordering orsort- ing tasks. These results affirm that the reported enhancement of RNNs is not merely attributableto the additional parameterization associated with the positional encoding.",
  "Alternative Implementations of Positional Encoding": "While this study implemented positional encoding by sinusoidal waves, there are alternative imple-mentations proposed in the previous studies. For instance, the BERT-based models typically encodeeach token position by a learnable embedding. Moreover, the original study of Transformer pointedout that even random vectors can function as positional encoding. Accordingly, these two alternative forms of positional encoding were tested on the LSTM performingthe reverse- ordering task. The random position-encoding vectors were uniformly and independentlysampled from the (512 1)- dimensional hypersphere. The learnable embeddings were implementedusing the canonical embedding module of PyTorch (torch.nn.Embedding). The input length andvocabulary size were set to 64 and 16,384 respectively. Both the random vectors and learnableembeddings improved the performance of LSTM. Among the different implementations of positional encoding, the sinusoidal encoding outperformedthe two alterna- tives. The advantage of the sinusoidal encoding became more apparent when the inputlength was variable between 32 and 64; the sinusoidal encoding was more robust to the variations inthe input length than the others.",
  "Language Modeling": "This section reports benchmark results for the language modeling task. Single-layer LSTMs withand without sinusoidal positional encoding were trained and tested on the WikiText-103 dataset.Due to constraints in computational resources, the vocabulary was reduced from the original size of267,735 to 32,768 by retokenizing the raw data using SentencePiece. The headings were removed,and the main text was segmented by paragraphs (separated by the line break). Additionally, only thefirst 1024 tokens of each paragraph were utilized for training and testing, ensuring that the absolutepositional encoding always aligned with the beginning of each paragraph. The hyperparameters wereconfigured as specified in 3.3. As illustrated, positional encoding proved effective only for marginally faster learning during theinitial phase of training. The difference diminished around 10,000/30,000 iterations, and the testperplexities of the position-encoded model were inferior to those of the vanilla model."
}