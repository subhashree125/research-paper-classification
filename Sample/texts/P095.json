{
  "Abstract": "Learning rational behaviors in open-world games such as Minecraft continues topose a challenge to Reinforcement Learning (RL) research, due to the combineddifficulties of partial observability, high-dimensional visual perception, and delayedrewards. To overcome these challenges, we propose JueWu-MC, a sample-efficienthierarchical RL method that incorporates representation learning and imitationlearning to handle perception and exploration. Our approach has two levels ofhierarchy: the high-level controller learns a policy to manage options, while thelow-level workers learn to solve each sub-task. To boost learning of sub-tasks,we propose a combination of techniques including: 1) action-aware represen-tation learning, which captures relations between action and representation; 2)discriminator-based self-imitation learning for efficient exploration; and 3) ensem-ble behavior cloning with consistency filtering for policy robustness. Extensiveexperiments demonstrate that JueWu-MC significantly enhances sample efficiencyand outperforms several baselines. We won the championship of the MineRL 2021research competition and achieved the highest performance score.",
  "Introduction": "Deep reinforcement learning (DRL) has achieved great success in numerous game genres includingboard games, Atari games, simple first-person-shooter (FPS) games, real-time strategy (RTS) games,and multiplayer online battle arena (MOBA) games. Recently, open-world games have garneredattention due to their playing mechanisms and their resemblance to real-world control tasks. Minecraft,as a typical open-world game, has been increasingly explored in recent years. Compared to other games, the properties of Minecraft make it an ideal testbed for RL research,because it emphasizes exploration, perception, and construction in a 3D open world. Agents are givenpartial observability and face occlusions. Tasks in the game are chained and long-term. Humans cantypically make rational decisions to explore basic items and construct more complex items with areasonable amount of practice, while it can be challenging for AI agents to do so autonomously. Tofacilitate the effective decision-making of agents in playing Minecraft, MineRL has been developedas a research competition platform, which provides human demonstrations and encourages thedevelopment of sample-efficient RL agents for playing Minecraft. Since its release, many effortshave been made to develop Minecraft AI agents. However, it remains difficult for current RL algorithms to acquire items in Minecraft due to severalfactors, which include the following. First, in order to reach goals, the agent is required to completemany sub-tasks that highly depend on each other. Due to the sparse reward, it is difficult for agentsto learn long-horizon decisions efficiently. Hierarchical RL from demonstrations has been exploredto take advantage of the task structure to accelerate learning. However, learning from unstructureddemonstrations without any domain knowledge remains difficult. Second, Minecraft is a flexible3D first-person game which revolves around gathering resources and creating structures and items.In this environment, agents are required to handle high-dimensional visual input to enable efficient",
  "Game AI": "Games have long been a testing ground for artificial intelligence research. AlphaGo mastered thegame of Go with DRL and tree search. Since then, DRL has been used in other sophisticated games,including StarCraft, Google Football, VizDoom, and Dota. Recently, the 3D open-world gameMinecraft has been attracting attention. Previous research has shown that existing RL algorithmscan struggle to generalize in Minecraft and a new memory-based DRL architecture was proposed toaddress this. Another approach combines a deep skill array and a skill distillation system to promotelifelong learning and transfer knowledge among different tasks. Since the MineRL competition beganin 2019, many solutions have been proposed to learn to play in Minecraft. These works can begrouped into two categories: 1) end-to-end learning; 2) hierarchical RL with human demonstrations.Our approach belongs to the second category, which leverages the structure of the tasks and learnsa hierarchical agent to play in Minecraft. ForgER proposed a hierarchical method with forgetfulexperience replay, and SEIHAI fully takes advantage of human demonstrations and task structure.",
  "Our work aims to create a sample-efficient RL agent for playing Minecraft, and we thereby develop acombination of efficient learning techniques. We discuss the most relevant works below": "Our work is related to HRL research that builds upon human priors. One approach proposes towarm-up the hierarchical agent from demonstrations and fine-tune it with RL algorithms. Anotherapproach proposes to learn a skill prior from demonstrations to accelerate HRL algorithms. Comparedto existing works, we are faced with highly unstructured demos in 3D first-person video games played",
  "by the crowds. We address this challenge by structuring the demonstrations and defining sub-tasksand sub-goals automatically": "Representation learning in RL has two broad directions: self-supervised learning and contrastivelearning. Self-supervised learning aims to learn rich representations for high-dimensional unlabeleddata to be useful across tasks. Contrastive learning learns representations that obey similarityconstraints. Our work proposes a self-supervised representation learning method that measures actioneffects in 3D video games. Existing methods use curiosity or uncertainty as a signal for exploration so that the learned agentis able to cover a large state space. The exploration-exploitation dilemma drives us to develop self-imitation learning (SIL) methods that focus on exploiting past good experiences for better exploration.We propose discriminator-based self-imitation learning (DSIL).",
  "Overview": "Our overall framework is shown in . We define human demonstrations as D = {0, 1, 2, ...}where i is a long-horizon trajectory containing states, actions, and rewards. The provided demon-strations are unstructured, without explicit signals that specify sub-tasks and sub-goals. We define an atomic skill as a skill that gets a non-zero reward. We define sub-tasks and sub-goalsbased on the atomic skills. To define sub-tasks, we examine the reward delay for each atomic skill,keeping those with long reward delays as individual sub-tasks and merging those with short rewarddelays into one sub-task. Through this process, we have n sub-tasks in total. To define sub-goals foreach sub-task, we extract the most common human behavior pattern and use the last state in eachsub-task as its sub-goal. Through this, we have structured demonstrations (D {D0, D1, ..., Dn1}) with sub-tasks and sub-goals used to train the hierarchical agent. With the structured demonstrations,we train the meta-policy using imitation learning and train sub-policies to solve sub-tasks usingdemonstrations and interactions with the environment.",
  "Meta- and Sub-policies": "Meta-policy. We train a meta-policy that maps continuous states to discrete indices (0, 1, ..., n - 1)that specify which option to use. Given state space S and discrete option o O, the meta-policy isdefined as m()(o|s), where s S, o O, and represents the parameters. m()(o|s) specifiesthe conditional distribution over the discrete options. To train the meta-policy, we generate trainingdata (s, i) where i represents the i-th stage and s Di is sampled from the demonstrations of the i-thstage. The meta-policy is trained using negative log-likelihood (NLL) loss:",
  "= argmaxom(o|s)": "Sub-policy. In Minecraft, sub-tasks can be grouped into two main types: gathering resources, andcrafting items. In the first type (gathering resources), agents need to navigate and gather sparserewards by observing high-dimensional visual inputs. In the second type (crafting items), agents needto execute a sequence of actions robustly. In typical HRL, the action space of the sub-policies is predefined. However, in the competition, ahandcrafted action space is prohibited. Additionally, the action space is obfuscated in both humandemonstrations and the environment. Learning directly in this continuous action space is challengingas exploration in a large continuous space can be inefficient. We use KMeans to cluster actions foreach sub-task using demonstration Di, and perform reinforcement learning and imitation learningbased on the clustered action space.",
  "Learning Sub-policies to Gather Resources": "To efficiently solve these sub-tasks, we propose action-aware representation learning anddiscriminator-based self-imitation learning to facilitate the learning of sub-policies. The modelarchitecture is shown in . Action-aware Representation Learning. To learn compact representations, we observe that in 3Denvironments, different actions have different effects on high-dimensional observations. We proposeaction-aware representation learning (A2RL) to capture the relation with actions. We learn a mask net on a feature map for each action to capture dynamic information between thecurrent and next states. Let the feature map be f(s) RCHW and the mask net be m(s, a) HW , where and represent parameters of the policy and mask net. Given a transition tuple(s, a, s), the loss function for training the mask is:",
  "Lg(a) = Es,a,sD[||f(s) ga(f(s))||2]": "This objective learns to recover information of s from s in latent space, which is equal to learning adynamic model to predict the next state given the current state and action. Note that the parameter is dependent on the action a. In the second stage, we fix the learned linear function ga and optimizethe mask net. By minimizing the loss function, the mask net will learn to focus on local parts of the current imagethat are uncertain to the dynamic model. This is similar to human curiosity, which focuses on thatwhich is uncertain. For policy-based methods, we integrate our learned representations into policy networks. For value-based methods, we combine our learned representations directly with Q-value functions. The learningof the Q-value function can be done using any Q-learning based algorithms. Discriminator-based Self-imitation Learning. We propose discriminator-based self-imitationlearning (DSIL). Unlike ASIL, DSIL does not use advantage clipping. Our intuition is that the agentshould be encouraged to visit the state distribution that is more likely to lead to goals. To do so, DSIL learns a discriminator to distinguish between states from successful and failedtrajectories, and then uses the learned discriminator to guide exploration. We maintain two replaybuffers B+i and Bi to store successful and failed trajectories. During learning, we treat data fromB+i as positive samples and data from Bi as negative samples to train the discriminator. Let thediscriminator be D : S which is parameterized by parameters . We train the discriminatorwith the objective:",
  "In this type of sub-task, agents must learn a sequence of actions to craft items. To finish such tasks,agents need to learn a robust policy to execute a sequence of actions": "We explore pure imitation learning (IL) to reduce the need for interactions with the environment, dueto the limited sample and interaction usage. We propose ensemble behavior cloning with consistencyfiltering (EBC). Consistency Filtering. Human demonstrations can be diverse and noisy. Directly imitating such noisydata can cause confusion for the policy. Therefore, we perform consistency filtering by extractingthe most common pattern of human behaviors. We extract the most common action sequence fromdemonstrations Di. For each trajectory, we keep those actions that lead to a state change whileappearing for the first time to form an action sequence, and count the occurrences of each pattern.We then get the most common action pattern. Afterward, we conduct consistency filtering using theextracted action pattern. Ensemble Behavior Cloning. Learning policy from offline datasets can lead to generalizationissues. Policies learned through behavior cloning may become uncertain when encountering unseenout-of-distribution states. To mitigate this, EBC learns a population of policies on different subsets ofdemonstrations to reduce the uncertainty of the agents decision. Specifically, we train K policies ondifferent demonstrations with NLL loss:",
  "Main Results": "shows all the MineRL competition results since 2019. The competition settings in 2020 and2021 were more difficult than in 2019. In these years, participants had to focus on the algorithmdesign itself. The scores in 2020 and 2021 are lower than in 2019. Our approach outperforms allprevious solutions. End-to-end baselines cannot achieve a decent result, showing it is difficult tosolve long-horizon tasks with end-to-end learning. Compared to the results of the 2020 competition,our method outperforms other solutions with a score (76.97) that is 3.4x higher than the second placescore (22.97). shows the conditional success rate of each stage between our approach andSEIHAI. Our approach outperforms SEIHAI in every stage. (a) shows the training curves. Due to a version update of MineRL 2021, our online scoredropped compared with the performance in our training curve. Our approach is sample-efficient andoutperforms prior best results with 0.5 million training samples. Our score reaches 100 with 2.5million training samples, which is less than the 8 million samples of the MineRL competition.",
  "Ablation Study": "To examine the effectiveness of our proposed techniques, we consider three variants of our approach:1) without A2RL, 2) without DSIL, and 3) without EBC. (b) shows the training curves. Eachtechnique contributes to the overall performance. EBC and A2RL contribute more than DSIL. DSILmainly boosts the performance for later sub-tasks, while A2RL and EBC have earlier effects onthe overall pipeline. EBC contributes significantly, demonstrating that learning a robust policy isimportant for solving long-horizon tasks.",
  "Visualization": "To understand why our techniques work, we conduct an in-depth analysis. To understand the learnedmask in A2RL, we compute saliency maps. For each action, we show the current state, the next state,and the saliency map of the learned mask on the current state. We find that the learned mask capturesthe dynamic information between two adjacent states, revealing curiosity on the effect of actions. Themask net learns to focus on uncertain parts of the current state. For the attack action, the learnedmask focuses on the objects in front of the agent. For the turn left and turn down actions, the masknet focuses on the parts that have major changes due to the rotation and translation of the agentsperspective. Our learned mask assists the agent in better understanding the 3D environment. To understand how DSIL works, we visualize the state distribution that the agent visits. We comparePPO, PPO+SIL, and PPO+DSIL. At the early training stage, both methods explore randomly andsometimes reach the goal state successfully. After getting samples and training, PPO+DSIL starts toexplore in a compact region, while PPO and PPO+SIL still explore in a wider region. DSIL pushesthe agent to stay close to a good state distribution, reproducing its past behaviors and exploring in abetter way, which incentivizes deep exploration for successful trajectories.",
  "Conclusion": "In this paper, we present JueWu-MC, a sample-efficient hierarchical reinforcement learning frame-work designed to play Minecraft. With a high-level controller and several auto-extracted low-levelworkers, our framework can adapt to different environments and solve sophisticated tasks. Ournovel techniques in representation learning and imitation learning improve both the performance andlearning efficiency of the sub-policies. Experiments show that our pipeline outperforms all baselinealgorithms and previous solutions from MineRL competitions. In future work, we would like to applyJueWu-MC to other Minecraft tasks, as well as other open-world games."
}