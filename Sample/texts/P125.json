{
  "Abstract": "We present DISCOSENSE, a benchmark for commonsense reasoning via un-derstanding a wide variety of discourse connectives. We generate compellingdistractors in DISCOSENSE using Conditional Adversarial Filtering, an extensionof Adversarial Filtering that employs conditional generation. We show that stateof-the-art pre-trained language models struggle to perform well on DISCOSENSE,which makes this dataset ideal for evaluating next generation commonsense rea-soning systems.",
  "Introduction": "This paper addresses the critical need for challenging benchmarks that can reliably target the limita-tions of current pre-trained language models (LMs) in commonsense reasoning. State-of-the-art LMshave achieved or even surpassed human performance on numerous commonsense downstream tasks.Nevertheless, these LMs are still very far from being able to perform commonsense reasoning as wellas humans. Hence, the fact that they have begun to ace existing benchmarks implies that time is ripeto design a new challenging benchmark that can reliably target their limitations. Motivated by this observation, we present DISCOSENSE, a benchmark for performing commonsensereasoning through understanding a wide variety of discourse connectives. shows an exampletaken from DISCOSENSE. As can be seen, an example is composed of a context (e.g., Our waitresswas very nice, but she kept on forgetting my stuff.) and a discourse connective (e.g., For example),and the goal is to choose the most plausible ending out of four options. If we ignore the discourseconnective, then all four options may",
  ": Example on commonsense reasoning with discourse connectives. The correct (i.e., mostplausible) option is boldfaced": "seem plausible because we do not know what the writers intent is. Once we consider both the contextand the discourse connective, then it is clear that only option b) is plausible. The reason is that Forexample signals an EXEMPLIFICATION relation between its arguments, and what follows thediscourse connective is expected to be an example of the waitress keeping on forgetting the writersstuff. Using commonsense knowledge, we know that (1) my beer and fries is an example of mystuff, and (2) her taking forever to bring the writer stuff implies she kept on forgetting his/her stuff. What if we replace For example with However in the example? Since However signals aCONTRAST relation, options a) and d) both seem viable. Specifically, option a) describes a situationin which she did not forget the writers stuff. While option d), unlike option a), does not describe any example that signals a contrast, one may infer a contrast between option d) and the context:being forgetful is fine for some customers. Nevertheless, option a) is arguably more plausible thanoption d) and should be chosen. The reason is that for d) to be sensible, one needs to assume that herforgetting the writers stuff implies that she is in general forgetful. Without this assumption, it maybe strange for other customers to have an opinion on her forgetting the writers stuff. In general, themost plausible option is the option that makes the smallest number of assumptions, and/or is the mostcoherent given the context and the discourse connective. Considering the commonsense knowledgeand the reasoning involved, it should not be difficult to see that this task is challenging. Our contributions are four-fold. First, we create DISCOSENSE, a new dataset aimed at testingLMs commonsense reasoning capabilities through discourse connectives. Second, we employ acontrolled text generation based adversarial filtering approach to generate compelling negatives.Third, we establish baseline results on DISCOSENSE with numerous state-of-the-art discriminatormodels and show that they struggle to perform well on DISCOSENSE, which makes our datasetan ideal benchmark for next-generation commonsense reasoning systems. Finally, we show theefficacy of using DISCOSENSE as a transfer learning resource through sequential fine-tuning ofLMs on DISCOSENSE followed by HELLASWAG and achieve near state-of-the-art results on theHELLASWAG test set. To stimulate work on this task, we make our code and data publicly available.",
  "Related Work": "In this section, we discuss related work, focusing our discussion on the differences between DIS-COSENSE and existing commonsense reasoning benchmarks. In addition, we present an overview ofAdversarial Filtering, which will facilitate the introduction of the Conditional Adversarial Filteringmechanism we propose in . Commonsense reasoning benchmarks. SWAG and HELLASWAG are arguably the most prominentcommonsense reasoning benchmarks. In SWAG, given a partial description along with four candidateendings, the task is to predict the most plausible ending. The synthetic options (a.k.a. distractors)are generated through a process called Adversarial Filtering (AF) (see below). HELLASWAG is anextension of SWAG that seeks to eliminate artifacts in the generated endings. Unlike SWAG andHELLASWAG, DISCOSENSE requires that the discourse connective be taken into account in thereasoning process, thus increasing the number of inference steps and potentially the task complexity.In addition, while the examples in SWAG and HELLASWAG come primarily from ActivityNet (abenchmark focused on dense captioning of temporal events),",
  "DISCOSENSE features a more diverse set of examples coming from varied domains that may onlybe solved with rich background knowledge": "There are benchmarks that aim to test different kinds of commonsense reasoning abilities, althoughnone of them focuses on reasoning over discourse connectives. SocialIQA, for instance, focuses onsocial and emotional commonsense reasoning. ABDUCTIVE NLI focuses on abductive reasoning.WINOGRANDE contains Winograd schema-inspired problems, which are essentially hard pronounresolution problems requiring world knowledge. PIQA examines physical commonsense reasoning.MCTACO and TIMEDIAL focus on temporal reasoning in comprehension and dialogue formats. More closely related to DISCOSENSE are commonsense reasoning benchmarks that involve reason-ing with a particular kind of relations. COPA (Choice of Plausible Alternatives) focuses exclusivelyon reasoning with CAUSAL relations and involves choosing the more plausible ending out of two(rather than four) options. P-MCQA focuses exclusively on reasoning with PRECONDITION rela-tions: given a commonsense fact, select the precondition that make the fact possible (enabling) orimpossible (disabling) out of four options. NLI, which aims to evaluate defensible inference, focusesexclusively on reasoning with the STRENGTHEN/WEAKEN relations: given a premise-claim pairwhere the premise supports the claim, generate a sentence that either strengthens or weakens thesupport. WINOVENTI, which is composed of Winogradstyle schemas, focuses exclusively onreasoning with ENTAILMENT relations: given two sentences with an entailment relation, such asPete says the pear is delicious. The pear is , the goal is to fill in the blank with one of two choices(e.g., edible, inedible). There are two key differences between these datasets and DISCOSENSE.First, rather than focusing on a particular type of relation, DISCOSENSE encompasses 37 discourseconnectives signaling different discourse relation types. Second, DISCOSENSE involves reasoning",
  "(e.g., the connective since may serve as a temporal or causal connective), a LM will likely need to(implicitly) perform sense disambiguation in order to perform well on DISCOSENSE": "There are datasets and knowledge bases where the semantic/discourse/commonsense relations areexplicitly annotated and which can provide data sources from which commonsense reasoning bench-marks can be derived. Examples include (1) the Penn Discourse TreeBank, where two sentences ortext segments are annotated with their discourse relation type, if any; (2) COREQUISITE, whichis used to provide the commonsense facts and the human-generated preconditions in the P-MCQAdataset mentioned above; (3) SNLI, where each premise-hypothesis pair is annotated as ENTAIL-MENT, CONTRADICTION, or NEUTRAL; (4) ATOMIC20, which is a commonsense knowledgegraph where the nodes correspond to propositions and the edges correspond to social/physicalcommonsense relations; and (5) SOCIAL-CHEM-101, which is a collection of statements aboutcommonsense social judgments made given everyday situations. One of the motivations behind the creation of DISCOSENSE is that state-of-the-art LMs have man-aged to achieve or even surpass human performance on various commonsense reasoning benchmarks. shows the best accuracies achieved by existing LMs on 10 widely used commonsense rea-soning benchmarks and the corresponding human performance levels. As can be seen, existing LMshave managed to achieve an accuracy of more than 80 Adversarial filtering (AF). Originally proposed by, AF aims to create examples that would be difficultfor models to solve, specifically by replacing the easy options in correctlysolved examples withdifficult ones. As shown in , AF has three components: data (i.e., examples with multipleoptions, one of which is correct), a discriminator LM (a classifier that is used to solve each example)and a generator LM (a model that generates new options for an example). In each AF iteration, thediscriminator LM is trained on the training set and used to solve each example in the test set. If a testexample is incorrectly solved (i.e., the discriminator LM chooses the wrong option), the exampleis deemed sufficiently difficult and no change is made to it. On the other hand, if a test exampleis correctly solved, then AF seeks to increase its difficulty by replacing the easiest option (i.e., thegenerated option that the discriminator LM classifies with the highest confidence) with a new optiongenerated by the generator LM. Training a new discriminator LM in each AF iteration ensures thatthe dataset is not just adversarial for one LM but a class of LMs, as training different instances ofthe same type of LMs results in models that have differently learned linguistic representations. Thisprocess is repeated on all correctly classified examples in the test set until the performance on the testset converges.",
  "Task Description": "DISCOSENSE aims to measure the commonsense inference abilities of computational modelsthrough the use of discourse connectives. The correct endings can be obtained after understandingthe purpose of the given discourse connectives. Given a context c <s, d>, which is composed of acontextual sentence s and a discourse connective d as well as a set of four options O = o1, o2, o3, o4,the task is to predict the most plausible ending oi belongs to O.",
  "Dataset Creation": "To assemble DISCOSENSE, we focus on source datasets that contain two sentences connected througha discourse connective. Specifically, we use two peer reviewed academic datasets, DISCOVERYand DISCOFUSE. In DISCOVERY, each sentence is composed of two sentences connected viaa discourse connective for the purpose of learning joint sentence representations with discourseconnectives. DISCOFUSE, on the other hand, is assembled for the task of sentence fusion (i.e.,joining several independent sentences into a single coherent sentence). We only consider thoseexamples where a discourse connective is needed for sentence fusion, and include in DISCOSENSEthe fused sentences in the Wikipedia split of DISCOFUSE. Since these datasets contain sentences fromCommon Crawl and Wikipedia articles, DISCOSENSE is diverse in the topics it covers. Importantly,since by construction the discourse connective is crucial in solving the underlying tasks (i.e., sentencerepresentation learning and sentence fusion), the crucial role played by the discourse connectivesin these sentences makes them suitable for our use case. Details of how the DISCOVERY andDISCOFUSE sentences are used to create DISCOSENSE are shown in Tables 2 and 3.",
  "Generating Options": "Next, we describe how we generate options for the examples in DISCOSENSE. Recall that eachexample contains one of 174 discourse connectives. Rather than generating options for examples thatcontain any of these 174 connectives, we select 37 discourse connectives and generate options onlyfor examples that contain one of them. The connectives that are discarded are primarily those thatimpose few constraints on the endings to be gen- erated given the context according to preliminary experiments. For instance, the connective andis discarded because numerous endings are equally plausible. Similarly for connectives that signala temporal relation (e.g., before, after): they also tend to allow numerous equally plausibleendings, as can be seen in examples such as John went to eat lunch after [ending]. The 37connectives that we end up choosing are shown in . These connectives are less likely to yieldoptions that look equally plausible to human annotators and which are indicative of different kindsof discourse relations, such as EXEMPLIFICATION (e.g., for instance), CONCESSION (e.g.,although), COMPARISON (e.g., in contrast), and CAUSAL (e.g., as a result). 94k examples inDISCOSENSE contain one of the 37 connectives. althoughin other wordsparticularlybecause of thisin sumspecificallybecause of thatinterestinglysubsequentlybutinsteadthereafterconsequentlylikewisetherebyconverselyneverthelessthereforefor examplenonethelessthoughfor instanceon the contrarythushenceon the other handyethoweverotherwisein contrastoverall",
  "Conditional Generator LM": "Pre-training does not explicitly teach how important a particular token or text span is in contributingto the semantics of a sentence. Hence, to be able to generate sentences that are coherent with notonly the context but also the discourse connective, we propose to use Controllable Text Generation,which aims to provide a more granular control over how generation happens to match a particularattribute. In the context of Transformer-based LMs, there are two lines of research on controllabletext generation. One examines how to steer generation by fine-tuning an extra set of parameters whilekeeping the base (unconditionally trained) model fixed while the other involves conditionally traininga generative model on a control variable to generate text w.r.t. a prompt prefix. We adopt the latter",
  "input: <d> <contexts> label: <endings>": "where d is a discourse connective. Specifically, each input context for CTRL is prepended with aconnective, and the training task for CTRL is to learn the conditional distribution p(e|d, context)over possible endings e. The predicted ending is then compared with the human generated ending tocompute loss. Since the original CTRL model is pre-trained with control codes suitable for openendedtext generation, we fine-tune CTRL on the portion of DISCOVERY shown in using all the174 connectives present in the selected splits. Comparing Tables 2 and 3, we can see that the datathe generator LM is fine-tuned on is not part of DISCOSENSE. Doing so ensures that the endingsgenerated by the generator LM are different from the ground truth (i.e., the human written endings).",
  "Decoding. We use Nucleus sampling for generating options for the training set with the value of p setto 0.7, which means the": "weights of the tail of the probability distribution are ignored (i.e., tokens with a cumulative probabilitymass of less than 0.3 are left out). Additionally, we use a length penalty of 0.8 to restrict the length ofthe generations to match the average length of the ground truth to avoid the induction of length bias. Efficacy of conditional generation. Recall that we propose the use of conditional generation, specifi-cally the use of discourse connectives as control codes, in our generator LM because of our hypothesisthat the resulting LM would generate options that are more compliant with the purpose of the dis-course connective. To test this hypothesis, we compare the text generation capability of CTRLwith that of GPT2-XL, a model that is trained unconditionally and has nearly the same number ofparameters (1.6B) as CTRL, under the same evaluation setting. Specifically, both LMs are fine-tunedon the same data (see ) using the same machine (a 2x Quadro RTX 8000 with a batch sizeof 24). The only difference between them lies in the format of the training examples: in CTRLthe discourse connective is used as the control code and therefore precedes the context, whereas inGPT2XL, the discourse connective follows the context. The two LMs are then independently applied to generate exactly one option for each example in theDISCOVERY validation set. CTRL achieves a much lower perplexity than GPT2-XL (2.39 vs. 2.53),which suggests that conditional training improves the quality of the generated sentences.",
  "Discriminator LM": "We use ROBERTA-LARGE as the discriminator LM, which takes the context, the discourse connec-tive, and the four endings as input and predicts the most plausible ending. This LM is trained on therandomly shuffled training split of DISCOSENSE and applied to the DISCOSENSE test set to getthe confidence scores associated with its predictions.",
  ": Data statistics for DISCOSENSE": "To generate the options for these 94k sentences, we begin by training 20 generator LMs on arandomly shuffled order of the generators training data (see ) and then inserting them into acircular queue. Although the underlying data is the same, random shuffling ensures that the learnedrepresentations of these 20 models are different. Since each example needs to have 3 syntheticoptions, we use the first 3 generator LMs from the circular queue to generate the initial options foreach example. After that, we begin CAF. In each CAF iteration, we (1) train the discriminator LM(see .3.2) on the DISCOSENSE training set for 4 epochs and use it to filter out the optionsdeemed as easiest by the discriminator LM; and (2) use the next generator LM in the circular queueto generate the options for the examples whose easiest option is removed by the discriminator LM. Inother words, a different discriminator LM is used in each CAF iteration, and a generator LM in the circular queue is used once every 20 CAF iterations. CAF is run separately for the DISCOSENSEtraining and test sets. After running CAF for approximately 150 iterations, the average accuracy of adiscriminator LM decreased from 8690",
  "Other Implementation Details": "For the models we use in CAF, we obtain the pre-trained weights and the implementations fromHugging Face Transformers. These models are trained using the AdamW optimizer with a learningrate of 2e-5. The training of each generator LM is performed on a 2x Quadro RTX 8000 with a batchsize of 24 and typically lasts for 3 days. The training of a discriminator LM is performed on a RTX3090 with a batch size of 16 and typically lasts for 56 hours.",
  "Human Verification": "Next, we perform human verification of the examples for which we have generated options. Theverification proceeds in two steps. In Step 1, we ask three human verifiers to independently identifythe correct option for each example, removing an example if at least one person fails to identify thecorrect option. We repeat this process until the number of examples that survive this verification",
  ": Accuracies (best results obtained among 8 epochs when averaged over 5 runs with randomseeds) of the LMs on the DISCOSENSE test set": "reaches 13,056. In Step 2, we ask three human verifiers not involved in Step 1 to independentlyidentify the correct option for each of the 13,056 examples verified in Step 1. We compute foreach verifier the accuracy of choosing the correct option and use the average accuracy as the humanperformance on DISCOSENSE. Appendix A contains the details on how the human verifiers arerecruited and the annotation instructions we present to them.",
  "Dataset Statistics": "Statistics on DISCOSENSE are shown in , in which we report the average number of tokensin (1) the context, (2) the ground truth and (3) the generated endings. The number of unique tokensprovides a rough characterization of the richness of the vocabulary. In addition, we report thedistribution of the examples over the discourse connectives in DISCOSENSE in .",
  "Baseline Systems": "Our baselines are composed of prominent LMs with different kinds of Transformer architectures. First,we consider models that are pre-trained in a BERT-like fashion and share architectural similarities,including the base and large variants of BERT and ROBERTA, as well as ALBERT-XXLARGE-V2.As an extension, we select LONGFORMER BASE, which is pre-trained in the same manner asROBERTA but has a sparse attention matrix. From the autoregressive/decoder based networks,we experiment with XLNET LARGE, which maximizes the learning of bidirectional contexts andGPT2-XL. For",
  "Second, models sharing a similar pre-training objective as that of BERT, such as ROBERTA andLONGFORMER, are among the worst baselines. A similar trend is observed with XLNET. Although": "ALBERT has the Masked Token Prediction task in its pre-training objective, its architectural differ-ences (i.e., larger hidden states and parameter sharing) and its Sentence Order Prediction objectiveseem to help it learn inter-sentence coherency properties better than its BERT counterparts. Third, pre-training appears to play a predominant role in our task. While the BERT family of modelsare trained with the masked-LM objective, the pre-training objective of ELECTRA (the best baseline)is designed to determine if a token in a human-written sentence has been replaced by a generator. Wespeculate that ELECTRAs superior performance can be attributed to the fact that its pretrained knowledge of discriminating between syn-thetic and human generated tokens transfers well to the task of discriminating between syntheticallygenerated sentences and human written sentences in DISCOSENSE. Nevertheless, the fact that itonly achieves an accuracy of 65.87 Finally, we report human performance in the last row of . Details of how these numbers areobtained are discussed in .4. As can be seen, the accuracy achieved by the best baseline,ELECTRA, lags behind that of humans by nearly 30",
  "Quantitative Error Analysis": "We perform a quantitative error analysis of our best-performing model, ELECTRA. Specifically,we compute for each discourse connective the percentage of examples in the DISCOSENSE testset that are misclassified by ELECTRA, with the goal of gaining a better understanding of thediscourse connectives that are perceived as easy as well as those that are perceived as difficult as faras commonsense reasoning is concerned.",
  "Results are shown in . As we can see,": "the misclassification rates are highest for those discourse connectives that express contrast (e.g.,otherwise, however, but, although). A plausible explanation for this result is that it is oftenhard to anticipate what a human would have in mind if they are trying to indicate the opposite of whatthey mean to say. On the other hand, the model finds it easy to predict sentences where the discourseconnective signals compliance and exemplification (e.g., similarly, likewise, hence, becauseof that, for example).",
  "To better understand the mistakes made by ELECTRA, we manually inspected 100 randomly selectedexamples that are misclassified and identified four major reasons why they are misclassified": "Less plausible endings. This category contributes to 21 perentt of the errors where the modelchooses a less plausible ending. Choosing a less plausible option could be associated with a partialunderstanding of the context or unwarranted assumptions. In Example 1 of , the model makesthe assumption that whatever is applicable to grass is also applicable to trees. However, the option itends up picking is non-factual in nature because of the phrase 7000 years ago. Abstract associations. 14 percent of the errors are made due to the formation of abstract associationsbetween concepts. The model seems to rely on certain spans of context for classification rather thanunderstand the semantics in its entirety. In Example 2 of , the model seems to wronglyassociate energy dense nutrients with obesity and fails to understand that the context is discussingthe correlation between nutrient deficit diet and people belonging to lower income groups.",
  "b) The dinosaurs were not billions of years old": "c) Several seeds were found encased within stems that are several months old, but they seemed quitefresh and alive. d) The trees, although only a day old when they sprouted forth, were neverthelesslike trees years old as they were fully grown. Low income people are less likely to consume a healthy diet than wealthier people, and energydense nutrients poor diets are preferentially consumed by persons of lower socioeconomic status.Consequently",
  "make a person do, in this case, ask bigger questions": "Lack of understanding of the discourse connective. In many cases it is difficult to pinpoint the reasonwhy an example is misclassified. Hence, if a misclassified example is not covered by any of the firstthree categories, we attribute the mistake to a lack of understanding of the discourse connective. Thiscategory contributes to 42",
  "Role of Context and Discourse connective": "To better understand the role played by the context and the discourse connective in a LMs reasoningprocess, we conduct two ablation experiments. In the first experiment, we remove the discourseconnective, so only the context and the endings are available to the LMs. In the second experiment,we strip the context and the discourse connective, exposing only the endings to the LMs."
}