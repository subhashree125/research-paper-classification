{
  "Abstract": "This paper details the solution developed for the 2021 Shifts Challenge, whichfocused on robustness and uncertainty in real-world distributional shifts. Thecompetition sought methods for addressing motion prediction in cross-domainscenarios. A key issue is the variance between input and ground truth data distribu-tions, known as the domain shift problem. The method proposed features a novelarchitecture utilizing a self-attention mechanism and a specifically designed lossfunction. Ultimately, this approach achieved 3rd place in the competition.",
  "Introduction": "This paper examines the crucial issue of prediction in autonomous driving. Predicting vehicletrajectories to generate control commands is essential for avoiding collisions. While deep learninghas shown promise in specific domains, real-world conditions, such as varying environments, weather,and driver behaviors, create challenges for models trained on single datasets. These models may notperform well across diverse datasets. The 2021 Shifts Challenge concentrated on prediction tasks across different domains. The goal wasto predict 25 timestamps of trajectories from given raster images. To address this, a new architecturewas developed using insights from current research. The feature extractor was modified using NFNetfor stability, and a self-attention layer was included to enhance time-related predictions. The lossfunction was also adjusted for improved robustness, leading to a 3rd place ranking with 8.637 R-AUCCNLL in the competition.",
  "Our Solution": "This section explains the solution for the domain-shift problem through the design of new modelarchitectures. The domain-shift problem arises when training and validation datasets come fromdifferent distributions. Given input raster images X that contain the first 5 seconds of vehicle data, theobjective is to predict the last 5 seconds of trajectories Y for the objects. These images include detailsabout the positions, orientations, accelerations, and velocities of dynamic objects. The proposedmodel has two main parts: (1) a new backbone model and feature extractor, and (2) a revised lossfunction for better performance.",
  "Z = f(X)(1)": "The baseline applies MobileNetV1 as its backbone. MobileNetV2 and MobileNetV3 were alsoconsidered but produced worse results, likely due to the simplicity of input data and model complexity.Ultimately, the NFNet was chosen as the backbone (feature extractor) because of its training stability. Self-Attention Layer To further refine the raster image features, a self-attention layer was in-corporated. Self-attention, a key part of the Transformer model, allows for the consideration oflong-range dependencies and global information. The feature map was divided into pixel groups, andself-attention was used to aggregate pixel-wise information. Recurrent Model The GRU model was selected for the recurrent component due to superiorperformance compared to other models. Using the embedding from feature extraction as hiddenstates, the recurrent model makes predictions recursively. Given the embedding Zt at time t, with theoutput vector Y0 as zero vector, the recurrent model g is used to generate predictions:",
  "Vehicle Motion Prediction includes 600000 scenes that vary in season, weather, location and time ofday": "Evaluations metrics The evaluation used three metrics: Average Distance Error (ADE), FinalDistance Error (FDE), and Negative log-likelihood (NLL). ADE measures the sum of squared errorsbetween predicted and actual positions at each time step. FDE calculates the sum of squared errors ofthe final positions. NLL measures the unlikelihood of predicted trajectories matching the actual ones.",
  "Ablation Study and Comparison Results": "Ablation Study displays the results of the ablation study. The baselines selected wereDIM and BC. Various backbones, including EfficientNet, NFNet, and MobileNet, were compared,but models with more parameters performed worse. This result suggests that simpler models aresufficient for extracting raster image information. Adding a self-attention mechanism improved theresults. Finally, incorporating ADE and FDE loss further improved performance, as shown in . Although the DIM method resulted in the lowest Negative Log Likelihood(NLL), it was not ascompetitive as other models. Therefore, the DIM model was not chosen to pursue performance.",
  "MethodADE In DomainFDENLLADE Out of DomainFDENLL": "DIM + MobileNetV2(baseline)2.4505.592-84.7242.4215.639-85.13BC + MobileNetV2(baseline)1.6323.379-42.9801.5193.230-46.88BC + NFNet181.2252.670-53.1491.3002.893-53.13BC + NFNet501.3602.963-50.6051.3923.066-51.31BC + NFNet18 + Attention1.1742.549-56.1991.3252.852-54.47BC + NFNet50 + Attention1.1552.504-56.2911.2652.770-54.73BC + NFNet18 + ADE Loss1.1972.55-54.0471.2992.821-53.05BC + NFNet18 + Attention + ADE Loss1.1392.488-55.2081.2272.714-54.28 Comparison Results After verifying the base models effectiveness, the aggregation model, RIP, wasused along with the Worst Case Method (WCM). The WCM method samples multiple predictionsper model and picks the one with the lowest confidence for more reliable results. shows thecompetition results, where our model outperformed baselines in weighted sums of ADE and FDE.However, the MINADE and MINFDE results were not as strong. Overall, this approach secured 3rdplace.",
  "Conclusion": "In this challenge focused on distributional shifts, we introduced a novel base model architecture,which combined with an ensemble method, yielded competitive results. Other state-of-the-art methodswere implemented, and results were compared with analysis. The robustness of the provided ensemblemethod was verified. This methodology resulted in the third prize in the competition."
}