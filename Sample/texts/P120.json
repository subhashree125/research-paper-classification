{
  "Introduction": "We present diagNNose, a publicly available library for analyzing the behavior ofdeep neural networks. The diagNNose library equips researchers with tools to gainenhanced understanding of the internal representations formed by these networks,providing a comprehensive suite of established analysis methods. It accommodatesa variety of model types, with a particular focus on NLP architectures, such asLSTMs and Transformers.The availability of open-source libraries has been instrumental in the advancementand wider adoption of NLP technologies. We enhance the open-source ecosystemby integrating several interpretability techniques.Recent years have witnessed significant interest in enhancing our understanding ofthe mechanisms by which deep neural networks function. The high-dimensionalarchitecture of these models makes deciphering their internal dynamics a complexendeavor. This complexity has spurred the emergence of a specialized subfieldwithin AI, dedicated to interpretability. diagNNose seeks to consolidate a range ofthese interpretability techniques into a unified library.The primary objective of diagNNose is to facilitate the discovery of linguisticknowledge encoded within a models representations. The library offers abstrac-tions that enable the investigation of recurrent models in a manner similar toTransformer models, using a modular design. It includes a module for extractingmodel activations. The analysis methods currently implemented in the libraryinclude targeted syntactic evaluation tasks, probing with diagnostic classifiers, andfeature attributions.This paper provides a comprehensive overview of the library and illustrates itsapplication in a case study centered on subject-verb agreement within languagemodels. Subsequently, we provide a survey of diagNNose and elaborate on itsspecific modules. We conclude with the case study.",
  "Language models have been central to numerous achievements in NLP. Thesemodels are trained to predict the probability of upcoming or masked tokens. To": "achieve success in this task, models must grasp various linguistic aspects, includingsyntax, semantics, and general domain knowledge. One notable area of researchinvestigating a models linguistic competence employs targeted syntactic evalu-ations. This analysis method contrasts a models outputs on minimally differentpairs of grammatical and ungrammatical constructions. If a model assigns a higherprobability to the grammatical construction, it suggests an understanding of therelevant linguistic principles.diagNNose supports a diverse set of syntactic tasks and offers an interface forincorporating new tasks seamlessly.",
  "Diagnostic Classifiers": "Another line of research evaluates a models comprehension of linguistic propertiesby training diagnostic classifiers on its representations. This technique, also knownas probing, has yielded valuable insights into the internal mechanisms of languagemodels. The activations used for training these classifiers are not limited to thehidden states of a language model at its top layer.There have been recent discussions regarding the extent to which high accuracy in adiagnostic classifier truly signifies that a property is actively encoded by the model.Several methods have been put forward to address this, such as using control tasksor assessing classifiers based on minimum description length. diagNNose currentlysupports the training of diagnostic classifiers and control tasks.",
  "Feature Attributions": "To gain a deeper understanding of why language models exhibit particularly poorperformance on the NAMEPP corpus, we employ the feature attribution module onthese constructions. The results for this experiment are presented below, illustrating the attributions for DistilRoBERTa on an example sentence from the corpus. Thishighlights the differential impact of the intervening attractor on the verbs number.The score at the top of the attribution represents the models full logit for thatclass; these logits are transformed into probabilities using SoftMax. This logit isdecomposed into a sum of contributions, indicated at the bottom of each token.It can be verified that the contributions sum to the logit, which is an importantcharacteristic of feature attribution methods, ensuring a degree of faithfulness tothe model. A negative value signifies a negative feature contribution to an outputclass: the influence of that feature diminished the preference for the class. Featureattributions also incorporate the influence of model biases.In the provided example sentence, DistilRoBERTa produces an incorrect prediction:the logit of the incorrect singular form approves is greater than that of the pluralapprove. The models error in predicting the correct verb form arises from thesubject athletes not providing sufficient contribution to outweigh the negativecontributions from other input features. A model with a comprehensive grasp ofsubject-verb agreement should assign a larger contribution to the subject whenpredicting the main verb.The attribute module is under active development. The exponential complexity ofcomputing Shapley values makes generating these explanations a challenging task.",
  "The library is organized into multiple modules that can be utilized as componentsfor constructing an experimental pipeline": "3.1.1Core ModulesThe foundational modules underpinning the various pipelines that can be builtusing diagNNose are detailed below.**models:** We offer a generalized framework for language models, enablingboth recurrent and Transformer models to be accessed through a unified interface.Importing pre-trained Transformer models is accomplished using the transform-ers library. For recurrent models, we provide an interface that allows access tointermediate activations, including gate activations.**corpus:** Corpora are imported as Datasets from the torchtext package. ACorpus can be converted into an iterator for processing. Tokenization can beperformed traditionally, token-by-token, or based on subword units, such as bytepair encodings.**extract:** The extraction of activations is fundamental to most analysis modules.We provide an Extractor class capable of extracting a models activations given acorpus. This process is not restricted to the top layer; intermediate (gate) activationscan also be extracted. Activations can be dynamically saved to disk to facilitate theextraction from large corpora with limited computational resources. **activations:** Extracted activations can be readily accessed using an Activation-Reader, which provides access to activations corresponding to specific subsets ofcorpus sentences. We also offer functionality for extracting only particular subsetsof activations, based on sentence and token information.**config:** The pipeline of diagNNose is driven by configuration defined in JSONformat. Individual attributes can also be directly set from the command line.",
  "Analysis Modules": "We presently offer three primary types of experimental modules.**syntax:** The library offers capabilities for a broad range of targeted syntacticevaluation tasks.**probe:** We furnish convenient tools for training diagnostic classifiers on ex-tracted activations to probe for linguistic information that may be embedded withinthem. Our extraction module also enables training diagnostic classifiers on in-termediate activations, including gate activations. To address concerns that highprobing accuracy does not necessarily indicate that linguistic information is activelyencoded, we have incorporated functionality for Control Tasks.**attribute:** We offer capabilities for model-agnostic feature attributions, en-abling the decomposition of a models output into a sum of contributions. Thisis accomplished by implementing a wrapper over PyTorch operations, allowingintermediate feature contributions to be propagated during a forward pass. Ourimplementation supports various Shapley-based attribution methods and facili-tates approximation procedures such as (Generalized) Contextual Decompositionand Shapley sampling values, in addition to the exact computation of propagatedShapley values.",
  "Requirements": "diagNNose can be installed using pip (pip install diagnnose) or cloned directlyfrom the GitHub repository. The library is compatible with Python 3.6 or later,and its primary dependencies are PyTorch (v1.5+), torchtext, and HuggingFacestransformers. diagNNose is released under the MIT License. It operates on bothCPUs and GPUs and has been optimized for smaller consumer setups.The diagNNose codebase is fully typed using Python type hints and formattedusing Black. All methods and classes are documented, with an overview availableonline.",
  "Corpora": "The corpora consist of seven tasks based on template-based syntactic constructions.These constructions feature an \"agreement attractor\" between the subject and theverb, which may mislead a language model into predicting the incorrect number ofthe verb. Consequently, a model must possess a robust understanding of sentencestructure.The seven tasks are defined by the following templates:* SIMPLE: The athletes approve * ADV: The uncle probably avoids * 2ADV: Theathlete most probably understands * COADV: The farmer overtly and deliberatelyknows * NAMEPP: The women near John remember * NOUNPP: The athletebeside the tables approves * NOUNPPADV: The aunt behind the bikes certainlyknows Each task encompasses 600 to 900 distinct sentences. Sentences are categorizedinto multiple conditions based on the number of the subject and the interveningnoun phrase.To assess these corpora on a recurrent model, we initially compute the modelshidden state at the verbs position by feeding it the sub-sentence up to that point.Based on this hidden state, we compute and compare the output probabilities of theverb with the correct number (vc) and the incorrect number (vx):P(vc | he) > P(vx | he)For bi-directional masked language models, such as BERT, we cannot computean intermediate hidden state by passing a sub-sentence because these models alsoincorporate input from future tokens. To address this, we substitute the verb ineach sentence with a <mask> token and evaluate the models probabilities at thistokens position.Many contemporary language models employ BPE tokenization, which may seg-ment a word into multiple subwords. Therefore, in our experiments, we onlycompare verb forms where both the plural and singular forms are split into a singletoken.",
  "CorpusConditionBERTRoBERTaDistilRoBERTaLSTM": "SIMPLES100100100100P100100100100ADVS100100100100P10010010099.62ADVS10010010099.2P10010010099.3COADVS10010010098.7P10010010099.3NAMEPPSS93.075.781.599.3PS88.465.932.468.9NOUNPPSS95.788.998.199.2SP93.384.791.187.2PS96.790.685.392.0PP10010010099.0NOUNPPADVSS99.610010099.5SP99.299.810091.2PS10010010099.2PP10010010099.8 It is evident that Transformer language models generally attain higher scorescompared to the LSTM model. Notably, the NAMEPP task presents a challenge forall models, with both RoBERTa and DistilRoBERTa scoring lower on this task thanthe LSTM. Another intriguing observation is the disparity in performance betweenRoBERTa and DistilRoBERTa on the NAMEPP and NOUNPP tasks. DespiteDistilRoBERTa being trained to mimic RoBERTas behavior, its performance ona downstream task like this differs considerably. These findings can serve as afoundation for more detailed analysis."
}