{
  "Abstract": "Recent experiments on word analogies demonstrate that contemporary word vectorseffectively encapsulate subtle linguistic patterns through linear vector displace-ments. However, the extent to which these straightforward vector displacementscan represent visual patterns across words remains uncertain. This research in-vestigates a particular image-word relevance relationship. The findings indicatethat, for a given image, word vectors of pertinent tags are positioned higher thanthose of unrelated tags along a primary axis within the word vector space. Drawinginspiration from this insight, we suggest addressing image tagging by determiningthe main axis for an image. Specifically, we utilize linear mappings and intricatedeep neural networks to deduce the primary axis from an input image. The re-sultant tagging model exhibits remarkable adaptability. It operates swiftly on testimages, with a processing time that remains constant regardless of the training setssize. Furthermore, it showcases exceptional performance not only in conventionaltagging tasks using the NUS-WIDE dataset but also in comparison to competitivebaselines when assigning tags to images that havent been seen during training.",
  "Introduction": "Recent advancements in representing words in vector spaces have proven advantageous for bothNatural Language Processing and various computer vision applications, including zero-shot learningand image caption generation. The rationale behind using word vectors in NLP is rooted in theobservation that detailed linguistic patterns among words are represented by linear offsets of wordvectors. This pivotal insight emerged from well-known word analogy studies. For example, syntacticrelationships like \"dance\" to \"dancing\" parallel \"fly\" to \"flying,\" and semantic connections like \"king\"to \"man\" mirror \"queen\" to \"woman.\" Nevertheless, it is yet to be determined whether the visualpatterns across words, implicitly employed in the aforementioned computer vision tasks, can similarlybe represented by these basic vector offsets. This paper focuses on the task of image tagging, where an image necessitates the division of a wordlexicon into two distinct groups based on image-word relevance. For example, an image of a zoo mighthave relevant tags like \"people,\" \"animal,\" and \"zoo,\" while irrelevant tags might include \"sailor,\"\"book,\" and \"landscape.\" This lexical division fundamentally differs from the nuanced syntactic orsemantic relationships examined in word analogy tests. Instead, it concerns the connection betweentwo sets of words as prompted by a visual image. This type of word relationship is semantic anddescriptive, emphasizing visual association, albeit at a broader level. Given this context, it is worthinvestigating whether word vectors maintain the property where simple linear vector offsets candepict visual or image-based associative relationships between words. In the zoo example, while itseasy for humans to recognize that words like \"people,\" \"animal,\" and \"zoo\" are more related to thezoo than words like \"sailor,\" \"book,\" and \"landscape,\" the question is whether such a zoo-associationrelationship can be represented by the nine pairwise vector offsets: \"people\" minus \"sailor,\" \"people\"minus \"book,\" and so on, up to \"zoo\" minus \"landscape,\" between the vectors of relevant and irrelevanttags. A primary contribution of this research is an empirical investigation of these questions. Each imageestablishes a visual association rule over words, represented as a pair (Y, Y). Leveraging the extensive",
  "This implies that the vector w ranks all relevant words Y ahead of irrelevant ones Y": "The visual association patterns among words manifest as the linear rank-abilities of their correspond-ing word vectors. This observation corroborates findings from word analogy studies, suggesting thatmultiple relationships for a single word are embedded within a high-dimensional space. Furthermore,these relationships can be articulated using basic linear vector arithmetic. Building on this discovery, we propose a solution to the image tagging challenge by identifying theprimary axis along which relevant tags are ranked higher than irrelevant ones within the word vectorspace. We employ both linear mappings and deep neural networks to infer this primary axis fromeach input image. This unique perspective on image tagging yields a highly adaptable tagging model.The model processes test images rapidly, maintaining a constant processing time irrespective of thetraining datasets size. It not only delivers outstanding results in traditional tagging tasks but alsoexcels at assigning new tags from a broad vocabulary that were not encountered during training. Ourmethod does not rely on prior knowledge of these new tags, as long as they exist within the samevector space as the tags used during training. Consequently, we designate our technique as \"fastzero-shot image tagging\" (Fast0Tag), acknowledging its strengths in both speed and its zero-shotlearning capabilities. In stark contrast to our approach, prior methods for image tagging are limited to assigning only thosetags to test images that were seen during training, with a notable exception. These methods areconstrained by the fixed and often limited number of tags present in the training data, which posespractical challenges. For example, Flickr hosts approximately 53 million tags, and this number israpidly increasing. The work of Fu et al. represents a pioneering effort to extend an image taggingmodel to previously unseen tags. However, when compared to our proposed method, it depends ontwo extra assumptions. Firstly, it assumes that unseen tags are known beforehand to enable modeladjustment toward these tags. Secondly, it assumes that test images are known in advance for modelregularization. Moreover, this method is restricted to a very limited number, U, of unseen tags, as itneeds to account for all 2Upossibletagcombinations. To recap, our primary contribution lies in analyzing visual association patterns in words as they relateto images and how these patterns are reflected in word vector offsets. We posit and confirm throughexperiments that a main direction exists in the word vector space for each visual association rule(Y, Y), where vectors of relevant words are ranked higher than others. Building on this, our secondcontribution is an innovative image tagging model, Fast0Tag, which is both swift and capable ofhandling an open vocabulary of unseen tags. Lastly, we explore three distinct image tagging scenarios:traditional tagging, which assigns seen tags to images; zero-shot tagging, which annotates imageswith numerous unseen tags; and seen/unseen tagging, which uses both seen and unseen tags. Existingresearch either addresses traditional tagging or zero-shot tagging with a limited number of unseentags. Our Fast0Tag method surpasses competitive baselines across all three scenarios.",
  "Related Work": "Image Tagging. The objective of image tagging is to allocate pertinent tags to an image or to generatea ranked list of tags. Within the academic community, this challenge has predominantly been tackledfrom the standpoint of tag ranking. Generative approaches, which incorporate topic models andmixture models, inherently rank candidate tags based on their conditional probabilities relative to thetest image. Conversely, non-parametric, nearest-neighbor-based techniques frequently rank tags fora test image by aggregating votes from a selection of training images. Although nearest-neighbormethods generally exhibit superior performance compared to those reliant on generative models,they are plagued by substantial computational demands during both training and testing phases. The recently introduced FastTag algorithm offers a significant speed advantage while maintainingperformance levels on par with nearest-neighbor methods. Our Fast0Tag method mirrors the reducedcomplexity of FastTag. Embedding techniques, on the other hand, determine tag ranking scores viaa cross-modal mapping between images and tags. This concept has been further developed usingdeep neural networks. Notably, aside from certain exceptions, the majority of these methods do nottrain their models with an explicit ranking objective, despite ultimately ranking candidate tags fortest images. This discrepancy between the trained models and their practical application contravenesthe principle of Occams razor. We incorporate a ranking loss in our approach, similar to theseexceptions. Unlike our Fast0Tag, which is capable of ranking both known and an unlimited numberof previously unseen tags for test images, the methods mentioned earlier are restrictedto assigning tags to images from a predetermined vocabulary encountered during train-ing.An exception to this is the work by Fu et al., where they address a predefinednumber, U, of unseen tags by developing a multi-label model that considers all possible2Ucombinationsofthesetags.However, thisapproachisconstrainedbythesmallnumberUofunseentagsitcanhandle. Word Embedding. Diverging from the conventional one-hot vector representation of words, wordembedding maps each word to a continuous-valued vector, primarily learning from the statisticalpatterns of word co-occurrences. While earlier studies on word embedding exist, our researchemphasizes the latest GloVe and word2vec vectors. As demonstrated in the well-known word analogyexperiments, both types of word vectors effectively capture detailed semantic and syntactic patternsthrough vector offsets. In this study, we further reveal that basic linear offsets can also represent thebroader visual association patterns among words. Zero-Shot Learning. The term \"zero-shot learning\" is frequently used interchangeably with \"zero-shotclassification,\" although the latter is actually a subset of the former. In contrast to weakly-supervisedlearning, which acquires new concepts by extracting information from noisy samples, zero-shotclassification aims to classify objects from unseen classes by learning classifiers from seen classes.Attributes and word vectors are two primary semantic sources that enable zero-shot classification. Our Fast0Tag, together with Fu et al., expands the domain of zero-shot learning to include zero-shot multi-label classification. Fu et al. approach this by converting the problem into zero-shotclassification, where each combination of multiple labels is treated as a separate class. We, on theother hand, model the labels directly, allowing us to assign or rank a large number of unseen tags foran image.",
  "The Linear Rank-Ability of Word Vectors": "Our Fast0Tag method is enhanced by the discovery that the visual relationship between words,specifically how a lexicon is divided based on relevance to an image, manifests in the word vectorspace as a main direction. Along this direction, words or tags that are relevant to the image are rankedhigher than those that are not. This section elaborates on this discovery.",
  "The Regulation Over Words Due to Image Tagging": "Lets denote S as the set of seen tags available for training image tagging models, and U as the setof tags unseen during the training phase. The training data is structured as (xm, Ym); m = 1, 2, ...,M, where xm represents the feature vector of image m in RD, and Ym is a subset of S, containingthe seen tags relevant to that image. For simplicity, we also use Ym to represent the collection ofcorresponding word or tag vectors. Traditional image tagging seeks to assign seen tags from S to test images. Zero-shot tagging, asdefined by Fu et al., aims to annotate test images using a predetermined set of unseen tags, U. Beyondthese two scenarios, this paper introduces seen/unseen image tagging, which identifies both relevantseen tags from S and relevant unseen tags from U for test images. Furthermore, the set of unseentags, U, can be open and continuously expanding. We define Ym as the complement of Ym in S, representing irrelevant seen tags. An image mestablishes a visual association rule among words, essentially partitioning seen tags into two distinctsets: Ym and Ym. Recognizing that various detailed syntactic and semantic patterns among words",
  "Principal Direction and Cluster Structure": "offers a visual representation of vector offsets (p - n), where p belongs to Ym and n belongsto Ym, using both t-SNE and PCA for two different visual association rules over words. One rule isdefined by an image associated with 5 relevant tags, and the other by an image with 15 relevant tags.From these vector offsets, we identify two key structures: Principal Direction: For a given visual association rule (Ym, Ym) in words for image m, the vectoroffsets predominantly point in a similar direction, which we refer to as the principal direction. Thissuggests that along this principal direction, relevant tags Ym are ranked higher than irrelevant onesYm. Cluster Structure: Within each visual association rule over words, there are discernible clusterstructures in the vector offsets. Moreover, all offsets that point to the same relevant tag in Ym aregrouped within the same cluster. In , we distinguish offsets pointing to different relevant tagsby using different colors. The question remains whether these two observations can be generalized. Specifically, do they remainvalid in the high-dimensional word vector space for a broader range of visual association rules definedby other images? To address this, we designed an experiment to confirm the existence of principaldirections in word vector spaces, or equivalently, the linear rank-ability of word vectors. We defer theinvestigation of the cluster structure to future research.",
  "Testing the Linear Rank-Ability Hypothesis": "The experiments in this section are performed using the validation set of the NUS-WIDE dataset,which includes 26,844 images, 925 seen tags (S), and 81 unseen tags (U). The number of relevantseen/unseen tags associated with an image varies from 1 to 20/117, with an average of 1.7/4.9. Furtherdetails can be found in . Our goal is to explore whether a primary direction exists for any visual association rule (Ym, Ym)created by image m, along which relevant tags Ym rank higher than irrelevant tags Ym. This can beconfirmed if we find a vector w in the word vector space that fulfills the ranking conditions (w, p) >(w, n) for all p in Ym and n in Ym. To achieve this, we train a linear ranking SVM for each visual association rule using all correspondingpairs (p, n). We then rank word vectors using the SVM and assess the number of violated constraints.Specifically, we use MiAP, with higher values being preferable, to compare the SVMs ranking listagainst the ranking constraints. This process is repeated for all validation images, resulting in 21,863unique visual association rules.",
  "Here, is a hyperparameter that balances the objective and regularization": "Results. The average MiAP outcomes across all distinct regulations are presented in (left).We evaluate 300D GloVe vectors and word2vec vectors of dimensions 100, 300, 500, and 1000. Thehorizontal axis represents various regularizations used for training the ranking SVMs, with highervalues indicating stronger regularization. In the 300D GloVe space and word2vec spaces of 300, 500,and 1000 dimensions, more than two ranking SVMs, with low values, produce nearly ideal rankingresults (MiAP 1). This demonstrates that seen tags S are linearly rankable under almost every visualassociation rule, satisfying all ranking constraints set by relevant Ym and irrelevant Ym tags forimage m. However, caution is advised before extending conclusions beyond the experimental vocabulary Sof seen tags. While an image m imposes a visual association rule over all words, this rule leadsto different partitions of distinct experimental vocabularies (e.g., seen tags S and unseen tags U).",
  "Therefore, we anticipate that the principal direction for seen tags should also apply to unseen tagsunder the same rule, if the questions at the end of .2 are answered affirmatively": "Generalization to Unseen Tags. We investigate whether the same principal direction applies to bothseen and unseen tags under each visual association rule induced by an image. This is partiallyvalidated by applying the previously trained ranking SVMs to unseen tag vectors, as the \"true\"principal directions are unknown. We use the 81 unseen tags U as \"test data\" for the trained rankingSVMs, each resulting from an image-induced visual association. NUS-WIDE provides annotationsfor these 81 tags. The results, shown in (right), significantly outperform the basic baseline ofrandom tag ranking, indicating that the directions produced by SVMs are generalizable to the newvocabulary U of words. Observation. We conclude that word vectors are an effective medium for transferring knowl-edgespecifically, rank-ability along the principal directionfrom seen to unseen tags. We haveempirically confirmed that the visual association rule (Ym, Ym) in words due to an image m can berepresented by the linear rank-ability of corresponding word vectors along a principal direction. Ourexperiments involve a total of |S| + |U| = 1,006 words. Future work should include larger-scale andtheoretical studies.",
  "Image Tagging by Ranking": "Based on the findings from , which indicate the existence of a principal direction, wm, in theword vector space for each visual association rule (Ym, Ym) generated by an image m, we propose adirect solution for image tagging. The core idea is to approximate this principal direction by learninga mapping function, f(00b7), that connects the visual space to the word vector space, such that:",
  "where wmistheprincipaldirectionforalloffsetvectorsoftheseentags, correspondingtothevisualassociationrule(Ym, YformsolutionforA": "However,achallengearisesaswedonotknowtheexactprincipaldirectionswm.ThetrainingdataonlyprovideimagesxmandrelevanttagsYm.Weoptforastraightforwardalternative, usingthedirAx.ThefirststagetrainsarankingSV Moverthewordvectorsofseentagsforeachvisualassociation(Ym, Ym).Thesecon Discussion. The use of linear transformation between visual and word vector spaces has beenpreviously explored, for instance, in zero-shot classification and image annotation/classification. Thiswork distinguishes itself by the clear interpretation of the mapped image f(x) = Ax as the principaldirection for tag assignment, which has been empirically validated. We further extend this to anonlinear transformation using a neural network.",
  "Approximation by Neural Networks": "We also explore a nonlinear mapping f(x; ) using a multi-layer neural network, where represents thenetwork parameters. The network architecture, illustrated in , includes two RELU layersfollowed by a linear layer that outputs the approximated principal direction, w, for an input imagex. We anticipate that the nonlinear mapping function f(x; ) will provide greater modeling flexibilitycompared to the linear approach. Training the neural network by regressing to the M directions obtained from ranking SVMs is notideal, as confirmed by both intuition and experiments. The number of training instances, M, is smallrelative to the networks parameter count, increasing the risk of overfitting. Moreover, the directionsfrom ranking SVMs are not the true principal directions, making it unnecessary to rely on them. Instead, we integrate the two stages from .2. We aim for the neural networks output f(xm; )to represent the principal direction, where all relevant tag vectors p Ym rank higher than irrelevantones n Ym for an image m. Lets define:",
  "where wm = 1/(|Ym||Ym|)normalizestheperimageRankNetlossbythenumberofrankingconstraintsimposedbyimabatchgradientdescent": "Practical Considerations.We use Theano for optimization,with a mini-batch sizeof1,000images.Eachimage,onaverage,imposes4,600pairwiserankingcon-straints, which are all used in the optimization.The normalization wmfortheper imagerankinglosshelpsbalancetheinfluenceofimageswithmanypositivetags, addressingtheissueofunbalancednum Besides the RankNet loss, we tested other per-image loss options, including hinge loss, Crammer-Singer loss, and pairwise max-out ranking. Hinge loss performed the worst, likely because itsnot designed for ranking. Crammer-Singer, pairwise max-out, and RankNet yielded comparableresults, with RankNet slightly outperforming the others by about 2% in MiAP, possibly due to easieroptimization control. Listwise ranking loss could also be considered.",
  "Dataset and Configuration": "NUS-WIDE Dataset. We primarily utilize the NUS-WIDE dataset for our experiments. This datasetis a standard benchmark for image tagging, originally containing 269,648 images. We were ableto retrieve 223,821 images, as some were either corrupted or removed from Flickr. Followingthe recommended protocol, we divide the dataset into a training set of 134,281 images and a testset of 89,603 images. We further allocate 20% of the training set as a validation set for tuninghyperparameters in both our method and the baselines, and for conducting the empirical analyses in. Annotations of NUS-WIDE. NUS-WIDE provides three sets of tags for its images. The first setincludes 81 \"ground truth\" tags, carefully selected to represent Flickr tags, encompassing both generalterms (e.g., \"animal\") and specific ones (e.g., \"dog,\" \"flower\"), and corresponding to frequent Flickrtags. These tags are annotated by students and are less noisy than those directly collected from theWeb, serving as the ground truth for evaluating image tagging methods. The second and third setscontain 1,000 popular and nearly 5,000 raw Flickr tags, respectively. Image Features and Word Vectors. We extract and normalize image feature representations usingVGG-19. Both GloVe and Word2vec word vectors are used in our empirical analysis in ,with 300D GloVe vectors used for the remaining experiments. Word vectors are also normalized. Evaluation. We assess tagging results using two types of metrics: mean image average precision(MiAP), which considers the entire ranking list, and precision, recall, and F1-score for the top K tagsin the list (K = 3 and K = 5). Both metrics are commonly used in image tagging research. For detailson calculating MiAP and top-K precision and recall, we refer readers to .3 of Li et al. (2015)and .2 of Gong et al. (2013), respectively.",
  "In this section, we present experimental results for traditional image tagging, using the 81 \"groundtruth\" annotated concepts in NUS-WIDE to benchmark various methods": "Baselines. We include TagProp as a primary competitive baseline, representing nearest-neighbor-based methods that generally outperform parametric methods built from generative models and haveshown state-of-the-art results in experimental studies. We also compare against two recent parametricmethods, WARP and FastTag, both based on deep architectures but using different models. For afair comparison, we use the same VGG-19 features across all methods, with code for TagProp andFastTag provided by the authors and WARP implemented based on our neural network architecture.Additionally, we compare to WSABIE and CCA, which correlate images and relevant tags in alow-dimensional space. Hyperparameters for all methods are selected using the validation set. Results. presents the comparison results among TagProp, WARP, FastTag, WSABIE, CCA,and our Fast0Tag models, implemented with both linear mapping and a nonlinear neural network.TagProp significantly outperforms WARP and FastTag, but its training and testing complexities arehigh, at O(M 2) and O(M) respectively, relative to the training set size M. In contrast, WARP andFastTag are more efficient, with O(M) training complexity and constant testing complexity due totheir parametric nature. Our Fast0Tag with linear mapping yields results comparable to TagProp,while Fast0Tag with the neural network surpasses the other methods. Both implementations maintainlow computational complexities similar to WARP and FastTag.",
  "which finds both relevant seen tags from S and relevant unseen tags from U for the test images. Theset of unseen tags U could be open and dynamically growing": "In our experiments, we treat the 81 concepts with high-quality user annotations in NUS-WIDE asthe unseen set U for evaluation and comparison. We use the remaining 925 out of the 1000 frequentFlickr tags to form the seen set S - 75 tags are shared by the original 81 and 1,000 tags.",
  "Baselines. Our Fast0Tag models can be readily applied to the zero-shot and seen/unseen imagetagging scenarios. For comparison, we study the following baselines": "Seen2Unseen. We first propose a simple method that extends an arbitrary traditional image taggingmethod to also work with previously unseen tags. It originates from our analysis experiment in. First, we use any existing method to rank the seen tags for a test image. Second, we train aranking SVM in the word vector space using the ranking list of the seen tags. Third, we rank unseen(and seen) tags using the learned SVM for zero-shot (and seen/unseen) tagging. LabelEM. The label embedding method achieves impressive results on zero-shot classification forfine-grained object recognition. If we consider each tag of S U as a unique class, though this impliesthat some classes will have duplicated images, the LabelEM can be directly applied to the two newtagging scenarios. LabelEM+. We also modify the objective loss function of LabelEM when we trainthe model, by carefully removing the terms that involve duplicated images. This slightly improvesthe performance of LabelEM. ConSE. Again by considering each tag as a class, we include a recentzero-shot classification method, ConSE in the following experiments. Note that it is computationallyinfeasible to compare with Fu et al., which might be the first work to our knowledge on expandingimage tagging to handle unseen tags, because it considers all the possible combinations of the unseentags. Results. summarizes the results of the baselines and Fast0Tag when they are applied tothe zero-shot and seen/unseen image tagging tasks. Overall, Fast0Tag, with either linear or neuralnetwork mapping, performs the best. Additionally, in the table, we add two special rows whose results are mainly for reference. TheRandom row corresponds to the case when we return a random list of tags in U for zero-shot tagging(and in U S for seen/unseen tagging) to each test image. We compare this row with the row ofSeen2Unseen, in which we extend TagProp to handle the unseen tags. We can see that the results ofSeen2Unseen are significantly better than randomly ranking the tags. This tells us that the simpleSeen2Unseen is effective in expanding the labeling space of traditional image tagging methods. Sometag completion methods may also be employed for the same purpose as Seen2Unseen. Anotherspecial row in is the last one with RankSVM for zero-shot image tagging. We obtain itsresults through the following steps. Given a test image, we assume the annotation of the seen tags,S, are known and then learn a ranking SVM with the default regularization = 1. The learned SVMis then used to rank the unseen tags for this image. One may wonder that the results of this rowshould thus be the upper bound of our Fast0Tag implemented based on linear regression because theranking SVM models are the targets of the linear regression. However, the results show that they arenot. This is not surprising, but rather it reinforces our previous statement that the learned rankingSVMs are not the \"true\" principal directions. The Fast0Tag implemented by the neural network is aneffective alternative for seeking the principal directions. It would also be interesting to compare theresults in (zero-shot image tagging) with those in (conventional tagging), because theexperiments for the two tables share the same testing images and the same candidate tags; they onlydiffer in which tags are used for training. We can see that the Fast0Tag (net.) results of the zero-shottagging in are actually comparable to the conventional tagging results in , particularlyabout the same as FastTags. These results are encouraging, indicating that it is unnecessary to useall the candidate tags for training in order to have high-quality tagging performance. Annotatingimages with 4,093 unseen tags. What happens when we have a large number of unseen tags showingup at the test stage? NUS-WIDE provides noisy annotations for the images with over 5,000 Flickrtags. Excluding the 925 seen tags that are used to train models, there are 4,093 remaining unseentags. We use the Fast0Tag models to rank all the unseen tags for the test images, and the resultsare shown in . Noting that the noisy annotations weaken the credibility of the evaluationprocess, the results are reasonably low but significantly higher than the random lists. Qualitativeresults. shows the top five tags for some exemplar images, returned by Fast0Tag underthe conventional, zero-shot, and seen/unseen image tagging scenarios. Those by TagProp under theconventional tagging are shown on the rightmost. The tags in green color appear in the ground truth",
  "Experiments on IAPRTC-12": "We present another set of experiments conducted on the widely used IAPRTC-12 dataset. We usethe same tag annotation and image training-test split as described in prior work for our experiments.There are 291 unique tags and 19,627 images in IAPRTC-12. The dataset is split into 17,341 trainingimages and 2,286 testing images. We further separate 15",
  "Configuration": "Similar to the experiments in the previous section, we evaluate our methods in three distinct tasks:conventional tagging, zero-shot tagging, and seen/unseen tagging. Unlike NUS-WIDE, where arelatively small set of 81 tags is considered the ground truth annotation, all 291 tags of IAPRTC-12are typically used in prior work to compare different methods. Therefore, we also use all of themfor conventional tagging. For the zero-shot and seen/unseen tagging tasks, we exclude 20The visualfeatures, evaluation metrics, word vectors, and baseline methods remain the same as described in themain text.",
  "Results": "Tables 4 and 5 display the results for all three image tagging scenarios (conventional, zero-shot, andseen/unseen tagging). The proposed Fast0Tag continues to outperform the other competitive baselineson this new IAPRTC-12 dataset. A notable observation, which is less apparent on NUS-WIDEprobably due to its noisier seen tags, is the significant performance gap between LabelEM+ andLabelEM. This indicates that traditional zero-shot classification methods may not be directly suitablefor either zero-shot or seen/unseen image tagging tasks. However, performance can be improvedby tweaking LabelEM and carefully removing terms in its formulation that involve comparisons ofidentical images.",
  "More Qualitative Results": "In this section, we provide additional qualitative results from different tagging methods on both theNUS-WIDE and IAPRTC-12 datasets. These are presented to supplement the findings discussed inthe main text. Due to the incompleteness and noise in tag ground truth, many accurate tag predictionsare often incorrectly assessed as mistakes because they dont match the ground truth. This issue isparticularly evident in the 4k zero-shot tagging results, where a wide variety of tag candidates areconsidered.",
  "Conclusion": "We have conducted a thorough examination of a specific visual pattern in words: the visual associationrule that divides words into two distinct groups based on their relevance to an image. We alsoinvestigated how this rule is captured by vector offsets within the word vector space. Our empiricalfindings demonstrate that for any given image, there exists a main direction in the word vectorspace along which vectors of relevant tags are ranked higher than those of irrelevant tags. Whileour experimental analyses involved 1,006 words, future research should encompass larger-scaleand theoretical investigations. Based on this discovery, we developed a Fast0Tag model to addressimage tagging by estimating the primary directions for input images. Our method is as efficient asFastTag and is capable of annotating images with a large number of previously unseen tags. Extensiveexperiments confirm the effectiveness of our Fast0Tag approach."
}