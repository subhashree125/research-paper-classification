{
  "Abstract": "Multi-agent systems (MAS) play a crucial role in the advancement of machineintelligence and its applications. To explore complex interactions within MASsettings, we introduce a novel \"GNN for MBRL\" model. This model employs astate-space Graph Neural Network alongside Model-based Reinforcement Learningto tackle MAS tasks, such as Billiard-Avoidance and Autonomous Driving. Theprocess involves using a GNN model to predict the future states and paths of severalagents. Subsequently, a Model Predictive Control, enhanced by the Cross-EntropyMethod (CEM), is used to guide the ego-agents action planning, facilitatingsuccessful completion of MAS tasks.",
  "Purpose": "Vision-based approaches have been extensively researched in various reinforcement learning (RL)areas, including mastering video games directly from raw pixels, managing simulated autonomousvehicles using complex image observations, and carrying out robotic tasks like grasping using staterepresentations derived from complicated visual data. However, it has been shown that RL fromcomplex observations such as raw pixels is time-consuming and needs a lot of samples. Additionally, itis widely acknowledged that learning policies from physical state-based characteristics is significantlymore effective and straightforward than learning from visual pixels. Therefore, this research focusedon learning control policies from states and exploring the use of a graph neural network (GNN)dynamics model to predict future states in multi-agent systems. We then utilized a Cross-EntropyMethod (CEM)-optimized model-based controller for motion planning of the ego-agent, whichenabled successful execution of specific MAS missions. These include multi-billiard avoidance andself-driving car scenarios.",
  "Background": "Inspired by a state-space model for videos that reasons about multiple objects and their positions,velocities, and interactions, our project seeks to develop a \"GNN for MBRL\" model. This model isbased on a multi-billiard simulator for sample-efficient model-based control in MAS tasks involvingmany interacting agents. Autonomous driving, a complicated multi-agent system, requires theego-agent to consider the situations of surrounding agents when conducting motion planning. Thegym-carla can be used for further study in this context. We begin by developing and testing our\"GNN for MBRL\" model on a MAS billiard avoidance scenario to investigate the possibilities ofGNNs and model-based RL. We aim to transfer the framework to real-world self-driving applications. Graph Neural Networks. GNNs have been proposed to create node and edge representations ingraph data, achieving remarkable success in applications such as recommendation systems, socialnetwork prediction, and natural language processing. Recognizing the capabilities of GNNs inphysical systems, we can utilize GNN-based reasoning to represent objects as nodes and relationsas edges, which allows for an effective approach to analyzing objects and relations. A successful",
  "t zp(zt|zt1)": "where x is the image observation and z represents object states. The latent positions and velocities ofmultiple agents act as the connection between the two components. The model uses simple uniformand Gaussian distributions to initialize the states. The STOVE model is trained on video sequencesby maximizing the evidence lower bound (ELBO). STOVE has also extended their video modelinto reinforcement learning (RL) tasks for planning. Empirical evidence demonstrates that an actorusing Monte-Carlo tree search (MCTS) on top of STOVE is comparable to model-free techniques,such as Proximal Policy Optimization (PPO), while needing a fraction of the samples. Inspired bythese RL experiments, we apply the GNN model directly to states rather than complex visual datato improve sample efficiency and predict agents future states. This is then combined with anothermodel-based RL approach, such as Model Predictive Control (MPC). In the experiment, we trainthe GNN dynamics model using ground truth states of video sequence data for multi-agent systemsinstead of visual data. Model-based Reinforcement Learning. Model-based RL is considered a solution to the highsample complexity of model-free RL. This method typically includes two primary steps: (1) creatinga dynamics model that predicts future states based on present states and actions, and (2) using aplanning method to learn a global policy and act in the environment effectively. The STOVE modeluses Monte-Carlo tree search (MCTS) to develop a policy based on the world model, which acts as aplanning simulator, and found that MCTS combined with STOVE could outperform the model-freePPO algorithm in a multi-billiards avoidance task.",
  "Framework": "The \"GNN for MBRL\" method consists of two primary stages: (1) a GNN dynamics model trainingphase, using offline recorded video sequences or low-dimensional states for video prediction, and(2) a motion planning phase using CEM-based Model Predictive Control (MPC). This involves afeedback control algorithm with a Cross-Entropy Method optimizer to interact with the billiardenvironment. The aim is to plan effective actions for the ego-agent in order to avoid collisions. There are two different cases in the GNN dynamics training stage. The \"Action-conditioned case\"follows the STOVE model-based control approach, training GNN with an object reconstruction modelon visual data. The \"Supervised RL case\" is designed for RL tasks directly on low-level states. Bothcases involve training GNN dynamics models for predicting future multi-agent states. Subsequently,the trained model is integrated into the model-based RL section to control the ego agent for motionplanning. After training, MPC uses a model to predict future outputs of a process. It handles multi-input multi-output (MIMO) systems with constraints and incorporates future reference information to improveperformance. Therefore, we established a continuous version of the multi-billiard environment fordata collection. It is possible to combine the previously trained GNN model with MPC to assess ifthis method can successfully address MAS tasks.",
  "Data Generation": "STOVE proposed an object-aware physics prediction model based on billiard simulations, includingavoidance, billiards, gravity, and multi-billiards modes. We wrapped them into a gym environmentstyle, \"gym-billiard,\" which can be easily used by Python API, aiding researchers in understandingthis system and creating efficient algorithms. Our project focuses on the avoidance billiard scenario, where the red ball is the ego-object and theRL agent controls it to avoid collisions. In STOVE, the ego-ball has nine actions: movement in eightdirections and staying at rest. A negative reward is given when the red ball hits another. We obtainedthe avoidance sequences datasets using the \"generate_billiards_w_actions\" function. 1000 sequences",
  "of length 100 for training and 300 sequences of length 100 for testing were generated using a randomaction selection policy. The pixel resolution was 32*32 with the ball mass set to 2.0": "We changed the environment to use continuous actions for agents, where the red ball is controlled by2-dimensional numpy values ranging in (-2, 2), representing the acceleration in x and y directions.Similar to the discrete setting, continuous datasets were produced with random actions from a uniformdistribution within (-2, 2). These datasets included the image observations, actions, states, dones,and rewards. The average rewards for the continuous mode are lower, indicating more frequentinteractions between the balls.",
  "GNN Dynamics Model Training": "We used supervised learning to train on ground-truth states rather than high-dimensional image data.The aim is to improve sample efficiency, then combine the trained model with CEM-optimized MPCfor predicting future states. Two cases were trained on both Discrete and Continuous datasets: (1) theAction-conditioned case, which makes predictions based on state and action and predicts reward, and(2) the Supervised RL case, where real states including positions and velocities were used as the inputfor GNN dynamics model. The model can learn to predict future states of multiple agents instead offirst extracting the states from visual data with a separate model. Training was performed for 500epochs, and the model parameters were saved. Training time for the Supervised condition was lessthan the Action-conditioned case. The GNN model could work on both action space 2 and 9 discreteactions without changing the GNN network architecture, resulting in a unified training framework.",
  "GNN with MBRL": "In the \"Model-based Control\" framework, we used MCTS on discrete datasets to generate qualitativevideos. We changed the mass of the ball agents to 1.0 and 2.0 and trained two GNN dynamics models.During MCTS, the GNN model predicted future sequences for 100 parallel environments with thelength of 100, using a maximal rollout depth of 10. We then calculated the mean collision rate andsaved 100 videos to show the ego-balls interaction with other agents, which demonstrates improvedcollision avoidance and lower collision rates. For the continuous datasets, we combined the trained GNN model into the CEM optimized MPCmethod and compared it with random and ground truth cases. The GNN model made accuratepredictions based on the current states by checking the code, changing the cuda device and data type.We computed the \"reward,\" the average collisions per epoch, for each method.",
  "GNN Training Results": "The datasets generated from the gym-billiard API environment, including image observations, actions,states, dones, and rewards, were stored in pickle files. GNN dynamics models were trained in twoconditions: (1) Action-conditioned case, which used video sequences with a visual reconstructionmodel, and (2) Supervised RL case, which used real states as input for the GNN dynamics model.Both conditions were trained for 500 epochs. The Supervised condition took less time to train thanthe Action-conditioned case. A notable finding is that the GNN model worked equally well for bothaction space 2 and 9 discrete actions without changing the original GNN architecture. This allows forunified training for both the Discrete and Continuous billiard avoidance environments. After training,model parameters were saved in the \"checkpoints\" folder. \"gifs\" folder stores the videos, and \"states\"contains state and reward files. In the Action-conditioned case, the reward MSE loss decreases for both continuous and discreteconditions. However, the continuous reward error decreased from 0.48 to 0, while the discrete onedropped from 0.16 to 0. The ELBO increased significantly from 450 to 3600. Position and velocityprediction errors decreased during training. The continuous position error was close to the discrete, but the velocity error showed a greater difference. The continuous V_error dropped from 0.65 to 0.05,while the discrete one decreased from 0.07 to 0.01. The four metrics met the criteria for a reasonableGNN dynamics model for the subsequent RL task. In the Supervised RL case, the model directlyinputs the ground truth states and actions for GNN training to predict future states. Reconstructionerrors were always zero since no image reconstruction was used on the true states. The discrete caseshowed a better performance compared to the continuous case with respect to the \"Prediction_error\".The continuous loss remained stable for Total_error, while the discrete loss showed a downtrendbefore stabilizing. Generated rollout videos indicated that the ego-red ball performed reasonablywell in avoiding collisions. Thus, the trained Supervised RL model can be used for the followingmodel-based RL phase.",
  "m=1100500.0558+0.00120.2790+0.0250.0707+0.066m=1501000.0565+0.00080.3543+0.04450.0408+0.0392m=2100500.0648+0.0010.2420+0.01780.0505+0.0480m=2501000.0455+0.00080.2690+0.03500.0612+0.0575": "The \"random\" case used randomly generated actions, while \"ground_truth\" used the true interactionenvironment for generating next states. The \"m=1\" version task differed slightly from \"m=2\" as the\"m=1\" model was trained on the old continuous datasets, making the red ball movement less flexible.The collision rates in \"GNN_MPC\" were lower than \"Random\" and close to \"ground_truth\". Theperformance of our proposed method was better than random cases, and the results of \"GNN_MPC\"were close to the \"Ground_truth\" case, which indicated that the trained GNN dynamics model predictsthe future states of multi-object systems as well as the ground truth interactive environment.",
  "Conclusions": "We introduced the \"GNN for MBRL\" concept, combining a graph neural network (GNN) dynamicsmodel with CEM-optimized Model Predictive Control (MPC) on a gym-billiard avoidance MAS task.We also conducted experiments on the \"Action-conditioned\" case with MCTS using discrete datasetsand explored the \"Supervised RL\" GNN dynamics model with CEM-optimized MPC on continuousdatasets. The proposed model predicted video sequences well and controlled the ego-agent to addressRL tasks, which may be applied to complex multi-agent systems like the gym-carla autonomousdriving environment."
}