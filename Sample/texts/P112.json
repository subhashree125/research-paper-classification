{
  "Abstract": "The rapid increase of genomic sequence data requires new methods for creating ro-bust sequence representations. Existing techniques often neglect detailed structuralinformation, focusing mainly on contextual information. We addressed this issueby developing k-mer embeddings that combine contextual and structural stringinformation, by enriching De Bruijn graphs with structural similarity connections.We also crafted a self-supervised method using Contrastive Learning, employing aheterogeneous Graph Convolutional Network encoder and constructing positivepairs based on node similarities. Our embeddings consistently outperform priormethods for Edit Distance Approximation and Closest String Retrieval tasks.",
  "Introduction": "Genomic sequence data is growing at an unprecedented rate, requiring the development of novelmethods that can provide both accurate and scalable sequence representations. These representationsare essential for various computational biology tasks, including gene prediction and multiple sequencealignment. Methods from Natural Language Processing (NLP), such as Word2Vec and Transformers,have been adopted to improve the representation of genomic sequences. These NLP-based approachesare effective at capturing the context within a sequence, which is important because the semantics ofwords often outweigh their precise letters. Character-level n-gram models might be used to capture structural nuances. However, a uniformrepresentation of each n-gram across all sequences can oversimplify the problem. Applying techniqueslike transformer-based models on n-grams can escalate computational demands. Consequently, thesemethods may overlook nuanced k-mer variations important for understanding single-nucleotidepolymorphisms and other minor sequence changes. These SNPs can influence disease susceptibility,phenotypic traits, and drug responses. Therefore, we developed a k-mer embedding approach that combines metagenomic context and stringstructure. In our method, contextual information refers to the relationships between k-mers closelysituated within sequences, and structural information examines nucleotide patterns within a k-merand their relations to other k-mers. We constructed a metagenomic graph that builds upon the DeBruijn Graph to capture k-mer transitions and structural similarities. Given the advances in Graph Neural Networks (GNNs), we grounded our method in GNNs butdesigned for heterogeneous graphs. This approach effectively recognizes and uses both contextualand structural connection types. Drawing from the success of self-supervised pre-training in NLPand Computer Vision, we designed a self-supervised objective for genomic graph data. We employedcontrastive loss aiming to align k-mers with similar context and structure in representation space. Finally, we tested our technique on two downstream tasks: Edit Distance Approximation and ClosestString Retrieval. The former estimates the minimum changes needed to transform one genomicsequence into another, avoiding quadratic computational complexity. The latter task, Closest StringRetrieval, involves finding sequences similar to a query.",
  "Genomic Sequence Representation": "Machine learning methods have emerged in computational biology to represent genomic sequences.A key component is the k-mer: a continuous nucleotide sequence of length k. The Word2Vec method,which represents words as vectors using their context, treats overlapping k-mers in genomic sequencesas words in sentences. Building on this, kmer2vec was introduced to apply Word2Vec to genomicdata for Multiple Sequence Alignment. Another strategy is to use the De Bruijn graph, where k-mersare nodes and their overlaps are edges, in conjunction with Node2Vec, which derives node featuresfrom the contextual information of biased random walks. This method underpins GRaDL for earlyanimal genome disease detection. K-mers also pair well with transformer-based models: DNABERTleverages a BERT-inspired objective and k-mer tokenization to predict genome-wide regulatoryelements. Metagenome2Vec blends Node2Vec with transformers to analyze metagenomes withlimited labeled data. Given the high computational demands of these transformer-based approaches,they are outside the scope of our benchmarks in this study.",
  "Graph Neural Networks": "Graph Convolutional Networks (GCNs) are foundational to several innovations in graph-basedmachine learning. In genomics, GNNs have been applied in metagenomic binning. Because we aimto enhance our node embeddings with structural similarity, both heterogeneity and heterophily arekey considerations. Recognizing the ubiquity of heterogeneity in real-world graphs, Relational GCNs(R-GCNs) were developed. These networks expand upon GCNs by generalizing the convolutionoperation to handle different edge types. To tackle heterophily, where distant nodes in a graph maybear similar features, Geom-GCN maps nodes to a latent space, while another approach suggests adistinct encoding approach for node embeddings and neighborhood aggregations.",
  "Self-Supervised Learning": "Self-supervised learning (SSL) enables effective use of unlabeled data and reduces dependence onannotated labels. Among SSL methods, contrastive learning has made a significant impact. At itscore, contrastive learning seeks to bring similar data instances closer in the embedding space whilepushing dissimilar ones apart. When applied to graph data, several techniques have been proposedfor obtaining positive pairs, including uniform sampling, node dropping, and random walk sampling.",
  "Metagenomic Graph": "The De Bruijn Graph, which is created from metagenomic sequences, forms the basis of our method.In this graph, each k-mer, a substring of length k from the sequences, is represented by a differentnode. An edge from node vi to node vj in the graph indicates that the k-mer at node vi directlyprecedes the k-mer at node vj in one of the sequences of the metagenome.",
  "When used, edge weights represent the frequency of these transitions, capturing genomic structureswithin the graph": "Although Node2Vec captures the sequential context in De Bruijn graphs, it overlooks structural k-mersimilarities. To address this, we expand the graph to include connections based on these similarities.We formulate two edge types for our graph, where nodes vi, vj, ... represent k-mers. De Bruijn Graphs edges The first edge type is designed to capture contextual information. LetT(vi, vj) be the count of transitions between k-mers within a dataset of genomic sequences. Theweight of an edge connecting nodes vi and vj, w(dBG)ij, is defined by,",
  "W (KFsub_k) = {w(KFsub_k)ij|w(KFsub_k)ij t}": "To accommodate graphs for larger k values, we have developed a more scalable approximation of theabove approach. It utilizes approximate nearest neighbor search on the sub-k-mer frequency vectors,which replaces the computationally demanding pairwise cosine similarity calculations. The metagenomic graph is defined as G = (V, E, W). Nodes V correspond to individual k-mers.The edges E can be categorized into two sets: De Bruijn Graphss edges E(dBG) and Sub-k-merFrequency edges E(KF ). Edges in E(KF ) may be further subdivided based on various sub_k values.Edge weights W can contain W (dBG) and several W (KFsub_k).",
  "Self-Supervised Task": "We use a contrastive learning method for k-mer representations. Graph nodes are initialized usinga sub-k-mer frequency vector. Positive and negative pairs are sampled and, along with the k-merrepresentations from the encoder, are used to compute the loss. Biased Random Walk Sampling We employ Biased Random Walk Sampling to capture k-mercontextual information. This approach uses w(dBG) edges to conduct walks, implemented exactlyas in Node2Vec. Given a walk of a set length, we extract positive pairs by applying a window ofsize m. Using a shrink factor , drawn uniformly from 1, ..., m, we determine the range i withinwhich nodes are considered positive pairs to node vi. Repeating this across multiple random walks,we gather a comprehensive set of positive pairs. Structural Similarity Sampling To capture the structural notion of k-mers, we sample pairs withprobability proportional to sub-k-mer frequency similarity, w(KFsub_k). The goal is for k-mers linkedby higher similarity to have similar representations. The probability of sampling is given by, P(vi, vj) w(KFsub_k)ijNegative Sampling We randomly select negative pairs from all node pairs in the graph, leveragingthe assumption that most pairs lack a high similarity edge. This approach ensures diversity in learnedrepresentations.",
  "Edit Distance Approximation": "presents the results obtained by using our pre-trained embeddings to estimate edit distancesbetween sequences on the RT988 and Qiita datasets. For the RT988 dataset, our Contrastive Learning(CL) and Node2Vec techniques surpassed Word2Vec and One-Hot. The increased losses in Qiitahighlight its greater complexity. In this context, our methods integration of k-mer structural similaritybecomes even more beneficial, outperforming all other tested methods. This benefit becomes moreevident as k increases, underscoring our embeddings capability to adapt to new nodes.",
  "Closest String Retrieval": "Tables 2a and 2b present the performance of our zero-shot sequence embeddings, directly derivedfrom the aggregation of our k-mer embeddings, in retrieving the nearest sequences in the Qiita dataset.The tables also showcase a comparison with the embeddings that were specifically fine-tuned for theEdit Distance Task. For direct k-mer aggregation, our Contrastive Learning (CL) embeddings are obtained throughconcatenation, while for k-mer aggregation with One-Hot, Word2Vec, and Node2Vec, we report theresults of the better performing method, either concatenation or averaging. The superior zero-shotnon-parametric retrieval performance of our CL method emphasizes the combined utility of bothcontext and structural similarity during self-supervised pre-training. Notably, while k-mers of sizearound three are optimal for Top 1 percent retrieval, larger k-mers excel in the Top 10 percent metrics.This suggests that smaller k-mers are better at discerning local sequence distances, while larger onescapture broader sequence distances. For embeddings fine-tuned using CNNs for Edit Distance Approximation, the complexity of CNNsobscures differences between the embeddings. Our method based solely on zero-shot concatenated k-mer embeddings outperforms this complex fine-tuning. This shows the advantage of our embeddingsover the method by previous work.",
  "Results and Analysis": "In all our experiments, the memory requirements of the One-Hot method increased exponentially,leading to its exclusion from our results for k > 7. When pre-training exclusively on the training set,our method, thanks to the GCN encoder, can generalize beyond k-mers present in the training set. Incontrast, Node2Vec and Word2Vec can only handle k-mer sizes up to the diversity of the trainingdataset. Therefore, for k > 6, where the test set introduces new k-mers, we excluded these methods.",
  "Conclusion": "In our study, we introduced a novel k-mer embedding technique that seamlessly integrates metage-nomic contextual and structural nuances, achieved through the enhancement of the De Bruijn graphand the use of contrastive learning. In the Edit Distance Approximation task, our technique con-sistently demonstrated superior performance compared to One-Hot, Word2Vec, and Node2Vec.Moreover, without requiring any downstream fine-tuning, our aggregated k-mer embeddings outper-formed the prior method in the Closest String Retrieval task. These findings suggest potential broaderuses in computational biology."
}