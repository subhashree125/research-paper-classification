{
  "Abstract": "The increasing popularity of video sharing has led to a growing need for automaticvideo analysis, including highlight detection. Emerging platforms that featurecrowdsourced, time-synchronized video comments offer a valuable resource foridentifying video highlights. However, this task presents several challenges: (1)time-synchronized comments often lag behind their corresponding shots; (2) thesecomments are frequently sparse and contain noise semantically; and (3) determiningwhich shots constitute highlights is inherently subjective. This paper introducesa novel framework designed to address these challenges. The proposed methoduses concept-mapped lexical chains to calibrate the lag in comments, modelsvideo highlights based on comment intensity and the combined concentrationof emotion and concept within each shot, and summarizes detected highlightsusing an enhanced SumBasic algorithm that incorporates emotion and conceptmapping. Experiments conducted on extensive real-world datasets demonstratethat our highlight detection and summarization methods substantially outperformexisting benchmark techniques.",
  "Introduction": "Billions of hours of video content are viewed daily on platforms like YouTube, with mobile devicesaccounting for half of these views. This surge in video sharing has intensified the demand for efficientvideo analysis. Consider a scenario where a user wishes to quickly grasp the essence of a lengthyvideo without manually navigating through it. Automatically generated highlights would enableusers to digest the videos key moments in a matter of minutes, aiding their decision on whether towatch the full video later. Furthermore, automated video highlight detection and summarization cansignificantly enhance video indexing, search, and recommendation systems. However, extracting highlights from a video is a complex task. Firstly, the perception of a \"highlight\"can vary significantly among individuals. Secondly, analyzing low-level features such as image, audio,and motion may not always capture the essence of a highlight. The absence of high-level semanticinformation poses a significant limitation to highlight detection in conventional video processing. The recent emergence of crowdsourced, time-synchronized video comments, also known as \"bullet-screen comments,\" presents a new avenue for highlight detection. These real-time comments, whichappear overlaid on the video screen, are synchronized with the video frames. This phenomenon hasgained widespread popularity on platforms like niconico in Japan, Bilibili and Acfun in China, andYouTube Live and Twitch Live in the USA. The prevalence of time-synchronized comments offers aunique opportunity for leveraging natural language processing in video highlight detection. Nevertheless, using time-synchronized comments for highlight detection and labeling still posessignificant challenges. Primarily, there is an almost unavoidable delay between comments and theircorresponding shots. As illustrated in , discussions about a particular shot may continueinto subsequent shots. Highlight detection and labeling without accounting for this lag may yieldinaccurate outcomes. Secondly, time-synchronized comments are often semantically sparse, bothin terms of the number of comments per shot and the number of words per comment. This sparsity can hinder the performance of traditional bag-of-words statistical models. Thirdly, determininghighlights in an unsupervised manner, without prior knowledge, involves considerable uncertainty.The defining characteristics of highlights must be clearly defined, captured, and modeled to ensureaccurate detection. To our knowledge, limited research has focused on unsupervised highlight detection and labelingusing time-synchronized comments. The most relevant work in this area proposes detecting highlightsbased on the topic concentration derived from semantic vectors of bullet-comments, and labeling eachhighlight using a pre-trained classifier based on predefined tags. However, we contend that emotionconcentration holds greater significance than general topic concentration in highlight detection.Another study suggests extracting highlights based on the frame-by-frame similarity of emotiondistributions. However, neither of these approaches addresses the combined challenges of lagcalibration, balancing emotion-topic concentration, and unsupervised highlight labeling. To overcome these challenges, this study proposes the following solutions: (1) employ word-to-concept and word-to-emotion mapping based on global word embedding, enabling the construction oflexical chains for calibrating the lag in bullet-comments; (2) detect highlights based on the emotionaland conceptual concentration and intensity of the lag-calibrated bullet-comments; and (3) summarizehighlights using a modified Basic Sum algorithm that considers emotions and concepts as fundamentalunits within a bullet-comment. The main contributions of this research are as follows: (1) We introduce a completely unsupervisedframework for detecting and summarizing video highlights using time-synchronized comments;(2) We introduce a lag-calibration method that uses concept-mapped lexical chains; (3) We havecreated extensive datasets for bullet-comment word embedding, an emotion lexicon tailored forbullet-comments, and ground-truth data for evaluating highlight detection and labeling based onbullet-comments.",
  "Highlight detection by video processing": "Following the definition from previous research, we define highlights as the most memorable shots ina video characterized by high emotional intensity. Its important to note that highlight detection differsfrom video summarization. While video summarization aims to provide a condensed representationof a videos storyline, highlight detection focuses on extracting its emotionally impactful content. In the realm of highlight detection, some researchers have proposed representing video emotions as acurve on the arousal-valence plane, utilizing low-level features such as motion, vocal effects, shotlength, and audio pitch, or color, along with mid-level features like laughter and subtitles. However,due to the semantic gap between low-level features and high-level semantics, the accuracy of highlightdetection based solely on video processing is limited.",
  "Temporal text summarization": "Research on temporal text summarization shares similarities with the present study but also exhibitskey distinctions. Several works have approached temporal text summarization as a constrainedmulti-objective optimization problem, a graph optimization problem, a supervised learning-to-rankproblem, and as an online clustering problem. This study models highlight detection as a simpler two-objective optimization problem with specificconstraints. However, the features employed to assess the \"highlightness\" of a shot diverge fromthose used in the aforementioned studies. Given that highlight shots are observed to correlate withhigh emotional intensity and topic concentration, coverage and non-redundancy are not primaryoptimization goals, as they are in temporal text summarization. Instead, our focus is on modelingemotional and topic concentration within the context of this study.",
  "Several studies have explored the use of crowdsourced time-synchronized comments for taggingvideos on a shot-by-shot basis. These approaches involve manual labeling and supervised training,": "temporal and personalized topic modeling, or tagging the video as a whole. One work proposesgenerating a summarization for each shot through data reconstruction that jointly considers textualand topic levels. One work proposed a centroid-diffusion algorithm to identify highlights. Shots are represented bylatent topics found through Latent Dirichlet Allocation (LDA). Another method suggests using pre-trained semantic vectors of comments to cluster them into topics and subsequently identify highlightsbased on topic concentration. Additionally, they utilize predefined labels to train a classifier forhighlight labeling. The current study differs from these two studies in several ways. First, beforeperforming highlight detection, we apply a lag-calibration step to mitigate inaccuracies causedby comment delays. Second, we represent each scene using a combination of topic and emotionconcentration. Third, we perform both highlight detection and labeling in an unsupervised manner.",
  "Lexical chain": "Lexical chains represent sequences of words that exhibit a cohesive relationship spanning multiplesentences. Early work on lexical chains used syntactic relationships of words from Rogets Thesaurus,without considering word sense disambiguation. Subsequent research expanded lexical chains byincorporating WordNet relations and word sense disambiguation. Lexical chains are also builtutilizing word-embedded relations for disambiguating multi-word expressions. This study constructslexical chains for accurate lag calibration, leveraging global word embedding.",
  "Problem Formulation": "The problem addressed in this paper can be formulated as follows: The input consists of a set oftime-synchronized comments, denoted as C = {c1, c2, c3, . . . , cn}, along with their correspond-ing timestamps T = {t1, t2, t3, . . . , tn} for a given video v. We are also given a compressionratio highlight that determines the number of highlights to be generated, and a compression ratiosummary that specifies the number of comments to be included in each highlight summary. Ourobjective is twofold: (1) to generate a set of highlight shots S(v) = {s1, s2, s3, . . . , sm}, and (2)to produce highlight summaries (v) = {C1, C2, C3, . . . , Cm} that closely align with the groundtruth. Each highlight summary Ci comprises a subset of the comments associated with that shot:Ci = {c1, c2, c3, . . . , ck}. The number of highlight shots m and the number of comments in eachsummary k are determined by highlight and summary, respectively.",
  "Word-Embedding of Time-Sync Comments": "As previously highlighted, a key challenge in analyzing time-synchronized comments is their semanticsparsity, stemming from the limited number of comments and their brevity. Two semantically relatedwords might not appear related if they dont co-occur frequently within a single video. To address this,we construct a global word embedding based on a large collection of time-synchronized comments. This word-embedding dictionary can be represented as: D = {(w1 : v1), (w2 : v2), . . . , (wn : vn)},where wi is a word, vi is its corresponding word vector, and n is the vocabulary size of the corpus.",
  "Emotion Lexicon Construction": "After training the word embedding, we manually select emotional words belonging to the five basicemotion categories from the 500 most frequent words in the embedding. We then iteratively expandthese emotion seeds using Algorithm 1. After each expansion iteration, we manually review theexpanded lexicon, removing any inaccurate words to prevent concept drift. The filtered expandedseeds are then used for further expansion in the next round. The minimum overlap overlap is set to0.05, and the minimum similarity simmin is set to 0.6. These values are determined through a gridsearch within the range of . The number of words for each emotion, both initially and after thefinal expansion, is presented in .",
  "Concept Mapping": "To tackle semantic sparsity in time-synchronized comments and build lexical chains of semanticallyrelated words, we first map words with similar meanings to the same concept. Given a set ofcomments C for a video v, we define a mapping F from the vocabulary VC of comments C to a set ofconcepts KC:",
  "Specifically, the mapping F assigns each word wi to a concept k = F(wi) as follows:": "F(wi) = F(w1) = F(w2) = . . . = F(wtop_n) = k,k KCs.t.{w|w top_n(wi) F(w) = k}/|top_n(wi)| overlaptop_n(wi) returns the n nearest neighbors of word wi based on cosine similarity. For each word wiin the comments C, we examine the percentage of its neighbors that have already been mapped toa concept k. If this percentage exceeds the threshold overlap, then word wi and its neighbors aremapped to concept k. Otherwise, they are assigned to a new concept, represented by wi itself.",
  "Lexical Chain Construction": "The next step involves constructing all lexical chains present in the time-synchronized comments forvideo v. This enables the calibration of lagged comments based on these chains. A lexical chain likconsists of a set of triples lik = {(w, t, c)}, where w is the actual word mentioned for concept k incomment c, and t is the timestamp of comment c. We create a lexical chain dictionary LC for thetime-synchronized comments C of video v:",
  "where ki KC represents a concept, and lik is the i-th lexical chain associated with concept k. Theprocedure for constructing these lexical chains is detailed in Algorithm 1": "Specifically, each comment in C can either be appended to an existing lexical chain or added to anew, empty chain. This decision is based on the comments temporal distance from existing chains,controlled by the maximum silence parameter tsilence. Its important to note that word senses within the constructed lexical chains are not disambiguated,unlike in most traditional algorithms. However, we argue that these lexical chains remain usefulbecause our concept mapping is built from time-synchronized comments in their natural order.This progressive semantic continuity naturally reinforces similar word senses for temporally closecomments. This continuity, combined with global word embedding, ensures the validity of ourconcept mapping in most scenarios.",
  "Comment Lag-Calibration": "With the lexical chain dictionary LC constructed, we can now calibrate the comments in C based ontheir respective lexical chains. Our observations indicate that the initial comment pertaining to ashot typically occurs within that shot, while subsequent comments may not. Therefore, we adjustthe timestamp of each comment to match the timestamp of the first element within its correspondinglexical chain. If a comment belongs to multiple lexical chains (concepts), we select the chain with thehighest score scorechain. The scorechain is calculated as the sum of the frequencies of each word in the chain, weighted by the logarithm of their global frequencies, denoted as log(D(w).count).Consequently, each comment will be assigned to its most semantically significant lexical chain(concept) for calibration. The calibration algorithm is presented in Algorithm 2. Its worth noting that if multiple consecutive shots, {s1, s2, . . . , sn}, contain comments with similarcontent, our lag-calibration method might shift many comments from shots s2, s3, . . . , sn to thetimestamp of the first shot, s1, if these comments are connected through lexical chains originatingfrom s1. This is not necessarily a drawback, as it helps us avoid selecting redundant consecutivehighlight shots and allows for the inclusion of other potential highlights, given a fixed compressionratio.",
  "Shot Importance Scoring": "In this section, we first segment comments into shots of equal temporal length, denoted as tshot. Wethen model the importance of each shot, enabling highlight detection based on these importancescores. A shots importance is modeled as a function of two factors: comment concentration and commentingintensity. Regarding comment concentration, as mentioned earlier, both concept and emotionalconcentration contribute to highlight detection. For instance, a cluster of concept-concentratedcomments like \"the background music/bgm/soundtrack of this shot is classic/inspiring/the best\" couldindicate a highlight related to memorable background music. Similarly, comments such as \"this plotis so funny/hilarious/lmao/lol/2333\" might suggest a highlight characterized by a single concentratedemotion. Therefore, our model combines these two types of concentration. We define the emotionalconcentration Cemotion(Cs) of shot s based on time-synchronized comments Cs and the emotionlexicon E as follows:",
  "wCs1": "log(D(w))where we calculate the inverse of the entropy of all concepts within a shot to represent topicconcentration. The probability of each concept k is determined by the sum of the frequencies ofits mentioned words, weighted by their global frequencies, and then divided by the sum of theseweighted frequencies for all words in the shot.",
  "Video Highlight Summarization": "Given a set of detected highlight shots S(v) = {s1, s2, s3, . . . , sm} for video v, each associated withits lag-calibrated comments Cs, our goal is to generate summaries (v) = {C1, C2, C3, . . . , Cm}such that Ci Csi, with a compression ratio of summary, and Ci closely resembles the groundtruth.",
  "We propose a simple yet highly effective summarization model, building upon SumBasic withenhancements that incorporate emotion and concept mapping, along with a two-level updatingmechanism": "In our modified SumBasic, instead of solely down-weighting the probabilities of words in a selectedsentence to mitigate redundancy, we down-weight the probabilities of both words and their mappedconcepts to re-weight each comment. This two-level updating approach achieves two key objectives:(1) it penalizes the selection of sentences containing semantically similar words, and (2) it allows forthe selection of a sentence with a word already present in the summary if that word occurs significantlymore frequently. Additionally, we introduce an emotion bias parameter, bemotion, to weight wordsand concepts during probability calculations. This increases the frequencies of emotional words andconcepts by a factor of bemotion compared to non-emotional ones.",
  "Crowdsourced Time-sync Comment Corpus": "To train the word embedding described earlier, we collected a large corpus of time-synchronizedcomments from Bilibili, a content-sharing website in China that features such comments. The corpuscomprises 2,108,746 comments, 15,179,132 tokens, and 91,745 unique tokens, extracted from 6,368long videos. On average, each comment contains 7.20 tokens. Before training, each comment undergoes tokenization using the Chinese word tokenization packageJieba. Repeated characters within words, such as \"233333,\" \"66666,\" and \"54c854c854c854c8,\" arereplaced with two instances of the same character. The word embedding is trained using word2vec with the skip-gram model. We set the number ofembedding dimensions to 300, the window size to 7, and the down-sampling rate to 1e-3. Words witha frequency lower than 3 are discarded.",
  "Seeds1713192114All157235258284226": "The dataset consists of 11 videos totaling 1333 minutes in length, with 75,653 time-synchronizedcomments. For each video, 3-4 video mix-clips are collected from Bilibili. Shots that appear in atleast two of these mix-clips are considered ground-truth highlights. These highlights are mapped tothe original video timeline, and their start and end times are recorded as ground truth. Mix-clips areselected based on the following criteria: (1) they are found on Bilibili using the search query \"videotitle + mixed clips\"; (2) they are sorted by play count in descending order; (3) they primarily focus onvideo highlights rather than a plot-by-plot summary or gist; (4) they are under 10 minutes in length;and (5) they contain a mix of several highlight shots instead of just one.",
  "Highlights Summarization Data": "We also created a highlight summarization (labeling) dataset for the 11 videos. For each highlightshot and its associated comments, we asked annotators to create a summary by selecting as manycomments as they deemed necessary. The guiding principles were: (1) comments with identicalmeanings should not be selected more than once; (2) the most representative comment among similarcomments should be chosen; and (3) comments that stand out and are irrelevant to the currentdiscussion should be discarded.",
  "Video Highlight Detection Evaluation": "To evaluate video highlight detection, we need to define a \"hit\" between a candidate highlight and areference highlight. A strict definition would require a perfect match between the start and end timesof the candidate and reference highlights. However, this criterion is overly stringent for any model.A more lenient definition would consider an overlap between a candidate and a reference highlight.However, this can still underestimate model performance, as users choices of highlight start and endtimes can sometimes be arbitrary. Instead, we define a \"hit\" with a relaxation parameter between acandidate h and the reference set R as follows:",
  "BP = { 1 if|C| > |R|e(1|R|/|C|)if|C| |R|": "where C is the candidate summary and R is the reference summary. Second, while the referencesummary contains no redundancy, the candidate summary might incorrectly select multiple similarcomments that match the same keywords in the reference. In such cases, precision would besignificantly overestimated. BLEU addresses this by counting matches one-by-one; the number ofmatches for a word will be the minimum of its frequencies in the candidate and reference summaries.",
  "For highlight detection, we compare different combinations of our model against three benchmarkmethods:": "* **Random-Selection:** Highlight shots are randomly selected from all shots in a video. ***Uniform-Selection:** Highlight shots are selected at equal intervals. * **Spike-Selection:** High-light shots are chosen based on the highest number of comments within the shot. * **Spike+E+T:**This is our method, incorporating emotion and topic concentration but without lag calibration. ***Spike+L:** This is our method, including only the lag-calibration step and not considering contentconcentration. * **Spike+L+E+T:** This represents our full model.",
  "For highlight summarization, we compare our method against five benchmark methods:": "* **SumBasic:** Summarization that relies solely on frequency for summary construction. * **LatentSemantic Analysis (LSA):** Text summarization based on singular value decomposition (SVD)for latent topic discovery. * **LexRank:** Graph-based summarization that calculates sentenceimportance using the concept of eigenvector centrality in a sentence graph. * **KL-Divergence:**Summarization based on minimizing KL-divergence between the summary and the source corpus,employing a greedy search approach. * **Luhn method:** A heuristic summarization method thatconsiders both word frequency and sentence position within an article.",
  "Results of Highlight Detection": "In our highlight detection model, the maximum silence threshold for lexical chains, tsilence, is setto 11 seconds. The threshold for concept mapping, overlap, is set to 0.5. The number of neighborsconsidered for concept mapping, top_n, is set to 15. The parameter , which controls the balancebetween emotion and concept concentration, is set to 0.9. A detailed parameter analysis is providedin . presents the precision, recall, and F1-scores for different combinations of our method and thebenchmark methods. Our full model (Spike+L+E+T) outperforms all other benchmarks across allmetrics. Random and uniform selection exhibit low precision and recall, as they dont incorporatestructural or content information. Spike-selection shows significant improvement by leveraging comment intensity. However, not all comment-intensive shots are highlights. For example, commentsat the beginning and end of a video are often high-volume greetings or goodbyes, which may not beindicative of highlights. Additionally, spike-selection tends to cluster highlights within consecutiveshots with high comment volumes. In contrast, our method can identify less intensive but emotionallyor conceptually concentrated shots that might be missed by spike-selection. This is evident in theperformance of Spike+E+T. We also observe that lag calibration alone (Spike+L) considerably enhances the performance ofSpike-selection, partially supporting our hypothesis that lag calibration is crucial for tasks involvingtime-synchronized comments.",
  "In our highlight summarization model, the emotion bias bemotion is set to 0.3": "compares the 1-gram BLEU, ROUGE, and F1-scores of our method and the benchmarkmethods. Our method outperforms all others, particularly in terms of ROUGE-1. LSA exhibits thelowest BLEU score, primarily because it statistically favors longer, multi-word sentences, which arenot representative in time-synchronized comments. The SumBasic method also performs relativelypoorly, as it treats semantically related words separately, unlike our method, which uses conceptsinstead of individual words.",
  "Conclusion": "This work presents a novel unsupervised framework for video highlight detection and summarization,based on crowdsourced time-synchronized comments. We introduce a lag-calibration techniquethat re-aligns delayed comments to their corresponding video scenes by using concept-mappedlexical chains. Video highlights are identified based on comment intensity and the concentrationof concepts and emotions within each shot. For summarization, a two-level SumBasic is proposedwhich updates word and concept probabilities iteratively when selecting sentences. Future workincludes integrating additional data sources such as video meta-data, audience profiles, and low-levelmulti-modal features."
}