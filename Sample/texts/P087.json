{
  "Abstract": "Feature tracking in video is a crucial task in computer vision. Usually, the trackingproblem is handled one feature at a time, using a single-feature tracker like theKanade-Lucas-Tomasi algorithm, or one of its derivatives. While this approachworks quite well when dealing with high- quality video and strong features, itoften falters when faced with dark and noisy video containing low-quality features.We present a framework for jointly tracking a set of features, which enables sharinginformation between the different features in the scene. We show that our methodcan be employed to track features for both rigid and non- rigid motions (possiblyof few moving bodies) even when some features are occluded. Furthermore, it canbe used to significantly improve tracking results in poorly-lit scenes (where thereis a mix of good and bad features). Our approach does not require direct modelingof the structure or the motion of the scene, and runs in real time on a single CPUcore.",
  "Introduction": "Feature tracking in video is an important computer vision task, often used as the first step in findingstructure from motion or simultaneous location and mapping (SLAM). The celebrated Kanade-Lucas-Tomasi algorithm tracks feature points by searching for matches between templates representingeach feature and a frame of video. Despite many other alternatives and improvement, it is still oneof the best video feature tracking algorithms. However, there are several realistic scenarios whenLucas-Kanade and many of its alternatives do not perform well: poor lighting conditions, noisy video,and when there are transient occlusions that need to be ignored. In order to deal with such scenariosmore robustly it would be useful to allow the feature points to communicate with each other to decidehow they should move as a group, so as to respect the underlying three dimensional geometry of thescene. This underlying geometry constrains the trajectories of the track points to have a low-rank structurefor the case when tracking a single rigid object under an affine camera model, and for non-rigidmotion and the perspective camera. In this work we will combine the low-rank geometry of thecohort of tracked features with the successful non-linear single feature tracking framework of Lucasand Kanade by adding a low-rank regularization penalty in the tracking optimization problem. Toaccommodate dynamic scenes with non-trivial motion we apply our rank constraint over a slidingwindow, so that we only consider a small number of frames at a given time (this is a common ideafor dealing with non-rigid motions). We demonstrate very strong performance in rigid environmentsas well as in scenes with multiple and/or non- rigid motion (since the trajectories of all features arestill low rank for short time intervals). We describe experiments with several choices of low-rankregularizers (which are local in time), using a unified optimization framework that allows real timeregularized tracking on a single CPU core.",
  "corresponding to very degenerate motion are lower-dimensional those corresponding to generalmotion": "Feature trajectories of non-rigid scenarios exhibit significant variety, but some low-rank modelsmay still be successfully applied to them. we consider a sliding temporal window, where overshort durations the motion is simple and the feature trajectories are of lower rank. The restrictionon the length of feature trajectories can also help in satisfying an approximate local affine cameramodel in scenes which violate the affine camera model. In general, depth disparities give rise tolow-dimensional manifolds which are only locally approximated by linear spaces. At last, even in the case of multiple moving rigid objects, the set of trajectories is still low rank(confined to the union of a few low rank subspaces). In all of these scenarios the low rank is unknownin general.",
  "Feature Tracking": "Notation: A feature at a location z1 R2 in a given N1 N2 frame of an N1 N2 N3 video ischaracterized by a template T, which is an n n sub-image of that frame centered at z1 (n is a smallinteger, generally taken to be odd, so the template has a center pixel). If z1 does not have integercoordinates, T is interpolated from the image. We denote = 1, ..., n 1, ..., n and we parametrize Tso that its pixel values are obtained by T(u)u. A classical formulation of the single-feature tracking problem is to search for the translation x1 thatminimizes some distance between a features template T at a given frame and the next frame of videotranslated by x1; we denote this next frame by I. That is, we minimize the single-feature energyfunction c(x1):",
  "Low Rank Regularization Framework": "If we want to encourage a low rank structure in the trajectories, we cannot view the tracking ofdifferent features as separate problems. For f 1, 2, ..., F, let xf denote the position of feature f inthe current frame (in image coordinates), and let x = (x1, x2, ..., xF ) R2F denote the joint state ofall features in the scene. We define the total energy function as follows:",
  "u(Tf(u) I(u + xf)) + P(x)": "where P(x) is an estimate of, or proxy for, the dimensionality of the set of feature trajectories over thelast several frames of video (past feature locations are treated as constants, so this is a function onlyof the current state, x). Notice that we have replaced the scale factor 1/(Fn2) from with the constant, as this coefficient is now also responsible for controlling the relative strength of the penalty term.We will give explicit examples for P in section 3.2. This framework gives rise to two different solutions, characterized by the strength of the penaltyterm (definition of ). Each has useful, real-world tracking applications. In the first case, we assume that most (but not necessarily all) features in the scene approximately obey a low rank model. Thisis appropriate if the scene contains non-rigid or multiple moving bodies. We can impose a weakconstraint by making the penalty term small relative to the other terms. If a feature is strong, it willconfidently track the imagery, ignoring the constraint (regardless of whether the motion is consistentwith the other features in the scene). If a feature is weak in the sense that we cannot fully determineits true location by only looking at the imagery, then the penalty term will become significant andencourage the feature to agree with the motion of the other features in the scene. In the second case, we assume that all features in the scene are supposed to agree with a low rankmodel (and deviations from that model are indicative of tracking errors). We can impose a strongconstraint by making the penalty term large relative to the other terms. No small set of features canoverpower the constraint, regardless of how strong the features are. This forces all features to move isa way that is consistent with a simple motion. Thus, a small number of features can even be occluded,and their positions will be predicted by the motion of the other features in the scene.",
  "Specific Choices of the Low-Rank Regularizer": "There is now a large body of work on low rank regularization. We will restrict ourselves to showingresults using three choices for P described below. Each choice we present defines P(x) in termsof a matrix M. It is the 2(L + 1) F matrix whose column f contains the feature trajectory forfeature f within a sliding window of L + 1 consecutive frames (current frame and L past frames).Specifically, M = [mi,j], where (m0,f , m1,f)T is the current (variable) position of feature f and(m2l+1,f , m2l+2,f)T , l = 1, ..., L contains the x and y pixel coordinates of feature f from l framesin the past (past feature locations are treated as known constants). One may alternatively center thecolumns of M by subtracting from each column the average of all columns. Most constraints derivedfor trajectories actually confine trajectories to a low rank affine subspace (as opposed to a linearsubspace). Centering the columns of M transforms an affine constraint into a linear one. Alternatively,one can forgo centering and view an affine constraint as a linear constraint in one dimension higher.We report results for both approaches.",
  "Explicit Factorizations": "A simple method for enforcing the structure constraint is to write M = BC, where B is a 2(L+1)dmatrix, and C is a d F matrix. However, as mentioned in the previous section, because the featuretracks often do not lie exactly on a subspace due to deviations from the camera model or non- rigidity,an explicit constraint of this form is not suitable.",
  "M = UV T": "denote the SVD of M, we can take P(x) to be ||BC M||, where B is the first three or four columns of U,and C is the first three or four rows of V T . Then this P corresponds to penalizing M via Fi=d+1 i,where i = ii is the ith singular value of M. As above, since the history is fixed, U, , and V T arefunctions of x.",
  "P(x) = ||M|| = ||||1": "This is a convex proxy for the rank of M. Here = (1 2 . . . 2(L+1)F )T is the vector of singularvalues of M, and || ||1 is the l1 norm. Unlike explicit factorization, where only energy outside the firstd principal components of M is punished, the nuclear norm will favor lower-rank M over higher-rankM even when both matrices have rank d. Thus, using this kind of penalty will favor simpler trackpoint motions over more complex ones, even when both are technically permissible.",
  "||||Notice that we use norm notation, although || || is only a pseudo-norm. When = 1, this is sometimescalled the effective rank of the data matrix": "Empirical dimension satisfies a few important properties. First, empirical dimension is invariantunder rotation and scaling of a data set. Additionally, in the absence of noise, empirical dimensionnever exceeds true dimension, but it approaches true dimension as the number of measurements goesto infinity for spherically symmetric distributions. Thus, d is a true dimension estimator (whereasthe nuclear norm is a proxy for dimension). To use empirical dimension as our regularizer, we defineP(x) = d(M). Empirical dimension is governed by its parameter, . An near 0 results in a strict estimator, whichis appropriate for estimating dimension in situations where you have little noise and you expect yourdata to live in true linear spaces. If is near 1 then d is a lenient estimator. This makes it lesssensitive to noise, and more tolerant of data sets that are only approximately linear. In all of theexperiments we present, we use = 0.6, although we found that other tested values also worked well.",
  "Implementation Details": "We fix L = 10 for the sliding window and let (x) = |x|. We use this form for so that all terms in the totalenergy function behave linearly in a known range of values. If our fit terms behaved quadratically,it would be more challenging to balance them against a penalty term. We also tested a Huber lossfunction for and have concluded that such a regularization is not needed. We fix a parameter m for each penalty form (selected empirically - see the supplementary materialfor our procedure), which determines the strength of the penalty. The weak and strong regularizationparameters are set as follows:",
  "mFn2": "The weak scaling implies that a perfectly-matched feature will contribute 0 to the total energy, and apoorly-matched feature will contribute an amount on the order of 1/m to the total energy. The penaltyterm will contribute on the order of 1 to the total energy. Since we do not divide the contributions ofeach feature by the number of features, the penalty terms contribution is comparable in magnitude tothat of a single feature. The strong scaling implies that the penalty term is on the same scale as thesum of the contributions of all of the features in the scene.",
  "Minimization Strategy": "The total energy function we propose for constrained tracking is non-convex since the contributionsfrom the template fit terms are not convex (even if P is convex); this is also the case with other featuretracking methods, including the Lucas-Kanade tracker. We employ a 1st-order descent approach fordriving the energy to a local minimum. To reduce the computational load of feature tracking, some trackers use 2nd-order methods foroptimization. This works well when tracking strong features, but in our experience it can beunreliable when dealing with weak or ambiguous features. Since we are explicitly trying to improvetracking accuracy on poor features we opt for a 1st-order descent approach instead. The simplest 1st-order descent method is (sub)gradient descent. Unfortunately, because there can bea very large difference in magnitude between the contributions of strong and weak features to ourtotal energy, our problem is not well-conditioned. If we pursue standard gradient descent, the strongfeatures dictate the step direction and the weak features have very little effect on it. Ideally, once thestrong features are correctly positioned, they will no longer dominate the step direction. If we wereable to perfectly measure the gradient of our objective function, this would be the case. In practice,the error in our numerical gradient estimate can be large enough to prevent the strong features from",
  "ever relinquishing control over the step direction. The result is that in a scene with both very strongand very weak features, the weak features may not be tracked": "To remedy this, we compute our step direction by blending the gradient of the energy function with avector that corresponds to taking equal-sized gradient descent steps separately for each feature. Weuse a fast line search in each iteration to find the nearest local minimum in the step direction. Thiscompromise approach allows for efficient descent while ensuring that each feature has some controlover the step direction (regardless of feature strength). Because the energy is not convex, it is important to choose a good initial state. We use a combinationof two strategies to initialize the tracking: first, we generate our initial guess of x by registering anentire frame of video with the previous (at lower resolution). Secondly, we use multi-resolution, orpyramidal tracking so that approximate motion on a large scale can help us get close to the minimumbefore we try tracking on finer resolution levels. We now explain the details of the algorithm. Let I denote a full new frame of video and let xprev bethe concatenation of feature positions in the previous frame. We form a pyramid for I where level 0is the full-resolution image and each higher level m (1 through 3) has half the vertical and half thehorizontal resolution of level m 1. To initialize the optimization, we take the full frame (at resolutionlevel 3) and register it against the previous frame (also at resolution level 3) using gradient descentand an absolute value loss function. We initialize each features position in the current frame by takingits position in the previous frame and adding the offset between the frames, as found through thisregistration process). Once we have our initial x, we begin optimization on the top pyramid level.When done on the top level, we use the result to initialize optimization on the level below it, andso on until we have found a local minimum on level 0. On any given pyramid level, we performoptimization by iteratively computing a step direction and conducting a fast line search to find a localminimum in the search direction. We impose a minimum and maximum on the number of steps to beperformed on each level (mini and maxi, respectively). Our termination condition (on a given level)is when the magnitude of the derivative of C is not significantly smaller than it was in the previousstep. To compute our search direction in each step, we first compute the gradient of C (which we willcall DC) and set a = This is done by breaking it into a collection of 2-vectors (elements 1 and 2 are together, elements3 and 4 are together, and so on) and normalizing each of them. We then recombine the normalized2-vectors to get b. We blend a with c to compute our step direction. Algorithm 1 summarizes the fullprocess.",
  "Efficiency and Complexity": "We have found that our algorithm typically converges in about 20 iterations or less at each pyramidlevel (with fewer iterations on lower pyramid levels). In our experiments, we used a resolutionof 640-by-480 (we have also done tests at 1000 562), and we found that 4 pyramid levels weresufficient for reliable tracking. Thus, on average, less than 80 iterations are required to track fromone frame to the next. A single iteration requires one gradient evaluation and multiple evaluationsof C. The complexity of a gradient evaluation is k1Fn2 + k2LF 2, and the complexity of an energyevaluation is k3Fn2 + k4L2F. Our C++ implementation (which makes use of OpenCV) can runon 35 features of size 7-by-7 with a temporal window of 6 frames (L = 5) on a 3rd-generationIntel i5 CPU at approximately 16 frames per second. SIMD instructions are used in places, but nomulti-threading was used, so faster processing rates are possible. With a larger window of L = 10 ouralgorithm still runs at 2-5 frames per second.",
  "Experiments": "To evaluate our method, we conducted tests on several real video sequences in circumstances that aredifficult for feature tracking. These included shaky footage in low-light environments. The resultingvideos contained dark regions with few good features and the unsteady camera motion and poorlighting introduced time-varying motion blur. In these video sequences it proved very difficult to hand-register features for ground-truth. In order topresent a quantitative numerical comparison we also collected higher-quality video sequences andsynthetically degraded their quality. We used a standard Lucas-Kanade tracker on the non-degraded videos to generate ground-truth (the output was human-verified and corrected). We therefore presentqualitative results on real, low-quality video sequences, as well as quantitative results on a set ofsynthetically degraded videos.",
  "Qualitative Experiments on Real Videos": "In our tests on real video sequences containing low- quality features, single-feature tracking doesnot provide acceptable results. When following a non-distinctive feature, the single-feature energyfunction often flattens out in one or more directions. A tracker may move in any ambiguous directionwithout realizing a better or worse match with the features template. This results in the trackedlocation drifting away from a features true location (i.e. wandering). This is not a technicallimitation of one particular tracking implementation. Rather, it is a fundamental problem due to thefact that the local imagery in a small neighborhood of a feature does not always contain enoughinformation to deduce the features motion between frames. This claim can be verified by attempting"
}