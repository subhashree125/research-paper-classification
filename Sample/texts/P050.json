{
  "Abstract": "Deep learning models have achieved remarkable success in natural language in-ference (NLI) tasks. While these models are widely explored, they are hard tointerpret and it is often unclear how and why they actually work. we take a steptoward explaining such deep learning based models through a case study on apopular neural model for NLI. we propose to interpret the intermediate layersof NLI models by visualizing the saliency of attention and LSTM gating signals.We present several examples for which our methods are able to reveal interestinginsights and identify the critical information contributing to the model decisions.",
  "Introduction": "Deep learning has achieved tremendous success for many NLP tasks. However, unlike traditionalmethods that provide optimized weights for human understandable features, the behavior of deeplearning models is much harder to interpret. Due to the high dimensionality of word embeddings, andthe complex, typically recurrent architectures used for textual data, it is often unclear how and why adeep learning model reaches its decisions. There are a few attempts toward explaining/interpreting deep learning-based models, mostly byvisualizing the representation of words and/or hidden states, and their importances (via saliency orerasure) on shallow tasks like sentiment analysis and POS tagging. we focus on interpreting thegating and attention signals of the intermediate layers of deep models in the challenging task ofNatural Language Inference. A key concept in explaining deep models is saliency, which determineswhat is critical for the final decision of a deep model. So far, saliency has only been used to illustratethe impact of word embeddings. we extend this concept to the intermediate layer of deep models toexamine the saliency of attention as well as the LSTM gating signals to understand the behavior ofthese components and their impact on the final decision. We make two main contributions. First, we introduce new strategies for interpreting the behavior ofdeep models in their intermediate layers, specifically, by examining the saliency of the attention andthe gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLItask and show that our methods reveal interesting insights not available from traditional methods ofinspecting attention and word saliency. our focus was on NLI, which is a fundamental NLP task that requires both understanding andreasoning. Furthermore, the state-of- the-art NLI models employ complex neural architecturesinvolving key mechanisms, such as attention and repeated reading, widely seen in successful modelsfor other NLP tasks. As such, we expect our methods to be potentially useful for other naturalunderstanding tasks as well.",
  "Attention": "Attention has been widely used in many NLP tasks and is probably one of the most critical partsthat affects the inference decisions. Several pieces of prior work in NLI have attempted to visualizethe attention layer to provide some understanding of their models. Such visualizations generate aheatmap representing the similarity between the hidden states of the premise and the hypothesis.Unfortunately the similarities are often the same regardless of the decision.",
  "Attention Saliency": "The concept of saliency was first introduced in vision for visualizing the spatial support on an imagefor a particular object class. In NLP, saliency has been used to study the importance of words towarda final decision. We propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair andthe models decision y, we consider the similarity between a pair of premise and hypothesis hiddenstates eij as a variable. The score of the decision S(y) is thus a function of eij for all i and j. Thesaliency of eij is then defined to be |S(y) / eij|. , the saliencies are clearly different across the examples, each highlighting different parts of thealignment. Specifically, for h1, we see the alignment between is playing and taking a nap and thealignment of in a garden to have the most prominent saliency toward the decision of Contradiction.For h2, the alignment of kid and her family seems to be the most salient for the decision ofNeutral. Finally, for h3, the alignment between is having fun and kid is playing have the strongestimpact toward the decision of Entailment. From this example, we can see that by inspecting the attention saliency, we effectively pinpoint whichpart of the alignments contribute most critically to the final prediction whereas simply visualizing theattention itself reveals little information.",
  "Although the two models make different predictions, their attention maps appear qualitatively similar": "We see that for both examples, ESIM-50 primarily focused on the alignment of ordered, whereasESIM-300 focused more on the alignment of John and Mary with man. interesting to note thatESIM-300 does not appear to learn significantly different similarity values compared to ESIM-50for the two critical pairs of words (John, man) and (Mary, man) based on the attention map.The saliency map, however, reveals that the two models use these values quite differently, with onlyESIM-300 correctly focusing on them. It is",
  "LSTM Gating Signals": "LSTM gating signals determine the flow of information. In other words, they indicate how LSTMreads the word sequences and how the information from different parts is captured and combined.LSTM gating signals are rarely analyzed, possibly due to their high dimensionality and complexity.we consider both the gating signals and their saliency, which is computed as the partial derivative ofthe score of the final decision with respect to each gating signal. Instead of considering individual dimensions of the gating signals, we aggregate them to considertheir norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers,the first (input) LSTM performs the input encoding and the second (inference) LSTM generates therepresentation for inference. , we first note that the saliency tends to be somewhat consistent across different gates within the sameLSTM, suggesting that we can interpret them jointly to identify parts of the sentence important forthe models prediction. Comparing across examples, we see that the saliency curves show pronounced differences across theexamples. For instance, the saliency pattern of the Neutral example is significantly different from theother two examples, and heavily concentrated toward the end of the sentence (with her family).Note that without this part of the sentence, the relationship would have been Entailment. The focus(evidenced by its strong saliency and strong gating signal) on this particular part, which presentsinformation not available from the premise, explains the models decision of Neutral. Comparing the behavior of the input LSTM and the inference LSTM, we observe interesting shiftsof focus. the inference LSTM tends to see much more concentrated saliency over key parts of thesentence, whereas the input LSTM sees more spread of saliency. For example, for the Contradictionexample, the input LSTM sees high saliency for both taking and in, whereas the inference LSTMprimarily focuses on nap, which is the key word suggesting a Contradiction. Note that ESIM usesattention between the input and inference LSTM layers to align/contrast the sentences, hence it makessense that the inference LSTM is more focused on the critical differences between the sentences.This is also observed for the Neutral example as well. It is worth noting that, while revealing similar general trends, the backward LSTM can sometimesfocus on different parts of the sentence, suggesting the forward and backward readings providecomplementary understanding of the sentence.",
  "Conclusion": "We propose new visualization and interpretation strategies for neural models to understand howand why they work. We demonstrate the effectiveness of the proposed strategies on a complex task(NLI). Our strategies are able to provide interesting insights not achievable by previous explanationtechniques. Our future work will extend our study to consider other NLP tasks and models with thegoal of producing useful insights for further improving these models.",
  "In this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding,2) attention, and 3) inference": "Let u = [u1, , un] and v = [v1, , vm] be the given premise with length n and hypothesis withlength m respectively, where ui, vj Rr are word embeddings of r-dimensional vector. The goal is topredict a label y that indicates the logical relationship between premise u and hypothesis v. Below webriefly explain the aforementioned parts.",
  "(3) eij = u^Ti v^j, i [1, n], j [1, m]": "where u^i and v^j are the hidden representations of u and v respectively which are computed earlierin Equations 1 and 2. Next, for each word in either premise or hypothesis, the relevant semantics inthe other sentence is extracted and composed according to eij. Equations 4 and 5 provide formal andspecific details of this procedure.",
  "(9) q^ = BiLSTM(q)": "where p^ Rn2d and q^ Rm2d are the reading sequences of p and q respectively. Finally theconcatenation max and average pooling of p^ and q^ are pass through a multilayer perceptron (MLP)classifier that includes a hidden layer with tanh activation and softmax output layer. The model istrained in an end-to-end manner."
}