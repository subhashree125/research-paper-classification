{
  "Abstract": "Optimizing long-term user satisfaction in recommender systems, such as news feeds, is crucial during continuoususer-system interactions. Reinforcement learning has shown promise in addressing this challenge. However,practical hurdles like low sample efficiency, potential risks, and high variance hinder the implementation of deepreinforcement learning in online systems. We introduce a new reinforcement learning approach called model-basedcounterfactual advantage learning (MBCAL) to tackle these challenges. MBCAL leverages the unique aspects ofrecommender systems and incorporates concepts from model-based reinforcement learning to enhance sampleefficiency. It consists of two main parts: an environment model that predicts immediate user behavior sequentiallyand a future advantage model that forecasts future utility. Counterfactual comparisons from the environment modelare used to mitigate the excessive variance when training the future advantage model. Consequently, MBCALachieves high sample efficiency and significantly reduced variance, while utilizing existing user logs to avoidstarting from scratch. Despite its capabilities, MBCAL maintains a relatively low implementation cost, making itsuitable for real-world systems. The proposed method surpasses other supervised learning and RL-based methodsin both sample efficiency and overall performance, as demonstrated through theoretical analysis and extensiveexperiments.",
  "Introduction": "Recommender systems are essential for delivering personalized content and improving the efficiency of information retrieval.Modern recommender systems, like news feeds, must consider multiple user-system interactions within a single session. The contentrecommended in past interactions can influence future user behavior. For example, exploring new topics might pique a users interestin related areas, while repeatedly showing similar content could lead to a rapid decline in user engagement. Traditional recommendersystems rely on collaborative filtering or neural networks to predict immediate user actions, such as clicks. However, solely focusingon immediate actions can result in issues like recommendation redundancy, ultimately harming the users long-term experience. Recently, deep reinforcement learning (Deep RL) has gained attention for its potential in recommender systems. Deep RL modelsuser-system interactions as Markov Decision Processes (MDPs). Many studies in this field focus on model-free reinforcementlearning (MFRL) methods. However, challenges persist, including the substantial data consumption during training, also known aslow sample efficiency. Another challenge is the practical risks associated with implementing MFRL. On-policy RL struggles toutilize off-policy user logs, leading to difficulties in online infrastructure and initial performance. Conversely, off-policy RL facesthe risk of non-convergence when combined with function approximation and offline training. Model-based RL (MBRL) offers an alternative with improved sample efficiency and reduced practical risks. MBRL employsan environment model to predict immediate feedback and state transitions, along with a planning module to find an optimaltrajectory. However, MBRL can be computationally intensive during inference. Planning is often infeasible in multi-stage retrievalframeworks commonly used in modern recommender systems. These systems generate candidate sets of items in earlier stages forsubsequent stages, making it impossible to predetermine candidates. To address these issues, Dyna algorithms have been proposedfor recommender systems. The Dyna algorithm accelerates convergence by generating virtual interactions using the environmentmodel. However, this faster convergence comes at the cost of reduced asymptotic performance due to error accumulation fromvirtual interactions. Another significant challenge in deploying RL is the excessive variance of gradients during optimization. This variance can stemfrom stochastic transitions, noisy rewards, and stochastic policies. Longer horizons tend to exacerbate the variance, significantlyslowing down convergence and introducing instability. Prior research has shown that using an advantage function instead of a valuefunction can reduce variance and improve performance. However, these proposals primarily target MFRL, and variance reduction inMBRL remains largely unexplored. In recommender systems, variance can arise from various factors. First, there is substantial noise in observed user feedback. Someusers may be more inclined to provide positive or negative feedback than others. Even individual users may exhibit different behaviors at different times of the day. Second, for stochastic policies, resampling trajectories from any state can lead to varyinglong-term returns. While a large amount of data can mitigate the impact of variance, it still negatively affects performance due todata sparsity for specific users and items. To address variance reduction, our work introduces the concept of comparing an observed trajectory with a counterfactual trajectory.This counterfactual trajectory shares all contexts with the original, including the user, historical interactions, and follow-up items,except for the current action being replaced. By comparing these trajectories, we can make more informed judgments about theadvantage of taking a specific action. While finding such counterfactual records in user logs is impossible, we can leverage theenvironment model to simulate future rollouts and generate these trajectories. Building on this idea, we propose a novel MBRL solution for recommender systems called Model-based Counterfactual AdvantageLearning (MBCAL). MBCAL decomposes overall utility into immediate utility (rewards from the current step) and future utility(rewards from future steps). The environment model naturally predicts immediate utility, while future utility is approximated throughsimulated rollout. To further reduce variance in future utilities, we perform two comparative simulated rollouts. We introduce amasking item to the environment model, enabling us to generate simulated rollouts by masking the action of interest. We thencalculate the counterfactual future advantage (CFA) as the difference in future utility with and without masking. Finally, we introducethe future advantage model to approximate the CFA. We conducted experiments using three real-world datasets and compared our method with supervised learning, MFRL, and MBRLapproaches. We also focused on Batch-RL and Growing Batch-RL settings, which are more aligned with practical infrastructures.The experimental results demonstrate the superiority of our proposed method.",
  "Methodology": "The core concept of MBCAL is illustrated by employing two models: the Masked Environment Model (MEM) and the FutureAdvantage Model (FAM). These models are designed to estimate immediate user behavior and future advantages, respectively. Thetraining process begins with optimizing the environment model to predict user behaviors, incorporating a masking item into themodel. Using the MEM, we compute the Counterfactual Future Advantage (CFA) by contrasting the future utility derived frommasking the action against not masking it. The CFA then serves as the target for training the FAM. During inference, we combineboth models to select actions.",
  "Environment Modeling": "Typically, an environment model predicts transitions and rewards separately. Here, we use approximations for the transitionprobability and the reward function. Specifically, to formulate the environment model in a recommender system context, we canexpress the transition probability as the probability of observing the next user behavior given the past trajectory and the currentaction. This means that predicting the transition simplifies to predicting the immediate user behavior. Since the reward also dependssolely on user behavior, a single model can replace the separate transition and reward approximations. We introduce a function withtrainable parameters to approximate the probability of the next user behavior. The transition and reward are then approximated usingthis function.",
  "Masked Environment Model": "To mitigate the intractable noise in user feedback, we introduce a masking item into the model. This allows us to create acounterfactual comparison to the current trajectory, answering the question: \"What would the future behavior be if this action werenot taken?\" We introduce a virtual item represented by a trainable embedding vector. Given an observation trajectory, we denote thetrajectory where actions at specific positions are replaced by this virtual item as a masked trajectory. Training is straightforward. We sample random positions for each trajectory, replacing each position with a uniform probability. TheMEM aims to recover the user behavior as closely as possible when some items are masked. Using the collected masked trajectories,we maximize the likelihood or minimize the negative log-likelihood (NLL). To model sequential observations, the MEMs architecture follows that of session-based recurrent recommender systems. We use aGated Neural Network to encode the trajectory. Since we need to encode both the trajectory and the current action, we concatenatethe input in a staggered manner. For each step, the model takes the previous behavior and the current action as input and outputs theprobability of the next possible behavior. An additional start symbol is introduced as the beginning of the observed user behavior.The architecture is formulated as follows: a representation layer, a concatenation operation, a multilayer perceptron, and a GatedRecurrent Unit.",
  "Counterfactual Future Advantage": "Using the Masked Environment Model (MEM), we can estimate the difference in future utilities between the original trajectory andits counterfactual counterpart, which we term the Counterfactual Future Advantage (CFA). Given a trained MEM, we first define theSimulated Future Reward (SFR) for an observed trajectory at a specific time step. We then calculate the CFA by subtracting the SFRof the counterfactual comparison from the original one. Finally, we introduce the Future Advantage Model (FAM), with its own setof trainable parameters, to approximate this CFA. To train the FAM, we minimize the mean square error. The FAM uses a similar neural architecture to the MEM, except for the final layer, but with different parameters. Instead of predictinga distribution, the FAMs last layer predicts a scalar value representing the advantage.",
  "Summary of MBCAL": "For inference, we select the item (action) based on both the MEM and FAM. Formally, given user information and the observationtrajectory, we choose the next action by maximizing the sum of the immediate reward predicted by the MEM and the future advantagepredicted by the FAM. To avoid local optima in policy improvement, we use an -greedy strategy. With probability , we select arandom action; otherwise, we select the action that maximizes the combined reward and advantage. MBCAL aligns well with the Growing Batch-RL settings. The algorithm involves iterative data collection and policy updates.Although we use the term \"policy,\" we do not require an explicit policy formulation, unlike common policy gradient methods, whichare often challenging to define in many recommender systems. The variance reduction in MBCAL is primarily achieved through the subtraction in the CFA calculation, which eliminates noisefrom user feedback and other sources. While we borrow ideas from the advantage function concept, our CFA differs in that we donot resample the trajectory but keep the remaining part unchanged. Although this could introduce bias in many MDP problems, weargue that recommender systems exhibit weaker correlations between sequential decisions compared to other domains (e.g., robot orgame control). Additionally, since the FAM averages the CFA across different trajectories, the bias becomes negligible compared tothe benefits of variance reduction.",
  "Datasets": "Evaluating RL-based recommender systems is challenging. The most reliable metric involves online A/B tests, but these are oftentoo costly and risky for comparing all baselines in an online system. Offline evaluation of long-term utility using user logs is difficultbecause we lack feedback for actions not present in the log. To thoroughly assess the performance of the proposed systems, wefollow previous works and construct simulators. However, instead of synthetic simulators, we use real-data-driven simulators. Thedatasets used include MovieLens, Netflix Prize, and NewsFeed. MovieLens: This dataset contains 5-star rating activities from MovieLens. User behavior corresponds to star ratings, withrewards matching these ratings. There are three types of features: movie-id, movie-genre, and movie-tag.",
  "Netflix Prize: This dataset consists of 5-star ratings from Netflix. Rewards follow the same setup as MovieLens. It includesonly one type of feature: movie-id": "NewsFeed: This dataset is collected from a real online news recommendation system. We focus on predicting the dwellingtime on clicked news, partitioned into 12 levels, each corresponding to a different user behavior. Rewards range from1 to 12. There are seven types of features: news-id, news-tag, news-title, news-category, news-topics, news-type, andnews-source.",
  "Experimental Settings": "To ensure a fair evaluation, it is crucial to prevent the agent in the evaluated system from exploiting the simulator. We implementtwo specific settings in the evaluation process. First, all agents are restricted to using only a subset of features, while the simulatoruses the full feature set. In MovieLens and Netflix, agents use only the movie-id feature. In NewsFeed, agents use four out of sevenfeatures (news-id, category, news-type, and news-source). Second, we intentionally set the model architecture of the simulator todiffer from that of the agents. We use LSTM units for the simulators, while agents use GRU units. To gauge the simulators accuracy, we report micro-F1, weighted-F1, and RMSE for user behavior classification. The properties ofthe datasets and simulators are detailed in . For the NewsFeed dataset, we also analyzed over 400 historical A-B test records.The correlation between our simulators predictions of long-term rewards (e.g., total clicks or session dwelling time) and the actualoutcomes is above 0.90.",
  "Evaluation Settings": "The evaluation process consists of two types of iterations: training rounds and test rounds. During a training round, the agentgenerates actions using an -greedy policy ( = 0.1 for all experiments) and updates its policy based on feedback from the simulator.In the test round, the agent uses a greedy policy, and the generated data is not used for training. Each session in both training and testrounds involves 20 steps of interaction between the simulator and the agent. Each round includes 256,000 sessions. For each experiment, we report the average reward per session in the test round, calculated as the sum of rewards over all sessions inthe test round divided by the number of sessions. Each experiment is repeated three times with different random seeds, and wereport the mean and variance of the scores. We simulate both Batch RL and Growing Batch-RL evaluations separately. In Batch RLevaluation, the agent trains only on static user logs and interacts with the simulator during testing. In Growing Batch RL evaluation,the agent interacts with the simulator during both training and test rounds, with the training round repeating up to 40 times.",
  "Methods for Comparison": "We compare various methods, including Supervised Learning (GRU4Rec), bandits (GRU4Rec (-greedy)), MFRL (MCPE, DQN,DDQN, and DDPG), and MBRL (Dyna-Q). For bandits, LinUCB is a common baseline, but it performs poorly in our environmentsdue to the limited representational power of linear models. Therefore, we use the -greedy version of NN models (GRU4Rec(-greedy)) instead of LinUCB.",
  "MBCAL (w/o variance reduction): An ablated version of MBCAL where we use SFR instead of CFA as the label for FAM": "All parameters are optimized using the Adam optimizer with a learning rate of 10-3, 1 = 0.9, and 2 = 0.999. The discount factor forlong-term rewards is = 0.95. Embedding sizes for item-id and other id-type features are set to 32. The hidden size for MLP is 32.For training MEM in MBCAL, we use pmask = 0.20 to generate masked trajectories. In DDPG, we use a 4-dimensional action spacedue to poor performance with higher dimensions, and an additional layer maps item representations to this 4-dimensional space.",
  "Results of Batch-RL Evaluation": "The results of the Batch-RL evaluation are presented in . We evaluate the reward per session based on the rewards generatedby the simulator. The results indicate that MFRL methods cannot outperform MBRL methods across all three environments. Due toits sample inefficiency, MFRL tends to exhibit poor initial performance. Notably, DDPG demonstrates the weakest performanceacross all environments. Upon closer examination of the value functions in DDPG, we observed significant overestimation comparedto other MFRL methods. This overestimation likely arises from value backups based on continuous actions that may not correspondto actual items. As anticipated, MBCAL outperforms all other tested systems by substantial margins, showcasing its sample efficiency. However, theadvantage of our method over the supervised learning method is less pronounced in the MovieLens and Netflix datasets compared toNewsFeed. This suggests that long-term rewards play a more significant role in the NewsFeed environment. Furthermore, while learning to predict long-term utility requires more data than immediate rewards, the dominance of RL is notyet fully apparent in Batch-RL settings. Nevertheless, it is crucial that MBCALs initial performance is already state-of-the-art,underscoring its low risk and high sample efficiency.",
  "Results of Growing Batch-RL Evaluation": "In all environments, GRU4Rec(-greedy) slightly outperforms the purely supervised GRU4Rec, highlighting the advantages ofexploration in online systems. The performance of DDPG remains surprisingly poor across all three environments. With the aid of the environment model, Dyna-Q initially gains some advantages but gradually diminishes as learning progresses.This observation aligns with expectations since the virtual experience loses its benefits as sufficient real user feedback accumulates.MBCAL maintains its performance lead over other methods in all environments. Even in Netflix and MovieLens, where otherRL-based systems fail to outperform traditional GRU4Rec, MBCAL achieves a considerable margin. In NewsFeed, where long-termrewards are more critical, MBCAL further extends its lead. MCPE, DQN, DDQN, and Dyna-Q lag behind other methods, including supervised learning baselines in MovieLens and Netflix, butnot in NewsFeed. Investigating further, we modified GRU4Rec to output the immediate reward instead of user behavior classification,turning the task into regression and replacing entropy loss with mean square error loss. This change resulted in a significantperformance drop in GRU4Rec, aligning more closely with the NewsFeed results. These findings suggest that classification andentropy loss benefit the system more than regression, and that user behavior contains richer information than rewards, giving MBRLan edge over MFRL.",
  "Analysis of the variance": "The critical aspect of MBCAL is variance reduction through counterfactual comparisons. Previous research indicates that themean square error (MSE) in a well-trained model comprises model bias and label variance (noise). Since we use equivalent neuralarchitectures across all comparison methods, they share the same model bias. Thus, the MSE is primarily influenced by noise. Toassess whether CFA effectively reduces variance, we compare the MSE from the value backup equation and the CFA equation. Weanalyze the MSE of MCPE, DQN, Dyna-Q, MBCAL (w/o variance reduction), and MBCAL using interactive logs from the testround of Batch-RL evaluation.",
  "DQN1.501.224.29MCPE17.19.2146.9Dyna-Q0.941.047.87MBCAL0.0040.0090.07MBCAL (w/o variance reduction)3.453.293.07": "The average MSE is presented in . Consistent with theoretical analysis, longer horizon value backups exhibit higher variance.MCPE has a higher variance than DQN and Dyna-Q due to using the entire trajectory for backup. MBCAL (w/o variance reduction)has the second-largest variance, lower than MCPE because the environment models simulated rollout partially eliminates noise.DQN and Dyna-Q have smaller variances due to one-step value backup. Compared to other methods, MBCAL shows significantlylower variance, confirming the expected variance reduction.",
  "Conclusion": "In conclusion, our work focuses on sequential decision-making problems in recommender systems. To maximize long-term utility,we propose a sample-efficient and variance-reduced reinforcement learning method called MBCAL. This method incorporates amasked environment model to capture immediate user behavior and a future advantage model to predict future utility. By employingcounterfactual comparisons, MBCAL significantly reduces learning variance. Experiments conducted on real-data-driven simulationsdemonstrate that our proposed method surpasses existing approaches in both sample efficiency and asymptotic performance. Futurework could involve theoretically calculating the error bound and extending the fixed horizon settings to infinite and dynamic horizonrecommender systems."
}