{
  "Abstract": "A handful of visual foundation models (VFMs) have recently emerged as thebackbones for numerous downstream tasks. VFMs like are trained with distinctobjectives, exhibiting unique characteristics for various downstream tasks. Wefind that despite their conceptual differences, these models can be effectivelymerged into a unified model through multi-teacher distillation. We name thisapproach AM-RADIO (Agglomerative Model Reduce All Domains Into One).This integrative approach not only surpasses the performance of individual teachermodels but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel- level understanding, and open vocabularysegmentation capabilities. Additionally, in pursuit of the most hardware-efficientbackbone, we evaluated numerous architectures in our multi-teacher distillationpipeline using the same training recipe. This led to the development of a novelarchitecture (E-RADIO) that exceeds the performance of its predecessors and is atleast 6x faster than the teacher models at matched resolution. Our comprehensivebenchmarking process covers downstream tasks including ImageNet classification,semantic segmentation linear probing, COCO object detection and integration intoLLaVa-1.5.",
  "Introduction": "Knowledge Distillation has been a very successful and popular technique for transferring the knowl-edge of a teacher model (or ensemble of models) into a typically smaller student model. In theoriginal formulation, both the student and the teacher operate on the same in-domain dataset, andthe student simultaneously matches the logits of the teacher, and the ground truth labels. Instead ofusing labeled images, an alternative approach is to train the student model to match the features ofthe teacher model. Instead of using a smaller student model, employ an iterative learning procedure with a high-capacitymodel where a student of equal or greater capacity than the teacher is trained with heavy augmentationapplied to the student. Once trained, they expand the dataset by pseudo-labeling new data using thetrained student. They then make the student become the teacher, and repeat the process. An importantfinding in this work is that the student is capable of surpassing the performance of the teacher. The authors of explore the concept of ensemble distillation, where there are multiple teachers, eachof which having restricted domain knowledge. provides an overview of multi-teacher distillation, andproposes that instead of matching the summary of an ensemble of teachers, the student can match thefeatures of each individual teacher via some learned non-shared mapping from the representationspace of the student to each teacher. Of interest in their approach is that the student and teacherdont need to share the same architecture, and also that treating teachers individually yields improvedperformance. Recently, the concept of Foundation Models (FMs) has emerged, with the general understandingthat these models are large, general, and expensive to train. Through training on very large datasetsthey are broadly applicable to numerous downstream tasks. A seminal example of such models is",
  "Related Work": "Knowledge Distillation The underpinning of our work is based on the method of Knowledge Dis-tillation which aims to train a student model using soft targets produced by an already-trainedteacher model, using the the teachers output logits as soft labels. Alternatively, distillation canbe performed using intermediate network activations. In general, due to the heterogeneous nature ofthe different teacher foundation models that we employ, we ignore any potential labels coming fromthe data, and we ignore the logits of teachers, and simply opt to match the feature representations ofthe teachers before any task-specific processing stages. Multi-Teacher Distillation There is also a body of work that studies distilling a student model jointlyfrom multiple teacher models simultaneously. Because of the heterogeneous domains that our teachermodels cover, we dont apply approaches that marginalize teachers into a unified label, and insteadmap students to each teacher independently using teacher-specific projection heads from the unifiedstudent representation. Although the reason behind this method in is different, we find the sameoverall strategy to be effective. While doesnt study matching the features of multiple teacherssimultaneously, we are able to extend their paradigm via the different projection heads. To preservedrop-in compatibility with teacher frameworks, we eliminate the feature normalization in the lossfunction. Distilling Foundation Models Foundation Models are meant to be generalist models that are trainedon massive amounts of data, and are typically resource intensive to train from scratch. In the veinof single-teacher distillation, employ self-distillation to train their smaller variants from the largerteacher. distills their model from a teacher. Instead of focusing our energy on one teacher in particular, we instead grab high-quality versions of (using OpenCLIP), , and . Concurrently with our work,describe a methodology for merging a model into a pretrained model via distillation, which is, inspirit, quite similar to our approach. In contrast to theirs, we include and also simplify the objectiveto straightforward feature matching. Since we dont rely on the student model to be pre-trained, italso gives us the flexibility to have the student be an architecture distinct from any teacher.",
  "Overview": "As an initial assumption, we expect that the teacher models are capable of representing a broad swathof images found on the internet, coming from datasets such as ImageNet (1k or 21k), LAION-400Mor DataComp-1B. With this in mind, we choose to study 3 seminal teacher model families: , ,and as they have demonstrated outstanding performance over a broad range of tasks (as in ), orspecifically strong performance on downstream dense tasks, such as semantic segmentation underlinear probe (as in ), or open-vocabulary segmentation (as in ). Because these teacher models comefrom such diverse domains, we omit any form of supplemental ground truth guidance and treat theaforementioned datasets simply as sources of images. To assess the quality of our models, we adopt aset of representative metrics across a few broad domains. Image level reasoning: (i) k-NN Top-1 accuracy on ImageNet-1K, and (ii) Zero-Shotaccuracy using the teachers language model. k-NN embeds the models summary featurevector for every image in the training set, and then for each validation image, it uses aweighted sum of the k nearest training vectors to elect a label.",
  "Adaptor Heads": "We opt for simplicity in design of the adaptor heads, and leave alternative architectures as futurework. To this end, we employ a simple 2-layer MLP, with a LayerNorm and GELU in between. Theinput dimension is the student embedding dimension, the intermediate dimension is the maximumembedding dimension of all teachers, and the output dimension matches the specific teacher. Foreach teacher, we employ two heads, one for the summary vector, and one for the spatial features.",
  "Distillation Dataset Choice": "In table 2 we study the effect of different datasets on downstream metrics. While the highest imageclassification metrics are achieved using ImageNet-1K as the training dataset, we argue that it doesntfairly measure zero shot performance as the student directly learns the teacher features in theevaluation domain. For this reason, we opt for the DataComp-1B dataset.",
  "Because we dont have ground truth data for each teacher for each image, we instead opt to matchthe features coming from each teachers vision encoder. In particular, we distinguish between the": ": Comparison of vision foundation and RADIO models. Zero-Shot and k-NN are computedon ImageNet-1K. ADE20K and VOC (PascalVOC2012) refer to linear probe semantic segmentationmIOU. GQA, POPE (popular), TextVQA, and VQAv2 are obtained via LLaVa 1.5 by replacing thevision encoder. COCO is the instance segmentation metric introduced by to evaluate distillation.RADIO attains the best metrics on most benchmarks, and is competitive with the rest, while E-RADIOenables high quality results in resource constrained settings. Note that Zero-Shot and COCO useteachers decoder head that is not finetuned. Throughput computed using NVIDIA A100 GPU, statedresolution, and TensorRT v8601. *Denotes teachers used to train our final RADIO. :We failed toexport DINOv2-g-reg to TensorRT, so we report DINOv2-g here, which should be fairly close. ::Wewere unable to get zero shot working using their model code.",
  "ImageNet 1K84.7980.4448.11ImageNet 21K84.6180.1048.65LAION-400M83.7777.4648.6DataComp-1B83.9178.5149.01": "summary feature vector and the spatial feature vectors for each teacher. The summary feature iscomputed differently based on the model. For and , we use the class token as the summary featurevector, and we dont match a summary for . Let f(x|0) be the student vision encoder with parameters 0, and yi = hi(x1|i) be the learnedstudent head matching teacher summary features zi = ti(x|i) with student adaptor parameters iand teacher parameters i.",
  "iiLcos(yi, zi)(1)": "We found empirically that cosine distance loss produced better models compared to L1, MSE,Smooth-L1. Additionally, supervising the spatial features of the model by matching the teacher wasnot only important for downstream dense tasks, but also improved the holistic quality of our model. For matching the spatial features, we employ a combination of cosine similarity and smooth L1.Similar to equation (2) where we found that cosine similarity produced the best results, we found thesame to be true for the spatial features. However, we want to allow our student model to be a drop-inreplacement in the teacher frameworks, thus its important that we match the magnitude of the teachervectors, and so we include smooth L1. In (3) we show the formulation of this loss. Let hi(x1|i)be the learned student head for matching teacher feature vectors, and corresponding ti(x|i) be theteacher feature vectors, with x1 = f(x|0), then the spatial feature loss is:",
  "Loss Balancing": "Due to the number of possible combinations of loss weights between the different teachers, andeven which teachers, and possible formulations of loss functions, we mostly opted toward naive lossbalancing with all teachers equally weighted for spatial features (i = 1). For summary features, wehave CLIP = DINO = 1 and SAM = 0. We did experiment with automatic loss balancing using predicted uncertainty, AdaLoss (momentum0.99) and separately with AMTML-KD, as ways to learn the balance of i and i. In the case ofAMTML-KD, the model would always collapse its entire weight around the teacher and wouldyield worse results than naive manual balancing. Based on the results in table 4, there is very littleadvantage to the more exotic balancing schemes, so we opt for the Naive method throughout therest of the paper. : Ablation over which teachers we supervise the spatial features. We use a ViT-L/14 studentmodel and train on the LAION-400M dataset. Adding this loss term is always beneficial. DINOv2appears to provide better spatial features than CLIP, but training the student to match both teachersproduces the best results. We dont ablate SAM as we solely want it for its spatial features.",
  "Implementation Details": "Performing heterogeneous multi-teacher distillation is not trivial due to a mismatch in featuredimensions, input resolutions, concepts for loss computation, and downsampling ratios, as well aschallenges in fitting multiple teachers into a single GPU. General. We train all student models using the AdamW optimizer, batch size 1024, cosine annealinglearning rate schedule and base learning rate of 0.001. We train for 600k steps, resulting in 614Mtotal examples seen. For our best student model, we train using DFN CLIP ViT-H/14 378px, OpenAICLIP ViT-L/14 336px, DINOv2 ViT-g/14 224px, and SAM ViTDet-H 1024px. We apply randomscale + cropping to both student and teacher inputs. We chose the DataComp-1B dataset due to ithaving the highest quality results of the web-scale datasets we had access to. We train in two stages,first with CLIP+DINOv2 for 300k steps at 256px, and second with CLIP+DINOv2 at 432px plusSAM at 1024px for 300k steps.",
  "Efficient architecture variants prioritizing high throughput on GPUs. See .1": "Multi-scale Teachers. We choose ViT-H/16 architecture for our student model. To match resolution offeatures, we feed the expected resolution of 10242. Given that our and teachers are patch-14 models,we opt to feed the student 4322 inputs, as that is the same effective resolution as 3782 for patch-14.We found that interpolating features doesnt degrade results, so the teacher operates at 224px and weupsample the outputs to match the student. Rank/Teacher Partitioning. We group teacher models by (batch size, student resolution), and thendistribute the groups to different GPUs, such that each GPU processes a consistent batch size andinput resolution. We also sample groups at different rates. For our training setups that include , wetrain with 64 GPUs, half of which get the CLIP+DINOv2 group with batch size 32 per GPU andinput resolution 432, and the other half get with batch size 2 per GPU and input resolution 1024. Thisresults in an effective batch size of 1,152. For CLIP+DINOv2 training, we use 32 GPUs, resulting inbatch size 1024. Multi-Resolution ViTs. Many of our student models use ViT as the base vision architecture. Tradition-ally, ViTs use a learned position embedding for each input patch in an image, which in turn enforcesthat the model always operates at a constant resolution. We employ the Cropped Position Embedding(CPE) augmentation with the number of positions being equal to 1282. The position embeddings arethen randomly cropped and interpolated to match the number of input patches for the student model.Even when training with CLIP+DINOv2 at 224 resolution, we found that this technique results in anegligible drop () in summary metrics, but improved semantic segmentation linear probingmIOU. For heterogeneous-resolution students, this is a seamless technique that allows ViT to operateat arbitrary resolutions within some envelope. In addition to enabling arbitrary resolutions, as shownin figure 3, CPE reduces the noise artifacts in the position embeddings as compared to other ViTmodels. High-Resolution ViT Student. In , they employ the ViTDet architecture as a way to reduce thecomputational and memory burden of ViT models at high-resolution. We reformulate this archinstead into a training augmentation, where we sample a window size from a set of possible windowsizes. This allows us to reduce the computational burden of training the student model with theteacher, and, as we make the window size flexible, it provides an additional throughput scalingmechanism during inference. demonstrates our ability to replace SAMs encoder. Separately,we found that high resolution training was unstable, so we apply spectral reparametrization and aweight decay of 0.02 to prevent attention entropy collapse. Student/Teacher Resolution Mismatch. When the student and teacher downsample images throughtheir processing stack at different rates, it results in the output feature vectors having differentresolutions. For example, if the teachers use a ViT-H/14 architecture and student a ViT-H/16, itmeans that the student outputs a 142 feature map, and the teachers a 162 feature map. For Lfeatureswe bilinearly interpolate the outputs to match the larger resolution between the student and teacherfeatures. Feature Summarization. In 3.4 we explained how teacher summary features are extracted using theclass token of their respective ViT models. We now turn our attention to the summarization ofstudent features. ViTs have 2 options: (i) a separate summarization CLS token or (ii) averagepooling patch tokens. We evaluate both options in . We observe that average pooling improves",
  "Results": "In this section, we analyze models obtained with the proposed AM-RADIO framework. First, wetouch upon backbone efficiency, then compare with the original teachers (CLIP, DINOv2, SAM), andbenchmark models under vision question answering in the LLaVa framework. We will see that theproposed models outperform the original teachers in multiple metrics, including throughput. Resultsare shown in and .",
  "Efficient Students": "We aim to find an efficient model architecture to speed up the inference of VFM. There are a numberof architectural designs aimed at high throughput on GPU devices. We use our distillation frameworkto evaluate several backbones with no change in training hyperparameters. Upon reviewing the literature on efficient vision backbones focused for high GPU throughput, wepick the following list of architectures: EfficientNetV2, ResNetv2, RegNetY, FasterViT, EfficientViT,ConvNext, NFNet, SwinV2, MaxViT, PoolformerV2 and MViTV2. We train all the backbonesvia distillation on the ImageNet-21k dataset, using OpenCLIP ViT-H/14 (laion2B-s32B-b79K) andDINOv2 g/14 as teachers. Results are compiled in . : Comparison of backbones. Throughput is measured using TensorRT 9.0.1 on A100 inmixed FP16/FP32 precision at batch size 128 on 2242px resolution. Sorted by descending throughputorder. FD loss is the Feature Distillation training loss against the DINOv2 teacher, it exhibits highcorrelation with the ADE20k mIoU. Bolded models form the speed/quality Pareto front.",
  "We observe that many models lag behind teachers. Additionally, CNN-like models are significantlyfaster than ViTs, while the latter are more accurate. The relatively low performance of existing": "efficient backbones on the dense ADE20k segmentation task is not unexpected since all of them applya spatial dimension reduction factor of 32 for final feature maps of size 72 for input resolution of2242px, thus hardly capable of capturing fine-grain spatial information. E-RADIO: To overcome this issue, we propose a novel hybrid architecture, named E-RADIO(Efficient RADIO). This design borrows ideas from existing literature and includes an input stemwith strided convolutions to downsample the input image by 4x. It then proceeds with 2 stages ofYOLOv8 C2f convolution blocks and 2 stages of transformer. For the transformer variant we pickwindowed attention (like in SWIN), and interleave local windowed attention with global windowedattention as done in and ViTDet. To perform global attention we first downsample the feature mapby 2x, apply windowed attention, and then upsample the feature maps back to the original resolution."
}