{
  "Abstract": "Bayesian optimization (BO) is a widely used technique for the global optimization of costly black-box functions.However, many real-world scenarios involve functions that are not entirely black-box. These functions may possessknown structures, such as symmetries, or the data generation process might be a composite one that providesvaluable intermediate information beyond the optimization objectives value. Traditional surrogate models usedin BO, like Gaussian Processes (GPs), do not scale well with large datasets and struggle to incorporate knownstructures. This paper introduces the use of Bayesian neural networks (BNNs), which are scalable and adaptablesurrogate models with inductive biases, to enhance BO for intricate, structured problems in high-dimensionalspaces. We showcase the application of BO on various practical challenges in physics and chemistry. This includesoptimizing the topology of photonic crystal materials using convolutional neural networks and refining chemicalproperties of molecules with graph neural networks. Our findings indicate that neural networks frequently surpassGPs as surrogate models for BO in these complex tasks, achieving greater sampling efficiency and reducedcomputational expenses.",
  "Introduction": "Bayesian optimization (BO) is a powerful technique for global optimization, particularly suited for expensive, derivative-freefunctions. It has found applications across various scientific and engineering domains, including hyperparameter tuning in machinelearning. BO operates by iteratively selecting the next data point to evaluate, aiming to maximize sampling efficiency and minimizethe number of evaluations needed to find the optimum. This is crucial when experiments or simulations are time-consuming orresource-intensive. In numerous fields, the system under investigation is not a complete black box. For instance, high-dimensional input spaces likeimages or molecules often exhibit known structures, symmetries, and invariances. Moreover, the function might be decomposableinto other functions, where the data collection process yields intermediate or auxiliary information that can be used to computethe objective function more efficiently. Examples include scientific experiments or simulations that produce high-dimensionalobservations or multiple measurements simultaneously, such as the optical scattering spectrum of a nanoparticle across variouswavelengths or multiple quantum chemistry properties of a molecule from a single density functional theory (DFT) calculation.These physically-informed insights into the system are valuable for designing surrogate models with appropriate inductive biases,but they are often underutilized in current methods. BO relies on a surrogate model to represent a distribution over potential functions, incorporating uncertainty in its predictions.Gaussian Processes (GPs) are commonly used as surrogate models due to their analytical tractability. However, GPs face challenges:(1) their inference time scales cubically with the number of observations and output dimensionality, making them less suitable forlarge datasets or problems with high output dimensionality without kernel approximations, and (2) they are most naturally applied tocontinuous, low-dimensional input spaces, requiring careful manual formulation of kernels for high-dimensional data with complexstructures. Consequently, encoding inductive biases can be difficult. Neural networks (NNs) and Bayesian neural networks (BNNs) have emerged as alternatives to GPs due to their scalability andflexibility. Another approach involves using neural networks to generate continuous latent spaces, making it easier to apply BOwith standard GPs. The ability of BNN architectures to incorporate various constraints, symmetries, and inductive biases opens uppossibilities for applying BO to more complex tasks involving structured data.",
  "This work demonstrates the application of deep learning to facilitate BO for complex, real-world scientific datasets, without relyingon pre-trained models. Specifically:": "We utilize auxiliary or intermediate information to enhance BO for tasks with high-dimensional observations. We apply BO to complex input spaces, including images and molecules, using convolutional and graph neural networks,respectively. We implement BO on several realistic scientific datasets, such as the optical scattering of a nanoparticle, topologyoptimization of a photonic crystal material, and chemical property optimization of molecules from the QM9 dataset. Our results demonstrate that neural networks can significantly outperform GPs as surrogate models on these problems. We believethese strong results will generalize to other contexts, enabling the application of BO to a wider range of problems. While ourmethods build upon existing techniques, we employ a novel combination of these methods to adapt existing BO frameworks toreal-world, complex applications.",
  "Related Work": "Several methods have been developed to improve the scalability of GPs for larger problems. For example, one framework formulti-output GPs scales linearly with the dimensionality of a low-dimensional subspace of the data. Multi-task GPs have also beenused for BO over problems with large output dimensionalities. Furthermore, GPs have been demonstrated on very large datasetsusing GPUs and intelligent preconditioners, or through various approximations. Another strategy for scaling BO to larger problems involves combining it with other methods, reducing the need for the surrogatemodel to train on the entire dataset. For instance, one method uses a collection of independent probabilistic models in different trustregions, iteratively deciding where to perform BO, effectively reducing the problem to a set of local optimizations. Other methodsbuild upon this approach and dynamically learn the partition function separating different regions. GPs have been adapted to complex problem settings to broaden the applicability of BO. For example, some approaches decomposesynthetic problems as a composition of other functions, leveraging the additional structure to improve BO. However, the multi-outputGP used in these approaches scales poorly with output dimensionality, limiting their use to simpler problems. GP kernels have alsobeen developed for complex input spaces, including convolutional and graph kernels. Graph kernels have been used to apply BO toneural architecture search (NAS), where the architecture and connectivity of a neural network itself can be optimized. Deep learning has been employed as a scalable and flexible surrogate model for BO. For instance, neural networks have been usedas adaptive basis functions for Bayesian linear regression, enabling BO to scale to large datasets. This approach also allows fortransfer learning of the adaptive basis across multiple tasks and modeling of auxiliary signals to improve performance. Additionally,Bayesian neural networks (BNNs) that use Hamiltonian Monte Carlo to sample the posterior have been used for single-task andmulti-task BO for hyperparameter optimization. A popular approach for BO in high-dimensional spaces is latent-space optimization. Here, an autoencoder, such as a VAE, is trainedon a dataset to create a continuous latent space representing the data. Then, conventional optimization algorithms, like BO with GPs,can be used to optimize over this continuous latent space. This approach has been applied to tasks such as arithmetic expressionoptimization and chemical design. Note that these approaches focus on both data generation and optimization, whereas our workfocuses solely on the optimization process. Random forests have also been used for iterative optimization, such as sequential model-based algorithm configuration (SMAC), asthey do not face scaling challenges. Tree-structured Parzen Estimators (TPE) are another popular choice for hyperparameter tuning.However, these approaches still encounter difficulties in encoding complex, structured inputs like images and graphs. Deep learning has also been applied to improve tasks other than BO. For example, active learning, similar to BO, aims to optimize amodels predictive ability with as few data points as possible. The inductive biases of neural networks have enabled active learningon various high-dimensional data, including images, language, and partial differential equations. BNNs have also been applied to thecontextual bandits problem, where the model chooses between discrete actions to maximize expected reward.",
  "Bayesian Optimization Prerequisites": "We will now briefly introduce the BO methodology. We formulate our optimization task as a maximization problem, where weaim to find the input x2217 2208 X that maximizes a function f, such that x2217 = arg maxx f(x). The input x can be a real-valuedcontinuous vector, but it can also be generalized to categorical variables, images, or discrete objects like molecules. The function freturns the objective value y = f(x), which we also refer to as the \"label\" of x, and can represent a performance metric we want tomaximize. In general, f can be a noisy function. A crucial component of BO is the surrogate model, which provides a distribution of predictions instead of a single point estimate.Ideally, these surrogate models are Bayesian, but in practice, various approximate Bayesian models or even frequentist distributionshave been used. In iteration N, a Bayesian surrogate model M is trained on a labeled dataset Dtrain = (xn, yn)N n=1. An acquisitionfunction 03b1 then uses M to suggest the next data point xN+1 2208 X to label, where:",
  "Acquisition Function": "A key consideration in BO is selecting the next data point xN+1 2208 X given the model M and labeled dataset Dtrain. This isparameterized through the acquisition function 03b1, which is maximized to determine the next data point to label, as shown inEquation 1. We utilize the expected improvement (EI) acquisition function 03b1EI. When the posterior predictive distribution of the surrogatemodel M is a normal distribution N(00b5(x), 03c32(x)), EI can be expressed analytically as:",
  "EI(x) = (x)[(x)((x)) + ((x))](2)": "where 03b3(x) = (00b5(x) 2212 ybest)/03c3(x), ybest = max(ynN n=1) is the best observed objective function value so far, and 03c6and 03a6 are the PDF and CDF of the standard normal distribution N(0, 1), respectively. For surrogate models without an analyticalform for the posterior predictive distribution, we sample from the posterior NMC times and use a Monte Carlo (MC) approximationof EI:",
  "i=1max((i)(x) ybest, 0)(3)": "where 00b5(i) is a prediction sampled from the posterior of M. While some works fit the surrogate models output to a Gaussian touse Equation 2 for acquisition, this is not valid when the model prediction for y is not Gaussian, which is generally the case forcomposite functions (see .4). EI has advantages over other acquisition functions because the MC approximation (1) remains differentiable, facilitating optimizationof the acquisition function in the inner loop (unlike the MC approximation of upper confidence bound (UCB), which is notdifferentiable and can result in ties), and (2) is inexpensive (unlike naive Thompson sampling for ensembles, which would requireretraining a model from scratch in each iteration).",
  "Continued Training with Learning Rate Annealing": "A challenge in BO is the computational cost of training a surrogate model on Dtrain from scratch in every optimization loop,especially since neural networks ideally require extensive training until convergence. To reduce the training time of BNNs in eachoptimization loop, we use the model trained in the Nth optimization loop iteration as the initialization (a \"warm start\") for the(N+1)th iteration, rather than starting from a random initialization. Specifically, we employ the cosine annealing learning rate, whichstarts with a high learning rate and gradually reduces it to 0. For more details, refer to Section A.3 in the Appendix.",
  "f(x) = h(g(x))(4)": "where g : X 2192 Z is the expensive labeling process, and h : Z 2192 Y is a known objective function that can be computed cheaply.This is also known as \"composite functions\". In this case, we train M : X 2192 Z to model g, and the approximate EI acquisitionfunction becomes:",
  "i=1max(h((i)(x)) ybest, 0)(5)": "which can be seen as a Monte Carlo version of the acquisition function presented in prior work. We denote models trained usingauxiliary information with the suffix \"-aux.\" Because h is not necessarily linear, h(00b5(i)(x)) is not generally Gaussian even if00b5(i) itself may be, making the MC approximation convenient or even necessary.",
  "Surrogate Models": "Bayesian models capture uncertainty associated with both data and model parameters in the form of probability distributions. Thisis achieved by placing a prior probability distribution P(03b8) on the model parameters and calculating the posterior belief of theparameters using Bayes theorem after observing new data. Fully Bayesian neural networks have been studied in small architecturesbut are impractical for realistically sized neural networks, as nonlinearities between layers make the posterior intractable, requiringMCMC methods to sample the posterior. However, in the last decade, numerous proposals for approximate Bayesian neural networkshave emerged, capable of capturing some Bayesian properties and producing a predictive probability distribution. In this work, wecompare several different options for the BNN surrogate model, along with other non-BNN baselines. We list some notable modelshere, with model details and results in Section A.4.1 of the Appendix. Ensembles combine multiple models to improve predictive performance by averaging their results. Ensembles of neural networkshave been reported to be more robust than other BNNs, and we use \"Ensemble\" to denote an ensemble of neural networks withidentical architectures but different random initializations, providing enough variation for individual models to give differentpredictions. Using individual models can be interpreted as sampling from a posterior distribution, so we use Equation 5 foracquisition. Our ensemble size is NMC = 10. Other BNNs: We also compare to variational BNNs, including Bayes by Backprop (BBB) and Multiplicative Normalizing Flows(MNF); BOHAMIANN; and NeuralLinear. For BBB, we also experiment with KL annealing, denoted by \"-Anneal.\" GP Baselines: GPs are largely defined by their kernel (also called \"covariance functions\"), which determines the prior and posteriordistributions, how different data points relate to each other, and the type of data the GP can operate on. In this work, \"GP\" refersto a standard specification using a Mat00e9rn 5/2 kernel, a popular kernel for real-valued continuous spaces. For images, we usea convolutional kernel, labeled as \"ConvGP\", implemented using the infinite-width limit of a convolutional neural network. Forgraphs, we use the Weisfeiler-Lehman (WL) kernel, labeled as \"GraphGP\", which can operate on undirected graphs with node andedge features, making it suitable for chemical molecule graphs. We also compare against \"GP-aux,\" which uses multi-output GPsfor problems with auxiliary information (composite functions). In the Appendix, we also examine GPs using infinite-width andinfinite-ensemble neural network limits as kernels, as well as TuRBO, which combines GP-based BO with trust regions. VAE-GP uses a VAE trained beforehand on an unlabeled dataset representative of X. This allows us to encode complex input spaces,such as chemical molecules, into a continuous latent space where conventional GP-based BO methods can be applied, even enablingthe generation and discovery of novel molecules not in the original dataset. Here, we modified an existing implementation that usesa junction tree VAE (JTVAE) to encode chemical molecules. More details can be found in the Appendix. Other Baselines: We compare against two variations of Bayesian optimization, TuRBO and TPE. We also compare against severalglobal optimization algorithms that do not use surrogate models and are computationally inexpensive, including LIPO, DIRECT-L,and CMA-ES. We emphasize that ensembles and variational methods can easily scale to high-dimensional outputs with minimal increase incomputational cost by simply changing the output layer size. Neural Linear and GPs scale cubically with output dimensionality(without covariance approximations), making them difficult to train on high-dimensional auxiliary or intermediate information.",
  "Results": "We now examine three real-world scientific optimization tasks, all of which provide intermediate or auxiliary information that can beleveraged. In the latter two tasks, the structure of the data also becomes important, and hence BNNs with various inductive biasessignificantly outperform GPs and other baselines. For simplicity, we highlight results from select architectures (see Appendix forfull results, dataset, and hyperparameter details). All BO results are averaged over multiple trials, and the shaded area in the plotsrepresents 00b1 one standard error over the trials.",
  "Multilayer Nanoparticle": "We first consider the problem of light scattering from a multilayer nanoparticle, which has various applications requiring a tailoredoptical response, including biological imaging, improved solar cell efficiency, and catalytic materials. The nanoparticle we considerconsists of a lossless silica core and 5 spherical shells of alternating TiO2 and silica. The nanoparticle is parameterized by the coreradius and layer thicknesses, which we restrict to the range of 30 nm to 70 nm. Due to the nanoparticles size being on the order ofthe wavelength of light, its optical properties can be tuned by adjusting the number and thicknesses of the layers. The scatteringspectrum can be calculated semi-analytically, as detailed in Section A.1.1 of the Appendix. Our goal is to optimize the scattering cross-section spectrum over a range of visible wavelengths. We compare two different objectivefunctions: the narrowband objective, which aims to maximize scattering in the small wavelength range of 600 nm to 640 nm andminimize it elsewhere, and the highpass objective, which aims to maximize scattering above 600 nm and minimize it elsewhere.While conventional GPs are trained using the objective function as the label directly, BNNs with auxiliary information can be trainedto predict the full scattering spectrum (the auxiliary information z 2208 R201), which is then used to calculate the objective function. The BO results are presented in . The addition of auxiliary information significantly improves BO performance for BNNs.They are also competitive with GPs, making BNNs a viable approach for scaling BO to large datasets. In Appendix A.5, we observesimilar trends for other types of BNNs. Due to the poor scaling of multi-output GPs with respect to output dimensionality, we canonly run GP-aux for a limited number of iterations within a reasonable time frame. Within these few iterations, GP-aux performspoorly, only slightly better than random sampling. We also find in the Appendix that BO with either GPs or BNNs is comparablewith or outperforms other global optimization algorithms, including DIRECT-L and CMA-ES.",
  "Photonic Crystal Topology": "Next, we examine a more complex, high-dimensional domain with symmetries that are not easily exploited by GPs. Photoniccrystals (PCs) are nanostructured materials engineered to exhibit unique optical properties not found in bulk materials, such asphotonic band gaps, negative refractive index, and angular selective transparency. With advancements in fabrication techniquesenabling smaller feature sizes, there is growing interest in inverse design and topology optimization to design more sophisticatedPCs for applications in photonic integrated circuits, flat lenses, and sensors. Here, we consider 2D PCs consisting of periodic unit cells represented by a 32 00d7 32 pixel image, with white and black regionsrepresenting vacuum (or air) and silicon, respectively. Optimizing over raw pixel values may lead to pixel-sized features orintermediate pixel values that are not physically realizable. Therefore, we parameterize the PCs with a level-set function 03c6 : X2192 V that converts a 51-dimensional feature vector x = [c1, c2, ..., c50, 2206] 2208 R51, representing the level-set parameters, intoan image v 2208 R3200d732 representing the PC. More details can be found in Section A.1.2 of the Appendix. We test BO on two different data distributions, PC-A and PC-B. In the PC-A distribution, x spans ci 2208 , 2206 2208. In the PC-B distribution, we arbitrarily restrict the domain to ci 2208 . The PC-A data distribution is translationinvariant, meaning that any PC with a translational shift will also be in the data distribution. However, the PC-B data distribution isnot translation invariant. The optical properties of PCs can be characterized by their photonic density of states (DOS). We choose an objective function thataims to minimize the DOS in a certain frequency range while maximizing it elsewhere, corresponding to opening up a photonic bandgap in that frequency range. We train GPs directly on the level-set parameters X, whereas we train the Bayesian convolutional NNs(BCNNs) on the more natural unit cell image space V. BCNNs can also be trained to predict the full DOS as auxiliary information z2208 R500. The BO results, shown in (a), demonstrate that BCNNs outperform GPs by a significant margin on both datasets. Thisis due to both the auxiliary information and the inductive bias of the convolutional layers, as shown in (b). Because thebehavior of PCs is determined by their topology rather than individual pixel values or level-set parameters, BCNNs are much bettersuited to analyze this dataset compared to GPs. Additionally, BCNNs can be made much more data-efficient since they directlyencode translation invariance and thus learn the behavior of a whole class of translated images from a single image. BecauseGP-aux is extremely expensive compared to GP (50000d7 longer on this dataset), we are only able to run GP-aux for a smallnumber of iterations, where it performs comparably to random sampling. We also compare to GPs using a convolutional kernel(201cConvGP-NNGP201d) in (a). ConvGP-NNGP only performs slightly better than random sampling, likely due to a lackof auxiliary information and inflexibility to learn the most suitable representation for this dataset. For our main experiments with BCNNs, we use an architecture that respects translation invariance. To demonstrate the effectof another commonly used deep learning training technique, we also experiment with incorporating translation invariance into atranslation-dependent architecture using a data augmentation scheme in which each image is randomly translated, flipped, androtated during training. We expect data augmentation to improve performance when the data distribution exhibits the correspondingsymmetries. As shown in (c), we indeed find that data augmentation improves the BO performance of the translation-dependent architecture when trained on the translation-invariant PC-A dataset, even matching the performance of a translation-invariant architecture on PC-A. However, on the translation-dependent PC-B dataset, data augmentation initially hurts the BOperformance of the translation-dependent architecture because the model is unable to quickly specialize to the more compactdistribution of PC-B, putting its BO performance more on par with models trained on PC-A. These results show that techniques usedto improve generalization performance (such as data augmentation or invariant architectures) for training deep learning architecturescan also be applied to BO surrogate models and, when used appropriately, directly translate into improved BO performance. Notethat data augmentation would not be feasible for GPs without a hand-crafted kernel, as the increased size of the dataset would causeinference to become computationally intractable.",
  "Organic Molecule Quantum Chemistry": "The Bayesian graph neural networks (BGNNs) used for the chemical property optimization task consist of 4 edge-conditioned graphconvolutional layers with 32 channels each, followed by a global average pooling operation, followed by 4 fully-connected hiddenlayers of 64 units each. The edge-conditioned graph convolutional layers are implemented by Spektral. More detailed results for the quantum chemistry dataset are shown in and . The architecture with the Bayes byBackprop variational approximation applied to every layer, including the graph convolutional layers (201cBBB201d), performsextremely poorly, even worse than random sampling in some cases. However, only making the fully-connected layers Bayesian(201cBBB-FC201d) performs surprisingly well.",
  "Discussion": "Introducing physics-informed priors (in the form of inductive biases) into the model is critical for performance. Well-knowninductive biases in deep learning include convolutional and graph neural networks for images and graph structures, respectively,which significantly improve BO performance. Another inductive bias we introduce is the addition of auxiliary information presentin composite functions, which significantly improves the performance of BO for the nanoparticle and photonic crystal tasks. Weconjecture that the additional information forces the BNN to learn a more consistent physical model of the system since it mustlearn features shared across the multi-dimensional auxiliary information, thus enabling the BNN to generalize better. For example,the scattering spectrum of the multilayer particle consists of multiple resonances (sharp peaks), the width and location of whichare determined by the material properties and layer thicknesses. The BNN could potentially learn these more abstract features,and thus the deeper physics, to help it interpolate more efficiently, akin to data augmentation. Auxiliary information can also beinterpreted as a form of data augmentation. Indeed, tracking the prediction error on a validation set shows that models with auxiliaryinformation tend to have a lower loss than those without (see Appendix A.5). It is also possible that the loss landscape for theauxiliary information is smoother than that of the objective function and that the auxiliary information acts as implicit regularizationthat improves generalization performance. Interestingly, GP-aux performs extremely poorly on the nanoparticle and photonic crystal tasks. One possible reason is that we areonly able to run GP-aux for a few iterations, and it is not uncommon for GP-based BO to require some critical number of iterationsto reach convergence, especially in high-dimensional systems where the size of the covariance matrix scales with the square of thedimensionality. It may also be possible that GP-aux only works on certain types of function decompositions and cannot be broadlyapplied to all composite functions, as the inductive biases in GPs are often hard-coded. There is an interesting connection between how well BNNs are able to capture and explore a multi-modal posterior distribution andtheir performance in BO. For example, we have noticed that larger batch sizes tend to significantly hurt BO performance. On the onehand, larger batch sizes may result in poorer generalization as the model finds sharper local minima in the loss landscape. Anotherexplanation is that the stochasticity inherent in smaller batch sizes allows the BNN to more easily explore the posterior distribution, which is known to be highly multi-modal. Indeed, BO often underperforms for very small dataset sizes N but quickly catches up asN increases, indicating that batch size is an important hyperparameter that must be balanced with computational cost. All our results use continued training (or warm restart) to minimize training costs. We note that re-initializing M and training fromscratch in every iteration performs better than continued training on some tasks (results in the Appendix), which points to how BNNsmay not sufficiently represent a multi-modal posterior distribution or that continued training may skew the training distribution thatthe BNN sees. Future work will consider using stochastic training approaches such as SG-MCMC methods for exploring posteriordistributions, as well as other continual learning techniques to further minimize training costs, especially for larger datasets. When comparing BNN architectures, we find that ensembles tend to consistently perform among the best, which is supported byprevious literature showing that ensembles capture uncertainty much better than variational methods, especially in multi-modal losslandscapes. Ensembles are also attractive because they require no additional hyperparameters and are simple to implement. Althoughtraining costs increase linearly with the size of the ensemble, this can be easily parallelized on modern computing infrastructures.Furthermore, recent work that aims to model efficient ensembles that minimize computational cost could be an interesting futuredirection. NeuralLinear variants are also quite powerful and cheap, making them very promising for tasks without high-dimensionalauxiliary information. Integrating Neural Linear with multi-output GPs is an interesting direction for future work. The other BNNseither require extensive hyperparameter tuning or perform poorly, making them difficult to use in practice. Additional discussion canbe found in Appendix A.5.5. As seen in Appendix A.5.4, VAE-GP performs worse than our method on two of the chemistry objectives and better on one objective.While latent-space optimization methods are often applied to domains where one wants to simultaneously generate data and optimizeover the data distribution, these methods can also be applied to the cases in this work, where a data pool (e.g., QM9 dataset forthe chemistry task) or separate data generation process (e.g., level-set process for the photonic crystal task) is already available. Inthese cases, the VAE is not used as a generative model but rather as a way to learn appropriate representations. While latent-spaceapproaches can take advantage of well-developed and widely available optimization algorithms, they also require unsupervisedpre-training on a sizable dataset and a suitable autoencoder model with the necessary inductive biases. Such models are available inchemistry, where there has been significant development, but are more limited in other domains such as photonics. On the otherhand, our method can incorporate the data structure or domain knowledge in an end-to-end manner during training, although futurework is needed to evaluate more carefully how much of an advantage this is and whether it depends on specific dataset or domaincharacteristics. For settings where we do not need a generative model, it would also be interesting to replace the autoencoder with aself-supervised model or semi-supervised model to create a suitable latent space.",
  "Conclusion": "We have demonstrated global optimization on multiple tasks using a combination of deep learning and BO. In particular, we haveshown how BNNs can be used as surrogate models in BO, enabling the scaling of BO to large datasets and providing the flexibility toincorporate a wide variety of constraints, data augmentation techniques, and inductive biases. We have demonstrated that integratingdomain knowledge on the structure and symmetries of the data into the surrogate model, as well as exploiting intermediate orauxiliary information, significantly improves BO performance, all of which can be interpreted as physics-informed priors. Intuitively,providing the BNN surrogate model with all available information allows the BNN to learn a more faithful physical model of thesystem of interest, thus enhancing the performance of BO. Finally, we have applied BO to real-world, high-dimensional scientificdatasets, and our results show that BNNs can outperform our best-effort GPs, even with strong domain-dependent structure encodedin the covariance functions. We note that our method is not necessarily tied to any particular application domain and can lower thebarrier of entry for design and optimization. Future work will investigate more complex BNN architectures with stronger inductive biases. For example, output constraints can beplaced through unsupervised learning or by variationally fitting a BNN prior. Custom architectures have also been proposed forpartial differential equations, many-body systems, and generalized symmetries, which will enable effective BO on a wider range oftasks. The methods and experiments presented here enable BO to be effectively applied in a wider variety of settings. There are alsovariants of BO, including TuRBO, which perform extremely well on our tasks, and so future work will also include incorporatingBNNs into these variants.",
  "Datasets": "The dimensionalities of the datasets are summarized in . The continuous input dimension for chemical molecules refersto the SOAP descriptor. While the space of chemical molecule graphs in general does not have a well-defined dimensionality aschemical molecules can be arbitrarily large and complex, we limit the size of molecules by only sampling from the QM9 dataset,and can define the dimensionality as the sum of the adjacency, node, and edge matrix dimensionalities. The high dimensionalities of all of these problems make Bayesian neural networks well-suited as surrogate models to enable scaling.Note that the nanoparticle scattering problem can be adjusted to be less or more difficult by either changing the input dimensionality(i.e. the number of nanoparticle layers) or the auxiliary dimension (i.e. the resolution or range of wavelengths that are sampled).",
  "Photonic Crystal": "The BNN and BCNN architectures that we use for the PC task are listed in . The size of the 201cFC201d architectures arechosen to have a similar number of parameters as their convolutional counterparts. Unless otherwise stated, all results in the maintext and here use the 201cConv-TI201d and 201cFC201d architectures for BCNNs and BNNs, respectively. The infinite-width convolutional neural networks (which act as convolutional kernels for GPs) in the PC task consist of 5 convolutionallayers followed by 4 fully-connected layers of infinite width. Because the pooling layers in the Neural Tangents library are currentlytoo slow for use in application, we increased the size of the filters to 5 00d7 5 to increase the receptive field of each filter. Detailed BO results for the PC problem are shown in . For algorithms that optimize over the level set parameterization R51,we see that GPs perform consistently well, although BNNs using auxiliary information (e.g. Ensemble-Aux) can outperform GPs.DIRECT-L and CMA perform extremely well on the PC-A distribution but performs worse than GP on the PC-B distribution. Adding convolutional layers and auxiliary information improves performance such that BCNNs significantly outperform GPs.Interestingly, the infinite-width networks perform extremely poorly, although this may be due to a lack of pooling layers in theirarchitecture which limits the receptive field of the convolutions. Examples of the optimized structures by the 201cEnsemble-aux201d architecture are shown in . The photonic crystal unitcells generally converged to the same shape: a square lattice of silicon posts with periodicity.",
  "(9)": "where each exponential term is composed from the 25 different pairs nx, ny with nx, ny 2208 22122, 22121, 0, 1, 2. We then choosea level-set offset 2206 to determine the PC structure, where regions with 03c6 > 2206 are assigned to be silicon and regions where03c6 2264 2206 are vacuum. Thus, the photonic crystal unit cell topology is parameterized by a 51-dimensional vector, [c1, c2, ...,c50, 2206] 2208 R51. More specifically,",
  "which is discretized to result in a 32 00d7 32 pixel image v 2208 03b50, 03b513200d732. This formulation also has the advantage ofenforcing periodic boundary conditions": "For each unit cell, we use the MIT Photonics Bands (MPB) software to compute the band structure of the photonic crystal, 03c9(k),up to the lowest 10 bands, using a 32 00d7 32 spatial resolution (or equivalently, 32 00d7 32 k-points over the Brillouin zone2212 03c0 a < k < 03c0 a ). We also extract the group velocities at each k-point and compute the density-of-states (DOS) via anextrapolative technique. The DOS is computed at a resolution of 20,000 points, and a Gaussian filter of kernel size 100 is used tosmooth the DOS spectrum. To normalize the frequency scale across the different unit cells, the frequency is rescaled via 03c9 219203c9norm, where 03b5avg is the average permittivity over all pixels. Finally, the DOS spectrum is truncated at 03c9norm = 1.2 andinterpolated using 500 points to give z 2208 R500.",
  "molKHeat capacity at 298.15 K": "The auxiliary information for this task consists of the properties listed in that are in the same category as the objectiveproperty, as these properties would be calculated together. The objective function then simply picks out the corresponding featurefrom the auxiliary information. More precisely, for the ground state objectives, the auxiliary information is:",
  "Algorithm 1 Bayesian optimization with auxiliary information": "1:Input: Labelled dataset Dtrain = {(xn, zn, yn)}Nstart=5n=12:for N = 5 to 1000 do3:Train M : X Z on Dtrain4:Form an unlabelled dataset, Xpool5:Find xN+1 = arg maxxXpool (x; M, Dtrain)6:Label the data zN+1 = g(xN+1), yN+1 = h(zN+1)7:Dtrain = Dtrain (xN+1, zN+1, yN+1)end for As mentioned in the main text, the inner optimization loop in line 5 of Algorithm 1 is performed by finding the maximum valueof 03b1 over a pool of |Xpool| randomly sampled points. We can see in that increasing |Xpool| in the acquisition steptends to improve BO performance. Thus, there is likely further room for improvement of the inner optimization loop using moresophisticated algorithms, possibly using the gradient information provided by BNNs. Unless otherwise stated, we optimize the innerloop of Bayesian optimization to choose the next data point to label by maximizing EI on a pool of |Xpool| = 105 randomly sampledpoints.",
  "[width=0.5]figures/figure6.png": ": Effect of m = |Xpool| used in the inner optimization loop to maximize the acquisition function on overall BO performance.ybest is taken from the narrowband objective function using the ensemble architecture. The 201caux201d in the legend denotesusing auxiliary information and the numbers represent the architecture (i.e. 8 layers of 256 units or 16 layers of 512 units).",
  "Continued Training": "As mentioned in .3 of the main text, the BNN is ideally trained from scratch until convergence in each iteration loop,although this comes at a great computational cost. An alternative is the warm restart method of continuing the training from theprevious iteration which enables the model2019s training loss to converge in only a few epochs. However, as shown in , wehave found that naive continued training can result in poor BO performance. This is likely because (a) training does not converge forthe new data point Dnew = (xN +1, yN +1) relative to the rest of the data under a limited computational budget, resulting in theacquisition function possibly labeling similar points in consecutive iterations, and (b) the BNN gets trapped in a local minima in theloss landscape that is not ideal for learning future data points. To mitigate this, we use the cosine annealing learning rate. The largelearning rate at the start of training allows the model to more easily escape local minima and explore a multimodal posterior, whilethe small learning rate towards the end of the annealing cycle allows the model to converge more easily. Note that the idea of warmrestart is similar to 201ccontinual learning,201d which is an open and active sub-problem in machine learning research. In particular,we re-train the BNN using 10 epochs.",
  "Additional Surrogate Models": "Variational BNNs model a prior and posterior distribution over the neural network weights but use some approximation on thedistributions to make the BNN tractable. In particular, we use Bayes by Backprop (BBB) (also referred to as the 201cmean field201d approximation), which approximates the posterior over the neural network weights with independent normal distributions. We alsocompare Multiplicative Normalizing Flows (MNF), which uses normalizing flows on top of each layer output for more expressiveposterior distributions. BOHAMIANN proposed to use BNNs in BO by using stochastic gradient Hamiltonian Monte Carlo (SGHMC) to approximatelysample the BNN posterior, combined with scale adaptation to adapt it for an iterative setting. NeuralLinear trains a conventional neural network on the data but then replaces the last layer with Bayesian linear regression suchthat the neural network serves as an adaptive basis for the linear regression. TuRBO (trust region Bayesian Optimization) is a method that maintains M trust regions and performs Bayesian optimization withineach trust region, maintaining M local surrogate models, to scale BO to high-dimensional problems that require thousands ofobservations. We use M = 1 and M = 5, labeled as 201cTuRBO-1201d and 201cTuRBO-5201d, respectively. TPE (Tree Parzen Estimator) is a method that instead of modeling p(y|x), models p(x|y) and p(y) for the surrogate model and fitsinto the BO framework. The tree-structure of the surrogate model allows it to define leaf variables only when node variables takeparticular values, which makes it well-suited for hyper-parameter search (e.g. the learning rate momentum is only defined formomentum-based gradient descent methods).",
  "LIPO is a parameter-free algorithm that assumes the underlying function is a Lipschitz function and estimates the bounds of thefunction. We use the implementation provided by the dlib library": "DIRECT-L (DIviding RECTangles-Local) systematically divides the search domain into smaller and smaller hyperrectangles toefficiently search the space. We use the implementation provided by the NLopt library. CMA-ES (covariance matrix adaptation evolution strategy) is an evolutionary algorithm that samples new data based on a multivariatenormal distribution and refines the parameters of this distribution until reaching convergence. We use the implementation providedby the pycma library.",
  "Implementation Details": "Unless otherwise stated, we set NMC = 30. All BNNs other than the infinitely-wide networks are implemented in TensorFlow v1.Models are trained using the Adam optimizer using the cosine annealing learning rate with a base learning rate of 1022123. Allhidden layers use ReLU as the activation function, and no activation function is applied to the output layer. Infinite-width neural networks are implemented using the Neural Tangents library. We use two different types of infinite networks:(1) 201cGP-201d refers to a closed-form expression for Gaussian process inference using the infinite-width neural network asa kernel, and (2) 201cInf-201d refers to an infinite ensemble of infinite-width networks that have been 201ctrained201d withcontinuous gradient descent for an infinite time. We compare NNGP and NTK kernels as well as the parameterization of the layers.By default, we use the NTK parameterization, but we also use the standard parameterization, denoted by 201c-std201d.",
  "Test Functions": "We test BO on several common synthetic functions used for optimization, namely the Branin and 6-dimensional Hartmann functions.We use BNNs with 4 hidden layers and 256 units in each hidden layer, where each hidden layer is followed by a ReLU activationfunction. Plots of the best value ybest at each BO iteration are shown in . As expected, GPs perform the best. Ensembles andBBB also perform competitively and much better than random sampling, showing that deep BO is viable even for low-dimensionalblack-box functions.",
  "Detailed BO results for the nanoparticle scattering problem are shown in": "All the BNNs used for the nanoparticle scattering problem use an architecture consisting of 8 hidden layers with 256 units each,with the exception of BOHAMIANN where we used the original architecture consisting of 2 hidden layers with 50 units each. Theinfinite-width neural networks for the nanoparticle task consist of 8 hidden layers of infinite width, each of which are followed byReLU activation functions.",
  "[width=0.45]figures/narrowbandbnn.png[width = 0.45]figures/highpassbnn.png[width =0.45]figures/narrowbandother.png[width = 0.45]figures/highpassother.png": ": Additional optimization result curves for the nanoparticle scattering task. (Top) Various BNNs. Note that results usingauxiliary information are denoted by a solid line, while those that do not are denoted by a dashed line. Also note that the y-axis iszoomed in to differentiate the curves. (Bottom) Various non-BO algorithms. Ensemble-aux is replicated here for ease of comparison. We also experiment with KL annealing in BBB, a proposed method to improve the performance of variational methods for BNNs inwhich the weight of the KL term in the loss function is slowly increased throughout training. For these experiments, we exponentiallyanneal the KL term with weight 03c3KL(i) = 10i/50022125 as a function of epoch i when training from scratch; during the continuedtraining, the weight is held constant at 03c3KL = 1022123. KL annealing in the BBB architecture significantly improves performance for the narrowband objective, although results are mixedfor the highpass objective. Additionally, KL annealing has the downside of introducing more parameters that must be carefully tunedfor optimal performance. MNF performs poorly, especially on the highpass objective where it is comparable to random sampling,and we have found that MNF is quite sensitive to the choice of hyperparameters for uncertainty estimates even on simple regressionproblems.",
  "The different variants infinite-width neural networks do not perform as well as the BNNs on both objective functions, despite thehyperparameter search": "LIPO seems to perform as well as GPs on both objective functions, which is impressive given the computational speed of the LIPOalgorithm. Interestingly DIRECT-L does not perform as well as LIPO or GPs on the narrowband objective, and actually performscomparably to random sampling on the highpass objective. Additionally, CMA performs poorly on both objectives, likely due to thehighly multimodal nature of the objective function landscape. We also look at the effect of model size in terms of number of layers and units in for ensembles. While including auxiliaryinformation clearly improves performance across all architectures, there is not a clear trend of performance with respect to the modelsize. Thus, the performance of BO seems to be somewhat robust to the exact architecture as long as the model is large enough toaccurately and efficiently train on the data.",
  "[width=0.5]figures/modelsize.png": ": Comparison of ybest at N = 1000 for the nanoparticle narrowband objective function for a variety of neural network sizes.All results are ensembles, and 201caux201d denotes using auxiliary information. Examples of the optimized structures by the 201cEnsemble-aux201d architecture are shown in . We can see that thescattering spectra peak in the shaded region of interest, as desired by the respective objective functions.",
  "Validation Metrics": "As in Appendix A.5.3, we track the MSE, NLL, and calibration error during optimization on the chemistry task. Results are shownin . The various metrics correlate with the respective methods2019 performances during BO. For example, VAE-GP hasan extremely high MSE and calibration error on the 03b1 objective, where it performs poorly, but has an MSE and calibrationerror more comparable with that of other methods as well as an extremely low NLL on the 03c9 2212 20acgap objective, where itperforms extremely well. Likewise, the metrics for GRAPHGP are very high on the 03b1 2212 20acgap objective, where it performspoorly. GraphEnsemble tends to be among the better methods in terms of these metrics, which translates into good BO performance.",
  "[width=0.45]figures/pcaoptimized.png[width = 0.45]figures/pcboptimized.png": ": Examples of optimized photonic crystal unit cells over multiple trials for (a) PC-A distribution and (c) PC-B distribution.(b,d) Examples of the optimized DOS. Note that the DOS has been minimized to nearly zero in a thin frequency range. Orange shadedregions mark the frequency range in which we wish to minimize the DOS. All results were optimized by the 201cEnsemble-aux201darchitecture. To explore more deeply why certain surrogate models perform well while others do not, we track various metrics of the modelduring BO on a validation dataset with 1000 randomly sampled data points. In particular, we look at the mean squared error (MSE),the mean absolute error (MAE), the negative log-likelihood (NLL), and the calibration error on the PC-A data distribution. Resultsare shown in (a). The calibration error is a quantitative measure of the uncertainty of the model, which is important for the performance of BO asthe acquisition function uses the uncertainty to balance exploration and exploitation. Intuitively, we expect that a 50% confidenceinterval contains the correct answer 50",
  "j=1(pj pj)2(15)": "where Fj is the CDF of the predictive distribution, pj is the confidence level, and 02c6pj is the empirical frequency. We choose tomeasure the error along the confidence levels pj = (j 2212 1)/10 for j = 1, 2, ..., 11. The CDF Fj(yj) an be analytically calculated formodels that have an analytical predictive distribution. For models that do not have an analytical predictive distribution, we use theempirical CDF:",
  "[width=]figures/figure13.png": ": (a) Various metrics tracked during BO of the PC-A dataset distribution on a validation dataset of 1000 datapoints. (b)Uncertainty calibration curves measured at various points during BO. Note that the calibration curve for GP-aux is only shown for N= 50, as it becomes computationally intractable for larger N. shows that the infinite neural network kernel (NTK) has the highest prediction error, which is likely a contributing factorto its poor BO performance. Interestingly, vanilla GPs have the lowest MSE, so the prediction error is not the only indicator forBO performance. Looking at the calibration, the infinite neural network kernel has the highest calibration error, and we see from(b) that it tends to be overconfident in its predictions. GPs have a higher calibration error than the ensemble neural networkmethods and tend to be significantly underconfident in their predictions. GP-aux has higher validation loss, calibration error, andNLL than most, if not all, of the other methods, which explain its poor performance. The ensemble NN methods tend to be reasonably well-calibrated. Within the ensemble NNs, the \"-aux\" methods have lower MSEand calibration error than their respective counterparts, and ConvEnsemble-aux has the lowest NLL calibration error out of all themethods, although interestingly Ensemble-aux seems to have the lowest MSE and MAE out of the ensemble NNs.",
  ": Additional BO results for several different objective functions on the chemistry dataset. GP and GraphEnsemble-auxcurves are replicated from the main text for convenience": "Ensembles trained with auxiliary information (201cEnsemble-aux201d) and neural linear (201cNeuralLinear201d) perform the beston all objective functions. Adding auxiliary information to ensembles helps for the 03b1 objective function, and neither helps norhurts for the other objective functions. Additionally, BNNs perform at least as well or significantly better than GPs in all cases. GPsperform comparably or worse than random sampling in several cases. As noted in the main text, the performance of VAE-GP depends on the quality of the pre-trained VAE, as shown in . TheVAE-GP benchmark uses the same pre-trained VAE, and 201cVAE-GP-2201d refers to the same method using a different randomseed for the VAE. Even with the exact same method, VAE-GP-2 performs significantly worse on both objective functions. We alsoincrease the latent space dimensionality from 52 to 128 in the 201cVAE-GP-LATENT128201d benchmark, which performs evenworse on the 03b1 2212 20acgap benchmark although it performs significantly better on the 03c9 benchmark. We also adjust thelearning rate momentum to 03b7 = 0.001 in 201cVAE-GP-LATENT128-BETA0.001201d, and the latent space dimensionality to 32in 201cVAE-GP-LATENT32201d. There is no clear trend with the different hyperparameters, which may point to the random seedof the VAE pre-training being a greater factor in BO performance than the hyperparameters.",
  "Additional Discussion": "BBB performs reasonably well and is competitive with or even better than ensembles on some tasks, but it requires significanthyperparameter tuning. The tendency of variational methods such as BBB to underestimate uncertainty is likely detrimental to theirperformance in BO. Additionally, prior work shows that BBB has trouble scaling to larger network sizes, which may make themunsuitable for more complex tasks such as those in our work. BOHAMIANN performs very well on the nanoparticle narrowbandobjective and comparable to other BNNs without auxiliary information on the nanoparticle highpass objective. This is likely due toits effectiveness in exploring a multi-modal posterior. However, the need for SGHMC to sample the posterior makes this methodcomputationally expensive, and so we were only able to run it for a limited number of iterations using a small neural networkarchitecture. Infinitely wide neural networks are another interesting research direction, as the ability to derive infinitely wide versions of variousneural network architectures such as convolutions, and more recently graph convolutional layers, could potentially bring the powerof GPs and BO to complex problems in low-data regimes. However, we find they perform relatively poorly in BO, are quite sensitiveto hyperparameters (e.g. kernel and parameterization), and current implementations of certain operations such as pooling are tooslow for practical use in an iterative setting. In particular, BO using an infinite ensemble of infinite-width networks performs poorlycompared to normal ensembles, suggesting that the infinite-width formulations do not fully capture the dynamics of their finite-widthcounterparts. Non-Bayesian global optimization methods such as LIPO and DIRECT-L are quite powerful in spite of their small computationaloverhead and can even outperform BO on some simpler tasks. However, they are not as consistent as BO, performing morecomparably to random sampling on other tasks. CMA-ES performs poorly on all the tasks here. Also, like GPs, these non-Bayesianalgorithms assume a continuous input space and cannot be effectively applied to structured, high-dimensional problems."
}