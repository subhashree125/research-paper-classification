{
  "Abstract": "The concept of a \"modular\" structure in artificial neural networks has been suggested as beneficial for learning,the ability to combine elements, and applying knowledge to new situations. However, a clear definition andmeasurement of modularity are still open questions. This paper reframes the identification of functional modules asthe identification of groups of units with similar functions. This raises the question of what constitutes functionalsimilarity between two units. To address this, we examine two main categories of methods: those that definesimilarity based on how units react to variations in inputs (upstream), and those that define similarity based onhow changes in hidden unit activations affect outputs (downstream). We perform an empirical analysis to measurethe modularity of hidden layer representations in simple feedforward, fully connected networks across varioussettings. For each model, we assess the relationships between pairs of hidden units in each layer using a rangeof upstream and downstream metrics, then group them by maximizing their \"modularity score\" with establishednetwork science tools. We find two unexpected results: first, dropout significantly increased modularity, whileother forms of weight regularization had smaller effects. Second, while we observe general agreement on clusterswithin upstream methods and within downstream methods, there is limited agreement on cluster assignmentsbetween these two categories. This has significant implications for representation learning, as it implies thatfinding modular representations that reflect input structure (e.g., disentanglement) may be a different objectivefrom learning modular representations that reflect output structure (e.g., compositionality).",
  "Introduction": "Modularity, a principle where complex systems are broken down into simpler subsystems, allows for independent analysis, debugging,and recombination for new tasks. This design approach offers benefits like enhanced robustness and quicker adaptation to newchallenges. It is recognized that learning systems gain advantages from structures tailored to the specific problem, and manyreal-world problems can indeed be divided into sub-problems. Consequently, modularity is viewed as a standard design principle inevolved biological systems, including biological neural networks, and one that can be advantageous for artificial neural networks(ANNs). Despite the intuitive appeal, formally defining and quantifying the modularity of a given system remains an unresolved issue. It isgenerally agreed that modular systems, by definition, break down into subsystems that carry out functions to solve sub-problems.Defining modules in ANNs, therefore, requires us to determine when two parts of a network are involved in the same \"function\". Inthis paper, we address this question at the level of pairs of hidden units. We explore various methods for assessing the \"functionalsimilarity\" of any two hidden units, and we define a \"module\" as a group of units with similar functions. This definition is notintended to be the definitive answer to what constitutes a module, but rather to offer a practical foundation for experimenting withdifferent concepts related to modularity, such as how regularization affects it. A key objective of this paper is to highlight the differences between \"upstream\" and \"downstream\" perspectives when consideringneural representations and their functions. In , we provide precise definitions and detail our method for identifying andquantifying functional modules in the hidden layers of trained neural networks by grouping units into functionally similar sets. Thisframework enables us to directly compare various indicators of a networks modularity. describes the experimental results.Besides quantitatively evaluating modularity, we further examine whether different similarity measures agree on the assignment ofunits to modules. Surprisingly, we find that modules identified using \"upstream\" measures of functional similarity are consistentlydifferent from those found using \"downstream\" measures. Although we do not examine regularization methods specifically designedto create modular designs, these initial findings call for a more in-depth examination of how the \"function\" of a representation isdefined, as well as why and when modules might be beneficial.",
  "Related Work": "The investigation of modularity in neural networks has a rich history. A frequent source of inspiration from biology is the separationof \"what\" and \"where\" pathways in the ventral and dorsal streams of the brain, respectively. Each pathway can be viewed as aspecialized module (and can be further divided into submodules). Numerous prior experiments on modularity in artificial neuralnetworks have investigated principles that would lead to similarly distinct what/where information processing in ANNs. A significantdistinction from this line of work is that, instead of predefining the functional role of modules, such as one module handling \"what\"and another handling \"where,\" our research aims to discover distinct functional groups in trained networks. Generally, there are two categories of approaches to modularity in neural networks, each corresponding to a different way ofunderstanding the function of network components. The structural modularity approach defines function based on network weightsand the connections between sub-networks. Modules are thus defined as sub-networks with dense internal connections and sparseexternal connections. The functional modularity approach focuses on network activations or the information represented by thoseactivations, rather than weights. This includes concepts like disentanglement, compositionality, and invariance. The connectionbetween structural and functional modules is not entirely clear. While they seem to be (or should be) correlated, it has been observedthat even very sparse inter-module connectivity does not always ensure functional separation of information processing. In this study,we adopt the functional approach, assuming that structural modularity is only useful to the extent that it supports distinct functionsof the units, and that often distinct functions must share information, making strict structural boundaries potentially detrimental. Forinstance, in a complex visual scene, knowing \"what\" an object is can aid in determining \"where\" it is, and vice versa. Our work is most closely related to a series of papers by Watanabe and colleagues in which trained networks are decomposed intoclusters of \"similar\" units with the aim of understanding and simplifying those networks. They quantify the similarity of units usinga combination of both incoming and outgoing weights. This is similar in spirit to our goal of identifying modules by clustering units,but an interesting contrast to our approach, where we find stark differences between \"upstream\" and \"downstream\" similarity.",
  "Quantifying modularity by clustering similarity": "We divide the task of identifying functional modules into two phases: evaluating the pairwise similarity of units, and then clusteringbased on this similarity. For simplicity, we apply these steps separately to each hidden layer, although in principle, modules could beassessed in the same way after combining layers. .1 defines the set of pairwise functional similarity methods we use, and.2 describes the clustering phase. While we concentrate on similarity between pairs of individual units, our method is connected to, and inspired by, the question ofwhat makes neural representations \"similar\" when comparing entire populations of neurons to each other. Instead of finding clustersof similar neurons as we do here, one could define modules in terms of dissimilarity between clusters of neurons. In preliminarywork, we explored such a definition of functional modules, using representational (dis)similarity between sub-populations of neurons.The primary challenge with this approach is that existing representational similarity methods are highly sensitive to dimensionality(the number of neurons in each cluster), and it is not clear how best to account for this when calculating dissimilarity between clustersso that the method is not biased towards larger or smaller cluster sizes. To further justify our method, note that representationalsimilarity analysis is closely related to tests for statistical (in)dependence between populations of neurons, and so the problem offinding mutually \"dissimilar\" modules is analogous to the problem of finding independent subspaces. In Independent SubspaceAnalysis (ISA), there is a similar issue of determining what constitutes a surprising amount of dependence between subspaces ofdifferent dimensions, and various methods have been proposed with different inductive biases. However, Palmer Makeig showedthat a solution to the problem of detecting independent subspaces is to simply cluster the individual dimensions of the space. Thisprovides some justification for the methods we use here: some technicalities notwithstanding, the problem of finding subspaces ofneural activity with \"dissimilar\" representations is, in many cases, reducible to the problem of clustering individual units based onpairwise similarity, as we do here.",
  "Quantifying pairwise similarity of hidden units": "What constitutes \"functional similarity\" between two hidden units? In other words, we are looking for a similarity function S thattakes a neural network N, a dataset D, and a task T as inputs, and produces an n x n matrix of non-negative similarity scores for allpairs among the n hidden units. We also require that the resulting matrix is symmetric, meaning Sij = Sji. Importantly, allowing S todepend on the task T opens up the possibility of similarity measures where units are considered similar based on their downstreamcontribution to a specific loss function. Similarity by covariance. The first similarity measure we examine is the absolute value of the covariance of hidden unit activitiesacross inputs. Let xk be the kth input in the dataset, and hi(x) be the response of the ith hidden unit to input x, with i in 1, 2, ..., n.Then, we define similarity as",
  "k=1|(hi(xk) hi)(hj(xk) hj)|(1)": "where K is the number of items in D and hi is the mean response of unit i on the given dataset. Intuitively, the absolute valuecovariance quantifies the statistical dependence of two units across inputs, making it an upstream measure of similarity. Similarity by input sensitivity. While Scov measures similarity of responses across inputs, we next consider a measure of similarsensitivity to single inputs, which is then averaged over D. Let Jhxk denote the n x d Jacobian matrix of partial derivatives of eachhidden unit with respect to each of the d input dimensions. Then, we say two units i and j are similarly sensitive to input changes oninput xk if the dot product between the ith and jth row of Jhxk has high absolute-value magnitude. In matrix notation over the entiredataset, we use",
  "where the superscript \"i-sens\" should be read as the \"input sensitivity.\"": "Similarity by last-layer sensitivity. Let y denote the last-layer activity of the network. Using the same Jacobian notation as above, letJyh denote the o x n matrix of partial derivatives of the last layer with respect to changes in the hidden activities h. Then, we definesimilarity by output sensitivity as",
  "likewise with \"o-sens\" to be read as \"output-sensitivity.\" Note that both h and y depend on the particular input xk, but this has beenleft implicit in the notation to reduce clutter": "Similarity by the loss Hessian. The \"function\" of a hidden unit might usefully be thought of in terms of its contribution to the task ortasks it was trained on. To quote Lipson, \"In order to measure modularity, one must have a quantitative definition of function... It isthen possible to take an arbitrary chunk of a system and measure the dependency of the system function on elements within thatchunk. The more that the dependency itself depends on elements outside the chunk, the less the function of that chunk is localized,and hence the less modular it is.\" Lipson then goes on to suggest that the \"dependence of system function on elements\" can be expressed as a derivative or gradient,and that the dependence of that dependence on other parts of the system can be expressed as the second derivative or Hessian.Towards this conception of modular functions on a particular task, we use the following definition of similarity:",
  "hihj|(4)": "where L is the scalar loss function for the task, and should be understood to depend on the particular input xk. Importantly, eachHessian on the right hand side is taken with respect to the activity of hidden units, not with respect to the network parameters as it istypically defined. To summarize, equations (1) through (4) provide four different methods to quantify pairwise similarity of hidden units. Scov andSisens are upstream, while Sosens and Shess are downstream. All four take values in [0, ). However, it is not clear if the rawmagnitudes matter, or only relative (normalized) magnitudes. For these reasons, we introduce an optional normalized version ofeach of the above four un-normalized similarity measures:",
  "max(Sii, Sjj, )(5)": "where 20ac is a small positive value included for numerical stability. Whereas Sij is in [0, ), the normalized values are restrictedto Sij in . In total, this gives us eight methods to quantify pairwise similarity. These can be thought of as 2x2x2 product ofmethods, as shown in the color scheme in : the upstream vs downstream axis, the unnormalized vs normalized axis, andthe covariance vs gradient (i.e. sensitivity) axis. We group together both Scov and Shess under the term \"covariance\" because theHessian is closely related to the covariance of gradient vectors of the loss across inputs.",
  "Quantifying modularity by clustering": "Decomposing a set into clusters that are maximally similar within clusters and maximally dissimilar across clusters is a well-studiedproblem in graph theory and network science. In particular, Girvan Newman proposed a method that cuts a graph into its maximallymodular subgraphs, and this tool has previously been used to study modular neural networks. We apply this tool from graph theory to our problem of detecting functional modules in neural networks by constructing an adjacencymatrix A from the similarity matrix S by simply removing the diagonal (self-similarity):",
  "ij Aij(7)": "or, more compactly, A = A/1TnA1n where 1n is a column vector of length n containing all ones. Let P be an n x c matrix thatrepresents cluster assignments for each of n units to a maximum of c different clusters. Cluster assignments can be \"hard\" (Pij in 0,1) or \"soft\" (Pij in ), but in either case the constraint P1c = 1n must be met, i.e. that the sum of cluster assignments for eachunit is 1. If an entire column of P is zero, that cluster is unused, so c only provides an upper-limit to the number of clusters, and inpractice we set c = n. Girvan Newman propose the following score to quantify the level of \"modularity\" when partitioning thenormalized adjacency matrix A into the cluster assignments P:",
  "Q( A, P) = Tr(P T AP) Tr(P T A1n1Tn AP)(8)": "The first term sums the total connectivity (or, in our case, similarity) of units that share a cluster. By itself, this term is maximizedwhen P assigns all units to a single cluster. The second term gives the expected connectivity within each cluster under a nullmodel where the elements of A are interpreted as the joint probability of a connection, and so A1n1Tn A is the product of marginalprobabilities of each units connections. This second term encourages P to place units into the same cluster only if they aremore similar to each other than \"chance.\" Together, equation (8) is maximized by partitioning A into clusters that are stronglyintra-connected and weakly inter-connected.",
  "P ( A) = argmaxP Q( A, P)Q( A) = Q( A, P )(9)": "To summarize, to divide a given pairwise similarity matrix S into modules, we first construct A from S, then we find the clusterassignments P that give the maximal value Q. Importantly, this optimization process provides two pieces of information: amodularity score Q which quantifies the amount of modularity in a set of neurons, for a given similarity measure. We also getthe actual cluster assignments P , which provide additional information and can be compared across different similarity measures.Given a set of cluster assignments P , we quantify the number of clusters by first getting the fraction of units in each cluster,r(P ) = 1TnP /n. We then use the formula for discrete entropy to measure the dispersion of cluster sizes: H(r) = ci=1 rilogri.Finally we say that the number of clusters in P is",
  "numclusters(P ) = eH(r(P ))(10)": "We emphasize that discovering the number of clusters in P is included automatically in the optimization process; we set themaximum number of clusters c equal to the number of hidden units n, but in our experiments we find that P rarely uses more than 6clusters for hidden layers with 64 units (Supplemental Figure S4). It is important to recognize that the sense of the word \"modularity\" in graph theory is in some important ways distinct from itsmeaning in terms of engineering functionally modular systems. In graph-theoretic terms, a \"module\" is a cluster of nodes that arehighly intra-connected and weakly inter-connected to other parts of the network, defined formally by Q. This definition of graphmodularity uses a particular idea of a \"null model\" based on random connectivity between nodes in a graph. While this null-modelof graph connectivity enjoys a good deal of historical precedence in the theory of randomly-connected graphs, where unweightedgraphs are commonly studied in terms of the probability of connection between random pairs of nodes, it is not obvious that thesame sort of null model applies to groups of \"functionally similar\" units in an ANN. This relates to the earlier discussion of ISA, andprovides a possibly unsatisfying answer to the question of what counts as a \"surprising\" amount of statistical independence betweenclusters; using Q makes the implicit choice that the product of average pairwise similarity, A1n1Tn A, gives the \"expected\" similaritybetween units. An important problem for future work will be to closely reexamine the question of what makes neural populationsfunctionally similar or dissimilar, above and beyond statistical similarity, and what constitutes a surprising amount of (dis)similaritythat may be indicative of modular design. Finding P exactly is NP-complete, so in practice we use a variation on the approximate method proposed by Newman. Briefly, theapproximation works in two steps: first, an initial set of cluster assignments is constructed using a fast spectral initialization methodthat, similar to other spectral clustering algorithms, recursively divides units into clusters based on the sign of eigenvectors of the matrix B = A A1n1Tn A and its submatrices. Only subdivisions that increase Q are kept. In the second step, we use a Monte Carlomethod that repeatedly selects a random unit i then resamples its cluster assignment, holding the other n-1 assignments fixed. Thisresampling step involves a kind of exploration/exploitation trade-off: Q may decrease slightly on each move to potentially find abetter global optimum. We found that it was beneficial to control the entropy of each step using a temperature parameter, to ensurethat a good explore/exploit balance was struck for all A. Supplemental Figure S2 shows that both the initialization and the MonteCarlo steps play a crucial role in finding P , consistent with the observations of Newman. Full algorithms are given in AppendixA.1.",
  "Setup and initial hypotheses": "Because our primary goal is to understand the behavior of the various notions of modularity above, i.e. based on the eight differentmethods for quantifying pairwise similarity introduced in the previous section, we opted to study a large collection of simple networkstrained on MNIST. All pairwise similarity scores were computed using held-out test data. We trained 270 models, comprising 9 runsof each of 30 regularization settings, summarized in . We defined x (input layer) as the raw 784-dimensional pixel inputs andy (output layer) as the 10-dimensional class logits. We used the same basic feedforward architecture for all models, comprisingtwo layers of hidden activity connected by three layers of fully-connected weights: Linear(784, 64), ReLU, dropout(p), Linear(64,64), ReLU, dropout(p), Linear(64, 10). We analyzed modularity in the two 64-dimensional hidden layers following the dropoutoperations. We discarded 21 models that achieved less than 80",
  "Before running these experiments, we hypothesized that": "1. Dropout would decrease modularity by encouraging functions to be \"spread out\" over many units. 2. L2 regularization (weightdecay) would minimally impact modularity since the L2 norm is invariant to rotation while modularity depends on axis-alignment. 3.L1 regularization on weights would increase modularity by encouraging sparsity between subnetworks. 4. All similarity measureswould be qualitatively consistent with each other.",
  "How modularity depends on regularization": "shows the dependence of trained networks modularity score (Q) as a function of regularization strength for each of threetypes of regularization: an L2 penalty on the weights (weight decay), an L1 penalty on the weights, and dropout. The top row of shows four example A matrices sorted by cluster, to help give an intuition behind the quantitative values of Q. In theseexamples, the increasing value of Q is driven by an increasing contrast between intra-cluster similarity and inter-cluster similarity.In this example, it also appears that the number and size of clusters remains roughly constant; this observation is confirmed byplotting the number of clusters versus regularization strength in Supplemental Figure S4. shows a number of surprising patterns that contradict our initial predictions. First, and most saliently, we had predicted thatdropout would reduce modularity, but found instead that it has the greatest effect on Q among the three regularization methods wetried. This is especially apparent in the upstream methods (first two columns of the figure), and is also stronger for the first hiddenlayer than the second (Supplemental Figure S3). In general, Q can increase either if the network partitions into a greater number ofclusters, or if the contrast between clusters is exaggerated. We found that this dramatic effect of dropout on Q was accompaniedby only minor changes to the number of clusters (Supplemental Figure S4), and so we can conclude that dropout increases Q byincreasing the redundancy of hidden units. In other words, hidden units become more clusterable because they are driven towardsbehaving like functional replicas of each other, separately for each cluster. This observation echoes, and may explain, why dropoutalso increases the \"clusterability\" of network weights in a separate study. The second surprising result in is that L2 regularization on the weights did, in fact, increase Q, whereas we had expected itto have no impact. Third, L1 regularization had a surprisingly weak effect, although its similarity to the L2 regularization resultsmay be explained by the fact that they actually resulted in fairly commensurate sparsity in the trained weights (Supplemental FigureS1 bottom row). Fourth, we had expected few differences between the eight different methods for computing similarity, but thereappear to be distinctive trends by similarity type both in Figure S3 as well as in the number of clusters detected (SupplementalFigure S4). The next section explores the question of similarity in the results in more detail.",
  "Comparing modules discovered by different similarity methods": "The previous section discussed idiosyncratic trends in the modularity scores Q as a function of both regularization strength andhow pairwise similarity between units (S) is computed. However, such differences in the quantitative value of Q are difficult tointerpret, and would largely be moot if the various methods agreed on the question of which units belong in which cluster. We nowturn to the question of how similar the cluster assignments P are across our eight definitions of functional modules. To minimizeambiguity, we will use the term \"functional-similarity\" to refer to S, and \"cluster-similarity\" to refer to the comparison of differentcluster assignments P . Quantifying similarity between cluster assignments is a well-studied problem, and we tested a variety of methods in the clusimPython package. All cluster-similarity methods we investigated gave qualitatively similar results, so here we report only the \"ElementSimilarity\" method of Gates et al., which is a value between 0 and 1 that is small when two cluster assignments are unrelated, andlarge when one cluster assignment is highly predictive of the other. Note that this cluster-similarity analysis is applied only to P cluster assignments computed in the same layer of the same model. Thus, any dissimilarity in clusters that we see is due entirely tothe different choices for functional-similarity, S. a summarizes the results of this cluster-similarity analysis: there is a striking difference between clusters of units identified by\"upstream\" functional-similarity methods (Scov, Scov, Sisens, Sisens) compared to \"downstream\" functional-similarity methods(Shess, Shess, Sosens, Sosens). This analysis also reveals secondary structure within each class of upstream and downstreammethods, where the choice to normalize not (S vs S) appears to matter little, and where there is a moderate difference betweenmoment-based methods (Scov, Shess) and gradient-based methods (Sisens, Sosens). It is worth noting that some of this secondarystructure is not robust across all types and levels of regularization; in particular, increasing L2 or L1 regularization strength appearsto lead to (i) stronger dependence on normalization in the downstream methods, and (ii) a stronger overall agreement among theupstream methods (Supplemental Figure S5). We next asked to what extent these cluster-similarity results are driven by training. As shown in b, much of the structurein the downstream methods is unaffected by training (i.e. it is present in untrained models as well), while the cluster-similarityamong different upstream methods only emerged as a result of training. Interestingly, this analysis further shows that the mainupstream-vs-downstream distinction seen in a is, in fact, attenuated slightly by training.",
  "Conclusions": "The prevalence of \"modular\" designs in both engineered and evolved systems has led many to consider the benefits of modularity asa design principle, and how learning agents like artificial neural networks might discover such designs. However, precisely definingwhat constitutes a \"module\" within a neural network remains an open problem. In this work, we operationalized modules in a neuralnetwork as groups of hidden units that carry out similar functions. This naturally leads to the question of what makes any two unitsfunctionally similar. We introduced eight functional similarity measures designed to capture various intuitions about unit similarityand empirically evaluated cluster assignments based on each method in a large number of trained models. One unexpected observation was that dropout increases modularity (as defined by Q), although this has little to do with thecommon-sense definition of a \"module.\" Instead, it is a byproduct of dropout causing subsets of units to behave like near-copiesof each other, perhaps so that if one unit is dropped out, a copy of it provides similar information to the subsequent layer. To ourknowledge, this redundancy-inducing effect of dropout has not been noted in the literature previously. Our main result is that there is a crucial difference between defining \"function\" in terms of how units are driven by upstream inputs,and how units drive downstream outputs. While we studied this distinction between upstream and downstream similarity in thecontext of modularity and clustering, it speaks to the deeper and more general problem of how best to interpret neural representations.For example, some sub-disciplines of representation-learning (e.g. \"disentanglement\") have long emphasized that a \"good\" neuralrepresentation is one where distinct features of the world drive distinct sub-populations or sub-spaces of neural activity. This is anupstream way of thinking about what is represented, since it depends only on the relationship between inputs and the unit activationsand does not take into account what happens downstream. Meanwhile, many have argued that the defining characteristic of a neuralrepresentation is its causal role in downstream behavior; this is, of course, a downstream way of thinking. At a high level, one wayto interpret our results is is that upstream and downstream ways of thinking about neural representations are not necessarily aligned,even in trained networks. This observation is reminiscent of recent empirical work finding that \"disentangled\" representations inauto-encoders (an upstream concept) do not necessarily lead to improved performance or generalization to novel tasks (a downstreamconcept). Despite its theoretical motivations, this is an empirical study. We trained over 250 feedforward, fully-connected neural networks onMNIST. While it is not obvious whether MNIST admits a meaningful \"modular\" solution, we expect that the main results we showhere are likely robust, in particular (i) the effect of weight decay, an L1 weight penalty, and dropout, and (ii) misalignment betweenupstream and downstream definitions of neural similarity. Our work raises the important questions: are neural representations defined by their inputs or their outputs? And, in what contextsis it beneficial for these to be aligned? We look forward to future work applying our methods to larger networks trained on morestructured data, as well as recurrent networks. We also believe it will be valuable to evaluate the effect of attempting to maximizemodularity, as we have defined it, during training, to see to what extent this is possible and whether it leads to performance benefits.Note that maximizing Q during training is challenging because (i) computing S may require large batches, and more importantly(ii) optimizing Q is highly prone to local minima, since neural activity and cluster assignments P will tend to reinforce each other,entrenching accidental clusters that appear at the beginning of training. We suspect that maintaining uncertainty over clusterassignments (e.g. using soft Pij in rather than hard P in 0, 1 cluster assignments) will be crucial if optimizing any of ourproposed modularity metrics during training.",
  "Melvyn A. Goodale and A. David Milner. Separate visual pathways for perception and action. TINS, 15(1): 20-25, 1992": "Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schlkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In S. Jain, H. U. Simon, and E. Tomita (eds.), Lecture Notes in Artificial Intelligence, volume 3734, pp.63-77. Springer-Verlag, Berlin, 2005. Harold W Gutch and Fabian J Theis. Independent Subspace Analysis is Unique, Given Irreducibility. In Mike E Davies,Christopher J James, Samer A Abdallah, and Mark D Plumbley (eds.), Independent Component Analysis and SignalSeparation, volume 7. Springer, Berlin, 2007.",
  "Aapo Hyvrinen, Patrik O. Hoyer, and Mika Inki. Topographic independent component analysis. Neural Computation,13(7):1527-1558, 2001": "Robert A Jacobs, Michael I Jordan, and Andrew G Barto. Task Decomposition Through Competition in a ModularConnectionist Architecture:The What and Where Vision Tasks. Cognitive Science, pp. 219-250, 1991. Nadav Kashtan and Uri Alon. Spontaneous evolution of modularity and network motifs. Proceedings of the NationalAcademy of Sciences of the United States of America, 102(39):13773-13778, 2005.Nadav Kashtan, Elad Noor, and Uri Alon. Varying environments can speed up evolution. Proceedings of the NationalAcademy of Sciences of the United States of America, 104(34):13711-13716, 2007.Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of Neural Network RepresentationsRevisited. ICML, 36, 2019.Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recogni-tion. Proceedings of the IEEE, 86(11):2278-2324, 1998.H Lipson. Principles of modularity, regularity, and hierarchy for scalable systems. Journal of Biological Physics andChemistry, 7(4):125-128, 2007.Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Schlkopf, and Olivier Bachem. ChallengingCommon Assumptions in the Unsupervised Learning of Disentangled Representations. arXiv, pp. 1-33, 2019.Milton Llera Montero, Casimir JJ Ludwig, Rui Ponte Costa, Guarav Malhotra, and Jeffrey Bowers. The role of disentangle-ment in generalization. ICLR, 2021.M. E.J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences ofthe United States of America, 103(23):8577-8582, 2006.M. E.J. Newman and M. Girvan. Finding and evaluating community structure in networks. Physical Review E - Statistical,Nonlinear, and Soft Matter Physics, 69(2 2):1-15, 2004.Jason A. Palmer and Scott Makeig. Contrast functions for independent subspace analysis. In Fabian J. Theis, A. Cichocki,A. Yeredor, and M. Zibulevsky (eds.), Independent Component Analysis and Signal Separation, volume LNCS 7191, pp.115-122. Springer-Verlag, Berlin, 2012.Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperativestyle, high- performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035.Curran Associates, Inc., 2019.Barnabs Pczos and Andrs Lorincz. Independent Subspace Analysis Using Geodesic Spanning Trees. ICML, 22:673-680,2005.Karl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the F-statistic loss. Advances inNeural Information Processing Systems, pp. 185-194, 2018.J. G. Rueckl, K. R. Cave, and S. M. Kosslyn. Why are \"what\" and \"where\" processed by separate cortical visual systems?A computational investigation. Journal of Cognitive Neuroscience, 1(2):171-186, 1989.Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and YoshuaBengio. Toward Causal Representation Learning. Proceedings of the IEEE, 109(5): 612-634, 2021.Herbert A Simon. The Architecture of Complexity. Proceedings of the American Philosophical Society, 106 (6), 1962.O. Tange. Gnu parallel - the command-line power tool. ;login: The USENIX Magazine, 36(1):42-47, Feb 2011.Gnter P. Wagner, Mihaela Pavlicev, and James M. Cheverud. The road to modularity. Nature Reviews Genetics,8(12):921-931, 2007.Chihiro Watanabe. Interpreting Layered Neural Networks via Hierarchical Modular Representation. Communications inComputer and Information Science, 1143 CCIS:376-388, 2019.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Modular representation of layered neural networks. NeuralNetworks, 97:62-73, 2018.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Understanding community structure in layered neural networks.Neurocomputing, 367:84-102, 2019.Chihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Knowledge discovery from layered neural networks based onnon-negative task matrix decomposition. IEICE Transactions on Information and Systems, E103D(2):390-397, 2020.Zongze Wu, Chunchen Su, Ming Yin, Zhigang Ren, and Shengli Xie. Subspace clustering via stacked independent subspaceanalysis networks with sparse prior information. Pattern Recognition Letters, 146: 165-171, 2021.",
  "logspace(-5,-1,9)0.00.01e-5logspace(-5,-2,7)0.01e-50.0linspace(0.05,0.7,14)": ": Each row describes one hyperparameter sweep, for a total of 30 distinct hyperparameter values. First row: varying weightdecay (L2 weight penalty) with no other regularization (9 values). Second row: varying L1 penalty on weights along with mildweight decay (7 values). Third row: varying dropout probability in increments of 0.05 along with mild weight decay (14 values).",
  "[width=]images.png": ": Basic performance metrics as a function of regularization strength. Each column corresponds to a different regularizationmethod, as in . Each row shows a metric calculated on the trained models. Thin colored lines are individual seeds, and thickblack line is the average standard error across runs. Horizontal gray line shows each metric computed on randomly initializednetwork. Sparsity (bottom row) is calculated as the fraction of weights in the interval [-1e-3, +1e-3].",
  "[width=0.45]image1.png [width=0.45]image2.png": ": Both spectral initialization and Monte Carlo optimization steps contribute to finding a good value of Q. Left: The x-axisshows modularity scores (Q) achieved using only the greedy spectral method for finding P . The y-axis shows the actual scores weused in the paper by combining the spectral method for initialization plus Monte Carlo search. The fact that all points are on orabove the y=x line indicates that the Monte Carlo search step improved modularity scores. Right: The x-axis now shows modularityscores (Q) achieved using 1000 Monte Carlo steps, after initializing all units into a single cluster (we chose a random 5% of thesimilarity-matrices that were analyzed in the main paper to re-run for this analysis, which is why there are fewer points in thissubplot than in the left subplot). The fact that all points are on or above the y=x line indicates that using the spectral method toinitialize improved the search.",
  "[width=]image3.png": ": Modularity score (Q) versus regularization, split by layer. Format is identical to , which shows modularity scoresaveraged across layers. Here, we break this down further by plotting each layer separately. The network used in our experiments hastwo hidden layers. The first two rows (white background) shows modularity scores for the first hidden layer h1, and the last tworows (gray background) shows h2.",
  "[width=]image4.png": ": Number of clusters in P versus regularization, split by layer. Layout is identical to Figure S3. Gray shading in thebackground shows 1, 2, and 3 quantiles of number of clusters in untrained (randomly initialized) networks. Note that, for themost part, training has little impact on the number of clusters detected, suggesting that consistently finding on the order of 2-6clusters is more a property of the MNIST dataset itself than of training. We computed the number of clusters using equation (10).This measure is sensitive to both the number and relative size of the clusters.",
  "[width=]image5.png": ": Further breakdown of cluster-similarity by regularization strength (increasing left to right) and type (L2/L1/dropout).Results in reflect an average of the results shown here. The six rows of this figure should be read in groups of two rows: ineach group, the top row shows the similarity scores (averaged over layers and runs), and the bottom row shows the difference tountrained models. A number of features are noteworthy here: (i) at low values of all three types of regularization, there is littlecluster- similarity within the upstream methods, but it becomes very strong at as regularization strength grows; (ii) at the highestvalues of L2 and L1 regularization, the pattern inside the 4x4 block of downstream methods changes to depend more strongly onnormalization; (iii) a moderate amount of agreement between upstream and downstream methods is seen for large L1 regularizationstrength, but curiously only for unnormalized downstream methods."
}