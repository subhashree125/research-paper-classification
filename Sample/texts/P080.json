{
  "Abstract": "Large language models (LLMs) represent a promising, but controversial, tool inaiding scientific peer review. This study evaluates the usefulness of LLMs in a con-ference setting as a tool for vetting paper submissions against submission standards.We conduct an experiment where 234 papers were voluntarily submitted to an201cLLM- based Checklist Assistant.201d This assistant validates whether papersadhere to the author checklist, which includes questions to ensure compliance withresearch and manuscript preparation standards. Evaluation of the assistant by paperauthors suggests that the LLM-based assistant was generally helpful in verifyingchecklist completion. In post-usage surveys, over 70",
  "Introduction": "Recent advancements in large language models (LLMs) have significantly enhanced their capabilitiesin areas such as question answering and text generation. One promising application of LLMsis in aiding the scientific peer-review process. However, the idea of using LLMs in peer reviewis contentious and fraught with potential issues. LLMs can hallucinate, exhibit biases, and maycompromise the fairness of the peer-review process. Despite these potential issues, LLMs may serveas useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuraciesthat need addressing. In this study, we take the first steps towards harnessing the power of LLMs in the application ofconference peer review. We conduct an experiment at a premier conference in the field of machinelearning. While the wider ethical implications and appropriate use cases of LLMs remain unclear andmust be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case:vetting paper submissions against submission standards, with results shown only to the authors. Specifically, the peer-review process requires authors to submit a checklist appended to theirmanuscripts. Such author checklists, utilized in as well as in other peer-review venues, containa set of questions designed to ensure that authors follow appropriate research and manuscript prepa-ration practices. The Paper Checklist is a series of yes/no questions that help authors check if theirwork meets reproducibility, transparency, and ethical research standards expected for papers. Thechecklist is a critical component in maintaining standards of research presented at the conference.Adhering to the guidelines outlined by these checklists helps authors avoid mistakes that could leadto rejection during peer review. We deploy and evaluate a Checklist Assistant powered by LLMs. This assistant scrutinizes au-thors2019 responses to the checklist, proposing enhancements for submissions to meet the confer-ence2019s requirements. To prevent any potential bias in the review process, we confine its usageexclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We thensystematically evaluate the benefits and risks of LLMs by conducting a structured study to understandif LLMs can enhance research quality and improve efficiency by helping authors understand if theirwork meets research standards. Specifically, we administered surveys both before and after use ofthe Checklist Assistant asking authors about their expectations for and perceptions of the tool. We",
  "Authors 2019 expectations of the assistant 2019s effectiveness were even more positivebefore using it than their assessments after actually using it (.1.3)": "Among the main issues reported by authors in qualitative feedback, the most frequently citedwere inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements(14/52 respon- dents) (.1.4). (2) While changes in paper submissions cannot be causally attributed to use of the checklist verifi-cation assistant, we find qualitative evidence that the checklist review meaningfully helped someauthors to improve their submissions. Analysis of the content of LLM feedback to authors indicates that the LLM providedgranular feedback to authors, generally giving 4-6 distinct and specific points of feedbackper question across the 15 questions (.2.1). Survey responses reflect that some authors made meaningful changes to their submissions201435 survey respondents described specific modifications they would make to theirsubmissions in response to the Checklist Assistant (.2.2). In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for80 total paper submissions.) Between these two submissions, authors tended to increase thelength of their checklist justifications significantly, suggesting that they may have addedcontent in response to LLM feedback (.2.3). Finally, we investigate how LLM-based tools can be easily manipulated 2013 specifically, we findthat with AI- assisted re-writing of the justifications, an adversarial author can make the ChecklistAssistant significantly more lenient (.1). In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significantpotential of LLMs to enhance scientific workflows 2014 whether by serving as direct assistants toauthors or helping journals and conferences verify guideline compliance. However, our findings alsounderscore that LLMs cannot fully replace human expertise in these contexts. A notable portion ofusers encountered inaccuracies, and the models were also vulnerable to adversarial manipulation.",
  "The Author Checklist": "We provide below the checklist questions used in submission template. We provide only the questionshere and give the full version including guidelines in Appendix A. These questions are designedby organizers, not specifically for this study, and questions are carried over from previous years.The authors had to provide a response to each question, comprising 201cYes,201d 2018No201d or201cNA201d (Not Applicable), along with a justification for their answer.",
  "Theory Assumptions and Proofs: For each theoretical result, does the paper provide the full set ofassumptions and a complete (and correct) proof?": "Experimental Result Reproducibility: Does the paper fully disclose all the information needed toreproduce the main experimental results of the paper to the extent that it affects the main claimsand/or conclusions of the paper (regardless of whether the code and data are provided or not)? Open access to data and code: Does the paper provide open access to the data and code, with sufficientinstructions to faithfully reproduce the main experimental results, as described in supplementalmaterial? Experimental Setting/Details: Does the paper specify all the training and test details (e.g., data splits,hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",
  "Broader Impacts: Does the paper discuss both potential positive societal impacts and negative societalimpacts of the work performed?": "Safeguards: Does the paper describe safeguards that have been put in place for responsible release ofdata or models that have a high risk for misuse (e.g., pretrained language models, image generators,or scraped datasets)? Licenses for existing assets: Are the creators or original owners of assets (e.g., code, data, models),used in the paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?",
  "New Assets: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?": "Crowdsourcing and Research with Human Subjects: For crowdsourcing experiments and researchwith human subjects, does the paper include the full text of instructions given to participants andscreen- shots, if applicable, as well as details about compensation (if any)? Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub- jects:Does the paper describe potential risks incurred by study participants, whether such risks weredisclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalentapproval/review based on the requirements of your country or institution) were obtained?",
  "Related work": "Language models have been used in the scientific peer review process for over a decade. The primaryapplication so far has been in assigning reviewers to papers. Here, a language model first computesa 201csimilarity score 201d between every reviewer-paper pair, based on the text of the submittedpaper and the text of the reviewer2019s previously published papers. A higher value of the similarityscore indicates that the language model considers this reviewer to have a higher expertise for thispaper. Given these similarity scores, reviewers are then assigned to papers using an optimizationroutine that maximizes the similarity scores of the assigned reviewer-paper pairs. There have been recent works that design or use LLMs to write the entire review of papers. Theoutcome measures for evaluating the effectiveness of the LLM- generated reviews are based onratings sourced from authors or other researchers. It is not entirely clear how these ratings translate tomeeting the objectives of peer review in practice namely that of identifying errors, choosing betterpapers, and providing useful feedback to authors. Moreover, it is also known that evaluation ofpeer reviews themselves are fraught with biases, and the aggregate effect of such biases on theseevaluations of reviews is not clear. Our work focuses on a more concrete task in reviewing papersthan generating an end-to-end review, namely validating that papers meet criteria specified in an",
  "Author Checklist. Moreover, we evaluate the efficacy of LLMs in the setting of an actual peer reviewconference": "Recent work also investigates whether LLMs can identify errors in papers and shows promisinginitial results. The paper constructs a set of short papers with deliberately inserted errors and asksLLMs to identify errors. GPT-4 does identify the error more than half the time. Another experimentdescribed asks GPT-4 to identify deliberately inserted errors in three full papers. It successfully andconsis- tently does so on one paper, partially and occasionally on a second paper, and is consistentlyunsuccessful on the third. Note that in both experiments, the prompts specifically asked the LLM tofind errors rather than generically asking the LLM to review the paper. Moreover, both experimentshad small sample sizes in terms of the number of papers. In another set of experiments presented,evaluated the ability of large language models (LLMs) to compare the 201cstrength201d of resultsbetween papers, mirroring the goals of conferences and journals in selecting 2018better 2019 papers.The experiment consisted of creating 10 pairs of abstracts, where one abstract in each pair wasmade 2018 clearly 2019 and objectively stronger than the other. To simulate diverse, yet irrelevantconditions, the language of the abstracts was deliberately varied. In this test, GPT-4 performed nobetter than random chance in identifying the stronger abstract, underscoring that while LLMs mayexcel at some complex tasks like scientific error identification, they often struggle with seeminglysimpler tasks. The papers investigate the performance of LLMs in evaluating checklist compliance. These studies,however, were retrospective studies of published papers, whereas our work is deployed live associatedto a peer-review venue and helps authors improve their checklist compliance before they make theirsubmission. Recent work has highlighted the prevalence of the use of LLMs both in preparation of scientific papermanuscripts and in the generation of scientific peer reviews. For example, estimates that as of January2024, 17.5",
  "Methodology": "We design an LLM-based tool (Checklist Assistant) to assist authors in ensuring their submittedchecklists are thoroughly answered. Our platform interfaced with a third-party LLM (GPT-4 fromOpenAI), using simple prompt engineering with these hyper-parameters: temperature = 1, topp = 1,and n = 1. For each checklist question, the LLM is provided with the author2019s checklist responseand justification, alongside the complete paper and any appendices. The LLM2019s role is to assessthe accuracy and thoroughness of each response and justification, offering targeted suggestions forimprovement. Each checklist item is treated as an individual task, i.e., an API call with only onequestion, its answer and justification by the author, and the paper and appendices. The API callreturns a review and score for the submitted question. illustrates examples of feedback provided by the Checklist Assistant for two different papers.In these examples, green indicates that the tool found 201cno significant concerns201d, while orangesignals 201cneeds improvement201d with the Paper Checklist standards. Authors are encouraged tocarefully review any orange feedback, validate the identified issues, and make the necessary revisionsto align with the checklist requirements.",
  "Deployment": "We deployed the Checklist Assistant on Codabench.org. We configured 15 Google Cloud CPUworkers, integrated with Codabench, to handle multiple paper submissions concurrently. The bulk ofthe computations were carried out by the LLM third-party software (GPT-4 from OpenAI) via APIcalls (one call per question, and additional calls in case of failure). Participation was fully voluntary, and participants were recruited through a blog post that was released8 days before the abstract submission deadline. Interested participants were asked to register thougha Google form. Participants who submitted registration requests through the Google form were thengiven access to the Assistant on the Codabench platform. The submissions were entirely optional andcompletely separate from the paper submission system and the review process. The papers had to beformatted as specified in the call for papers (complete with appendices and checklist). Informationprovided in external links was not taken into account by the assistant. We asked submitters to fill out",
  "Result Compilation: LLM responses were combined for all questions and formatted in an HTMLdocument with proper colors and structure for readability and user-friendliness": "We encountered several parsing issues with both paper texts and checklists. Initially, our parserstruggled with subsections and titles, prompting code improvements to handle sections accurately.Checklist parsing also faced issues due to spacing and incomplete checklists, which we addressed byrefining the code. Special characters, especially merged letters like 201cfi 201d and 201cfl 201d inthe submitted PDFs required further parsing updates.",
  "In this section we discuss design of a prompt given to the LLM, tasked to behave as ChecklistAssistant. We provide the full prompt in Appendix B": "While preparing the Checklist Assistant, we experimented with various prompt styles. Tuning wascarried out using a dozen papers. Some checklists were filled out with our best effort to be correct,and others included deliberately planted errors to verify robustness and calibrate the scores. Weobserved that the LLM performed better with clear, step-by-step instructions. Our final prompt provided a sequence of instructions covering different aspects of the requiredreview, designed as follows: first, the context is set by indicating that the paper is under review forthe conference. Next, the main goal is clarified, specifying that the LLM2019s primary task is toassist the author in responding to the checklist question. The LLM is then directed to review theauthor2019s answer and justification, identifying any discrepancies with the paper based on thespecific guidelines of the question. It is instructed to provide itemized, actionable feedback accordingto the guidelines, offering suggestions for improvement, with clear examples for responses such as201cYes, 201d 201cNo, 201d or 201cNA. 201d At the end of the review, the LLM is asked to assigna score: Score=1 for no issues, Score=0.5 for minor improvements, and Score=0 for critical issues.Finally, the LLM is provided with the checklist question, the author 2019s answer, justification, therelevant guidelines, and the paper content. Before prompt adjustments, LLM responses often mixed the review with the score. To fix this, wespecified that the score should be returned on a separate line at the end of the review. For long papersexceeding 35 pages (or 15,000 words), we processed only the first 15,000 words and notified authorswith a warning. We hypothesized that users might find the LLM responses overly strict, vague, and lengthy (whichwas indeed later confirmed), so we added prompt instructions like 201cuse 0 score sparingly 201d,201cprovide itemized, actionable feedback 201d, and 201cfocus on significant improvements. 201dAlthough the Checklist Assistant returned scores of 0, 0.5, and 1, we combined the 0 and 0.5 scoresto indicate that improvement was needed, rather than differentiating between two levels of severity(with red for 0 and orange for 0.5). This decision was made due to concerns that the LLM 2019sevaluations might be too harsh. User feedback on LLM strictness and other issues is analyzed in. We also tested whether the LLM was consistent in generating answers for reiterations of the sameinput. As a sanity check, we test for each question, whether the variation of the output scores formultiple runs on the same paper is comparable to the variation across papers. We find that thevariation in scores for multiple runs on the same paper is significantly lower than variation acrosspapers (p < 0.05; based on a one sided permutation test after BH correction) for all but one question.The only question that had a comparable variance within and across papers was the question on ethics(Q9; p > 0.4).",
  "Anonymity, confidentiality, and consent": "The authors could retain their anonymity by registering to Codabench with an email that did notreveal their identity, and by submitting anonymized papers. The papers and LLM outputs werekept confidential and were not be accessible to reviewers, meta reviewers, and program chairs. It isimportant to note that while authors retained ownership of their submissions, the papers were sent tothe API of an LLM service, and treated under their conditions of confidentiality.",
  "(2) Does the use of an Author Checklist Assistant meaningfully help authors to improve their papersub- missions?": "In order to understand author experience using the provided Author Checklist Assistant, we surveyedauthors before and after submitting to the Author Checklist Assistant. Additionally, we analyzed thecontent and submission patterns of author 2019s checklists and the LLM responses. A summary of ourmain findings is given in . In this subsequent section we provide detailed analyses of surveyresponses and usage of the Checklist Assistant. In .1, we give results on author perceptionand experience and in .2 we analyze changes made by authors to their submissions afterusing the Author Checklist Assistant.",
  "Author Perception and Experience": "First, we analyze the authors 2019 usage patterns and perceptions of the Author Checklist Assistant,as captured through surveys. In .1.1, we provide an overview of how authors filled out thechecklist and the responses given by the LLM on their checklists. In .1.2, we detail thesurvey methodology used to understand author experience and in .1.3, we analyze results ofthe survey. Finally, in .1.4, we overview the main challenges identified by authors whenusing the Author Checklist Assistant.",
  "Overview of Checklist Usage and Responses": "A total of 234 papers, each accompanied by a checklist, were submitted to the assistant. For eachchecklist question, authors could respond with Yes, No, NA, or TODO. As illustrated in a, most questions received a Yes response, indicating that the authors confirmed their paper metthe corresponding checklist criteria. However, for the questions on Theory, Impacts, Safeguards,Documentation, Human Subjects, and Risks, a significant portion of authors selected NA. Additionally,a notable number of authors responded No to the questions on Code and Data, and Error Bars. In response to the authors 2019 checklists, the LLM provided written feedback, with green indicating2018No Concerns 2019 and orange indicating 2018Needs improvement 2019. b illustratesthe distribution of LLM feedback for each checklist question. For most questions, the majority offeedback suggested that the checklist or manuscript could be improved. However, for the questionson Theory, Human Subjects, and Risks, many NA responses were deemed appropriate, leading theLLM to respond with 2019No Concerns. 2019 This likely reflects the LLM 2019s confidence inconfirming that certain papers did not include theory, human subjects research, or clear broader risks,making those checklist items irrelevant. In , we show the distribution of LLM evaluationsper submission. All submissions received several 2018Needs improvement 2019 ratings, with eachbeing advised to improve on 8 to 13 out of the 15 checklist questions.",
  "Survey Methodology": "To assess authors 2019 perceptions of the usefulness of the Author Checklist Assistant, we conducteda survey with all participants both at registration (pre-usage) and immediately after using the AuthorChecklist Assistant (post-usage). We provide the content of the surveys in . Both surveyscontained the same four questions, with the pre-usage survey focusing on expectations and the post-usage survey on actual experience. Responses were recorded on a four-point Likert scale, rangingfrom strongly disagree to strongly agree. In the post-usage survey, we also asked authors to providefreeform feedback on (1) any changes they planned to make to their paper, and (2) any issues theyencountered while using the Checklist Assistant. We received 539 responses to the pre-usage survey and 234 papers submitted. However, we receivedonly 78 responses to the post-usage survey, representing 63 unique participants (due to multiplesubmissions for the same paper). While completing the pre-registration survey was mandatory for allparticipants, the post-usage survey was optional. As a result, all participants in the post-usage surveyhad also completed the pre-registration survey.",
  "Survey Responses": "presents the survey responses collected before and after using the checklist verification tool.We include responses from authors who completed both surveys (n=63). In cases where authorssubmitted the survey multiple times for the same paper, we included only the earliest post-usageresponse. Including the duplicated responses made a negligible difference, with the proportion ofpositive responses changing by less than 0.02 across all questions.",
  "Overall, the majority of authors responded positively regarding their experience with the ChecklistAssis- tant. 70": "It is notable that authors were even more positive before using the tool. Comparing pre- and post-usage responses, there was a statistically significant drop in positive feedback on the 201cUseful 201dand 201cExcited to Use 201d questions 2014we run a permutation test with 50,0000 permutations totest whether the difference between proportion of positive responses pre and post-usage is non-zero,which gives Benjamini-Hochberg adjusted p-values of 0.007 and 0.013 for 201cExcited to Use 201dand 201cUseful 201d respectively with effect sizes of 22120.23 and 22120.2. We also assessed the correlation between post-usage survey responses and the number of 2018needsimprove- ment 2019 scores given by the LLM to authors. In , we show mean number ofneeds improvement scores for authors responding positively or negatively to each survey question.We find no substantial ef- fect of number of 2018needs improvement 2019 scores on survey responses.This may reflect that the number of 2018needs improvement 2019 scores was less important in author2019s perception than the written content of the LLM 2019s evaluation. Finally, we examined potential selection bias due to the drop-off in participation in the post-usagesurvey by analyzing the pre-usage survey responses across different groups. As noted earlier, onlya portion of the 539 participants who completed the pre-usage survey went on to submit papers(234 Submitters), and an even smaller group responded to the post-usage survey (78 Post-UsageRespondents). In , we compare the pre-usage survey responses between Submitters andNon-Submitters, as well as between Post- Usage Respondents and Non-Respondents. No substantialdifferences in rates of positive responses were found (using a permutation test for the difference inmean response, gave p-values of > 0.3 for all questions before multiple testing correction), suggestingthere is no significant selection bias.",
  "Challenges in Usage": "In addition to the structured survey responses, 52 out of the 78 post-usage survey submissionsincluded freeform feedback detailing issues with the Checklist Assistant 2019s usage. We manuallycategorized the reported issues from these responses and identified the following primary concerns,listed in order of decreasing frequency (summarized in ): Inaccurate: 20 authors reported that the LLM was inaccurate. Note that it is not possible to tell fromthe responses how many inaccuracies participants found in individual questions since the survey didnot ask about individual checklist questions. Many participants noted specific issues, in particularthat the LLM overlooked content in the paper, requesting changes to either the checklist or the paper for elements that the authors believed were already addressed. Additionally, some authors reportedmore nuanced accuracy issues. For instance, one author mentioned that the LLM misinterpreted a201cthought experiment 201d as a real experiment and incorrectly asked for more details about theexperimental setup. Another author reported that the LLM mistakenly assumed human subjects wereinvolved due to a discussion of 201cinterpretability 201d in the paper.",
  "Changes to Submissions in Response to Feedback": "In the following analysis, we integrate an assessment of the LLM 2019s feedback with the authors2019 checklist answers, to better understand whether the Checklist Assistant helped authors makeconcrete and meaningful changes to their papers. In .2.1, we analyze the types of feedbackgiven by the LLM to authors. In .2.2, we overview the changes to their papers that authorsself-reported making in survey responses. Lastly, in .2.3, we analyze changes made inmultiple submissions of the same paper to the Author Checklist Assistant.",
  "Characterization of LLM Feedback by Question": "For authors to make meaningful changes to their papers, the Author Checklist Assistant must provideconcrete feedback. In this section, we analyze the type of feedback given by the Checklist Assistantto determine whether it is specific to the checklist answers or more generic. Given the large volume of feedback, we employed an LLM to extract key points from the ChecklistAssistant 2019s responses for each question on the paper checklist and to cluster these points intooverarching categories. Specifically, for each of the 15 questions across the 234 checklist submissions,we used GPT-4 to identify the main points of feedback provided to authors. We manually inspectedthat the main points extracted by GPT-4 matched the long-form feedback on 10 randomly selectedsubmitted paper checklists and found that GPT-4 was highly accurate in extracting these key feedbackpoints. We then passed the names and descriptions of these feedback points to GPT-4 to hierarchicallycluster them into broader themes.",
  "The LLM tended to provide 4-6 distinct points of feedback per question (for each of the 15 questions)": "The LLM is capable of giving concrete and specific feedback for many questions. For example, onthe 201cClaims 201d question, the LLM commented on consistency and precision in documentingclaims on 50 papers, including feedback like matching the abstract and introduction and referencingappendices. On the 201cCompute resources 201d question the LLM commented specifically ondetailing compute / execution time of methods.",
  "There are certain topics that appear across many questions, in particular discussion of limitations andimproved documentation": "The LLM often expands the scope of checklist questions. For example, the LLM brings up repro-ducibility as a concern in feedback to the code of ethics question and brings up anonymity quitefrequently in the code and data accessibility question. We provide a full list of the summarized main themes of feedback in Appendix C. In summary, ouranalysis of the feedback given by the LLM suggests that the LLM gave concrete and actionablefeedback to authors that they could potentially use to modify their paper submissions. Our analysisalso suggests that a more detailed checklist could be developed to provide more granular feedback,based on the rubrics covered by the Author Checklist Assistant. Such a detailed checklist could beprocessed automatically by an LLM to systematically identify specific, commonly overlooked issuesin scientific papers and flag concrete issues for authors to resolve.",
  "Authors2019 Descriptions of Submission Changes": "We obtain additional evidence of changes made by authors in response to the Checklist Assistantthrough the post-usage survey. In the survey, we asked authors to detail in freeform feedback anychanges they had made or planned to make in responses to feedback from the LLM. Of the 78survey responses, 45 provided feedback to this question. Of these 45 responses, 35 actually describedchanges they would make (the remainder used this freeform feedback to describe issues that they hadin using the assistant). Based on manual coding of the comments, we identified the main themes inchanges they planned to make:",
  "Analysis of Re-submissions": "Finally, we analyze changes made between submissions to the Checklist Assistant when authorssubmitted multiple times. There were 40 instances where an author submitted the same paper tothe checklist verification multiple times (out of 184 total distinct paper submissions to the checklistverification). In this analysis, we assess changes made to the paper checklist between the first andsecond submission to our checklist verifier in order to understand whether authors made substantivechanges to their checklists and/or paper manuscripts in response to feedback from the checklistverification."
}