{
  "Abstract": "This document outlines our contribution to the ActivityNet Challenge, focusing onactive speaker detection. We employ a 3D convolutional neural network (CNN)for feature extraction, combined with an ensemble of temporal convolution andLSTM classifiers to determine whether a person who is visible is also speaking.The results demonstrate substantial improvements compared to the establishedbaseline on the AVA-ActiveSpeaker dataset.",
  "Introduction": "The field of multimodal speech perception has garnered significant attention in recent times, withmajor advancements in audio-visual methodologies facilitated by deep learning. The capacity toidentify which individuals are speaking at any moment is crucial for a variety of applications. Theintroduction of the AVA-ActiveSpeaker dataset has been a significant development, allowing for thetraining of deep-learning-based active speaker detection (ASD) models with complete supervision.This document provides a concise analysis of this dataset and elaborates on the methodology behindour submission to the challenge.",
  "Train1202,676KVal33768KTest1092,054K": "This dataset presents several challenges. The durations of speaking segments are notably brief, withan average of 1.11 seconds for segments that are both spoken and audible. Consequently, the systemneeds to deliver precise detection with a limited number of frames. Traditional methods, whichdepend on smoothing the output over a time window of several seconds, are not effective under theseconditions. Additionally, the dataset includes many older videos where the audio and video recordings appear tohave been captured separately or are significantly out of sync. As a result, the temporal alignmentbetween audio and visual speech representations is not a reliable indicator of a persons speakingstatus.",
  "Front-end architecture": "For the extraction of audio and video representations, pre-trained networks are employed. Theseencoder networks have undergone training for the audio-visual correspondence task through a self-supervised approach on unlabeled videos. The video encoder utilizes a convolutional neural network (CNN), processing 5 RGB image framesto produce a 512-dimensional representation. The architecture draws inspiration from the VGG-Mnetwork, known for its compactness and efficiency, but incorporates a 3D convolution in the initiallayer instead of the conventional 2D convolution. The audio encoder receives an input comprising 20 frames in the temporal dimension and 13 cepstralcoefficients in the other, generating a 512-dimensional representation that aligns with the videorepresentations embedding space.",
  "Back-end architecture": "Both the audio and video encoders process an input of 5 video frames (equivalent to 0.2 seconds),advancing 1 video frame (0.04 seconds) at a time. Consequently, for an input of T frames, the outputdimensions are 512 x (T - 4). In this study, two straightforward back-end classifiers are evaluated.Although our experiments utilize T = 9, no significant performance variations were noted for T valueswithin the range of 7 to 15. LSTM classifier. The audio and video representations are channeled into two distinct bi-directionalLSTM networks, each comprising 2 layers with a hidden size of 128. The outputs from these networksare merged and subsequently processed through a linear classification layer. This layer determineswhether the individual is speaking, and it is trained using the softmax cross-entropy loss. TC classifier. In place of LSTM layers, the encoder outputs are directed to two temporal convolutionlayers, each equipped with 128 filters. The outputs are similarly concatenated and forwarded to theclassifier, mirroring the approach used with the LSTM classifier. Ensemble. Ensemble methods in machine learning have been demonstrated to frequently surpassthe performance of any individual classifier. In this approach, the predictions generated by both theLSTM and TC classifiers are averaged with equal weighting to produce the final prediction.",
  "Experiments": "Our model, implemented using the PyTorch library, was trained on a single Tesla M40 card with24GB of memory. Training utilized the ADAM optimizer with default settings and a fixed learningrate of 10-2. To counteract any bias in the training data, the number of samples for positive andnegative classes was balanced within each mini-batch during the training process.",
  "The evaluation metric for this task is the mean Average Precision (mAP), with the evaluation codesupplied by the challenge organizers": "Results on the validation set for the various back-end classifiers are presented in . The bestmodel achieved an mAP of 0.878 on the sequestered test set for the challenge. In contrast, theGRU-based baseline model yielded an mAP of 0.821. The qualitative outcomes of the proposed method significantly surpass those of existingcorrespondence-based methods on this dataset because it does not depend on accurate audio-to-video synchronization."
}