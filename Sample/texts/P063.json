{
  "Abstract": "Deep neural networks, which are built from multiple layers with hierarchicaldistributed representations, tend to learn low-level features in their initial layersand shift to high-level features in subsequent layers. Transfer learning, multi-tasklearning, and continual learning paradigms leverage this hierarchical distributedrepresentation to share knowledge across different datasets and tasks. This paperstudies the layer-wise transferability of representations in deep networks acrossseveral datasets and tasks, noting interesting empirical observations.",
  "Introduction": "Deep networks, constructed with multiple layers and hierarchical distributed representations, learnlow-level features in initial layers and shift to high-level features as the network becomes deeper.Generic hierarchical distributed representations allow for the sharing of knowledge across datasetsand tasks in paradigms such as transfer learning, multi-task learning, and continual learning. Intransfer learning, for example, the transfer of low-level features from one dataset to another canboost performance on the target task when data is limited, provided that the datasets are related.Transferring high-level features, with the learning of low-level features, can also be useful when thetasks are similar but the data distributions differ slightly. This paper studies the layer-wise transferability of representations in deep networks across severaldatasets and tasks, and reports some interesting observations. First, we demonstrate that the layer-wisetransferability between datasets or tasks can be non-symmetric, with features learned from a sourcedataset being more relevant to a target dataset, despite similar sizes. Secondly, the characteristics ofthe datasets or tasks and their relationship have a greater effect on the layer-wise transferability ofrepresentations than factors such as the network architecture. Third, we propose that the layer-wisetransferability of representations can be a proxy for measuring task relatedness. These observationsemphasize the importance of curriculum methods and structured approaches to designing systemsfor multiple tasks that maximize knowledge transfer and minimize interference between datasets ortasks.",
  "Methods": "We have produced a citation graph using citation data from NeurIPS papers from SemanticScholar,and institutional information about authors from AMiner. From the NeurIPS website, we first gatheredall paper titles from 2012 to 2021. We then mapped the paper titles to their Semantic Scholar paperIDs using the Semantic Scholar Academic Graph (S2AG) API. Unmatched papers were manuallysearched for, with all but one being found in the Semantic Scholar database. For each paper, we usedthe S2AG API to identify authors, and the authors of their references.",
  "Results": "Our results show how American and Chinese papers fail to cite each other. While 60% of the data setcomes from American papers, they only compose 34% of Chinese citations. American citations ofChinese papers are even more dramatic, with the 34% of the dataset coming from Chinese papers onlyaccounting for 9% of American citations. These numbers are even more significant when comparedto American citations of European papers; we found that American institutions cite European papersmore often than Chinese papers despite our dataset containing six times more Chinese papers thanEuropean. Each region tends to cite its own papers more often: China 21%, the USA 41%, and Europe 14%.The separation between American and Chinese research is more pronounced than would be expectedbased solely on regional preference. American and European research communities demonstratesimilar citation patterns with few citations to Chinese papers. Chinese institutions, on the other hand,cite both American and European papers less than either of those regions.",
  "Limitations": "The results presented here have some limitations. Firstly, while we have labeled the work of anyuniversity located in the United States as American, it is possible that such labs still have close ties toChina, leading to an underestimate of the divide between US and Chinese AI research. Secondly, wehave excluded papers where author information was not available on AMiner, a Chinese company,and therefore, there could be more Chinese papers in our dataset than we have determined. The 43%of discarded papers due to missing author information also likely represent a biased sample.",
  "Consequences": "While American and Chinese researchers publish in the same venues, they represent two parallelcommunities with limited impact on each others research. This can, partly, be attributed to differingresearch interests arising from distinct cultural norms that influence research priorities. For instance,multi-object tracking is an active area of research in China with large scale benchmarks, whereas,concerns surrounding misuse of biometric data in North America have led researchers there to avoidsuch research. Likewise, US researchers are heavily represented at conferences regarding fairness inAI, while the Chinese are not. This separation impacts not only the research topics, but also how they evolve. In addition, abstracttopics or architectures that are popular in one region may not be popular in the other. For example,PCANet which is a popular image classification architecture has most of its 1200 citations from EastAsian institutions, while Deep Forests has most of its 600 citations from Chinese institutions.",
  "Another limitation is related to differences in the approach to ethics. The North American and Euro-pean AI communities have begun to publish research on the ethics of AI and have included systems": "for reviewers to flag ethical concerns and ask authors to provide ethics statements. Engagementwith Chinese researchers in this topic remains limited, even though ethics statements from ChineseAI institutions show many similarities to western ones. A clear example of this disconnect is theProvisional Draft of the NeurIPS Code of Ethics where, at the time of initial publication, all theauthors were based in the US or Australia, but none were based in Asia. Although similar statementsexist across regions, disagreements in research practice still arise. One such example is where DukeUniversity stopped using the Duke-MTMC dataset because researchers had not obtained consentfrom the students they collected images from, yet similar datasets like Market-1501 from Chinacontinue to be used. The divide between these two communities impacts individual researchers, the machine learningcommunity as a whole, and potentially the societies impacted by AI research, highlighting the needfor a discussion to overcome this barrier."
}