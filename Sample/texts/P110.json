{
  "Abstract": "Dialogue systems are highly dependent on the quality of the data used to train them. It is therefore important todevelop good dialogue annotation tools which can improve the speed and quality of dialogue data annotation.With this in mind, we introduce LIDA, an annotation tool designed specifically for conversation data. As far as weknow, LIDA is the first dialogue annotation system that handles the entire dialogue annotation pipeline from rawtext, as may be the output of transcription services, to structured conversation data. Furthermore it supports theintegration of arbitrary machine learning models as annotation recommenders and also has a dedicated interface toresolve inter-annotator disagreements such as after crowdsourcing annotations for a dataset. LIDA is fully opensource, documented and publicly available.",
  "Introduction": "Dialogue systems are becoming one of the most active research areas in Natural Language Processing (NLP) and Machine Learning(ML). Creating a high-quality dialogue dataset incurs a large annotation cost, which makes good dialogue annotation tools essentialto ensure the highest possible quality. Many annotation tools exist for a range of NLP tasks but none are designed specifically fordialogue with modern usability principles in mind. LIDA is a web application designed to make dialogue dataset creation and annotation as easy and fast as possible. In addition tofollowing modern principles of usability, LIDA integrates best practices from other state-of-the-art annotation tools, most importantlyby allowing arbitrary ML models to be integrated as annotation recommenders to suggest annotations for data. Any system with thecorrect API can be integrated into LIDAs back end, meaning LIDA can be used as a front end for researchers to interact with theirdialogue systems and correct their responses, then save the interaction as a future test case. When data is crowdsourced, it is good practice to have multiple annotators label each piece of data to reduce noise and mislabelling.Once you have multiple annotations, it is important to be able to resolve conflicts by highlighting where annotators disagreed sothat an arbiter can decide on the correct annotation. To this end, LIDA provides a dedicated interface which automatically findswhere annotators have disagreed and displays the labels alongside a percentage of how many annotators selected each label, with themajority annotated labels selected by default.",
  "Related Work": "Various annotation tools have been developed for NLP tasks in recent years. compares LIDA with other recent annotationtools. TWIST is a dialogue annotation tool which consists of two stages: turn segmentation and content feature annotation. Turnsegmentation allows users to highlight and create new turn segments from raw text. After this, users can annotate sections of text ina segment by highlighting them and selecting from a predefined feature list. However, this tool doesnt allow users to specify customannotations or labels and doesnt support classification or slot-value annotation. INCEpTION is a semantic annotation platform for interactive tasks that require semantic resources like entity linking. It providesmachine learning models to suggest annotations and allows users to collect and model knowledge directly in the tool. GATE is an",
  "LIDAYESYESYESYESYESINCEpTIONNOYESNOYESYES/NOGATENOYESNONOYES/NOTWISTYESNOYESNONOBRATNOYESNOYESNODOCCANONOYESNONONODialogueViewYESYESYESNONO": "open source tool that provides predefined solutions for many text processing tasks. It is powerful because it allows annotators toenhance the provided annotation tools with their own Java code, making it easily extensible and provides an enormous number ofpredefined features. However, GATE is a large and complicated tool with a significant setup cost. Despite their large feature sets,INCEpTION and GATE are not designed for annotating dialogue and cannot display data as turns, an important feature for dialoguedatasets. BRAT and Doccano are web-based annotation tools for tasks such as text classification and sequence labeling. They have intuitiveand user-friendly interfaces which aim to make the creation of certain types of dataset such as classification or sequence labellingdatasets as fast as possible. BRAT also supports annotation suggestions by integrating ML models. However, like INCEpTION andGATE, they are not designed for annotating dialogues and do not support generation of formatted conversational data from a rawtext file such as may be output by a transcription service. LIDA aims to fill these gaps by providing a lightweight, easy-to-setupannotation tool which displays data as a series of dialogues, supports integration of arbitrary ML models as recommenders andsupports segmentation of raw text into dialogues and turns. DialogueView is a tool for dialogue annotation. However, the main use-cases are not focused on building dialogue systems, rather it isfocused on segmenting recorded conversations. It supports annotating audio files as well as discourse segmentation - hence, granularlabelling of the dialogue, recommenders, inter-annotator agreement, and slot-value labelling is not possible with DialogueView.",
  "System Overview": "LIDA is built according to a client-server architecture with the front end written in standard web languages (HTML/CSS/JavaScript)that will run on any browser. The back end written in Python using the Flask web framework as a RESTful API. The main screen which lists all available dialogues. The buttons below this list allow a user to add a blank or formatted dialoguefile. Users can also drag and drop files in this screen to upload them. The user is then able to add, delete or edit any particulardialogue. There is also a button to download the whole dataset as a JSON file on this page. Clicking on a dialogue will take users tothe individual dialogue annotation screen. LIDA uses the concept of a turn to organise how a dialogue is displayed and recorded. A turn consists of a query by the userfollowed by a response from the system, with an unlimited number of labels allowed for each user query. The user query andsystem response are displayed in the large area on the left of the interface, while the labels for each turn are shown in the scrollablebox on the right. There are two forms that these labels can currently take which are particularly relevant for dialogue: multilabelclassification and slot-value pair. An example of multilabel classification is whether the user was informing the system or requesting a piece of information. Anexample of a slot-value pair is whether the user mentioned the type of restaurant theyd like to eat at (slot: restaurant-type) and if sowhat it was (value: italian, for example). The front-end code is written in a modular form so that it is easy for researchers",
  "Experimenting with Dialogue Systems": "LIDA is designed with this in mind - a dialogue system can be integrated into the back end so that it will run whenever the userenters a new query in the front end. The user will then be able to evaluate whether the system gave the correct answer and correct thelabels it gets wrong using the front end. LIDA will record these corrections and allow the user to download the interaction with theirdialogue system with the corrected labels so that it can be used as a test case in future versions of the system.",
  "Creating a New Dialogue Dataset": "Users can create a blank dialogue on LIDAs home screen, then enter queries in the box shown at the bottom of the screen. Alongwith whole dialogue systems, arbitrary ML models can be added as recommenders in the back end. Once the user hits \"Enter\",the query is run through the recommender models in the back end and the suggested annotations displayed for the label. If norecommender is specified in the back end, the label will be left blank. Users can delete turns and navigate between them using",
  "Annotating An Existing Dataset": "Datasets can be uploaded via drag-and-drop to the home screen of the system, or paths can be specified in the back end if thesystem were being used for crowdsourcing. Datasets can be in one of two forms, either a \".txt\" file such as may be produced by atranscription service, or a formatted \".json\" file, a common format for dialogue data. Once the user has uploaded their data, theirdialogue(s) will appear on the home screen. The user can click on each dialogue and will be taken to the single dialogue annotationscreen to annotate it. If the user uploaded a text file, they will be taken to a dialogue and turn segmentation screen. Following thesame constraints imposed in previous works, this turn segmenter assumes that there are only two participants in the dialogue: theuser and the system, and that the user asks the first query. The user separates each utterance in the dialogue by a blank line, andseparates dialogues with a triple equals sign (\"===\"). Once the user clicks \"Done\", the text file will automatically be parsed into thecorrect JSON format and each query run through the recommenders in the back-end to obtain annotation suggestions.",
  "Resolving Annotator Disagreement": "Researchers could use LIDAs main interface to crowdsource annotations for a dialogue dataset. Once they have several annotationsfor each dialogue, they can upload these to the inter-annotator resolution interface of LIDA. The disagreements between annotatorswill be detected, with a percentage shown beside each label to show how many annotators selected it. The label with the highestpercentage of selections is checked by default. The arbiter can accept the majority label simply by pressing \"Enter\" and can changeerrors with the arrow keys to facilitate fast resolution. This interface also displays an averaged (over turns) version of Cohens Kappa,the total number of annotations, the total number of errors, and the averaged (over turns) accuracy.",
  "Features": "Specifying Custom Labels LIDAs configuration is controlled by a single script in the back end. This script defines which labelswill be displayed in the UI and is easy to extend. Users can define their own labels by altering this configuration script. If a userwishes to add a new label, all they need to do is specify the labels name, its type (classification or slot-value pair, currently) and thepossible values the classification can take. Alongside the label specification, they can also specify a recommender to use for the labelvalues. The label will then automatically be displayed in the front end. Note that labels in uploaded datasets will only be displayed ifthe label has an entry in the configuration file. Custom Recommenders When creating a dialogue dataset from scratch, LIDA is most powerful when used in conjunction withrecommenders which can suggest annotations for user queries to be corrected by the annotator. State-of-the-art tools emphasize theimportance of being able to use recommenders in annotation systems. Users can specify arbitrary ML models to use for each label inLIDAs back end. The back end is written in Python, the de facto language for machine learning, so researchers can directly integratemodels written in Python to the back end. This is in contrast to tools such as INCEpTION and GATE which are written in Javaand so require extra steps to integrate a Python-based model. To integrate a recommender, the user simply provides an instantiatedPython object in the configuration file that has a method called \"transform\" that takes a single string and returns a predicted label. Dialogue and Turn Segmentation from Raw Data When uploading a .txt file, users can segment each utterance and each dialoguewith a simple interface. This means that raw dialogue data with no labels, such as obtained from a transcription service, can beuploaded and processed into a labelled dialogue. Segmented dialogues and turns are automatically run through every recommenderto give suggested labels for each utterance.",
  "Evaluation": "To test LIDAs capabilities, we designed a simple experiment: we took a bespoke dataset of 154 dialogues with an average of 3.5turns per dialogue and a standard deviation of 1.55. The task was to assign three classification labels to each user utterance in eachdialogue. Each annotator was given a time limit of 1 hour and told to annotate as many dialogues as they could in that time. We hadsix annotators perform this task, three of whom were familiar with the system and three of whom had never seen it before. These annotators annotated an average of 79 dialogues in one hour with a standard deviation of 30, which corresponds to anaverage of 816.5 individual annotations. The annotators who had never seen the system before annotated an average of 60 dialoguescorresponding to an average of 617 individual annotations. Once we had these six annotations, we performed a second experiment whereby a single arbiter resolved inter-annotator disagree-ments. In one hour, the arbiter resolved 350 disagreements and noted that resolution."
}