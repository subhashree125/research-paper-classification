{
  "Abstract": "This study explores the application of Named Entity Recognition (NER) on anovel form of user-generated text, specifically call center conversations. Thesedialogues present unique challenges, blending the complexities of spontaneousspeech with issues specific to conversational Automatic Speech Recognition (ASR),such as inaccuracies. By employing a custom corpus with manual annotations,training contextual string embeddings, and implementing a BiLSTM-CRF model,we achieve results that are on par with the state-of-the-art for this new task.",
  "Introduction": "This paper addresses the crucial need to identify and handle sensitive personal information withincall center transcripts, which are generated as a result of speech recognition systems. Although thesetranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often containa callers name and internal ID number, which can be useful for quality assurance. However, newprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringentguidelines concerning data collection, storage, and an individuals right to withdraw consent fordata usage. To adhere to these regulations without losing the datas value, it is essential to pinpointnon-public personal and personally identifiable information (NPI/PII) in call transcripts. We utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,remove them, and replace them with appropriate tags that denote the type of removed data. Forinstance, a transcript such as \"This is john doe reference number 12345\" would be transformed into\"This is [NAME] reference number [NUMBER]\". This task is distinctive to call centers for severalreasons. First, these transcripts consist of natural human conversations, which have many commonproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,transcript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible toerrors, as will be described in .1. Even though modern ASR systems are usually reliable, thesource audio is from phone calls, which is often low quality and contains background noise. The pooraudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understandingthe call semantics and identifying features essential to NER systems more difficult. Moreover, calltranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucialfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,addresses, or spellings, which makes it difficult to use pre-trained NER models. In this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-CRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-mance on standard datasets by using our model with annotated data and custom contextual stringembeddings.",
  "Related Work": "Named Entity Recognition has become a focus in the field of Natural Language Processing (NLP),particularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003shared task in 2003 concentrated on language-independent NER and popularized feature basedsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research. Following the CoNLL task, Conditional Random Field (CRF) based models became the mostsuccessful, which requires that features be manually produced. Current research utilizes neuralnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-CNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similarresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.Embeddings have been used for both words and entity types to create more robust models. Flair, withcharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh usesFlair embeddings to address mishandled annotations. In 2006, the word confidence scores from ASR systems were used as a feature for NER. Similarexperiments were done on French radio and TV audio. Neither of those used natural conversation,and the quality of the audio was superior, making ASR a more accurate task.",
  ": An example of turns of a conversation, where each persons line in the dialogue representstheir turn. This output matches the format of our data described in": "The most similar research area to this is work on Twitter data. Similar to our transcripts, tweets areuser-generated and may not have conventional grammar or spelling. Initial research tackled thisproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-stepneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-generated Text (W-NUT). The success of pooled contextualized string embeddings was also shownwith this data. We use prior work on tweets to direct our model creation for call center data.",
  "Data": "Our dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample representsa complete speaker turn from a debt collection call center. A speaker turn is defined as a completetranscription from one speaker before another speaker starts, as shown in . The training set isa random sample of turns from 4 months of call transcripts. The transcripts were generated using aproprietary speech recognition system, which outputs all lowercase transcripts without punctuationor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letterand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizermodule was added, which defaults to this capitalization and period structure.",
  "We created a schema for annotating the training and validation data with different types of NPI/PII,which are shown in": "Initial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,and were instructed to err on the side of caution in unclear instances. Ambiguity often came fromerrors in the ASR model. The lack of audio meant it was sometimes unclear if \"I need oak leaves\"was actually \"Annie Oakley\". The opposite was also true such as when \"Brilliant and wendy jeff to",
  "Entity TypeDescription": "NUMBERSA sequence of numbers related to a customers information (e.g. phone numbers or internal ID number)NAMEFirst and last name of a customer or agentCOMPANYThe name of a companyADDRESSA complete address, including city, state, and zip codeEMAILAny email addressSPELLINGLanguage that clarifies the spelling of a word (e.g. \"c as in cat\")",
  ": A brief description of our annotation schema": "process the refund\" was actually \"Brilliant and when did you want to process the refund\". Emailswere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.Also, the transcripts were pre-redacted for PCI compliance. This redaction can obscure importantdata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. Tolessen false negatives, we use context to include the [redacted] tag as part of the numbers sequencewhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left forthe model to interpret. Due to limitations with spaCy and the complexity of nested entities, we only allowed one annotationper word in the dataset. This means, for instance, that \"c a t as in team at gmail dot com\" would belabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to theposition of words in the text. This ultimately results in a lower count of SPELLING entities, becausethese are often part of EMAIL or ADDRESS entities, which influences our analysis in .",
  "Model Design": "We utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wroteour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.After preprocessing, we trained the model on the training set and used the validation set for modeltuning. All numbers in this paper are reported on the test set. A visualization of our model is shownin .",
  "Basic Hyperparameter Tuning": "We used a grid search algorithm to maximize model performance. The word embedding layer usesFastText embeddings trained on the clients call transcripts. This aids in mitigating the impacts ofpoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:epochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,with 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), andthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters werea learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion ofbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128GB of memory. Each experiment took a few hours to run. To understand the performance of the model, we broke down the measurements of precision, recall,and F1 by entity type. shows these results for the best model configuration. This model used46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.",
  "Training Word Embeddings": "Most past research has fine-tuned existing word embeddings, but the task of mitigating misrecognitionseemed more complex than domain adaptation. To lessen the impact of the errors, we understand thatfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives amisrecognized word a vector similar to the word it should be and not to the other meaning it has. Theimportance of domain specific word embeddings when using ASR data has been shown in research. We ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Ourembeddings were trained on roughly 216 million words. The results from the best epoch of thismodel (16) are shown in .",
  "Using Flair": "Previous experiments highlighted the importance of custom word embeddings to account for mis-recognition in call center transcripts. Here, we test the performance of Flair and its contextual stringembeddings. We begin by training custom contextual string embeddings based on the results of the first experiments.We use the same corpus as in .1. The tutorial on the Flair GitHub page was used with thefollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use thenewline to indicate a document change, and each turn as a separate document for consistency. Themodels validation loss stabilized after epoch 4, and the best version of the model was used.",
  "Discussion": "shows that using custom embeddings is beneficial over using GloVe embeddings, with theexception of the EMAIL category. The Flair embeddings show a large improvement over other wordembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. Thebest performing Flair models are those that use both the custom contextualized string embeddingsand the custom FastText embeddings. Across all of the models in this paper, EMAIL and SPELLING consistently performed worse thanother categories. This is due to the overlap in their occurrences and their variable appearance. Thecustom embeddings model often identified parts of an email correctly but labeled some aspects, suchas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLINGoften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLINGentity had a limited presence in our training data, with many EMAIL and ADDRESS entitiescontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING andvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, whichwas poorly represented in training. The Flairmean pooling model outperforms the other models inEMAIL by a large margin. The results in highlight that the NUMBERS category contains strings that appear frequentlyin the text. There are a finite number of NUMBER words in our corpus (those numeric words alongwith many instances of \"[redacted]\"), and the numbers of interest in our dataset appear in very similarcontexts and do not often get misrecognized. The COMPANY entity performs well for similarreasons; when the model was able to identify the company name correctly, it was often in a commonerror form and in a known context. The models failures can be attributed to the training data becausethe company name is a proper noun that is not in standard ASR language models, including the onewe used. Thus, it is often misrecognized since the language model has higher probabilities assigned togrammatically correct phrases that have nothing to do with the company name. This causes variabilityin appearance, which means that not every version of the company name was present in our trainingset. Interesting variability also occurred in ADDRESS entities. Both models that used Flair and FastTextembeddings strongly outperformed the models that used only Flair, and standard Flair embeddingsstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identifiedaddresses in which numbers were shown as \"[redacted]\" but both models that utilized FastText hadno issue with these instances.",
  "Conclusion and Future Work": "Through the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achievestate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.We also show the importance of training word embeddings that fully capture the intricacies of thetask. Although we cannot release our data for privacy, we have shown that existing state-of-the-arttechniques can be applied to less common datasets and tasks. Future work will include evaluatingthe model with call transcripts from other industries. We would also like to explore how well thesetechniques work on other user-generated conversations like chats and emails."
}