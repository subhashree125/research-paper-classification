{
  "Abstract": "This study introduces a novel concept of training-free graph neural networks (TFGNNs) for transductive nodeclassification, which can function immediately without any training and can optionally be enhanced throughsubsequent training. Initially, we put forward the idea of using labels as features (LaF), a valid yet relativelyunexplored method in graph neural networks. Our analysis demonstrates that incorporating labels as featuressignificantly improves the representational capacity of GNNs. The design of TFGNNs is based on these findings.Empirical evaluations show that TFGNNs surpass current GNNs in scenarios where training is not performed, andwhen training is optionally applied, they achieve convergence much faster than conventional GNNs.",
  "Introduction": "Graph Neural Networks (GNNs) have gained prominence as effective models for handling graph-structured data. They havedemonstrated impressive performance across a range of tasks, including chemical structure analysis, question answering systems,and recommender systems. A common application for GNNs is transductive node classification. In this task, the objective is to infer the labels of specific nodeswithin a graph, given the labels of other nodes. This approach finds utility in various real-world scenarios, such as classifyingdocuments, analyzing e-commerce data, and studying social networks. Several GNN architectures, including Graph ConvolutionalNetworks (GCNs) and Graph Attention Networks (GATs), have successfully addressed transductive node classification, yieldingexcellent results. A significant hurdle in the practical application of GNNs is their computational demand. Real-world graphs, such as thoserepresenting social networks or the structure of the web, can be enormous, containing billions of nodes. Processing these massivegraphs can be computationally prohibitive. While various methods have been developed to enhance the efficiency of GNNs, such asnode and edge sampling techniques, these methods still necessitate numerous training iterations. Other approaches, like PinSAGE,utilize parallel training and importance pooling to accelerate the training process, but they demand substantial computationalresources. Consequently, the immediate deployment of GNNs with limited resources remains a challenge. In this work, we introduce the concept of training-free graph neural networks (TFGNNs). To realize TFGNNs, we first propose theinnovative idea of using labels as features (LaF). In the context of transductive node classification, utilizing node labels as features isa permissible approach. GNNs employing LaF can leverage label information, like the distribution of classes among neighboringnodes, to generate node embeddings. These embeddings are richer in information compared to those derived solely from nodefeatures. We establish that incorporating labels as features demonstrably augments the expressive capability of GNNs. TFGNNs possess the unique ability to operate without any training, enabling immediate deployment upon initialization. Thiseliminates the need for extensive hyperparameter tuning when used in training-free mode. Furthermore, TFGNNs can be refinedthrough optional training. Users have the flexibility to employ TFGNNs without training or to train them for a limited number ofiterations when computational resources are constrained. This adaptability is particularly valuable in online learning scenarios,where data arrives sequentially, and the model needs to be updated promptly. TFGNNs can also undergo full training when resourcesare plentiful or when higher accuracy is paramount. In essence, TFGNNs offer the advantages of both nonparametric models andtraditional GNNs.",
  "The primary contributions of this research are outlined below:": "* We propose the utilization of labels as features (LaF) in transductive learning settings. * We provide formal proof that LaF enhancesthe representational power of GNNs. * We introduce a novel architecture for training-free graph neural networks (TFGNNs). * Weempirically demonstrate that TFGNNs outperform existing GNNs in the absence of training.",
  "Notations": "For any positive integer n, [n] represents the set {1, 2, ..., n}. A graph is represented by a tuple comprising (i) a set of nodes V , (ii) aset of edges E, and (iii) node features X = [x1, x2, ..., xn]T Rnd. We assume nodes are numbered from 1 to n. Y denotes theset of possible labels. yv R|Y | is the one-hot encoded label for node v. N(v) represents the set of neighboring nodes of nodev. We use numpy-like indexing notation. For instance, X:,1 denotes the first column of X, X:,1 denotes the last column, X:,5:denotes the last five columns, and X:,:5 denotes all columns except the last five.",
  "Transductive Node Classification": "**Problem (Transductive Node Classification).** **Input:** A graph G = (V, E, X), a set of labeled nodes Vtrain V , andthe corresponding labels Ytrain Y Vtrain for these nodes. **Output:** Predicted labels Ytest Y Vtest for the remaining nodesVtest = V \\ Vtrain. The node classification problem has two distinct settings: transductive and inductive. In the transductive setting, a single graphis provided along with the labels for a subset of its nodes, and the task is to predict the labels for the unlabeled nodes within thesame graph. This contrasts with the inductive setting, where separate graphs are used for training and testing. For example, in thecontext of spam detection, if we label spam accounts on a social network like Facebook and then use a trained model to identifyspam accounts on the same network, this is a transductive scenario. Conversely, if we use the model trained on Facebook data toidentify spam accounts on a different platform like Twitter, this is an inductive scenario. Transductive node classification is a widely studied problem in the GNN community. It has been employed in well-known GNNmodels like GCNs and GATs and is used in popular benchmark datasets such as Cora, PubMed, and CiteSeer. This setting also hasnumerous practical applications, including document classification and fraud detection.",
  "LaF is Admissible, but Not Explored Well": "We remind the reader of the transductive node classification problem setup. We are given the node labels yv of the training nodes. Astandard approach is to input the node features xv of a training node v into the model, predict its label, calculate the loss based onthe true label yv, and update the model parameters. However, the use of yv is not restricted to this. We can also incorporate yv as afeature for node v. This is the core concept behind LaF.",
  "yv = { [ 1; yv](v Vtrain)01+|Y |(v Vtest),": "is the label vector for node v, and 0d is a zero vector of dimension d. LaF allows GNNs to utilize label information, such as the classdistribution in neighboring nodes, to compute node embeddings. These embeddings are likely to be more informative than thosewithout label information. LaF is considered admissible because it only uses information available in the transductive setting. We emphasize that LaF has not been thoroughly investigated in the GNN literature, despite its simplicity, with a few exceptions.For instance, GCNs and GATs use the transductive setting and could potentially use label information as features. However, theyinitialize node embeddings as h(0)v= xv without using label information. One of the contributions of this paper is to highlight thatLaF is permissible in the transductive setting. Care must be taken when training GNNs with LaF. LaF might negatively impact generalization by creating a shortcut where themodel simply copies the label feature h(0)v,d+1: to the prediction. To avoid this, we should remove the labels of the center nodes in theminibatch and treat them as test nodes. Specifically, if B Vtrain is the set of nodes in the minibatch, we set",
  "LaF Strengthens the Expressive Power of GNNs": "We demonstrate that incorporating labels as features (LaF) provably enhances the expressive capabilities of Graph Neural Networks(GNNs). Specifically, we show that GNNs utilizing LaF can effectively represent the label propagation algorithm, a crucial methodfor transductive node classification, whereas GNNs without LaF cannot achieve this. This finding is significant in its own right andprovides a strong motivation for the design of TFGNNs. Label propagation is a well-established method for transductive node classification. It operates by initiating random walks from atest node and generating the label distribution of the labeled nodes that these random walks encounter first. The following theoremestablishes that GNNs with LaF can effectively approximate label propagation. **Theorem 4.1.** GNNs with LaF can approximate label propagation with arbitrary precision. Specifically, there exists a series ofGNNs {f (l)agg}l and fpred such that for any positive , for any connected graph G = (V, E, X), for any labeled nodes Vtrain V andnode labels Ytrain Y Vtrain, and test node v V \\ Vtrain, there exists L Z+ such that the l( L)-th GNN (f (1)agg, ..., f (l)agg, fpred)with LaF outputs an approximation of label propagation with an error of at most , i.e.,",
  "p0,v,i = { 1 [i=yv] (v Vtrain)0(v V \\ Vtrain),": "can be computed from yv in h(0)v . Let f (l)agg always concatenate its first argument (h(l1)v) to the output so the GNN retains inputinformation. f (l)agg handles two cases based on yv,1 {0, 1}, indicating whether v is in Vtrain. If v Vtrain, f (l)agg outputs 1[i=yv],computable from yv in h(l1)v. If v / Vtrain, f (l)agg aggregates pl1,u,i from u N(v) and averages them, as in the recursiveequation, realizable by message passing in the second argument of f (l)agg.",
  "We then show that GNNs without LaF cannot represent label propagation": "**Proposition 4.2.** GNNs without LaF cannot approximate label propagation. Specifically, for any series of GNNs {f (l)agg}l andfpred, there exists a positive , a connected graph G = (V, E, X), labeled nodes Vtrain V , node labels Ytrain Y Vtrain, and atest node v V \\ Vtrain, such that for any l, the GNN (f (1)agg, ..., f (l)agg, fpred) without LaF has an error of at least , i.e.,",
  "where yLPvis the output of label propagation for test node v": "**Proof.** We construct a counterexample. Let G be a cycle of four nodes numbered 1, 2, 3, 4 clockwise. All nodes have the samefeature x. Let Vtrain = {1, 2} and Ytrain = T . Label propagation classifies node 4 as class 1 and node 3 as class 0. However,GNNs without LaF always predict the same label for nodes 3 and 4 since they are isomorphic. Thus, for any GNN without LaF,there is an irreducible error for either node 3 or 4. Theorem 4.1 and Proposition 4.2 demonstrate that LaF provably enhances the expressive power of GNNs. These results indicate thatGNNs with LaF are more powerful than traditional message-passing GNNs like GCNs, GATs, and GINs without LaF. Notably, whileGINs are considered the most expressive message-passing GNNs, they cannot represent label propagation without LaF, whereasmessage-passing GNNs with LaF can. This does not lead to a contradiction since the original GINs do not take the label informationas input. In other words, the input domains of the functions differ. These findings highlight the importance of considering both theinput and the architecture of GNNs to maximize their expressive power.",
  "**Definition 5.1 (Training-free Model).** We say a parametric model is training-free if it can be used without optimizing theparameters": "It should be noted that nonparametric models are training-free by definition. The real worth of TFGNNs is that it is training-freewhile it can be improved with optional training. Users can enjoy the best of both worlds of parametric and nonparametric models bychoosing the trade-off based on the computational resources for training and the accuracy required.",
  "Experimental Setup": "We use the Planetoid datasets (Cora, CiteSeer, PubMed), Coauthor datasets, and Amazon datasets in the experiments. We use 20nodes per class for training, 500 nodes for validation, and the rest for testing in the Planetoid datasets following standard practice,and use 20 nodes per class for training, 30 nodes per class for validation, and the rest for testing in the Coauthor and Amazondatasets. We use GCNs and GATs for the baselines. We use three-layered models with a hidden dimension of 32 unless otherwisespecified. We train all models with AdamW with a learning rate of 0.0001 and weight decay of 0.01.",
  "TFGNNs Outperform Existing GNNs in Training-free Setting": "We compare the performance of TFGNNs with GCNs and GATs in the training-free setting by assessing the accuracy of the modelswhen the parameters are initialized. The results are shown in . TFGNNs outperform GCNs and GATs in all the datasets.Specifically, both GCNs and GATs are almost random in the training-free setting, while TFGNNs achieve non-trivial accuracy. Theseresults validate that TFGNNs meet the definition of training-free models. We can also observe that GCNs, GATs, and TFGNNs donot benefit from LaF in the training-free settings if randomly initialized. These results indicate that both LaF and the initialization ofTFGNNs are important for training-free performance. : Node classification accuracy in the training-free setting. The best results are shown in bold. CS: Coauthor CS, Physics:Coauthor Physics, Computers: Amazon Computers, Photo: Amazon Photo. TFGNNs outperform GCNs and GATs in all the datasets.These results indicate that TFGNNs are training-free. Note that we use three-layered TFGNNs to make the comparison fair althoughdeeper TFGNNs perform better in the training-free setting as we confirm in .3.",
  "Deep TFGNNs Perform Better in Training-free Setting": "We confirm that deeper TFGNNs perform better in the training-free setting. We have used three-layered TFGNNs so far to makethe comparison fair with existing GNNs. Proposition 5.2 shows that the initialized TFGNNs converge to label propagation as thedepth goes to infinity, and we expect that deeper TFGNNs perform better in the training-free setting. shows the accuracy ofTFGNNs with different depths for the Cora dataset. We can observe that deeper TFGNNs perform better in the training-free settinguntil the depth reaches around 10, where the performance saturates. It is noteworthy that GNNs have been known to suffer from theoversmoothing problem, and the performance of GNNs degrades as the depth increases. It is interesting that TFGNNs do not sufferfrom the oversmoothing problem in the training-free setting. It should be noted that it does not necessarily mean that deeper modelsperform better in the optional training mode because the optional training may break the structure introduced by the initialization ofTFGNNs and may lead to oversmoothing and/or overfitting. We leave it as a future work to overcome these problems by adoptingcountermeasures such as initial residual and identity mapping, MADReg, and DropEdge.",
  "TFGNNs Converge Fast": "In the following, we investigate the optional training mode of TFGNNs. We train the models with three random seeds and report theaverage accuracy and standard deviation. We use baseline GCNs without LaF (i.e., the original GCNs) as the baseline. First, we confirm that TFGNNs in the optional training mode converge faster than GCNs. We show the training curves of TFGNNsand GCNs for the Cora dataset in . TFGNNs converge much faster than GCNs. We hypothesize that TFGNNs converge faster because the initialized TFGNNs are in a good starting point, while GCNs start from a completely random point and requiremany iterations to reach a good point. We can also observe that fully trained TFGNNs perform on par with GCNs. These resultsindicate that TFGNNs enjoy the best of both worlds: TFGNNs perform well without training and can be trained faster with optionaltraining.",
  "TFGNNs are Robust to Feature Noise": "As TFGNNs use both node features and label information while traditional GNNs rely only on node features, we expect thatTFGNNs are more robust to feature noise than traditional GNNs. We confirm this in this section. We add i.i.d. Gaussian noise withstandard deviation to the node features and evaluate the accuracy of the models. We train TFGNNs and GCNs with the Coradataset. The results are shown in . TFGNNs are more robust to feature noise especially in high noise regimes where theperformance of GCNs degrades significantly. These results indicate that TFGNNs are more robust to i.i.d. Gaussian noise to thenode features than traditional GNNs.",
  "Labels as Features and Training-free GNNs": "The most relevant work is by Wang et al., who proposed to use node labels in GNNs. This technique was also used by Addanki et al.and analyzed by Wang et al. The underlying idea is common with LaF, i.e., use of label information as input to transductive GNNs.A similar result as Theorem 4.1 was also shown in Wang et al. However, the focus is different, and there are different points betweenthis work and theirs. We propose the training-free + optional training framework for the first time. The notable characteristics ofGNNs are (i) TFGNNs receive both original features and LaF, (ii) TFGNNs can be deployed without training, and (iii) TFGNNs canbe improved with optional training. Besides, we provide detailed analysis and experiments including the speed of convergence andnoise robustness. Our results provide complementary insights to the existing works. Another related topic is graph echo state networks, which lead to lightweight models for graph data. The key idea is to use randomlyinitialized fixed weights for aggregation. The main difference is that graph echo state networks still require to train the output layer,while TFGNNs can be used without training. These methods are orthogonal, and it is an interesting direction to combine them tofurther improve the performance.",
  "Speeding up GNNs": "Various methods have been proposed to speed up GNNs to handle large graph data. GraphSAGE is one of the earliest methods tospeed up GNNs. GraphSAGE employs neighbor sampling to reduce the computational cost of training and inference. It samples afixed number of neighbors for each node and aggregates the features of the sampled neighbors. An alternative sampling method islayer-wise sampling introduced in FastGCN. Huang et al. further improved FastGCN by using an adaptive node sampling techniqueto reduce the variance of estimators. LADIES combined neighbor sampling and layer-wise sampling to take the best of both worlds.Another approach is to use smaller training graphs. ClusterGCN uses a cluster of nodes as a mini-batch. GraphSAINT samplessubgraphs by random walks for each mini-batch.",
  "It should also be noted that general techniques to speed up neural networks, such as mixed-precision training, quantization, andpruning can be applied to GNNs": "These methods mitigate the training cost of GNNs, but they still require many training iterations. In this paper, we proposetraining-free GNNs, which can be deployed instantly as soon as the model is initialized. Besides, our method can be improved withoptional training. In the optional training mode, the speed up techniques mentioned above can be combined with our method toreduce the training time further.",
  "Expressive Power of GNNs": "Expressive power (or representation power) means what kind of functional classes a model family can realize. The expressive powerof GNNs is an important field of research in its own right. If GNNs cannot represent the true function, we cannot expect GNNs towork well however we train them. Therefore, it is important to elucidate the expressive power of GNNs. Originally, Morris et al. andXu et al. showed that message-passing GNNs are at most as powerful as the 1-WL test, and they proposed k-GNNs and GINs, whichare as powerful as the k-(set)WL and 1-WL tests, respectively. GINs are the most powerful message-passing GNNs. Sato and Loukasshowed that message-passing GNNs are as powerful as a computational model of distributed local algorithms, and they proposedGNNs that are as powerful as port-numbering and randomized local algorithms. Loukas showed that GNNs are Turing-completeunder certain conditions (i.e., with unique node ids and infinitely increasing depths). Some other works showed that GNNs cansolve or cannot solve some specific problems, e.g., GNNs can recover the underlying geometry, GNNs cannot recognize bridges andarticulation points. There are various efforts to improve the expressive power of GNNs by non-message-passing architectures. Werefer the readers to survey papers for more details on the expressive power of GNNs. We contributed to the field of the expressive power of GNNs by showing that GNNs with LaF are more powerful than GNNs withoutLaF. Specifically, we showed that GNNs with LaF can represent an important model, label propagation, while GNNs without LaFcannot. It should be emphasized that GINs, the most powerful message-passing GNNs, and Turing-complete GNNs cannot representlabel propagation without LaF because they do not have access to the label information label propagation uses, and also noted thatGINs traditionally do not use LaF. This result indicates that it is important to consider what to input to the GNNs as well as thearchitecture of the GNNs for the expressive power of GNNs. This result provides a new insight into the field of the expressive powerof GNNs.",
  "Limitations": "Our work has several limitations. First, LaF and TFGNNs cannot be applied to inductive settings while most GNNs can. We do notregard this as a negative point. Popular GNNs such as GCNs and GATs are applicable to both transductive and inductive settings andare often used for transductive settings. However, this also means that they do not take advantage of transductive-specific structures(those that are not present in inductive settings). We believe that it is important to exploit inductive-specific techniques for inductivesettings and transductive-specific techniques (such as LaF) for transductive settings in order to pursue maximum performance. Second, TFGNNs cannot be applied to heterophilious graphs, or its performance degrades as TFGNNs are based on label propagation.The same argument mentioned above applies. Relying on homophilious graphs is not a negative point in pursuing maximumperformance. It should be noted that LaF may also be exploited in heterophilious settings as well. Developing training-free GNNsfor heterophilious graphs based on LaF is an interesting future work.",
  "In this paper, we made the following contributions": "* We advocated the use of LaF in transductive learning (). * We confirmed that LaF is admissible in transductive learning,but LaF has not been explored in the field of GNNs such as GCNs and GATs. * We formally showed that LaF strengthens theexpressive power of GNNs (). * We showed that GNNs with LaF can represent label propagation (Theorem 4.1) whileGNNs without LaF cannot (Proposition 4.2). * We proposed training-free graph neural networks, TFGNNs (). * Weshowed that TFGNNs defined by Eqs. (19) (29) meet the requirementsarticle graphicx"
}