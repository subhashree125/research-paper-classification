{
  "Abstract": "Deep Image Prior (DIP) and its variations have demonstrated significant promise in addressing inverse problems incomputational imaging, without the need for separate training data. Often, practical DIP models are significantlyoverparameterized. These models initially capture the intended visual content during the learning phase andsubsequently incorporate potential modeling and observational noise, demonstrating a pattern of initial learningfollowed by overfitting (ELTO). Consequently, the practical application of DIP depends on an early stopping (ES)mechanism capable of identifying this transitional period. Most previous DIP research in computational imaginghas focused on demonstrating the models potential by reporting peak performance against ground truth, withoutproviding practical methods to achieve near-peak performance without access to ground truth. This paper aims toovercome this practical limitation of DIP by introducing an efficient ES strategy that reliably identifies near-peakperformance across various computational imaging tasks and DIP variants. This ES method, based on the runningvariance of intermediate reconstructions in DIP, not only surpasses existing methods that are limited to specificconditions but also maintains its effectiveness when combined with techniques aimed at reducing overfitting.",
  "Introduction": "Inverse problems (IPs) are widespread in the field of computational imaging, encompassing tasks from fundamental image denoising,super-resolution, and deblurring to complex 3D reconstruction and significant challenges in scientific and medical imaging. Despitethe variety of settings, all these problems involve recovering a visual object x from an observation y = f(x), where f represents theforward physical process. Usually, these visual IPs are underdetermined, meaning x cannot be uniquely ascertained from y. Thisambiguity is further complicated by potential modeling inaccuracies (such as using a linear f to approximate a nonlinear process)and observational noise (like Gaussian or shot noise), represented as y 2248 f(x). To address nonuniqueness and enhance stabilityagainst noise, researchers often integrate a range of problem-specific priors on x when formulating IPs.",
  "Related Work": "There are three primary methods to counteract the overfitting of DIP models. The first one is Regularization: Overfitting is lessenedby limiting the size of G03b8 to the underparameterization range. Layer-wise weights or the network Jacobian are regularized toregulate the network capacity. The total-variation norm or trained denoisers are used as additional regularizers R(G03b8(z)). Toprevent overfitting, these techniques need the proper amount of regularization, which varies depending on the kind and degree ofnoise. They may nevertheless cause overfitting if the regularization level is incorrect. Furthermore, even when they are successful,the performance peak is delayed until the last few iterations, which frequently increases the computing cost by several times. Thesecond method is Noise modeling: In their optimization objective, sparse additive noise is explicitly represented. Regularizers andES criteria are created especially for Gaussian and shot noise. Subgradient techniques using decreasing step size schedules arebeing investigated for impulse noise with the 21131 loss, and they have shown some early promise. These techniques are ineffectiveoutside of the noise types and levels that they are designed to address, and our understanding of the noise in a particular visualIP is often constrained. The third method is Early stopping (ES): Progress is tracked using a ratio of no-reference blurriness andsharpness, however, as the authors point out, the criterion is only applicable to their modified DIP models. It is unclear how to applythe noise-specific regularizer and ES criterion to unknown noise types and levels. It is suggested to monitor DIP reconstruction bytraining a coupled autoencoder. Although it performs similarly to ours, the additional autoencoder training significantly increases theoverall processing time. By dividing the elements of y into \"training\" and \"validation\" sets, it is possible to simulate validation-basedES in supervised learning. However, in IPs, particularly nonlinear ones (such as blind image deblurring (BID), where y 2248 k2217 x and 2217 denotes linear convolution), elements of y may not be i.i.d., which could impair the effectiveness of validation.Furthermore, withholding a portion of the observation in y can significantly diminish peak performance.",
  "Methodology": "We advocate for the ES approach because, even when effective, regularization and noise modeling techniques frequently fail toenhance peak performance; instead, they extend it to the final iterations, potentially requiring ten times more iterations than would benecessary to reach the peak in the original DIP models. Furthermore, both approaches necessitate extensive knowledge of the noisetype and level, which is often unavailable for most applications. If their essential models and hyperparameters are not appropriatelyconfigured, overfitting is likely to persist, and ES will still be necessary. This paper introduces a novel ES criterion applicable tovarious DIP models, based on monitoring the trend of the running variance in the reconstruction sequence.",
  "Detecting transition by running variance:": "Our lightweight method only involves computing the VAR curve and numerically detecting its valley2014 the iteration stops once thevalley is detected. To obtain the curve, we set a window size parame- ter W and compute the windowed moving variance (WMV). Torobustly detect the valley, we introduce a patience number P to tolerate up to P consecutive steps of variance stagnation. Obviously,the cost is dominated by the calculation of variance per step, which is O(W N ) (N is the size of the visual object). In comparison, atypical gradient update step for solving Eq. (2) costs at least 2126(|03b8|N ), where |03b8| is the number of parameters in the DNNG03b8. Since |03b8| is typically much larger than W (default: 100), our running VAR and detection incur very little compu- tationaloverhead.",
  "Experiments": "ES-WMV is tested for DIP in a variety of linear and nonlinear IPs, including image denoising, inpainting, demosaicing, super-resolution, MRI reconstruction, and blind image deblurring. ES-WMV is also systematically assessed for major DIP variants, suchas deep decoder, DIP-TV, and GP-DIP, for image denoising. It is shown to be a dependable helper in identifying effective ESpoints. The specifics of the DIP variants are covered in Appendix A.5. In addition, ES-WMV is contrasted with the primary rivaltechniques, such as DF-STE, SV-ES, DOP, SB, and VAL. The specifics of the primary ES-based techniques are found in AppendixA.6. Reconstruction quality is evaluated using both PSNR and SSIM, and detection performance is shown using PSNR and SSIMgaps, which are the differences between our detected and peak values.",
  "Image Denoising": "The majority of earlier research on DIP overfitting has concentrated on image denoising and often assessed their techniques usingonly one or two forms of noise with modest noise levels, such as low-level Gaussian noise. We use the traditional 9-image datasetfor each noise type, and we create two noise levels2014low and high2014for each.",
  "Image Super-Resolution": "In this task, we try to recover a clean im- age x0 from a noisy downsampled ver- sion y = Dt(x0) + 03f5, where Dt(00b7) : 300d7tH00d7tW 2192 300d7H00d7W is a down- sampling operator that resizes an im- age by the factor t and 03f5 modelsex- tra additive noise. We consider the fol- lowing DIP-reparametrized formulation . = 2225Dt(G03b8(z)) 2212 y22252 min03b82113(03b8) F , where G03b8 is a trainable DNN parameterized by 03b8 and z is a frozen random seed. Then we conduct experimentsfor 200d7 super- resolution with low-level Gaussian and impulse noise. We test our ES-WMV for DIP and a state-of-the-art zero-shotmethod based on pre-trained diffusion model2014DDNM+ on the standard super-resolution dataset Set14, as shown in Tab. 5, , and Appendix A.7.9. We note that DDNM+ relies on pre-trained models from large external training datasets, while DIP doesnot. We observe that (1) Our ES-WMV is again able to detect near-peak performance for most images: the average PSNR gap is2264 1.50 and the average SSIM gap is 2264 0.07; (2) DDNM+ is sensitive to the noise type and level: from Tab. 5, DDNM+ trainedassuming Gaussian noise level 03c3y = 0.12 outperforms DIP and DIP+ES-WMV when there is Gaus- sian measurement noise atthe level 03c3y = 0.12, which is unrealistic in practice, as the noise level is often unknown beforehand. When the noise level is notset correctly, e.g., as 03c3y = 0 in the DDNM+ (03c3y = .00) row of Tab. 5, the performance of DDNM+ is much worse than that ofDIP and DIP+ES-WMV. Also, for super-resolution with impulse noise, DIP is also a clear winner that leads DDNM+ by a largemargin; and (3) in Appendix A.8, we show that DDNM+ may also suffer from the overfitting issue.",
  "MRI Reconstruction": "We also test ES-WMV on MRI reconstruction, a typical linear IP with a nontrivial forward mapping: y 2248 F(x), where F is thesubsampled Fourier operator, and we use 2248 to indicate that the noise encountered in practical MRI imaging may be hybrid (e.g.,additive, shot) and uncertain. Here, we take the 8-fold undersampling and parameterize x using 201cConv-Decoder201d, a variant ofdeep decoder. Due to the heavy over-parameterization, overfitting occurs and ES is needed.",
  "Blind Image Deblurring": "In BID, a blurry and noisy image is given, and the goal is to recover a sharp and clean image. The blur is mostly caused by motionand/or op- tical non-ideality in the camera, and the forward process is often modeled as y = k 2217 x + n, where k is the blurkernel, n models additive sensory noise, and 2217 is linear convolution to model the spa- tial uniformity of the blur effect. BIDis a very challenging visual IP due to bilin- earity: (k, x) 72192 k 2217 x. Recently, researchers have tried to use DIP models tosolve BID by modeling k and x as two separate DNNs, i.e., min03b8k,03b8x 2225y 2212 G03b8k (zk) 2217 G03b8x(zx)22252 2 +03bb22252207G03b8x (zx)22251/22252207G03b8x (zx)22252, where the regular- izer is to promote sparsity in the gradient domainfor the reconstruction of x, as stan- dard in BID. We follow previous work and choose a multilayer perceptron (MLP) with softmaxactivation for G03b8k , and the canonical DIP model (CNN-based encoder-decoder architecture) for G03b8x(zx). We change theirregularizer from the original 22252207G03b8x (zx)22251 to the current, as their original formulation is tested only at a very lownoise level 03c3 = 1022125 and no overfitting is observed. We set the test with a higher noise level 03c3 = 1022123, and find that itsoriginal formulation does not work.",
  "Results": ": Summary of performance of our DIP+ES-WMV and competing methods on image denoising and blind image deblurring(BID). 2713: working reasonably well (PSNR 2265 2dB less of the original DIP peak); -: not working well (PSNR 2264 2dB less ofthe original DIP peak): N/A: not applicable (i.e., we do not perform comparison due to certain reasons). Note that DF-STE, DOP,and SB are based on modified DIP models.",
  "(0.030)13.027 (3.872)0.301 (0.016)0.003 (0.003)": "The results of our experiments are summarized in the tables above. shows the performance of our DIP+ES-WMV methodagainst competing methods for image denoising and BID. reports the performance of ES-WMV on real-world imagedenoising for 1024 images. compares the wall-clock time of DIP and three ES methods per epoch. comparesES-WMV and SB for image denoising on the CBSD68 dataset. compares ES-WMV for DIP and DDNM+ for 200d7 imagesuper-resolution. shows the performance of ConvDecoder on MRI reconstruction. compares BID detection betweenES-WMV and VAL on the Levin dataset. compares DIP with ES-WMV vs. DOP on impulse noise. comparesES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise. compares detectionperformance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024 images. comparesdetection performance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on the PolyU dataset. shows the performance of DIP with ES-WMV for image inpainting.",
  "WMV28.7(3.2)3962(2506)27.4(2.6)3068(2150)24.2(2.3)1548(1939)SB29.0(3.1)4908(1757)27.3(2.2)5099(1776)23.0(1.0)5765(1346)": ": Comparison of ES-WMV for DIP and DDNM+ for 200d7 image super-resolution with low-level Gaussian and impulsenoise: mean and (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter forDDNM+ (03c3y = 0.12), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.",
  "Conclusion": "This paper introduces an innovative ES detection approach, ES-WMV, along with its variant, ES-EMV, which has demonstratedrobust performance across a range of visual IPs and different DIP variations. In contrast to most competing ES methods that arespecific to certain types of noise or DIP models and have limited applicability, our method exhibits broad effectiveness. Whilethere is a method with comparable performance, it significantly increases processing time. Another method, validation-based ES,performs well in simple denoising tasks but falls short in more complex nonlinear IPs like BID.",
  "DIP-ES31.64 (5.69) 0.85 (0.18)24.74 (3.23) 0.67 (0.19)DOP32.12 (4.52) 0.92 (0.07)27.34 (3.78) 0.86 (0.10)": ": Comparison of ES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise: meanand (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter for DDNM+ (03c3y =0.18), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.",
  "PSNRSSIMGaussianImpulseGaussianImpulse": "DIP (peak)24.63 (2.06)37.75 (3.32)0.68 (0.06)0.96 (0.10)DIP + ES-WMV23.61 (2.67)36.87 (4.29)0.60 (0.13)0.96 (0.10)DDNM+ (03c3y = .18)26.93 (2.25)22.29 (3.00)0.78 (0.07)0.62 (0.12)DDNM+ (03c3y = .00)15.66 (0.39)15.52 (0.43)0.25 (0.10)0.30 (0.10) : Detection performance comparison between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024images from the RGB track of NTIRE 2020 Real Image Denoising Challenge: mean and (std). Higher PSNR and SSIM are in red.(D: Detected)"
}