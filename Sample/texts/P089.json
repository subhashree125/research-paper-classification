{
  "Abstract": "This research investigates the conditions under which the neural tangent kernel (NTK) approximation remainsvalid when employing the square loss function for model training. Within the framework of lazy training, asintroduced by Chizat et al., we demonstrate that a model, rescaled by a factor of = O(T), maintains the validityof the NTK approximation up to a training time of T. This finding refines the earlier result from Chizat et al.,which necessitated a larger rescaling factor of = O(T 2), and establishes the preciseness of our establishedbound.",
  "Introduction": "In contemporary machine learning practice, the weights w of expansive neural network models fw : Rdin Rdout are trainedusing gradient-based optimizers. However, a comprehensive theoretical understanding remains elusive due to the non-linear natureof the training dynamics, which complicates analysis. To bridge this gap, an approximation to these dynamics, termed the NTKapproximation, was introduced, and its validity for infinitely wide networks trained via gradient descent was demonstrated. The NTKapproximation has proven highly influential, offering theoretical insights into various phenomena, including deep learnings capacityto memorize training data, the manifestation of spectral bias in neural networks, and the differential generalization capabilities ofdiverse architectures. Nevertheless, empirical evidence suggests that the training dynamics of neural networks frequently deviatefrom the NTK approximations predictions. Consequently, it becomes crucial to delineate the precise conditions under which theNTK approximation remains applicable. This paper seeks to address the following inquiry:",
  "The Lazy Training Framework": "The work demonstrated that the NTK approximation is applicable to the training of any differentiable model, provided the modelsoutputs are rescaled appropriately. This rescaling ensures that significant changes in the models outputs can occur even with minoradjustments to the weights. The validity of the NTK approximation for models of infinite width stems from this observation, as themodel is inherently rescaled as its width approaches infinity. Consider a smoothly parameterized model h : Rp F, where F is a separable Hilbert space. Let > 0 be a parameter governingthe models rescaling, which should be considered large. We train the rescaled model h using gradient flow to minimize a smoothloss function R : F R+. The weights w(t) Rp are initialized at w(0) = w0 and evolve according to the gradient flow:",
  "wR(h( w(t))).(3)": "The NTK approximation asserts that:h(w(t)) h( w(t)).(4)In essence, this implies that the linearization of the model h remains valid throughout the training process. This greatly simplifiesthe analysis of training dynamics, as the model h is linear in its parameters, allowing the evolution of h( w) to be understood througha kernel gradient flow in function space. The validity of the NTK approximation is contingent on the magnitude of the rescaling parameter . Intuitively, a larger implies that the weights need not deviate significantly from their initialization to induce substantial changes in the models output,thereby prolonging the validity of the linearization. This regime of training, where weights remain close to their initialization,is referred to as \"lazy training.\" The following bound was established, where R0 = R(h(w0))) is the loss at initialization, and = T1Lip(Dh)R0 is a quantity that will also feature in our main results:",
  ".(7)": "In contrast to prior work, our bound does not depend on the Lipschitz constant of h, and it exhibits a more favorable dependence onT. Specifically, if Lip(Dh), Lip(h), and R0 are bounded by constants, our result indicates that the NTK approximation, up to anerror of O(), holds for times T = O(), whereas the previously known bound was valid for T = O(). Given the practicalinterest in long training times T 1, our result demonstrates that the NTK approximation is valid for significantly longer timehorizons than previously recognized.",
  "Application to Neural Networks": "The bound established in Theorem 1.2 is applicable to the lazy training of any differentiable model. As a specific example, we detailits application to neural networks. We parameterize the networks in the mean-field regime, where the NTK approximation does nothold even as the width approaches infinity. Consequently, the NTK approximation is valid only when training is conducted in thelazy regime.",
  "Under certain regularity assumptions on the activation function (satisfied, for instance, by the sigmoid function) and a bound on theweights, it can be shown that Lip(Dh) is bounded": "**Lemma 2.1 (Bound on Lip(Dh) for mean-field 2-layer network).** Suppose there exists a constant K such that (i) the activationfunction is bounded and has bounded derivatives , , , K, (ii) the weights have bounded normUa K, and (iii) the data points have bounded norm x K. Then there exists a constant K depending only on K such that",
  "h(w(T)) h( w(T)) C min(T/, 1).(12)": "Training in the NTK parametrization corresponds to training the model mfw, where fw is the network in the mean-fieldparametrization. This is equivalent to setting the lazy training parameter = m in the mean-field setting. Therefore, under theNTK parametrization with width m, the bound in Corollary 2.2 indicates that the NTK approximation is valid until training timeO(m) and the error bound is O(T/m).",
  "T 3/2Lip(h)Lip(Dh)R0/.(19)": "This improves over Proposition 1.1 for long time horizons, as the time dependence scales as T 3/2 instead of T 2. However, it stilldepends on the Lipschitz constant Lip(h) and falls short of the linear in T dependence of Theorem 1.2. **Second attempt: new approach to prove Theorem 1.2** To avoid dependence on Lip(h) and achieve a linear dependence in T,we develop a new approach. We cannot use (7), which was central to the original proof, as it depends on Lip(h). Furthermore, toachieve linear T dependence using (7), we would need w w0 = O(1) for a constant independent of the time horizon, which isnot true unless the problem is well-conditioned. In the full proof in Appendix A, we bound r(T) r(T), which requires working with a product integral formulation of thedynamics of r to handle the time-varying kernels Kt. The main technical innovation in the proof is Theorem A.8, which is a new,general bound on the difference between product integrals.",
  "Discussion": "A limitation of our result is that it applies only to gradient flow, which corresponds to SGD with infinitesimally small step size.However, larger step sizes are beneficial for generalization in practice, so it would be interesting to understand the validity of theNTK approximation in that setting. Another limitation is that our result applies only to the square loss and not to other popularlosses such as the cross-entropy loss. Indeed, the known bounds in the setting of general losses require either a \"well-conditioning\"assumption or taking exponential in the training time T. Can one prove bounds analogous to Theorem 1.2 for more general losses,with depending polynomially on T, and without conditioning assumptions? A natural question raised by our bounds in Theorems 1.2 and 1.3 is: how do the dynamics behave just outside the regime where theNTK approximation is valid? For models h where Lip(h) and Lip(Dh) are bounded by a constant, can we understand the dynamicsin the regime where T C for some large constant C and C, at the edge of the lazy training regime?"
}