{
  "Abstract": "This study delves into how linguistic understanding, extracted from extensive textdatasets, can be leveraged to enhance the generation of natural language videodescriptions. Specifically, we integrate both a neural language model and distribu-tional semantics, trained on large text corpora, into a contemporary LSTM-basedframework for video description. Our evaluation, conducted on a collection ofYouTube videos and two substantial movie description datasets, reveals consider-able advancements in grammatical correctness, accompanied by subtle improve-ments in descriptive quality.",
  "Introduction": "The capacity to automatically generate natural language (NL) descriptions for videos has numeroussignificant applications, such as content-based video retrieval and aiding visually impaired individuals.Recent effective approaches, use recurrent neural networks (RNNs), treating the problem as a machinetranslation (MT) task, converting from video to natural language. Deep learning methods like RNNsrequire extensive training data; however, theres a shortage of high-quality video-sentence pairs.Conversely, vast raw text datasets are readily available, exhibiting rich linguistic structure usefulfor video description. Most work in statistical MT employs a language model, trained on extensivemonolingual target language data, and a translation model, trained on restricted parallel bilingualdata. This paper investigates methods to incorporate knowledge from language datasets to capturegeneral linguistic patterns to improve video description. This study integrates linguistic data into a video-captioning model based on Long Short Term Memory(LSTM) RNNs, known for state-of-the-art performance. Additionally, LSTMs function effectivelyas language models (LMs). Our initial method (early fusion) involves pre-training the networkusing plain text prior to training with parallel video-text datasets. Our subsequent two methods,influenced by current MT research, incorporate an LSTM LM with the existing video-to-text model.Furthermore, we explore substituting the standard one-hot word encoding with distributional vectorsderived from external datasets. We present thorough comparisons across these methods, assessing them on a typical YouTube corpusand two recently released extensive movie description datasets. The findings indicate notable gains indescription grammaticality (as assessed by crowdsourced human evaluations) and moderate gains indescriptive quality (as determined by human judgements and automated comparisons against human-generated descriptions). Our main contributions include: (1) numerous approaches to integrateknowledge from external text into a current captioning model, (2) comprehensive experimentscomparing methods on three large video-caption datasets, and (3) human assessments demonstratingthat external linguistic knowledge notably impacts grammar.",
  "LSTM-based Video Description": "We employ the S2VT video description framework, which we describe briefly here. S2VT adopts asequence-to-sequence approach that maps an input video frame feature sequence to a fixed-dimensionvector, which is then decoded into a sequence of output words. As depicted in the architecture employs a dual-layered LSTM network. The input to the initial LSTMlayer is a sequence of frame features extracted from the second-to-last layer (fc7) of a ConvolutionalNeural Network (CNN) after the ReLU operation. This LSTM layer encodes the video sequence. Ateach step, the hidden state is fed into the subsequent LSTM layer. Following the processing of allframes, the second LSTM layer is trained to transform this state into a sequence of words. This can bethought of as using one LSTM to model visual features and another to model language, conditionedon the visual data. We modify this structure to incorporate linguistic information during training andgeneration. Although our techniques are based on S2VT, they are sufficiently general and could beapplied to other CNN-RNN based captioning models.",
  "Early Fusion": "Our early fusion method involves initially pre-training the language-modeling components of thenetwork on large raw NL text datasets, before fine-tuning these parameters on video-text paireddatasets. An LSTM model can learn the probability of an output sequence given an input. To learn alanguage model, we train the LSTM layer to predict the next word based on the preceding words.Following the S2VT design, we embed one-hot encoded words into reduced-dimension vectors. Thenetwork is trained on extensive text datasets, and its parameters are learned using backpropagationwith stochastic gradient descent. The weights from this network initialize the embedding and weightsof the LSTM layers in S2VT, which is then trained on video-text data. This trained LM is also utilizedas the LSTM LM in both late and deep fusion models.",
  "Late Fusion": "Our late fusion approach draws inspiration from how neural machine translation models incorporatea trained language model during decoding. At each step of sentence generation, the video captionmodel generates a probability distribution over the vocabulary. We then utilize the language modelto re-score the final output by considering a weighted average of the scores from the LM and theS2VT video-description model (VM). Specifically, for output at time step t, and given proposaldistributions from the video captioning model and the language model, we can calculate the re-scoredprobability of each new word as:",
  "Deep Fusion": "In the deep fusion approach, we integrate the LM more profoundly in the generation process. Weachieve this by concatenating the hidden state of the language model LSTM (hLM) with the hiddenstate of the S2VT video description model (hV M) and use the resulting combined latent vector topredict the output word. This is similar to the method employed to incorporate language modelsfrom monolingual data for machine translation. However, our method differs in two ways: (1) Weconcatenate only the hidden states of the S2VT LSTM and language LSTM, without additionalcontext. (2) We keep the weights of the LSTM language model constant while training the entirevideo captioning network. The probability of a predicted word at time step t is:",
  "Distributional Word Representations": "The S2VT network, like many image and video captioning models, uses a one-hot encoding forwords. During training, the model learns to embed these one-hot words into a 500-dimensionalspace via linear transformation. This embedding, however, is learned from the limited and possiblynoisy caption data. Many techniques exist that leverage large text datasets to learn vector-spacerepresentations of words, capturing nuanced semantic and syntactic structures. We aim to capitalizeon these to enhance video description. Specifically, we replace the embedding matrix from one-hotvectors with 300-dimensional GloVe vectors, pre-trained on 6B tokens from Gigaword and Wikipedia2014. We further explore variations where the model predicts both the one-hot word (softmax loss)and the distributional vector from the LSTM hidden state using Euclidean loss. The output vector (yt)is computed as yt = (Wght + bg), and the loss is:",
  "Human Evaluation": "We also collect human judgments on a random subset of 200 video clips for each dataset throughAmazon Turk. Each sentence was evaluated by three workers on a Likert scale from 1 to 5 (higher isbetter) for relevance and grammar. Grammar evaluations were done without viewing videos. Movieevaluation focused solely on grammar due to copyright.",
  "YouTube Video Dataset Results": "The results show Deep Fusion performed well for both METEOR and BLEU scores. The integrationof Glove embeddings considerably increased METEOR, and combining both techniques performedbest. Our final model is an ensemble (weighted average) of the Glove model and two Glove+DeepFusion models trained on external and in-domain COCO sentences. While the state-of-the-art on thisdataset is achieved using attention to encode the video our work focuses on language modeling.",
  ": Results on the YouTube dataset: METEOR and BLEU@4 scores (in %), along with humanratings (1-5) on relevance and grammar. * denotes a significant improvement over S2VT": "Human ratings align closely with METEOR scores, indicating modest gains in descriptive quality.Linguistic knowledge enhances the grammar of the results. We experimented multiple ways toincorporate word embeddings: (1) GloVe input: Using GloVe vectors at the LSTM input performedbest. (2) Fine-tuning: Initializing with GloVe and subsequently fine-tuning reduced validation resultsby 0.4 METEOR. (3) Input and Predict: Training the LSTM to accept and predict GloVe vectors, asdescribed in , performed similarly to (1).",
  "Related Work": "Following the advancements of LSTM-based models in Machine Translation and image captioning,video description works propose CNN-RNN models that create a vector representation of the video,which is decoded by an LSTM sequence model to generate a description. Some works also incorporateexternal data to improve video description, however, our focus is on integrating external linguisticknowledge for video captioning. We explore the use of distributional semantic embeddings andLSTM-based language models trained on external text datasets. LSTMs have proven to be effective language models. Other works have developed an LSTM modelfor machine translation that incorporates a monolingual language model for the target language,achieving improved results. We utilize similar techniques (late fusion, deep fusion) to train an LSTMfor video-to-text translation. This model uses large monolingual datasets to enhance RNN-basedvideo description networks. Unlike other approaches where the monolingual LM is used solely forparameter tuning, our approach utilizes the output of the language model as an input for training thefull underlying video description network. Other recent works propose video description models that focus primarily on improving the videorepresentation itself with hierarchical visual pipelines and attention mechanisms. Without the attentionmechanism their models achieve good METEOR scores on the YouTube dataset. The interestingaspect is that the contribution of language alone is considerable. Hence, it is important to focus onboth aspects to generate better descriptions.",
  "Conclusion": "This study investigates methods to integrate linguistic knowledge from text datasets for videocaptioning. Our assessments on YouTube videos and two movie description datasets show improvedresults according to human evaluations of grammar while also modestly improving the descriptivequality of sentences. Although the proposed methods are assessed on a particular video-captioningnetwork, they are applicable to other video and image captioning models."
}