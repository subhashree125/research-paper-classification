{
  "Abstract": "This document provides a summary of the challenges faced in the domain ofAutonomous Driving. The dataset incorporated into the study includes 150 minutesof labeled Trajectory and 3D Perception data, comprising approximately 80,000lidar point clouds and 1000 kilometers of trajectories in urban traffic conditions.The competition is divided into two main segments: (1) Forecasting Trajectoriesand (2) 3D Lidar Object Recognition. Over 200 teams provided their results on theleaderboard, and more than 1,000 individuals took part in the workshop.",
  "Dataset": "The Apolloscape Dataset is utilized as a research tool designed to advance autonomous driving invarious dimensions, including perception, navigation, prediction, and simulation. This dataset iscomprised of labeled street view images and simulation resources that can accommodate user-definedstrategies. The dataset includes tasks such as Trajectory Prediction, 3D Lidar Object Detection,3D Lidar Object Tracking, lane marking segmentation, online self-positioning, 3D car instancecomprehension, Stereo, and Inpainting Dataset. A dedicated online assessment platform and usertoolkit are provided for each task. For data collection related to Trajectory Prediction and 3D Perception, a data-gathering vehiclewas utilized to amass traffic information, including camera-captured images and LiDAR-generatedpoint clouds. Our vehicle operates in urban settings during peak traffic times. The dataset featurescamera imagery, 3D point cloud data, and paths of traffic agents within the LiDARs operational area.This newly created dataset, which includes 150 minutes of sequential information, is extensive andconcentrates on urban roadways, with a particular emphasis on 3D perception, prediction, planning,and simulation activities involving a variety of traffic agents.",
  "Evaluation Metric": "The evaluation metric is analogous to the one defined in prior work. The aim of the 3D objectdetection task is to develop detectors for vehicle, pedestrian, and bicyclist categories. Thesedetectors should estimate the 3D bounding box (dimensions and position) and provide a detectionscore or confidence. It is important to note that not all objects within the point clouds are labeled.The performance of 3D object detection is assessed using the mean Average Precision (mAP),based on Intersection over Union (IoU). The evaluation standard aligns with the 2D object detectionbenchmark, utilizing 3D bounding box overlap. The ultimate metric is the average mAP acrossvehicles, pedestrians, and bicyclists, with IoU thresholds set at 0.7 for cars, and 0.5 for both pedestriansand cyclists.",
  "Data Structure": "Each annotated file for 3D Lidar object detection represents a one-minute sequence captured attwo frames per second. An entry within each file includes the frame number, object ID, objectclassification, positions along the x, y, and z axes, object dimensions (length, width, height), andorientation. Object classifications are consistent with those in the trajectory data. In this evaluation, thefirst two categoriessmall and large vehiclesare considered as a single vehicle class. Positionaldata is relative, with units in meters, and the heading angle denotes the objects steering direction.",
  "Trajectory prediction": "One team utilized an encoder-decoder framework based on LSTM for predicting trajectories on citystreets. To enhance prediction accuracy, they implemented four sequence-to-sequence sub-modelsto capture the distinct movement characteristics of various traffic participants. They produced afuture trajectory for each agent through a three-step process: encoding, perturbation, and decoding.Initially, an encoder was employed to embed the past trajectory. Subsequently, they introduced a16-dimensional random noise to the encoders output to accommodate the multimodal distribution ofthe data. Finally, they generated the predicted trajectory via a decoder that mirrored the encodersstructure. In addition, they attempted to capture the collective influence among road agents using an interactiontechnique. Improving upon the original methodology, they conducted an interaction operation at eachmoment during the encoding and decoding phases. The interaction module embedded the positionsof all agents and generated a comprehensive 128-dimensional spatiotemporal representation usingan LSTM unit. The derived feature was then relayed to the encoders or decoders for the primaryprediction task. Each encoder or decoder, linked to a particular individual, produced the privateinteraction within a confined area through an attention operation, utilizing the aforementioned globalfeature and the agents position. Their experimental findings indicated that the interaction moduleenhanced prediction accuracy on the dataset.",
  "D Detection": "One team introduced an innovative approach termed sparse-to-dense 3D object detector (STD). STDis characterized as a two-stage, point-based detection system. The initial phase involves a bottom-upnetwork for generating proposals, where spherical anchors are seeded on each point to encompassobjects at various orientations. This spherical anchor design reduces computational load and shortensinference time by eliminating the need to account for differently oriented objects during anchorcreation. Subsequently, points within these spherical anchors are collected to form proposals foradditional refinement. In the second phase, a PointsPool layer is introduced to transform the featuresof proposals from point-based representations to compact grid formats. These dense features are thenprocessed through a prediction head, which includes two extra fully-connected layers, to derive thefinal detection outcomes. A 3D intersection-over-union (IoU) branch is also incorporated into theprediction head to estimate the 3D IoU between the final predictions and the ground-truth boundingboxes, thereby enhancing localization precision. During the training process, four distinct data augmentation techniques were employed to mitigateoverfitting. Initially, similar to previous methods, ground-truth bounding boxes with their correspond-ing interior points were randomly added from different scenes to the existing point cloud, simulatingobjects in varied settings. Subsequently, each bounding box was randomly rotated based on a uniformdistribution and subjected to random translation. Additionally, every point cloud was randomlyflipped along the x-axis with a 50% probability. Lastly, random rotation and scaling were applied toeach point cloud using uniformly distributed random variables. In the testing phase, predictions werefirst obtained on both the original and the x-axis flipped point clouds, and these results were thenmerged using Soft-NMS to produce the final predictions. Another teams strategy is based on the PointPillars framework. The network configuration largelymirrors that of the original work, with adjustments made to accommodate multiple anchors foreach class. The substantial variation in the size of objects within each class suggested that a singleanchor might be inadequate. The k-means algorithm was utilized to create five anchors for each class.Another modification involved deactivating the direction classification in the loss function, as theevaluation metric relies on IOU, which is not affected by direction. Detailed settings for each classare presented in . To enhance training data, global translation and scaling of the point cloud, along with rotation andtranslation for each ground truth, were implemented. Global rotation of the point cloud was omittedas it was found to produce less favorable outcomes. The specific parameters for these adjustments aredetailed in .",
  "[0.2,0.2,0.2][0.95,1.1][-/20, /20][0.25,0.25,0.25]": "Test Time Augmentation was employed to enhance performance. For every point cloud, four iterationswere generated: the original, and versions flipped along the x-axis, y-axis, and both axes. Eachiteration was processed by the network to obtain bounding box predictions, which were subsequentlyunflipped. Due to the flipping operation, anchors across iterations have a one-to-one correspondence.For each anchor, the corresponding predicted boxes were combined by averaging the location, size,and class probability. Redundant boxes were then eliminated using Non-Maximum Suppression(NMS). Another Team introduced enhancements to the PointPillars method. Their approach incorporatedresidual learning and channel attention mechanisms into the baseline architecture. The network iscomposed of the original Pillar Feature Network, an extended 2D CNN backbone, and a detectionhead for foreground/background classification and regression. The deeper backbone significantlyimproves detection accuracy compared to the original PointPillars. A separate network was trainedfor each class in the Apollo training dataset to perform binary classification, resulting in four distinctnetworks. Final predictions were compiled by aggregating all foreground predictions from thesenetworks. For dataset preprocessing, methods from the KITTI dataset were adapted, including positive examplesampling, global rotation, individual object rotation, and random scaling for each object. However,unlike the KITTI approach, global rotation was excluded, and the ranges for scaling and rotation werereduced. Additionally, more foreground point clouds were sampled to augment positive examples. details the specific settings for each class.",
  "ClassPointcloud Range (m)Pillar Size (m)Anchor Size (m)MSN": "Vehiclesx: -70.8 to 70, y: -67.2 to 67.2, z: -3 to 1x: 0.16, y: 0.16, z: 3x: 1.6, y: 3.9, z: 1.5615Pedestrianx: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5x: 0.2, y: 0.2, z: 3x: 0.6, y: 1.76, z: 1.7315Motor&bicyclistx: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5x: 0.2, y: 0.2, z: 3x: 0.6, y: 0.8, z: 1.7315",
  "Conclusion and Future Work": "This paper provides a review of the challenges encountered in the domain of Autonomous Driving,with a focus on the analysis of 3D Detection and Trajectory prediction. It is anticipated that this paperwill offer contemporary insights into these research areas. Future endeavors will aim to refine the open-source tools and dataset for autonomous driving.Moreover, additional workshops and challenges are planned to foster the exchange of concepts and tocollectively propel the field of autonomous driving research forward."
}