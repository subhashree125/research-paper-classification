{
  "Abstract": "This study introduces Dual PatchNorm, a modification for Vision Transformers that incorporates two LayerNormalization layers (LayerNorms) positioned before and after the patch embedding layer. The effectiveness ofDual PatchNorm is demonstrated through its superior performance compared to alternative LayerNorm placementstrategies within the Transformer block, as determined through extensive testing. Experimental results acrossvarious tasks, including image classification, contrastive learning, semantic segmentation, and transfer learning ondownstream classification datasets, consistently show that this simple adjustment leads to improved accuracy overwell-optimized standard Vision Transformers, without any negative impact.",
  "Introduction": "Layer Normalization is essential for the successful and stable training of Transformer models, enabling high performance acrossdiverse tasks. This normalization technique is equally vital in Vision Transformers (ViTs), which largely adhere to the standardarchitecture of the original Transformer model. This research investigates whether a different arrangement of LayerNorms can enhance ViT models. Initially, we evaluate fiveViT architectures on ImageNet-1k and find that an exhaustive search for optimal LayerNorm placements within the Transformerblocks components does not yield improvements in classification accuracy. This suggests that the pre-LN approach in ViTs is nearlyoptimal. Further investigation reveals that alternative LayerNorm placements, such as NormFormer and Sub-LN, also do not surpassthe performance of robust ViT classification models when used independently. A significant finding of this study is the observation that the addition of LayerNorms before and after the standard ViT-projectionlayer, termed Dual PatchNorm (DPN), can substantially improve performance over well-tuned baseline ViTs. Experiments conductedon image classification across three datasets with varying sample sizes, as well as contrastive learning, confirm the effectivenessof DPN. Notably, qualitative analysis indicates that the LayerNorm scale parameters assign greater weight to pixels located at thecenter and corners of each patch.",
  "Related Work": "Prior research has explored modifications to the patch-embedding layer in ViTs. For instance, one study demonstrated that adding aLayerNorm after patch-embedding enhances ViTs resilience to image corruptions on smaller datasets. Another study replaced thestandard Transformer stem with a series of stacked stride-two 3x3 convolutions with batch normalizations, resulting in improvedsensitivity to optimization hyperparameters and increased final accuracy. Further analysis of LayerNorm has shown that the derivatives of the mean and variance significantly contribute to performance, asopposed to forward normalization. Alternative strategies like Image-LN and Patch-LN have been considered for efficiently traininga single model across different patch sizes. Some researchers have added extra LayerNorms before the final dense projection inthe self-attention block and the non-linearity in the MLP block, employing a different initialization strategy. Others have proposedadding LayerNorms after the final dense projection in the self-attention block, along with a LayerNorm after the non-linearity in theMLP block. In contrast to previous studies, our work demonstrates that applying LayerNorms both before and after the embedding layerconsistently enhances performance in classification and contrastive learning tasks. While other research has focused on incorporatingconvolutional inductive biases into Vision Transformers, our study exclusively and thoroughly examines LayerNorm placementswithin the standard ViT architecture.",
  "Patch Embedding Layer in Vision Transformer": "Vision Transformers consist of a patch embedding layer (PE) followed by multiple Transformer blocks. The PE layer first transformsan image x RHW 3 into a sequence of patches xp RP 2P 2HW , where P is the patch size. Each patch is then independentlyprojected using a dense layer, creating a sequence of \"visual tokens\" xt RHW P 2D. The patch size P determines the trade-offbetween the granularity of the visual tokens and the computational demands of subsequent Transformer layers.",
  "Setup": "We utilize the standard Vision Transformer formulation, which has demonstrated broad applicability across various vision tasks. Wetrain ViT architectures, both with and without DPN, in a supervised manner on three datasets with varying numbers of examples:ImageNet-1k (1M), ImageNet-21k (21M), and JFT (4B). In our experiments, we apply DPN directly to the baseline ViT recipeswithout any additional hyperparameter tuning. We divide the ImageNet training set into training and validation subsets and use thevalidation set to finalize the DPN recipe. For ImageNet-1k, we train five architectures: Ti/16, S/16, S/32, B/16, and B/32 using a standard recipe for 93,000 steps with a batchsize of 4,096. We report the accuracy on the official ImageNet validation split. Additionally, we evaluate an S/16 baseline (S/16+)with extensive hyperparameter tuning on ImageNet. We also apply DPN to the base and small DeiT variants.",
  "On ImageNet-21k, we use a similar setup as ImageNet-1k and report ImageNet 25-shot accuracies in two training regimes: 93K and930K steps": "For JFT, we evaluate the ImageNet 25-shot accuracies of three variants (B/32, B/16, and L/16) in two training regimes (220K and1.1M steps) with a batch size of 4,096, without additional data augmentation or mixup regularization. We report the 95% confidence interval across at least three independent runs on ImageNet-1k. Due to the computational cost oftraining on ImageNet-21k and JFT, we train each model once and report the mean 25-shot accuracy with a 95% confidence intervalacross three random seeds.",
  "DPN versus alternate LayerNorm placements": "Each Transformer block in ViT includes a self-attention (SA) and an MLP layer. Following the pre-LN strategy, LN is placed beforeboth the SA and MLP layers. We first demonstrate that the default pre-LN strategy in ViT models is nearly optimal by evaluatingalternative LN placements on ImageNet-1k. We then compare this with the performance of NormFormer, Sub-LN, and DPN. For each SA and MLP layer, we evaluate three LN placements: Pre, Post, and Pre+Post, resulting in nine total LN placementconfigurations. Additionally, we assess the LayerNorm placements in NormFormer and Sub LayerNorm, which add extra Layer-Norms within the self-attention and MLP layers in the transformer block. shows that none of these placements significantlyoutperform the default Pre-LN strategy, indicating that the default strategy is close to optimal. NormFormer provides someimprovements on ViT models with a patch size of 32. However, DPN consistently enhances performance across all five architectures. : This plot illustrates the accuracy gains achieved by various LayerNorm placement strategies over the default pre-LNstrategy. Each blue point represents a different LN placement within the Transformer block. None of the alternative placementssurpass the default Pre-LN strategy on ImageNet-1k. The application of DPN (represented by the black cross) consistently improvesperformance across all five architectures.",
  "Comparison to ViT": "(left) shows that DPN improved the accuracy of B/16, the best ViT model, by 0.7, while S/32 achieved the maximumaccuracy gain of 1.9. The average gain across all architectures is 1.4. On top of DeiT-S and DeiT-B, DPN provides improvements of0.3 and 0.2, respectively. Furthermore, we fine-tune B/16 and B/32 models with and without DPN on high-resolution ImageNet(384x384) for 5,000 steps with a batch size of 512. Applying DPN improves the high-resolution, fine-tuned B/16 and B/32 by 0.6and 1.0, respectively. DPN enhances all architectures trained on ImageNet-21k (, right) and JFT () in shorter training regimes, with averagegains of 1.7 and 0.8, respectively. In longer training regimes, DPN improves the accuracy of the best-performing architectures onJFT and ImageNet-21k by 0.5 and 0.4, respectively. In three cases (Ti/16 and S/32 with ImageNet-21k, and B/16 with JFT), DPN matches or slightly underperforms compared to thebaseline. Nevertheless, across a large proportion of ViT models, simply applying DPN out-of-the-box on top of well-tuned ViTbaselines leads to significant improvements. : Left: ImageNet-1k validation accuracies of five ViT architectures with and without Dual PatchNorm after 93,000 steps.Right: Training ViT models on ImageNet-21k in two regimes (93k and 930k steps) with a batch size of 4,096, showing ImageNet25-shot accuracies with and without Dual PatchNorm.",
  "Finetuning on ImageNet with DPN": "We fine-tune four models trained on JFT-4B with two resolutions on ImageNet-1k: (B/32, B/16) (220K, 1.1M) steps at resolutions224x224 and 384x384. For B/32, we observe consistent improvement across all configurations. With L/16, DPN outperforms thebaseline in three out of four configurations. : Left: Training three ViT models on JFT-4B in two regimes (200K and 1.1M steps) with a batch size of 4,096, showingImageNet 25-shot accuracies with and without DPN. Right: Corresponding full fine-tuning results on ImageNet-1k.",
  "Finetuning on VTAB": "We fine-tune ImageNet-pretrained B/16 and B/32 models, both with and without DPN, on the Visual Task Adaptation Benchmark(VTAB), which consists of 19 datasets categorized as Natural, Specialized, and Structured. Natural datasets contain imagescaptured with standard cameras, Specialized datasets have images from specialized equipment, and Structured datasets require scenecomprehension. We use the VTAB training protocol, which defines a standard training split of 800 examples and a validation split of200 examples per dataset. We perform a lightweight sweep across three learning rates for each dataset and select the best modelbased on the mean validation accuracy across three seeds. The corresponding mean test scores across three seeds are reported in. On Natural datasets, which are most similar to the source dataset ImageNet, B/32 and B/16 with DPN significantly outperform thebaseline in 7 out of 7 and 6 out of 7 datasets, respectively. The only exception is Sun397, where DPN performs worse. However,additional experiments show that DPN is beneficial when B/16 is trained from scratch on Sun397. On Structured datasets, applyingDPN improves accuracy in 4 out of 8 datasets and remains neutral in 2 for both B/16 and B/32. On Specialized datasets, DPNimproves performance in 1 out of 4 datasets and is neutral in 2. In conclusion, DPN offers the most significant improvements whenfine-tuned on Natural datasets. For Structured and Specialized datasets, DPN serves as a lightweight alternative that can enhance orat least not harm performance in most cases. : Evaluation of DPN on VTAB. When fine-tuned on Natural datasets, B/32 and B/16 with DPN significantly outperform thebaseline in 7 out of 7 and 6 out of 7 datasets, respectively. On Structured datasets, DPN improves both B/16 and B/32 in 4 out of 8datasets and remains neutral in 2. On Specialized datasets, DPN improves performance in 1 out of 4 datasets and is neutral in 2.",
  "Contrastive Learning": "We apply DPN to image-text contrastive learning. Each minibatch consists of image and text pairs. We train a text and imageencoder to map an image to its correct text over all other texts in the minibatch. Specifically, we adopt a method where we initializeand freeze the image encoder from a pretrained checkpoint and train the text encoder from scratch. To evaluate zero-shot ImageNetaccuracy, we represent each ImageNet class by its text label, which the text encoder maps into a class embedding. For a given imageembedding, the prediction is the class corresponding to the nearest class embedding. We evaluate four frozen image encoders: two architectures (B/32 and L/16) trained with two schedules (220K and 1.1M steps). Wereuse standard hyperparameters and train only the text encoder using a contrastive loss for 55,000 steps with a batch size of 16,384. shows that on B/32, DPN improves over the baselines in both setups, while on L/16, DPN provides improvement when theimage encoder is trained with shorter training schedules.",
  "Semantic Segmentation": "We fine-tune ImageNet-pretrained B/16 models, with and without DPN, on the ADE-20K 512x512 semantic segmentation task.Following established methods, a single dense layer maps the ViT features into per-patch output logits. A bilinear upsampling layerthen transforms the output distribution into the final high-resolution 512x512 semantic segmentation output. We fine-tune the entireViT backbone with a standard per-pixel cross-entropy loss. reports the mean mIOU across 10 random seeds and differentfractions of training data. The improvement in IoU is consistent across all setups. : Fine-tuning ImageNet pretrained B/16 models with and without DPN on the ADE20K Semantic Segmentation task, withvarying fractions of ADE20K training data. The table reports the mean IoU across ten random seeds. Applying DPN improves IoUacross all settings.",
  "Ablations": "Is normalizing both the inputs and outputs of the embedding layer optimal? In Eq 4, DPN applies LN to both the inputs and outputsof the embedding layer. We evaluate three alternative strategies: Pre, Post, and Post PosEmb. Pre applies LayerNorm only tothe inputs, Post applies it only to the outputs, and Post PosEmb applies it to the outputs after they are summed with positionalembeddings. shows the accuracy gains with these alternative strategies. Pre is unstable on B/32, leading to a significant drop in accuracy,and it also results in minor accuracy drops on S/32 and Ti/16. Post and Post PosEmb perform worse on smaller models (B/32, S/32,and Ti/16). Our experiments demonstrate that applying LayerNorm to both inputs and outputs of the embedding layer is necessaryfor consistent accuracy improvements across all ViT variants. : Ablations of various components of DPN. Pre: LayerNorm only to the inputs of the embedding layer. Post: LayerNormonly to the outputs of the embedding layer. No learnable: Per-patch normalization without learnable LayerNorm parameters. Onlylearnable: Learnable scales and shifts without standardization.",
  "Pre-0.10.0-2.6-0.2-0.3Post0.0-0.2-0.5-0.7-1.1Post PosEmb0.0-0.1-0.4-0.9-1.1Only learnable-0.8-0.9-1.2-1.6-1.6RMSNorm0.0-0.1-0.4-0.5-1.7No learnable-0.50.0-0.2-0.1-0.1": "Normalization vs. Learnable Parameters: As seen in Sec. 3.2, LayerNorm involves a normalization operation followed by learnablescales and shifts. We also ablate the effect of each of these operations in DPN. Applying only learnable scales and shifts without normalization significantly decreases accuracy across all architectures (See: Onlylearnable in ). Additionally, removing the learnable parameters leads to unstable training on B/16 (No learnable in ).Finally, removing the centering and bias parameters, as done in RMSNorm, reduces the accuracy of B/32, S/32, and Ti/16. Weconclude that while both normalization and learnable parameters contribute to the success of DPN, normalization has a greaterimpact.",
  "Gradient Norm Scale": "We present per-layer gradient norms for B/16, both with and without DPN. (Left) displays the mean gradient norm of the last1000 training steps as a function of depth. Notably, the gradient norm of the base ViT patch embedding (black) is disproportionatelylarge compared to other layers. Applying DPN (red) scales down the gradient norm of the embedding layer. (Right) furthershows that the gradient norm of the embedding layer is reduced not only before convergence but also throughout the training process.This characteristic is consistent across ViT architectures of different sizes.",
  "Visualizing Scale Parameters": "The first LayerNorm in Eq. 4 is applied directly to patches, i.e., raw pixels. Thus, the learnable parameters (biases and scales) ofthe first LayerNorm can be visualized directly in pixel space. shows the scales of our smallest and largest models: Ti/16trained on ImageNet for 90,000 steps and L/16 trained on JFT for 1.1M steps, respectively. Since the absolute magnitude of the scaleparameters varies across the R, G, and B channels, we visualize the scale separately for each channel. Interestingly, for both models,the scale parameter increases the weight of the pixels in the center of the patch and at the corners."
}