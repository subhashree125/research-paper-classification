{
  "Introduction": "The intersection of machine learning and privacy has become a significant area of study within thefield of computer science. While privacy-preserving techniques such as differential privacy offerpotential solutions, some machine learning systems, particularly those designed for biometric analysisor behavioral profiling, inherently compromise individual privacy. Therefore, there is a crucial needto explore methods beyond these traditional approaches. Although various definitions and frameworks for privacy have been proposed, a universal consensusremains elusive. This paper focuses on specific harms to privacy caused or made worse by machinelearning systems. In an era of powerful algorithms and massive datasets, maintaining privacy isincreasingly challenging, given that facial recognition systems can identify individuals in publicspaces, targeted advertising can exploit user profiles, and predictive policing algorithms can singleout individuals for surveillance. This paper addresses these unique threats to privacy that machinelearning systems enable. This research provides an overview of strategies developed to combat privacy-threatening machinelearning systems and advocates for increased collaboration between the machine learning communityand experts in the field of human-computer interaction (HCI). Two main approaches are discussed:first, challenging the data that feeds these models through obfuscation or data withholding, andsecond, directly challenging the model itself through public pressure or regulation. This papersuggests that computer scientists have an important role to play in both these approaches.",
  "Challenging Data": "Machine learning systems depend on data for both training and operation. Data is used to trainmachine learning models, and new data is fed into the models to generate predictions. These trainingand deployment stages can be iterative; models can be updated using new data over time. One way tooppose a machine learning system is by disrupting the data it relies on. This involves strategies suchas data obfuscation or withholding of data.",
  "Obfuscation": "One method for avoiding machine learning surveillance is by altering either the data used to makepredictions or the data used to train the system. For example, research has shown that glasses canbe designed to deceive facial recognition systems. This type of method uses adversarial examples,where a slight modification to a data point is enough to cause misclassification by a machine learning",
  "Withholding Data": "An alternative approach to altering data is to withhold it entirely. This can be achieved throughprivacy-enhancing technologies that block web tracking. While tracker-blocking browser extensionscan provide some privacy to individuals, data can also be withheld collectively. Data strikes, a formof digital boycott, can apply pressure to technology companies. Protest non-use is another way ofwithholding data, where people stop using platforms due to privacy concerns. These methods gobeyond simple evasion, using the act of withholding data as a way to launch broader campaignsagainst surveillance systems.",
  "Challenging Models": "While data-oriented approaches are helpful, policy solutions may offer a more effective way toresist machine learning surveillance systems. For example, while strategies can help evade facialrecognition, banning the technology would render those strategies unnecessary. There are manyforms that regulation can take and many roles that computer scientists can play in this process. One method of pressuring companies that develop surveillance technologies is through auditing.Research audits of facial recognition systems have shown they perform poorly on darker-skinnedsubjects, which has led to wrongful arrests. These audits have led some companies to stop sellingfacial recognition technology. However, audits do have limitations, as they can sometimes normalizeharmful tasks for certain communities. Some technologies are difficult to audit due to restricted access. Nevertheless, these systems cansometimes be reverse-engineered to show potential societal harms. Predictive policing systems, forinstance, can amplify existing biases. Algorithmic audits or reverse engineering should focus onbroader societal implications of the technology to avoid merely shifting goal posts and algorithmicreformism. Researchers have partnered with community organizations to resist surveillance technologies, debunk-ing the myth that critics do not understand the technology, and demystifying complex algorithms. Itis important for researchers to approach these collaborations with humility, as community organizersbring their own areas of expertise. It is also crucial to recognize the academic communitys role in creating and upholding surveillancetechnologies. Computer science educators should make computings role in injustice more visible.Student-led efforts can help educate future computer scientists about the consequences of their work.",
  "Conclusion": "This paper has outlined various methods for resisting machine learning-based surveillance technolo-gies. It emphasizes the need for participatory methods when developing anti-surveillance technologies. While these participatory methods are common in HCI research, the machine learning communityhas paid less attention to it. The impact of surveillance technologies is disproportionately borne byalready marginalized groups. Therefore, it is critical that the design of anti-surveillance technologiesbe led by those who are most affected."
}