{
  "Abstract": "Deep learning models are known to be vulnerable to small input perturbations,which are known as adversarial examples. Adversarial examples are commonlycrafted to deceive a model either at training (poisoning) or testing (evasion). Westudy the combination of poisoning and evasion attacks. We show that using boththreat models can significantly improve the damaging effect of adversarial attacks.Specifically, we study the robustness of Graph Neural Networks (GNNs) understructural perturbations and develop a memory-efficient adaptive end-to-end attackfor this novel threat model using first-order optimization.",
  "Introduction": "Graph neural networks (GNNs) are increasingly used across many different fields, including productrecommendations and drug discovery. GNNs are, however, vulnerable to adversarial attacks in manydifferent tasks such as node classification, graph classification, link prediction and node embeddings.Given that such attacks are able to scale to very large graphs, studying the adversarial robustness ofGNNs has become increasingly important. GNNs can be attacked at test time (evasion) or duringtraining (poisoning). However, a combined threat model that includes both evasion and poisoninghas not been considered in prior literature. Such a model, is, nonetheless, plausible given the publicavailability of graphs or those extracted from sources such as social media sites. Our work is based on the concept of a symbiotic attack, which combines both evasion and poisoningattacks. A symbiotic attack aims to minimize classification accuracy on a test set. The attacker isconstrained by a global budget and manipulates the entire graph, rather than individual nodes. Weprovide a comparison of our approach against plain poisoning and evasion attacks. To this end, weadapt the previous PR-BCD attack to the symbiotic threat model, which results in attacks that arememory-efficient and scalable to large graphs. Our main findings are that symbiotic attacks are moreeffective than poisoning attacks alone, and that evasion attacks are affected by the size of the test set,while symbiotic attacks are less sensitive to test set size. The potential improvement given by thesymbiotic threat model indicates that it requires further study.",
  "Preliminaries": "Notation. We denote a graph by G, with n nodes, an adjacency matrix A {0, 1}nn, and a featurematrix X Rnd. A GNN applied to the graph is represented by f(G) with parameters . Wedenote the set of possible adversarial graphs that can be created from G as (G). Also, Latk andLtrain denote the adversarial and training objectives.",
  "PR-BCD": "Our work extends on the Projected Randomized Block Coordinate Descent (PR-BCD) attack. Simi-larly to the Projected Gradient Descent (PGD) attack, the adjacency matrix is relaxed to P nn,enabling continuous gradient updates. Each entry indicates the probability of flipping an edge, with thefinal perturbations sampled from Bernoulli(P). However, as the adjacency matrix grows quadraticallywith the number of nodes, scaling of the PGD becomes difficult with larger graphs. PR-BCD uses Randomized Block Coordinate Descent (R-BCD), updating a block of P at eachiteration. The projection step ensures the budget is enforced in expectation, i.e. E[Bernoulli(P)] = P < and P nn. After each iteration, rather than sampling the block again, thepromising entries of the block are kept, and only the remaining entries are resampled. PGD can also be applied for a poisoning attack (Meta-PGD). In our attacks, we employ the sameprinciple with PR-BCD for better scalability. While we only consider a single global budget , it ispossible to include more complex constraints when needed for a given application.",
  "Here, Lpois and Lev are separated for clarity even though they could be the same loss": "Threat Model. We model an attacker who aims to reduce a models performance on node classifica-tion tasks. Our attacker has full access to the graph, has knowledge of the models architecture, cancreate surrogate models, and can only access the trained model as a black-box. Finally, our attackerhas a limited global budget of edge insertions/removals. The Sequential Attack. A simple way to launch a symbiotic attack is to divide the budget and launcha poisoning attack with the first half, followed by an evasion attack with the second half. In thisattack, the poisoning step is not aware of a future evasion, but can improve performance by reducingthe classification margin of certain nodes. The Joint Attack. The poisoning attack can be designed to \"fit\" the future evasion graph by includingthe evasion attack in the poisoning loss. The poisoning loss is computed using the poisoned modelover the evasion graph. This results in a poisoning attack which not only reduces the models accuracy,but also makes it more vulnerable to evasion. Both the sequential and joint attacks can be instantiated using different evasion/poisoning attacks. Webuild upon PR-BCD because it scales well to larger graphs. Note that the sequential attack is actuallya special case of the joint attack, with zero iterations per inner evasion attack.",
  "Setup": "We compare the symbiotic threat model with evasion and poisoning attacks, using PR-BCD toimplement the evasion and poisoning attacks. These are evaluated on Cora, CiteSeer, and PubMeddatasets. We study the robustness of GCN, GAT, APPNP, and GPRGNN models. We also considerR-GCN and Jaccard purification as potential defense mechanisms. For each dataset, we allocate 20nodes of each class for the labeled training set and 10",
  "Results": "displays the perturbed accuracy values on the test set (10 percent of nodes) for our benchmarkdatasets and models, averaged over 10 runs, with the standard error of the mean also shown. Theattacker is given a 5 percent budget of the number of edges, and this budget is split equally betweenpoisoning and evasion for the symbiotic attacks. We report the best performing of the two symbioticattacks, and also note that the symbiotic attacks are consistently stronger than the poisoning attacks,and stronger than plain evasion. The symbiotic threat model is especially evident on the largerPubMed graph, where the accuracy drops to almost zero, for example, using a GCN.",
  "Effect of the Number of Test Nodes": "To highlight the differences between poisoning and evasion objectives, shows the perturbedaccuracies for evasion, poisoning, and symbiotic attacks with varying fractions of test nodes with aGCN and a 5 As the number of test nodes increases, evasion becomes much more challenging across all datasets.Although poisoning and symbiotic attacks also become more difficult with more test nodes, especiallyon PubMed, they are more robust than the evasion attack. Therefore, the reduction in performancecannot be explained by the attacks having to target a larger number of nodes with the same budget.The poisoning attack is less affected since it can manipulate the flow of information during training.The symbiotic attacks also benefit from this since they can reduce the base accuracy, making nodeseasier to misclassify during the evasion phase. The symbiotic attacks are also stronger than poisoningalone.",
  "Hyperparameters": "Block size. shows the results of the four attacks with varying block sizes, using a fixed 5percent budget and 125 iterations against a GCN. For small block sizes, the attacks are less effectivesince the PR-BCD optimization can only cover a small part of the adjacency matrix. However, largerblocks have decreasing marginal benefit when a large part of the adjacency matrix can be covered. Budget. shows how all four attacks follow a similar trend when increasing budget size. OnPubMed, changing 5 percent of edges is enough to achieve near-zero accuracy under the symbioticmodel. This highlights the devastating effect of joint attacks, especially in larger graphs with a smallnumber of labeled train nodes.",
  "ModelDatasetCleanEvasionPoisoningSymbiotic": "GCNCiteSeer0.68 0.010.41 0.010.4 0.010.38 0.01CiteSeer (ind.)0.67 0.010.41 0.010.62 0.010.33 0.01CiteSeer-J0.68 0.010.41 0.010.41 0.020.38 0.01Cora0.78 0.010.41 0.010.46 0.020.35 0.01Cora (ind.)0.75 0.020.42 0.010.68 0.030.3 0.01Cora-J0.74 0.010.39 0.010.43 0.020.36 0.01PubMed0.78 0.010.41 0.010.12 0.020.03 0.01PubMed-J0.77 0.010.41 0.010.11 0.010.02 0.0GATCiteSeer0.62 0.020.27 0.020.41 0.020.3 0.03CiteSeer (ind.)0.68 0.010.37 0.010.64 0.020.56 0.02CiteSeer-J0.64 0.010.32 0.030.41 0.030.3 0.03Cora0.69 0.020.22 0.020.48 0.030.29 0.02Cora (ind.)0.77 0.010.21 0.010.61 0.040.35 0.03Cora-J0.67 0.010.23 0.020.45 0.020.28 0.02PubMed0.73 0.010.38 0.040.41 0.010.2 0.03PubMed-J0.74 0.010.34 0.040.38 0.040.19 0.02APPNPCiteSeer0.69 0.010.45 0.010.56 0.010.47 0.01CiteSeer (ind.)0.71 0.010.47 0.010.66 0.020.4 0.01CiteSeer-J0.68 0.010.43 0.010.52 0.020.45 0.02Cora0.82 0.020.48 0.030.64 0.020.51 0.04Cora (ind.)0.82 0.020.53 0.020.78 0.010.37 0.01Cora-J0.82 0.010.5 0.010.67 0.010.54 0.01PubMed0.79 0.00.46 0.010.21 0.020.09 0.01PubMed-J0.77 0.010.45 0.010.19 0.030.1 0.02GPRGNNCiteSeer0.66 0.010.34 0.010.44 0.020.33 0.01CiteSeer (ind.)0.67 0.010.37 0.010.56 0.010.34 0.01CiteSeer-J0.65 0.010.35 0.010.44 0.010.35 0.01Cora0.82 0.010.46 0.010.53 0.010.4 0.01Cora (ind.)0.8 0.020.44 0.010.74 0.010.35 0.01Cora-J0.79 0.010.44 0.010.54 0.010.4 0.01PubMed0.78 0.010.42 0.010.28 0.030.08 0.02PubMed-J0.78 0.010.42 0.010.38 0.040.15 0.04RGCNCiteSeer0.63 0.010.39 0.010.59 0.020.47 0.01Cora0.74 0.020.44 0.010.74 0.010.52 0.02PubMed0.77 0.010.43 0.010.42 0.040.15 0.03",
  "showed that symbiotic attacks can be more effective than the evasion or poisoning approaches ontheir own. We will outline several avenues for future work": "The joint attack can be implemented using other evasion attacks, or attacks designed for the symbioticthreat model. In addition, our work considered global budgets, but it is easy to consider per-nodelocal budgets and targeted attacks as well. Moreover, we did not consider the use of different lossfunctions for the poisoning and evasion parts, which may also further improve attack performance.We plan to include further evaluations on these settings as our next step. Finally, novel poisoningattacks can be developed which utilize knowledge of a future evasion attack.",
  "B Sub-Gaussian Covering Numbers for ReLU Networks": "depicts an example of applying our safe predictor to a notional regression problem. Thisexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained networkconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.The safe predictor shares this structure with constrained predictors, G0 and G1, but each predictorhas its own fully connected layer. The training uses a sampled subset of points from the input space. shows an example of applying the safe predictor to a notional regression problem with a 2-Dinput and 1-D output and two overlapping constraints. The unconstrained network has two hiddenlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrainedpredictors, G00, G10, G01, and G11, share the hidden layers and have an additional hidden layer ofsize 20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset ofpoints from the input space and the learned predictors are shown for the continuous input space.",
  "C.1 Safeability Constraints": "The \"safeability\" property from prior work can be encoded into a set of input-output constraints. The\"safeable region\" for a given advisory is the set of input space locations where that advisory can beselected such that future advisories exist that will prevent an NMAC. If no future advisories exist, theadvisory is \"unsafeable\" and the corresponding input region is the \"unsafeable region\". Examples ofthese regions, and their proximity functions are shown in for the CL1500 advisory.",
  "C.2 Proximity Functions": "We start by generating the unsafeable region bounds. Then, a distance function is computed betweenpoints in the input space (vO vI, h, ), and the unsafeable region for each advisory. These are nottrue distances but are 0 if and only if the data point is within the unsafeable set. These are then usedto produce proximity functions. shows examples of the unsafeable region, distance function,and proximity function for the CL1500 advisory.",
  "C.3 Structure of Predictors": "The compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hiddenlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for theunconstrained network. For constrained predictors, we use a similar architecture, but share the firstfour layers for all predictors. This provides a common learned representation of the input space, whileallowing each predictor to adapt to its constraints. Each constrained predictor has two additionalhidden layers and their outputs are projected onto our convex approximation of the safe output region,using Gb(x) = minj Gj(x) . In our experiments, we used = 0.0001. With this construction, we needed 30 separate predictors to enforce the VerticalCAS safeabilityconstraints. The number of nodes for the unconstrained and safe implementations were 270 and2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders ofmagnitude."
}