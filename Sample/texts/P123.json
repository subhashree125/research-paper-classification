{
  "Abstract": "This research delves into the application of a vast collection of emoji occurrencesto acquire versatile representations applicable to diverse domains for the purposeof identifying sentiment, emotion, and sarcasm. Natural Language Processing(NLP) tasks frequently encounter limitations due to the deficiency of manuallylabeled data. In the realm of social media sentiment analysis and associated tasks,researchers have thus employed binarized emoticons and specific hashtags as ameans of distant supervision. Our study demonstrates that by broadening distantsupervision to include a more varied array of noisy labels, models can achievericher representations. Through emoji prediction on a dataset encompassing 1,246million tweets, each including one of 64 prevalent emojis, we achieve state-of-the-art results on eight benchmark datasets focusing on sentiment, emotion, andsarcasm detection, all with the aid of a singular pre-trained model. Our findingsaffirm that the diversity inherent in our emotional labels leads to an enhancementin performance compared to previous distant supervision methods.",
  "Introduction": "This paper addresses the challenge that numerous Natural Language Processing (NLP) tasks face dueto the lack of sufficient manually annotated data. Consequently, emotional expressions that co-occurwith text have been utilized for distant supervision in sentiment analysis and related tasks withinsocial media. This allows models to acquire valuable text representations before directly modelingthese specific tasks. For example, state-of-the-art methods for sentiment analysis in social mediafrequently use positive and negative emoticons to train their models. Similarly, in prior research,hashtags like #anger, #joy, #happytweet, #ugh, #yuck, and #fml have been categorized into emotionallabels for use in emotion analysis. The practice of using distant supervision on noisy labels often leads to enhanced performance inthe target task. In this paper, we present evidence that expanding distant supervision to a morevaried selection of noisy labels enables models to develop more detailed representations of emotionalcontent in text. This, in turn, improves performance on benchmark datasets designed for the detectionof sentiment, emotions, and sarcasm. We further demonstrate that the representations learned by asingle pre-trained model can be successfully generalized across five different domains.",
  "showcases example sentences which were scored by our model. For every sentence, the fivemost probable emojis are displayed, alongside the models estimated probabilities": "Emojis do not always function as straightforward labels of emotional content. For instance, apositive emoji might clarify an ambiguous sentence or supplement text that might otherwise beseen as somewhat negative. While this is true, our results demonstrate that emojis can still beused to accurately categorize the emotional content of texts in numerous scenarios. Our DeepMojimodel, for instance, is able to capture various interpretations of the word love and slang termslike this is the shit as having positive connotations (as illustrated in ). To enable others toexplore the prediction capabilities of our model, we have made an online demonstration available atdeepmoji.mit.edu. Our work makes the following contributions: We demonstrate that a vast number of readily accessibleemoji occurrences on Twitter can be used to pre-train models for richer emotional representation thanis typically achieved through distant supervision. We then transfer this learned knowledge to targettasks using a novel layer-wise fine-tuning approach. This technique yields significant improvementsover state-of-the-art methods in areas such as emotion, sarcasm, and sentiment detection. Throughextensive analyses on the influence of pre-training, our results highlight that the variety present in ouremoji set plays a crucial role in the transfer learning capabilities of our model. We have made ourpre-trained DeepMoji model publicly available to aid in a range of NLP tasks.",
  "Related work": "The use of emotional expressions as noisy labels in text to address the scarcity of labels is not a newconcept. Initially, binarized emoticons served as noisy labels, but subsequent research has utilizedhashtags and emojis. Previous studies have always manually determined which emotional categoryeach emotional expression should belong to. Prior efforts have made use of emotion theories, such asEkmans six basic emotions and Plutchiks eight basic emotions. Such manual categorization necessitates an understanding of the emotional content inherent to eachexpression, which can be challenging and time-consuming for complex emotional combinations.Furthermore, any manual selection and categorization carries the potential for misinterpretationsand might overlook essential details concerning usage. In contrast, our methodology requires noprior knowledge of the corpus and can capture the diverse usage of 64 emoji types ( presentsexamples, and shows how the model implicitly organizes emojis). An alternative approach to automatically interpreting the emotional content of an emoji involveslearning emoji embeddings from the words defining emoji-semantics, as found in official emoji tables.In our study, this approach has two significant limitations: (a) It requires emojis to be present duringtesting, whereas several domains have limited or no emoji usage. (b) The tables fail to capture thedynamic nature of emoji use, such as shifts in an emojis intended meaning over time. Knowledge from the emoji dataset can be transferred to target tasks in several ways. Multi-tasklearning, which involves training on multiple datasets at once, has been shown to have promisingresults. However, multi-task learning requires access to the emoji dataset whenever the classifierneeds to be adjusted for a new target task. Requiring access to the dataset can be problematic whenconsidering data access regulations. Data storage issues also arise, as the dataset used in this studycomprises hundreds of millions of tweets (see ). Instead, we use transfer learning which doesnot require access to the original dataset.",
  "Pretraining": "In many instances, emojis function as a stand-in for the emotional content of text. Therefore, pre-training a model to predict which emojis were initially part of a text can improve performance in thetarget task. Social media contains many short texts that use emojis which can be used as noisy labelsfor pretraining. We used data from Twitter spanning from January 1, 2013, to June 1, 2017, but anydata set containing emoji occurrences could be used. The pretraining data set uses only English tweets that do not contain URLs. We think the contentobtained from the URL is important for understanding the emotional content of the text in the tweet.Because of this we expect emojis associated with tweets containing URLs to be noisier labels thanthose in tweets without URLs, therefore the tweets with URLs have been removed. Proper tokenization is crucial for generalization. All tweets are tokenized word-by-word. Wordscontaining two or more repeated characters are shortened to the same token (for example, loool andlooooool are tokenized as the same). We also use a special token for all URLs (which is relevantonly for the benchmark datasets), user mentions (for example, @acl2017 and @emnlp2017 aretreated the same), and numbers. To be included in the training set, a tweet must have at least onetoken that is not a punctuation mark, emoji, or special token. Many tweets repeat the same emoji or contain multiple distinct emojis. To address this in our trainingdata, for each unique emoji type, we save a separate tweet for pretraining, using that emoji type asthe label. Regardless of the number of emojis associated with the tweet, we save only a single tweetfor the pretraining for each unique emoji type. This pre-processing of data enables the pretraining tocapture that multiple kinds of emotional content can be associated with the tweet. It also makes ourpretraining task a single-label classification instead of a more complex multi-label classification. To ensure that the pretraining encourages the models to learn a thorough understanding of theemotional content of text instead of just the emotional content associated with frequently used emojis,we create a balanced pretraining dataset. The pretraining data is split into training, validation, and testsets. The validation and test sets are randomly sampled such that each emoji is represented equally.The remaining data is upsampled to generate a balanced training dataset.",
  "Model": "With the availability of millions of emoji occurrences, we are able to train expressive classifierswith a limited risk of overfitting. We utilize a variant of the Long Short-Term Memory (LSTM)model, which has been successful in numerous NLP tasks. Our DeepMoji model uses an embeddinglayer with 256 dimensions to project each word into a vector space. A hyperbolic tangent activationfunction is used to ensure each embedding dimension remains within the range . To understandeach word in the context of the text, we use two bidirectional LSTM layers with 1024 hidden unitseach (512 in each direction). Lastly, we employ an attention layer that accepts all these layers asinput through skip connections. ( presents an illustration). The attention mechanism enables the model to determine the importance of each word for theprediction task by weighting the words as it creates the text representation. A word like \"amazing\" ishighly informative of the emotional meaning of a text and so should be treated accordingly. We use abasic method, taking inspiration from prior work, with a single parameter for each input channel:",
  "j=1 exp(ej)v =aihi(1)": "Here, ht stands for the representation of the word at time step t, and wa is the weight matrix forthe attention layer. The attention importance scores for each time step, at, are determined bymultiplying the representations by the weight matrix, and then normalizing them to establish aprobability distribution across the words. Finally, the texts representation vector, v, is found using aweighted summation over all time steps, with the attention importance scores used as weights. Therepresentation vector that comes from the attention layer is a high-level encoding of the whole text.This is used as input into the final Softmax layer for classification. We have found that the addition ofthe attention mechanism and skip connections enhances the models capabilities for transfer learning. The only form of regularization used for the pretraining is L2 regularization with a coefficient of106 on the embedding weights. For fine-tuning, further regularization is applied. We implementedour model using Theano and have made an easy-to-use version available that utilizes Keras.",
  "Transfer learning": "Our pre-trained model can be fine-tuned for a target task in several ways. Some methods involvefreezing layers by disabling parameter updates to prevent overfitting. One popular approach isto utilize the network as a feature extractor, where all model layers except the final one are frozenduring fine-tuning (we will call this the \"last\" approach). An alternative method is to use the pre-trained model for initialization, where the full model is unfrozen (which we will refer to as the fullapproach). We put forward a new, simple transfer learning approach we are calling \"chain-thaw.\" This approachsequentially unfreezes and fine-tunes one layer at a time. It increases accuracy on the target task, butrequires more computational power for the fine-tuning process. By separately training each layer,the model can adjust individual patterns across the network while reducing the risk of overfitting. Itappears that this sequential fine-tuning has a regularizing effect, similar to the layer-wise trainingexplored for unsupervised learning. More specifically, the chain-thaw approach starts by fine-tuning any new layers (often only a Softmaxlayer) to the target task until the validation set converges. Then, the approach individually fine-tuneseach layer, starting with the first layer in the network. Lastly, the entire model is trained with alllayers. Each time the model converges (as measured on the validation set), the weights are restored totheir optimal setting, preventing overfitting in a similar manner to early stopping. illustratesthis process. If only step a) in the figure is performed, this is the same as the last approach, wherethe existing network is used as a feature extractor. Likewise, only performing step d) is the same asthe full approach, where the pre-trained weights are used as the initialization for a fully trainablenetwork. While the chain-thaw procedure may seem extensive, it can be implemented with just a fewlines of code. Also, the added time spent on fine-tuning is not large, when considering the use ofGPUs on small datasets of manually annotated data which is often the case. The chain-thaw approach has the benefit of expanding the vocabulary to new domains with a lowrisk of overfitting. For a given dataset, up to 10,000 new words from the training set are added to thevocabulary.",
  "Emoji prediction": "We use a raw dataset of 56.6 billion tweets, which is filtered down to 1.2 billion relevant tweets. Inthe pretraining dataset, a single copy of a tweet is stored for every unique emoji, resulting in a datasetwith 1.6 billion tweets. shows the distribution of tweets across different emoji types. We useda validation set and a test set, both containing 640K tweets (10K of each emoji type), to evaluateperformance on the pretraining task. The remaining tweets were used for the training set, which wasbalanced using upsampling. The performance of the DeepMoji model on the pretraining task was evaluated, with the results shownin . We use both top 1 and top 5 accuracy for the evaluation as the emoji labels are noisyand multiple emojis can potentially be appropriate for a given sentence. For comparison purposes,we also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-wordsclassifier, fastText, which has recently shown competitive results. We use a 256 dimension vectorfor the fastText classifier, making it almost identical to only using the embedding layer from theDeepMoji model. The difference in top 5 accuracy between the fastText classifier (36.2%) and thelargest DeepMoji model (43.8%) highlights the difficulty of the emoji prediction task. Since the twoclassifiers only differ in that the DeepMoji model has LSTM layers and an attention layer betweenthe embedding and the Softmax layer, this difference in accuracy demonstrates the importance ofcapturing each words context.",
  "Benchmarking": "We evaluate our method on 3 distinct NLP tasks using 8 datasets across 5 domains. For faircomparison, DeepMoji is compared to other methods that utilize external data sources in additionto the benchmark dataset. We used an averaged F1 measure across classes for evaluating emotionanalysis and sarcasm detection, as these consist of unbalanced datasets. Sentiment datasets areevaluated using accuracy. Many benchmark datasets have an issue with data scarcity, especially in emotion analysis. Manystudies that introduce new methods for emotion analysis often evaluate their performance on a singlebenchmark dataset, SemEval 2007 Task 14, which contains only 1250 data points. There has beencriticism regarding the use of correlation with continuous ratings as a measure, making only thesomewhat limited binary evaluation possible. We only evaluate the emotions Fear, Joy, Sadnessbecause the remaining emotions are found in less than 5 To fully assess our method on emotion analysis, we make use of two other datasets. First, a datasetof emotions in tweets about the Olympic Games, created by Sintsova et al. which we convert toa single-label classification task. Second, a dataset of self-reported emotional experiences from alarge group of psychologists. Because these two datasets have not been evaluated in prior work,we compare against a state-of-the-art approach based on a valence-arousal-dominance framework.The scores extracted using this framework are mapped to the classes in the datasets using logisticregression with cross-validation parameter optimization. We have made our preprocessing codeavailable so that these two datasets may be used for future benchmarking in emotion analysis. We assessed the performance of sentiment analysis using three benchmark datasets. These smalldatasets were chosen to highlight the significance of the transfer learning capabilities of the evaluatedmodels. Two datasets, SS-Twitter and SS-Youtube, are from SentiStrength and follow the relabelingas described by prior work to create binary labels. The third dataset is from SemEval 2016 Task4A.Because tweets are often deleted from Twitter, the SemEval dataset has experienced data decay. Thismakes comparisons across papers difficult. Approximately 15 The current state of the art in sentiment analysis on social media (and winner of SemEval 2016 Task4A) uses an ensemble of convolutional neural networks that are pre-trained on a private dataset oftweets with emoticons. This makes it difficult to replicate. As a substitute, we pre-train a model thatuses the hyperparameters of the largest model in their ensemble on the positive/negative emoticondataset. Using this pretraining as an initialization, we fine-tune the model on the target tasks, utilizingearly stopping based on a validation set. We implemented Sentiment-Specific Word Embeddings(SSWE), using embeddings available on the authors website, but found that it performed worse thanthe pretrained convolutional neural network, and these results have been excluded. presents a description of the benchmark datasets. Datasets that did not have pre-existingtraining/test splits were split by us, and these splits are publicly available. Data from the training setwas used for hyperparameter tuning.",
  "IdentifierStudyTaskDomainClassesNtrainNtest": "SE0714(Strapparava and Mihalcea, 2007)EmotionHeadlines32501000Olympic(Sintsova et al., 2013)EmotionTweets4250709PsychExp(Wallbott and Scherer, 1986)EmotionExperiences710006480SS-Twitter(Thelwall et al., 2012)SentimentTweets210001113SS-Youtube(Thelwall et al., 2012)SentimentVideo Comments210001142SE1604(Nakov et al., 2016)SentimentTweets3715531986SCv1(Walker et al., 2012)SarcasmDebate Forums21000995SCv2-GEN(Oraby et al., 2016)SarcasmDebate Forums210002260 : Description of benchmark datasets. Datasets without pre-existing training/test splits are splitby us (with splits publicly available). Data used for hyperparameter tuning is taken from the trainingset. For sarcasm detection, we used versions 1 and 2 of the sarcasm dataset from the Internet ArgumentCorpus. It should be noted that the results from these benchmarks that are shown elsewhere are notdirectly comparable, as only a subset of the data is available online. We establish a state-of-the-artbaseline by modeling embedding-based features alongside unigrams, bigrams, and trigrams withan SVM. GoogleNews word2vec embeddings are used to compute the embedding-based features.Cross-validation was used to perform a hyperparameter search for regularization parameters. Thesarcasm dataset version 2 includes both a quoted text and a sarcastic response, but only the responsewas used to keep models consistent across the datasets. displays a comparison across benchmark datasets. The reported values are averages across5 runs. Variations refer to the transfer learning approaches that we discussed, and new refers to amodel trained without pretraining.",
  ": Comparison across benchmark datasets. Reported values are averages across five runs.Variations refer to transfer learning approaches with new being a model trained without pretraining": "We used the Adam optimizer for training, with the gradient norm clipped to 1. For training all newlayers, we set the learning rate to 103 and to 104 when fine-tuning any pre-trained layers. Toprevent overfitting on the small datasets, 10 demonstrates that the DeepMoji model outperforms the state of the art across all the benchmarkdatasets and that our new chain-thaw method yields the highest transfer learning performance. Theresults are averaged across 5 runs to reduce the variance. We confirm statistical significance usingbootstrap testing with 10,000 samples, our model performance was statistically better than thestate-of-the-art across all benchmark datasets (p < 0.001). Our model exceeds the performance of the state of the art even on datasets that come from differentdomains than the tweets that the model was pre-trained on. A crucial difference between thepretraining dataset and the benchmark datasets is the length of the observations. The average numberof tokens per tweet in the pretraining dataset is 11. Meanwhile, board posts from the InternetArgument Corpus version 1 (for example), have an average of 66 tokens, with some posts being muchlonger.",
  "Importance of emoji diversity": "A key difference between this work and prior research that used distant supervision is the variety innoisy labels. For example, other studies only used positive and negative emoticons as noisy labels.Other studies used more nuanced sets of noisy labels, but our set is the most varied known to us. Toinvestigate the effect of using a diverse set of emojis, we created a subset of our pretraining data thatincluded tweets with one of 8 emojis, which are similar to the positive/negative emoticons used inother work. Because the dataset based on this reduced set of emojis contains 433 million tweets, anyperformance differences on benchmark datasets are more likely linked to the diversity of the labelsthan to differences in dataset sizes. We trained our DeepMoji model to predict whether tweets contained positive or negative emojis,and we evaluated this pre-trained model on benchmark datasets. We call this the DeepMoji-PosNegmodel. To assess the emotional representations learned by the two pre-trained models, we used thelast transfer learning approach to allow the models to map already learned features to classes in the target datasets. shows that DeepMoji-PosNeg performs worse than DeepMoji across all 8benchmarks. This demonstrates that the diversity of our emoji types enables the model to acquirericher representations of emotional content in text, which in turn is more useful for transfer learning. compares benchmarks using a smaller emoji set (Pos/Neg emojis) or a standard architecture(standard LSTM). Results for DeepMoji from have been added for comparison. The evaluationmetrics are the same as in . Reported values are averages across 5 runs.",
  "SE0714.32.35.36Olympic.55.57.61PsychExp.40.49.56SS-Twitter.86.86.87SS-Youtube.90.91.92SE1604.56.57.58SCv1.66.66.68SCv2-GEN.72.73.74": ": Benchmarks using a smaller emoji set (Pos/Neg emojis) or a classic architecture (standardLSTM). Results for DeepMoji from are added for convenience. Evaluation metrics are as in. Reported values are the averages across five runs. Many emojis express similar emotional content, but have subtle variations in usage that our modelcan capture. By using hierarchical clustering on the correlation matrix of the DeepMoji modelspredictions on the test set, we can see that the model captures many expected similarities ().For example, the model groups emojis into broad categories related to negativity, positivity, or love.It also differentiates within these categories. For example, mapping sad emojis to one subcategory ofnegativity, annoyed emojis to another subcategory, and angry emojis to a third.",
  "Model architecture": "Our DeepMoji model architecture employs an attention mechanism and skip connections, which assistin transferring learned representations to new domains and tasks. Here, we compare the DeepMojimodel architecture to a standard 2-layer LSTM. Both were compared using the last transfer learningapproach, and all regularization and training parameters were consistent. shows that the DeepMoji model performs better than a standard 2-layer LSTM across all thebenchmark datasets. These two architectures performed equally on the pretraining task. This indicatesthat the DeepMoji model architecture is better for transfer learning, even if it is not necessarily betterfor a single supervised classification task with an abundance of available data. We believe that the improvements in transfer learning can be attributed to two factors: (a) Theattention mechanism with skip connections provides straightforward access to learned low-levelfeatures for any time step, making it easy to use this information if needed for a new task. (b) The skipconnections improve the gradient flow from the output layer to the early layers in the network. Thisis useful when parameters in early layers are adjusted as a part of transfer learning to small datasets.Further analysis of these factors in future work would allow us to confirm why our architectureoutperforms a standard 2-layer LSTM.",
  "Analyzing the effect of pretraining": "The target tasks performance benefits significantly from pretraining, as shown in . Here,we separate the effects of pretraining into two factors: word coverage and phrase coverage. Thesetwo effects provide regularization to the model, preventing overfitting (the supplementary materialincludes a visualization of this regularization). There are multiple ways of expressing sentiment, emotion, or sarcasm. Because of this, the test setmay contain language use not present in the training set. Pretraining helps the target task modelsfocus on low-support evidence by having already seen similar language in the pretraining dataset.To examine this effect, we measure the improvement in word coverage on the test set when using pretraining. Word coverage is defined as the percentage of words in the test dataset that were alsoseen in the training/pretraining dataset (as shown in ). One key reason that the chain-thawapproach outperforms other transfer learning approaches is its ability to tune the embedding layerwith a low risk of overfitting. shows how adding new words to the vocabulary as part of thetuning process increased word coverage. It is important to note that word coverage can be misleading in this context. In many small datasets, aword may occur only once in the training set. In contrast, all the words in the pretraining vocabularyare present in thousands or even millions of observations, enabling the model to learn a goodrepresentation of the emotional and semantic meaning. Therefore, the benefits of pretraining for wordrepresentations likely extend beyond the differences seen in . shows the word coverage on benchmark test sets. This compares the use of only the vocabularygenerated by finding words in the training data (own), the pretraining vocabulary (last), or acombination of both vocabularies (full / chain-thaw).",
  "SE071441.9%93.6%94.0%Olympic73.9%90.3%96.0%PsychExp85.4%98.5%98.8%SS-Twitter80.1%97.1%97.2%SS-Youtube79.6%97.2%97.3%SE160486.1%96.6%97.0%SCv188.7%97.3%98.0%SCv2-GEN86.5%97.2%98.0%": ": Word coverage on benchmark test sets using only the vocabulary generated by finding wordsin the training data (own), the pretraining vocabulary (last) or a combination of both vocabularies(full / chain-thaw). To analyze how important capturing phrases and the context of each word are, we evaluated theaccuracy on the SS-Youtube dataset using a fastText classifier that was pre-trained using the sameemoji dataset as our DeepMoji model. This fastText classifier is similar to only using the embeddinglayer from the DeepMoji model. We then evaluated the representations learned by fine-tuning themodels as feature extractors (using the last transfer learning approach). The fastText model achievedan accuracy of 63",
  "Comparing with human-level agreement": "To see how well our DeepMoji classifier performs compared to humans, we created a dataset ofrandomly selected tweets that were annotated for sentiment. Each tweet was annotated by a minimumof 10 English-speaking Amazon Mechanical Turkers (MTurks) who lived in the USA. The tweetswere rated on a scale from 1 to 9, with a Do not know option. Guidelines were provided to thehuman raters. The tweets were selected to contain only English text and no mentions or URLs, sothey could be rated without extra contextual information. Tweets where more than half the evaluatorschose Do not know were removed (98 tweets). For every tweet, we randomly select a single MTurk rating as the human evaluation. We average theremaining nine MTurk ratings to make the ground truth. The sentiment label for a given tweet is thusdefined as the overall consensus among raters, excluding the randomly selected human evaluationrating. To ensure clear separation between the label categories, we removed neutral tweets that fellwithin the interval [4.5, 5.5] (roughly 29",
  "Conclusion": "We have demonstrated how the abundance of text on social media containing emojis can be usedto pre-train models. This enables them to acquire representations of emotional content in text. Ourfindings demonstrate that the diversity of our emoji set is crucial to our methods performance. Thiswas found by comparing the model performance against an identical model that was pre-trained on asubset of emojis. Our pre-trained DeepMoji model is available for other researchers to use for diverseemotion-related NLP tasks."
}