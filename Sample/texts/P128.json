{
  "Introduction": "Discourse deixis (DD) resolution, also known as abstract anaphora resolution, is an under-investigatedtask that involves resolving a deictic anaphor to its antecedent. A deixis is a reference to a discourseentity such as a proposition, a description, an event, or a speech act. DD resolution is arguablymore challenging than the extensively-investigated entity coreference resolution task. Recall that inentity coreference, the goal is to cluster the entity mentions in narrative text or dialogue, which arecomposed of pronouns, names, and nominals, so that the mentions in each cluster refer to the samereal-world entity. Lexical overlap is a strong indicator of entity coreference, both among names (e.g.,President Biden, Joe Biden) and in the resolution of nominals (e.g., linking the president toPresident Biden). DD resolution, on the other hand, can be viewed as a generalized case of eventcoreference involving the clustering of deictic anaphors, which can be pronouns or nominals, andclauses, such that the mentions in each cluster refer to the same real-world proposition/event/speechact, etc. An example of DD resolution in which the deictic anaphor the move refers to Salomonsact of issuing warrants on shares described in the preceding sentence. DD resolution is potentiallymore challenging than entity coreference resolution because (1) DD resolution involves understandingclause semantics, which are arguably harder to encode than noun phrase semantics; and (2) stringmatching plays little role in DD resolution, unlike in entity coreference. We focus on end-to-end DD resolution in dialogue. While the deictic anaphors in dialogue are alsocomposed of pronouns and nominals, the proportion of pronominal deictic anaphors in dialogue ismuch higher than that in narrative text. For instance, the percentage of pronominal deictic anaphorsrises to 93 Since DD resolution can be cast as a generalized case of event coreference, a natural question is:how successful would a state-of-the-art entity coreference model be when applied to DD resolution?Recently, a re-implementation of a span-based entity coreference model has been applied to resolvethe deictic anaphors in the DD track after augmenting it with a type prediction model. Not onlydid they achieve the highest score on each dataset, but they beat the second-best system, which is anon-span-based neural approach combined with hand-crafted rules, by a large margin. These resultssuggest that a span-based approach to DD resolution holds promise. Our contributions are three-fold. First, we investigate whether task-specific observations can beexploited to extend a span-based model originally developed for entity coreference to improveits performance for end-to-end DD resolution in dialogue. Second, our extensions are effectivein improving model performance, allowing our model to achieve state-of-the-art results. Finally,we present an empirical analysis of our model, which, to our knowledge, is the first analysis of astate-of-the-art span-based DD resolver.",
  "Broadly, existing approaches to DD resolution can be divided into three categories, as describedbelow": "Rule-based approaches. Early systems that resolve deictic expressions are rule-based.Specifically, they use predefined rules to extract anaphoric mentions, and select antecedentfor each extracted anaphor based on the dialogue act types of each candidate antecedent. Non-neural learning-based approaches. Early non-neural learning-based approaches toDD resolution use hand-crafted feature vectors to represent mentions. A classifier is thentrained to determine whether a pair of mentions is a valid antecedent-anaphor pair. Deep learning-based approaches. Deep learning has been applied to DD resolution. Forinstance, a Siamese neural network is used, which takes as input the embeddings of twosentences, one containing a deictic anaphor and the other a candidate antecedent, to scoreeach candidate antecedent and subsequently rank the candidate antecedents based on thesescores. In addition, motivated by the recent successes of Transformer-based approachesto entity coreference, a Transformer-based approach to DD resolution has recently beenproposed, which is an end-to-end coreference system based on SpanBERT. Their modeljointly learns mention extraction and DD resolution and has achieved state-of-the-art results.",
  "Corpora": "We use the DD-annotated corpora provided as part of the shared task. For training, we use theofficial training corpus from the shared task, ARRAU, which consists of three conversational sub-corpora (TRAINS-93, TRAINS-91, PEAR) and two non-dialogue sub-corpora (GNOME, RST).For validation and evaluation, we use the official development sets and test sets from the sharedtask. The shared task corpus is composed of four well-known conversational datasets: AMI, LIGHT,Persuasion, and Switchboard. Statistics on these corpora are provided in .",
  "We employ three baseline systems": "The first baseline, coref-hoi, is a re-implementation of a widely-used end-to-end entity coreferencemodel. The model ranks all text spans of up to a predefined length based on how likely theycorrespond to entity mentions. For each top-ranked span z, the model learns a distribution P(y) overits antecedents y Y(z), where Y(z) includes a dummy antecedent and every preceding span:",
  "sc(z, y) = gTx Wcgy(4)sa(z, y) = FFNNa([gx, gy, gx gy, (x, y)])(5)": "where gx and gy are the vector representations of x and y, Wc is a learned weight matrix for bilinearscoring, FFNN() is a feedforward neural network, and () encodes features. Two features are used,one encoding speaker information and the other the segment distance between two spans. The second baseline, UTD_NLP, is the top-performing system in the DD track of the shared task.It extends coref-hoi with a set of modifications. Two of the most important modifications are:(1) the addition of a sentence distance feature to (), and (2) the incorporation into coref-hoia type prediction model, which predicts the type of a span. The possible types of a span i are:ANTECEDENT (if i corresponds to an antecedent), ANAPHOR (if i corresponds to an anaphor),and NULL (if it is neither an antecedent nor an anaphor). The types predicted by the model are thenused by coref-hoi as follows: only spans predicted as ANAPHOR can be resolved, and they can onlybe resolved to spans predicted as ANTECEDENT. The third baseline, coref-hoi-utt, is essentially the first baseline except that we restrict the candidateantecedents to be utterances. This restriction is motivated by the observation that the antecedents ofthe deictic anaphors in the datasets are all utterances.",
  "Next, we describe our resolver, dd-utt, which augments coref-hoi-utt with 10 extensions": "E1. Modeling recency. Unlike in entity coreference, where two coreferent names (e.g., Joe Biden,President Biden) can be far apart from each other in the corresponding document (because namesare non-anaphoric), the distance between a deictic anaphor and its antecedent is comparatively smaller.To model recency, we restrict the set of candidate antecedents of an anaphor to be the utterancecontaining the anaphor as well as the preceding 10 utterances, the choice of which is based on ourobservation of the development data, where the 10 closest utterances already cover 9699% of theantecedent-anaphor pairs. E2. Modeling distance. While the previous extension allows us to restrict our attention to candidateantecedents that are close to the anaphor, it does not model the fact that the likelihood of beingthe correct antecedent tends to increase as its distance from the anaphor decreases. To model thisrelationship, we subtract the term 1Dist(x, y) from s(x, y) (see Equation (1)), where Dist(x, y) isthe utterance distance between anaphor x and candidate antecedent y and 1 is a tunable parameterthat controls the importance of utterance distance in the resolution process. Since s(x, y) is used torank candidate antecedents, modeling utterance distance by updating s(x, y) will allow distance tohave a direct impact on DD resolution. E3. Modeling candidate antecedent length. Some utterances are pragmatic in nature and do notconvey any important information. Therefore, they cannot serve as antecedents of deictic anaphors.Examples include Umm, Ahhhh... okay, thats right, and I agree. Ideally, the model canidentify such utterances and prevent them from being selected as antecedents. We hypothesize thatwe could help the model by modeling such utterances. To do so, we observe that such utterancestend to be short and model them by penalizing shorter utterances. Specifically, we subtract the term21",
  "Length(y) from s(x, y), where Length(y) is the number of words in candidate antecedent y and 2is a tunable parameter that controls the importance of candidate antecedent length in resolution": "E4. Extracting candidate anaphors. As mentioned before, the deictic anaphors in dialogue arelargely composed of pronouns. Specifically, in our development sets, the three pronouns that,this, and it alone account for 7488% of the anaphors. Consequently, we extract candidate deicticanaphors as follows: instead of allowing each span of length n or less to be a candidate anaphor, weonly allow a span to be a candidate anaphor if its underlying word/phrase has appeared at least oncein the training set as a deictic anaphor. E5. Predicting anaphors. Now that we have the candidate anaphors, our next extension involvespredicting which of them are indeed deictic anaphors. To do so, we retrain the type prediction modelin UTD_NLP, which is a FFNN that takes as input the (contextualized) span representation gi ofcandidate anaphor i and outputs a vector oti of dimension 2 in which the first element denotes thelikelihood that i is a deictic anaphor and the second element denotes the likelihood that i is not adeictic anaphor. i is predicted as a deictic anaphor if and only if the value of the first element of oti isbigger than its second value:oti = FFNN(gi)(6)ti = argmaxx{A,NA} oti(x)(7) where A (ANAPHOR) and NA (NON-ANAPHOR) are the two possible types. Following UTD_NLP,this type prediction model is jointly trained with the resolution model. Specifically, we compute thecross-entropy loss using oti, multiply it by a type loss coefficient , and add it to the loss function ofcoref-hoi-utt. is a tunable parameter that controls the importance of type prediction relative to DDresolution. E6. Modeling the relationship between anaphor recognition and resolution. In principle, themodel should resolve a candidate anaphor to a non-dummy candidate antecedent if it is predictedto be a deictic anaphor by the type prediction model. However, type prediction is not perfect, andenforcing this consistency constraint, which we will refer to as C1, will allow errors in type predictionto be propagated to DD resolution. For example, if a non-deictic anaphor is misclassified by the typeprediction model, then it will be (incorrectly) resolved to a non-dummy antecedent. To alleviateerror propagation, we instead enforce C1 in a soft manner. To do so, we define a penalty function p1,which imposes a penalty on span i if C1 is violated (i.e., a deictic anaphor is resolved to the dummyantecedent), as shown below:",
  "p1(i) =0if arg maxyY s(i, y) = and ti = NAoti(A) oti(NA)otherwise(8)": "Intuitively, p1 estimates the minimum amount to be adjusted so that span is type is not ANAPHOR.We incorporate pi into the model as a penalty term in s (Equation (1)). Specifically, we redefines(i, ) as shown below:s(i, ) = s(i, ) [3p1(i)](9)where 3 is a positive constant that controls the hardness of C1. The smaller 3 is, the softer C1 is.Intuitively, if C1 is violated, s(i, ) will be lowered by the penalty term, and the dummy antecedentwill less likely be selected as the antecedent of i. E7. Modeling the relationship between non-anaphor recognition and resolution. Anotherconsistency constraint that should be enforced is that the model should resolve a candidate anaphorto the dummy antecedent if it is predicted as a non-deictic anaphor by the type prediction model. Asin Extension E6, we will enforce this constraint, which we will refer to as C2, in a soft manner bydefining a penalty function p2, as shown below:",
  "p2(i) =oti(NA) oti(A)if arg maxyY s(i, y) = and ti = NA0otherwise(10)": "Then we redefine s(i, j) when j = as follows:s(i, j) = s(i, j) [4p2(i)](11)where 4 is a positive constant that controls the hardness of C2. Intuitively, if C2 is violated, s(i, j)will be lowered by the penalty term, and j will less likely be selected as the antecedent of i. E8. Encoding candidate anaphor context. Examining Equation (1), we see that s(x, y) is computedbased on the span representations of x and y. While these span representations are contextualized,the contextual information they encode is arguably limited. As noted before, most of the deicticanaphors in dialogue are pronouns, which are semantically empty. As a result, we hypothesize thatwe could improve the resolution of these deictic anaphors if we explicitly modeled their contexts.Specifically, we represent the context of a candidate anaphor using the embedding of the utterance inwhich it appears and add the resulting embedding as features to both the bilinear score sc(x, y) andthe concatenation-based score sa(x, y):",
  "Reporting verbs": "command, mention, demand, request, reveal, believe,guarantee, guess, insist, complain, doubt, estimate,warn, learn, realise, persuade, propose, announce,advise, imagine, boast, suggest, remember, claim,describe, see, understand, discover, answer, wonder,recommend, beg, prefer, suppose, comment, think,argue, consider, swear, ask, agree, explain, report,know, tell, decide, discuss, repeat, invite, reply,expect, forget, add, fear, hope, say, feel, observe,remark, confirm, threaten, teach, forbid, admit,promise, deny, state, mean, instruct",
  "where Wc and Wa are learned weight matrices, gs is the embedding of the utterance s in whichcandidate anaphor x appears, and (x, y) encodes the relationship between x and y as features": "E9. Encoding the relationship between candidate anaphors and antecedents. As noted inExtension E8, (x, y) encodes the relationship between candidate anaphor x and candidate antecedenty. In UTD_NLP, (x, y) is composed of three features, including two features from coref-hoi-utt(i.e., the speaker id and the segment distance between x and y) and one feature that encodes theutterance distance between them. Similar to the previous extension, we hypothesize that we couldbetter encode the relationship between x and y using additional features. Specifically, we incorporatean additional feature into (x, y) that encodes the utterance distance between x and y. Unlike the oneused in UTD_NLP, this feature aims to more accurately capture proximity by ignoring unimportantsentences (i.e., those that contain only interjections, filling words, reporting verbs, and punctuation)when computing utterance distance. The complete list of filling words and reporting verbs that wefilter can be found in . E10. Encoding candidate antecedents. In coref-hoi-utt, a candidate antecedent is simply encodedusing its span representation. We hypothesize that we could better encode a candidate antecedentusing additional features. Specifically, we employ seven features to encode a candidate antecedent yand incorporate them into (x, y): (1) the number of words in y; (2) the number of nouns in y; (3)the number of verbs in y; (4) the number of adjectives in y; (5) the number of content word overlapsbetween y and the portion of the utterance containing the anaphor that precedes the anaphor; (6)whether y is the longest among the candidate antecedents; and (7) whether y has the largest number ofcontent word overlap (as computed in Feature #5) among the candidate antecedents. Like ExtensionE3, some features implicitly encode the length of a candidate antecedent. Despite this redundancy,we believe the redundant information could be exploited by the model differently and may thereforehave varying degrees of impact on it.",
  "Experimental Setup": "Evaluation metrics. We obtain the results of DD resolution using the Universal Anaphora Scorer.Since DD resolution is viewed as a generalized case of event coreference, the scorer reports perfor-mance in terms of CoNLL score, which is the unweighted average of the F-scores of three coreferencescoring metrics, namely MUC, B3, and CEAFe. In addition, we report the results of deictic anaphorrecognition. We express recognition results in terms of Precision (P), Recall (R) and F-score, con-",
  "sidering an anaphor correctly recognized if it has an exact match with a gold anaphor in terms ofboundary": "Model training and parameter tuning. For coref-hoi and coref-hoi-utt, we use SpanBERTLarge asthe encoder and reuse the hyperparameters with the only exception of the maximum span width: forcoref-hoi, we increase the maximum span width from 30 to 45 in order to cover more than 97% ofthe antecedent spans; coref-hoi-utt we use 15 as the maximum span width, which covers more than99% of the anaphor spans in the training sets. For UTD_NLP, we simply take the outputs producedby the model on the test sets and report the results obtained by running the scorer on the outputs. Fordd-utt, we use SpanBERTLarge as the encoder. Since we do not rely on span enumerate to generatecandidate spans, the maximum span width can be set to any arbitrary number that is large enoughto cover all candidate antecedents and anaphors. In our case, we use 300 as our maximum spanwidth. We tune the parameters (i.e., , 1, 2, 3, 4) using grid search to maximize CoNLL score ondevelopment data. For the type loss coefficient, we search out of {0.2, 0.5, 1, 200, 500, 800, 1200,1600}, and for , we search out of {1, 5, 10}. All models are trained for 30 epochs with a dropout rate of 0.3 and early stopping. We use 1 105as our BERT learning rate and 3 104 as our task learning rate. Each experiment is run using arandom seed of 11 and takes less than three hours to train on an NVIDIA RTX A6000 48GB. Train-dev partition. Since we have four test sets, we use ARRAU and all dev sets other thanthe one to be evaluated on for model training and the remaining dev set for parameter tuning. Forexample, when evaluating on AMItest, we train models on ARRAU, LIGHTdev, Persuasiondev andSwitchboarddev and use AMIdev for tuning.",
  "Recall that our goal is to perform end-to-end DD resolution, which corresponds to the Predictedevaluation setting in the shared task": "Overall performance. Recognition results (expressed in F-score) and resolution results (expressedin CoNLL score) of the three baselines and our model on the four test sets are shown in ,where the Avg. columns report the macro-averages of the corresponding results on the four testsets, and the parameter settings that enable our model to achieve the highest CoNLL scores on thedevelopment sets are shown in . Since coref-hoi and coref-hoi-utt do not explicitly identifydeictic anaphors, we assume that all but the first mentions in each output cluster are anaphors whencomputing recognition precision; and while UTD_NLP (the top-performing system in the sharedtask) does recognize anaphors, we still make the same assumption when computing its recognitionprecision since the anaphors are not explicitly marked in the output (recall that we computed resultsof UTD_NLP based on its outputs). We test the statistical significance among the four models using two-tailed Approximate Random-ization. For recognition, the models are statistically indistinguishable from each other w.r.t. theirAvg. score (p < 0.05). For resolution, dd-utt is highly significantly better than the baselines w.r.t.Avg. (p < 0.001), while the three baselines are statistically indistinguishable from each other. Theseresults suggest that (1) dd-utts superior resolution performance stems from better antecedent selec-tion, not better anaphor recognition; and (2) the restriction of candidate antecedents to utterances incoref-hoi-utt does not enable the resolver to yield significantly better resolution results than coref-hoi. Per-anaphor results. Next, we show the recognition and resolution results of the four models on themost frequently occurring deictic anaphors in after micro-averaging them over the four testsets. Not surprisingly, that is the most frequent deictic anaphor on the test sets, appearing as ananaphor 402 times on the test sets and contributing to 68.8% of the anaphors. This is followed by it(16.3%) and this (4.3%). Only 8.9% of the anaphors are not among the top four anaphors. Consider first the recognition results. As can be seen, that has the highest recognition F-scoreamong the top anaphors. This is perhaps not surprising given the comparatively larger number ofthat examples the models are trained on. While it occurs more frequently than this as a deicticanaphor, its recognition performance is lower than that of this. This is not surprising either: this,when used as a pronoun, is more likely to be deictic than it, although both of them can serve asa coreference anaphor and a bridging anaphor. In other words, it is comparatively more difficult todetermine whether a particular occurrence of it is deictic. Overall, UTD_NLP recognizes moreanaphors than the other models. Next, consider the resolution results. To obtain the CoNLL scores for a given anaphor, we retain alland only those clusters containing the anaphor in both the gold partition and the system partition andapply the official scorer to them. Generally, the more frequently occurring an anaphor is, the betterits resolution performance is. Interestingly, for the Others category, dd-utt achieves the highestresolution results despite having the lowest recognition performance. In contrast, while UTD_NLPachieves the best recognition performance on average, its resolution results are among the worst. Results of the four resolvers (UTD_NLP, coref-hoi, coref-hoi-utt, and dd-utt) on the CODI-CRAC2021 shared task test sets in terms of MUC, B3, and CEAFe scores are reported in Table. Theirmention extraction results in terms of recall (R), precision (P), and F-score (F) are provided in Table. dd-utt achieves the best CoNLL scores on all four datasets, via achieving the best MUC, B3, andCEAFe F-scores. In terms of MUC F-score, the performance difference between dd-utt and thesecond best resolver on each dataset is substantial (2.2%-14.9% points). These results suggest thatbetter link identification, which is what the MUC F- score reveals, is the primary reason for thesuperior performance of dd-utt. Moreover, Persuasion appears to be the easiest of the four datasets,as this is the dataset on which three of the four resolvers achieved the highest CoNLL scores. Notethat Persuasion is also the dataset on which the differences in CoNLL score between dd-utt and theother resolvers are the smallest. These results seem to suggest that the performance gap betweendd-utt and the other resolvers tends to widen as the difficulty of a dataset increases. In terms of anaphor extraction results in Table, dd-utt lags behind UTD_NLP on two datasets, AMIand Switchboard, in terms of F-score. Nevertheless, the anaphor extraction precision achieved bydd-utt is often one of the highest in each dataset.",
  "Further Analysis": "An example is analyzed. In this example, dd-utt successfully extracts the anaphor \"that\" and resolvesit to the correct antecedent, \"Losing one decimal place, that is okay\". UTD_NLP fails to extract \"that\"as a deictic anaphor. While coref-hoi correctly extracts the anaphor, it incorrectly selects \"You wantyour rating to be a two?\" as the antecedent. From a cursory look at this example, one could infer thatthis candidate antecedent is highly unlikely to be the correct antecedent since it is 10 utterances awayfrom the anaphor. As for coref-hoi-utt, the resolver successfully extracts the anaphor but incorrectlyselects \"Its just two point five for that one\" as the antecedent, which, like the antecedent chosen bycoref-hoi, is farther away from the anaphor than the correct antecedent. Coref-hoi and coref-hoi-uttfail to identify the correct antecedent because they do not explicitly model distance and therefore maynot have an idea about how far a candidate antecedent is from the anaphor under consideration. The",
  "additional features that dd-utt has access to, including those that encode sentence distance as well asthose that capture contextual information, may have helped dd-utt choose the correct antecedent": "A: You want your rating to be a two?A: Is that what youre saying?B: Yeah, I just got it the other way.B: Uh in Yep, I just gotA: Okay.A: So, Ill work out the average for that again at the end.A: Its very slightly altered. Okay, and were just waiting for your rating.B: two point fiveC: Its just two point five for that one.A: Two point five, okay.D: Yeah.A: Losing one decimal place, that is okay.",
  "Error Analysis": "DD anaphora recognition precision errors. A common type of recognition precision errors involvesmisclassifying a coreference anaphor as a deictic anaphor. Consider the first example in , inwhich the pronoun \"that\" is a coreference anaphor with \"voice recognition\" as its antecedent but ismisclassified as a deictic anaphor with the whole sentence as its antecedent. This type of error occursbecause virtually all of the frequently occurring deictic anaphors, including \"that\", \"it\", \"this\", and\"which\", appear as a coreference anaphor in some contexts and as a deictic anaphor in other contexts,and distinguishing between the two different uses of these anaphors could be challenging.DD anaphor recognition recall errors. Consider the second example in , in which \"it\" is adeictic anaphor that refers to the boldfaced utterance, but dd-utt fails to identify this and many otheroccurrences of \"it\" as deictic, probably because \"it\" is more likely to be a coreference anaphor than adeictic anaphor: in the dev sets, 80% of the occurrences of \"it\" are coreference anaphors while only5% are deictic anaphors.DD resolution precision errors. A major source of DD resolution precision errors can be attributed",
  "AntecedentUTD_NLP60.821.531.7coref-hoi46.327.234.3coref-hoi-utt43.325.532.1dd-utt66.240.049.8": "to the models failure in properly understanding the context in which a deictic anaphor appears.Consider the third example in , in which \"that\" is a deictic anaphor that refers to the boldfacedutterance. While dd-utt correctly identifies \"that\" as a deictic anaphor, it erroneously posits theitalicized utterance as its antecedent. This example is interesting in that without looking at theboldfaced utterance, the italicized utterance is a plausible antecedent for \"that\" because \"I am notsurprised to hear that at all\" can be used as a response to almost every statement. However, whenboth the boldfaced utterance and the italicized utterance are taken into consideration, it is clear thatthe boldfaced utterance is the correct antecedent for \"that\" because winning over seven awards forsome charitable work is certainly more surprising than seeing a place bring awareness to the needs ofthe young. Correctly resolving this anaphor, however, requires modeling the emotional implication ofits context.A: The design should minimize R_S_I and be easy to locate and we were still slightly ambivalent asto whether to use voice recognition there, though that did seem to be the favored strategy, but therewas also, on the sideline, the thought of maybe having a beeper function.A: Sounds like a blessed organization.B: Yes, it does.A: Did you know theyve won over 7 different awards for their charitable work?"
}