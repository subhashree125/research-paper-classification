{
  "Abstract": "This paper explores the adaptation of video alignment to improve multi-step infer-ence. Specifically, we first utilize VideoCLIP to generate video-script alignmentfeatures. Afterwards, we ground the question-relevant content in instructionalvideos. Then, we reweight the multimodal context to emphasize prominent features.Finally, we adopt GRU to conduct multi-step inference. Through comprehensiveexperiments, we demonstrate the effectiveness and superiority of our method.",
  "Introduction": "This paper addresses the critical task of assisting users in navigating unfamiliar events for specificdevices by providing step-by-step guidance using knowledge acquired from instructional videos.Due to the substantial disparity among specific tasks, the integration of multimodal input, and thecomplexity of multi-step inference, this is still a challenging task. Several studies have been proposed to address this task. For instance, one study proposes a Question-to-Actions (Q2A) Model, which employs vision transformer (ViT) and BERT to extract visual andtextual features, respectively. Moreover, attention mechanisms are leveraged to anchor question-relevant information in instructional videos. Another study proposes a two-stage Function-centricapproach, which segments both the script and video into function clips instead of sentences orframes. Additionally, they substitute BERT with XL-Net for text encoding. Despite the advancementsachieved through these techniques, all of them adopt the unaligned pretrained encoders to extractvisual and textual features, leading to significant semantic gaps between modalities, thereby hinderingbetter results. To alleviate the negative effects of modalities unalignment, in this paper, we leverage pretrainedvideo-text models to achieve instructional video-text alignment, facilitating a more robust groundingof question-relevant knowledge for multi-step inference. We build the pipeline with four steps:Instructional Video Alignment, Question-Aware Grounding, Multimodal Context Reweighting andMulti-Step Inference. Specifically, we employ pretrained VideoCLIP for generating video-scriptalignment features, which are beneficial to cross-modal grounding. Subsequently, we anchor thequestion-relevant content in instructional videos by the combination of hard and soft grounding.Afterwards, we leverage additive attention to adjust the weighting of the multimodal context toemphasize the salient features. Finally, we employ GRU for performing multi-step inference. Wereduce the proportion of teacher forcing linearly to bridge the gap between training and inference,which boosts the multi-step inference.",
  "In this section, we formulate the problem of AQTC": "Given an instructional video, which contains numerous frames and scripts, AI assistant extractsrelevant information from the video in accordance with the user2019s question q. Then, it deducesthe correct answer ai j based on the image U as perceived by the user, from the candidate answer setAnsi = ai 1, ai 2, ..., ai n in i-th step. Following previous work, we segment the video into several clipsbased on scripts. Each clip illustrates one specific function of the device in video. We concatenate",
  "Instructional Video Alignment": "To align the videos and the text for better cross-modal understanding, we leverage pretrained Video-CLIP to generate the features of instructional videos. For the video part, we initially utilize pretrainedS3D to generate an embedding for each second of the video, with a frame rate of 30 frames per second.Next, to represent each function within the videos, we utilize the pretrained visual transformer fromVideoCLIP to process the embeddings generated by S3D in each function. Then, we apply averagepooling over the processed sequence of embeddings to form the video embedding Vi correspondingto a given visual function F v i . For the text part, we use the pretrained textual transformer ofVideoCLIP to encode the scripts of a textual function F t i . Similarly, we employ average pooling toaggregate the processed sequence of text, generating the text embedding Ti of a given textual functionF t i . Finally, we obtain the video feature sequence [V1, V2, ..., Vm] and the text feature sequence[T1, T2, ..., Tm] of the given function sequence. Besides, we also utilize VideoCLIP to encode the questions q, the answer ai j and the masked buttonimage bk. We duplicate the images 30 times to ensure consistent video encoding. We get the questionfeature Q, answer feature Ai j and visual button feature Bk.",
  "Question-Aware Grounding": "Owing to the extensive pretraining of VideoCLIP on a vast collection of videos, the features of videosand text are cross-modal aligned. Therefore, we can utilize the question Q to ground the video andtext feature sequence directly. Specifically, we leverage three grounding mechanisms: soft, hard andcombined grounding. Soft grounding employs attention to learn the similarity between the questionfeature Q and the video feature sequence [V1, V2, ..., Vm] directly. And, it uses another attentionnetwork to compute the similarity between the question feature Q and the text feature sequence [T1,T2, ..., Tm]. Soft grounding adopts the similarity from two attention networks to perform a weightedaverage of the two feature sequences, respectively. Instead of relying on deep learning methods, hardgrounding follows previous work, which uses TF-IDF model to calculate the similarity between thequestion q and each textual function F t i from textual function sequence [F t 1, F t 2, ..., F t m].Then, it uses the similarities as the weights to compute the averages of the video feature sequence[V1, V2, ..., Vm] and the text feature sequence [T1, T2, ..., Tm], respectively. Besides, the combinedgrounding utilizes soft grounding and hard grounding simultaneously. Then, the two features fromtwo grounding methods are averaged. Ultimately, we obtain the aggregated question-aware videofeature V and text feature T .",
  "Multimodal Context Reweighting": "After obtaining multimodal question-aware context features from instructional videos, we need tomodel the answers to determine the correct one. Specifically, we utilize the gate network to fusethe candidate answer feature Ai j with the corresponding button feature Bk, which generates themultimodal answer feature 02c6Ai j. We concatenate these multimodal contexts into a sequence [V,T, Q, 02c6Ai j] for each candidate answer. Due to the varying importance of different context featuresin determining the correct answers, we utilize additive attention to reweight the multimodal contextand get the fused feature. Finally, the fused feature is processed using a two-layer MLP to obtain thecandidate answer context feature C i j.",
  "Multi-Step Inference": "Owing to the requirement for multi-step guidance in order to respond to the given questions, it isessential for models to perform multi-step inference. Following previous work, we utilize GRU toinfer the current correct answer by incorporating historical knowledge. Specifically, we feed theprevious hidden state H i22121 and the contextual features C i j of candidate answers in Ansi intothe GRU. Then, the resulting current hidden state H i j for each candidate answer in Ansi is utilizedto predict the correct answer in the i-th step. We adopt a two-layer MLP and the softmax functionon the concatenated current hidden states [H i 1, H i 2, ..., H i n] to generate the probability of thecorrect answer. Cross entropy is used to compute the loss. While previous works utilize the state ofthe ground truth as the historical state of the next step H i. This causes a huge gap between trainingand inference. To bridge this gap, we reduce the reliance on teacher forcing linearly. In other words,we choose the hidden state of the most probable answer predicted by models as the historical state ofthe next step H i, when a sample is selected for autoregressive training.",
  "Performance Evaluation": "We present the performance evaluation on the test dataset in a. We find that our methodoutperforms baseline methods. This superiority can be attributed to our utilization of a video-textaligned pretrained encoder for feature extraction. The aligned features are beneficial to multi-stepinference. Furthermore, our method exhibits improved performance when the results are ensembled.",
  "Ablation Study": "Pretrain Feature To validate the efficacy of video-text aligned features, we conduct the ablationstudy, which adopts ViT for processing the visual features and XL-Net for processing the text features.As shown in b, we observe that the performance of method that uses the unaligned featuresdrops sharply. Grounding Methods To validate the effectiveness of various grounding methods, we use differentgrounding techniques to train this model. The result is presented in . We find that the modelachieves optimal performance when the text grounding leverages combined grounding and the videogrounding utilizes soft grounding.",
  ": (a) Impact of the reweighting mechanism and SSL": "Reweighting Mechanism We show the result of the model without attention reweighting in a.We observe a considerable decrease in performance for the model lacking attention reweighting. Thisis because the attention reweighting can discern and prioritize the most informative features withincomplex multimodal contexts. Multi-Step Inference We evaluate different multi-step inference strategies, as demonstrated in b. We find that the performance of TeacherForcing is inferior to that of the Linear Decay strategy,which is employed by our approach. This is because TeacherForcing widens the gap between trainingand inference. We also observe that Linear Decay outperforms AutoRegression. This is becauseteacher forcing is beneficial in preventing models from accumulating mistakes during the early stagesof training.",
  "Conclusion": "In this paper, we present a solution aimed at enhancing video alignment to achieve more effectivemulti-step inference for the AQTC challenge. We leverage VideoCLIP to generate alignment featuresbetween videos and scripts. Subsequently, we identify and highlight question-relevant contentwithin instructional videos. To further improve the overall context, we assign weights to emphasizeprominent features. Lastly, we employ GRU for conducting multi-step inference. Besides, we conductexhaustive experiments to validate the effectiveness of our method."
}