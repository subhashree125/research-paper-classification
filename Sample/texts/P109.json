{
  "Abstract": "This paper delves into the utilization of machine learning techniques for identify-ing hate speech, while addressing the persisting technical challenges to enhancetheir performance to match human-level accuracy. We explore several currentvisual-linguistic Transformer models and suggest enhancements to boost their ef-fectiveness for this task. The model we propose demonstrates superior performancecompared to the established benchmarks, achieving a 5th place ranking out of over3,100 participants.",
  "Introduction": "This paper addresses the critical influence of the internet on our daily lives, where our online presenceshowcases our personalities and beliefs, as well as our biases. Daily, billions of individuals engagewith various forms of online content, and despite some of this content being valuable and informative,an increasing portion is harmful, including hate speech and misinformation. There is a growing needto quickly detect this content, improve the review process and automate decisions to rapidly removeharmful material, thereby reducing any harm to viewers. Social media platforms are frequently used for interactions, sharing messages and images with privategroups and the public. Facebook AI launched a competition to tag hateful memes that include bothimages and text. For this, a dataset of 10,000+ labeled multimodal memes was provided. The aim ofthe challenge is to develop an algorithm that identifies multimodal hate speech in memes, while alsobeing robust to their benign alterations. A memes hateful nature could stem from its image, text, orboth. Benign alteration is a technique used by organizers to switch a memes label from hateful tonon-hateful, requiring modifications to either the text or the image. The core assessment metric for this binary classification task is the area under the receiver operatingcharacteristic curve (AUROC), representing the area under the ROC curve. This curve plots the TruePositive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds. Theprimary objective is to maximize the AUROC.",
  "Related Work": "Transformer models pre-trained on extensive datasets have shown state-of-the-art results in numerouslanguage processing tasks. BERT is one of the most popular due to its ease of use and strongperformance. Recently, training these large models on combined visual-linguistic embeddingshas shown very promising outcomes for visual-linguistic tasks such as visual question answering,reasoning, and image captioning. LXMERT uses dual networks to process text and images, learningcross-modality encoder representations by using a Transformer to combine the two streams ofinformation. The images features are derived using a Faster R-CNN feature extractor. This is alsoused in single-stream architectures, VL-BERT and UNITER, which employ a single Transformeron top of the combined image-text embeddings. A unified model for visual understanding andvision-language tasks has also been proposed.",
  "Methodology": "One goal of this research is to leverage the fact that single and dual stream Transformer models havebeen pre-trained on a variety of datasets across various fields. Transformer attention models excel atNLP tasks, and the masked language modeling pre-training method in BERT is both powerful andversatile. Studies show that the pre-training process can better align visual-linguistic embeddingsand help downstream tasks like visual question answering and reasoning. Given that pre-training avisual-linguistic Transformer architecture is helpful for downstream tasks, might ensembling differentmodels pre-trained on different datasets yield better results?",
  "UNITER with Meme Text and Inferred Caption Cross-Attention": "The Natural Language for Visual Reasoning for Real (NLVR2) is an academic dataset of humanwritten sentences connected to pairs of photos. The dataset includes pairs of visually intricate imagescoupled with a statement and a binary label. UNITER was among the top models in this challengeby adding a cross-attention module between text-image pairs, dividing each sample in two andrepeating the text. They then apply attention pooling to each sequence, concatenate them and add theclassification head, a multi-layer perceptron. Similar to this, we propose to repeat the meme image ineach half-sequence and add an inferred meme caption as the second text. We generate captions usingthe Show and Tell model. This way, the model could learn from both the original meme text and thenew captions generated by a model trained on a different dataset.",
  "Experiments": "We carry out several experiments using LXMERT, VLP, VL-BERT, and UNITER. We apply bidirec-tional cross-attention using inferred captions for UNITER, VL-BERT, and VLP, but not for LXMERTdue to its low performance on the dataset. We also experiment with a dataset from previous research. We filter and balance it down to 16Ksamples by excluding cartoon memes and memes with little text. We fine-tune VL-BERTLARGEusing the reduced dataset for four rounds, then fine-tune it using the hateful memes dataset for anotherfour rounds. The results were lower than the majority of the other models.",
  "Results": "Our best performing solutions are derived from averaging probabilities using a single VL-BERTLARGE and one UNITERLARGE+PA (UNITERLARGE with extra attention). We usedthe default training parameters of the vanilla pre-trained UNITERLARGE model, but changed thetraining steps according to the dataset size. A deep ensemble of UNITERLARGE+PA models gotthe best performance. For this ensemble, we simply rerun training using various random seeds andaverage the predictions from each model. displays the top results for the final competitionphase as well as the improvements cross-attention brings to the UNITER model in the first phase.The final results are significantly better than the baselines.",
  "The most important findings are as follows:": "Single-stream Transformer models pre-trained on the Conceptual Captions (CC) dataset givethe best results, and deep ensembles improve the overall performance further. The choice ofpre-training datasets matters in terms of domain similarity to the fine-tuning dataset. We believe that UNITER gets better results due to being pre-trained on the COCO datasetwhich has less noise. Similarly to the Hateful Memes dataset this is also high quality. Furtherwork should investigate if pre-training VL-BERT on COCO would improve its results.",
  "Conclusion": "We present effective techniques to detect hate speech in a distinct dataset of multimodal memes fromFacebook AI. The aim is to identify hate speech using a multimodal model, and to be robust to thebenign confounders that cause the binary label of a meme to change. We have performed tests on various large pre-trained Transformer models and fine-tuned state-of-the-art single-stream models like VL-BERT, VLP, and UNITER, and dual-stream models like LXMERT.We compare their performance against the baselines, showing that the single-stream models performsignificantly better. Our choice for these models stems from their pre-training on a wide variety ofdatasets from different fields. We also adapt a novel bidirectional cross-attention mechanism thatlinks caption information with meme text. This leads to increased accuracy in identifying hatefulmemes. Furthermore, deep ensembles can improve single model predictions. Training the modelsfrom scratch performed poorly due to the small dataset size. We also observed that the pre-trainingdataset influences results. We conclude that despite the improvements in multimodal models, there is still a gap when comparingto human performance. This suggests considerable scope for the development of better algorithmsfor multimodal understanding."
}