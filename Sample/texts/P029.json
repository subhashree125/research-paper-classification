{
  "Abstract": "Multimodal conversational systems are increasingly sought after for their abilityto facilitate natural and human-like interactions. However, comprehensive, col-laborative development and benchmarking solutions remain scarce. Proprietarymodels like GPT-4o and Gemini have showcased impressive integration of audio,visual, and textual data, achieving response times between 200-250 milliseconds.Nonetheless, challenges persist in managing the trade-offs between latency, pre-cision, financial cost, and data confidentiality. To address these complexities, weintroduce OpenOmni, an open-source, end-to-end pipeline benchmarking platform.OpenOmni incorporates advanced technologies such as Speech-to-Text, EmotionDetection, Retrieval Augmented Generation, and Large Language Models, whilealso offering the capability to integrate custom models. It supports both local andcloud deployment, thereby guaranteeing data privacy and providing latency andaccuracy benchmarking capabilities. This adaptable architecture allows researchersto tailor the pipeline to pinpoint performance bottlenecks and expedite the de-velopment of proof-of-concept solutions. OpenOmni holds significant potentialto improve applications, including indoor assistance for individuals with visualimpairments, thereby advancing human-computer interaction.",
  "Introduction": "Large Language Models (LLMs) have shown remarkable proficiency in interpreting user intent andadhering to instructions. However, text-based human-computer interaction (HCI) is often inadequate.The recent introduction of models that process audio, video, and text in real-time highlights theprogress towards multimodal interaction. The impressive performance, characterized by responsetimes of 200-250 milliseconds, makes these models suitable for large-scale applications. This marksa trend towards multimodal generative models and applications. One of the early publicly availablesolutions for multimodal large models that integrate text and images is available, but an open-source,end-to-end conversational agent implementation has not yet been made publicly accessible online. The preferred mode of multimodal HCI should replicate human interaction, incorporating visualand auditory inputs alongside audio outputs. Despite the existence of various modular components,a comprehensive, integrated, open-source implementation that fosters research and developmentin this domain is lacking. The integration of existing models, such as audio speech recognition(Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS), into a mul-timodal conversation framework reveals substantial difficulties in managing latency and ensuringaccuracy. Traditionally, accuracy has posed a significant challenge. However, progress in largelanguage models (LLMs) has significantly enhanced contextual relevance. The primary challengenow lies in minimizing end-to-end latency while maintaining high accuracy. Although it has beenshown that this is feasible, the open-source community has not yet replicated these results. Data privacy is another concern. The closed-source nature of certain solutions raises issues related tocost and data confidentiality. Since these models are not open-source, users are required to uploadtheir data to servers via paid APIs, leading to privacy concerns. The privacy policy indicates thatvarious types of personal information are collected when users create accounts to access services,such as account details, user-generated content, communication data, and social media information. To facilitate the swift and responsible development of this new form of HCI, it is crucial to establishrobust evaluation and benchmarking protocols. For instance, if a user initiates a conversation with asad and urgent tone, the system should respond appropriately and with patience. Evaluating theseinteractions is both crucial and difficult for widespread adoption. This project aims to bridge thesegaps by:",
  "Establishing tools for annotating and benchmarking latency and accuracy, allowing for rapidproof-of-concept development and research": "To accomplish this, we propose the OpenOmni framework, an open-source, end-to-end multimodalpipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), EmotionDetection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-Speech (TTS). This framework collects video and audio data via cameras and microphones, processesthe data through a customizable agent pipeline, and responds using a speaker. OpenOmni can bedeployed on a local server, ensuring secure data management and addressing privacy concerns. For research purposes, OpenOmni includes tools for straightforward annotation and benchmarking,offering real-time monitoring and performance evaluation of latency. Users can annotate individ-ual components and entire conversations, generating comprehensive benchmark reports to identifybottlenecks. The open-source nature of OpenOmni allows for adaptation across various applicationdomains, such as aged care and personal assistants. Each pipeline component can be enabled ordisabled based on specific use cases, facilitating flexible and efficient deployment. Moreover, theframework supports the easy addition of new models, enabling comparisons and further experi-mentation. The OpenOmni framework allows researchers to focus on solving critical bottleneckswithout reinventing the wheel, fostering innovation in multimodal conversational agents. It enablesrapid proof-of-concept development, such as indoor conversational robots assisting visually impairedindividuals.",
  "Related Work": "Traditional end-to-end multimodal conversation systems typically employ a divide-and-conquerapproach, separating the process into sub-tasks: speech-to-text (automatic speech recognition), image-to-text, text generation, and text-to-speech. Speech-to-text transforms spoken language into writtentext, while image-to-text produces textual descriptions of images. Text generation, often driven bylarge language models, generates contextually appropriate responses, and text-to-speech convertsthese responses back into spoken form. These core components constitute the fundamental structureof the conversational pipeline. The inclusion of image-to-text provides essential context, enhancingnatural human-computer interaction, and additional functions like emotion detection adjust responsesbased on the users emotional state. An optional safeguard module can be integrated to guarantee thatresponses are suitable, non-harmful, and controlled, maintaining interaction integrity, particularly indelicate situations. Although this modular design enables the optimization of individual components,the cumulative latency and accuracy errors can make the complete system impractical for real-worlduse. While certain models are presented as fully end-to-end solutions, capable of handling video, audio, ortext inputs and producing audio, image, or text outputs, their technical specifics remain undisclosed.It is postulated that audio and video frames are processed by modules that generate text, audio, andimage outputs. Demonstrations suggest that these models possess memory capabilities, though thedetails and limitations are not fully understood. Whether the system can directly incorporate externalprivate data is also unknown. Unlike the divide-and-conquer method, a fully end-to-end neural network can integrate more contex-tual information, such as tone, the presence of multiple speakers, and background noises, leading tomore adaptable outputs. Theoretically, this method can decrease latency by removing orchestrationbottlenecks. Nonetheless, both methods face substantial challenges because of the extensive datainput and output, especially from video. The large size of video files puts a strain on servers and models, raising computational costs and introducing latency from data transfer and model inference.Real-time conversation necessitates streaming processing, posing additional latency challenges. Itwas highlighted that a stable internet connection is needed to ensure smooth operation, underscoringthese challenges. A technology company has introduced a planned open-source, fully end-to-end multimodal conver-sational AI, which supports text and audio modalities but excludes images. This model claims toachieve an end-to-end latency of 200 milliseconds. Integrating video modality through an Image2Textmodule into this model is possible, creating a hybrid solution that combines divide-and-conquerand fully end-to-end approaches. Another viable hybrid solution involves using speech-to-text toconvert audio into text, then feeding this text along with video (processed into image sequences)to a vision language model, which generates text responses. These responses can subsequently beprocessed through text-to-speech. Multimodal end-to-end conversational agents show promise, yetlarge-scale implementation is challenging due to the need to balance latency, accuracy, and cost.Generating real-time responses within 200-400 milliseconds is difficult. The primary objective is todecrease latency and cost while enhancing accuracy, thereby improving the real-world applicabilityof conversational agents.",
  "Evaluation Metrics": "To ensure productive and effective collaboration, it is crucial to have consistent and comparableevaluation metrics. For speech-to-text, the Word Error Rate (WER) is used to assess transcriptionaccuracy, where a lower WER signifies better performance. Evaluating text-to-speech involvesobjective metrics like the Mean Opinion Score (MOS) for naturalness and intelligibility, and theSignal-to-Noise Ratio (SNR) for clarity, along with subjective human ratings. Text generation is themost difficult to evaluate, using metrics such as BLEU, ROUGE, and METEOR, which comparegenerated text to reference texts but may not completely capture the quality and relevance of responses.Assessing text generation often necessitates large-scale datasets, which are not always accessible.These metrics are widely adopted by the research community. Nevertheless, real-world applicationsrequire evaluation in production environments, taking into account various factors beyond thesemetrics. For instance, a conversational agent designed for aged care should steer clear of sensitivetopics that may be specific to each individual. Subjective opinions differ by region, emphasizingthe necessity for adaptable and innovative automatic or semi-automatic evaluation methods forconversational agents.",
  "Requirement Analysis": "The system is designed to accept audio and video inputs and produce audio as output. Initially, twomodules are required: one for gathering audio and video data from the microphone and camera, andanother for emitting audio through a speaker. These Client modules must be compatible with a varietyof devices, such as smartphones, laptops, or Raspberry Pi. The data collected will be transmitted to aserver. The server, known as the API, should handle audio and video data along with associated metadata.It should have access to a storage layer that includes a relational database, file management, and agraph database for potential GraphRAG integration. Although the API can be located on the samedevice as the Client module, it is preferable to keep them separate for enhanced adaptability. Thisseparation introduces the difficulty of transferring large volumes of data between modules. If theAPI is cloud-based, audio and video data must be uploaded to the cloud, for instance, using AWSS3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can introduce abottleneck, making data transfer time-intensive. If the server is local, within the same network as theClient, transfer latency will be reduced. Nevertheless, this configuration necessitates running the largelanguage model locally, which addresses data ownership and privacy issues but may increase modelinference latency and reduce accuracy due to limited computational resources. Another approach isedge computing, where video data is pre-processed on edge devices and summarized for the API.Although this could be a research direction, data compression might result in information loss anddecrease overall performance. The pipeline components will require adjustments if developers intend to adopt the framework andintegrate it with their work. To maintain flexibility, this part should be an independent module capableof running locally or in the cloud. Researchers and developers should be able to easily incorporatenew components into this Agent module, further complicating the sharing of large datasets betweenmodules. Finally, benchmarks are needed to comprehend the latency and accuracy performance of the entirepipeline. For tasks that are challenging to evaluate automatically, such as assessing the appropriatenessof the LLM response, we propose and develop an annotation module to allow human annotators toeasily evaluate results and generate benchmark reports.",
  "System Architecture": "Based on these requirements, the system architecture was designed as depicted in . Thesystem is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarilydeveloped in Python. The Client module includes two submodules: the Listener for collecting videoand audio data, and the Responder for playing audio. The Storage module consists of file storage formedia, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potentialGraphRAG integration. The API module, built with the Django framework, extends Djangos admininterface and permission control system to develop the benchmark and annotation interface. Djangosmaturity and large support community make it ideal for production development. The Agent module,also in Python, includes all agent-related submodules, allowing deployment on suitable computenodes without altering the architecture. Communication between the Client, API, and Agent moduleswill be via RESTful endpoints. For sharing large data between modules, local deployments (e.g.,Client on Raspberry Pi, API and Agent on local servers) will use FTP for file synchronization. Incloud solutions (e.g., AWS), files will be uploaded to AWS S3, triggering a Lambda function todownload files to an AWS Elastic File Storage (EFS) shared by the API and Agent modules. Dockerand Docker Compose are used to manage all modules, allowing easy setup with a single dockercompose up command.",
  "Datasets": "Most multimodal question-answering datasets concentrate on multiple-choice questions rather thanopen-ended conversations. Some datasets involve multimodal conversations with images as additionalinput, but the output is often limited to multiple-choice or text. A significant challenge in developingmultimodal conversational agents is the scarcity of suitable datasets. Although there is an abundance of data from human-human interactions or data extracted from moviesand YouTube videos, efficient methods to organize this data into structured datasets are lacking. Forspecific domain applications, collecting data from human interactions and extracting datasets to trainsystems would be advantageous, enabling the agents to mimic human behavior. The OpenOmniFramework offers both capabilities: extracting conversational datasets from videos and testing themthrough the pipeline to assess agents responses, or gathering data from real-world scenarios to createdatasets for further research.",
  "Can \"AI\" be your president?": "One intensive conversational scenario is a debate. Segments were extracted from a US PresidentialDebate, focusing on a candidate addressing the public and handling questions. After downloadingthe videos, a prepared script in our codebase can be used to split them into segments. This scriptallows for the specification of the start and end times of each conversation, enabling the creationof a conversational dataset from the videos. These segments were fed into our pipeline to evaluateits performance under different configurations: one using a commercial speech-to-text model, avision model, and text-to-speech (Configuration A); a locally deployed quantization LLM with aspeech-to-text model, text-to-speech, and our emotion detection model for video input (ConfigurationB); a version using a different LLM for inference (Configuration C); and a version using only a speech-to-text model, a language model, and text-to-speech, ignoring the video modality (Configuration D).The Agent modules were run on a specific GPU with 12GB memory. The latency benchmark statistics are automatically generated. For example, Configuration A hasan average latency of 45 seconds, with the vision model accounting for 31 seconds. The fastestconfiguration is Configuration D, averaging around 15 seconds, with most of the time consumedby the text-to-speech part, because the generated content is quite long and comprehensive. Theslowest configuration is Configuration C, taking around 189 seconds, with the LLM model inferencestep taking the longest time. Configuration B takes an average of 60 seconds, with the LLM modelinference averaging 28 seconds and our emotion detection model averaging around 10 seconds.",
  "TRACK IDUSER IDOVERALL COMMENT": "f11As the question is quite subjective, the answer is good and in contextf22The answer is quite general, while the candidate is doing much better work with supported evidenf31Failed to generate proper in-context response; the response is talking about how to respond, not acf41Generate some general comments without strong support evidencef51General response, however, no good evidence to support. After annotation with our interface, accuracy statistics are automatically generated. The accuracymetrics here include evaluation metrics like WER, CER for the speech-to-text task, and overallscores given by the annotators. As shown in , the average score for each conversation is 2.4.Text-to-speech can be improved with more natural emotion or personality. The generated contentis often too general and sometimes inappropriate. The candidates responses are more in-contextand evidence-supported. The pipeline excelled only in answering a subjective question about thecandidates age, where Configuration A performed well. Configuration D had the best overallaccuracy, but its responses were often in-context yet pompous. Thus, the candidate still outperformsAI. In conclusion, \"AI cannot be the President of the US just yet, considering both latency andaccuracy.\"",
  "Assist the Visually Impaired": "While latency and the need for external information currently prevent AI from undertaking mission-critical tasks, conversational agents can be production-ready and useful for non-latency-critical areasthat do not require extensive external knowledge. Assisting indoor activities for the visually impairedis one such application, where high-speed internet can be utilized, or data transfer can be limited tolocal exchanges. These types of applications can benefit from maintaining high input/output rates,helping to mitigate latency issues. Questions were prepared for the visually impaired, includinglocating objects, navigating indoors, and inquiries about the surroundings. Six questions weresampled and fed to the Configuration A pipeline. One scenario demonstration is included in ourprovided video. In this scenario, video and audio data stream from the client side and are saved tostorage along with exportable metadata accessible via the admin portal. This setup allows for theexportation of annotated datasets, including raw video and audio data, for developing new models.The latency statistics show responses within approximately 30 seconds. Annotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visuallyimpaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffeecup rather than just a general description. This indicates that while conversational agents are nearlyready for assisting the visually impaired with indoor activities, improvements in latency and responsequality are still needed.",
  "Conclusion": "Multimodal conversational agents offer a more natural form of human-computer interaction, asdemonstrated by models like GPT-4o. However, real-world constraints require a balance betweencost, latency, and accuracy, which may explain why the full capabilities of such models are not yetaccessible. Several technical options exist to achieve this balance, including traditional divide-and-conquermethods, fully end-to-end models, and hybrid approaches. The fully end-to-end approach inherentlyallows for lower latency, while the divide-and-conquer method faces latency issues when coordinating multiple components. Both approaches must address the challenge of handling large data I/O. Ifmodels are deployed locally, local network I/O issues can be more manageable. However, somemodels are closed-source, making local deployment impractical. While deploying other vision modelslocally is feasible, achieving high accuracy may be limited by local computational resources. Hybridsolutions provide alternative approaches: pre-processing or compressing large data locally and thenutilizing cloud-based models, or converting video to text and integrating it into the end-to-end voicemodel. We developed the OpenOmni framework to enable researchers to integrate their work into an end-to-end pipeline. The framework supports various solutions, allows for pipeline customization, generateslatency performance reports, and provides an annotation interface for accuracy review. These featuresfacilitate the creation of benchmark reports to identify and address key issues. Testing with the US Presidential debate scenario highlighted latency as a critical issue, particularlywith large video data. Integrating external knowledge remains a challenge, emphasizing the needfor efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for thevisually impaired, latency improvements and model adaptation are both essential. The OpenOmni framework can significantly benefit the research community by facilitating thecollection and management of new datasets, integrating various conversational agents approaches,and generating automatic latency benchmarks. Its annotation interface aids in accuracy performancereview, making OpenOmni production-ready for suitable application scenarios and fostering furtherdevelopment in multimodal conversational agents."
}