{
  "Abstract": "This paper examines the present methods for quantifying the level of privacyprotection offered by tabular synthetic data (SD). Currently, there is no standardizedapproach for measuring the degree of privacy protection these datasets offer. Thisdiscussion contributes to the development of SD privacy standards, encouragesinterdisciplinary discourse, and aids SD researchers in making well-informedchoices concerning modeling and assessment.",
  "Introduction and Relation to Prior Research": "Synthetic data (SD) has emerged as a powerful tool for enhancing privacy, preserving the analyticutility of data while decoupling it from real individuals. However, the wide variety of SD generationapproaches makes the degree of privacy protection they offer difficult to assess. Therefore, this paperoutlines the typical technical assessment frameworks for individual privacy in SD sets. This increasesinterdisciplinary awareness of privacy in SD and helps SD researchers make informed modeling andassessment choices. While several surveys mention privacy as a use case for SD, they do not cover its assessment in adetailed way. In addition, reviews of privacy in AI fail to mention SD, and surveys, reviews, andexperimental comparisons of SD techniques often do not focus on privacy metrics. Furthermore,legal analyses of SD are scarce and do not address quantitative methods for privacy assessment on acase-by-case basis.",
  "Definition 2.1. (Synthetic data) Synthetic data (SD) are data generated through a purpose-builtmathematical model or algorithm (the \"generator\"), intended to solve a set of data science tasks": "We let D denote a database describing data subjects with attributes A(D). Rows d D are |A(D)|-tuples, with a value v(d, a) for each attribute a A(D). An attribute a A(D) is categorical ifits domain is finite and numerical if its domain is a subset of R. We use the terms row and recordinterchangeably. We denote by G a generator, and D G(D) to represent a synthetic dataset Dobtained from generator G trained on D. Seed-based generators are a specific type of generators thatproduce a unique synthetic record denoted by G(d) for every real record d. This is different frommost models (e.g., GANs, VAEs) which probabilistically represent overall dataset properties andproduce synthetic data by sampling from the obtained distribution.",
  "Synthetic Data Privacy Risks": "Three significant risks identified in prior works serve as a basis for a proper anonymization. Theseare: singling out, linkability, and inference. Privacy risks in SD can occur due to various factors,which include: Model and data properties: Improperly trained generators may overfit, memorizing andreproducing training data rather than inferring them stochastically. Records that emergein isolation with little variability in their attribute values are difficult to generalize. Assuch, datasets containing outliers or sparse data are more at risk of memorization than morehomogeneous sets. Such datasets are also more susceptible to singling-out. The approach to data synthesis: Most generators create stochastic models of datasets,creating synthetic records via sampling. This detaches real data subjects from syntheticrecords. However, some methods create a single synthetic record for each real record. Thisapproach poses greater risk as it retains the link between a subject and its data. Mode collapse: GANs can focus on the minimal information necessary to deceive thediscriminator, failing to capture the nuances and variations of the real data. In such cases,the SD resembles a small selection of real data subjects well, but not the entire population.This causes data clutter around specific real records, leaking their information. The threat model: A threat model describes the information an adversary leverages besidesthe SD. This can range from no access to the generator, to full knowledge includingmodel parameters. Threat models also include scenarios where an adversary uses auxiliaryinformation and can be: No box: the adversary only has access to the SD. Black box: the adversary also has limited generator access (no access to the modelclass or parameters, but access to the model2019s input-output relation). White box: the adversary has full generator access (model class and parameters). Uncertain box: the adversary has stochastic model knowledge (model class and knowl-edge that parameters come from a given probability distribution).",
  "P[M(D) S] e P[M(D) S] + ,": "for all databases D, D such that d D : D = D \\ {d}. Generators are information-releasingsystems and can therefore be DP. Suppose there are two real datasets, D and D, with D = D \\ {d}.A generator G is considered DP if a data controller with access to D G cannot infer if G wastrained on D or D. Approaches to train generators with built-in mechanisms to guarantee DP can befound in the literature. In this context, DP is a property of generators, not of the synthetic data theyproduce.",
  ".2k-Anonymity": "Privacy risks persist, even if identifying attributes are removed. Combinations of attribute values maystill be used to single out an individual. The notion of k-anonymity was introduced to address these risks. A dataset is k-anonymous if at least k individuals share each combination of attribute values.Further restrictions such as l-diversity, t-closeness, and (, k)-anonymity have been introduced tooffer additional protection. Synthetic data based on autoregressive models can implement k-anonymity directly into the generationprocess. For example, pruning in decision trees can guarantee that each combination of attributevalues is sampled at least k times in mathematical expectation. Unlike DP, k-anonymity is a propertyof synthetic datasets, not the algorithms producing them.",
  "Plausible Deniability": "A degree of plausible deniability is inherent in synthetic datasets, as their records do not pertain toreal data subjects. Two approaches have emerged to formalize this notion, with one most relevant toseed-based synthetic data. Definition 4.2. (Plausible deniability) Let D be a dataset and G be a generator that converts any recordd D into a corresponding synthetic record d = G(d). For any dataset D where |D| > k, and anyrecord d such that d = G(d1) for d1 D, we say that d is releasable with (k, )-plausible deniabilityif there exist at least k 1 distinct records d2, ..., dk D \\ {d1} such that for all i, j {1, 2, ..., k}:",
  "P[d = G(di)] P[d = G(dj)]": "In other words, a generator producing synthetic records from a seed has PD if, for each syntheticrecord produced from a particular seed, k other seeds could have resulted in roughly the same(quantified through ) synthetic record. Like DP, and unlike k-anonymity, PD is a property of(seed-based) generators, though it is related to both.",
  "Identical Records, Distances, and Nearest Neighbors": "Most indicators quantify the frequency of synthetic records being identical or suspiciously similar toreal records. Unlike DP and PD, these indicators measure properties of synthetic datasets, not theirgenerators. The proportion of synthetic records that match real records is called the identical matchshare (IMS). The IMS has been generalized to similarity metrics, and further to Nearest neighbor(NN)-based methods. These can be classified based on the following properties, summarized in of Appendix C:",
  "Similarity metrics. of Appendix C contains an overview of commonly invokedmeasures": "Metric evaluation. Because structured datasets can have a mix of different datatypes, metricevaluation is complex. Several approaches exist, such as binning numeric attributes; com-bining multiple metrics; ignoring specific attributes; or evaluating distances in embeddingspaces. Evaluated distances. For a given synthetic record d D, we can find its closest real recordd D. The distance between these records is the synthetic to real distance (SRD) of d, andis denoted as SRD( d):",
  "Other Statistical Indicators": "The targeted correct attribution probability (TCAP) is an indicator of parameter inference attacksuccess rates. It measures how often synthetic parameter values correspond to real values in l-diverseequivalence classes. Furthermore, there are several probabilistic techniques to quantify the risks byusing real hold-out sets as baselines. Maximum mean discrepancy (MMD) can be also used as aprivacy metric to test if the generator overfits.",
  "Computer Scientific Experimental Privacy Assessment": "Computer-scientific privacy assessment involves performing privacy attacks using synthetic data.The effectiveness of these attacks is used to measure the degree of protection SD provides. Attackframeworks, as classified in of Appendix D, are based on threat models and the followingfactors: Attack Frameworks. These include Vulnerable Record Discovery (VRD), which identifiessynthetic records that are the result of overfitting generators. Other frameworks includeModel inversion, membership inference attacks (MIAs), and shadow modeling, which canall compromise confidentiality. Attack Mechanisms. Nearest Neighbors (NN) is one such attack mechanism, where anadversary infers missing attribute values based on its k synthetic nearest neighbors. Machinelearning (ML) techniques are another approach, where classifiers are trained to re-identifyreal data subjects. Additionally, information theory (IT) measures, such as Shannon entropyand mutual information, are sometimes used to identify records that may be more likely tobe memorized by the generator. Baselines and Effectiveness Estimation. The efficacy of a model can be measured in a fewdifferent ways. Absolute metrics include the probability with which records can be singledout, and the proportion of real records that can be re-identified. A random baseline approachuses random guessing to determine how effective an attack is. In a control baseline, the realdata is split into a training set and a control set. A model is trained on the training set, andthen the estimated success rate of attacks is compared on the training and control data sets.Another approach involves the deliberate insertion of secrets in training data or in the SDafter generation.",
  "Relation to WP29 Attack Types": "Singling out. VRD attacks directly implement singling-out attacks, identifying outlier SDrecords. MIAs can also model singling out, where an adversary quantifies the likelihood ofa unique real records attribute combination. Linkage. NN-based attacks usually require auxiliary information and can be interpreted aslinkage attacks. Anonymeter and information theory based VRD are the only methods thatexplicitly model linkage attacks.",
  "The Assessment Frameworks": "Mathematical privacy properties, such as differential privacy (DP), do not offer a clear choice of therequired parameters (, ). Large parameter values offer weak privacy guarantees, and a given canresult in different degrees of protection depending on the application. DP may still be vulnerable tolinkage and inference attacks, giving a false sense of security, and is a property of generators and not their synthetic data. The difficulty with k-anonymity is that implementing it causes considerableinformation loss and is an NP-hard problem. Furthermore, k-anonymity was shown to offer sufficientprotection only when the utility of the data is completely removed. In addition, k-anonymity is aproperty of synthetic data, and not the methods to produce them. Plausible deniability (PD) is onlyapplicable to seed-based methods. It shares properties with both DP and k-anonymity, making arecord protected if it can be confused with other records. Statistical privacy indicators are difficult to interpret, with many options and decision points, such asthe choice of similarity metric. Statistical indicators measure properties of the synthetic data, and nottheir generators. Computer-scientific experiments allow for flexible modeling using various threat models, and caninclude properties of both synthetic data and their generators. However, they require more data andcomputation than mathematical properties.",
  "Relation to Synthetic Data Risks": "All assessment frameworks address the issue of generator memorization. Mathematical propertiesfocus on the uniqueness of records. DP measures the impact of individual training records, withoutliers having large impacts, and both k-anonymity and PD focus on limiting the uniqueness ofrecords. Distance-based indicators are sensitive to outliers, because synthetic neighbors of outliershave small SRDs, while the RRD of corresponding real outliers is large. Furthermore, some methodsexplicitly search for outliers.",
  "For the future research directions we identify are:": "Standardizing privacy assessment: More interdisciplinary research is required to developan inclusive understanding of synthetic data. Standards should be developed for researchfindings to be more easily interpreted, and there should be a consensus formed over whetherprivacy is a property of synthetic datasets, the generators, or both. Synergies between assessments: A comparison between mathematical, statistical, andempirical approaches would be useful to evaluate their consistency, and to identify theirindividual merits and weaknesses. Experiments should use open-source generators andpublicly available datasets. It would also be useful to include information regarding the usedmetrics, and the use of a holdout set, and the statistical interpretation of the results. Outlier protection: Future research should investigate methods for outlier protection throughbinning and aggregating attributes or using innovative techniques. It would also be beneficialto see how outlier detection can be used to guide vulnerable record discovery. Incorporating privacy into generators: While DP is used in some generators, the sameis not true for all privacy metrics and empirical privacy methods. Future research shouldfocus on incorporating these, by integrating metrics in loss functions, or by combinatorialoptimization. Assessment for advanced data formats: More work is needed to assess privacy in relationaldatasets that have information contained in multiple, interconnected tables. In particular,profiling attacks, which re-identify subjects based on behavioral patterns, may play a keyrole in the assessment of relational databases.",
  "BB Example on Synthetic Datasets": "depicts an example of applying our safe predictor to a notional regression problem with 1-Dinput and outputs, and one input-output constraint. The unconstrained network has a single hiddenlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictorshares this structure with constrained predictors, G0 and G1, but each predictor has its own fullyconnected layer. The training uses a sampled subset of points from the input space and the learnedpredictors are shown for the continuous input space. shows an example of applying the safe predictor to a notional regression problem with a 2-Dinput and 1-D output and two overlapping constraints. The unconstrained network has two hiddenlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrainedpredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of pointsfrom the input space and the learned predictors are shown for the continuous input space.",
  "C.1 Safeability Constraints": "The safeability property from previous work can be encoded into a set of input-output constraints.The \"safeable region\" for a given advisory is the set of input space locations where that advisory canbe chosen, for which future advisories exist that will prevent a NMAC. If no future advisories exist,the advisory is \"unsafeable\" and the corresponding input region is the \"unsafeable region\". shows an example of these regions for the CL1500 advisory.",
  "C.2 Proximity Functions": "We start by generating the unsafeable region bounds. Then, a distance function is computed betweenpoints in the input space (vO vI, h, ), and the unsafeable region for each advisory. These are nottrue distances, but are 0 if and only if the data point is within the unsafeable set. These are then usedto produce proximity functions. shows examples of the unsafeable region, distance function,and proximity function for the CL1500 advisory.",
  "C.3 Structure of Predictors": "The compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hiddenlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for theunconstrained network. For constrained predictors, we use a similar architecture, but share the firstfour layers for all predictors. This provides a common learned representation of the input space, whileallowing each predictor to adapt to its constraints. Each constrained predictor has two additionalhidden layers and their outputs are projected onto our convex approximation of the safe output region,using Gb(x) = minj Gj(x) . In our experiments, we used = 0.0001. With this construction, we needed 30 separate predictors to enforce the VerticalCAS safeabilityconstraints. The number of nodes for the unconstrained and safe implementations were 270 and2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders ofmagnitude."
}