{
  "Abstract": "Studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined the scalingcharacteristics of transformers in next-token language prediction, yielding differentrecommendations for configuring the number of parameters (N) and training tokens(D) to minimize loss within a set compute budget (C). Kaplan suggested an optimalparameter count scaling with Noptimal C0.73, whereas Chinchilla proposedNoptimal C0.50. This paper demonstrates that a significant portion of thisdifference can be traced back to Kaplans focus on non-embedding parameters,rather than the total parameter count, along with their studys concentration on asmaller scale. When the Chinchilla study is simulated under similar circumstances,biased scaling coefficients similar to those of Kaplan are produced. As a result, thiswork confirms Chinchillas scaling coefficients by clarifying the primary reason forKaplans initial overestimation. Additionally, this research clarifies variations inthe stated correlations between computational loss and budget. As a result of thesefindings, we advocate for upcoming scaling investigations to utilize total parametercounts and overall computational resources.",
  "Introduction": "Two important studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined how scaleaffects large language models (LLMs). Both studies provided advice on how to balance modelparameters (N) and training tokens (D) for a fixed computing budget (C), but their suggestionsconflicted. The conclusion drawn from Kaplans discovery that Noptimal C0.73 and Doptimal C0.27 was that \"large models might be more crucial than extensive data.\" Subsequently, LLMstrained in the following years allocated more resources to model size and less to data size. TheChinchilla research that came after that discovered that Noptimal C0.50 and Doptimal C0.50,which resulted in their main argument that \"for many current LLMs, smaller models should havebeen trained on more tokens to achieve the most performant model.\" This sparked a trend in whichLLMs with smaller model sizes were trained using more data. What caused the discrepancy in these scaling coefficient estimates, which resulted in a significantwaste of computer resources, emissions, and money? There have been theories suggesting thatvariations in optimization techniques or datasets might account for the differences. This paper arguesthat these explanations are insufficient and proposes a straightforward substitute: the majority ofthe discrepancy is caused by Kaplans decision to count non-embedding parameters instead of totalparameters, together with the limited scale of their investigation.",
  "Set Up": "Kaplan et al. (2020) and Hoffmann et al. (2022) conducted empirical studies to model the relationshipsbetween the number of parameters (N), training tokens (D), training compute (C), and loss (L) intransformers used for language modeling. The primary functional relationship explored was a powerlaw, y = axb, which is frequently employed in various scientific fields to illustrate the connectionbetween two quantities (x and y) that span multiple orders of magnitude. The two studies differed in their definitions of N and C. Kaplan investigated relationships regardingnon-embedding parameters (NE) and non-embedding compute (CE), excluding contributions from embedding layers for vocabulary and position indices (NE). Incontrast, Chinchilla studied total parameters (NT) and total compute (CT). We define,",
  "NT = NE + NE,(1)NE = (h + v)d,(2)": "where d represents the transformer residual streams dimension, v denotes the vocabulary size, andh stands for the context length (included only when positional embeddings are learned). Utilizingthe typical approximation for training compute FLOPs C = 6ND (where a factor of 6 accounts for aforward and backward pass), we establish total and non-embedding compute as:",
  "CT = 6NTD = 6(NE + NE)D,(3)CE = 6NED.(4)": "The definition of compute, C = 6ND, indicates a direct trade-off between the number of parametersand training tokens for a specified compute budget. The focus of the two research studies is on\"compute optimal\" configurations, which are the parameter and token combinations that result in thelowest loss for a given compute budget. This is expressed as follows for total parameters (using todenote \"optimal\"):",
  "L(NT, D) = NcNT + DcD03b2 + E,(8)": "where Nc, Dc, , > 0 are empirically determined constants, and E represents the irreducible lossinherent in language. This equation conveniently generates power-law relationships: N T Ca Twith a = / ( + ), D T Cb T with b = / ( + ), and L T - E C T with = / ( + ). There are two possible specifications based on the constants in Equation 8: those originally reportedin the Chinchilla study and those from a re-analysis by Besiroglu et al. (2024), which claims tocorrect minor errors in the fitting procedure. Our work presents results using both specifications.",
  "Analysis Overview": "In our analysis, we use data and insights from the Chinchilla and Kaplan studies to predict the scalinglaws that would result if the Chinchilla relationship were stated in terms of NE and CE, and this was done using the smaller model sizes used in Kaplans study. It will be demonstrated that when NT is large, NE becomes an insignificant component of the models parameters and computing cost. As a result, thetwo coefficients are in direct opposition to one another in the large parameter regime. The embeddingparameters are not insignificant when NT is smaller (this is the regime examined in Kaplans study,which used parameters ranging from 768 to 1.5B). We discover that the relationship between NE and CE is not, in fact, a power law at the lower end of this range. However, fitting a \"local\" power law atthis modest scale yields a coefficient that is comparable to Kaplans, roughly reconciling these twofindings.",
  "Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used inthe Kaplan study. Fit a local power law for NE in terms of CE": "provides experimental validation of our analysis by training a set of language models at avery small scale and examining scaling laws under different settings. Simply changing the basis fromNT to NE yields coefficients consistent with Chinchilla and Kaplan, respectively, while varying token budgetsand decay schedules does not. A second, connected contribution is made in . The two studies suggested relationshipsbetween loss and computation are reconciled by us. In order to examine the relationship between theideal loss LE and compute CE, Steps 3 and 4 are carried out once more using a similar analysis as before. To do this, we start withChinchilla data and adjust for the smaller model sizes utilized in Kaplans investigation, the exclusionof embedding parameters and compute, and a different fitting function option. We are able to roughlyrecover Kaplans compute-loss coefficient and reconcile the two studies by making these adjustments.",
  "NT = 12ld2 + NE,(12)": "where l represents the number of layers. While Kaplan does not explicitly list their model configura-tions, they do explore varying the aspect ratio A = d/l for a fixed model size. They determine thatmodels of a given size exhibit similar performance across a range of aspect ratios, and this is notinfluenced by model scale (their ). Consequently, we could propose a sizing scheme with afixed aspect ratio (A 40 appears reasonable from their plots). Assuming this sizing allows us tostate (with l = d/A in Equation 12):",
  "Chinchilla perspective. We empirically fit a function NT = NE + NE (note the learnable exponent) to the Chinchilla model configurations listed in Table A9 of Hoffmann": "et al. (2022) for a range of NT (44M to 16B). We calculate NE from Equation 2, using the reportedvocabulary size of 32,000, but disregard the context length of 2,048 since Chinchilla used non-learnable position embeddings (though their inclusion only slightly affects the coefficients). Fitting a model with numpys polyfit yields coefficients = 47491 and = 0.34. The exponent isclose to 1/3, with an implied aspect ratio A = 39.2 (inferred from ). This further supports the formin Equation 11.",
  "+ )1 (20)": "This indicates that, generally, the relationship between NE and CE is not a power law. Nevertheless, we can think about a \"local\" power law approximation. That is,for a specific value of NE, there exists a constant g that provides a first-order approximation (denoted by ) NE, where g is defined as:",
  "+E, consistent with the NT case in Equation 17": "A transition phase exists where g briefly increases. This occurs between the two limits whenN2/3E is of the same order as . Indeed, at exactly the point N2/3E = , we have NT = NE + N1/3E = NT = 2NE, indicating a 50/50 split between embedding and non-embedding parameters.",
  "Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in theKaplan study. Fit a local power law for NE in terms of CE": "By reading g, we could estimate a local power law and thus a scaling coefficient for a specific valueof NE. However, it is unclear which NE value is representative of the Kaplan study. We choose a more accurate estimation approach,creating synthetic training curves from Equation 18 over the range of model sizes employed in theKaplan study, and fitting coefficients using models that lie on the compute-efficient frontier. This willalso validate our analytical expression for NE and CE in Equation 19. We simulated 20 models with NE ranging from 790 parameters to 1.58B (Kaplan reports using model sizes \"ranging in size from768 to 1.5 billion non-embedding parameters\"). For other constants in Equation 18, we adopt theEpoch AI specification (Equation 10) and = 47491, though we also report results for the Chinchillaspecification (Equation 9). Main result. The estimated scaling coefficient is shown when a power law is fitted to the computeoptimal frontier (Chinchillas Method 1) generated by these synthetic training curves. This representsour primary finding - by starting with a model from the Chinchilla study and modifying two aspectsto match Kaplans study (NT NE, small model sizes 0.79k - 1.58B parameters), we obtain local scaling coefficients:",
  "Experiment 1. First, we confirm if scaling coefficients approximate those of Chinchilla and Kaplanwhen employing NT and NE, respectively": "Five models with sizes NT [0.8M, 1.6M, 2.1M, 3.3M, 4.6M] were trained using the BookCorpusdataset. The GPT-2 tokenizer was used, with a vocabulary size of 50,257 and a context length of16 (although this is much less than normal, our tests indicate that context length has no impact onscaling coefficients). To estimate scaling coefficients, Chinchillas Method 1 was applied, using theapproximation C = 6ND. Models were trained for updates , with a batch size of 65,536tokens per update, for a total of training tokens D [262M, 262M, 262M, 524M, 524M]. For eachmodel size, the optimal learning rate was selected from [0.001, 0.005, 0.01, 0.05], and no annealingwas implemented.",
  "Scheme 3. The best learning rate is chosen per model. A single model is trained per size,and cosine annealing is applied at the update budget. (Kaplan study used this.)": "Scheme 4. The best learning rate is chosen per model. Six models are trained per size atdifferent budgets [0.25D, 0.5D, 0.75D, 1.0D, 1.5D, 2.0D], and cosine annealing is applied.(Chinchilla study used this.) Result 2. The optimization technique has less of an influence on scaling coefficients than switchingfrom NT to NE. Using a single set of models without annealing (scheme 2) yields coefficients that are identical tothose of the more computationally demanding scheme 4. In contrast to Chinchillas assertion thatswitching from Kaplans scheme 3 to scheme 4 would lower the scaling coefficient, our researchindicates the opposite, with an increase from 0.46 to 0.49. This might account for our minoroverestimation of the scaling coefficients in Equations 21 and 22.",
  "EpochAI : LTEC0.178T(30)": "(Refer to Section A.3 for Chinchillas compute coefficient.) Similar to the compute-parameter scalingcoefficient, Kaplans coefficient of 0.057 initially appears significantly different from Chinchillasrange of 0.155 to 0.178. However, we will again demonstrate that by starting with the Chinchillastudy and adjusting for Kaplans non-embedding compute, smaller scale, and their compute-lossform, these two coefficients can be largely reconciled. Our analysis follows the same four-step approach as in . We can directly reuse Steps 1 and2, while Steps 3 and 4 are now modified to study the relationship between optimal loss and compute,rather than optimal parameters and compute as previously.",
  "Related work": "After early research that established how language models get better with parameters, data, andtraining computation, there has been research into the theoretical underpinnings of these scaling lawsand whether they apply to other domains. Several concurrent studies that have looked at how different design decisions affect scaling lawanalyses are more closely related to the spirit of our work. The methodology for determining scalingcoefficients is revisited by Su et al. (2024). Hagele et al. (2024) discovered that multiple shortdecays with a constant learning rate or stochastic weight averaging may be used to recreate numerousindependent cosine schedules more effectively. Our discovery is subtly different; a straightforwardfixed learning rate will recover extremely comparable compute-parameter scaling coefficients asmany cosine schedules. The impact of different hyperparameters on scaling laws is examined by Biet al. (2024). They point out that different text datasets yield somewhat different optimal coefficients,with \"cleaner\" data exhibiting more parameter-hungry scaling behavior, which they believe maypartially account for the discrepancy between the Kaplan and Chinchilla coefficients. The goal of Porian et al. (2024)s concurrent work is to clarify the discrepancies between the Kaplanand Chinchilla coefficients, which is the same goal as that of our paper. They conduct a numberof large-scale experiments that replicate Kaplans study, and they come to the conclusion that thediscrepancy is caused by, in decreasing order of importance: 1) Kaplans use of non-embeddingcompute rather than total compute; 2) Kaplans use of an excessively long fixed-length warmupperiod for smaller models, which made them appear less efficient; and 3) Kaplans failure to fullyoptimize hyperparameters. We believe that these findings complement our own. We have usedan entirely analytical method to identify the main \"first order\" cause using just the data that wasmade publicly available in the two papers. (As a form of verification, tiny-scale experiments wereconducted post-hoc.) This shows how mathematical techniques can be used in scalings empiricalscience.",
  "Discussion": "This study sought to account for the disparity between the scaling coefficients of Kaplan andChinchilla. We discovered two problems with Kaplans study that, when taken together, biased theirestimated scaling coefficients: they focused on smaller model sizes and only counted coefficients:they focused on smaller model sizes and only counted non-embedding parameters. This impliesa curvature in the actual relationship between Nand NT (). At greater values of NT, theembedding parameter counts become negligible, NT = N, and differences would not arise. Alterna-tively, had Kaplan investigated relationships directly in terms of NT, this issue would also not occur,even at this smaller scale (confirmed by our Experiment 1 finding NT ()C(0.49)TevenforNT <5M).TheformKaplanusedtopredictlossfromcomputefurthercontributedtodifferencesinthereportedcomputelossscalingcoefficients. Inconsistency across scaling studies. Existing literature on scaling is not consistent in its use ofnon-embedding vs. total compute. Some studies follow Kaplans approach, using non-embeddingparameters or compute, while others adhere to the Chinchilla approach, using total parameters. Ourwork indicates that this choice can substantially alter scaling exponents, complicating cross-studycomparisons. Similarly, the choice of compute-loss equation varies through the literature. Studiessuch as opt for the Kaplan compute-loss form without offsets. In contrast, employ the Chinchillacompute-loss form with non-zero offsets. Again, our work suggests that these methodologicaldifferences can lead to significant variations in scaling predictions and interpretations. The lack of a standardized approach in scaling studies risks making comparisons misleading andinsights less clear. We see our work as helping to understand certain decisions made in previousstudies that should be standardized. Concretely, we advise future studies to report total, rather thannon-embedding, parameters, and to include an offset in the compute-loss fitting models. We discussmotivation for these choices below. Furthermore, our initial evidence does not support using multiple",
  "cosine decays per model size we find a single fixed learning rate per model size is sufficient formeasuring compute-optimal parameter coefficients": "Why should embedding parameters contribute to scaling behavior? Several works provide evidencethat embedding parameters capture meaningful language properties. Word embeddings can befactorized into semantically interpretable factors (even the shallow Word2vec). LLMs learn linearembeddings of space and time across scales. Developing such meaningful embedding structuresallows LLMs to perform high-level language operations, such as arithmetic. Therefore, if one believesthat the embedding layer does more than just translate tokens to a vector of the correct dimension,we see no reason to exclude them in the parameter count. Why should a non-zero offset be used in loss-compute predictions? The Chinchilla compute-loss formwith a non-zero offset (Equation 27) is a more appropriate form from the perspective of statisticallearning. This approach accounts for the concept of irreducible risk, which posits a lower bound onachievable loss regardless of model or dataset size. This may arise from various factors: inherentbiases or limitations in the learning algorithm, or noise in the original task. As a concrete example inlanguage modeling, the best a model can do for the prediction of the first token in a sequence is toestimate the marginal distribution of all tokens, which leads to a non-zero loss. Limitations. We acknowledge several limitations of our analysis. We have aimed to capture theprimary first order reason for the difference between the Kaplan and Chinchilla scaling coefficients.But there are multiple other differences between the two studies that likely also affect scaling coeffi-cients (); datasets (Kaplan used OpenWebText2, Chinchilla used MassiveText), transformerdetails (Kaplan used learnable position embeddings while Chinchillas were fixed, also differingtokenizers, vocabularly sizes), optimization scheme (Kaplan used scheme 3, Chinchilla scheme4, also differing warmup schedules), differences in computation counting (Kaplan used C = 6ND,Chinchillas Method 1 and 2 used a full calculation). However, our work suggested these factorsimpact coefficients in a more minor way."
}