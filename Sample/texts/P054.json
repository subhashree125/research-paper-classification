{
  "Abstract": "The growing focus on computer vision for applications in nutritional monitoringand dietary tracking has spurred the creation of sophisticated 3D reconstructionmethods for various food items. A lack of high-quality data, combined withinsufficient collaboration between academic research and industry applications,has hindered advancements in this area. This paper outlines a comprehensiveworkshop and challenge centered on physically informed 3D food reconstruction,leveraging recent progress in 3D reconstruction technologies. The central objectiveof this challenge is to create volume-accurate 3D models of food using 2D images,with a visible checkerboard serving as a critical size reference. Participants wereassigned the task of building 3D models for 20 distinct food items, each presentingvarying degrees of difficulty: easy, medium, and hard. The easy category offers200 images, the medium provides 30, and the hard level includes only a singleimage to facilitate the reconstruction process. During the final evaluation stage, 16teams presented their results. The methodologies developed during this challengehave yielded encouraging outcomes in 3D food reconstruction, demonstratingconsiderable potential for enhancing portion estimation in dietary evaluations andnutritional tracking.",
  "Introduction": "The merging of computer vision with the culinary domain has unveiled new possibilities in dietaryoversight and nutritional evaluation. The 3D Food Modeling Workshop Challenge signifies a notableadvancement in this domain, responding to the escalating demand for precise and adaptable techniquesfor estimating food portions and monitoring nutritional consumption. These technological solutionsare essential for encouraging beneficial eating patterns and addressing health issues related to diet. This initiative aims to close the divide between current methodologies and practical needs byconcentrating on the development of accurate 3D models of food items from multi-view and single-view image data. The challenge promotes the creation of novel methods capable of managing theintricacies of food forms, textures, and variations in lighting, all while adhering to the practicallimitations inherent in real-world dietary assessment situations. Conventional methods for diet assessment, like 24-Hour Recall or Food Frequency Questionnaires(FFQ), frequently depend on manual data entry, which can be imprecise and difficult to manage.Additionally, the lack of 3D data in 2D RGB food images poses significant hurdles for methodsthat rely on regression to estimate food portions directly from images of eating occasions. Bymaking progress in 3D reconstruction techniques for food, the aim is to provide tools for nutritionalassessment that are more accurate and easier to use. This technology holds the potential to enhancethe way food experiences are shared and could significantly influence areas such as nutritional scienceand public health initiatives. Participants were tasked with creating 3D models of 20 different food items from 2D images,simulating a scenario where a smartphone equipped with a depth-sensing camera is employed fordietary recording and nutritional oversight. The challenge was divided into three levels of complexity:",
  "Related Work": "Estimating food portions is a crucial part of image-based dietary assessment, with the objective ofdetermining the volume, energy content, or macronutrient breakdown directly from images of meals.Unlike the extensively researched area of food recognition, determining food portions presents adistinct difficulty because of the lack of 3D data and physical benchmarks, which are necessaryfor precisely deducing the actual sizes of food portions. Specifically, accurately estimating portionsizes requires an understanding of the volume and density of the food, aspects that cannot be easilydetermined from a two-dimensional image, which highlights the need for advanced methodologiesand technologies to address this issue. Current methods for estimating food portions are classifiedinto four primary categories. Stereo-Based Approaches. These techniques depend on multiple frames to deduce the 3D con-figuration of food items. For instance, some methods calculate food volume through multi-viewstereo reconstruction based on epipolar geometry, while others use a two-view dense reconstructionapproach. Another technique, Simultaneous Localization and Mapping (SLAM), is employed forcontinuous, real-time estimation of food volume. However, the need for multiple images limits thepracticality of these methods in real-world situations. Model-Based Approach. This approach uses predefined shapes and templates to estimate the targetvolume. Some methods assign specific templates to foods from a reference set and make adjustmentsbased on physical cues to gauge the size and position of the food. A similar approach that matchestemplates is employed to estimate food volume from just one image. However, these methods struggleto accommodate foods with shapes that do not conform to the established templates. Depth Camera-Based Approach. This method utilizes depth cameras to create maps that indicatethe distance from the camera to the food in the picture. The depth map is then used to create a voxelrepresentation of the image, which aids in estimating the foods volume. The primary drawbacks arethe need for high-quality depth maps and the additional processing steps required for depth sensorsused by consumers. Deep Learning Approach. Techniques based on neural networks use the vast amount of image dataavailable to train sophisticated networks for estimating food portions. Some use regression networksto estimate the caloric value of food from a single image or from an \"Energy Distribution Map\" thatcorrelates the input image with the energy distribution of the foods shown. Others use regressionnetworks trained on images and depth maps to deduce the energy, mass, and macronutrients of thefood in the image. These methods require extensive data for training and are generally not transparent.Their performance can significantly decline if the input test image deviates substantially from thetraining data. Despite the progress these methods have made in estimating food portions, they each have limitationsthat restrict their broad use and precision in practical scenarios. Methods based on stereo are notsuitable for single-image inputs, those based on models have difficulty with a variety of food shapes,approaches using depth cameras necessitate specialized equipment, and deep learning methods are noteasily interpretable and have difficulty with samples that are different from those they were trained on.To tackle these issues, 3D reconstruction provides a viable solution by offering thorough spatial data,accommodating different food shapes, possibly functioning with just one image, presenting resultsthat are visually understandable, and facilitating a uniform method for estimating food portions. Thesebenefits were the driving force behind the organization of the 3D Food Reconstruction challenge,which seeks to surmount the current limitations and create techniques for food portion estimationthat are more accurate, user-friendly, and broadly applicable, thereby making a significant impact onnutritional assessment and dietary monitoring.",
  "Dataset Description": "The dataset for the 3D Food Modeling Challenge includes 20 carefully chosen food items, eachhaving been scanned with a 3D scanner and also captured on video. To ensure the reconstructed 3Dmodels accurately represent size, each food item was captured alongside a checkerboard and patternmat, which provide a physical reference for scaling. The challenge is segmented into three levels ofdifficulty, based on the number of 2D images provided for reconstruction:",
  "Teams that perform well in Phase-I are asked to provide full 3D mesh files for each food item. Thisphase includes multiple steps to guarantee both accuracy and fairness:": "1. Model Verification: Submitted models are checked against the final submissions fromPhase-I to ensure they are consistent. Visual inspections are also conducted to prevent anyviolations of the rules, such as submitting basic shapes (like spheres) rather than detailedreconstructions. 2. Model Alignment: Participants are given the true 3D models and the script used forcalculating the final Chamfer distance. They must align their models with these true modelsand create a transformation matrix for each item submitted. The ultimate Chamfer distancescore is then calculated using the submitted models and their corresponding transformationmatrices.",
  "yYminxX x y22": "This metric offers a thorough assessment of how closely the reconstructed 3D models match theactual models. The ultimate ranking is determined by merging the scores from both Phase-I (accuracyof volume) and Phase-II (accuracy of shape). It should be noted that after evaluating Phase-I, someissues with the data quality for object 12 (steak) and object 15 (chicken nugget) were found. Tomaintain the competitions quality and fairness, these two items have been removed from the finaloverall evaluation.",
  "Overview": "The teams method integrates computer vision and deep learning to accurately estimate food volumefrom RGBD images and masks. Keyframe selection, supported by perceptual hashing and blurdetection, ensures data quality. The estimation of camera poses and object segmentation establishesthe basis for neural surface reconstruction, resulting in detailed meshes for volume estimation.Refinement processes, such as removing isolated parts and adjusting the scaling factor, improveaccuracy.",
  "The Teams Proposal: VolETA": "The team starts their process by obtaining input data, specifically RGBD images and their correspond-ing food object masks. These RGBD images are denoted as ID = {IDi }ni=1, where n is the totalnumber of frames, providing the necessary depth information alongside the RGB images. The foodobject masks, denoted as {M Fi }ni=1, help identify the regions of interest within these images. Next, the team proceeds with keyframe selection. From the set {IDi }ni=1, keyframes {IKj }kj=1 {IDi }ni=1 are selected. The team implements a method to detect and remove duplicates and blurryimages to ensure high-quality frames. This involves applying the Gaussian blurring kernel followedby the fast Fourier transform method. Near-Image Similarity employs a perceptual hashing andhamming distance thresholding to detect similar images and keep overlapping. The duplicates andblurry images are excluded from the selection process to maintain data integrity and accuracy.",
  "using a matching algorithm, and refining them). The outputs are the set of camera poses {Cj}kj=1,which are crucial for spatial understanding of the scene": "In parallel, the team utilizes a segmentation algorithm for reference object segmentation. Thisalgorithm segments the reference object with a user-provided segmentation prompt (i.e., user click),producing a reference object mask M R for each keyframe. This mask is a foundation for tracking thereference object across all frames. The team then applies a memory tracking method, which extendsthe reference object mask M R to all frames, resulting in a comprehensive set of reference objectmasks {M Ri }ni=1. This ensures consistency in reference object identification throughout the dataset. To create RGBA images, the team combines the RGB images, reference object masks {M Ri }ni=1, andfood object masks {M Fi }ni=1. This step, denoted as {IRi }ni=1, integrates the various data sources intoa unified format suitable for further processing.",
  "The team converts the RGBA images {IRi }ni=1 and camera poses {Cj}kj=1 into meaningful metadataand modeled data Dm. This transformation facilitates the accurate reconstruction of the scene": "The modeled data Dm is then input into a neural surface reconstruction algorithm for mesh recon-struction. This algorithm generates colorful meshes {Rf, Rr} for the reference and food objects,providing detailed 3D representations of the scene components. The team applies the \"RemoveIsolated Pieces\" technique to refine the reconstructed meshes. Given that the scenes contain onlyone food item, the team sets the diameter threshold to 5% of the mesh size. This method deletesisolated connected components whose diameter is less than or equal to this 5% threshold, resulting ina cleaned mesh {RCf, RCr}. This step ensures that only significant and relevant parts of the meshare retained. The team manually identifies an initial scaling factor S using the reference mesh via a mesh processingtool for scaling factor identification. This factor is then fine-tuned Sf using depth information andfood and reference masks, ensuring accurate scaling relative to real-world dimensions. Finally, thefine-tuned scaling factor Sf is applied to the cleaned food mesh RCf, producing the final scaledfood mesh RFf. This step culminates in an accurately scaled 3D representation of the food object,enabling precise volume estimation.",
  "Detecting the scaling factor": "Generally, 3D reconstruction methods generate unitless meshes (i.e., no physical scale) by default.To overcome this limitation, the team manually identifies the scaling factor by measuring the distancefor each block for the reference object mesh. Next, the team takes the average of all blocks lengthslavg, while the actual real-world length is constant lreal = 0.012 in meter. Furthermore, the teamapplies the scaling factor S = lreal/lavg on the clean food mesh RCf, producing the final scaledfood mesh RFf in meter. The team leverages depth information alongside food and reference object masks to validate thescaling factors. The teams method for assessing food size entails utilizing overhead RGB imagesfor each scene. Initially, the team determines the pixel-per-unit (PPU) ratio (in meters) using thereference object. Subsequently, the team extracts the food width (fw) and length (fl) employing afood object mask. To ascertain the food height (fh), the team follows a two-step process. Firstly, theteam conducts binary image segmentation using the overhead depth and reference images, yielding asegmented depth image for the reference object. The team then calculates the average depth utilizingthe segmented reference object depth (dr). Similarly, employing binary image segmentation with anoverhead food object mask and depth image, the team computes the average depth for the segmentedfood depth image (df). Finally, the estimated food height fh is computed as the absolute differencebetween dr and df. Furthermore, to assess the accuracy of the scaling factor S, the team computesthe food bounding box volume ((fw fl fh) PPU). The team evaluates if the scaling factor Sgenerates a food volume close to this potential volume, resulting in Sfine. For one-shot 3D reconstruction, the team leverages a single view reconstruction method for recon-structing a 3D from a single RGBA view input after applying binary image segmentation on bothfood RGB and mask. Next, the team removes isolated pieces from the generated mesh. After that, theteam reuses the scaling factor S, which is closer to the potential volume of the clean mesh.",
  "Implementation settings": "The experiments were conducted using two GPUs: a GeForce GTX 1080 Ti with 12GB of memoryand an RTX 3060 with 6GB of memory. For near-image similarity detection, the Hamming distancewas set to 12. To identify blurry images, even numbers within the range of [0...30] were used as theGaussian kernel radius. In the process of removing isolated pieces, a diameter threshold of 5% wasapplied. Neural surface reconstruction involved 15,000 iterations, with a mesh resolution of 512x512.The unit cube parameters were set with an \"aabb scale\" of 1, \"scale\" at 0.15, and \"offset\" at [0.5, 0.5,0.5] for each food scene.",
  "VolETA Results": "The team extensively validated their approach on the challenge dataset and compared their results withground truth meshes using MAPE and Chamfer distance metrics. More Briefly, the team leveragestheir approach for each food scene separately. A one-shot food volume estimation approach is appliedif the number of keyframes k equals 1. Otherwise, a few-shot food volume estimation is applied. Theteams keyframe selection process chooses 34.8% of total frames for the rest of the pipeline, where itshows the minimum frames with the highest information.",
  "Dij = (P ki P kj )2i = j": "To determine the final computed length of each checkerboard square in image k, the team takes theminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. Themedian of this vector is then used. The final scale calculation formula is given by the followingequation, where 0.012 represents the known length of each square (1.2 cm):",
  "Considering the differences in input viewpoints, the team utilizes two pipelines to process the firstfifteen objects and the last five single view objects": "For the first fifteen objects, the team uses a Structure from Motion algorithm to estimate the posesand segment the food using the provided segment masks in the dataset. Then, they apply advancedmulti-view 3D reconstruction methods to reconstruct the segmented food. In practice, the teamemploys three different reconstruction methods. They select the best reconstruction results from thesemethods and extract the mesh from the reconstructed model. Next, they scale the extracted meshusing the estimated scale factor. Finally, they apply some optimization techniques to obtain a refinedmesh. For the last five single-view objects, the team experiments with several single-view reconstructionmethods. They choose a specific method to obtain a 3D food model consistent with the distributionof the input image. In practice, they use the intrinsic camera parameters from the fifteenth objectand employ an optimization method based on reprojection error to refine the extrinsic parametersof the single camera. However, due to the limitations of single-view reconstruction, the team needsto incorporate depth information from the dataset and the checkerboard in the monocular image todetermine the size of the extracted mesh. Finally, they apply optimization techniques to obtain arefined mesh.",
  "Alignment": "The team designs a multi-stage alignment method for evaluating reconstruction quality. illustrates the alignment process for Object 14. First, the team calculates the central points of both thepredicted model and the ground truth model, and moves the predicted model to align the central pointof the ground truth model. Next, they perform ICP registration for further alignment, significantlyreducing the Chamfer distance. Finally, they use gradient descent for additional fine-tuning, andobtain the final transformation matrix. The total Chamfer distance between all 18 predicted modelsand the ground truths is 0.069441169.",
  "Methodology": "To achieve high-quality food mesh reconstruction, the team designed two pipeline processes. Forsimple and medium cases, they employed a structure-from-motion approach to determine the pose ofeach image, followed by mesh reconstruction. Subsequently, a series of post-processing steps wereimplemented to recalibrate scale and enhance mesh quality. For cases with only a single image, theteam utilized image generation methods to aid in model generation.",
  "Multi-View Reconstruction": "For Structure from Motion (SfM), the team extended the state-of-the-art method by incorporatingmethodologies. This significantly mitigated the issue of sparse keypoints in weakly textured scenes.For mesh reconstruction, the teams method is based on a differentiable renderer and incorporatesregularization terms for depth distortion and normal consistency. The Truncated Signed DistanceFunction (TSDF) results are used to generate a dense point cloud. In the post-processing stage, theteam applied filtering and outlier removal techniques, identified the contour of the supporting surface,and projected the lower mesh vertices onto the supporting surface. They used the reconstructedcheckerboard to rectify the scale of the model and used Poisson reconstruction to generate a watertight,complete mesh of the subject.",
  "Experimental Results": "Through a process of nonlinear optimization, the team sought to identify a transformation thatminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimizationaimed to align the two meshes as closely as possible in three-dimensional space. Upon completion ofthis process, the average Chamfer distance across the final reconstructions of the 20 objects amountedto 0.0032175 meters. As shown in , Team FoodRiddle achieved the best scores for bothmulti-view and single-view reconstructions, outperforming other teams in the competition.",
  "Conclusion": "In this report, we provide a summary and analysis of the methodologies and findings from the3D Food Reconstruction challenge. The primary goal of this challenge was to push the envelopein 3D reconstruction technologies, with an emphasis on the unique challenges presented by fooditems, such as their varied textures, reflective surfaces, and complex geometries. The competitionfeatured 20 diverse food items, captured under various conditions and with varying numbers of inputimages, specifically designed to challenge participants in developing robust reconstruction models.The evaluation was based on a two-phase process, assessing both portion size accuracy throughMean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance metric.Of all participating teams, three made it to the final submission, showcasing a range of innovativesolutions. Team VolETA won first place with the overall best performance on both Phase-I andPhase-II, followed by team ININ-VIAUN who won second place. In addition, FoodRiddle teamdemonstrated superior performance in Phase-II, indicating a competitive and high-caliber field ofentries for 3D mesh reconstruction. The challenge has successfully pushed the boundaries of 3D foodreconstruction, demonstrating the potential for accurate volume estimation and shape reconstructionin nutritional analysis and food presentation applications. The innovative approaches developed bythe participating teams provide a solid foundation for future research in this field, potentially leadingto more accurate and user-friendly methods for dietary assessment and monitoring."
}