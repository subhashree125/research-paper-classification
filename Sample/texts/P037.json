{
  "Abstract": "This paper introduces a novel dataset for Chinese machine reading comprehension,focusing on span extraction. The data set is constructed using roughly 20,000 real-world questions that are annotated by experts on passages extracted from Wikipedia.A challenge set is also created with questions that demand a deep understandingand inference across multiple sentences. We also show several baseline models andanonymous submission scores to emphasize the challenges present in this dataset.The release of this dataset facilitated the Second Evaluation Workshop on ChineseMachine Reading Comprehension, also called CMRC 2018. We anticipate that thisdataset will further facilitate research in Chinese machine reading comprehension.",
  "Introduction": "The capacity to interpret and comprehend natural language is a crucial component of achievingadvanced artificial intelligence. Machine Reading Comprehension (MRC) is designed to understandthe context of given texts and respond to related questions. Numerous types of MRC datasets havebeen developed, such as cloze-style reading comprehension, span-extraction reading comprehension,open-domain reading comprehension, and multiple-choice reading comprehension. Along with theincreasing availability of reading comprehension datasets, several neural network methods have beenproposed, leading to substantial advancements in this area. There have also been various efforts to create Chinese machine reading comprehension datasets.In cloze-style reading comprehension, a Chinese cloze-style reading comprehension dataset wasproposed, namely Peoples Daily Childrens Fairy Tale. To increase the difficulty of the dataset, theyalso release a human-annotated evaluation set in addition to the automatically generated developmentand test sets. Later, another dataset was introduced using childrens reading materials. To promotediversity and explore transfer learning, they also offer a human-annotated evaluation dataset usingmore natural queries compared to the cloze type. This dataset was the main component in the firstevaluation workshop on Chinese machine reading comprehension (CMRC 2017). Furthermore, alarge-scale open-domain Chinese machine reading comprehension dataset (DuReader) was created,containing 200k queries from search engine user query logs. There is also a reading comprehensiondataset in Traditional Chinese. While current machine learning techniques have outperformed human-level performance on datasetslike SQuAD, it is still unclear whether similar results can be achieved on datasets using differentlanguages. To accelerate the progress of machine reading comprehension research, we present aspan-extraction dataset tailored for Chinese.",
  "Human Annotation": "The questions in this dataset were created entirely by human experts, setting it apart from prior worksthat relied on automated data generation methods. Initially, documents are divided into passages,each containing no more than 500 Chinese words. Annotators are required to assess each passage forits suitability, discarding those that are too difficult for public understanding. Passages were discardedbased on the following rules: If more than 30% of the passage consists of non-Chinese characters. If the passage includes too many specialized or professional terms. If the passage has a large number of special characters or symbols. If the paragraph is written in classical Chinese. After determining that the passage is suitable, annotators generate questions and their correspondingprimary answers based on the provided passage. During this question annotation, the following rulesare used.",
  "Long answers (over 30 characters) will be discarded": "For the evaluation sets, which include the development, test, and challenge sets, three answers areavailable for a more thorough assessment. Besides the primary answer generated by the questionproposer, two additional annotators write a second and third answer for each question. Theseadditional annotators do not see the primary answer to avoid biased answers.",
  "Challenge Set": "A challenge set was made to evaluate how effectively models can perform reasoning over diverseclues in the context, while still maintaining the span-extraction format. This annotation was alsocompleted by three annotators. The questions in this set need to meet the following criteria: The answer can not be deduced from a single sentence in the passage if the answer is asingle word or a short phrase. The annotation should encourage asking complex questionsthat need an overall view of the passage to answer correctly. If the answer is a named entity or belongs to a particular genre, it cannot be the only instancein the passage. There should be more than one instance to make the correct choice moredifficult for the model.",
  "F1-Score": "The F1-score evaluates the fuzzy overlap at the character level between the prediction and the groundtruth answers. Instead of treating the answers as a bag of words, we calculate the longest commonsequence (LCS) between the prediction and the ground truth and then compute the F1-score. Themaximum F1 score among all the ground truth answers is taken for each question.",
  "Estimated Human Performance": "The estimated human performance is computed to measure the difficulty of the proposed dataset.Each question in the development, test, and challenge set has three answers. We use a cross-validationmethod to compute the performance. We treat the first answer as a human prediction and consider theother two answers as ground truth. Using this process, three human prediction scores are generated.Finally, we calculate the average of these three scores as the estimated human performance.",
  "Baseline System": "We use BERT as the foundation of our baseline system. We modified the original script to accommo-date our dataset. The initial learning rate was set to 3e-5, with a batch size of 32, and the trainingwas conducted for two epochs. The document and query maximum lengths were set to 512 and 64respectively.",
  "Results": "The results are in . Besides the baseline results, we include the results of the participantsin the CMRC 2018 evaluation. The training and development sets were released to the public, andsubmissions were accepted to evaluate the models on the hidden test and challenge sets. As we cansee that most of the participants achieved an F1 score above 80 in the test set. On the other hand, theEM metric shows considerably lower scores in comparison to the SQuAD dataset, highlighting thatdetermining the precise span boundary is crucial for performance enhancement in Chinese readingcomprehension.",
  "DevelopmentTestChallengeEMF1EMF1EMF1": "Estimated Human Performance91.08397.34892.40097.91490.38295.248Z-Reader (single model)79.77692.69674.17888.14513.88937.422MCA-Reader (ensemble)66.69885.53871.17588.09015.47637.104RCEN (ensemble)76.32891.37068.66285.75315.27834.479MCA-Reader (single model)63.90282.61868.33585.70713.69033.964OmegaOne (ensemble)66.97784.95566.27282.78812.10330.859RCEN (single model)73.25389.75064.57683.13610.51630.994GM-Reader (ensemble)58.93180.06964.04583.04615.67537.315OmegaOne (single model)64.43082.69964.18881.53910.11929.716GM-Reader (single model)56.32277.41260.47080.03513.69033.990R-NET (single model)45.41869.82550.11273.3539.92129.324SXU-Reader (ensemble)40.29266.45146.21070.482N/AN/ASXU-Reader (single model)37.31066.12144.27070.6736.54828.116T-Reader (single model)39.42262.41444.88366.8597.34122.317BERT-base (Chinese)63.683.967.886.018.442.1BERT-base (Multi-lingual)64.184.468.686.818.643.8 human performance remains similar across the development, test, and challenge sets, indicatingthat the difficulty is consistent across all three data sets. Even though Z-Reader achieved the bestperformance on the test set, its EM metric performance was not consistent on the challenge set. Thishighlights that current models are limited in their ability to process difficult questions that requirecomplex reasoning over numerous clues throughout the passage. BERT-based methods demonstrated competitive performance compared to the submissions of par-ticipants. Traditional models have higher scores in the test set. However, the BERT-based modelsperform better on the challenge set, indicating the importance of rich representations to addresscomplex questions.",
  "Conclusion": "This paper introduces a span-extraction dataset for Chinese machine reading comprehension, con-sisting of roughly 20,000 questions annotated by human experts, along with a challenge set whichcontains questions that need reasoning over different clues in the passage. The results from theevaluation suggest that models can achieve excellent scores on the development and test sets, closeto the human performance in F1-score. However, the scores on the challenge set decline drastically,while human performance remains consistent. This shows there are still potential challenges increating models that can perform well on difficult reasoning questions. We expect that this datasetwill contribute to linguistic diversity in machine reading comprehension and facilitate additionalresearch on questions that require comprehensive reasoning across multiple clues."
}