{
  "Abstract": "Inference with modern Large Language Mod-els (LLMs) is expensive and time-consuming,and speculative sampling has proven to bean effective solution. Most speculative sam-pling methods such as EAGLE use a staticdraft tree, implicitly assuming that the accep-tance rate of draft tokens depends only on theirposition. Interestingly, we found that the ac-ceptance rate of draft tokens is also context-dependent. In this paper, building upon EA-GLE, we propose EAGLE-2, which introducesa new technique of context-aware dynamicdraft tree into drafting modeling. This improve-ment leverages the fact that the draft modelof EAGLE is well-calibrated: the confidencescores from the draft model approximate ac-ceptance rates with small errors.We con-ducted extensive evaluations on three series ofLLMs and six tasks, with EAGLE-2 achievingspeedup ratios 3.05x-4.26x, which is 20%-40%faster than EAGLE-1. EAGLE-2 also ensuresthat the distribution of the generated text re-mains unchanged, making it a lossless acceler-ation algorithm. The code is open sourced at",
  "Introduction": "Modern Large Language Models (LLMs) (OpenAI,2023; Touvron et al., 2023) exhibit impressive ca-pabilities and are widely applied across various do-mains. However, their parameter sizes have grownsubstantially, even exceeding hundreds of billions.During autoregressive generation, each token gen-eration requires accessing all model parameters. Ina single dialogue, hundreds to thousands of tokensmight be generated, making LLM inference slowand expensive. Speculative sampling (Leviathanet al., 2023; Chen et al., 2023a) methods aim to ad-dress this issue by rapidly generating draft tokens",
  "Speedup": "3.62x 2.90x 1.91x1.82x 4.26x 3.07x 2.07x1.93x 3.43x 2.78x 1.61x N/A 4.21x 3.03x 1.58x N/A 3.51x 3.01x 1.52x1.45x 3.46x 2.72x 1.50x N/A 3.29x 2.83x 1.43x1.41x EAGLE-2EAGLEMedusaLookaheadSpeculative sampling : Speedup ratios of different methods at temperature=0. For speculative sampling, the Vicuna series usesVicuna-68M as the draft model. LLaMA2-Chat 7B, 13B, and LLaMA3-Instruct 8B lack suitable draft models andare marked as N/A. LLaMA2-Chat 70B and LLaMA3-Instruct 70B use LLaMA2-Chat 7B and LLaMA3-Instruct8B as draft models, respectively. In , we present comparisons with additional methods, but this figure onlyshowcases a subset, including the fastest among these methods, EAGLE. on the acceptance rates of draft tokens in differentcontexts can yield better results.However, obtaining the acceptance rate of drafttokens requires the forward results from the origi-nal LLM, which conflicts with the goal of specula-tive sampling to reduce the number of forwards forthe original LLM. Fortunately, we find that EAGLEis well-calibrated: the confidence score (probabil-ity) of the draft model is a good approximation ofthe acceptance rate of draft tokens (see .2).This makes it feasible to use a context-dependentdynamic draft tree structure.We propose EAGLE-2, which leverages the con-fidence scores from the draft model to approximateacceptance rates. Based on this, it dynamicallyadjusts the draft tree structure, increasing the num-ber of accepted tokens. We conducted comprehen-sive and extensive tests on six tasks: multi-turnconversation, code generation, mathematical rea-soning, instruction following, summarization, andquestion answering. The datasets used were MT-bench (Zheng et al., 2023), HumanEval (Chen et al.,2021), GSM8K (Cobbe et al., 2021), Alpaca (Taoriet al., 2023), CNN/Daily Mail (Nallapati et al.,2016), and Natural Questions (Kwiatkowski et al.,2019). Our comparisons included six advancedspeculative sampling methods: standard specula-tive sampling (Leviathan et al., 2023; Chen et al.,2023a; Joao Gante, 2023), PLD (Saxena, 2023),Medusa (Cai et al., 2024), Lookahead (Fu et al.,2023), Hydra (Ankner et al., 2024), and EAGLE(Li et al., 2024b). We conducted experiments onthree series of LLMs: Vicuna, LLaMA2-Chat, andLLaMA3-Instruct. In all experiments, EAGLE-2 demonstrated the best performance, achieving a speedup of 2.5x-5x. Figures 1 and 2 show thespeedup ratios of EAGLE-2 and other speculativesampling methods on MT-bench. MT-bench is amulti-turn conversation dataset that closely resem-bles real-world scenarios for models like ChatGPTand is frequently used to evaluate state-of-the-artopen-source and closed-source models. On the MT-bench dataset, EAGLE-2 is approximately 2x fasterthan Medusa and about 2.3x faster than Lookahead,while ensuring the output distribution remains un-changed.Besides performance, EAGLE-2 offers the fol-lowing advantages: Out-of-the-box usability. Comparing to EA-GLE, EAGLE-2 does not require training anyextra models.It does not train a separatemodel to predict the draft tree structure. In-stead, it adjusts the draft tree structure basedon the confidence scores from the draft model,which is essential for speculative sampling.Therefore, EAGLE-2 requires no additionaltraining. Reliability. EAGLE-2 does not fine-tune orupdate the parameters of the original LLM,nor does it relax acceptance conditions. Thisensures that the distribution of the generatedtext remains exactly the same with that of theoriginal LLM, provably.",
  "and then verifying them in parallel. These methodsgenerate multiple tokens in a single forward pass,significantly reducing inference latency.Standard speculative sampling (Leviathan et al.,": "2023; Chen et al., 2023a) uses a chain-structureddraft. To improve acceptance length, recent work inspeculative sampling has employed tree-structureddrafts. Sequoia (Chen et al., 2024) explicitly as-sumes that the acceptance rate of a draft token de-pends only on its position in the tree. EAGLE (Liet al., 2024b) and Medusa (Cai et al., 2024) use thesame static draft tree structure in all contexts: at thei-th step of the draft phase, k candidates are added,with k being fixed. This implicitly assumes theaforementioned hypothesis. However, this assump-tion appears to contradict the insight of speculativesampling that some tokens are simpler and canbe predicted by smaller models. Our experiments(see .1) reveal that the acceptance rate ofdraft tokens is not only position-dependent but alsohighly context-dependent. Therefore, the staticstructure of draft trees has inherent limitations. Dy-namically adjusting the draft tree structure based Vicuna 7B Vicuna 13B LLaMA2-Chat 7B LLaMA2-Chat 13B LLaMA2-Chat 70B LLaMA3-Instruct 8B LLaMA3-Instruct 70B",
  "(b) Verification stage": ": Comparison of standard speculative sampling and EAGLE. For simplicity, EAGLEs tree-structured draftis shown only in the verification stage, while the illustration of the drafting stage uses a chain-structured draft. Here,ti denotes the i-th token embedding, and fi denotes the i-th feature vector in the second-to-top-layer of LLM beforeLM head. is to first draft and then verify: quickly generatea potentially correct draft and then check whichtokens in the draft can be accepted. We use ti todenote the i-th token and Ta:b to represent the tokensequence ta, ta+1, , tb. Speculative samplingalternates between drafting and verification stages.Consider a prefix T1:j, in the drafting stage,speculative sampling invokes a draft model (asmaller LLM than original LLM) to autoregres-sively generate a draft Tj+1:j+k with T1:j as theprefix, while also recording the probability p foreach token. In the verification stage, speculativesampling calls the original LLM to check the draftTj+1:j+k and record its probability p. Then, specu-lative sampling determines the acceptance of drafttokens sequentially from front to back. For to-ken tj+i, the probability of it being accepted ismin(1, pj+i(tj+i)/pj+i(tj+i)). If the token is ac-cepted, it proceeds to check the next one. Oth-erwise, it samples a token from the distributionnorm(max(0, pj+i pj+i)) to replace tj+i and dis-cards the remaining tokens in the draft. AppendixA.1 of (Leviathan et al., 2023) proves that specula-tive sampling is consistent with the distribution ofvanilla autoregressive decoding. Both EAGLE andEAGLE-2 apply this framework.",
  "EAGLE": "EAGLE (Li et al., 2024b) is an improvement overspeculative sampling. At the submission of thiswork, EAGLE ranks first in the Spec-Bench (Xiaet al., 2024), a comprehensive benchmark designedfor assessing speculative decoding methods acrossdiverse scenarios.Drafting Stage. Unlike standard speculativesampling, which autoregressively predicts tokensequences, EAGLE performs autoregression at themore structured feature (before LM head) level andthen uses the LM Head of original LLM to obtainthe draft tokens. The sampling process introducesuncertainty in the feature sequence. To address this,EAGLE also inputs a token sequence advanced byone time step into the draft model, as shown in",
  "=+": ": Differences between EAGLE and EAGLE-2. EAGLE always uses a fixed draft shape. Whenthe query is 10+2=, the next token is very likely tobe correctly predicted as 1. However, with a staticdraft tree, EAGLE would still add two candidates, eventhough the probability of the other candidate 3 beingcorrect is very low. EAGLE-2, on the other hand, adjuststhe shape of draft tree based on the context. When thequery is 10+2, the next token is difficult to predict, soEAGLE-2 adds two candidates. For the simpler query10+2=, EAGLE-2 adds only one candidate 1. a.Verification Stage. In standard speculative sam-pling, the draft is chain-structured, requiring thediscarding of all subsequent tokens if a draft tokenis rejected. EAGLE uses a tree-structured draft,allowing alternative branches to be attempted if adraft token is rejected. b illustrates thedifferences between the two.Differences between EAGLE and EAGLE-2.The shape of EAGLEs draft tree is fixed, with thedrafting phase filling in the corresponding positions.EAGLE-2 aims to improve this by introducing a dy-namically adjustable draft tree. illustratesthe difference between EAGLE and EAGLE-2 witha simple example.",
  "(b) Acceptance rates of to-kens at different positions,with each point representinga query": ": Acceptance rates of draft tokens at differentpositions. In the left figure, P1-P6 indicate positionsin the token tree, corresponding to positions 1-6 on thehorizontal axis in the right figure. The right figure showsthe acceptance rates of draft tokens at positions P1-P6. . Overall, the acceptance rate of draft to-kens is position-dependent, with the highest accep-tance rate at position P1 and the lowest at positionP6. Draft tokens in the upper left side of the drafttree (such as position P1) have higher acceptancerates, while those in the lower right side (such asposition P6) have lower acceptance rates. Thissupports the rationale for having more nodes in theupper left and fewer in the lower right in static drafttrees used by methods like EAGLE and Medusa.However, we also observed significant variance inacceptance rates at the same position, indicatingthat the probability of a draft token being accepteddepends not only on its position but also on the con-text. This suggests that a context-aware dynamicdraft tree has greater potential than a static drafttree.",
  "Well-Calibrated Draft Model": "To apply a dynamic draft tree, we need a low-costmethod to estimate the acceptance rates of drafttokens without invoking the original LLM. We con-ducted experiments on the Alpaca dataset to ex-plore the relationship between the draft modelsconfidence score (the output probability of LLMw.r.t. each token) and the acceptance rate. Asshown in , there is a strong positive corre-lation between the draft models confidence scoreand the acceptance rate of the token. Draft to-kens with confidence score below 0.05 have anacceptance rate of approximately 0.04, while thosewith confidence score above 0.95 have an accep-tance rate of about 0.98. Therefore, we can use thedraft models confidence score to estimate accep-tance rates without additional overhead, enablingdynamic adjustments to the draft tree. Similar phe-nomena are observed with draft models in other 0.00.20.40.60.81.0",
  "Context-Aware Dynamic Draft Tree": "Building on the aforementioned observations, weintroduce EAGLE-2, an acceleration algorithm forLLM inference that dynamically adjusts the drafttree. EAGLE-2 does not alter the training and in-ference of the draft model, nor does it affect theverification stage. Its improvements focus on twoaspects: how to expand the draft tree (.1)and how to rerank draft tokens (.2). Dur-ing the expansion phase, we input the most promis-ing nodes from the latest layer of the draft tree intothe draft model to form the next layer. During thereranking phase, we select the tokens with higheracceptance probabilities to form the input for theoriginal LLM during the verification phase.In the draft tree, a node represents a token. Inthe following text, we use node and token in-terchangeably.",
  "Expansion Phase": "Thanks to tree attention, the draft model can simul-taneously input all tokens from the current layerand compute the probabilities for the next tokens ina single forward pass, thereby expanding all tokensin the current layer. However, inputting too manytokens at once can slow down the draft models for-ward pass, and the number of tokens in each layerof the draft tree grows exponentially. Therefore,we need to selectively expand the draft tree.We choose the top-k tokens with the highestglobal acceptance probabilities from the currentlayer for expansion. In speculative sampling, reject-ing a draft token leads to discarding all subsequenttokens; a token is ultimately accepted only if all its",
  "tjPath(root,ti)cj,": "where Path (root, ti) represents the path from theroot node to the node ti in the draft tree, pj rep-resents the acceptance rate of the node tj, and cjrepresents the confidence score of tj from the draftmodel. Experiments in .2 show that confi-dence score is strongly positively correlated withacceptance rate. We leverage this relationship toapproximate the value.Branches starting from tokens with higher valuesare more likely to be accepted. Therefore, we selectthe top-k nodes with the highest values in the lastlayer as the input to the draft model and expand thedraft tree based on the output. The top of illustrates the expansion phase.",
  "Reranking Phase": "The purpose of the expansion phase is to deepenthe draft tree. Since acceptance rates range be-tween 0 and 1, the value of a deeper token is lower.Some shallow nodes that were not expanded mayhave higher values than the deeper expanded nodes.Therefore, we do not use the tokens selected duringthe expansion phase as the draft directly. Instead,we rerank all draft tokens and select the top m to-kens with the highest values. The value of a nodeis always less than or equal to that of its parentnode. For nodes with the same value, we prioritizeselecting shallower nodes. This ensures that thetop m tokens selected after reranking still form aconnected tree.Afterwards, we flatten the selected tokens intoa one-dimensional sequence to serve as the inputfor the verification phase. To ensure consistencywith vanilla autoregressive decoding, we also needto adjust the attention mask. In vanilla autoregres-sive decoding, each token can see all precedingtokens, resulting in a lower triangular attention ma-trix. When using a draft tree, tokens from differentbranches should not be visible to each other. There-fore, the attention mask must be adjusted accordingto the tree structure to ensure that each token canonly see its ancestor nodes. The bottom of illustrates the reranking Phase.",
  "Itishasathetogoodbe": ": Illustration of EAGLE-2. The numbers besidethe edges represent the confidence scores of the draftmodel, and the numbers in brackets within the blocksrepresent the value of the nodes. During the expan-sion phase, we select the top 2 nodes with the highestvalue from the current layer (orange blocks) as inputsto the draft model and connect the generated tokens(green blocks) to the draft tree. In the rerank phase, weselect the top 8 nodes with the highest value from allnodes (blue blocks), flatten them into a 1-dimensionalsequence to form the final draft. We then construct theattention mask according to the tree structure, ensuringeach token can only see its ancestor nodes. Models. We conduct experiments on Vicuna 7B,13B (Chiang et al., 2023), LLaMA2-Chat 7B, 13B,70B (Touvron et al., 2023), and LLaMA3-Instruct8B, 70B models (Meta, 2024).Tasks. We conduct comprehensive evaluationson six generation tasks. For multi-turn conversa-tion, code generation, mathematical reasoning, in-struction following, summarization, and questionanswering tasks, we chose the MT-bench (Zhenget al., 2023), HumanEval (Chen et al., 2021),GSM8K (Cobbe et al., 2021), Alpaca (Taori et al.,2023), CNN/Daily Mail (Nallapati et al., 2016),and Natural Questions (Kwiatkowski et al., 2019)datasets, respectively.Metrics. EAGLE-2 neither fine-tunes the origi-nal LLM nor relaxes acceptance conditions, mak-ing it a lossless acceleration method. Therefore, wedo not evaluate the generation quality and insteaduse the following metrics to assess accelerationperformance:",
  "Speedup Ratio: The actual test speedup ratiorelative to vanilla autoregressive decoding": "Average Acceptance Length : The aver-age number of tokens generated per drafting-verification cycle, which corresponds to thenumber of tokens accepted from the draft. Theadvantage of average acceptance length is thatit is independent of hardware and runtimeenvironment, while its disadvantage is thatit does not reflect the overhead of the draftmodel. Why is acceptance rate not included? Theacceptance rate only reflects the performance ofthe draft model. Since EAGLE-2 does not modifythe structure of the draft model, the acceptance rateremains the same as that of EAGLE.Comparison. We use vanilla autoregressive de-coding as the baseline, which serves as the bench-mark for speedup ratios (1.00x).We compareEAGLE-2 with recent lossless speculative sam-pling methods, including standard speculative sam-pling (Leviathan et al., 2023; Chen et al., 2023a;Joao Gante, 2023), PLD (Saxena, 2023), Medusa(Cai et al., 2024), Lookahead (Fu et al., 2023), Hy-dra (Ankner et al., 2024), and EAGLE (Li et al.,2024b). The speedup ratio is hardware-dependent,so we tested different methods on the same devicesto ensure fairness. Our comparative experimentsutilized Spec-Bench (Xia et al., 2024). The imple-mentation details of these methods and EAGLE",
  "Effectiveness": "Figures 1 and 2, along with Tables 1 and 2, presentthe speedup ratios of different methods. Across alldatasets and LLMs we tested, EAGLE-2 achievedthe highest speedup ratios. Most speculative sam-pling methods exhibit the highest speedup on thecode generation task (HumanEval), benefiting fromthe extensive use of fixed templates in code. EA-GLE achieved a speedup of up to 5x on code gener-ation tasks. PLD achieved the highest speedup ra-tio on summarization tasks (CNN/DM) when usingVicuna as the original LLM, due to PLDs retrieval-based draft generation and the high overlap in con-text when Vicuna performs summarization. Stan-dard speculative sampling, using Vicuna-68M asthe draft model, also achieved significant speedupsbut had much higher training overhead comparedto other methods. PLD and Lookahead do not re-quire training, while Medusa, Hydra, EAGLE, andEAGLE-2 use SFT datasets for training their draftmodels. Vicuna-68M used both pre-training andSFT datasets, with the pre-training dataset beingmuch larger than the SFT dataset.Tables 1 and 2 show the average acceptancelengths for different methods, which is a hardware-independent metric. Across all datasets and LLMswe tested, EAGLE-2 achieved the longest averageacceptance length. Each drafting-verification cycleof EAGLE-2 generates approximately 4-5.5 tokens,significantly higher than other methods, roughlytwice that of standard speculative sampling andMedusa. PLD and Lookahead have shorter aver-age acceptance lengths, but since they either lacka draft model or their draft model is not a neuralnetwork, the overhead during the drafting phase isvery low, resulting in a speedup ratio very close totheir average acceptance length.Medusa, Hydra, EAGLE, and EAGLE-2 havelower average acceptance lengths on QA (NaturalQuestions) and summarization (CNN/DM) taskscompared to other tasks, whereas standard specu-lative sampling does not show this reduction. Thesame pattern is observed for the speedup ratios.This discrepancy may be attributed to differencesin the training data for the draft models.Thedraft model for standard speculative sampling usesboth pretraining and SFT datasets, while Medusa,Hydra, EAGLE, and EAGLE-2 only use the SFTdataset. Natural Questions involves questions aboutworld knowledge, such as Where was the 2015 : Speedup ratios and average acceptance lengths of different methods. V represents Vicuna, L2 representsLLaMA2-Chat. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methodslike Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration.Therefore, we do not compare EAGLE-2 with these methods.",
  "Value and Confidence Score": "EAGLEs draft model provides a good approxima-tion of acceptance rates, but it is local and cannotreflect the actual probability of a draft token beingaccepted. Therefore, when selecting nodes for ex-pansion, we use the value, which is the product of a draft tokens confidence score and its ancestornodes confidence scores, as the basis for rank-ing. In this section, we compare the performanceimpact of expanding based on value versus con-fidence score. The experimental results in show that the speedup ratio and average accep-tance length are both higher when expanding basedon value, demonstrating the rationale behind theEAGLE-2 approach.",
  "Reranking": "The purpose of EAGLE-2s expansion phase is todeepen the draft tree, but the tokens selected maybe globally less optimal than shallow nodes thatwere not selected. Therefore, during the rerankingphase, we rerank all the draft tokens. We conductedan ablation study on this operation using the MT-bench and GSM8K dataset. As shown in ,",
  "Related Work": "With widespread applications of LLMs, there hasbeen significant work (Liu et al., 2023b) focused onaccelerating LLM inference, such as low-bit quanti-zation (Hubara et al., 2018; Shen et al., 2020; Kimet al., 2021; Zadeh et al., 2020; Zafrir et al., 2019),pruning (Gale et al., 2019; Sanh et al., 2020), andknowledge distillation (Hinton et al., 2015). Thesemethods reduce generation latency by decreasingthe computational cost of each forward pass of theLLM. However, these approaches often degradeLLM performance to some extent, resulting in atrade-off between generation quality and computa-tional overhead.Speculative sampling methods achieve losslessacceleration by using the original LLM for verifi-cation. Early speculative decoding methods (Sternet al., 2018; Sun et al., 2021) accelerated generationin greedy settings, while Leviathan et al. (2023);Chen et al. (2023a) proposed speculative sampling to extend the draft-verification framework to non-greedy generation. Subsequent work has largelyfocused on reducing draft overhead and enhanc-ing consistency between the draft and the origi-nal LLM. SpecInfer (Miao et al., 2023) integratesmultiple small models as the draft model, aggre-gating their drafts into a tree and using tree atten-tion for parallel verification. Medusa (Cai et al.,2024) trains a set of MLPs to parallelly predictmultiple tokens using the original LLMs features,significantly reducing the latency during the draft-ing phase. EAGLE (Li et al., 2024b) autoregres-sively predicts feature sequences instead of tokensequences and inputs the sampling results into thedraft model to address uncertainty at the featurelevel, substantially improving the draft models ac-curacy. This principle of eliminating uncertainty isalso used in Hydra (Ankner et al., 2024) and Recur-rent Drafter (Zhang et al., 2024). Parallel Decoding(Santilli et al., 2023), Lookahead (Fu et al., 2023),Ouroboros (Zhao et al., 2024), and CLLMs (Kouet al., 2024) generate drafts using Jacobi iterations.Methods (Hooper et al., 2023; Yang et al., 2023b;Monea et al., 2023; Li et al., 2024a; Yi et al., 2024;Liu et al., 2024; Sun et al., 2024a; Elhoushi et al.,2024; Svirschevski et al., 2024) like Draft & Ver-ify (Zhang et al., 2023) utilize techniques such aslayer skipping or early exit, using parts of the orig-inal LLMs parameters as the draft model. REST(Fu et al., 2024) and LLMA (Yang et al., 2023a)generate drafts through retrieval. Online Specula-tive Decoding (Liu et al., 2023a) and DistillSpec(Zhou et al., 2024) further align the draft modelwith the original LLM through additional training.Cascade Speculative Drafting (Chen et al., 2023b)and Staged Speculative Decoding (Spector and Re,2023) cascade draft models of different sizes.Speculative sampling methods can achieve loss-less acceleration, but they can also trade off qual-ity for higher speedup ratios. For example, BiLD(Kim et al., 2024) relaxes the acceptance condi-tions, while Medusa-2 (Cai et al., 2024), CLLMs(Kou et al., 2024), and SPACE (Yi et al., 2024)fine-tune the original LLMs.Sorted Llama (Kavehzadeh et al., 2024) andLITE (Varshney et al., 2023) use confidence asan indicator of token quality. Some works havealready employed partially dynamic draft trees byleveraging confidence. BiLD (Kim et al., 2024)and Kangaroo (Liu et al., 2024) use early stoppingbased on the draft models confidence to control thetrees depth. GLIDE and CAPE (Du et al., 2024) adds additional candidates when the top-1 tokenconfidence is low, controlling the trees depth, butthe additional candidates are not further expanded,resulting in a structurally limited tree. In contrast,EAGLE-2 has no such limitations and can dynami-cally adjust the draft tree structure flexibly, leadingto better performance.",
  "Conclusion": "In this paper, we introduce EAGLE-2, an efficientand lossless speculative sampling method.Wefound that EAGLEs draft model confidence isa good approximation of the acceptance rate fordraft tokens. Based on this, EAGLE-2 employs acontext-dependent draft tree structure, significantlyincreasing the number of accepted draft tokens andresulting in better speedup ratios. EAGLE-2 en-sures that the generated results are consistent withthe original LLMs and does not require additionaltraining. We conducted extensive evaluations usingvarious LLMs across multiple datasets and com-pared EAGLE-2 with several state-of-the-art spec-ulative sampling methods. In all our experiments,EAGLE-2 achieved the highest speedup ratios.",
  "Limitations": "The limitations of EAGLE-2 are similar to those ofEAGLE, as it requires training draft models. Fortraining the draft model, we used the SFT datasetShareGPT. Training the draft model also requirescertain computational resources; training a draftmodel for a 70B original LLM requires 4 A100(40G) GPUs for one to two days. However, inlarge-scale deployment and application, such train-ing costs are negligible. Additionally, EAGLE-2uses the confidence scores from the draft model toapproximate acceptance rates, which requires thedraft model to be well-calibrated.",
  "Acknowledgements": "Yuhui Li and Chao Zhang are supported by theNational Nature Science Foundation of China un-der Grant 62071013 and National Key R&D Pro-gram of China under Grant 2018AAA0100300.Hongyang Zhang is supported by the NSERC Dis-covery Grant RGPIN-2022-03215, DGECR-2022-00357 and the Compute Canada. Zachary Ankner, Rishab Parthasarathy, AniruddhaNrusimha, Christopher Rinard, Jonathan Ragan-Kelley, and William Brandon. 2024.Hydra:Sequentially-dependent draft heads for medusa de-coding. arXiv preprint arXiv:2402.05109. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,Jason D. Lee, Deming Chen, and Tri Dao. 2024.Medusa: Simple llm inference acceleration frame-work with multiple decoding heads. arXiv preprintarXiv: 2401.10774. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. 2023a. Accelerating large language modeldecoding with speculative sampling. arXiv preprintarXiv:2302.01318. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374. Zhuoming Chen, Avner May, Ruslan Svirschevski,Yuhsun Huang, Max Ryabinin, Zhihao Jia, andBeidi Chen. 2024. Sequoia: Scalable, robust, andhardware-aware speculative decoding. arXiv preprintarXiv:2402.12374.",
  "Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,Jie Huang, and Kevin Chen-Chuan Chang. 2023b.Cascade speculative drafting for even faster llm infer-ence. arXiv preprint arXiv:2312.11462": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu,Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, LiqiangNie, Zhaopeng Tu, et al. 2024. Glide with a cape: Alow-hassle method to accelerate speculative decoding.arXiv preprint arXiv:2402.02082. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,Basil Hosmer, Bram Wasti, Liangzhen Lai, AnasMahmoud, Bilge Acun, Saurabh Agarwal, AhmedRoman, et al. 2024.Layer skip: Enabling earlyexit inference and self-speculative decoding. arXivpreprint arXiv:2404.16710.",
  "Ziteng Sun, Jae Hun Ro, Ahmad Beirami, andAnanda Theertha Suresh. 2024b.Optimal block-level draft verification for accelerating speculativedecoding. arXiv preprint arXiv:2403.10444": "Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-mad Beirami, Himanshu Jain, and Felix Yu. 2024c.Spectr: Fast speculative decoding via optimal trans-port. Advances in Neural Information ProcessingSystems, 36. Ruslan Svirschevski, Avner May, Zhuoming Chen,Beidi Chen, Zhihao Jia, and Max Ryabinin. 2024.Specexec: Massively parallel speculative decodingfor interactive llm inference on consumer devices.arXiv preprint arXiv:2406.02532.",
  "Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, andChitta Baral. 2023. Accelerating llm inference byenabling intermediate layer decoding. arXiv preprintarXiv:2310.18581": "Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhi-fang Sui. 2024. Unlocking efficiency in large lan-guage model inference: A comprehensive survey ofspeculative decoding. Preprint, arXiv:2401.07851. Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, DaxinJiang, Linjun Yang, Rangan Majumder, and FuruWei. 2023a. Inference with reference: Lossless ac-celeration of large language models. arXiv preprintarXiv:2304.04487. Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dim-itris Papailiopoulos, and Kangwook Lee. 2023b.Predictive pipelined decoding: A compute-latencytrade-off for exact llm decoding.arXiv preprintarXiv:2307.05908. Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xi-aotian Yu, and Rong Xiao. 2024. Generation meetsverification: Accelerating large language model infer-ence with smart parallel auto-correct decoding. arXivpreprint arXiv:2402.11809. Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,and Andreas Moshovos. 2020.Gobo: Quantiz-ing attention-based nlp models for low latency andenergy efficient inference.In 2020 53rd AnnualIEEE/ACM International Symposium on Microarchi-tecture (MICRO), pages 811824. IEEE. Ofir Zafrir, Guy Boudoukh, Peter Izsak, and MosheWasserblat. 2019. Q8bert: Quantized 8bit bert. In2019 Fifth Workshop on Energy Efficient MachineLearning and Cognitive Computing-NeurIPS Edition(EMC2-NIPS), pages 3639. IEEE.",
  "Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao,Zhiyuan Liu, and Maosong Sun. 2024. Ouroboros:Speculative decoding with large model enhanceddrafting. arXiv preprint arXiv:2402.13720": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.Judging llm-as-a-judge with mt-bench and chatbotarena. arXiv preprint arXiv:2306.05685. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,Aditya Krishna Menon, Afshin Rostamizadeh, SanjivKumar, Jean-Franois Kagy, and Rishabh Agarwal.2024. Distillspec: Improving speculative decodingvia knowledge distillation. In The Twelfth Interna-tional Conference on Learning Representations.",
  "AImplementation Details": "Vanilla:We use models from the Hugging-face.transformers library with the PyTorch backendand pre-allocated KV cache. Other methods alsouse these models as their base.(Standard) Speculative Sampling: We use theassisted generation feature from the HuggingFaceTransformers library.PLD, Lookahead, Medusa, and Hydra: Weuse the default settings and the officially releasedweights.EAGLE: Vicuna and LLaMA2-Chat draft mod-els use the officially released weights, whileLLaMA3-Instruct is trained using the ShareGPTdataset (consistent with Medusa and Hydra).EAGLE-2: For the 7B (8B), 13B, and 70B orig-inal LLMs, we set the total number of draft tokensto 60, 50, and 48, respectively, with a draft treedepth of 6, and select 10 nodes during the expan-sion phase."
}