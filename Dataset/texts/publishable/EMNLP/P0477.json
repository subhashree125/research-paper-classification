{
  "Abstract": "Recent advances in image tokenizers, such asVQ-VAE, have enabled text-to-image genera-tion using auto-regressive methods, similar tolanguage modeling. However, these methodshave yet to leverage pre-trained language mod-els, despite their adaptability to various down-stream tasks. In this work, we explore this gapby adapting a pre-trained language model forauto-regressive text-to-image generation, andfind that pre-trained language models offer lim-ited help. We provide a two-fold explanationby analyzing tokens from each modality. First,we demonstrate that image tokens possess sig-nificantly different semantics compared to texttokens, rendering pre-trained language modelsno more effective in modeling them than ran-domly initialized ones. Second, the text tokensin the image-text datasets are too simple com-pared to normal language model pre-trainingdata, which causes the catastrophic degradationof language models capability.",
  "Introduction": "Recent works in text-to-image generation primar-ily employ two kinds of methods: diffusion mod-els (Ramesh et al., 2022; Saharia et al., 2022;Rombach et al., 2022) and auto-regressive mod-els (Ramesh et al., 2021; Yu et al., 2022b). Thelatter is facilitated by image tokenizers, such asVQ-VAE (van den Oord et al., 2017; Razavi et al.,2019) and VQ-GAN (Esser et al., 2021; Yu et al.,2022a), which transform an image into a sequenceof discrete tokens, similar to text tokens (Left). Consequently, image and text tokens can bejointly modeled using auto-regressive algorithmslike the Transformer (Vaswani et al., 2017) (Fig-ure 2 Right).The superiority of diffusion-based models whencompared with auto-regressive-based methods fortext-to-image generation still remains unclear. Ope-nAIs pioneering work, DALL-E (Ramesh et al.,",
  "2022 2023": ": Auto-regressive and diffusion based modelsachieve similar performances on text-to-image gener-ation. However, while all the diffusion models lever-age pre-trained language models, all the auto-regressivemodels do not. 2021), showcased the potential of auto-regressivemethods in this domain.Yet, its successor,DALL-E 2 (Ramesh et al., 2022), transitioned toa diffusion-based architecture and achieved en-hanced image generation quality. Later, Googlereleased Imagen (Saharia et al., 2022) (diffusion-based) and Parti (Yu et al., 2022b) (auto-regressive-based) at the same time and demonstrated theircomparable generation quality.Similarly, theretrieval-augmented methods, Re-Imagen (Chenet al., 2022) (diffusion-based) and CM3leon (Yuet al., 2023b) (auto-regressive-based), display simi-lar performance in text-to-image generation tasks.A comparison based on zero-shot FID (Heuselet al., 2017) on the COCO dataset (Lin et al., 2014)can be found in .While these two approaches achieve similar per-formance, it is intriguing that diffusion-based mod-els consistently utilize pre-trained text encoders,whereas their auto-regressive counterparts gener-ally do not. For instance, Imagen (Saharia et al.,2022) (diffusion-based) reports that employinga stronger pre-trained text encoder, specificallyT5 (Raffel et al., 2020), yields substantial improve-ments to using CLIP (Radford et al., 2021). Fur-thermore, they observe that scaling up the T5 textencoder leads to more pronounced improvements : Adapting language models for auto-regressive text-to-image generation. (Left) An image is fed intoan image tokenizer (MoVQGAN (Zheng et al., 2022)) and converted to a grid of discrete tokens, and it can bewell-reconstructed with these image tokens. (Right) As images are converted to tokens similar to text tokens, wecan enable language models to generate images by adapting its embedding layer and output layer. than scaling up the diffusion models. Conversely,Parti (Yu et al., 2022b) (auto-regressive-based)shows that using a pre-trained text encoder does notnecessarily improve image quality in its Appendix.However, Parti employs an encoder-decoder archi-tecture and uses BERT (Devlin et al., 2019), a rela-tively inferior text encoder, to initialize the encoderonly. It remains unclear whether a decoder-only ap-proach would benefit from recent advances in largelanguage models (LLMs), given the clear similaritybetween language modeling and auto-regressivetext-to-image generation.In this work, we explore the potential of pre-trained LLMs for auto-regressive text-to-imagegeneration. To enable the model to process bothtext and image tokens, we expand the size of theembedding and output layers by incorporating animage vocabulary from the image tokenizer. Weinitialize these added weights either randomly or us-ing a novel contrastive alignment (elaborated laterin .2), while the remaining weights aredirectly copied from the original models. Subse-quently, we fine-tune the model on image-captiondatasets, as depicted in Right.Surprisingly, the results show that pre-trainedlanguage models achieve the same loss and im-age generation quality as the model that is entirelyrandomly initialized and trained from scratch (Fig-ure 3). Furthermore, we observe a catastrophicdeterioration in the models text capabilities, suchas world knowledge or in-context learning, afteronly minimal steps of fine-tuning ().To understand this phenomenon, we break downthe cross-entropy loss on image and text tokens,and find that 1) the loss on image tokens is thesame between the pre-trained and randomly ini-tialized model, and 2) the loss on text tokens ofthe pre-trained model is significantly lower at thebeginning compared to the randomly initializedmodels, but the gap soon disappears after training().The first finding of the loss on the image tokensis particularly interesting. We hypothesize that im- age tokens obtained from image tokenizers mighteither lack semantics or possess significantly dif-ferent semantics compared to text tokens, whichrenders language pre-training not transferable tothe image modeling task. To verify this hypoth-esis, we conduct unconditional image generationexperiments by training the model on image to-kens only. Our results show that 1) the pre-trainedmodel achieves the same loss as the randomly ini-tialized model, and 2) freezing any part of the pre-trained model results in a loss degradation (Fig-ure 6). These indicate that optimal weights forlanguage and image modeling are fundamentallydifferent, making language pre-training not trans-ferable to image modeling.In summary, we share our experimental findingsabout pre-trained language models do not help auto-regressive text-to-image generation, and offer anexplanation: 1) the intrinsic differences betweenimage and text tokens make language pre-trainingineffective for the image token modeling, and 2)the disproportionate ratio between image and texttokens (usually 30:1 for image-caption datasets)minimizes the impact of loss on text tokens andleads to catastrophic forgetting.",
  "Experimental Setup": "Language model.We use the publicly availableopen_lm codebase and its open_lm-1b model forour experiments (Gururangan et al., 2023). Thislanguage model contains 1B parameters andis trained on 1.6T tokens on a mix of RedPa-jama (Computer, 2023), Pile (Gao et al., 2020),S2ORC (Lo et al., 2020), The Pile of Law (Hen-derson et al., 2022), Deepmind Math (Saxton et al.,2019), and RealNews (Zellers et al., 2019b). Itachieves better or comparable performance com-pared to models with similar size such as OPT-1.3B (Zhang et al., 2022), Pythia-1B (Bidermanet al., 2023), Neox-1.3B (Black et al., 2022), OPT- : Pre-trained language models do not help auto-regressive text-to-image generation. Models are trainedon the HQITP-134M image-caption dataset with 64 A100 80GB GPUs using batch size 1M tokens. EMA isExponential Moving Average. IML-1.3B (Iyer et al., 2022) on an average of 11tasks such as HellaSwag (Zellers et al., 2019a) andMMLU (Hendrycks et al., 2021). More details canbe found in the open_lm repository (Gururanganet al., 2023). Imagetokenizer.WeuseSBER-MoVQGAN (Zheng et al., 2022) as the imagetokenizer, which is the current state-of-the-artpublicly available image tokenizer that achieves0.686 FID on Imagenet image reconstruction.Given an image with 256 256 resolution,it converts an image to 1,024 tokens with avocabulary size of 16,384. (Left) shows areal reconstruction example from this tokenizer. Dataset.For multi-modal training, we use an in-ternal dataset referred to as High Quality Image-Text Pairs (HQITP) (Ranasinghe et al., 2023a),which contains 134M high-quality image-captionpairs. The primary sources of image-caption pairsin HQITP are from the web, similar to the com-monly used image-caption datasets such as Con-ceptual Captions (CC) (Changpinyo et al., 2021).We chose HQITP because it is larger, has higherquality, and includes a broader range of conceptsand objects, thus validating our conclusions on alarger scale. Previous works leveraging HQITPhave shown that conclusions transfer well betweenHQITP and CC (Ranasinghe et al., 2023b).We pre-process the dataset before training. Eachimage is center-cropped to 256 256 and con-verted to 1,024 tokens. Each caption is tokenizedwith NeoX tokenizer with an average of 30 tokens.We add six special tokens corresponding to the be-ginning and end of document, text segment, andimage, respectively. This results in input sequencesof the form <doc> <text> ...text tokens... </text>",
  "<image> ...image tokens... </image> </doc>, andpad them into 1,152 tokens with the special <pad>token": "Training setups.Models are trained with 100Btokens using 64 A100 80GB GPUs with batch size1M tokens. We use the AdamW (Loshchilov andHutter, 2019) optimizer with a cosine learning rateschedule with 2K warm-up steps and a peak learn-ing rate of 0.0003. This mimics the settings re-ported in (Aghajanyan et al., 2023). We also trieddifferent hyperparameters, such as learning ratesfrom 0.00005 to 0.0003 and batch size from 0.5Mto 2M tokens, and found no significant influenceson the conclusions.",
  "Results": "In , we present the perplexity (exponentialof loss) during training for both the pre-trained andrandomly initialized models. Intriguingly, acrossthe entire 100B token training regimen, the loss ofthe pre-trained model aligns closely with that of therandomly initialized one. Beyond this, a sharp de-cline in text capabilities of the pre-trained model isobserved after training on 5B tokens, as illustratedin . At this point, both the models worldknowledge and its in-context learning ability areentirely diminished.To delve deeper into this phenomenon, we sep-arate the cross-entropy loss into two components:text tokens and image tokens, displayed separatelyin . As anticipated, the pre-trained modelbegins with a significantly lower text loss in com-parison to its randomly initialized counterpart. Yet,due to the overwhelming image-text token ratio(30:1), this initial advantage is obscured in theaggregate loss. Furthermore, any benefit the pre-trained model offers in text loss diminishes soon",
  "plushgirafe=>girafepeluchecheese => fromagecheese => I love cheese": ": Concrete examples of forgetting. We observe asevere deterioration of the models language capability,such as knowledge and in-context learning, after a smallamount of training. Model completions are bolded. during training. In contrast, for image tokens, thereis no difference between the pre-trained and ran-domly initialized models. We hypothesize that theinability of effectively transferring a pre-trainedlanguage model to image token modeling is causedby the distinction between image and text tokens.Moreover, loss on text tokens is substantiallylower than image tokens, and even lower than typi-cal language models trained on text-only data. Thisis because texts in image-caption datasets such asHQITP are less complex than those in standardtext-only pre-training corpora, which also explainsthe catastrophic degradation of the models textcapability.We use perplexity as our main evaluation met-ric for its ability to provide finer-grained insightsinto training dynamics, which is essential for ourconclusion that pre-trained language models do notenhance auto-regressive text-to-image generation.Unlike time-consuming metrics like FID (FrchetInception Distance) (Heusel et al., 2017), perplex-ity is computationally inexpensive and allows usto compare models at nearly every training step.",
  ": Examples of generated images. We achieve12.21 FID on MS-COCO at the end of training": "Our results show that perplexity on image tokens isnearly identical for both pre-trained and randomlyinitialized models, supporting our claim. Addi-tionally, FID scores at the end of training on MS-COCO further validate this, with both models show-ing nearly identical performance (12.21 for pre-trained language models vs. 12.27 for randomly-initialized language models), demonstrating thatpre-training offers no significant advantage in thissetting. FID scores are slightly below DALL-E2 (Ramesh et al., 2022), due to training on only100B tokens; continued training enhances quality.We provide some generation examples in .",
  "Image Tokens Are Drastically DifferentFrom Text Tokens": "Why there is no difference between the loss of pre-trained and randomly initialized models on the im-age tokens? We hypothesize image tokens are sig-nificantly different from text tokens, for example,they lack semantics or have drastically differentsemantics compared to text tokens, which makesthe pre-trained language model not transferable to : Pre-trained language models do not help tomodel image tokens. Models are trained only on theHQITP datasets image tokens without any text tokens.We also compare the full fine-tuning with electively fine-tuning components of the pre-trained models (shown inparenthesis). EMA 0.95 is applied to the plot.",
  "Unconditional Image Generation": "To assess if pre-trained language models benefitimage tokens, we perform unconditional imagegeneration experiments. Unlike the text-to-imagegeneration setup, we removed all text tokens, leav-ing only the image tokens. This approach rigor-ously examines if image tokens benefit from pre-trained language models. As shown in ,pre-trained language models yield the same loss asmodels initialized randomly.Additionally, we selectively tune components ofthe pre-trained models: 1) only the embedding andoutput layer; 2) 1 plus layer norm and positionalembedding; and 3) 2 plus the first half of layers;4) 2 plus the feed-forward layers (FFN). presents these loss metrics. The findings revealthat none of these configurations achieves as low aloss as a fully tunable model. This underscores thedivergence in optimal weights for modeling textand image tokens, suggesting that any part of thetext-trained weights is sub-optimal to transfer toimage tokens.",
  "Image-Text Token Contrastive Alignment": "To understand whether image tokens have similarsemantics as text tokens, we aligned image tokenswith text tokens using a contrastive approach, in-spired by methods like CLIP (Radford et al., 2021).Given an image, we tokenize it into 1024 tokensand compute its bag-of-words image embeddingsas its representation. Similarly, we tokenize the cor-responding caption and compute its bag-of-wordstext embeddings. The text embeddings are initial- : Image-text token contrastive alignment. (Top)The contrastive loss plateaus quickly, indicating a dif-ficulty in aligning text and image tokens directly at abag-of-words level. (Bottom) The learnable temperaturein the contrastive loss during training for reference. ized from a pre-trained language model while theimage embeddings are randomly initialized. Fora batch of N = 1024 image-caption pairs, thecontrastive objective from CLIP is employed tomaximize the cosine similarity between matchedimage-caption l2-normalized representations andto minimize the similarity for non-matching pairs.Only the image embeddings are updated duringtraining.In , we illustrate that the contrastive lossplateaus quickly, indicating a difficulty in aligningtext and image tokens directly at a bag-of-wordslevel. Indeed, after training, when querying theclosest text tokens for any image token, we observethat they predominantly align with noisy, semanti-cally void text tokens. Furthermore, when we usethe trained image embeddings as initialization fortext-to-image generation, as opposed to randominitialization, there is no discernible improvement.",
  "Conclusion": "This study highlights the difficulty of naively adapt-ing a text-only language model to handle multi-modal contents, such as texts and images. Giventhe challenge of the disparities between image to-kens and text tokens, a valuable avenue for futureexperiments is to employ tokenizers that align se-mantically with text tokens, such as SEED (Geet al., 2023) or SPAE (Yu et al., 2023a).",
  "Limitations": "Our study has some limitations. First, the resultsare based on the VQGAN image tokenizer, whichdoes not align semantics between image tokens andtext tokens. Tokenizers that semantically align im-age tokens with text tokens might yield differentoutcomes. Second, we observed severe degradationin language model capabilities during fine-tuning,suggesting that exploring methods to avoid catas-trophic forgetting could be a promising future re-search direction. Additionally, our experimentsused internal image-caption datasets and requiredextensive computational resources, which mightlimit the reproducibility of exact numbers. Despitethese limitations, our findings remain useful andtransferable and provide valuable information forfuture research. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-NingHsu, Karen Hambardzumyan, Susan Zhang, StephenRoller, Naman Goyal, Omer Levy, and Luke Zettle-moyer. 2023. Scaling laws for generative mixed-modal language models. In ICML. Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language modelsacross training and scaling. In ICML. Sidney Black, Stella Biderman, Eric Hallahan, QuentinAnthony, Leo Gao, Laurence Golding, HoraceHe, Connor Leahy, Kyle McDonell, Jason Phang,Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-hit, Laria Reynolds, Jonathan Tow, Ben Wang, andSamuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model. In ACL Work-shop.",
  "Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, andYing Shan. 2023. Planting a seed of vision in largelanguage model. arXiv preprint arXiv:2307.08041": "Suchin Gururangan, Mitchell Wortsman, Samir YitzhakGadre, Achal Dave, Maciej Kilian, Weijia Shi,Jean Mercat, Georgios Smyrnis, Gabriel Ilharco,Matt Jordan, Reinhard Heckel, Alex Dimakis, AliFarhadi, Vaishaal Shankar, and Ludwig Schmidt.2023. open_lm: a minimal but performative lan-guage modeling (lm) repository. GitHub repository. Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha,Christopher D Manning, Dan Jurafsky, and DanielHo. 2022. Pile of law: Learning responsible datafiltering from the law and a 256gb open-source legaldataset. In NeurIPS.",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In ICLR": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In ICML. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. JMLR.",
  "Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruom-ing Pang, James Qin, Alexander Ku, Yuanzhong Xu,Jason Baldridge, and Yonghui Wu. 2022a. Vector-quantized image modeling with improved VQGAN.In ICLR": "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexan-der Ku, Yinfei Yang, Burcu Karagol Ayan, et al.2022b. Scaling autoregressive models for content-rich text-to-image generation. TMLR. Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar,Wolfgang Macherey, Yanping Huang, David A Ross,Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al.2023a. Spae: Semantic pyramid autoencoder for mul-timodal generation with frozen llms. arXiv preprintarXiv:2306.17842. Lili Yu, Bowen Shi, Ramakanth Pasunuru, BenjaminMuller, Olga Golovneva, Tianlu Wang, Arun Babu,Binh Tang, Brian Karrer, Shelly Sheynin, et al.2023b. Scaling autoregressive multi-modal models:Pretraining and instruction tuning. arXiv preprintarXiv:2309.02591."
}