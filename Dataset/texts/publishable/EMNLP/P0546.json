{
  "In response, we present Understanding Punwith Image Explanations (": "UNPIE)1, anovel benchmark designed to assess the im-pact of multimodal inputs in resolving lexicalambiguities. Puns serve as the ideal subject forthis evaluation due to their intrinsic ambiguity.Our dataset includes 1,000 puns, each accom-panied by an image that explains both mean-ings. We pose three multimodal challengeswith the annotations to assess different aspectsof multimodal literacy; Pun Grounding, Disam-biguation, and Reconstruction. The results2 in-dicate that various Socratic Models and Visual-Language Models improve over the text-onlymodels when given visual context, particularlyas the complexity of the tasks increases.",
  "Introduction": "Humans can actively integrate information frommultimodal sources without being explicitly toldto. For example, a wink can reveal the insinceritybehind a statement about dieting. Similarly, visualaids such as Venn diagrams help students under-stand abstract concepts such as set theory. Thisactive understanding capacity is often denoted asmultimodal literacy (Mills and Unsworth, 2017).In contrast, current multimodal models lack thiscapacity for active understanding and typically op-erate under two assumptions: (1) all instructionsrequire visual inputs, and (2) these inputs are rel-evant (Cui et al., 2023; Zhang et al., 2024). Such",
  "Code: github.com/JiwanChung/VisualPun_UNPIE": ": Puns naturally occur with images to enhanceunderstanding (Zenner and Geeraerts, 2018), makingthem natural candidates for testing active multimodalunderstanding capacity of machines. Examples of punsaccompanied by visual explanations from r/puns sub-reddit on Reddit. limitations hinder their applicability in real-worldscenarios, such as summarizing long blog posts,where irrelevant images must be excluded, and onlycontextually significant visuals should be used toenhance the understanding of disparate text seg-ments.An essential component of multimodal literacy isthe ability to resolve multimodal ambiguities effec-tively, which refers to the capacity to disambiguateconflicting or unclear information in modality withinformation from another modality (Kottur et al.,2021; Guo et al., 2022). Owing to its explicit re-quirement of multimodal information gathering,disambiguation can serve as a controlled bench-mark for evaluating multimodal literacy.Puns stand as a unique challenge within ambi-guity modeling. They are intrinsically ambiguousand understanding a pun requires grasping multipleinterpretations of a single phrase or word simultane-ously. Understanding puns can be difficult even forhumans, often necessitating visual cues to clarifythe intended interpretation, as demonstrated in Fig-ure 1. Compared to verbose textual explanations,visual cues can deliver instant insight, preservingthe humor and cleverness of the pun (Morreall,1983). Therefore, puns provide an ideal testingground for assessing models capabilities in multi-",
  "(b)": ": The UNPIE benchmark comprises three multimodal tasks: 1. Identifying the specific phrase in an Englishsentence that constitutes a pun, using the provided (a) pun explanation image; 2. Choosing the translation of the punsentence that aligns more closely with the given (b) pun disambiguator image; and 3. Reconstructing the Englishpun sentence from its translated version, aided by the corresponding (a) pun explanation image.",
  "modal interpretation.In this work, we explore model capabilities in re-solving textual ambiguities through visual context.To this end, we propose Understanding Pun withImage Explanations (": "UNPIE), a novel bench-mark consisting of 1,000 text-based puns pairedwith illustrative images that highlight the incon-gruity within the puns. Additionally, our datasetapproaches pun comprehension as a translationtask with incomplete information. This methodprovides a tangible way to measure the often sub-jective skill of reconstructing puns. Each Englishpun is accompanied by translations in three differ-ent languagesGerman, French, and Koreantocapture the challenge of reconstructing the punsacross diverse linguistic contexts.We design three tests based on UNPIE to studyhow models can exploit visual context to aid pununderstanding. summarizes the taskscomprising our benchmark.We first consideran English-only task pun grounding that chal-lenges machines to identify the specific phrase ina sentence that forms a pun. Next, we formulatea multilingual challenge of pun disambiguationwhere models must choose the translation that bestmatches the image provided as a pun disambiguator.The final test, pun reconstruction, is a comprehen-sive task where models should recreate the originalEnglish pun sentence using a translated versionwith potentially no ambiguity. For both the pungrounding and reconstruction tasks, we addition-ally provide the pun explanation images as inputsto verify whether models can consider multimodalcontext when dealing with ambiguous text.Our comprehensive experiments on UNPIE af-firm the presence of multimodal literacy capacity in two model types: monolithic Visual-LanguageModels and modular Socratic Models. Incorpo-rating visual context consistently improved perfor-mance across our three pun comprehension tests.Notably, this improvement was more pronouncedin more challenging tasks. Moreover, VLMs per-formed better than Socratic Models built on simpleimage captions. The result suggests that detailed vi-sual understanding is necessary in our benchmark.Finally, fine-tuning with a standard multimodal ma-chine translation dataset adversely affects perfor-mance in the pun reconstruction task. This degrada-tion aligns with findings from prior studies (Futeralet al., 2023) stating that web-based multimodaltranslation datasets may not effectively capture vi-sual dependencies.Overall, our contributions are as follows:",
  ": Comparison of UNPIE against multimodalmachine translation benchmarks.The statistics forMulti30k are from the test-2017-flickr subset. Gen de-notes a generative benchmark": "sources to resolve ambiguity in text. Our datasetleverages puns that inherently contain such ambi-guity to study the challenge of multimodal literacyin a natural environment.UNPIE extends puns in two directions: visualcontext and multilingual translations. First, we col-lect images for each pun that 1. describes bothmeanings of the pun to explain it and 2. depictsonly one meaning of the pun to disambiguate thepun (section 2.1). While one can naturally retrieveimages for disambiguation from the web, imagesthat illustrate the ambiguity of the pun in a singlecanvas are rare. Thus, we use an off-the-shelf text-to-image model (Betker et al., 2023) to generatesuch images. We then employ human annotatorsto filter the images so that they correctly explainthe given pun. Secondly, we ask human annotatorsto translate the English pun sentences into multilin-gual targets (section 2.2). Importantly, the ambigu-ity should not carry on to the translation target.",
  "Collecting Puns with Visual Context": "Base Text-Only Pun Data. We build our multi-modal multilingual benchmark on top of the text-only English pun dataset of SemEval 2017 Task7 (Miller et al., 2017). The dataset bounds the pununderstanding problem in two ways to rely lesson external requirements: first, each sentence con- . Draw a picture explaining the pun: \" A pop singer bought a new house for a song. The description of the singer singing is good, but so please describe the price of the house by adding it. \"for a song\" means \"very cheap,\" The image should explain both meanings of a pun. please \"bought the house for a song\" remove the text The scene text should not directly explain the pun. : An example of our pun explanation imagegeneration process. A human worker interacts with anoff-the-shelf text-to-image model, iteratively guidingthe model to produce an image that satisfies each speci-fied criterion. tains a maximum of one pun. Hence, a sentenceslexical ambiguity is regulated, at least in terms ofpuns. Second, most pun has a lexical entry in Word-Net 3.1 (81% of the whole data). This vocabularylimit keeps our pun generation problem from beingdominated by many out-of-vocabulary words.The data is divided into Homographic and Het-erographic puns, depending on the surface formof the puns. As shown in , homographicpuns have identical spelling and pronunciation butdifferent meanings, while heterographic puns differin spelling and meanings. We inherit this catego-rization scheme and report our experiment resultscategory-wise (Homographic and Heterographic).From the SemEval 2017 collection of 2,878 En-glish pun sentences, we selected 500 homographicand 500 heterographic puns with concrete conceptsthat are more easily visualized through images.Generating Pun Explanation Images. UNPIE isdesigned to assess a VLMs capability to resolvelexical ambiguity with visual context. In termsof a pun, the context should depict both meanings within the pun. Such images are hard to find amongnatural images due to their complex and sometimesambivalent meanings. Further, such visual designsare typically proprietary, which contradicts our goalof an open-source dataset. Hence, we resort tocreating new images that fit our requirements.We recruited three NLP researchers to activelyprompt the text-to-image generation model DALL-E 3 (Betker et al., 2023) to create images fitting ourpun criteria while maintaining a natural appearance.The base text-only dataset provided the puns as dataseeds (Miller et al., 2017). While we allow rela-tive freedom in the choice of prompts, the workersreported that DALL-E 3 typically produced satis-factory images with straightforward instructions,as illustrated in . Thanks to DALL-E 3smulti-turn interface, the researchers could requestfurther image revisions if the initial output was un-suitable. On average, 24% samples needed suchmulti-step modification. We obtained 1000 punexplanation images after this process.Retrieving Pun Disambiguator Images. UNPIEoffers an alternative visual context: per each pun,we attach two images that describe each meaningof the pun. These images disambiguate the pun andare intended to be used in the binary classificationtask of pun disambiguation explained in section 3.As a pun disambiguator image is aligned to asingle meaning, searching for the required imageis easier compared to the pun explanation imagesthat require encoding both meanings in the sameimage. Hence, we opt for image retrieval from theLAION 2B web image-text dataset (Schuhmannet al., 2022) rather than image generation. Usingthe CLIP (Radford et al., 2021)-based image searchAPI (Beaumont, 2022), we retrieve ten images perthe meaning of a pun. Then, we manually select thetop one that best fits the description. We discard thewhole sample when there is no suitable image. Weconsidered two criteria when selecting the images:first, images that explicitly contain the meaning orthe pun word itself as printed text are discouragedas such images reward OCR capability rather thangeneral visual understanding. Second, images withwatermarks are filtered out to avoid confusion.",
  "GPT4Eval78.10.39": ": Experiment on the effect of meaning frequencyin puns. Top: division of pun reconstruction task resultsaccording to the commonality of meanings. Bottem:assessment of GPT-4-based meaning frequency orderingagainst an independent dataset with human-annotatedmeaning frequencies (Rice et al., 2019). ability of the assessment process, while machine-based evaluation, such as using models like GPT-4 (OpenAI, 2023), may introduce undesirable bi-ases (Liu et al., 2023c; Hada et al., 2023). To over-come these challenges, we suggest an alternativeevaluation method via a downstream task in transla-tion, intentionally aligning with previous researchin the field of multimodal machine translation.Translation with Machine Assistance. We trans-late the original English pun sentence into threelanguages (German, French, and Korean). Notethat we should ensure that the ambiguity in Englishdoes not carry over into the translated targets.We here design a cooperative framework be-tween machines and humans for pun translation.Per each language pair (e.g. En De), we recruita bilingual worker whose native language is thetarget language (e.g. De). Frst, we use off-the-shelf translation models to generate three candi-dates. Then, the human workers select the best oneand make further modifications to finalize the trans-lation. This machine-assisted translation alignswith common practices in the industry (Federicoet al., 2012). We chose machine-human coopera-tion for two reasons: firstly, we saw that our humantranslators find pun translation difficult. Machinesuggestions can serve as starting points here. Sec-ondly, this method expedited the annotation processand reduced costs.Addressing Lingering Ambiguity. Certain casesarise where the ambiguity in the source language isretained in the translated text in literal translation.For example, consider the sentence: A baseballplayer was a thief. He was always trying to steal.The pun in this sentence relies on the dual meanings of stealto take without permission and tosteal a base in baseball. The challenge in transla-tion is twofold: Some languages contain equivalentidiomatic expressions (e.g., stehlen in German),which can result in similar ambiguities in the targettext. To address this, translators were instructedto select alternative words that avoid unintendeddouble meanings whenever possible. The punshumor is implied contextually within the first sen-tence, even if the pun word itself is not explicitlymentioned. For such instances, indirect transla-tions were permitted, allowing human translatorsto render distinct interpretations of the pun with-out preserving its exact wording. To further refinethe outputs, we applied text-based deduplication toeliminate closely matching translations. Refer toappendix B for more details.",
  "Dataset Analysis": "Our pipeline yields a dataset comprising 500 ho-mographic and 500 heterographic pun sentences,each accompanied by one pun explanation image,two pun disambiguator images, and translations tothree languages.How natural are the generated images? Giventhe limited availability of real-world images accu-rately depicting puns, we opted to use AI-generatedvisuals. To gauge the difference between generatedand authentic images, we conducted two humanevaluation studies, comparing our generated im-ages against natural image-pun pairs sourced fromthe web ( the first study, human evaluators were askedto identify the correct text pun associated with eachimage from a set of potential matches. Resultsshowed that natural images achieved an accuracyof 86%, while our generated images achieved aslightly higher accuracy of 92%. This test wasconducted using a set of 50 randomly selectedimages. In the second study, we conducted anA/B comparison to assess the perceived natural-ness of the images. To ensure consistency, naturalimages containing multiple panels, written text,or well-known characters were excluded from theevaluation. Across three independent evaluators,the naturalness test resulted in accuracy rates of66%, 72%, and 74%, respectively, using anotherset of 50 random images. Overall, despite slightdistributional differences between the generatedand natural pun images, the disparity is consideredacceptable. These findings indicate that evaluationsperformed within our benchmark can be reasonably",
  ":Statistical differences between uncondi-tional translation and pun-aware translation, averagedacross languages. Text similarity was evaluated usingBERTScore (Zhang et al., 2019)": "extrapolated to real-world settings.Common vs. uncommon meanings. In UNPIE,each sample contains a pun phrase with two dis-tinct meanings.This section explores how thepopularity, or frequency, of each meaning influ-ences downstream performance. To investigate,we rank the meanings of each word by their fre-quency using zero-shot GPT-4. To ensure the ac-curacy of GPT-4s assessments, we cross-referencethese with human-annotated frequency data fromRice et al. (Rice et al., 2019), which includes 890homonyms with annotated frequencies. The lowersection of compares GPT-4s frequencyrankings with the human-annotated ground truth.Next, using GPT-4, we categorize our data into twogroups based on more and less frequent meanings.This categorization is then analyzed through thepun reconstruction task outlined in section 3. Asillustrated in the upper part of , the pun re-construction task reveals that inputs with commonmeanings present more challenges than those withuncommon ones when using GPT-4. This suggeststhat texts with an uncommon meaning supplementthe models inherent understanding of the morefrequent meaning.How different are disambiguated translationsfrom unconditional ones? When disambiguationis enforced as a strict criterion, the resulting trans-lations are expected to differ from straightforward,unconditional translations. To quantify the extentof this difference, we compare the unconditionaltranslation y0 against two baselines: (1) anotherunconditional translation produced by a differentannotator (y1), and (2) the disambiguated transla-tion (y). We measure text similarity scores for eachpair: s1 = sim(y0, y1) and s2 = sim(y0, y), andcompute the win rate as the proportion of caseswhere s2 exceeds s1. The results, summarizedin , show that although disambiguation in-structions lead to noticeable changes, the overall",
  "Task Overview": "We pose three multimodal pun understanding taskson the collected annotations to test models capabil-ity to use visual context in addressing lexical ambi-guity, as illustrated in . Each task evaluatesdifferent aspects: the easier Pun Grounding taskcan be solved without image input. It is aimed atdetermining if less advanced models, which mightnot fully resolve such challenges, can enhance theirperformance with added visual information. Thesecond task of pun disambiguation is designed tonecessitate the usage of visual context. Finally, thepun reconstruction task replicates a practical multi-modal literacy scenario. This task necessitates thatmodels not only use the given translation but alsoinfer or extract the underlying pun meaning that thetranslation does not explicitly convey, potentiallydrawing on visual inputs to do so.Pun Grounding. The first step in understandinga pun is to identify it. Our initial task examineswhether visual context aids models in identifyingpun phrases within sentences. Given the wholeEnglish sentence xi = [xi0, . . . , xit] containing apun phrase si = [xik, . . . , xil] and its correspond-ing pun explanation image vie, the model returns apun phrase candidate si. Note that while the actualtarget phrase si is part of the full sentence xi, themodels output si is not bound by this constraint.We purposefully formulate this task as a sequence-to-sequence problem to facilitate zero-shot evalua-tion across various baselines. The models outputis then assessed for exact text match with the actualpun phrase to determine accuracy.Pun Disambiguation. Once models pinpoint a puns location, they must then interpret its seman-tics. Understanding a pun hinges on recognizingthe different meanings of the pun phrase, as itshumor lies in this ambiguity. In this task, we as-sess the models proficiency in correlating eachmeaning of the pun with its associated visual con-text. Given the English sentence xi and the pundisambiguator image vid aligned with one of themeanings constructing the pun, the model shouldproduce a translation of the sentence into a targetlanguage (e.g. German yiDe). Notably, the trans-lated text should be free of any ambiguity stemmingfrom the pun, closely aligning with the meaningdepicted in the provided image. We compare themodel-generated translation yi,jDe with two trans-lation targets yi,0De, yi,1De, each corresponding to adifferent meaning of the pun. The models outputis considered correct if it more closely resemblesthe ground-truth translation yi,jDe that correspondsto the meaning depicted in the image vi,jd , j 0, 1.Refer to section 4.3 for the implementation details.Pun Reconstruction. The final task is to recon-struct the complete pun sentence. To make theproblem deterministic, we provide two types ofinputs to the model: a non-English language trans-lation of the original pun sentence that has beenclarified of any ambiguities (e.g. German yi,jDe) andthe related pun explanation image vie. The modelthen generates an output xi, which we comparewith the original English pun sentence xi to de-termine if both English sentences encapsulate thesame pun. It is a complex task to determine whethertwo sentences contain the same pun, and we resortto machine-based evaluation with GPT-4 to obtainthe binary decision. We verify GPTEvals validityhere using human evaluation in appendix D.",
  "Models": "LM. To measure the effectiveness of multimodalmodeling, we establish baselines using unimodaltext-only language models.We incorporate anopen-source model (Vicuna-13B (Chiang et al.,2023)) and the advanced proprietary languagemodel (GPT-4 (OpenAI, 2023)). Furthermore, weappropriate a visual-language model, LLaVA, for atext-only scenario by inputting only text promptswithout the images. This approach assesses theconcept of multimodal alignment tax (Chen et al.,2023) in the context of pun interpretation, imply-ing that fine-tuning a model on visual data might",
  ": Experimental results on the pun disambiguation task. All scores are reported in terms of binary classificationaccuracy. The best scores are bolded and the second-best ones are underlined": "impair its original linguistic capabilities. We donot test LM baselines against pun disambiguationas the task necessitates visual context.SM (Socratic Models). SM (Zeng et al., 2022),also called pipelining (Bitton-Guetta et al., 2023),is a two-staged framework extending text-onlyLMs to multimodal tasks by first encoding the mul-timodal context to textual descriptions. To imple-ment SMs, we employ the same language mod-els as previously mentioned and use BLIP-2 OPT2.7B (Li et al., 2023) as the visual description gen-erator to encode the images into textual captions.VLM. Monolithic visual-language models directlytake the raw images and user queries as inputs toproduce textual responses. We employ two pop-ular and high-performing VLMs for this purpose:LLaVA 1.5 13B (Liu et al., 2023a) and Qwen-VL-Chat 7B (Bai et al., 2023). (We refer to Qwen-VL-Chat as Qwen-VL in result tables due to space con-straints.) For the tasks of pun disambiguation andpun reconstruction, we also introduce a machinetranslation baseline. We thus fine-tune LLaVAwith the Multi30k multimodal machine translationdataset (Elliott et al., 2016), yielding the LLaVA-MMT variant. We choose LoRA (Hu et al., 2021)over full fine-tuning for efficient implementation.",
  "Do Images Help Pun Grounding?": "Metrics. We report accuracy based on the equalityof the model-estimated pun phrase and the ground-truth pun phrase. To check the equality, we use theexact match of the surface text form and report theaccuracy of the outputs.Results. As anticipated, the incorporation of vi-sual context led to a consistent improvement in pungrounding performance across all models, includ-ing Socratic Models and Visual-Language Models(refer to ). Also, GPT-4, a stronger model, could solve the task even without visual context,verifying our original intention of proposing thistask to test the helpfulness of visuals where the taskis straightforward but the models are less capable.For evaluation fairness, we employed a standardprompt template across all models (details in ap-pendix E). Note that while careful prompt engineer-ing can further improve the scores, our findingsfocus on understanding the role of visual context inrealistic scenarios rather than extracting the maxi-mum potential from each model.",
  "Can VLMs Disambiguate with Images?": "Metrics. We conduct a generative evaluation forthe pun disambiguation test. The task for the ma-chines is to translate a given pun sentence into atarget language, using the accompanying image asa guide to disambiguate the meaning of the punphrase. In this generative test, the model generatesa sequence of text, which is then evaluated againsttwo potential translation targets. The models out-put is considered accurate if it aligns more closelywith the translation that corresponds to the contextof the provided image. We use BERTScore (Zhanget al., 2019) to measure the text similarity followingthe human evaluation results in appendix C.Results. All the considered baselines have demon-strated their ability to disambiguate translation out-puts based on visual context, as illustrated in Ta-ble 5. Both strengthening the language model (Vi-cuna vs. GPT-4) and improving visual context pro-cessing (Vicuna with image captions from BLIP-2vs. LLaVA) led to more accurate disambiguation.Still, comprehending puns in the textual form wasa more decisive factor for pun disambiguation thana stronger visual understanding, as GPT-4 withimage captions outperforms all other models. In-terestingly, fine-tuning with the Multi30k multi-",
  "LLaVA-MMTV + L28.06.338.518.315.046.723.310.742.6": ": Outcomes for the pun reconstruction task, where and signify the performance change attributed to theinclusion of visual context. The model with the largest performance increase is marked bold in each language. modal machine translation dataset (Elliott et al.,2016) harmed the accuracy of visual alignment.The fine-tuned model (LLaVA-MMT) underper-forms the zero-shot LLaVA in nearly all aspects,except in the English-to-French translation of het-erographic puns. This finding echoes previous re-search (Futeral et al., 2023), which suggests thatmultimodal machine translation datasets cannotproperly evaluate multimodal literacy capability.",
  "Do Images Help Pun Reconstruction?": "Metrics. The pun reconstruction task involves ma-chines using both the human-translated text andthe image context to recreate the original pun sen-tence. Then, the reconstructed pun is comparedwith the original sentence for consistency in puns.Still, determining whether two sentences share thesame pun is a complex task. To tackle this, weuse a machine-based evaluation method with GPT-4 (OpenAI, 2023) to determine if the puns in both sentences are equivalent. To ensure the validityof this approach, known as GPTEval, we furthercompare it with human annotations in appendix D.Additionally, we report on common text evaluationmetrics, such as Bleu-4 (Papineni et al., 2002) andMETEOR (Banerjee and Lavie, 2005)metricswidely used in the machine translation domain.Results. The results in affirm that visualcontext significantly enhances machines ability toreconstruct puns and manage their inherent ambigu-ity. For all tested models, the inclusion of imagesconsistently improved the accuracy of pun recon-struction. The only exception was the weakestmodel in both language processing and visual com-prehension (SM based on Vicuna). Notably, unlikethe main metric of correctness, the automatic textevaluation scores (Bleu-4 and METEOR) did notreflect a clear trend. Through manual inspectionof the generated outputs, we saw that such scoreswere more aligned with changes in the surface form of the text, which did not necessarily correlate withthe accurate identification of puns. This resonateswith previous reports stating that such text scoresare not fully effective outside of their original do-main of machine translation (Liu et al., 2016).Due to the differences in their forms, modelsfound it more challenging to reconstruct hetero-graphic puns than homographic ones. Notably, in-corporating visual context in these more complexscenarios led to significant improvements. Further-more, the benefit of visual context became evenmore evident when dealing with Korean inputs; alanguage typically considered more divergent fromEnglish than either German or French. This re-inforces the idea that machines depend more onvisual cues when tackling complex linguistic tasks.Finally, as in the pun disambiguation task, the fine-tuned LLaVA-MMT suffered from a decline in per-formance compared to the zero-shot LLaVA. Thisfurther supports the notion that visual understand-ing is necessary to handle UNPIE.",
  "Related Work": "Multimodal Machine Translation. By integrat-ing backtranslation as a downstream task, UNPIEcontributes to the literature on Multimodal Ma-chine Translation (MMT), a widely studied areathat extends neural machine translation with ad-ditional visual contexts (Specia et al., 2016; El-liott et al., 2017; Barrault et al., 2018). Previousresearch argues that visual information can helpresolve ambiguities in the source text (Li et al.,2022; Hatami et al., 2022). However, the primarydataset for MMT, Multi30K (Elliott et al., 2016),has limited examples of such ambiguities, leadingto questions about the use of MMT for assessingmultimodal literacy capacity (Elliott, 2018; Wuet al., 2021; Futeral et al., 2023). Another bench-mark counteracts this phenomenon with manual an-notation (Futeral et al., 2023; Bawden et al., 2018).Nevertheless, this dataset is relatively small (155samples) due to the difficulty in pinpointing ambi-guities within sentences. Additionally, the bench-mark is limited to classification models.Computational Pun Understanding. After earlyresearch (Ritchie, 2005) pointed out ambiguity asa key in pun generation, numerous studies haveinvestigated automatic pun generation regardingheterographic puns, which slackens the surfaceform identity requirement for each meaning of thepun (He et al., 2019; Yu et al., 2020; Mittal et al., 2022). Other research explored homographic pungeneration which is based on multiple meaningsof a polysemous word (Yu et al., 2018; Luo et al.,2019; Tian et al., 2022). Recently, Sun et al. (Sunet al., 2022) extended the pun generation problemto consider contextual cues. We extend this line ofresearch with multimodal understanding.Visual-Language Models. The field has seen rapidgrowth since Flamingo (Alayrac et al., 2022) illus-trated the advantages of applying large languagemodels to the visual domain. BLIP-2 (Li et al.,2023), utilizing the OPT language model (Zhanget al., 2022), made significant strides in imagecaptioning. The introduction of a stronger lan-guage model (Touvron et al., 2023) further enabledprompt-based control of the models. MiniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023b)pioneered the field of visual instruction tuning.InstructBLIP (Dai et al., 2023), an extension ofBLIP-2, improved its capability to follow instruc-tions more accurately. Further developments inthis domain include other models such as LLAMA-Adapter (Zhang et al., 2023) and Qwen-VL (Baiet al., 2023). Our research puts visual languagemodels (VLMs) to the test regarding their multi-modal literacy capabilities.",
  "We introduced": "UNPIE, a new benchmark forthe multimodal literacy capability. Based on UN-PIE, we craft three tests to measure how machinescan utilize visual context to resolve inherent ambi-guity in puns. Our findings indicate that machinescan indeed leverage visual information to enhancetheir understanding of text, as shown by their im-proved performance across all tasks.However, achieving human proficiency in multi-modal literacy is still a challenge. While our resultsare encouraging, there remains a considerable gapin machine capability to fully grasp and interpretthe intricate relationship between text and visuals,particularly in more complex tasks like pun recon-struction. Therefore, we envision UNPIE as notonly a platform for testing but also as a startingpoint for the development of future multimodalmodels to actively navigate and integrate informa-tion from multiple modalities.",
  "UNPIE, while being a multilingual dataset, is builton the English-only pun corpus (Miller et al., 2017)": "As such, it primarily models lexical ambiguitiesunique to English, stemming from polysemies orsimilar surface forms of the language. To enhanceits linguistic diversity and applicability, expandingthe dataset to include ambiguities inherent in otherlanguages would be beneficial. Such expansionwould not only diversify the linguistic challengesin the dataset but also offer deeper insights intohow lexical ambiguities manifest differently acrossvarious languages and cultures.Although UNPIEs size is much larger than thatof the previous multimodal literacy dataset thatfeatures explicit ambiguities (Futeral et al., 2023),its total size is insufficient for creating a trainingsplit suitable for fine-tuning. This limitation stemsfrom the scarcity of puns, which are inherentlychallenging for humans to create as well and arenot readily available in large quantities online. Wethus plan to expand the dataset for multilingualpuns in the future.Ethical Considerations. UNPIE, constructed us-ing existing English puns, may inadvertently per-petuate cultural biases and stereotypes presentwithin the humor. Although human annotatorswere instructed to eliminate any puns expressingexplicit hatred, subtle biases can still be perpetu-ated through seemingly innocuous humor.To address ethical concerns in the data curationprocess, we confirmed that all human annotatorseither volunteered willingly or were compensatedfairly for their contributions. We defer the detailsto appendix B. This work was partly supported by an IITP grantfunded by the Korean Government (MSIT) (No.RS-2020-II201361, Artificial Intelligence GraduateSchool Program (Yonsei University) and RS-2024-00353131) and the National Research Foundationof Korea (NRF) grant funded by the Korea govern-ment (MSIT) (No. RS-2024-00354218). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736.",
  "and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572. Loc Barrault, Fethi Bougares, Lucia Specia, ChiraagLala, Desmond Elliott, and Stella Frank. 2018. Find-ings of the third shared task on multimodal machinetranslation.In THIRD CONFERENCE ON MA-CHINE TRANSLATION (WMT18), volume 2, pages308327. Rachel Bawden, Rico Sennrich, Alexandra Birch, andBarry Haddow. 2018. Evaluating discourse phenom-ena in neural machine translation. In Proceedings ofthe 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Pa-pers), pages 13041313.",
  "Romain Beaumont. 2022. Clip retrieval: Easily com-pute clip embeddings and build a clip retrieval sys-tem with them": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,Joyce Lee, Yufei Guo, Wesam Manassra, PrafullaDhariwal, Casey Chu, Yunxin Jiao, and AdityaRamesh. 2023. Improving image generation withbetter captions. Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel,Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky,and Roy Schwartz. 2023. Breaking common sense:Whoops! a vision-and-language benchmark of syn-thetic and compositional images. In Proceedingsof the IEEE/CVF International Conference on Com-puter Vision, pages 26162627.",
  "Delong Chen, Jianfeng Liu, Wenliang Dai, and BaoyuanWang. 2023. Visual instruction tuning with politeflamingo. arXiv preprint arXiv:2307.01003": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu,Linjun Zhang, James Zou, and Huaxiu Yao. 2023.Holistic analysis of hallucination in gpt-4v (ision):Bias and interference challenges.arXiv preprintarXiv:2311.03287.",
  "Desmond Elliott, Stella Frank, Khalil Simaan, andLucia Specia. 2016.Multi30k:Multilingualenglish-german image descriptions. arXiv preprintarXiv:1605.00459": "Marcello Federico, Alessandro Cattelan, and MarcoTrombetti. 2012. Measuring user productivity in ma-chine translation enhanced computer assisted trans-lation. In Proceedings of the 10th Conference of theAssociation for Machine Translation in the Americas:Research Papers. Association for Machine Transla-tion in the Americas. Matthieu Futeral, Cordelia Schmid, Ivan Laptev, BenotSagot, and Rachel Bawden. 2023. Tackling ambi-guity with images: Improved multimodal machinetranslation and contrastive evaluation. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 53945413. Danfeng Guo, Arpit Gupta, Sanchit Agarwal, Jiun-YuKao, Shuyang Gao, Arijit Biswas, Chien-Wei Lin,Tagyoung Chung, and Mohit Bansal. 2022. Gravl-bert: Graphical visual-linguistic representations formultimodal coreference resolution. In Proceedingsof the 29th International Conference on Computa-tional Linguistics, pages 285297. Rishav Hada, Varun Gumma, Adrian de Wynter,Harshita Diddee, Mohamed Ahmed, Monojit Choud-hury, Kalika Bali, and Sunayana Sitaram. 2023. Arelarge language model-based evaluators the solutionto scaling up multilingual evaluation? arXiv preprintarXiv:2309.07462. Ali Hatami, Paul Buitelaar, and Mihael Arcan. 2022.Analysing the correlation between lexical ambiguityand translation quality in a multimodal setting usingwordnet. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies: Student Research Workshop, pages 8995. He He, Nanyun Peng, and Percy Liang. 2019. Pungeneration with surprise. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), pages 17341744. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Yi Li, Rameswar Panda, Yoon Kim, Chun-Fu RichardChen, Rogerio S Feris, David Cox, and Nuno Vascon-celos. 2022. Valhalla: Visual hallucination for ma-chine translation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 52165226. Chia-Wei Liu, Ryan Lowe, Iulian Vlad Serban, MikeNoseworthy, Laurent Charlin, and Joelle Pineau.2016. How not to evaluate your dialogue system:An empirical study of unsupervised evaluation met-rics for dialogue response generation. In Proceedingsof the 2016 Conference on Empirical Methods in Nat-ural Language Processing, pages 21222132.",
  "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023c. Gpte-val: Nlg evaluation using gpt-4 with better humanalignment. arXiv preprint arXiv:2303.16634": "Fuli Luo, Shunyao Li, Pengcheng Yang, Lei Li, BaobaoChang, Zhifang Sui, and Xu Sun. 2019. Pun-gan:Generative adversarial network for pun generation.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 33883393. Tristan Miller, Christian F Hempelmann, and IrynaGurevych. 2017. Semeval-2017 task 7: Detectionand interpretation of english puns. In Proceedings ofthe 11th International Workshop on Semantic Evalu-ation (SemEval-2017), pages 5868.",
  "OpenAI. 2023. Gpt-4 technical report. arXiv preprintarXiv:2303.08774": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. CaitlinARice,BarendBeekhuizen,VladimirDubrovsky, Suzanne Stevenson, and Blair C Arm-strong. 2019. A comparison of homonym meaningfrequency estimates derived from movie and televi-sion subtitles, free association, and explicit ratings.Behavior research methods, 51:13991425.",
  "Graeme Ritchie. 2005. Computational mechanisms forpun generation. In Proceedings of the Tenth Euro-pean Workshop on Natural Language Generation(ENLG-05)": "Christoph Schuhmann, Romain Beaumont, RichardVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis,Mitchell Wortsman, et al. 2022. Laion-5b: An openlarge-scale dataset for training next generation image-text models. Advances in Neural Information Pro-cessing Systems, 35:2527825294. Lucia Specia, Stella Frank, Khalil SimaAn, andDesmond Elliott. 2016. A shared task on multimodalmachine translation and crosslingual image descrip-tion. In Proceedings of the First Conference on Ma-chine Translation: Volume 2, Shared Task Papers,pages 543553. Jiao Sun,Anjali Narayan-Chen,Shereen Oraby,Shuyang Gao, Tagyoung Chung, Jing Huang, YangLiu, and Nanyun Peng. 2022. Context-situated pungeneration. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 46354648. Yufei Tian, Divyanshu Sheth, and Nanyun Peng. 2022.A unified framework for pun generation with humorprinciples. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, pages 32533261. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, andBen Kao. 2021. Good for misconceived reasons: Anempirical revisiting on the need for visual contextin multimodal machine translation. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 61536166. Zhiwei Yu, Jiwei Tan, and Xiaojun Wan. 2018. A neuralapproach to pun generation. In Proceedings of the56th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages16501660. Zhiwei Yu, Hongyu Zang, and Xiaojun Wan. 2020. Ho-mophonic pun generation with lexically constrainedrewriting. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 28702876. Andy Zeng, Maria Attarian, Krzysztof Marcin Choro-manski, Adrian Wong, Stefan Welker, FedericoTombari, Aveek Purohit, Michael S Ryoo, VikasSindhwani, Johnny Lee, et al. 2022. Socratic models:Composing zero-shot multimodal reasoning with lan-guage. In The Eleventh International Conference onLearning Representations.",
  "Eline Zenner and Dirk Geeraerts. 2018. One does notsimply process memes: Image macros as multimodalconstructions. Cultures and traditions of wordplayand wordplay research, pages 167194": "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, andYu Qiao. 2023. Llama-adapter: Efficient fine-tuningof language models with zero-init attention. arXivpreprint arXiv:2303.16199. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "AHyperparameters & Setup": "Models.In all GPT-4 (OpenAI, 2023) usage,we use the gpt-4-0613 endpoint.When con-ducting experiments with open-source models,we leverage the official implementation codes inconjunction with publicly available weights fromthe Huggingface Hub ( For our work with the Vicuna (Chi-ang et al., 2023) language model, we employedthe lmsys/vicuna-7b-v1.5 endpoint. Addition-ally, for the LLaVA 1.5 13B (Liu et al., 2023a) andQwen-VL-Chat (Bai et al., 2023) visual-languagemodels, we used the following model parame-ters: liuhaotian/llava-v1.5-13b and Qwen/Qwen-VL-Chat, respectively.Text Generation. We use deterministic greedysampling for all experiments, introducing no ran-domness or external hyperparameter in the textgeneration process. Except for GPT-4, all modelswere allowed to generate up to 200 tokens with thefreedom of an early stopping.Computational Resources & Fine-tuning. Weused the OpenAI API for inferring GPT-4 (Ope-nAI, 2023) outputs. For open-source models suchas Vicuna (Chiang et al., 2023), LLaVA (Liu et al.,2023a), and Qwen-VL-Chat (Bai et al., 2023), weuse a single NVIDIA A100 40GB GPU for infer-ence. While the exact inference speed varies de-pending on the length of prompts and responses, aquery takes about 0.8 seconds to terminate whenutilizing batch processing. Fine-tuning LLaVA wasalso possible in a single A100 40GB GPU thanksto the efficient LoRA-based implementation (Huet al., 2021). We trained each translation modelfor ten epochs in the training split of the Multi30kdataset (Elliott et al., 2016) with early stopping,which took 20 hours on average.",
  "BData Collection Details": "Generating Pun Explanation Images. We relyon the DALL-E 3 (Betker et al., 2023) image-to-text model to generate images that explain bothmeanings of the pun at the same time. DALL-E3 can be accessed either from the GPT-4 ( or Bing Image Generator( web interface.Our annotators employed both interfaces.In contrast to previous studies that employeddesigners for image generation tasks involvingmachine-human collaboration (Bitton-Guetta et al.,2023), we chose to recruit three NLP researchers to generate images with DALL-E 3 and curate the out-puts. This decision was driven by the necessity foran accurate representation of the puns meaningsrather than the artistic quality of the images, con-sidering the specific requirements of our researchtask. The NLP researchers participated voluntarilyin this annotation task.Translating Puns. To streamline the translationprocess while ensuring clarity in meaning, weadopted a machine-assisted translation approach.This method simplifies the task for human anno-tators, who are required to ensure that the transla-tions clearly reflect the intended meaning of thetext. Initially, we provide three machine-generatedtranslation options for the annotators to selectand refine. These options are created using GPT-4, which we prompt in two distinct ways, andDeepL ( proprietary translation service.Human annotators then use these machine-provided translations as a base to craft the finalversion of the translated pun sentence, as depictedin the user interface shown in . For eachof the three target languages (German, French, andKorean), we engaged a native speaker who is alsoproficient in English, ensuring both linguistic ac-curacy and fidelity to the original puns meaning.The extent for how much the translations writtenby human annotators were drifted from uncondi-tional translation is reported in . We payeach annotator 1215$ per hour. We have receivedapproval from the university department and con-ducted data collection.",
  "CTesting the Validity of BERTScore": "In the Pun Disambiguation task, we require themodels to generate the disambiguated text. Thus,we need an automatic algorithm to decide whetherthe generated output aligns with the intended punmeaning. We formulate this as a text-only prob-lem of matching the output with the ground-truthdisambiguated translation result.We consider various options here: three trans-lation metrics (Bleu, METEOR, and Rouge-L)and two model-based metrics (BERTScore (Zhanget al., 2019) and GPT-based Evaluation). Note thatGPTEval here does not receive images as inputsfollowing the other options. Given the generationoutput of a VLM (LLaVA) or a Socratic Model(GPT4), we ask human annotators for a ternaryclassification: Match, No Match, and Invalid. A",
  ": We compare each metric with human judg-ments on a set of 100 samples. denotes the Phi corre-lation coefficient": "sample is classified as matched if it is better alignedto the intended translation target than the other oneand as not matched vice versa. We also let the an-notators mark invalid outputs in which the text doesnot align with either target. Finally, we compare thehuman decisions with automatic algorithms on 100valid samples. We filtered out 32 invalid outputsfrom LLaVA and none from GPT4.The results show that BERTScore greatly im-proves over the traditional translation metric base-lines. As the disambiguation of a pun sentencetypically lies in the correct translation of salientphrases, conventional metrics without semantic un-derstanding are not sufficient for the task. On theother hand, BERTScore shows an acceptable cor-relation with human annotations. We thus employit as the metric for the Pun Disambiguation task.Perhaps surprisingly, the strong LLM backbone ofGPTEval yields the worst outcome. Upon qualita-tive examination, we saw that GPT tends to favorcertain targets regardless of the input. We leavealleviating this bias of GPTEval to future research.",
  "Pun Sentence: He and his partner made knives, and they shared a cut": "Fr: Lui et son partenaire ont fabriqu des couteaux, et ils se sont engags dans un processus de division ou de sparation de matriaux avec des objets pointus.De: Er und sein Partner produzierten Messer und beschftigten sich mit dem Prozess des Teilens oder Trennens von Materialien mit scharfen Objekten.Kr: , ."
}