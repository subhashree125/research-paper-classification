{
  "Abstract": "Soccer is a globally popular sport with a vast au-dience, in this paper, we consider constructingan automatic soccer game commentary modelto improve the audiences viewing experience.In general, we make the following contribu-tions: First, observing the prevalent video-textmisalignment in existing datasets, we manuallyannotate timestamps for 49 matches, establish-ing a more robust benchmark for soccer gamecommentary generation, termed as SN-Caption-test-align; Second, we propose a multi-modaltemporal alignment pipeline to automaticallycorrect and filter the existing dataset at scale,creating a higher-quality soccer game commen-tary dataset for training, denoted as MatchTime;Third, based on our curated dataset, we trainan automatic commentary generation model,named MatchVoice. Extensive experimentsand ablation studies have demonstrated the ef-fectiveness of our alignment pipeline, and train-ing model on the curated dataset achieves state-of-the-art performance for commentary gen-eration, showcasing that better alignment canlead to significant performance improvementsin downstream tasks.",
  "Introduction": "Soccer, as one of the most popular sports globally,has captivated over 5 billion (FIFA, 2023) viewerswith its dynamic gameplay and intense moments.Commentary plays a crucial role in improving theviewing experience, providing context, analysis,and emotional excitement to the audience. How-ever, creating engaging and insightful commentaryrequires significant expertise and can be resource-intensive. In recent years, advancements in artifi-cial intelligence, particularly in foundational visual-language models, have opened new possibilitiesfor automating various aspects of content creation.This paper aims to develop an high-quality, auto-matic soccer commentary system. In the literature on video understanding, therehas been relatively little attention on sports videos.Pioneering work such as SoccerNet (Giancola et al.,2018a) introduces the first soccer game dataset,containing videos of 500 soccer matches. Subse-quently, SoccerNet-Caption (Mkhallati et al., 2023)compiles textual commentary data for 471 of thesematches from the Internet, establishing the firstdataset and benchmark for soccer game commen-tary. However, upon careful examination, we ob-serve that the quality of existing data is often un-satisfactory. For instance, as illustrated in (left), since the textual commentaries are often col-lected from the text live broadcast website, therecan be a delay with respect to the visual content,leading to prevalent misalignment between textualcommentaries and video clips.In this paper, we start by probing the effect ofthe above-mentioned misalignment on the soccergame commentary systems. Specifically, we man-ually correct the timestamps of commentaries for49 matches in the SoccerNet-Caption test set to ob-tain a new benchmark, termed as SN-Caption-test-align. With manual check, we observe that thesemisalignments can result in temporal offsets forup to 152 seconds, with an average absolute offsetof 16.63 seconds. As depicted in (right),after manual correction, pre-trained off-the-shelfSN-Caption model (Mkhallati et al., 2023) has ex-hibited large performance improvements, under-scoring the effect of temporal alignment.To address the aforementioned misalignment is-sue between textual commentaries and visual con-tent, we propose a two-stage pipeline to automat-ically correct and filter the existing commentarytraining set at scale. We first adopt WhisperX (Bainet al., 2023) to extract narration texts with corre-sponding timestamps from the background audio,which are then summarised into event descriptionsby LLaMA-3 (AI@Meta, 2024) at fixed intervals.Subsequently, we utilize LLaMA-3 to select the",
  "w/o Alignw/ Align": ": Overview. (a) Left: Existing soccer game commentary datasets contain significant misalignment betweenvisual content and textual commentaries. We aim to align them to curate a better soccer game commentarybenchmark. (b) Right: While evaluating on manually aligned videos, existing models can achieve better commentaryquality in a zero-shot manner. (The temporal window size is set to 10 seconds here.) most appropriate time intervals based on the sim-ilarity between these timestamped event descrip-tions and textual commentaries. Given such anoperation only provides rough alignment, we fur-ther align the video and commentary by training amulti-modal temporal alignment model on a smallset of manually annotated videos. Our alignment pipeline enables to significantlymitigate the temporal offsets between the visualcontent and textual commentaries, resulting in anhigher-quality soccer game commentary dataset,named MatchTime. With such a curated dataset,we further develop a video-language model byconnecting visual encoders with language model,termed as MatchVoice, that enables to generateaccurate and professional commentaries for soc-cer match videos. Experimentally, we have thor-oughly investigated the different visual encoders,demonstrating state-of-the-art performance in bothprecision and contextual relevance. To summarize, we make the following contribu-tions: (i) we show the effect of misalignment in au-tomatic commentary generation evaluation by man-ually correcting the alignment errors in 49 soccermatches, which can later be used as a new bench-mark for the community, termed as SN-Caption-test-align, as will be detailed in Sec. 2; (ii) wefurther propose a multi-modal temporal video-textalignment pipeline that corrects and filters existingsoccer game commentary datasets at scale, result-ing in an high-quality training dataset for commen-tary generation, named MatchTime, as will be de-tailed in Sec. 3; (iii) we present a soccer game com-mentary model named MatchVoice, establishinga new state-of-the-art performance for automaticsoccer game commentary generation, as will bedetailed in Sec. 4.",
  "Benchmark Curation": "To probe the effect of misalignment on the perfor-mance of soccer game commentary models, wehave manually annotated the timestamps of tex-tual commentaries for 49 matches in the test set ofSoccerNet-Caption, resulting in a new benchmark,denoted as SN-Caption-test-align. Mannual Annotations. We recruit 20 footballfans to manually align textual commentaries withvideo content for 49 matches from the test set ofSoccerNet-Caption (Mkhallati et al., 2023), follow-ing several rules: (i) Volunteers should watch theentire video, and adjust the timestamps of originaltextual commentaries to match the moments whenevents occur; (ii) To ensure the continuity of ac-tions such as shots, passes, and fouls, the manuallyannotated timestamps are adjusted 1 second earlierto capture the full context; (iii) For scenes with re-plays, the timestamp of the events first occurrenceis marked as the corresponding commentary times-tamp to maintain visual integrity and consistency.Here, our annotated dataset serves two purposes:first, it acts as a more accurate benchmark for evalu-",
  ": frozen parameters !=60:41": ": Temporal Alignment Pipeline. (a) Pre-processing with ASR and LLMs: We use WhisperX to extractnarration texts and corresponding timestamps from the audio, and leverage LLaMA-3 to summarize these into aseries of timestamped events, for data pre-processing. (b) Fine-grained Temporal Alignment: We additionally traina multi-modal temporal alignment model on manually aligned data, which further aligns textual commentaries totheir best-matching video frames at a fine-grained level.",
  "ating soccer game commentary generation; second,it can be used as supervised data for training andevaluating temporal alignment pipelines": "Data Statistics. After manually annotating the testset videos, we obtain a total of 3,267 video-textpairs. As depicted in , we show the tem-poral offset between the original noisy timestampsof the textual commentary and the manually an-notated ground truth, which ranges from -108 to152 seconds, with an average offset of 13.85 sec-onds and a mean absolute offset of 16.63 seconds.Only 26.29%, 60.21%, 74.96%, and 85.03% of thedata falls within 10s, 30s, 45s, and 60s windowsaround the key frames, respectively. This high-lights the severe misalignment in existing datasets,which will potentially confuse the model trainingfor automatic commentary generation.",
  "Aligning Commentary and Videos": "In this section, we develop an automatic pipelinefor aligning the timestamps of given textual com-mentaries to the corresponding video content inexisting soccer game commentary datasets.InSec. 3.1, we start with the problem formulation fortemporal alignment, and subsequently, in Sec. 3.2,we elaborate on the details of our proposed multi-modal temporal alignment pipeline.",
  "Problem Formulation": "Given a soccer match video from the SoccerNet-Caption dataset, i.e., X = {V, C}, where V ={(I1, t1), . . . , (In, tn)} denotes key frames of thevideo and their corresponding timestamps, and C ={(C1, t1), . . . , (Ck, tk)} represents the k textualcommentaries and their provided timestamps in thevideo, with n k. Here, our goal is to improve thesoccer game commentary dataset by better aligningtextual commentaries with key frames. Concretely,we adopt a contrastive alignment pipeline to up-date their timestamps: t = (V, C; 1), where 1denotes the trainable parameters of the alignmentmodel , and t represents the modified timestampsfor all textual commentaries.",
  "Method": "As depicted in , we propose a two-stagetemporal alignment pipeline: (i) pre-processingwith an off-the-shelf automatic speech recognitionmodel (ASR) and large language model (LLMs),(ii) train an alignment model with contrastive learn-ing. We will elaborate on the details as follows.Pre-processing with ASR and LLMs. We pro-pose to roughly align the textual commentary withvideo content by leveraging the audio narration,which may include key event descriptions. Specifi-cally, we first adopt WhisperX (Bain et al., 2023) for automatic speech recognition (ASR), to obtainthe converted narration text with correspondingtimestamp intervals from the audio. Given thatlive soccer commentary tends to be fragmented andcolloquial, we use LLaMA-3 (AI@Meta, 2024) tosummarize the ASR results into event descriptionsfor each 10-second video clip with the prompt de-scribed in Appendix A.2. Subsequently, we feedthese event descriptions and the textual commen-taries into LLaMA-3 to predict new timestamps forthe textual commentaries based on sentence simi-larities using the prompt detailed in Appendix A.2.Note that, as some videos may not have audiocommentary, or narrations that are irrelevant to thevideo content, such as the background informationfor certain players, such pre-processing only allowsfor a coarse-grained alignment of the commentaryto video key frames.Fine-grained Temporal Alignment. Here, we fur-ther propose to train a multi-modal temporal align-ment model with contrastive learning. Concretely,we adopt pre-trained CLIP (Radford et al., 2021)to encode textual commentaries and key frames,followed by trainable MLPs, i.e., f() and g():",
  "||Ci||||Vj||, A Rkn": "With the manual annotated SN-Caption-test-alignas introduced in Sec. 2, we can construct theground truth label matrix with the same form, i.e.,Y {0, 1}kn, Y[i, j] = 1 if the i-th commentarycorresponds to the j-th key frame, otherwise 0.We train the joint visual-textual embeddings foralignment with contrastive learning (Oord et al.,2018), by maximising similarity scores between thecommentary and its corresponding visual frame:",
  ": Data Statistics on our SN-Caption-test-alignand MatchTime datasets": "the manually annotated ground truth timestampsare utilized for training. At inference time, con-sidering that data pre-processing has provided acoarse alignment, and there might be replays insoccer match videos, we sample frames at 1FPSfrom 45 seconds before and 30 seconds after thecurrent textual commentary timestamp as visualcandidates for alignment. To validate the effective-ness of our alignment model, we evaluate it on 292samples of 4 unseen annotated matches, results canbe found in Sec. 5.1.With the trained model, we perform fine-grainedtemporal alignment for each textual commentaryCi by updating its timestamp to ti with tj of thevisual frame Ij, which exhibits the highest cross-modal similarity score among all the candidates:",
  "ti := tj, where j = arg max(A[i, :])": "Using the alignment pipeline described above, wehave aligned all the pre-processed training datafrom SoccerNet-Caption. As for the matches lack-ing audio, which cannot undergo pre-processing,we directly apply our fine-grained temporal align-ment model. As a result, we have aligned 422videos (373 as the training set and 49 as the vali-dation set), amounting to 29,476 video-text pairs(26,058 for training and 3,418 for validation) in to-tal. This contributes a high-quality dataset, termedas MatchTime, for training an automatic soccergame commentary system. The detailed statisticsof our datasets are listed in .",
  "Automatic Soccer Game Commentary": "Based on the curated dataset, we consider traininga visual-language model for automatic commentarygeneration on given input video segments, termedas MatchVoice. Specifically, we start by describingthe problem scenario, and followed by detailing onour proposed architecture.Problem Formulation. Given a soccer game videowith multiple clips, i.e., V = {V1, V2, . . . , VT },our goal is to develop a visual-language model thatgenerates corresponding textual commentary foreach video segment, i.e., Ci = (Vi; 2), where2 refers to the trainable parameters.",
  "Learnable queries": ": MatchVoice Architecture Overview. Our proposed MatchVoice model leverages a pretrained visualencoder to encode video frames into visual features. A learnable temporal aggregator aggregates the temporalinformation among these features. The temporally aggregated features are then projected into prefix tokens of LLMvia a trainable MLP projection layer, to generate the corresponding textual commentary. Architecture. As depicted in , our pro-posed model comprises of three components. Here,we focus on processing one segment, and ignorethe subscripts for simplicity.First, we adopt the frozen, pre-trained visualencoder to compute the framewise features withinthe video clip, i.e., {v1, v2, . . . , vn} = enc(V).Note that, all visual encoders are framewise, exceptInternVideo, which takes 8 frames per second andaggregates them into 1 feature vector by itself.Second, we use a Perceiver-like architecture (Jae-gle et al., 2021) aggregator to aggregate the tempo-ral information among visual features. Specifically,we adopt two transformer decoder layers, with afixed-length learnable query, and visual featuresas keys and values, to obtain the temporally-awarefeatures, i.e., F = agg(v1, v2, . . . , vn).Last, an MLP projection layer is used to mapthe output queries into desired feature dimensions,used as prefix tokens for a decoder-only large lan-guage model (LLMs), to generate the desired tex-tual commentary, i.e., C = dec(proj(F)). Withthe ground truth commentary for the soccer videoclips, the model is then trained with standard nega-tive log-likelihood loss for language generation.",
  "window10 (%)35.3234.8677.0680.73window30 (%)65.6069.7283.4991.28window45 (%)77.9880.2886.7095.41window60 (%)88.0785.3290.3798.17": ": Alignment Statistics. We report the tempo-ral offset statistics on 4 manually annotated test videos(comprising a total of 292 samples). and windowtrepresent the temporal offset and the percentage of com-mentaries that fall within a window of t seconds aroundthe visual key frames, respectively. ment pipeline, followed by a quantitative compari-son and analysis of the alignment results.Implementation Details. We use pretrained off-the-shelf CLIP ViT-B/32 model to extract visualand textual features for our alignment pipeline,which are then passed through two MLP layersto get 512-dim features for contrastive learning.We use the AdamW (Loshchilov and Hutter, 2019)optimizer and the learning rate is set to 5 104 to train the alignment model for 50 epochs.Evaluation Metrics. To evaluate temporal video-text alignment quality, we report various metrics on4 unseen videos (with 292 samples) from our cu-rated SN-Caption-test-align benchmark, includingthe average temporal offset (avg()), the averageabsolute temporal offset (avg(||)), and the per-centage of textual commentaries falling within 10s,30s, 45s, and 60s windows around each key frame.",
  "Rank = 32Baidu31.559.3326.5321.6242.007.23Rank = 64Baidu30.718.6326.3624.3235.337.35": ": Quantitative Comparison on Commentary Generation. All variants of SN-caption baseline methods, ourMatchVoice are retrained on both the original unaligned SoccerNet-Caption and our temporally aligned MatchTimetraining sets, while MatchVoice with LoRA applied on LLM decoder was trained on MatchTime training sets foronly. All the commentary models are evaluated on our manually curated SN-Caption-test-align benchmark. In eachunit, we denote the best performance in RED and the second-best performance in BLUE. effectively aligns visual content and textual com-mentary in a coarse-to-fine manner. Specifically,our approach reduces the average absolute off-set by 7.0s (from 13.89 seconds to 6.89 seconds)and significantly enhances the alignment of tex-tual commentary with key frames. It is importantto highlight that, in comparison to solely using acontrastive alignment model, incorporating datapre-processing enhances coarse alignment. Thisprovides a robust foundation for subsequent fine-grained alignment, consistently leading to furtherimprovements in performance. Furthermore, theproportion of commentary that aligns within a pre-cise 10-second window increases dramatically by45.41% (from 35.32% to 80.73%). Remarkably,nearly all (98.17%) textual commentaries now fallwithin a 60-second window surrounding the keyframes, underscoring the efficacy of our two-stagealignment pipeline.",
  "Soccer Commentary Generation": "In this part, we first detail on the implementationdetails and evaluation metrics of the commentarygeneration model. Then, we analyze the resultsfrom both quantitative and qualitative perspectives.Finally, we validate the effectiveness of the mod-ules through ablation experiments. Implementation Details. Our automatic commen-tary model can employ various visual features suchas C3D (Tran et al., 2015), ResNet (He et al., 2016),Baidu (Zhou et al., 2021), CLIP (Radford et al.,2021), and InternVideo (Wang et al., 2022). All vi-sual features are extracted from the video at 2FPS,except for InternVideo and Baidu, which are ex-tracted at 1FPS. The number of query vectors inthe temporal aggregator is fixed at 32, and the MLPprojection layer projects the aggregated featuresto a 768-dimensional prefix token that is then fedinto LLaMA-3 (AI@Meta, 2024) for decoding the",
  "a": ": Qualitative results on commentary generation. Our MatchVoice demonstrates advantages in multipleaspects: (a) richer semantic descriptions, (b) full commentaries of multiple incidents in a single video, (c) accuracyof descriptions, and (d) predictions of incoming events. trained on the two-stage aligned data exhibits thelargest performance improvement, which demon-strates the necessity of temporal alignment to boostcommentary generation quality.(iii) LoRA on LLMs Decoder. Given that theBaidu visual encoder pretrained on soccer datacould potentially boost performance, we furtherinvestigate the impact of fine-tuning the languagedecoder on soccer-specific data. Considering thehigh computational cost of fine-tuning the entireLLM, we introduce a small number of trainableLoRA (Hu et al., 2022) layers within the LLMsdecoder to capture the priors from soccer gamecommentary data. As presented in , intro-ducing these LoRA layers leads to notable perfor-mance improvements, highlighting the necessity ofleveraging soccer-specific priors within the dataset.",
  "Related Works": "Temporal video-text alignment aims to preciselyassociate textual descriptions or narratives withtheir corresponding video segments. Large-scaleinstructional videos such as HowTo100M (Miechet al., 2019) and YouCook2 (Zhou et al., 2018) havealready catalyzed extensive multi-modal alignmentworks based on vision-language co-training. Con-cretely, TAN (Han et al., 2022) directly aligns pro-cedure narrations transcribed through AutomaticSpeech Recognition (ASR) with video segments.DistantSup (Lin et al., 2022) and VINA (Mavroudiet al., 2023) further explore leveraging externalknowledge bases (Koupaee and Wang, 2018) toassist the alignment process, while Li et al. (2024c)propose integrating both action and step textual in-formation to accomplish the video-text alignment. In this paper, we train a multi-modal alignmentmodel to automatically correct existing data andbuild a higher-quality soccer game commentarydataset.Moreover, we further demonstrate thesuperiority and indispensability of our alignmentpipeline through downstream commentary tasks,confirming its critical significance.Video captioning has been a long-standing re-search challenge in computer vision (Krishna et al.,2017; Yang et al., 2023), primarily due to the lim-ited annotation and expensive computation. Bene-fiting from the development of LLMs, recent mod-els, such as LLaMA-VID (Li et al., 2024b) andVideo-LLaMA (Zhang et al., 2023) propose strate-gies for linking visual features to language prompts,harnessing the capabilities of LLaMA (Touvronet al., 2023a,b) models for video description. Fur-thermore, VideoChat (Li et al., 2023, 2024a) treatsvideo captioning as a subtask of visual questionanswering, while StreamingCaption (Zhou et al.,2024) can generate captions for streaming videosusing a memory mechanism.Notably, the AutoAD series (Han et al., 2023b,a, 2024) apply video captioning to a specific domain synthesizing descriptive narrations for moviescenes to assist visually impaired individuals inwatching movies. Similarly, in the context of soc-cer, a distinctive sports scenario, we develop a tai-lored soccer game commentary model to enrich theviewing experience for audiences.Sports video understanding (Thomas et al., 2017)has widely attracted the interest of researchersdue to its complexity and professional relevance.Early works such as FineGym (Shao et al., 2020)and FineDiving (Xu et al., 2022) aim to develop",
  ": Qualitative results on Temporal Alignment. For the same commentary text, original timestamps inSoccerNet-Caption are in Orange, those timestamps after alignment in MatchTime are in Green": "fine-grained datasets for action recognition andunderstanding in specific sports.Subsequently,focusing on soccer, a series of SoccerNet (Gi-ancola et al., 2018a) datasets systematically ad-dress various challenges related to soccer, includingplayer detection (Vandeghen et al., 2022), actionspotting (Giancola et al., 2018a), replay ground-ing (Held et al., 2023), player tracking (Cioppaet al., 2022), camera calibration (Giancola et al.,2018b) and re-identification (Deliege et al., 2021).These endeavours have paved the way for moreambitious research goals, such as utilizing AI forsoccer game commentary (Mkhallati et al., 2023;Qi et al., 2023). Additionally, other approacheshave targeted aspects of sports analysis, such asbasketball game narration (Yu et al., 2018) andtactics analysis (Wang et al., 2024).A concurrent work, SoccerNet-Echoes (Gautamet al., 2024) proposes to leverage audio from videosfor ASR and translation to obtain richer text com-mentary data. However, this approach overlooksthat unprocessed audios often contain non-game-related utterances, which may confuse model train-ing. Building upon the aforementioned progress,our goal is to construct a dataset with improvedalignment to train a more professional soccer gamecommentary model, thereby achieving a better un-derstanding of sports video.",
  "Conclusion": "In this paper, we consider a highly practical andcommercially valuable task: automatically gener-ating professional textual commentary for soccergames. Specifically, we have observed a preva-lent misalignment between visual contents andtextual commentaries in existing datasets. To ad-dress this, we manually correct the timestampsof textual commentary in 49 videos in the ex-isting dataset, establishing a new benchmark for the community, termed as SN-Caption-test-align.Building upon the manually checked data, we pro-pose a multi-modal temporal video-text alignmentpipeline that automatically corrects and filters ex-isting data at scale, which enables us to constructa higher-quality soccer game commentary dataset,named MatchTime. Based on the curated dataset,we present MatchVoice, a soccer game commen-tary model, which can accurately generate profes-sional commentary for given match videos, signifi-cantly outperforming previous methods. Extensiveexperiments have validated the critical performanceimprovements achieved through data alignment, aswell as the superiority of our proposed alignmentpipeline and commentary model.",
  "Limitations": "Although our proposed MatchVoice model cangenerate professional textual commentary for givensoccer game videos, it still inherits some limitationsfrom existing data and models: (i) Following pre-vious work, our commentary remains anonymousand cannot accurately describe player informationon the field. This is left for future work, where weaim to further improve the dataset and incorporateknowledge and game background information asadditional context; and (ii) MatchVoice may some-times struggle to distinguish between highly similaractions, such as corner kicks and free kicks. Thismainly stems from the current frozen pre-trainedvisual encoders and language decoders. Our pre-liminary findings suggest that fine-tuning on soccer-specific data might effectively address this issue inthe future.",
  "Max Bain, Jaesung Huh, Tengda Han, and Andrew Zis-serman. 2023. Whisperx: Time-accurate speech tran-scription of long-form audio. INTERSPEECH 2023": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization, pages 6572. Anthony Cioppa, Silvio Giancola, Adrien Deliege,Le Kang, Xin Zhou, Zhiyu Cheng, Bernard Ghanem,and Marc Van Droogenbroeck. 2022.Soccernet-tracking: Multiple object tracking dataset and bench-mark in soccer videos. In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 34913502. Adrien Deliege, Anthony Cioppa, Silvio Giancola,Meisam J Seikavandi, Jacob V Dueholm, Kamal Nas-rollahi, Bernard Ghanem, Thomas B Moeslund, andMarc Van Droogenbroeck. 2021. Soccernet-v2: Adataset and benchmarks for holistic understandingof broadcast soccer videos. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 45084519.",
  "FIFA. 2023. The football landscape the vision 2020-2023 | fifa publications": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and PengfeiLiu. 2024. Gptscore: Evaluate as you desire. InProceedings of the Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics. Sushant Gautam, Mehdi Houshmand Sarkhoosh, JanHeld, Cise Midoglu, Anthony Cioppa, Silvio Gian-cola, Vajira Thambawita, Michael A Riegler, PlHalvorsen, and Mubarak Shah. 2024.Soccernet-echoes: A soccer game audio commentary dataset.arXiv preprint arXiv:2405.07354. Silvio Giancola, Mohieddine Amine, Tarek Dghaily,and Bernard Ghanem. 2018a. Soccernet: A scal-able dataset for action spotting in soccer videos. InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition Workshops, pages17111721. Silvio Giancola, Mohieddine Amine, Tarek Dghaily,and Bernard Ghanem. 2018b. Soccernet: A scal-able dataset for action spotting in soccer videos. InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition Workshops, pages17111721. SilvioGiancolaandBernardGhanem.2021.Temporally-awarefeaturepoolingforactionspotting in soccer broadcasts. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 44904499. Tengda Han, Max Bain, Arsha Nagrani, Gul Varol,Weidi Xie, and Andrew Zisserman. 2023a. Autoadii: The sequel-who, when, and what in movie audiodescription. In Proceedings of the International Con-ference on Computer Vision, pages 1364513655. Tengda Han, Max Bain, Arsha Nagrani, Gl Varol,Weidi Xie, and Andrew Zisserman. 2023b. Autoad:Movie description in context. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 1893018940. Tengda Han, Max Bain, Arsha Nagrani, Gl Varol,Weidi Xie, and Andrew Zisserman. 2024. Autoadiii: The prequel - back to the pixels. In Proceed-ings of the IEEE Conference on Computer Vision andPattern Recognition, pages 1816418174.",
  "Tengda Han, Weidi Xie, and Andrew Zisserman. 2022.Temporal alignment networks for long-term video.In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 29062916": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 770778. Jan Held, Anthony Cioppa, Silvio Giancola, AbdullahHamdi, Bernard Ghanem, and Marc Van Droogen-broeck. 2023. Vars: Video assistant referee systemfor automated soccer decision making from multi-ple views. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages50855096.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Longshort-term memory. Neural Computation, 9(8):17351780": "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In Proceedings of the Inter-national Conference on Learning Representations. Andrew Jaegle, Felix Gimeno, Andy Brock, OriolVinyals, Andrew Zisserman, and Joao Carreira. 2021.Perceiver: General perception with iterative attention.In Proceedings of the International Conference onMachine Learning, pages 46514664. PMLR.",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In Proceedings of theInternational Conference on Learning Representa-tions": "Effrosyni Mavroudi, Triantafyllos Afouras, and LorenzoTorresani. 2023. Learning to ground instructionalarticles in videos through narrations. In Proceedingsof the IEEE Conference on Computer Vision andPattern Recognition, pages 1520115213. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,Makarand Tapaswi, Ivan Laptev, and Josef Sivic.2019. Howto100m: Learning a text-video embed-ding by watching hundred million narrated videoclips. In Proceedings of the International Confer-ence on Computer Vision, pages 26302640. Hassan Mkhallati, Anthony Cioppa, Silvio Giancola,Bernard Ghanem, and Marc Van Droogenbroeck.2023. Soccernet-caption: Dense video captioningfor soccer broadcasts commentaries. In Proceedingsof the IEEE Conference on Computer Vision and Pat-tern Recognition Workshops, pages 50745085.",
  "Ji Qi, Jifan Yu, Teng Tu, Kunyu Gao, Yifan Xu, XinyuGuan, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi": "Li, et al. 2023. Goal: A challenging knowledge-grounded video captioning benchmark for real-timesoccer commentary generation. In Proceedings of the32nd ACM International Conference on Informationand Knowledge Management, pages 53915395. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In Proceedings of theInternational Conference on Machine Learning. Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. 2020.Finegym:A hierarchical video dataset for fine-grained action understanding. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 26162625. Graham Thomas, Rikke Gade, Thomas B Moeslund,Peter Carr, and Adrian Hilton. 2017. Computer vi-sion for sports: Current applications and researchtopics. Computer Vision and Image Understanding,159:318. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-resani, and Manohar Paluri. 2015. Learning spa-tiotemporal features with 3d convolutional networks.In Proceedings of the International Conference onComputer Vision, pages 44894497. Renaud Vandeghen, Anthony Cioppa, and MarcVan Droogenbroeck. 2022. Semi-supervised trainingto improve player and ball detection in soccer. InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 34813490. Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 45664575. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, BingkunHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,Zun Wang, et al. 2022. Internvideo: General videofoundation models via generative and discriminativelearning. arXiv preprint arXiv:2212.03191.",
  "Bachrach, Romuald Elie, Li Kevin Wenliang, Fed-erico Piccinini, et al. 2024. Tacticai: an ai assistantfor football tactics. Nature Communications, 15(1):113": "Jinglin Xu, Yongming Rao, Xumin Yu, Guangyi Chen,Jie Zhou, and Jiwen Lu. 2022. Finediving: A fine-grained dataset for procedure-aware action qualityassessment. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages29492958. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-toine Miech, Jordi Pont-Tuset, Ivan Laptev, JosefSivic, and Cordelia Schmid. 2023. Vid2seq: Large-scale pretraining of a visual language model for densevideo captioning. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition,pages 1071410726. Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang,Jian Zhang, and Xiaokang Yang. 2018. Fine-grainedvideo captioning for sports narrative. In Proceedingsof the IEEE Conference on Computer Vision andPattern Recognition, pages 60066015. Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-tuned audio-visual languagemodel for video understanding. In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processinng.",
  "A.1Dataset Split": "We split the total 471 matches of our dataset (in-cluding automatically aligned MatchTime andmanually curated SN-Caption-test-align bench-mark) into training (373 matches), validation (49matches), and test (49 matches) sets, consistingof 26,058, 3,418, and 3,267 video clip-text pairs,respectively. Notably, all test samples are from ourmanually checked SN-Caption-test-align, whichserves as a better benchmark on soccer game com-mentary generation for the community.",
  "In this section, we provide additional details regard-ing the implementations as follows": "Baseline Methods. For baselines, we retrain sev-eral variants of SN-Caption (Mkhallati et al., 2023)with its official implementation. NetVLAD++ (Gi-ancola and Ghanem, 2021) is adopted to aggre-gate the temporal information of the extracted fea-tures. Then the pooled features are decoded by anLSTM (Hochreiter and Schmidhuber, 1997). Event Summarization. Considering that the nar-rations by commentators may be fragmented andcolloquial, we feed the ASR-generated narrationtexts into the LLaMA-3 (AI@Meta, 2024) modeland use the following prompt to summarize theminto event descriptions for every 10 seconds: \"I will give you an automatically recognizedspeech with timestamps from a soccer gamevideo. The narrator in the video is comment-ing on the soccer game. Your task is to summa-rize the key events for every 10 seconds, eachcommentary should be clear about the personname and soccer terminology. Here is thisautomatically recognized speech: \\n \\n {times-tamp intervals: ASR sentences} \\n \\n You needto summarize 6 sentence commentaries for 0-10s, 10-20s, 20-30s, 30-40s, 40-50s, 50-60saccording to the timestamps in automaticallyrecognized speech results, every single sen-tence commentary should be clear and consiseabout the incidents happened within that 10seconds for around 20-30 words. Now pleasewrite these 6 commentaries.\\n Answer:\"",
  "Timestamp Prediction. With the event descrip-tions and their corresponding timestamps, we in-put them along with the textual commentaries into": "LLaMA-3 (AI@Meta, 2024) to predict the times-tamps for the textual commentaries based on sen-tence similarity, providing a solid foundation forfine-grained alignment. The prompt used for thisstep is as follows: \"I have a text commentary of a soccer gameevent at the original time stamp: \\n \\nOrig-inal timestamp here: {Original commentaryhere (from SoccerNet-Caption)} \\n \\n and Iwant to locate the time of this commentaryamong the following events with timestamp:\\n {timestamp intervals of 10s: summarizedevents}. \\n These are the words said by nar-rator and I want you to temporally align thefirst text commentary according to these wordsby narrators since there is a fair chance thatthe original timestamp is somehow inaccuratein time. So please return me with a numberof time stamp that event is most likely to hap-pen. I hope that you can choose a numberof time stamp from the ranges of candidates.But if really none of the candidates is suitable,you can just return me with the original timestamp. Your answer is:\"",
  "A.3Evaluation Metrics": "In this paper, most evaluation metrics (BLEU (Pap-ineni et al., 2002), METEOR (Banerjee and Lavie,2005), ROUGE-L (Lin, 2004), CIDEr (Vedantamet al., 2015)) are calculated using the same functionsettings with SoccerNet-Caption (Mkhallati et al.,2023), by the implementation of pycocoevalcaplibrary. GPT-score (Fu et al., 2024) is given byGPT-3.5 with the following text as prompt: \"You are a grader of soccer game commen-taries. There is a predicted commentary byAI model about a soccer game video clip andyou need to score it comparing with groundtruth. \\n \\n You should rate an integer scorefrom 0 to 10 about the degree of similaritywith ground truth commentary (The higher thescore, the more correct the candidate is). Youmust first consider the accuracy of the soccerevents, then to consider about the semantic in-formation in expressions and the professionalsoccer terminologies. The names of playersand teams are masked by \"[PLAYER]\" and\"[TEAM]\". \\n \\n The ground truth commen-tary of this soccer game video clip is: \\n \\n\"{Ground truth here.}\" \\n \\n I need you to rate",
  ": Alignment Results of Different Windows": "As depicted in , we have experimentedwith sampling windows of different lengths andobserved that using a 120-second window aroundthe manually annotated ground truth (i.e., 60 sec-onds before to 60 seconds after) can yield optimalalignment performance. Specifically, for each textcommentary, we treat the key frame correspondingto its ground truth timestamp as the positive sam-ple, while other samples within a fixed window size,sampled at 1 FPS, serve as negative samples (i.e.,those within 5 to 60 seconds temporal distance tothe ground truth timestamp).Considering that data pre-processing based onASR and LLM provides a coarse alignment andthat there might be replays in soccer game videos,during the inference stage, we use key frames from45 seconds before to 30 seconds after the currenttextual commentary timestamp as candidates.",
  "A.5Divergence Among Annotators": "Although the recruited volunteers are all footballenthusiasts, there exists noticeable subjectivity andvariability in manual annotations due to differentunderstandings of soccer terminology and actions.Toquantifythis,threevolunteersareaskedtoannotatetwomatchesfromourSN-Caption-test-align benchmark. We observean alignable/unalignable disagreement amongdifferent annotators on 6.29% of the samples.Additionally, the average of maximum discrepancybetween the timestamps provided by differentannotators is 5.57 seconds, which can somehow",
  "MatchVoice:": "[PLAYER] ([TEAM]) latches on to a precise pass on the edge of the box and unleashes a low drive towards the middle of the goal. [PLAYER] pulls off a comfortable save. The ball goes out of play and [TEAM] will have a goal kick.[PLAYER] ([TEAM]) shoots from the edge of the penalty area. The ball travels towards the bottom right corner, but [PLAYER] easily deals with the threat.GT:"
}