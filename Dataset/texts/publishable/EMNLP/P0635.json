{
  "Abstract": "Autoformalization is the task of automaticallytranslating mathematical content written in nat-ural language to a formal language expression.The growing language interpretation capabil-ities of Large Language Models (LLMs), in-cluding in formal languages, are lowering thebarriers for autoformalization. However, LLMsalone are not capable of consistently and reli-ably delivering autoformalization, in particularas the complexity and specialization of the tar-get domain grows. As the field evolves intothe direction of systematically applying auto-formalization towards large mathematical li-braries, the need to improve syntactic, termino-logical and semantic control increases. This pa-per proposes the coordinated use of three mech-anisms, most-similar retrieval augmented gen-eration (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. Theempirical analysis, across different models,demonstrates that these mechanisms can de-liver autoformalizaton results which are syn-tactically, terminologically and semanticallymore consistent. These mechanisms can be ap-plied across different LLMs and have shown todeliver improve results across different modeltypes.1",
  "Introduction": "Mathematical reasoning constitutes an essential as-pect of human intelligence (Saxton et al., 2019;Lu et al., 2023). It centers on symbolic-level rea-soning, as manifested through systematic, abstractand and step-wise logical inference. Mathemati-cal reasoning models has been clustered under twotypes: deep learning models (Hendrycks et al.,2021; Wei et al., 2022; Meadows and Freitas, 2023;Liu et al., 2023) and formal models (Polu and",
  "Codeanddatasetsareavailableat": "Sutskever, 2020; Wang and Deng, 2020; Han et al.,2022; Jiang et al., 2022, 2023b). Mathematicalreasoning in Large Language Models (LLMs) pre-dominantly uses statements expressed in informalmathematical statements. More recent models haveaimed towards bridging both informal and formalmathematical reasoning (Wu et al., 2022; Firstet al., 2023; Azerbayev et al., 2023; Quan et al.,2024a), where the material (content-based) infer-ence strengths of LLMs are complemented by ex-ternal formal/symbolic reasoning methods such asautomated theorem provers (e.g. Isabelle (Paulson,2000) and Lean (de Moura et al., 2015)), which cansystematically assess the logical validity of the rea-soning process (Wu et al., 2022), facilitating LLMsto perform controlled and consistent inference.However, formal and verifiable mathematicalreasoning with theorem provers requires the man-ual formalization of logical formulae from informalstatements, in order to build the supporting math-ematical libraries, knowledge bases (KBs) whichexpress previous axioms, definitions, theorems andproofs, a process that demands considerable ef-fort and domain-specific knowledge. A prototyp-ical case in point is the liquid tensor experiment(Scholze, 2022), an initiative aimed at formaliz-ing analytical geometry results from Scholze &Clausen, requiring a community coordinated effortof experts.Contemporary LLMs have demonstrated consid-erable efficacy (Wu et al., 2022; Xin et al., 2023;First et al., 2023) for supporting autoformalizationefforts within an in-context learning paradigm, be-ing largely evaluated in less specialized domainsand tasks. Existing methods are still limited indelivering a method for systematically and consis-tently building large formal and specialized mathe-matical libraries. The essence of the challenge istwofold: (i) specialization and out-of-distribution(OOD) drifts: as one moves towards more spe-cialized and newer domains to be autoformalized, : The overall framework consists of three stages: Stage 1 contains one round for retrieval augmentedautoformalization; Stage 2 contains one round for denoising; Stage 3 is composed of several iterative roundsto refine the code based on syntax errors. For better illustration, we change \\<in>, \\<nat>, \\<lsq>, $+$ to theirLaTeX version , N, , +. The ground truth code is lemma (in int0) Int_ZF_1_5_L7A: assumes \"a\\<in>\\<int>\"\"b \\<in>\\<int>\\<sub>+\"shows \"a \\<lsq>a\\<ra>b\" \"a \\<noteq>a\\<ra>b\" \"a\\<ra>b \\<in>\\<int>\" (assumes\"a Z\" \"b Z+\" shows \"a a + b\" \"a = a + b\" \"a + b Z\"). models are progressively exposed to more chal-lenging OOD cases, and (ii) library consistencyand coherence: new formalized need to be consis-tently built-up on previously statements, coheringterminologically, syntactically and semantically. This work targets the overarching research ques-tion: how to systematically support the creationof consistent and coherent formal mathematical li-braries from informal mathematical statements?.In order to address this task, we decompose thisbroader aim into the following research questions:RQ1: To what extent contemporary LLMs arecapable of formalizing specialized mathematicalstatements into formal representations for mathe-matical libraries?; RQ2: Which metrics can beused to assess the quality of the formalized out-puts?; RQ3: Which mechanisms can be used toextend the autoformalization properties of LLMsto achieve better generative control and enhanceterminological, syntactic and semantic consistencyand coherence?. To address these research ques-tions, we propose a novel framework (See ) that leverages LLMs with most-similar retrievalaugmented generation (MS-RAG), denoising stepsand iterative feedback-guided syntax error refine-ment cycles (Auto-SEF) to deliver a syntacticallyconsistent and semantically coherent autoformal-ization.",
  "To assess the effectiveness of our proposedframework, we construct a supporting dataset forthe task of mathematical library autoformalization(MathLibForm) and build a supporting empirical": "analysis methodology guided by a critical selectionof a set of automated metrics. We conduct a sys-tematic empirical analysis with a diverse sampleof state-of-the-art LLMs, in order to compare andcontrast their autoformalization properties and theimpact of the proposed library autoformalizationmechanisms. Our results demonstrate that leverag-ing LLMs with MS-RAG and Auto-SEF, combinedwith denoising strategies, can significantly enhancethe syntactic correctness of formalization results,reaching improvements from 5.47% to 33.58%. Insummary, the contributions of the paper are: 1. Proposal of a novel neuro-symbolic frame-work targeting the autoformalization of math-ematical libraries, which employs LLMs withMS-RAG, denoising and Auto-SEF to consis-tently and iteratively enhance and refine theformalization results;",
  "Proposed Approach": "In this section, we start by defining the target taskand then describe the proposed mechanisms forimproving autoformalization.Autoformalization: An autoformalization is atransformation function which maps an informalmathematical statement s in the domain of natural language and LaTeX symbols S into a formal math-ematical statement , under a formal language F,f : S F, such that for every s S, there existsa F where f(s) = .Semantic correctness: A transformation f(s) = is semantically correct if there exists a model Msuch that:",
  "M:M |= sandM |= ,": "where |= denotes that the former item satisfies orcorrectly interprets the latter.Library-based autoformalization:Given aKnowledge Base (KB) of formalised mathematicalstatements under a formal language F, a library-based autoformalization transformation functionf is defined such that the generated statement issemantically consistent with the set of statements KB.Semantic consistency: A statement is seman-tically consistent with respect to KB if all termsin that have references in KB are used consis-tently with the terms in KB. Formally, let bea statement and KB be a knowledge base. issemantically consistent with respect to KB if:",
  "Denoising Formalization Results": "Bias inherited from instruction fine-tuning (Ouyanget al., 2022) causes LLMs during autoformaliza-tion to occasionally generate redundant texts notintegral to the formal statement, thereby infusingthe final output with noisy information. Conse-quently, the direct output of LLMs frequently failsto meet the criteria for a valid formal code. Pleasenote that despite the fact that output conditions canbe communicated on the initial prompt, typicallythe output behaviour of the models cannot be fullycontrolled, nor fully enforceable. To alleviate thisissue, we propose two types of denoising:Code-Based Denoising (CBD). Definition of a setof post-processing rules R to remove irrelevantoutputs such as extra explanations and unsolicitedproofs , where a new formal statement is obtained:d(s) = R(f(s)).Prompt-Based Denoising (PBD). The rigidityof a CBD method can be contrasted to a post-hoc prompt-based approach for the same purpose.Hence, we propose the design of a prompt pdenwhich performs the denoising of the autoformal-ization results.Denoising with only a promptraises the risk of losing semantic consistency be-cause of the bias in the training data of LLMs.Therefore, the set of retrieved items MS(s) fromMS-RAG could be used to maintain semanticconsistency.The denoising becomes: d(s) =LLM(pden, {(si, i)}s, f(s)).Using reported syntax errors as a feedback havebeen established as a systematic mechanism forguiding the correction of formal models (Quanet al., 2024a,b) for LLMs potentially automaticallycorrect the formalization results. In contrast, PBDand CBD provides a template-based/prescribedmechanism for output control.",
  "Auto-correction with Syntax ErrorFeedback (Auto-SEF)": "The validity of a formal code can be checkedby a theorem prover T P that supports its writtenformal language F. If the formal code is not valid,the theorem prover can output a set of syntax er-rors {ek} = T P(). Using reported syntax errors as feedback has been established as a systematicmechanism for guiding the correction of formalmodels (Quan et al., 2024a,b), potentially allow-ing LLMs to automatically correcting the resultsof formalization. Hence, we design a prompt perrto add an auto-correction component to let LLMsrecognize previously produced errors and correctmistakes. To maintain semantic consistency, re-trieved examples are also used and the generationbecomes:",
  "MathLibForm": "Formal mathematical datasets, such as miniF2F(Zheng et al., 2022), predominantly concentrate ondistinct mathematical problems representing sim-pler mathematical solving tasks. In contrast, thecreation of mathematical libraries demands the aut-oformalization of statements which can be morespecialized, conceptually more complex and poten-tially out-of-distribution. In this work we use Isar-MathLib2, as a reference setting within the environ-ment of the Isabelle/ZF theorem prover framework.Formal statements in IsarMathLib are frequentlyaccompanied by textual comments, which serve asthe corresponding natural language statements ofthe formal expressions. Mathematical items, suchas lemma, definition, corollary, theorem, along withtextual comments and proofs, were first systemat-ically extracted via a script. This led to a total of2,744 items, which were then randomly dividedinto training and test sets in a 90% to 10% split,resulting in 2,470 training samples and 274 testsamples for constructing the MathLibForm dataset.To enrich the information contained in MathLib-Form, we also informalize formal statements withMistral and add the generated textual descriptions.The training and testing sets are used to define theknowledge base KB and the evaluation.",
  "Evaluation Metrics": "Assessing the overall correctness of autoformal-ized code outputs requires resource intensivespecialized/expert-level human feedback. In ad-dition, human evaluations can become largely sub-jective in situations where the assessment criteriais too complex to be elicited (i.e., the validationprocess cannot be systematized into a protocol),which, we argue, is the case for autoformalization.The dependency on multiple human validators withskills in both theorem provers and the underlyingmulti-domain mathematics makes this problem par-ticularly severe.We mentioned semantic correctness and consis-tency in as the final desirable proper-ties as an outcome of an autoformalization process,which can become too strict for evaluating auto-formalization tasks with current LLMs. Therefore,in this work, we propose two distinct proxies toassess code correctness: semantic similarity andsyntactic correctness. Utilizing the ground truth asa reference, we measure semantic similarity usingpairwise metrics, including BLEU (Papineni et al.,2002), ChrF (Popovic, 2015), RUBY (Tran et al.,2019), and CodeBERTScore (CBS) (Zhou et al.,2023). The description of these metrics are pro-vided in the Appendix. For syntactic correctness,we use the Isabelle theorem prover to detect syntaxerrors in formal statements and use the Pass metricwhich represents the success rate at which the gen-erated formal statement does not exhibit any syntaxerrors, as verified by the theorem prover. The in-tegration between the transformer and Isabelle isdone on a ToolFormer setting with the support ofan Isabelle client3 (Shminke, 2022).",
  "Retrieval Augmented Autoformalization": "We establish baselines in zero-shot and 3-shotsettings on several state-of-the-art LLMs: Mis-tral (Jiang et al., 2023a), Llemma 7B (Azerbayevet al., 2024), Mixtral (Jiang et al., 2024a), GPT-3.5-Turbo (descriptions of the models can be foundin the Appendix). The inclusion criteria for the se-lected foundation models prioritized: (i) samplediversity across the three modalities (model size,type, and specialization level), leading to 4 base-line foundation models; and (ii) a priority on openmodels, where the underlying modeling strategies",
  "GPT-3.5-TurboQuery: T Index: T+S37.1158.5657.7178.8962.77": ": Autoformalization results for different settings. BM25 retriever is used to retrieve Top-3 most similarsamples for retrieval augmented autoformalization. Greedy decoding is used in generation for reproducibility.Code-based denoising is applied to all outputs. The query used to retrieve relevant exemplars includes: (T): naturallanguage textual description; (ZS): zero-shot autoformalization result from Mistral. The index used for knowledgebase has the following options: (T): natural language textual description; (I): informalization of formal statementgenerated from Mistral; (S): formal statement. The setting with highest scores is highlighted in bold. are more transparent.For MS-RAG, BM25 (Robertson et al., 1994)is used as the primary ranking function to retrieveTop-k (k=3) most similar samples for exemplars(BM25 will concentrate a terminological similar-ity function). Different settings are contrasted forquerying and indexing the reference KB. There aretwo choices for query: 1. natural language textualdescription; 2. description along with zero-shotautoformalization result from Mistral. The choicesfor indexing KB elements combine three contentsources: 1. natural language textual description;2. informalization of formal statements; 3. formalstatements. For this specific analysis, we constrainthe foundation model to Mistral. All results arereported in .MS-RAG can improve autoformalization inmathematical libraries settings. As shown in, for the same type of LLMs, using retrievedexamples rather than fixed examples leads to an improvement in both semantic similarity and syn-tactic correctness of the generated formal state-ments. This mechanism can lift the performanceof smaller models: e.g. as a smaller model, Mis-tral (7B) with MS-RAG can outperform Mixtral(87B) with standard prompting across all metricsand is comparable to GPT-3.5 (175B) without MS-RAG according to some metrics such as RUBY.Similarity-based few-shot outperforms zero-shot learning. For all LLMs, autoformalizationresults with 3-shot exemplars are generally betterthan those from the zero-shot setting in terms ofsemantic similarity metrics. For syntactic correct-ness, Llemma 7B and GPT-3.5 in the zero-shotsetting have slightly higher pass rates compared tothe 3-shot setting.MS-RAG levels the playing field across modelsof different scales. As the largest LLM in thisexperimental setting, GPT-3.5 with MS-RAG sig-nificantly outperforms all other models. However,",
  ": The effect of denoising on Mistral. The change of scores after applying CBD is recorded in round brackets.The setting with highest final scores is marked in bold": "comparing its best performance with MS-RAGto its performance in the 3-shot setting, its rela-tive change in syntactic correctness (67%) is muchlower than that with Mistral (307%). The relativechange for Llemma 7B is the smallest (62%). Weattribute this to the fact that Llemma was not fine-tuned with instructions. These differences suggestthat smaller LLM with instruction tuning benefitsmore from RAG.Augmenting the index with auto-informalizationor the query with zero-shot auto-formalizationdoes not lead to better retrieval. Among all re-sults in , GPT-3.5 with textual descriptionquery and textual description index achieves high-est scores in four metrics except BLEU-2. Thisquery and index combination setting is also an op-timal choice for Mistral, as this setting leads tohighest scores in ChrF, RUBY and CBS and sec-ond highest scores in Pass. Incorporating zero-shotresults from Mistral as queries generally yieldsworse results compared to its counterpart. This isprobably caused by the low quality of zero-shot for-malization results. The application of informalizeddescriptions during indexing also does not lead toa performance improvement.",
  "Output Denoising": "In this section, we investigate the impact of denois-ing. We select the result of MS-RAG (Query: T,Index: T) to apply PBD with four prompts: (1A)The prompt only contains instructions to remove ex-planations and proofs; (1B) 1A adds an additionalinstruction for stylistic alignment to declare that thefinal output after refinement should maintain thesame syntactic style; (1C) Includes some fixed for-mal statement examples for the stylistic alignmentinstruction in 1B; (1D) Changes the fixed examplesin 1C to retrieved examples from MS-RAG. Werecord the results of Mistral in .Denoising significantly impacts the quality of theformal statements. Compared to results without denoising, using either denoising method can sig-nificantly improve BLEU and RUBY scores. Ap-plying CBD to the original MS-RAG results canlead to an improvement across metrics. However,the effect of CBD decreases after we apply PBD tothe results. For the Pass metric, performing CBDafter PBD had no observable impact. This demon-strates the impact of PBD as a syntactic controlmechanism. Our results suggest that a compositionof PBD and CBD can yield the best performance insyntactic correctness while maintaining semanticsimilarity at the same or higher level.Denoising can reduce the performance gap be-tween smaller and larger LLMs. We also con-ducted similar experiments on GPT-3.5 (results inAppendix). Denoising methods have a compara-tively lower effect on the results of GPT-3.5, serv-ing more as a function of control for smaller mod-els, approaching their performance to larger mod-els.Stylistic alignment is necessary when applyingPBD. Without the explicit declaration of stylisticalignment (1A), the syntactic correctness drops10.58% compared to the results of MS-RAG. Thereason is that when only prompted to remove re-dundant strings, some models tend to neglect theoriginal syntactic style of the formal statementsand rewrite them in the style that it was trainedon. However, merely specifying that the modelshould maintain such a style without explicit ex-amples (1B) does not effectively communicate theintent to preserve the style. This is demonstratedby the higher performance of 1C compared to 1B.In addition, using retrieved examples (1D) ratherthan fixed examples (1C) can further improve theresults. Case StudyThe example in communi-cates the necessity of denoising. As shown in Ta-ble 3, both 3-shot and MS-RAG results includean additional textual description in the final outputwhich does not form a formal statement. PBD 1A",
  ": An example using Mistral shows that only MS-RAG and PBD 1D have no syntax errors of formalization": "changes \\<in> into :: which is another way ofexpressing but this expression is not providedin its prompt, so this behaviour is highly likely tobe an inherited bias. PBD 1B and 1C mitigate thisbehaviour but they also introduce other syntacticerrors, such as the missing word of or the spe-cial character `. Only PBD 1D maintains thevalidity of the formal statement because the sim-ilarity of the retrieved examples and are thusemphasized during generation.",
  "Iterative Symbolic Refinement": "In this section, we mainly focus on answering thequestion on whether syntactic errors can be cor-rected by LLMs in coordination with symbolicsolvers. This process is iteratively run for up tonine cycles. To better illustrate the changes, weplot the scores of each iteration on the Pass metricin .Iterative Auto-SEF improves syntactic correct-ness of the formalization results. As shown in, both GPT-3.5 and Mistral can receive im-provements from the iterative Auto-SEF method.This result demonstrates that Auto-SEF can indeedenable LLMs to fix some syntactic errors. The",
  "first iteration brings the largest increase (2.56% forMistral, 4.38% for GPT-3.5) in pass rate. Afterthat, the change becomes smoother and iterative": "improvements are limited to a small number ofcycles.Smaller LLM tends to trade-off semantic sim-ilarity for syntactic correctness when applyingAuto-SEF. We focus on BLEU-2 as a proxy forsemantic similarity and illustrate the scores of eachiteration in .The BLEU-2 scores forGPT-3.5 remain steady across different iterations,whereas for Mistral, the scores decrease in the firstfew iterations. Combining this result with the im-provement in pass rate, we hypothesize that a trade-off occurs due to the comparatively lower capacityof the model to perform syntactic correction whilecontrolling for semantic drifting during the Auto-SEF prompting.",
  "A Critique of the Metrics": "Evaluation metrics for autoformalization can dis-agree with each other (Evtikhiev et al., 2023). Weuse all the results under CBD to calculate the Pear-son product-moment correlation coefficients acrossthe metrics, illustrating these coefficients with aheatmap in .RUBY can serve as an initial metric when eval-uating formalization results. All correlation co-efficients are larger than 0.6. This suggests that allmetrics are positively related to each other and thatany one of them is a reasonable indicator for evalu-ating formalization results. Among these metrics,RUBY has the strongest correlation (> 0.85) withthe other metrics.Pass and BLEU metrics should be jointly usedto prevent evaluation bias. Some zero-shot resultsin lead to a high score on the Pass metricbut lower scores on other metrics due to internalLLM style biases. According to , amongmetrics for semantic similarity, BLEU-2 has thestrongest correlation with the Pass metric and hencecan indicate syntactic correctness to some extent.We suggest considering both BLEU scores and Passrate when comparing results.",
  "Related Work": "AutoformalizationAutoformalization bridgesthe gap between natural language and formal lan-guage. Cunningham et al. (2022) trained a stan-dard transformer for proof autoformalization inCoq. Lu et al. (2024) proposed a process-drivenframework for autoformalization in Lean4. Withthe increased inference capabilities of LLMs in re-cent years, Wu et al. (2022); Jiang et al. (2023b) BLEU-2ChrFRUBYCBSPass PassCBSRUBYChrFBLEU-2 0.940.790.860.681 0.760.980.9210.68 0.920.9710.920.86 0.8610.970.980.79 10.860.920.760.94",
  "employ LLMs to autoformalize mathematical con-tents to Isabelle/HOL. Building on this foundation,our work also focuses on the improvement of auto-formalization capabilities over LLMs": "Retrieval Augmented Generation (Lewis et al.,2020)RAG has demonstrated improvements forcode generation (Lu et al., 2022; Zhang et al., 2023)and in formal settings, Yang et al. (2023) traineda retrieval-augmented language model for formalpremise selection and theorem proving. Mean-while, our work focuses on utilizing RAG for thetask of improving autoformalization performanceand coherence with respect to mathematical li-braries. LLMs RefinementThrough feedback-guidedrefinement strategies LLMs can perform self-correction (Pan et al., 2024).Recent stud-ies (Madaan et al., 2023; Quan et al., 2024a) eval-uate strategies using iterative feedback to refineLLM-generated answers for downstream tasks. Pre-vious work has used error messages generated bytheorem provers as a mechanism to support theinterface between LLMs and formal models (Panet al., 2023; Quan et al., 2024a; Jiang et al., 2024b;Quan et al., 2024b) and also repair models (Firstet al., 2023) to address syntactic or proof errors.Similarly, our work applies prompt-based refine-ment from external feedback error messages gen-erated by Isabelle/ZF to iteratively refine the for-malized logical forms with specific error code lo-cations.",
  "Conclusion": "This paper examined the effects of using RAGfor autoformalization with LLMs and exploredmethods to refine formalization results. Our ex-periments demonstrated the effectiveness of incor-porating a retrieval process for autoformalization.Further experiments indicated that denoising anditeratively refining syntax errors can enhance aut-oformalization quality. We evaluated results ondifferent LLMs and found that smaller LLMs withinstruction fine-tuning benefited more from the pro-posed methods, pointing in the direction of servingas a mechanism for reducing the formal perfor-mance gaps between larger and smaller models.We also built a dataset and assessed metrics forevaluating autoformalization, which could serve asresources/methodological contributions for formalmathematical reasoning tasks. We believe combin-ing the semantic similarity metrics with the syn-tactic correctness metric is a reasonable proxy forsemantic correctness.",
  "Limitations": "Some natural language statements in our datasetare too general or informal, failing to provide a sub-stantial content for autoformalization. Althoughour proposed framework, Auto-SEF, enhances syn-tactic control in autoformalization, increasing itera-tions do not yield significant improvements in thePass metric. This limitation is due to the inabilityof LLMs to generate syntactically correct complexformal representations. ZhangirAzerbayev,BartoszPiotrowski,HaileySchoelkopf, Edward W. Ayers, Dragomir Radev, andJeremy Avigad. 2023. Proofnet: Autoformalizingand formally proving undergraduate-level mathemat-ics. Preprint, arXiv:2302.12433. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,Marco Dos Santos, Stephen Marcus McAleer, Al-bert Q. Jiang, Jia Deng, Stella Biderman, and SeanWelleck. 2024. Llemma: An open language modelfor mathematics. In The Twelfth International Con-ference on Learning Representations. Steven Bird and Edward Loper. 2004. NLTK: The natu-ral language toolkit. In Proceedings of the ACL In-teractive Poster and Demonstration Sessions, pages214217, Barcelona, Spain. Association for Compu-tational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Garett Cunningham, Razvan Bunescu, and DavidJuedes. 2022. Towards autoformalization of math-ematics and code correctness: Experiments with el-ementary proofs. In Proceedings of the 1st Work-shop on Mathematical Natural Language Processing(MathNLP), pages 2532, Abu Dhabi, United ArabEmirates (Hybrid). Association for ComputationalLinguistics. Leonardo de Moura, Soonho Kong, Jeremy Avigad,Floris van Doorn, and Jakob von Raumer. 2015. Thelean theorem prover (system description). In Auto-mated Deduction - CADE-25, pages 378388, Cham.Springer International Publishing.",
  "Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov,and Timofey Bryksin. 2023. Out of the bleu: Howshould we assess quality of the code generation mod-els? J. Syst. Softw., 203(C)": "Emily C First, Markus Norman Rabe, Talia Ringer, andYuriy Brun. 2023. Baldur: Whole-proof generationand repair with large language models. Proceedingsof the 31st ACM Joint European Software Engineer-ing Conference and Symposium on the Foundationsof Software Engineering. Jesse Michael Han, Jason Rute, Yuhuai Wu, EdwardAyers, and Stanislas Polu. 2022. Proof artifact co-training for theorem proving with language models.In International Conference on Learning Representa-tions. Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, andJacob Steinhardt. 2021. Measuring mathematicalproblem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track (Round 2). Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,",
  "Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023a. Mistral 7b. Preprint,arXiv:2310.06825": "Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024a.Mixtral of experts. Preprint, arXiv:2401.04088. Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski,Konrad Czechowski, Tomasz Odrzygzdz, PiotrMi os, Yuhuai Wu, and Mateja Jamnik. 2022. Thor:Wielding hammers to integrate language models andautomated theorem provers. In Advances in NeuralInformation Processing Systems, volume 35, pages83608373. Curran Associates, Inc. Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,Timothee Lacroix, Jiacheng Liu, Wenda Li, MatejaJamnik, Guillaume Lample, and Yuhuai Wu. 2023b.Draft, sketch, and prove: Guiding formal theoremprovers with informal proofs. In The Eleventh Inter-national Conference on Learning Representations.",
  "Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen.2024b. Leanreasoner: Boosting complex logical rea-soning with lean. Preprint, arXiv:2403.13312": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela. 2020.Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-mation Processing Systems, volume 33, pages 94599474. Curran Associates, Inc. Jiayu Liu, Zhenya Huang, ChengXiang Zhai, and Qi Liu.2023. Learning by applying: A general frameworkfor mathematical reasoning via enhancing explicitknowledge learning. Proceedings of the AAAI Con-ference on Artificial Intelligence, 37(4):44974506.",
  "Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-wonHwang, and Alexey Svyatkovskiy. 2022. ReACC:": "A retrieval-augmented code completion framework.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 62276240, Dublin, Ireland.Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,Shashank Gupta, Bodhisattwa Prasad Majumder,Katherine Hermann, Sean Welleck, Amir Yazdan-bakhsh, and Peter Clark. 2023.Self-refine: It-erative refinement with self-feedback.Preprint,arXiv:2303.17651. Jordan Meadows and Andr Freitas. 2023. Introduc-tion to mathematical language processing: Informalproofs, word problems, and supporting tasks. Trans-actions of the Association for Computational Linguis-tics, 11:11621184. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744.Curran Associates, Inc. Liangming Pan, Alon Albalak, Xinyi Wang, andWilliam Wang. 2023. Logic-LM: Empowering largelanguage models with symbolic solvers for faithfullogical reasoning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages38063824, Singapore. Association for Computa-tional Linguistics. Liangming Pan, Michael Saxon, Wenda Xu, DeepakNathani, Xinyi Wang, and William Yang Wang. 2024.Automatically correcting large language models: Sur-veying the landscape of diverse automated correctionstrategies. Transactions of the Association for Com-putational Linguistics, 11:484506. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
  "Stanislas Polu and Ilya Sutskever. 2020. Generativelanguage modeling for automated theorem proving.Preprint, arXiv:2009.03393": "Maja Popovic. 2015. chrF: character n-gram F-scorefor automatic MT evaluation. In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 392395, Lisbon, Portugal. Association forComputational Linguistics. Xin Quan, Marco Valentino, Louise Dennis, and An-dre Freitas. 2024a. Enhancing ethical explanationsof large language models through iterative symbolicrefinement. In Proceedings of the 18th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages122, St. Julians, Malta. Association for Computa-tional Linguistics.",
  "Mingzhe Wang and Jia Deng. 2020. Learning to provetheorems by learning to generate theorems. In Ad-vances in Neural Information Processing Systems,volume 33, pages 1814618157. Curran Associates,Inc": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Yuhuai Wu,Albert Qiaochu Jiang,Wenda Li,Markus Norman Rabe, Charles E Staats, Mateja Jam-nik, and Christian Szegedy. 2022. Autoformalizationwith large language models. In Advances in NeuralInformation Processing Systems. Huajian Xin, Haiming Wang, Chuanyang Zheng, LinLi, Zhengying Liu, Qingxing Cao, Yinya Huang,Jing Xiong, Han Shi, Enze Xie, Jian Yin, ZhenguoLi, Xiaodan Liang, and Heng Liao. 2023. Lego-prover: Neural theorem proving with growing li-braries. ArXiv, abs/2310.00656. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chala-mala, Peiyang Song, Shixing Yu, Saad Godil, RyanPrenger, and Anima Anandkumar. 2023. LeanDojo:Theorem proving with retrieval-augmented languagemodels. In Neural Information Processing Systems(NeurIPS). Kun Zhang, Xiexiong Lin, Yuanzhuo Wang, Xin Zhang,Fei Sun, Cen Jianhe, Hexiang Tan, Xuhui Jiang,and Huawei Shen. 2023.ReFSQL: A retrieval-augmentation framework for text-to-SQL generation.In Findings of the Association for Computational Lin-guistics: EMNLP 2023, pages 664673, Singapore.Association for Computational Linguistics.",
  "ALarge Language Models": "We describe the large language models used in ourexperiments in this section.Mistral (Jiang et al., 2023a) Mistral is a large lan-guage model with 7 billion parameters which isfine-tuned on instruction datasets. It leverages sev-eral techniques such as Sliding Window Attentionto boost model efficiency. To the best of our knowl-edge, it is also the strongest model in the domainof code and mathematics at this size.Llemma (Azerbayev et al., 2024) Llemma is anopen large language model trained specifically formathematics.It is pre-trained on Proof-Pile-2which is a diverse mixture of math-related textualand code content. However, it has not been trainedto follow instructions. Llemma has two scales in7B and 34B. We only use the 7B model in ourexperiments.Mixtral (Jiang et al., 2024a) Mixtral is a large lan-guage model with sparse mixture of experts methodand instruction fine-tuning. It has the same architec-ture as Mistral 7B but each layer of it consists of 8feed-forward blocks, making it a 87B size model.However, during inference, only 13B parametersare activated.GPT-3.5-Turbo GPT-3.5-Turbo is a large languagemodels of OpenAI GPT-3.5 series. It shares thesame architecture as GPT-3 (Brown et al., 2020)and is instruction fine-tuned. The number of pa-rameters in GPT-3.5-Turbo is 175 billion.",
  "BEvaluation Metrics": "We describe the implementation of metrics to mea-sure semantic similarity in this section.BLEU (Papineni et al., 2002) The autoformal-ization task is a type of translation task so themost common metric in translation tasks, BLEU,is used as one evaluation metric for autoformaliza-tion. This metric is also used in (Wu et al., 2022)within the same context. An implementation fromNLTK (Bird and Loper, 2004) is used.ChrF (Popovic, 2015) ChrF is another n-gram met-ric in translation task that focuses on charactersinstead of words in BLEU. We leverage this char-acter level metric in NLTK to take character-levelgranularity into account.RUBY (Tran et al., 2019) The autoformalizationtask is also a code generation task. RUBY is a met-ric designed specifically for code generation evalu-ation and uses edit distance to calculate the similar-ity score. If program dependence graph (PDG) orabstract syntax tree (AST) is provided, it calculatesgraph similarity based on graph edit distance or treesimilarity based on tree edit distance. Otherwise,it calculates string edit distance to determine thestring similarity between the reference code andcandidate code as the score. In our experiments, be-cause of the difficulty of obtaining PDG or AST offormal statements, we use string edit distance fromNLTK to calculate string similarity as the score.This implementation focuses on characters ratherthan tokens as in the original paper but it still makesthe score a reasonable indicator of performance.CodeBERTScore (Zhou et al., 2023) Code-BERTScore is a model-based metric to evaluateperformance on code generation. It uses tokenrepresentations of reference code and candidatecode to determine a final score. The original pa-per trained different models for different program-ming languages to get representations: howeverIsabelle is not within this scope. Therefore, we usea mathematical specific model, Llemma 7B (Azer-bayev et al., 2024), as the supporting representationmodel. Although this model is not a BERT-basedmodel, it can still generate meaningful representa-tions for score calculation.",
  "Prompt": "PBD 1AYou are an expert in Isabelle theorem prover. You will be provided with an Isabelle/ZF code generatedby a language model. Your task is to clean the provided Isabelle/ZF code with following instructions.Instructions:1. The provided code might contain several lemmas or definitions or theorems. The cleaned code mustonly keep the best one lemma or definition or theorem.2. Do not write any proof and if there is a proof in the provided code, remove it from the cleaned code.3. You should only output tokens that compose the cleaned code. Anything else, including but not limitedto note, description, explanation and comment, must be removed from the final answer. Giving anyadditional text is prohibited.Strictly follow the instructions that I have claimed.Provided Isabelle/ZF Code: {isabelle code}Cleaned Code:",
  "PBD 1B1A + An additional instruction:4. The cleaned code must have the same style and usage of operators as the original provided code.Operators usually start with \\ such as \\<in>, \\<cdot>": "PBD 1C1A + An additional instruction:4. The cleaned code must have the same style and usage of operators as the original provided code.Operators usually start with \\ such as \\<in>, \\<cdot>. Here are some additional Isabelle/ZF codeexamples which have the same style as the original provided code:{fixed 3-shot formal statements} PBD 1D1A + An additional instruction:4. The cleaned code must have the same style and usage of operators as the original provided code.Operators usually start with \\ such as \\<in>, \\<cdot>. Here are some additional Isabelle/ZF codeexamples which have the same style as the original provided code:{retrieved 3-shot formal statements}",
  ": Prompts for informalization": "You are an expert in Isabelle theorem prover. You will be provided with an Isabelle/ZF code generated by a languagemodel. The provided code has some Isabelle/ZF syntax errors according to the Isabelle prover. You will also beprovided with the error details and where the error code is located in the code. Your task is to fix related errors in theprovided Isabelle/ZF code with following instructions. Instructions:1. Only refine the code part which is related to provided error details. You must keep other code parts unchanged.2. The syntax errors might cause by the mismatch of brackets, incorrect using of operators or invalid representationof Isabelle/ZF code. You should only refine the error codes based on the error details by rewriting, fixing or removingerror codes.3. You should only output tokens that compose the cleaned code. Anything else, including but not limited tonote, description, explanation and comment, must be removed from the final answer. Giving any additional text isprohibited.4. The cleaned code must have the same style and usage of operators as the original provided code. Operatorsusually start with \\ such as \\<in>, \\<cdot>. Here are some additional Isabelle/ZF code examples which havethe same style as the original provided code:{retrieved 3-shot formal statements}Strictly follow the instructions that I have claimed.Provided Isabelle/ZF Code:{isabelle code}{first syntax error details}Refined Code:"
}