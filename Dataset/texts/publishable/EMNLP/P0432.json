{
  "Abstract": "We introduce EmphAssess, a prosodic bench-mark designed to evaluate the capability ofspeech-to-speech models to encode and repro-duce prosodic emphasis. We apply this to twotasks: speech resynthesis and speech-to-speechtranslation. In both cases, the benchmark evalu-ates the ability of the model to encode emphasisin the speech input and accurately reproduceit in the output, potentially across a change ofspeaker and language. As part of the evalua-tion pipeline, we introduce EmphaClass, a newmodel that classifies emphasis at the frame orword level.",
  "Introduction": "In recent years, significant advancements havebeen made in the development of Self-SupervisedLearning (SSL) models for speech, extending be-yond the traditional text-only methods prevalentin the field (Mohamed et al., 2022). Such speech-based models find successful application acrossvarious domains from generative language mod-elling (Lakhotia et al., 2021; Borsos et al., 2023;Nguyen et al., 2023b) to speech-to-speech transla-tion (S2ST) (Jia et al., 2019, 2022; Lee et al., 2021;Rubenstein et al., 2023; Barrault et al., 2023). Un-like text-only models, they exploit additional cuespresent in the speech signal which are absent intextual input.One crucial speech-only cue is prosody. Alsotermed the music of speech (Wennerstrom, 2001),prosody is marked by the perceived loudness,rhythm, and pitch of speech. Prosody not only addsnaturalness to an utterance but also has the capacityto modify the meaning of the conveyed message,both at a global level, such as in the expressionof different emotions, and at a local level, by in-fluencing the interpretation of individual phrasesor words (Cutler et al., 1997; Dahan, 2015). For",
  "Currently at Apple": "instance, slower speech may suggest hesitation,while altering something like pause placement canactually change the segmentation into words or syn-tactic constituents, with downstream consequencesfor the meaning. Hence, accurately capturing theseprosodic elements is essential in SSL speech mod-els for any application (Avila and Ward, 2023).To address this, Kharitonov et al. (2021) pro-posed explicitly adding prosodically-relevant infor-mation such as fundamental frequency and durationto the speech representations models learn, whileothers aimed at explicitly modelling emotions insuch representations (Gan et al., 2022; Duret et al.,2023). Although some progress has been made, ro-bust evaluation metrics for prosody remain scarce,and human evaluation, while insightful, is subjec-tive - which can limit reproducibility; as well asbeing expensive and time intensive - which canhinder its utility in large-scale applications.Objective evaluations of prosody fall into twomain categories: one focuses on utterance-level fea-tures like emotion and speech rate to assess globalprosody, and the other examines local prosody,which is concerned with prosodic effects at thelevel of a word or a phrase, such as breaks, turnends and emphasis.In addition, one may ad-dress prosody for two classes of models: gener-ative decoder-only models (the speech equivalentof GPT (Radford et al., 2018) (e.g. GSLM, Lakho-tia et al., 2021; AudioLM, Borsos et al., 2023;dGSLM, (Nguyen et al., 2023b)), and speech-to-speech (encoder-decoder) approaches, which takespeech as input and produce output in a differentvoice (speech resynthesis) or a different language(S2ST). In this paper, we address the second classof models.In the context of speech-to-speech (S2S) mod-els, evaluating global prosody can be relativelystraightforward, as the features are not directly re-lated to the lexical content. The assessment of localprosody, however, presents more of a challenge, as it necessitates mapping at the lexical level. This canbe relatively feasible in the context of speech resyn-thesis, where the model directly reconstructs theinput signal and, therefore, preserves lexical con-tent (e.g., by correlating prosodic attributes such asduration and fundamental frequency (F0) betweeninput and output utterances;Suni et al., 2020).However, this becomes more complicated whenevaluating S2ST models, as one needs to ensurethe correct prosodic feature is applied to the correctword(s) (Duret et al., 2023) (alignment problem).Although scarce, there have been recent effortsmade to establish benchmarks in the prosodic eval-uation of speech models allowing models compar-ison, including evaluation corpora and pipelines,both at the global prosodic level (pragmatic infor-mation : Lin et al. (2023)) and at the local prosodiclevel (prosodic pauses: de Seyssel et al. (2023)).Yet, there is a need for more benchmarks to coverother aspects of prosody, and all types of speechmodels.In this work, we introduce the EmphAssessbenchmark, which is focused on local prosodyfor speech-to-speech models and includes: (i) anew, automatic pipeline for emphasis evaluationthat is modular, handles multiple languages andkinds of outputs (including paraphrases and trans-lations, (ii) a novel dataset, the EmphAssess testset, for evaluating model emphasis preservationin English and Spanish according to our pipeline,and (iii) EmphaClass, an emphasis classifier thatwe finetuned with English data over an existingmultilingual SSL model to support our pipeline.",
  "Background": "Emphasis as a prosodic feature.Emphasis, thephonetically-realized importance given to partic-ular words or phrases, is critical for interpretinglanguage. Some of the most important correlatesof emphasis are fundamental frequency (f0), du-ration, and amplitude (Terken and Hermes, 2000;Mo, 2008), although the weight and behaviour ofeach can vary across languages (Ladd and Arvan-iti, 2023). These acoustic attributes collectivelyshape the prosodic contours that signal emphasisin speech. Altering the emphasis in a sentencesuch as I never said he stole my bag\" from he\"to stole\" can drastically change its meaning. Suchnuances are essential for models to process, if theyare to have an accurate representation of speech, bethey generative language models or S2ST systems. In fact, the issue of accurate emphasis transfer inS2ST models has attracted some research attentionover the years. Studies by Tsiartas et al. (2013); Doet al. (2016, 2018) approach this topic using cas-caded models (with separate Automatic SpeechRecognition, Machine Translation, and Text-to-Speech models). A more recent approach by Huanget al. (2023) integrates the two first componentsinto a single encoder module capable of multilin-gual embeddings. Similar to other prosodic fea-tures, emphasis in S2S models is primarily eval-uated through human evaluation (Tsiartas et al.,2013; Huang et al., 2023), although Do et al. (2016,2018) proposed leveraging an emphasis classifica-tion algorithm to calculate F1 scores by matchingemphasised words in the input and output utter-ances. Yet, this method is limited to a single lan-guage pair and cannot handle variations in trans-lation outputs, only recognising one gold trans-lation per dataset utterance. Consequently, thismetric is ill-suited for comprehensive automaticbenchmarking across various models. Word-level emphasis classification.As sug-gested by Do et al. (2016, 2018), a robust word-level emphasis classification system is criticalin automatic evaluation of emphasis transfer inS2ST models.Existing algorithms, predomi-nantly designed for text-to-speech applications, of-ten rely on traditionally engineered features (e.g.MFCCs or Fbanks), sometimes augmented withother prosodic-related information (e.g. F0, dura-tion) (Do et al., 2016; Heba et al., 2017; Ning et al.,2017; Zhang et al., 2018). Some also incorporatelexical information from textual transcripts (Bre-nier et al., 2005; Zhou et al., 2020). However, thesemodels frequently suffer from limited generalisabil-ity across different datasets, voice types, and lan-guages. There is a compelling argument for usingthe speech waveform directly as input to enhancegeneralisability. To our knowledge, the only studyto have adopted this approach is that of Vaidyaet al. (2022), which employed a CRNN frameworkfor classifying emphasis in childrens speech; theirwork, however, was limited to a single language(and is not open-sourced). We propose that lever-aging pretrained models trained on multilingualdatasets could result in significant advancements inthis field.",
  "Introducing EmphAssess": "In this study, we introduce EmphAssess, a versa-tile automatic benchmark for evaluating emphasispreservation in S2S models, including S2ST ones.Essentially, this benchmark comprises a carefullycurated dataset of English utterances with empha-sised words, accompanied by an automatic evalu-ation pipeline, and results on some of the most re-cent S2S SSL models. Our evaluation framework,inspired by the methodology of Do et al. (2016,2018), assesses emphasis alignment between thesource and the models output utterances. Ourbenchmarks novelty lies in its capacity to han-dle various output types, including paraphrases andtranslations.Guided by the data we have for setting optimalbaselines, the EmphAssess benchmark is specifi-cally designed for English-to-English and English-to-Spanish S2S models. However, our work goesfurther, laying the groundwork for extending thisbenchmark to other language pairs. Moreover, theevaluation pipeline itself is already capable of be-ing applied to a broad spectrum of language pairs.Also, while we focus here on unsupervised speechlanguage models, EmphAssess is versatile enoughto be applied to any S2S framework.The EmphAssess evaluation pipelines modu-lar structure is a key feature, with each module designed to function independently and allow forstraightforward modifications. We leverage a suiteof distinct open-source models, each finetuned forparticular tasks. The pipeline can therefore be up-graded to incorporate improvements in each mod-ule seamlessly. Although such enhancements maynecessitate a re-evaluation of the models withinour benchmark, this inherent adaptability is a con-siderable benefit, ensuring EmphAsses can remaincurrent with the latest research for years to come.Finally, we introduce and open-source, as part ofthis automatic evaluation pipeline, a novel empha-sis classifier at the word level: EmphaClass. Thisclassifier is finetuned over an existing multilingualSSL model with the hope of enhancing its robust-ness across multiple languages and variability.The evaluation code, emphasis classifier anddataset introduced in this paper are available inour related repository 1.",
  "The EmphAssess Dataset": "The EmphAssess dataset comprises syntheticallygenerated speech utterances, each containing atleast one emphasised word. Accompanying theseutterances are metadata detailing the transcription,the positional index of the emphasised word(s), andinformation about the synthetic voice employed for synthesis. In total, the dataset boasts 3652 speechsamples derived from 913 unique transcripts (witheach transcript being rendered in 4 distinct voices).The dataset generation started with a selectionof transcripts from a list of handwritten transcriptswith emphasis annotations2 previously created forcompany-internal Text-to-Speech purposes. Tran-scripts containing characters beyond letters or spe-cific punctuation marks3 or those featuring propernouns (identified using the NLTK toolkit; Bird2006) were excluded, to ensure the translationsare as straightforward as possible. Moreover, weensured a minimum of two distinct versions withdifferent emphases for string identical sentences(those with matching word tokens but possibly dif-fering emphasis position indices). This approachwas adopted to mitigate any bias should a modelexhibit a preference for emphasising a particularword over others. Finally, we filtered out tran-scripts that could face alignment challenges withemphasised words during translation. We set up analgorithm to assess the difficulty of aligning em-phasised words in an English sentence with theircounterparts in multiple target languages, usingthe SimAlign word-alignment tool (Sabet et al.,2020). Simply put, if an emphasised word in thesource matched consistently to a correspondingword across a list of other languages (German,French, Spanish, and Chinese), the sentence waslabelled easy; otherwise, it was deemed diffi-cult. Only easy transcripts were retained forour dataset. We were left with 913 distinct tran-scriptions (with varying emphases) derived from apool of 299 unique transcriptions. We ensured thatthe distribution of transcripts was well balanced, interms of where the emphasis was located.Next, we employed an internal Text-to-Speech(TTS) tool with a 16 kHz sample rate to synthesiseall 913 transcripts, each in the four distinct open-source English Expresso voices (Nguyen et al.,2023a), namely ex01, ex02, ex03 and ex04, re-sulting in a comprehensive set of 3,652 speechsamples.Finally, we compiled a dataset that is avail-able as part of the benchmark. This dataset com-prised four columns: an id column that denotesthe unique identifier for each speech segment,a src_sentence column that contains the corre-sponding tokenised text transcript presented in list",
  "The EmphaAssess Evaluation Pipeline": "The evaluation pipeline, as illustrated in ,is divided into two main stages. The first one (leftpanel) corresponds to the generation of utterancesfrom the evaluated S2S model. That is, for eachutterance from the EmphAssess dataset, we need togenerate the corresponding utterance output fromthe evaluated model. Hence, this inference stageis dependent on the model tested, and we will notexpand on it here.In the second stage (right panel), we perform theautomatic evaluation by comparing the input andoutput utterances. The objective is twofold: firstly,to ascertain whether the emphasis is retained inthe generated utterance, and secondly, to determinewhether the emphasis is correctly positioned onthe corresponding word. At this stage, availableresources include the input (original) utterance, thecorresponding output utterance, and the tokenisedtranscript of the input with the location of the em-phasised word(s) identified. A schematic overviewof the evaluation pipeline is shown in the rightpanel of . Initially, we obtain a transcrip-tion of the generated utterance (1) and the time-aligned word boundaries (2). This information canbe used in addition to the raw waveform to detectemphasis at the word level in the output utteranceusing a classifier (3). At this stage, we must de-termine which word(s) in the generated utteranceshould be emphasised to obtain evaluation scores(4). We use word-to-word alignment at the textlevel to address this, a technique borrowed fromthe machine translation field. Finally, we can usethis information to compute precision, recall andF1 score (5). We will now detail our methodologyfor each of these steps.",
  "Word Emphasis Classification": "As the next step requires detecting emphasis at theword level from the waveform and its correspond-ing transcription, we propose EmphaClass, a newmodel for emphasis classification. Our approachwas centred around finetuning a pretrained SSLspeech model through a frame-classification taskto classify a frame as either emphasised or not.We can then aggregate frame-level scores to deriveword-level emphasis classifications. Data. We utilised speech sourced from the En-glish Expressive Expresso dataset (Nguyen et al.,2023a). Indeed, this dataset comprises utterancesthat contain emphasised words, accompanied bytheir annotations, presented in a diverse range ofspeaking styles. We retained only those utterancesthat had at least one word emphasised. We dividedthe four speakers into two for validation (ex03 andex04) and two for the test set (ex01 and ex02). Ad-ditionally, we had utterances from six other speak-ers recorded under identical conditions and withsimilar emphasis annotations. These were utilisedto create an internal training set, amounting to 2.06hours of speech. We then used the Montreal ForcedAligner to align the transcription with the audioand obtain reliable word boundaries (McAuliffeet al., 2017). We subsequently processed the datato provide annotations at the frame level regardingemphasis. We deem a frame as emphasised if itfalls within a word annotated as such, with eachframe corresponding to 20ms of speech. Emphasis classifier architecture. We finetunedthe multilingual SSL speech model, XLS-R (Babuet al., 2021), grounded in the Wav2Vec 2.0 archi-tecture (Baevski et al., 2020). This finetuning en-compassed a binary frame classification task us-ing cross-entropy loss, and was carried out us-ing the Wav2Vec2ForAudioFrameClassificationmethod from HuggingFace (Wolf et al., 2019). Ourchoice of the XLS-R model for extended trainingand evaluations stemmed from its exceptional per-formance metrics and promising potential for cross-language generalisation. Evaluation. We use F1 score as the primary metricfor evaluating our emphasis classifier, both at theframe and word level. For word-level classification,we compute the average accuracy of the frameswithin the boundaries of each word. A word was deemed emphasised if more than 50% of its frameswere classified as such. A representative exampleof this classification is illustrated in .We evaluate the classifier on our test set splitof the Expresso dataset, but also on the utterancesused in our EmphAssess dataset. Results are pre-sented in . The scores suggest that themodel performs well at classifying emphasis inboth the Expresso dataset 78.4% and the Emphas-ses dataset 93.48%. The lower scores from theExpresso dataset, compared to the EmphAssessdataset, can be attributed to two factors. Firstly,the Expresso dataset incorporates utterances withspeaking styles where the emphasis is notably chal-lenging to discern, such as whispering and laughing.Secondly, using synthetic voices in EmphAssessmight offer more consistent and clearer patternsof emphasis than the natural utterances from Ex-presso, making it easier for the classifier to discern,and thus leading to higher accuracy scores.",
  "Word-to-word alignment": "Returning to the automatic emphasis evaluationpipeline, we can detect which word(s) is empha-sised in an output utterance with the classifier de-scribed above, given a waveform, its transcriptionsand word boundaries. At this point, we need toidentify which word(s) should be emphasised inthe output utterance to compute a score for thequality of emphasis transfer. This step is vitalbecause it lets us evaluate any output utterance,including paraphrases and translations, without be-ing limited to a gold output. To do this, we usea word-to-word alignment algorithm, often seenin machine translation, especially the SimAlignone (Sabet et al., 2020). This tool can align words",
  ": Illustrative example of emphasis classification with the trained classifier. Top: gold annotations.Bottom: Emphasis classifier predictions": "between two text sentences. Although typicallyused in machine translation, its also effective forparaphrases in the same language. A key benefit ofSimAlign is that it works across many languageswithout requiring finetuning. For our needs, wecompare the original text input with the output ut-terance transcription from the ASR to see whichword(s) match the emphasised word in the originalsentence.",
  "English S2S models": "We first present results on models that generatespeech with the target and source language beingidentical, here English (left panel of ). Thisencompasses models that undergo an encoding-decoding method, simply resynthesising the learntunits and those which can learn paraphrases.For a topline evaluation, we matched the input ut-terances from EmphAssess with themselves (that is,we pretended the output utterances were the same as the input ones). This gave us an insight into thebest achievable scores, with any potential loss inperformance due to problems in the dataset or thevarious comparison stages. This topline producedan F1 score of 89%, indicating that our cascadedpipeline performs well. It should also be notedthat we consider chance-level to yield scores of0, corresponding to a model which does not en-code emphasis and thus should not produce anyemphasis.We first assessed the generative GSLM model(Nguyen et al., 2023b), specifically the HuBert,100 units version. This model initially encodesspeech into continuous forms using HuBert (Hsuet al., 2021), which are then quantised into unitsfor language modelling. Subsequently, a synthe-siser converts these units back to speech. In ourstudy, we extracted the quantised representationsfrom our EmphAssess datasets speech samplesand directly resynthesised them, bypassing the gen-erative language modelling phase. Despite scoringnotably lower than the topline with an F1 of 42%,the model successfully transferred some emphasisto the output utterances. This indicates the presenceof prosodic information within these units learnedfrom SSL speech model, a finding supported byde Seyssel et al. (2022, 2023).We also assessed the pGSLM variant, whichincorporates extra prosodic features during trainingto enhance prosody modelling (Kharitonov et al.,2021)4. Notably, the pGSLM models achievedscores close to the topline, with an F1 of 88%,",
  ": Precision, recall and F1 scores on the EmphAssess benchmark. Left : English-to-English models andEnglish Emphasis classifier. Right : English-to-Spanish models and Spanish Emphasis classifier": "highlighting their excellent proficiency in encodingemphasis accurately.Finally, we assessed the Seamless M4T model(Barrault et al., 2023), forcing it to generate out-puts in English. Contrary to the previous models,which generate output constrained in their lexicalinput, this one is primarily a S2ST model and canoutput paraphrases. We did not expect these mod-els to encode any prosodic information given totheir architecture, an expectation which was actu-ally supported by a very low score on EmphAssess(18%).",
  "Generalising the pipeline to S2Stranslation": "We now want to discuss how we can adapt ourpipeline to S2ST capabilities. While most targetlanguages can be evaluated directly using the ex-isting pipeline, there are several considerations toremember. Firstly, it is essential to establish a val-idated topline. In other words, when introducinga new target language, we require validated trans-lated utterances of the input English dataset in thedesired language to have a topline in this targetlanguage. This process necessitates human vali-dation, not only for the text translation, but alsoto either synthesise or record this translation withthe correct emphasis, depending on the available re-sources. This new set of utterances can additionallyserve as an input test set when we want to modifythe source language to one other than English.Furthermore, we might want to modify or adaptsome of the stages of the automatic evaluationpipeline in order to be better suited to the newlanguage. For example, we have gathered evidenceindicating that the emphasis classifier performs bet-ter when trained in the specific language it willbe evaluated in. Thus, retraining it with emphasis",
  "data in the target language can prove advantageous,albeit demanding the corresponding larger dataset": "We undertook a two-step process to modifyour evaluation for English-to-Spanish translation.Firstly, external annotators translated the input sen-tences into Spanish, ensuring the inclusion of em-phasis annotations. Subsequently, these translatedsentences were synthesised into Spanish using ourin-house TTS (Text-to-Speech) voices designed forSpanish, with a focus on retaining emphasis. Addi-tionally, we adjusted the emphasis classifier to onespecifically trained for Spanish as it yielded betterresults on Spanish data (see Appendix A). As depicted in the right panel of , thetopline, which aligns the English input with thesynthesised Spanish voices as the output, achieveda score of 58%. While this result is reasonable,it notably lags behind the English topline. Thisdecline may be attributed to various factors, in-cluding challenges in the synthesised voices, aswe observed that our Spanish TTS voices do notemphasise as effectively as desired. Furthermore,issues in different stages of our automatic evalu-ation pipeline might contribute (for instance, theSpanish emphasis classifiers performance on span-ish is not as optimal as its English counterparton English data). Additionally, linguistic differ-ences could play a role, with Spanish emphasispotentially being less prominent than in Englishor conveyed through alternative means, possiblyparaphrastically in the text itself. Nonetheless, hav-ing this topline facilitates the comparison of othermodels and the assessment of their relative perfor-mance. Subsequently, we evaluated the SeamlessM4T model (Barrault et al., 2023) in its English-to-Spanish translation capability, which yielded anF1 score of 14%. This result, akin to its English-to-",
  "Human Evaluation": "To gauge human performance on the task, we con-ducted an evaluation with expert annotators. Theseannotators were presented with an utterance andits word-tokenised transcription, and were taskedwith marking words they considered to be empha-sised. Importantly, they were not obliged to markany word as emphasised if they didnt perceive any.This evaluation was carried out on a subset of thedata, incorporating both English and Spanish ut-terances, with native annotators for each language. shows precision, recall, and F1 scores forEnglish-to-English and English-to-Spanish, respec-tively5. These metrics were calculated by com-paring the annotators identification of emphasisagainst the gold standard annotation with whichwe synthesised the utterances.Focusing first on the English dataset, the anno-tators achieved a commendable precision score of86%, although this was offset by a lower recallscore (50%). The lower recall could be attributedto annotators not perceiving emphasis in numer-ous sentences (Note: it is often harder to perceiveemphasis in utterances taken out of their general,wider context); nonetheless, the high precisionscore is encouraging. Turning our attention to theSpanish dataset, both recall and precision scoreswere lower. This aligns with our hypothesis thatthe quality of voice synthesis in Spanish was notup to par - with the larger drop of recall comparedto the topline could be explained by the Spanishemphasis classifier model picking up very subtlecues that are not obvious to the human ear. It mayalso suggest that the nuances of emphasis might belinguistically specific, thereby differing betweenEnglish and Spanish.",
  "For English-to-Spanish, the human topline is set usinga subset of the Spanish utterances synthesized the Spanishtopline": "the acquisition of a relevant dataset to establish areliable gold standard.Additionally,wehaveopen-sourcedanemphasis-classification model that has beenfinetuned on English data. The model builds ona multilingual SSL architecture and has shownimpressive accuracy in classifying emphasisedspeech in English on our dataset, along withreasonable performance in other languages (forfurther details, refer to the Appendix).Themodels robustness in English makes it a plausiblestarting point for finetuning classifiers in otherlanguages, potentially minimising the volumeof data needed for training.Interestingly, thefact that the successful results were achievedwithout retraining the encoder, suggests that theinherent features in the original XLS-R modelwere adequate for emphasis classification.There is an existing agenda for future researchcentring around the evaluation of prosody withinSSL models. Firstly, on the subject of empha-sis, we aim to scrutinise its functional role morecloselyspecifically, its ability to convey impor-tance. We intend to investigate whether such a func-tion is intrinsically represented within these models.Beyond emphasis, other aspects of prosody, suchas turn-taking and speech grouping, merit attention.We are interested in determining whether theseelements, too, are encoded within SSL models.Improved benchmarks and evaluations for theseprosodic features could pave the way for the devel-opment of more expressive and nuanced models.To conclude, the EmphAssess benchmark sets anew standard for the evaluation of prosodic featuresin S2S models, offering both methodological con-tributions and actionable insights that could pavethe way for more natural and effective machine-generated speech across various applications.",
  "Limitations": "While pioneering in its approach to evaluating em-phasis in S2S models, our study encounters certainlimitations. First, the emphasis classifier presentedin this paper was made to be used with this exactdataset, and we recommend constraining its use tothis particular use case (that is, with the presentedbenchmark and evaluation pipeline). Indeed, fur-ther testing is required to enhance its robustnessand ensure its efficacy in detecting more nuancedforms of emphasis across other datasets.Furthermore, the robustness of our evaluation process relies on the quality of multiple pipelinecomponents, including Automatic Speech Recog-nition, forced alignment, and word-to-word align-ment. Therefore, it is crucial to be mindful that er-rors could arise at various stages. Yet, the modularnature of the pipeline allows for continual improve-ments and assures that inter-model comparisonsremain valid.Another limitation of our work lies in the use ofsynthesised speech to create our dataset. While thisapproach provides a more controlled and consistentdatasetfor instance, by enabling the synthesis ofidentical textual content with varying word em-phases and voicesit may fail to capture the fullrange of characteristics found in natural speech.Consequently, this limitation could affect how wellthe benchmark results can be applied to practicaluse cases.Lastly, our study is currently limited to binarycategorisation of emphasis.Future endeavourscould explore varying degrees of emphasis, al-though this would require more advanced models.For instance, capturing subtle differences in empha-sis between the input and output of an S2S systemcould be a valuable addition to this line of research.",
  "Jonathan E Avila and Nigel G Ward. 2023. Towardscross-language prosody transfer for dialog. arXivpreprint arXiv:2307.04123": "Arun Babu, Changhan Wang, Andros Tjandra, KushalLakhotia, Qiantong Xu, Naman Goyal, Kritika Singh,Patrick von Platen, Yatharth Saraf, Juan Pino, et al.2021. Xls-r: Self-supervised cross-lingual speechrepresentation learning at scale.arXiv preprintarXiv:2111.09296. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,and Michael Auli. 2020. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations.Advances in neural information processing systems,33:1244912460.",
  "Delphine Dahan. 2015. Prosody and language compre-hension. Wiley Interdisciplinary Reviews: CognitiveScience, 6(5):441452": "Maureen de Seyssel, Marvin Lavechin, Yossi Adi, Em-manuel Dupoux, and Guillaume Wisniewski. 2022.Probing phoneme, language and speaker informa-tion in unsupervised speech representations. In Inter-speech 2022. Maureen de Seyssel, Marvin Lavechin, Hadrien Titeux,Arthur Thomas, Gwendal Virlet, Andrea Santos Re-villa, Guillaume Wisniewski, Bogdan Ludusan, andEmmanuel Dupoux. 2023.Prosaudit, a prosodicbenchmark for self-supervised speech models. InInterspeech 2023. Quoc Truong Do, Sakriani Sakti, and Satoshi Nakamura.2018. Sequence-to-sequence models for emphasisspeech translation. IEEE/ACM Transactions on Au-dio, Speech, and Language Processing, 26(10):18731883. Quoc Truong Do, Tomoki Toda, Graham Neubig, Sakri-ani Sakti, and Satoshi Nakamura. 2016. Preservingword-level emphasis in speech-to-speech translation.IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, 25(3):544556.",
  "Jarod Duret, Benjamin OBrien, Yannick Estve, andTitouan Parcollet. 2023.Enhancing expressiv-ity transfer in textless speech-to-speech translation.arXiv preprint arXiv:2310.07279": "Wendong Gan, Bolong Wen, Ying Yan, Haitao Chen,Zhichao Wang, Hongqiang Du, Lei Xie, KaixuanGuo, and Hai Li. 2022. Iqdubbing: Prosody mod-eling based on discrete self-supervised speech rep-resentation for expressive voice conversion. arXivpreprint arXiv:2201.00269. Abdelwahab Heba, Thomas Pellegrini, Tom Jorquera,Rgine Andr-Obrecht, and Jean-Pierre Lorr. 2017.Lexical emphasis detection in spoken french usingf-banks and neural networks. In International Confer-ence on Statistical Language and Speech Processing,pages 241249. Springer. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460. Wen-Chin Huang, Benjamin Peloquin, Justine Kao,Changhan Wang, Hongyu Gong, Elizabeth Salesky,Yossi Adi, Ann Lee, and Peng-Jen Chen. 2023. Aholistic cascade system, benchmark, and human eval-uation protocol for expressive speech-to-speech trans-lation. In ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 15. IEEE. Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, andRoi Pomerantz. 2022. Translatotron 2: High-qualitydirect speech-to-speech translation with voice preser-vation.In International Conference on MachineLearning, pages 1012010134. PMLR. Ye Jia, Ron J Weiss, Fadi Biadsy, Wolfgang Macherey,Melvin Johnson, Zhifeng Chen, and Yonghui Wu.2019.Direct speech-to-speech translation witha sequence-to-sequence model.arXiv preprintarXiv:1904.06037. Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi,Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Mor-gane Rivire, Abdelrahman Mohamed, EmmanuelDupoux, et al. 2021. Text-free prosody-aware gen-erative spoken language modeling. arXiv preprintarXiv:2109.03264.",
  "D Robert Ladd and Amalia Arvaniti. 2023. Prosodicprominence across languages. Annual Review of Lin-guistics, 9:171193": "Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-AnhNguyen, Jade Copet, Alexei Baevski, AbdelrahmanMohamed, et al. 2021. On generative spoken lan-guage modeling from raw audio. Transactions of theAssociation for Computational Linguistics, 9:13361354. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne,Holger Schwenk, Peng-Jen Chen, Changhan Wang,Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al.2021. Textless speech-to-speech translation on realdata. arXiv preprint arXiv:2112.08352. Guan-Ting Lin, Chi-Luen Feng, Wei-Ping Huang, YuanTseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, andNigel G Ward. 2023. On the utility of self-supervisedmodels for prosody-related tasks. In 2022 IEEE Spo-ken Language Technology Workshop (SLT), pages11041111. IEEE. Michael McAuliffe, Michaela Socolof, Sarah Mihuc,Michael Wagner, and Morgan Sonderegger. 2017.Montreal forced aligner: Trainable text-speech align-ment using kaldi. In Interspeech, volume 2017, pages498502.",
  "Yoonsook Mo. 2008. Acoustic correlates of prosodicprominence for naive listeners of american english.In Annual Meeting of the Berkeley Linguistics Society,volume 34, pages 257267": "Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt,Jakob D Havtorn, Joakim Edin, Christian Igel, Ka-trin Kirchhoff, Shang-Wen Li, Karen Livescu, LarsMaale, et al. 2022. Self-supervised speech represen-tation learning: A review. IEEE Journal of SelectedTopics in Signal Processing. Tu Anh Nguyen, Wei-Ning Hsu, Antony dAvirro,Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-mez, Jade Copet, Gabriel Synnaeve, Michael Hassid,et al. 2023a. Expresso: A benchmark and analy-sis of discrete expressive speech resynthesis. arXivpreprint arXiv:2308.05725. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, YossiAdi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello,Robin Algayres, Benoit Sagot, Abdelrahman Mo-hamed, et al. 2023b. Generative spoken dialoguelanguage modeling. Transactions of the Associationfor Computational Linguistics, 11:250266. Yishuang Ning, Zhiyong Wu, Runnan Li, Jia Jia, Mingx-ing Xu, Helen Meng, and Lianhong Cai. 2017. Learn-ing cross-lingual knowledge with multilingual blstmfor emphasis detection with limited training data. In2017 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 56155619. IEEE. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine McLeavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In International Conference on MachineLearning, pages 2849228518. PMLR.",
  "Jacques Terken and Dik Hermes. 2000. The perceptionof prosodic prominence. In Prosody: Theory andexperiment: Studies presented to Gsta Bruce, pages89127. Springer": "AndreasTsiartas,PanayiotisGGeorgiou,andShrikanth S Narayanan. 2013. A study on the ef-fect of prosodic emphasis transfer on overall speechtranslation quality. In 2013 IEEE International Con-ference on Acoustics, Speech and Signal Processing,pages 83968400. IEEE. Mithilesh Vaidya, Kamini Sabu, and Preeti Rao. 2022.Deep learning for prominence detection in childrensread speech. In ICASSP 2022-2022 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 81578161. IEEE.",
  "Ann Wennerstrom. 2001. The music of everyday speech:Prosody and discourse analysis. Oxford UniversityPress": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771. Long Zhang, Jia Jia, Fanbo Meng, Suping Zhou, WeiChen, Cunjun Zhang, and Runnan Li. 2018. Empha-sis detection for voice dialogue applications usingmulti-channel convolutional bidirectional long short-term memory network. In 2018 11th InternationalSymposium on Chinese Spoken Language Processing(ISCSLP), pages 210214. IEEE. Suping Zhou, Jia Jia, Long Zhang, Yanfeng Wang, WeiChen, Fanbo Meng, Fei Yu, and Jialie Shen. 2020.Inferring emphasis for real voice data: an attentivemultimodal neural network approach. In MultiMe-dia Modeling: 26th International Conference, MMM2020, Daejeon, South Korea, January 58, 2020, Pro-ceedings, Part II 26, pages 5262. Springer.",
  "ACross-language generalisation in theclassifier": "Using a Spanish company-internal variant of theExpresso dataset, we trained and tested the classi-fier on Spanish data in an identical manner to ourapproach with English. We should however notethat the version of the data we had was of lesserrecording quality than the English one.The classifiers outcomes when evaluated onboth the English and Spanish train sets are pre-sented in . The most important observationfrom the results is the classifiers superior perfor-mance when trained and tested on the same lan-guage.Cross-language assessments, especiallyfrom English-trained models tested on Spanishdata, manifested a decline in performance. Nev-ertheless, despite the noted challenges, the resultsdemonstrate that the classifier is able to detect em-phasis, even across languages. It is also worththat the Spanish dataset was of considerably lowerquality than the English one and is just used herefor demonstration purposes. It is plausible thatthis quality might have affected the models perfor-mance. Therefore, a more definitive assessment ofits cross-language generalisation potential wouldnecessitate testing on datasets of other languages,ideally of comparable quality to the English ver-sion.We also extended the evaluation of the Englishand Spanish emphasis classifiers to additional lan-guages, using internal datasets to compile test setsmirroring the structure of the English ones, eachfeaturing 2 to 3 speakers. These are summarisedin . Intriguingly, the Spanish classifier out-performed across all tested languages, a findingreadily attributable to linguistic similarities in thecase of Italian, French, and Portuguese, but less sofor Vietnamese. Furthermore, in some instances,performance on non-native test sets was on parwith, or even surpassed, native datasets; for exam-ple, a word-level F1 score of 84.4% was achievedon the Portuguese test set. These observations im-ply the feasibility of applying classifiers to lan-guages they were not specifically trained on, par-ticularly when sufficient training data is lacking,and suggest the merit in experimenting with clas-sifiers based on different languages. Additionalresults could potentially advocate for the benefitsof multi-language training approaches. An addi-tional point of interest arises from the performanceof the Vietnamese test sets. Vietnams tonal nature, which distinctly shapes its emphasis patterns, os-tensibly diverges from the prosodic systems used inRomance and Germanic languages. Despite thesefundamental differences, the fact that the Spanish-trained classifier achieved commendable resultswith Vietnamese indicates that it may be recognis-ing universal features of emphasis that transcendlanguage-specific prosodic systems."
}