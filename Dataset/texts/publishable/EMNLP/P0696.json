{
  "Abstract": "In Machine Translation (MT) evaluations, theconventional approach is to compare a trans-lated sentence against its human-created refer-ence sentence. MT metrics provide an absolutescore (e.g., from 0 to 1) to a candidate sen-tence based on the similarity with the referencesentence. Thus, existing MT metrics give themaximum score to the reference sentence. How-ever, this approach overlooks the potential fora candidate sentence to exceed the referencesentence in terms of quality. In particular, re-cent advancements in Large Language Models(LLMs) have highlighted this issue, as LLM-generated sentences often exceed the quality ofhuman-written sentences. To address the prob-lem, we introduce the Residual score Metric(RESUME), which evaluates the relative qual-ity between reference and candidate sentences.RESUME assigns a positive score to candidatesentences that outperform their reference sen-tences, and a negative score when they fallshort. By adding the residual scores from RE- SUME to the absolute scores from MT metrics,it can be possible to allocate higher scores tocandidate sentences than what reference sen-tences are received from MT metrics. Experi-mental results demonstrate that RESUME en-hances the alignments between MT metrics andhuman judgments both at the segment-level andthe system-level.",
  "Introduction": "Evaluation metrics for Machine Translation (MT)mainly rely on comparisons with human-craftedreference sentences. Most frequently used metricscount the number of matching tokens between thereference and candidate sentences (Papineni et al.,2002; Popovic, 2015) or estimate the similaritywithin the embedding space (Zhang et al., 2020).These metrics impose penalties on candidate sen-tences that show less alignment with the reference",
  "Major in Bio Artificial Intelligence": "sentence while rewarding those that closely matchthe reference.However, the current design of MT metricsleads to a limitation: they overlook the possibil-ity that candidate sentences outperform referencesentences in terms of quality. Candidate sentencescan be better than their reference sentences in re-ality for two reasons. Firstly, reference sentences,subject to the variability of human translators, maycontain errors or inconsistencies. Thus, referencesentences are not guaranteed to be of best qual-ity and often include critical translation and gram-matical errors (Freitag et al., 2023).Secondly,Large Language Models (LLMs) exhibit remark-able generative capability. As recent advancementsfocus on training LLMs to follow human feed-backs (Ouyang et al., 2022), LLMs align with hu-man preferences increasingly. Consequently, LLMsshow strong performance across various genera-tive tasks and even surpass average human per-formance in academic exams originally designedfor humans (Achiam et al., 2023). Notably, someLLMs have been shown to produce better trans-lations than gold human references according toautomatic evaluation metrics (Xu et al., 2024) andcan be rated higher than a human reference systemfor specific language pairs (Freitag et al., 2023).Nevertheless, conventional MT metrics inherentlyassign the highest scores to candidate sentences thatare identical to the reference sentence, thereby suf-fering from the reference-bias problem (Fomichevaand Specia, 2016). As a result, current MT met-rics do not allocate a higher score to a candidatesentence than the scores assigned to the referencesentence even when the candidate sentence is su-perior to its reference sentence. This situation re-quires a metric design that recognizes the potentialsuperiority of machine-generated content.To address the problem mentioned above, wepresent Residual score Metric (RESUME), a novelapproach for MT evaluation. While traditional MT metrics allocate an absolute score to the candidatesentence based on the similarity with the referencesentence, our approach RESUME quantifies the rel-ative quality of the candidate sentence compared tothe reference sentence. The relative score is positivewhen the candidate sentence is better than the ref-erence sentence, and negative when the referencesentence surpasses the candidate sentence. Then,we add the relative score to the absolute score com-puted by an existing MT metric. By introducingRESUME, it has become possible to assign a higherscore to the candidate sentence better than to thereference sentence itself. As a result, the proposedmethod can be utilized to appropriately evaluateLLMs that provide translation results superior tothose created by humans. In the experimental sec-tion, we report an interesting observation that GPT-4 outperforms conventional MT systems with theproposed method RESUME.Labeling a substantial amount of residual scoresto train our model is both time-consuming andcostly. For this reason, we propose a method totrain our RESUME model using existing labeleddata with standard absolute scores, eliminating theadditional labeling costs. We conduct extensive ex-periments to validate the performance of RESUMEboth with the segment-level and the system-levelevaluations. Specifically, RESUME encourages thealignment between MT metrics and human expertratings across various language pairs at the seg-ment level evaluation even without residual scorelabels. In system-level evaluation, our approach af-fords more accurate assessments for MT systemsespecially for LLMs which are underestimated byconventional MT metrics. Furthermore, RESUMEcan assess candidates outperforming references ac-curately compared to other MT metrics. The codeis available via a GitHub repository1. Our maincontributions are summarized as follows: We identify a new issue that current MT met-rics fail to assign elevated scores to better can-didates than references compared to the scoresgiven to the references, suffering from the ref-erence bias problem.",
  "Related Work": "Lexical overlap metrics. Conventional MT met-rics focus on evaluating candidate sentences at thesurface-from level. Such metrics (Papineni et al.,2002; Banerjee and Lavie, 2005; Popovic, 2015,2017) count n-grams appearing in both referenceand candidate sentences simultaneously. Neverthe-less, these metrics fail to recognize semanticallyequivalent, but lexically diverse words, resultingin incorrect scores on paraphrased sentences. Thisattribute leads to a lower correlation with humanjudgements. Embedding similarity metrics. Word embeddingsare employed in order to capture semantic similar-ity between words. Specifically, since Pre-trainedLanguage Models (PLMs) (Devlin et al., 2019; Liuet al., 2019) are learned from huge amounts of textdata, its contextual embedding enables a deeperunderstanding of words between reference and can-didate sentences. Zhang et al. (2020) extracts tokenembeddings corresponding to reference and can-didate sentences using BERT or RoBERTa. Then,the final score is derived by calculating the cosinesimilarity among these embeddings. Trainable metrics. Recent works (Shimanakaet al., 2018; Sellam et al., 2020) attempt to trainmetrics using a dataset containing human rating.These methods involve adding a regression layeron the top of PLMs and training it to directly pre-dict human rating between reference and candidatesentences. In the machine translation, several stud-ies (Rei et al., 2020a; Kocmi et al., 2022b; Wanet al., 2022) utilize the source sentence as semi-reference jointly during training since it holds thesame meaning with reference sentence, playing animportant role in translation quality assessment.This approach represents improved alignment withhuman ratings in machine translation evaluation.",
  "bias of MT metrics. The reference biasproblem is raised by Fomicheva and Specia (2016);Ma et al. (2017); Freitag et al. (2020). Previous": "studies highlight the bias in conventional MT eval-uation, where subpar candidates sharing a similarstyle with references receive high scores. With ad-vancements in LLMs, a new issue of MT metricshas emerged, which has not been considered by theprevious studies. Although LLM-based models cangenerate high-quality candidates that surpass thereferences, these candidates do not receive higherscores than those given to the references. In thiswork, we identify the new issue and propose a novelmethod to overcome the problem. Zouhar and Bojar (2024) emphasizes the impor-tance of reference quality in the automatic MT eval-uation. Low-quality references can significantly de-grade the performance of MT metrics, leading tounreliable assessments of candidates. However, ob-taining high-quality references is both expensiveand time-consuming. In this work, we introduce anew MT metric that measures the relative qualitybetween the reference and candidates, allowing foran accurate evaluation regardless of the referencequality.",
  "Conventional learnable MT metrics": "The evaluation of MT commonly involves the useof source, reference, and candidate sentences, de-noted as s, r, and c respectively. Given a tripletof (s, r, c), an MT metric is formulated as a scor-ing function f : (s, r, c) R. A dataset fora learnable MT metric comprises a set of tuples{(si, ri, ci, yi)}Ni=1 with size of N, where yi rep-resents the human rating that assesses the qualityof the candidate sentence ci. The primary objec-tive of a learnable metric is to generate scores thatare correlated with human ratings. To achieve this,the training process for a learnable metric involvesminimizing the mean squared error between theprediction of the metric and human ratings",
  "The Residual Metric RESUME": "Current MT metrics are designed to yield the maxi-mum score when evaluating the reference sentenceitself, resulting in a lower score when assessing thecandidate sentence (i.e., f(s, r, c) f(s, r, r)). Asa result, existing methods are inherently incapableof assigning a higher score to a candidate sentenceeven if it is better than the reference sentence (i.e.,f(s, r, c) f(s, r, r)).To address this problem, we introduce a residualscore RESUME to indicate the relative quality ofthe candidate sentence compared to the referencesentence. The RESUME score ranges from -1 to1, outputting a positive value when the candidatesentence c surpasses the reference sentence r inquality, and a negative value when it is worse thanthe reference. We use the summation of RESUMEand an existing absolute score such as BLEU (Pa-pineni et al., 2002) as the evaluation metric forMT (i.e., f(s, r, c) + RESUME(s, r, c)). isa hyperparameter to adjust the importance andscale of the residual score. By adding this resid-ual score to the output of an MT metric, RESUMEenables a better candidate sentence to receive ahigher evaluation than the reference sentence (i.e.,f(s, r, c) + RESUME(s, r, c) > f(s, r, r)).",
  "Training RESUME with Residual Scores": "RESUME takes source, reference, and candidatesentences as input and should consider the rela-tionships between the sentences. We use existingmodel architecture suitable for this process such asUniTE (Wan et al., 2022) and COMET (Rei et al.,2020a). We empirically demonstrate that regardlessof the specific architecture used, RESUME consis-tently enhances the performance of existing MTmetrics in 6.6.Our goal of learning RESUME involves two keyaspects: (1) to assign a positive score when a candi-date sentence is better, a negative score otherwiseand, (2) to achieve a higher correlation than thoseachieved by an existing MT metric without RE-",
  "Training RESUME with Absolute Scores": "In existing datasets for training MT metrics (Bo-jar et al., 2017, 2018; Barrault et al., 2019, 2020),only absolute scores for candidate sentences areavailable, typically ranging from 0 to 100, sinceour work is the first study to measure the relativescore. To utilize the existing datasets, we transformthese absolute scores to the score quantifying thedifference between sentences. We train RESUMEusing the WMT 17-20 shared task data. Since therange of RESUME is from -1 to 1, we normalizethe ratings in the data to a range from 0 to 1.The straightforward method for generating resid-ual scores is to compute the difference in absolutescores among candidate sentences that share thesame source sentence. However, because multiplehuman raters were involved in measuring absolutescores, and their scoring strategies are varied in theexisting dataset, utilizing the difference betweencandidate sentences can be inappropriate (Grahamet al., 2013; Sellam et al., 2020; Kocmi et al.,2022b). To solve the problem, we use referencesentences by assigning a maximum score of 1 toall reference sentences in the dataset. Despite thepresence of some noisy reference sentences, thisapproach is generally effective for learning residualscores, as the training data from WMT 17-20 datalacks translations generated by the recent LLMs, re-sulting in the majority of reference sentences havehigher quality than their candidate sentences. shows the training process of RESUME.Given a tuple (s, r, c, y), where y , we canset the residual score y = score(c) score(r) =y 1 by the above assumption. RESUME is trainedto predict the residual score y by minimizing thefollowing MSE loss",
  "However, training solely through the above processresults in RESUME only returning negative values,": "thereby failing to allocate additional scores to thecandidate sentence better than the reference.Thus, we propose using the reference sentencer as the candidate and the candidate sentence cas the reference conversely to the previous pro-cess. In that case, the residual score is y =score(r) score(c) = 1 y which is a positivevalue. Then, we enable RESUME to output a posi-tive score to the case when the higher quality can-didate sentences are assessed, using the followingobjective function:",
  "Baseline Methods": "We classify the baseline metrics based on whetherthey are trained or not on human ratings. For un-supervised metrics, we select BLEU (Post, 2018),chrF++ (Popovic, 2017), and BERTScore (Zhanget al., 2020). We employ BLEURT (Sellam et al.,2020), COMET (Rei et al., 2020a), UniTE (Wanet al., 2022), MS-COMET-22 (Kocmi et al., 2022b),and xCOMET-XL (Guerreiro et al., 2023) as super-vised metric. The implementation details for eachmetric are as follows:",
  "Datasets": "WMT dataset. The WMT Metrics Shared Taskdatasets are widely used human-rated datasets fortraining MT metrics. The datasets employ twodistinct forms of annotations for human ratings,namely Direct Assessments (DA) and Multidimen-sional Quality Metrics (MQM). DA ratings are de-rived from crowd-sourced evaluations, with scoresranging from 0 to 100. Conversely, MQM ratingsare determined by experts, using a comprehensiveset of error annotations and guidelines (Freitaget al., 2021). As outlined in 4.3, we constructthe train set for RESUME using WMT 17-20 DAdatasets (Bojar et al., 2017, 2018; Barrault et al.,2019, 2020) while comprising the valid set basedon WMT 21 DA dataset (Akhbardeh et al., 2021).To validate the effectiveness of RESUME, we useWMT 22 MQM dataset (Freitag et al., 2022) asthe test set. Notably, the MQM dataset includesonly three language pairs (en-de, en-ru, and zh-en)due to the high cost of expert labeling. There areno overlapping sentences among the train, valid,and test sets. Training examples are provided in",
  "|Concordant| + |Discordant|": "For candidate sentence pairs derived from the samesource sentence, |Concordant| represents the num-ber of sentence pairs where the metric assigns ahigher score to the candidate sentence that also re-ceived a higher human judgement. |Discordant| isthe opposite case.System-level evaluation. We use pairwise accu-racy (Kocmi et al., 2021) on the MQM dataset. Foreach system pair, the difference in metric scores(metric) and the difference in human scores(human) are calculated. The pairwise accuracy isdefined as:",
  "Implementation Details of RESUME": "We train RESUME using COMET Framework (Reiet al., 2020a) using the unified input approach (Wanet al., 2022). The model consists of a XLM andfeedforward layers built on top of the XLM. TheinfoXLMBASE model (Chi et al., 2021) is employedas XLM. The feedforward network comprises 3 lin-ear layers where the output dimensions of eachlayer are , respectively. We useAdamW optimizer (Loshchilov and Hutter, 2018)to train our model and learning rates 1e5 for PLMand 3e5 for feedforward layer with a batch sizeof 8. The number of epochs is 5. For a third of thefirst epoch, we freeze the encoder model to opti-mize the feedforward network, following Rei et al.(2020a). We select the checkpoint which is bestcorrelated with the valid set. The model is trainedon an NVIDIA RTX3090 GPU for 25 hours. When calculating the final score with RESUME,to avoid the complexity of individual adjustments,we employ a common = 0.2 value in our mainexperiments. Despite the fixed value, we empir-ically demonstrate that RESUME consistently en-hances the correlation of various MT metrics withhuman judgments in 6.1. We report the optimal value for each MT metric in Appendix B. Theimpact of the value is explored in 6.7.",
  "Main Results": "shows the performance changes of base-line methods on the WMT 22 MQM dataset whenintegrating RESUME. First, at the segment-level,we observe that RESUME enhances the averagecorrelation for both supervised and unsupervisedmetrics. Specifically, the correlation consistently in-creases across all metrics for non-English-targetedas well as English-targeted pairs. This indicatesthat RESUME can enhance alignments of MT met-rics with expert scores. Second, at the system-level,RESUME also improves the average of pairwise ac-curacy across all metrics. Particularly, considerableperformance gains are observed in unsupervisedmetrics. For instance, there is a larger increase of0.08 in BERTScore, which presents comparableresults to other supervised metrics such as COMETand MS-COMET-22. This suggests that RESUMEcontributes to a more accurate rank of systems thatare underestimated by conventional measurements.Note that the performance could be improved byapplying the optimal value for each metric. Ad-ditionally, we provide the results on medium- andlow-resource language pairs in Appendix D.",
  "Analysis on Reference Bias of MT Metrics": "Using the post-editing dataset, we conduct exper-iments to verify that RESUME actually assignshigher scores to a candidate that is superior to thereference rather than the scores assigned to the ref-erence itself. Post-edited translations refined by ex-perts exhibit superior quality than pre-edited trans-lations. Thus, we examine whether MT metrics as-sign higher scores to post-edited translations than topre-edited translations when pre-edited translationsare used as the references. represents theproportion of instances where each metric assignshigher scores to post-edited translations. ExistingMT metrics allocate higher scores to post-editedtranslations at a very low rate, regardless of whether they are trained or not. For example, widely-usedMT metrics such as COMET and UniTE exhibitrates below 7%. This demonstrates that existingMT metrics fail to evaluate candidates of superiorquality than the reference correctly. However, ourRESUME significantly outperforms other MT met-rics, achieving a much higher rate of 59%. For thecomparison with the reference-free metric, pleaserefer to Appendix A.",
  "Case Study": "We aim to investigate whether RESUME assignsa positive score when the quality of the candidatesentence surpasses the reference sentence utilizingthe post-editing dataset. In the post-editing dataset,we use pre-edited translations containing errorssuch as omission of words as the reference, andpost-edited translations, refined by experts, as can-didates. shows the scoring results of MTmetrics.red indicates the error locations in thepre-edited translations whileyellow shows cor-rected parts in the post-edited translations.The first example represents a case where pre-edited translations contain mistranslation errors.For BERTScore and MS-COMET-22, these met-rics fail to award higher scores to the post-editedtranslation compared to the score of the pre-editedtranslation. However, RESUME allocates a positivescore to the post-edited translation, enabling it tobe evaluated higher than its original scores.The second example showcases a scenario wherethe sources words are omitted in a pre-edited trans-lation in addition to a mistranslation error. Dueto the lexical difference from the pre-edited trans-lation, the post-edited translation receives lowerscores from BERTScore and MS-COMET-22 thanscores of the pre-edited translation, despite of theirhigher quality. Nevertheless, RESUME assigns ad-ditional positive scores to post-edited translations,allowing them to receive a more accurate eval-uation. For more examples, please refer to Ap-pendix F.",
  "Following Hendy et al. (2023), we compare GPTmodels and the best-performing MT system in theWMT22 en-zh pair. represents the scor-ing results when RESUME is either applied or not": "The top-ranked system in WMT22 en-zh pair ismarked in Online-W (Kocmi et al., 2022a). Incontrast to the human evaluation in Hendy et al.(2023), BLEU selects the Online-W system as thehighest-performing system in the en-zh pair. Nev-ertheless, the incorporation of RESUME into theevaluation process aligns with the previous finding,showcasing GPT-3s superiority over the Online-Wsystem. Additionally, the summation of RESUMEwith conventional MT metric produces an interest-ing result by choosing GPT-4, known for its supe-rior generative performance (Achiam et al., 2023),as the best MT system in the en-zh pair. Theseresults indicate that RESUME precisely evaluateshigh-performance MT systems.",
  "Architecture of RESUME": "We investigate whether the performance enhance-ment of MT metrics is due to the specific model ar-chitecture employed for RESUME. showsthe variations in the average correlation of MT met-rics on WMT 22 MQM dataset at the segment-levelwhen varying the model architecture of RESUME.RESUME (UniTE) denotes the model trainedusing the UniTE architecture while RESUME(COMET) indicates the mode trained using theCOMET architecture. A consistent trend is ob-",
  ": The impact of model architecture for RESUMEon the average correlation of MT metrics": "ual score. represents the changes in theaverage correlation of MT metrics on WMT 22MQM dataset at the segment-level. Depending onthe existing metric, the optimal values of are dif-ferent. Specifically, many supervised metrics attainhigher correlation when < 0.5, while most un-supervised metrics attain near = 1. Thus, werecommend using the weight 1 for existingMT metrics. In our experiment, we use = 0.2for all metrics instead of using individual optimalvalues to avoid overfitting.",
  ": Changes in the average correlation of MTmetrics according to RESUME score ratios": "isting MT metrics, we address the reference-biasproblem in current MT evaluation, which ariseswith significant advancements in MT systems: thecandidate sentences, produced by MT systems, ex-cel the quality of the reference sentences. The em-pirical results demonstrate that RESUME not onlyincreases the alignment between MT metrics andhuman judgements but also enables to discrimi-nate between top MT systems. Furthermore, wedisclose analyses that RESUME indeed evaluateshigher quality translations over references moreaccurately against other MT metrics.",
  "Limitations": "Although RESUME improves the performance ofexisting MT metrics, the evaluation time may beslightly longer because the final scores are obtainedby adding the residual scores with the output ofother MT metrics. One of the methods generatingresidual scores could calculate the scores of can-didate sentences in existing human-rated datasets,as mentioned in 4.3. However, when we trainedRESUME containing this method, we observed thedecreased performance of RESUME. Despite the good performance of RESUME inour experiments, we partly agree that the assump-tion of assigning a reference score of 1 during thetraining process is revisited in future works to fullyleverage recent datasets (e.g., WMT 22, 23) thatinclude a growing proportion of LLM-generatedtranslations. In addition, we believe that RESUMEcould be enhanced by including human-annotatedresidual scores into the training data. This work was supported by Institute of In-formation & communications Technology Plan-ning & Evaluation(IITP) grant funded by the Ko-rea government(MSIT) (No.RS-2023-00261068,Development of Lightweight Multimodal Anti-Phishing Models and Split-Learning Techniquesfor Privacy-Preserving Anti-Phishing). This workwas supported by Institute of Information &communications Technology Planning & Evalu-ation (IITP) grant funded by the Korea govern-ment(MSIT) (No.RS-2022-00155885, ArtificialIntelligence Convergence Innovation Human Re-sources Development (Hanyang University ER-ICA)). Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Farhad Akhbardeh, Arkady Arkhangorodsky, Mag-dalena Biesialska, Ondrej Bojar, Rajen Chatter-jee, Vishrav Chaudhary, Marta R. Costa-jussa,Cristina Espaa-Bonet, Angela Fan, Christian Fe-dermann, Markus Freitag, Yvette Graham, Ro-man Grundkiewicz, Barry Haddow, Leonie Harter,Kenneth Heafield, Christopher Homan, MatthiasHuck, Kwabena Amponsah-Kaakyire, Jungo Kasai,Daniel Khashabi, Kevin Knight, Tom Kocmi, PhilippKoehn, Nicholas Lourie, Christof Monz, MakotoMorishita, Masaaki Nagata, Ajay Nagesh, ToshiakiNakazawa, Matteo Negri, Santanu Pal, Allahsera Au-guste Tapo, Marco Turchi, Valentin Vydrin, and Mar-cos Zampieri. 2021. Findings of the 2021 conferenceon machine translation (WMT21). In Proceedings ofthe Sixth Conference on Machine Translation, pages188, Online. Association for Computational Linguis-tics. Satanjeev Banerjee and Alon Lavie. 2005. METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments. In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 6572, Ann Arbor,Michigan. Association for Computational Linguis-tics. Loc Barrault, Magdalena Biesialska, Ondrej Bo-jar, Marta R. Costa-juss, Christian Federmann,Yvette Graham, Roman Grundkiewicz, Barry Had-dow, Matthias Huck, Eric Joanis, Tom Kocmi,Philipp Koehn, Chi-kiu Lo, Nikola Ljubeic, ChristofMonz, Makoto Morishita, Masaaki Nagata, Toshi-aki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference onmachine translation (WMT20). In Proceedings ofthe Fifth Conference on Machine Translation, pages155, Online. Association for Computational Linguis-tics. Loc Barrault, Ondrej Bojar, Marta R. Costa-juss,Christian Federmann, Mark Fishel, Yvette Gra-ham, Barry Haddow, Matthias Huck, Philipp Koehn,Shervin Malmasi, Christof Monz, Mathias Mller,Santanu Pal, Matt Post, and Marcos Zampieri. 2019.Findings of the 2019 conference on machine trans-lation (WMT19). In Proceedings of the Fourth Con-ference on Machine Translation (Volume 2: SharedTask Papers, Day 1), pages 161, Florence, Italy. As-sociation for Computational Linguistics. Ondrej Bojar, Rajen Chatterjee, Christian Federmann,Yvette Graham, Barry Haddow, Shujian Huang,Matthias Huck, Philipp Koehn, Qun Liu, VarvaraLogacheva, Christof Monz, Matteo Negri, Matt Post,Raphael Rubino, Lucia Specia, and Marco Turchi.2017. Findings of the 2017 conference on machinetranslation (WMT17). In Proceedings of the SecondConference on Machine Translation, pages 169214,Copenhagen, Denmark. Association for Computa-tional Linguistics. Ondrej Bojar, Christian Federmann, Mark Fishel, YvetteGraham, Barry Haddow, Matthias Huck, PhilippKoehn, and Christof Monz. 2018. Findings of the2018 conference on machine translation (WMT18).In Proceedings of the Third Conference on MachineTranslation: Shared Task Papers, pages 272303, Bel-gium, Brussels. Association for Computational Lin-guistics. Zewen Chi, Li Dong, Furu Wei, Nan Yang, SakshamSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,Heyan Huang, and Ming Zhou. 2021. InfoXLM: Aninformation-theoretic framework for cross-linguallanguage model pre-training. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 35763588, On-line. Association for Computational Linguistics. Hyung Won Chung, Thibault Fevry, Henry Tsai, MelvinJohnson, and Sebastian Ruder. 2020. Rethinking em-bedding coupling in pre-trained language models. InInternational Conference on Learning Representa-tions. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 7782, Berlin, Germany. As-sociation for Computational Linguistics": "Marina Fomicheva, Shuo Sun, Erick Fonseca, ChrysoulaZerva, Frdric Blain, Vishrav Chaudhary, FranciscoGuzmn, Nina Lopatina, Lucia Specia, and AndrF. T. Martins. 2022.MLQE-PE: A multilingualquality estimation and post-editing dataset. In Pro-ceedings of the Thirteenth Language Resources andEvaluation Conference, pages 49634974, Marseille,France. European Language Resources Association. Markus Freitag, George Foster, David Grangier, VireshRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.Experts, errors, and context: A large-scale study ofhuman evaluation for machine translation. Transac-tions of the Association for Computational Linguis-tics, 9:14601474. Markus Freitag, David Grangier, and Isaac Caswell.2020. BLEU might be guilty but references are notinnocent. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 6171, Online. Association forComputational Linguistics. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Elefthe-rios Avramidis, Ricardo Rei, Brian Thompson, TomKocmi, Frederic Blain, Daniel Deutsch, Craig Stew-art, Chrysoula Zerva, Sheila Castilho, Alon Lavie,and George Foster. 2023. Results of WMT23 metricsshared task: Metrics might be guilty but referencesare not innocent. In Proceedings of the Eighth Con-ference on Machine Translation, pages 578628, Sin-gapore. Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,Craig Stewart, Eleftherios Avramidis, Tom Kocmi,George Foster, Alon Lavie, and Andr F. T. Martins.2022. Results of WMT22 metrics shared task: Stopusing BLEU neural metrics are better and morerobust. In Proceedings of the Seventh Conferenceon Machine Translation (WMT), pages 4668, AbuDhabi, United Arab Emirates (Hybrid). Associationfor Computational Linguistics. Yvette Graham, Timothy Baldwin, Alistair Moffat, andJustin Zobel. 2013. Continuous measurement scalesin human evaluation of machine translation. In Pro-ceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 3341,Sofia, Bulgaria. Association for Computational Lin-guistics. Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, LuisaCoheur, Pierre Colombo, and Andr FT Martins.2023. xcomet: Transparent machine translation eval-uation through fine-grained error detection. arXivpreprint arXiv:2310.10482.",
  "Awadalla. 2023. How good are gpt models at ma-chine translation? a comprehensive evaluation. arXivpreprint arXiv:2302.09210": "Tom Kocmi, Rachel Bawden, Ondrej Bojar, AntonDvorkovich, Christian Federmann, Mark Fishel,Thamme Gowda, Yvette Graham, Roman Grund-kiewicz, Barry Haddow, Rebecca Knowles, PhilippKoehn, Christof Monz, Makoto Morishita, MasaakiNagata, Toshiaki Nakazawa, Michal Novk, MartinPopel, and Maja Popovic. 2022a. Findings of the2022 conference on machine translation (WMT22).In Proceedings of the Seventh Conference on Ma-chine Translation (WMT), pages 145, Abu Dhabi,United Arab Emirates (Hybrid). Association for Com-putational Linguistics. Tom Kocmi, Christian Federmann, Roman Grund-kiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-sushita, and Arul Menezes. 2021. To ship or not toship: An extensive evaluation of automatic metricsfor machine translation. In Proceedings of the SixthConference on Machine Translation, pages 478494,Online. Association for Computational Linguistics. Tom Kocmi, Hitokazu Matsushita, and Christian Feder-mann. 2022b. MS-COMET: More and better humanjudgements improve metric performance. In Proceed-ings of the Seventh Conference on Machine Transla-tion (WMT), pages 541548, Abu Dhabi, United ArabEmirates (Hybrid). Association for ComputationalLinguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "Ilya Loshchilov and Frank Hutter. 2018. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Qingsong Ma, Yvette Graham, Timothy Baldwin, andQun Liu. 2017. Further investigation into referencebias in monolingual evaluation of machine transla-tion. In Proceedings of the 2017 Conference on Em-pirical Methods in Natural Language Processing,pages 24762485, Copenhagen, Denmark. Associa-tion for Computational Linguistics. Qingsong Ma, Johnny Wei, Ondrej Bojar, and YvetteGraham. 2019.Results of the WMT19 metricsshared task: Segment-level and strong MT sys-tems pose big challenges.In Proceedings of theFourth Conference on Machine Translation (Volume2: Shared Task Papers, Day 1), pages 6290, Flo-rence, Italy. Association for Computational Linguis-tics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Com-putational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Maja Popovic. 2015. chrF: character n-gram F-scorefor automatic MT evaluation. In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 392395, Lisbon, Portugal. Association forComputational Linguistics. Maja Popovic. 2017. chrF++: words helping charac-ter n-grams. In Proceedings of the Second Confer-ence on Machine Translation, pages 612618, Copen-hagen, Denmark. Association for Computational Lin-guistics. Matt Post. 2018. A call for clarity in reporting BLEUscores. In Proceedings of the Third Conference onMachine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computa-tional Linguistics. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020a. COMET: A neural framework for MTevaluation. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 26852702, Online. Associationfor Computational Linguistics. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020b. Unbabels participation in the WMT20metrics shared task. In Proceedings of the Fifth Con-ference on Machine Translation, pages 911920, On-line. Association for Computational Linguistics.",
  "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "BLEURT: Learning robust metrics for text genera-tion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78817892, Online. Association for ComputationalLinguistics. Hiroki Shimanaka, Tomoyuki Kajiwara, and MamoruKomachi. 2018. RUSE: Regressor using sentenceembeddings for automatic machine translation eval-uation. In Proceedings of the Third Conference onMachine Translation: Shared Task Papers, pages 751758, Belgium, Brussels. Association for Computa-tional Linguistics. Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang,Boxing Chen, Derek Wong, and Lidia Chao. 2022.UniTE: Unified translation evaluation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 81178127, Dublin, Ireland. Associationfor Computational Linguistics.",
  ": The percentage of instances where the MT met-rics assign a higher score to the post-edited translationcompared to the pre-edited translation": "We investigate that reference-free metric assignshigher scores to post-edited translations when us-ing pre-edited translations as references, as out-lined in 6.2. shows the result of COMET-QE, the reference-free version of COMET, on thepost-editing dataset. COMET-QE allocates greaterscores to post-edited translations more frequently,compared to COMET, as it is not influenced by thequality of references. However, COMET-QE showsa lower rate compared to our RESUME. This in-dicates that our RESUME evaluates higher-qualitytranslations more accurately than the method with-out references.",
  "SUME, we experiment with the WMT 20, 21, and23 MQM datasets. To avoid overlap between thetraining and evaluation datasets, we utilize RE-": "SUME trained on the WMT 17-18 DA datasets forthe WMT 20 MQM dataset and RESUME trainedon the WMT 17-19 DA datasets for the WMT 21MQM dataset. For the WMT 23 MQM dataset,we employ the same RESUME checkpoint used inour main experiment without any additional train-ing. As shown in , 12, and 13, RESUMEenhances the performance of baseline metrics inthe newly introduced dataset at both segment- andsystem-levels.",
  ": Training examples used for RESUME": "2024). These candidate sentences are then scored. shows the scoring results assigned by ex-isting MT metrics and RESUME.The first example presents that the English wordcorresponding to In Bangladesch does not existin reference. In the case of BERTScore and MS-COMET-22, these metrics do not assign higherscores to the candidate sentence rather than theevaluation of the reference sentence itself. How-ever, RESUME gives a positive value to the can-didate sentence, allowing it to receive a higher as-sessment.The second example shows a case where cer-tain words, found in the reference, are absent fromthe source. Notably, there are no English counter-parts for Wrzburg and echzig in the reference.Moreover, the the 1860s word appears in the ref-erence sentence but does not exist in the source sen-tence. The assessment of the candidate is decreasedusing BERTScore and MS-COMET-22. However,RESUME assigns an additional score, resulting inproper evaluation for the candidate sentence.Furthermore, we also provide examples from thepost-editing dataset. As shown in , existingMT metrics struggle to assign higher scores to post-edited translations than to erroneous pre-editedtranslations when pre-edited translations serve asreferences. Nevertheless, since RESUME can as-sess translations regardless of the reference quality,post-edited translations can receive higher evalua-tions, when RESUME is applied, despite the pres-ence of flawed references."
}