{
  "Abstract": "Charts are important for presenting and ex-plaining complex data relationships. Recently,multimodal large language models (MLLMs)have shown remarkable capabilities in chart un-derstanding. However, the sheer size of thesemodels limits their use in resource-constrainedenvironments. In this paper, we present Tiny-Chart, an efficient MLLM for chart understand-ing with only 3B parameters. TinyChart over-comes two key challenges in efficient chart un-derstanding: (1) reduce the burden of learn-ing numerical computations through Program-of-Thoughts (PoT) learning, which trains themodel to generate Python programs for nu-merical calculations, and (2) reduce lengthyvision feature sequences through Vision TokenMerging, which gradually merges most similarvision tokens. Extensive experiments demon-strate that our 3B TinyChart achieves SOTAperformance on various chart understandingbenchmarks including ChartQA, Chart-to-Text,Chart-to-Table, OpenCQA, and ChartX. It out-performs several chart-understanding MLLMswith up to 13B parameters, and close-sourcedMLLM GPT-4V on ChartQA, with higherthroughput during inference due to a smallermodel scale and more efficient vision encoding.",
  "Introduction": "As an important information source, charts canintuitively visualize data in various visual pre-sentation forms and have become an indispens-able part of information dissemination, businessdecision-making, and academic research (Huanget al., 2024a). Automatic chart understanding re-ceived increasing attention from the research com-munity (Han et al., 2023; Meng et al., 2024; Masryet al., 2024; Chen et al., 2024a). Recently, Mul-timodal Large Language Models (MLLMs) haveshown strong capability in comprehending images",
  ": Our TinyChart-3B outperforms several 13BMLLMs on a variety of chart understanding benchmarks(a), while achieving larger inference throughput (b)": "and following instructions (OpenAI, 2023b; Liuet al., 2024a; Ye et al., 2024; Liu et al., 2023d; Linet al., 2023; Ye et al., 2023c; Zhang et al., 2023b;Dong et al., 2024a,b). Based on MLLMs, somerecent works (Han et al., 2023; Meng et al., 2024;Masry et al., 2024; Hu et al., 2024a) further buildchart understanding models by constructing versa-tile chart comprehension datasets and performingsupervised fine-tuning, They achieve significantperformance increase in chart understanding bench-marks (Masry et al., 2022; Kantharaj et al., 2022b). Despite their success, current chart understand-ing models still face three main limitations: (1)Considerable amount of parameters makes trainingand deployment challenging. For example, ChartL-lama (Han et al., 2023) has 13 billion parameters,which is hard to deploy on a single GPU with lessthan 26 GB of VRAMs. (2) They are prone toerrors when tackling questions involving numer-ical calculations (Meng et al., 2024), which aredifficult to answer directly without any reasoning steps. (3) They struggle with efficiently encodingfor high-resolution images since the standard vi-sion transformer would produce lengthy featuresequences.To overcome such limitations in chart under-standing, we propose an efficient and powerfulMLLM, namely TinyChart. As shown in ,through the efficient visual encoding and Program-of-Thoughts learning strategy, TinyChart achievesstate-of-the-art performances on various chart un-derstanding benchmarks with only 3B parameters,while excelling in faster inference throughput.For efficient visual encoding, we propose tomerge visual tokens based on the observation thatchart images often contain large areas of color andwhite spaces. Inspired by Bolya et al. (2023), weadopt a parameter-free Visual Token Merging mod-ule inside each vision transformer layer, which ag-gregates the most similar visual tokens and grad-ually reduces the length of the visual feature se-quence, This enables controllable computation loadwhen encoding high-resolution images.To learn chart understanding more efficiently, in-spired by Chen et al. (2023), we propose Program-of-Thoughts (PoT) learning that trains the modelto generate Python programs for the computa-tion problems step by step.The programs arethen passed to a Python interpreter to producefinal answers. To support PoT learning, we fur-ther construct the ChartQA-PoT dataset based onChartQA (Masry et al., 2022). The QA pairs inChartQA-PoT are constructed in two ways: (1)Template-based PoT, which generates questionsand programs by filling in manually written tem-plates with chart data. (2) GPT-based PoT, whichleverages gpt-3.5-turbo (OpenAI, 2023a) to pro-duce programs for human-written questions. Exper-imental results show that PoT learning can signifi-cantly improve the question-answering, especiallynumerical question-answering ability of TinyChart.The main contributions of this work are as follows: We introduce TinyChart, an efficient multimodalchart understanding model. It outperforms sev-eral 13B MLLMs and achieves state-of-the-artperformances on various chart understandingbenchmarks, while excelling in inference speed.",
  "Related Work": "Chart Understanding requires the model to com-prehend chart contents and accomplish relatedtasks such as data extraction (Liu et al., 2023a),QA (Masry et al., 2022; Methani et al., 2020;Kafle et al., 2018), summarization (Kantharajet al., 2022b; Obeid and Hoque, 2020), and re-rendering (Han et al., 2023). It demands robust textrecognition capabilities and computational reason-ing from the model (Meng et al., 2024). Early ap-proaches (Singh et al., 2019; Methani et al., 2020;Liu et al., 2023a; Fu et al., 2022; Zhang et al.,2023a; Hu et al., 2021) rely on off-the-shelf OCRtools to transform charts into textual representa-tions such as data tables, and use language mod-els to complete specified tasks. These approachessuffer from error accumulation since can not beoptimized jointly.",
  "Recent studies (Masry et al., 2023; Liu et al.,": "2023b; Han et al., 2023; Meng et al., 2024;Masry et al., 2024; Liu et al., 2023c) have shiftedtowards end-to-end methods based on MLLMs.They enhance MLLMs (Liu et al., 2024a, 2023d;Ye et al., 2024, 2023c; Lin et al., 2023) chartunderstanding abilities through supervised fine-tuning (Ouyang et al., 2022) on chart instructiondata (Han et al., 2023; Meng et al., 2024; Masryet al., 2024). Although these models achieves supe-rior performance, the extensive model size preventsthem from being easily trained or deployed underresource-constrained scenarios. In this paper, wedemonstrate that a 3B MLLM is enough to achieveSOTA performance on chart understanding. Meanwhile, it is observed that MLLMs are proneto numerical errors (Masry et al., 2024; Meng et al.,2024). To resolve this, Meng et al. (2024) constructtemplate-based executable command lines for nu-merical calculations. Their usage is constrained bythe specific template and backend. In contrast, wetrain the model to generate Python code, which ismore versatile. Also, we include GPT-generatedprograms from human-written questions in trainingdata, which further improves the coverage of ourmethod to different questions. Multimodal Large Language Models (MLLMs)exhibit strong capabilities in visual understandingand instruction following (OpenAI, 2023b; Teamet al., 2023). These models are generally trained onextensive general image-text data for cross-modalalignment and instruction fine-tuning (Liu et al.,2024a, 2023d; Li et al., 2024; Zhou et al., 2024;Zhang et al., 2023b; Dong et al., 2024a; Ye et al.,2023b; Chen et al., 2024b). Some studies (Liuet al., 2024b; Zhang et al., 2020) demonstrate adegree of OCR capability in these MLLMs, theirperformance on document and chart understand-ing benchmarks remains sub-optimal due to theirlow input resolution (Ye et al., 2023a; Dong et al.,2024b). Efforts in the general document domainhave attempted to improve the fine-grained under-standing capabilities of MLLMs by increasing res-olution (Bai et al., 2023), segmenting images (Yeet al., 2023a; Hu et al., 2024b; Dong et al., 2024b),utilizing frequency domain signals (Feng et al.,2023), and introducing additional high-resolutionencoders (Hong et al., 2023). However, these mod-els often suffer from low efficiency, primarily dueto the excessive length of the high-resolution vi-sual sequences. The visual token merging methodadopted in this paper can effectively reduce thelength of visual sequences and reduce the compu-tational cost of processing high-resolution input.",
  "Model Architecture": "shows the overview of TinyChart.Itconsists of a vision transformer encoder, a vision-language connector, and a large language model.To encode high-resolution visual input effectively,we insert the visual token merging module insideeach vision transformer layer.Vision Transformer Encoder process chart im-ages into vision features. A standard vision trans-former (Dosovitskiy et al., 2020) first resizes theinput image I into a fixed resolution and crops theimage into patches. Then the patches are treatedas vision tokens and processed with transformerencoder layers (Vaswani et al., 2017). Suppose theinput image INN is in resolution N N, andthe patch size is P P, the length of vision fea-ture would be N",
  "P 2. In practice, when N is large,the vision feature sequence can be very long andinefficient for the language model to handle.Visual Token Merging. Since key information": "(such as OCR words) in a chart can be unrecog-nizable in low-resolution images (Hu et al., 2024b),high-resolution input is essential for chart under-standing. However, charts typically contain a largenumber of color blocks and blank spaces, wherepatches are visually similar. To achieve efficientchart understanding, we apply Visual Token Merg-ing (Bolya et al., 2023) in each transformer layer.The process of Visual Token Merging is shown in. By merging the r most similar tokenpairs, it reduces the length of the vision feature byr in each layer. We measure the similarity betweentwo tokens using the cosine distance between Keysfrom self-attention following (Bolya et al., 2023).As shown in the lower part of , the mergingprocess finds the top-r similar token pairs throughbipartite graph matching. It first divides the visiontokens into two disjoint sets. Then, for each to-ken in one set, it finds the most similar tokens inthe other set and draws an edge between the twotokens. After that, it only keeps the top-r mostsimilar edges and merges the features of the twoendpoints through average pooling. Note that non-adjacent tokens can also be merged if they belongto different subsets.Proportional Attention. The visual token mergingoperation aggregates tokens with a similar featureinto one. Therefore, it will reduce the proportion ofthis visual feature in the attention calculation in thefollowing transformer layer, since the number ofthis feature has decreased. To address this, we letthe attention operation consider the actual numberof patches s represented by each token as follows:",
  "d+ log sV(1)": "Where Q, K, V denotes the query, key, and valueof self-attention which are linear projected fromthe hidden states (Vaswani et al., 2017). By addinglog s inside softmax, the token that merged froms patches are duplicated by s times in the attentioncalculation (Bolya et al., 2023).Vision Language Connector projects the visionfeatures into the embedding space of the large lan-guage model. Following (Liu et al., 2023d; Zhouet al., 2024), we implement the vision-languageconnector as a MLP with one hidden layer andGeLU (Hendrycks and Gimpel, 2016) activation.Large Language Model comprehend both visualfeatures and language instructions, and then gen-erate responses to accomplish chart understand-ing tasks. It is implemented as a transformer de-",
  "Program-of-Thoughts Learning": "Program-of-Thoughts (PoT) learning aims to en-hance the learning efficiency of models for numer-ical computation. In PoT learning, the model istrained to generate Python codes, whose executeresults are answers of given questions. Compared to short answers that only contain the calculatedvalues, the Python code includes comments andmulti-step reasoning processes, which is easy toproduce by large language models.ChartQA-PoT Dataset To support PoT learn-ing on chart understanding, we construct theChartQA-PoT dataset based on the training split ofChartQA (Masry et al., 2022). ChartQA-PoT con-tains 140,584 (question, PoT answer) pairs. EachPoT answer consists of multiple lines of Pythoncode. We provide natural language comments foralmost all code lines to explain their behaviors. Weemploy two approaches for constructing (question,PoT answer) pairs: Template-based PoT, and GPT-based PoT.Template-based PoT Based on the chart imagesin ChartQA, we construct template-based (ques-tion, PoT answer) pairs. As illustrated in the up-per half of , the Template-based PoT is PoT Answer# Get the values of 'Good' and 'Bad' assessments for each year, set to Good and Bad respectivelyGood=Bad=# Calculate the absolute difference between Good and Bad for each year, set to DiffDiff=np.abs(np.subtract(Good, Bad))# Find the index that maximizes Diff, set to MaxIndexMaxIndex=np.argmax(Diff)# Get the year corresponding to MaxIndex, set to AnswerAnswer=2010+MaxIndex",
  ": The demonstration of constructing Template-based PoT (upper half) and GPT-based PoT (lower half) inthe ChartQA-PoT dataset": "constructed based on human-written templates con-taining placeholders for both questions and code.The template questions involve common numericaloperations such as calculating the sum, average,minimal, and maximum values. We adopt the 40template questions proposed by PlotQA (Methaniet al., 2020) and manually write their correspond-ing template Python code to solve them. As shownin the top-left part of , the template codeconsists of several variable assignment operationswith NumPy (van der Walt et al., 2011) functionsto perform calculations. The beginning steps usu-ally involve extracting the relevant data from thechart and assigning them to variables. The finalcomputed result is stored in a variable named \"An-swer\". For each placeholder in the template, weidentify all possible values from the data table ofthe chart and randomly select one to fill in theplaceholder. We then apply rule-based strategiesto remove non-executable PoT answers and unrea-sonable fill-ins, and finally successfully construct119,281 (question, PoT pairs) over 17,498 imagesfrom ChartQA. GPT-based PoT Although the template-basedmethod allows for the construction of a large num-ber of question-answer pairs, the diversity of thesepairs is limited due to the fixed templates. To im-prove the generalization ability of PoT learning, wehave additionally built GPT-generated PoT data byleveraging the powerful command-following andcode-generation capabilities of large language mod- els. Specifically, we prompt gpt-3.5-turbo (Ope-nAI, 2023a) to generate PoT answers similar tothe template PoT format for questions annotated inChartQA using in-context examples. As shownin , since gpt-3.5-turbo does not ac-cept image input, we also provide the data ta-ble corresponding to the chart as text input togpt-3.5-turbo. We screen the quality of the gen-erated PoT answers by running them through aPython interpreter. If the annotated PoT answercan not run on the Python interpreter, or if the an-swer obtained is different from the annotated onein ChartQA, then the corresponding PoT Answer isdeleted. In the end, we construct 21,303 (question,PoT Answer) pairs on 15,521 chart images.",
  "Multitask Learning": "We perform multitask learning to train the Tiny-Chart model. We collect a chart understandingdataset that contains 1.36M samples for supervisedfine-tuning. It covers various chart understandingtasks including chart question answering, chart-to-text generation, chart-to-table generation, and chartinstruction following. We mix data in each task to-gether jointly for training and distinguish them withtask-specified instructions. Note that in ablationstudies, we train solely with benchmark datasetsdue to limited computational resources. The bench-mark datasets consist of basic chart understandingevaluations including QA, summary, and chart-to-table generation. We present the detailed composi-",
  "Evaluation Benchmarks": "ChartQA provides a short answer to each ques-tion based on the chart content (Masry et al., 2022).We report the relaxed accuracy that allows numeri-cal error within 5% as the metric following Masryet al. (2022); Han et al. (2023); Meng et al. (2024).Note that TinyChart with PoT learning can performChartQA in the following five settings:1) Direct: model produces short answers directly.2) PoT: model produces Python code and gets finalanswers through a Python interpreter.3) Combine: we let the model produce PoT an-swers for calculative questions, and Direct answersfor others. Questions contain one of the keywords1 are considered calculative questions.4) Auto: we train the model to automatically decidewhether to produce PoT answers or Direct answersfor each question.5) Oracle: we let the model produce both Directand PoT answers for each question separately, andalways choose the correct one.We evaluate TinyChart under the Combine settingby default.Chart-to-Text aims to generate a summary textbased on chart content. We evaluate the model withthe Pew dataset (Kantharaj et al., 2022b), and reportBLEU4 (Papineni et al., 2002) as the metric2.Chart-to-Table aims to extract the underlying datatable presented by the chart. We evaluate the per-formance of Chart-to-Table with the data table an-notation provided by ChartQA (Masry et al., 2022)following (Han et al., 2023; Meng et al., 2024). Wereport RMSF1 (Liu et al., 2023a) as the metric.OpenCQA Different from ChartQA, OpenCQAevaluates the ability of models to generate free-form answers to the chart-related questions (Kan-tharaj et al., 2022a). We report BLEU4 as the met-ric following (Masry et al., 2024).ChartX is a chart comprehension benchmark thatcontains more visual types (Xia et al., 2024). Weevaluate the ChartX cognition tasks since they aremore challenging. It covers Question Answering,Chart Description, Chart Summary, and Chart Re- 1sum, mean, average, ratio, mode, divide, dividing, differ,subtract, add, division, times, absolute, minus, exceed, below,less, fewer, bigger, biggest, greater, higher, longer, tallest,lowest, number, how many colors, what is the value2We calculate BLEU4 with sacrebleu==2.4.1",
  "Main Results": "shows an extensive comparison betweenTinyChart and existing models on 4 chart un-derstanding benchmarks. Our TinyChart modelachieves state-of-the-art performance on ChartQA,Chart-to-Text, Chart-to-Table, and OpenCQA,while excelling in inference throughput. Specif-ically, TinyChart@768 achieves an accuracy of83.60% on ChartQA (Masry et al., 2022), surpass-ing several closed-source models including GPT-4V (OpenAI, 2023b), Gemini-Ultra (Team et al.,2023), and Qwen-VL-Max (Bai et al., 2023). Italso outperforms the previous open-source SOTAmodel ChartAst (Meng et al., 2024).We find that previous models performed poorlyon the ChartQA-human compared to ChartQA-Aug, with none of them achieving over 70%. Thisis because the questions posed by human annota-tors involve more computational problems (Masryet al., 2022) and are more challenging. By leverag-ing the PoT learning, TinyChart achieves 73.34%on ChartQA-human, which is an improvement of7.44% over ChartAst (Meng et al., 2024). Thisdemonstrates the effectiveness of the proposedmethod based on the Program-of-Thoughts.We observed that models with higher input res-olutions generally perform better on chart under-standing tasks. However, encoding high-resolutioncharts leads to a decrease in inference speed. Byleveraging visual token merging, TinyChart canaccept higher-resolution inputs with a limited in-crease in computing, thus achieving better perfor-mance. Due to the smaller model size and theefficient visual token merging, TinyChart achievessignificantly larger inference throughput comparedto previous models. It demonstrates that TinyChartcan achieve efficient chart understanding with en-hanced performance and faster inference.ChartQA performance in each setting. shows the performance comparison under differentsettings. Note that the performance of ChartAst un-der the Combine setting is from Meng et al. (2024),which leverages a combination of Direct answersand executive JSON answers. The results indicatethat our TinyChart model could achieve SOTA per-formance on the Direct answer. By combining withPoT answers, TinyChart could make further im-provements. In addition, since the combinationof Direct and PoT answers is straightforward, the",
  "@@768Auto79.6385.1683.48": "performance under both the Combine setting andAuto setting falls behind the Oracle setting a lot.Further study can be conducted to combine the twoanswers better.Calculative and non-calculative questions. Wedivide the questions in the ChartQA test into twocategories: calculative questions (761 of 2500) andnon-calculative questions (1739 of 2500) by check-ing whether they contain any calculative keywordsmentioned above. From , we observe thatPoT significantly improves the performance oncalculative questions compared to Direct settings(78.98 vs. 56.64) and thus it shows overall per-formance gains (80.84 vs. 76.36). By combiningDirect and PoT answers, both Combine setting andAuto setting make further improvements.Evaluation on ChartX. To further assess the gen-",
  "Chart": "eralizability of TinyChart, We evaluate TinyCharton ChartX-Cognition (Xia et al., 2024) since it cov-ers visually diverse chart types. Note that we donot perform additional fine-tuning in this evalua-tion. As shown in , benefiting from PoTlearning, TinyChart achieves a 33.35 GPT-Acc onthe QA task, even surpassing GPT-4V. Though itfalls behind GPT-4V in the other three tasks, Tiny-Chart still outperforms Open-source Chart MLLMsincluding ChartLlama and ChartAst. It indicatesthat TinyChart has a strong capability to generalizeacross various chart types.",
  "Ablation Studies": "We further conduct extensive ablation studies onPoT learning and visual token merging in .Ablation on PoT Learning. The upper block in shows the model performance with andwithout training on the PoT data. Comparing Row2 with 1, we observe that training with template-based PoT improves the accuracy of direct answers(71.12 vs. 70.72). It indicates that PoT learning",
  ",916OOM-----9768768r=847323.14 it/s73.2477.7281.0416.4388.90": "enhances the models reasoning abilities. At thispoint, the PoT answers produced by the model areworse than direct answers (55.44 vs. 71.12), whichmay be due to the inability of the templates to coverall questions. However, when we produce PoT an-swers for calculative questions and combine themwith direct answers, the result outperforms solelydirect answers (73.00 vs. 71.12), indicating the ad-vantage of PoT in solving computational problems.After incorporating GPT-based PoT into training,the performance of PoT answering surpasses di-rect answering (76.88 vs. 72.44), and both direct(72.44 vs. 71.12) and combined answering (79.48vs. 73.00) show further improvements. These re-sults suggest that PoT learning not only strengthenscalculations but also enhances reasoning capability.Ablation on Visual Token Merging. The middleblock in compares the performance withand without visual token merging under resolution512512, and with different numbers of tokensto merge in each layer. Comparing Row 4 and 3,increasing resolution from 384 to 512 brings sig-nificant improvements on all benchmarks, demon-strating that high resolution is crucial for compre-hending chart images. However, a direct increasein resolution leads to a substantial drop in the in-ference throughput (2.38 it/s vs. 3.73 it/s), sincelengthy visual features from standard ViT bringconsiderable computational expenses for the LLMto process. By adopting the visual token merging,we can control the length of the visual feature byregulating the number of tokens to merge at eachlayer, thereby achieving efficient high-resolutionencoding. When setting r=20, we attain an infer-ence throughput nearly equal to that with an inputresolution of 384, while providing the performancebenefits of higher resolutions.Extending to Higher Resolution. To further high- light the advantages of visual token merging, weincrease the input resolution to 768 in the bottomblock of . At this point, the length of thevisual feature sequence is 2,916, which could notbe trained using 32GB V100 due to insufficientVRAM. However, after employing the visual to-ken merging module with r=84, the input sequencelength is reduced to 732 and we can perform train-ing normally. In this setting, the models inferencethroughput is 3.14 it/s, and demonstrates perfor-mance benefits on ChartQA (81.04 vs. 80.76) andChart-to-Table (88.90 vs. 87.81). It illustrates thatby utilizing visual token merging, we can leveragehigher-resolution chart images under constrainedresources, thereby improving performance.",
  "Visualization": "Visual Token Merging. To investigate the effectsof visual token merging, we visualized the tokenmerging results at the final layer of the vision en-coder. In , we visualize the top 10 groupswith the largest numbers of tokens. Each groupis outlined with a different color. It reveals thatthe merged groups typically correspond to blank orcolored areas. By compressing these areas downto a single token, our visual token merging modulecan thus reduce the length of the encoded sequencewithout losing much information, thereby achiev-ing efficient visual encoding.Case Study. In , we present a case studyon ChartQA. As shown in (a), much keyinformation within the chart is provided by visuallysituated texts within the image, which requires themodel to have the ability to process high-resolutionimages. Since ChartLlama only supports 336 reso-lutions, it struggles to retrieve accurate informationin these charts. In contrast, our TinyChart can ac-cept higher-resolution inputs and thus successfully",
  "TinyChart PoT: 20.98": "# Get the names of all food items, set to X X=['Mutton & Goat Meat (non-organic)', 'Beef]# Get all the values of 'Land use per 100 kilocalorieY=[20.98, 13.55, 8.27, 3.01,]# Get the index of 'Mutton & Goat Meat (non-organic)'Index=X.index('Mutton & Goat Meat (non-organic))# Get the value of Y at position Index, set to AnswerAnswer=Y[Index]",
  "Conclusion": "This paper introduces TinyChart, a chart under-standing MLLM with 3 billion parameters. Toaddress the inefficiency of lengthy visual token se-quences with high-resolution images, TinyChart in-jects the visual token merging module that mergessimilar vision tokens, thereby enabling efficient en-coding of high-resolution images. To tackle thechallenges of learning numerical computations, wepropose a Program-of-Thoughts learning methodthat trains the model to generate Python programsto answer questions. TinyChart achieves SOTA per-formance on multiple chart understanding bench-marks with fewer parameters and larger inferencethroughput. Extensive ablation studies confirm theeffectiveness of our methods.",
  "While TinyChart demonstrates notable perfor-mance and efficiency, it is still susceptible to hal-lucination issues in summary generation that are": "commonly associated with MLLMs, and thereforeit may generate misleading information about chartcontent. Further research can be conducted to miti-gate this hallucination problem in chart understand-ing. Meanwhile, we observed that TinyChart en-counters challenges in estimating the value of a datapoint without surrounding OCR words, even withincreased input resolution. Potential future workcould focus on developing methods to improve theaccuracy of these non-OCR estimations.",
  "This work was partially supported by the BeijingNatural Science Foundation (No. L233008) andthe National Natural Science Foundation of China(No. 62072462)": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, textreading, and beyond. Preprint, arXiv:2308.12966. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, PeizhaoZhang, Christoph Feichtenhofer, and Judy Hoffman.2023. Token merging: Your vit but faster. In TheEleventh International Conference on Learning Rep-resentations. Jinyue Chen, Lingyu Kong, Haoran Wei, ChenglongLiu, Zheng Ge, Liang Zhao, Jianjian Sun, ChunruiHan, and Xiangyu Zhang. 2024a. Onechart: Purifythe chart structural extraction via one auxiliary token.Preprint, arXiv:2404.09987. Wenhu Chen, Xueguang Ma, Xinyi Wang, andWilliam W Cohen. 2023.Program of thoughtsprompting: Disentangling computation from reason-ing for numerical reasoning tasks. Transactions onMachine Learning Research. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye,Zhangwei Gao, Erfei Cui, Wenwen Tong, KongzhiHu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xi-aoyi Dong, Hang Yan, Hewei Guo, Conghui He,Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang,Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang,Pinlong Cai, Licheng Wen, Xiangchao Yan, MinDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin,Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024b.How far are we to gpt-4v? closing the gap to com-mercial multimodal models with open-source suites.Preprint, arXiv:2404.16821. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, SongyangZhang, Haodong Duan, Maosong Cao, et al.2024a.Internlm-xcomposer2:Mastering free-form text-image composition and comprehensionin vision-language large model.arXiv preprintarXiv:2401.16420. Xiaoyi Dong, Pan Zhang, Yuhang Zang, YuhangCao, Bin Wang, Linke Ouyang, Songyang Zhang,Haodong Duan, Wenwei Zhang, Yining Li, et al.2024b.Internlm-xcomposer2-4khd:A pioneer-ing large vision-language model handling resolu-tions from 336 pixels to 4k hd.arXiv preprintarXiv:2404.06512. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2020.An imageis worth 16x16 words:Transformers for imagerecognition at scale. CoRR, abs/2010.11929. Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, HouqiangLi, and Can Huang. 2023. Docpedia: Unleashing thepower of large multimodal model in the frequencydomain for versatile document understanding. arXivpreprint arXiv:2311.11810. Jiayun Fu, Bin B Zhu, Haidong Zhang, Yayi Zou, SongGe, Weiwei Cui, Yun Wang, Dongmei Zhang, Xi-aojing Ma, and Hai Jin. 2022. Chartstamp: Robustchart embedding for real-world applications. In Pro-ceedings of the 30th ACM International Conferenceon Multimedia, pages 27862795.",
  "Dan Hendrycks and Kevin Gimpel. 2016. Bridging non-linearities and stochastic regularizers with gaussianerror linear units. CoRR, abs/1606.08415": "Wenyi Hong, Weihan Wang, Qingsong Lv, JiazhengXu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang,Yuxiao Dong, Ming Ding, et al. 2023. Cogagent: Avisual language model for gui agents. arXiv preprintarXiv:2312.08914. Anwen Hu, Shizhe Chen, and Qin Jin. 2021. Question-controlled text-aware image captioning. In Proceed-ings of the 29th ACM International Conference onMultimedia, MM 21, page 30973105, New York,NY, USA. Association for Computing Machinery. Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, QinghaoYe, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, andFei Huang. 2024a. mplug-paperowl: Scientific di-agram analysis with the multimodal large languagemodel. Preprint, arXiv:2311.18248. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, LiangZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, FeiHuang, and Jingren Zhou. 2024b. mplug-docowl1.5: Unified structure learning for ocr-free documentunderstanding. Preprint, arXiv:2403.12895. Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung,Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-FuChang, and Heng Ji. 2024a.From pixels to in-sights: A survey on automatic chart understandingin the era of large foundation models.Preprint,arXiv:2403.12027. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, WeimingZhang, and Nenghai Yu. 2024b. Opera: Alleviatinghallucination in multi-modal large language modelsvia over-trust penalty and retrospection-allocation.Preprint, arXiv:2311.17911. Chaoya Jiang, Haiyang Xu, Mengfan Dong, JiaxingChen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang,Fei Huang, and Shikun Zhang. 2024. Hallucinationaugmented contrastive learning for multimodal largelanguage model. Preprint, arXiv:2312.06968. Kushal Kafle, Brian Price, Scott Cohen, and Christo-pher Kanan. 2018. Dvqa: Understanding data visual-izations via question answering. In Proceedings ofthe IEEE conference on computer vision and patternrecognition, pages 56485656. Shankar Kantharaj, Xuan Long Do, Rixie TiffanyLeong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty.2022a. OpenCQA: Open-ended question answeringwith charts. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1181711837, Abu Dhabi, United ArabEmirates. Association for Computational Linguistics. Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin,Ahmed Masry, Megh Thakkar, Enamul Hoque, andShafiq Joty. 2022b.Chart-to-text: A large-scalebenchmark for chart summarization. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 40054023, Dublin, Ireland. Association forComputational Linguistics. Sicong Leng, Hang Zhang, Guanzheng Chen, XinLi, Shijian Lu, Chunyan Miao, and Lidong Bing.2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding.Preprint, arXiv:2311.16922.",
  "Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie DelGiorno, Suriya Gunasekar, and Yin Tat Lee. 2023b.Textbooks are all you need ii: phi-1.5 technical report.Preprint, arXiv:2309.05463": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, LongtianQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,Keqin Chen, Jiaming Han, Siyuan Huang, YichiZhang, Xuming He, Hongsheng Li, and Yu Qiao.2023. Sphinx: The joint mixing of weights, tasks,and visual embeddings for multi-modal large lan-guage models. Preprint, arXiv:2311.07575. Fangyu Liu, Julian Eisenschlos, Francesco Piccinno,Syrine Krichene, Chenxi Pang, Kenton Lee, Man-dar Joshi, Wenhu Chen, Nigel Collier, and YaseminAltun. 2023a. DePlot: One-shot visual language rea-soning by plot-to-table translation. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 1038110399, Toronto, Canada. Associ-ation for Computational Linguistics. Fangyu Liu, Francesco Piccinno, Syrine Krichene,Chenxi Pang, Kenton Lee, Mandar Joshi, YaseminAltun, Nigel Collier, and Julian Eisenschlos. 2023b.MatCha: Enhancing visual language pretraining withmath reasoning and chart derendering. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1275612770, Toronto, Canada. Associationfor Computational Linguistics. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, andDong Yu. 2023c.Mmc: Advancing multimodalchart understanding with large-scale instruction tun-ing. arXiv preprint arXiv:2311.10774.",
  "Ahmed Masry, Mehrad Shahmohammadi, Md RizwanParvez, Enamul Hoque, and Shafiq Joty. 2024.Chartinstruct: Instruction tuning for chart compre-hension and reasoning. Preprint, arXiv:2403.09028": "Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao,Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024. Char-tassisstant: A universal chart multimodal languagemodel via chart-to-table pre-training and multitaskinstruction tuning. arXiv preprint arXiv:2401.02384. Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, andPratyush Kumar. 2020. Plotqa: Reasoning over sci-entific plots. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision,pages 15271536.",
  "OpenAI. 2024. Hello gpt-4o": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Raian Rahman, Rizvi Hasan, Abdullah Al Farhad, Md.Tahmid Rahman Laskar, Md. Hamjajul Ashmafee,and Abu Raihan Mostofa Kamal. 2023. Chartsumm:A Comprehensive Benchmark for Automatic ChartSummarization of Long and Short Summaries. Pro-ceedings of the Canadian Conference on Artificial In-telligence. Https://caiac.pubpub.org/pub/ujhjycsw. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2018. Object hallu-cination in image captioning. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 40354045. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition(CVPR). Benny Tang, Angie Boggust, and Arvind Satyanarayan.2023. VisText: A benchmark for semantically richchart captioning. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 72687298,Toronto, Canada. Association for Computational Lin-guistics. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805.",
  "Stefan van der Walt, S. Chris Colbert, and Gael Varo-quaux. 2011. The numpy array: A structure for effi-cient numerical computation. Computing in Science& Engineering, 13(2):2230": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang,Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Ji-tao Sang. 2023a.An llm-free multi-dimensionalbenchmark for mllms hallucination evaluation. arXivpreprint arXiv:2311.07397. Junyang Wang, Yiyang Zhou, Guohai Xu, PengchengShi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, MingYan, Ji Zhang, Jihua Zhu, Jitao Sang, and HaoyuTang. 2023b.Evaluation and analysis of halluci-nation in large vision-language models. Preprint,arXiv:2308.15126. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan,Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Bo-tian Shi, Junchi Yan, and Yu Qiao. 2024. Chartx& chartvlm: A versatile benchmark and foundationmodel for complicated chart reasoning. Preprint,arXiv:2402.12185. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye,Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian,Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin,and Fei Huang. 2023a. Ureader: Universal ocr-freevisually-situated language understanding with multi-modal large language model. In EMNLP (Findings),pages 28412858. Association for ComputationalLinguistics. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, MingYan, Yiyang Zhou, Junyang Wang, Anwen Hu,Pengcheng Shi, Yaya Shi, Chenliang Li, YuanhongXu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang,Fei Huang, and Jingren Zhou. 2024. mplug-owl:Modularization empowers large language modelswith multimodality. Preprint, arXiv:2304.14178. Qinghao Ye, Haiyang Xu, Ming Yan, Chenlin Zhao,Junyang Wang, Xiaoshan Yang, Ji Zhang, Fei Huang,Jitao Sang, and Changsheng Xu. 2023b.mplug-octopus: The versatile assistant empowered by amodularized end-to-end multimodal llm. In Proceed-ings of the 31st ACM International Conference onMultimedia, pages 93659367. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, AnwenHu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, andJingren Zhou. 2023c. mplug-owl2: Revolutioniz-ing multi-modal large language model with modalitycollaboration. Preprint, arXiv:2311.04257.",
  "Zihao Yue, Liang Zhang, and Qin Jin. 2024.Lessis more: Mitigating multimodal hallucination froman eos decision perspective.arXiv preprintarXiv:2402.14545": "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,and Lucas Beyer. 2023. Sigmoid loss for languageimage pre-training. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages1197511986. Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and QinJin. 2023a. Mpmqa: multimodal question answeringon product manuals. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 37, pages1395813966. Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, ChaoXu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,Songyang Zhang, Haodong Duan, Hang Yan, et al.2023b.Internlm-xcomposer: A vision-languagelarge model for advanced text-image comprehensionand composition. arXiv preprint arXiv:2309.15112. Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu,Jing Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020. Trie:end-to-end text reading and information extractionfor document understanding. In Proceedings of the",
  "A.2Instructions for GPT-based PoT": "shows the instructions for constructingGPT-based PoT answers. Note that we promptgpt-3.5-turbo to provide Python code consistingof assignment statements and avoid using loops orjudgment statements. This can simplify the pro-gram and reduce syntax errors. We also providemeta information including the chart title, type,and colors to gpt-3.5-turbo since some questionsrely on this information to answer.",
  "A.3Training details": "TinyChart is initialized from TinyLlava (Zhou et al.,2024), which utilizes the SigLIP (Zhai et al., 2023)as the vision encoder and Phi-2 (Li et al., 2023b)as the large language model. The origin input res-olution of the vision encoder is 384384. We ex-tend the input resolution to 512512 and 768768and apply visual token merging with r = 20 andr = 84 in each transformer layer respectively. Wetrain the entire model for 3 epochs with a batch sizeof 512. The learning rate is set to 1e 4, with awarmup in the beginning 3% steps, and then decaysto 0 at the end of training. The total training pro-cess costs 3 days on 32 Tesla V100 GPUs with 32GB VRAMs. We present the detailed compositionof our training data and their license in .",
  "BFurther improve with GPT-4o": "We further apply the more recent MLLM GPT-4o (OpenAI, 2024) to generate the PoT an-swers. Instead of using data tables, we providethe chart images directly to GPT-4o. The con-structed GPT-4o-PoT dataset contains 23,437 QAsover 16,474 charts after removing incorrect an-swers, demonstrating higher accuracy than GPT-3.5-Turbo, which resulted in 21,303 QAs over15,521 charts. We finetune our TinyChart@768on GPT4o-PoT. The results in show thatlearning with GPT4o-generated PoT leads to fur-ther improvements. This demonstrates that our PoTlearning strategy is compatible with stronger datagenerators.",
  "Instructions to gpt-3.5-turbo": "Please generate a list of assignment statements in Python to answer the question of a chart. Youcan only use the following operators in each statement: <function_list>a. Do not use anycirculation or if-branch. Do not include any unnecessary statement that is not used. The chart ispresented by a data table with color information. Note that the colors are estimated and may notmatch the description in the question. You can choose the most possible data if necessary. Youmust provide a one-line comment before each assignment statement. The last variable must beAnswer. Here are some examples:Example Input #1:Chart title: Long-term price index in food commodities, 1850-2015, World, 1934Chart type: Horizontal bar chartChart table:| Food | Long-term price index in food commodities, 1850-2015, World, 1934 ||:|:|| Lamb (color: steelblue) | 103.7 || Corn (color: sienna) | 103.13 || Barley (color: mediumvioletred) | 102.46 || Rye (color: tomato) | 87.37 || Beef (color: sienna) | 85.27 || Wheat (color: slategray) | 83.73 |Question: What is the sum of the price index that is greater than 100?Answer: 309.29Example Output #1:# Get the values of all Long-term price index of each food, set to YY=[103.7, 103.13, 102.46, 87.37, 85.27, 83.73]# Check whether Y is greater than 100, set to GreaterGreater=np.greater(Y,100)# Find the indices where Greater is True, set to IndicesIndices=np.where(Greater)# Get the values at position Indices, set to YY=np.array(Y)[Indices]# Calculate the sum of all elements in Y, set to AnswerAnswer=np.sum(Y)Input: <target_input>Output: afunction_list=[len, all, any, index, np.sort, np.abs, np.add, np.argmax, np.argmin, np.diff,np.divide,np.greater, np.greater_equal, np.less, np.max, np.mean, np.median, np.min, np.subtract,np.sum, np.count_nonzero, np.where, +, -, *, /, >, <, =]",
  "to-table, chart-to-text, and chart redrawing in Fig-ure 9, 10, 11, and 12": "Chart Question Answering.In , Asshown in (a-c), compared to ChartL-lama, TinyChart captures visual details for prob-lem solving due to higher input resolution. Also,our Program-of-Thoughts learning strategy enablesTinyChart to produce Python codes for numericalcalculations and successfully avoids errors in com-putation ((d-e)). These examples furtherillustrate the advantages of our methods. Chart-to-Table. For chart-to-table extraction, wefind that our TinyChart model can successfully ex-tractive values from several visually diverse chartsin (a-c), thanks to its excellent text recog-nition ability with high-resolution input. However,as shown in (d), the model struggles toestimate the values of data points in the absence ofOCR words. It seems that the model could makereasonable predictions based on surrounding points,but hardly provide accurate values based on the co-ordinate axis. This indicates that the model still : Datasets used for training TinyChart. The benchmark datasets consist of basic chart understandingevaluations including QA, summary, and chart-to-table generation. Note that in ablation studies, we only use thebenchmark datasets for training due to limited computational resources.",
  "DatasetLicenseBenchmarkSamples": "Chart question answerChartQA (Masry et al., 2022)GPL-3.028,299ChartQA-PoT-140,584PlotQA (Methani et al., 2020)CC-BY-4.0157,070DVQA (Kafle et al., 2018)Tencent200,000OpenCQA (Kantharaj et al., 2022a)GPL-3.05,407 Chart-to-text generationPew (Kantharaj et al., 2022b)GPL-3.07,892Statista (Kantharaj et al., 2022b)GPL-3.029,589OpenCQA (Kantharaj et al., 2022a)GPL-3.05,407Vistext (Tang et al., 2023)GPL-3.011,171ChartSumm (Rahman et al., 2023)-75,255Chart2Text-8k (Obeid and Hoque, 2020)-7,862 Chart-to-table generationChartQA (Masry et al., 2022)GPL-3.019,373PlotQA (Methani et al., 2020)CC-BY-4.0190,720Chart2Text-8k (Obeid and Hoque, 2020)-8,305DVQA (Kafle et al., 2018)Tencent300,000Statista (Kantharaj et al., 2022b)GPL-3.029,589",
  "Total1,364,921": "lacks the ability to understand spatial relationshipsacross large areas.Chart-to-Text. From , we observe thatthe model can understand the data presented inthe chart and generate descriptions and summariesin natural language. Though it can retrieve thedata values correctly, we find it sometimes pro-duces contents that do match the chart as shown in (c-d). This may be due to the inherentlimitations of hallucination in MLLMs (Rohrbachet al., 2018; Li et al., 2023a; Wang et al., 2023b,a),and may be alleviated by addressing hallucina- tions (Leng et al., 2023; Huang et al., 2024b; Jianget al., 2024; Yue et al., 2024).Chart redrawing. We present four cases of chartredrawing in . As shown in (a-c), our TinyChart model can generate Python codeto redraw visually diverse chart types includinglines, heatmaps, and rings. However, it can behard to draw unseen chart types such as 3D barcharts ( (d)). This may be mitigated byimproving the coverage of different chart types intraining data through automatic data constructiontechniques (Han et al., 2023; Xia et al., 2024).",
  ": More cases on ChartQA. We compare TinyChart@768 with ChartLlama": "Characteristic | More than 10 times a day | 2-10 times a day | Once a day | Less often than dailyAB | 19% | 42% | 14% | 25%C1 | 24% | 40% | 19% | 16%C2 | 25% | 41% | 17% | 17%DE | 34% | 38% | 14% | 14%",
  "TinyChart:Input: Generate underlying data table for the chart.Input: Generate underlying data table for the chart.TinyChart:": "Characteristic | Male | FemaleSeptember 2018 | 1.6% | 2.2%August 2018 | 1.5% | 1.8%July 2018 | 1.2% | 1.7%June 2018 | 1.5% | 2.1%May 2018 | 1.3% | 2.4%April 2018 | 1.7% | 2.3%March 2018 | 1.9% | 2.5%February 2018 | 2.3% | 3.2%January 2018 | 2.6% | 3.6%December 2017 | 1.9% | 2.8%November 2017 | 1.4% | 2.3%October 2017 | 1.4% | 2.1%",
  "(c)(d)": "Characteristic | Adult total | Adult male | Adult female2008 | 98.29% | 97.83% | 98.51%2009 | 98.36% | 97.67% | 98.62%2010 | 98.28% | 97.65% | 98.56%2011 | 98.44% | 98.07% | 98.70% 2012 | 98.47% | 98.10% | 98.74%2013 | 98.43% | 98.06% | 98.70%2014 | 98.49% | 98.03% | 98.81%2015 | 98.54% | 98.08% | 98.87% 2016 | 98.59% | 98.17% | 98.86%2017 | 98.65% | 98.21% | 98.93%2018 | 98.70% | 98.32% | 98.98%",
  ": Examples of chart-to-table extraction of TinyChart@768. Wrong values are marked red": "Input: Write a summary for this chart. TinyChart: The share of Americans who own desktop or laptop computers has remained relatively stable in recent years, while the share who own tablet computers and e-readers has grown. Today, about three-quarters of U.S. adults (74%) own a desktop or laptop computer, up from 70% in early 2014. Input: Create a brief summarization or extract key insights based on the chart image. TinyChart: The chart illustrates the retail and e-commerce sales figures in four major online markets: USA, UK, Germany, and France in 2021. Retail sales dominated in the USA with a value of 4500, followed by the UK with 4000. Germany and France saw 3500 and 3000 retail sales respectively, while e-commerce sales were the highest in France with 4000, and second highest in the USA with 2500. Input: Create a brief summarization or extract key insights based on the chart image.TinyChart: The chart presents the popularity of various sports in the United States in 2023. Football is the most popular sport with a 30% popularity rating, followed by Basketball at 25%. Baseball and Hockey have a 15% and 10% popularity respectively. Meanwhile, Golf and Tennis have a 5% and 10% popularity rating respectively. The least popular sport is Other with a 5% rating. Input: Create a brief summarization or extract key insights based on the chart image.TinyChart: The chart illustrates the sales trends for online and store sales from 2018 to 2022. It shows that online sales steadily increased from 500 in 2018 to 1200 in 2022, while store sales also experienced an increase from 400 in 2018 to 1000 in 2022. The data indicates that while online sales growth has been consistently positive, store sales growth has been relatively slower. This could suggest a shift in consumer preference or a need for a more extensive retail network."
}