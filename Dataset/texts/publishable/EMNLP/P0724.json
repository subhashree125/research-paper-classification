{
  "Abstract": "Long-context modeling capabilities have gar-nered widespread attention, leading to the emer-gence of Large Language Models (LLMs) withultra-context windows.Meanwhile, bench-marks for evaluating long-context LLMs aregradually catching up.However, existingbenchmarks employ irrelevant noise texts toartificially extend the length of test cases, di-verging from the real-world scenarios of long-context applications. To bridge this gap, we pro-pose a novel long-context benchmark, Loong,aligning with realistic scenarios through ex-tended multi-document question answering(QA). Unlike typical document QA, in Loongstest cases, each document is relevant to the fi-nal answer, ignoring any document will lead tothe failure of the answer. Furthermore, Loongintroduces four types of tasks with a range ofcontext lengths: Spotlight Locating, Compar-ison, Clustering, and Chain of Reasoning, tofacilitate a more realistic and comprehensiveevaluation of long-context understanding. Ex-tensive experiments indicate that existing long-context language models still exhibit consider-able potential for enhancement. Retrieval aug-mented generation (RAG) achieves poor perfor-mance, demonstrating that Loong can reliablyassess the models long-context modeling capa-bilities.1",
  ": Previous benchmarks vs. Loong": "marks theexistence of evidence related to the answer in that docu-ment. Compared to centralized distribution in previousones, evidence in Loong are scattered in different partsacross multi-document long contexts, necessitating thatno document can be ignored for success. tasks that require delving deeply into long texts.A few LLM (e.g. GPT-4o, Gemini-Pro) websiteshave been equipped with the intelligent documentanalysis function, allowing users to upload docu-ments for answering queries. These demand themodel leverage its long-context capability to con-duct an in-depth analysis of multiple long docu-ments. Meanwhile, retrieval-augmented generation(RAG) has emerged as a widely adopted frame-work that prompts LLMs with multiple relevantretrieved contents and can significantly improvemodel performance (Wu et al., 2024; Chen et al.,2024a).However, there remains a lack of appropriatebenchmarks for evaluating long-context under-standing in real-world multi-document scenarios.Multi-document input as long-context modelingpossesses extensive application scenarios of LLMs,such as analysis of financial reports over the years.Nevertheless, most existing benchmarks only placeemphasis on single-document long contexts (Anet al., 2024; Li et al., 2024; Kamradt, 2023) or in-volve multi-document question answering settings",
  ": Characteristics of Loong, where the evidences are scattered across multi-document long contexts": "by adding distracting information to the input ofexisting short-context QA datasets (Hsieh et al.,2024). As shown in , evidence support-ing the answer in previous benchmarks is relativelycentralized, such as being contained within a singledocument. Yet, such a centralized distribution ofevidence may cause the model to overlook certaindocuments and take shortcuts to formulate an an-swer, simplifying the modeling of the real context.Moreover, the prevalent evaluation tasks, such asneedle in a haystack (NIAH) (Kamradt, 2023),only scratch the surface of long-context understand-ing by searching from context, far from real-worlddemands. We commence with leave no documentbehind and scatter the evidence across multi-document long contexts. In this context, bypassingany document will lead to an erroneous answer,which better tests the long-context modeling ability.To this end, this paper develops Loong, an innova-tive benchmark crafted to evaluate the long-contextability of LLMs across multiple documents in real-world scenarios. Loong typically consists of 11documents per test instance on average, nearly allof which are selected from the year 2024, spanningthree real-world scenarios in English and Chinese:(1) Financial Reports, (2) Legal Cases, and (3) Aca-demic Papers. Meanwhile, Loong introduces newevaluation tasks from the perspectives of SpotlightLocating, Comparison, Clustering, and Chain ofReasoning, every test case is newly annotated. Fur-thermore, Loong features inputs of varying lengths(e.g., 10K-50K, 50K-100K, 100K-200K, >200K)and evaluation tasks of diverse difficulty, enablingfine-grained assessment of LLMs across differentcontext lengths and task complexities.",
  "Long-Context Language Models": "With support for increasingly larger context win-dows, closed-source LLMs have taken the leadin the field of long-context modeling.From128k to 1000k, GPT-4o (OpenAI, 2023), Claude3-200k (Anthropic, 2024a) and Gemini-pro1.5-1000k (Reid et al., 2024) are capable of modelingincreasingly longer documents, expanding the newscenarios that LLMs can handle.Considering the quadratic complexity of Trans-former (Vaswani et al., 2017), training LLMs withextensive context windows from scratch necessi-tates substantial computational resources, exceed-ing the capabilities of the general researchers. Con-sequently, recent studies have explored ways to",
  "Multi-Doc Context:<d1></d1><d2></d2><dx> BIOETHICSxxx</dx><dy>BIOETHICS...xxx...</dy><dz> Dominari Holdingsxxx</dz><dn> </dn>": ": Showcase of four evaluation tasks in Loong (<di>...</di> marks the content of the i-th document). a)Spotlight Locating: Locate the evidence. b) Comparison: Locate and compare the evidence. c) Clustering: Locateand cluster the evidence into groups. d) Chain of Reasoning: Locate and reasoning along a logical chain. expand the context length of these models duringthe fine-tuning stage. For example, PI (Chen et al.,2023), NTK-aware (bloc97, 2023), YaRN (Penget al., 2024), Giraffe (Pal et al., 2023), CodeLLaMA (Roziere et al., 2023), and PoSE (Zhuet al., 2024) adapts position embedding based onthe rotary position encoding (RoPE) (Su et al.,2024), with only a few fine-tuning steps, the con-text length can be efficiently extended.Another strong baseline for long-context mod-eling is the sliding window method. Various slid-ing window-based variants such as ALibi (Presset al., 2022), xPos (Sun et al., 2023), PCW (Rat-ner et al., 2023), LM-Infinit (Han et al., 2024),StreamingLLM (Xiao et al., 2023) are used toachieve efficient context scaling. Yet they divergefrom the global perception characteristic of theTransformer, failing to exploit the entire context.",
  "Long-Context Benchmarks": "Long-context modeling methods are rapidly evolv-ing, yet the quality of existing benchmarks doesnot align with this progress. Synthetic task suchas Needle-in-a-Haystack (NIAH) (Kamradt, 2023)and Counting stars (Song et al., 2024) are initiallyutilized for evaluating long-context language mod-els (LCLMs) due to their lower construction costs,but they are indicative of only a surface form of",
  "long-context understanding.Longbench (Bai et al., 2024), LooGLE (Li et al.,": "2024) and Marathon (Zhang et al., 2024a) are ear-lier benchmarks for comprehensive assessment oflong context. However, the average length for mosttasks is between 5k and 25k, far less than the win-dow size of LCLMs. L-Eval (An et al., 2024),BAMBOO (Dong et al., 2024), CLongEval (Qiuet al., 2024) and InfiniteBench (Zhang et al., 2024b)contain sufficiently long evaluation data, and thewide variety of tasks makes the assessment morecomprehensive. RULER (Hsieh et al., 2024) cre-ates a comprehensive testing method with flexi-bly adjustable length and difficulty, yet they onlyadd distracting information to the input of existingshort-context QA datasets.While these long-context benchmarks have theiradvantages, there still lack a benchmark that is suffi-ciently long, free from data contamination (Golchinand Surdeanu, 2023), and fully aligned with thereal-world multi-document question answering sce-nario. We conduct a detailed comparison with ex-isting works in .",
  "Leveraging long documents as external knowledge,Retrieval Augmented Language Models (RALMs)have achieved comparable or even better perfor-": "mance than LCLMs fine-tuned for specific taskswith long documents. In previous study, RALMscould directly utilize the content retrieved duringthe inference phase. REPLUG (Shi et al., 2024)treats the language model as a black box and theretrieval component as an adjustable plug-and-playmodule. RETRO (Borgeaud et al., 2022) use achunked cross-attention module to incorporate theretrieved text. Additionally, Xu et al. (2024) ex-plored whether RALMs or LCLMs are more suit-able for long-context tasks under a larger parametersetting. However, there is currently a lack of analy-sis on what tasks RALMs and LCLMs each excelat, thus making it difficult to determine which typea black box model belongs to.",
  "Overview": "The Loong benchmark comprises tasks across fourcategories: Spotlight Locating, Comparison, Clus-tering, and Chain of reasoning. To align with re-alistic scenarios, we collect documents from threedomains: financial reports, academic papers, andlegal cases. Furthermore, all tasks are presentedin the question-answering format, which are allnewly annotated by GPT-4o and humans. Totally,Loong includes 1600 test instances in both Chi-nese and English, featuring four sets with differentintervals of context size: Set1 (10-50K), Set2 (50-100K), Set3 (100-200K) and Set4 (200-250K).We use tiktoken2 tokenizer to tokenize the in-put and report the number of tokens. andAppendix C show the details of data statistics. Ap-pendix E presents a comparison of evidence distri-bution between Loong and LongBench. The fol-lowing sections will provide a detailed descriptionof the evaluation task and benchmark construction.",
  "Spotlight Locating": "The spotlight locating task is designed to assessthe models capability for knowledge localization,which constitutes the foundation ability of long-context processing. In this task, the evidence is con-tained in only one of multiple documents, which isthe atomic setting of the key information locating.The spotlight locating task is aimed at examiningthe LLMs ability to search for evidence within onedocument from multiple ones. Other documents,which are in the same domain and have similar se-mantics as the document but are unrelated to thequestion, will serve as noise texts. The upper leftof provides an example of the spotlightlocating task.",
  "Comparison": "The comparison task is primarily aimed at evaluat-ing the models ability to compare multi-source in-formation with long contexts. In this event, the evi-dence supporting the answer are distributed acrossmultiple documents, testing the LLMs ability tolocate dispersed evidence and to correlate and com-pare them.Comparison task includes three sub-tasks: 1)Sequential Enumeration: Based on the concretenumerical value of a specific attribute, it requiresthe model to list all specific values corresponding to that attribute across multiple documents in agiven order. 2) Extremum Acquisition: It requiresthe model to deduce the extremum of all valuescorresponding to certain attributes in multiple doc-uments. 3) Range Awareness: Given a specificnumerical or conceptual range, the model shouldoutput all objects within multiple documents thatmeet the condition. The upper right of gives an example of comparison task.",
  "Clustering": "The clustering task entails an assessment of themodels ability to aggregate multi-source informa-tion from multi-document long contexts. The taskrequires the LLM to perform two main functions:first, to extract relevant data from multiple docu-ments, and second, to integrate this informationbased on specific criteria. Ultimately, the LLMmust cluster related evidence spread across thesedocuments into coherent groups.The clustering task encompasses three sub-tasks:1) Report Integration: This sub-task requires themodel to group the evidence existing in the pro-vided financial reports into corresponding setsbased on textual or numerical criteria. 2) Cita-tion&Reference: For a given paper, the model istasked with identifying its citations and referencesfrom the candidate papers. 3) Case Classification:Given the causes of several legal cases, the modelis required to accurately categorize judgment doc-uments. The bottom left of depicts anexample of the clustering task.",
  "Chain of Reasoning": "The chain of reasoning task requires the model toengage in multi-document reasoning along a logi-cal pathway. This task evaluates the models profi-ciency in logical reasoning, which requires LLMsto locate the corresponding evidence within multi-ple documents and model the logical relationshipsamong them for deducing the answer.The chain of reasoning task contains four sub-tasks: 1) Temporal Analysis: This task requires themodel to analyze the changes or trends of a partic-ular attribute based on the temporal relationship,such as taking into account the financial reports ofa certain company over consecutive years or multi-ple quarters. 2) Citation Chain: This task requiresthe model to accurately understand each paperscontent and its interconnections, ultimately infer-ring the linear citation relationships among them.3) Link the Links: This task involves presenting fact descriptions and trial results from different judg-ment documents separately. The model is taskedwith accurately pairing each fact description withits corresponding trial result. 4) Solitaire: Thistask first requires the model to match causes of ac-tion with judgment documents correctly, and thento sequentially infer multiple judgment documentsbased on the given sequence of causes of action.The bottom right of gives an example ofthe chain of reasoning task.",
  "Data Collection": "We established six criteria for the manual collectionof the required English and Chinese documents: (1)Timeliness: The majority of the documents are thelatest ones from the year 2024; (2) Accessibility:The data is publicly available and permitted fordownload and collection; (3) Appropriate Length:Collecting longer documents as much as possibleand ensure they fit within the four designated lengthsets; (4) Parseability: Chosen documents are easyto process and parse, facilitating conversion intonatural language text; (5) Categorizability: Doc-uments can be manually sorted based on certainattributes, such as case type, research theme, orcompany category, allowing for organized archival;(6) Authoritativeness: All documents are collectedfrom scratch from official websites (e.g. ChinaJudge Online, U.S. SEC, cninf, Arxiv, SemanticScholar), ensuring the quality and authority of thedocuments. The detailed URL can be seen in Ap-pendix H. Specifically, regarding financial reports, we pri-marily collect the latest quarterly and annual re-ports for the year 2024, totaling 574 documents.For legal documents, our collection consists ex-clusively of cases adjudicated by the higher andintermediate courts in 2024, amounting to 629 doc-uments. As for academic papers, our focus is onprocuring the latest articles from arXiv in 2024,with a total of 764 papers. Additionally, to meetthe requirements of the chain of reasoning task,we gather a small portion of financial reports andacademic papers from before 2024. Upon the col-lection of documents, we first parse these docu-ments, converting them uniformly into TXT format.Subsequently, we carry out further data cleansing,removing any portions that contain personal infor-mation.",
  "Annotation Process": "Compared to annotating short texts, annotatinglong texts is more challenging. To address this is-sue, we designed innovative annotation workflowsto reduce the cost of annotation while ensuringquality.For financial reports, we compress the informa-tion contained within the long context, breakingdown the annotation process into numerous sim-ple tasks. We initially manually identify hundredsof key attributes that cover the important informa-tion in the long context. Subsequently, we employGPT-4o to execute the relatively simple task ofinformation extraction, pulling the values corre-sponding to these key attributes. After obtainingthe key attributes and their corresponding values,we can proceed to annotate only the compressed in-formation, eliminating the need to refer back to theoriginal lengthy texts. For legal cases, we followthe classification provided by China Judge Online,manually downloading judgment documents sortedby different causes of action and case types. Ad-ditionally, we use a rule-based method to segmenteach judgment document into its factual statementand verdict sections. For academic papers, weleverage the Semantic Scholar websites API toaccess the target papers citations and references.Moreover, by utilizing the bbl files of each arXivpaper, we write scripts to recursively collect arti-cles that meet the requirements of the linear citationchain task.During the question-and-answer annotationphase, we adopt two approaches: (1) Template-based: We design question types and templates,and based on pre-classified documents, we con-struct Q&A pairs using rules. (2) Free annotation:Referring to the compressed information of multi-ple documents, we design prompts with four dif-ferent task descriptions. We employ GPT-4o togenerate Q&A pairs for each task.",
  "Quality Control": "Throughout the annotation process, we employ sev-eral methods to ensure accuracy: (1) Evidence Re-call: By designing prompts that not only promptGPT-4o to generate labels but also to recall evi-dence supporting the labels from the text, signifi-cantly enhancing the accuracy in practical applica-tions. (2) Self-Check: GPT-4o reviews the originaltext to re-evaluate and correct any mistakes in thegenerated labels. (3) Manual Check: We manuallyreview and confirm the quality of annotations, elim-inating any unreasonable or low-quality questions.Additionally, we also take into account the distribu-tion and number of different length sets, sub-tasks,and language. From a pool of 2,814 entries, weconduct a secondary selection process, ultimatelychoosing 1,600 entries for our final benchmark.",
  "Experimental Setup": "Models We evaluate six advanced long-contextLLMs, with their context window sizes rang-ingfrom128Kto1000K,includingAPI-based LLMs:GPT-4o-128K (OpenAI, 2023),Gemini-1.5-pro-1000K(Reidetal.,2024),Claude3.5-Sonnet-200K(Anthropic,2024b),Claude3-Haiku-200K(Anthropic,2024a),Kimi-Chat-200K3andOpen-sourcedLLMs:Qwen2-72B-Instruct-128K (Bai et al., 2023),GLM4-9B-Chat-1000K (Du et al., 2022).Evaluation Metric In the long-context question-answering scenarios, traditional evaluation metricsF1 and Rouge-L may lead to inaccurate responses.Recent research (Zhang et al., 2023; Liu et al.,2024; Wang et al., 2024) indicates that the GPT-4 (OpenAI, 2023) evaluator demonstrates high con-sistency with human evaluations, making it a rea-sonably reliable annotator. Building on these con-",
  "Kimi-Chat (200k)20.170.129.170.005.650.0022.610.1113.500.05GLM4-9B-Chat (1000K)15.670.1221.330.0512.350.0021.040.0516.840.05": ": The performance of LLMs on four evaluation tasks with different length sets. For each task, the indicatoron the left represents the Avg Scores (0~100), while the right one represents the Perfect Rate (0~1). siderations, we prompt GPT-4 as a judge to evaluatethe models output based on the golden answer andthe questions requirements from three aspects: Ac-curacy, Hallucinations, and Completeness, scoringfrom 0 to 100. For a detailed prompt, please referto the Appendix A. We also design two indicators:(1) Avg Scores: the average value of scores givenby GPT-4 for all questions; (2) Perfect Rate: theproportion of cases scoring 100 out of the totalcases. The latter is a more stringent evaluationmetric compared to the former.Prompt Templates For different sub-tasks, we re-quire the model to follow the given instructions andoutput the answer according to the specific promptsshown in Appendix B.Input Truncation Due to input length limits, weassess whether adding a document would exceedthe models processing length when concatenatingmultiple documents. If appending the document would surpass the models capacity, we discard itfrom the concatenation process. The evaluation andselection process continues until we have reviewedall documents that need concatenation.Implement Details We set temperature = 0to eliminate randomness and keep other hyper-parameters default. For API-Based LLMs, we di-rectly utilize the official API for testing. Since theKimi-Chat-200k currently does not provide an in-terface, we manually input content on the web. Asfor open-source models, we conduct experimentson a server with 8A100 80GB.",
  "Main Results": "We assess seven advanced LLMs on the Loongbenchmark. The main results are shown in and . We can see that Gemini-1.5-pro showsthe best overall performance, especially excellingin the processing of ultra-long context within Set3 and Set4. Its comprehensive score reached 55.37with the perfect rate of 27%, followed by GPT-4o. Besides, the long-context modeling capacity ofopen-source models still falls short when comparedto that of the most powerful closed-source modelsin the Loong. Additionally, larger-parameter mod-els outperform their smaller counterparts withinthe same window size, indicating the advantages ofscaling up model sizes for improved long-contextmodeling. The overall assessment results highlightthat even the most advanced long-context LLMscurrently fail to achieve passing marks, particu-larly in terms of the perfect rate. This suggests thatthere exists significant room for improvement inthe long-context modeling capabilities of LLMs.We also compare the current results with other cur-rent Long-Context benchmarks, which can be seenin Appendix F.",
  "Task Analysis": "Analyzing performance across different tasks, mod-els exhibit their best performance in the spotlightlocating task. This can be attributed to the tasksrelative simplicity, which tests the foundational ca-pabilities of long-context modeling. Moreover, theevidence is only distributed within a single doc-ument, making it easier to locate and less proneto confusion. In contrast, due to the requirementsof multi-source information inference, the compar-ison and cluster tasks present greater challenges,leading to model underperformance. These tasksnecessitate not only the collection of evidenceacross documents but also involve complex reason-ing processes such as matching, contrasting, andclassification. Thus, they more rigorously test thehigher-order capabilities of long-context modeling,revealing significant gaps in the current modelsabilities. Regarding the chain of reasoning task,models perform well within Set1. However, as thecontext length increases, their performance drasti-cally declines. This suggests that within the scopeof long-context modeling capabilities, LLMs pos-sess adequate skills in temporal analysis, logicalsequencing, and linking multiple concepts. Nev-ertheless, an overflow in context length leads tothe loss of key evidence, severely impacting theaccuracy of chain reasoning tasks.",
  ": The results on all tasks after adding RAGmodule. We only represents the Avg Scores (0~100)": "that for the same task, models perform well withinsmall length sets but exhibit a notable performancedecline as the length increases. This indicates thatthe models possess a certain capability to processthe task, yet their performance is constrained by thecontext window. Moreover, despite being trainedon 128K data, the GPT-4o and Qwen2-72B-Instructbegin to show performance degradation within the50-100K interval, revealing that their actual ca-pability boundary is significantly lower than theclaimed window size. This suggests the presenceof an ineffective zone within the claimed window.There exists a Scaling Law for model window sizes:to truly equip an LLM with the ability to handle128K long texts, it should be trained on data exceed-ing 128K, meaning the training length should begreater than the actual processable length. Amongnumerous models, only the Gemini is less affectedby changes in context length, which was trainingon the ultra-long context of 1000K. To ensure yourmodel genuinely possesses the desired context win-dow size, train it on longer data!",
  "RAG or Not": "We have also incorporated the Embedding RAGmodule into the GPT-4o and Qwen2-72B-Instructto explore whether RAG can enhance the modelsperformance on Loong. For the Embedding choice,we employ two distinct models: the OpenAI Em-bedding model4 and the BGE Embedding model5.Besides, we set the top-k value of 5, 10, 30, and50 for each model respectively, and the chunk sizeis 1024. The result is shown in and thedetails can be seen in Appendix D.Benchmark Analysis It is evident that the inclu-sion of RAG does not enhance the models overallperformance on the Loong, and there is a notice-able decline in assessment. This is because the evi-dence in the Loong is distributed relatively evenlyacross multiple documents, requiring a compre-hensive understanding of long texts by the model.RAG, being more limited, only shows some effec-tiveness in the task with sparse evidence, such asspotlight locating. However, RAGs negative im-pact is significant for tasks requiring a high levelof comprehensiveness. Integrating RAG does notenhance the performance, indicating that Loong fo-cuses on evaluating the models complex reasoningand comprehensive analysis capabilities for longcontexts, thereby effectively assessing the LLMslong-context modeling ability.Model Analysis Comparing the performance be-tween GPT-4o and Qwen2-72B-Instruct, it is evi-dent that the powerful long-context LLM signifi-cantly outperforms RAG. On the other hand, theperformance of RAG is closer to the original per-formance when used with a weak context model.This is because a strong long-context LLM canfully exploit the complete information flow of longcontexts, capturing complex dependencies and se-mantic information. However, RAG causes contextfragmentation and information loss, impairing themodels understanding and reasoning capabilities,thereby preventing the full utilization of its inher-ent modeling advantages. Consequently, a stronglong-text modeling capability is not suitable forenhancement through RAG. Conversely, a weakmodel with poor long-context modeling capabilitycannot effectively capture information, and RAGcannot compensate for this deficiency.Length Analysis Within the context window size that the model can handle, RAG does not offer anadvantage. However, for ultra-long context sets,a high top-k setting of RAG can produce certaineffects. This is because, in short context sets, themodels inherent modeling capability can effec-tively handle the entire text length without losinginformation. The introduction of RAG, conversely,may result in the loss of certain evidence, leading toinformation gaps. In ultra-long context collections,RAG can effectively compress information, recall-ing evidence that the LLM could not access due tolength truncation, thereby enhancing the modelsperformance on Loong.We also provide analytical experiment to showthat RAG does not cover all evidence, which canbe seen in Appendix G. Relying on RAG cannotresolve all the problems associated with long-textmodeling. To genuinely improve the long-contextmodeling capability, stronger training methodsand effective training on longer texts are required,rather than merely integrating the RAG module.",
  "Conclusion": "In this study, we propose Loong, a question-answering format benchmark designed to evaluatelong-context comprehension in real-world multi-document scenarios. We analyze six advanced lan-guage models (LLMs), considering variations intheir parameter sizes and context windows, includ-ing GPT-4o and Gemini-Pro1.5. Notably, even themost powerful long-context LLMs fail to achievesatisfactory performance. Furthermore, we conductin-depth analyses to enhance long-context model-ing capabilities by comparing the RAG approachand the scaling laws related to context size.",
  "Limitations": "Here we list some of the limitations that are notconsidered when designing Loong: (1) LimitedDomains. The purpose of Loong is to evaluatethe long-context understanding capabilities in real-world multi-document scenarios. However, a seaof multi-document domains exists in the real world.Considering annotation costs and model evaluationefficiency, we only cover the most representativeparts of them: financial, legal, and academic. (2)High Annotation Cost. To enhance the reliabilityof Loong in assessing the LLMs long-context un-derstanding capabilities, we recruited a group ofexperts for each of the three domains to proofreadthe data, and they are proficient in both English and Chinese. They need to understand the question andsearch for relevant evidence in multiple documentswith an average length of up to 110k to judge theconsistency between the question and the answer,which requires a significant amount of time andeffort. Min Yang was supported by National Key Re-search and Development Program of China(2022YFF0902100), National Natural ScienceFoundation of China (Grant No.62376262),the Natural Science Foundation of GuangdongProvince of China (2024A1515030166), Shen-zhen Science and Technology Innovation Program(KQTD20190929172835662), Shenzhen Basic Re-search Foundation (JCYJ20210324115614039). Chenxin An, Shansan Gong, Ming Zhong, XingjianZhao, Mukai Li, Jun Zhang, Lingpeng Kong, andXipeng Qiu. 2024. L-eval: Instituting standardizedevaluation for long context language models. In Pro-ceedings of ACL, pages 1438814411.",
  "Shouyuan Chen, Sherman Wong, Liangjian Chen, andYuandong Tian. 2023. Extending context window oflarge language models via positional interpolation.arXiv preprint arXiv:2306.15595": "Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,and Ji-Rong Wen. 2024. BAMBOO: A comprehen-sive benchmark for evaluating long text modelingcapacities of large language models. In Proceedingsof LREC-COLING, pages 20862099. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:General language model pretraining with autoregres-sive blank infilling. In Proceedings of ACL, pages320335.",
  "Shahriar Golchin and Mihai Surdeanu. 2023. Timetravel in llms: Tracing data contamination in largelanguage models. arXiv preprint arXiv:2308.08493": "Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong,Yu Chen, Heng Ji, and Sinong Wang. 2024. LM-infinite: Zero-shot extreme length generalization forlarge language models. In Proceedings of NAACL,pages 39914008. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-tanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-burg. 2024. RULER: Whats the real context size ofyour long-context language models? In Proceedingsof COLM.",
  "Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong,and Irwin King. 2024. Clongeval: A chinese bench-mark for evaluating long-context large language mod-els. arXiv preprint arXiv:2403.03514": "Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,Inbal Magar, Omri Abend, Ehud Karpas, AmnonShashua, Kevin Leyton-Brown, and Yoav Shoham.2023. Parallel context windows for large languagemodels. In Proceedings of ACL, pages 63836402. Machel Reid, Nikolay Savinov, Denis Teplyashin,Dmitry Lepikhin, Timothy Lillicrap, Jean-baptisteAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions oftokens of context. arXiv preprint arXiv:2403.05530. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, et al. 2023.Code llama: Open foundation models for code. arXivpreprint arXiv:2308.12950. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-joon Seo, Richard James, Mike Lewis, Luke Zettle-moyer, and Wen-tau Yih. 2024. REPLUG: Retrieval-augmented black-box language models. In Proceed-ings of NAACL, pages 83718384.",
  "Guangxuan Xiao, Yuandong Tian, Beidi Chen, SongHan, and Mike Lewis. 2023. Efficient streaminglanguage models with attention sinks. arXiv preprintarXiv:2309.17453": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,Prajjwal Bhargava, Rui Hou, Louis Martin, RashiRungta, Karthik Abinav Sankararaman, Barlas Oguz,Madian Khabsa, Han Fang, Yashar Mehdad, SharanNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,Sergey Edunov, Mike Lewis, Sinong Wang, and HaoMa. 2024. Effective long-context scaling of foun-dation models. In Proceedings of NAACL, pages46434663. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,Chen Zhu, Zihan Liu, Sandeep Subramanian, EvelinaBakhturina, Mohammad Shoeybi, and Bryan Catan-zaro. 2024. Retrieval meets long context large lan-guage models. In Proceedings of ICLR. Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi Yang, JunhaoLiu, Longze Chen, Run Luo, and Min Yang. 2024a.Marathon: A race through the realm of long contextwith large language models. In Proceedings of ACL,pages 52015217.",
  "[The Start of Assistants Predicted Answer]<LLMs response>[The End of Assistants Predicted Answer]": "[System]We would like to request your feedback on theperformance of the AI assistant in response to theuser question displayed above according to the goldanswer. Please use the following listed aspects andtheir descriptions as evaluation criteria: - Accuracy and Hallucinations:The assistantsanswer is semantically consistent with the goldanswer; The numerical value and order need to beaccurate, and there should be no hallucinations.- Completeness: Referring to the reference answers,the assistants answer should contain all the keypoints needed to answer the users question; furtherelaboration on these key points can be omitted.Please rate whether this answer is suitable for thequestion. Please note that the gold answer can beconsidered as a correct answer to the question. The assistant receives an overall score on a scale of 1to 100, where a higher score indicates better overallperformance.Please note that if the assistants answerand the gold answer fully meet the above criteria, itsoverall rating should be the full marks (100). Pleasefirst provide a comprehensive explanation of yourevaluation, avoiding any potential bias.Then, outputa line indicating the score of the Assistant.",
  "B.2Sequential Enumeration": "<Multi_Documents>Prompt: We kindly ask you to review the fi-nancial statements of the companies providedabove and answer the following questionsbased solely on the information you have seen.If the question involves content not found inthe financial statements, you may ignore thispart and only answer the other parts.Question: Please list the Cash and CashEquivalents of each of the aforementionedcompanies in ascending order?Answer: $ 1,273 thousand, $ 1,360 thousand,$ 9,364 thousand",
  "B.3Extremum Acquisition": "<Multi_Documents>Prompt: We kindly ask you to review the fi-nancial statements of the companies providedabove and answer the following questionsbased solely on the information you have seen.If the question involves content not found inthe financial statements, you may ignore thispart and only answer the other parts.Question: Which company has the highestTotal Non-current Assets?Answer: BLUE DOLPHIN ENERGY COwith $56,787,000",
  "B.5Report Integration": "<Multi_Documents>Prompt: We kindly ask you to review the fi-nancial statements of the companies providedabove and answer the following questionsbased solely on the information you have seen.If the question involves content not found inthe financial statements, you may ignore thispart and only answer the other parts.Question: Please categorize the companieslisted above by Total Shares Outstandinginto the following groups: below 10,000,000shares and 10,000,000 shares or more. Placecompanies into the same collection for thesame category and into different collectionsfor different categories.Answer: {\"below 10,000,000 shares\": [\"GSESYSTEMS INC\", \"CROSS TIMBERS ROY-ALTY TRUST\"], \"10,000,000 shares or more\":[\"HUGOTON ROYALTY TRUST\"]}",
  "B.6Citation&Reference": "<Multi_Documents>Prompt: We hope you will carefully studythe provided papers and determine the cita-tion relationships between them. Please followthe instructions below strictly to complete thetask:#Specific Requirements:1. Reference: When a given paper mentionsother provided papers, those other papers areconsidered as \"references\" for the given paper.To summarize in this specific context, refer-ences are about what the given paper is using.2. Citation: Conversely, when other providedpapers mention the given paper in their works,the given paper is being \"cited\" by those otherpapers. To summarize in this specific context,citations are about who is using the given pa- per.3. Given a paper, you need to determine thecitation or reference relationship between thispaper and the other papers. Do not considerpapers that are not provided.3. Please present the paper titles in a jsonformat as follows: {\"Reference\":[\"ReferenceTitle 1\", \"Reference Title 2\", ..., \"ReferenceTitle n\"], \"Citation\":[\"Citation Title 1\", \"Cita-tion Title 2\", ..., \"Citation Title n\"]}.4. If a paper does not have any references orcitations, please leave the corresponding listempty, e.g.{\"Refernce\":[]}, {\"Citation\":[]}.Question: The paper you need to analyze:Self-Discover:Large Language Models Self-Compose Reasoning StructuresAnswer: {Reference: [# Plan, Verify andSwitch: Integrated Reasoning with DiverseX-of-Thoughts , # StrategyLLM: Large Lan-guage Models as Strategy Generators, Execu-tors, Optimizers, and Evaluators for ProblemSolving ], Citation: [# Can LLMs SolveLonger Math Word Problems Better? ]}",
  "B.7Case Classification": "<Multi_Documents>Prompt: Please answer the following ques-tions based only on the judgment documentsyou have seen above. You only need to givethe titles of the judgment documents that meetthe requirements.Question: After reading the above judgments,please classify all the judgments accordingto the following three types of cases: CivilCases, Enforcement Cases, and Adminis-trative Cases.Answer: {\"Civil Cases\": [\"Judgment Doc-ument 2\"], \"Enforcement Cases\":[\"Judg-ment Document 4\"], \"Administrative Cases\":[\"Judgment Document 1\", \"Judgment Docu-ment 3\"]}",
  "<Multi_Documents>Prompt: We kindly ask you to review the fi-nancial statements of the companies providedabove and answer the following questionsbased solely on the information you have seen": "If the question involves content not found inthe financial statements, you may ignore thispart and only answer the other parts.Question: What is the trend in ARVANAINCs share capital from 2021 to 2024?Answer: ARVANA INCs share capital hasconsistently increased from $4,611 in 2021to $34,149 in 2022, $35,949 in 2023, and$107,847 in 2024.",
  "B.9Citation Chain": "<Multi_Documents>Prompt: We kindly ask you to thoroughlyreview the provided papers and construct a ci-tation chain from them. Please adhere to thefollowing instructions strictly while complet-ing the task:#Task Instructions:Given several papers, you are required to iden-tify and list the longest citation chain, whichdemonstrates the citation relationship amongthe provided papers.#Specific Requirements:1.Please present the titles of the papers in theform of a list, as follows: [\"Title of Paper 1\",\"Title of Paper 2\", ..., \"Title of Paper n\"].2.Ensure that the citation chain in the list islinear and continuous, meaning that the firstpaper title in the list (Paper 1) should not citeany other works. Instead, it should be citedby the next paper in the list (Paper 2); subse-quently, each paper should then be cited bythe next one in the list, continuing up to thelast paper (Paper n).3.Consider only the citation relationshipswithin the supplied collection of papers, andensure that the citation chain accurately re-flects the sequential citation order among thesedocuments.4.Do not take into account any articles not pro-vided, and disregard other non-linear citationrelationships.Answer: [\"# Very Deep Transformers forNeural Machine Translation \", \"# Understand-ing the Difficulty of Training Transformers \",\"# MonaCoBERT: Monotonic attention basedConvBERT for Knowledge Tracing\"]",
  "B.10Link the Links": "<Multi_Documents>Prompt: Answer the following questionsbased solely on the judgment document youhave seen above.Question:After reading the above judgmentdocument, I will give you several judgmentresults:<a list of judgment result:judg-ment1~judgment6> You need to determinethe most likely judgment result for each ofthe above judgment documents.Answer: {\"Judgment Document 1\": \"Judg-ment Result 1\", \"Judgment Document 2\":\"Judgment Result 6\", \"Judgment Document3\": \"Judgment Result 2\", \"Judgment Docu-ment 4\": \"Judgment Result 5\"}",
  "B.11Solitaire": "<Multi_Documents>Prompt: Answer the following questionsbased solely on the judgment document youhave seen above.Question: Reading the above judgments, Iwill provide several case types arranged in aleft-to-right sequence: [CaseType1, Case-Type2, CaseType3, CaseType4]. You needto sort all the judgment documents accordingto the above sequence of case types. The judg-ment documents only need to include the titles.Please provide the answer:Answer: {\"CaseType1\": \"Judgment Docu-ment 3\", \"CaseType2\": \"Judgment Document1\", \"CaseType3\": \"Judgment Document 4\",\"CaseType4\": \"Judgment Document 6\"}",
  "EComparison of Evidence Distribution": "In the same multi-document question-answeringtask, we compared the distribution of evidence re-lated to the answers in the context for Loong (Ours)and Longbench (Bai et al., 2024). As shown in Ta-ble 11, we present data examples from Loong andLongBench. It can be observed that although Long-bench contains a large number of passages, theevidence is only distributed within Passage 1. Incontrast, in our Loong, the evidence is distributedacross every document, requiring the model to un-derstand each document in order to provide thecorrect answer.",
  "FComparison of Results with OtherBenchmarks": "Due to the rapid iteration of models and the highcost of closed-source model APIs, previous stud-ies have rarely evaluated models that support con-text windows of 128k or more. Evaluating thelatest models with ultra-long context windows andexpensive closed-source models is our advantagecompared to previous methods. Users can evaluatethe gap between their models and the latest open-source and closed-source models at a low cost. Todiscover more interesting conclusions, we com-pared Loong with RULER (Hsieh et al., 2024),",
  ": Compare Loong with RULER and NOCHA": "and the most recent work, NOCHA (Karpinskaet al., 2024). The experimental results are shownin the .From the result, we discovered that: (1) Al-though RULER focuses on synthetic tasks andNOCHA focuses on the domain of novels, the per-formance of the models on the three benchmarks isessentially consistent. (2) Gemini-1.5-pro achievedthe best results in Loong and RULER, but GPT-4o had a significant lead on NOCHA. We believethat since NOCHAs data sources only includenovels, GPT-4o might have stronger capabilitiesin the domain of novel literature. (3) The large-parameter Qwen2-72B-Instruct underperformed inRULER compared to the smaller-parameter modelsGLM4-9B-Chat and Llama3.1-8B, which is incon-sistent with the results of Loong. We believe thatRULER emphasizes synthetic tasks, where the per-formance of such tasks is as sensitive as NeedleIn A Haystack. Specifically, simply adding somesynthetic task data to the training corpus can yieldgood results. In real-world tasks like Loong, suchanomalous phenomena generally do not occur.",
  "GResults of Recall Rate by RAG": "To show that RAG does not cover all evidence, wesampled all the questions from the financial testcase in the Loong and analyzed RAGs retrievalresults. Since the answers to our questions are dis-tributed across all documents on average, we evalu-ated whether the retrieved top-k passages cover alldocuments to reflect the RAGs recall of evidence.This metric will be higher than the actual evidencerecall rate because even if all required documentsare retrieved, the specific passages may not nec-essarily contain the evidence. If the model failsto retrieve all documents, it will certainly be un-able to retrieve all evidence. The evaluation metricused is Recall@n, where n represents the number of retrieved passages. If the n passages contain alldocuments, Recall@n is set to 1; otherwise, it is0. The results can be seen in . Evidently,in the Loong dataset, RAG struggles to retrieve allrelevant documents. Even when the topK reaches50, the highest recall is only 0.64. In reality, therecall rate for all evidence will be even lower.",
  "ModelSpotlight LocatingComparisonClusteringChain of ReasoningOverall": "Set1 (10K-50K)Qwen2-72B-Instruct (128K)68.490.5560.600.3747.080.0870.390.3660.110.29w/ Openai Embedding, Top k=554.620.4526.170.0829.600.0334.410.0834.510.12w/ BGE Embedding, Top k=562.920.5330.920.0831.280.0332.950.1136.910.15w/ Openai Embedding, Top k=1059.810.4334.930.1529.330.0241.270.1538.960.15w/ BGE Embedding, Top k=1072.130.6232.420.1231.900.0544.120.2042.270.20w/ Openai Embedding, Top k=3057.260.4045.430.2840.040.0657.320.3549.060.24w/ BGE Embedding, Top k=3056.370.3346.270.3038.350.1051.490.2946.690.23w/ Openai Embedding, Top k=5051.080.3544.530.2737.960.0553.950.3546.110.23w/ BGE Embedding, Top k=5053.470.3747.310.2936.420.0654.650.3546.620.24 Set2 (50K-100K)Qwen2-72B-Instruct (128K)64.530.4342.600.2138.520.0551.180.2045.710.17w/ Openai Embedding, Top k=556.640.4036.680.1930.910.0328.380.0134.540.10w/ BGE Embedding, Top k=567.290.4743.390.2828.310.0332.220.0736.950.14w/ Openai Embedding, Top k=1067.070.5344.300.2734.310.0534.030.0640.170.15w/ BGE Embedding, Top k=1071.740.5447.680.3030.550.0330.570.0338.800.14w/ Openai Embedding, Top k=3066.270.4646.280.3138.950.0546.150.2245.420.19w/ BGE Embedding, Top k=3057.350.4146.920.2935.300.0542.820.2042.040.18w/ Openai Embedding, Top k=5055.940.3247.940.3134.320.0346.640.2142.600.16w/ BGE Embedding, Top k=5059.410.3938.520.2135.400.0645.470.2441.510.17 Set3 (100K-200K)Qwen2-72B-Instruct (128K)46.990.2737.060.1331.500.0235.010.0735.940.09w/ Openai Embedding, Top k=563.910.4433.560.1725.980.0128.980.0434.480.12w/ BGE Embedding, Top k=564.810.4730.270.1425.880.0127.860.0533.700.12w/ Openai Embedding, Top k=1067.500.4633.440.1627.940.0231.620.0636.470.13w/ BGE Embedding, Top k=1075.880.5633.760.1527.200.0130.170.0437.280.14w/ Openai Embedding, Top k=3073.690.5542.200.2732.780.0237.650.1342.600.18w/ BGE Embedding, Top k=3067.500.4742.420.1832.340.0337.850.1241.350.15w/ Openai Embedding, Top k=5067.440.5041.820.2431.590.0437.290.1240.900.18w/ BGE Embedding, Top k=5062.560.4240.410.1829.820.0240.310.1439.840.15 Set4 (200K-250K)Qwen2-72B-Instruct (128K)33.180.1626.590.0829.840.0125.810.0428.920.06w/ Openai Embedding, Top k=551.490.2617.120.0321.590.0016.370.0025.590.06w/ BGE Embedding, Top k=548.400.2614.550.0020.690.0018.070.0024.630.05w/ Openai Embedding, Top k=1050.320.2820.300.0324.560.0016.380.0027.080.06w/ BGE Embedding, Top k=1051.020.2821.880.0325.450.0017.290.0028.100.06w/ Openai Embedding, Top k=3052.170.2424.600.1026.780.0017.790.0029.290.07w/ BGE Embedding, Top k=3047.980.2126.820.1026.700.0020.020.0029.440.06w/ Openai Embedding, Top k=5051.630.2623.620.0824.490.0021.840.0229.140.07w/ BGE Embedding, Top k=5047.230.2827.780.0826.480.0024.440.0230.520.08",
  "LongBench": "Passage 1: The Real Glory. The Real Glory is a 1939 Samuel Goldwyn Productions adventure filmstarring Gary Cooper , David Niven, Andrea Leeds and Broderick Crawford released by UnitedArtists in the weeks immediately following Nazi Germanys invasion of Poland. Based on a 1937novel of the same name by Charles L. Clifford and directed by Henry Hathaway, the film is setagainst the backdrop of the Moro Rebellion during the American occupation of the Philippines at thebeginning of the 20th century . . .Passage 2: Jay Sheffield. Jay Howard Sheffield (September 25, 1934 June 25, 1998) was anAmerican actor, who appeared on the stage, in films, and on television. He married Barbara Babcockon June 9, 1962, in San Mateo, California. They later divorced . . .Passage 3: David Niven. James David Graham Niven (; 1 March 1910 29 July 1983) was a Britishactor, soldier, memoirist, and novelist. Niven was known as a handsome and debonair leading man inClassic Hollywood films. He received an Academy Award and a Golden Globe Award . . .Passage 4: Phileas Fogg snacks. Phileas Fogg Ltd is a company that produces snack products in theUnited Kingdom that was created in 1982 by Derwent Valley Foods. The brand is named for PhileasFogg, the protagonist of Jules Vernes Around the World in Eighty Days . . .Passage 5: Jules Verne Trophy. The Jules Verne Trophy is a prize for the fastest circumnavigation ofthe world by any type of yacht with no restrictions on the size of the crew provided the vessel hasregistered with the organization and paid an entry fee. A vessel holding the Jules Verne trophy willnot necessarily hold the absolute round the world record . . .Question: The actor that plays Phileas Fogg in \"Around the World in 80 Days\", co-starred with GaryCooper in a 1939 Goldwyn Productions film based on a novel by what author?Answer: Charles L. Clifford",
  "Document 1:THE ARENA GROUP HOLDINGS, INC .Proceeds from Simplify loan:$7,748.Unearnedrevenue:$(11,665).Amortizationofdebtdiscounts:$536": "Cash and cash equivalents: $4,003 . Noncash and accrued interest: $2,839. Loss on impairment ofassets: $40,589. Accounts receivable, net: $12,029. Subscription refund liability: $18. Accountspayable: $(102). Subscription acquisition costs: $6,131. Change in fair value of contingent considera-tion: $313. ($ in thousands, except share data) . . .Document 2: General Enterprise Ventures, Inc . For purposes of balance sheet presentation andreporting of cash flows, the Company considers all unrestricted demand deposits, money market fundsand highly liquid debt instruments with an original maturity of less than 90 days to be cash and cashequivalents. The Company did not have any cash equivalents at March 31, 2024. The Company had cash of $549,755 at March 31, 2024 . . .Document 3: BROAD STREET REALTY, INC . The carrying amounts of cash and cash equivalents,restricted cash, receivables and payables are reasonable estimates of their fair value as of March 31,2024 due to the short-term nature of these instruments. Reconciliation of cash and cash equivalentsand restricted cash: Cash and cash equivalents: $14,631 (in thousands) . . .Question: Please list the Cash and Cash Equivalents of the aforementioned companies in ascendingorder.Answer: 1. General Enterprise Ventures, Inc.: $549,755. 2. Arena Group Holdings, Inc.: $4,003 inthousands. 3. Broad Street Realty, Inc.: $14,631 in thousands."
}