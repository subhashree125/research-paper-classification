{
  "Abstract": "The recent emergence of Medical Large Vi-sion Language Models (Med-LVLMs) has en-hanced medical diagnosis. However, currentMed-LVLMs frequently encounter factual is-sues, often generating responses that do notalign with established medical facts. Retrieval-Augmented Generation (RAG), which utilizesexternal knowledge, can improve the factualaccuracy of these models but introduces twomajor challenges. First, limited retrieved con-texts might not cover all necessary information,while excessive retrieval can introduce irrele-vant and inaccurate references, interfering withthe models generation. Second, in cases wherethe model originally responds correctly, apply-ing RAG can lead to an over-reliance on re-trieved contexts, resulting in incorrect answers.To address these issues, we propose RULE,which consists of two components. First, weintroduce a provably effective strategy for con-trolling factuality risk through the calibratedselection of the number of retrieved contexts.Second, based on samples where over-relianceon retrieved contexts led to errors, we curatea preference dataset to fine-tune the model,balancing its dependence on inherent knowl-edge and retrieved contexts for generation. Wedemonstrate the effectiveness of RULE on med-ical VQA and report generation tasks acrossthree datasets, achieving an average improve-ment of 47.4% in factual accuracy. We pub-licly release our benchmark and code in",
  ". .": ": (a) An example of factuality issue in Med-LVLM. (b) Utilizing either too few or too many retrievedcontexts as references may not provide effective guid-ance for the models generation. Calibrating the numberof retrieved contexts can effectively control the riskof factual inaccuracies. (c) Med-LVLMs often overlyrely on retrieved contexts, leading to incorrect responseseven when the original answers are correct without RAG.A stronger fine-tuned model can effectively balance itsown knowledge with the retrieved contexts.introduced more accurate and customized solutionsto clinical applications (Li et al., 2023; Moor et al.,2023; Zhang et al., 2023; Wu et al., 2023). WhileMed-LVLMs have demonstrated promising perfor-mance, they remain prone to generating responsesthat deviate from factual information, potentiallyresulting in inaccurate medical diagnoses. Thissusceptibility to hallucination underscores the needfor enhanced mechanisms to ensure factual align-ment in critical medical applications (see an exam-ple in (a)) (Royer et al., 2024; Xia et al.,2024a)). Such errors pose a significant risk to clini-cal decision-making processes and can lead to ad-verse outcomes. Recently,Retrieval-AugmentedGeneration(RAG) (Gao et al., 2023; Qu et al., 2024a,b) hasemerged as a promising method for enhancing thefactual accuracy of responses from Med-LVLMs.By integrating external, reliable data sources, RAGguides the model in producing factual medical re-sponses, enriching its knowledge base with sup-plementary information. For example, RAG hasbeen used in tasks such as visual question answer-ing (VQA) (Yuan et al., 2023) and report gen-eration (Kumar and Marttinen, 2024; Tao et al.,2024). However, as illustrated in (b) and(c), directly applying RAG strategy to Med-LVLMs presents two significant challenges: (1) Asmall number of retrieved contexts may not coverthe reference knowledge required for the question,thus limiting the models factual accuracy. Con-versely, a large number of retrieved contexts mayinclude low-relevance and inaccurate references,which can interfere with the models generation;(2) Med-LVLMs may overly rely on the retrievedinformation. In this situation, the model mightcorrectly answer on its own, but incorporating theretrieved contexts could lead to incorrect responses. To tackle these challenges, we propose theReliable mUltimodaL RAG called RULE for MEd-LVLMs. First, RULE introduces a provable strat-egy for factuality risk control through calibratedselection of the number of retrieved contexts k, en-suring that Med-LVLMs provably achieve high ac-curacy without the need for additional training (An-gelopoulos et al., 2021). Specifically, this strategymodifies the Med-LVLM through a post-processingstep that performs hypothesis testing for each kto determine whether the risk can be maintainedabove an acceptable threshold. This process be-gins by calculating the p-value for each k. Fixedsequence testing is then used to determine which kvalues can be accepted. Second, to mitigate over-reliance on retrieved knowledge, we introduce aknowledge balanced preference fine-tuning strat-egy. This strategy harmonizes the models internalknowledge with retrieved contexts during medi-cal response generation. Here, we identify sam-ples where the model initially responds correctlybut gives incorrect answers after incorporating re-trieved contexts as dispreferred samples, indicat-ing retrieval over-dependence. Conversely, ground-truth responses are considered as preferred samples.The curated preference data is then utilized for fine-tuning the preferences in Med-LVLMs. Our primary contributions of this paper is RULE,which introduces an innovative approach to en-hance retrieval-based Med-LVLMs. RULE notonly controls factual risk by calibrating the selec-tion of reference contexts but also balances themodels knowledge and retrieved contexts throughpreference fine-tuning using a curated preferencedataset. Across three medical Visual Question An-swering (VQA) and report generation benchmarks,including radiology and ophthalmology, our empir-ical results demonstrate that RULE effectively im-proves the factual accuracy of Med-LVLMs, achiev-ing a 14.46% improvement over the best prior meth-ods for mitigating hallucination. In addition, em-pirically verify the effectiveness of the proposedcomponents and demonstrate the compatibility ofRULE.",
  "Preliminaries": "In this section, we will provide a brief overview ofMed-LVLMs and preference optimization.Medical Large Vision Language Models. Med-LVLMs connects the LLMs and medical visualmodules, enabling the model to use medical im-ages xv and clinical queries xt as inputs x. Thisallows the model to autoregressively predict theprobability distribution of the next token. The textoutput of Med-LVLMs is denoted as y.Preference Optimization. Preference optimiza-tion has achieved remarkable results in efficientlyfine-tuning LLMs, significantly aligning their be-havior with the goals. Typically, give an input x,a language model policy can produce a condi-tional distribution (y | x) with y as the outputtext response. The recently popular DPO (Rafailovet al., 2023) utilizes preference data achieve ob-jective alignment in LLMs. The preference datais defined as D = {x(i), y(i)w , y(i)l }Ni=1, where y(i)wand y(i)lrepresent preferred and dispreferred re-sponses given an input prompt x. The probablyof obtaining each preference pair is p(yw yl) =",
  "Selection of k": ": The framework of RULE comprises two main components: (1) a factuality risk control strategy throughthe calibrated selection of k; (2) knowledge-retrieval balance tuning. During the tuning phase, we initially constructa preference dataset from samples where the model errs due to excessive reliance on retrieved contexts. Wesubsequently fine-tune the Med-LVLM using this dataset by employing preference optimization.",
  "Methodology": "In this section, as illustrated in , we willintroduce RULE as an efficient solution for improv-ing factuality of Med-LVLMs. Specifically, our ap-proach consists of three main modules that work to-gether to optimize the models performance. First,we apply the retrieval strategy to Med-LVLMs, en-hancing the models ability to leverage retrievedinformation. Second, we implement a statisticalmethod to control the factuality risk through cal-ibrated selection of retrieved contexts. Third, wedevelop a preference optimization method to bal-ance the models reliance on its own knowledgeand the retrieved contexts. Next, we will detailthese three key modules in detail as follows:",
  "Context Retrieval for Reference": "Med-LVLMs often generate non-factual responseswhen dealing with complex medical images. RAGcan provide the model with external knowledge as areference, thereby effectively enhancing the factualaccuracy. In the multimodal knowledge retrievalstage, RULE retrieves textual descriptions/reportsthat are most similar to the features of the targetmedical images. These references contain a wealthof image-based medical facts and serve to guidethe generation of responses for the medical image.Following the design of CLIP (Radford et al.,",
  "), the retriever will first encode each image and": "the corresponding reports into embeddings usinga vision encoder and a text encoder, respectively.Specifically, all medical images Ximg are encodedinto image representations Vimg RNP by avision encoder Eimg (i.e., Vimg = Eimg(Ximg)),where N is the number of medical images thatneed to be retrieved, and P is the dimension ofthe embedding. Similarly, we generate text embed-dings Vtxt RNP for all corresponding medicalreports Xtxt by applying a text encoder Etxt, i.e.,Vtxt = Etxt(Xtxt). Subsequently, to adapt the gen-eral vision and text encoders to the medical domain,we fine-tune the encoders using the training datawith a contrastive learning loss, defined as:",
  "where S RNN represents the similarity matrixbetween image and text modalities, calculated as:S =Vimg|Vimg| ( Vtxt": "|Vtxt|)T , where each element Si,jrepresents the similarity between the image repre-sentation of example i and the text representationof example j. Equation (2) aims to learn the repre-sentations by maximizing the similarity of text andimage modalities representing the same example, while minimizing the similarity of text and imagemodalities representing different examples.After fine-tuning the image and text encoders,during inference, when faced with a target medicalimage xt requiring the generation of its medical re-port, we extract the top-K similar medical reportsTopKj{1...N}St,j. We then use the retrieved med-ical report to guide the generation of the medicalreport for the target medical image. with the follow-ing prompt guidance: \"You are provided witha medical image, a image-related questionand a reference report. Please answer thequestion based on the image and report.[Question] [Reference Report] [Image]\".",
  "Factuality Risk Control ThroughCalibrated Retrieved Context Selection": "For the RAG strategy, the top-3/5 result is typicallyused as a reference (Gao et al., 2023). However, itsometimes fails to encompass all relevant retrievedcontexts, especially when facing the fine-grainedfeatures of medical images. Additionally, an exces-sive amount of retrieved contexts may introducelow-relevance and inaccurate references, which caninterfere with the models generation. Thus, analgorithm that can automatically determine the op-timal number of retrieved contexts, based on therisk of factual errors, is particularly crucial.In this section, motivated by (Angelopouloset al., 2021), we propose the following strategyto choose a subset for the number of retrievalsk from a candidate set CK N such that the fac-tuality risk FR(k) can be provably controlled forany k . Specifically, first, for each k CK, thestrategy first calculates the factuality risk FR(k),computed as 1 ACC(M(x, (q, Tk))), where xdenotes the target medical image, q denotes thequestion, Tk means the selected top-K retrievedcontexts, and ACC() measures the ratio of correctanswers provided by the Med-LVLM M to the to-tal number of answers. Next, two probabilities pk1and pk2 are computed as:",
  "pk2 = e P(Bin(n, ) nFR(k)),(3)": "where h1(a, b) := a log(a/b) + (1 a) log((1 a)/(1 b)) is the Kullback-Leibler divergence be-tween two Bernoulli distributions and denotesrisk upper bound. pk2 representing the probabil-ity that, in a binomial distribution with param-eters n and , denoted by Bin(n, ), the ob-served value is less than or equal to nFR(k). Then, the minimum of these two probabilitiespk = min (pk1, pk2) is taken. Finally, we use anyfamily-wise error rat (FWER)-controlling proce-dure, such as Bonferroni correction (Van der Vaart,2000) or sequential graphical testing (Bretz et al.,2009), to choose . For example, for Bonferronicorrection, if pk is less than or equal to /|CK|,where denotes tolerance level, then k is addedto the set . The proposed strategy calculates themodels factuality risk under different k values,computes the corresponding probabilities using twoapproaches, and selects those k values that meetthe risk tolerance to control the overall factualityrisk.We have the following result that ensures withprobability at least 1 , the factuality risk pro-duced is controlled by .",
  "Knowledge Balanced Preference Tuning": "In addition to selecting the optimal number k ofretrieved contexts, it is likely that these contentsoften fail to fully capture the details of every le-sion or normal area in medical images. Therefore,when the retrieved contexts is inaccurate, a reliableMed-LVLM is expected to remain unaffected bythe unreliable information and independently useits own knowledge to answer medical questions.However, empirically, as illustrated in , ap-proximately half of all incorrect responses by theretrieval-augmented Med-LVLM are due to an over-reliance on retrieved contexts. This significantlyaffects the application of the retrieval augmentedgeneration strategy to Med-LVLMs.",
  "Experiment": "In this section, we evaluate the performance ofRULE, aiming to answer the following questions:(1) Can RULE effectively improve the factualityof Med-LVLMs compared to other baselines andopen-sourced Med-LVLMs? (2) Do all proposedcomponents boost the performance? (3) How doesRULE change attention weights of retrieved con-texts to balance model knowledge and retrievedcontexts? (4) How do different types of data ormodels influence DPO fine-tuning?",
  "Experimental Setups": "Implementation Details. We utilize LLaVA-Med-1.5 7B (Li et al., 2023) as the backbone model.During the preference optimization process, weadapt LoRA fine-tuning (Hu et al., 2021). Forthe training of retriever, the vision encoder is aResNet-50 (He et al., 2016), and the text encoderis a bio-BioClinicalBERT (Alsentzer et al., 2019).We use the AdamW optimizer with a learning rateof 103, weight decay of 102 and a batch size of32. The model is trained for 360 epochs. For moredetailed information on training hyperparametersand training data, please see Appendix A and C.Baselines. We compare RULE with LVLM hal-lucination mitigation methods that have alreadyshown promising results in natural images, includ-ing Greedy Decoding, Beam Search (Sutskeveret al., 2014),DoLa (Chuang et al., 2023),OPERA (Huang et al., 2023), VCD (Leng et al.,2023). These methods manipulate the logits of themodels output tokens to enhance factual accuracy.Furthermore, we compare the performance withother open-source Med-LVLMs, including Med-Flamingo (Moor et al., 2023), MedVInT (Zhanget al., 2023), RadFM (Wu et al., 2023).Evaluation Datasets.To ensure that the re-trieved report content is relevant to the visualquestion content and to facilitate experimentation,we utilize three medical vision-language datasets,i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), and Harvard-FairVLMed (Luo et al., 2024), encompassing radi-ology and ophthalmology. The training set is splitinto two parts: one part is used to train the retriever(.1), and the other part is used to constructthe preference dataset for KBPT (.3).Additionally, we construct VQA pairs for KBPTand evaluation. Specifically, the reports from train-ing set for preference dataset and reports from orig- : Factuality performance (%) of Med-LVLMs on the three VQA datasets. Notably, we report the accuracy,precision, recall, and F1 score. The best results and second best results are bold and underlined, respectively.",
  "+ RULE (Ours)27.5323.1627.9918.6115.9617.4222.3514.9317.74": "inal test set are input into GPT-4 (OpenAI, 2023)to create closed-ended VQA data with yes or no an-swers, e.g., \"Is there any pulmonary nodule?\". Bysampling segments from a medical report, we cangenerate a sequence of concise, closed-ended ques-tions posed to the model, each with accurate an-swers. The questions are in yes/no format, makingit easier to analyze errors caused by over-relianceon retrieved contexts compared to open-ended ques-tions. The detailed construction process and datasetstatistics are provided in the Appendix A.Evaluation Metrics.For Med-VQA task, weuse Accuracy as the primary metric and, for de-tailed comparisons, we also adopt Precision, Re-call, and F1 Score. For report generation task, weuse BLEU Score (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie,2005) as the metrics.",
  "Results": "In this section, we provide comprehensive compar-ison results with different baseline methods andother open-sourced Med-LVLMs.Comparison with Baseline Methods. We presentthe results of a comparison between RULE andvarious hallucination reduction methods in .According to these results, RULE demonstratesthe best overall performance, effectively and accu-rately diagnosing diseases with an average accu-racy improvement of 47.4% on two tasks across all datasets.We also observe that RULE per-forms notably better on the IU-Xray and Harvard-FairVLMed compared to MIMIC-CXR. This differ-ence is attributed to the excessive length of the re-ports available for retrieval in MIMIC-CXR, whereoverly long references tend to confuse the Med-LVLM. Even when dealing with the relatively nicheophthalmology data (i.e., Harvard-FairVLMed),RULE demonstrates superior results, significantlyenhancing the factual accuracy of the Med-LVLM.In contrast, the performance of decoding meth-ods is quite unstable, showing significant ratesof missed or incorrect diagnoses across differentdatasets, as indicated by the precision and recallvalues. Comparison with Other Med-LVLMs. In Ta-ble 4, we present the comparison with differentopen-sourced Med-LVLMs. RULE demonstratesstate-of-the-art (SOTA) performance across alldatasets. Although the second-best model, Med-VInT, outperforms other models, RULE achievesan average accuracy improvement of 47.4% over it.Whether in radiology or ophthalmology, RULEdemonstrates remarkable performance, signifi-cantly surpassing other open-source Med-LVLMs.This indicates that RULE is generally applicableand effective in the medical multimodal diagnosis,providing consistent improvements across variousmedical image modalities.",
  "ORR#58.6931.35": ": Comparison of over-reliance metrics and attention maps. After optimizing the model with knowledgebalanced preference tuning, first, (a) the Med-LVLMs error (1-acc) and over-reliance ratio significantly decrease.Second, (b) the attention scores for the latter half of the text tokens, i.e., the retrieved contexts, are significantlyreduced, while the attention scores for the first half of the text tokens, i.e., the questions, have increased. It indicatesthat RULE effectively mitigates the models over-reliance on retrieved contexts and enhances factual accuracy.",
  "How Does RULE Improve thePerformance?": "In this section, we conduct a set of analyses demon-strate how different components contribute to theperformance and illustrate how RULE enhancesoverall performance, which are details as follows:Ablation Studies. To further illustrate the effec-tiveness of the components of RULE, we conductablation experiments on three datasets. The resultsare shown in . We find that the basic RAGstrategy (\"R\") slightly improves factual accuracy ontwo datasets but decreases it on MIMIC-CXR. Thelimited retrieved contexts can not cover the fine-grained features of medical images, resulting inunstable factual accuracy improvements. With theaid of the factuality risk control strategy (\"FRC\"),retrieval performance see a stable increase, out-performing the original Med-LVLM. Consideringthe models over-reliance on retrieved contexts, theknowledge balanced preference tuning (\"KBPT\")further enhances the models reliability and signif-icantly improves its performance. Ultimately, bycombining these two strategies, RULE achievesoptimal performance.How does RULE Mitigate the Issue of Over-Reliance on Retrieved Contexts? To better un-derstand how RULE mitigates the Med-LVLMs",
  "+ KBPT + FRC (Ours)87.8487.1283.92": "over-reliance on retrieved contexts, we measurethe Med-LVLMs error and over-reliance ratios,and visualize the text and image attention mapsof the models before and after fine-tuning usinga randomly selected case, as shown in .The quantitative results in (a) demonstratethe significant positive impact of RULE in mitigat-ing the models over-reliance on retrieved contexts,with the error rate and over-reliance rate decreasingby an average of 42.9% and 47.3%, respectively.Attention maps (b) illustrate the modelsattention scores for text and image tokens. We findthat, on the text side, the model with knowledgebalanced preference tuning shows a significantlyreduced focus on retrieved contexts, effectively mit-igating over-reliance on such information. Themodel focuses more on the question and leveragesits own knowledge to answer, rather than relyingsolely on the retrieved contexts, effectively enhanc-ing factual accuracy.Analyzing Preference Data Type in KBPT. Wefurther conduct a thorough analysis of the datatypes used in constructing preference data forKBPT. Three formats are considered: medicalimage captioning (prompted as Please describe w/o KBPTw/ KBPT ACC (%) IU-Xray LLaVA-Med-1.5LLaVA-Med-1.0 w/o KBPTw/ KBPT Harvard-FairVLMed LLaVA-Med-1.5LLaVA-Med-1.0 w/o KBPTw/ KBPT MIMIC-CXR LLaVA-Med-1.5LLaVA-Med-1.0",
  ": Results of RULE on different backbones.KBPT\": knowledge balanced preference tuning": "this medical image\"), visual question-answering(VQA), and a mixture of both. The selected dataare samples where the model makes errors due toover-reliance on retrieved contexts. The resultsare shown in . We observe that modelsfine-tuned using VQA data perform the best acrossall three datasets.This indicates that when re-trieved contexts are incorporated into VQA ques-tions, the Med-LVLM, through KBPT, can learnthis paradigm of integrating and balancing its ownknowledge with retrieved context to maximize fac-tual accuracy. However, when the data is in theform of captioning, it may enhance the modelsability to describe medical facts, but it merely dis-tances the models answers from the retrieved con-texts. The model fails to understand how to balanceretrieval content with its own knowledge.",
  "Compatibility Analysis": "To demonstrate the compatibility of RULE, weconduct KBPT on LLaVA-Med-1.0 as well. Theexperimental results on three datasets are shownin . We find that our knowledge balancedpreference tuning method demonstrates good com-patibility across different models, significantly im-proving factual accuracy across multiple datasets.Based on LLaVA-Med-1.0, RULE increases accu-racy by an average of 16.7%. This indicates thatRULE has a noticeable positive effect on mitigatingover-reliance on retrieved contexts, thereby enhanc-ing the Med-LVLMs factual accuracy.",
  "No, the fundus image does not show any presbyopia": ": Illustrations of factuality enhancement byRULE in radiology and ophthalomology.LLaVA-Med provides a factually incorrect answer.After applying the RAG strategy, the model stillexhibits factual issues, whereas our method effec-tively addresses this and improves accuracy. Incase 2, LLaVA-Med initially provides a correctanswer, but due to the models over-reliance onretrieved contexts, it subsequently produces an in-correct response. RULE balances the weight ofinherent knowledge and retrieved contexts, enhanc-ing factual accuracy.",
  "Related Work": "Factuality in Med-LVLMs.The rapid devel-opment of Large Vision and Language Models(LVLMs) (Liu et al., 2023b,a; Zhu et al., 2023;Alayrac et al., 2022; Zhou et al., 2024a,b; Xia et al.,2024c, 2023) has begun to impact medical diag-nosis. A series of Med-LVLMs (Li et al., 2023;Moor et al., 2023; Wu et al., 2023; Zhang et al.,2023), represented by LLaVA-Med, have emerged,demonstrating impressive performance across var-ious medical image modalities. However, Med-LVLMs still exhibit significant factual errors, pro-ducing medical responses that conflict with thevisual medical information (Xia et al., 2024a; Suet al., 2024). This could potentially lead to mis-diagnoses or missed diagnoses. Recently, severalbenchmarks (Royer et al., 2024; Xia et al., 2024a)have been established to evaluate the accuracy ofMed-LVLMs in tasks such as VQA or report gen-eration. Beyond evaluating factuality, improvingthe factual accuracy of Med-LVLMs remains anunderexplored area.Retrieval Augmented Generation.RAG hasrecently been recognized as a promising solu-tion (Gao et al., 2023; Sun et al., 2024). It enhances the models ability to generate accurate facts by in-corporating contextual information from externaldatasets. In medical multimodal analysis, the RAGapproach has been applied to various tasks suchas medical VQA (Yuan et al., 2023) and reportgeneration (Kumar and Marttinen, 2024; Tao et al.,2024; He et al., 2024). However, in Med-LVLMs,applying RAG-based approaches overlook two crit-ical issues: the number of retrieved contexts andwhether the model overly relies on these reference.These factors can significantly affect the modelsperformance and may even degrade it. In RULE,we systematically address these challenges and en-hance the factuality of Med-LVLMs.",
  "Conclusion": "In this work, we aim to enhance the factuality ofMed-LVLM by addressing two key challenges inmedical RAG. Specifically, we first introduce aprovably effective strategy for controlling factu-ality risk through the calibrated selection of re-trieved contexts. Second, we develop a preferenceoptimization strategy that addresses errors stem-ming from the models excessive dependence onretrieved contexts, aiming to balance its intrinsicknowledge and the retrieved information. Experi-ments on three medical imaging analysis datasetsdemonstrate the effectiveness of RULE.",
  "of the American Medical Informatics Association,23(2):304310": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and HaofenWang. 2023. Retrieval-augmented generation forlarge language models: A survey. arXiv preprintarXiv:2312.10997. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages770778. Sunan He, Yuxiang Nie, Zhixuan Chen, ZhiyuanCai, Hongmei Wang, Shu Yang, and Hao Chen.2024. Meddr: Diagnosis-guided bootstrapping forlarge-scale medical vision-language learning. arXivpreprint arXiv:2404.15127.",
  "arXiv:2106.09685": "Ming Hu, Lin Wang, Siyuan Yan, Don Ma, QingliRen, Peng Xia, Wei Feng, Peibo Duan, Lie Ju,and Zongyuan Ge. 2024a. Nurvid: A large expert-level video database for nursing procedure activ-ity understanding. Advances in Neural InformationProcessing Systems, 36. Ming Hu, Peng Xia, Lin Wang, Siyuan Yan, FeilongTang, Zhongxing Xu, Yimin Luo, Kaimin Song, Ju-rgen Leitner, Xuelian Cheng, et al. 2024b.Oph-net: A large-scale video benchmark for ophthalmicsurgical workflow understanding.arXiv preprint",
  "arXiv:2406.07471": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, WeimingZhang, and Nenghai Yu. 2023. Opera: Alleviatinghallucination in multi-modal large language modelsvia over-trust penalty and retrospection-allocation.arXiv preprint arXiv:2311.17911. Alistair EW Johnson, Tom J Pollard, Nathaniel R Green-baum, Matthew P Lungren, Chih-ying Deng, YifanPeng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz,and Steven Horng. 2019. Mimic-cxr-jpg, a large pub-licly available database of labeled chest radiographs.arXiv preprint arXiv:1901.07042.",
  "Yogesh Kumar and Pekka Marttinen. 2024. Improvingmedical multi-modal contrastive learning with expertannotations. arXiv preprint arXiv:2403.10153": "Sicong Leng, Hang Zhang, Guanzheng Chen, XinLi, Shijian Lu, Chunyan Miao, and Lidong Bing.2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding.arXiv preprint arXiv:2311.16922. Chunyuan Li, Cliff Wong, Sheng Zhang, NaotoUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med: Training a large language-and-vision assis-tant for biomedicine in one day. In Thirty-seventhConferenceonNeuralInformationProcessing",
  "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, andJianfeng Dong. 2024a. Alleviating hallucination inlarge vision-language models with active retrievalaugmentation. arXiv preprint arXiv:2408.00555": "Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. 2024b.Look, compare, decide: Alleviating hallucination inlarge vision-language models via multi-view multi-path reasoning. arXiv preprint arXiv:2408.17150. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision.",
  "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina.2024. Multimedeval: A benchmark and a toolkit forevaluating medical vision-language models. arXivpreprint arXiv:2402.09262": "Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, YanshuLi, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng.2024. Conflictbank: A benchmark for evaluatingthe influence of knowledge conflicts in llm. arXivpreprint arXiv:2408.12076. Jiashuo Sun, Jihai Zhang, Yucheng Zhou, ZhaochenSu, Xiaoye Qu, and Yu Cheng. 2024.Surf:Teaching large vision-language models to selec-tively utilize retrieved information. arXiv preprintarXiv:2409.14083.",
  "Aad W Van der Vaart. 2000. Asymptotic statistics, vol-ume 3. Cambridge university press": "Chunhao Wang, Xiaofeng Zhu, Julian C Hong, andDandan Zheng. 2019.Artificial intelligence inradiotherapy treatment planning: present and fu-ture.Technology in cancer research & treatment,18:1533033819873922. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang,Weidi Xie, and Yanfeng Wang. 2024.Pmc-llama: toward building open-source language mod-els for medicine. Journal of the American MedicalInformatics Association, page ocae045.",
  "arXiv:2406.06007": "Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wen-hao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, andZongyuan Ge. 2024b. Generalizing to unseen do-mains in diabetic retinopathy with disentangled rep-resentations. In arXiv preprint arXiv:2406.06384. Peng Xia, Di Xu, Ming Hu, Lie Ju, and Zongyuan Ge.2024c.Lmpt: Prompt tuning with class-specificembedding loss for long-tailed multi-label visualrecognition. In Proceedings of the 3rd Workshopon Advances in Language and Vision Research(ALVR), pages 2636, Bangkok, Thailand. Associa-tion for Computational Linguistics. Peng Xia, Xingtong Yu, Ming Hu, Lie Ju, ZhiyongWang, Peibo Duan, and Zongyuan Ge. 2023. Hg-clip: Exploring vision-language models with graphrepresentations for hierarchical understanding. arXivpreprint arXiv:2311.14064. Qing Ye, Chang-Yu Hsieh, Ziyi Yang, Yu Kang, Jim-ing Chen, Dongsheng Cao, Shibo He, and TingjunHou. 2021. A unified drugtarget interaction pre-diction framework based on knowledge graph andrecommendation system. Nature communications,12(1):6775. Zheng Yuan, Qiao Jin, Chuanqi Tan, ZhengyunZhao, Hongyi Yuan, Fei Huang, and SongfangHuang. 2023. Ramm: Retrieval-augmented biomed-ical visual question answering with multi-modalpre-training.In Proceedings of the 31st ACM",
  "A.1Data statistics": "The quantities of all the data used are shown in and . It is notable to note that fortraining the retriever, this refers to the number ofimage-text pairs; for fine-tuning, it refers to thenumber of QA items. All\" represents the totalquantity used to construct the preference dataset,where only the samples with correct original an-swers that become incorrect after adding retrievedcontexts are included in the training of knowledgebalanced preference tuning (KBPT\").",
  "A.2Instructions": "We convert the medical reports into a series ofclosed-ended questions with yes or no answers. Toensure the quality of the VQA data, we perform around of self-checks using GPT-4 (OpenAI, 2023).Finally, we conduct an round of manual filteringto remove questions with obvious issues or thoserelated to multiple images or patient histories. Theprompt templates used are shown in .",
  "MIMIC-CXR (Johnson et al., 2019) is a largepublicly available dataset of chest X-ray images": "Instruction [Round1]You are a professional medical expert. I will provideyou with some medical reports. Please generate somequestions with answers (the answer should be yes orno) based on the provided report. The subject of thequestions should be the medical image or patient, notthe report.Below are the given report:[REPORT]Instruction [Round2]Please double-check the questions and answers, includ-ing how the questions are asked and whether the answersare correct. You should only generate the questions withanswers and no other unnecessary information.Below are the given report and QA pairs in round1:[REPORT][QA PAIRS R1]",
  "BEvaluated Models": "We evaluate four open-source Med-LVLMs,i.e.,LLaVA-Med(Lietal.,2023),Med-Flamingo (Moor et al., 2023), MedVInT (Zhanget al., 2023), RadFM (Wu et al., 2023). The se-lected models are all at the 7B level. LLaVA-Med (Li et al., 2023) is a vision-languageconversational assistant, adapting the general-domain LLaVA (Liu et al., 2023b) model forthe biomedical field. The model is fine-tunedusing a novel curriculum learning method, whichincludes two stages: aligning biomedical vocabu-lary with figure-caption pairs and mastering open-ended conversational semantics. It demonstratesexcellent multimodal conversational capabilities. Med-Flamingo (Moor et al., 2023) is a mul-timodal few-shot learner designed for themedical domain.It builds upon the Open-Flamingo (Alayrac et al., 2022) model, contin-uing pre-training with medical image-text datafrom publications and textbooks. This model",
  "aims to facilitate few-shot generative medicalvisual question answering, enhancing clinical ap-plications by generating relevant responses andrationales from minimal data inputs": "RadFM (Wu et al., 2023) serve as a versatilegeneralist model in radiology, distinguished byits capability to adeptly process both 2D and 3Dmedical scans for a wide array of clinical tasks. Itintegrates ViT as visual encoder and a Perceivermodule, alongside the MedLLaMA (Wu et al.,2024) language model, to generate sophisticatedmedical insights for a variety of tasks. This de-sign allows RadFM to not just recognize imagesbut also to understand and generate human-likeexplanations. MedVInT (Zhang et al., 2023), which stands forMedical Visual Instruction Tuning, is designedto interpret medical images by answering clin-ically relevant questions. This model featurestwo variants to align visual and language under-standing (Wu et al., 2024): MedVInT-TE andMedVInT-TD. Both MedVInT variants connecta pre-trained vision encoder ResNet-50 adoptedfrom PMC-CLIP (Lin et al., 2023), which pro-cesses visual information from images. It is anadvanced model that leverages a novel approachto align visual and language understanding.",
  "CImplementation Details": "Following the settings of CLIP (Radford et al.,2021), we adopt the same architecture and hy-perparameters for the vision and text encoders.The vision encoder is a ResNet-50 (He et al.,2016), and the text encoder is a bio-bert-basedmodel (Alsentzer et al., 2019). We use the AdamWoptimizer with a learning rate of 103, weight de-cay of 102 and a batch size of 32. The modelis trained for 360 epochs. The reports availablefor retrieval are from the training set of the corre-sponding dataset. In our experiments, we applycross-validation to tune all hyperparameters withgrid search. All the experiments are implementedon PyTorch 2.1.2 using four NVIDIA RTX A6000GPUs. It takes roughly 2.5 and 4 hours for fine-tuning CLIP and LLaVA-Med-1.5 7B, respectively."
}