{
  "Abstract": "Although Dense Passage Retrieval (DPR) mod-els have achieved significantly enhanced per-formance, their widespread application is stillhindered by the demanding inference efficiencyand high deployment costs. Knowledge distil-lation is an efficient method to compress mod-els, which transfers knowledge from strongteacher models to weak student models. Pre-vious studies have proved the effectivenessof knowledge distillation in DPR. However,there often remains a significant performancegap between the teacher and the distilled stu-dent. To narrow this performance gap, we pro-pose MTA4DPR, a Multi-Teaching-Assistantsbased iterative knowledge distillation methodfor Dense Passage Retrieval, which transfersknowledge from the teacher to the student withthe help of multiple assistants in an iterativemanner; with each iteration, the student learnsfrom more performant assistants and more dif-ficult data. The experimental results show thatour 66M student model achieves the state-of-the-art performance among models with sameparameters on multiple datasets, and is verycompetitive when compared with larger, evenLLM-based, DPR models.",
  "Introduction": "Although PLM/LLM-based Dense Passage Re-trieval (DPR) models (Karpukhin et al., 2020; Qinet al., 2024) have superior performance, those mod-els inference efficiency and deployment costs arestill cumbering their wide applications. To obtainan efficient and effective DPR model, researchersare paying more attention to knowledge distillation.Previous studies (Zeng et al., 2022; Sun et al., 2024;Lu et al., 2022) have proved the effectiveness of",
  "Distillation": ": MTA4DPR Framework. MTA4DPR transfersknowledge from the teacher to the student with the helpof the best assistant. The Fusion Module is used togenerate fused assistants from the original assistants,and the Selection Module is used to select the best assis-tant among all original and fused assistants. The dottedarrows indicate that the corresponding procedures arenot involved in the backpropagation of the training.",
  "knowledge distillation in DPR. However, the per-formance gap between the teacher and the distilledstudent often remains significant, especially whenthe teacher is a very good one": "In this paper, we hypothesize that incorporatingassistants into knowledge distillation can help im-prove students performance, just as teaching assis-tants in universities can assist students in learningcourse content. In addition, inspired by curricu-lum learning (Bengio et al., 2009), we also be-lieve that multiple iterations can further narrow thegap between the teacher and the student since thelatter is capable of learning from more challeng-ing data and more effective assistants as the itera-tions go on. Therefore, we introduce MTA4DPR,a multi-teaching-assistants based iterative distil-lation method. Specifically, MTA4DPR transfersknowledge from the teacher to the student withthe help of multiple assistants iteratively. For each iteration, we first use off-the-shelf teacher/assistantDPR models to generate datasets for training andevaluation. Then, we use a fusion module to gen-erate a series of fused assistants. After that, wetrain the student to learn from the teacher with thehelp of the best assistant selected among all fusedand original assistants by our selection module, asillustrated in . At the end of each itera-tion, we evaluate the students performance andreplace the worst-performing assistant with it if itoutperforms any existing assistants. Whats more,we also incorporate data that the student predictedincorrectly in the previous iteration into the newlyconstructed dataset, by which the difficulty of eachiterations dataset is increased. In this way, as thetraining iterates, the student can learn from moreperformant assistants and more difficult data.The experimental results on MS MARCO,TREC DL 2019 and 2020 and Natural Questionsshow the effectiveness of our method. Our 66Mstudent model achieves the state-of-the-art perfor-mance among models with same parameters onmultiple datasets, and is competitive when com-pared with larger, even LLM-based, DPR models.To summarize, our main contributions are:1) We propose a novel distillation methodMTA4DPR, which improves the students retrievalperformance with the help of assistant models.2) The experimental results show the effective-ness of our proposed method, achieving very com-petitive results even when compared with larger,even LLM-based, DPR models.3) Not constrained by model structures and tasks,MTA4DPR is orthogonal to existing distillationmethods and can be combined with other distilla-tion pipelines to further improve the performance.",
  "Dense Retrieval": "Despite its wide applications, sparse retrieval, suchas BM25, can not thoroughly solve the lexical mis-match problem, although query/document expan-sion (Nogueira et al., 2019; Formal et al., 2021)and term-weighting (Lin and Ma, 2021; Gao andCallan, 2021a) have been proposed to help miti-gate the problem. For this reason, dense retrievers,especially those built upon PLMs or LLMs, havereceived more and more attention. They map both passages and queries into dense vectors, the rele-vance between which can be computed by dot prod-ucts. Recently, a large number of methods havebeen proposed to improve dense retrievers perfor-mance, including negative sampling (Xiong et al.),knowledge distillation (Zeng et al., 2022; Sun et al.,2024; Lin et al., 2023) and joint optimization ofretrievers and rankers (Ren et al., 2021b).",
  "Knowledge Distillation": "Knowledge Distillation transfers knowledge fromthe teacher to the student, allowing the latter tohave good performance with high efficiency. Toachieve this goal, students are forced to learnknowledge representations provided by teachers,including response-based knowledge (Hinton et al.,2015; Beyer et al., 2022), intermediate knowledge(Adriana et al., 2015; Chen et al., 2018; Heo et al.,2019) and relation-based knowledge (Peng et al.,2019; Huang et al., 2022; Yang et al., 2022).Recently, more and more studies focus on multi-teacher distillation, which can draw diverse knowl-edge from multiple teacher models, improving thestudent models performance(Wu et al., 2021; Sonet al., 2021; Lin et al., 2023). Mirzadeh et al. (2020)proposes TAKD, a multi-step knowledge distilla-tion method to bridge the gap between the teacherand the student, in which a larger teacher model dis-tills a smaller teacher model and the latter distillsa much smaller student model. Yuan et al. (2021)proposes a reinforced method to combine multipleteacher models prediction to get the final knowl-edge, which is used to distill the student model. Inall the above studies, researchers tend to treat allteachers equally, combining their predictions usingvarious strategies to train the student model. Weargue that treating all teachers equally might besuboptimal given their varying performance.Different from previous studies, in MTA4DPR,the best-performing model is considered as the pri-mary teacher and involved in the entire trainingprocess, while the remaining models serve as assis-tants, only one of which participates in each train-ing batch. This concept can be analogized to uni-versity students learning from a professor with thehelp of multiple assistants, only one of which is se-lected for each topic based on their speciality. Fur-thermore, we experiment with iteratively replacingunderperforming assistants with better-performing",
  "Preliminary": "3.1.1Task DescriptionAssumewehaveatrainingsetD={(qi, Pi, Si)}ni=1where qiis the query,Piconsists of a positive passage p+i and k hard nega-tives Pi = {pi,j}kj=1 (passages that are difficultto distinguish from the positive passage) andSi = {Si,1, Si,2, ..., Si,d, ...} consists of relevancescores computed by the teacher/assistants andSi,d = {Sji,d}k+1j=1 denotes scores calculated by thed-th model, our target is to train a DPR model thatretrieves the positive passage p+i for the query qi. 3.1.2Dual-Encoders and Cross-EncodersDepending on how queries and passages are en-coded, we categorize DPR models into dual-encoders and cross-encoders.Dual-encoders (Karpukhin et al., 2020) mapquery qi and passage pj into dense vectors, and therelevance between qi and pj is computed by thedot product of their representations:",
  "SDE(qi, pj) = EDE(qi)T EDE(pj)(1)": "where EDE() is the dense vector, and SDE(qi, pj)represents the relevance score of qi and pj.Cross-encoders (Kenton and Toutanova, 2019)concatenate qi and pj as the input to PLMs/LLMs.The relevance between qi and pj is calculated bythe representation of [CLS] in the final layer witha projection layer W: SCE(qi, pj) = WT ECE([CLS]; qi; [SEP]; pj)(2)where [;] is the concatenation operation, andSCE(qi, pj) is the similarity of qi and pj.In practice, we use contrastive loss, which en-courages qi, p+i to be closer together and qi, pi to be further apart, to train DPR models:",
  "(3)": "3.1.3Knowledge Distillation for DPRRecent studies have successfully applied knowl-edge distillation to training more compact DPRmodels. A common approach is to use a teachermodel to compute relevance scores S for q, ppairs, which are then used as the training data forknowledge distillation. To distill the soft labels(scores) from teachers to students, KL divergenceLKL(tea,stu) is used as the loss function:",
  "The MTA4DPR Framework": "MTA4DPR transfers knowledge from the teacherDPR model to the student with the help of m (m 1) assistant models. For each iteration, we first usethese models to generate training and evaluationdatasets (.2.1) which become increasinglydifficult as the iterations go on; then, we selectthe best assistant for each training batch (.2.3) and train the student model using the teachertogether with the selected assistant (.2.4).The training of one iteration is shown in .",
  "Data PreparationAt the start of each iteration, we use the teacher andassistants to generate the corresponding datasets": "Retrieve top-k passagesWe first use each ofthe m assistants to retrieve the top-k most relevantpassages (except the positive passage(s)) for eachquery q. Then, we merge all retrieved passages to-gether and collect scores from each assistant modelfor each q, p pair. In this way, query q has oneor more positive(s) and d negatives (k d mk)each of which has m scores computed by the afore-mentioned m assistant models. Re-rank using RRF scoresFrom the previousstep, we have d negatives for each query qi, andthen we sort these passages in the descending orderbased on the scores assigned by each assistant, re-sulting in a set of rankings R, each ranking r beinga permutation on p1, ..., p|d|. Then, we use RRF(Cormack et al., 2009), Reciprocal Rank Fusion, tore-rank these d passages, taking the top-k passageswith the highest scores as the final hard negativesPi for query qi:",
  "c + r(p)(7)": "where c = 60 following Cormack et al. (2009),and r(p) denotes the position of p in ranking r.Finally, we use the teacher to calculate the rel-evance score for each qi, pj pair where pj Pi.By performing the above operations on all trainingqueries, we obtain the base dataset for the currentiteration, from which we extract 1% as the evalua-tion dataset Deval, leaving the rest as the trainingdataset Dtrain.In addition, inspired by Lin et al. (2023), wecollect the queries for which the teacher can predictthe positive as top-1 while the student from theprevious iteration can not predict correctly. Thesequeries with the positive passage and the top-khard negative passages predicted by the studentwill be added to the generated dataset.",
  "k=1Si,k(8)": "where Si,k is the score distribution between qi andPi computed by the k-th assistant models.Specifically, say we have Si,A, Si,B and Si,C R|Pi| respectively computed by assistants A, Band C; by just taking the average of Si,A and Si,B,Si,A and Si,C, Si,B and Si,C, and all three assis-tants, we can obtain four different new score dis-tributions, i.e. (Si,A+Si,B)",
  "Assistant SelectionTo select the best assistant for each training batch,we investigate three heuristic selection strategies:": "KL DivergenceKL divergence measures thesimilarity between two distributions. The higherthe similarity, the smaller the KL divergence. Wecalculate the KL divergence between the score dis-tributions of the teacher model and each assistant,and consider the assistant that achieves the mini-mum KL divergence as the best teaching assistant. Spearmans FootruleSpearmans Footrule mea-sures the absolute distance between two sorted lists,similar to edit distance. It is suitable for compar-ing the similarity between two permutations, withsmaller values indicating more similar permuta-tions. We calculate the Spearmans Footrule dis-tances between the teacher and each assistant, andconsider the assistant that has the minimum dis-tance with the teacher as the best. Rank Biased OverlapRank Biased Overlap(RBO) compares the overlap of two ranked lists atincreasing depths. Unlike Spearmans Footrule, itassigns different weights to different depths, withtop-1 having the highest weight. The value of RBOranges from 0 to 1, and larger values indicate moresimilar sorted lists. We calculate the RBO mea-sures between the teacher and each assistant, andconsider the assistant that has the maximum RBOvalue as the best assistant.Please note that since this computation processis only for selecting the best assistant, it does notparticipate in the gradient backpropagation. 3.2.4The Student Model OptimizationFor each training batch, we first use the selectionmethod described in 3.2.3 to select the best assis-tant model. Then, we use LCL, LKL to optimizethe student model which is also a dual-encoder:",
  "Ltotal = LCL + LKL(tea,stu) + LKL(ta,stu)(9)where , , are hyper-parameters, LCL is thecontrastive loss of the student model (see more": "in eq(3)). We also calculate the KL divergenceLKL(ta,stu), LKL(tea,stu) as part of the loss duringtraining, forcing the student to learn the score dis-tributions of the best assistant and the teacher.At the end of each iteration, we evaluate thestudents performance on the evaluation dataset,replace the worst-performing assistant with the stu-dent if it outperforms any of the existing assistants,and then regenerate the training/evaluation dataset.We repeat all the above operations, from generat-ing datasets to optimizing the student model, untilthe training ends. The entire training process isintroduced in Algorithm 1 in Appendix A.",
  "Experimental Settings": "We conduct experiments on four retrieval datasets:MS MARCO passage, TREC DL 2019, TREC DL2020 (Craswell et al., 2020a,b) and Natural Ques-tions (NQ) (Kwiatkowski et al., 2019) datasets. Weuse the averaged [CLS] representations of the stu-dent models last three layers to represent eachquery/passage, and dot product to compute thesimilarity between the query and passage. Fol-lowing previous studies, we report MRR@10, Re-call@50 and Recall@1k on MS MARCO dev set,and nDCG@10 on TREC DL 2019 and 2020; andwe choose Recall@5, Recall@20 and Recall@100as the evaluation metrics for Natural Questions. BaselinesTo make a comprehensive compari-son, we compare MTA4DPR with three groupsof baselines: sparse retrieval models and denseretrieval models with/without knowledge distil-lation. Specifically, sparse retrieval models in-clude BM25 (Robertson et al., 2009), DeepCT(Dai and Callan, 2019), GAR (Mao et al., 2021),docT5query (Nogueira et al., 2019), COIL-full(Gao et al., 2021), UniCOIL(Lin and Ma, 2021)and SPLADE-max (Formal et al., 2021); denseretrieval models without knowledge distillation in-clude DPR (Karpukhin et al., 2020), ANCE (Xionget al.), Condenser (Gao and Callan, 2021b), XTR-base (Lee et al., 2024), CotMAE (Wu et al., 2023),GTR-XXL (Ni et al., 2022) and RepLLaMA-7B(Ma et al., 2024); dense retrieval models withknowledge distillation include RocketQAv1 (Quet al., 2021), PAIR (Ren et al., 2021a), Rock-etQAv2 (Ren et al., 2021b), ERNIE-Search (Lu",
  "et al., 2022), SimLM (Wang et al., 2023), Retro-MAE (Xiao et al., 2022), LEAD (Sun et al., 2024),CL-DRD (Zeng et al., 2022) and PROD (Lin et al.,2023)": "Model InitializationFor MS MARCO, to bal-ance the trade-off between efficiency and effective-ness, we choose dual-encoders as the assistantsand the cross-encoder as the teacher. Specifically,we set CotMAE, SimLM-distilled, RetroMAE andM2DPR (Lu, 2024) as assistants, since they arethe most performant off-the-shelf dense retriev-ers to our knowledge. Their MRR@10 on MSMARCO dev set are 39.4, 41.1, 41.6 and 42.0,respectively. SimLM-reranker, a well performantcross-encoder, is considered as the teacher modelwith 43.7 MRR@10. Besides, to validate the ef-fectiveness on NQ dataset, we simply use Rock-etQAv1 and PAIR as the assistants, and ERNIE-search as the teacher model with Recall@20 82.7,83.5 and 85.3 on NQ test set. The student DPRmodels are initialized with the SimLM-base model. Training DetailsFor MS MARCO, we set theiterations to 3, as our experiments show that theperformance improvement becomes marginal be-yond the 3rd iteration. For each iteration, we use 1Tesla A100 80G GPU to train our student model for20,000 steps using AdamW optimizer with learn-ing rate of 3 105. Each query in the trainingset has several positive passages and k = 100 hardnegatives. Each training batch has 64 queries, eachof which has 1 positive passage and 34 hard nega-tives randomly sampled from the training set. Theweight decay is set to 0.01. The max query lengthis 32, and the max passage length is 144. To bal-ance each term of the final loss, , and are setto 0.2, 1, 15. For NQ, we reuse the same settingsas those on MS MARCO with a few exceptions.The training steps for each iteration is set to 10,000steps, and the max passage length is 192.",
  "Main Results": "The results comparing MTA4DPR with multiplebaselines on the MS MARCO, TREC DL 19 and20 and NQ datasets are shown in and Ta-ble 2. From the tables, we can observe that the66M student model trained by MTA4DPR achievesMRR@10 41.1 on MS MARCO, nDCG@10 71.2on TREC DL 19, nDCG@10 71.1 on TREC DL",
  ": Main results on NQ. #Params represents thenumber of model parameters": "20 and Recall@20 83.6 on NQ, which outperformsmost 66M distilled student models, and is competi-tive when compared with larger DPR models (the110M ones), even with the LLM-based models.In addition, we have the following observations:1) RepLLaMA-7B achieves MRR@10 41.2 onMS MARCO, nDCG@10 74.3 and 72.1 on TRECDL 19 and 20, far surpassing most baselines with-out knowledge distillation, which means that, with-out knowledge distillation, the larger the model,the better the retrieval performance.2) 110M DPR models trained with knowledgedistillation, such as SimLM (MRR@10 41.1 onMS MARCO dev) and ERNIE-Search ( on NQ test), can achieve better retrieval per-formance when compared with the models with the same or even much bigger sizes without knowledgedistillation, from which we can see that knowledgedistillation can effectively transfer knowledge fromlarge teacher DPR models to small student models.3) RepLLaMA-7B performs about better than 66M DPR models on DL 20 whichis mainly used to test models ability to capturefine-grained semantics. This implies that, in cap-turing fine-grained semantics, large DPR modelsare much better than small models, which moti-vates us to further optimize small models abilityto capture fine-grained semantic.",
  "Ablation Study": "To validate the effectiveness of each module ofour method, we conduct the ablation study. Allablation results come from 3-iteration training, ex-cept for w/o iterations in which we deliberatelydisabled the iteration to show its effectiveness.The results in demonstrate the effective-ness of our model. We can see that removing anymodule will decrease the final performance, withthe removal of the teaching assistants resulting inthe most significant performance drop. Addition-ally, we also have the following observations.:1) Without teaching assistants, the studentmodels performance drops to MRR@10 39.9on MS MARCO and Recall@20 82.2 on NQ,which indicates that using teaching assistants can",
  ": Ablation results on MS MARCO and NQ": "help students better learn the knowledge fromteacher/assistant models.2) The performance also drops to on MS MARCO and Recall@20 83.4 on NQwithout fusion strategy. Through further analy-sis, we find that the KL divergence between fusedscore distributions and the teachers score distri-bution tends to be smaller than that of originalassistants, which means students can learn moreuseful information from fused assistants than theoriginal assistants.3) Finally, without training iterations, the perfor-mance of the student model drops to on MS MARCO and Recall@20 82.7 onNQ. This indicates that our iterative trainingmethod which enable students to learn from betterteacher/assistants and more difficult data at eachiteration improves the students performance.",
  "Analysis": "We further analyze our proposed method from thefollowing perspectives, i.e. the performance of thestudent model at each iteration, the assistant se-lection methods, student models scale, assistantmodels performance, the assistants selected, thecomplexity of the training process and the compu-tational costs of the student models. 4.4.1Multi-iteration Retrieval PerformanceWe report the retrieval performance of our 66MDPR model in each iteration, as shown in .As expected, as the number of iterations increases,the performance also improves, from to 41.1 on MS MARCO and from to 83.6 on NQ. This indicates that to someextent, better assistant models combined with moredifficult data will further improve the performanceof the student model.",
  "The impact of the number of layers andthe embedding sizes of student models": "We use the proposed method to distill student DPRmodels with different number of layers and em-bedding sizes. As shown in , we can seethat:1) MTA4DPR can improve the retrieval perfor-mance of the student models with different numberof layers and embedding sizes; and as the num-ber of layers and the number of embedding sizeincrease, the performance improves.2) It is worth noting that our 33M DPR modelis almost equivalent to the existing 110M DPRmodels on Recall@1k on MS MARCO. Due to thefact that retrievers are often used in the first stageof retrieve-rerank pipeline in practical scenarios, a33M DPR model can be used to reduce query time.3) Finally, we also find that the 12-layer 384-dimensional models outperform the 3-layer 768-dimensional models, despite having fewer param-eters. We speculate that this might be due to the12-layer models ability to capture more complextext interactions owing to its greater depth. We willinvestigate this further in future work.",
  "MRR@10R@50R@1k": "638417M36.0 ( 1.1)81.6 ( 1.3)96.3 ( 0.3)1238433M40.1 ( 0.8)87.2 ( 0.7)98.4 ( 0.0)376845M39.4 ( 0.9)86.5 ( 1.1)98.4 ( 0.1)676866M41.1 ( 1.2)88.4 ( 1.6)98.7 ( 0.2)12768110M41.8 ( 0.7)88.6 ( 0.8)98.8 ( 0.1) : Results of MTA4DPR models with differentsizes on MS MARCO. #Layers denotes the numberof layers of the model, and #Emb denotes the embed-ding size of the model. #Params denotes the number ofmodel parameters. denotes the improvement com-pared with traditional knowledge distillation methods.",
  "The impact of the performance ofassistant models": "We wonder how the performance of the assistantsaffects the distillation process. To this end, we con-ducted five groups of experiments, i.e. No assistant,Single-assistant distillation, Double-assistant distil-lation, Triple-assistant distillation and Quadruple-assistant distillation. No assistant involved distil-lation using only the teacher model without anyassistants. Single-assistant distillation experimentsare done using just one assistant and one teacherfor distillation. Double-assistant distillation uti-lized one teacher and two assistants along with afusion strategy for distillation, and so on.The results are listed in . From the table,we have the following observations:1) Compared to not using assistants, even theresult of using the weakest assistant model is betterthan the no-assistant way. For example, using only",
  ": The composition of the best teaching assis-tants selected on MS MARCO. R denotes RetroMAE,S denotes SimLM, M denotes M2DPR and R&Mdenotes the fusion result of RetroMAE and M2DPR": "CotMAE can increase the value of MRR@10 from39.9 to 40.2 on MS MARCO dev set. This stronglyproves the effectiveness of using assistant models.2) R&M is better than other double-assistantcombinations, S&R&M is better than other triple-assistant combinations. This implies that the betterthe performance of assistants, the better the perfor-mance of the distilled student model.",
  "The composition of the best assistant": "We explore which assistant is selected as the bestone in each batch during the whole training proce-dure. The composition of the best teaching assis-tants selected on MS MARCO is shown in . From the figure, we can see that the fusion re-sult of RetroMAE and M2DPR is chosen for nearly50% of the time, which confirms once again the",
  "G66M163.23s87.86s1238412.8G33M297.06s131.67s1276825.2G110M304.30s135.82s": ": The computational costs of student DPR mod-els with different sizes. Encoding Time is the timetaken to encode the whole MS MARCO corpus. #Embdenotes the embedding size of the model. Please notethat this metric is pure GPU computation time anddoesnt include the time for data loading or other opera-tions. bs denotes the batch size.",
  "The complexity of the training process": "The time consumption of our method can be di-vided into two parts: model training and data con-struction. The time taken to train a 6-layer 768-dimensional student model is shown in .Since the teachers/assistants are not actually in-volved in the training process but only providequery-passage pair scores, which can be obtainedduring data construction, the training time of ourmethod is only about 25 minutes longer than thatof the traditional knowledge distillation, primarilydue to the selection of the best teaching assistantfor each batch. For the data construction, we re-quire approximately 4.7 more hours compared tothe traditional knowledge distillation method. Theadditional time is mainly spent on scoring unseenquery-passage pairs using both the teacher and as-sistants models, which will be used for the nextiteration. While time-consuming, this process pro-vides us a more difficult dataset, which can furtherimprove the performance of the student model.",
  "The computational costs of MTA4DPR": "We also conduct more experiments to further vali-date the efficiency and the computational costs ofthe student model distilled by our proposed methodunder three different settings, as shown in . From the table, we can see that: reducing theembedding size is more efficient than reducingmodel layers in terms of the model size (decreasedfrom 110M to 33M) and index size (decreased from 25.2G to 12.8G); while reducing model layersprovide more improvement in terms of the modelencoding time (decreased from 304.30s to 163.23swith the 512 batch size, and from 135.82s to 87.86swith the 1024 batch size).",
  "Conclusion": "In this paper, we propose MTA4DPR, an iterativemulti-assistant distillation method for DPR. It dis-tills the student with the help of the teaching as-sistants in an iterative manner, with each iterationcreating more difficult datasets and more perfor-mant assistants. The experimental results on MSMARCO, TREC DL 2019 and 2020 and NaturalQuestions show the effectiveness of our method.Our 66M DPR model can achieve the state-of-the-art performance among models with same pa-rameters on multiple datasets and is very com-petitive when compared with larger, even LLM-based, DPR models. MTA4DPR confirms thatthe iterative distillation with multiple assistantscan improve the distillation performance. Sinceit is orthogonal to existing distillation methods,other distillation pipelines can be combined withMTA4DPR to further improve their performance.In addition, MTA4DPR is not constrained bymodel structures and tasks, and can be broadlyapplicable other fields than DPR, including textclassification, question answering and text summa-rization, etc.",
  "Limitations": "We consider the following four points as the limi-tations of this work:First, due to flexibility and scalability consid-erations, we only distill the score distributionsprovided by teacher/assistants, while ignoringinformation provided by intermediate layers ofteacher/assistant models which can be beneficial tofurther improve the student models performance.Second, at the first training iteration, our methodrequires multiple off-the-shelf DPR models, butwhen there are not enough available models, weneed to train teacher/assistant DPR models fromscratch, which may increase the training costs.Third, for the sake of the training phases sim-plicity and efficiency, we only use heuristic strate-gies when generating fused scores and selectingthe best teaching assistant. To further improve stu-dent performance, we can design more complexand effective generation and selection methods.Finally, in the future, we can continue to explorethe impact of the number and performance of teach-ing assistants on the final retrieval result of studentmodels, and find out how to determine what kindof teaching assistant is good.",
  "Yoshua Bengio, Jrme Louradour, Ronan Collobert,and Jason Weston. 2009. Curriculum learning. InProceedings of the 26th annual international confer-ence on machine learning, pages 4148": "Lucas Beyer, Xiaohua Zhai, Amlie Royer, Larisa Mar-keeva, Rohan Anil, and Alexander Kolesnikov. 2022.Knowledge distillation: A good teacher is patient andconsistent. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition,pages 1092510934. Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.2018. Darkrank: Accelerating deep metric learningvia cross sample similarities transfer. In Proceed-ings of the AAAI conference on artificial intelligence,volume 32. Gordon V Cormack, Charles LA Clarke, and StefanBuettcher. 2009. Reciprocal rank fusion outperformscondorcet and individual rank learning methods. InProceedings of the 32nd international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 758759.",
  "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, DanielCampos, and Ellen M Voorhees. 2020b. Overviewof the trec 2019 deep learning track. arXiv preprintarXiv:2003.07820": "Zhuyun Dai and Jamie Callan. 2019. Deeper text un-derstanding for ir with contextual neural languagemodeling. In Proceedings of the 42nd internationalACM SIGIR conference on research and developmentin information retrieval, pages 985988. Thibault Formal, Benjamin Piwowarski, and StphaneClinchant. 2021. Splade: Sparse lexical and expan-sion model for first stage ranking. In Proceedingsof the 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 22882292.",
  "Luyu Gao and Jamie Callan. 2021b. Is your languagemodel ready for dense representation fine-tuning?CoRR, abs/2104.08253": "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Coil:Revisit exact lexical match in information retrievalwith contextualized inverted list. In Proceedings ofthe 2021 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, pages 30303042. Byeongho Heo, Minsik Lee, Sangdoo Yun, andJin Young Choi. 2019. Knowledge transfer via dis-tillation of activation boundaries formed by hiddenneurons. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 37793787.",
  "Tao Huang, Shan You, Fei Wang, Chen Qian, andChang Xu. 2022.Knowledge distillation from astronger teacher. Advances in Neural InformationProcessing Systems, 35:3371633727": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781. Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. 2019. Bert: Pre-training of deep bidirec-tional transformers for language understanding. InProceedings of naacL-HLT, volume 1, page 2. Min-neapolis, Minnesota. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Al-berti, Danielle Epstein, Illia Polosukhin, Jacob De-vlin, Kenton Lee, Kristina Toutanova, Llion Jones,Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.Natural questions: A benchmark for question an-swering research. Transactions of the Associationfor Computational Linguistics, 7:452466. Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu,Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vin-cent Zhao. 2024. Rethinking the role of token re-trieval in multi-vector retrieval. Advances in NeuralInformation Processing Systems, 36.",
  "Qixi Lu. 2024. M2DPR: A multi-task multi-view rep-resentation learning framework for dense passageretrieval. In NAACL Student Research Workshop2024": "Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi,Zhengjie Huang, Shikun Feng Yu Sun, Hao Tian,Hua Wu, Shuaiqiang Wang, Dawei Yin, et al. 2022.Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for dense pas-sage retrieval. arXiv preprint arXiv:2205.09153. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, andJimmy Lin. 2024. Fine-tuning llama for multi-stagetext retrieval. In Proceedings of the 47th Interna-tional ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 24212425. Yuning Mao, Pengcheng He, Xiaodong Liu, YelongShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.2021.Generation-augmented retrieval for open-domain question answering. In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 40894100.",
  "Ibomoiye Domor Mienye, Yanxia Sun, and ZenghuiWang. 2020. Improved predictive sparse decompo-sition method with densenet for prediction of lungcancer. Int. J. Comput, 1:533541": "Seyed Iman Mirzadeh, Mehrdad Farajtabar, AngLi, Nir Levine, Akihiro Matsukawa, and HassanGhasemzadeh. 2020. Improved knowledge distil-lation via teacher assistant. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 51915198. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernandez Abrego, Ji Ma, Vincent Zhao,Yi Luan, Keith Hall, Ming-Wei Chang, et al. 2022.Large dual encoders are generalizable retrievers. InProceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pages98449855.",
  "models are effective text rankers with pairwise rank-ing prompting. In Findings of the Association forComputational Linguistics: NAACL 2024, pages15041518": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, RuiyangRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,and Haifeng Wang. 2021. Rocketqa: An optimizedtraining approach to dense passage retrieval for open-domain question answering. In Proceedings of the2021 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, pages 58355847. Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu,Wayne Xin Zhao, Qiaoqiao She, Hua Wu, HaifengWang, and Ji-Rong Wen. 2021a.Pair: Leverag-ing passage-centric similarity relation for improvingdense passage retrieval. In Findings of the Associ-ation for Computational Linguistics: ACL-IJCNLP2021, pages 21732183. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-RongWen. 2021b. Rocketqav2: A joint training methodfor dense passage retrieval and passage re-ranking.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages28252835.",
  "Stephen Robertson, Hugo Zaragoza, et al. 2009. Theprobabilistic relevance framework: Bm25 and be-yond. Foundations and Trends in Information Re-trieval, 3(4):333389": "Wonchul Son, Jaemin Na, Junyong Choi, and WonjunHwang. 2021. Densely guided knowledge distilla-tion using multiple teacher assistants. In Proceed-ings of the IEEE/CVF International Conference onComputer Vision, pages 93959404. Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, JingwenLu, Yan Zhang, Linjun Yang, Rangan Majumder,and Nan Duan. 2024. Lead: liberal feature-baseddistillation for dense retrieval. In Proceedings of the17th ACM International Conference on Web Searchand Data Mining, pages 655664. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,Linjun Yang, Daxin Jiang, Rangan Majumder, andFuru Wei. 2023. SimLM: Pre-training with repre-sentation bottleneck for dense passage retrieval. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 22442258, Toronto, Canada.Association for Computational Linguistics. Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021.One teacher is enough? pre-trained language modeldistillation from multiple teachers. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 44084413. Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin,Zhongyuan Wang, and Songlin Hu. 2023. Contex-tual masked auto-encoder for dense passage retrieval.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 37, pages 47384746. Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.2022. Retromae: Pre-training retrieval-oriented lan-guage models via masked auto-encoder. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 538548. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N Bennett, Junaid Ahmed, andArnold Overwijk. Approximate nearest neighbornegative contrastive learning for dense text retrieval.In International Conference on Learning Represen-tations. Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang,Yongjun Xu, and Qian Zhang. 2022. Cross-image re-lational knowledge distillation for semantic segmen-tation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages1231912328. Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, MingGong, Yan Fu, and Daxin Jiang. 2021. Reinforcedmulti-teacher selection for knowledge distillation. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pages 1428414291."
}