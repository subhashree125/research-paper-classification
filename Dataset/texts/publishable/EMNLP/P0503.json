{
  "Abstract": "Recent progress with LLM-based agents hasshown promising results across various tasks.However, their use in answering questions fromknowledge bases remains largely unexplored.Implementing a KBQA system using tradi-tional methods is challenging due to the short-age of task-specific training data and the com-plexity of creating task-focused model struc-tures. In this paper, we present Triad, a unifiedframework that utilizes an LLM-based agentwith multiple roles for KBQA tasks. The agentis assigned three roles to tackle different KBQAsubtasks: agent as a generalist for masteringvarious subtasks, as a decision maker for theselection of candidates, and as an advisor for an-swering questions with knowledge. Our KBQAframework is executed in four phases, involvingthe collaboration of the agents multiple roles.We evaluated the performance of our frame-work using three benchmark datasets, and theresults show that our framework outperformsstate-of-the-art systems on the LC-QuAD andYAGO-QA benchmarks, yielding F1 scores of11.8% and 20.7%, respectively.",
  "Introduction": "A question-answering system is designed to extractinformation by converting a natural language ques-tion into a structured query that can retrieve pre-cise information from an existing knowledge base(Omar et al., 2023a). The resolution of KnowledgeBase Question Answering (KBQA) typically in-volves phases including question understanding,URI linking, and query execution.TraditionalKBQA systems require the use of specialized mod-els trained with domain datasets for question pars-ing and entity linking (Hu et al., 2018; Omar et al.,2023a; Hu et al., 2021). Large language models(LLMs), however, have shown promising compe-tencies in in-context learning using task-specific",
  "Corresponding authors": "demonstrations (Dong et al., 2022). LLMs haverecently been employed as agents in the executionof complex problems. A framework that employsLLM-augmented agents can generate actions or co-ordinate multiple agents, thus improving the capac-ity to handle complex situations (Liu et al., 2023).Despite the remarkable performance of LLMs invarious tasks as evidenced in previous studies, acomprehensive qualitative and quantitative evalu-ation of KBQA frameworks empowered with anLLM-based agent remains insufficiently explored.",
  ": A system with multiple roles who focus onsub-problems of each phase to solve a complex task": "Studies on KBQA with LLMs has attracted con-siderable attention. Some works focus primarilyon highlighting the inability of LLMs to gener-ate complete factoid results (Hu et al., 2023b; Tanet al., 2023c) or demonstrating their potential ef-ficacy in future research (Omar et al., 2023b; Tanet al., 2023b). Other works concentrates on gener-ating answers by prompt learning and incorporat-ing external knowledge bases (Baek et al., 2023;Tan et al., 2023a). Concurrently, LLMs can bedeployed to address each phase within Text2SQLchallenges(Li et al., 2023, 2024) or theorem prooftasks(Dong et al., 2023). However, each phase of KBQA can be further decomposed into subtasksand completed through an agentic approach thatprovides feedback and cooperation. Additionally,decomposing the task reduces the complexity of co-operative working by allowing each role to concen-trate on smaller sub-problems(Wang et al., 2020).As illustrated in , three roles in an orga-nization work together to provide the final answerfor the overall task. The above observations spurour exploration into the following question: Howdoes an LLM-based agent solve KBQA tasks byserving as multiple roles, and its performance iscomparable to systems trained specifically?In this study, we introduce Triad, a unifiedframework that leverages an LLM-based agent withthree roles to address KBQA tasks. Specifically, weimplement the agent consisting of an LLM as thecore, supplemented by various task-specific mod-ules such as memory and executing functions. Theagent is assigned three distinct roles: a general-ist (G-Agent) adept at mastering numerous smalltasks by the given examples, a decision maker (D-Agent) proficient at identifying options and select-ing candidates, and an advisor (A-Agent) skilledat providing answers using internal and externalknowledge. The cooperation of these agent rolescomposes a KBQA process containing four phases:question parsing, URI linking, query construction,and answer generation. We evaluate our frameworkon three benchmark datasets in various difficulties.The results show that our framework outperformsstate-of-the-art systems, demonstrated by 11.8%and 20.7% F1 scores on the LC-QuAD and YAGO-QA benchmarks, respectively1.The contributions of this study can be summa-rized as follows:",
  "Task Formulation": "A KBQA task refers to the process of solving a setof subtasks S. Each subtask St S contributes toone phase of the whole process. An LLM-basedagent Agentr with a role r can be used to resolve atype of subtasks by its task-specific components, in-cluding a language model LLM, a memory Memt,a function Ft, a prompt Pmtt and a set of parame-ters t, using the set of role-related hyperparame-ters r. The task can be formulated as follows:",
  "G-Agent as a Generalized Solver": "A generalized agent (G-Agent) proficiently man-ages numerous tasks by leveraging learning fromlimited examples through an LLM. In our frame-work, a G-Agent can perform question parsing,query template generation, or answer type classi-fication as actions solely utilizing an LLM. Thesethree subtasks are illustrated as follows: Triplet mention extraction:The process of ex-tracting triplet mentions in question parsing in-volves the conversion of a naturally phrased ques-tion, denoted as Q, into formatted triplets of entitiesand relations. This subtask is executed employingan LLM, which is guided by a prompt with a setof prerequisites and a selection of examples. Thissubtask can be represented as follows:",
  "Pmttri = [Instri, Shottri, CoTtri](2)": ", where Agentg is the agent as a generalist to per-form the triplet extraction subtask with N exam-ples. Pmttri is the prompt to guide LLM to gen-erate triplets from the question Q, which consistsof instruction Instri, examples Shottri, and chain-of-thought prompt CoTtri (Kojima et al., 2022). SPARQL template generation:The generationof SPARQL templates in query construction in-volves the use of an LLM to create a SPARQLtemplate that articulates the question using stan-dard SPARQL syntax, replacing URI identifierswith entity and relation variables. To derive pre-cise and comprehensive answers from the knowl-edge base using SPARQL queries, there are twopotential strategies. One approach involves the di-rect generation of an executable SPARQL using an LLM, though this method may significantly in-crease LLM call times and error rates when nu-merous candidate queries are in play.Alterna-tively, a SPARQL template can initially be gen-erated with entity and relation variables, which aresubsequently replaced with linked URIs. For thesake of stability and efficiency, we opt for the sec-ond strategy. This subtask can be denoted as:",
  "(3)": ", where Agentg is the agent as generalist to per-form SPARQL template generation with N exam-ples, f(Stri) is the triplets derived from the pre-vious subtask, Pmtqt is the prompt for LLM togenerate SPARQL template. Answer type classification:In the phase of an-swer generation, the answer type classification sub-task refers to the process of assigning a specificcategory to a response according to the question.This process serves as a guiding mechanism for theframework to generate comprehensive and accurateanswers. This classification subtask is denoted as:",
  "D-Agent as a Decision-Maker": "An agent as a decision maker (D-Agent) is capableof making candidate selections step by step throughfiltering and choosing from given options, harness-ing the capabilities of an LLM and KB as memory.The D-Agent can effectively handle three subtasks,which are delineated as follows: Candidate entity selection:The selection of can-didate entities in URI linking is pivotal to the ul-timate efficacy of KBQA. Prior research has fo-cused primarily on developing a semantic similaritymodel to address this linking challenge. However,the linking task requires numerous iterations ofsearching within the knowledge base, which posesa compatibility issue for LLM-oriented methods.In our framework, an agent as a decision maker isutilized initially to filter all potential entity URIsfrom the knowledge base, subsequently deployingan LLM to select candidate URIs from a pool of",
  "Triplets": ": Our Triad framework leverages an LLM-based agent with three different roles including a generalist, adecision-maker, and an advisor to cooperatively handle a series of subtasks in the four phases of a KBQA process. potential identifiers. For each entity, our aim is tofind the K most possible entity URIs which can beused to traverse over the KB to get the final answer.The entity selection subtask can be denoted as:",
  "(5)": ", where Agentd is the agent as a decision maker toperform the entity selection subtask with questionQ, extracted triplets f(Stri) and memory Memes,Memes is composed of a knowledge base KB anda list of entity URIs Listes filtered from KB usinga text similarity matching function Fes, Pmtes isthe prompt for LLM to perform the subtask, Kis the hyperparameter of Agentd, indicating thenumber of candidates selected by LLM. Candidate relation selection:The task of can-didate relation selection in URI linking presentsconsiderable challenges due to the discrepanciesbetween word forms and meanings. Nevertheless,the existence of reasoning paths in the knowledgebase can be utilized to allow for a significant reduc-tion of the search space in relation linking. In ourframework, an agent as a decision maker endeavorsto sieve through all potential relation URIs by nav-igating the knowledge base with candidate entityURIs generated from the previous subtask. Subse-quently, an LLM is used to select the top K mostprobable relation URIs for output. The relation",
  "(6)": ", where memory Memrs is composed of the knowl-edge base KB and a list of possible relation URIsListrs filtered from KB using a one-order travers-ing function Frs. Pmtrs is the prompt for LLMto perform relation selection. K is the number ofrelation URIs selected by LLM. Candidate SPARQL selection:The subtask ofcandidate SPARQL selection in query construc-tion involves determining the appropriate SPARQLqueries to obtain the final answers.Given aSPARQL template generated by the G-Agent,along with multiple candidate URIs selected fromthe D-Agent in previous subtasks, our D-Agent istargeted to identify the most plausible query. To fur-ther reduce the difficulty of selection, an executorfunction is applied to eliminate queries that can-not retrieve any results from the knowledge base.In conclusion, our aim in this subtask is to use D-Agent to construct executable SPARQLs and findthe most possible one given a query candidate listwith supported information. The SPARQL selec-tion subtask can be denoted as:",
  "(7)": ", where memory Memqs is composed of a knowl-edge base KB and a list of possible SPARQLsListqs constructed with SPARQL template f(Sqt),entity URIs f(Ses), and relation URIs f(Srs) bythe function Fqs, Pmtqs is the prompt for LLM toperform query selection, K = 1 is the number ofqueries selected by LLM.",
  "A-Agent as a Comprehensive Advisor": "An advisory agent (A-Agent) is capable of process-ing a question and a corresponding type of answeras input. Its response is generated by either extract-ing information from an external knowledge baseor by utilizing its internal knowledge to providea direct answer. This comprehensive answeringsubtask can be described as follows: Comprehensive answering:The objective ofcomprehensive answering in the answer genera-tion phase is to derive a definitive response basedon an incoming question. Previous work (Omaret al., 2023b) has demonstrated that LLMs aremore proficient in delivering single-fact responsesand making Boolean judgments. Given this un-derstanding, we implement an advisory agent thatincorporates a simple policy to facilitate a com-prehensive answering approach. Specifically, if aquestion yields a final SPARQL generated from thepreceding steps, A-Agent extracts elements fromthe knowledge base to give the answer. Conversely,if the agent does not receive a feasible SPARQL,A-Agent provides a direct response with LLMsinternal knowledge, following the prompt based onthe type of the answer. Additionally, A-Agent willsend a retry signal to previous phases if no resultis generated. The subtask can be formulated asbelow:",
  "(8)": ", where Agenta is the agent as an advisor to per-form a comprehensive answering for the questionQ with a memory Memca of knowledge base,Pmtca is the prompt for LLM to perform a di-rect response according to the type of the answer,f(Sqs) is the final query and f(Scls) is the answertype, T is the maximum times to retry for previousphases if no result is returned from KB.",
  "Experimental Settings": "Indexed Knowledge Bases:The efficacy of ourframework is assessed through the collection oftwo real knowledge bases, specifically DBpediaand YAGO. DBpedia (Auer et al., 2007) servesas an accessible knowledge base extracted fromWikipedia, while YAGO (Pellissier Tanon et al.,2020) is a large knowledge base that includes in-dividuals, cities, nations, and organizations. Weindex the triples and the mentions of entities and re-lations in a Virtuoso endpoint and an Elasticsearchserver, respectively. KBQA Benchmark Datasets:We evaluate ourframework on datasets including YAGO-QA, LC-QuAD 1.0, and QALD-9, which have various diffi-culties in interpreting the questions. These datasetscontain questions in English, paired with their re-spective SPARQL queries, and accurate responsesderived from a specific knowledge base. QALD-9(Usbeck et al., 2018) and LC-QuAD 1.0 (Trivediet al., 2017) are frequently used to evaluate QA sys-tems with DBpedia. The recently published YAGO-QA in (Omar et al., 2023a), features questions ac-companied by annotated SPARQL queries sourcedfrom YAGO. The statistics for three benchmarks,along with their associated knowledge bases, aredepicted in . Baseline Methods:We evaluate Triad againsttraditional KBQA systems such as KGQAN (Omaret al., 2023a), EDGQA (Hu et al., 2021) and gAn-swer (Hu et al., 2018). This comparison showshow our LLM-based agent framework can rivalfull-shot systems with just a few examples. Addi-tionally, we contrast our framework with pure GPTmodels like GPT-3.5 Turbo and GPT-4 2 to exhibitTriads architectural performance. We treat thesefoundation models as few-shot methods to answerthe questions referring to some examples. Implementation Details:Triad is implementedwith Python 3.9. We incorporate LLM capabilitiesto our multi-role agent via OpenAIs API services.The names of entities and relations from knowl-edge bases are indexed in an ElasticSearch 7.5.2server for text matching. All triples are importedinto an SPARQL endpoint of Virtuoso 07.20.3237for retrieval. Triad requires four hyperparameters:the number of examples G-Agent uses for subtask",
  "Triad-GPT40.5610.5680.5640.4080.4250.4160.6900.6640.677": ": The performance of our proposed Triad on three benchmarks, comparing with traditional KBQA systems(full-shot) and pure LLM (few-shot) baselines. The optimal and suboptimal scores are highlighted with bold andunderlined text, respectively. learning, the number of candidates D-Agent selectsfor entity and relation linking, and the retry timesfor handling non-response SPARQLs. The opti-mal values for these parameters are 3, 2, 2, and3, respectively. The framework and its variantsare tested five times on each benchmark, with theaverage scores reported as the final results. Fortraditional systems, we report the results recordedin their papers. For pure LLM baselines, we writeprompts to hire an LLM to answer questions di-rectly referring to examples, and then link the men-tions from the responses to the URIs in our indexedknowledge bases via built-in similarity search.",
  "Performance Comparison": "The performance of Triad compared to traditionalKBQA systems and pure LLM generation meth-ods is shown in . Evaluation metrics preci-sion(P), recall(R), and F1-score(F1) are reported.We can observe from the experimental results that: Few-shot can be competitive with full-shot.Our multi-role LLM-based agent framework,though executing a few-shot prompt learning, ex-hibits competitive performance with cutting-edgefull-shot KBQA systems.",
  "outperforms GPT-3.5 on all benchmarks, demon-strating the importance of the underlying capabili-ties of an agent": "Explicit knowledge is necessary.Pure LLMmodels with GPT-3.5 and GPT-4 display deficien-cies in generating accurate responses without anauxiliary knowledge base as a memory for interme-diary steps such as URI linking. Performance varies with complexity.Triaddemonstrates superior results on the LC-QuADand YAGO-QA benchmarks compared to QALD-9,due to an increasing failure in response to complexquestions, which will be discussed later.",
  "Study on Capabilities of Agent Roles": "We assess the efficacy of G-Agent with variousother language models as the core. The frameworkwithout G-task uses the text-davinci-002, which isnot as powerful as GPT-3.5 and GPT-4 in solvingmany tasks, and the one without G-chat uses text-davinci-003 to eliminate the chat and alignmentabilities. We test the ability of D-Agent withoutD-uri and D-query by replacing the URI selectionand query selection with URI matching and querygeneration, respectively. We evaluate the contribu-tion of A-Agent eliminating A-llm and A-fact byresponding to questions without using LLMs assis-tance or use an LLM to answer Boolean questions for auxiliary rather than single-fact questions. TheF1 results of the role ablation experiments on tworepresentative datasets are shown in . Theresults indicate that every component pertaining toeach role contributes to the overall performance.More specifically, a G-Agent that employs a lesspowerful LLM as its core can drastically undermineperformance. D-Agent assumes a more pivotal roleduring the linking phase compared to the queryconstruction phase. A-Agent, on the other hand,proves to be an efficient solution for managing sit-uations where SPARQL results are absent.",
  "Analysis of Role Hyperparameters": "We concentrate on three hyperparameters ofroles, including the number of examples (N {1, 2, 3}) provided for G-Agent to learn sub-tasks, the number of URI candidates (K{(1, 1), (1, 2), (2, 2), (2, 3)}) selected by D-Agentfor query construction, and the number of retrytimes (T {1, 2, 3}) launched by A-Agent whenthere is no response. presents the F1 resultsof Triads performance, employing three hyperpa-rameters on two benchmarks. We discover that:",
  "Analysis of Linking Recall": "The process of linking is a relatively complex sub-task in both the Text2SQL and the KBQA process(Li et al., 2024). Calculating the recall ratio ofaccurate URIs using D-Agent provides clarity onwhich step most adversely impacts performance.In the entity linking phase, considering all URIsof entities in the testing set as the ground truth ofthe linking results, 80. 75% of the correct URIs arecontained from the output of the entity matchingfilter in D-Agent and 70. 50% of the correct URIsare retained from the entity selection performedby the LLM in D-Agent. Whereas, in the relationlinking phase, only 52. 54% of the correct relationURIs survive from the selection of LLM, whichindicates a greater difficulty in relation linking.",
  "templates such as the example: Which frequentflyer program has the most airlines?": "Unexploited Semanticsindicates that semanticsof an implicit entity should be comprehended inorder to exclude irrelevant URIs. In the exampleGive me all Argentine films, the meaning of filmsshould be used to narrow down the scope of poten-tial entities in order to eliminate unrelated answers. Implicit Reasoningpresents a challenge that re-quires a deeper level of traversal by the frameworkto deduce accurate results from the posed question.For example, another failure question, How manygrand-children did Jacques Cousteau have?, theterm grand-children must be interpreted to son ofson to ensure an accurate response.",
  "Cost Comparison and Analysis": "According to our evaluation on the three datasets,the average cost of running a single case is 0.007USD on average using Triad-GPT3.5 and 0.05 USDon average using Triad-GPT4. Specifically, mostAPI calls occur in the phases of URL linking andcomprehensive answering. Meanwhile, traditionalKBQA baselines require a lot of training data andlocal training resources to achieve the SOTA perfor-mance, whereas Triad follows a zero- or few-shotmanner to save computational cost locally. Further-more, as shown in .4, in practice, adjust-ing the hyperparameters can make the cost as lowas possible while preserving overall performance.As the cost of LLM services decreases, the valueof Triad will increase accordingly.",
  "SPARQL-based and LLM-based KBQA": "Traditional KBQA methods transform natural lan-guage queries into SPARQL requests for data ex-traction. Specific models are employed either forquestion understanding or URI linking, utilizingdomain-based training datasets. Hu et al. (2018)introduces a semantic query graph to structurallyrepresent the natural language query, thereby sim-plifying the task into a subgraph matching prob-lem. Hu et al. (2021) proposes an entity descrip-tion graph to represent natural language queries forquestion parsing and element linking. Omar et al.(2023a) restructures the question parsing task as atext generation issue using a sequence-to-sequencemodel. With the advent of LLMs, certain phasesof KBQA can be enhanced with LLM-integratedmethods. Baek et al. (2023) aims to augment LLM-based QA tasks with pertinent facts extracted fromknowledge bases, offering a fully zero-shot archi-tecture. Tan et al. (2023a) leverages the general ap-plicability of LLMs to filter linking candidates bymaking selections via few-shot in-context learning.Omar et al. (2023b) provides a thorough compari-son between LLMs and QA systems, recommend-ing further studies to improve KBQA with LLMcapabilities. However, apart from the above studies,our study proposes a complete framework incorpo-rating both an LLM and few-shot learning acrossall KBQA phases from a systematic perspective.",
  "LLM-based Agents for Complex Tasks": "LLMs have recently gained significant attentiondue to their ability to approximate human-level in-telligence. This has led to numerous studies focus-ing on LLM-based agents. A recent survey(Wanget al., 2023) proposes a unified architecture forLLM-based agents, which consists of four mod-ules that include profile, memory, plan, and ac-tion. CHATDB(Hu et al., 2023a) employs an LLMcontroller to generate SQL instructions, which al-lows for symbolic memory and complex multi-hopreasoning. ART(Paranjape et al., 2023) uses afrozen LLM to generate reasoning steps and fur-ther integrates tools for new tasks with minimal hu-man intervention. Toolformer(Schick et al., 2024)takes a different approach by training an LLM toplan and execute tools for the next token predic-tion by learning API calls generation. ReAct(Yaoet al., 2023) focuses on overcoming LLM hallucina-tion by interacting with external knowledge bases, thus generating interpretable task-solving strate-gies. CodeAgent(Tang et al., 2024) designs a multi-agent collaboration system across four phases ina code review process. Divergent from the afore-mentioned studies, our framework concentrates onthe solving KBQA tasks by introducing a multi-role LLM-based agent that specializes in varioussubtasks distributed across different phases.",
  "Conclusion": "In this study, we aim to bridge the gap betweenKBQA tasks and the investigation of LLM-basedagents. We introduce Triad, a framework to ad-dress the KBQA task through an LLM-based agentacting as multiple roles, including a generalist ca-pable of mastering diverse tasks given minimal ex-amples, a decision-maker concentrating on optionanalysis and candidate selection, and an advisorskilled in answering questions with the aid of bothexternal and internal knowledge. Triad achievesthe best or competitive performance across threebenchmark datasets compared to traditional KBQAsystems and pure LLM models. In future research,we plan to broaden our framework to handle moreintricate questions, such as multi-hop reasoning,and exploring the integration between our frame-work and retrieval-augmented generation.",
  "Limitations": "The limitation of our research lies in followingaspects: (1) In terms of data, a broader range ofQA datasets needs to be evaluated, encompassingdatasets from different domains, languages, anddifficulty levels. (2) In terms of model, more LLMsneed to be evaluated, including open-source andcommercial models from different organizationsand on various scales. (3) In terms of framework,more types of agent collaboration methods can beexplored to solve KBQA problems.",
  "Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, JunboZhao, and Hang Zhao. 2023a. Chatdb: Augmentingllms with databases as their symbolic memory. arXivpreprint arXiv:2306.03901": "Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen,Jeff Z Pan, and Zafar Ali. 2023b. An empirical studyof pre-trained language models in simple knowledgegraph question answering. World Wide Web, pages132. Sen Hu, Lei Zou, Jeffrey Xu Yu, Haixun Wang, andDongyan Zhao. 2018. Answering natural languagequestions by subgraph matching over knowledgegraphs. IEEE Transactions on Knowledge and DataEngineering, page 824837. Xixin Hu, Yiheng Shu, Xiang Huang, and Yuzhong Qu.2021. Edg-based question decomposition for com-plex question answering over knowledge bases. InThe Semantic Web ISWC 2021: 20th InternationalSemantic Web Conference, ISWC 2021, VirtualEvent, October 2428, 2021, Proceedings, page128145, Berlin, Heidelberg. Springer-Verlag.",
  "Reham Omar, Ishika Dhall, Panos Kalnis, and EssamMansour. 2023a. A universal question-answeringplatform for knowledge graphs. Proceedings of theACM on Management of Data, 1(1):125": "Reham Omar, Omij Mangukiya, Panos Kalnis, and Es-sam Mansour. 2023b.Chatgpt versus traditionalquestion answering for knowledge graphs: Currentstatus and future directions towards knowledge graphchatbots. arXiv preprint arXiv:2302.06466. Bhargavi Paranjape, Scott Lundberg, Sameer Singh,Hannaneh Hajishirzi, Luke Zettlemoyer, and Mar-coTulio Ribeiro. 2023. Art: Automatic multi-stepreasoning and tool-use for large language models.arXiv preprint arXiv:2303.09014.",
  "Thomas Pellissier Tanon, Gerhard Weikum, and FabianSuchanek. 2020. Yago 4: A reason-able knowledgebase. In The Semantic Web, pages 583596, Cham.Springer International Publishing": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom.2024. Toolformer: Language models can teach them-selves to use tools. Advances in Neural InformationProcessing Systems, 36. Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, WenliangChen, Zhefeng Wang, Baoxing Huai, and Min Zhang.2023a. Make a choice! knowledge base questionanswering with in-context learning. arXiv preprintarXiv:2305.13972. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. 2023b. Can chatgptreplace traditional kbqa models? an in-depth analysisof the question answering performance of the gpt llmfamily. In International Semantic Web Conference,pages 348367. Springer.",
  "arXiv:2303.07992": "Daniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song,Haoye Tian, Saad Ezzini, Yongfeng Huang, andJacques Klein Tegawende F Bissyande. 2024. Col-laborative agents for software engineering. arXivpreprint arXiv:2402.02172. Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey,and Jens Lehmann. 2017. Lc-quad: A corpus forcomplex question answering over knowledge graphs.In The Semantic Web ISWC 2017, pages 210218,Cham. Springer International Publishing.",
  "AResponse Time Analysis": "We analyze various QA frameworks in responsetime to a question. The average latency of eachphase including question parsing (QP), URI link-ing (UL), and answer generation (AG) for eachknowledge base is reported. We randomly select10 samples from each dataset for evaluation to ob-tain the average response times for Triad-1 andTriad-3, which represent retrying three times andgenerating an answer in one go, respectively, dur-ing the answer generation phase. The compari-son between traditional QA systems and Triad isshown in . Triad generally shows a compet-itive time-consuming performance to latest tradi-tional QA systems. Specifically, compared to otherphases, URL linking consumes more time due tothe need to invoke LLM multiple times. Moreover,according to .4, with smaller retry timesof A-Agent, Triad can significantly reduce timecost while only causing slight performance degra-dation, revealing the advantages of our frameworkin balancing performance and efficiency.",
  "BRole Performance on YAGO-QA": "We choose LC-QuAD 1.0 and QALD-9 as our tworepresentative datasets in .3, as the ques-tions among them vary in difficulty, and the tasks inthese two datasets are relatively more challengingthan YAGO-QA. We provide the performance ofagent roles on YAGO-QA in , which showsa consistent result with other datasets in .",
  "You are an assistant to identify triples within aprovided sentence. Please adhere to the followingguidelines:": "1.Triples should be structured in the format<entity1, relation, entity2>.2. The sentence must contain at least one triple, soyou should provide at least one.3. Entities should represent the smallest semanticunits and should not contain descriptive details.4. Entities can take the form of explicit or implicitreferences. Explicit entities refer to specific namedresources, whereas implicit entities are less certain.5. When an entity is implicit, utilize a variableformat such as ?variable to denote it, for example,?location or ?person. Here are some examples:Which citys founder is John Forbes? : <?city,foundeer, John Forbes>How many races have the horses bred by JacquesVant Hart participated in? : <?horse, participatedin, ?race> <?horse, breeder, Jacques Vant Hart>Is camel of the chordate phylum?: <camel,phylum, chordate>",
  "You are an assistant to generate a SPARQLquery to address a specific question. Here are theguidelines to follow:": "1.Ensure that the resulting SPARQL query isdesigned to answer the provided question.2. Adhere to the commonly accepted SPARQLstandards when generating the query.3.Make an effort to leverage the informationprovided to assist in the creation of the SPARQLquery.4. Strive to keep the generated SPARQL query asstraightforward as possible.5. Avoid including PREFIX or : in the SPARQLquery.6. Enclose condition entities and predicates withinangle brackets, such as <entity> or <predicate>.7. Maintain the original order of the given tripleswithout altering their sequence.",
  "You are an assistant to select <K> URIs from aprovided list of possible URIs for a specified entity,following these guidelines:": "1. Identify the <K> most appropriate URIs fromthe given list that best represent the entity inquestion.2. Seek to understand the semantic informationassociated with the specified entity by examiningthe provided question.3. The output should consist of <K> URIs chosenfrom the provided list of possible URIs.4. Simply output these <K> target URIs, each ona separate line, without providing any additionalexplanations.",
  "You are an assistant tasked with selecting the <K>relation URIs between entities mentioned in asentence. Here are the guidelines:": "1. The two entities are listed one after the other,without a specific order.2.Use the provided sentence to discern thesemantic meaning of these entities.3. The potential relation URIs are listed one byone.4. Your output should consist of a maximum of<K> possible relation URIs, although you mayalso output fewer if appropriate.5. Ensure that your output is organized, prioritizingthe most likely relationship first.6. Provide a list of no more than <K> relationURIs (each on a separate line if there are multiple)without any additional descriptions."
}