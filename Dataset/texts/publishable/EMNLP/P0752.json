{
  "Abstract": "Prompt-based fine-tuning has become an es-sential method for eliciting information en-coded in pre-trained language models for a va-riety of tasks, including text classification. Formulti-class classification tasks, prompt-basedfine-tuning under low-resource scenarios hasresulted in performance levels comparable tothose of fully fine-tuning methods. Previousstudies have used crafted prompt templatesand verbalizers, mapping from the label termsspace to the class space, to solve the classi-fication problem as a masked language mod-eling task. However, cross-domain and fine-grained prompt-based fine-tuning with an au-tomatically enriched verbalizer remains unex-plored, mainly due to the difficulty and costsof manually selecting domain label terms forthe verbalizer, which requires humans with do-main expertise. To address this challenge, weintroduce SCIPROMPT, a framework designedto automatically retrieve scientific topic-relatedterms for low-resource text classification tasks.To this end, we select semantically correlatedand domain-specific label terms within the con-text of scientific literature for verbalizer aug-mentation. Furthermore, we propose a new ver-balization strategy that uses correlation scoresas additional weights to enhance the predic-tion performance of the language model duringmodel tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methodson scientific text classification tasks under fewand zero-shot settings, especially in classifyingfine-grained and emerging scientific topics1.",
  "Our code is available at": "Schick and Schtze, 2020; Liu et al., 2023a), trans-ferring the text classification problem as a maskedlanguage modeling task. Masked Language Mod-els (MLMs) are developed by extensively trainingon large text corpora with a percentage of the inputtokens being randomly replaced with a [MASK]token. Traditional fine-tuning, which requires addi-tional training on labeled domain- or task-specificdata (Ovadia et al., 2023), may not be suitable inlimited data scenarios, such as few and zero-shotsettings. Prompt-based fine-tuning has emergedas an effective alternative. This approach uses aprompt to guide the MLM in generating a specifictoken through masking a [MASK] token in theprompt template, addressing the text classificationtasks (Schick and Schtze, 2020; Hu et al., 2021;Chen et al., 2022b; Gao et al., 2021a) under low-resource conditions (i.e., few and zero-shot set-tings) through a verbalizer. As defined by Schickand Schtze (2020), the verbalizer refers to themapping from label words (e.g., cryptanalysis)to the corresponding class (e.g., Cryptography),serving as a projection function between the vo-cabulary and the class label space. However, inthe context of classifying scientific literature, thecomplexity of scientific language and scarcity offine-grained (i.e., a wide range of scientific fieldsthat are labeled with sub-categories) or emergingtopics make it hard to automatically classify cross-domain scholarly articles with limited training sam-ples and manually created verbalizers (Schick andSchtze, 2020).The goal of this paper is to address the chal-lenge of multi-class classification in low-resourcesettings, specifically focusing on classifying sci-entific abstracts into different domains with onlya limited number of labeled examples. We intro-duce a prompt-based fine-tuning approach enrichedwith domain knowledge as a new strategy for re-trieving domain-adaptive label terms (i.e., scien-tific terms in various fields) without manual in- : Overall framework of SCIPROMPT. The left side shows the overall process of masked language modelingfor performing the text classification task. The right side shows our proposed knowledge retrieval and domain-adaptive filtering phase (3). The prediction results, such as CR and SE, correspond to the class labels forCryptography and Software Engineering, respectively, and are used for scientific knowledge retrieval. tervention.We enhance our approach for low-resource scenarios by retrieving scientific phrasesfrom external knowledge bases (KBs) to expandlabel terms of the verbalizer (Hu et al., 2021) fromthe token-level to term phrases. We fine-tune Nat-ural Language Inference (NLI) models for seman-tic similarity search between retrieved label termsand class labels to select domain-related scientificphrases. Our method differs from previous studies(Hu et al., 2021; Ding et al., 2022b), which rely onword frequency filtering and are limited to single-token verbalizer projection for text classification.Given the complexity of scientific terminology (seeAppendix B for more details), we refine the tradi-tional verbalization approach (Ding et al., 2022a)by integrating scientific terms through deploying aweight-aware label term mapping function. This ap-proach improves the projection performance fromMLMs predictions to probabilities of a specificclass compared with prior studies (Hu et al., 2021;Gao et al., 2021b; Chen et al., 2022a).Our approach consists of three stages: 1) re-trieval of scientific terms, 2) label term filtering,and 3) prediction of scientific topics.Initially,we use a cloze-style prompt and an input scien-tific abstract to guide the MLM to generate la- bel words to fill the [MASK] token ().Then, we use each class label as a query to retrieveclass-related domain phrases (also denote as labelterms) from external KBs. To filter the potentiallyirrelevant terms gathered in the retrieval stage, wefine-tune both bi-encoder and cross-encoder mod-els using the SciNLI dataset (Sadat and Caragea,2022), enabling the selection of the most relevantdomain phrases. Finally, with the selected sets ofknowledge-enriched scientific terms, we incorpo-rate these label terms into the verbalizer to convertthe MLMs prediction into a specific class througha semantic score-weighted average loss, enhanc-ing the precision of the probability projections forthe augmented verbalizer. Our method extends be-yond token-to-token verbalization by encompass-ing token-to-phrase verbalization that enriches thesemantic meaning of scientific domain vocabulary.This broader scope allows for an advanced interpre-tation of scientific language and classifying emerg-ing topics under weak supervision.In summary, our contributions are the presenta-tion of:",
  "Knowledge-Powered Prompting for TextClassification": "A Pattern-Exploiting Training (PET) framework(Schick and Schtze, 2021a,b), which initially in-vestigated how cloze-based prompt templates canguide language models to tackle classification tasks(Han et al., 2022; Ding et al., 2022b; Min et al.,2022; Wang et al., 2022a; Zhang et al., 2022; Wanget al., 2022b), has inspired research on incorpora-tion more diverse label words into the verbalizer.Specifically, Hu et al. (2021) added external knowl-edge to the verbalizing process to help an MLM pre-dict masked tokens more accurately. AdaPrompt(Chen et al., 2022b) applied a different knowledgeinjection method that leveraged task and promptcharacteristics to retrieve external knowledge forcontinuous pre-training of MLMs adaptively. How-ever, classifying scientific literature presents chal-lenges that previous methods have not addressed,including projecting phrase-level label terms in theverbalization process. Other challenges, to whicha broad range of solutions have been developed,include handling complex semantic structures in awide range of scientific topics (Eykens et al., 2021;Khadhraoui et al., 2022) and the scarcity or im-balance of labeled data across multiple disciplines(Cunha et al., 2021).",
  "Label Terms Refinement": "Prior research on prompt-based fine-tuning hasused the verbalizer module to map MLMs pre-dictions to specific classes. Schick and Schtze(2021a) introduced an automatic verbalizer searchthat identifies suitable label words from trainingdata and language models to enrich the verbalizer. This approach has been further explored in differentstudies to improve the classification performance(Gao et al., 2021a; Shin et al., 2020; Liu et al.,2023b), although these methods typically need ex-tensive training data, making them less suitablefor low-resource scenarios. To address these chal-lenges, one can manually expanding the verbalizerwith more label words (Shin et al., 2020), whichhas limitations when classifying fine-grained anddomain-related categories that need expert knowl-edge. Recently, external KBs have been used toenrich the verbalizer by sourcing class-related labelwords (Hu et al., 2021; Chen et al., 2022b).",
  "Cloze-Style Masked Language Modeling": "MLMs M (e.g., SciBERT (Beltagy et al., 2019))are created by randomly masking tokens in thetraining text and training the model to predictthe masked tokens. Similarly, prompt-based fine-tuning typically leverages a cloze- or prefix-basedprompt template, reformulating the input into amasked language modeling task. This strategy en-ables M to predict the masked token, facilitatingthe execution of downstream tasks based on Moutputs. Building upon Hu et al. (2021), our frame-work employs a few-shot prompt-based fine-tuningstrategy that conceptualizes scientific text classifi-cation as an N-way K-shot task, where N indicatesthe number of classes and K is the number of la-beled examples per class.We provide a limited number of labeled exam-ples for each class to tune M. We construct atraining Dtrain and validation set Dval followingprevious studies (Gao et al., 2021a; Perez et al.,2021; Wang et al., 2022a; Hu et al., 2021) withn examples per class. For the few-shot setting,given a cloze-based prompt template Pt and an in-put abstract an, where an Dtrain, M predictsthe label word l to fill into the [MASK] positionin the prompt template. After that, the verbalizerfunction fv maps the predicted label word l ontopre-defined label term set L to classify it into aclass, i.e., L Y. We use a cross-entropy loss(Gao et al., 2021a) to update the parameters of Mthrough verbalization outputs. For instance, the prompt is designed as [Abstract]. The fieldof this article is related to: [MASK]. M will pre-dict suitable label word l to fill into the [MASK].Then, fv calculates the probability of classifying linto a topic yi, where yi Y:",
  "Scientific Knowledge Retrieval": "Predicting masked tokens using an MLM involvesgenerating a range of potential label words, eachwith varying probabilities of matching a specificclass. Enhancing the verbalizer with a more ex-tensive set of label terms has been proven to im-prove the accuracy of word-to-class mapping (Huet al., 2021; Chen et al., 2022b; Wang et al., 2022a;Shin et al., 2020). To implement this approach,we use two external KBs, Related Words2 andReverse Dictionary3 for scientific knowl-edge retrieval. Related Words identifies rel-evant terms using vector similarity and resourceslike word embeddings and ConceptNet. ReverseDictionary, which acts as a word search en-gine, finds terms based on definitions or phrases.Reverse Dictionary is particularly usefulin phrase-level retrieval, where straightforwardlabels from Related Words may not sufficegiven a domain-specific phrase (e.g., Networkingand Internet Architecture). We set class labelsC = {y1, y2, ..., yn} as queries to retrieve fromRelated Words GRW .When GRW fails to produce terms with sim-ilarity scores above zero, we use ReverseDictionary, denoted as GRD, for additionalphrase retrieval. Each retrieved term is assigneda single relevance score. Initially, we adopted thesame threshold (i.e., threshold = 0) as KPT (Huet al., 2021) for term retrieval based on topic names.Subsequently, we impose two additional thresh-olds for further selection of retrieved terms (3.3).Utilizing these KBs enables the compilation ofknowledge-enhanced term sets Ti = t1, t2, ..., tmfor each dataset, where i n and t represents the",
  "Domain Adaptive Model Tuning": "To effectively identify the most relevant label wordsfor each class from a set of initial raw terms, it iscrucial to use a model tailored or adaptable to spe-cific fields. Drawing from Chen et al. (2022b), whoemployed a pre-trained NLI model to filter labelwords produced by an MLM, we present a methodthat enhances the accuracy of selecting label termsrelated to specific topics by integrating domainknowledge. We apply a newly introduced scientificNLI dataset DSciNLI (Sadat and Caragea, 2022),consisting of labeled sentence pairs (si, sj) fromscholarly articles in the fields of NLP and compu-tational linguistics. This dataset serves to fine-tuneboth cross-encoder Mce and bi-encoder Mbe NLImodels4, where Mbe produces for a given sentencea sentence embedding and Mce passes a sentencepair to the encoder to produce an output value be-tween 0 and 1 indicating the similarity of the inputsentence pair (Reimers and Gurevych, 2019). Thetraining labels are defined as Entailment or Con-tradiction, thus framing the model fine-tuning asa binary classification task:",
  "Semantic Knowledge Filtering": "We merge each retrieved scientific label term with astandard prompt (see Appendix G), encode promptsusing the fine-tuned Mbe, and use these encodedembeddings as queries for sentence-level semanticsearches to select topic-related label terms and cal-culate semantic similarity scores wl for each labelterm. We apply SentenceTransformers5 to conductthe cosine similarity search using Mbe within eachretrieved label term set. Then, we use Mce to re-rank these label terms for every prompt pair of eachtopic, selecting relevant sentences based on prede-fined thresholds (be = 0.5, ce = 0.1). As thesescores also help predict label words, we apply thismethod in few and zero-shot scenarios (for moredetails, see Appendix F).Following KPT (Hu et al., 2021), we also apply alabel term calibration approach with a full training set to directly remove irrelevant label terms in theverbalizer that are less likely to be predicted by M.The retrieved label terms for each class with lowerprobabilities (i.e., less than 0.5) produced by Mare removed. The probability of t is:",
  "Weighted Verbalizer Transformation": "Given that retrieved label terms may be tokenizedinto multiple tokens, we adopt a mean methodto average the tokens of a label term (Ding et al.,2022b), considering all parts of a term as signifi-cant.Adopting the verbalizer structure from Ding et al. (2022b), we introduce a verbalization approach thatmaps Ms output to specific classes yi, using prede-fined semantic scores wl as weights for each labelterm. This method aims to enhance the accuracy ofclassifying Ms predictions l into topic yi:",
  "Vector-Based Verbalizer Mapping": "Incorporating the filtered label terms into the ver-balizer is crucial for making accurate predictionsand eliminating noise simultaneously.Movingbeyond simple summing (Wang et al., 2022a) orweighted averaging (Hu et al., 2021) of labelwords, the Word-level Adversarial ReProgramming(WARP) model introduced in (Hambardzumyanet al., 2021) uses vector representations for classmapping, which is distinct from conventional sin-gle word projection. We introduce a new methodnamed SCIPROMPTSoft based on the uniquenessof our phrase-level verbalizer. Specifically, we re-fine the verbalization in SCIPROMPTSoft by draw-ing from the soft verbalizer concept introduced by WARP. In the experiments with SCIPROMPTSoft,we aggregate all retrieved label terms per topic withsemantic scores into a vector for topic probabilityprediction and optimize the aggregated vector dur-ing model tuning (detailed in Appendix A).",
  "Datasets": "We use three publicly available datasets in En-glish for our experiments: SDPRA 2021 (Reddyand Saini, 2021), arXiv (Meng et al., 2019), andS2ORC (Lo et al., 2020). SDPRA 2021 containsscientific articles from computer science acrossseven categories. arXiv (Meng et al., 2019) in-cludes abstracts sourced from the arXiv website6 across 53 sub-categories, and S2ORC contains aca-demic papers from across 19 disciplines. For theS2ORC data, we only select abstracts with a sin-gle discipline label through the Semantic ScholarPublic API7. The statistics and category examplesof these datasets are shown in and Ap-pendix B.",
  "Experimental Settings": "SCIPROMPT is built upon the OpenPrompt frame-work (Ding et al., 2022b). We apply a consistentprompt template across all experiments (see Ap-pendix G for more details). The experimental de-tails are shown in Appendix A.Inthefew-shotsetting,webenchmarkSCIPROMPTalongsidestandardfine-tuning,simplified prompt-tuning (PT), and previousstate-of-the-art text classification models, includ-ing LM-BFF (Gao et al., 2021b), RetroPrompt(Chen et al., 2022a), and KPT (Hu et al., 2021).Standard fine-tuning takes all labeled trainingexamples as input to tuning an MLM for textclassification.We take the final representationof the [CLS] token as the output vector of theabstract (Cohan et al., 2020). Standard PT with amanually defined verbalizer (Ding et al., 2022b)only takes each lowercase topic name as a seedword for verbalization. We apply the same settingas in SCIPROMPT, including a unified prompttemplate, MLM, and the models hyper-parameters.",
  "Full SetFine-tuning (Full) *90.7154.5853.7466.34": ": Experimental results under few-shot settings. We report the mean accuracy (expressed in percentages %)and standard deviation based on five iterations across five learning shots. Fine-tuning (Full)* represents using afully labeled training set. RetroPrompt experiments are only conducted in settings above five shots, as this methodrequires at least two labeled examples for model tuning. KPT (Hu et al., 2021) applied external knowledgeto enrich the verbalizer with additional wordrelevance and frequency filtering strategies. Ourexperiments use the same MLM (i.e., SciBERT)for equal comparison.Besides, training andvalidation examples per class (Ding et al., 2022b;Hu et al., 2021; Wang et al., 2022a) are uniformduring model tuning, conducting tests with 1, 5, 10,20, and 50 shots across all datasets and reportingaccuracy as an evaluation metric. We evaluatemodel performance across five random seeds toaccount for variability (Hu et al., 2021; Ding et al.,2022b). For the zero-shot setting, we sample approxi-mately 10% of each dataset for testing, ensuringadequate representation for each topic. For broadermodel comparison, we introduce two additionalmodels specific to the zero-shot scenario: SimPTC(Fei et al., 2022) and NPPrompt (Zhao et al., 2023).Moreover, we extend our evaluation to includeLlama 2 (Touvron et al., 2023), ChatGPT (OpenAI,2024), and the latest Llama 3 (AI@Meta, 2024)using in-context learning for a broader range ofcomparisons. Random seeds are applied in KPT,which samples an unlabeled support set of 200 ex-amples to calibrate label words. : Performance comparison of few-shot methods over three datasets in . We report the mean accuracyof each setting. Our method shows high stability in the accuracy distribution compared to the considered baselinemodels.",
  "Main Results": "We highlight the performance of SCIPROMPTagainst baseline models across our three considereddatasets in both few-shot and zero-shot settings,focusing on the fine-grained and cross-domain sci-entific text classification tasks. The experimentalshown are listed in . Results are averagedover five runs as the same as KPT (Hu et al., 2021)to counteract sampling randomness, reported asmean accuracy with standard deviation.Few-shot Results.SCIPROMPT achievesthe best average accuracy on all three datasetsfor all settings.Specifically, SCIPROMPT andSCIPROMPTSoft excel in low-data scenarios (e.g.,one-shot and five-shot), particularly on arXiv andS2ORC, often outperforming baseline models.SCIPROMPT also outperforms KPT by 8.93% inthe one-shot setting and 2.83% in the five-shot set-ting. As the number of training examples increases,the margin of improvement over baseline modelsnarrows. Notably, SCIPROMPT exceeds the full-set fine-tuning by an average of 0.57%, 3.67%,and 5.51% with 10, 20, and 50 shots, respectively.Despite variability in performance improvementsacross different training sizes, our method consis-tently achieves the highest accuracy on arXiv andS2ORC across all configurations. Also, the stan-dard deviation of all three datasets decreases as thenumber of input training examples increases acrossall three datasets.Additionally, provides a comprehensivecomparison of performances across all few-shot set-tings, ranging from one-shot to fifty-shot, for eachdataset as outlined in . SCIPROMPT con-sistently delivers high and stable accuracy acrossall three datasets compared to the baseline mod- els. Particularly on S2ORC, SCIPROMPT achievesa higher median accuracy and a narrower in-terquartile range, indicating more consistent per-formance across different few-shot scenarios. TheSCIPROMPTSoft method shows high stability on theSDPRA 2021 dataset, while SCIPROMPT is moreeffective in fine-grained datasets.",
  ": Performance of zero-shot setting. Only KPTis reported through mean accuracy (%) and standarddeviation (4.2). We apply the same instruction forChatGPT, Llama 2, and Llama 3 on the test sets": "Zero-shot Results.Shown in , theLlama 3 70B model leads in performance across alldatasets. Nonetheless, SCIPROMPT outperformsother baseline models, especially on arXiv andS2ORC, where it outperforms PT and KPT by mar-gins of 1.47% and 2.88%, respectively. Meanwhile,LM-BFF leads among all baseline models on theSDPRA 2021 dataset. These results underscore theeffectiveness of SCIPROMPT in leveraging domain-specific knowledge for fine-grained scientific textclassification, even in the absence of labeled train-ing data.Llama 3s average accuracy exceedsSCIPROMPT by 23.35% and Llama 2s by 18.76%.However, on the S2ORC dataset, SCIPROMPT sur-passes Llama 2. Note that SCIPROMPTSoft is notdesigned for zero-shot testing since it needs train-able tokens in the decoding layer during modeltuning.",
  "Emerging Topics Classification": "To assess our methods effectiveness in classify-ing emerging scientific topics, we manually collecta dataset centered around recent developments inthe field of NLP, drawing inspiration from Ahmadet al. (2024). Specifically, we first extract NLPtopics from Taxonomy4CL8, focusing on topicsthat have emerged since 2000, as identified throughSemantic Scholar9. We then select scientific ar- ticles published after 2019 that are beyond theknowledge cutoff of the SciBERT model. For eachselected topic, we gather 30 abstracts, applyingthe same random seeds for few-shot experimentsas those introduced in . We create a newdataset named Emerging NLP by collecting 21fine-grained NLP-related topics and their corre-sponding abstracts. Appendix B provides detaileddataset statistics and topic examples. com-pares the performance of various baseline models.Notably, SCIPROMPT exceeds the performance ofthe Llama 2 70B model by 31.91% and outperformsthe PT method by 6.67% in the zero-shot setting.Overall, our method outperforms all state-of-the-artmethods in classifying emerging scientific topics,especially in the zero-shot setting, highlighting ourmethods efficacy in highly low-resource scenarios.",
  "Ablation Study": "Our ablation study on the arXiv dataset ()demonstrates the advantages of our models overKPT, with a 1.45% increase in zero-shot accuracy.SCIPROMPT and SCIPROMPTSoft outperform KPTby 2.38% and 3.42%, respectively, in terms of av-erage accuracy under the few-shot setting. Weexamine the impact of removing full-size calibra-tion (w/o CL), semantic scores (w/o SS), andboth (w/o SS+CL), finding that both componentsimprove the performance, especially in the zero-shot setting where their absence lowers accuracyby 0.41% (w/o CL) and 16.11% (w/o SS) com-pared to SCIPROMPT, underlining the critical roleof SS in bolstering the models effectiveness.Interestingly, SCIPROMPTSoft performs betterwithout SS than when both components are in-cluded. Removing both SS and CL yields the best 1-shot performance, suggesting that less intervention",
  ": The usage percentage of GPU memory duringmodel tuning": "optimizes model tuning in low-data contexts. Fur-thermore, comparing setups without pre-filteringand calibration (w/o FL+CL) to those with pre-filtering shows an accuracy increase by 3.39% and0.94% for SCIPROMPT and SCIPROMPTSoft respec-tively, highlighting the effectiveness of pre-filteringof augmented verbalizer for text classification. Theablation studies of SDPRA and S2ORC shows thesame pattern as on arXiv.",
  "Model Tuning Efficiency": "shows that SCIPROMPTSoft reduces GPUmemory usage by 16.5 percentage points (p.p.)for SDPRA 2021, 38 p.p. for arXiv, and 46.2p.p. for S2ORC compared to SCIPROMPTs full-size label term calibration. Although SCIPROMPTachieves higher average accuracy rates in thefew-shot setting on the S2ORC dataset (see Ta-ble 6 in Appendix C), SCIPROMPTSoft outperformsSCIPROMPT on SDPRA 2021 and arXiv, suggest-ing that SCIPROMPTSoft can achieve competitiveresults with less GPU usage. Moreover, while Chat-GPT and Llama 2 exhibit superior performance inthe zero-shot setting, as shown in , it isworth noting that these language models are eithermainly for commercial use or require substantialGPU resources, incurring higher costs or more time.For instance, for the S2ORC dataset, our methodnot only cuts down the combined training and test-ing (inference) time by 93 p.p. compared to Llama2 70B but also enhances accuracy by 1 p.p. overLlama 2, highlighting the efficiency and effective-ness of our approach.",
  "Conclusion": "We introduced a knowledge-enhanced, prompt-based fine-tuning framework for fine-grained sci-entific text classification using minimally or no la-beled abstracts. Acknowledging the complexity ofdomain knowledge within scientific literature, we employed a prompt-tuned MLM augmented withdomain knowledge injection and semantic filtering.This approach enables the automatic extraction ofdomain-specific phrases and their integration intoa weighted verbalizer for topic projection. Ourfindings highlight the effectiveness of our methodsover existing state-of-the-art models and standardfull-set fine-tuning, particularly for emerging topicclassification and scenarios requiring high levelsof topic granularity. Notably, SCIPROMPT demon-strates competitive accuracy compared to the ad-vanced Llama 2 70B model in the zero-shot setting,showing its potential to categorize scholarly topicswith a lightweight and efficient approach.",
  "Limitations": "Our studys limitations are as follows: 1) Our ex-ternal knowledge sources are limited to two non-scientific domain databases for retrieving topicwords, potentially missing fine-grained scientificterminologies. Despite the challenge of identify-ing a universally applicable, cross-domain, scien-tific knowledge resource, future efforts should aimto discover more precise terminology databases(Han et al., 2020). 2) We focus solely on a multi-class classification task and exclude abstracts thatspan multiple scientific sub-domains. Advancingtowards a multi-label classification system capableof identifying publications across various domainswould enhance the robustness of our approach.3) Although SCIPROMPT and SCIPROMPTSoft sur-passed baseline methods during evaluation, theenhancements are modest, and results fluctuate,particularly with an increase in labeled trainingdata. Further investigation into the causes of theseminimal gains as well as more comprehensive, in-terpretable experiments are needed to better un-derstand and improve the model performance. 4)We only used classification accuracy and standarddeviation as model evaluation metrics. The experi-mental results can change when using other metrics(e.g., Micro F1 and Macro F1). Additionally, whilethe standard deviation of our methods shrinks asthe number of training examples increases, onecould do statistical significance testing to drawrobust conclusions by comparing system perfor-mance against baseline models.",
  "The datasets and MLM employed in our study arepublicly accessible and extensively utilized in the": "research community. To enhance the quality of ourdata, we applied heuristic filtering to exclude short-length abstracts across these datasets, acknowledg-ing that this process may impact experimental ac-curacy. Our methodology includes extracting datafrom external knowledge bases via public APIs.Furthermore, as we used MLMs as the foundationof our approach, it is essential to note that the pre-dictive behavior of these models can be challengingto regulate due to the implicit knowledge embed-ded within the MLMs, which is difficult to decodeexplicitly. Therefore, caution should be exercisedwhen adapting our method to other tasks, espe-cially in the context of text classification throughprompting. Raia Abu Ahmad, Ekaterina Borisova, and Georg Rehm.2024. Forc4cl: A fine-grained field of research classi-fication and annotated dataset of nlp articles. In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 73897394.",
  "AI@Meta. 2024. Llama 3 model card": "Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-ERT: A pretrained language model for scientific text.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 36153620, Hong Kong, China. Association for Computa-tional Linguistics. Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang,Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, andHuajun Chen. 2022a. Decoupling knowledge frommemorization: Retrieval-augmented prompt learning.Advances in Neural Information Processing Systems,35:2390823922. Yulong Chen, Yang Liu, Li Dong, Shuohang Wang,Chenguang Zhu, Michael Zeng, and Yue Zhang.2022b. AdaPrompt: Adaptive model training forprompt-based NLP. In Findings of the Associationfor Computational Linguistics: EMNLP 2022, pages60576068, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Arman Cohan, Sergey Feldman, Iz Beltagy, DougDowney, and Daniel Weld. 2020.SPECTER:Document-levelrepresentationlearningusingcitation-informed transformers.In Proceedingsof the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 22702282,Online. Association for Computational Linguistics. Washington Cunha, Vtor Mangaravite, ChristianGomes, Srgio Canuto, Elaine Resende, CeciliaNascimento, Felipe Viegas, Celso Frana, Welling-ton Santos Martins, Jussara M Almeida, et al. 2021.On the cost-effectiveness of neural and non-neuralapproaches and representations for text classification:A comprehensive comparative study. InformationProcessing & Management, 58(3):102481. Ning Ding, Yulin Chen, Xu Han, Guangwei Xu, Xi-aobin Wang, Pengjun Xie, Haitao Zheng, ZhiyuanLiu, Juanzi Li, and Hong-Gee Kim. 2022a. Prompt-learning for fine-grained entity typing. In Findingsof the Association for Computational Linguistics:EMNLP 2022, pages 68886901. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,Zhiyuan Liu, Haitao Zheng, and Maosong Sun.2022b. Openprompt: An open-source frameworkfor prompt-learning. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics: System Demonstrations, pages 105113. Joshua Eykens, Raf Guns, and Tim CE Engels. 2021.Fine-grained classification of social science journalarticles using textual data: A comparison of super-vised machine learning approaches. QuantitativeScience Studies, 2(1):89110. Yu Fei, Zhao Meng, Ping Nie, Roger Wattenhofer,and Mrinmaya Sachan. 2022. Beyond prompting:Making pre-trained language models better zero-shotlearners by clustering representations. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 85608579.",
  "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021a": "Making pre-trained language models better few-shotlearners. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 38163830, Online. Association for Computa-tional Linguistics. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021b.Making pre-trained language models better few-shotlearners. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 38163830. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.2022. Ppt: Pre-trained prompt tuning for few-shotlearning. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 84108423. Karen Hambardzumyan,Hrant Khachatrian,andJonathan May. 2021. WARP: Word-level AdversarialReProgramming. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: Long",
  "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, andMaosong Sun. 2022. Ptr: Prompt tuning with rulesfor text classification. AI Open, 3:182192": "Shengding Hu, Ning Ding, Huadong Wang, ZhiyuanLiu, Juanzi Li, and Maosong Sun. 2021. Knowl-edgeable prompt-tuning: Incorporating knowledgeinto prompt verbalizer for text classification. arXivpreprint arXiv:2108.02035. Mayara Khadhraoui, Hatem Bellaaj, Mehdi Ben Ammar,Habib Hamam, and Mohamed Jmaiel. 2022. Surveyof bert-base models for scientific text classification:Covid-19 case study. Applied Sciences, 12(6):2891. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing.ACM Computing Surveys, 55(9):135.",
  "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,Yujie Qian, Zhilin Yang, and Jie Tang. 2023b. Gptunderstands, too. AI Open": "Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-ney, and Daniel S Weld. 2020. S2orc: The semanticscholar open research corpus. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, pages 49694983. Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.2019. Weakly-supervised hierarchical text classifi-cation. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 68266833. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, andLuke Zettlemoyer. 2022. Noisy channel languagemodel prompting for few-shot text classification. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 53165330, Dublin, Ireland. As-sociation for Computational Linguistics.",
  "Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.True few-shot learning with language models. Ad-vances in neural information processing systems,34:1105411070": "Saichethan Miriyala Reddy and Naveen Saini. 2021.Overview and insights from scope detection of thepeer review articles shared tasks 2021. In Pacific-Asia Conference on Knowledge Discovery and DataMining, pages 7378. Springer. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 39823992. Mobashir Sadat and Cornelia Caragea. 2022. Scinli: Acorpus for natural language inference on scientifictext. In Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 73997409.",
  "Timo Schick and Hinrich Schtze. 2020.Exploit-ing cloze questions for few shot text classificationand natural language inference.arXiv preprintarXiv:2001.07676": "Timo Schick and Hinrich Schtze. 2021a. Exploitingcloze-questions for few-shot text classification andnatural language inference. In Proceedings of the16th Conference of the European Chapter of the Asso-ciation for Computational Linguistics: Main Volume,pages 255269, Online. Association for Computa-tional Linguistics. Timo Schick and Hinrich Schtze. 2021b. Its not justsize that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Online. Association for ComputationalLinguistics. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, EricWallace, and Sameer Singh. 2020. AutoPrompt: Elic-iting Knowledge from Language Models with Auto-matically Generated Prompts. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 42224235,Online. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Han Wang, Canwen Xu, and Julian McAuley. 2022a.Automatic multi-label prompting: Simple and inter-pretable few-shot classification. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 54835492. Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan,Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang,and Ming Gao. 2022b. Towards unified prompt tun-ing for few-shot text classification. In Findings of theAssociation for Computational Linguistics: EMNLP2022, pages 524536. Zhiwen You, HaeJin Lee, Shubhanshu Mishra, SullamJeoung, Apratim Mishra, Jinseok Kim, and Jana Dies-ner. 2024a. Beyond binary gender labels: Revealinggender bias in LLMs through gender-neutral namepredictions. In Proceedings of the 5th Workshopon Gender Bias in Natural Language Processing(GeBNLP), pages 255268, Bangkok, Thailand. As-sociation for Computational Linguistics. Zhiwen You, Shruthan Radhakrishna, Shufan Ming, andHalil Kilicoglu. 2024b. UIUC_BioNLP at BioLay-Summ: An extract-then-summarize approach aug-mented with Wikipedia knowledge for biomedicallay summarization. In Proceedings of the 23rd Work-shop on Biomedical Natural Language Processing,pages 132143, Bangkok, Thailand. Association forComputational Linguistics. Haoxing Zhang, Xiaofeng Zhang, Haibo Huang, and LeiYu. 2022. Prompt-based meta-learning for few-shottext classification. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 13421357. Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu,and Lei Li. 2023. Pre-trained language models canbe fully zero-shot learners. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1559015606.",
  "AExperimental Details": "All models use the maximum input length of256 tokens over 5 epochs, using the same hyper-parameters as KPT (Hu et al., 2021), with a learn-ing rate of 3e-5 and a batch size of 5. The experi-ments are performed on a 32 GB Tesla V100 GPU.In few-shot setting, we apply the same back-bone MLM for all experiments, with the exceptionof RetroPrompt (Chen et al., 2022a). RetroPromptonly supports RoBERTa-based models and requiresat least two examples per class for model tuning.Therefore, we applyroberta-base as basemodel for RetroPrompt and only conduct experi-ments with more than five shots.The main distinction between SCIPROMPT andSCIPROMPTSoft lies in the verbalization, as dis-cussed in .6. Unlike SCIPROMPT, whichuses single label term projection, SCIPROMPTSoftemploys a vector-based mapping method to repre-sent each filtered set of label terms.In zero-shot setting, we include ChatGPT10,open-sourced Llama 211, and the latest Llama312forzero-shotclassificationusingthesameinstruction.ForChatGPT,weusegpt-3.5-turbo-instruct, which contains175 million model parameters developed byOpenAI. We apply llama-2-70b-chat andmeta-llama-3-70b-instruct as the back-bone models for Llama 2 and Llama 3 respectivelythrough the Replicate API13. We additionallyinvestigate the classification performance of theLlama 2 models with 7B and 13B parametersunder the zero-shot setting. However, their outputsare not coherent with the predefined class label setsand often include redundant information, making",
  "BDatasets and Examples of DomainTopic Categories": "We present a more detailed introduction to datasetsused for our experiments.SDPRA 2021 contains topics of scientific arti-cles from the field of computer science, consistingof abstracts sourced from arXiv and categorizedunder one of seven predefined domain labels. Wecombined the training and validation sets, reallo-cating them into new training (90%) and validation(10%) sets.arXiv includes abstracts sourced from the arXivwebsite collected by Meng et al. (2019), catego-rized into 53 sub-categories and 3 parent categories(i.e., Math, Physics, and CS). We select 100 sam-ples for each category as test set.S2ORC includes academic papers across 19 dis-ciplines. We filter abstracts to those with a sin-gle discipline label from the 2023-11-07 releasethrough the Semantic Scholar Public API14.Emerging Topics of NLP encompasses 21newly developed research fields within the broadercategory of Computation and Language15. We col-lect 30 examples for each topic, assigning five in-stances for training and another five for validation.The rest of the examples are used for testing.In our experiments, abstracts shorter than 30tokens were excluded to remove invalid abstracts,leading to final training and test sizes of 25,110and 2,790 for SDPRA, 49,300 and 5,300 for arXiv,60,000 and 5,700 for S2ORC, and 210 and 420 forEmerging NLP. We used sub-categories for arXivand parent categories for both SDPRA and S2ORCin text classification tasks. Detailed class labels foreach dataset are presented in . We reportparent and sub-categories of four datasets.",
  "EOverall Model Performance Analysis": "We present an overview comparison of the resultsfrom across all three datasets (i.e., SD-PRA 2021, arXiv, and S2ORC) in . Over-all, SCIPROMPT exhibits the most stable perfor-mance compared to other baseline methods. No-tably, SCIPROMPT consistently outperforms thestate-of-the-art model KPT across all three datasets.In contrast, SCIPROMPTSoft demonstrates variabil-ity and inconsistency compared with SCIPROMPTwhile showing a similar median accuracy. We ex-cluded the RetroPrompt method from this compari-son due to its inability to perform in the one-shotsetting.",
  "FKnowledge-Retrieval ThresholdSelection": "As we introduced in .4, during the labelterm filtering stage, we employ a bi-encoder forMbe and a cross-encoder for Mce calculation. Inour experimentation, a higher Mbe score indicatesa more notable similarity between the topic labelsand the retrieved label terms, thus enhancing therelevance of the selected terms. Conversely, a lowerMce score signifies higher relevance during the re-ranking stage. Our analysis of the SDPRA datasetreveals that Mce scores predominantly clusteredabove 0.9 and below 0.1. Consequently, the me-dian value of Mce exerts minimal influence on thefinal Verbalization process. Even when reducingthe threshold of Mce to 0.5, only a marginal differ-ence in the number of selected label terms acrossvarious Mce scores within the range of 0.1 to 0.9is observed ().We also explored the impact of different Mbevalues under the fixed Mce score (0.1) to assess per-formance variations of SCIPROMPT in the 1-shot setting through the SDPRA dataset. Our findingsindicate that while Mbe > 0.9 yields the optimalperformance, Mbe > 0.5 kept the lowest standarddeviation (). Consequently, we assume set-ting Mbe > 0.5 as the filtering threshold is morestable across different experimental conditions.",
  ": Ablation study of SCIPROMPT in various Mcevalues under the fixed Mbe using the SDPRA 2021dataset": "To further validate our choices, we conducted ex-periments of SCIPROMPT with varying Mce valuesunder the 1-shot setting using the SDPRA datasetwhile maintaining a constant Mbe threshold of 0.5.Notably, performance consistently improved andthe standard deviation is stable when Mce is setbelow 0.1 (). Therefore, we adopted Mce =0.1 as the filtering threshold.",
  "Abstract. The field of this study is re-lated to: [MASK]": "Above is the cloze-based prompt template we ap-plied for all MLM prompt-based fine-tuning tasks.We also explored various prompt templates as in-troduced by (Hu et al., 2021; Gao et al., 2021a; Youet al., 2024b) to evaluate performance variationsusing the SDPRA 2021 dataset, where the resultsare found to be similar. Note that our method fo-cuses on improving domain-related verbalization process rather than creating diverse prompts formodel tuning.As detailed in .1, we used ChatGPT,Llama 2, and Llama 3 to perform the task of scien-tific text classification guided by specific instruc-tions. The same instructions were applied to allLLMs to infer the topics from scientific abstracts.We employed a distinct task-oriented (You et al.,2024a) prompt from that used with MLMs dueto our observation that the original prompt fromSCIPROMPT fails to yield relevant field names,given the LLMs limitations in comprehension.Consequently, we crafted a more elaborate set of in-structions to direct the LLMs in classifying topics,employing a projection of pre-defined class namessimilar to those used in the verbalization.",
  "Emerging NLPNatural Language Processing (21)": "sign language and fingerspelling recognition,rule-based machine translation (RBMT),transformer models, prompt engineeringrecurrent neural networks (RNNs),large language models (LLMs),bilingual lexicon induction (BLI),hate and offensive speech detection,email spam and phishing detection,fake news detection,fake review detection,aspect-based sentiment analysis (ABSA),dialogue state tracking (DST),visual question answering (VQA),open-domain question answering,multiple choice question answering (MCQA),nlp for for social media,nlp for the legal domain,acronyms and abbreviations detection and expansion,paraphrase and rephrase generation,named entity recognition for nested entities"
}