{
  "Abstract": "This study examines the effectiveness of transfer learning and multi-task learningin the context of a complex semantic classification problem: understanding themeaning of noun-noun compounds. Through a series of detailed experiments andan in-depth analysis of errors, we demonstrate that employing transfer learning byinitializing parameters and multi-task learning through parameter sharing enables aneural classification model to better generalize across a dataset characterized by ahighly uneven distribution of semantic relationships. Furthermore, we illustratehow utilizing dual annotations, which involve two distinct sets of relations appliedto the same compounds, can enhance the overall precision of a neural classifier andimprove its F1 scores for less common yet more challenging semantic relations.",
  "Introduction": "Noun-noun compound interpretation involves determining the semantic connection between twonouns (or noun phrases in multi-word compounds). For instance, in the compound \"street protest,\"the task is to identify the semantic relationship between \"street\" and \"protest,\" which is a locativerelation in this example. Given the prevalence of noun-noun compounds in natural language and itssignificance to other natural language processing (NLP) tasks like question answering and informationretrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,psycholinguistics, and computational linguistics. In computational linguistics, noun-noun compound interpretation is typically treated as an automaticclassification task. Various machine learning (ML) algorithms and models, such as MaximumEntropy, Support Vector Machines, and Neural Networks, have been employed to decipher thesemantics of nominal compounds. These models utilize information from lexical semantics, likeWordNet-based features, and distributional semantics, such as word embeddings. However, noun-noun compound interpretation remains a challenging NLP problem due to the high productivityof noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics ofnoun-noun compounds from their constituents. Our research contributes to advancing NLP researchon noun-noun compound interpretation through the application of transfer and multi-task learning. The application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significantattention in recent years, yielding varying outcomes based on the specific tasks, model architectures,and datasets involved. These varying results, combined with the fact that neither TL nor MTL hasbeen previously applied to noun-noun compound interpretation, motivate our thorough empiricalinvestigation into the use of TL and MTL for this task. Our aim is not only to add to the existingresearch on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertaintheir specific advantages for compound interpretation. A key reason for utilizing multi-task learning is to enhance generalization by making use of thedomain-specific details present in the training data of related tasks. In this study, we demonstrate thatTL and MTL can serve as a form of regularization, enabling the prediction of infrequent relationswithin a dataset marked by a highly skewed distribution of relations. This dataset is particularlywell-suited for TL and MTL experimentation, as elaborated in .",
  "Our contributions are summarized as follows:": "1. Through meticulous analysis of results, we discover that TL and MTL, especially when appliedto the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in ahighly skewed dataset, compared to a robust single-task learning baseline. 2. Although our researchconcentrates on TL and MTL, we present, to our knowledge, the first experimental results on therelatively recent dataset from Fares (2016).",
  "Related Work": "Approaches to interpreting noun-noun compounds differ based on the classification of compoundrelations, as well as the machine learning models and features employed to learn these relations. Forinstance, some define a broad set of relations, while others employ a more detailed classification.Some researchers challenge the idea that noun-noun compounds can be interpreted using a fixed,predetermined set of relations, proposing alternative methods based on paraphrasing. We centerour attention on methods that frame the interpretation problem as a classification task involving afixed, predetermined set of relations. Various machine learning models have been applied to thistask, including nearest neighbor classifiers that use semantic similarity based on lexical resources,kernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropymodels that incorporate a wide range of lexical and surface form features, and neural networks thatrely on word embeddings or combine word embeddings with path embeddings. Among these studies,some have utilized the same dataset. To our knowledge, TL and MTL have not been previouslyapplied to compound interpretation. Therefore, we review prior research on TL and MTL in otherNLP tasks. Several recent studies have conducted extensive experiments on the application of TL and MTL to avariety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentimentclassification, super-tagging, chunking, and semantic dependency parsing. The consensus amongthese studies is that the advantages of TL and MTL are largely contingent on the characteristics of thetasks involved, including the unevenness of the data distribution, the semantic relatedness betweenthe source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasksthat quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structuralsimilarity between the tasks. Besides differing in the NLP tasks they investigate, the aforementionedstudies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies inthat we apply TL and MTL to learn different semantic annotations of noun-noun compounds usingthe same dataset. However, our experimental design is more akin to other work in that we experimentwith initializing parameters across all layers of the neural network and concurrently train a singleMTL model on two sets of relations.",
  "Task Definition and Dataset": "The objective of this task is to train a model to categorize the semantic relationships between pairsof nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity ofthis task is influenced by factors such as the label set used and its distribution. For the experimentsdetailed in this paper, we utilize a noun-noun compounds dataset that features compounds annotatedwith two distinct taxonomies of relations. This means that each noun-noun compound is associatedwith two different relations, each based on different linguistic theories. This dataset is derived fromestablished linguistic resources, including NomBank and the Prague Czech-English DependencyTreebank 2.0 (PCEDT). We chose this dataset for two primary reasons: firstly, the dual annotation ofrelations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly,aligning two different annotation frameworks on the same data allows for a comparative analysisacross these frameworks. Specifically, we use a portion of the dataset, focusing on type-based instances of two-word compounds.The original dataset also encompasses multi-word compounds (those made up of more than twonouns) and multiple instances per compound type. We further divide the dataset into three parts:training, development, and test sets. details the number of compound types and the vocabularysize for each set, including a breakdown of words appearing in the right-most (right constituents)and left-most (left constituents) positions. The two label sets consist of 35 PCEDT functors and 18",
  "Compounds69329201759Vocab size410211631772Right constituents2304624969Left constituents2405618985": "Many relations in PCEDT and NomBank conceptually describe similar semantic ideas, as they areused to annotate the semantics of the same text. For instance, the temporal and locative relations inNomBank (ARGM-TMP and ARGM-LOC, respectively) and their PCEDT counterparts (TWHENand LOC) exhibit relatively consistent behavior across frameworks, as they annotate many of thesame compounds. However, some relations that are theoretically similar do not align well in practice.For example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBankexpress a somewhat related semantic concept (purpose), but there is minimal overlap between thesets of compounds they annotate. Nevertheless, it is reasonable to assume that the semantic similarityin the label sets, where it exists, can be leveraged through transfer and multi-task learning, especiallysince the overall distribution of relations differs between the two frameworks.",
  "Transfer vs. Multi-Task Learning": "In this section, we employ the terminology and definitions established by Pan and Yang (2010) toarticulate our framework for transfer and multi-task learning. Our classification task can be describedin terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the inputfeature space, Y denotes the set of all labels, and N is the training data size. The domain of a task isdefined by X, P(X). Our goal is to learn a function f(X) that predicts Y based on the input features X.Considering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functionsfa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb arerelated, either explicitly or implicitly, TL and MTL can enhance the generalization of either or bothtasks. Two tasks are deemed related when their domains are similar but their label sets differ, or whentheir domains are dissimilar but their label sets are identical. Consequently, noun-noun compoundinterpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,but the label sets are distinct. For clarity, we differentiate between transfer learning and multi-task learning in this paper, despitethese terms sometimes being used interchangeably in the literature. We define TL as the utilization ofparameters from a model trained on Ta to initialize another model for Tb. In contrast, MTL involvestraining parts of the same model to learn both Ta and Tb, essentially learning one set of parametersfor both tasks. The concept is to train a single model simultaneously on both tasks, where one taskintroduces an inductive bias that aids the model in generalizing over the main task. It is important tonote that this does not necessarily imply that we aim to use a single model to predict both label setsin practice.",
  "Single-Task Learning Model": "In our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural networkinspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises fourlayers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input layer consists of two integers that indicate the indices of a compounds constituents in the embeddinglayer, where the word embedding vectors are stored. These selected vectors are then passed to a fullyconnected hidden layer, the size of which matches the dimensionality of the word embedding vectors.Finally, a softmax function is applied to the output layer to select the most probable relation. The compounds constituents are represented using a 300-dimensional word embedding model trainedon an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model wastrained by Fares et al. (2017). If a word is not found during lookup in the embedding model, wecheck if the word is uppercased and attempt to find the lowercase version. For hyphenated wordsnot found in the embedding vocabulary, we split the word at the hyphen and average the vectors ofits parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, adesignated vector for unknown words is employed.",
  "Architecture and Hyperparameters": "Our selection of hyperparameters is informed by multiple rounds of experimentation with the single-task learning model, as well as the choices made by prior work. The weights of the embedding layerare updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)optimization function across all models, with a learning rate set to 0.001. The loss function employedis the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.All models are trained with mini-batches of size five. The maximum number of epochs is cappedat 50, but an early stopping criterion based on the models accuracy on the validation split is alsoimplemented. This means that training is halted if the validation accuracy does not improve over fiveconsecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TLand MTL models are trained using the same hyperparameters as the STL model.",
  "Transfer Learning Models": "In our experiments, transfer learning involves training an STL model on PCEDT relations and thenusing some of its weights to initialize another model for NomBank relations. Given the neuralclassifier architecture detailed in .1, we identify three ways to implement TL: 1) TLE:Transferring the embedding layer weights, 2) TLH: Transferring the hidden layer weights, and 3)TLEH: Transferring both the embedding and hidden layer weights. Furthermore, we differentiatebetween transfer learning from PCEDT to NomBank and vice versa. This results in six setups,as shown in . We do not apply TL (or MTL) to the output layer because it is task- ordataset-specific.",
  "Multi-Task Learning Models": "In MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,meaning all MTL models have two objective functions and two output layers. We implement twoMTL setups: MTLE, which features a shared embedding layer but two task-specific hidden layers,and MTLF, which has no task-specific layers aside from the output layer (i.e., both the embeddingand hidden layers are shared). We distinguish between the auxiliary and main tasks based on whichvalidation accuracy (NomBanks or PCEDTs) is monitored by the early stopping criterion. Thisleads to a total of four MTL models, as shown in .",
  "Experimental Results": "Tables 2 and 3 display the accuracies of the various TL and MTL models on the development and testsplits for NomBank and PCEDT. The top row in both tables indicates the accuracy of the STL model.All models were trained solely on the training split. Several insights can be gleaned from thesetables. Firstly, the accuracy of the STL models decreases when evaluated on the test split for bothNomBank and PCEDT. Secondly, all TL models achieve improved accuracy on the NomBank testsplit, although transfer learning does not significantly enhance accuracy on the development split ofthe same dataset. The MTL models, especially MTLF, have a detrimental effect on the developmentaccuracy of NomBank, yet we observe a similar improvement, as with TL, on the test split. Thirdly,both TL and MTL models demonstrate less consistent effects on PCEDT (on both development andtest splits) compared to NomBank. For instance, all TL models yield an absolute improvement of",
  "MTLE77.9378.4559.8956.96MTLF76.7478.5158.9156.00": "Overall, the STL models accuracy declines when tested on the NomBank and PCEDT test splits,compared to their performance on the development split. This could suggest overfitting, especiallysince our stopping criterion selects the model with the best performance on the development split.Conversely, TL and MTL enhance accuracy on the test splits, despite using the same stopping criterionas STL. We interpret this as an improvement in the models ability to generalize. However, sincethese improvements are relatively minor, we further analyze the results to understand if and how TLand MTL are beneficial.",
  "Relation Distribution": "To illustrate the complexity of the task, we depict the distribution of the most frequent relations inNomBank and PCEDT across the three data splits in . Notably, approximately 71.18% of therelations in the NomBank training split are of type ARG1 (prototypical patient), while 52.20% of thePCEDT relations are of type RSTR (an underspecified adnominal modifier). Such a highly skeweddistribution makes learning some of the other relations more challenging, if not impossible in certaincases. In fact, out of the 15 NomBank relations observed in the test split, five are never predictedby any of the STL, TL, or MTL models. Similarly, of the 26 PCEDT relations in the test split, onlysix are predicted. However, the unpredicted relations are extremely rare in the training split (e.g., 23PCEDT functors appear less than 20 times), making it doubtful whether any ML model could learnthem under any circumstances. Given this imbalanced distribution, it is evident that accuracy alone is insufficient to determine thebest-performing model. Therefore, in the subsequent section, we report and analyze the F1 scores ofthe predicted NomBank and PCEDT relations across all STL, TL, and MTL models.",
  "Count8914118326216900": "STL43.9042.1122.7842.8320.5168.81TLE49.3770.9727.6741.6030.7769.67TLH53.9962.0725.0043.0126.0968.99TLEH49.0864.5228.5742.9128.5769.08MTLE54.0966.6724.0542.0327.2169.31MTLF47.8042.1125.6440.7319.2268.89 Several noteworthy patterns emerge from Tables 4 and 5. Firstly, the MTLF model appears to bedetrimental to both datasets, leading to significantly degraded F1 scores for four NomBank relations,including the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated asLOC and MNR in ), which the model fails to predict altogether. This same model exhibitsthe lowest F1 score compared to all other models for two PCEDT relations: REG (expressing acircumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracyon the NomBank test split (as shown in ), it becomes even more apparent that relying solelyon accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task anddataset. Secondly, with the exception of the MTLF model, all TL and MTL models consistently improvethe F1 score for all PCEDT relations except PAT. Notably, the F1 scores for the relations TWHENand ACT show a substantial increase compared to other PCEDT relations when only the embeddinglayers weights are shared (MTLE) or transferred (TLE). This outcome can be partially understoodby examining the correspondence matrices between NomBank arguments and PCEDT functors,presented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank argumentsin the training split () and vice versa (). reveals that 80% of the compoundsannotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. Additionally, 47% ofACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. While this mapping is not as distinctas one might hope, it is still relatively high when compared to how other PCEDT relations map toARG0. The correspondence matrices also demonstrate that the presumed theoretical similaritiesbetween NomBank and PCEDT relations do not always hold in practice. Nevertheless, even suchimperfect correspondences can provide a training signal that assists the TL and MTL models inlearning relations like TWHEN and ACT. Since the TLE model outperforms STL in predicting REG by ten absolute points, we examinedall REG compounds correctly classified by TLE but misclassified by STL. We found that STLmisclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STLsovergeneralization in RSTR prediction. The two NomBank relations that receive the highest boost in F1 score (about five absolute points)are ARG0 and ARGM-MNR, but the improvement in the latter corresponds to only one additionalcompound, which might be a chance occurrence. Overall, TL and MTL from NomBank to PCEDTare more helpful than the reverse. One explanation is that five PCEDT relations (including the fourmost frequent ones) map to ARG1 in NomBank in more than 60% of cases for each relation, as seenin the first rows of Tables 6 and 7. This suggests that the weights learned to predict PCEDT relations",
  "Count493271549535811910379": "offer little to no inductive bias for NomBank relations. Conversely, the mapping from NomBank toPCEDT shows that although many NomBank arguments map to RSTR in PCEDT, the percentagesare lower, making the mapping more diverse and discriminative, which seems to aid TL and MTLmodels in learning less frequent PCEDT relations. To understand why the PCEDT functor AIM is never predicted despite being more frequent thanTWHEN, we found that AIM is almost always misclassified as RSTR by all models. Furthermore,AIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occurin other compounds annotated as RSTR. This explains the models inability to learn AIM but raisesquestions about their ability to learn relational representations, which we explore further in .3.",
  "STL52.6640.15TLE52.8348.34TLH52.9846.52TLEH53.3147.12MTLE53.2147.23MTLF42.0740.73": "Finally, to demonstrate the benefits of TL and MTL for NomBank and PCEDT, we report the F1macro-average scores in . This is arguably the appropriate evaluation measure for imbalancedclassification problems. Note that relations not predicted by any model are excluded from the macro-average calculation. clearly shows that TL and MTL on the embedding layer yield significantimprovements for PCEDT, with about a 7-8 point increase in macro-average F1, compared to just0.65 in the best case for NomBank.",
  "Generalization on Unseen Compounds": "We now analyze the models ability to generalize to compounds not seen during training. Recentresearch suggests that gains in noun-noun compound interpretation using word embeddings andsimilar neural classification models might be due to lexical memorization. In other words, the modelslearn that specific nouns are strong indicators of specific relations. To assess the role of lexicalmemorization in our models, we quantify the number of unseen compounds that the STL, TL, andMTL models predict correctly. We differentiate between partly and completely unseen compounds. A compound is partlyunseen if one of its constituents (left or right) is not present in the training data. A completelyunseen compound is one where neither the left nor the right constituent appears in the training data.Overall, nearly 20% of the compounds in the test split have an unseen left constituent, about 16%have an unseen right constituent, and 4% are completely unseen. compares the performanceof the different models on these three groups in terms of the proportion of compounds misclassifiedin each group.",
  "Count3512867235128672": "STL27.9239.5150.0045.0147.5541.67TLE25.9336.7148.6143.8747.5541.67TLH26.2138.1150.0046.1549.3047.22TLEH26.5038.8152.7845.8747.5543.06MTLE24.5033.2238.8944.4447.2043.06MTLF22.7934.2740.2844.1647.9038.89 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reducegeneralization error in NomBank across all scenarios, with the exception of TLH and TLEH forcompletely unseen compounds, where error increases. The greatest error reductions are achievedby MTL models across all three types of unseen compounds. Specifically, MTLE reduces the errorby approximately six points for compounds with unseen right constituents and by eleven points forfully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituentis unseen. Its important to interpret these results in conjunction with the Count row in fora comprehensive view. For example, the eleven-point error decrease in fully unseen compoundsrepresents eight compounds. In PCEDT, the largest error reduction is on unseen left constituents,which is about 1.14 points, corresponding to four compounds; its 0.35 on unseen right constituents(one compound) and 2.7 on fully unseen compounds, or two compounds. Upon manual inspection of compounds that led to substantial reductions in the generalization error,specifically within NomBank, we examined the distribution of relations within correctly predictedunseen compound sets. Compared to the STL model, MTLE reduces generalization error forcompletely unseen compounds by a total of eight compounds, of which seven are annotated with therelation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,MTLEs 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. Asimilar pattern arises when examining TLE model improvements, where most gains come from betterpredictions of ARG1 and ARG0 relations. A large portion of unseen compounds, whether partly or entirely unseen, that were misclassified byevery model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along withcorrectly predicted unseen compounds primarily annotated with the most common relations, suggeststhat classification models rely on lexical memorization to learn the compound relation interpretation. To better comprehend lexical memorizations impact, we present the ratio of relation-specific con-stituents in both NomBank and PCEDT, as depicted in . We define a relation-specificconstituent as a left or right constituent that appears with only one specific relation within the trainingdata. Its ratio is calculated as its proportion in the full set of left or right constituents for each relation. Analyzing reveals that NomBank relations possess higher ratios of relation-specificconstituents compared to PCEDT. This potentially makes learning the former easier if the modelsolely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN inPCEDT have distinctly high ratios compared to other relations in . These relations alsohave the second-highest F1 score in their datasetsexcept for STL on PCEDT (see Tables 4 and5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed thatlower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG inPCEDT. Based on these insights, we cant dismiss the possibility that our models show some degreeof lexical memorization, despite manual analysis also presenting cases where models demonstrategeneralization and correct predictions in situations where lexical memorization is impossible.",
  "Conclusion": "The application of transfer and multi-task learning in natural language processing has gained sig-nificant traction, yet considerable ambiguity persists regarding the effectiveness of particular taskcharacteristics and experimental setups. This research endeavors to clarify the benefits of TL andMTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence ofminimally contrasting experiments and conducting thorough analysis of results and prediction errors,we demonstrate how both TL and MTL can mitigate the effects of class imbalance and drasticallyenhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,are better at making predictions both quantitatively and qualitatively. Notably, the improvements areobserved on the most challenging inputs that include at least one constituent that was not present inthe training data. However, clear indications of lexical memorization effects are evident in our erroranalysis of unseen compounds. Typically, the transfer of representations or sharing between tasks is more effective at the embeddinglayers, which represent the models internal representation of the compound constituents. Furthermore,in multi-task learning, the complete sharing of model architecture across tasks degrades its capacityto generalize when it comes to less frequent relations. The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compoundinterpretation because it links this sub-problem with broad-coverage semantic role labeling orsemantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporatingadditional natural language processing tasks defined using these frameworks to understand noun-nouncompound interpretation using TL and MTL."
}