{
  "Abstract": "Modern large language models (LLMs) haveexhibited cooperative synergy on complex task-solving, and collective decision-making (CDM)is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on52 recent such systems uncovers a severe lackof diversity, with a heavy reliance on dictatorialand plurality voting for CDM. Through the lensof social choice theory, we scrutinize widely-adopted CDM methods and identify their lim-itations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoralCDM module that incorporates various ordinalpreferential voting mechanisms. Our empiri-cal case study across three benchmarks showsthat the integration of certain CDM methodscan markedly improve the reasoning capabili-ties and robustness of some leading LLMs, allwithout requiring intricate system designs. Ad-ditionally, we find that some CDM mechanismsgenerate positive synergies even with as few asthree agents. The voting-based methods alsodemonstrate robustness against single pointsof failure, as well as diversity in terms of hit-rate@k and subject-wise impacts.1",
  "Introduction": "While multi-agent systems have constantly gar-nered attention even before the advent of large lan-guage models (LLMs), as evidenced by prior works(Wooldridge, 2009; Dorri et al., 2018). The recentadvancements in LLMs have significantly sparkedinterest in LLM-based agents. Furthermore, noveltechniques such as effective prompt engineering(Wei et al., 2023; Wang et al., 2023c) and agent-interaction schemes (Yao et al., 2023; Shinn et al.,2023) have propelled a surge in research on collab-orative LLM agents (Xi et al., 2023; Wang et al.,2023b). Researchers have deployed LLM-basedagents in various environments and scenarios: from",
  "Unspecified": "simulating small community (Liu et al., 2023a;Park et al., 2023) to predicting court judgement(Hamilton, 2023), crafting digital avatars (Jarrettet al., 2023; Yang et al., 2024), and participating indialogue-based games (Xu et al., 2023a; Stepputtiset al., 2023; Li et al., 2023c), among others.However, the existing accounts on LLM-basedmulti-agent collaboration has been heavily focus-ing on inter-agent communication and interactionworkflows. In contrast, another vital aspect, collec-tive decision-making (CDM), appears to have beenlargely neglected and overly simplified. Our reviewof 52 recent LLM collaboration systems ( 2) re-veals that systems either appoint a dictator agentto make decisions for the group (Hao et al., 2023;Nair et al., 2023) or depend on simplistic pluralityvoting (Chan et al., 2023; Zhang et al., 2023b; Xuet al., 2023b), with one case adopting an utilitarianapproach (Jarrett et al., 2023).This study examines prevalent CDM methodsthrough the lens of social choice theory (Arrowet al., 2010) and illustrate their failure to meet fun-damental criteria ( 3): dictatorial methods arefragile for their absolute dependency on one singleagent; plurality voting, while simple and intuitivelyflawless, disqualifies Independence from IrrelevantAlternatives (IIA) and Condorcet criterion; utilitar-ian violates both Majority and Condorcet criteria.Such deviations from key criteria may impede thetransition from individual preferences to collectivedecisions among LLM-based agents.While Arrows theorems (Arrow, 1951) estab-lishes the axiomatic impossibility of designing a perfect voting-based CDM system, we can stillcircumvent some limitations and risks by incorpo-rating a variety of CDM methods into LLM-basedmulti-agent frameworks. To this end, we developan electoral CDM module, GEDI ( 4), which of-fers a range of CDM mechanisms that were not pre-viously tested in such frameworks. To evaluate thepotential impact of various CDM methods, we con-duct an empirical case study ( 5) on three multiple-choice question-answering (MCQA) benchmarks:MMLU (Hendrycks et al., 2021), MMLU-Pro(Wang et al., 2024a), and ARC-Challenge (Clarket al., 2018), using a suite of models with varioussizes and architectures.Our key findings ( 5.2) are as follows: (1) ap-plying a CDM method generally leads to better re-sults compared to a single-agent decision-makingon MCQA benchmarks, though at the cost of in-creased computation; (2) the degree of synergydepends significantly on the backbone model andthe benchmark. Some LLMs exhibit substantialimprovements with voting-based methods, whileothers show little to no effect under any CDM;(3) most voting methods require only a minimalquorum, as few as three agents, to be effective;and (4) CDM methods exhibit varying levels ofrobustness against unreliable agents, different hit-rates@k, and varying impacts across different sub-ject domains.We hope these observations will encourage fur-ther evaluation of the effectiveness of LLM-basedmulti-agent frameworks and provide valuable in-sights for advancing LLM-based Multi-Agent Sys-tems (MAS).",
  "Background": "Multi-agent systems are composed of multiple com-puting elements with autonomous action and in-teraction capabilities (i.e., agent) (Wooldridge,2009).Prior to the advent of LLMs, researchon multi-agent systems had already been a focalpoint various across disciplines (Silver et al., 2017;Dorri et al., 2018). The swift progression of LLMshas since ignited an intensified interest in employ-ing LLMs as agents (Xi et al., 2023). Notably,the advent of effective prompting schemes hasgreatly boosted the performance of individual LLMagent: Chain-of-Thought (Wei et al., 2023), Self- Consistency (Wang et al., 2023c), ReAct (Yao et al.,2023), Reflexion (Shinn et al., 2023), DiVeRSe (Liet al., 2023e) among others. Although single-agentframeworks have shown remarkable success in cer-tain NLP tasks, they often struggle with more intri-cate challenges, such as common sense reasoningand long-term planning (Wang et al., 2023b). Inresponse, some researchers advocate multi-LLM-agent collaboration as a promising path.",
  "Collective Decision-Making in LLM-basedMulti-Agent Collaboration": "Collective decision-making (CDM) is the processby which a group of autonomous entities arrives ata decision (Bose et al., 2017). This phenomenon isprevalent in both animal societies and human com-munities, with numerous interdisciplinary studiescorroborating that CDM typically yields superiordecisions compared to those made by individualsalone (King and Cowlishaw, 2007; Couzin et al.,2011).Recent development of LLMs has made self-governing CDM processes feasible in LLM-basedmulti-agent systems. However, our survey of 52newly proposed frameworks indicates that CDMmechanisms have not received adequate focus.Specifically, most systems either depend on thedictatorial judgment of a single agent (often bypreassigned role) or employ plurality voting fordecision-making. As depicted in , we cancategorize current LLM-based multi-agent systemsinto four groups based on their CDM approaches:(1) dictatorial, (2) plurality, (3) utilitarian, and (4)those with no CDM or unspecified. DictatorialAmong the reviewed papers, dicta-torial methods are most popular. As the nameimplies, it is a one-agent-rule system in which asingle agent, often pre-designated, has the right toratify a decision. Nonetheless, such system canbe collective in a sense that the dictator may becounseled by and communicated with other agents.Most dictatorial frameworks designate a spe-cial agent who oversees collaboration, evaluatesoutcomes, and has the final say over system-leveldecisions. Such agent has many alias: leader(Hao et al., 2023; DArcy et al., 2024), decider(Nair et al., 2023), commander (Wu et al., 2023),critic (Li et al., 2023a), teacher (Jinxin et al.,2023), judge (Liang et al., 2023; Xiong et al.,2023; Sun et al., 2023; Talebirad and Nadiri, 2023),evaluator (Tang et al., 2023), planner (Zhang et al., 2023a; Fang et al., 2024), recruiter (Liet al., 2023f), inspector(Hua et al., 2024; Wanget al., 2024b), discriminator (Hang et al., 2024),task agent(Li et al., 2023b), QA-Checker (Tanget al., 2024). Some specific cases include creatingvirtual software and game development companieshosting LLM-agents of various roles to achieverapid and low-cost development of software (Qianet al., 2023; Chen et al., 2023a). Specially, Chenet al. (2023b) suggest an oligarchic small groupof planner and observers instead of a singledecision-maker. Plurality VotingPlurality voting selects the op-tion with the most first-preference votes (i.e., rela-tive majority). For simplicity, we consider majorityvoting, which that requires more-than-half votes(i.e., absolute majority), and consensus, which de-mands an unanimous agreement from every agents,to be two variations of plurality voting.Frameworks that adapt plurality voting often in-troduce multi-round discussion to reach resolutionor majority agreement (Xu et al., 2023b). Multi-agent debate process is found to improve LLMsfactuality (Du et al., 2023), reasoning capabilities(Zhang et al., 2023b), and financial trading per-formances (Li et al., 2023d). Chan et al. (2023)also improve the quality of evaluation providedby LLM-agents on natural language generationtasks via debates. Chen et al. (2023d) fashion au-tomatic team formation and LLM-agent experts re-cruitment. Chen et al. (2023c) quantify consensus-seeking process by appending self-assigned statevalues of LLM-agents and measuring their con-vergence. Notably, Wang et al. (2023d) showcasethat multiple personas of a single LLM can alsoself-collaborate. In some cases, plurality voting ischosen to match simulated target scenarios. Hamil-ton (2023) trains nine separated agents as judgesto simulate the U.S. Supreme Court and achievebetter-than-random judgement prediction accuracyon 96 real-world cases. In textual or conceptualgames like Werewolf (Xu et al., 2023a) and Avalon(Stepputtis et al., 2023; Shi et al., 2023), agents arebound by the game rule to take this method. UtilitarianUtilitarian approaches quantify theimpacts of possible decisions and choose the onethat maximizes the collective utility or rewardgained by a group. However, utilitarian is distinctfrom other methods for its non-self-governing: theutilities are externally predetermined or updated.Jarrett et al. (2023) propose to train LLM agents as digital proxy to represent individual preferencesvia an utilitarian payoff function. Although utili-tarian is rare in newly proposed LLM-based frame-works, it is a pillar method in many previous non-LLM multi-agent systems (Dorri et al., 2018). No CDM or UnspecifiedSome multi-agent sce-narios necessitate no CDM. For instance, simulat-ing social interaction and behaviors among LLM-agents (Park et al., 2023; Liu et al., 2023a; Ghaf-farzadegan et al., 2023; Hua et al., 2023; Zhanget al., 2024; Wei et al., 2024), while one-to-oneagreement can happen occasionally. Other systemsintrinsically deny a CDM process, such as strictlylinear collaboration workflow (Hong et al., 2023;Wang et al., 2023a; Ding et al., 2023; Rasheedet al., 2024) or decentralized team arrangements (Liet al., 2023c; Nakajima, 2023; He et al., 2023). Inaddition, some frameworks involve human judge-ment for system-level decisions (Ghafarollahi andBuehler, 2024; Ni and Buehler, 2024).Thus far, having seen a great lack of diversityof CDM methods in LLM-based multi-agent col-laboration, we draw our inspiration from socialchoice theory and scrutinize the pros and cons ofthe widely-used methods.",
  "A Social Choice Theory Perspective onCollective Decision-Making": "Social choice theory concerns passing from individ-ual preferences to collective decisions (Arrow et al.,2010). While humans have practiced and refinedcollective decision-making since antiquity, modernsocial choice theory has not been established untilthe publishing of Kenneth J. Arrows renowned So-cial Choice and Individual Values (Arrow, 1951),which axiomatically formalizes the theory and com-paratively analyzes various electoral systems.",
  "Related Work Incorporating SocialChoice Theory into NLP Research": "The related research to date has tended to focus onintegrating social choice theory into model align-ment (Mishra, 2023), model ensemble (Jiang et al.,2023b), text generation and preference extrapola-tion (Fish et al., 2023). More specifically, Jarrettet al. (2023) take an utilitarian approach to employLLM agents as digital representatives of human.Irurozki et al. (2022); Rofin et al. (2023) point outthe limitations of canonical mean-average aggre-gation of multi-task scores in NLP benchmarkingand propose novel aggregation methods based on social choice theory. Wang et al. (2023c); Xue et al.(2023) propose to select answers from multiple gen-erated reasoning paths by plurality voting and yieldimproved results over utilitarian approaches.Most recently, Li et al. (2024) demonstrate thesynergy of plurality voting on gpt-3.5 (Ouyanget al., 2022) and Llama-2 (Touvron et al., 2023),echoing some of our findings, yet it lacks compar-isons with other CDM methods. Another concur-rent work (Yang et al., 2024) examines the differ-ences between human and LLM from a voting be-havior perspective. Nevertheless, previous studiesdo not overlap with our primary aim of diversifyingLLM-based multi-agent CDM methods.",
  "Criticism on Prevalent CDM Methods inLLM-based Multi-Agent Collaboration": "In the context of LLM-agent collaboration, dictato-rial methods rely on a single agent who is informedand counseled by other agents to decide for thegroup. While dictatorship is often computing-wiseefficient, its absolute dependency on a sole agentmakes it is more biased and less robust than moredemocratic processes.In contrast, utilitarian and cardinal voting meth-ods certainly aggregate and disclose broader indi-vidual preferences from group members. However,an unignorable drawback of these methods is theunstable and arbitrary nature of externally imposedutilities (Brandt et al., 2016). Provided that agentshave accurate cardinal utilities over choices, whichis a strong assumption, then an uneven distribu-tion of utilities is another potential concern: such asystem could easily violate Majority criterion (see) or even collapse to autocratic if oneagent with dominant utility impact was present.Plurality voting showcases a paradigmatic exam-ple of ordinal voting (also known as preferentialor ranked voting), another decentralized decision-making family. Although there are other widely-practiced ordinal voting methods available, to thebest of our knowledge, all existing LLM-agent col-laboration frameworks that employ voting methodsselect plurality voting, as shown in . Thesimple method may seem intuitively safe. How-ever, through the lens of Arrows theorems (Ar-row, 1951), this method contradicts some ratherself-evident criteria. To name two, the methodviolates both Independence from Irrelevant Alter-natives (IIA) criterion, as shown in , andCondorcet criterion, as illustrated in .In fact, Arrows theorems mathematically prove",
  "Dictatorial (Blind)RankingRange VotingScoresPluralitySingle*Borda CountRankingIRVRankingRanked PairsRanking": ": Criteria compliance of some typical CDMmethods. Range Voting can be viewed as a specialutilitarian method. IIA denotes Independence fromIrrelevant Alternatives. *Single ballots can be derivedfrom ranking ones. Find some examples in Appendix D. that every electoral system has some fundamentalflaws, as exemplified in . The axiomaticimpossibility of constructing a perfect voting sys-tem, however, motivates us to reduce the risk offalling into a single point of failure. To this end,we argue it is of great pragmatic values to diver-sify current landscape of LLM-agent with moderndecentralized voting systems. In order to leverageLLM-agents natural-language-based judgementrather than imposed utility or reward, we place aparticular emphasis on ordinal preferential voting.",
  "Diversifying LLM-based Multi-AgentCDM": "To enhance the diversity of CDM approacheswithin LLM-agent frameworks, we propose incor-porating a range of CDM methods rooted in humansocio-political practices. Specifically, we craft anelectoral CDM module, named General ElectoralDecision-making Interface (GEDI), which inte-grates several common ordinal preferential votingsystems. highlights a few key distinctionsbetween GEDI and other commonly used CDMmethods in LLM-based MAS.",
  "Definition": "Following conventional practice (Arrow et al.,2010; Brandt et al., 2016), consider a multi-alternative decision-making process. Let N ={1, 2, ..., n} be a finite set of n agents, A ={a1, a2, ...am} be a finite set of m distinct alter-natives, where m 2 and for all a, b A, a = b.A preferential ranking ballot (i.e., vote) can be de-fined as a strict partial ordering of A (Rosen,2007). Specifically, is transitive: for all a, b, c A, if a b and b c then a c); and com-plete: for all a, b A, a b or a b. Note thatthere is also a weak ordering variation that acceptsvoters stating indifference to two alternatives (i.e., UtilitarianDictatorial (Informed)PluralityPreferential Voting (GEDI)",
  "Counsel": "VoteProcessing : Comparison among different LLM-based multi-agent CDM structures: utilitarian, dictatorial, plurality andour expansion. Agenda refers to assigned tasks or interactive environment. Blue and green arrows denote interactionbetween agents and preference communication to CDM systems respectively. Rather than generate a single decision,GEDI uniquely outputs ordinal rankings, providing more information on agents collective preferences. preferential ties).Concretely, the input of GEDI is composed of:(1) a profile P = (1, 2, ... n), which denotesa collection of ballots from each voter i N; (2) avoting system (i.e., social choice function (SCF)),which is defined as a map f : L(A)n C(A)that returns a set of alternatives for each profile ofstrict preferences. The output f(P) is a nonemptyordered subset of the alternative set A.",
  "Assessed Electoral Methods": "We select 10 CDM methods to assess in the fol-lowing case study: Blind Dictatorial, InformedDictatorial, Mis-informed Dictatorial, Range Vot-ing, Plurality, Borda Count, Bucklin, Minimax, andRanked Pairs, along with random baselines. DictatorialBlind dictatorial (or random ballot)arbitrarily chooses an agent and admits its pref-erence ranking as the decision without consultingwith fellow agents (Aziz et al., 2013). Alternatively,informed dictatorial is a counselled version inwhich a dictator agent first reviews ballots of thevoting ensemble and then forms its own decision.We also entail a mis-informed variation to verifythe impact of information communicated via bal-lots, in which the dictator is consulted by randomballots rather than actual ones from the ensemble.",
  "PluralitySimple plurality (relative majority)considers only the first-preference in each vote,ignoring any later preferences. The winner is thecandidate who receives the most top-choice votes": "Bucklin VotingThe first-preference votes areaccounted for first, and if no choice has absolutemajority, next-in-line preference votes are then ac-counted. Repeat the process until an absolute ma-jority winner emerge (Erdlyi et al., 2015). Borda CountChoices gain points from theirplaces on each ballot, and the overall points de-termine the winner (Emerson, 2013; Davies et al.,2014). In standard Borda count, a preference balloton m alternatives awards m i points for the i-thranked alternative. Unlike range voting, Borda isdistinct from utilitarian methods because Bordastill utilises ordinal preferences. Instant-Runoff Voting (IRV)A multi-roundmechanism that repeatedly eliminates the alterna-tive with fewest first-preference votes and trans-fers the votes of the eliminated to surviving alter-natives (Freeman et al., 2014). The reverse orderof elimination comprise the sorted list of collec-tively preferred options. While there are variousearly exit designs (e.g., choose a winner once anabsolute majority appears), we include the standardIRV to get the full sorted list. MinimaxA method that selects the choice withleast worst disfavor (Brams et al., 2007). For-mally, let f(a, b) represent the overall favor ofa over b (i.e., the number of pairwise wins of aagainst b across all ballots) for a distinct pair ofalternatives a, b A.f(a, b) can be negativeif more voters favor b over a. The worst disfa-vor of a is defined as max f(b, a). The winningalternative is the one minimizes the worst disfa-vor, i.e., the one with the minimum max f(b, a):aw = arg mina (maxbf(b, a)). Ranked PairsConcretely, let (a, b) denotes theaggregated pairwise comparison result of a, b A,and a positive (a, b) indicates more agents prefera over b. Ranked pairs method breaks down com-plete ballots into preferential pairs and ranks themby prevalence. Starting from the most frequentpairs (i.e., arg maxa,bA(a, b)), the method fills a pair-",
  "Experiment Setup": "DatasetsAs the primary scope of this study ison decision-making process rather than choice-generation, MCQA benchmarks particularly suitsthe research interest, for they have alternatives (i.e.,choices) predefined. Following previous studieson benchmarking general performances of LLMagents (Park et al., 2022; Liu et al., 2023b; Zhanget al., 2023b; Google, 2023; Jiang et al., 2023a), weselect MMLU (Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024a), and ARC-Challenge(Clark et al., 2018) as the case study testbeds. Backbone ModelsIn an effort to simulate agentsbuilt on language models of diverse architecturesand parameter sizes, we curate a collection ofsix open-sourced models, including mistral-7b(Jiang et al., 2023a), glm-4-9b (Zeng et al.,2023), llama-3-8b/70b (AI@Meta, 2024) andqwen-1.5-72b/110b (Qwen, 2024). In addition,since most existing LLM-based multi-agent col-laboration frameworks employ high performancemodels with huge parameter sizes to create agents,following their suits, we also test two widely-usedproprietary models: gpt-3.5 (Ouyang et al., 2022) and gpt-4 (OpenAI, 2023). Specifications of se-lected models can be found in Appendix A Ta-ble 5. The temperature of all models are fixed at0.7 (within a 0.0 to 1.0 range) except for OpenAImodels, whose temperatures are maintained at 1.0(within a 0.0 to 2.0 range). Metric and AssessmentFor simplicity, we har-ness unmodified language models as test agentsand prefix a short instruction You are the {randomnumber}-th rater ahead of questions to identifythem. A decision ensemble is composed of a des-ignated number of agents built on the same back-bone model. Every agent is requested to providea preferential ranking of choices on each questionindependently. Having gathered all rankings (i.e.,votes) to form a profile, GEDI outputs collectivepreferential ranks conforming to selected votingrules. Uniquely, under informed and mis-informeddictatorial rules, a dictator agent besides the vot-ing ensemble is provided with other agents votesand then enquired (see 4.2). As described in 4.1, given a profile P containing 10 preferentialrankings from agents, a voting system f of GEDIoutputs an ordered list f(P) of all choices. Weconsider a question is correctly answered if the firstelement of the output list match the correspondinggold label. In accordance with the original setupof MMLU (Hendrycks et al., 2021), we implementa 5-shot example prompting that utilises the devel-opment sets of the datasets. All methods take thesame preferential ranking format votes except forrange voting that requires numerical preferentialscores in addition to the rankings.",
  "The 5-run average overall accuracy results are re-ported in , and corresponding statistics ofvalid ranking profiles are detailed in Appendix C": "Random Baselines and Range VotingThe ac-curacies for random baselines hover around 25.0for the 4-choice MMLU and ARC-Challenge, andapproximately 10.0 for the 10-choice MMLU-Pro.These figures confirm a balanced distribution ofcorrect choices within the test sets. Most models,especially the smaller ones, exhibit inferior per-formance when implementing score-based rangevoting (i.e., cardinal ranking) compared to ordi-nal ranking methods.However, llama-3-70b,gpt-3.5, and gpt-4 are exceptions, as their rangevoting outcomes exceed those of blind dictatorial.",
  "gpt-425.092.9 (+0.4)92.592.8 (+0.3)87.3 (-5.2)92.9 (+0.4)92.7 (+0.2)92.8 (+0.3)92.8 (+0.3)92.8 (+0.3)92.9 (+0.4)": ": Overall accuracy results on MMLU, MMLU-Pro and ARC-Challenge benchmarks. Rand. and Dicta.denote random and dictatorial, respectively. The numbers in parentheses are relative to the blind dictatorialbaselines. Performance gains are marked in red, and loss in blue. Notable cases are marked in bold. *Resultsmarked with asterisk are calculated utilizing partial profiles (see Appendix C). Dictatorial MethodsThe colored numbers in Ta-ble 2 indicate results relative to blind dictatorial,which serves as the baseline for comparing. Al-though most models perform better under informeddictatorial than under blind dictatorial, they do notoutperform other ordinal ranking methods.It should be noted that informed dictatorial costmore than voting-based methods computationally,since it necessitates a complete ballot profile fromthe ensemble, in addition to the dictator. The sub-par performance of informed dictatorial impliesthat a dictator agent is unable to utilize the in-formation from ensemble ballots more effectivelythan the voting systems.As anticipated, the significantly reduced accu-racies under mis-informed dictatorial demonstratethe detrimental effect of providing the dictatorwith random ballots. Remarkably, glm-4-9b andgpt-4 exhibit a relatively minor decline comparedto other models across the three datasets, indicatingtheir resilience to misleading information.",
  "Ordinal Ranking MethodsIt is consistently ob-served that the application of voting-based ordinalranking methods, even those as straightforward as": "plurality, results in accuracies that match or sur-pass those achieved by blind dictatorial. The ex-tent of improvement varies depending on the spe-cific model in question. Notably, models built onsmaller models (<10B) and those within the GPTseries exhibit substantial performance enhance-ments when electoral CDM methods are employed,in stark contrast to medium models (10-110B).For MMLU benchmark, the adoption of a votingmethod leads to average accuracy increases of ap-proximately 2.9%, 4.9%, and 6.5% for glm-4-9b,gpt-3.5, and gpt-4, respectively.Given thatMMLU-Pro is a 10-choice test, the relative im-provements due to CDM may appear less pro-nounced. Nonetheless, llama-3-8b and glm-4-9bstill register noticeable accuracy gains under votingmethods. In particular, minimax and ranked pairsmethods demonstrate robustness, showing positiveeffects on all models across the three benchmarks.These findings call for a reassessment on existingLLM-agent collaboration frameworks, particularlyregarding the extent to which the impacts of theirproposed systems may be attributed to the imple-mentation of specific CDM methods. However, itis also observed that some CDM methods exhibit",
  ": Accuracy comparison of voting ensembles ofdifferent sizes built on the same backbone models. TheRange results of glm-4-9b is excluded for insufficientprofiles (see Appendix C)": "illustrates the notable impact on accu-racy when varying the number of voting agents,using llama-3-8b and glm-4-9b on the ARC-Challenge dataset, and gpt-3.5 and gpt-4 on theMMLU benchmark. Overall, most CDM meth-ods start exhibiting significant improvements andsurpass the blind dictatorial baseline in situationsinvolving more than two agents, where a majoritycan be established.For GPT models, noticeable drops occurs whenthe voting group increases to two. Borda takesa few more agents to reach the plateau, which islikely attributed to its ballot weight scale that isbased on the number of choices (4 in our case).Range voting starts higher yet stabilizes lower thanother methods. Surprisingly, for gpt-4, simply requiring a range vote rather than ordinal prefer-ential vote greatly increases its judgement evenwithout multiple agents! However, the results ofrange voting vary slightly when increasing numberof voting agents, demonstrating a ensemble-size-independent property that is not seen on gpt-3.5.In particular, llama-3-8b shows the most variancewhen applying different CDM, mostly due to asmaller number of valid profiles (see Appendix C).Nonetheless, since the ensemble size directly im-pacts the required computational resources, a con-sideration of cost-benefit trade-offs is essential. Robustness against Unreliable AgentsThe vot-ing quorum scenario presupposes that all agentscan accurately express their preferences. However,one might wonder: What if LLM agents are unreli-able (i.e., malfunctioning or incapable)? An extraadvantage of involving more agents in decision-making is the increased robustness against a singlepoint of failure. To assess the resilience of variousmethods to unreliable voters, we incrementally re-placed the voting ensemble of 10 fully functionalagents with unreliable ones who cast random votes. Blind Dict.RangeInofrmed Dict.PluralityBucklin BordaIRVMinimaxRanked Pairs GPT-3.5 RobustnessGPT-4 Robustness Number of Unreliable Agents",
  ": Accuracy impact of increasing number ofunreliable agents built on gpt-3.5 and gpt-4": "depicts the performance of compro-mised voting ensembles under different votingrules. Most voting methods maintain their integrityuntil the number of unreliable agents reaches 4, andthen their accuracies converges to the random base-line at 25%. As anticipated, informed dictatorialis the first to collapse, since the entire system failsonce the dictator is incapable of making a rea-sonable judgment (utilitarian methods relying onsingle external utility-calculation module would bethe same case). Contrary to expectations, pluralityexhibits a commendable robustness compared tomore sophisticated methods.",
  "Difference in Hit-Rate@KLet hit-rate@k de-notes a cumulative accuracy of taking the first k": "preferences of an answer. We find that although afew methods yield seemingly even performancegains, they are distinguishable in terms of hit-rate@k, as illustrated in . Notably, de-spite being robust against unreliable agent, plural-ity falls short in scenarios where the eliminationof the worst choices is of the higher priority thanthe selection of the best. On the other hand, Borda,ranked pairs, and informed dictatorial methodshave the strongest discriminant power on exclud-ing the wrong choices. Intriguingly, while blinddictatorial performs poorly on the first choice, itshit-rate@3 surpasses some electoral methods. HitRate@1Add. HitRate@2Add. HitRate@3 Blind Dicta.",
  "Conclusion and Future Work": "In the midst of the expanding research on LLM-based agents, we have surveyed 52 multi-agent col-laboration frameworks and revealed a significantlack of diversity in CDM. We have scrutinized pop-ular CDM methods and indicate their fundamentallimitations through a social choice theory perspec-tive. Aiming to diversify the current CDM land-scape, we have drew inspiration from human soci-etal practices and explored various CDM methodsin an empirical case study across multiple bench-marks. Our experiments have produced a wealth ofobservations and insights, demonstrating how suchdiversification can illuminate the study of collectivebehaviors in LLMs.Our study also opens up numerous avenues forfuture research. For instance, matching specifictasks with appropriate CDM methods to enhanceagent decision-making quality holds promisingpractical value. Moreover, since social choice the-ory addresses collective preferences, we expect thatit could inspire broader interdisciplinary NLP re-search, particularly language model alignment andaggregation.",
  "Limitations": "Multi-Choice Question-Answering (MCQA)Benchmarks as CDM Testbedwhile the experi-ments on MMLU, MMLU-Pro and ARC yield no-table and insightful observations, we acknowledgethat MCQA is not fully aligned with collectivedecision-making. Foremost, LLM have demon-strated inconsistency in multi-choice ranking task(Zhao et al., 2024). Secondly, most MCQA bench-marks have predetermined correct answers; how-ever, CDM processes can also be relevant in scenar-ios where there is no absolute right or wrong. Forinstance, measuring bias in LLM agents involvesaggregating the preferences of individual agents,where no objectively correct choices exist. There-fore, an additional avenue for future work couldinvolve constructing a benchmark that measurespreference representativeness rather than one basedon true-or-false judgments. Self-contained TestingAll experiments are self-contained systems of sole backbone model. In otherwords, we do not test any ensemble containingvoting agents built on different LLMs, which couldbe another future direction. Unexhausted Inclusion of Voting Strategies inGEDIAlthough we attempted to cover commonmodern electoral systems, the CDM method list ofGEDI is not exhaustive. For instance, in an effortto keep the module compact and lightweight, we donot include compound mechanisms that combinemultiple voting strategies. However, such mech-anisms are achievable by arranging a pipeline ofmultiple GEDI modules if so wish. Voting TaxThe voting tax of electoral CDMmethods refers to the computation cost of imple-menting such methods. The tax is composed of twoparts: agent actions and ballot processing. Agentactions takes largest proportion as operating LLMagents is highly costly. The cost of inter-agent com-munication should be taken into consideration aswell. Compared with vast computational resourcesrequired by model inference, ballot processing partconsumes much minor.Another aspect to consider the cost-benefit trade-offs is the participation in decision-making. Hu-man voters could feel certain degree of fulfillmentby participation alone regardless of results, as theyhave expressed their preferences in social decision-making processes.LLM agents, however, can",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing": "Sirui Hong, Mingchen Zhuge, Jonathan Chen, XiawuZheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,Zili Wang, Steven Ka Shing Yau, Zijuan Lin, LiyangZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,and Jrgen Schmidhuber. 2023. Metagpt: Meta pro-gramming for a multi-agent collaborative framework. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei,Jianchao Ji, Yingqiang Ge, Libby Hemphill, andYongfeng Zhang. 2023. War and peace (waragent):Large language model-based multi-agent simulationof world wars.",
  "Ekhine Irurozki, Pierre Colombo, Nathan Noiry, andStphan Clmenon. 2022. What are the best sys-tems? new perspectives on nlp benchmarking. InConference on Neural Information Processing Sys-tems": "Daniel Jarrett, Miruna Pislar, Michael Tessler, MichielBakker, Raphael Koster, Jan Balaguer, RomualdElie, Christopher Summerfield, and Andrea Tacchetti.2023. Language agents as digital representatives incollective decision-making. In NeurIPS 2023 Foun-dation Models for Decision Making Workshop. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023a. Mistral 7b.",
  "Guohao Li, Hasan Abed Al Kader Hammoud, HaniItani, Dmitrii Khizbullin, and Bernard Ghanem.2023a. Camel: Communicative agents for \"mind\"exploration of large language model society": "Haoyuan Li, Hao Jiang, Tianke Zhang, Zhelun Yu, Aox-iong Yin, Hao Cheng, Siming Fu, Yuhao Zhang, andWanggui He. 2023b. Traineragent: Customizableand efficient model training through llm-poweredmulti-agent system. Huao Li, Yu Chong, Simon Stepputtis, Joseph Camp-bell, Dana Hughes, Charles Lewis, and Katia Sycara.2023c. Theory of mind for multi-agent collabora-tion via large language models. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 180192, Singapore.Association for Computational Linguistics.",
  "Noah Shinn, Federico Cassano, Edward Berman, Ash-win Gopinath, Karthik Narasimhan, and Shunyu Yao.2023. Reflexion: Language agents with verbal rein-forcement learning": "David Silver, Julian Schrittwieser, Karen Simonyan,Ioannis Antonoglou, Aja Huang, Arthur Guez,Thomas Hubert, Lucas Baker, Matthew Lai, AdrianBolton, et al. 2017. Mastering the game of go withouthuman knowledge. nature, 550(7676):354359. SimonStepputtis,JosephCampbell,YaqiXie,Zhengyang Qi, Wenxin Zhang, Ruiyi Wang, SankethRangreji, Charles Lewis, and Katia Sycara. 2023.Long-horizon dialogue understanding for role iden-tification in the game of avalon with large languagemodels. In Findings of the Association for Compu-tational Linguistics: EMNLP 2023, pages 1119311208, Singapore. Association for ComputationalLinguistics.",
  "Daniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song,Haoye Tian, Saad Ezzini, Yongfeng Huang, JacquesKlein, and Tegawende F. Bissyande. 2024. Collabo-rative agents for software engineering": "Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang,Yang Liu, Tianshui Chen, and Liang Lin. 2023. To-wards causalgpt: A multi-agent approach for faithfulknowledge reasoning via promoting causal consis-tency in llms. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Michael Wooldridge. 2009. An introduction to multia-gent systems. John wiley & sons": "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-lah, Ryen W White, Doug Burger, and Chi Wang.2023. Autogen: Enabling next-gen llm applicationsvia multi-agent conversation. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, YiwenDing, Boyang Hong, Ming Zhang, Junzhe Wang,Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,Xiao Wang, Limao Xiong, Yuhao Zhou, WeiranWang, Changhao Jiang, Yicheng Zou, XiangyangLiu, Zhangyue Yin, Shihan Dou, Rongxiang Weng,Wensen Cheng, Qi Zhang, Wenjuan Qin, YongyanZheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.2023. The rise and potential of large language modelbased agents: A survey. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and BingQin. 2023. Examining inter-consistency of large lan-guage models collaboration: An in-depth analysis viadebate. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 75727590,Singapore. Association for Computational Linguis-tics. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-aolong Wang, Weidong Liu, and Yang Liu. 2023a.Exploring large language models for communica-tion games: An empirical study on werewolf. arXivpreprint arXiv:2309.04658.",
  "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik Narasimhan, and Yuan Cao. 2023.React: Synergizing reasoning and acting in languagemodels": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. 2023. GLM-130B:an open bilingual pre-trained model. In The EleventhInternational Conference on Learning Representa-tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang,Guanghe Li, Yihang Sun, Cheng Zhang, ZhaoweiZhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,Junge Zhang, Feng Yin, Yitao Liang, and YaodongYang. 2023a. Proagent: Building proactive coopera-tive ai with large language models.",
  "For MMLU and MMLU-Pro datasets, we curatesubject-wise balanced test subsets by selecting first100 cases of each subject (i.e., discipline). Thus,the subset contains 5,700 questions for MMLU and": "1,400 for MMLU-Pro. Regarding ARC-Challenge,the whole test set of 1,172 cases are used.We consider a profile to be valid if (1) the profilecomprises ballots from all voting agents, and (2) ev-ery ballot includes a complete and non-duplicatedranked list of choices and matches the instructedformat. Only valid profiles are forwarded to GEDIand processed. The statistics of main experimentsare summarized in .",
  "ARC-Challenge": "mistral-7b373103311311163llama-3-8b25231710241043glm-4-9b1 (1096*)108111531159llama-3-70b901113511721172qwen1.5-72b1068117211721172qwen1.5-110b1166116911711171gpt-3.5-trubo1172117211721172gpt-41172117211711172 : Overview statistics of output profile validity ofdifferent models on tested datasets. Specifically, sincevoting profiles of all non-dictator agents is a prerequisitefor informed dictatorial, we filter out incomplete pro-files of other agents before feeding them to the dictator.Therefore, the valid profile counts for informed dictato-rial are bound to be fewer than the original ones. *Sincellama-3-8b and glm-4-9b yield too few complete pro-files under range voting for certain benchmarks, weutilize incomplete profiles with valid ballots to calculatethose accuraies in the main experiments.",
  "Agent Votes": ": An example of violating monotonicity crite-rion (Woodall, 1997) in preferential instant-runoff vot-ing (IRV): repeatedly eliminating the option with theleast first preference votes each round until a winner isleft. In Scenario 2 (right), two agents alter their votes byputting Coral first, but this favorable action actuallyharms Coral and prevent it from supposed winning.",
  "3": "Agent VotesPlurality Winner Breakdown into pairwise comparisons AggregationCondorcet Winner : An example of plurality voting violating Con-dorcet criterion. While Blue is the plurality winner forgetting the most first-preference votes, Amber is actu-ally the Condorcet Winner, meaning that Amber getsmore preferential votes in every pairwise-comparisonwith other alternatives. This misalignment is due to thatplurality voting takes only first-preference into account.",
  "Aggregation": "simplify : An example of utilitarian method violatingMajority and Condorcet criteria. Blue is the utilitarianwinner for getting more utilities from votes, but Amberis preferred by the majority of the agents (10 out of12). In addition, Amber is also the Condorcet Winner,meaning that Amber gets more preferential votes inpairwise-comparison with other alternatives."
}