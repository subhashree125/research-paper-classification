{
  "Abstract": "Large language models (LLMs) have emergedas valuable tools for enhancing textual featuresin various text-related tasks. Despite their su-periority in capturing the lexical semantics be-tween tokens for text analysis, our preliminarystudy on two popular LLMs, i.e., GPT-3.5 andLlama2, shows that simply applying news em-beddings from LLMs is ineffective for fakenews detection.Such embeddings only en-capsulate the language styles between tokens.Meanwhile, the high-level semantics amongnamed entities and topics, which reveal the de-viating patterns of fake news, have been ig-nored. Therefore, we propose a topic model to-gether with a set of specially designed promptsto extract topics and real entities from LLMsand model the relations among news, entities,and topics as a heterogeneous graph to facilitateinvestigating news semantics. We then proposea Generalized Page-Rank model and a consis-tent learning criterion for mining the local andglobal semantics centered on each news piecethrough the adaptive propagation of featuresacross the graph. Our model shows superiorperformance on five benchmark datasets overseven baseline methods and the efficacy of thekey ingredients has been thoroughly validated.",
  "Introduction": "The ubiquity of fake news on social media posesa significant threat to public discourse and soci-etal well-being (Prieur et al., 2023; Chen et al.,2023; Ma et al., 2024). To alleviate the far-reachingconsequences, many fake news detection methodsprobe the information dissemination process or so-cial structure (Mehta et al., 2022; Ma et al., 2021)to detect fake news. Unfortunately, despite theimpressive detection performance, their applica-bility is substantially constrained when the socialcontext is unavailable or incomplete due to the",
  ": Irregular co-occurrence of meaningful entitiesin fake news on a specific topic (red arrows)": "evolving nature of social networks and data privacyconcerns (Zhou and Zafarani, 2020; Zhang andGhorbani, 2020). Facing limited access to socialcontext, other text-mining methods (Yang et al.,2016; Zhang et al., 2024) investigate the intrica-cies of news content to uncover hierarchical textualsemantics (e.g., sentence and document level se-mantics) and formulate fake news detection as aclassification problem, using only textual contentfrom the social media.Following the latter approach, in which news em-beddings are critical for providing a discriminatorydescription of authentic and fake news, we are pro-pelled to enhance them with Large Language Mod-els (LLMs), which have been renowned for theirremarkable capabilities in language understanding,and context modeling (Thota et al., 2018; Zhaoet al., 2023; Li et al., 2024b). A fundamental ques-tion that guides our research in this under-exploredrealm is, Are the LLMs output news embeddingseffective for fake news detection?\"To answer this question, we conducted a pre-liminary study by comparing the detection perfor-mance of an MLP classifier trained using newsembeddings extracted from GPT-3.51, Llama22,BERT (Kenton and Toutanova, 2019) and Het-eroSGT (Zhang et al., 2024), respectively. Fromthe results depicted in (and ), we",
  ": A comparison between fake news detectionperformance on two datasets w.r.t. accuracy, precision,recall and F1 score": "found that simply applying the LLMs and BERTextracted news embeddings is ineffective for fakenews detection because they primarily focus on lex-ical semantics between tokens. When fake newsmimics the language styles of authentic news, thisapproach fails. On the other hand, the better performance of arecent method, HeteroSGT, which investigates thehigh-level semantic relations among news, entities,and topics for fake news detection, affirms previousfindings that the knowledge of real entities and top-ics is crucial for identifying fake news (Huang et al.,2019; Xie et al., 2023; Jeong et al., 2022). Takennews #2 depicted in as an example, it is fakebecause the named entity Genetically modifiedcrops is not responsible for COVID-19 whendiscussing the #Spread of COVID-19. These dis-coveries signify high-level semantics for fake newsdetection, however, two further sub-problems exist:P1. How can we apply LLMs to explore high-level news semantics? From the above study, weaffirm that the exploration of high-level semanticsenables the model to acquire a better perception ofdeeper contextual nuances, which encompass fabri-cated knowledge among entities with real meaningon a particular topic (Zhang et al., 2024), for dis-tinguishing fake news. We identify the keys forhigh-level semantics exploration using LLMs areto extract meaningful entities and topics. P2. How can we identify the irregular semanticsin fake news? Given the LLM-derived entities andtopics, one can aggregate their features to enhancethe centered news embeddings for fake news detec-tion. But this primarily focuses on the informationwithin individual news pieces (local semantics),lacking the ability to explicitly explore the broaderrange of knowledge across news pieces (global se-mantics) to identify narrative inconsistencies andmanipulations in fake news. For example, in de-tecting news #2 as fake, we identify the relationbetween COVID-19 and Genetically modified",
  ": Overview of existing methods. Comparisonsare made upon the source information, the semanticseach method explores, and how they enforce learningon unlabeled data": "crops to be irregular because they rarely co-appearin other news discussions about the #Spread ofCOVID-19. Therefore, to identify the deviatingsemantic patterns of fake news, it is crucial to inves-tigate both the local semantics of individual articlesand the global semantics across news pieces. To address P1, by prompting LLMs for entity ex-traction, we first propose a refined topic model thatsummarizes news topics through LLM-generatedembeddings. We then construct a heterogeneousgraph to model the relationships among news, en-tities, and topics by representing them as nodesand connecting them with edges, which facilitatesfurther exploration of local and global news seman-tics. For P2, we apply short- and long-scale featurepropagation centered on news nodes to encapsulatethe local and global semantics into news representa-tions. With these two scales of feature propagation,we can identify inconsistencies between each indi-vidual news text and the broader knowledge acrossnews, and involve unlabeled news for training withour specially designed consistency training crite-rion. Our major contributions are:",
  "Fake News Detection": "Current investigations into fake news detection canbe categorized into content-based and graph-basedmethodologies, in terms of their focus on specificaspects of news articles for feature mining. Specifi-cally, the content-based methods concentrate on an-alyzing the textual content of news articles, extract-ing linguistic, syntactic, stylistic, and other textualfeatures to differentiate between genuine and fakenews. For example, Horne and Adali (2017) andKaliyar et al. (2021) analyzed the language styles todistinguish between fake and real news while Yanget al. (2016) introduced a dual-attention model toexplore hierarchical news semantics. Other worksalso explored the incorporation of supplementarytextual information, such as comments (Shu et al.,2019; Rao et al., 2021), and emotion signals (Zhanget al., 2021), to further improve detection capabili-ties. These content-based methods strive to explorediverse textual features associated with each singlearticle to identify their authenticity. However, thedetection performance is compromised when fakenews is specially fabricated to mimic the wordsand language styles of genuine news, which inher-ently necessitates the need to explore higher-levelsemantics, such as the relations among news, realentities, and topics that are explored in this paper.Moving beyond the content-based methods,graph-based methods explicitly model and learnpotential structures (Ding et al., 2022, 2024), suchas word-word relations (Yao et al., 2019; Linmeiet al., 2019; Li et al., 2023), news disseminationgraphs (Ma et al., 2018, 2023; Bian et al., 2020),and social structure (Su et al., 2023; Dou et al.,2021). Concrete examples under this category in-clude: Yao et al. (2019) which first constructed aweighted graph using the words within the newscontent and then applied the graph convolutionalnetwork (GCN) for classifying fake news; Linmeiet al. (2019) that built a similar graph but employeda heterogeneous graph attention network for classi-fication (Linmei et al., 2019); and Bian et al. (2020)which employed recurrent neural networks and bi-directional GCN to capture the new features fromtheir propagation process. There are other worksthat model the relations between news and users(Su et al., 2023; Dou et al., 2021), or even newsand external knowledge sources (Hu et al., 2021;Xu et al., 2022; Xie et al., 2023; Wang et al., 2018)to complement fake news detection. Despite their progress, the reliance on supplementary sourcesposes a notable challenge in their applicability, andeven when this auxiliary information is available,the associated computational costs remain an ad-ditional hurdle. For clarity, we compare our workand the existing methods in .",
  "LLMs for Feature Mining": "LLMs such as GPT (Brown et al., 2020),Llama2 (Touvron et al., 2023), and pre-trained lan-guage models like BERT (Kenton and Toutanova,2019) have emerged as powerful tools for featuremining due to their remarkable adaptability in lan-guage understanding and sentiment analysis (Minet al., 2023; Liu et al., 2023; Wu and Ong, 2021).LLMs for feature mining primarily focus on en-riching the embeddings of texts. The most straight-forward application involves feeding the outputfeatures into specific models for tasks such as timeseries analysis and graph learning (Jin et al., 2023).To get more specific information and further en-rich the textual features, more advanced methodsprompt LLMs to generate supplementary content,such as related knowledge and background infor-mation (Min et al., 2023). This additional contentis then combined with the original texts for down-stream modeling (He et al., 2023; Li et al., 2024a).In summary, LLMs showcase their potential foradvancing various natural language processing-related tasks, and this paper addresses the twoprior recognized sub-problems to take advantageof LLMs for fake news detection.",
  "Preliminaries": "DEFINITION 1. Heterogeneous Graph. A het-erogeneous graph HG = {V, L, X} models theintricate relations (in L), among diverse types of in-stances in V. For fake news detection, our node setV = {ni}|N|i=0 {ei}|E|i=0 {ti}|T|i=0 comprises threedistinct types of nodes: news nodes (N), entitynodes (E) and topic nodes (T). Each link/edge inL denotes the explicit relation between two nodes.X = {Xn, Xe, Xt} encompasses the feature vec-tors for all nodes, in which Xn R|N|d is thenews node feature matrix, Xe R|E|d for entitiesand Xt R|T|d for topics.DEFINITION 2. Fake News Detection. In thispaper, we define fake news detection as to learn amodel M() using the text of both labeled news(NL, YL) and unlabeled news NU, to infer the la-",
  "LLM-Enhanced Semantics Modeling": "News articles naturally encompass various entitieswith real meaning, such as people, locations, andorganizations, and usually focus on specific topics.These named entities and topics comprise rich high-level semantic information and narratives aboutnews articles, which are crucial for identifying thenuance of fake news. Driven by our preliminarystudy results, as depicted in , we further in-vestigate LLMs, particularly GPT-3.5 and Llama2,to address our devised P1 as follows. For brevity,we use LLM to denote GPT-3.5 or Llama2. Entity Extraction.For news entity extraction,we prompt the LLM following for identi-fying specific entities in all news pieces includingpersons, dates, locations, organizations, and mis-cellaneous entities3. News and Entity Embedding.We obtain thenews embeddings and entity embeddings by di-rectly querying the API provided by OpenAI2 andMeta3 to encode the corresponding lexical seman-tics in the text. The resulting news embeddings areprocessed as Xn, and the entity embeddings arestored in Xe. Topic Modeling.In addition to entities, model-ing the topics across news pieces not only enablesus to summarize the news focus and link differentnews pieces, but also to explore the relation be-tween the target news and entities in another news,",
  ". Notably, we only input the widely-used and publicly avail-able datasets for querying the LLM in case of any privacy andethical concerns": "PROMPT:# TaskExtract the following entities from the given news article:1. PERSON: Person Definition. 2. DATE: DATE Definition.3. LOC: LOC Definition. 4. ORG: ORG Definition.5. MISC: MISC Definition.Return the results in a dictionary with corresponding keys.# ExamplesExample 1: \"The iPhone, created by Apple Inc., was released onJune 29, 2007.\"Output1: \"PERSON\": [\"None\"], \"DATE\": [\"June 29, 2007\"],\"LOC\": [\"None\"], \"ORG\": [\"Apple Inc.\"], \"MISC\": [\"iPhone\"]Examples 2: ...Output2: ...# Input News ArticleGiven news article: < The SpaceX CEO, Elon Musk, announcesambitious plans to build a self-sustaining underwatercity on Mars by Dec 2030 ...>GPT-3.5:\"PERSON\": [\"Elon Musk\", ... ],\"DATE\": [\"Dec 2030\", ... ],\"LOC\": [\"Mars\", ... ],\"ORG\": [\"SpaceX\", ... ],\"MISC\": [\"CEO\", ... ]",
  ": Prompt for entity extraction": "as supported by the empirical results in Sec. 4.3.For involving the topic information for fake newsdetection, we adopt Bertopic (Grootendorst, 2022)to derive the topics involved in all news, whichtypically outputs the topic words and the corre-sponding weights for each topic. We then feed thetopic words into the API call to extract their embed-dings from LLM and formulated the embeddingof each topic as the weighted sum of topic wordswithin it following:",
  "jB(ti)wj,thj;xti Xt,(1)": "where B(ti) is the topic word list output byBertopic, wj,t is the corresponding weight of wordj to topic ti, and hj is the topic word embeddingfrom LLM.For replication purposes, we detail the practicalsettings in entity extraction, embedding, and topicmodeling in Sec. 4, accompanied by an in-depthanalysis of their empirical impact. Heterogeneous Graph Construction.Given thenews pieces, entities, topics, and their correspond-ing embeddings, we then follow Definition 1 andconstruct a heterogeneous graph HG, in which weconsider two types of explicit relations: <news,contains, entity> and <news, focuses on, topic>.In summary, we construct a heterogeneous graph,HG, to capture: 1) high-level relationships amongnews items, entities, and topics, represented asedges; and 2) sentence/document-level narrativesencapsulated within the embeddings of news items, entities, and topics, denoted by X. This approachaddresses our recognized P1 and facilitates a thor-ough examination of local semantics around eachnews item, exemplified by the 1-hop or 2-hop sub-graphs centered on news nodes in HG, as well asglobal semantics across broader ranges, all empow-ered by LLM.",
  "Generalized Feature Propagation": "Given HG, we propose to learn fine-grained newsrepresentations by encapsulating the valuable infor-mation in entities, topics, and other similar newsthat share common topics or entities. It is worthnoting that we highlight the significance of explor-ing these high-level semantics not only becauseof the preliminary results reported in , butalso regarding the consensus that fake news carriesfalse knowledge about real entities on a particulartopic (Zhou and Zafarani, 2020). Therefore, wetake news, entities, and topics into account so as todistinguish the nuances of fake news.We propose to use Generalized PageRank (GPR)for propagating the features of entities, topics, andother news pieces to the target, by simply learninga weighing scalar for each propagation step. Tobe specific, we first apply a two-layer MLP, f(),and project the news, entities, and topics featuresinto the same space following H = f(X), andX = [Xn, Xe, Xt] is the vertical stack ofthe three feature matrices. As to facilitate featurepropagation, we then unify the index of all threetypes of nodes based on their index in X and trans-form the heterogeneous graph structure into a ho-mogeneous adjacency matrix, A, with regard to theedges in HG and by adding self-loops. A particularelement A[i,j] = 1 if there exists an edge betweennodes i and j in HG.With the projected node features H and adja-cency matrix A, we can promptly propagate thefeatures following:",
  "s=0wsHs,(3)": "where ws is a learnable weight corresponding tostep s and the value can be either positive or nega-tive, indicating how the information at a particularstep contributes to the prediction. Thus, the learnednews representations comprise the high-level se-mantics information within S steps, and the prob-abilities of a news piece being authentic or fakeis predicted as pi = softmax(zi),which can bedirectly applied to enforce the learning of andw using the cross-entropy loss on labeled news.However, this only preserves the semantics withina particular scale S.",
  "Global and Local Semantics Mining": "During feature propagation, a larger step allows theexploration of global semantics across HG sinceneighbors across broader ranges are involved, whilea smaller step stresses more the local semantics be-tween the target news piece and its highly relatedentities, topics, and news. Both scales of seman-tics offer complementary perspectives on the targetnews and we can firmly apply two divergent scalevalues sg and sl to encode the global and local se-mantics into news embeddings, respectively. Bysetting a small step sl (e.g., 2) and a larger stepsg (e.g., 20), we can obtain two representations,zli Zl and zgi Zg for each news pieces fol-lowing Eq. (3). Indeed, these representations canbe viewed as two divergent augmentations of thenews pieces from the perspective of data augmen-tation, and we enforce the cross-entropy loss onboth views to train the model on the labeled news,which is to minimize:",
  "D(pi||pli) + gD(pi||pgi ),": "(6)where D() measures the KL-divergence.Notably, our model design features an end-to-end optimization of both the scale weights (w) andthe MLP parameters (). The inclusion of this con-sistency loss not only regularizes the propagationof more valuable features into new representations- capturing both local and global semantics effec-tively; but also enhances the detectors generaliza-tion capabilities on unlabeled data.",
  "Experiment": "Evaluation Dataset. Our evaluation datasets coverdiverse domains, including health-related datasets(MM COVID (Li et al., 2020) and ReCOVery(Zhou et al., 2020)), a political dataset (LIAR(Wang, 2017)), and multi-domain datasets (MCFake (Min et al., 2022) and PAN2020 (Rangel et al.,2020)). Notably, the MC Fake dataset includesnews articles across politics, entertainment, andhealth, sourced from reputable debunking websites,such as PolitiFact4 and GossipCop5. Statistics ofthese datasets are provided in Appendix A.1.Baselines. We compare LESS4FD6 against sevenrepresentative baselines in text classification and",
  "Fake New Detection Performance": "Overall Performance. The results summarizedin Tables 3, and 4, and reveal that ourmethod surpasses all baseline models w.r.t. the fiveevaluation metrics. The performance gaps, whichare over 5% on MM COVID and 2% on the restdatasets, affirm the effectiveness of our approach ininvestigating the LLM-enhanced news semanticsfor fake news detection. It is also worth notingthat there are firm differences between LESS4FD*and LESS4FD, which indicate both GPT-3.5- andLlama2-derived embeddings are effective. By com-parison with different categories of baselines, wealso observe that:High-level Semantic Exploration is Pivotal. De-spite the effectiveness of traditional classifiers likeTextCNN, TextGCN, HAN, BERT, and Sentence-",
  ": Coherence, Diversity, and Sil Score with thedifferent numbers of topics on three datasets": "BERT in capturing word-level narratives, theystruggle with the relationships among news pieces,entities, and topics, limiting their performance. Incontrast, our method, along with HeteroSGT andHGNNR4FD, excels by modeling these high-levelsemantics in a graph and analyzing the relationsand features of news, entities, and topics. Mining the Global and Local Semantics Resultsin the Better Performance. While HGNNR4FDand HeteroSGT employ heterogeneous graphs toanalyze news, entities, and topics, their perfor-mance has deteriorated due to the insufficient ex-ploration of global and local semantics. Specifi-cally, HGNNR4FD only focuses on local seman-tics, while HeteroSGT suffers from informationloss through random walks. Our method addressesthese issues by mining global and local semanticsat lower computational costs (see ).",
  "Topic Modeling Validation": "Topic modeling is pivotal to constructing the HG.In this section, we specifically validate the choicesfor the optimal topic numbers and their impact onthe detection performance.Optimal Topic Number. We use a multi-metricapproach to select the optimal number of topicsfor each dataset, considering topic coherence forinterpretability, topic diversity for variety, and theSilhouette Score for topic separation and compact-ness. The evaluation spans a range of topic num-bers, from 3 to 60. Ideally, the optimal numberof topics corresponds to the point where all threemetrics reach their peak values, but as depicted inFigs. 4 and 10 no point meets this criterion. There-fore, we compromise by selecting six topic num-bers for each dataset, which yield the highest ornear-highest values for at least one metric.The Impact of Topic Numbers on the DetectionPerformance. As depicted in , we observeslight variations in the performance of LESS4FDacross different topic numbers on each dataset,while the optimal topic numbers for each datasetare: 44 for MM COVID, 58 for ReCOVery, 8 forMC Fake, 10 for LIAR, and 40 for PAN2020.",
  ": Ablation results of LESS4FD* on five datasets": "tion; T and E remove topic and entity nodesfrom the graph, respectively; and CR omits theconsistency learning module.From the results in Tables 5 and 10, we observea notable decrement in performance when directlyusing LLM-extracted embeddings for fake newsdetection, exemplified by the case of HG. Af-ter incorporating the heterogeneous graph into thetraining process, as demonstrated by E, T,and CR, the results are enhanced across alldatasets. Such performance gaps before and af-ter engaging with HG further support our motiva-tion to learn high-level semantics for fake newsdetection. Meanwhile, the better performance ofE and T, compared to HG, showcasethat each of them benefits our model from cap-turing the nuances of fake news. As proposed toengage unlabeled news for a fine-gained training ofthe detector, the consistency loss is capable of im-",
  "Further Analysis": "We further study the impacts of different parametersettings and training cost of our news representa-tion learning method. We use LESS4FD* unlessspecified.Scales of Feature Propagation. The scales of fea-ture propagation determine the local and globalsemantics to be explored. Both scales can be ad-justed upon two parameters sl and sg, as presentedin Sec. 3.4. We vary their values and depict theirinfluence in Figs. 7 and 11. It is evident that themodel performs best when sl is around 5 denotingthat the local semantics within 5-hops is optimal,while a larger sg always leads to better performancesince more global information is involved.Impact of ce. This hyperparameter balances theweights of training loss on labeled and unlabelednews. A higher value of ce makes the model em-phasize more on labeled data. To assess its impact,we adjust ce between 0.1 and 0.9 and depict theresults in (a). We see that increasing ceis beneficial to the detection performance, partic-ularly when it remains below 0.4. Beyond this point, marginal fluctuations in performance emergeacross datasets and the optimal range for ce con-sistently lies between 0.4 and 0.6.Impact of g. g is to regularize the training signalfrom the exploration of global semantics. As illus-trated in (b), we find that our model maintainsalmost steady performance despite variations in theweights of global semantics.Impact of Potential Data Contamination. At thetime of this study, all datasets had already beenpublished before the LLMs training date and theymight have been involved in tuning the textualtokens in LLMs. However, for our task of fakenews detection, we clarify that such potential datacontamination merely impacts our research find-ings because: 1) The LLMs we use, specificallyGPT-3.5 and Llama2, are primarily trained for text-generation rather than fake news detection; 2) Inour preliminary experiments, as reported in and , the news embeddings derived fromthese LLMs proved to be ineffective for fake newsdetection; and 3) Through our extensive ablationstudy, we demonstrate that our performance gainsstem from the novel model design of exploringhigh-level semantics as well as the local and globalinformation, which is typically ignored in the tok-enized training text of LLMs.To validate this claim, we further compare theperformance of our method with that of the bestbaseline method, HeteroSGT, by incorporating en-tities, topics, and news embeddings derived fromGPT-3.5 into both models. As both our methodand HeteroSGT utilize the same sets of entities,topics, news, and embeddings, this setup allowsfor a fair comparison of the model designs for fakenews detection. According to the results presentedin , our design consistently demonstratessuperior detection performance.Computational Costs. In addition to the detec-tion performance improvement, we also evaluateLESS4FDs efficiency, showcasing reduced timeper training epoch with moderate GPU memoryusage, as detailed in .",
  ": Running time & GPU memory cost": "heterogeneous graph, we then propose an effectivefeature propagation algorithm to encode both thelocal and global semantics into news embeddingsto enrich the training of the detector. Through ex-tensive experiments on five widely-used datasets,our method demonstrates better performance thanseven baseline methods while the efficacy of keyingredients is further validated in the case studies.Limitations. In this work, we only adopt the twomost popular LLMs as enhancers to explore thenews semantics. Extending our method to tuningLLMs, particularly for fake news detection is animportant direction for future efforts.Ethical issues. The datasets utilized in our re-search for detecting fake news are widely accessedand publicly available for academic research. Ourproposed method exclusively relies on the textualcontent of news articles from these datasets as in-put, without requiring any additional user-specificinformation (e.g., personal identifiers) or user so-cial information (e.g., retweet/comment behavior).We employed publicly accessible APIs providedby OpenAI and Meta to obtain embeddings. Ourprompts, which are made publicly available, areused exclusively for extracting entities and topicsfrom LLMs. Therefore, our method ensures mini-mal risk of privacy infringement.Applications. Detecting fake news is criticaldue to its significant implications for society, poli-tics, and individual decision-making. Our proposedmodel demonstrates efficacy in distinguishing au-thentic and false content, which could contribute tomitigate the spread of false information and publicdistrust.",
  "Qi Huang, Chuan Zhou, Jia Wu, Mingwen Wang, andBin Wang. 2019. Deep structure learning for rumordetection on twitter. In IJCNN": "Ujun Jeong, Kaize Ding, Lu Cheng, Ruocheng Guo, KaiShu, and Huan Liu. 2022. Nothing stands alone: Re-lational fake news detection with hypergraph neuralnetworks. In 2022 IEEE International Conferenceon Big Data (Big Data). Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang,Siqiao Xue, Xue Wang, James Zhang, Yi Wang,Haifeng Chen, Xiaoli Li, et al. 2023. Large mod-els for time series and spatio-temporal data: A surveyand outlook. arXiv preprint arXiv:2310.10196.",
  "A.3Baselines": "For a fair evaluation of the overall detection per-formance and considering the availability of addi-tional sources, we compared LESS4FD with sevenrepresentative baseline algorithms including:textCNN (Kim, 2014) is designed to capture local-ized patterns and features within input texts. It uti-lizes Convolutional Neural Network layers (CNNs)to small windows of words in the text to extractpatterns and features for news classification.textGCN (Yao et al., 2019) represents input textsas nodes in a graph, employing graph convolutionaloperations on both the textual content of each doc-ument and the graph structure. This process aimsto learn effective representations for fake news de-tection.HAN (Yang et al., 2016), or Hierarchical AttentionNetwork, employs attention mechanisms to repre-sent intricate relationships at both word-sentenceand sentence-article levels, enhancing its abilityto capture hierarchical features for improved fakenews detection performance.BERT (Kenton and Toutanova, 2019) is a promi-nent transformer-based language model. In ourexperimentation, we utilize the embedded represen-tation of the [CLS] token from BERT for the taskof fake news classification.SentenceBERT (Reimers and Gurevych, 2019)is an extension of BERT that is specifically de-signed for sentence embeddings. It uses siameseand triplet network structures during training togenerate semantically meaningful sentence embed-dings HGNNR4FD (Xie et al., 2023) models news ar-ticles in a heterogeneous graph and incorporatesexternal entity knowledge from Knowledge Graphsto enhance the learning of news representations forfake news detection.HeteroSGT (Zhang et al., 2024) proposes a hetero-geneous subgraph transformer to exploit subgraphsin the news heterogeneous graph that contains rela-tions between news articles, topics, and entities.",
  "A.4Hyperparameter and ComputationalSettings": "Hyperparameters.For constructing HG, wechoose the optimal number of topics |T| for eachdataset through the comprehensive topic modelevaluation detailed in Sec. 4.2. For a fair com-parison between LESS4FD* and LESS4FD, weuse the same set of entities, topics, and their embed-dings from GPT-3.5, while the news embeddingsare derived from GPT-3.5 and Llama2, respectively.We perform a grid search to determine the remain-ing hyperparameters, with the search space definedas follows:Feature propagation scale sl: Feature propagation scale sg: Trade-off parameter g: [0.1, 0.9]Cross-entropy loss weight ce: [0.1, 0.9]",
  "A.5Addition Experimental Results": "Optimal Topic Number. We depict the Coher-ence, Diversity, and Silhouette Score with differentnumbers of topics on ReCOVery and MC Fake in and similar to that on MM COVID, LIAR,and PAN2020, no point meets the criterion whereall three metrics reach their peak values.Fake News Detection Performance. From Ta-bles 4 and 3, we see that our proposed methodLESS4FD performs better than all baseline meth-ods. To demonstrate the statistical significance ofperformance improvement, we conduct further pair-wise t-test at a 95% confidence level (a = 0.05).The results in Tables 11, 12, 13, and 14 show thatthe performance improvement is significant.Ablation Study. In addition to the ablation studyon LESS4FD, we report the results on LESS4FD"
}