{
  "Abstract": "Language models struggle with handling nu-merical data and performing arithmetic opera-tions. We hypothesize that this limitation canbe partially attributed to non-intuitive textualnumbers representation. When a digit is reador generated by a causal language model itdoes not know its place value (e.g. thousandsvs. hundreds) until the entire number is pro-cessed. To address this issue, we propose asimple adjustment to how numbers are repre-sented by including the count of digits beforeeach number. For instance, instead of \"42\",we suggest using \"2:42\" as the new format.This approach, which we term NumeroLogic,offers an added advantage in number genera-tion by serving as a Chain of Thought (CoT).By requiring the model to consider the numberof digits first, it enhances the reasoning pro-cess before generating the actual number. Weuse arithmetic tasks to demonstrate the effec-tiveness of the NumeroLogic formatting. Wefurther demonstrate NumeroLogic applicabilityto general natural language modeling, improv-ing language understanding performance in theMMLU benchmark. 1IntroductionLarge Language Models (LLMs) struggle with nu-merical and arithmetical tasks. Despite continu-ous improvements, even the most advanced modelslike GPT-4 (Achiam et al., 2023) still exhibit poorperformance when confronted with tasks such asmultiplying 3-digit numbers (Shen et al., 2023). Re-cent studies ((Lee et al., 2024; Shen et al., 2023))have proposed techniques to improve arithmetic inLLMs, such as the Chain of Thought (CoT; (Weiet al., 2022)) method, which pushes the model toanticipate the entire sequence of algorithmic stepsrather than just the final output. While these strate-gies offer valuable insights into the capabilities ofLLMs, they primarily concentrate on post-hoc solu-tions for specific arithmetic challenges and do notpresent a practical solution for pretraining LLMs. : Reading numbers in a causal manner from leftto right is sub-optimal for LLMs, as it is for humans.The model has to reach the final digits of a numberbefore it can infer the place value of the first digit. Toaddress this, we propose NumeroLogic\", a numericalformat where digit count is indicated before the actualnumber. Image by DALL-E 3 (Betker et al., 2023).",
  "Our research, however, focuses on solutions ap-plicable to self-supervised language modeling ingeneral, utilizing arithmetic exercises primarily forevaluating their impact": "We hypothesize that one of the challenges LLMsface when dealing with numerical tasks is the tex-tual representation of numbers. In todays mostpopular decoder-based LLMs, each token attendsonly to previous tokens. When a model reads\"a token representing a digit (or multiple digits) itcannot tell its place value, i.e. 1 can represent 1million, 1 thousand, or a single unit. Only whenreaching the end of the number might the model up-date its representation of the previous digit tokensto be related to their real place value.",
  "To address this issue, we propose a straight-forward reformatting technique called \"Numero-Logic,\" which involves adding the number of digits": "as a prefix to numbers. This lets the model knowin advance what is the place value of a digit beforeit is read. This simple change also offers anotherbenefit, when the model is generating a number itneeds to first reason about what is going to be thenumber of digits. This acts as a Chain of Thought(CoT) (Wei et al., 2022), encouraging the model toperform some reasoning before it begins to predictdigits. Implementing the suggested reformattingdoes not necessitate any alterations to the modelsarchitecture; it can be accomplished through textpre- and post-processing based on regex.We demonstrate that NumeroLogic enhances thenumerical abilities of LLMs across both small andlarger models (up to 7B parameters).This en-hancement is showcased through supervised train-ing on arithmetic tasks and its application in self-supervised causal language modeling to enhancegeneral language comprehension.",
  "Related Work": "Recently, there has been a significant interest in en-hancing the numerical capabilities of LLMs. Oneapproach to investigating these capabilities is by as-sessing their performance in arithmetic tasks. Sev-eral recent studies have proposed methods to en-hance performance in these tasks. One strategyinvolves reversing the expected result order fromthe least to the most significant digit (Lee et al.,2024). Another strategy is using an elaborated CoTwhere the model is taught to predict all steps of analgorithm predefined for each arithmetic task (Leeet al., 2024). In (Shen et al., 2023), it is noted thatthe model learns to rely too heavily on positionalencoding when trained for a specific arithmetic task.They suggest ways to overcome it, e.g. adding ran-dom white spaces in the middle of numbers. Thesestudies aim to enhance the performance of arith-metic tasks by offering tailored solutions to theassociated challenges. In contrast, our focus is onidentifying solutions that benefit general languagemodeling rather than just arithmetic tasks, witharithmetic tasks being used solely for measuringimprovements.Another aspect important for LLMs numericalcapabilities is the tokenization process. The com-monly used Byte Pair Encoding (BPE) based meth-ods (Gage, 1994; Sennrich et al., 2015) for to-kenization are based on the corpus distributionand can split a number to tokens in unintuitiveways. Different foundation models took differentapproaches when dealing with number tokeniza- tion. PaLM (Chowdhery et al., 2023), Llama (Tou-vron et al., 2023), and Mistral (Jiang et al., 2023)force each digit to have a single token. GPT-3.5and GPT-4 define a token for each up to 3-digitnumber (Achiam et al., 2023). Somewhat relatedto our work, in (Singh and Strouse, 2024), theyhighlighted an issue with the GPT approach. Theyshow that dividing large numbers into 3-digit seg-ments from left to right undermines arithmetic per-formance. They suggest overcoming it by insertingcommas between digits to control the splitting. An-other related work, is (Kim et al., 2021). Theyfocus on the extrapolation ability of LLMs to un-seen numbers and use a special number encodingthat lets the LLM know the digit place-value.",
  "NumeroLogic": "We introduce NumeroLogic, a technique for boost-ing causal LLMs numerical capabilities. The con-cept involves adding a digit count before numbers,enabling the model to know the place values ofdigits before reaching the final digits of a num-ber. Additionally, the model needs to predict thetotal number of digits before generating a number,acting as a simplified CoT, prompting it to reasonabout the number that is going to be generated.We add special tokens to help representnumberswiththenumber-of-digitprefix,\"<startnumber>\",\"<midnumber>\",and\"<endnumber>\"(or,forsimplicity,\"<sn>\",\"<mn>\", and \"<en>\").For floating points, theprefix includes both the number of digits of theinteger part and the decimal part. For example,\"42\" is replaced by \"<sn>2<mn>42<en>\" and\"3.14\" is replaced by \"<sn>1.2<mn>3.14<en>\".When using the LLM to generate numbers, wedisregard the information about the number ofdigits and only retain the generated number itself.Although not within the scope of this study, it maybe feasible to leverage the additional informationto identify discrepancies, wherein the modelpredicts a certain digit count but produces anumber with a different count of digits.For small transformers, we train all parametersfrom scratch with character-level tokenization. Forsmall transformers, we also replace the special to-kens with single characters, \"<sn>\", \"<mn>\", and\"<en>\" are replaced with \"{\", \":\", and \"}\", re-spectively. For larger transformers, we start frompre-trained models. We add the new special tokensto the tokenizers vocabulary and expand the em-bedding layer and the final fully connected layer to fit the new vocabulary size. When continuing train-ing on causal language modeling or fine-tuning onsupervised arithmetic tasks, we use low-rank adap-tation (LoRA) (Hu et al., 2021). We apply LoRAfor the attention block projection matrices (Q, K,V, O) and train the modified embedding layer andthe final fully-connected layer in full rank.The NumeroLogic approach includes basic textpre-processing and post-processing steps that oc-cur before and after the tokenizers encoding anddecoding methods, respectively. Both can be im-plemented based on regular expressions:def preprocess_all_numbers(text):def f(match):num = match.group (0)i = match.group (1)li = len(i)d = match.group (3)ld = len(d) if d else 0if d:prefix = f'<sn >{li}.{ld}<mn>'else:prefix = f'<sn >{li}<mn >'return prefix + num + '<en>'pattern = '(\\d+)(\\.(\\d+))?'return re.sub(pattern , f, text)",
  "Experiments": "To test the effect of NumeroLogic we conductedseveral experiments. First, we tested supervisedtraining of a small language model (NanoGPT) onvarious arithmetic tasks. We then test the scalabil-ity to larger models (Llama2-7B). Finally, we testself-supervised pretraining of Llama2-7B, with thesuggested formatting, and test on general languageunderstanding tasks. 4.1Arithmetic tasks with small modelWe trained NanoGPT (Karpathy, 2022) fromscratch in a supervised manner jointly on 5 arith-metic tasks: addition, subtraction, multiplication,sine, and square root. Addition and subtraction areperformed with up to 3-digit integer operands. Mul-tiplications are performed with up to 2-digit integeroperands. Sine and square root with 4 decimal-places floating point operands and results. Theoperand range for sine is within [/2, /2]. Theoperand range for the square root is within .The model is trained in a multi-task fashion on all5 tasks, with 10K training samples for each taskexcept for multiplication, for which 3K samples",
  ": NanoGPT arithmetic tasks accuracy withNumeroLogic encoding. A single model is jointlytrained for all tasks. The encoding produces high accu-racy gains for all tasks": "are used. We followed the protocol from SectionD.2 in (Lee et al., 2024).Tab. 1 compares the results of training withplain numbers to training with the NumeroLogicencoding. For addition and subtraction, a modeltrained with plain numbers reached 88.37% and73.76% accuracy, respectively, while with the Nu-meroLogic encoding, the tasks are almost solved(99.96% and 97.2%). For multiplication, we ob-serve more than doubling of the accuracy, from13.81% to 28.94%. Furthermore, for the floatingpoint operations, sine and square root, we see asignificant improvement of 4% for both tasks. 4.2Arithmetic tasks with larger modelNext, we test how the method scales to a largermodel. For this experiment, we fine-tune a pre-trained Llama2-7B model (Touvron et al., 2023).In this experiment, we again tested the same fivearithmetic tasks: addition, subtraction, multiplica-tion, sine, and square root. For addition (5 digit),subtraction (5 digit), and multiplication (3 digit)we tested on two versions - integers and floatingpoint numbers. For generating a random N-digitfloating point operand we first sample an up to N-digit integer and then divide it by a denominatoruniformly sampled from100, 101, ..., 10N. Foreach of the addition, subtraction, and multiplica-tion tasks, we generated 300K random equationsas a training set. The sine and square root operandsand results are generated with 5 decimal place ac-curacy, we generated 30K random equations forthe training sets of these tasks. Since we are work-ing with a pretrained model we add new tokens(\"<sn>\", \"<mn>\", and \"<en>\") to the tokenizersvocabulary. We finetune one model per task withLoRA (Hu et al., 2021) (rank 8), we also train infull-rank the embedding layer and the final linearlayer since they are extended to fit the larger vocab.size.The results are presented in Tab. 2. Additionand subtraction of integers are mostly solved by a",
  ": Llama2-7B arithmetic tasks accuracy withNumeroLogic encoding. We observe significant gainsthanks to the NuemroLogic encoding for all tasks whereperformance is not saturated": "model as large as Llama2-7B even for much largernumbers (e.g. 20-digit). For our 5-digit experi-ments, the plain text baselines reached 99.86% and99.6% performance, for addition and subtraction,respectively. Despite the high performance of plaintext, we still observe an improvement when usingNumerLogic, with a perfect 100% for addition andrectification of more than 80% of the subtractionmistakes, reaching 99.93% accuracy for subtrac-tion. For all other, non-saturated, tasks we observedsignificant gains of 1%-6%. 4.3Self-Supervised PretrainingOur approach differs from other methods in that itis not specialized for a specific task, such as arith-metic, but rather designed for general languagemodeling tasks involving text with numerical val-ues. To test this capability we continue the pre-training of LLama2-7B with the causal text mod-eling objective (next token prediction). We trainon text from the RefinedWeb dataset (Penedo et al.,2023). The goal is to teach the model to read andwrite numbers in the NumeroLogic format withoutforgetting its previously acquired knowledge. Tofacilitate this, we perform the continued pretrain-ing with LoRA. We then test the model in a 0-shotmanner on MMLU (Hendrycks et al., 2021b,a).In , we present the MMLU 0-shot resultsobtained from training the model using plain num-bers versus NumeroLogic encoding on an equalnumber of tokens. While training with plain num-bers does not enhance the models accuracy com-pared to the pretrained model, employing Numero-Logic encoding results in a statistically significantimprovement of 0.5%. The MMLU benchmarkencompasses tasks from diverse domains, someemphasizing analytical skills and numerical com-prehension while others do not. In Tab. 3, we delveinto the impact of NumeroLogic on MMLU taskscategorized by field. As anticipated, tasks in STEM 2M4M6M8M10M12M14M16M18M20M22M",
  "Accuracy": "MMLU Results EncodedPlainOriginal pre-trained : MMLU Accuracy of Llama2-7B. Continuingself-supervised pretraining on web-curated text tokens,when numbers are encoded with NumeroLogic, helpsimprove the performance beyond the pretrained modelor a model trained on the same text with plain numbers. fields exhibit more substantial enhancements com-pared to those in social sciences and humanities.Tab. 4 provides a detailed analysis of Numero-Logics performance boost across tasks containingnumbers versus those that do not. Consistently,tasks involving numbers show higher improvement.",
  ": MMLU accuracy change due to NumeroLogicencoding on tasks with and without numbers. Taskswith numbers enjoy higher improvement": "4.4Ablation studies4.4.1Encoding operands vs. resultsWe experimented to test the effect of operand en-coding vs. the expected output (equation result)encoding. Operand encoding primarily influencesthe models comprehension of numerical values inthe input, while result encoding is more associatedwith CoT, prompting the model first reason aboutthe expected number of digits. We repeat the exper-iment from .1, but with the NumeroLogicencoding applied only to the operands or to theresults and report the 3-digit addition results forthe different variants. The results are presented",
  ": Different encoding alternatives performanceon 3-digit integer multiplications": "in Tab. 5. We find that both operands and resultsencodings are beneficial, with a stronger impact at-tributed to encoding the results. Applying Numero-Logic to all numbers, both operands and results,yields the highest level of accuracy. 4.4.2Different EncodingsWe experimented with different formats for provid-ing the number of digits. One alternative we testedis defining a set of new special tokens representingeach possible number of digits, {<1digitnumber>,<2digitnumber>,...}. We observed that the per-formance of having multiple special tokens is evenlower than plain numbers. It might be due to theunbalanced distribution of numbers. E.g. numberswith a single digit are much less frequent in thedata of 3-digit additions, it is possible the modelhas not seen enough single-digit numbers to learn agood representation of the <1digitnumber> token.Another alternative we tested is removing the endof number\" token (<en>), keeping only the numberprefix, e.g. \"<sn>3<mn>100\". This works betterthan plain but slightly worse than the full Numero-Logic encoding. The results are summarized inTab. 6. 4.4.3Is it the extra tokens?It has been shown that the advantage of CoT is atleast partially due to the extra tokens that allowthe model to perform more computations or storeinformation (Pfau et al., 2024). To understand theeffect of the extra tokens we run an experimentwhere all the extra tokens introduced by Numero-Logic are replaced with filler white-space tokens.",
  "NumeroLogic{1:1}*{1:1}={1:1}31.03%White-spaces___1_*___1_=___1_24.37%Random white-spaces____1*__1__=1____27.76%Plain1*1=124.73%": ": Extra tokens effect: Just adding filler whitespace tokens is not helpful and is comparable to theplain format. The random white-space method (Shenet al., 2023) of adding filler tokens at random locationsis helpful but less effective compared to NumeroLogic. Additionally, in (Shen et al., 2023), it has beenshown that the model learns to rely too heavilyon positional encoding when trained on arithmetictasks. It causes failures when the model is testedwith numbers less frequent in the training data (e.g.1 1-digit numbers when the model is trained onup to 3-digit numbers). To deal with this limita-tion, they suggest adding filler white-space tokensat random locations between the digits. We alsoreport the results of their approach (Shen et al.,2023) where we use the same number of tokensas NumeroLogic would have required, just thatthey are replaced with white-space tokens at ran-dom locations. These experiments were performedby finetuning Llama2-7B on 3-digit floating-pointmultiplication. The results are reported in .We observe that just adding the extra tokens doesnot help and the performance is similar to the plainformat. Adding the same amount of extra tokens inrandom locations is somewhat helpful but not as ef-fective as NumeroLogic. It eliminates the modelsreliance on positional encoding but does not pro-vide place-value information like NumeroLogic.",
  "Conclusions": "We introduced NumeroLogic, a novel method toimprove language models handling of numericaldata. Our approach prefixes numbers with theirdigit count, enhancing models understanding ofplace value and prompting better reasoning aboutnumbers magnitude, akin to chain-of-thought rea-soning. We tested NumeroLogic on both arithmeticand broader language understanding tasks. The re-sults showed substantial enhancements in numeri-cal tasks, including integer and floating-point calcu-lations, and in broader modeling contexts like theMMLU benchmark. In summary, NumeroLogicis a straightforward yet effective enhancement forlanguage models numerical abilities, applicableacross various tasks without requiring changes tothe models architecture.",
  "Limitations": "The NumeroLogic encoding, while enhancing nu-merical reasoning, might increase the number oftokens per number. Moreover, it introduces ad-ditional steps in pre- and post-processing. Thisraises the computational costs and also potentiallyincreases the models latency during inference.These factors might impact the efficiency of Nu-meroLogic, especially in numerical-processing-intensive applications.Our experiments predominantly involved fine-tuning pre-trained language models (LLMs) ratherthan training them from scratch with NumeroLogic.While this limits our ability to conclusively predictthe impacts from the pre-training phase, incorporat-ing NumeroLogic early in the pre-training wouldlikely have a stronger positive rather than negativeeffect on the performance. Additionally, our testingdid not extend to models larger than 7B parameters.However, it has been demonstrated that both smalland large models exhibit similar learning behaviors(Warstadt et al., 2023); therefore, it is plausibleto predict that scaling up the model size will notdiminish the effectiveness of NumeroLogic.Lastly, our evaluation was confined to controlledacademic benchmarks, which might not fully repre-sent the complexities of real-world numerical data.Extending testing to diverse, real-world datasets isessential to fully understand NumeroLogics prac-tical effectiveness and ensure it can handle the un-predictable nature of real-world numerical data.Similarly, despite caring mainly about numericalaspects, we checked English-focused datasets anddata. The cross effects with different languages,scripts and even numerical writing system is left asan open question. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,Joyce Lee, Yufei Guo, et al. 2023. Improving imagegeneration with better captions. Computer Science. openai. com/papers/dall-e-3. pdf, 2(3):8.",
  "Philip Gage. 1994. A new algorithm for data compres-sion. The C Users Journal, 12(2):2338": "Dan Hendrycks, Collin Burns, Steven Basart, AndrewCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.2021a. Aligning ai with shared human values. Pro-ceedings of the International Conference on LearningRepresentations (ICLR). Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. 2021b. Measuring massive multitask languageunderstanding. Proceedings of the International Con-ference on Learning Representations (ICLR).",
  "Andrej Karpathy. 2022. Nanogpt": "Jeonghwan Kim, Giwon Hong, Kyung-min Kim, JunmoKang, and Sung-Hyon Myaeng. 2021. Have youseen that number?investigating extrapolation inquestion answering models. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 70317037, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kang-wook Lee, and Dimitris Papailiopoulos. 2024. Teach-ing arithmetic to small transformers. In The TwelfthInternational Conference on Learning Representa-tions. Guilherme Penedo, Quentin Malartic, Daniel Hesslow,Ruxandra Cojocaru, Alessandro Cappelli, HamzaAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,and Julien Launay. 2023. The RefinedWeb datasetfor Falcon LLM: outperforming curated corporawith web data, and web data only. arXiv preprintarXiv:2306.01116.",
  "Aaditya K Singh and DJ Strouse. 2024. Tokenizationcounts: the impact of tokenization on arithmetic infrontier llms. arXiv preprint arXiv:2402.14903": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Alex Warstadt, Aaron Mueller, Leshem Choshen, EthanWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-quera, Bhargavi Paranjabe, Adina Williams, TalLinzen, et al. 2023. Findings of the babylm chal-lenge: Sample-efficient pretraining on developmen-tally plausible corpora.In Proceedings of theBabyLM Challenge at the 27th Conference on Com-putational Natural Language Learning. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837."
}