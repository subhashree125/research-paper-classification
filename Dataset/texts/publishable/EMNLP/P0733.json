{
  "Abstract": "Emotional intelligence (EI) in artificial intel-ligence (AI), which refers to the ability of anAI to understand and respond appropriately tohuman emotions, has emerged as a crucial re-search topic. Recent studies have shown thatlarge language models (LLMs) and vision largelanguage models (VLLMs) possess EI and theability to understand emotional stimuli in theform of text and images, respectively. How-ever, factors influencing the emotion predictionperformance of VLLMs in real-world conver-sational contexts have not been sufficiently ex-plored. This study aims to analyze the key el-ements affecting the emotion prediction per-formance of VLLMs in conversational con-texts systematically. To achieve this, we recon-structed the MELD dataset, which is based onthe popular TV series Friends, and conductedexperiments through three sub-tasks: overallemotion tone prediction, character emotion pre-diction, and contextually appropriate emotionexpression selection. We evaluated the per-formance differences based on various modelarchitectures (e.g., image encoders, modalityalignment, and LLMs) and image scopes (e.g.,entire scene, person, and facial expression). Inaddition, we investigated the impact of provid-ing persona information on the emotion predic-tion performance of the models and analyzedhow personality traits and speaking styles in-fluenced the emotion prediction process. Weconducted an in-depth analysis of the impactof various other factors, such as gender andregional biases, on the emotion prediction per-formance of VLLMs. The results revealed thatthese factors significantly influenced the modelperformance.",
  "*Equal contribution.Corresponding author": "derstand and respond appropriately to human emo-tions, is a crucial topic in AI research. EI involvesthe ability to interpret and manage the emotionsthat are embedded in information and is essentialfor various cognitive tasks, from problem solving tobehavior regulation (Salovey et al., 2009). Humanemotions play a significant role in various domainssuch as academics, competitive sports, and dailylife and are shaped by internal and external factors(Koole, 2010; Pekrun et al., 2002; Lazarus, 2000;Li et al., 2023b). Equipping AI systems with EIenhances the quality of human-AI interactions, im-proves user experience, and enables more naturaland effective communication based on emotionalempathy.Large language models (LLMs), which are con-sidered a crucial step towards achieving artificialgeneral intelligence, have exhibited exceptional per-formance in various fields (Bubeck et al., 2023). Asa result, there has been growing interest in whetherLLMs possess EI. Wang et al. (2023) evaluatedthe EI of LLMs through psychological measure-ments and discovered that GPT-4 achieved highEQ scores. Moreover, studies by Li et al. (2023b)and Li et al. (2023c) showed that LLMs can un-derstand emotional stimuli in the form of text andimages, perceiving emotions in a manner similarto humans. However, these studies have limitationsas they focus on a single modality, whereas variousfactors such as verbal cues, visual cues, and con-textual information interact in a complex mannerin real-world conversational situations.Vision large language models (VLLMs) have re-cently gained attention to overcome the above limi-tations. As VLLMs can process text and images si-multaneously, they have the potential to solve morecomplex and multifaceted emotion prediction tasks.For example, VLLMs can infer emotional states bycomprehensively analyzing the facial expressionsand verbal cues of conversation participants or pre-dict appropriate emotional responses considering the conversational context. However, despite theirpotential, the key factors influencing the emotionprediction of VLLMs in conversational situationshave not yet been sufficiently explored.This study aimed to analyze the factors influenc-ing the emotion prediction of VLLMs, such as themodel architecture, persona information, and bi-ases, systematically to explore means of improvingemotion prediction performance in conversationalsituations. To achieve this, we reconstructed theMELD dataset (Poria et al., 2018) based on thepopular TV series Friends and augmented it byintegrating various elements, such as images, con-versational context, and persona information, toevaluate the performance of VLLMs comprehen-sively. We conducted an extensive assessment ofthe emotion understanding and expression perfor-mance of VLLMs through three sub-tasks: overallemotion tone prediction, character emotion predic-tion, and contextually appropriate emotion expres-sion selection.The experimental results showed that differencesin the model architecture had a distinct impact onthe emotion prediction performance. This suggeststhat the structural characteristics of VLLMs, suchas the method of integrating image and text in-formation and the LLM Backbone, play crucialroles in emotion prediction performance. In ad-dition, models that included persona informationexhibited notable differences in the emotion predic-tion process. This implies that information on thepersonality traits and speaking styles of an individ-ual significantly influences the emotion understand-ing and response performance of the model. Wealso conducted an in-depth analysis of the impactof various factors related to emotion prediction,such as gender and regional biases, on the emotionprediction performance of VLLMs. The analysisrevealed that factors such as gender and regionalbiases significantly influenced the emotion predic-tion process of VLLMs, revealing the biases andlimitations that may arise in this process.",
  "Related Work": "The rapid development of LLMs has led to substan-tial progress in language generation, knowledge uti-lization, and complex reasoning tasks. However, asthese models are being integrated into various appli-cation domains, enhancing their EI and mitigatingsocial biases have become increasingly important.Wang et al. (2023) explored the EI of LLMs using psychological methods, thereby laying the foun-dation for further research on how these modelsperceive and respond to emotional stimuli. Build-ing on this work, Li et al. (2023b) and Li et al.(2023c) investigated the ability of LLMs to under-stand emotional content and demonstrated that cur-rent models can react to emotional stimuli similarlyto humans. Paech (2023) introduced a new criterionfor evaluating the EI of LLMs through EQ-Bench,which is a benchmark that measures the ability ofa model to predict the emotional states of char-acters within conversations. Sabour et al. (2024)proposed EMOBENCH, which is a benchmark thatis designed to evaluate the EI of LLMs comprehen-sively by assessing not only emotion recognition,but also emotional regulation and the applicationof emotional understanding.Along with research on the EI of LLMs, ad-dressing the social biases that are inherent in thesemodels is a crucial task for the development ofethical AI. Sheng et al. (2019) and Schick et al.(2021) emphasized the importance of recogniz-ing and mitigating gender stereotypes and otherbiases in the training data. Nadeem et al. (2021)measured stereotypical biases using the StereoSetbenchmark, while Parrish et al. (2022) evaluatedbiases in question-answering tasks using the BBQdataset. These studies provided important insightsinto the EI and social biases of LLMs. Building onthis foundation, the present study aimed to analyzethe key factors influencing the emotion predictionof VLLMs in conversational contexts systemati-cally. Specifically, we intended to investigate theimpact of factors such as persona, gender, and re-gional biases on the emotion prediction processesof VLLMs in depth.",
  "Dataset and Task Overview": "We reconstructed the MELD dataset (Poria et al.,2018), which is based on popular TV series Friends,to investigate the key factors influencing the emo-tion prediction performance of VLLMs in con-versational contexts. The MELD dataset providesfull-scene images for each scene and the corre-sponding conversational context, along with thenames of the characters who engage in the dia-logue and the emotion and sentiment labels forthe feelings of each character. The dataset includesemotion and sentiment labels for each utterance.Emotions are categorized into seven types: \"fear,\"\"disgust,\" \"joy,\" \"sadness,\" \"surprise,\" \"anger,\" and : Overview of the data reconstruction process for evaluating the emotion prediction performance of VLLMsusing the MELD dataset. The process involved three main stages: (1) dialogue selection, which filtered and adjusteddialogues based on the number of turns and presence of characters with personal information; (2) image scopereconstruction, which extracted images from video frames and categorized them into three scopes (entire scene,person, and facial expression) to capture different aspects of emotional information; and (3) incorrect sentenceselection, which selected distractor sentences for each sub-task using SBERT.",
  "Persona Information": "As the characteristics of an individual greatly in-fluence emotion expression and understanding, weconstructed additional persona information for theMELD dataset. The persona information consistedof the personality traits and speaking styles of eachcharacter.Personality traits influence how individuals per-ceive and express emotions, and play a crucialrole in understanding and modeling emotional re-sponses in conversational contexts. We carefullydefined the personality traits of the characters ofFriends to provide comprehensive persona infor-mation. By including these personality traits in themodel, we could investigate their impact on theemotion prediction performance.Speaking styles affect how individuals conveytheir emotions and intentions. Each character ofFriends has a unique manner of speaking. By in-tegrating these speaking styles into the model, wecould analyze their influence on emotion predictionperformance.",
  "Quantitative Evaluation": "To evaluate the emotion prediction performanceof VLLMs comprehensively, we approached theproblem by selecting the most appropriate emotionexpression in each conversational turn, beyond sim-ply recognizing the emotions of the speaker. Weused a multiple-choice question format in whicheach question consisted of one correct utteranceand three incorrect utterances. The three subtaskswere designed to assess different aspects of theemotion understanding and expression abilities ofthe model, as follows:Overall emotion tone prediction task assessedthe ability of the model to predict the overall emo-tional tone of the dialogue by selecting the mostappropriate utterance from the options with differ-ent sentiments.Character emotion prediction task evaluatedthe ability of the model to predict the emotions ofspecific characters in a given context by selectingthe most appropriate utterance from the optionsexpressing different emotions.Contextually appropriate emotion expressionselection task assessed the ability of the model tounderstand the context in depth and to select the",
  "Dialogue Selection": "The original dataset includes various characters andsentences that are commonly used in real-life con-versations. These characteristics are useful for iden-tifying the key elements that influence the emotionprediction of VLLMs in conversational contexts.For data selection, we removed samples that eitherincluded dialogue with very long turns or could notreflect persona information.Adjusting dialogues with very long turns. Inconversations, instances arise in which very longturns appear. In such situations, models generallyrely more heavily on the previous conversationalcontext than on facial expressions or gestures. Thiscan significantly affect emotion prediction, particu-larly for VLLMs that use LLMs as their backbonemodels. This is because these models may priori-tize the textual context over visual cues. This canact as noise when identifying the key factors thatinfluence the emotion prediction of the model.Therefore, we decided to reduce dialogues ex-ceeding 15 turns randomly, to between 9 and 15turns. The reason for randomly adjusting the num-ber of turns rather than fixing them was to preventbias associated with the number of turns. In addi-tion, the randomization ensured the inclusion ofsamples with various dialogue lengths within thedataset to aid in evaluating the model performancein real-life conversational scenarios with varyinglengths.Removing characters lacking persona infor-mation. We also aimed to evaluate the emotionprediction performance of VLLMs based on theinclusion or exclusion of persona information. Tothis end, we structured the dialogue data such thatcharacters to whom persona information couldbe assigned appeared during the final utterance turn. However, collecting persona information forsome characters (e.g., hosts, customers, and airlineemployees), is difficult or impossible. Therefore,dialogues involving such characteristics were ex-cluded from the dataset. The final dataset includeda pool of characters consisting of six main charac-ters with persona information and 18 surroundingcharacters.",
  "Image Scope Reconstruction": "Text-based information is often effective for ex-plicit communication, but has limitations in con-veying complex emotional states or atmospheres.In contrast, images enrich these emotional nuancesthrough nonverbal elements and visual context. Par-ticularly in human conversations, emotions varysignificantly depending on the context and environ-ment. Therefore, the visual information containedin images, such as the posture, facial expressions,and gestures of the conversation partners, can cap-ture the subtleties of emotions that are difficult todiscern from text alone.For image processing, the original videos weredivided into frames and image information was ex-tracted from each relevant frame. The most suitableframe was selected and used for the entire sceneimage. Person and facial expression images wereextracted separately from the selected image, andthe entire process was performed manually by theauthors. At the end of each stage, cross-validationwas performed to improve the image accuracy andensure strict quality control.",
  "Incorrect Sentence Selection": "The final stage of the data construction involved theselection of incorrect sentences for each dialogue.In this stage, we selected incorrect sentences cor-responding to the multiple-choice questions. Weselected sentences with sentiments or emotions thatdiffered from the correct sentences for the over-all emotion tone and character emotion predictiontasks. For the contextually appropriate emotionalexpression selection task, we selected sentenceswith the same emotion as the correct sentence.The selected sentences were filtered usingSBERT (Reimers and Gurevych, 2019). Some sen-tences may have high semantic similarity and canbe used interchangeably with the correct sentence;therefore, we removed sentences that received se-mantic similarity scores above a certain level toeliminate such cases. In addition, we constructedthe dataset with two difficulty levels (easy and",
  "Prompt typeCoT": "InstructBLIPVicuna (13B)46.1346.4644.6645.7445.5344.0946.7946.8245.8934.9234.4733.6937.2936.5136.3935.5535.5534.9240.63LLaVA-1.5Vicuna (13B)57.5257.3756.7754.9554.9854.8055.2554.9554.3541.1340.7141.1043.3543.2943.3241.0140.9240.0848.66LLaVA-NextVicuna (13B)57.2255.8555.4953.2151.9851.8656.1856.1854.8645.2044.9643.9143.5043.4442.3943.5643.0242.0949.16InstructBLIPFLAN (11B)62.8363.0762.8063.1963.7063.9762.2362.5362.4149.0449.1649.2548.5048.1748.3847.6347.6047.6955.67LLaVA-1.5Vicuna (7B)47.8447.9346.7947.0046.9446.9445.7745.3544.5135.9136.3936.2135.8835.7335.2233.4834.0232.2840.78LLaVA-NextVicuna (7B)53.4553.2151.9852.4652.5851.4751.9252.4651.3838.9738.8839.1238.7638.0737.6236.6337.0536.5745.14LLaVA-NextMistral (7B)55.2254.4752.9152.4653.5751.9553.6652.4052.1641.7941.3141.7642.0341.1041.1341.6740.7138.7347.17Qwen-VL-ChatQwen (7B)43.4743.5643.7941.3740.4741.4341.1040.6240.1135.6134.9235.1033.6334.2334.6832.6132.6732.5237.89MiniGPT-4Vicuna (7B)29.6229.6829.4428.0927.6127.4627.2227.2526.8626.1726.3226.4426.5627.0126.2025.5725.0924.9727.09OtterMPT (7B)40.0538.9138.7939.0038.7638.7038.8839.3038.9435.6735.4035.4636.1235.6136.0633.6934.0233.4537.05InstructBLIPFLAN (3B)60.7360.3460.6759.7159.4160.0160.2860.4960.2543.5643.6843.2344.8744.6943.7941.6141.3740.7151.64 : Comparative performance analysis of VLLM models using different prompt types (original, personalitytraits, speaking styles, and CoT) for both easy and hard difficulty levels. The results, presented as accuracy scores,are averaged across three distinct prompts for each prompt type and are reported separately for Easy and Harddifficulties, allowing for a more detailed comparison of model performance across different complexity levels.",
  "Baselines": "The experiments were conducted using variousopen-source VLLMs. Specifically, factors such asmodality alignment, model size, and LLMs wereconsidered in the model selection. Modality align-ment is a technique for effectively integrating andprocessing various types of data, such as text andimages, in VLLMs. We analyzed key modalityalignment techniques, including Direct Mapping(Liu et al., 2023), Q-Former (Li et al., 2023d),and Customization Perceiver (Alayrac et al., 2022;Awadalla et al., 2023). In addition, following gen- erally known scaling laws, we thoroughly inves-tigated how the emotion prediction performanceof VLLMs interacted with other factors. To thisend, we also conducted experiments on modelswith the same architecture but different sizes. Theselected VLLMs included LLaVA-1.5 (Liu et al.,2023), MiniGPT-4 (Zhu et al., 2023), InstructBLIP(Dai et al., 2024), Qwen-VL-Chat (Bai et al., 2023),LLaVA-Next (Liu et al., 2024), and Otter (Li et al.,2023a). We selected various LLMs and model sizesand performed experiments on 11 VLLMs. Somehigh-performance models, such as GPT-4V, wereexcluded from the detailed analysis because theirinternal workings and parameter configurationshave not been disclosed.",
  "Main Results": "presents the performance based on the av-erage values of the easy and hard difficulty levels.The individual performances for easy and hard canbe found in Appendix B. We provide answers tothe following questions according to the main ex-perimental results:Q1: What is the most influential factor in theemotion prediction performance of the model?Answer: LLM. Our experiments show that themost important factor in the emotion predictionperformance of a model is the LLM itself. In par-ticular, we observed that as the size of the LLMincreased, the performance consistently improvedacross all models used in the experiments (Instruct-BLIP, LLaVA-1.5, LLaVA-Next, etc.). This trendwas evident across all prompt types, including orig-inal, personality traits, speaking styles, and chain ofthought (CoT). These results suggest that the LLMBackbone plays a more crucial role in predictinghuman emotions than focusing on specific imageregions does. This aligns with existing research, in-dicating that the architecture and scaling of LLMsenhance the performance.Q2: What is the most outstanding modelarchitecture for emotion prediction? Answer:InstructBLIP(FLAN 11B). InstructBLIP(FLAN11B) consistently achieved the highest performancein most cases. To verify whether these results weresimply owing to the instruction-tuning dataset, weconducted a comparative experiment with Instruct-BLIP(Vicuna 13B), which was trained using thesame data. Consequently, FLAN exhibited supe-rior performance over Vicuna, indicating that thearchitecture of FLAN itself, rather than merely thetuning data, provides excellent emotion prediction performance.Q3: Do additional persona information andCoT affect the emotion prediction performanceof the model? Answer: Yes. The integration ofpersona information and CoT prompts influencedthe emotion prediction performance of the model.The experimental results indicated that the effectsof these elements varied depending on the model.Some models (e.g., LLaVA-1.5 and Qwen-VL-Chat) exhibited slight performance improvementswhen persona information or CoT prompts wereadded, whereas other models (e.g., InstructBLIPand LLaVA-Next) showed no significant differ-ences or performance degradation. This suggeststhat persona information and CoT prompts mayhave different effects depending on the model ar-chitecture or pre-training data. However, consider-ing that the overall performance improvement wasnot substantial, the effects of these elements appearto be limited. Therefore, future research should ex-plore means of using persona information and CoTprompts more effectively.",
  "How do different prompts affect theoverall emotion prediction performance?": "We analyzed the performance for each emotion inthe emotion prediction. As shown in , allprompt types showed the highest performance inpredicting the \"joy\" emotion, with the speakingstyles prompt achieving the best result of 50.82%.This suggests that the tone and style of conversationplay an important role in predicting positive emo-tions. The personality traits prompt also showedhigh performance in predicting \"joy,\" at 49.98%,indicating that individual personality traits are cru-cial elements in understanding and expressing joy.These results demonstrate that the model can pre-dict positive emotions more accurately based onthe personality and speaking style of the speaker.In contrast, all prompt types showed relativelylower performance in predicting \"fear\" than otheremotions. In the case of the speaking styles prompt,the performance for predicting the \"fear\" emotionwas the lowest among all emotions, at 39.58%, andsimilar trends were observed for the other prompttypes. This indicates that predicting negative emo-tions such as fear is challenging. Fear may requirecomplex and subtle contexts and the limitations ofthe model may be exposed when accurately pre-dicting such emotions.",
  ": Comparison of emotion prediction perfor-mance across different prompt types (original, personal-ity traits, speaking styles, and CoT)": "In addition, the prediction of the \"neutral\" emo-tion showed relatively low performance acrossall prompt types, particularly in the personalitytraits prompt, which had the lowest performanceat 39.44%. This suggests that individual person-ality traits may add complexity to the process ofdiscerning emotional neutrality. Neutral emotionsare difficult to predict owing to the absence of clearpositive or negative signals, indicating that addi-tional research is required to improve the modelperformance to respond in situations in which clearemotional signals are lacking.",
  "How does emotion prediction performancediffer based on the image scope?": "We demonstrated the differences in emotion pre-diction performance based on the image scope, asshown in . For \"joy,\" high performancewas observed across all scopes, with the \"all\" scopeachieving the best result at 48.69%. However, theperformance of the \"face\" and \"person\" scopes wasnot significantly different, at 48.61% and 48.24%,respectively. This suggests that various cues, suchas facial expressions, individuals, and the overallcontext, may be equally important when predictingjoy.For \"sadness,\" the \"face\" scope showed the high-est performance at 47.34%, suggesting that facialexpressions are a crucial factor in predicting sad-ness. However, for \"fear,\" the \"all\" scope exhibitedthe highest performance, at 41.64%. This impliesthat the overall image information can be helpfulin the prediction of fear because it is an emotionthat arises in complex contents.For \"disgust,\" the \"face\" scope achieved the high-est performance at 44.50%, whereas for \"surprise,\"",
  ": Changes in emotion prediction performancebased on image scope (all, person, and face) for eachemotion category": "the \"person\" scope showed the highest performanceat 44.75%. This indicates that facial expressionsand posture or movements of an individual can playimportant roles in predicting disgust and surprise,respectively. For \"anger,\" the performance differ-ence between the image scopes was not significant,with the \"face\" scope showing a slightly higherlevel at 44.84%.In contrast, \"neutral\" showed relatively low per-formance across all scopes, particularly in the\"face\" scope, which had the lowest performance at40.78%. This suggests that facial expressions alonemay not provide sufficient cues for predicting neu-tral emotions. The \"all\" scope showed the highestperformance at 41.40%, but this low level suggeststhat more sophisticated context analysis may benecessary to predict neutral emotions accurately.",
  "Is the emotion prediction performance ofthe model influenced by gender?": "In this section, we analyze whether differences inemotion prediction performance occurred basedon gender. The experimental results presented in clearly show how the emotion predictionperformance varied depending on the gender of thesubject that the model aimed to predict. The resultsrevealed that the emotion prediction performanceof female was higher than that of male for mostemotions. Notably, for the \"disgust\" emotion, theprediction performance for females (54.21%) wassignificantly superior to that for males (34.78%).A detailed analysis is provided in Appendix E.For the \"joy\" and \"surprise\" emotions, the predic-tion performance for females was also higher at50.59% and 46.19%, respectively, compared tomales (46.63% and 41.68%, respectively). This",
  ": This radar chart illustrates the differences inemotion prediction performance based on the targetgender": "implies that positive emotions such as \"joy\" and\"surprise\" may be more prominently expressed byfemales.In contrast, when recognizing the \"sadness\" emo-tion, the performance for males (48.77%) washigher than that for females (45.90%). This sug-gests that male emotional expressions may be betterrecognized by the model when identifying \"sad-ness.\" In the recognition of the \"anger\" and \"neu-tral\" emotions, the performance difference betweenmales and females was not significant, indicatingthat the expression differences of \"anger\" and \"neu-tral\" based on gender may be relatively small.",
  "Do regional biases influence the emotionprediction performance?": "The experimental results presented in clearly demonstrate the impact of regional biaseson the emotion prediction performance of themodel. According to the analysis, most regionsshowed a tendency for the model performance todegrade when regional persona information wasadded. In particular, for the Middle East and Africa,the performance decreased by -2.40% and -2.20%,respectively, compared to the original prompt, indi-cating that regional biases had a negative impact onthe emotion prediction performance. Performancedegradation was also observed for East Asia (-1.90%), South Asia (-1.87%), and Nordic countries(-1.71%).In contrast, North America was the only region",
  ": Changes in emotion prediction performancebased on region, calculated according to the differencefrom the emotion prediction performance of the originalprompt": "for which the performance improved by +0.07%.This suggests that the data used to train the modelreflect the characteristics of the North Americanregion relatively well. For Latin America and West-ern Europe, the performance decreases were rela-tively small, at -1.28% and -1.02%, respectively;however, they still appeared to be influenced byregional biases. Additional details are provided inAppendix F.",
  "Conclusion": "This study has systematically analyzed the keyfactors influencing the emotion prediction per-formance of VLLMs. The experimental resultsshowed that the model architecture and size, partic-ularly the LLM Backbone, had the most significantimpact. The integration of persona information andCoT prompts exhibited varying effects dependingon the model, and differences in the prediction per-formance were observed based on the image scopefor each emotion. However, biases in emotion pre-diction performance based on gender and regionwere identified, indicating the need for efforts tomitigate these biases. Future research should focuson developing emotionally intelligent VLLMs byminimizing data and model biases using advanceddataset composition and model training methods.",
  "Limitations": "Although this study provides valuable insights intothe factors that influence the emotion predictionperformance of VLLMs, it has some limitationsthat should be acknowledged. First, we excludedhigh-performing models, such as GPT-4V, fromour detailed analysis because their internal struc-tures and model sizes have not been publicly dis-closed. Although these models are likely to employadvanced architectures that can further our under-standing of emotion prediction, their lack of trans-parency makes it difficult to analyze the specificfactors that contribute to their performance system-atically. However, as more information on thesemodels becomes available, future research shouldinvestigate their emotion prediction capabilities inrelation to the factors identified in this study.Second, although our experiments revealed thepresence of gender and regional biases in the emo-tion predictions of VLLMs, proposing comprehen-sive solutions to these biases is beyond the scopeof this study. Addressing these biases is crucialfor developing fair and unbiased VLLMs, and westrongly encourage future research to focus on mit-igating these issues.Finally, it is important to note that although theMELD dataset, which is based on the TV seriesFriends, reflects many real-world emotional situa-tions, it may not capture the full range of emotionsand contexts that are present in human interactions.Although TV shows are designed to mirror real life,they are ultimately scripted and may not always rep-resent the spontaneity and complexity of real-worldconversations. Future research could expand thescope of this study by incorporating datasets fromdiverse sources, such as real-world conversations,to validate and generalize our findings further.Despite these limitations, we believe that ourstudy provides a solid foundation for understand-ing the factors that influence the emotion predic-tion performance of VLLMs. We have identified key areas for future research and development inthis field by systematically analyzing the effects ofthe model architecture, persona information, andvarious biases. As VLLMs continue to advance,it will be crucial to address these limitations andbuild emotionally intelligent models that can un-derstand and respond to human emotions in a fairand unbiased manner. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in neuralinformation processing systems, 35:2371623736. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.2023. Openflamingo: An open-source framework fortraining large autoregressive vision-language models.arXiv preprint arXiv:2308.01390. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, textreading, and beyond. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36.",
  "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Fanyi Pu, Jingkang Yang, Chunyuan Li, and ZiweiLiu. 2023a. Mimic-it: Multi-modal in-context in-struction tuning. arXiv preprint arXiv:2306.05425": "Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang,and Xing Xie. 2023b. Large language models un-derstand and can be enhanced by emotional stimuli.arXiv preprint arXiv:2307.11760. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,Xinyi Wang, Wenxin Hou, Jianxun Lian, Fang Luo,Qiang Yang, and Xing Xie. 2023c. The good, thebad, and why: Unveiling emotions in generative ai.arXiv preprint arXiv:2312.11111. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023d. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR.",
  "Samuel J Paech. 2023. Eq-bench: An emotional intelli-gence benchmark for large language models. arXivpreprint arXiv:2312.06281": "Alicia Parrish, Angelica Chen, Nikita Nangia, VishakhPadmakumar,JasonPhang,JanaThompson,Phu Mon Htut, and Samuel Bowman. 2022. Bbq: Ahand-built bias benchmark for question answering.In Findings of the Association for ComputationalLinguistics: ACL 2022, pages 20862105. Reinhard Pekrun, Thomas Goetz, Wolfram Titz, andRaymond P Perry. 2002. Academic emotions in stu-dents self-regulated learning and achievement: Aprogram of qualitative and quantitative research. Ed-ucational psychologist, 37(2):91105. Soujanya Poria, Devamanyu Hazarika, Navonil Ma-jumder, Gautam Naik, Erik Cambria, and Rada Mihal-cea. 2018. Meld: A multimodal multi-party datasetfor emotion recognition in conversations.arXivpreprint arXiv:1810.02508.",
  "and the image to identify the correct answer.Only one of the options is correct, and theothers are incorrect": "Using a given dialogue involving multiplespeakers and a related image, identify themost suitable reply for the last speaker basedon the overall Tone, details in the conversa-tion, and visual elements from the image. Re-member, only one response is correct; the oth-ers do not fit the context as well. Based on the interaction among multiplespeakers and the visual cues from the accom-panying image, deduce which statement wouldmost accurately reflect the final speakers in-tended communication. Assess the tone, im-plied sentiments, and emotional context pre-sented both verbally and visually. Only oneoption is the right answer; all others are in-correct. In addition, we used corresponding persona in-formation as an additional input to analyze thedifferences in emotional prediction performanceaccording to personality traits and speaking styles.We utilized the Friends Fandom Wiki to generatepersona information, providing the relevant data asinput to GPT-4 to create the persona information.Considering the maximum token limit for specificmodels, we only added two persona information in-puts. The persona information input for personalitytraits follows this format:Last speakers personality traits:1. [Personality trait]2. [Personality trait]Similarly, the speaking styles are input as fol-lows:Last speakers speaking styles:1. [Speaking style]2. [Speaking style]The overall prompt can be found in .",
  "EAnalysis of \"Disgust\" EmotionPrediction Differences by Gender": "The conversational samples in Figures 10 and 11 re-veal notable differences in how males and femalesexpress the emotion of \"disgust.\" In the femalesamples, disgust is often expressed through strongexclamations such as \"Oh my God!\" and \"Ewww!\"(, Samples 1 and 3). These expressionssuggest a more overt and emphatic display of the",
  ": Emotion distribution used for emotion analysis": "\"disgust\" emotion by females. In addition, femalecharacters tend to provide more detailed descrip-tions of the disgusting situation, such as \"Shes gother tongue in his ear\" (, Sample 2), whichvividly conveys their sense of revulsion.In contrast, the male samples show a relativelymore subdued expression of \"disgust.\" For instance,in , Sample 1, the male character ex-presses his aversion to drinking breast milk in amore matter-of-fact manner, stating, \"Not even ifCarols breast had a picture of a missing child onit.\" While still conveying disgust, the expressionis less emotionally charged compared to the fe-male samples. Similarly, in , Sample 3,the comment by the male character, \"OK, is therea mute button on this woman?\" suggests annoy-ance and disgust, but lacks the same level of overtexpression that appears in the female samples.These differences in the expression of disgust be-tween males and females could potentially explainthe higher performance in predicting \"disgust\" forfemale (54.21%) compared to male (34.78%). The more explicit and emphatic expressions of dis-gust by females may provide clearer cues for theVLLMs to identify the emotion accurately.However, it is important to acknowledge the limi-tations of this analysis. The conversational samplesprovided, while based on a TV show reflectingmany real-world situations, do not fully capturethe entire spectrum of \"disgust\" emotion expres-sions that occur in real-life interactions. In addition,the differences observed in these specific samplesmay be influenced by individual character traitsand situational contexts, rather than being solelyattributable to gender.A more comprehensive study with a larger andmore diverse dataset would be necessary to drawmore definitive conclusions regarding gender-baseddifferences in \"disgust\" emotion expression. Sucha study should consider various factors, includingindividual personality traits, cultural backgrounds,and conversational contexts, to determine whetherthe observed differences are truly representative ofgender-based patterns or whether other factors playa more significant role.In summary, while the analysis of the providedconversational samples suggests potential differ-ences in how males and females express disgust,further research is required to establish the extentto which these differences are generalizable acrossa wider population and to determine the relativeinfluence of gender compared to other factors inshaping \"disgust\" emotion expression.",
  "FRegional Bias Problem in EmotionPrediction of VLLMs": "This study identified a general trend towards de-creased emotion prediction performance when per-sona prompts containing regional information wereprovided to the models. This suggests that the mod-els may inherently hold prejudices or stereotypestowards specific regions.The prompts used in the experiments were struc-tured as follows:Last speakers characteristics:1. The last speaker has lived in ## throughouttheir life, deeply rooted in the language, religion,and customs of that region.2. The last speaker uses the communication stylecommonly employed in ## to interact with others.In the above, ## was replaced with the corre-sponding region name. These prompts provided themodel with the information that the last speaker is",
  ": Changes in sentiment prediction perofrmanceaccording to various prompts": "from a specific region and is deeply connected tothe language, religion, customs, and communica-tion style of that region.However, the research results showed that theemotion prediction performance of the model dete-riorated when such regional information was pro-vided, demonstrating the possibility that the mod-els harbor stereotypes or biases towards specificregions. These models may make inappropriateassumptions based on the regional information pro-vided through prompts, leading to inaccurate emo-tion predictions.This finding is directly related to the fairness andbias issues in VLLMs. If the models make biasedpredictions about specific regions, this can leadto unfair treatment of individuals in those regions.Therefore, future research is necessary to minimizesuch biases and enhance the fairness of the models.",
  "GHow do different prompts affect theoverall sentiment predictionperformance?": "We analyzed the impact of various prompt types onthe overall sentiment prediction performance of themodels systematically, based on the experimentalresults presented in . The results revealedthat the inclusion of persona information, such aspersonality traits and speaking styles, influencedthe sentiment prediction performance significantly,with notable variations observed across differentsentiments. For the recognition of \"positive\" senti-ments, the models exhibited a substantial improve-ment in performance when persona informationwas incorporated. In contrast, for the recognition of\"negative\" sentiments, the original prompt, whichdid not include any persona information, recordedthe highest performance. When it comes to \"neu- tral\" sentiments, the CoT prompt, which involved asystematic thought process, demonstrated the high-est performance.The experimental results showed that the inclu-sion of persona information had mixed effects onthe sentiment prediction performance of the mod-els, with the overall performance improvement be-ing limited. While the personality prompt showedpromising results for positive sentiments, it did notbenefit all sentiment prediction tasks consistently.Similarly, the speaking styles prompt, although ef-fective for positive sentiments, did not yield signifi-cant improvements in the recognition of negative orneutral sentiments. These findings suggest that theimpact of persona information on sentiment predic-tion performance varies depending on the specificemotion being analyzed.Our analysis highlights the importance of con-sidering the interplay between persona informationand sentiment prediction in conversational contexts.While the inclusion of personality traits and speak-ing styles can enhance the models understandingof certain sentiments, such as positive sentiments,its impact is not uniform across all sentiment cate-gories. Further research is needed to explore moresophisticated approaches for integrating personainformation into sentiment prediction tasks, takinginto account the nuances and challenges associatedwith different emotional expressions. : Example of the prompt template used for testing. This figure illustrates the detailed structure of the promptused in our experiments, including sections for instruction, historical content, personality traits, speaking styles,response options, and the CoT. This comprehensive prompt format ensures that the model evaluates multiple aspectsof context and persona information to determine the most appropriate response."
}