{
  "Abstract": "Code retrieval aims to identify code from ex-tensive codebases that semantically aligns witha given query code snippet. Collecting a broadand high-quality set of query and code pairs iscrucial to the success of this task. However, ex-isting data collection methods struggle to effec-tively balance scalability and annotation qual-ity. In this paper, we first analyze the factorsinfluencing the quality of function annotationsgenerated by Large Language Models (LLMs).We find that the invocation of intra-repositoryfunctions and third-party APIs plays a signifi-cant role. Building on this insight, we proposea novel annotation method that enhances the an-notation context by incorporating the content offunctions called within the repository and infor-mation on third-party API functionalities. Ad-ditionally, we integrate LLMs with a novel sort-ing method to address the multi-level functioncall relationships within repositories. Further-more, by applying our proposed method acrossa range of repositories, we have developed theQuery4Code dataset. The quality of this synthe-sized dataset is validated through both modeltraining and human evaluation, demonstratinghigh-quality annotations. Moreover, cost anal-ysis confirms the scalability of our annotationmethod. 1",
  "Introduction": "Code retrieval aims to find the most relevant codesnippet in a database given a user query, facilitat-ing the reuse of programs in the software devel-opment process (Bui et al., 2021; Li et al., 2022;He et al., 2024) and driving recent research onretrieval-augmented code generation (Zhou et al.,2022; Zhao et al., 2024). To achieve good per-formance in practical applications, the key lies incollecting a wide range of high-quality, dual-modal",
  ": Example of code snippet and correspondingquery and docstring": "pairing data between natural language queries andcode snippets.An efficient approach to collect code retrievaldatasets involves directly gathering code data fromonline repositories (e.g., GitHub2) and processingit to extract code snippets along with their cor-responding docstrings. As depicted in ,since the docstring serves as a description of thefunction code, it can be utilized as a query. How-ever, a significant difference exists between thedocstring and the users query, resulting in a devi-ation from queries encountered in real-world sce-narios. To bridge this gap and obtain queries thatclosely resemble those of actual users, some re-searchers (Heyman and Van Cutsem, 2020; Yaoet al., 2018) tend to collect user questions and thecorresponding code snippets from programmingcommunities such as Stack Overflow3. Anotherapproach explored by researchers (Rao et al., 2021;Huang et al., 2021) involves gathering user searchqueries from browser logs and subsequently enlist-ing experts to annotate corresponding code snip-pets based on these queries. Regrettably, the for- mer approach often produces code snippets of in-ferior quality because of the presence of block andstatement-level code within the community. Onthe other hand, the latter approach allows for theacquisition of a high-quality dataset but proves tobe cost-prohibitive and challenging to scale. There-fore, we pose a question: Can a more efficient,low-cost method be developed to obtain a high-quality code retrieval dataset?The formidable capabilities of Large LanguageModels (LLMs) present a remarkable opportunity.Firstly, previous research (Rodriguez-Cardenaset al., 2023) has demonstrated the profound codecomprehension ability of LLMs in various codeunderstanding tasks, such as code summarization(Geng et al., 2023). Secondly, existing LLMs, em-ploying preference alignment techniques (Genget al., 2023), can generate content that aligns withhuman preferences. In the domain of search, somestudies (Bonifacio et al., 2022; Dai et al., 2022)have proposed generating the query from the docu-ments, yielding highly promising outcomes. Hence,a straightforward approach is to employ LLMsto generate user-like queries from the code snip-pets. However, there are some differences betweencode snippets and traditional documents. For in-stance, intra-repository function calls refer to thecalls between different functions within a repos-itory project, as depicted in . Functionexport_nb calls function savefile, which makesit challenging for LLMs to comprehend functionexport_nb if only provided as input, without con-sidering the function savefile it calls.Addi-tionally, third-party API calls involve invokingfunctions from external APIs, as shown in Fig-ure 1. Function export_nb calls the third-partyAPI PythonExporter.from_filename, and LLMneeds to understand the functionality of this APIfor a better understanding of the function.In this paper, we first analyze the main factorsaffecting the quality of annotations for functions inrepositories. Through preliminary experiments ona development set from 100 selected repositories,we observe that the presence of intra-repositoryfunction calls exerts a substantial influence on thequality of annotations, with a greater number ofcall relationships resulting in a heightened degreeof impact. Additionally, we uncover that infrequentthird-party calls have the greatest impact on annota-tion quality. This observation may be attributed tothe limited pretraining knowledge of LLMs regard-ing these external libraries. Based on these findings, we propose an annotation algorithm aimed at usingLLMs for high-quality code retrieval query anno-tations. We start by parsing the relationships ofintra-repository function calls and use a topologi-cal sorting approach to guide the LLM annotationsequence. For third-party function calls, we se-lect third-party functions based on popularity anduse web scraping to annotate features of unpopularthird-party functions, adding this information tothe annotation context. To substantiate the efficacy of our annotation ap-proach, we initially employed our method to obtaina large-scale code retrieval dataset Query4Code,which includes 237.2K queries and code pairs from12.3K repositories. We use Query4Code a pretrain-ing corpus for various code retrieval models. Sub-sequently, comprehensive evaluations on multiplereal-world benchmarks confirmed that our methodsignificantly enhances the performance of code re-trieval models in real scenarios.",
  "Code Retrieval Datasets": "Representation learning (Zhang et al., 2023b; Gaoet al., 2021; Liu et al., 2023) has achieved sig-nificant results in multiple fields. The previouscode retrieval methods (Sedykh et al., 2023) ofcode retrieval data collection can be summarizedinto three categories: 1). Some researchers (Wanget al., 2023b) parse functions and correspondingdocstrings from online repositories to form pairs.For example, Husain et al. (2019) collected 2.1Mpaired data of 6 programming languages from anopen-source repository on GitHub, constituting theCodeSearchNet. 2). Others (Yin et al., 2018) gatherquestions posted by users on Stack Overflow alongwith the accepted code snippets to create datasetssuitable for code searching. Heyman and Van Cut-sem (2020) attempts this by collecting the mostpopular dataset posts on Stack Overflow and gath-ering code snippets from highly upvoted responses.3). The use of manual annotation methods: Huanget al. (2021) initially collects human queries usedin code searches from search engines and then man-ually gathers relevant code snippets from GitHubto match these queries.",
  "Code Retrieval Models": "In token-level pre-training methods, CodeBERT(Feng et al., 2020) attempts to leverage the exten-sive programming and natural language bimodaldata within repositories for pre-training. Buildingupon this, GraphCodeBERT (Guo et al., 2021) en-deavors to incorporate data flow graph signals todevise new pre-training tasks, thereby enhancingthe understanding of code semantics. UniXcoder(Guo et al., 2022) introduces a unified cross-modalpre-training model specifically designed for pro-gramming languages. Recently, some studies haveexplored the use of contrastive learning approachesto augment code search tasks. ContraCode (Jainet al., 2021) and Corder (Bui et al., 2021) employsemantic-preserving variation techniques for dataaugmentation and utilize contrastive learning ob-jectives to distinguish between similar and dissimi-lar code snippets. CodeRetriever (Li et al., 2022)attempts to combine unimodal and bimodal con-trastive learning to train code search models.",
  "LLM in Data Annotation": "Given the strong generalization capabilities exhib-ited by Large Language Models (LLMs), they ap-ply across multiple domains (Samuel et al., 2023;Zhang et al., 2024) for data synthesis, facilitatingthe transfer of rich knowledge from larger mod-els to smaller ones.In Unnatural Instructions(Honovich et al., 2023) and Self-Instruct (Wanget al., 2023a), LLMs utilize to generate the in-structional datasets required during the fine-tuningphase. Samuel et al. (2023) utilize a minimal set oforiginal data to guide LLMs in generating datasetsrequired for reading comprehension tasks. Westet al. (2022) propose a two-step process for sym-bolic knowledge distillation rather than the creationof content-related datasets. In the field of informa-tion retrieval, Zhang et al. (2023a) utilize LLMs togenerate positive and negative samples during thetraining process of contrastive learning.This paper is the first to use LLMs to annotatecode retrieval dataset, focusing on the key factorsthat affect LLMs in generating queries: library callsand third-party API calls.",
  "Setup": "Based on the selection of high-quality repositoriesidentified from prior research (Husain et al., 2019),we randomly chose 100 repositories to form ourdevelopment set. Subsequently, we employ the tree-sitter4 library to parse code files within these repos-itories, acquiring all function-level code snippetsand their invocation relationships. These relation-ships are further categorized into intra-repositorycalls and third-party API calls.",
  "Impact of Intra-Repository Function Calls": "Due to the existence of multiple functions in therepository, these functions are usually involved incomplex call relationships. After parsing, from Ta-ble 1, we can observe the proportion of functionswith call relationships, as well as the average andmaximum call frequencies. We observe that 46.5%of the code has call relationships, and the maxi-mum number of calls can reach 137 times. Thishighlights the widespread use of function calls in",
  "Pairing": ": The overview of our annotation method. (a) Files in the repository. (b) Function call graph obtained fromparsing. (c) API calls obtained from parsing and their corresponding popularity. (d) Construct annotated contextbased on call relationships and current API calls. (e) Pipeline for annotation method. the repository. Subsequently, we analyze the im-pact of these call relationships on the quality offinal query annotations generated by LLMs. Weuse two annotation methods: direct annotation andadding calling function context for annotation. Af-ter obtaining the final annotated results, we pairannotated queries with code and used the GPT-4-turbo model to score (0-3) and evaluate the qualityof generated queries. The final results are shownin , from which we observe that includinginformation about called functions significantly af-fects annotation quality. Furthermore, more callrelationships will lead to a greater degree of influ-ence, and model capability also significantly affectsthe quality of final annotations. [0.7, 1.6] [1.6, 2.6] [2.6, 3.5] [3.5, 4.2] [4.2, 5.0] [5.0, 5.7] [5.7, 6.4] [6.4, 7.3][7.3, 10.6]",
  "Impact of Third-Party APIs Calls": "After analyzing the invocation of third-party APIsin functions, as shown in , we observe that53.5% of the functions involve third-party APIcalls, with the maximum number of calls reach-ing 120 times. We next examine the impact ofthird-party APIs on annotation quality. Inspiredby previous research (Mallen et al., 2023), we con-sider that the impact of APIs on annotation qualityis closely related to the APIs popularity. Therefore,we initially use the frequency of API calls in therepositories as a proxy for API popularity. We thenannotate functions in our development set usingLLMs, including all available API documentation.GPT-4-turbo is used to compare LLM explanationsof API functions against the actual API documen-tation, with results categorized according to pop-ularity. Our findings, presented in , showthat LLMs often lack a comprehensive grasp ofmany API details, particularly for unpopular APIs.This phenomenon adversely affects the quality ofLLM annotations for queries. And even for modelswith stronger performance (e.g., gpt-3.5-turbo), theunderstanding of low-popularity APIs is also poor.",
  "In the preceding analysis, we demonstrate how theinvocation relationships within a repository andthose in third-party libraries can impact the quality": "of Large Language Models (LLMs) in annotatingqueries. As shown in , we attempt to pro-pose an annotation method to address these issues.We endeavor to collect information about functionswith invocation relationships, as well as functional-ities of unpopular APIs, and incorporate them intothe annotation context. Then, we use this context toprompt LLMs to generate queries (see the promptin Appendix B).",
  "Task Decomposition": "Inspired by previous research work (Wei et al.,2022), a complex task can be simplified by de-composing it into multiple simpler tasks, therebyeasing the models inference load. For the task ofquery annotation, we consider that the model firstneeds to understand the code of the currently an-notated function and then generate queries that auser might write during the development processbased on this understanding of code semantics. Asshown in (e), we initially use LLMs forcode interpretation and then proceed to annotatequeries based on the interpretation and the contentof the code snippets:",
  "Analyzing Function and API Calls": "Since in , we have analyzed that the mainfactors affecting the quality of LLM annotationsfor queries are function calls within the repositoryand third-party API calls. Therefore, as shown inthe upper of , for a given repository, wefirst use the tree-sitter tool to parse all functions inthe code files within the repository. Then, we ana-lyze each functions calls to other intra-repositoryfunctions and third-party APIs separately.",
  "Annotation Algorithm Based on FunctionCall Graph": "Having established the function invocation rela-tionships within the repository, a straightforwardapproach would be to include the relevant con-text of the function to be annotated along withthe query into the LLMs input context. How-ever, as shown in (b), there are multi-levelcall relationships between functions in the repos-itory. Understanding the train function requires knowing the train_batch function because it callsthe train_batch function, which then calls thecontrastive_loss function. Similarly, to graspthe train_batch function properly, its essentialto understand the contrastive_loss function. Di-rectly incorporating all functions into the contextwould pose challenges associated with multi-levelreasoning.Thus, we propose a novel annotation algorithmbased on topological ordering. The intuition behindthis algorithm is the decoupling of multi-level invo-cation relationships into single-level relationships.Specifically, we first construct a directed graphG(V, E) of function calls, where each node v Vrepresents a function in the repository. If functionA is called by function B, there will be a directededge e E from vA to vB. Based on topologicalsorting, we first annotate functions without depen-dency relationships. During the annotation process,when encountering recursive calls, we randomlydelete an edge to continue with the annotation. Sub-sequently, we annotate functions with invocationrelationships, thus breaking down multi-level invo-cation relationships into single-level relationships.For the annotation context of the function currentlybeing annotated, it is only necessary to include in-formation about its directly called functions. Wesummarized the algorithm in Algorithm 1.",
  "Collection of Third-Party APIDocumentation Based on Popularity": "In , our analysis indicates that LLMsstruggle to understand unpopular APIs. Therefore,we aim to add descriptions of unpopular third-partyAPI functionalities in the annotation context. Asshown in (c), first, we need to assess thepopularity of APIs, using the frequency of APIcalls in the repository as a basis for popularity. Ouranalysis concludes that LLMs understand APIs bet-ter if they exceed a popularity threshold. Therefore,we set a popularity threshold and for third-partyAPIs below this threshold in the function, we usethe DuckDuckGo5 search engine to look up docu-mentation and employ LLM to summarize the APIfunctionalities. Then, we add this information intothe annotation context.",
  "Data Filtering": "To further enhance the quality of generated queriesand improve the explainability of the annotationprocess, we attempt to incorporate a reverse vali-dation and an explanation phase for the query andcode snippet pairs into the annotation framework.Specifically, as shown in (e), after com-pleting the annotation to obtain aligned query andcode snippet pairs, we first use LLMs for reversevalidation. Inspired by Huang et al. (2021), we no-tice that the code in the annotated query-code pairscannot fully answer the query. It may exceed, par-tially satisfy, or completely fail to meet the queryrequirements. Specifically, we focus on the follow-ing four scenarios: 1) If the code can answer andexceed the query requirements, it is considered acorrect answer. 2) If the code can satisfy certaincategories of query requirements, it is also deemeda correct answer. 3) If the code satisfies less than50% of the query requirements, it cannot correctlyanswer the query. 4) The code has almost no rel-evance to the query. Based on this principle, weconstruct the CLS prompt language model to obtainclassification results:",
  "Annotation": "To facilitate comparison, we followed the selectionof GitHub repositories in CodeSearchNet (Husainet al., 2019), choosing only Python repositories forcost reasons. Please note that the code retrievaldata in the CodeSearchNet dataset consists of pairsof docstrings and code obtained through syntaxparsing, and does not include manually annotatedqueries. We then applied a certain method to fil-ter high-quality functions within these repositories.Subsequently, we used the GPT-3.5-turbo model togenerate queries using the annotation method men-tioned above. Ultimately, we successfully anno-tated a total of 237.2K pairs of natural language andcode snippets, forming the Query4Code dataset.Due to filtering operations during the annotationprocess, the final Query4Code dataset can be re-garded as a subset of the CodeSearchNet Python.",
  "Model Validation": "To validate the quality of the Query4Code dataset,which we obtain through our final annotation pro-cess, we pre-train existing pre-trained code repre-sentation models using both the CodeSearchNetand Query4Code. We aim to evaluate model per-formance across multiple real-world code retrievalbenchmarks in a zero-shot setting. Furthermore,we fine-tune the models on real-world datasets toassess the adaptability of the Query4Code datasetto downstream benchmarks.",
  "Benchmark and Metric": "In order to evaluate the performance of the modelin real-world code retrieval scenarios, we haveselected a wide range of benchmarks for valida-tion.Among them, the datasets CoNaLa (Yinet al., 2018), SO-DS (Heyman and Van Cutsem,2020), and StaQC (Yao et al., 2018) are col-lected from Stackoverflow questions, and queriesin CoSQA (Huang et al., 2021) and WebQueryTest(Lu et al., 2021) are collected from web search en-gines. Therefore, the queries in these datasets arecloser to real code search scenarios. The statisticsof benchmark datasets are listed in . Follow-ing prior research works (Kanade et al., 2020; Liet al., 2024), we employed Mean Reciprocal Rank(MRR) (He et al., 2023) as the evaluation metric:",
  "Implementation details": "All experiments are implemented using PyTorch.During the pre-training phase, for all settings re-lated to model architecture and hyperparameters,we follow the original paper.During the fine-tuning phase, to adapt to variations between dif-ferent datasets, we conducte a grid search on thedownstream dataset to find the learning rate, settingthe range in our experiments as {1e-5, 2e-5, 5e-5},and utilize the AdamW optimizer. The optionsfor batch size included {32, 64, 128}. Trainingis set for 10 epochs and to prevent overfitting, weadopte an early stopping strategy. The experimentsdescribed in this paper are conducted with threerandom seeds: 0, 1, and 2, and we will report theaverage results in the paper. All experiments meetthe p < 0.01 significance threshold. Experimentsare conducted on a GeForce RTX 4090 GPU.",
  "Results": "Zero-shot PerformanceThe final zero-shot ex-perimental results, as shown in , indicatethat pre-training on the Query4Code dataset sig-nificantly enhances performance compared to pre-training on the CodeSearchNet dataset, with im-provements observed across multiple code repre-sentation models. Additionally, we note substantialperformance gains on both the CoSQA and Web-QueryTest datasets. We attribute this improvementto the fact that the queries in these two datasetswere extracted from logs of real-world search en-gines, which closely match the distribution of ourannotated queries. Conversely, the improvementon the SO-DS dataset was minimal, likely due to agreater disparity between the code snippets in theSO-DS dataset and our annotated dataset. Fine-tuning PerformanceIn the fine-tuning ex-periment, it is worth noting that since the Web-QueryTest dataset is specifically designed for as-sessing real-world code retrieval task performancewithout available training data, its related resultswere not reported. The final experiments demon-strate that pretraining with the Query4Code datasetbefore fine-tuning yielded superior performanceacross all other datasets, confirming that modelspretrained through Query4Code exhibit enhancedadaptability in real-world code retrieval scenarios.",
  ": Using different data pairs with Query4Code totrain CodeBERT for zero-shot performance": "Although this paper mainly focuses on generat-ing annotations for query retrieval of code, our two-stage annotation method can obtain functional sum-maries of functions. We are interested in whetherthe functional summary of functions can enhancethe ability of the current code retrieval model. Asshown in , compared with only using (q, c)pairs (denoted as Cqc) for contrastive learning, us-ing only (s, c) pairs (denoted as Csc) achieved com-parable performance and performed better on the SO-DS and CoSQA datasets. Furthermore, utiliz-ing both annotated query q and summary c dataachieved the best performance. For detailed experi-mental settings, please refer to Appendix B.2. Thisdemonstrates the potential of the our annotationmethod.",
  "Human Evaluation": "To evaluate the quality of the data generated bythe annotation algorithm we proposed, we em-ployed a manual assessment approach. We extracte200 pairs of queries and code snippets from theQuery4Code dataset and invited three experts toscore them according to the four types mentionedin .6. We then calculate the Pearsons rand Kendalls correlation coefficients betweenthe scores and the results generated by the model.The results are summarized in . Observa-tion reveals that the query-code pairs we annotatedemonstrate a strong correlation, confirming theeffectiveness of our filtering method.To understand the correlation of annotationsamong experts, we calculated Krippendorffs Al-pha for the scores of three experts, resulting in afinal consistency score of 0.858, which proves thatthere is a high level of consistency in the scoresamong the experts.",
  "Cost Analysis": "Our annotation algorithm surpasses traditional ex-pert annotation methods in both cost-effectivenessand time efficiency. The API call cost for the GPT-3.5-turbo model we used generally ranges from$0.001 to $0.004, allowing for the processing ofapproximately 3K requests per minute. In contrast,based on crowdsourcing platform rates, the costfor pairing a query with a code snippet is around$0.2; meanwhile, the time required for an expert toannotate, including reading the query and findinga matching code snippet, typically takes about 3minutes. This demonstrates the superior scalabilityof our method.",
  "Case Study": "As illustrated in , there exists a discrepancybetween the docstring of the code snippet and thequery annotated by us. Docstrings are typicallyemployed to elucidate the functions purpose andusage, possibly encompassing descriptions of inputand output parameters. In contrast, a query repre-sents the functionality requirements described byusers in natural language. We will present morecases in Appendix C.",
  "Conclusion": "In this paper, we addressed the trade-off betweenquality and scalability inherent in the constructionmethods of previous code retrieval datasets by at-tempting to generate queries based on Large Lan-guage Models (LLMs). Initially, we analyzed thekey factors affecting the annotation of queries byLLMs and identified that both intra-repository func-tion calls and third-party API calls significantlyimpacted annotation quality. Based on this un-derstanding, we had designed an annotation algo-rithm that constructed appropriate contexts by pars-ing call relationships to generate function queries.Moreover, we had utilized existing code snippetsto create the Query4Code dataset. Through modelvalidation and manual assessment, the high qual-ity of the Query4Code dataset was confirmed, andcost analysis had demonstrated the scalability ofour annotation approach.",
  "Limitations": "This study primarily focuses on utilizing LargeLanguage Models (LLMs) for the construction ofcode retrieval datasets and demonstrates the signifi-cant impact of call relations on the understandingof function-level code snippets in repositories bylanguage models. However, this paper has certainlimitations. Due to cost considerations, we onlyanalyzed and annotated a Python dataset. Although our analytical method is adaptable across differ-ent programming languages, we cannot guaran-tee that our conclusions will perform consistentlyacross various languages. Therefore, we aim toexplore the construction of code retrieval datasetsfor other programming languages using LLMs infuture work.",
  "Ethical consideration": "This paper explores how large language models(LLMs) can be used for code retrieval data synthe-sis, focusing on their advantages and challenges.One major issue is that LLMs may produce halluci-nations, meaning that the information they generatesometimes appears correct but is actually incorrector irrelevant. This inaccuracy can undermine thequality of the synthetic data, leading to errors incode retrieval. Additionally, using synthetic datamay introduce biases, which could affect the effec-tiveness of the retrieval process, potentially makingit less accurate or fair. This research was supported by grants from theNational Natural Science Foundation of China(Grants No. 62337001, 623B1020), the Funda-mental Research Funds for the Central Universi-ties, and the CIPSCSMP-Zhipu.AI Large ModelCross-Disciplinary Fund. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, andRodrigo Nogueira. 2022. Inpars: Data augmentationfor information retrieval using large language models.arXiv preprint arXiv:2202.05144": "Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-supervised contrastive learning for code retrieval andsummarization via semantic-preserving transforma-tions. In Proceedings of the 44th International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 511521. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, JianmoNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. In The EleventhInternational Conference on Learning Representa-tions. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-bert: A pre-trained model for programming and nat-ural languages. In Findings of the Association forComputational Linguistics: EMNLP 2020, OnlineEvent, 16-20 November 2020, volume EMNLP 2020of Findings of ACL, pages 15361547. Weibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi,Mu-Chun Wang, Jianhui Ma, Shijin Wang, and Yu Su.2021. Rcd: Relation map driven cognitive diagnosisfor intelligent education systems. In Proceedingsof the 44th international ACM SIGIR conference onresearch and development in information retrieval,pages 501510. Mingyang Geng, Shangwen Wang, Dezun Dong, Hao-tian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xi-angke Liao. 2023. An empirical study on using largelanguage models for multi-intent comment genera-tion. arXiv preprint arXiv:2304.11384. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, MingZhou, and Jian Yin. 2022. Unixcoder: Unified cross-modal pre-training for code representation. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), ACL 2022, Dublin, Ireland, May 22-27,2022, pages 72127225. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, DuyuTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-atkovskiy, Shengyu Fu, Michele Tufano, Shao KunDeng, Colin B. Clement, Dawn Drain, Neel Sundare-san, Jian Yin, Daxin Jiang, and Ming Zhou. 2021.Graphcodebert: Pre-training code representationswith data flow. In 9th International Conference onLearning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. Liyang He, Zhenya Huang, Enhong Chen, Qi Liu, Shi-wei Tong, Hao Wang, Defu Lian, and Shijin Wang.2023.An efficient and robust semantic hashingframework for similar text search. ACM Trans. Inf.Syst., 41(4). Liyang He, Zhenya Huang, Jiayu Liu, Enhong Chen,Fei Wang, Jing Sha, and Shijin Wang. 2024. Bit-mask robust contrastive knowledge distillation forunsupervised semantic hashing. In Proceedings ofthe ACM on Web Conference 2024, pages 13951406.",
  "Geert Heyman and Tom Van Cutsem. 2020. Neural codesearch revisited: Enhancing code snippet retrievalthrough natural language intent.arXiv preprintarXiv:2008.12193": "Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. 2023. Unnatural instructions: Tuning lan-guage models with (almost) no human labor.InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pages 1440914428. Association for Computa-tional Linguistics. Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan.2021. Cosqa: 20,000+ web queries for code searchand question answering. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 56905700.",
  "Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,and Kensen Shi. 2020. Learning and evaluating con-textual embedding of source code. In InternationalConference on Machine Learning, pages 51105121": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-zalez, Haotong Zhang, and I. Stoica. 2023. Efficientmemory management for large language model serv-ing with pagedattention. Symposium on OperatingSystems Principles. Raymond Li, Loubna Ben Allal, Yangtian Zi, NiklasMuennighoff, Denis Kocetkov, Chenghao Mou, MarcMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.2023. Starcoder: may the source be with you! arXivpreprint arXiv:2305.06161. Rui Li, Liyang He, Qi Liu, Yuze Zhao, Zheng Zhang,Zhenya Huang, Yu Su, and Shijin Wang. 2024. Con-sider: Commonalities and specialties driven multi-lingual code retrieval framework. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 86798687. Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu,Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang,Weizhu Chen, and Nan Duan. 2022. Coderetriever:A large scale contrastive pre-training method for codesearch. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 28982910. Jiayu Liu, Zhenya Huang, Chengxiang Zhai, and Qi Liu.2023. Learning by applying: A general frameworkfor mathematical reasoning via enhancing explicitknowledge learning. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 37, pages44974506. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, AlexeySvyatkovskiy, Ambrosio Blanco, Colin B. Clement,Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-dong Zhou, Linjun Shou, Long Zhou, Michele Tu-fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-daresan, Shao Kun Deng, Shengyu Fu, and ShujieLiu. 2021. Codexglue: A machine learning bench-mark dataset for code understanding and generation.In Proceedings of the Neural Information Process-ing Systems Track on Datasets and Benchmarks 1,NeurIPS Datasets and Benchmarks 2021, December2021, virtual. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822. Nikitha Rao, Chetan Bansal, and Joe Guan. 2021.Search4code: Code search intent classification usingweak supervision. In 2021 IEEE/ACM 18th Interna-tional Conference on Mining Software Repositories(MSR), pages 575579. IEEE. Daniel Rodriguez-Cardenas, David N Palacio, DipinKhati, Henry Burke, and Denys Poshyvanyk. 2023.Benchmarking causal study to interpret large lan-guage models for source code. In 2023 IEEE Inter-national Conference on Software Maintenance andEvolution (ICSME), pages 329334. IEEE. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, et al. 2023.Code llama: Open foundation models for code. arXivpreprint arXiv:2308.12950. Vinay Samuel, Houda Aynaou, Arijit Ghosh Chowd-hury, Karthik Venkat Ramanan, and Aman Chadha.2023. Can llms augment low-resource reading com-prehension datasets? opportunities and challenges.arXiv preprint arXiv:2309.12426. Ivan Sedykh, Dmitry Abulkhanov, Nikita Sorokin,Sergey Nikolenko, and Valentin Malykh. 2023.Searching by code: a new searchbysnippet datasetand snipper retrieval model for searching by codesnippets. arXiv preprint arXiv:2305.11625. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. 2023a. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), ACL 2023, Toronto, Canada, July 9-14, 2023,",
  "pages 1348413508. Association for ComputationalLinguistics": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare,Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023b.Codet5+: Open code large language models forcode understanding and generation. arXiv preprintarXiv:2305.07922. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Peter West, Chandra Bhagavatula, Jack Hessel, JenaHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,Sean Welleck, and Yejin Choi. 2022.Symbolicknowledge distillation: from general language mod-els to commonsense models. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 46024625. Ziyu Yao, Daniel S Weld, Wei-Peng Chen, and HuanSun. 2018. Staqc: A systematically mined question-code dataset from stack overflow. In Proceedings ofthe 2018 World Wide Web Conference, pages 16931703. Pengcheng Yin, Bowen Deng, Edgar Chen, BogdanVasilescu, and Graham Neubig. 2018. Learning tomine aligned code and natural language pairs fromstack overflow. In Proceedings of the 15th interna-tional conference on mining software repositories,pages 476486.",
  "Junlei Zhang, Zhenzhong Lan, and Junxian He. 2023a.Contrastive learning of sentence embeddings fromscratch. arXiv preprint arXiv:2305.15077": "Zheng Zhang, Qi Liu, Zirui Hu, Yi Zhan, Zhenya Huang,Weibo Gao, and Qingyang Mao. 2024. Enhancingfairness in meta-learned user modeling via adaptivesampling. In Proceedings of the ACM on Web Con-ference 2024, pages 32413252. Zheng Zhang, Qi Liu, Hao Jiang, Fei Wang, YanZhuang, Le Wu, Weibo Gao, and Enhong Chen.2023b. Fairlisa: Fair user modeling with limitedsensitive attributes information. In Thirty-seventhConference on Neural Information Processing Sys-tems. Yuze Zhao, Zhenya Huang, Yixiao Ma, Rui Li, KaiZhang, Hao Jiang, Qi Liu, Linbo Zhu, and Yu Su.2024.RePair: Automated program repair withprocess-based feedback. In Findings of the Associa-tion for Computational Linguistics ACL 2024, pages1641516429, Bangkok, Thailand and virtual meet-ing. Association for Computational Linguistics.",
  "AAnalysis Settings": "We use the CodeLlama-Instruct 7B and GPT-3.5-turbo, where we load the checkpoint forCodeLlama-Instruct 7B from huggingface. ForGPT-3.5-turbo, we chose to experiment with thegpt-3.5-turbo-0613 version. And we use the GPT-4-turbo model for scoring, where we select thegpt-4-1106-preview version for experimentation.For GPT model, we use the official OpenAI APIand employ the default temperature parameters andsampling methods.",
  "System Prompt for Generating Query (w/ Context)": "Please act as a query generator. For the given function-level codesnippet in the repository and the information about functions called within those code snippets, please provide a query that the user might use. This query should be able to search for that function in a search engine. Note that you should not provide any other information.",
  "Verification System Prompt for Query": "Please play the role of a programming expert. For the given user queries and function pairs, please judge whether the code can meet the needs of the user's query based on the following principles:1. The code can answer and exceed the requirements for query needs (3 points);2. The code can satisfy a certain category of query needs (2 points);3. The code only meets less than 50% of query needs (1 points);4. The code is only minimally related to the query (0 point).Please provide an explanation along with corresponding scores, noting that you need to output in JSON format as follows: `{\"Explanation\": <explanation>, \"Score\": <score>}`, without providing any other information",
  "System Prompt for Rating APIs": "Please play the role of a programming expert. For a given API and its corresponding documentation explanation, as well as a user's description of the API's functionality, please help me confirm the degree to which the user-provided description of the API's functionality matches with what is described in the documentation. If it completely matches semantically, award 2 points; if it partially matches, give 1 point; if there is no match, give 0 points.Please provide an explanation along with corresponding scores, noting that you need to output in JSON format as follows: `{\"Explanation\": <explanation>, \"Score\": <score>}`, without providing any other information.",
  "System Prompt for Generating Summary": "Please play the role of a programmingexpert.For the functions in a given repositoryand the description of third-party APIfunctionalities called within thosefunctions, as well as summaries offunctionalities for functions calledwithin the repository, please provide asummary of the specified code'sfunctionality. Note that you need tooffer a concise summary of the coderather than step-by-step explanations,and there is no need to reply with anyadditional information.",
  "return text": "Ground Truth: remove all non numeric characters python Generated Query: Python function to clean text by replacing unicodecharacters and genitives Generated Summary: The clean_text function performs text cleaning by replacing specific Unicode characters with their ASCII equivalents and removing genitive constructions. It logs the cleaned text for debugging purposes and then returns it.",
  "return rounded": "Ground Truth: change letter into number in pythonGenerated Query: Round to nearest integer pythonGenerated Summary: The round_to_int function rounds a given number to the nearest integer multiple of a specified precision. From the above example, it can be seen that thequery is often more concise and may exist in theform of questions or phrases, while the summaryincludes a description of the overall functionality ofthe code. Note that when the function is relativelysimple, the summary and query are often similar."
}