{
  "Abstract": "Fine-tuning large language models (LLMs) hasachieved remarkable performance across var-ious natural language processing tasks, yet itdemands more and more memory as modelsizes keep growing. To address this issue, therecently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tuneLLMs using only forward passes, therebyavoiding the need for a backpropagation graph.However, significant performance drops anda high risk of divergence have limited theirwidespread adoption. In this paper, we pro-pose the Adaptive Zeroth-order Tensor-TrainAdaption (AdaZeta) framework, specificallydesigned to improve the performance and con-vergence of the ZO methods.To enhancedimension-dependent ZO estimation accuracy,we introduce a fast-forward, low-parameter ten-sorized adapter. To tackle the frequently ob-served divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive querynumber schedule that guarantees convergence.Detailed theoretical analysis and extensiveexperimental results on Roberta-Large andLlama-2-7B models substantiate the efficacy ofour AdaZeta framework in terms of accuracy,memory efficiency, and convergence speed.1",
  "Code available on GitHub": "training process consumes progressively moreGPU memory. In recent years, approaches suchas quantization (Tian et al., 2023; Dettmers et al.,2024) and parameter-efficient fine-tuning (PEFT)(Hu et al., 2021) have been proposed to reducememory costs during training by storing data withlower bit-depth or updating only a portion of theparameters. Despite these strategies effectivelyreducing memory costs, overall memory usageremains high due to the continuous reliance on abackpropagation graph. To further reduce the memory overhead, (Mal-ladi et al., 2023) proposed the Memory-efficientZeroth-order (MeZO) method for LLM fine-tuning,which shows over 8 memory reduction comparedwith the first-order (FO) fine-tuning methods likeSGD (Amari, 1993) and AdamW (Loshchilovand Hutter, 2018). Unlike FO methods, whichcalculate gradients via backpropagation,theMeZO method estimates gradients based on thedifference between loss values obtained fromtwo forward passes, thereby eliminating the needfor a backpropagation graph.However, twomain challenges persist in the zeroth-order (ZO)fine-tuning of LLMs: 1) a significant performancegap between FO and ZO approaches, and 2)increased risk of divergence, particularly in theZO fine-tuning of large-scale LLMs, as observedin recent studies (Gautam et al., 2024). Toimprovetheperformance,variousFOoptimization techniques have been adapted for ZOfine-tuning scenarios, like the ZO-AdaMU method(Jiang et al., 2024). However, these approaches failto accommodate the specific needs of ZO methods,and add significant memory overhead from theoptimizer state. Given the dimensionality-related : The evaluation loss curves for the SST-2, WiC, and CB tasks using the Llama-2-7B model. The proposedAdaZeta method converges faster and effectively addresses the divergence problem using a much smaller batch size(BS). Both MeZO-LoRA and AdaZeta use a learning rate of 1e-4, while Sparse-MeZO utilizes a 1e-6 learning rate. nature of ZO convergence rates, (Liu et al., 2024)propose the Sparse-MeZO method that generatespruning masks based on the value of the weightelements. Nevertheless, the Sparse-MeZO methodyields inconsistent performance across varioustasks and hyperparameter configurations.Incontrast to this approach, we consider using thePEFT method to reduce the number of trainableparameters. Although the ZO PEFT method likeMeZO-LoRA has been considered in (Malladiet al., 2023), the improvements are limited as theLoRA adapter fails to offer high representationalability with an ultra-low rank.To solve thisproblem, we involve tensorized adapters, whichoffer high performance with even lower trainableparameters than LoRA adapters. Toaddressthevariance-relateddivergenceissue in large-scale ZO fine-tuning, previousstudies (Malladi et al., 2023; Jiang et al., 2024)have primarily focused on adjusting the batchsize, as increasing the batch size can reduce thenoise in ZO gradient estimation. However, theseapproaches introduce significant runtime overheadand fail to improve performance significantly. Tofurther reduce variance, (Gautam et al., 2024)introduced the MeZO-SVRG method, adaptingthe first-order SVRG technique to the ZO context.Despite its success, MeZO-SVRG suffers from aslow and memory-inefficient fine-tuning processdue to the additional parameter copies and compu-tation process that even doubles the memory costof the MeZO methods. In contrast to these works,we consider reducing the ZO gradient variance",
  ": Illustration for tensorized linear layer andtensorized adapters": "Liang, 2021; Liu et al., 2022). In (Malladi et al.,2023), researchers try to employ the LoRA andprefix-tuning (Li and Liang, 2021) methods duringthe ZO fine-tuning. However, the improvementis limited and the detailed analysis of ZO PEFTtuning is not discussed. In this paper, we explore tensorized adapters,an ultra-low-parameter PEFT method that com-presses the weight matrices of adapter layersusing Tensor-Train (TT) decomposition.Thisapproach is examined in (Yang et al., 2024a),where it demonstrates strong performance inFO fine-tuning tasks. However, the contractionprocess of TT format (Oseledets, 2011; Novikovet al., 2015) involving a sequence of small tensorfactors slows down the forward pass, making itless suitable for ZO methods that require twoforward passes per step. To solve this problem, wepropose parallel contraction methods to improvethe inference speed of tensorized adapter methods.",
  "Tensorized Adapters": "As shown in (a), the tensorized adapters,which are built upon tensorized linear layers, arelightweight components injected during the fine-tuning process to reduce the number of trainableparameters. The weight in tensorized linear layersis represented in the TT format. Compared with a standard weight matrix W Rmn in a typicallinear layer, the TT format represents its reshaped2o-way tensor W Rk1k2o as a sequenceof tensor factors [G1, , Go, Go+1, G2o] (Os-eledets, 2011), where each tensor factor Gi Rri1kiri has rank ri1 and ri. The dimensionski are constrainted such that oi=1ki = m and2oj=o+1kj = n. During the forward pass, the se-quence of tensor factors is contracted and reshapedback into the shape of a weight matrix as",
  "W = Reshape(G1 G2o).(1)": "Note that in this paper, the tensor rank is heldconstant, with the exception of the first and lastranks, which are set r0 = r2o = 1. Also, theweights in tensorized layers are initialized, stored,and updated in TT-format instead of the matrixform in a traditional linear layer. The structure of tensorized adapters is shownin (b). Each tensorized adapter containstwo tensorized layers and a non-linear layerin between.For each encoder/decoder block,the tensorized adapters are attached after theattention and feed-forward layer. Different from(Yang et al., 2024a) that makes both tensorizedadapters and layer norm trainable, we freezethe layer norm during the ZO fine-tuning, asnoisy gradient estimation of the scaling factor inlayer normalization can seriously degrade modelperformance.The tensorized adapters reducetrainable parameters by over 80, making them abetter fit for ZO fine-tuning.",
  "Methods": "In this section, we first introduce some basic knowl-edge of the ZO gradient estimator.Then, wepresent our AdaZeta method, a powerful frame-work designed to improve the performance of ZOLLM fine-tuning with two main components: 1)the fast-forward tensorized adapters, and 2) anadaptive query number schedule. Finally, we pro-vide a theoretical analysis of the convergence rateof the AdaZeta method, demonstrating the im-proved convergence rate theoretically.",
  "Traditional ZO estimation has been widely studiedin both convex and non-convex optimization se-": "tups (Ghadimi and Lan, 2013; Malladi et al., 2023;Chen et al., 2019). In our problem, considering asupervised dataset D, mini-batch B with the sizeof D and B respectively, we set the loss functionfor our fine-tuning problem to be (w; B), wherethe trainable parameter in the tensorized adaptersw Rd has a size of d. Then, the RandomizedZeroth-order Gradient Estimation (RGE) at train-ing step k is given as:",
  "where Qk is the query number at the trainingstep k, zq N(0, Id) is the vector-wise randomperturbation for each query q, and is a scalingfactor for the perturbation": "Unlike FO fine-tuning, which relies on back-propagation, RGE requires only two forwardpasses with perturbations added to the weightsof tensorized adapters, eliminating the needfor a backpropagation graph.Additionally, bysublinearly increasing the number of queries at thebeginning of each epoch, we effectively reducethe variance of the ZO gradient estimation byinvolving distinct perturbations zq at each time ofquery. Details of the setup will be discussed in thefollowing section.",
  "The AdaZeta Framework": "Previous ZO fine-tuning methods, such as MeZO,typically estimate the gradient for a large numberof trainable parameters simultaneously using RGE.This approach results in high variance due to thedimension-related nature of the RGE method.Although techniques like LoRA and prefix tuninghave been considered, few works consider thetasks-specific PEFT adapters for the ZO LLMsfine-tuning. Additionally, as shown in , wehave observed an increased risk of divergencewhen using the MeZO-LoRA method duringfine-tuning. To address these issues, we proposeour AdaZeta framework to improve performanceand solve the instability problem of the vanillaMeZO method.Our framework includes thefollowing components:",
  ": end for": "Parameter-efficient issue has been widely studiedin the FO cases, where people often freezethe pre-trained model parameters and fine-tunethe LLMs by adding trainable adapters alongwith the frozen pretrain weights. Since the ZOestimationaccuracyisdimension-dependent,reducing dimensionality can significantly helpimprove the gradient estimation quality.Thus,we consider injecting the ultra-low parametertensorized adapters in our AdaZeta framework toreduce the number of trainable parameters whileretaining the performance. As we have mentioned, ZO fine-tuning mainlyrelies on gradient estimation with two forwardpasses at each step. Thus, the speed of the forwardpass is a crucial factor for the overall speed ofZO fine-tuning. Instead of using the sequentialcontraction method during the forward pass asin previous work, we propose a new parallelcontraction method to speed up the forward passes.This method divides the sequence of tensor factorsinto several groups to enable parallel processingand avoid the presence of high-dimensionaltensors. Taking a bipartite case as an example, the",
  "j=o+1Gj),": "where Gi represents the i-th tensor factor, R()represents the reshape operation.For largermodels, the tensor factors can be organized intotripartite or quadripartite structures to acceleratethe inference speed of the tensorized methods. Adaptive Query Adjustment for ZO esti-mation. As previously noted, the training processfor existing ZO methods often exhibits instability,particularly with large-size models where diver-gence issues frequently occur. Previous studies(Chen et al., 2019; Jiang et al., 2024) have exploredusing a fixed multiple queries scheme to improvethe estimation accuracy in the optimizationcommunity. However, utilizing a fixed numberof queries may significantly hinder the trainingefficiency of large-scale ZO fine-tuning tasks, asnaively increasing the number of perturbationsgreatly escalates training durations.To solvethis problem, we consider a simple but effectivesublinear increasing query number adjustmentschedule, where the number of queries is updatedat the beginning of each epoch ek. By expressingthe epoch in terms of the global training steps asek = k/ D",
  "Qk := min(ek, Qmax)(2)": "with a fixed scaling factor (0, 1), a sublinearincreasing factor (0, 1) and a max querythreshold Qmax. Then, the query number is fixedfor all training steps within each epoch.Thisadjustment solves all divergence problems weobserved with theoretical guarantee and performseven faster than the traditional way to solve thedivergence problem for ZO LLMs fine-tuning byincreasing the batch size. Thecorrespondingoptimizationalgorithmused in the AdaZeta framework is shown inAlg.1.We adjust the query number at thebeginning of each epoch.Different from theMeZO algorithm, we obtain the gradient usedfor the model update by taking the average overmultiple query results. Note that we fix the query number to be 1 when fine-tuning medium-sizemodels like Roberta-Large since the noise of ZOestimation is relatively low when the number oftrainable parameters is small. Later, we will showthat a sublinear increasing query number benefitsthe convergence of the problem when the modelsize is large, both theoretically and experimentally.",
  "Theoretical Analysis": "In this subsection, we give the theoretical analysisfor the AdaZeta framework.Our theoreticalanalysis highlights why the tensorized adapter andadaptive query schedule can significantly helpto improve the ZO convergence rate. Unlike thetheoretical analysis in the MeZO paper, whichfocuses on the effective rank for the Hessian ofloss, we focus on the dimension of the optimizedmodels d (number of trainable parameters)instead. As the trainable parameters with PEFTadapters are much smaller than the model size,the theoretical analysis based on the exact dimen-sion of the optimization problem can better helpus explore the behavior of different PEFT methods. To align our analysis with LLM fine-tuning,we consider a non-convex optimization setup andstudy the convergence behavior regarding thetraining steps k. It is important to note that the ZOestimated gradient by the RGE, is an unbiasedestimation of the true gradient when 0,which gives the fact Ez[] = (Nesterovand Spokoiny, 2017). First, we list the followingassumptions for our analysis:",
  ",": "which guarantees the true gradient approaches zerowhen K . In contrast, using a small con-stant such as Q = 1 results in an upper boundof O(C(d, )/K), which becomes challenging tominimize due to the term C(d, ) is directly propor-tional to the model size d. Additionally, we observethat the convergence rate is significantly influencedby the model dimension d. Consequently, in thispaper, we also try to reduce the number of trainableparameters with the tensorized adapters.",
  "Experiments": "In this section, we conduct comprehensive experi-ments to evaluate the performance of our proposedAdaZeta framework across several LLMs withdifferent scales on a variety of natural languageunderstanding and generation tasks (Socher et al.,2013; Williams et al., 2017; Rajpurkar et al.,2016). We demonstrate that our methods surpassacomprehensivearrayofmemory-efficientbaselines, including inference-only methods such as Zero-shot (Brown et al., 2020), In-ContextLearning (ICL), and Linear Probing (LP) (Kumaret al., 2021), as well as ZO fine-tuning methodslike MeZO, MeZO-LoRA (Malladi et al., 2023),and Sparse-MeZO (Liu et al., 2024).Also,the first-order fine-tuning (FT) baseline is alsoprovided as a reference. Initially,wepresentexperimentalevidenceusing Roberta-Large models (Liu et al., 2019),illustrating that the integration of tensorizedadapters can significantly enhance the efficiencyof ZO fine-tuning by reducing the number oftrainable parameters. Subsequently, we enabledour proposed adaptive query schedule method toshow the effectiveness of the AdaZeta frameworkon large-scale Llama-2-7B models (Touvron et al.,2023), which not only enhances performance butalso ensures robust convergence. All experimentsare conducted on NVIDIA Tesla A100-40GBGPUs, with further details about the experimentalsetup available in Appendix A.",
  "Medium-size Roberta-Large Models": "We initially evaluated the effectiveness of usingtensorized adapters on RoBERTa-large modelsacross various tasks, including single-sentencetasks like SST-2 and SST-5, natural languageinference tasks such as QNLI, MNLI, SNLI,RTE, and the sentiment analysis dataset MovieReviews (MR). The results are summarized in.Experiments were conducted undera 16-shot setup, with 16 data samples in eachclass of the datasets. We monitored the best testaccuracy every 500 steps, using a test pool of1,000 data samples. Note that, similar to previous",
  "FT61.766.184.663.465.994.045.486.081.190.7LoRA85.567.884.862.573.994.885.081.079.490.5": "Zero-Shot49.532.165.136.550.679.755.859.780.954.7ICL54.558.967.465.452.781.258.784.480.167.1MeZO54.673.068.652.857.885.862.686.070.872.5MeZO-LoRA59.674.071.653.055.286.867.289.072.080.0Sparse-MeZO58.676.067.853.056.885.261.286.070.664.4AdaZeta74.075.079.452.258.091.068.294.071.280.0 ZO fine-tuning studies, we fixed the number ofqueries to 1 in this subsection. This decisionis based on the observation that gradient noiseis relatively small in medium-sized Bert-basedmodels.The following conclusions have beenreached: AdaZetaShowsHigherAccuracythanOther ZO Fine-Tuning Methods. According toour observations in , AdaZeta outperformsother ZO fine-tuning approaches in terms of eval-uation accuracy. Compared with MeZO-LoRA,which also involves PEFT adapters, AdaZetaoutperforms in 5 out of 7 tests under both 16 and64 batch size (BS) settings. This advantage showsthe effectiveness of improving ZO estimationaccuracy by further reducing the number oftrainable parameters with the tensorized adapter.This is supported by the dimension-relatedconvergence rate proved in .3. AdaZetaDemonstratesImprovedConver-gence. Compared to the MeZO-LoRA method,the AdaZeta method exhibits superior convergencewhen the batch size is 16.Given our 16-shottraining setup, it is reasonable to expect that the16 batch size scenario would outperform the 64batch size scenario if the fine-tuning processconverges effectively. However, a performancedecline is observed with the MeZO-LoRA method,indicating that it is adversely affected by ZOgradient noise.Comparatively, the AdaZetamethod achieves consistent results across bothsetups by reducing such noise with less trainableparameters, effectively showcasing its ability toaid in convergence.",
  "Large-scale Llama-2 Models": "In the previous section, we demonstrated howutilizing the tensorized adapter method enhancesZO fine-tuning performance by reducing gradientnoise through a decrease in trainable parameters.In this section, we assess the effectiveness of theAdaZeta framework with the large-scale Llama-2-7B model. Differing from the experiments on theRoberta-Large models, we enabled the adaptivequery schedule method proposed in our AdaZetaframework to mitigate the commonly observeddivergence issues in large-scale ZO fine-tuning. To highlight the challenge of our experiments,we adopt a low-data resource approach usingdatasets from SuperGLUE (Wang et al., 2019)and generative tasks such as SQuAD (Rajpurkaret al., 2016) and DROP (Dua et al., 2019). Ourexperimental protocol follows the prompted-basedfine-tuning strategy outlined in the MeZO paper(Malladi et al., 2023). The quantitative resultsare summarized in and the trainingcurves have been shown in . Note that it isreasonable to observe some large accuracy gapbetween different methods under different tasks,which has also been observed in previous MeZOand PEFT papers (Malladi et al., 2023; Hu et al.,2023). The following conclusions are drawn: AdaZetaMethodDemonstratesSuperiorPerformance Over Traditional ZO Fine-Tuning.The AdaZeta framework delivers exceptionalaccuracy results across a variety of tasks, out-performing all ZO baseline methods such asMeZO and MeZO-LoRA in 8 out of 10 tasks.Compared with traditional inference-only methodslike ICL and Zero-shot, AdaZeta significantlysurpasses them with respect to test accuracy.",
  "Moreover, the AdaZeta method even outperformsthe FO-AdamW methods over several tasks likeRTE, CB, and COPA, which require 8 moreGPU memory": "AdaZetaMethodEffectivelyAddressesDivergence Issues in ZO Fine-Tuning.Wecan observe from the table that the MeZO andMeZO-LoRA methods achieve unsatisfied resultsin some tasks like SST2, RTE, and BoolQcompared with our proposed method, which isled by the convergence issue.Also, we haveshown that the AdaZeta method achieves lowerevaluation loss much faster than the MeZO-LoRAand Sparse-MeZO methods across all tasks in. For example, the MeZO-LoRA methodrequires nearly 6K steps to achieve a loss of 0.4,whereas the AdaZeta method achieves the samedegree of loss minimization in less than 1K steps,which represents a 6 speed-up with the same1e-4 learning rate. Traditional ways to solve suchdivergence issues through increasing the batchsize are hard to follow in the large-scale LLMsfine-tuning tasks. In contrast, the adaptive queryschedule in the AdaZeta framework successfullymitigates this issue without increasing the trainingmemory, thereby improving training outcomes.Additionally, we observed that combining LoRAwith the adaptive query schedule significantlyimproves performance in certain tasks. Futurework could also explore incorporating the adaptivequery schedule into the MeZO-LoRA method tofurther enhance stability.",
  "According to (refer to Appendix B.1": "for numerical results), the AdaZeta methodrequires only 14GB of memory to fine-tune theSST2 tasks on the Llama-2-7B model, whichachieves over 8 Memory Reduction Relative tothe FT Method. Also, compared with other ZOfine-tuning methods like MeZO, MeZO-LoRA,and Sparse-MeZO, the AdaZeta method utilizessimilar or even less memory to achieve variancereduction.Traditional ways to reduce the ZOgradient estimation noise like increasing the batchsize, consume significantly more memory than theAdaZeta method as shown in . In , we measure the total GPU hoursrequired to achieve a certain threshold of trainingloss across four tasks (SST2, WIC, CB, MultiRC).For the applicability of the experiments, weestablished an evaluation loss threshold that allmethods could achieve. According to the results,it is evident that the AdaZeta method convergeson-par or faster than other ZO fine-tuning methodswith even better results than the MeZO-LoRA andSparse-MeZO methods under the large-batch sizecase. Note that we did not utilize the gradientaccumulation technique for the 64 batch size case,which may significantly increase the training time.",
  "Further Comparison with LoRA": "In this section, we further compare our AdaZetamethod with the first-order LoRA method in termsof training memory usage across different ranksand batch sizes. The results for the CB task arepresented in .We make the followingobservations under two scenarios: Reducing the LoRA Rank:Reducing theLoRA rank (even down to 1) has minimal impacton training memory in the first-order setting. Thereason is that the backpropagation graphwhichcontains intermediate gradient informationstillneeds to be retained, spanning almost the entiremodel in the vanilla LoRA approach. Reducing the Batch Size:Reducing thebatch size is a more effective way to reduce thetraining memory for both FO and ZO cases. Withthe existence of a backpropagation graph, it isreasonable to observe a larger reduction of trainingmemory of the FO method than ZO when reducingthe number of batch sizes.However, we canobserve that even when comparing our methodwith the LoRA method using a batch size of 1,our method is still 2.5 more memory-efficient.Additionally, even comparing AdaZeta/r=8/BS=16with LoRA/r=1/BS=1, we still achieve nearly a50% reduction in memory usage. However, wewould like to remark that the batch size of 1 setupis rarely used in practice due to the followingreasons:",
  "In this paper, we propose an adaptive zeroth-orderfine-tuning framework with tensor-train decompo-": "sition, named AdaZeta. Compared with previousZO fine-tuning works, the AdaZeta methodachieves significantly better fine-tuning resultsacross various tasks and models.Theoreticalanalysis has confirmed that our proposed methodsenjoy better convergence, which is consistent withour experimental results on both Roberta-Largeand Llama-2 models across various fine-tuningtasks. Futureworkcouldexploreimprovingtheefficiency of the AdaZeta method by implementingdistributed optimization across multiple GPUsfor handling multiple queries concurrently ateach step.Additionally, applying the adaptivequery schedule to other PEFT methods may yieldsignificantly better performance compared to theoriginal MeZO algorithm.",
  "Acknowledgements": "This project was supported by Amazon. We extendour gratitude to Siegfried Kunzmann, JiajunZhou, Clement Chung, Samridhi Choudhary, HieuNguyen and the many other colleagues at AmazonAGI and UCSB who engaged in discussions thatshaped this work. Thisresearchalsoutilizedresourcesfromthe National Energy Research Scientific Comput-ing Center (NERSC), a U.S. Department of EnergyOffice of Science User Facility, supported underContract No.DE-AC02-05CH11231 throughNERSC award ASCR-ERCAP0030039.",
  "Limitations": "The primary limitation of this work is related toaccelerating the proposed method. Currently, mul-tiple queries at each training step are executed se-quentially in a for-loop, which restricts furtherspeed enhancements.This process can poten-tially be optimized by implementing parallel ordistributed optimization techniques on GPUs, al-lowing for the simultaneous execution of multiplequeries, as these queries are independent of each",
  "Potential Risks": "This paper provides a cost-effective solution thatoperates with a minimal memory footprint. Eventhough we need to fine-tune large-scale models,the proposed method can alleviate the burden ondata centers and reduce CO2 emissions. However,we acknowledge that prolonged training times, es-pecially with multiple GPUs, can pose environ-mental challenges. Consequently, our ongoing re-search endeavors are focused on developing moreefficient training methods and preserving compu-tational power with ecological considerations inmind.",
  "Shun-ichi Amari. 1993. Backpropagation and stochas-tic gradient descent method. Neurocomputing, 5(4-5):185196": "Samuel Bowman, Gabor Angeli, Christopher Potts, andChristopher D Manning. 2015. A large annotatedcorpus for learning natural language inference. InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages 632642. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Sebastien Bubeck et al. 2015. Convex optimization: Al-gorithms and complexity. Foundations and Trendsin Machine Learning, 8(3-4):231357": "Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, XueLin, Mingyi Hong, and David Cox. 2019.Zo-adamm: Zeroth-order adaptive momentum methodfor black-box optimization. Advances in neural in-formation processing systems, 32. Xuxin Cheng, Zhihong Zhu, Ziyu Yao, Hongxiang Li,Yaowei Li, and Yuexian Zou. 2023. Ghostt5: gener-ate more features with cheap operations to improvetextless spoken question answering. In Proc. INTER-SPEECH, pages 11341138.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-janya Poria. 2023. Llm-adapters: An adapter familyfor parameter-efficient fine-tuning of large languagemodels. arXiv preprint arXiv:2304.01933. Shuoran Jiang, Qingcai Chen, Youcheng Pan, Yang Xi-ang, Yukang Lin, Xiangping Wu, Chuanyi Liu, andXiaobao Song. 2024. Zo-adamu optimizer: Adapt-ing perturbation by the momentum and uncertaintyin zeroth-order optimization.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1836318371.",
  "Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. 2019. Bert: Pre-training of deep bidirec-tional transformers for language understanding. InProceedings of NAACL-HLT, pages 41714186": "Ananya Kumar, Aditi Raghunathan, Robbie MatthewJones, Tengyu Ma, and Percy Liang. 2021. Fine-tuning can distort pretrained features and underper-form out-of-distribution. In International Confer-ence on Learning Representations. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-fel. 2022. Few-shot parameter-efficient fine-tuningis better and cheaper than in-context learning. Ad-vances in Neural Information Processing Systems,35:19501965. Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, PaishunTing, Shiyu Chang, and Lisa Amini. 2018. Zeroth-order stochastic variance reduction for nonconvexoptimization. Advances in Neural Information Pro-cessing Systems, 31. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016.Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprintarXiv:1606.05250": "Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts. 2013. Recursive deep mod-els for semantic compositionality over a sentimenttreebank. In Proceedings of the 2013 conference onempirical methods in natural language processing,pages 16311642. Jiayi Tian, Chao Fang, Haonan Wang, and ZhongfengWang. 2023. Bebert: Efficient and robust binaryensemble bert. In ICASSP 2023-2023 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 15. IEEE. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix,Baptiste Rozi`ere, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel Bowman. 2019. Superglue: A stick-ier benchmark for general-purpose language under-standing systems. Advances in neural informationprocessing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. arXiv preprintarXiv:1804.07461.",
  "Yifan Yang, Jiajun Zhou, Ngai Wong, and ZhengZhang. 2024a.Loretta:Low-rank economictensor-train adaptation for ultra-low-parameter fine-tuning of large language models.arXiv preprintarXiv:2402.11417": "Zi Yang, Samridhi Choudhary, Xinfeng Xie, CaoGao, Siegfried Kunzmann, and Zheng Zhang. 2024b.Comera: Computing-and memory-efficient trainingvia rank-adaptive tensor optimization. arXiv preprintarXiv:2405.14377. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.2022. Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 19. Tianyi Zhang, Faisal Ladhak, Esin Durmus, PercyLiang,Kathleen McKeown,and Tatsunori BHashimoto. 2024. Benchmarking large languagemodels for news summarization. Transactions of theAssociation for Computational Linguistics, 12:3957.",
  "SST-2AccuracySST-5AccuracyQNLIAccuracyMNLIMatched Acc.SNLIAccuracyRTEAccuracy": "Our research utilized a variety of tasks tomeasure the performance of the Roberta-Largemodel, including sentiment analysis (SST-2, SST-5(Socher et al., 2013), MR (Pang et al., 2002)),and natural language inference (MNLI (Wanget al., 2018), QNLI (Williams et al., 2018), SNLI(Bowman et al., 2015), RTE (Wang et al., 2018))tasks. summarizes the evaluation metricsused for these tasks. Further, we extended our experiments on alarge-scale Llama-2-7B model to include tasksfrom the SuperGLUE benchmark (Wang et al.,2019), which involves both classification (CB,BoolQ, WSC) and reasoning tasks (COPA andReCoRD), as well as additional generation tasks,SQuAD (Rajpurkar et al., 2016). For these tests,we introduced a challenging low-resource datacondition, limiting our samples to 1,000 fortraining, 500 for validation, and 1,000 for testing,as detailed in the prompt-based task settings fromAppendix D of (Malladi et al., 2023). The metricsfor these evaluations are outlined in .",
  "In this section, we provide a detailed introductionto the baseline method considered in our experi-ments, which are listed as follows:": "Full-modelFirst-OrderFine-Tuning(FT)is the most widely used method for fine-tuningLLMs. In this process, the model is initializedwith pre-trained weights, and all model parametersare updated by the first-order optimizer. In thispaper, the AdamW optimizer (Loshchilov andHutter, 2018) is used to conduct the first-orderexperiments. Zero-shot/In-context-learning(ICL)isthemost widely used method for fine-tuning largelanguage models (LLMs). In this process, themodel is initialized with pre-trained weights,and all model parameters are updated by thefirst-order (FO) optimizer.In this paper, theAdamW optimizer (Loshchilov and Hutter, 2018)is used to conduct the first-order experiments. Linear-probing (LP) method involves freezingthe pretrained weights of the model and addinga final linear classifier layer, implemented usingthe scipy package. By fine-tuning this layer withthe first-order method, we only need to constructa small backpropagation graph. However, thismethod is not suitable for generative tasks.Therefore, we only apply the LP method in theRoberta-Large experiments. Memory-EfficientZeroth-Order(MeZO)was first proposed in (Malladi et al., 2023),which fine-tunes LLMs using only the forwardpass. The MeZO method significantly reducesmemory costs by eliminating the need for abackpropagation graph and has demonstratedsuperior performance compared to inference-onlymethods like Zero-shot, ICT, and LP methodsacross various downstream tasks. Memory-Efficient Zeroth-Order with LoRAadapters (MeZO-LoRA) is a derivative methodintroduced in (Malladi et al., 2023), which freezesthe pretrained weights and fine-tunes only theinjected LoRA adapters (Hu et al., 2021). TheMeZO-LoRA method is the most relevant baseline in this field compared to our work.However,its performance improvement over the MeZOmethod is limited, and the mechanisms behindzeroth-order parameter-efficient fine-tuning arenot extensively discussed. SparseMemory-efficientZeroth-Order(Sparse-MeZO) is a recently proposed methodaiming to enhance the performance and conver-gence speed of the MeZO method (Liu et al., 2024).However, as the code and detailed layer-wisehyperparameter setup have not been released, wehave reproduced the method using a fixed sparsityratio for each layer. This ratio is selected based onthe best overall outcome as presented in oftheir paper.",
  "A.3Hyperparameters": "In this section, we outline the detailed setup ofhyperparameters utilized in our study. The specificchoices of hyperparameters, such as learning rate,training steps, and batch size, are summarized in. In our experiments, we strive to maintaina consistent learning rate across different methodsfor the same tasks.However, for approacheslike full-model fine-tuning, we opt for a lowerlearning rate to ensure convergence. This principleis also applied in our large-scale experimentson the Llama-2-7B model, details of which aresummarized in . In addition to the standard hyperparameterconfiguration, we also consider the shape oftensor factors in our methods.To representa layer with input and output dimensions ofo and p, respectively, we employ a list of mtensor factors Gi Rrkir, where the productk1 km = o p. The specific shapes of kicorresponding to different values of o and p, givena bottleneck size of 8 or 64 for the tensorizedmethods, are detailed in . Note that theoptimal factors shape and tensor rank for thetensor-train method can only be determined bythe experiments trail. However, previous workalso explores the possibility of utilizing theadaptive rank to improve the performance (Yanget al., 2024b), which may further improve theperformance of our AdaZeta method. : The hyperparameter grids used for Roberta-Large experiments are detailed as follows. We fine-tuneeach task for 80K steps, except for the FT method,which is conducted over 20 epochs. We record the bestmodel checkpoint based on the validation loss every200 training steps.",
  "B.1Additional Momeory Comparison results": "In this section, we provide more quantitativeresults about the training memory comparisonbetween the FO and ZO fine-tuning methods. Inaddition to the training memory on SST2 taskswe measure in .3, we further profile thememory cost on WIC, CB, and MultiRC tasks.The results are shown in . We can observe from the table that the AdaZetamethod achieves 5-8 memory reduction ondifferent tasks. Also, the AdaZeta method utilizessimilar or even less memory than the other MeZO,MeZo-LoRA, and Sparse-MeZO methods withan additional variance reduction feature, whichlargely improves the ZO fine-tuning accuracy. : The hyperparameter grids used for Llama-2-7B experiments are outlined as follows. We fine-tuneeach task for 5K steps using our AdaZeta method, 10Ksteps for other ZO fine-tuning methods (MeZO, MeZO-LoRA, Sparse-MeZO), and 5 epochs for the first-orderFull-model Fine-Tuning (FT) method. We record thebest model checkpoint based on the validation loss every200 training steps.",
  "Ewt[(wk)2],": "where (a) can be proved with the use of the Lipschitz smoothness gradient denied in Assumption A1 thatgives x and y, we have (x) (y) Lx y. Additionally, by the mean value theorem forvector-valued functions, there exists for any point c on the line segment between x and y such that:"
}