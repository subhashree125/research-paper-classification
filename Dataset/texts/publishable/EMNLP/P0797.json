{
  "Abstract": "Large Language Models (LLMs) have shownimpressive capabilities but also a concerningtendency to hallucinate. This paper presentsREFCHECKER, a framework that introducesclaim-triplets to represent claims in LLM re-sponses, aiming to detect fine-grained halluci-nations. In REFCHECKER, an extractor gen-erates claim-triplets from a response, whichare then evaluated by a checker against a ref-erence. We delineate three task settings: Zero,Noisy and Accurate Context, to reflect vari-ous real-world use cases. We curated a bench-mark spanning various NLP tasks and anno-tated 11k claim-triplets from 2.1k responsesby seven LLMs. REFCHECKER supports bothproprietary and open-source models as the ex-tractor and checker. Experiments demonstratethat claim-triplets enable superior hallucinationdetection, compared to other granularities suchas response, sentence and sub-sentence levelclaims. REFCHECKER outperforms prior meth-ods by 18.2 to 27.2 points on our benchmarkand the checking results of REFCHECKER arestrongly aligned with human judgments1.",
  "Introduction": "Large Language Models (LLMs) have sparked arevolution in Natural Language Processing (NLP),covering diverse tasks with a unified architec-ture (Zhao et al., 2023). However, LLMs exhibit atendency to generate hallucinated contents that canbe difficult to discern, posing a potential risk of mis-leading users. (Huang et al., 2023). Consequently,hallucination detection has received increasing at-tention (Manakul et al., 2023; Min et al., 2023;Chern et al., 2023).Detecting hallucination is essentially a job ofcomparing a generated response against a refer-ence. To this end, several challenges remain, in-",
  "A Flock of Seagulls is English new wave band": ": An example response split into sentence, sub-sentence (Min et al., 2023), triplets, and the hallucina-tion 1983. Triplets define the boundary of claims moreclearly, are fine-grained and covers non-overlappingfacts (unlike sub-sentences). cluding determining the appropriate unit of analy-sis for comparison and developing a unified, au-tomated framework that scales detection acrossdiverse tasks. To address these challenges, exist-ing work has considered checking hallucinationsat various levels of granularity. Specifically, Linet al. (2022) and Li et al. (2023) conduct response-level checking by taking the whole response as thechecking unit. Manakul et al. (2023) assesses eachsentence in the response for fine-grained evalua-tion. Min et al. (2023) and Chern et al. (2023)further extracts short phrases (we term them assub-sentences) as the claims, as one sentence maycontain multiple hallucinations, or one hallucina-tion may span across sentence boundaries.Existing research, however, leaves several openissues unaddressed. For instance, response levelchecking suffices if the query and response is abouta simple fact, but when responses are complex andlong, it can be uninformative and also cause false-negative when hallucination is local. This is com-mon in real-world use cases, for example, the re-sponse from Llama 2 (Touvron et al., 2023) in ourexperiments (described later) contains 150 tokenson average. Sentence level detection cannot captureknowledge across sentences, and sub-sentences arestructurally difficult to define, making it challeng- : The REFCHECKER framework comprises two main components: an extractor denoted as E and a checkerdenoted as C. Given a text to be checked, typically a response generated by an LLM, the extractor takes it as inputand generates a set of knowledge triplets, referred to as claim-triplets. Subsequently, the checker assesses eachclaim-triplet by comparing it against a reference, assigning a hallucination label based on the evaluation. ing to form high-quality demonstrations to be usedby LLMs with in-context learning. To address thischallenge, we propose to extract knowledge tripletsas checking units. Triplets are commonly usedfor representing knowledge (Ji et al., 2022) whichfollow a (subject, predicate, object) struc-ture. Comparing with other granularities, tripletsexhibit fine-grained and clearly separated seman-tics as exampled in . These triplets arecalled claim-triplets.",
  "Using claim-triplets, we build REFCHECKER(), a fully automated framework that scaleshallucination detection across different tasks. RE-": "FCHECKER consists of two main components: anextractor and a checker. The extractor generatesclaim-triplets from the response and the checkerevaluates each of the claim-triplets by comparingthem with the reference.In contrast to recentwork that only differentiates factual and non-factualclaims, the checker in REFCHECKER also consid-ers unverifiable claims when the reference is insuf-ficient for checking. Both the extractor and checkersupports proprietary (e.g. GPT-4 (OpenAI et al.,2023)) and open-source models (e.g. Mistral (Jianget al., 2023) and RoBERTa (Liu et al., 2019) basedmodels). Existing datasets such as SelfCheckGPT (Man-akul et al., 2023), FActScore (Min et al., 2023) andFacTool Chern et al. (2023) can be used to evalu-ate REFCHECKER. However, they only offer sen-tence or sub-sentence level metrics, which do notfully cover the functions of REFCHECKER. We cu-rate a comprehensive dataset, KNOWHALBENCH,on which we can benchmark hallucination underdifferent context quality and availability. UsingKNOWHALBENCH, we conducted human evalua-tion on 2,100 responses from 7 LLMs. We anno- tated 11k claim-triplets with 95% Inter-AnnotatorAgreement on 23% of the annotations (due to re-source limitations). Compared with these datasets,it covers a more diverse range of domains and tasks,with more LLMs and responses evaluated (see Ta-ble 1). As expected, we found by human evalua-tion that hallucination is the most pronounced (cf.Appendix A.4) when LLMs are asked to generateresponses solely from its memory (Zero Context),followed by responding to noisy references in RAG(retrieval augmented generation) setting (Shusteret al., 2021) (Noisy Context) and finally when ref-erences are more or less noise-free (Accurate Con-text). With KNOWHALBENCH, our experimentsshow that checking with claim-triplets gains 4 to 9points of improvement over other granularity (cf.Sec. 6.1) and REFCHECKER achieves 18.2 to 27.2points of improvement over the best alternative(Sec. 6.3).Our key contributions include:",
  ",100 from 7 LLMs": ": A comparison of REFCHECKER with previous approaches for hallucination detection. The * symbolsalongside the extractors and checkers indicate that these models are open-sourced. REFCHECKER uses triplets asthe claims instead of sentences or sub-sentences. The REFCHECKER benchmark covers more context settings andmore diverse tasks. The human evaluation covers more LLMs and responses. REFCHECKER pipeline supports bothproprietary and open-source models, facilitating broader adoption across various applications.",
  "We undertake a review of prior work relevant toour study and compare them with REFCHECKER.The comparative analysis with three representativemethods is encapsulated in": "Hallucination in LLMs.Hallucinations, whichfrequently occur in NLP tasks like summariza-tion (Maynez et al., 2020; Cao et al., 2022), ma-chine translation (Guerreiro et al., 2023a,b), dialogsystems (Honovich et al., 2021; Dziri et al., 2022)and RAG (Shuster et al., 2021), can be categorizedto factuality hallucinations and faithfulness hallu-cinations (Huang et al., 2023). Factuality halluci-nations involve claims contradicted by real-worldfacts, while faithfulness hallucinations are incon-sistent with the input content. Recent researchon hallucination detection primarily concentrateson factuality hallucinations, such as SelfCheck-GPT (Manakul et al., 2023), FActScore (Min et al.,2023) and FacTool (Chern et al., 2023). We addressboth factuality and faithfulness hallucinations andfurther categorizing them into three contextual set-tings to align with real-world use cases. Granularity of Claims.Claims are pivotal forevaluating responses generated by LLMs.Re-sponse level checking (Lin et al., 2022; Li et al.,2023) is too coarse-grained for long-form re-sponses.For fine-grained detection, sentencelevel (Manakul et al., 2023) and sub-sentence levelchecking (Min et al., 2023; Chern et al., 2023)have been proposed. However, these approachesstill face limitations, as discussed in Sec. 1. In thispaper, we employ knowledge triplets, which havebeen widely adopted as claims or facts (Li et al.,2022) for entailment reasoning (Liang et al., 2022;",
  "Arakelyan et al., 2021). Extracted claim-tripletsprovide a structured framework for defining claimgranularity": "Hallucination Checking.One line of workfor hallucination checking focuses on reference-free checking.They mainly depend on self-contradiction (Mndler et al., 2023) or uncer-tainty (Zhang et al., 2023b) of the LLMs, orself-consistency between randomly sampled re-sponses (Manakul et al., 2023; Chen and Mueller,2023; Zhang et al., 2023a).The effectivenessof these methods depends on the LLM-basedcheckers capability and requires multiple responsesamples, which is costly. REFCHECKER is alignedwith another line of work which requires referencesto check with (Min et al., 2023; Chern et al., 2023).In addition, REFCHECKER adopts a 3-way classi-fication framework to cover unverifiable claims asopposed to the binary classification used in previ-ous work, which can only distinguish factual andnon-factual claims. Hallucination Detection Benchmarks.The ex-isting benchmarks for hallucination detection pri-marily focus on response-level detection (Lin et al.,2022; Yang et al., 2023), or limited to specificdomains and tasks (Manakul et al., 2023; Minet al., 2023), or solely address factuality hallucina-tions (Chen et al., 2023; Chern et al., 2023; Wanget al., 2023). In contrast, our proposed benchmarkoffers a broader scope, encompassing a diverserange of tasks and domains. Moreover, our humanevaluation process involves a more extensive exam-ination of various LLMs with more responses.",
  "ReferenceReferenceReference": ": Illustration of three settings of context, tasks and references. Zero Context is about seeking factualknowledge from the internal memory of the LLMs. Noisy Context has context information retrieved from aknowledge source, which is a RAG use case. Accurate Context has context provided in the input prompt. For Noisyand Accurate Context, we take the input context as the reference.",
  "Context settings.We differentiate three contextsettings covering various tasks and employ differ-ent benchmarks for each setting as shown in Fig-ure 3": "Zero Context (ZC) Tasks in this setting can be re-ferred to as closed-book question answering whichrequires the LLM to respond solely based on itsinternal knowledge. Therefore, in principle, refer-ences should be in the training corpus. In practice,for benchmarking purposes, we use a ground truthreference for each question which contains the an-swer, and we expect the reference can be retrievedfrom a trusted knowledge source when deployed toreal-world applications. Noisy Context (NC) In this setup, the LLM re-ceives additional context retrieved from some ex-ternal knowledge source, which may contain noisyor irrelevant information. NC is also known asRAG, a crucial use case frequently encountered inreal-world applications. Accurate Context (AC) This setting is similar toNC but the reference is typically noise-free. Ex-amples include text summarization, closed-QA andinformation extraction tasks. The main differencebetween AC and NC is that the context in AC istrustworthy, while the context in NC contains a lotof noise. Granularity of checking.Informally, claimsare the units for the checking.This work ex-plores the approach of representing claims withknowledge triplets. Knowledge triplets adopt a(head_entity, relation, tail_entity) struc-ture to capture fine-grained information within theresponse. We call the triplet-format claims as claim-triplets, examples of which are shown in . Definition of hallucinations.The claim-tripletsare then compared with a reference to determinethe type of hallucinations. If a claim-triplet canbe directly inferred from the reference, we clas-sify it as Entailment. Conversely, if it contradictsthe information in the reference, it is labeled asContradiction. However, in cases where the ref-erence is insufficient to verify the claim-triplets,we classify it as Neutral. In this study, we focuson verifying hallucinations in the response and donot consider unmentioned aspects in the reference,which may also be important for certain tasks.",
  "The REFCHECKER framework": "As illustrated in , the REFCHECKER frame-work is designed as a 2-stage pipeline: an Extrac-tor E decomposes the LLM response into a set oftriplets, with each of them verified by the CheckerC. The categorization of the triplets can be option-ally aggregated according to specified rules. Weexplain them in the subsequent subsections. ExtractorOur checking framework hinges on akey assumption: the decomposition of the originaltext into triplets facilitates finer-grained detectionand more accurate evaluation. The extraction ofthese triplets plays a pivotal role in achieving thisobjective. We apply LLMs to extract knowledgetriplets from the given text. We began with GPT-4and, for both cost and efficiency concern, Mixtral 8x7B and Mistral. More specifically, we performedknowledge distillation to train a 7B Mistral-basedextractor with Mixtral 8x7B as the teacher. Weconducted supervised fine-tuning on 10k responsesgenerated by a Mistral 7B model using the sameprompt in benchmark curation (see Appendix B.1for details). Evaluation in Sec. 6.4 shows competi-tive extraction quality of the open-source extractor.Refer to Appendix B.1 for prompts used for extrac-tion and details on extractor training. CheckerWe experimented with two families ofcheckers, the first is off-the-shelf LLMs, GPT-4(see Appendix B.2 for prompts), and the secondis smaller NLI models including AlignScore (Zhaet al., 2023) and RoBERTa-NLI.2 Long referencesin AC/NC setting are split to fit into small contextwindows of these small models (e.g. 200 tokens),and the results are aggregated later.Mistral 7B (Jiang et al., 2023) lies in between,offering both massive knowledge obtained duringpre-training and the opportunity for tuning theopen model weights with NLI data. There aremany options we have experimented: 1) fine-tuneby adding small amount of new parameters usingLoRA (LoRA-sft) (Hu et al., 2021), 2) attach ashallow classifier, eg. SVM, 2-layer MLP, KNNafter NCA projection (Goldberger et al., 2004), ontop of the internal states of the model. We callsuch checker RepC (for Representation-based Clas-sifier). Such states can be selected from one layer(layer selection, LS) or an ensemble of all layers(layer ensemble, LE). As we will report in Sec. 6.4,RepC checkers are competitive in general. AggregationTriplet results can be aggregated toobtain the ratio of each category, therefore gives anoverall measure of hallucination distribution in aresponse. To derive the performance of a particularLLM, we take a macro average on Entailment/Neu-tral/Contradiction ratios of all responses. If a scalaris preferred, we can assign certain numeric valuesto the catogories, for instance 1, 0, 1 for contra-dictory, neutral and entail, respectively.The aggregation can be customized and this isone of the benefits of the fine-grained hallucinationdesign in REFCHECKER. For instance, to com-pare against other response-level approaches (cf.Sec. 6.1), we adopt a rule where the response isflagged as contradictory if any one of the claimtriplet is contradictory.",
  "The KNOWHALBENCH dataset": "We assembled a benchmark dataset comprising 300examples from public datasets, with 100 for eachof the three context settings mentioned in .We further collected responses from 7 LLMs onthe established benchmark for human evaluation. shows the summary and statistics of thebenchmark, and we describe the dataset curationand human annotation process in the rest of thissection.",
  "Curation of benchmark data": "The 300 examples are obtained through a processof filtering, sampling and hard case selection. Thedata sources, tasks and the corresponding refer-ences are summarized in of Appendix A.We describe them in detail as follows.For ZC, we sample examples from the dev set ofNaturalQuestions (NQ) (Kwiatkowski et al., 2019),a open domain question answering dataset, for thebenchmark. Each question in NQ has a human-annotated long answer and we take the long answeras the reference. However, we found that somequestions in NQ may cause the LLMs refuse toanswer or have low quality reference to check with.Thus, we prompted ChatGPT (GPT-3.5-Turbo) tofilter out these examples from the developmentset. The details of filtering are described in Ap-pendix A.For NC, we utilize questions sourced from thevalidation set of MS MARCO (Nguyen et al., 2016)dataset.3 Each question in this dataset is accompa-nied by a list of documents retrieved from the inter-net, serving as the input context. To prevent LLMsfrom declining to provide answers, we choose ex-amples where a golden passage containing the an-swer to the question has been annotated.For AC, we employ the databricks-dolly-15k4 (Conover et al., 2023) instruction tuningdataset for the benchmark. Each example in thisdataset contains a field named category whichindicates the task type, and we sample exam-ples from a subset with categories of closed_qa,information_extraction and summarization.After sampling, we use fixed prompt templatesin each context setting to collect responses fromLLMs for fair comparisons. For ZC, the prompt is",
  "Human evaluation": "We performed human evaluation on responsesgenerated by seven LLMs on this benchmarkdataset, including GPT-4, GPT-3.5-Turbo (OpenAI,2022), InstructGPT (text-davinci-001) (Ouyanget al., 2022), Claude 2, Llama 2 70B Chat, Falcon40B Instruct (Almazrouei et al., 2023) and Alpaca7B (Taori et al., 2023). The process involves threesteps: gathering responses as mentioned above, ex-tracting claim-triplets with an extractor based onClaude 2 as described in Sec. 4, and asking hu-man annotators to evaluate these claim-triplets. Weannotated a total of 11k claim-triplets for 2.1k re-sponses. 23% of the claim-triplets were doubleannotated, with 95.0% Inter-Annotator Agreement.See Appendix A.3 for the details of the annotationprocess.",
  ": Performance statistics of 6 checkers underdifferent claim granularities on 2.1k manually annotatedresponses. The detailed checker performance can befound in of Appendix B.2": "comparable to each other, we first breakdown the2.1k annotated responses into different granular-ities, then collect corresponding checker predic-tions respectively, and finally aggregate finer-levelresults all into the response-level. We utilize astrict aggregation rule with zero-tolerance on hal-lucinations, which means we apply max-pooling(Entailment < Neutral < Contradiction) over claimpredictions within a response. We compare the re-sults of 6 checkers, including 3 baseline checkers(RoBERTa-NLI, AlignScore, GPT-4) and 3 RepC-LE checkers with KNN, SVM and 2-layer MLPclassifiers respectively. The evaluation metric ismacro-f1 on three categories. As shown in , checking at triplet-levelclaims is superior over other granularities, witha significant lead against response-level (10 ptsmacro-f1 score on average). Checking at sentence-level improves over response-level by 5 pts. How-ever, we see a 3.5 pts drop moving to sub-sentence,one of the reasons being sub-sentence claimscan overlap. Apparently, the flexibility of sub-sentences leads to poor quality of claim extraction,which subsequently affects checking.",
  "Results on the SelfCheckGPT dataset": "We further compare REFCHECKER with Self-CheckGPT (Manakul et al., 2023) using theirdataset to evaluate the entire framework. Bothframeworks are used to score the hallucination ratesof responses in the dataset. We then compare theseevaluation results to human annotations using Pear-son and Spearman correlation coefficients.SelfCheckGPT adopts sentence-level halluci-nation detection.We aggregate scores of sen-tences (0, 0.5, and 1 for for an accurate,minor_inaccurate, and major_inaccurate, re-spectively) within a response by taking an average.Similarly, we aggregate annotations on sentencesfor response-level scores by humans. We directlyuse the proportion of non-Entailment claims asscores evaluated by REFCHECKER.As shown in , REFCHECKER signifi-cantly outperforms SelfCheckGPT (over 2 pts) witha Mistral-SFT + GPT4 combination. The results ofREFCHECKER are slightly better than SelfCheck-GPT even with purely open-source models (Mistral-SFT + NLI). Note that SelfCheckGPT requires 20LLM (ChatGPT) calls on each sentence. Full re-sults with more configurations are listed in (Appendix C). Knowledge-centric hallucination de-",
  "Results on KNOWHALBENCH": "We perform comparisons on our KNOWHAL-BENCH dataset, which provides more contextsettings.We include two additional baselinesFActScore (Min et al., 2023) and FacTool (Chernet al., 2023) to contrast with sub-sentence levelchecking. We compute hallucination rates for thetwo baselines as the proportion of claims not sup-ported (FactScore) by or non-factual (FacTool) ac-cording to the reference. presents Pearson and Spearman correla-tions between the hallucinations rates evaluated byhumans and checking frameworks. REFCHECKERsignificantly outperforms previous methods acrossall three context settings with both proprietary andopen-source models. Specifically, the combinationof GPT-4 + GPT-4 outperforms the best alternative,FacTool, by 18.2 to 27.2 pts. Consistent with ourfindings in Sec. 6.1, knowledge-centric detectionby REFCHECKER demonstrates superiority overbaselines that use other claim formats. Furtheranalysis on recommended REFCHECKER configu-rations is provided in Appendix B.3.",
  "RepC-LE-nn-n2000-e200081.2760.8075.2371.9882.0847.5686.5062.86": ": Checker evaluation results on 11k human annotated claim triplets. In Mistral-based checkers, the modelnames start with the variant types, eg. LoRA-sft indicates the LoRA fine-tuned variant and RepC-LE-nn indicatesthe representation based classification variant using layer ensemble with 2-layer MLP as the classifier. Here nxxxand exxx indicates the number of training samples and ensemble learning samples. Due to the space limitation,we do not include all variant results here, please refer to of Appendix B.2 for full results. ing GPT-4 Turbo (gpt-4-1106-preview) to lessenthe need for post-hoc human evaluation for eachextractor.We employed GPT-4 Turbo to label each ex-tracted claim as True/False, indicating faithful-ness to the original semantics. Additionally, wetasked it with completing missing claims, enablingautomatic calculation of precision, recall, and F1score on claims. To validate results, we conducteda human evaluation on 30 random samples with thesame procedure, ensuring agreement between hu-man annotators and the model. The comparison in (Appendix B.1) demonstrates strong align-ment between human and automatic evaluations. Itachieves 93.7% agreement on precision and 91.9%on recall.Leveraging the reliability of our automatic eval-uation pipeline, we evaluated the performance offour extractors (see ). Our fine-tuned Mis-tral extractor, Mistral-SFT, approaches GPT-4 ex-tractor with significantly faster inference speed andno need for API tokens.CheckersAs described in Sec. 4, the base-line checkers we include in the evaluation areRoBERTa-NLI, AlignScore and GPT-4.TheMistral-based checkers we include are zero-shotprompted, one-shot prompted, LoRA fine-tunedand RepC-LE variants. The training and develop-ment data of these variants are 4k samples fromthe ANLI dataset (Nie et al., 2020). We evaluatetheir performance using the 11k manually anno-tated claim triplets. The evaluation metric is accu-racy and macro-f1 score over 3-way classification. shows the evaluation results. Among thebaseline checkers, AlignScore is a strong competi-tor to GPT-4. Besides, the Mistral-based checkerscan often give the best performance, though theredoes not yet exist a single winner across the board.The weakness of Mistral-based checkers lies in theNC setting. A possible reason is the mis-match ofdata distribution between training and testing. Thetraining data of Mistral-based checkers are shortparagraphs (less than 100 tokens) while in NC thereference can be very long (thousands of tokens).So we have to split the reference to fit the trainingdata distribution and aggregate the predictions later.These results suggest ample room of improvementfor the checkers.",
  "Limitations": "Despite the effectiveness of REFCHECKER, it stillhas limitations, which we discuss as follows.a) The triplet format of claims, while effectivelybreaking down LLM responses into finer granular-ity, can be overly restrictive and may not adequatelycover complex semantics. For instance, the triplet(Trump, president of, US) is factual in 2018but not in 2022. Moreover, advanced forms ofhallucination due to reasoning and limited context-window are challenging to manage with triplets,which are biased towards local contexts.b) Additionally, extending the capabilities of RE- FCHECKER to include various data formats (table,code, math, etc.) and specific domains (business,medical, legal, etc.) are worthy of consideration.c) REFCHECKER has rudimentary support forsource attribution, as detailed in Appendix B.4.Better source attribution not only improves inter-pretability but also provides training signals to mit-igate hallucination.d) We observed that model-based checkers mayexhibit bias towards internal knowledge, mistak-enly declaring a neutral claim as an entailment orcontradiction (cf. Appendix D). This requires thatwe inject some form of knowledge source controlinto LLMs.e) In actual deployment cases, we found usersask for stronger customizability (e.g. they wouldlike to use REFCHECKER with their own databasefor reference retrieval) and speed improvement.",
  "Ethics Statement": "We contend that REFCHECKER poses no nega-tive ethical implications for the public; rather, itholds the potential for positive impact by enablingthe identification of non-factual content withinthe responses generated by large language models(LLMs). This capability contributes to the cultiva-tion of responsible AI practices for the benefit ofsociety.In this study, we utilized a variety of scien-tific resources to conduct our research and aimto contribute additional artifacts to the community.Specifically, to curate the benchmark dataset, wesample 100 examples from each of the followingdatasets:",
  "The databricks-dolly-15k dataset, which is un-der Creative Commons Share-Alike 3.0 Li-cense": "These datasets are publicly accessible and utilizeEnglish language corpora. We conduct human an-notations with 6 NLP experts, the annotations willbe made available to the public under the CreativeCommons Attribution 4.0 International License.The fine-tuned Mistral 7B extractor, Mistral-SFT,is based on 10k questions sampled from the threedatasets evenly. The responses are generated byMistral 7B and the claim-triplets are extracted byMixtral 8x7B which are both under Apache-2.0License. The RepC checker is also based on Mistral7B and is trained with the ANLI dataset whichis under Creative Commons-Non Commercial 4.0License. The fine-tuned models will be released tothe public under Apache-2.0 License. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Merouane Debbah, Etienne Goffinet, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.2023. Falcon-40B: an open large language modelwith state-of-the-art performance.",
  "Jiuhai Chen and Jonas Mueller. 2023. Quantifying un-certainty in answers from any language model viaintrinsic and extrinsic confidence assessment. arXivpreprint arXiv:2308.16175": "Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern,Siyang Gao, Pengfei Liu, and Junxian He. 2023.Felm: Benchmarking factuality evaluation of largelanguage models. In Thirty-seventh Conference onNeural Information Processing Systems Datasets andBenchmarks Track. I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan,Kehua Feng, Chunting Zhou, Junxian He, GrahamNeubig, Pengfei Liu, et al. 2023. Factool: Factu-ality detection in generative aia tool augmentedframework for multi-task and multi-domain scenar-ios. arXiv preprint arXiv:2307.13528. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,Matei Zaharia, and Reynold Xin. 2023. Free dolly:Introducing the worlds first truly open instruction-tuned llm. Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Za-iane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022.FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. Transactions of the Associationfor Computational Linguistics, 10:14731490.",
  "Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, andRuss R Salakhutdinov. 2004. Neighbourhood com-ponents analysis. Advances in neural informationprocessing systems, 17": "Nuno M. Guerreiro, Duarte M. Alves, Jonas Waldendorf,Barry Haddow, Alexandra Birch, Pierre Colombo,and Andr F. T. Martins. 2023a. Hallucinations inLarge Multilingual Translation Models.Transac-tions of the Association for Computational Linguis-tics, 11:15001517. Nuno M. Guerreiro, Elena Voita, and Andr Martins.2023b. Looking for a needle in a haystack: A com-prehensive study of hallucinations in neural machinetranslation. In Proceedings of the 17th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 10591075, Dubrovnik,Croatia. Association for Computational Linguistics. Or Honovich, Leshem Choshen, Roee Aharoni, EllaNeeman, Idan Szpektor, and Omri Abend. 2021.q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and ques-tion answering. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 78567870, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. Preprint, arXiv:2311.05232. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti-nen, and Philip S. Yu. 2022. A survey on knowledgegraphs: Representation, acquisition, and applications.IEEE Transactions on Neural Networks and LearningSystems, 33(2):494514. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, andJi-Rong Wen. 2023. HaluEval: A large-scale hal-lucination evaluation benchmark for large languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,",
  "pages 64496464, Singapore. Association for Com-putational Linguistics": "Manling Li, Revanth Gangi Reddy, Ziqi Wang, Yi-shyuan Chiang, Tuan Lai, Pengfei Yu, Zixuan Zhang,and Heng Ji. 2022. COVID-19 claim radar: A struc-tured claim extraction and tracking system. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 135144, Dublin, Ireland. Associa-tion for Computational Linguistics. Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenx-uan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, andFuchun Sun. 2022. A survey of knowledge graphreasoning on graph types: Static, dynamic, and mul-timodal. arXiv preprint arXiv:2212.05767.",
  "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022": "TruthfulQA: Measuring how models mimic humanfalsehoods. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 32143252, Dublin,Ireland. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023": "SelfCheckGPT: Zero-resource black-box hallucina-tion detection for generative large language models.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages90049017, Singapore. Association for Computa-tional Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 19061919, On-line. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100, Singa-pore. Association for Computational Linguistics.",
  "with the 30th Annual Conference on Neural Infor-mation Processing Systems (NIPS 2016), Barcelona,Spain, December 9, 2016, volume 1773 of CEURWorkshop Proceedings. CEUR-WS.org": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,Jason Weston, and Douwe Kiela. 2020. Adversarialnli: A new benchmark for natural language under-standing. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 48854901. OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christo-pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brockman,Tim Brooks, Miles Brundage, Kevin Button, TrevorCai, Rosie Campbell, Andrew Cann, Brittany Carey,Chelsea Carlson, Rory Carmichael, Brooke Chan,Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,Ruby Chen, Jason Chen, Mark Chen, Ben Chess,Chester Cho, Casey Chu, Hyung Won Chung, DaveCummings, Jeremiah Currier, Yunxing Dai, CoryDecareaux, Thomas Degry, Noah Deutsch, DamienDeville, Arka Dhar, David Dohan, Steve Dowl-ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, NikoFelix, Simn Posada Fishman, Juston Forte, Is-abella Fulford, Leo Gao, Elie Georges, ChristianGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,Rapha Gontijo-Lopes, Jonathan Gordon, MorganGrafstein, Scott Gray, Ryan Greene, Joshua Gross,Shixiang Shane Gu, Yufei Guo, Chris Hallacy, JesseHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-hannes Heidecke, Chris Hesse, Alan Hickey, WadeHickey, Peter Hoeschele, Brandon Houghton, KennyHsu, Shengli Hu, Xin Hu, Joost Huizinga, ShantanuJain, Shawn Jain, Joanne Jang, Angela Jiang, RogerJiang, Haozhun Jin, Denny Jin, Shino Jomoto, BillieJonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser,Ali Kamali, Ingmar Kanitscheider, Nitish ShirishKeskar, Tabarak Khan, Logan Kilpatrick, Jong WookKim, Christina Kim, Yongjik Kim, Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine Thompson, PhilTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and Bar-ret Zoph. 2023. Gpt-4 technical report. Preprint,arXiv:2303.08774.",
  "OpenAI. 2022. Introducing chatgpt": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. Preprint, arXiv:2203.02155. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,and Jason Weston. 2021. Retrieval augmentationreduces hallucination in conversation. In Findingsof the Association for Computational Linguistics:EMNLP 2021, pages 37843803, Punta Cana, Do-minican Republic. Association for ComputationalLinguistics.",
  "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti": "Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Yuxia Wang, Revanth Gangi Reddy, Zain MuhammadMujahid, Arnav Arora, Aleksandr Rubashevskii, Ji-ahui Geng, Osama Mohammed Afzal, LiangmingPan, Nadav Borenstein, Aditya Pillai, Isabelle Au-genstein, Iryna Gurevych, and Preslav Nakov. 2023.Factcheck-gpt: End-to-end fine-grained document-level fact-checking and correction of llm output.ArXiv, abs/2311.09000.",
  "Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.2023. Alignscore: Evaluating factual consistencywith a unified alignment function. arXiv preprintarXiv:2305.16739": "Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Ma-lin, and Sricharan Kumar. 2023a. SAC3: Reliablehallucination detection in black-box language modelsvia semantic-aware cross-check consistency. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 1544515458, Singapore.Association for Computational Linguistics. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,Yue Zhang, Zheng Zhang, Chenghu Zhou, XinbingWang, and Luoyi Fu. 2023b. Enhancing uncertainty-based hallucination detection with stronger focus.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages915932, Singapore. Association for ComputationalLinguistics. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, ChenYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.A survey of large language models. arXiv preprintarXiv:2303.18223.",
  "ADetails of the Benchmark DataCuration Process": "For Zero Context, we sample examples from thedevelopment set of the NQ dataset for the bench-mark. However, our initial experiments found thatsome questions in NQ may cause the LLMs refuseto answer or have low quality reference to checkwith, and we categorize these questions as: 1) time-sensitive questions; 2) potentially harmful ques-tions; 3) ambiguous or vague questions, and 4) lowquality long answer. We talk about the data filteringlater. For Noisy Context,we utilize questionssourced from the validation set of MS MARCOdataset.5(Nguyen et al., 2016) To prevent LLMsfrom declining to provide answers, we choose ex-amples where a golden passage containing the an-swer to the question has been annotated. For Accurate Context, we employ the databricks-dolly-15k6 instruction tuning dataset for the bench-mark. Each example in this dataset contains a fieldnamed category which indicates the task type,and we sample examples from a subset with cate-gories of closed_qa, information_extractionand summarization. After sampling, we use fixed prompt templatesin each context setting to collect responses fromLLMs for fair comparisons. For Zero Context set-ting, the prompt is the question itself. For Noisyand Accurate Context settings, we use prompt tem-plates shown in .",
  "We employ ChatGPT (GPT-3.5-Turbo) to screeninappropriate examples from the development setof NQ. The specific prompts utilized are illustratedin": "Note that we utilize a conversational approachfor prompting to identify examples with low-quality references given as annotated long answersin the dataset. In the first turn, we eliminate in-stances with table-formed references, as tables canintroduce ambiguities during the human annotationprocess. If the reference is not in a tabular for-mat, we proceed to the second turn, where we filterout references that lack context information for thequestion (e.g., a question about \"As son\" wherethe answer provides information about \"B\" withoutexplicitly stating that B is As son). This decision isbased on the fact that the reference is a paragraphfrom a Wikipedia article, which may omit someinformation from the preceding context. Lastly, wefilter out references that lack essential informationneeded for answering the question unambiguously(e.g. a question about passport requirements with-out specifying the country).",
  "A.2Details of Hard Case Selection": "For each task setting, we sort a set of 1,000 ran-domly sampled examples based on the extent ofhallucination they demonstrate. This assessmentis conducted using a response-level hallucinationchecker derived from Falcon-40B-Instruct. Specifi-cally, responses were gathered from four differentLLMs , GPT-3.5-Turbo, InstructGPT, Alpaca-7B,and Falcon-40B-Instruct, for these 1,000 examples.Falcon-40B-Instruct is subsequently employed toevaluate whether these responses contain halluci-nations according to the prompt template depictedin . In this prompt, the claim refers tothe response generated by an LLM. Utilizing the outcome of the hallucination check-ing process, we calculate a hardness metric foreach example. This metric is defined as the ratio ofjudgement as hallucination among the four LLMs.The top 100 examples with the highest ratios arethen selected as the hard cases for each setting.",
  "A.3Human Annotation": "We developed a web-based annotation tool to fa-cilitate the human evaluation. A screenshot of theannotation tool is presented in . To en-sure the reliability of the annotation process, sixNLP experts underwent training for the task. Theclaim-triplets for human evaluation are extractedby a Claude 2 extractor as described in .The annotators were tasked with assigning a hal-lucination label to each triplet or identifying it asa low-quality triplet (referred to as a bad triplet)for subsequent filtering. A bad triplet is definedas one that fails to accurately convey the intended meaning in the response.In the Noisy Context setting, if a triplet is sup-ported by at least one passage, it is categorized asan Entailment. Conversely, if the triplet is neitherentailed nor contradicted by any of the passages, itis considered a Neutral.",
  "A.4Observations from Human Evaluation": "We analyze the results of human evaluation to gaina deeper understanding the patterns of hallucina-tions. We establish our evaluation metric as fol-lows. Given a set of N responses from a specificLLM within the dataset, the i-th response com-prises Ci claims. Among these, Cyi claims areannotated with the specific hallucination type la-beled as y {Entailment, Neutral, Contradiction}.We define the hallucination rate for type y that theLLM exhibits in the i-th response as ryi , which is",
  "calculated as ryi = Cyi": "Ci .We can see that ryi has definition when Ci > 0,however, the LLMs may refuse to answer somecertain questions, and the claim extractor will notextract any claim-triplets from such response, i.e.,Ci = 0. To cover these cases in the metric, we de-fine a new metric of Abstain Rate rabstain as did in",
  "We organize the conclusions drawn from the dataanalysis into several findings:": "Context Information is Critical dis-plays hallucination label distributions and abstainrates across the three context settings, averagedfrom the seven LLMs. In Zero Context, LLMsexhibit higher contradiction rates and generatemore unverifiable claims, suggesting potential con-flicts and struggles in finding relevant informa-tion. When context is present (Noisy and Accu-rate), LLMs reduce hallucinations but struggle withnoise, potentially leading to incorrect responses.In conclusion, the reliability of LLMs internalknowledge is questionable, highlighting the need",
  "for clean and precise contextual information forgenerating factual responses": "Copy from Context is SaferReplicating contentin the context enhances the factuality, as illustratedin . In order to quantitatively assess therelationship between copying and hallucination inboth Noisy and Accurate Context settings, we in-troduce the concept of Copy Rate. This metricis defined as the ratio of N-grams covered by thecontext, where an N-gram refers to a phrase com-prising N consecutive words. Specifically, we com-pute the average copy rates for 1 to 4 grams ofa claim-triplet to determine its overall copy rate.The findings presented in reveal a dis-cernible trend: a higher copy rate corresponds toan increased likelihood of entailment.",
  "B.1Extractor": "The prompts used for few-shot claim extraction areshown in . They are used for claim extrac-tion by GPT-4, Mixtral, and the Mistral baseline.For Mistral-SFT, we removed the in-context exam-ples in the prompt because we find it doesnt affectthe extraction quality after supervised fine-tuning 0.10.20.30.40.50.60.70.80.91 0.2 0.4 0.6 0.8 EntailmentNeutralContradiction Copy Rate of Claim-Triplet",
  "GPT-497.292.594.298.292.294.8Mixtral87.785.285.587.685.585.4": "but saves context length. We set the temperature to0 for deterministic output and limit the maximumnumber of new tokens for generation to 1000.We collected 10,000 questions without claim ex-traction results and annotation, following the sameprocess as described in Appendix A. The collectedquestions cover the three context settings evenly.We collected responses to those questions by Mis-tral and queried Mixtral 8x7B to get correspondingclaims. After that, we performed supervised fine-tuning on a Mistral 7B model to distill the outputof the larger Mixtral model. We trained the modelfor 1 epoch with a initial learning rate 1e-5.",
  "B.2Checker": "The prompts used for the LLM-based checkers areshown in .As a supplement of , shows thedetailed checker performance under different claimgranularities. As a supplement of , shows the full results of checker evaluation.We also study the performance tendency ofRepC-LS and RepC-LE in . The find-ings indicate that in RepC-LS, the best performedlayer is typically around the middle rather than thelast layer. Despite RepC-LS trailing behind RepC-LE, it maintains its advantages in model size and : Detailed performance of 7 checkers under dif-ferent claim granularities on 2.1k manual annotatedresponses. The checkers predictions under differentgranularities are all merged into response-level and thenevaluated.",
  "B.3Recommendation on Extractor CheckerCombinations": "To find the best configurations of REFCHECKER,we checked 7 LLMs responses on our bench-mark for model rankings (ordered by ratios macro-averaged on responses of each LLM), and com-pared rankings by REFCHECKER and humans withSpearmans rank correlation coefficient. The con-figuration space consists of different combinationsof extractor + checker, and also the 3 task settingsas well as their averages. The results are reportedin .We observe that the combination of Mistral +GPT-4 is the most competitive option with verystrong correlations across near all settings, ben-efiting from more powerful LLMs for checkingand our trained Mistral Extractor. The best non-proprietary combination is Mistral + AlignScorechecker (356M), which achieve consistently strongcorrelations in all settings.The Mistral-RepCchecker is robust against different extractors, ow-ing to its stronger reasoning capability than smallNLI-based checker. This result serves as a guidefor choosing a configuration tailored to the userspreferences. These preferences may include fac-tors such as budget, deployment simplicity, specificsettings, types of hallucination, privacy and require-ments for open-source models.",
  "B.4Source Attribution": "In many cases, users of hallucination detection sys-tems care not only the verdicts of the checker, butalso where the hallucination happens in the originalresponse, as well as which evidence in the reference Given a question and a candidate answer to the question, please extract a KG from the candidateanswer condition on the question and represent the KG with triples formatted with (\"subject\",\"predicate\", \"object\"), each triplet in a line.Please note that this is an EXTRACTION task, so DO NOT care about whether the content of thecandidate answer is factual or not, just extract the triplets from it.",
  ": The prompt used for the LLM-based extractors. It requires a question and response from the LLM, andis provided with two in-context examples": "supports such verdicts. We provided a rudimentorysupport of such demand. Specifically, we apply asentence embedding model (SimCSE (Gao et al.,2021)) to encode spans in responses and references,compare them to the encoding of elements in claim-triplets, then use a threshold to filter matched spansas source attribution results. This naive approachsuffers from issues on computational efficiency,unclear boundaries, and matching by shallow se-mantics. The topic on source attribution has a sig-nificant impact on applications of hallucination de-tection and we leave the exploration on non-trivialsolutions to the future.",
  "C.1Comparison on the REFCHECKERBenchmark": "We compare REFCHECKER with recently proposedhallucination detection frameworks, SelfCheck-GPT, FActScore and FacTool, on our benchmark.The four frameworks use different representationsof claims and hallucination labels as described in, we aggregate the claim-level results intotwo types of response-level results: Response-level binary classification. We ag-gregate the claim-level labels into response-level binary labels as Factual and Non-Factual.Thus, we use Accuracy, Factual F1 (Fact.F1 for short) and Non-Factual F1 (Non-Fact.F1) as the evaluation metrics.We use a : Checker evaluation results on 11k human annotated claim triplets. In Mistral-based checkers, the modelnames start with the variant types, eg. LoRA-sft indicates the LoRA fine-tuned variant and RepC-LE-nn indicatesthe representation based classification variant using layer ensemble with 2-layer MLP as the classifier. Here nxxxand exxx indicates the number of training samples and ensemble learning samples.",
  "RepC-LE-svm-n1000-e100079.0360.0577.9873.5379.5651.2979.5455.34RepC-LS-nn-n200080.1757.3175.5071.9581.7846.9083.2253.07RepC-LE-nn-n2000-e200081.2760.8075.2371.9882.0847.5686.5062.86": "strict configuration that a response is non-factual if at least one claim contains hallu-cination.For SelfCheckGPT, we considerminor_inaccurate and major_inaccuratelabels as hallucination. For RefChecker, weconsider both Contradiction and Neutralas hallucination. Correlations of response-level hallucinationrate. Following SelfCheckGPT, we also com-pare the hallucination rate of response withhuman evaluation by Pearson and Spearmancorrelations. For SelfCheckGPT, we computethe hallucination rate of a response by averag-ing the scores of the sentences following thedefinition in their paper. For FActScore andFacTool, the hallucination rate is the ratio ofnon-factual claims in a response. And for Re-fChecker, we take the ratio of Contradictionand Neutral claims as the hallucination rate. The results are shown in for Zero Con-text setting, for Noisy Context settingand for Accurate Context setting. Fol-lowing their configurations in their papers, we ap-ply InstructGPT(text-davinci-003) and GPT-4as the extractors for FActScore and FacTool, re-spectively, apply ChatGPT(gpt-3.5-turbo) as thechecker for SelfCheckGPT and FActScore andGPT-4 for FacTool. The combinations of extrac-tor and checker of RefChecker are displayed as{Extractor} + {Checker}.",
  "DAnalysis of Internal Knowledge Bias": "In this section, we further analyze the emergenceof the hallucination from the perspective of theLLMs bias to the internal knowledge. We analyzewhether the evaluated model and the checker gen-erate response based on their internal knowledge : A comparison of RefChecker with previous works on our benchmark under Zero Context setting. Wehighlight the best results using proprietary LLMs with blue colors and best results results using pure open-sourcemodels with orange colors.",
  "REFCHECKERGPT-4 + GPT-493.8286.8995.9683.9582.35": "GPT-4 + NLI83.9871.2888.8960.8162.32GPT-4 + AlignScore90.5478.9793.9071.9570.37GPT-4 + RepC89.9681.1693.1677.4277.26Mistral-SFT + GPT-492.4783.4095.1380.8878.88Mistral-SFT + NLI89.9678.8693.4272.8972.07Mistral-SFT + AlignScore90.5478.7993.9175.8174.16Mistral-SFT + RepC89.3880.4392.7277.1476.74 : A comparison of RefChecker with previous works on our benchmark under Noisy Context setting. Wehighlight the best results using proprietary LLMs with blue colors and best results results using pure open-sourcemodels with orange colors.",
  "D.1Internal Knowledge Bias of EvaluatedModel": "In order to analyze whether the evaluated LLMsgenerate responses based on their own knowledgeor the provided context in Noisy and AccurateContext settings, we convert each claim-triplet ex-tracted from the response into a simple interroga-tive query for knowledge checking. For simplicity,we design a prompt template and ask GPT-4-Turboto generate these queries (). Then wefeed the query into the evaluated LLM to checkwhether it has such knowledge. The answer fromthe evaluated LLMs could be one of the following:",
  ". Unsure, means the evaluated LLM does nothave this knowledge or it has confusion on theknowledge": "The label pairs (Yes, Contradiction) and (Yes, Neu-tral) indicate that the model is utilizing internalinformation to generate this claim-triplet. On theother hand, (No, Entailment) and (Unsure, Entail-ment) signify that the model is relying on contex-tual information for generation. Pairs like (No, Con-tradiction) suggest that the evaluated model may beless proficient in processing context information,leading to the production of less reliable claim-triplets.The outcomes of the Accurate Context are illus-trated in . Upon examination, it is evi-dent that GPT-4-Turbo demonstrates the most no-table performance, primarily generating responsesaligned with the reference context. Conversely,GPT-3.5-Turbo tends to generate responses by re-lying on its internal knowledge to some extent,leading to contradictions or neutrality to the ref- : Spearmans rank correlation coefficients be-tween REFCHECKER and human evaluation. Resultsare grouped regarding extractors and checkers used. Re-sults for each extractor (row) and checker (column) arearranged into a sub-matrix in the figure, with correla-tions for rankings on 4 context settings (one additionalfor average) and 3 ranking metrics. : REFCHECKER results on the SelfCheckGPTdataset. The results of SelfCheckGPT are from theirpaper. We highlight the best results using proprietaryLLMs with blue colors and best results results usingpure open-source models with orange colors.",
  "Mistral-SFT + AlignScore75.1076.08Mistral-SFT + RepC76.5976.70": "erence context. In the case of InstructGPT, themodel further generates unsure information, whichalso contradicts the reference context. This be-havior may stem from contradictions within themodels internal knowledge or difficulties in com-prehending the amalgamated content of internaland reference information. Regarding LLaMA-2-70B, and Falcon-40B-Instruct, our observationsindicate that these models exhibit inferior perfor-mance. They generate information that contradictsinternal knowledge and is irrelevant to the refer-ence context. Alpaca 7B performs similarly toGPT-3.5-Turbo, but seldom generates informationcontradicting to its internal knowledge, Differentfrom the accurate context setting, all the modelstend to generate more Neutral labels in the noisycontext setting ().",
  "D.2Internal Knowledge Bias of Checker": "We also conduct an analysis to determine whetherthe checker provides predictions based on its inter-nal knowledge. In this analysis, a triplet extractedfrom the response is taken, and we mask the sub-jective or objective information in the context with####. The modified context, along with the triplet,is then inputted into the checker to obtain the label.In theory, the prediction label should be neutralbecause the relevant information in the context ismasked. If the label is not neutral, it implies thatthe model is making inferences based on its in-ternal knowledge. For the implementation of thisanalysis, we query GPT-4-Turbo with a specificallydesigned prompt to mask the triplet information, asillustrated in . Specifically, in the noisy-context setting, we implement the query for eachreference document and keep the document un-changed if there is no relevant information to theextracted triplet.The results of the accurate context setting areshown in . As we observe, RoBERTa-NLI achieves the most significant Neutral labels,62.64% and 53.70% for evaluated model GPT-3.5-",
  "GPT-4GPT-465.716.2928.00RoBERTa-NLI8.5711.1480.28": "Turbo and GPT-4-Turbo, respectively. The checkerGPT-4-Turbo achieves the second performance.The results of the zero context setting are in a sim-ilar pattern with those of accurate-context setting(). But in the noisy context setting (), RoBERTa-NLI outperforms GPT-4-Turbo witha large margin in the ratio of Neutral labels. Theresults may results from the strong bias to internalknowledge of GPT-4-Turbo when the context is ex-tremely long, or the RoBERTa-NLI model has lessassociative ability to the memorized knowledge.",
  "EPotential Risks": "As hallucination detection techniques become morerefined, there is a risk of overreliance on automatedsystems for determining the veracity of informa-tion. This could reduce critical engagement withcontent among users, potentially leading to a lackof scrutiny when systems fail to give a correct pre-diction. : The results of knowledge checking for evaluated models in the noisy-context setting. The label Yes, Noand Unsure are the respones to the interrogative sentences generated from knowledge triplets. Each value refers tothe percentage of each checking pairs in the total number of triplets."
}