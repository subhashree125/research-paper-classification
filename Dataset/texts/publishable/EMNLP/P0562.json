{
  "Abstract": "Recent studies have explored the workingmechanisms of In-Context Learning (ICL).However, they mainly focus on classificationand simple generation tasks, limiting theirbroader application to more complex gener-ation tasks in practice. To address this gap,we investigate the impact of demonstrationson token representations within the practicalalignment tasks. We find that the transformerembeds the task function learned from demon-strations into the separator token representation,which plays an important role in the genera-tion of prior response tokens. Once the priorresponse tokens are determined, the demon-strations become redundant. Motivated by thisfinding, we propose an efficient Progressive In-Context Alignment (PICA) method consistingof two stages. In the first few-shot stage, themodel generates several prior response tokensvia standard ICL while concurrently extract-ing the ICL vector that stores the task func-tion from the separator token representation. Inthe following zero-shot stage, this ICL vectorguides the model to generate responses with-out further demonstrations. Extensive exper-iments demonstrate that our PICA not onlysurpasses vanilla ICL but also achieves com-parable performance to other alignment tuningmethods. The proposed training-free methodreduces the time cost (e.g., 5.45) with im-proved alignment performance (e.g., 6.57+).Consequently, our work highlights the appli-cation of ICL for alignment and calls for adeeper understanding of ICL for complex gen-erations. The code will be available at",
  "Corresponding author": "LLMs achieve notable improvements and produceimpressive few-shot performance across a rangeof downstream tasks (Wei et al., 2022). After that,numerous studies have explored the working mech-anism of ICL and propose several effective methodsto enhance ICL (Hendel et al., 2023; Todd et al.,2023; Wang et al., 2023a; Li et al., 2024).However, these works mainly focus on classi-fication tasks and simple generation tasks, whichlimits the exploration of these methods in morecomplex generation tasks, such as aligning LLMswith human preferences. As a complex and practi-cal task, alignment typically requires training themodel, such as Supervise Fine-Tuning (SFT) (Zhouet al., 2023) and Reinforcement Learning from Hu-man Feedback (RLHF) (Ouyang et al., 2022). Arecent work (Lin et al., 2023) proposed URIAL, asimple method using in-context examples to alignseveral powerful base LLMs and achieves notableinstruction-following performance. The success ofURIAL demonstrates the feasibility of in-contextalignment and encourages us to explore and opti-mize ICL in the alignment task.In this paper, we investigate the impact of demon-strations during in-context alignment. We visualizethe token distribution KL-divergence of instruc-tions and responses in zero-shot and few-shot set-tings (). To reduce context noise, we set uptwo few-shot settings with different demonstrationsas control groups and have the following observa-tions through comparative experiments: (1) Themodel likely stores the task function learned fromthe demonstration in the separator token representa-tion. (2) Demonstrations play a crucial role in priorresponse generation but are redundant in posteriorresponse generation. These observations highlightthe influence of demonstrations on token represen-tation in ICL for alignment tasks, indicating thatdemonstrations are not always indispensable duringthe entire response generation stage.Motivated by these findings, we propose a Progressive In-Context Alignment (PICA) methodto enhance both the efficiency and effectiveness ofregular ICL. Specifically, Our approach involvesa two-stage progressive generation strategy: thefew-shot stage and the zero-shot stage. During thefew-shot stage, the model generates prior part ofthe response using the standard ICL settings. Aftergenerating a specific number of tokens, we transi-tion the model into the zero-shot stage, eliminatingthe need for further demonstrations to generatethe remaining part of the response. To capitalizeon the task-related information embedded in theseparator tokens, we introduce an ICL vector guid-ance method. Inspired by the work of task vector inICL (Hendel et al., 2023; Todd et al., 2023; Li et al.,2024), we extract the ICL vector from the hiddenstates of specific transformer layers. This vector isthen used to steer the model during the zero-shotstage by intervening in the forward pass. PICAminimize the need for demonstrations while im-proving output quality, thereby reducing the com-putational cost associated with demonstrations andenhancing overall performance. Extensive experi-ments show that PICA outperforms regular ICL inboth of efficiency and effectiveness. As a training-free method, it is also comparable to other align-ment methods (i.e., SFT and RLHF). For example,on average, our PICA boosts the performance ofMistral-7b to reach 90% of the performance ofGPT-4-0613. These results support our observa-tions and show the effectiveness of our method invarious aspects of alignment. Additionally, we con-duct ablation studies to investigate the robustnessand generalizability of our method. Our contribu-tions are summarized as follows:",
  "We delve into the impact of demonstrations ontoken representation in ICL and qualitatively ex-plore the working mechanism of task functionslearned from demonstrations in complex align-ment tasks": "We propose a progressive in-context alignmentmethod that incorporates progressive generationand ICL vector guidance. This method efficientlyaligns models and significantly reduces the com-putational cost associated with demonstrations. We conduct extensive evaluation and ablation ex-periments on the proposed method, where theresults have fully demonstrated its efficiency andeffectiveness. Our experiments and analyses pro-vide in-depth insights for future research on in-",
  "Related Work": "LLM Alignment.Prior works have exploredalignment tuning through supervised fine-tuning us-ing public instruction datasets (Wang et al., 2022;Zhou et al., 2023; Stiennon et al., 2020) or rein-forcement learning from human feedback (Stien-non et al., 2020; Rafailov et al., 2023). A commonapproach is to fine-tune models using instructiondata to enable them to follow instructions effec-tively. To rapidly accumulate a vast amount of in-struction tuning data, Wang et al. (2023b) proposesa pipeline to obtain instruction data from power-ful models, such as GPT-4. LIMA leverages only1000 high-quality instruction data points to fine-tune a 65B parameter LLM (Zhou et al., 2023). Itshows that the minimal tuning surprisingly resultsin a high win rate against ChatGPT. Following in-struction fine-tuning, the reinforcement learning isapplied to further align the models (Stiennon et al.,2020). Rafailov et al. (2023) introduces a train-ing method for alignment that does not require areward model. Its powerful convenience and effec-tiveness have made it one of the de facto methods.However, these methods necessitate substantial re-sources and there is evidence to suggest that suchtraining approaches cause model forgetting of pre-viously acquired knowledge in base LLMs (Wanget al., 2023b; Shen et al., 2023; Wang et al., 2022).In contrast to training-based methods, Lin et al.(2023) experiment with ICL for LLM alignmentand Confirm the feasibility of ICL for the align-ment task. Building on this finding, we explore atraining-free ICL approach. We do not merely uti-lize ICL. Instead, we initially investigate its work-ing mechanism in token representation learning.This investigation helps enhance the effectivenessof in-context alignment. Similar to us, a very recentconcurrent work (Zhan et al., 2024) also identifiesthe critical role of prior answer token selectionin alignment tasks, and proposes a SFT model orexternal resources guided generation method formultilingual instruction following. Differing fromtheir approach, we focus on the working mech-anisms and optimization methods of ICL in themainstream English alignment tasks.",
  "(d) Output Control Group": ": The KL-divergence of token probability distributions on Llama2-7b. Experimental Group compares zero-shot andfew-shot settings, while Control Group compares two few-shot settings with different demonstrations. We visualize the input andoutput separately and mark the prior query tokens and prior response tokens with purple circles. attention patterns in ICL and the process of gradi-ent descent (Akyrek et al., 2023; Dai et al., 2023).From a more practical perspective, another line ofresearch suggests that the ICL may function bylearning a mapping function from demonstrations,which it then applies to input queries to make pre-dictions (Hendel et al., 2023; Todd et al., 2023; Liet al., 2024). Hendel et al. (2023) extract an ICLtask vector from the hidden states and utilize it forintervention during zero-shot inference. Todd et al.(2023) extract a function vector from attention acti-vations using the causal mediation method, whichis subsequently added to the hidden states of cer-tain transformer layers during inference. Li et al.(2024) derive a state vector from attention activa-tions and propose several optimization strategies.Unlike these works, we focus on using comparativeexperiments to explore the impact of demonstra-tions on token representation, and leverage thesefindings to enhance the efficiency of ICL.",
  "Motivation": "In this section, we aim to shed light on the workingmechanisms of in-context learning by investigat-ing the following question: What is the impactof demonstration on token representation in in-context alignment? To explore this, we design acomparative experiment to highlight how token rep-resentations differ between zero-shot and few-shotsettings. We use token probability distributionsas a proxy for token representations and utilizeKL-divergence to measure the shifts in these distri-butions. By visualizing and quantifying the shiftsin token probability distributions caused by demon-strations, we can understand the role of demon-strations in aligning the model and provide furtheroptimization for in-context alignment.Regarding the experimental setup, we randomly selected 100 data instances of similar length fromUltra-chat (Ding et al., 2023), a commonly useddataset for alignment tuning, as our experimentaldataset. For the input prompt, we use a straightfor-ward design by adding several tokens at the end ofthe query to serve as separator tokens, explicitlydistinguishing between the query and the response.We present the visualization results based on theLlama2-7b model in the , while the resultsfor other models are provided in Appendix C. Webreak the token distribution of the whole instanceinto the input and output parts. A straightforwardreason is that the input token distribution shift rep-resents differences in understanding the instruction,while the output token distribution shift representsthe ability to respond. By observing and analyzingthe visualization, we have two hypotheses: (1) theICL alignment task function might be encoded intothe separator token representation. (2) the qualityof response is highly reliant on the quality of priorresponse tokens. Input Token Distribution.By comparing theinput token probability distributions between zero-shot and few-shot settings, a significant shift is ob-served in both the prior tokens of the query and theseparator tokens. The KL-divergence decreases asthe number of query tokens increases. By compar-ing the experimental group and the control group,we find that the shift in the query distribution alsooccurs in the control group. However, this shiftin the separator tokens is not consistent across dif-ferent demonstration settings, suggesting distinctunderlying causes for these shifts. We attributethe shift in the querys prior token distribution to acontext shift, and we attribute the shift in the sep-arator tokens distribution to a task shift. Giventhat LLMs are trained to predict the next token based on the provided context, altering the contextdirectly impacts the token distribution, which werefer to as the context shift. However, as the num-ber of query tokens increases, the decision spacegradually aligns for both zero-shot and few-shotsettings, leading to higher consistency in query to-ken prediction and thus a reduced KL-divergence.On the contrary, the trend observed in the querydistribution is not mirrored in the separator tokendistribution. In the control group, the separatortoken representations remain highly similar. We at-tribute the large KL-divergence observed in the sep-arator token distribution of the experimental groupto the differing tasks, indicating that separator to-kens likely encode task-specific information duringICL. We reasonably speculate that the primary im-pact of demonstration on instruction understandingis reflected in the encoding of separator tokens,where the alignment task function learned throughICL is stored. This hypothesis aligns with priorwork (Hendel et al., 2023; Li et al., 2024), yet ourfindings contribute additional evidence supportingthis perspective. Output Token Distribution.Observing the vi-sualization of output token distribution, we findthat when comparing zero-shot and few-shot set-tings, the response token distribution shows simi-larity in the posterior tokens. This indicates thatthe model selects posterior tokens with high con-sistency in both zero-shot and few-shot settings.When comparing the prior response tokens of theexperimental group and the control group, we ob-serve a pattern similar to that of the separator to-kens, suggesting that demonstrations play a cru-cial role in the prior response tokens. Based onthese observations and analyses, we speculate thatthe primary impact of demonstrations on responsegeneration is reflected in the generation of prioranswer tokens. Compared to zero-shot settings,demonstrations guide the generation of accurateprior response tokens, which implicitly helps themodel successfully follow the instructions. Thisobservation also suggests that once the prior re-sponse tokens are determined, the influence of thedemonstration diminishes and becomes redundant.",
  "Discard": ": Overview of PICA, which include few-shot stageand zero-shot stage. The gray block denotes the hidden stateand orange block denotes the separator token hidden state thatforms the ICL vector. The blue block denotes the generatedanswer token from few-shot stage. formance, we introduce a progressive in-contextalignment approach. This methodology enhancesthe efficiency and efficacy of in-context alignmentthrough two innovations: (1) a progressive gen-eration strategy that reduces the computationalcost associated with demonstrations, and (2) in-context learning vector guidance that compressesthe task function from demonstrations to assist inhigh-quality response generation.Inspired by underscoring the redundancy ofdemonstrations once the pivotal prior response to-kens are determined, we introduce a progressivegeneration strategy, dividing response generationinto few-shot and zero-shot stages. During the few-shot stage, the model generates a specific numberof prior response tokens by employing a standardin-context learning:",
  "Y fewi= arg maxY VP(Y |D, Q, S, Y few1:i1),(1)": "where D is the demonstration, Q is the query, Sis the separator token, and Y fewiis the i-th answertoken generated in few-shot stage. After obtain-ing several prior answer tokens, the model oper-ates within a more certain and simplified decisionspace for token generation, allowing the omissionof the demonstration to reduce computational costs.",
  "where N is the number of prior tokens, and Y zeroithe i-th answer token generated in zero-shot stage": "In-context Learning Vector Guidance.We ob-serve that transformers show task-specific encod-ing behavior with the separator token.Recentworks (Hendel et al., 2023; Todd et al., 2023) havesimilar observations, demonstrating that functionslearned by ICL can be represented through com-pressed vectors derived from transformers and canperform simple generation tasks in zero-shot set-tings. Building on this, we propose the ICL vectorguidance to assist the model in generating high-quality responses during the zero-shot stage. Un-like these previous works that intervene single hid-den state of the last separator token, we intervenein the initial L layer of all separator tokens. Ourpreliminary experiments found that this method ismore effective for the alignment task, where theoutput is much longer than that of the simple gen-eration tasks focused on in previous works.Specifically, during the forward pass in the few-shot generation, we extract the separator token hid-den state Hfewifrom the first L layers, which wecombine and refer to as the ICL vector. Subse-quently, in the zero-shot stage, we intervene in theseparator token representation by replacing the hid-den state with the extracted hidden state from thefew-shot stage:",
  "Hfewiif i LLayer(Hzeroi1)otherwise ,(3)": "where Layer() is the process function of trans-former layer. By intervening with the ICL vec-tor, the model receives implicit guidance from thedemonstration during generation, thereby improv-ing the quality of the zero-shot stage responses.Overall, Our progressive in-context alignmentprocess is: In the few-shot stage, we utilize stan-dard ICL to generate pivotal prior response tokenswhile extracting the ICL vector from the separatortoken representation. Subsequently, we discard thedemonstration and employ the ICL vector to guidethe model in generating the complete response inthe zero-shot setting. This dual-stage progressivein-context alignment approach fully capitalizes on the potential of the ICL vector and the text comple-tion capabilities of foundational language modelsin the zero-shot setting. By effectively harnessingthese capabilities, the approach not only reducescomputational cost but also maintains high fidelityin response generation across various settings.",
  "Datasets and Models": "Recent research demonstrates that utilizing power-ful AI assistants such as ChatGPT and GPT-4 forscoring and comparing achieves close alignmentwith human evaluations while reducing costs (Liuet al., 2023; Dubois et al., 2024). Consequently,we evaluate our method using two automatic align-ment benchmarks: alpaca-eval (2.0) (Dubois et al.,2024) and just-eval (Lin et al., 2023). Alpaca-evalcomprises 805 instructions and provides a length-controlled win rate from the judge model by com-paring the assessed results with those from a refer-ence model. For fast and validated evaluation, weselect GPT-3-text-davinci-003 and GPT-4 as refer-ence models, while employing GPT-4-0314 as thejudge model. Just-eval includes 800 regular instruc-tions and 200 red-teaming and malicious instruc-tions selected from diverse open-source datasets,offering detailed evaluations across six aspects. Oneach aspect, scores range from 1 to 5, represent-ing the degree of evaluation. In line with priorwork (Lin et al., 2023), we use GPT-4-0314 as theevaluator and report the performance across threerandom seeds. For efficiency analysis, we evaluatethe average inference time on 1000 test data withstrictly generated 4096 tokens without using anyadditional decoding optimization techniques. Wereport the speedup compared to the standard ICL.We conduct our experiments using three prin-cipal fundamental LLMs: Llama2-7b, Llama2-13b (Touvron et al., 2023) and Mistral-7b(v0.1) (Jiang et al., 2023). These models are se-lected based on their moderate sizes, open-sourceavailability, and proficiency in ICL. For compara-tive analysis, we utilized their respective alignment-tuned versions: Llama2-7b-chat, Llama2-13b-chat,and Mistral-7b-Instruct, facilitating a direct com-parison with SFT and RLHF. Additionally, ourstudy includes results from OpenAIs GPT models(i.e., GPT-3.5-turbo-0611 and GPT-4-0613), allow-ing comparison with the state-of-the-art AI assis-tants. We follow the inference guidelines providedby the authors of these tuned models.",
  "Implementation Detail": "For the in-context learning prompt, we follow previ-ous work (Lin et al., 2023) and use the mainstreamsystem message employed in aligned LLMs. Wemeticulously designed the demonstrations for in-context learning, creating six examples for alpaca-eval and three examples for just-eval, as they em-phasize different evaluation aspects. We utilizegreedy generation with a beam size of 1 and setthe maximum token length to 4096. The in-contextlearning vector guidance method we described ear-lier has a key hyper-parameter, specifically thelayer L. Previous studies (Hendel et al., 2023) havedemonstrated that the choice of L influences perfor-mance. We determine the intervention layer basedon the win rate on alpaca-eval. We set the numberof prior tokens to 10 as a trade-off between gener-ation quality and efficiency. For consistency andreproducibility, we apply greedy decoding acrossall experiments. All experiments were conductedon a single NVIDIA A800 80G GPU, with each ex-periment consuming between 3 to 5 hours of GPUtime, depending on the dataset and models used.",
  "SFT or RLHF is the baseline with alignmenttuning method. We strictly follow the guidelinesprovided by the creators of these tuned modelsduring inference": "Zero-shot is the baseline for the zero-shot set-ting that uses only the given query as input, andVanilla ICL is the regular ICL which makes pre-dictions on the label by taking both demonstra-tion and instruction. Vec. is the ablation variants that only utilize ICLvector guidance in zero-shot setting, while Prog.is the ablation variants that apply progressivegeneration strategy without ICL vector guidanceduring zero-shot stage.",
  "the two proposed innovations. The combination ofthese innovations constitutes our PICA method": "PICA outperforms the baseline with tuning-freebaselines. As shown in the , our methodoutperforms zero-shot and vanilla ICL baselinesacross three models on alpaca-eval. On the just-eval dataset, our PICA also surpasses the tuning-free baseline in the majority of aspects. Comparedto regular ICL, our method effectively improveshelpfulness, factuality, engagement, and safety.However, in terms of clarity and depth, our methodshows a minor decline. We attribute this to the factthat our approach still has limitations in generatingconsistently information-rich responses, indicatingthat the ICL vector cannot fully encapsulate all theinformation provided by the demonstration. PICA is comparable to the alignment tuningmethods. When compared to SFT or RLHF mod-els, our approach demonstrates superior perfor-mance on the alpaca-eval dataset, indicating anoverall advantage over SFT and RLHF methods.However, on the just-eval dataset, the results varyacross different aspects. For instance, in the aspectsof helpfulness and factuality, our method excels,highlighting its capability to follow instructionsand generate high-quality and accurate responses.This also supports the widespread hypothesis thatalignment tuning may cause models to forget someof their knowledge (Wang et al., 2023b; Shen et al.,2023). Conversely, in terms of clarity, depth, andengagement, our method lags slightly, suggestingthat SFT and RLHF have an advantage in produc-ing high-quality response styles over ICL. In termsof safety, our method surpasses SFT but does notexceed RLHF, indicating that ICL provides rela-tively basic safety alignment. On the other hand,with strong models such as Llama2-13b or Mistral-7b, the performance of our PICA can reach 90% ofthe performance of GPT-3.5 and GPT-4. PICA achieves high efficiency compared tovanilla ICL. Analyzing the speedup shown in Ta-ble 1, our method significantly reduces the timecost compared to vanilla ICL (e.g., achieving a5.45 speedup on Llama2-7b) and is close to thezero-shot method across three models. This im-provement is attributed to our progressive genera-tion strategy, which successfully saves a substan-tial amount of time by discarding the demonstra-tion. Notably, our method is orthogonal to attentionspeedup techniques, such as flash attention (Daoet al., 2022) and page attention (Kwon et al., 2023).",
  "Mistral-7b (PICA)66.3844.334.794.864.794.424.594.344.93": ": Comparison of alignment performance and efficiency. Alpaca-eval presents the win rate against competitor models,while Just-eval presents the scores across six aspects (scores are on a scale of 1-5). Results highlighted in gray represent ourmethods: Vec. denotes the ICL vector guidance and Prog. denotes progressive generation ablation variants. The best results ineach aspect are marked in bold. Speedup indicates the efficiency improvement compared to vanilla ICL.",
  "We will leave further exploration for future work": "Both progressive generation strategy and ICLvector guidance contribute to performance im-provement. We conduct ablation experiments onour proposed progressive generation strategy andICL vector guidance, as indicated by the resultshighlighted in grey in . When only one ofthese methods is used, the models performance de-clines, with a more significant drop observed whenthe progressive generation strategy is removed.This clearly demonstrates the effectiveness of bothmethods, with the progressive generation strategyplaying a more critical role. It also indicates thelimitations of ICL vector guidance, which, whileeffective in simpler tasks (Hendel et al., 2023; Toddet al., 2023), shows constraints in more complexalignment tasks.Overall, our method outperforms ICL in perfor-mance and efficiency, achieving results comparableto alignment tuning. These promising outcomesvalidate the effectiveness of our approach and em-pirically support our understanding of the role ofdemonstrations in in-context alignment.",
  ": Win rate comparing with GPT-3-text-davinci-003on alpaca-eval for each choice of the intermediate layer L": "3-text-davinci-003 on the alpaca-eval datasets, asshown in . Our results reveal a dual-phasetrend: initially, increasing the number of layers im-proves performance, but this improvement stops orslightly declines in the later layers. This indicatesthat the ICL function is dynamically stored withinthe separator token representation. In the initial lay-ers, transformers primarily focus on learning andencapsulating the ICL function within the hiddenstate, where additional layers enhance the richnessof the functional information in the ICL vector. Incontrast, the later layers prioritize applying thislearned information for prediction tasks. Here, ad-ditional layers tend to introduce noise, causing aslight drop in performance. This also suggests that Number of Prior Tokens 0.7 0.8 0.9 1.0 1.1 %ICL Mistral-7b-v0.1Llama2-7bLlama2-13b",
  "Prior token Ablation": "presents an ablation study on the numberof prior tokens across three models, normalized bythe vanilla ICL results. An intuitive conclusion isthat increasing the number of prior tokens improvesthe models performance, and with about 8 priortokens, PICA surpasses vanilla ICL. However, thisimprovement trend gradually diminishes. When thenumber of prior tokens reaches 10, the performancegain becomes less significant. This indicates thatthe demonstration aligns approximately the first 10tokens to human performance. After generating 10tokens, the base model can largely complete theresponse generation independently.",
  ": Results of human evaluation: The win rate of pairwisecomparisons between PICA and SFT or RLHF": "models using five different sets of demonstrations.The results, including the mean and standard devia-tion of the performance metrics, are shown in Fig-ure 5. We observe that the ICL method is more sen-sitive to changes in the demonstrations compared tothe PICA method across all three models. This in-dicates that PICA effectively enhances robustness.We attribute this to our approach of explicitly incor-porating demonstrations only in the prior responsetokens, while using implicit demonstration repre-sentations during the zero-shot generation stage.This strategy effectively mitigates the impact ofsuboptimal demonstrations on performance.",
  "Human Evaluation": "We randomly sampled 100 examples each fromthe alpaca-eval and just-eval datasets, presentingthe responses generated by PICA alongside thosefrom the SFT or RLHF models to computer sciencegraduate students who serve as annotators. Weasked the annotators to choose which response wasbetter or if it was a tie. shows the results,which align with the automated evaluation.",
  "Conclusion": "In this paper, we investigate and analyze the im-pact of demonstrations on token representation inin-context alignment through comparative exper-iments. Based on our observations and analyses,we introduce a novel progressive in-context align-ment method that significantly reduces the need fordemonstrations while preserving alignment perfor-mance. Extensive experiments indicate that PICAoutperforms tuning-free baselines in both effective-ness and efficiency, achieving performance that isbetter or comparable to SFT or RLHF. Our exper-iments and analyses provide in-depth insights forfuture research on ICL in alignment. In the fu-ture, we aim to further explore the mechanisms andoptimizations of ICL in more complex tasks.",
  "Despite our discoveries and improvements, wemust acknowledge certain limitations in our work:": "Model Size:We evaluated our method onLlama2-7b, Llama2-13b, and Mistral-7b, and theseexperiments were conducted on a limited scale withmoderately sized models. This limits our explo-ration of the application of PICA on larger models.We will explore the use of PICA on larger modelssuch as llama2-70b in future work. Theoretical Foundation:Our conclusions aboutthe role of demonstration and ICL working mech-anism lack rigorous theoretical grounding. In ex-ploring the working mechanism of ICL, we de-rived some hypotheses through comparative ex-periments on token representation. While thesehypotheses provided insights, they lack solid math-ematical derivation and a theoretical basis, limitingthe generalizability of our method. For example, inAppendix B, we analyze a kind of instruction thatPICA does not handle well. Evaluation Datasets:Most of our experimentsutilized the alpaca-eval and just-eval datasets,which are based on AI assistant automated evalua-tion pipelines. Related work (Dubois et al., 2024)has shown that these GPT-4-based evaluation meth-ods can introduce biases, such as a preference forlonger responses, which may affect the accuracy ofour experimental results. Additionally, our datasetquantity is still limited, and the evaluation metricsdo not fully cover all aspects of alignment, suchas mathematics, reasoning, and coding. We willcontinue to explore our method with more compre-hensive evaluation metrics in future work.",
  "In-context learning creates task vectors. In EMNLP(Findings), pages 93189333. Association for Com-putational Linguistics": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-lez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-ory management for large language model servingwith pagedattention. In Proceedings of the 29th Sym-posium on Operating Systems Principles, SOSP 2023,",
  "Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun,Baotian Hu, and Min Zhang. 2024. In-context learn-ing state vector with inner and momentum optimiza-tion. CoRR, abs/2404.11225": "Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu,Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chan-dra Bhagavatula, and Yejin Choi. 2023. The unlock-ing spell on base llms: Rethinking alignment viain-context learning. CoRR, abs/2312.01552. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023. G-eval:NLG evaluation using gpt-4 with better human align-ment. In EMNLP, pages 25112522. Association forComputational Linguistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,John Schulman, Jacob Hilton, Fraser Kelton, LukeMiller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, and Ryan Lowe.2022. Training language models to follow instruc-tions with human feedback. In Advances in NeuralInformation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023.",
  "Eric Todd, Millicent L. Li, Arnab Sen Sharma, AaronMueller, Byron C. Wallace, and David Bau. 2023.Function vectors in large language models. CoRR,abs/2310.15213": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023a. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. In EMNLP,pages 98409855. Association for ComputationalLinguistics. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, JackHessel, Tushar Khot, Khyathi Chandu, David Wad-den, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,and Hannaneh Hajishirzi. 2023b. How far can camelsgo? exploring the state of instruction tuning on openresources. In Advances in Neural Information Pro-cessing Systems 36: Annual Conference on NeuralInformation Processing Systems 2023, NeurIPS 2023,New Orleans, LA, USA, December 10 - 16, 2023. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-jana Arunkumar, David Stap, Eshaan Pathak, Gi-annis Karamanolakis, Haizhi Gary Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuz-nia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-tel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-hit, Neeraj Varshney, Phani Rohitha Kaza, PulkitVerma, Ravsehaj Singh Puri, Rushang Karia, SavanDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,Sujan Reddy A, Sumanta Patro, Tanay Dixit, andXudong Shen. 2022. Super-naturalinstructions: Gen-eralization via declarative instructions on 1600+ NLPtasks. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2022, Abu Dhabi, United Arab Emirates, De-cember 7-11, 2022, pages 50855109. Associationfor Computational Linguistics. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. 2022. Emer-gent abilities of large language models. Trans. Mach.Learn. Res., 2022.",
  "ACase Study": "We present a case study comparing SFT, ICL, andPICA on Mistral-7b in . The SFT modelincorrectly stated that Canada was colonized bythe British in 1607, leading to poor performance infactuality with a score of 1. This highlights a com-mon issue with SFT models, where they may forgetacquired knowledge over time. As a result, the SFTmodel received low marks in helpfulness (3) andengagement (2), despite a reasonable clarity score(4). This misrepresentation shows the limitationsof the SFT approach in retaining and accurately re-calling historical facts. The ICL model is relativelybetter in factuality. However, the generated con-tent lacked depth and richness, scoring 2 in depthand 2 in helpfulness, suggesting that while the ICLmethod generates some stylistic tokens, it does notproduce sufficiently detailed or useful responses.Our PICA model provided a comprehensive andaccurate response, detailing the colonization his-tory of Canada, resulting in high scores across allaspects: helpfulness (5), clarity (5), factuality (5),depth (4), and engagement (4). The PICA modeleffectively combined stylistic tokens with detailedand accurate information, showcasing its capabil-ity to generate high-quality responses that are bothinformative and engaging.",
  "BError Analysis": "In our preliminary experiments, we found that theproposed PICA approach frequently performedpoorly in generating enumeration-type responses(e.g. Give me a list of some famous world mu-sic artists.). Consequently, we analyzed the KL-divergence of responses to these instructions inzero-shot and few-shot settings. The visualizationresults are shown in . Our observationsindicate that, although the trend of KL-divergenceis generally similar to what we observed in 3 thereare differences in each enumeration of the response.We found that the KL-divergence of prior tokens isusually larger than the posterior tokens in each enu-meration, indicating that these prior enumerationtokens are pivotal. The quality of responses to enu-merative instructions is influenced not only by theselection of prior response tokens but also by theselection of prior enumeration tokens. We attributethis to the fact that each enumerated item is rela-tively independent of each other. When generatingthese enumerations, the model requires more sub-stantial guidance from the demonstrations. How- ever, the proposed ICL vector and the positionsof previous enumeration responses do not provideenough information for generation, thus reducingthe quality of each enumeration. This highlights alimitation of our current PICA approach, which wewill explore and optimize in future work.",
  "CMore Exploration on Demonstration": "We present additional comparative experiments tofurther delve into the impact of demonstrations ontoken representation. We conduct experiments onLlama2-7b and Mistral-7b models with the samedata as in 3. The experimental group includes bothzero-shot and few-shot methods, while the controlgroup includes two different demonstrations in few-shot settings. In addition to the KL-divergence oftoken distributions, we introduce two new metricsfor measuring the difference between the two meth-ods, i.e., Top Token Prob and Top Token Rank.Top Token Rank refers to the ranking position ofa token predicted by one method within the tokendistribution of another method. Specifically, giventhe context, we first obtain the next predicted to-ken from one method and then determine its rankwithin the token distribution of the other method.A lower Top Token Rank manifests a greater over-lap in the decision space under the greedy searchsetting. Analogously, Top Token Prob indicatesthe probability of a token predicted by one methodwithin the distribution of another method. In thiscase, we first obtain the next predicted token fromone method and then report its probability in thetoken distribution of the other method. A higherTop Token Prob manifests a higher likelihood of ob-taining the same result under the greedy samplingsetting. These metrics directly reflect the similaritybetween the generation results of the two methods.By evaluating both Top Token Rank and Top TokenProb, we can obtain a comprehensive understand-ing of how closely the methods align in terms oftheir token predictions.The KL-divergence results on Mistral-7b areshown in , and we find a similar pattern tothat in .The Top Token Rank results are shown in Fig-ure 9 and . For convenience, we set therank of the token to 10 if it is greater than 10. Wefind that tokens with a large KL-divergence differ-ence typically also have a higher Top Token Rank,indicating that our understanding of demonstra-tions applies to this observation as well. Notably,",
  "When was Canada colonized?": "Canada was colonized by the British in 1607. The first successful British colony was established at Jamestown, Virginia, in 1607, and three years later, the Pilgrims established the Plymouth Colony in present-day Massachusetts. Over the next two centuries, the British gradually established more colonies along the eastern seaboard, eventually forming the thirteen colonies that would become the United States. Hello, I'm happy to help you! Canada was colonized by European settlers in the 16th century. The first European settlement in Canada was established by the French in 1534, when Jacques Cartier explored the St. Lawrence River and claimed the land for France. The British later established their own colonies in Canada, and the two nations fought for control of the territory for centuries. The British eventually gained control of most of Canada, and the country became a self-governing dominion of the British Empire in 1867. Canada gained full independence from Britain in 1982, and has since become a thriving democracy and a leading member of the international community.",
  ": KL-divergence of response token distributions of enumerative instructions on Llama2-7b and Mistral-7b": "even though the separator token distribution dif-fers significantly, the Top Token Rank remains low.This observation suggests that though demonstra-tions have a lot of influence on the separator tokenrepresentation, the predicted next token rank stillremains unchanged.The Top Token Prob results are shown in Fig-ure 11 and , where we find that tokenswith a large KL-divergence difference typicallyalso have a low Top Token Prob. This further sup-ports our understanding of the role that demonstra-tions play in the ICL. Similar to the result of TopToken Rank, the predicted separator token proba-bility is high, indicating that demonstration will notchange the selection of separator token. Overall, we observe similar patterns across KL-Divergence, Top Token Rank, and Top Token Probmetrics, despite minor differences. This demon-strates the generalizability and universality of ourunderstanding of the impact of demonstrations.",
  "# Instruction": "Below is a list of conversations between a human and an AI assistant (you).As an AI assistant, you will engage in conversations with users, responding to their queries which are presented underthe heading \"# Query:\".Your responses should be entered under the heading \"# Answer:\".You excel in a wide range of tasks including, but not limited to, providing general information, conducting reasoning,engaging in role-play, creative writing, planning, and solving mathematical and coding problems.Your responses should be well-structured, comprehensive, and aim to thoroughly address the users query or problem athand.When enumerating items in your responses, limit the examples to no more than ten, and avoid completely redundantcontent.Please ensure that your responses are encapsulated within triple backticks (```) at the start and end to maintainformatting consistency throughout the conversation.",
  "Llama2-70b (PICA)68.6645.314.854.854.824.214.584.706.73": ": Comparison of alignment performance and efficiency on Llama2-70b. Alpaca-eval presents the win rate againstcompetitor models, while Just-eval presents the scores across six aspects (scores are on a scale of 1-5). Results highlighted ingray represent our methods: Vec. denotes the ICL vector guidance and Prog. denotes progressive generation ablation variants.Speedup indicates the efficiency improvement compared to vanilla ICL."
}