{
  "Abstract": "Efficiently deriving structured workflows fromunannotated dialogs remains an underexploredand formidable challenge in computational lin-guistics. Automating this process could signif-icantly accelerate the manual design of work-flows in new domains and enable the groundingof large language models in domain-specificflowcharts, enhancing transparency and con-trollability.In this paper, we introduce Di-alog2Flow (D2F) embeddings, which differfrom conventional sentence embeddings bymapping utterances to a latent space wherethey are grouped according to their commu-nicative and informative functions (i.e., the ac-tions they represent). D2F allows for modelingdialogs as continuous trajectories in a latentspace with distinct action-related regions. Byclustering D2F embeddings, the latent space isquantized, and dialogs can be converted intosequences of region/action IDs, facilitating theextraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset byunifying twenty task-oriented dialog datasetswith normalized per-turn action annotations.We also introduce a novel soft contrastive lossthat leverages the semantic information of theseactions to guide the representation learning pro-cess, showing superior performance comparedto standard supervised contrastive loss. Evalua-tion against various sentence embeddings, in-cluding dialog-specific ones, demonstrates thatD2F yields superior qualitative and quantitativeresults across diverse domains.1",
  "Action:THANK_YOU": ": Example segment of the dialog SNG1533 fromthe hospital domain of the SpokenWOZ dataset. Ac-tions are defined by concatenating the dialog act label(in bold) with the slot label(s) associated to each utter-ance. with the latter focusing on helping users achievespecific tasks (Jurafsky, 2006). In TOD, struc-tured workflows guide agents in assisting userseffectively. This paper explores the underexploredterrain of automatically extracting such workflowfrom a collection of conversations. Extracting workflows automatically is crucial forenhancing dialog system design, discourse analysis,data augmentation (Qiu et al., 2022), and traininghuman agents (Sohn et al., 2023). Additionally, itcan ground LLMs in domain-specific workflows,improving transparency and control (Raghu et al.,2021; Chen et al., 2024). Recent works have at-tempted to induce structural representations fromdialogs using either ground truth annotation or adhoc methods (Hattami et al., 2023; Qiu et al., 2022;Sun et al., 2021; Qiu et al., 2020). We believethat models specifically pre-trained for this purposecould significantly advance the field.",
  "In Task-Oriented Dialog (TOD), dialog acts andslots are key concepts (Jurafsky, 2006). Dialog": ": Directed graph representing the hospital do-main workflow obtained from all the hospital dialogsin the SpokenWOZ dataset. Nodes correspond to indi-vidual actions. The width of edges and the underlinethickness of nodes indicate their frequency. User actionsare colored to distinguish them from system actions. acts represent the speakers communicative intent,while slots capture task-specific information. A di-alog action encapsulates both the dialog act and itscorresponding slots, enabling us to view dialogs assequences of canonical steps that convey both com-municative and informative functions ().Motivated by this perspective, we propose embed-ding sentences into a latent space grouped by rep-resentative actions rather than solely by sentencesemantics. Similar to how aggregating action se-quences from multiple dialogs reveals a commonunderlying workflow (), clustering sen-tence embeddings in this latent space could uncovercommon conversational steps, potentially reveal-ing the underlying workflow. The main contribu-tions of this work are threefold: (a) we consoli-date twenty task-oriented dialog datasets to createthe largest publicly available dataset with standard-ized action annotations; (b) we introduce a novel soft contrastive loss that leverages the semanticinformation of dialog actions to guide the repre-sentation learning process, outperforming standardsupervised contrastive loss; and (c) we introduceand release Dialog2Flow (D2F), to the best of ourknowledge, the first sentence embedding modelpre-trained specifically for dialog flow extraction.",
  "Related Work": "Sentence EmbeddingsTransformer-based en-coders like Universal Sentence Encoder (Cer et al.,2018) and Sentence-BERT (Reimers and Gurevych,2019) outperformed RNN-based ones such as Skip-Thought (Kiros et al., 2015) and InferSent (Con-neau et al., 2017). These models use a poolingstrategy (e.g., mean pooling, [CLS] token) to ob-tain a single sentence embedding optimized forsemantic similarity. However, specific domains re-quire different similarity notions. In the context ofdialogs, models like TOD-BERT (Wu et al., 2020),DialogueCSE (Liu et al., 2021) and Dialog Sen-tence Embedding (DSE) (Zhou et al., 2022) haveshown that conversation-based similarity outper-forms semantic similarity across different TODtasks. Likewise, we hypothesize that action-basedsimilarity can yield meaningful workflow-relatedsentence embeddings.Contrastive Learning Contrastive learning hasachieved success in representation learning for bothimages (Chen et al., 2020; He et al., 2020; Henaff,2020; Tian et al., 2020; Chen et al., 2020; Hjelmet al., 2019) and text (Zhou et al., 2022; Zhanget al., 2022, 2021; Gao et al., 2021; Wu et al.,2020). It learns a representation space where sim-ilar instances cluster together and dissimilar in-stances are separated. More precisely, given ananchor with positive and negative counterparts, thegoal is to minimize the distance between anchor-positive pairs while maximizing the distance be-tween anchor-negative pairs. Negatives are typi-cally obtained through in-batch negative sampling,where positives from different anchors in the mini-batch are used as negatives.",
  "Representation Learning Framework": "Following common practices (Zhou et al., 2022;Chen et al., 2020; Tian et al., 2020; Khosla et al.,2020), the main components of our framework are: Encoder, f() Rn, which maps x to arepresentation vector, x = f(x).Following Sentence-BERT (Reimers and Gurevych, 2019)and DSE (Reimers and Gurevych, 2019), f() con-sists of a BERT-based encoder with mean poolingstrategy trained as a bi-encoder with shared weights(siamese network). Contrastive head, g() Rd, used duringtraining to map representations x to the spacewhere contrastive loss is applied. Following Chenet al. (2020) and DSE, we instantiate g() as themulti-layer perceptron with a single hidden layerz = g(x) = ReLU(xW1)W2 where W1 Rnn and W2 Rnd. Similarity measure, sim(u, v), used to learn therepresentation is cosine similarity. Thus, similarityis then measured only by the angle between u andv, making our latent space geometrically a unit hy-persphere. Hence, in this study, we treat similarityand alignment interchangeably. Additionally, weassume f() and g() vectors are L2-normalized,leading to sim(u, v) = cos(u, v) = u v. 3.1.1Supervised Contrastive LossFor a batch of N randomly sampled anchor, posi-tive, and label triples, B = {(xi, x+i , yi)}Ni=1, thesupervised contrastive loss (Khosla et al., 2020),for each i-th triplet (xi, x+i , yi) is defined as:",
  "Nk=1 eziz+k /(1)": "where Pi = {j | yi = yj} is the set of indexes ofall the samples with the same label as the i-th sam-ple in the batch, and is the softmax temperatureparameter that controls how soft/strongly positivepairs are pulled together and negative pairs pushedapart in the embedding space.2 The final loss iscomputed across all the N pairs in the mini-batchas Lsup = 1",
  "Dataset#U#D #DA #S": "ABCD (Chen et al., 2021)20.4K 10010BiTOD (Lin et al., 2021)72.5K613 33Disambiguation (Qian et al., 2022)114.3K 8928DSTC2-Clean (Mrkic et al., 2017)25K128FRAMES (El Asri et al., 2017)20K121 46GECOR (Quan et al., 2019)2.5K1210HDSA-Dialog (Chen et al., 2019)91.9K8624KETOD (Chen et al., 2022)107.7K 2015 182MS-DC (Li et al., 2018)71.9K311 56MulDoGO (Peskov et al., 2019)74.8K6063MultiWOZ2.1 (Eric et al., 2020)108.3K 8927MultiWOZ2.2 (Zang et al., 2020)55.9K8226SGD (Rastogi et al., 2020)479.5K 2015 184Taskmaster1 (Byrne et al., 2019)30.7K6159Taskmaster2 (Byrne et al., 2019)147K 111 117Taskmaster3 (Byrne et al., 2019)589.7K 1121WOZ2.0 (Mrkic et al., 2017)4.4K1210SimJointMovie (Shah et al., 2018)7.2K1145SimJointRestaurant (Shah et al., 2018) 20K1159SimJointGEN (Zhang et al., 2024)1.3M1165",
  "Training Targets": "We experiment with four types of training targets,distinguished by whether the dialogue action labelis used directly or decomposed into dialogue actand slot labels, and by the type of contrastive lossemployed. Specifically, we consider the followingtwo targets using the proposed soft contrastive loss: D2Fsingle: L = Lsoftact+slots D2Fjoint: L = Lsoftact+ Lsoftslotsand the two corresponding targets using the defaultsupervised contrastive loss: D2F-Hardsingle: L = Lsupact+slots D2F-Hardjoint: L = Lsupact + LsupslotsThe subscript in bold indicates the type of labelused to compute the loss, either the dialog action asa single label (act+slots), or the dialog act and slotsseparately. In the case of the joint loss, separatecontrastive heads g() are employed.",
  "We identified and collected 20 TOD datasets fromwhich we could extract dialog act and/or slot anno-": "tations, as summarized in . We then man-ually inspected each dataset to locate and extractthe necessary annotations, manually standardizingdomain names and dialog act labels across datasets.Finally, we unified all datasets under a consistentformat, incorporating per-turn dialog act and slotannotations. The resulting unified TOD datasetcomprises 3.4 million utterances annotated with18 standardized dialog acts, 524 unique slot labels,and 3,982 unique action labels (dialog act + slots)spanning across 52 different domains (details inAppendix A).",
  "Experimental Setup": "For training D2F we mostly follow the experimen-tal setup of DSE (Zhou et al., 2022) and TOD-BERT (Wu et al., 2020), using BERTbase as thebackbone model for the encoder to report resultsin the main text. Additional configurations are re-ported in the ablation study (Appendix C) whileimplementation details are given in Appendix B.",
  "Baselines": "General sentence embeddings. GloVe: the av-erage of GloVe embeddings (Pennington et al.,2014). BERT: the vanilla BERTbase modelwith mean pooling strategy, corresponding toour untrained encoder. Sentence-BERT: themodel with the best average performance re-ported among all Sentence-BERT pre-trained mod-els, namely the all-mpnet-base-v2 model pre-trained using MPNet (Song et al., 2020) andfurther fine-tuned on a 1 billion sentence pairsdataset. GTR-T5: the Generalizable T5-baseddense Retriever (Ni et al., 2022) pre-trained ona 2 billion web question-answer pairs dataset,outperforming previous sparse and dense retriev-ers on the BEIR benchmark (Thakur et al.,2021). OpenAI: the recently released OpenAIstext-embedding-3-large model (OpenAI, 2024;Neelakantan et al., 2022)Dialog sentence embeddings. TOD-BERT: theTOD-BERT-jnt model reported in Wu et al. (2020)pre-trained to optimize a contrastive response se-lection objective by treating utterances and theirdialog context as positive pairs. The pre-trainingdata is the combination of 9 publicly availabletask-oriented datasets around 1.4 million total ut-terances across 60 domains. DSE: pre-trained onthe same dataset as TOD-BERT, DSE learns sen-tence embeddings by simply taking consecutive utterances of the same dialog as positive pairs forcontrastive learning. DSE has shown to achievebetter representation capability than the other di-alog and general sentence embeddings on TODdownstream tasks (Gung et al., 2023; Zhou et al.,2022). SBD-BERT: the TOD-BERT-SBDMWOZmodel reported in Qiu et al. (2022) in which sen-tences are represented as the mean pooling ofthe tokens that are part of the slots of the utter-ance, as identified by a Slot Boundary Detection(SBD) model trained on the original MultiWOZdataset (Budzianowski et al., 2018). DialogGPT:following TOD-BERT and DSE, we also report re-sults with DialogGPT (Zhang et al., 2020) using themean pooling of its hidden states as the sentencerepresentation. SPACE-2: a dialog representationmodel pre-trained on a corpus of 22.8 million utter-ances, 3.3 million of which are annotated with TODlabels (He et al., 2022). The annotation is used forsupervised contrastive learning and follows a four-layer domainintentslotvalue semantic treestructure.3",
  "Evaluation Data": "Most of the TOD datasets are constructed solelybased on written texts, which may not accuratelyreflect the nuances of real-world spoken conver-sations, potentially leading to a gap between aca-demic research and real-world spoken TOD sce-narios. Therefore, we evaluate our performancenot only on a subset of our unified TOD datasetbut also on SpokenWOZ (Si et al., 2023), the firstlarge-scale human-to-human speech-text datasetfor TOD designed to address this limitation. Moreprecisely, we use the following two evaluation sets: Unified TOD evaluation set: 26,910 utteranceswith 1,794 unique action labels (dialog act + slots)extracted from the training data. These utteranceswere extracted by sampling and removing 15 ut-terances for each action label with more than 100utterances in the training data. SpokenWOZ: 31,303 utterances with 427 uniqueaction labels corresponding to all the 1,710 singledomain conversations in SpokenWOZ. We are onlyusing complete single-domain conversations so thatwe can also use them later to extract the domain-specific workflow for each of the 7 domains inSpokenWOZ.4 3Except DSE and SBD-BERT, models are optimized for dia-log context and may underperform on isolated sentences dueto reliance on dialog-specific features like turns and roles.4There are no single-domain calls for the profile domain",
  "Similarity-based Evaluation": "Before the dialog flow-based evaluation, we assessthe quality of the representation space geometrythrough the similarity of the embeddings represent-ing different actions. We use the following methodsas quality proxies: Anisotropy. Following Jiang et al. (2022); Etha-yarajh (2019), we measure the anisotropy of a setof embeddings as the average cosine (absolute) sim-ilarity among all embeddings in the set.5 Ideally,embeddings of the same action should be simi-lar (high intra-action anisotropy) while being dis-similar to those of other actions (low inter-actionanisotropy). We report the average intra- and inter-action anisotropy across all actions. Similarity-based few-shot classification. Weuse Prototypical Networks (Snell et al., 2017) toperform a similarity-based classification. A pro-totype embedding for each action is calculated byaveraging k of its embeddings (k-shot). All otherembeddings are then classified based on the closestprototype embedding. We report the macro aver-aged F1 score and Accuracy for k = 1 and k = 5(i.e., 1-shot and 5-shot classification). Ranking. For each action, we randomly selectone utterance as the query and retrieve the top-kclosest embeddings, creating a ranking with theiractions. Ideally, the top-k retrieved embeddingsshould predominantly correspond to the same ac-tion as the query, thus ranked first. We report Nor-malized Discounted Cumulative Gain (nDCG@10),averaged over all actions.",
  "Similarity-based Results": "Tables 2 and 3 present the similarity-based classifi-cation and anisotropy results on the unified TODevaluation set and SpokenWOZ, respectively. Re-sults are averaged over 1,794 and 427 differentaction labels for both datasets, respectively. Forclassification results, we report the mean and stan-dard deviation from 10 repetitions, each samplingdifferent embeddings for the 1-shot and 5-shot pro-totypes. All D2F variants consistently outperformthe baselines across all metrics. This is expected,as D2F models, unlike the baselines, are explic-itly trained to learn a representation space whereembeddings are clustered by their correspondingactions. However, the baseline results serve asa proxy for assessing the inherent suitability of",
  "j=i cos(xi, xj) for given {x1, , xn}": "existing sentence embedding models for our ob-jective.6 For instance, as shown in , DSE,which clusters sentences based on conversationalcontext similarity (i.e., how often they appear con-secutively in task-oriented dialogs), outperformsgeneral-purpose embeddings that rely on semanticsimilarity. Notably, D2F embeddings trained withthe proposed soft contrastive loss exhibit superiorperformance compared to D2F-Hard embeddingstrained with the standard supervised contrastiveloss. In , the difference among the vari-ous embeddings narrows, and standard deviationsincrease significantly compared to . Thisindicates that results vary considerably depend-ing on the sampled prototypes, suggesting that theSpokenWOZ data is considerably noisier than theunified TOD evaluation set. This is expected asSpokenWOZ utterances were obtained by an ASRmodel from real-world human-to-human spokenTOD conversations, thus affected by ASR noiseand various linguistic phenomena such as back-channels, disfluencies, and incomplete utterances.7 Classification results provide a local view of therepresentation space quality around the differentsampled prototypes. Actions spread into multiplesub-clusters could still yield good classification re-sults. Thus, we also consider anisotropy resultsfor a more global view of the representation spacequality. Among the baselines, TOD-BERT has thehighest intra-action anisotropy but also the high-est inter-action value, which means that, on av-erage, embeddings of different actions are closerthan embeddings of the same action! (negative values). Sentence-BERT has the lowest inter-action anisotropy, indicating different actions arethe most dissimilar, although embeddings of thesame action are less similar ( = 0.094) comparedto DSE ( = 0.108) in . D2F embeddingsexhibit the best anisotropy values, with a differ-ence between intra- and inter-action embeddings of0.597 and 0.451 in , and 0.193 and 0.103in , for single and joint targets, respectively,roughly doubling their D2F-Hard counterparts. 6Throughout this paper, baseline results are intended toprovide the reader with insights into the potential usability ofavailable sentence embedding models if they were to be usedfor automatic dialog flow extraction, compared to our task-adaptive pre-trained embeddings (Gururangan et al., 2020).7Indeed, SpokenWOZ authors conducted experiments us-ing newly proposed LLMs and dual-modal models, showingthat current models face challenges on this more-realistic spo-ken dataset (Si et al., 2023).",
  "(c) D2Fjoint": ": Spherical Voronoi diagram of embeddings projected onto the unit sphere using UMAP with cosinedistance as the metric. The embeddings represent system utterances from the police domain of the MultiWOZ2.1dataset. Legends indicate the ground-truth action associated to each embedding and the centroids used to generatethe partitions for all the actions in this domain.",
  ": Ranking-based results on the unified TODevaluation set () and SpokenWOZ ()": "dings. For instance, shows the projectionof the embeddings onto the unit sphere for a sub-set of six related actions.8 Sentence-BERT clus-ters embeddings into roughly two main semanticgroups, with price-related actions on top and othersat the bottom. D2F-Hard correctly clusters embed-dings of the same action together while maintain-ing separation among centroids of different actions.However, the arrangement among different clus-ters is better in D2F, guided by action semanticsnamely, all clusters are adjacent, with [requestprice] next to [inform price]; [inform nameprice] between [inform name] and [informprice]; and [inform name price area] be-tween [inform name price] and [inform namearea].Finally, presents the ranking-based re-sults on both evaluation sets. We report the meanand standard deviation from 10 repetitions, eachsampling different query utterances for all actions.We observe a similar pattern across both datasets:an increase in variability and a drop in perfor-mance for all embedding types in SpokenWOZ.However, D2F embeddings still outperform allbaselines and their D2F-Hard counterparts. Fora more qualitative analysis, provides an ex-ample of the rankings obtained for the query \"yourphone please\" with the target action [requestphone_number] on SpokenWOZ. As seen, DSEerrors arise due to embeddings being closer if theycorrespond to consecutive utterances (inform and",
  "The original manifold in which utterances are embeddedcorrespond to the unit hyper-sphere, thus, we believe the unitsphere provides a more truthful visualization than a 2D plane": "request utterances). Sentence-BERT errors oc-cur due to the retrieval of utterances semanticallyrelated to \"number\" and \"phone.\" In contrast, allD2F-retrieved utterances correctly represent differ-ent ways to request a phone number, even thoughhalf were considered incorrect due to the lackof slot name standardization across different do-mains (e.g., phone_number and phone).9 Nonethe-less, for clustering utterances by similarity to ex-tract a dialog flow without annotation, D2F wouldsuccessfully cluster these 10 utterances togetheras they correspond to semantically equivalent ac-tions ([requestphone_number] and [requestphone]).",
  "Dialog Flow Extraction Evaluation": "Dialog flow extraction is an underexplored hard-to-quantify and challenging task with nuances indefinition. However, to evaluate embedding qual-ity, we formally define the problem as follows:Let U and A denote sets of TOD utterances andactions, respectively.Let U and A be sets ofTOD utterances and actions, respectively.Let : U A be a (usually unknown) functionmapping an utterance to its corresponding action.Let di = (u1, , uk) be a dialog with uj U,and ti = ((u1), , (uk)) = (a1, , ak)its conversion to a sequence of actions, referredto as a trajectory.Given a set of m dialogs,D = {d1, , dm}, and after conversion to a setof action trajectories, Dt = {t1, , tm}, the goalis to extract the common dialog flow by combin-ing all the trajectories in Dt. We represent thecommon dialog flow as a weighted actions tran-sition graph (Ferreira, 2023).10 More precisely,the common flow is represented as a weightedgraph GD = A, E, wA, wE where A is the setof actions, E represents edges between actions,the edge weight wE(ai, aj) indicates howoften ai is followed by aj, and the action weightwA(ai) is its normalized frequency.",
  ".-okay okay now please get your number-may i get your phone number-okay may i have your phone number please": "3.-okay may i have your phone number please-okay may i know your telephone number please-okay may i know your telephone number please4.-thank you on the phone number-okay can i please get your id number-may i get your phone number5.-okay may i know your telephone number please-okay may i have your phone name in case for cookingthe table",
  "-um can i please have their phone number": "6.-okay great emma please have your contact number-okay and may i have your number please-okay so may i have the phone number with me7.-my number is 2 10-okay and may i have your number please-okay im i also need phone number 8.-the number is you see-okay and may i have your number please-no problem um but for the information can i haveyour phone number9.-okay and may i have your number please-okay and your car number-thank you on the phone number",
  "D2Fsingle9.68% (+3)4.35% (-1)11.11% (-2)2.04% (+1)5.08% (-3)8.89% (+4)6.86%": "D2Fjoint3.23% (+1)8.70% (-2)5.56% (-1)10.20% (-5)23.73% (-14)0.00% (0)8.57%D2F-Hardsingle12.90% (-4)26.09% (-6)16.67% (-3)10.20% (-5)10.17% (-6)15.56% (+7)15.26%D2F-Hardjoint0.00% (0)8.70% (-2)33.33% (-6)20.41% (-10)25.42% (-15)13.33% (-6)16.87% DSE32.26% (-10) 17.39% (-4)33.33% (-6)30.61% (-15)27.12% (-16)26.67% (-12)27.90%SPACE-232.26% (-10) 30.43% (-7)38.89% (-7)18.37% (-9)32.20% (-19)33.33% (-15)30.91%DialoGPT32.26% (-10) 34.78% (-8)22.22% (-4)44.90% (-22)64.41% (-38)51.11% (-23)41.61%BERT54.84% (-17) 30.43% (-7)22.22% (-4)46.94% (-23)59.32% (-35)42.22% (-19)42.66%OpenAI54.84% (-17) 52.17% (-12) 55.56% (-10) 42.86% (-21)49.15% (-29)44.44% (-20)49.84%Sentence-BERT48.39% (-15) 43.48% (-10) 55.56% (-10) 57.14% (-28)50.85% (-30)55.56% (-25)51.83%GTR-T541.94% (-13) 43.48% (-10) 66.67% (-12) 51.02% (-25)61.02% (-36)53.33% (-24)52.91%SBD-BERT77.42% (-24) 43.48% (-10)38.89% (-7)71.43% (-35)86.44% (-51)86.67% (-39)67.39%TOD-BERT74.19% (-23) 78.26% (-18) 55.56% (-10) 85.71% (-42)83.05% (-49)82.22% (-37)76.50% : Comparison of induced graph size vs. reference graph size for each single-domain in SpokenWOZ,measured by the number of nodes (actions). The table shows the normalized absolute difference (%) and rawdifference in parentheses. Column headers indicate the size of each reference graph (GD). Lower differencessuggest a better match in graph complexity. reference graph GD is built from the trajectoriesDt generated using the ground truth action labelse.g. is indeed Ghospital. In contrast, theinduced graph GD is built without any annotationby clustering all the utterance embeddings in Dand using the cluster ids as action labels to gener-ate the trajectories Dt. That is, for GD, we have(ui) = ai, while for GD, we have (ui) = ciwhere ci is the cluster id assigned to ui. To com-pare the induced and reference graphs, we reportthe difference in the number of nodes between themas the evaluation metric.11 Despite its simplicity,this metric allows us to compare the complexity ofthe induced vs. reference graph in terms of theirsizes (i.e. the number of discovered/extracted ac-tions by each embedding model). Furthermore, toavoid the influence of infrequently occurring ut-terances/actions on graph size, we prune them by",
  "One cluster id ci can correspond to multiple ais and viceversa, preventing a direct comparison between GD and GD": "removing all nodes a with wA(a) < = 0.02(noise threshold).In practice, the total number of actions to clusteris unknown in advance. For instance, a hierarchicalclustering algorithm can be used to approximatethis number (see Appendix F). However, for eval-uation purposes, we set the number of clusters ineach domain to be equal to the ground truth num-ber so that all the embeddings are evaluated underthe same best-case scenario in which this numberis known in advance. Therefore, all the inducedgraphs are built and processed equally, making theinput embeddings the only factor influencing thefinal graph.",
  "Dialog Flow Extraction Results": "shows the results obtained when compar-ing the different extracted graphs. We can see thatgraphs obtained with available sentence embed-ding models tend to underestimate the complexityof each domain, producing less meaningful graphs with fewer states/actions than their references. Wehypothesize this is due to available models group-ing the utterances either by conversational contextor semantic similarity, thus, only allowing us todiscover either semantic or conversational-context\"steps\" (clusters/actions) in the dialogs from eachdomain. For instance, Figure A1 and A2 in Ap-pendix show the extracted graphs Ghospital withSentence-BERT and DSE containing 10 and 6 lessnodes (\"steps\") than the reference graph (),respectively.Among the baseline embeddings, DSE standsout (27.90% average difference across domains),suggesting that conversational-context embeddingsare better at capturing the communicative and infor-mative functions of dialog utterances than semanti-cally meaningful embeddings. Notably, D2F em-beddings trained with the proposed soft contrastiveloss extract graphs closest in complexity to thereferences across domains (6.86% and 8.57% aver-age difference for D2Fsingle and D2Fjoint, respec-tively) compared to both D2F-Hard embeddingstrained with the vanilla supervised contrastive lossand the other embeddings. For instance, shows the corresponding Ghospital obtained withD2Fjoint.12 Finally, it is also worth noting that theD2F graphs are relatively consistent across differ-ent domains, even though some domains had onlya small amount of in-domain data during training.For instance, the hospital and police domainsmake up only 0.11% and 0.07% of the training set(details in Table A1).",
  "Conclusions": "This paper introduced Dialog2Flow (D2F), embed-dings pre-trained for dialog flow extraction group-ing utterances by their communicative and informa-tive functions in a latent space. D2F embeddingswere trained on a comprehensive dataset of twentytask-oriented dialog datasets with standardized ac-tion annotations, released along with this work.Future work will enhance D2F embeddings byexploring larger backbone models and advancedmethods for sentence embeddings (Jiang et al.,2023, 2022). We will also investigate more sophis-ticated techniques for extracting and representingdialog flows, such as using subtask graphs (Sohnet al., 2023) or adapting dependency parsing for",
  "Source code is provided to generate graphs for any givendialogue collection and any embedding model, allowing man-ual assessment of superior D2F graph quality": ": Ghospital graph obtained with D2Fjoint con-taining only one node less than the reference graphin . Node labels correspond to the cluster IDalong a representative utterance (the closest to the clus-ter centroid). Although not the exact same graph as thereference, this graph still allows us to understand thecommon flow of the conversations with a similar degreeof detail: first, the user and system greet each other(U0 and S6), then the user inform the reason of the callrequesting the phone number of a department (U4), theagent may confirm the department (S7) or request moreinformation (S4) before providing the phone number(S2). The user may then either confirm the number (U3)or thank the system (U5). Finally, the system asks ifanything else is required (S5), to which the user mayeither finish the conversation (U6) or, more likely, thankthe system (U2) before the system says goodbye (S0). complex dialog structures (Qiu et al., 2020). Addi-tionally, potential applications include using D2Fembeddings to ground LLMs in domain-specificflows for improved transparency and controllabil-ity (Raghu et al., 2021), and integrating D2F em-beddings into various TOD downstream tasks likedialog state tracking and policy learning.",
  "Limitations": "Our work represents a preliminary exploration witha focus on task-oriented dialogues (TODs) using arelatively simple encoder model. While this workaims to draw attention to this underexplored area,there are a number of limitations that must be ac-knowledged:1. Scope of Dialogues: Our study is restrictedto task-oriented dialogues. Consequently, the find-ings and methods may not generalize well to morecomplex and diverse types of dialogues, particu-larly those of a non-task-oriented nature. Futureresearch should explore these methods in a broaderrange of dialogue types to assess their generaliz-ability.2. Domain Specificity: The model has beentrained on a specific collection of domains, dia-logue acts, and slots. This limits its ability to gen-eralize to unseen domains or dialogues that involvemore complex and varied interactions. Expandingthe range of training data to include a wider vari-ety of domains and dialogue types is necessary toimprove the models robustness and applicability.3. Model Complexity: The encoder model usedin this work is relatively standard. There is poten-tial for improvement by employing larger and moreadvanced models to obtained the final sentence em-beddings.4. Data Size: Despite being the largest datasetwith standardized utterance annotations and thelargest spoken TOD dataset, the datasets used inthis study are limited in size. Larger datasets arenecessary to fully explore and validate the proposedmethods. We encourage the research communityto build upon this work by utilizing more extensivedatasets to enhance the reliability and validity ofthe results. For instance, perhaps named entity tagsmay be used as slots to expand annotation beyondpure task-oriented dialogues.5. Evaluation Metrics: The evaluation met-rics employed in this study, while standard, maynot capture all aspects of performance relevant toreal-world applications. Developing and utilizing abroader set of evaluation metrics would provide amore comprehensive assessment of model perfor-mance. Specifically for dialogue flow evaluation,since there is not a standard metric yet, we encour-age the research community to explore better waysto represent and quantify the quality of dialogueflows.By highlighting these limitations, we hope to",
  "Ethical Considerations": "We are committed to ensuring the ethical use of ourresearch outcomes. To promote transparency andreproducibility, we will release the source code andpre-trained model weights under the MIT license.This allows for wide usage and adaptation whilemaintaining open-source principles.However, to prevent potential license incompat-ibilities among the various task-oriented dialogue(TOD) datasets we have utilized, we will not re-lease our unified TOD dataset directly. Instead, wewill provide a script that can generate the unifieddataset introduced in this paper. This approachallows users to select the specific TOD datasetsthey wish to include, ensuring compliance withindividual dataset licenses.We acknowledge that gender bias present in theoriginal data could be partially encoded in the em-beddings. This may manifest as assumptions aboutthe agents gender, such as the agent being maleor female. We advise users to be aware of thispotential bias and encourage further research tomitigate such issues. Continuous efforts to auditand address biases in data and models are essentialto ensure fair and equitable AI systems. This work was supported by EU Horizon2020 project ELOQUENCE13 (grant number101070558). In addition, this work was inspiredby insights gained from the 2023 Jelinek MemorialSummer Workshop on Speech and Language Tech-nologies (JSALT)14 and was partially supportedwith funds from Johns Hopkins University and EUproject ESPERANTO (grant number 101007666). Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with GPT-4. arXiv preprintarXiv:2303.12712.",
  "Pawe Budzianowski, Tsung-Hsien Wen, Bo-HsiangTseng, Iigo Casanueva, Stefan Ultes, Osman Ra-": "madan, and Milica Gaic. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 50165026, Brussels,Belgium. Association for Computational Linguistics. Bill Byrne, Karthik Krishnamoorthi, ChinnadhuraiSankar, Arvind Neelakantan, Ben Goodrich, DanielDuckworth, Semih Yavuz, Amit Dubey, Kyu-YoungKim, and Andy Cedilnik. 2019. Taskmaster-1: To-ward a realistic and diverse dialog dataset. In Pro-ceedings of the 2019 Conference on Empirical Meth-ods in Natural Language Processing and the 9th In-ternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 45164525,Hong Kong, China. Association for ComputationalLinguistics. Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,Nicole Limtiaco, Rhomni St. John, Noah Constant,Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil.2018.Universal sentence encoder.Preprint,arXiv:1803.11175. Derek Chen, Howard Chen, Yi Yang, Alexander Lin,and Zhou Yu. 2021.Action-based conversationsdataset: A corpus for building more in-depth task-oriented dialogue systems. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 30023017, On-line. Association for Computational Linguistics. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.2024.Benchmarking large language models inretrieval-augmented generation. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1775417762. Ting Chen, Simon Kornblith, Mohammad Norouzi, andGeoffrey Hinton. 2020. A simple framework forcontrastive learning of visual representations. In In-ternational conference on machine learning, pages15971607. PMLR. Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan,and William Yang Wang. 2019. Semantically con-ditioned dialog response generation via hierarchicaldisentangled self-attention. In Proceedings of the57th Annual Meeting of the Association for Computa-tional Linguistics, pages 36963709, Florence, Italy.Association for Computational Linguistics. Zhiyu Chen, Bing Liu, Seungwhan Moon, Chinnad-hurai Sankar, Paul Crook, and William Yang Wang.2022. KETOD: Knowledge-enriched task-orienteddialogue. In Findings of the Association for Compu-tational Linguistics: NAACL 2022, pages 25812593,Seattle, United States. Association for ComputationalLinguistics.",
  "Plappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168": "Alexis Conneau, Douwe Kiela, Holger Schwenk, LocBarrault, and Antoine Bordes. 2017.Supervisedlearning of universal sentence representations fromnatural language inference data. In Proceedings ofthe 2017 Conference on Empirical Methods in Nat-ural Language Processing, pages 670680, Copen-hagen, Denmark. Association for Computational Lin-guistics. Layla El Asri, Hannes Schulz, Shikhar Sharma, JeremieZumer, Justin Harris, Emery Fine, Rahul Mehrotra,and Kaheer Suleman. 2017. Frames: a corpus foradding memory to goal-oriented dialogue systems.In Proceedings of the 18th Annual SIGdial Meetingon Discourse and Dialogue, pages 207219, Saar-brcken, Germany. Association for ComputationalLinguistics. Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, AnujGoyal, Peter Ku, and Dilek Hakkani-Tur. 2020. Mul-tiWOZ 2.1: A consolidated multi-domain dialoguedataset with state corrections and state tracking base-lines. In Proceedings of the Twelfth Language Re-sources and Evaluation Conference, pages 422428,Marseille, France. European Language ResourcesAssociation. Kawin Ethayarajh. 2019. How contextual are contextu-alized word representations? Comparing the geom-etry of BERT, ELMo, and GPT-2 embeddings. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 5565,Hong Kong, China. Association for ComputationalLinguistics. Patrcia Ferreira. 2023. Automatic dialog flow extrac-tion and guidance. In Proceedings of the 17th Confer-ence of the European Chapter of the Association forComputational Linguistics: Student Research Work-shop, pages 112122, Dubrovnik, Croatia. Associa-tion for Computational Linguistics.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "SimCSE: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 68946910, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. James Gung, Raphael Shu, Emily Moeng, Wesley Rose,Salvatore Romeo, Arshit Gupta, Yassine Benajiba,Saab Mansour, and Yi Zhang. 2023. Intent inductionfrom conversations for task-oriented dialogue trackat DSTC 11. In Proceedings of The Eleventh Dia-log System Technology Challenge, pages 242259,Prague, Czech Republic. Association for Computa-tional Linguistics. SuchinGururangan,AnaMarasovic,SwabhaSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,and Noah A. Smith. 2020. Dont stop pretraining:Adapt language models to domains and tasks. InProceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages83428360, Online. Association for ComputationalLinguistics. Amine El Hattami, Issam H. Laradji, Stefania Rai-mondo, David Vzquez, Pau Rodrguez, and Christo-pher Pal. 2023. Workflow discovery from dialoguesin the low data regime. Transactions on MachineLearning Research, 2023. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, andRoss Girshick. 2020. Momentum contrast for un-supervised visual representation learning. In 2020IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 97269735. Wanwei He, Yinpei Dai, Binyuan Hui, Min Yang, ZhengCao, Jianbo Dong, Fei Huang, Luo Si, and YongbinLi. 2022. SPACE-2: Tree-structured semi-supervisedcontrastive pre-training for task-oriented dialog un-derstanding. In Proceedings of the 29th InternationalConference on Computational Linguistics, pages 553569, Gyeongju, Republic of Korea. InternationalCommittee on Computational Linguistics.",
  "Daniel Jurafsky. 2006. Pragmatics and computationallinguistics. The handbook of pragmatics, pages 578604": "Prannay Khosla, Piotr Teterwak, Chen Wang, AaronSarna,YonglongTian,PhillipIsola,AaronMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-pervised contrastive learning. In Advances in NeuralInformation Processing Systems, volume 33, pages1866118673. Curran Associates, Inc. Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, RichardZemel, Raquel Urtasun, Antonio Torralba, and SanjaFidler. 2015. Skip-thought vectors. In Advances inNeural Information Processing Systems, volume 28.Curran Associates, Inc.",
  "Xiujun Li, Sarah Panda, JJ (Jingjing) Liu, and JianfengGao. 2018. Microsoft dialogue challenge: Buildingend-to-end task-completion dialogue systems. In SLT2018": "Zhaojiang Lin, Andrea Madotto, Genta Winata, PengXu, Feijun Jiang, Yuxiang Hu, Chen Shi, and Pas-cale N Fung. 2021. BiToD: A bilingual multi-domaindataset for task-oriented dialogue modeling. In Pro-ceedings of the Neural Information Processing Sys-tems Track on Datasets and Benchmarks, volume 1. Che Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang,and Luo Si. 2021. DialogueCSE: Dialogue-basedcontrastive learning of sentence embeddings. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 23962406, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. In The 36th Conference on Neu-ral Information Processing Systems (NeurIPS). Nikola Mrkic, Diarmuid Saghdha, Tsung-HsienWen, Blaise Thomson, and Steve Young. 2017. Neu-ral belief tracker: Data-driven dialogue state tracking.In Proceedings of the 55th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 17771788, Vancouver, Canada.Association for Computational Linguistics. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-ford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, GustavoHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.Large dual encoders are generalizable retrievers. InProceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pages98449855, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "OpenAI. 2024. New embedding models and api up-dates": "Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. GloVe: Global vectors for wordrepresentation. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 15321543, Doha, Qatar.Association for Computational Linguistics. Denis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor,Yi Zhang, Adel Youssef, and Mona Diab. 2019.Multi-domain goal-oriented dialogues (MultiDoGO):Strategies toward curating and annotating large scaledialogue data. In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 45264536, Hong Kong, China. Associationfor Computational Linguistics. Kun Qian, Satwik Kottur, Ahmad Beirami, ShahinShayandeh, Paul Crook, Alborz Geramifard, ZhouYu, and Chinnadhurai Sankar. 2022. Database searchresults disambiguation for task-oriented dialog sys-tems.In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 11581173, Seattle, United States.Association for Computational Linguistics.",
  "Liang Qiu, Chien-Sheng Wu, Wenhao Liu, and CaimingXiong. 2022. Structure extraction in task-orienteddialogues with slot clustering.arXiv preprintarXiv:2203.00073": "Liang Qiu, Yizhou Zhao, Weiyan Shi, Yuan Liang, FengShi, Tao Yuan, Zhou Yu, and Song-Chun Zhu. 2020.Structured attention for unsupervised dialogue struc-ture induction. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 18891899, Online. As-sociation for Computational Linguistics. Jun Quan, Deyi Xiong, Bonnie Webber, and ChangjianHu. 2019. GECOR: An end-to-end generative el-lipsis and co-reference resolution model for task-oriented dialogue. In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 45474557, Hong Kong, China. Associationfor Computational Linguistics. Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, andMausam. 2021. End-to-end learning of flowchartgrounded task-oriented dialogs. In Proceedings ofthe 2021 Conference on Empirical Methods in Natu-ral Language Processing, pages 43484366, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics.",
  "schema-guided dialogue dataset. In Proceedings ofthe AAAI conference on artificial intelligence, vol-ume 34, pages 86898696": "Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics. Pararth Shah, Dilek Hakkani-Tr, Bing Liu, and GokhanTr. 2018. Bootstrapping a neural conversationalagent with dialogue self-play, crowdsourcing andon-line reinforcement learning. In Proceedings ofthe 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 3 (Indus-try Papers), pages 4151, New Orleans - Louisiana.Association for Computational Linguistics. Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu,Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, FeiHuang, and Yongbin Li. 2023. SpokenWOZ: A large-scale speech-text benchmark for spoken task-orienteddialogue agents. In Thirty-seventh Conference onNeural Information Processing Systems Datasets andBenchmarks Track.",
  "Jake Snell, Kevin Swersky, and Richard Zemel. 2017.Prototypical networks for few-shot learning.Ad-vances in neural information processing systems, 30": "Sungryull Sohn, Yiwei Lyu, Anthony Liu, LajanugenLogeswaran, Dong-Ki Kim, Dongsub Shim, andHonglak Lee. 2023. TOD-Flow: Modeling the struc-ture of task-oriented dialogues. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 33553371, Singapore.Association for Computational Linguistics. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-YanLiu. 2020. Mpnet: masked and permuted pre-trainingfor language understanding. In Proceedings of the34th International Conference on Neural InformationProcessing Systems, NIPS 20, Red Hook, NY, USA.Curran Associates Inc.",
  "of information retrieval models. In Thirty-fifth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track (Round 2)": "Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.Contrastive multiview coding. In Computer VisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part XI 16,pages 776794. Springer. Chien-Sheng Wu, Steven C.H. Hoi, Richard Socher,and Caiming Xiong. 2020. TOD-BERT: Pre-trainednatural language understanding for task-oriented di-alogue. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 917929, Online. Association forComputational Linguistics. Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,Raghav Gupta, Jianguo Zhang, and Jindong Chen.2020.MultiWOZ 2.2 : A dialogue dataset withadditional annotation corrections and state trackingbaselines. In Proceedings of the 2nd Workshop onNatural Language Processing for Conversational AI,pages 109117, Online. Association for Computa-tional Linguistics. Dejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu,Ramesh Nallapati, Andrew O. Arnold, and Bing Xi-ang. 2021. Pairwise supervised contrastive learningof sentence representations. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 57865798, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Dejiao Zhang, Wei Xiao, Henghui Zhu, Xiaofei Ma,and Andrew Arnold. 2022. Virtual augmentationsupported contrastive learning of sentence represen-tations.In Findings of the Association for Com-putational Linguistics: ACL 2022, pages 864876,Dublin, Ireland. Association for Computational Lin-guistics. Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke,Rui Meng, Ye Liu, Zhou Yu, Huan Wang, SilvioSavarese, and Caiming Xiong. 2024. DialogStudio:Towards richest and most diverse unified dataset col-lection for conversational AI. In Findings of the Asso-ciation for Computational Linguistics: EACL 2024,pages 22992315, St. Julians, Malta. Associationfor Computational Linguistics. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,Chris Brockett, Xiang Gao, Jianfeng Gao, JingjingLiu, and Bill Dolan. 2020. DialoGPT : Large-scalegenerative pre-training for conversational responsegeneration. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 270278, Online. As-sociation for Computational Linguistics.",
  "Table A1: Standardized dialog act and domain labels inour unified TOD datasets, ordered by their proportionof utterances": "Our training data is sourced from a diverse rangeof TOD datasets meticulously curated in DialogStu-dio (Zhang et al., 2024). DialogStudio comprisesover 80 dialog datasets, with 30 focusing on task-oriented conversations. We conducted a compre-hensive manual analysis of these 30 TOD datasetsto identify those from which we could extract dia-log act and/or slot annotations. From this analysis,we identified 20 datasets that met our criteria, assummarized in . The datasets in DialogStu-dio are unified under a consistent format whileretaining their original information. However, thisformat only unifies the access to the conversationsper se, omitting annotations and components oftask-oriented dialogs. We then manually inspectedeach dataset to locate and extract the necessary an-notations. This process involved identifying whereand how annotations were stored originally in eachdataset, extracting dialog act and/or slot annota-tions for each turn, either explicitly or implicitlyby keeping track of the changes in the dialog stateannotation from one turn to the next, and standard-izing domain names and dialog act labels acrossdatasets.To standardize dialog act labels, we mapped the44 unique labels found across datasets to 18 nor-malized dialog act labels, informed by the semanticmeaning described in the original dataset papers(mapping detailed in Table A3). After this process,we unified all datasets under a consistent format,detailed in the next subsection, incorporating per-turn dialog act and slot annotations. The resultingunified TOD dataset comprises 3.4 million utter-ances annotated with 18 standardized dialog acts, 524 unique slot labels, and 3,982 unique actionlabels (dialog act + slots). These annotations spanacross 52 different domains, as detailed in .Our unified TOD dataset is a valuable resourceproviding a comprehensive and standardized collec-tion of annotated utterances across diverse domainsunder a common format.",
  "}": "The JSON structure has two main parts: a\"stats\" header and a \"dialogs\" body.The\"stats\" field provides statistics about the labelsand domains in the dataset.The \"dialogs\"field contains dialog IDs, each linked to a listof annotated utterance objects.Each utteranceobject includes its speaker, text, domains, andassociated labels for dialog acts, slots, and in-tents. Dialog act labels contain the original labels(\"original_acts\") as well as their standardizedvalues (\"acts\") and parent values (\"main_acts\")as mapped in Table A3.",
  "Figure A2:Ghospital graph obtained with DSE (12nodes/actions in total). Node labels correspond to thecluster ID along a representative utterance (the closestto the cluster centroid)": "C.For the soft contrastive loss, the semanticsimilarity measure (yi, yj)=yi yj wascomputed using label embeddings y obtainedwith the best-performing pre-trained Sentence-BERT model on semantic search, namely themulti-qa-mpnet-base-dot-v1 model. As shownin Appendix C, we also experimented with theall-mpnet-base-v2 model, which has the best av-erage performance among all pre-trained Sentence-BERT models. The soft label temperature parame-ter was set to = 0.35 after a preliminary studydetermined it to be a reasonable threshold for bothjoint and single training targets (Appendix E).In line with the settings of DSE and TOD-BERT,the learning rates for the contrastive head and theencoder model were set to 3e-4 and 3e-6, respec-tively. The contrastive temperature parameter",
  "* DSE Backbone+0.65+0.011* all-mpnet-base-v2 Label -0.34-0.038+ Self-Supervision-8.06-0.126 Contrastive Head-3.78-0.073": "Table A2: Ablation study results for various D2F con-figurations. Additions, subtractions, and replacementsof components are marked with +, , and * symbols,respectively. Values show the impact on 5-shot classifi-cation F1 score and anisotropy as reported in . was set to 0.05. Models were trained for 15 epochsand then saved for evaluation. The maximum se-quence length for the Transformer encoder wasempirically set to 64 to accommodate at least 99%of the samples, as most TOD utterances are short.Finally, the batch size was set to 64 since we foundthat, contrary to typical self-supervised contrastivelearning, larger batch sizes resulted in lower perfor-mance.16",
  "Label Encoder: Using the Sentence-BERTmodel all-mpnet-base-v2, which has the": "16A grid search with batch sizes 64, 128, 256, and 512was performed, training models for one epoch and evaluatingthe similarity-based 5-shot F1 score on our evaluation set.Larger batch sizes consistently yielded lower scores acrossall models (both standard and soft supervised contrastive lossmodels). For instance, DFDjoint scored 63.23, 61.64, 58.77,and 56.30 for batch sizes 64, 128, 256, and 512, respectively. Figure A3: Change in F1 score (top) and Anisotropy(bottom) with respect to the label temperature (x-axis). The blue and orange curves represent D2Fsingleand D2Fjoint, respectively. Horizontal lines indicatethe performance of their D2F-Hard counterparts usingthe standard hard supervised contrastive loss.",
  "Contrastive Head Removal: Removing thecontrastive head used during training": "The results of these variations are summarized inTable A2. The only configuration that consistentlyimproved performance was the replacement of thebackbone model with the pre-trained DSE model,increasing the F1 score and anisotropy across allvariations.In contrast, adding self-supervision generally de-graded performance, indicating that the additionalDSE self-supervised loss Lself may not comple-ment our targets effectively when trained jointly.Similarly, removing the contrastive head duringtraining resulted in a notable performance drop,highlighting its importance.17",
  "Nk=1 eziz+k /": "Note that the target distribution in Equation 3 treatsall samples with different labels as equally negative,independently of the semantics of the labels. How-ever, we hypothesize that better representations canbe obtained by taking advantage of the semanticsof the labels to model more nuanced relationships.More precisely, let (yi, yj) be a semantic similar-ity measure between both labels, we define a newtarget distribution p(pos=j |xi) (yi, yj) as:",
  "Nk=1 e(yi,yk)/ (4)": "where is the temperature parameter to con-trol how soft/hard the negative labels are (Ap-pendix E).18 Note that unlike Equation 3,19 thisequation allows searching for an encoder that triesto separate anchors and negatives by degrees pro-portional to how semantically similar their labelsare. Therefore, by replacing Equation 4 in Equa-tion 2, our soft contrastive loss is finally definedas:",
  "ESoft Contrastive Loss Temperature": "To understand the benefits of the \"softness\" intro-duced by our proposed contrastive loss comparedto the conventional hard supervised contrastive loss,we conducted a preliminary study examining theimpact of the label temperature parameter . Wetrained models over three epochs, varying the tem-perature across a range of values from 0.05 to1.0 in increments of 0.05. This resulted in 42 dif-ferent model variants: 20 each for D2Fsingle andD2Fjoint, and one for each D2F-Hard counterpart.For each value, we recorded the 5-shot classifi-cation F1 score and anisotropy values as outlinedin . The results are depicted in Figure A3.The plots reveal that as the temperature in-creases from 0, indicating a transition from hardto softer negative labels, both F1 scores and anisotropy values improve beyond those obtainedwith the standard supervised contrastive loss. Forboth D2Fsingle and D2Fjoint models, increasingthe temperature leads to greater separation betweenintra-class and inter-class embeddings, as indicatedby higher anisotropy values.The performance metrics exhibit a steady riseup to a temperature around between 0.35 and 0.4,beyond which anisotropy values begin to plateauand F1 scores become less stable. The advantageof using softer contrast is more pronounced for thejoint target (D2Fjoint, represented by the orangeline), as evidenced by the larger gap between the orange curve and its corresponding horizontal line(D2F-Hardjoint).However, its important to note that theseimprovements diminish with additional trainingepochs. The final difference in performance met-rics between soft and hard labels narrows afterextended training, as reflected in the results re-ported in , where models were trained for15 epochs.",
  "FHow Many Actions to Cluster?": "In practice, determining the optimal number of clus-ters (actions) in dialog flow extraction is challeng-ing because it directly affects the granularity of theextracted flows. Hierarchical clustering algorithms,such as agglomerative clustering, are preferred overcentroid-based methods like k-means because theyprovide a visual representation of the datas hierar-chical structure, which can be examined to decidethe number of clusters or set a distance threshold.Figure A4 illustrates dendrograms obtainedby hierarchically clustering user utterances inthe hospital domain using Sentence-BERTembeddings and D2Fjoint embeddings.Theclustering and plotting were performed us-ing the AgglomerativeClustering class fromscikit-learn, with the number of clusters set to4, represented by different colors.The dendrograms reveal notable differences be-tween the embeddings. The Sentence-BERT den-drogram (left) shows a structure with two main(semantic) groups with low variability in the dis-tances between child and parent nodes, resultingin a more stretched plot. In contrast, the D2Fjointdendrogram (right) displays a clearer separation into four main groups, with larger gaps betweenchild and parent nodes at a certain level of thehierarchy, indicating distinct clusters. D2Fjointembeddings were trained to minimize intra-actiondistances (pushing them towards the bottom of thedendrogram) and maximize inter-action distances(pushing parent nodes towards the top) facilitatingeasier identification of clusters. For instance, inthe D2Fjoint dendrogram, the number of actionscould be estimated to be between 4 and 7, or a dis-tance threshold around 0.4 could be used to formthe clusters.In our experiments (), we used theground truth number of clusters from annotationsto ensure consistency in evaluation across the dif-ferent embeddings. However, agglomerative clus-tering was employed to mimic closer a realisticscenario where the number of actions is not prede-fined.Thus, hierarchical clustering methods provide apractical approach for approximating the number ofactions in practice when such number is unknown.",
  "GDeriving Action Labels from Clusters": "In practice, as illustrated in Figures A1, A2, and 4,actions are identified by cluster IDs after clustering.However, for certain tasks, such as manual analysisof the extracted dialogue flow, a descriptive actionname representing the cluster may be necessary.Following a prompt-based approach similar to thatof Sreedhar et al. (2024) for creating weak intentlabels, we can leverage instruction-tuned LLMs toassign representative labels to each cluster based onits constituent utterances. For instance, using thelatest OpenAI GPT-4 model (gpt-4o) with the fol-lowing prompt, where \"<CLUSTER_UTTERANCES>\"is replaced with the utterances of a given cluster:"
}