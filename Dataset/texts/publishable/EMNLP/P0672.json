{
  "Abstract": "Despite the significant progress of large lan-guage models (LLMs) in various tasks, theyoften produce factual errors due to their lim-ited internal knowledge. Retrieval-AugmentedGeneration (RAG), which enhances LLMs withexternal knowledge sources, offers a promisingsolution. However, these methods can be mis-led by irrelevant paragraphs in retrieved docu-ments. Due to the inherent uncertainty in LLMgeneration, inputting the entire document mayintroduce off-topic information, causing themodel to deviate from the central topic andaffecting the relevance of the generated con-tent. To address these issues, we propose theRetrieve-Plan-Generation (RPG) framework.RPG generates plan tokens to guide subsequentgeneration in the plan stage. In the answerstage, the model selects relevant fine-grainedparagraphs based on the plan and uses them forfurther answer generation. This plan-answerprocess is repeated iteratively until completion,enhancing generation relevance by focusingon specific topics. To implement this frame-work efficiently, we utilize a simple but effec-tive multi-task prompt-tuning method, enablingthe existing LLMs to handle both planning andanswering. We comprehensively compare RPGwith baselines across 5 knowledge-intensivegeneration tasks, demonstrating the effective-ness of our approach. 1",
  "Introduction": "With the persistent scaling up of training parame-ters and datasets (Kaplan et al., 2020), large lan-guage models (LLMs) (Touvron et al., 2023; Jianget al., 2023a; Bai et al., 2023; Achiam et al., 2023)have made remarkable advancements, becomingthe cornerstone of many Natural Language Process-ing (NLP) tasks in recent years. Despite improve-",
  "system and neural cells capable of sensing external stimuli such as lightand touch[...]They likely coordinate these functions through a nerve": "ring [...] Jellyfish are invertebrates [...] Some species have remarkably brief lifespans, lasting merely hours. Yet their vitality is remarkably resilient[...] Most strikingly, jellyfish possess an astounding capacity for regeneration [...] Normal RAG: Despite their seemingly lifeless appearance, jellyfish areamong the most wondrous creatures on Earth! Though lacking a brain,jellyfish possess a rudimentary nervous system and neural[...]Their",
  "bodies also exhibit a peculiar self-healing ability, rapidly regenerating even when cut into pieces and reassembled. Furthermore, jellyfish can bioluminescent, illuminating the dark [...]": "Plan Before Generate: [Plan: Body composition and structure of jellyfish] <p>JellyfishTheir bodies are almost entirely made up ofgelatinous hydrogel tissue, with a water content of more than98%...</p>Jellyfish are indeed very simple creatures, almost more than98% of which are made up of water. [Plan: Nervous system of jellyfish] <p>Jellyfish barely have nervous systemsThey have primitive nervecells that help them</p>However, although jellyfish do not have brains,they do have a rough nervous system that can sense external stimulisuchaslightandtouch[...][Plan: Nerve ring of jellyfish]",
  "<p>..</p>Jellyfish coordinate through nerve rings [...]": ": The retrieval documents contain off-topicparagraphs (highlighted in yellow), causing potential de-viations in RAG outputs. By planning first (highlightedin green), selecting relevant fine-grained paragraphs,and then answering, the plan-answer iteration ensures amore consistent and relevant generation. ments in model architecture and the expansion oftraining data, LLMs still struggle with factual er-rors (Lyu et al., 2022; He et al., 2022). To addressthis issue, the Retrieval-Augmented Generation(RAG) system has been introduced (Lewis et al.,2020; Guu et al., 2020). By retrieving external in-formation and incorporating it into the input, theRAG system demonstrates excellent performancein knowledge-intensive tasks. The most common approach in RAG involvesusing the user input as a query for a single-timeretrieval (Lewis et al., 2020), with LLMs then gen-erating answers based on the retrieved information.However, documents retrieved for input into theLLM are often lengthy, and not all paragraphs may be practically helpful for answering the question.Recent research (Lan and Jiang, 2021; Sun et al.,2023) indicates that off-topic paragraphs can bedetrimental to the generation. As illus-trates, due to the inherent uncertainty in the genera-tion process of LLMs, inputting the entire retrieveddocument can lead to those off-topic paragraphsmisleading the model, causing a shift in focus andresulting in content that gradually deviates fromthe main topic.Currently, many researchers have acknowledgedthis issue and have adopted various solutions.Some works (Jiang et al., 2023b; Asai et al., 2023)determine whether retrieval is necessary before gen-erating an answer and input the retrieved documentonly when required. Self-RAG (Asai et al., 2023)further introduces reflection tokens to evaluate thequality of retrieved documents, thereby excludingirrelevant documents. Despite significant advance-ments with these methods, their effectiveness di-minishes when dealing with longer retrieved texts,particularly those that are generally relevant butcontain some irrelevant details. Additionally, whenthe retrieved documents are too lengthy, it becomeschallenging for users to verify the correctness ofspecific details in the generated content.We propose that the susceptibility of LLMs toirrelevant content stems from a lack of explicitpre-planning in generating subsequent content. Asillustrated in , if the model continuouslyplans the next topic at each step and only focuses onhighly relevant paragraphs, it can avoid being mis-led by irrelevant material during lengthy generationprocesses. To implement this plan-answer process,we introduce the Retrieve-Plan-Generation (RPG)framework. RPG iterates through two stages: theplan stage and the answer stage. In the plan stage,the model generates tokens representing upcomingtext topics. During the answer stage, the modelselects highly on-topic paragraphs from retrieveddocuments based on these topics, and uses themto generate targeted answers. This iterative pro-cess between planning and answering continuesuntil the generation is complete. Unlike traditionalfull-text input methods, RPG provides detailed con-trol over content generation by focusing on spe-cific topics at each step, ensuring the generationis highly relevant and accurate. Additionally, thisfine-grained approach makes it easier for users toverify the correctness of answer details, even whendealing with long documents.Existing LLMs struggle to effectively integrate both planning and answering capabilities. Since theplan must be incrementally developed during thegeneration process, relying solely on pre-designedprompts for plan generation is challenging. Addi-tionally, prompts need to guide the model in gen-erating both the plan and the answer based on gen-erated context and relevant paragraphs, which im-poses high demands on the models ability to com-prehend complex prompts. Therefore, we promptChatGPT to create supervision for plan generationand fine-grained paragraph utilization based on ex-isting datasets (Asai et al., 2023; Yang et al., 2018),then train our model end-to-end on this dataset.Fully fine-tuning an LLM is resource-intensiveand often unnecessary. To balance the learningcapabilities of the LLMs with training efficiency,prompt tuning has emerged as a promising method.Given that the input and output formats for plan-ning and answering tasks differ, we adopt a multi-task prompt tuning approach, training two learnableprompt tokens specifically for plan and answer gen-eration. These two prompt tokens share the samesoft prompt. During the training stage, each data issimultaneously used for both planning and answer-ing tasks. To train task-specific prompts, we firsttransform the soft prompt to the corresponding taskmode, and then exclude the impact of other partsduring loss computation.Empirical results on 5 tasks, including long-form,multi-hop,and short-form generation,demonstrate that RPG significantly outperformsinstruction-tuned LLMs with more parameters andwidely adopted RAG approaches. Technical contri-butions of this paper can be summarized as follows:",
  "Ends when <EOS> generated": ": Illustration of the proposed RPG. The left shows the training process, where plan and answer tasks use thesame example data, different loss functions, and train two task-specific prompts simultaneously. The right showsthe inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrievingrelevant passages, thereby improving both the qual-ity and accuracy of generated content, particularlyin knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorpo-rating a fixed number of retrieved passages with aquery into LLMs to generate a response. Recentresearch indicates that adaptive retrieval, tailoredto the demands of LLMs, can further enhance gen-eration. FLARE (Jiang et al., 2023b) uses the gen-erated sentence with a low confidence score as thequery to retrieve external knowledge adaptively andthen regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokensallowing the model to adaptively retrieve and reflectthe quality of generated content. SuRe (Kim et al.,2024) generates conditional summarizations of re-trieval and evaluating them with carefully designedprompts. However, existing approaches may nottake full advantage of the planning capabilities ofLLMs. Additionally, these methods may struggleto extract relevant content from retrieved passagesand are easily influenced by irrelevant information.",
  "Parameter-Efficient Fine-Tuning. Despite thepowerful generative capabilities of LLMs, fine-": "tuning them requires substantial computational re-sources (Lester et al., 2021; Ding et al., 2022; Liuet al., 2023). To achieve more efficient fine-tuning,parameter-efficient tuning methods have emerged.These methods either fine-tune a small portion ofthe model parameters or introduce additional learn-able parameters without fine-tuning the model it-self (Hu et al., 2021; Liu et al., 2021; Ding et al.,2022; Wang et al., 2023). LoRA (Low-Rank Adap-tation) (Hu et al., 2021) reduces the number of pa-rameters to be updated by decomposing the weightmatrices into low-rank components. Prompt tun-ing (Liu et al., 2021, 2023) introduces task-specificprompts by concatenating learnable tokens beforethe input sequence, requiring minimal parameterupdates. Multi-task Prompt Tuning (MPT) (Wanget al., 2023) further highlights the commonalitiesbetween multi-task learning, suggesting that usinga shared soft prompt and task-specific low-rankmatrices can yield better results.",
  "Task Definition & Notation": "Given a user input x, a retriever R and documentcorpus D = {d1, d2, . . . , dn}, RAG aims to en-hance the quality of a language models (LMs)output y by retrieving relevant passages from Dand incorporating them into the answer. For a queryq, the retriever R can retrieve a list of documentsDq = R(q, D) from corpus D.Vanilla Retrieval Augmented Generation. Themost common approach is to use the user input xdirectly as the query for retrieval, and then gen-erate the complete answer in a single step y =LM([Dx, x]).Dynamic Retrieval Generation. To aid long-formgeneration with retrieval, dynamic retrieval gen-eration further refines the RAG approach by dy-namically retrieving information according to themodels needs during the generation process. Al-though dynamic retrieval can reduce the factualerrors of LM, the lack of explicit planning maylead to interference from irrelevant information, re-sulting in the focus shift phenomenon. Based onthis fundamental structure, this paper innovativelyproposes a two-stage method using the distinct planand answer stage to achieve generated content withreduced focus shift.",
  "Method Overview": "To enhance the factuality of LLMs and improvetopic consistency in long-form generation, LLMsshould be capable of generating a preliminary planto select fine-grained evidence, guiding subsequentcontent generation on specific topics. Based on thisconsideration, our RPG framework is designed intotwo stages: plan and answer. During the plan stage,the LLM should generate a topic for the upcominganswer, reflecting pre-planned thoughts and guid-ing the subsequent generation. This approach effec-tively prevents the output from deviating from thespecific topic. In the answer stage, by removing ir-relevant information at the sentence level, a founda-tional denoising capability is achieved. This decou-ples the processes of filtering and utilizing relevantinformation during the generation, thereby enhanc-ing the models ability to leverage fine-grained rel-evant evidence. Through the iterative alternation ofthese two stages, the focus shift phenomenon dur-ing long text generation can be effectively avoided.Specifically, to train an LLM end-to-end with bothplanning and fine-grained evidence utilization ca-",
  "Dataset Construction": "To train the aforementioned LLM, we reconstructa fine-grained dataset based on the existing Self-RAG (Asai et al., 2023) and HotpotQA (Yang et al.,2018) datasets, where the annotated data has beensplit into segments with retrieved documents.Data collection for plan. Since answer segmentsare specific implementations of an individualsplanning at each step, we can treat the intent ofthese segments as human planning, thereby avoid-ing topic deviation. As shown in , to obtainthe intent of each segment, we prompt ChatGPT tosummarize the segment and use these summariesas labels for the plan stage. For data that do notrequire additional retrieved information, we attach<not_need_extra_info> directly at the start ofthe answer, indicating no planning is needed andthe LLMs inherent ability to answer is sufficient.Data collection for answer. As mentioned before,the coarse-grained documents provided in existingdatasets often contain off-topic paragraphs, whichhas been shown to be adverse to generation (Yoranet al., 2023). After filtering the paragraphs at thesentence level, we retain only the information re-lated to the plan tokens and the corresponding an-swer segment for the answer stage training. Specifi-cally, we provide ChatGPT with pre-generated plantokens, along with corresponding coarse-grained documents and the answer segment. We then re-quire ChatGPT to select sentences related to theplan and answer from the document as fine-grainedevidence, which is further used to train the LLMsability of fine-grained evidence utilization. The an-swer segments are the labels for the answer stage.Finally, we collect 50k supervised training datato form a new dataset for RPG training. Moredetails about our dataset are shown in Appendix B.Prompts and examples are shown in Appendix E.",
  "RPG Training": "To efficiently leverage the information within thedata, we introduce a multi-task training method forthe RPG framework. During the training phase,we utilize different components of the samples,plan and answer, from the constructed dataset totrain the model. Simultaneously, we train two task-specific learnable prompts with different loss func-tions. This approach enables the frozen LLM toacquire planning and answering capabilities with-out requiring any modifications to the model itself.As shown in , to achieve more parameter-efficient fine-tuning, we opt to freeze the LLMand train the additional continuous prompt vectorsprepended to the input. Recent research (Wanget al., 2023) indicates that commonalities existacross various tasks, paving the way for more ef-ficient prompt tuning. Following them, we firstemploy a soft prompt P as the shared promptacross plan and answer tasks. To adapt to the dis-tinct requirements of these two tasks, we furtherutilize two different low-rank matrices, Wplan andWans, to transform the soft prompt to the specifictask mode. The task prompts for plan and answergeneration task are parameterized as follows: Ptask = P Wtask = P (utask vTtask),(1)where denotes the Hadamard product betweentwo matrices, and task {plan, ans} denotes thespecific generation task.To enhance the efficiency of multi-task training,we utilize different components of the samples to si-multaneously train the plan prompt and the answerprompt. Specifically, we adopt to mask differentparts of the same data instance to guide the learningof corresponding tasks. For the plan stage train-ing, tokens other than the plan tokens in the groundtruth are masked, guiding the LLM to focus solelyon plan generation. Similarly, for the answer stage,tokens that are not part of the answer are excluded",
  ": return y": "and Algorithm 1 presents an overviewof RPG at inference. During the inference phase,the RPG framework enhances response qualityby iteratively invoking the plan-answer capabil-ity. This approach not only provides additionalknowledge to the LLM but also ensures topic con-sistency. To reduce costs, the bge-reranker (Xiaoet al., 2023) is employed instead of ChatGPT toselect fine-grained on-topic paragraphs during theinference phase.Specifically, for every user input x and retrievedpassages Dx, the LLM M first determines whetheradditional information is needed. If M generates <not_need_extra_info>, the LLM M predictsthe output y directly using prompt Pans and inputx. In other cases, relevant information about plantokens P in retrieved passages is selected as fine-grained paragraphs to supplement M with externalknowledge. Furthermore, the LLM M, using an-swer prompt Pans, then incorporates fine-grainedparagraphs into the generation of the next outputsegment yt. This segment yt is subsequently ap-pended to y. The plan-answer process is repeateduntil the <EOS> token is generated, at which pointy is output as the final answer.",
  "Tasks and Datasets": "Long-form generation tasks.The long-formQA tasks aim to generate comprehensive answersto questions seeking complex information, whichis a primary application scenario for our model.And evaluations of these tasks can serve as evi-dence to the frameworks capability of generatingon-topic and comprehensive answers. We utilizeASQA (Stelmakh et al., 2022) and ELI5 (Fan et al.,2019) as our testbed, where inputs are ambigu-ous questions with multiple interpretations, andoutputs are expected to address them comprehen-sively. Following Self-RAG (Asai et al., 2023) andALCE (Gao et al., 2023), we use ROUGE- L (Lin,2004) and MAUVE (Pillutla et al., 2021) for cor-rectness and fluency evaluations. Multi-hop generation tasks.A multi-hop QAtask aims to test reasoning and inference skills byrequiring a model to read multiple paragraphs andanswer a given question. We use the 2WikiMulti-HopQA (Ho et al., 2020) dataset and adopt the F1score as the metric. Short-form generation tasks.The short-formQA tasks aim to generate precise answers forusers, which evaluate the models ability to effec-tively leverage retrieved information to responseprecisely. We use two open-domain QA datasets,",
  "PopQA (Mallen et al., 2022) and PubHealth (Zhanget al., 2023), where models need to answer arbitraryquestions about factual knowledge. We processthese two datasets following (Asai et al., 2023)": "4.1.2BaselinesOur training dataset is derived from Self-RAG andHotpotQA, where each sample is divided into plan-ning and answering segments using ChatGPT. Toensure a fair comparison, we select baseline modelsthat are fundamentally consistent with Self-RAGand categorize them into three major groups. Baselines without retrieval.To explore the spe-cific impact of external knowledge on model per-formance, several retrieval-free baselines are es-tablished. We evaluate the open-source modelsLlama27B, 13B and Alpaca7B, 13B (Touvron et al.,2023), which have shown outstanding performanceon various tasks. Baselines with retrieval.We further set up base-line models with retrieval, covering the standardRAG systems. The standard RAG generates con-tent by merging the query and retrieved documentsinto the input. We also compare the full-parameterfine-tuned version of Llama2-FT based on Self-RAG train data. Additionally, We included baselinemethods related to knowledge extraction, such asSuRe (Kim et al., 2024) and RECOMP (Xu et al.,2023). These methods help LLMs make more accu-rate predictions by summarizing and extracting keyinformation from retrieved text paragraphs once.However, they do not incorporate an iterative re-finement process to fully utilize the knowledge.And we evaluate the Self-RAG model, which en-hances the standard RAG by introducing dynamicretrieval and reflection tokens. ChatGPT-Based baselines.Lastly, we conducta comparison with the SOTA in the field of LLMs:ChatGPT and Ret-ChatGPT (ChatGPT with re-trieval passage). As a leading LLM, ChatGPThas demonstrated exceptional performance acrossmultiple domains, providing a strong comparativebenchmark for our model. 4.1.3Implementation DetailsTraining.As mentioned before, our training datais reconstructed from the Self-RAG dataset. Weadopt Llama27B as our foundational LLM, and usethe prompt tuning implementation of the Hugging-face PEFT (Mangrulkar et al., 2022) library tofine-tune LLama27B on 4 Nvidia A6000 GPUs.",
  ": Experimental results on long-form tasks. Bold numbers indicate the best performance, except for ChatGPT": "Inference.During inference, the plan and answerstages alternate, using a simple greedy decodingstrategy. The plan phase has a token limit of 30,and the answering phase is 100. For short-formQA, the model only completes one plan-answercycle. For long-form and multi-hop QA tasks, themodel alternates between planning and answeringuntil it generates a termination symbol or reachesthe operation limit (3 in this paper). In multi-hopQA, a special \"[Combine]\" symbol indicates thatthe model will summarize the previous content toproduce a concise answer. For the retriever model,we use Contriever-MS MARCO for PopQA, Pub-Health, and ASQA datasets, and BM25 for the2WikiMultiHop datasets, aligning with all base-lines.",
  "Experiment Results": "Long-form generation.Our model demonstratesbrilliant performance in the domain of long-formgeneration, which is the primary application sce-nario for our model. As displayed, the ex-perimental results demonstrate that our model hasachieved a significant improvement in long-formgeneration performance with only a slight tuning of 0.3 billion parameters. Notably, our model out-performs the prior SOTA Self-RAG. Compared tothe knowledge-extraction baseline, both SuRe andRECOMP-abstractive extract key information bycompressing and rewriting the retrieved contentonce. In contrast, our method is iterative, extract-ing key information based on explicit planning untilthe content is sufficient. RPG outperforms thesemethods, underscoring the significance of iterativeplanning. Single-step extraction and answering canoverlook intermediate information, while iterativeplanning with on-demand retrieval is more effec-tive. Even when compared to the current SOTAmodel, ChatGPT with retrieved knowledge, ourmodel achieves comparable results. Similar find-ings are also observed in the ELI5 dataset. These results underscore our models strong ca-pabilities in long-form generation tasks, demon-strating the comprehensiveness and relevance ofour models responses. The iterative alternationbetween the planning and answering phases en-sures that the generated text remains on-topic. Ourapproach not only enhances fluency but also main-tains factual accuracy, further highlighting the su-periority of our method.",
  ": Experimental results on Multi-Hop and Short-form generation tasks. Bold numbers indicate the bestperformance, except for ChatGPT": "Multi-hop generation.For multi-hop genera-tion tasks, the model needs to integrate all gen-erated information to provide a concise answer.Experimental results in indicate that ourRPG framework outperforms other Llama-basedbaseline models, demonstrating the benefit of pre-planning and utilizing fine-grained evidence forreasoning. While the GPT-based SuRe (Kim et al.,2024) model performs better than ours, the Llama-based SuRe model performs poorly due to its de-pendence on rewriting retrieved content, a pro-cess reliant on LLMs capabilities. In contrast,our model avoids this rewriting process and stillachieves good performance on multi-hop datasets. Short-form generation.Although short-formgeneration is not the primary application scenarioof our model, we still demonstrate its performancein this context to prove its versatility and applicabil-ity. In some short-form generation tasks, especiallyon the Pub dataset, we find that retrieved content isnot always effective. In fact, retrieval-augmentedChatGPT often underperforms compared to its non- retrieval-augmented counterpart due to the incorpo-ration of irrelevant information. By focusing on therelevance of retrieved content and excluding irrele-vant details, our model shows progress in variousshort-form generation tasks.",
  "As shown in , we conduct a comprehensiveablation study on the RPG framework to clarifywhich factors play a decisive role in the trainingand inference processes": "Training Phase.We investigate the impact of re-moving the planning phase on model performance.By eliminating all plan texts from the trainingdataset and using prompt tuning to train the modelwith only answer texts, we observe a significantdrop in performance across all three tasks. In long-form generation, the absence of planning causedthe model to deviate from the topic. In short-formgeneration, unscreened retrieved texts were not al-ways beneficial. Thus, the planning phase is crucialfor maintaining the relevance of generated content.",
  ": Ablations in training and inference": "Furthermore, we investigate the differences be-tween fine-tuning a model with uniform learnableprompt tokens for both plans and answers versususing distinct tokens for each. Results show thatuniform tokens diminished performance in bothlong-term and short-term generation tasks, sug-gesting that planning and answering function asseparate tasks. Thus, it is more appropriate to usemulti-task learning to train LLMs for both planningand answering capabilities.Additionally, we study the models performancewith varying scales of training datasets as displayed. The results show that performancegradually improves as the dataset size increases.We believe further expanding the training data willcontinue to enhance the models performance. Inference Phase.In the inference phase, we as-sess the impact of retrieval on model performance.Results show that retrieval is crucial for long-formgeneration tasks, which require comprehensiveanswers. Without retrieval, generating completeanswers is significantly more challenging. Con-versely, for short-term generation tasks, retrievalhas a minor impact, since these tasks may typicallydo not require extensive knowledge.Additionally, we examine the effects on modelperformance when using retrieved passages di-rectly. The results show a significant decline inperformance across all tasks, highlighting the detri-mental impact of off-topic paragraphs on the qual-ity of generated outputs.Eventually, we compared global planning andlocal planning methods. Since RPG involves iter-atively generated local plans, we employed GPT-3.5 model to globally annotate all plans for thequestion. The results in the table shows iterativeplanning (RPG) outperforms global planning (Plan 10k30k50k",
  "Conclusion": "In this paper,we propose a Retrieve-Plan-Generation (RPG) framework, which integratesan explicit plan stage into the lengthy generationprocess. By generating plan tokens, the model isguided to selectively utilize retrieved paragraphs.The iterative alternation between plan and answerstages ensures that the generated content remainsrelevant to the topic. To implement this framework,we adopt an efficient multi-task fine-tuning methodthat equips existing models with both planningand answering capabilities. Experimental resultsdemonstrate that RPG outperforms state-of-the-artmodels across five tasks, validating the effective-ness of our approach.",
  "Limitations": "Due to computational resource constraints, we onlypresent the specific implementation of the RPGframework under the Llama27B, without explor-ing further experiments on larger models, suchas Llama213B, Llama270B. Additionally, due tothe API costs associated with accessing ChatGPT,we conducted experiments solely on a 50k recon-structed dataset, without collecting and analyzingmore extensive data to provide more experimentalresults on larger datasets.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen,Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li,and Yanghua Xiao. 2023. Hallucination detection:Robustly discerning reliable answers in large lan-guage models. In Proceedings of the 32nd ACMInternational Conference on Information and Knowl-edge Management, pages 245255. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-han Yang, Yusheng Su, Shengding Hu, Yulin Chen,Chi-Min Chan, Weize Chen, et al. 2022. Delta tuning:A comprehensive study of parameter efficient meth-ods for pre-trained language models. arXiv preprintarXiv:2203.06904.",
  "Shahul Es, Jithin James, Luis Espinosa-Anke, andSteven Schockaert. 2023. Ragas: Automated eval-uation of retrieval augmented generation.arXivpreprint arXiv:2309.15217": "Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. 2019. ELI5:Long form question answering. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 35583567, Florence,Italy. Association for Computational Linguistics. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.2023. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64656488, Singapore. Associa-tion for Computational Linguistics.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021. Unsupervised dense in-formation retrieval with contrastive learning. arXivpreprint arXiv:2112.09118. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023a. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Zhengbao Jiang, Frank F Xu, Luyu Gao, ZhiqingSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,Jamie Callan, and Graham Neubig. 2023b.Ac-tive retrieval augmented generation. arXiv preprintarXiv:2305.06983": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv:2001.08361. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, JongjinPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,and Jinwoo Shin. 2024.Sure: Summarizing re-trievals using answer candidates for open-domainqa of llms. arXiv preprint arXiv:2404.13081. Yunshi Lan and Jing Jiang. 2021. Modeling transitionsof focal entities for conversational knowledge basequestion answering. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 32883297.",
  "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gptunderstands, too. AI Open": "Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong,Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu,Tong Xu, and Enhong Chen. 2024.Crud-rag:A comprehensive chinese benchmark for retrieval-augmented generation of large language models.arXiv preprint arXiv:2401.17043. Yuanjie Lyu, Chen Zhu, Tong Xu, Zikai Yin, and En-hong Chen. 2022. Faithful abstractive summarizationvia fact-aware consistency-constrained transformer.In Proceedings of the 31st ACM International Con-ference on Information & Knowledge Management,pages 14101419. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2022.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. arXiv preprint arXiv:2212.10511.",
  "Sourab Mangrulkar, Sylvain Gugger, Lysandre De-but, Younes Belkada, Sayak Paul, and BenjaminBossan. 2022.Peft: State-of-the-art parameter-efficient fine-tuning methods": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernndez brego, Ji Ma, Vincent Y Zhao,Yi Luan, Keith B Hall, Ming-Wei Chang, et al.2021. Large dual encoders are generalizable retriev-ers. arXiv preprint arXiv:2112.07899. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. Mauve: Measuring the gap be-tween neural text and human text using divergencefrontiers. Advances in Neural Information Process-ing Systems, 34:48164828.",
  "Xinyue Shen, Zeyuan Chen, Michael Backes, and YangZhang. 2023.In chatgpt we trust?measuringand characterizing the reliability of chatgpt. arXivpreprint arXiv:2304.08979": "Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid questions meetlong-form answers. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 82738288, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics. Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xi-aochi Wei, Shuaiqiang Wang, Yan Zhang, and DaweiYin. 2023. Towards verifiable text generation withevolving memory and self-reflection. arXiv preprintarXiv:2312.09075.",
  "AMore PRG Implementation Details": "TrainingAs previously mentioned, our trainingdata is structured based on the Self-RAG dataset.During the training phase, we utilize theLlama27B as our foundational language model. Forthe retriever model, we have selected the readilyaccessible Contriever-MS MARCO for the PopQA,Pub, and ASQA datasets, and the BM25 algorithmfor the 2Wiki datasets, aligning with the baselines InferenceDuring the inference process, the plan-ning and answering stages alternate, and we haveemployed a simple greedy decoding strategy forboth. In the planning phase, we set a maximumtoken generation limit of 30, while in the answerphase, it is 100. As for the retrieved documents,by default, we use the top five documents rankedby Contriever-MS MARCO (Izacard et al., 2021);For ASQA, we utilized the top five documents se-lected by the authors from GTR-XXL (Ni et al.,2021), which is done to ensure a fair comparisonamong all baseline models. In short-form QA, themodel executes a single planning and answering cy-cle. Conversely, in long-form QA tasks, the modelalternates between planning and answering multi-ple times until it generates a termination symbolor reaches the limit of operations(3 in this paper).Multi-hop QA follows a similar approach to long-form QA. However, there is a minor difference: asthe generation process nears completion, our modelgenerates a special \"[Combine]\" symbol. This indi-cates that the model will then summarize the previ-ously generated content and ultimately produce aconcise answer to the original question.",
  "CAdditional Analysis for the PlanModule": "As a crucial component of the RPG framework,the Plan module plays an essential role in guidingmodel generation and extracting fine-grained evi-dence. We further discuss the impact of the Planmodule under different settings on model genera-tion, providing intuitive explanations and analysis using examples.Plan globally vs locally. Plan Tokens can begenerated both globally and locally. We use Chat-GPT to create an initial global plan. Our work,however, emphasizes an iterative plan-answer pro-cess, viewed as a local plan. A case analysis in shows that RPG produces more compre-hensive plans, while ChatGPT focuses on a singlequestion aspect, leading to less detailed responses,which also demonstrates the importance of the iter-ative process.More plans, better generation? Although theplan can enhance the models understanding ofquestions, allowing it to focus on relevant fine-grained information and improve generation qual-ity, an excess of planning can lead to a declinein results. For better illustration, we forced RPGto generate one more plan, a corresponding exam-ple is shown in . It is clear that forcingthe model to continue generating when it has al-ready produced sufficient content results in repet-itive plans and answers, leading to a decline ingeneration quality.",
  "DSubjective Evaluation of Generation": "We further explore the subjective evaluation resultsof the content generated by the RPG method. Giventhat existing evaluation metrics are already suffi-cient for short-form and multi-hop QA tasks, wefocus primarily on the subjective assessment forlong-form QA tasks.To save costs, we adopted an evaluation methodconsistent with SelfRAG. For the ASQA dataset,we extracted 50 different questions and responsesfrom two different models, and evaluated them us-ing two human annotators and GPT-4o from thefollowing two aspects:",
  "EPrompts for Dataset construction andExamples": "In this section, we provide a detailed explanationof the construction methods for each dataset. Wefirst introduce the instructions used for dataset con-struction and then provide corresponding examplesfor each dataset.To construct our own dataset,we utilizegpt-3.5-turbo-0125 to generate comprehensiveannotations leveraging existing datasets and few-shot examples. Given the straightforward natureof the short-form questions, we prompt ChatGPTto summarize their statements as the Plan, whichis outlined in . We apply this methodto Natural Questions, FEVER, OpenBoookQA,and Arc-Easy. ASQA consists of numerous am-biguous questions, where each problem withinthe annotated dataset is further divided into mul-tiple sub-problems post artificial disambiguation.These segments address specific parts of the ques-tion, and due to their close resemblance, ChatGPT may generate very similar topics based on answersummaries. ChatGPT should identify which sub-problems the current answer corresponds to, andthen summarize these sub-problems into a state-ment. The process is guided by prompts detailed in. As ShareGPT does not encounter manyambiguous questions, for each part of the answer,we directly prompt ChatGPT to summarize the cur-rent segments topic based on the provided answercontext as the label for the Plan Generation. De-tailed information is provided in . ForHotpotQA, since there is sufficient evidence in theannotated data and the question only needs twojumps at most, we believe that ChatGPT is suffi-cient to give good planning based on the questionand answer. The instructions are shown in .Prompts used for fine-grained evidence selectionare shown in . Examples of our datasetcan be found in , , and .",
  ": Subjective Evaluation Results": "Question: Which is the most popular sport in Australia?PlangloballywithChatGPT:<plan_start>IdentifypopularsportsinAustralia<plan_end>Fine-grained evidence<answer_start>Australian Rules Football (AFL) is considered the most popular sportin Australia,<answer_end><plan_start>Participation rates and viewership<plan_end>Fine-grained evi-dence<answer_start>with the highest participation rates and viewership, particularly in Victoria, South Australia,Western Australia, and Tasmania.<answer_end>Ours: There are a lot of popular sports in Australia.<plan_start>most popular sport in Australia<plan_end>Fine-grained evidence<answer_start>As a whole, the most popular sport to play in Australia is Australian Rules Foot-ball.<answer_end><plan_start>Most popular spectator sport in different regions of Australia<plan_end>Fine-grained evidence<answer_start>Historically, rugby league and rugby union football codes have been morepopular than Australian rules football in New South Wales and Queensland, whereas Australian rules foot-ball has been more popular in Victoria, Tasmania, South Australia, North Territory and Western Aus-tralia.<answer_end><plan_start>Most popular spectator sport in different genders of Australia<plan_end>Fine-grained evidence<answer_start>For adult women in Australia, the number one sport activity they participatein is walking with 30% having done this in 2009 and 2010.For men, the most popular sport activity was alsowalking with a participation rate of 15.6%.<answer_end>",
  ": An example for global/local plan": "Question: When was the first Toyota Land Cruiser made?Ours: <plan_start>toyota land cruiser<plan_end>Fine-grained evidence<answer_start>The Land Cruiseris a series of four-wheel drive vehicles produced by the Japanese automobile manufacturer Toy-ota.<answer_end><plan_start>first Toyota Land Cruiser made in Japan<plan_end>Fine-grained evi-dence<answer_start>The first Toyota Land Cruiser was made in Japan in 1951. <answer_end>Forced continuation: <plan_start>toyota land cruiser<plan_end>Fine-grained evidence<answer_start>TheLand Cruiser is a series of four-wheel drive vehicles produced by the Japanese automobile manufac-turer Toyota.<answer_end><plan_start>first Toyota Land Cruiser made in Japan<plan_end>Fine-grained evi-dence<answer_start>The first Toyota Land Cruiser was made in Japan in 1951. <answer_end><plan_start>firstToyota Land Cruiser made in Japan<plan_end>Fine-grained evidence<answer_start>The Land Cruiser is a seriesof four-wheel drive vehicles produced by the Japanese automobile manufacturer Toyota.<answer_end>",
  "Instructions for Plan Generation of ASQA": "Plan Generation:Instructions:Given several short qa-pairs and a sentence, you need to decide which qa-pair is this sentencerelevant to. Always cite for any factual claim. When citing several search results, use .If multiple qa-pairs support the sentence, only cite a minimum sufficient subset of the qa-pairs.QA-Pairs: Where Haier Pakistan is located? Pakistan. When was Haier Pakistan established? 2000.Sentence:Established in 2000, it is a subsidiary of the Chinese multinational group Haier.Out: QA-Pairs: When does episode 42 of bunkd come out? May 24, 2017. When does episode 41 of bunkd come out?? April 28, 2017. When does episode 40 of bunkd come out? April 21, 2017.Sentence:The new bunkd episode 41 comes out on April 21, 2017, episode 42 comes out on April 28, 2017and episode 42 is due to come out on May 24, 2017.Out: QA-Pairs:your qa-pairsSentence:your sentenceOut: Given a number of questions, you need to summarize them as concisely and accurately as possibleinto one question, avoiding missing information about each question. You dont have to answerthese questions.Questions:",
  "Instructions for Plan Generation of ShareGPT": "Plan Generation:Instructions:Generate appropriate Plan token in the following format: [Plan: xx], for each [Plan] based onrelevant context. Be sure always generate a Plan Token for each [Plan] in order, Keep the details tobe as different as possible from other Plan tokens. Do not generate a Plan Token where there is no[Plan].Input: AB is famous for his work in Parkistan Haier.[Plan] Established in 2000, it is a subsidiary ofthe Chinese multinational group Haier.Output:AB is famous for his work in Parkistan Haier.[Plan: Parkistan Haier establish time]Established in 2000, it is a subsidiary of the Chinese multinational group Haier.Input: answer segmentOutput:",
  "Instructions for Fine-grained evidence selection": "Fine-grained evidence selection:Instructions:Write an accurate, engaging, and concise answer for the given question answer pair using onlythe provided search results (some of which might be irrelevant) and cite them properly. Use anunbiased and journalistic tone. Always cite for any factual claim. When citing several searchresults, use . If multiple documents support the sentence, only cite a minimum sufficientsubset of the documents.Question: When was Haier Pakistan established?Answer: 2000. Haier Pakistan is a consumer electronics and home appliances company in Pakistan. Established in 2000, it is a subsidiary of the Chinese multinational group Haier. It is one of the largest companies in Pakistans home appliances market, in terms of sales andrevenues generated.Out: Ouestion: questionAnswer: answer first evidence second evidence.. .Out:",
  "Instructions for HotpotQA": "Plan Generation:Instructions:Given a question and corresponding short answer. Expand the short answer to an accurate, fine-grained, and concise answer with thinking steps for the given question using only the providedsearch results (some of which might be irrelevant) and cite them properly. During the generation,make sure the plan token in answers start with the question and work their way up logically fromthe answers you already have. Use an unbiased and journalistic tone. Always cite for any factualclaim. Cite at most one evidence in each sentence. If multiple documents support the sentence,only cite the first one.Question: In what year was the company, for which Javed Afridi is best known as CEO, established?Answer: 2000.Evidence: Established in 2000, Haier Pakistan is a subsidiary of the Chinese multinational group Haier. Javed Afridi is best known as the CEO of Haier Pakistan and owner of MG Motors Pakistan.Out: [Plan: Javed Afridi best known company]Javed Afridi is best known as the CEO of HaierPakistan,[Plan: Haier Pakistan establish]which was established in 2000.As mentioned before, the first plan token should be generated from question [Plan: Javed Afridibest known company], considering the answer already generated and further plan the establish timeof Haier Pakistan.Question: Where are Steph Curry and Lebron James both from?Answer: America.Evidence: Stephen Curry is a professional American basketball player with the Golden State Warriors.. Lebron James is a professional American basketball player with the Lakers.Out: [Plan: Steph Curry from]Steph Curry is from America,[Plan: Lebron James from]LebronJames is also from America.Question: questionAnswer: answerEvidence: your evidencesOut:",
  ": Instructions for HotpotQA": "Input: What harmful phenomenon can cause atmospheric warming? A: gasses with pollution B: acid rain C:nuclear war D: melting ice caps (OpenBoookQA)Output: <plan_start>Harmful phenomenon causing atmospheric warming<plan_end><paragraph>Asian browncloud past century. The report suggested that air pollution regulations can have large amplifying effects onglobal warming. Another major impact is on the polar ice caps. Black carbon (soot) in the Asian Brown Cloudmay be reflecting sunlight and dimming Earth below but it is warming other places by absorbing incomingradiation and warming the atmosphere and whatever it touches. Black carbon is three times more effectivethan carbon dioxidethe most common greenhouse gasat melting polar ice and snow. Black carbon insnow causes about three times the temperature change as carbon dioxide in the atmosphere. On snowevenat concentrations</paragraph><fparagraph>The report suggested that air pollution regulations can have largeamplifying effects on global warming.</fparagraph><answer_start>A<answer_end>Input:how long does a president have secret service protection (Natural Questions)Output: <plan_start>president secret service protection duration<plan_end><paragraph>Former PresidentsAct health insurance under the Federal Employees Health Benefits Program; a GSA legal opinion ruled JimmyCarter ineligible. Former presidents were entitled from 1965 to 1996 to lifetime Secret Service protection,for themselves, spouses, and children under 16. A 1994 statute, (), limited post-presidential protection to tenyears for presidents inaugurated after January 1, 1997. Under this statute, Bill Clinton would still be entitled tolifetime protection, and all subsequent presidents would have been entitled to ten years protection. On January10, 2013, President Barack Obama signed the Former Presidents Protection Act of 2012, reinstating lifetimeSecret Service protection for his</paragraph><fparagraph>Former presidents were entitled from 1965 to 1996to lifetime Secret Service protection, for themselves, spouses, and children under 16. Under this statute, BillClinton would still be entitled to lifetime protection, and all subsequent presidents would have been entitled toten years protection. </fparagraph><answer_start>lifetime<answer_end>Input: Is the following statement correct or not? Say true if its correct; otherwise say false.## Input: EvanGoldberg is a Canadian director. (FEVER)Output: <plan_start>Evan Goldberg is a Canadian director.<plan_end><paragraph>Evan Goldberg EvanGoldberg Evan Goldberg (born May 11, 1982) is a Canadian screenwriter, film producer, and director. He hascollaborated with his childhood friend Seth Rogen in several films, including \"Superbad\" (2007) (which theyfirst conceived as teenagers), \"Pineapple Express\" (2008), \"This Is the End\" (2013) (their directorial debut),and \"The Interview\" (2014). Goldberg was born in Vancouver, British Columbia, to a Jewish family. He wasraised in Marpole. He attended Point Grey Secondary School (where he met Rogen) and McGill University,and is married to Lisa (Yadavaia) Goldberg. Goldberg started his writing career joining the staff of \"Da AliG</paragraph><fparagraph>Evan Goldberg Evan Goldberg Evan Goldberg (born May 11, 1982) is a Canadianscreenwriter, film producer, and director.</fparagraph><answer_start>true<answer_end>Input: Given four answer candidates, A, B, C and D, choose the best answer choice. ## Input: Darwins theorythat animal species can change over time was inspired by his research on which set of islands? A: the PhilippineIslands B: the Virgin Islands C: the Hawaiian Islands D: the Galapagos Islands(Arc-Easy)Output:<plan_start>Darwinstheoryanimalspecieschangeovertimeinspiredresearchis-lands<plan_end><paragraph>Speciation tendency of small, isolated genetic pools to produce unusual traits.Examples include insular dwarfism and the radical changes among certain famous island chains, for example onKomodo. The Galpagos Islands are particularly famous for their influence on Charles Darwin. During his fiveweeks there he heard that Galpagos tortoises could be identified by island, and noticed that finches differedfrom one island to another, but it was only nine months later that he reflected that such facts could show thatspecies were changeable. When he returned to England, his speculation on evolution deepened after expertsinformed him that these were</paragraph><fparagraph>The Galpagos Islands are particularly famous for theirinfluence on Charles Darwin.</fparagraph><answer_start>D<answer_end>",
  ": Examples for short-form datasets": "Input: Answer the following question. The question may be ambiguous and have multiple correct answers, andin that case, you have to provide a long-form answer including all correct answers.## Input: Where is superoxidedismutase found in the body?(ASQA)Output: <plan_start>different types of superoxide dismutase found in the body<plan_end><paragraph>Reactiveoxygen species nearly all cells exposed to oxygen. In mammals and most chordates, three forms of superoxidedismutase are present. SOD1 is located primarily in the cytoplasm, SOD2 in the mitochondria and SOD3is extracellular. The first is a dimer (consists of two units), while the others are tetramers (four subunits).SOD1 and SOD3 contain copper and zinc ions, while SOD2 has a manganese ion in its reactive centre.The genes are located on chromosomes 21, 6, and 4, respectively (21q22.1, 6q25.3 and 4p15.3-p15.1). TheSOD-catalysed dismutation of superoxide may be written with the following half-reactions: where M = Cu(\"n\" =</paragraph><fparagraph>SOD1 is located primarily in the cytoplasm, SOD2 in the mitochondria andSOD3 is extracellular.The genes are located on chromosomes 21, 6, and 4, respectively (21q22.1, 6q25.3 and4p15.3-p15.1).</fparagraph><answer_start>Three forms of superoxide dismutase are present in humans, inall other mammals, and in most chordates, SOD1, SOD2, and SOD3, each located in a different part of thebody.<answer_end><plan_start>different types of superoxide dismutase (SOD1, SOD2, SOD3) found in thebody<plan_end><paragraph>Reactive oxygen species nearly all cells exposed to oxygen. In mammals and mostchordates, three forms of superoxide dismutase are present. SOD1 is located primarily in the cytoplasm, SOD2in the mitochondria and SOD3 is extracellular. The first is a dimer (consists of two units), while the others aretetramers (four subunits). SOD1 and SOD3 contain copper and zinc ions, while SOD2 has a manganese ionin its reactive centre. The genes are located on chromosomes 21, 6, and 4, respectively (21q22.1, 6q25.3 and4p15.3-p15.1). The SOD-catalysed dismutation of superoxide may be written with the following half-reactions:where M = Cu (\"n\" =</paragraph><fparagraph>SOD1 is located primarily in the cytoplasm, SOD2 in themitochondria and SOD3 is extracellular.</fparagraph><answer_start>Superoxide dismutase 1, or SOD1, islocated in the cytoplasm, the material within a eukaryotic cell enclosed by the cell membrane except for thecell nucleus.<answer_end><plan_start>different types of superoxide dismutase (SOD1, SOD2, SOD3) found inthe body<plan_end><paragraph>Reactive oxygen species nearly all cells exposed to oxygen. In mammals andmost chordates, three forms of superoxide dismutase are present. SOD1 is located primarily in the cytoplasm,SOD2 in the mitochondria and SOD3 is extracellular. The first is a dimer (consists of two units), while the othersare tetramers (four subunits). SOD1 and SOD3 contain copper and zinc ions, while SOD2 has a manganeseion in its reactive centre. The genes are located on chromosomes 21, 6, and 4, respectively (21q22.1, 6q25.3and 4p15.3-p15.1). The SOD-catalysed dismutation of superoxide may be written with the following half-reactions: where M = Cu (\"n\" =</paragraph><fparagraph>SOD1 is located primarily in the cytoplasm, SOD2in the mitochondria and SOD3 is extracellular.SOD1 and SOD3 contain copper and zinc ions, while SOD2has a manganese ion in its reactive centre.</fparagraph><answer_start>SOD2 is found in the mitochondria, anorganelle found in most eukaryotic organisms.<answer_end><plan_start>Superoxide dismutase SOD3 found inthe body<plan_end><paragraph>SOD3 SOD3 Extracellular superoxide dismutase is an enzyme that in humansis encoded by the \"SOD3\" gene. This gene encodes a member of the superoxide dismutase (SOD) protein family.SODs are antioxidant enzymes that catalyze the dismutation of two superoxide radicals into hydrogen peroxideand oxygen. The product of this gene is thought to protect the brain, lungs, and other tissues from oxidative stress.The protein is secreted into the extracellular space and forms a glycosylated homotetramer that is anchoredto the extracellular matrix (ECM) and cell surfaces through an interaction with heparan sulfate proteoglycanand collagen. A fraction of</paragraph><fparagraph>SOD3 SOD3 Extracellular superoxide dismutase is anenzyme that in humans is encoded by the \"SOD3\" gene.The protein is secreted into the extracellular space andforms a glycosylated homotetramer that is anchored to the extracellular matrix (ECM) and cell surfaces throughan interaction with heparan sulfate proteoglycan and collagen.</fparagraph><answer_start>Finally, SOD3 isextracellular, meaning it occurs in the space outside the plasma membrane of a cell.<answer_end>",
  ": An Example for ASQA datasets": "Input: Which movie came out first Escape to Witch Mountain or Petes Dragon ?(HotpotQA)Output: <plan_start>Escape to Witch Mountain release<plan_end><paragraph>Escape to Witch Mountainis a 1975 American fantasy-childrens film, adapted from the 1968 science fiction novel of the same namewritten by Alexander H. Key. The film was produced by Walt Disney Productions, released in March 1975by Buena Vista Distribution Company and directed by John Hough. </paragraph><fparagraph>Escape toWitch Mountain is a 1975 American fantasy-childrens film, adapted from the 1968 science fiction novel ofthe same name written by Alexander H. Key</fparagraph><answer_start>Escape to Witch Mountain came outfirst,<answer_end><plan_start>Petes Dragon release<plan_end><paragraph>Petes Dragon is a 2016 Americanfantasy comedy-drama adventure film directed by David Lowery, written by Lowery and Toby Halbrooks, andproduced by James Whitaker. The film is a live-action reimagining of Disneys 1977 live-action/animatedmusical film of the same name written by Malcolm Marmorstein. The film stars Bryce Dallas Howard, OakesFegley, Wes Bentley, Karl Urban, Oona Laurence, and Robert Redford. The film tells the story of an orphanedferal boy who befriends a dragon in the Pacific Northwest, and the ensuing repercussions of their discovery bythe towns local residents. </paragraph><fparagraph>Petes Dragon is a 2016 American fantasy comedy-dramaadventure film directed by David Lowery, written by Lowery and Toby Halbrooks, and produced by JamesWhitaker. </fparagraph><answer_start>before Petes Dragon. <answer_end>[Combine]<answer_start>Escapeto Witch Mountain<answer_end>"
}