{
  "Abstract": "Training a unified multilingual model promotesknowledge transfer but inevitably introducesnegative interference. Language-specific mod-eling methods show promise in reducing inter-ference. However, they often rely on heuris-tics to distribute capacity and struggle to fos-ter cross-lingual transfer via isolated modules.In this paper, we explore intrinsic task modu-larity within multilingual networks and lever-age these observations to circumvent interfer-ence under multilingual translation. We showthat neurons in the feed-forward layers tendto be activated in a language-specific manner.Meanwhile, these specialized neurons exhibitstructural overlaps that reflect language prox-imity, which progress across layers. Basedon these findings, we propose Neuron Special-ization, an approach that identifies specializedneurons to modularize feed-forward layers andthen continuously updates them through sparsenetworks. Extensive experiments show thatour approach achieves consistent performancegains over strong baselines with additional anal-yses demonstrating reduced interference andincreased knowledge transfer.1",
  "Introduction": "Jointly training multilingual data in a unifiedmodel with a shared architecture for different lan-guages has been a trend (Conneau et al., 2020;Le Scao et al., 2022) encouraging knowledge trans-fer across languages, especially for low-resourcelanguages (Johnson et al., 2017; Pires et al., 2019).However, such a training paradigm also leads tonegative interference due to conflicting optimiza-tion demands (Wang et al., 2020). This interferenceoften causes performance degradation for high-resource languages (Li and Gong, 2021; Pfeifferet al., 2022) and can be further exacerbated by lim-ited model capacity (Shaham et al., 2023).",
  "We release code at": "Modular-based methods, such as Language-specific modeling (Zhang et al., 2020b) andadapters (Bapna and Firat, 2019), aim to mitigateinterference by balancing full parameter sharingwith isolated or partially shared modules (Pfeifferet al., 2023). However, they heavily depend onheuristics for allocating task-specific capacity andface challenges in enabling knowledge transfer be-tween modules (Zhang et al., 2020a). Specifically,such methods rely on prior knowledge for man-aging parameter sharing such as language-familyadapters (Chronopoulou et al., 2023) or directlyisolate parameters per language, which impedestransfer (Pires et al., 2023).Research in vision and cognitive science hasshown that unified multi-task models may sponta-neously develop task-specific functional specializa-tions for distinct tasks (Yang et al., 2019; Dobset al., 2022), a phenomenon also observed inmixture of experts Transformer systems (Zhanget al., 2023). These findings suggest that throughmulti-task training, networks naturally evolve to-wards specialized modularity to effectively man-age diverse tasks, with the ablation of these spe-cialized modules adversely affecting task perfor-mance (Pfeiffer et al., 2023). Despite these insights,exploiting the inherent structural signals for multi-task optimization remains largely unexplored.In this work, we explore the intrinsic task-specific modularity within multi-task networks inMultilingual Machine Translation (MMT), treatingeach language pair as a separate task. We focuson analyzing the intermediate activations in theFeed-Forward Networks (FFN) where most modelparameters reside. To our knowledge, our study isthe first to show that neurons activate in a language-specific way, yet they present structural overlapsthat indicate language proximity in general. More-over, this pattern evolves across layers in the model,suggesting that neurons consistently transition fromlanguage-specific to language-agnostic. Building on these observations, we introduceNeuron Specialization, a novel method that lever-ages intrinsic task modularity to reduce interfer-ence and enhance knowledge transfer. In general,our approach selectively updates the FFN parame-ters during back-propagation for different tasks toenhance task specificity. Specifically, we first iden-tify task-specific neurons from pre-trained unifiedtranslation models, using standard forward-passvalidation processes without decoding. We thenspecifically modularize FFN layers using these spe-cialized neurons and continuously update FFNs viasparse networks.Extensive experiments on small- (IWSLT) andlarge-scale EC30 (Tan and Monz, 2023) transla-tion datasets show that our method consistentlyachieves performance gains over strong baselineswith various configs. Moreover, we conduct in-depth analyses to show that our method effectivelymitigates interference and enhances knowledgetransfer in high and low-resource languages, re-spectively. Our main contributions are summarizedas follows:",
  "Related Work": "Multilingual Interference.Multilingual trainingenables knowledge transfer but also causes interfer-ence, largely due to optimization conflicts amongvarious tasks (Wang and Zhang, 2022). Methodsalleviating task conflicts hold promise to reduceinterference (Wang et al., 2020), yet they showlimited effectiveness in practice (Xin et al., 2022).Scaling up model size may reduce interference butleads to overly large models (Chang et al., 2023),with risks of overfitting (Aharoni et al., 2019).",
  "Language-Specific Modeling.Recent methodsenhance the unified model by utilizing language-specific (LS) modules such as adapters (Bapna": "and Firat, 2019), LS layers (Zhang et al., 2020b;Pires et al., 2023) and LS hidden states (Xie et al.,2021). Although the unified model serves as acommon foundation, these methods strictly iso-late modules per language. Such designs presentno knowledge sharing among modules and thusoffer fewer benefits to low-resource languages.Alternatively, approaches like language familyadapters Chronopoulou et al. (2023) seek to fa-cilitate sharing among language-specific modules,however, they heavily depend on heuristics such asusing priori linguistic knowledge to enable moreflexible parameter sharing.Additionally, these modular-based methods ex-hibit parameter inefficiency when handling numer-ous languages, resulting in increased memory re-quirements and extended inference times (Liaoet al., 2023a,b). Similarly, techniques such as pa-rameter differentiation (Wang and Zhang, 2022)and language clustering training (Tan et al., 2019)alleviate interference by expanding the unifiedmodel with substantial extra parameters. Sub-networks in Multi-task Models.The lot-tery ticket hypothesis (Frankle and Carbin, 2018)states that within dense neural networks, sparsesubnetworks can be found with iterative pruning toachieve the original networks performance. Fol-lowing this premise, recent studies attempt to iso-late sub-networks of a pre-trained unified modelthat captures task-specific features (Choenni et al.,2023a; Lin et al., 2021; He et al., 2023). Nonethe-less, unlike our method that identifies intrinsicmodularity within the model, these approaches de-pend on fine-tuning to extract the task-specific sub-networks. This process may not reflect the origi-nal model modularity and also can be particularlyresource-consuming for multiple tasks.Specifically, these methods extract the task-specific sub-networks by fine-tuning the originalunified multi-task model on specific tasks, fol-lowed by employing pruning to retain only the mostchanged parameters. We argue that this processfaces several issues: 1) The sub-network might bean artifact of fine-tuning, suggesting the originalmodel may not inherently possess such modular-ity. 2) This is further supported by the observationthat different random seeds during fine-tuning leadto varied sub-networks and performance instabil-ity (Choenni et al., 2023a). 3) The process is highlyinefficient for models covering multiple tasks, as itnecessitates separate fine-tuning for each task.",
  "Neuron Structural Analysis": "Recent work aims to identify a subset of parame-ters within pre-trained multi-task networks that aresensitive to distinct tasks. This exploration is doneby either 1) selecting hidden states that greatly in-fluence task performance (Dobs et al., 2022) orpossess high magnitude values (Xie et al., 2021);or 2) fine-tuning the unified model on task-specificdata to extract sub-networks (Lin et al., 2021; Heet al., 2023; Choenni et al., 2023b). These ap-proaches, however, raise a fundamental question,namely whether the modularity is inherent to theoriginal model, or simply an artifact introduced bynetwork modifications.In this paper, we perform a thorough identifica-tion of task-specific modularity through the lensof neuron behaviors, without altering the originalparameters or architectures. We focus on the neu-rons the intermediate activations inside the Feed-Forward Networks (FFN) to investigate if theyindicate task-specific modularity features.As FFN neurons are active (>0) or inactive (=0)due to the ReLU activation function2, this binaryactivation state offers a clear view of their contribu-tions to the networks output. Intuitively, neuronsthat remain inactive for one task but show signif-icant activation for another may be indicative ofspecialization for the latter. More importantly, thisapproach ensures that both parameters and hiddenstates remain unchanged, affirming the observedmodularity is inherent to the original model.",
  "Identifying Specialized Neurons": "We choose multilingual translation as a testbed,treating each translation direction as a distinct taskthroughout the paper. We start with a pre-trainedmultilingual model with dff as its dimension of theFFN layer. We hypothesize the existence of neuronsubsets specialized for each task and describe theidentification process of an FFN layer as follows. ActivationRecording.Givenavalidationdataset Dt for the t-th task, we measure activationfrequencies in an FFN layer during validation.For each sample xi Dt, we record the stateof each neuron after the activation function (),reflecting whether the neuron is active or inactiveto the sample. We use a binary vector ati Rdffto store this neuron state information. Note that",
  "For activation functions like GeLU, we consider neuronsinactive when their values are 0, as discussed in .2": "this vector aggregates neuron activations for alltokens in the sample by taking the neuron union ofthem. By further merging all of the binary vectorsfor all samples in Dt, an accumulated vectorat = xiDt ati can be derived, which denotes thefrequency of each neuron being activated during aforward pass given a task-specific dataset Dt. Neuron Selection.We identify specialized neu-rons for each task t based on their activation fre-quency at. A subset of neurons Stk is progressivelyselected based on the highest at values until reach-ing a predefined threshold k, where",
  "Here, the value at(i) is the frequency of the ac-": "tivation at dimension i, and dffi=1 at(i) is the totalactivation of all neurons for an FFN layer. k is athreshold factor, varying from 0% to 100%, indi-cating the extent of neuron activation deemed nec-essary for specialization. A lower k value resultsin higher sparsity in specialized neurons; k = 0means no neuron will be involved, while k = 100fully engages all neurons, the same as utilizing thefull capacity of the original model. This dynamicapproach emphasizes the collective significance ofneuron activations up to a factor of k. In the end,we repeat these processes to obtain the specializedneurons of all FFN layers for each task.",
  "Neuron Analysis on EC30": "In this section, we describe how we identify special-ized neurons on the EC30 dataset (Tan and Monz,2023), where we train an MMT model coveringall directions. EC30 is a multilingual translationbenchmark that is carefully designed to considerdiverse linguistic properties and real-world datadistributions. It collects high to low-resource lan-guages, resulting in 30 diverse languages from 5language families, allowing us to connect our ob-servations with linguistic properties easily. SeeSections 5 for details on data and models. 3.2.1Neuron Overlaps Reflect LanguageProximityWe identified specialized neurons following Sec-tion 3.1, while setting the cumulative activationthreshold k at 95%. This implies that the set ofspecialized neurons covers approximately 95% ofthe total activations. Intuitively, two similar tasks : Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the first decoderFFN layer across all out-of-English translation directions to measure the degree of overlap. Darker cells indicatestronger overlaps, with the color threshold set from 40 to 80 to improve visibility. should have a high overlap between their special-ized neuron sets. Therefore, we examined the over-laps among specialized neurons across differenttasks by calculating the Intersection over Union(IoU) scores: For task ti and tj, with specializedneurons denoted as sets Si and Sj, their overlap isquantified by Eq. 2.",
  "|Si Sj|(2)": "shows the IoU scores for specializedneurons across tasks in the first decoder layer. Fig-ures of other layers are in A.9. We observe a struc-tural separation of neuron overlaps, indicating apreference for language specificity. Notably, neu-ron overlap across language families is relativelylow, a trend more pronounced in encoder layers(). In addition, this structural distinctiongenerally correlates with language proximity as in-dicated by the clustering pattern in . Thisimplies that target languages from the same fam-ily are more likely to activate similar neurons inthe decoder, even when they use different writingsystems, e.g., Arabic (ar) and Hebrew (he). We also provide a phylogenetic tree analysisquantifying the correlation between neuron over-laps and linguistic distances in A.9. Moreover, weshow that neuron overlaps show linguistic traitsbeyond family ties, exemplified by notable over-laps between Maltese (mt) and languages in theRomance family due to vocabulary borrowing. : Progression of distribution of IoU scores forspecialized neurons across layers on the EC30 dataset.The scores are measured for different source and targetlanguages in the Encoder and Decoder, respectively.",
  "The Progression of Neuron Overlaps": "To analyze how specialized neuron overlaps acrosstasks evolve within the model, we visualize the IoUscore distribution across layers in . Foreach layer, we compute the pair-wise IoU scoresbetween all possible tasks and then show them in adistribution. Overall, we observe that from shallowto deeper layers, structural distinctions intensify inthe decoder (decreasing IoU scores) and weaken inthe encoder (increasing IoU scores).Furthermore, all neuron overlaps increase as wemove up the encoder, regardless of whether thesetasks are similar or not. This observation may sug-gest that the neurons in the encoder become morelanguage-agnostic, as they attempt to map differentscripts into semantic concepts. As for the Decoder,the model presents intensified modularity in termsof overlaps of specialized neurons. This can be seen by all overlaps becoming much smaller, indi-cating that neurons behave more separately.Our findings align with the common assump-tion of the transformation process in Seq2Seq mod-els. Similarly, Kudugunta et al. (2019) observedthat multilingual embeddings gradually, though notperfectly, align within the encoder. However, ourresearch diverges as it focuses on binary neuronactivation patterns, rather than high-dimensionalembeddings. Moreover, unlike them, we show thatour findings can be leveraged to improve MMT.",
  "Neuron Specialization Training": "Our neuron structural analysis showed the presenceof specialized neurons within the Feed-ForwardNetwork (FFN) layers of a multilingual network.We hypothesize that continuously training themodel, while leveraging these specialized neuronsintrinsic modular features, can further enhance task-specific performance. Building on this hypothesis,we propose Neuron Specialization, an approachthat leverages specialized neurons to modularizethe FFN layers in a task-specific manner.",
  "Vanilla Feed-Forward Network": "We first revisit the Feed-Forward Network (FFN)in Transformer (Vaswani et al., 2017). The FFN,crucial to our analysis, consists of two linear lay-ers (fc1 and fc2) with an activation function ().Specifically, the FFN block first processes the hid-den state H Rnd (n denotes number of tokensin a batch) through fc1 layer W1 Rddff . Thenthe output is passed to () and the fc2 layer W2,as formalized in Eq 3, with bias terms omitted.",
  "Specializing Task-Specific FFN": "Next, we investigate continuous training upon asubset of specialized parameters within FFN foreach task. Given a pre-trained vanilla multilingualTransformer model with tags to identify the lan-guage pairs, e.g., Johnson et al. (2017), we canderive specialized neuron set Stk for each layer of atask3 t and threshold k following the method out-lined in .1. Then, we derive a booleanmask vector mtk {0, 1}dff from Stk, where the i-th element in mtk is set to 1 only when i Stk, andapply it to control parameter updates. Specifically,",
  "FFN(H) = (H(mtk W1))W2.(4)": "mtk plays the role of controlling parameter up-date, where the boolean value of i-th element inmtk denotes if the i-th row of parameters in W1 canbe updated or not for each layer4 during continuestraining. Broadly speaking, our approach selec-tively updates the first FFN (fc1) weights duringback-propagation, tailoring the model more closelytowards specific translation tasks and reinforcingneuron separation.Note that while fc1 is selectively updated forspecific tasks, other parameters are universally up-dated to maintain stability, and the same maskingis applied to inference to ensure consistency. Inaddition, applying a mask to W1 will nullify thecontribution of the corresponding row in W2 tothe final output, thus, there is no need to applytask-specific masks to W2, as the masking of suf-ficiently controls the influence on the output. Ourpseudocode is in Appendix A.10.Relevant studies like Xie et al. (2021), selec-tively pruning output hidden states for moduleslike attention and FFNs during training and infer-ence. In contrast, we utilize sparse sub-networks(fc1 weights) since we found FFN neurons are al-ready specialized.",
  "Note that mtk is layer-specified, we drop layer indexeshereon for simplicity of notation": "English-centric directions, representing five lan-guage families and various writing systems. Weclassify language pairs into low-resource (=100k),medium-resource (=1M), and high-resource (=5M)categories. We build a 128k size shared unigramvocabulary. Aligning with the original EC30 setups,we use Ntrex-128 (Federmann et al., 2022) as thevalidation set. Also, we use Flores-200 (mergingdevtest and test) as the test set for evaluation.",
  "Fine-Tune.We finetune baselines with the sameroutine as our Neuron Specialization Training": "Adapters.We employ two adapter methods: 1)Language Pair Adapter (AdapterLP) and 2) Lan-guage Family Adapter (AdapterFam). We omitAdapterFam for IWSLT due to its limited languages.AdapterLP inserts adapter modules based on lan-guage pairs, demonstrating strong effects in re-ducing interference while presenting no parame-ter sharing (Bapna and Firat, 2019). In contrast,AdapterFam (Chronopoulou et al., 2023) facilitatesparameter sharing across similar languages by train-ing modules for each language family. Their bottle-neck dimensions are 128 and 512 respectively. SeeAppendix A.2 for more training details. LaSS.Lin et al. (2021) proposed LaSS to lo-cate language-specific sub-networks following thelottery ticket hypothesis, i.e., finetuning all transla-tion directions from a pre-trained model and thenpruning based on magnitude. They then continu-ally train the pre-trained model by only updatingthe sub-networks for each direction. We adoptthe strongest LaSS configuration by applying sub-networks for both attention and FFNs.",
  "Implementation and Evaluation": "To ensure fair comparisons, we use the fixed train-ing routine for all compared methods, see detailedtraining and model specifications in Appendix A.2.For evaluation, We adopt the tokenized BLEU (Pa-pineni et al., 2002) for the IWSLT and detokenizedSacreBLEU5 (Post, 2018) for the EC30. In ad-dition, we report ChrF++ (Popovic, 2017) andCOMET (Rei et al., 2020) in Appendix A.6.",
  "Small-Scale Results on IWSLT": "We show results on IWSLT in . For Many-to-One (M2O) directions, our method receives anaverage +1.7 BLEU gain over the baseline, achiev-ing the best performance among all approaches.The AdapterLP, with a 67% increase in parame-ters over the baseline model, shows weaker im-provements (+0.8) than our method. As for One-to-Many (O2M) directions, we observed weakerperformance gains for all methods. While the gainsare modest (averaging +0.3 BLEU), our methoddemonstrates consistent improvements across var-ious languages in general. Finally, we show thatfine-tuning the baseline with the same setting asour approach does not bring performance gains.",
  ": BLEU improvements over the baseline (mT-small) on IWSLT. denotes the relative parameter increaseover the baseline, and Fine-Tune signifies finetuning mT-small with the same setting as Ours": "found scaling up the model capacity reduces in-terference, even under low-resource settings. Wethen investigate the trade-off between performanceand model capacity by employing mT-shallow, ashallower version of mT-small with three fewer lay-ers (with = 39% for parameters, see for details). Surprisingly, in , we showthat reducing parameters improved Many-to-One(X-En) performance but weakened One-to-Many(En-X) results. This result indicates that scaling upthe model capacity does not always reduce interfer-ence, but may show overfitting to have performancedegradation. Furthermore, we show that imple-menting Neuron Specialization with mT-shallowenhances X-En performance in all directions whilelessening the decline in En-X translation quality.",
  "Large-Scale Results on EC-30": "Similar to what we observed in the small-scalesetting, we find notable improvements when wescale up on the EC30 dataset. shows con-sistent improvements across high-, medium-, andlow-resource languages, with an average gain of+1.3 SacreBLEU over the baseline. LaSS, whileeffective in high-resource O2M pairs, presents lim-itations with negative impacts (-1.0 score) on low-resource languages, highlighting difficulties in sub-network extraction for low-resource languages.In contrast, our method achieves stable and con-sistent gains and passes statistical significance testsin A.4. The AdapterLP , despite increasing parame-ters by 87% compared to the baseline, falls short of our method in boosting performance. Similarto experiments on IWSLT, we found fine-tuningthe baseline on EC30 also brings worse/unchangedperformance, suggesting the effectiveness of ourmethod. Additionally, we show that applying Neu-ron Specialization in the encoder or decoder de-livers similar gains, with both combined offeringstronger performance. Random Mask.We applied Neuron Specializa-tion Training using random masks that masked30% fc1 weights to validate the effectiveness of ourmethod in locating task-specific neurons. shows that such random strategy sacrifices perfor-mance, especially for low-resource languages.",
  "OursEnc0%+1.2+1.1+1.1+1.0+1.0+1.0+0.7+0.8+0.8+1.0+1.0+1.0OursDec0%+1.2+1.1+1.1+0.9+1.1+1.0+0.7+1.1+0.9+0.9+1.1+1.0Ours0%+1.8+1.4+1.6+1.4+1.1+1.3+1.4+0.9+1.2+1.5+1.1+1.3": ": Average SacreBLEU improvements on the EC30 dataset over the baseline (mT-big), categorized by High,Medium, and Low-resource translation directions. Random denotes continually updating the model with randomlyselected task-specific neurons. OursEnc and OursDec indicate Neuron Specialization applied solely to the Encoderand Decoder, respectively, while Ours signifies the method applied to both components. Zero-Shot Translation.We further evaluated ourmethod on 870 zero-shot translation directions. Foran unseen zero-shot direction (Src-Tgt), we con-struct its mask during inference using the Encodermask from Source-to-English (Src-En) and the De-coder mask from English-to-Target (En-Tgt). Weshow an average gain of +7.4 and +3.1 on ChrFand SacreBLEU scores over the baseline (mT-big).Of these, 847 directions improved, while 23 haveminor declines. We present the comparison in Fig-ure 4, and more details can be found in A.7. Wider and Deeper Models.We experiment withlarger models by scaling up the width and depth(see details in A.5). shows we achieveconsistent performance gains, confirming the effec-tiveness of our approach for larger configurations.",
  ": Performance comparison between baselinemodels and our methods on three configurations": "The role of threshold factor.We explore theimpact of our sole hyper-parameter k (neuron se-lection threshold factor) on performance. As men-tioned in .1, a smaller k results in moresparse specialized neuron selection and fc1 weights. shows that our method delivers consistentand positive gains without extensive tuning. Moreexplanations can be found in A.8. : SacreBLEU gains of our method over the mT-large baseline on EC30. The x-axis represents the factork and the dynamic sparsity of fc1 layers, with valuesranging from minimum to maximum achieved sparsity. Efficiency Comparisons.We compare efficiencyacross three aspects ().First, addinglightweight language pair adapters results in an+87% increase in trainable parameters over thebaseline. Second, our method, which locates spe-cialized neurons in just 5 minutes, is significantlyfaster than LaSS, which takes 33 hours with 4Nvidia A6000 GPUs. Finally, regarding memorycosts essential for handling multiple languages indeployment, our method is more economical, re-quiring only 1-bit masks for the FFN neurons.",
  "Ours-0.3+1.7+1.8-0.2-0.3+15.3+12.4+11.3+19.6+14.1+0.3+14.5": ": SacreBLEU score comparisons for Multilingual baseline and Neuron Specialization models againstBilingual ones on the EC30 dataset, limited to 5 high- and low-resource languages due to computational constraints.Red signifies negative interference, Blue denotes positive synergy, with darker shades indicating better effects. Neuron Specialization Beyond ReLU.We vali-date the adaptability of our method with the GeLUactivation function, as it produces negative activa-tion values. We first train an mT-big baseline modelwith GeLU, defining non-active neurons as 0,keeping all other settings unchanged. The results() show that our method also works withGeLU, yielding consistent improvements (see de-tails in ). We leave exploring other thresh-olds for defining inactive FFN neurons to futurework.",
  "The Impact of Reducing Interference": "In this section, we measure to what extent ourmethod mitigates interference and enhances knowl-edge transfer.Similar to Wang et al. (2020),we train bilingual models that do not contain in-terference or transfers, then compare results be-tween bilingual models, the multilingual baselinemodel (mT-big), and our method (ours). We trainTransformer-big and Transformer-based models forhigh- and low-resource tasks, see details in A.2.In , we show that the multilingual model(mT-big) facilitates clear positive transfer for low-resource languages versus bilingual setups, leading to +8.2 (O2M) and +13.3 (M2O) score gains. How-ever, training a unified multilingual model incursnegative interference, causing performance degra-dation for high-resource languages (averaged -3.7and -1.1 drops). In contrast, our Neuron Specialization method re-duces interference for high-resource settings, lead-ing to +1.8 and +1.4 SacreBLEU gains over mT-big in O2M and M2O directions. Moreover, ourmethod enhances low-resource task performancewith average gains of +1.6 (O2M) and +1.2 (M2O)SacreBLEU over the mT-big, demonstrating itsability to foster cross-lingual knowledge transfer.",
  "Conclusions": "In this paper, we have identified and leveraged in-trinsic task-specific modularity within multilingualnetworks to mitigate interference. We showed thatFFN neurons activate in a language-specific way,and they present structural overlaps that reflect lan-guage proximity, which progress across layers. Wethen introduced Neuron Specialization Training toleverage these natural modularity signals to struc-ture the network, enhancing task specificity andimproving knowledge transfer. Our experimen-tal results, spanning various resource levels, showthat our method consistently outperforms strongbaseline systems, with additional analyses demon-strating reduced interference and increased knowl-edge transfer. Our work deepens the understandingof multilingual models by revealing their intrinsicmodularity, offering insights into how multi-taskmodels can be optimized without extensive modifi-cations.",
  "Limitations": "This study primarily focuses on Multilingual Ma-chine Translation, a key approach in multi-tasklearning, which serves as our primary testbed.However, the exploration of multi-task capabilitiescan extend beyond translation to a wider range ofNatural Language Processing tasks. Recent studieshave begun investigating the specific roles of FFNneurons in multilingual processing (Tang et al.,2024; Kojima et al., 2024), safety (Chen et al.,2024), and information aggregation (Voita et al.,2023) within Large Language Models. These ar-eas remain unexplored in our research and presentpromising directions for future work.Recent research has shown that MLP modules re-call and store knowledge, while Attention modulesare more likely to aggregate information (Menget al., 2022; Geva et al., 2021; Elhage et al., 2021;Wang et al., 2023).In addition, extensive re-search, including Mixture-of-Experts and language-specific modules, studies and modifies FFN blocks.Therefore, our focus on FFN specialization alignswith these insights and complements existing re-search. We leave investigations of other Trans-former components, such as the layer normaliza-tion modules, to future work.Furthermore, we validated our method primar-ily on models using the ReLU activation function,though we also conducted experiments with GeLU,which showed smaller performance improvements.Exploring alternative thresholds for defining inac-tive FFN neurons (beyond simply 0) may lead tofurther gains, which we leave for future work.",
  "Broader Impact": "Recognizing the inherent risks of mistranslationin machine translation data, we have made effortsto prioritize the incorporation of high-quality data,such as two open-sourced Multilingual MachineTranslation datasets: IWSLT and EC30. Addition-ally, issues of fairness emerge, meaning that the ca-pacity to generate content may not be equitably dis-tributed across different languages or demographicgroups. This can lead to the perpetuation and am-plification of existing societal prejudices, such asbiases related to gender, embedded in the data.",
  "This research was funded in part by the NetherlandsOrganization for Scientific Research (NWO) under": "project number VI.C.192.080. We would like tothank David Stap, Evgeniia Tokarchuk, Vlad Nicu-lae, and Denis Paperno for their useful commentsand discussions. We would also like to thank thereviewers for their feedback. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.Massively multilingual neural machine translation.In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pages 38743884. Ali Araabi and Christof Monz. 2020. Optimizing trans-former for low-resource neural machine translation.In Proceedings of the 28th International Conferenceon Computational Linguistics, pages 34293435. Naveen Arivazhagan, Ankur Bapna, Orhan Firat,Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,Mia Xu Chen, Yuan Cao, George Foster, ColinCherry, et al. 2019. Massively multilingual neuralmachine translation in the wild: Findings and chal-lenges. arXiv preprint arXiv:1907.05019. Ankur Bapna and Orhan Firat. 2019.Simple, scal-able adaptation for neural machine translation. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 15381548.",
  "Rochelle Choenni, Ekaterina Shutova, and Dan Garrette.2023b. Examining modularity in multilingual lms vialanguage-specialized subnetworks. arXiv preprintarXiv:2311.08273": "Alexandra Chronopoulou, Dario Stojanovski, andAlexander Fraser. 2023. Language-family adaptersfor low-resource multilingual neural machine trans-lation. In Proceedings of the The Sixth Workshopon Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023), pages 5972. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, douard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451. Marta R Costa-juss, James Cross, Onur elebi, MahaElbayad, Kenneth Heafield, Kevin Heffernan, ElaheKalbassi, Janice Lam, Daniel Licht, Jean Maillard,et al. 2022.No language left behind: Scalinghuman-centered machine translation. arXiv preprintarXiv:2207.04672.",
  "Katharina Dobs, Julio Martinez, Alexander JE Kell,and Nancy Kanwisher. 2022. Brain-like functionalspecialization emerges spontaneously in deep neuralnetworks. Science advances, 8(11):eabl8913": "Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al.2021. A mathematical framework for transformercircuits. Transformer Circuits Thread, 1(1):12. Angela Fan, Shruti Bhosale, Holger Schwenk, ZhiyiMa, Ahmed El-Kishky, Siddharth Goyal, MandeepBaines, Onur Celebi, Guillaume Wenzek, VishravChaudhary, et al. 2021. Beyond english-centric multi-lingual machine translation. The Journal of MachineLearning Research, 22(1):48394886. Christian Federmann, Tom Kocmi, and Ying Xin. 2022.Ntrex-128news test references for mt evaluation of128 languages. In Proceedings of the First Workshopon Scaling Up Multilingual Evaluation, pages 2124.",
  "Jonathan Frankle and Michael Carbin. 2018. The lotteryticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on LearningRepresentations": "Markus Freitag, George Foster, David Grangier, VireshRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.Experts, errors, and context: A large-scale study ofhuman evaluation for machine translation. Transac-tions of the Association for Computational Linguis-tics, 9:14601474. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,Craig Stewart, Eleftherios Avramidis, Tom Kocmi,George Foster, Alon Lavie, and Andr FT Martins.2022. Results of wmt22 metrics shared task: Stopusing bleuneural metrics are better and more ro-bust. In Proceedings of the Seventh Conference onMachine Translation (WMT), pages 4668. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers arekey-value memories. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 54845495. Dan He, Minh Quang Pham, Thanh-Le Ha, and MarcoTurchi. 2023. Gradient-based gradual pruning forlanguage-specific multilingual neural machine trans-lation. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 654670. Melvin Johnson, Mike Schuster, Quoc V Le, MaximKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,Fernanda Vigas, Martin Wattenberg, Greg Corrado,et al. 2017. Googles multilingual neural machinetranslation system: Enabling zero-shot translation.Transactions of the Association for ComputationalLinguistics, 5:339351. Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hit-omi Yanaka, and Yutaka Matsuo. 2024. On the multi-lingual ability of decoder-based pre-trained languagemodels: Finding and controlling language-specificneurons. In Proceedings of the 2024 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (Volume 1: Long Papers), pages 69126964. Taku Kudo and John Richardson. 2018. Sentencepiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671. Sneha Kudugunta, Ankur Bapna, Isaac Caswell, andOrhan Firat. 2019. Investigating multilingual nmtrepresentations at scale.In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 15651575. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2022.Bloom:A 176b-parameter open-access multilingual language model.",
  "Daniel Mllner. 2011.Modern hierarchical, ag-glomerative clustering algorithms. arXiv preprintarXiv:1109.2378": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,Sam Gross, Nathan Ng, David Grangier, and MichaelAuli. 2019. fairseq: A fast, extensible toolkit for se-quence modeling. arXiv preprint arXiv:1904.01038. Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. 2021.Contrastive learning for many-to-many multilingualneural machine translation. In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 244258. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, JamesCross, Sebastian Riedel, and Mikel Artetxe. 2022.Lifting the curse of multilinguality by pre-trainingmodular transformers. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 34793495.",
  "Matt Post. 2018. A call for clarity in reporting bleuscores. In Proceedings of the Third Conference onMachine Translation: Research Papers, pages 186191": "Ricardo Rei, Jos GC De Souza, Duarte Alves,Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,Alon Lavie, Luisa Coheur, and Andr FT Martins.2022. Comet-22: Unbabel-ist 2022 submission forthe metrics shared task. In Proceedings of the Sev-enth Conference on Machine Translation (WMT),pages 578585. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020. Comet: A neural framework for mt eval-uation. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 26852702. Stefan Riezler and John T Maxwell III. 2005. On somepitfalls in automatic evaluation and significance test-ing for mt. In Proceedings of the ACL workshopon intrinsic and extrinsic evaluation measures formachine translation and/or summarization, pages5764. Uri Shaham, Maha Elbayad, Vedanuj Goswami, OmerLevy, and Shruti Bhosale. 2023. Causes and cures forinterference in multilingual translation. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),Toronto, Canada. Association for Computational Lin-guistics. Shaomu Tan and Christof Monz. 2023. Towards a betterunderstanding of variations in zero-shot neural ma-chine translation performance. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1355313568. Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, andTie-Yan Liu. 2019.Multilingual neural machinetranslation with language clustering. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 963973. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dong-dong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei,and Ji-Rong Wen. 2024. Language-specific neurons:The key to multilingual capabilities in large languagemodels. In Proceedings of the 62nd Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 57015715, Bangkok,Thailand. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30.",
  "Elena Voita, Javier Ferrando, and Christoforos Nalmpan-tis. 2023. Neurons in large language models: Dead, n-gram, positional. arXiv preprint arXiv:2309.04827": "Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pages 98409855. Qian Wang and Jiajun Zhang. 2022. Parameter differen-tiation based multilingual neural machine translation.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 36, pages 1144011448. Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.2020. Gradient vaccine: Investigating and improv-ing multi-task optimization in massively multilingualmodels. In International Conference on LearningRepresentations. Di Wu and Christof Monz. 2023. Beyond shared vocab-ulary: Increasing representational word similaritiesacross languages for multilingual machine translation.In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, Singapore.Association for Computational Linguistics. Di Wu, Shaomu Tan, Yan Meng, David Stap, andChristof Monz. 2024.How far can 100 samplesgo? unlocking overall zero-shot multilingual trans-lation via tiny multi-parallel data. arXiv preprintarXiv:2401.12413. Di Wu, Shaomu Tan, David Stap, Ali Araabi, andChristof Monz. 2023. Uva-mts participation in thewmt 2023 general translation shared task. In Pro-ceedings of the Eighth Conference on Machine Trans-lation, pages 175180. Wanying Xie, Yang Feng, Shuhao Gu, and Dong Yu.2021. Importance-based neuron allocation for multi-lingual neural machine translation. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 57255737. Derrick Xin, Behrooz Ghorbani, Justin Gilmer, AnkushGarg, and Orhan Firat. 2022. Do current multi-taskoptimization methods in deep learning even help?Advances in neural information processing systems,35:1359713609. Guangyu Robert Yang, Madhura R Joglekar, H Fran-cis Song, William T Newsome, and Xiao-Jing Wang.2019. Task representations in neural networks trainedto perform many cognitive tasks.Nature neuro-science, 22(2):297306. Biao Zhang, Ankur Bapna, Rico Sennrich, and OrhanFirat. 2020a. Share or not? learning to schedulelanguage-specific capacity for multilingual transla-tion. In International Conference on Learning Rep-resentations.",
  "A.1Dataset details": "Due to the difficulties of mining non-English-centric Translation data, recent research (Johnsonet al., 2017; Zhang et al., 2020b,a; Tan and Monz,2023; Wu and Monz, 2023; Shaham et al., 2023;Pires et al., 2023) has increasingly focused on uti-lizing English-centric datasets to explore Multilin-gual Neural Machine Translation (MNMT). Fur-thermore, Fan et al. (2021) have observed that train-ing in M2M settings does not necessarily enhanceperformance in supervised directions. Therefore,our approach prioritizes English-centric datasets toremain computationally feasible while still provid-ing valuable insights into MNMT dynamics.",
  "IWSLTWe collect and pre-processes the IWSLT-14 dataset following Lin et al. (2021). We referreaders to Lin et al. (2021) for more details": "EC30We utilize the EC30, a subset of the EC40dataset (Tan and Monz, 2023) (with 10 extremelylow-resource languages removed in our experi-ments) as our main dataset for most experimentsand analyses. We list the Languages with their ISOand scripts in , along with their number ofsentences.In general, EC30 is an English-centric Multi-lingual Machine Translation dataset containing 61million sentences covering 30 languages (exclud-ing English). It collected data from 5 representativelanguage families with multiple writing scripts. Inaddition, EC30 is well balanced at each resourcelevel, for example, for all high-resource languages,the number of training sentences is 5 million. Notethat the EC30 is already pre-processed and tok-enized (with Moses tokenizer), thus we directlyuse it for our study.",
  "mT-largeEC30615M12161,0244,0967,680210.1Ours-largeEC30615M12161,0244,0967,680210.1": ": Configuration and hyper-parameter settings for all models in this paper. Num. Layer and Attn Head denotethe number of layers and attention heads, respectively. dim represents the dimension of the Transformer model, dffmeans the dimension of the feed-forward layer. bilingual-low and -high represent the bilingual models for low andhigh-resource languages. (EC30) in . To maintain consistency andcomparability across all experiments, we employedthe same early stopping settings rather than fix-ing the training duration for all experiments. Weuse 4 NVIDIA A6000 (48G) GPUs to conductmost experiments and implement them based onFairseq (Ott et al., 2019) with FP16. Lastly, we uti-lize the same training data for both pre-training andmethods involving fine-tuning or continual train-ing.",
  "A.2.1Global training settings": "For all systems on both datasets, we adopt thepre-norm and share the decoder input output em-bedding. In addition, we use the Adam optimizer(1 = 0.9, 2 = 0.98, = 109) with 5e-4 learn-ing rate and 4k warmup steps in all methods. Fur- thermore, we use cross entropy with label smooth-ing to avoid overfitting (smoothing factor=0.1) andset early stopping to 20. Similar to Fan et al. (2021),we prepend language tags to the source and targetsentences to indicate the translation directions forall multilingual translation systems.More importantly, we applied the same fixed rou-tine across all experiments to ensure a fair compar-ison among all multilingual systems. Other globalsettings are the same for all systems to make faircomparisons, such as learning rate, warm-up steps,and batch size.",
  "We train bilingual models in section 6.3 to studyhow much our method can reduce interference andfoster knowledge transfer. For bilingual models of": "low-resource languages, we adopt the suggestedhyper-parameter settings from Araabi and Monz(2020), such as dff = 512, number of attentionhead as 2, and dropout as 0.3. Furthermore, Wetrain separate dictionaries for low-resource bilin-gual models to avoid potential overfitting instead ofusing the large 128k shared multilingual dictionary.For bilingual models of high-resource languages,we adopt the 128k shared multilingual dictionaryand train models with the Transformer-big archi-tecture as the multilingual baseline (mT-big). Thedetailed configurations can be found in .",
  "A.2.3Language Pair Adapters": "We implement Language Pair Adapters (Bapna andFirat, 2019) by ourselves based on Fairseq. TheLanguage Pair Adapter is learned depending oneach pair, e.g., we learn two modules for en-de,namely an english adapter on the Encoder sideand a German adapter on the Decoder side. Notethat, except for the unified pre-trained model, lan-guage pair adapters do not share any parameterswith each other, preventing potential knowledgetransfers. We set its bottleneck dimension as 128for all experiments of IWSLT and EC30. IWSLT.For the IWSLT dataset that contains 8languages with 16 translation directions, the mT-small base model size is 76M. AdapterLP insert3.2M extra trainable parameters for one direction,thus resulting in 51.2M added parameters for all,leading to 67% relative parameter increase over thebaseline model. EC30.For the EC30 dataset that contains 30 lan-guages with 60 translation directions, the mT-bigbase model size is 439M. AdapterLP inserts 6.4Mextra trainable parameters for one direction, thusresulting in 384M added parameters for all direc-tions, leading to 87% relative parameter increaseover the baseline model. When training AdapterLPfor low-resource languages, we increased dropout(0.1 -> 0.3) and decreased batch size (max-token:7680 -> 2560) to avoid overfitting as suggestedby Bapna and Firat (2019). A.2.4Language Family AdaptersThe Language Family Adapter (Chronopoulouet al., 2023) is learned depending on each lan-guage family, e.g., for all 6 Germanic languages inthe EC30, we learn two modules for en-Germanic,namely the en adapter on the Encoder side and theGermanic adapter on the Decoder side. We set its",
  "bottleneck dimension as 512 for all experimentsfor the EC30": "EC30.For the EC30 dataset that contains 30 lan-guages with 60 translation directions, the mT-bigbase model size is 439M. AdapterFam insert 25.3Madditional trainable parameters for one family (onEN-X directions), thus resulting in 303.6M addedparameters for all families on both EN-X and X-En directions, leading to 69% relative parameterincrease over the baseline model.",
  "When reproducing LaSS (Lin et al., 2021), weadopt the code from their official Github page6": "with the same hyper-parameter setting as they sug-gested in their paper. For IWSLT, we finetunethe mT-small for each translation direction withdropout=0.3, and we set dropout=0.1 for large-scale EC30. We then identify the language-specificparameters for attention and feed-forward modules(the setting with the strongest gains in their paper)with a pruning rate of 70%. We continue to trainthe sparse networks while keeping the same settingas the pre-training phase as they suggested.Note that we observed different results as theyreported in the paper, even though we used thesame code, hyper-parameter settings, and corre-sponding Python environment and package version.We also found that He et al. (2023) reproducedLaSS results in their paper, which shows similarimprovements (around +0.6 BLUE gains) over thebaseline of our reproductions. As for an improvedmethod over LaSS proposed by He et al. (2023),we do not reproduce since no open-source code hasbeen released.",
  "A.2.6Parameter Differentiation": "Wang and Zhang (2022) suggest that parameterswith conflicting inter-task gradients might lead tooptimization conflicts among tasks. To alleviatesuch parameter interference, they propose PD thatdynamically differentiates shared parameters intotask-specialized ones. PD includes a crucial hyper-parameter: the differentiation upper bound (ub).This parameter limits the models size relative tothe original model size. For example, ub = 1.5restricts the model size to 150% of the original,while ub = None allows the model to grow withoutrestriction, which can result in bilingual modelswith unlimited parameter differentiation, i.e., each",
  "A.3Comparisons with M2M-100 Models": "We choose multilingual Transformer architectureas our baseline backbone, which has been com-monly used as a strong baseline in many MNMTstudies (Pires et al., 2023; Shaham et al., 2023;Arivazhagan et al., 2019; Wu et al., 2024), and iswidely recognized as a strong baseline within thecommunity (Chen et al., 2023; Wu et al., 2023; Panet al., 2021; Wu and Monz, 2023).We further establish the strength of our baselinemodels by comparing them to the M2M-100 mod-els, which are state-of-the-art systems trained on anextensive corpus of 7.5 billion parallel sentences.In specific, we directly evaluated the trained M2M-100 models provided in Fairseq 7. The results,presented in , demonstrate that both ourbaseline model (mT-big) and our proposed method(Ours) achieve performance that is comparable to,or even surpasses, the M2M-100 models.",
  "A.4.2Statistical Significance Test": "We conducted Paired approximate randomiza-tion (Riezler and Maxwell III, 2005) paired sig-nificance test to show that the improvements ofour method over the baseline (mT-big) on EC30are statistically significant regarding SacreBleu andChrF++ metrics. In sum, for both metrics, 59/60directions passed the test (p-value < 0.05) excepten-ha. The test is performed with the SacreBleuPython packages paired significance testing fea-ture (paired-ar).",
  "A.5Experiments on wider and deeper models": "We conducted further experiments to determineif our method retains its effectiveness with largermodels. We expanded the baseline model, mT-big, in two key dimensions: a) the feed-forwardnetwork (FFN) size, indicating the width of thenetwork; b) the number of layers, representingthe depth of the network. Specifically, we in-troduced mT-wide, which features an expandedFFN dimensionality (from 4,096 to 8,192), and mT-large, which has increased layer count (from 6-6 to12-12). See model config details in .Following these modifications, we applied ourneuron specialization approach to these models.The results, as shown in , demonstrate con-sistent performance gains across both configura-tions, further validating the efficacy of our method.",
  "A.6EC30 result using ChrF++ and COMET": "Recent studies (Rei et al., 2020; Costa-juss et al.,2022) show that ChrF and COMET present highlevels of correlation with human judgments, andautomatic metrics based on pre-trained embeddingscan outperform human crowd workers (Freitaget al., 2021). Notably, Costa-juss et al. (2022)found an increase of +0.5 in ChrF++ has been cor-related with statistically significant improvementsin human evaluations, with a change of +1.0 in",
  "LanguageFaPlArHeNlDeItEsAvgSize89k128k139k144k153k160k167k169k": "One-to-Many (O2M / En-X)mT-shallow-14.38.710.811.816.019.116.317.314.3Param-Diff+51%-0.1+0.5+0.2+0.4+0.3+0.1+0.3+0.2+0.2Param-Diff+252%+0.8+0.8+0.6+1.4+0.5+0.9+0.6+0.3+0.6Ours0%+0.2+0.6+1.0+1.1-0.10-0.1-0.3+0.3 Many-to-One (M2O / X-En)mT-shallow-10.99.411.714.313.115.511.912.512.4Param-Diff+51%+0.3+0.6+0.9+0.8+0.8+1.8+1.2+1.0+0.9Param-Diff+252%+1.7+1.7+1.5+1.3+2.1+2.9+2.5+1.5+1.9Ours0%+1.3+1.3+2.1+2.1+1.6+2.5+2.0+1.7+1.8 : Experiment results (BLEU) using mT-shallow backbone models on IWSLT. denotes the relativeparameter increase over the mT-shallow. For Parameter Differentiation, we run the differentiation upper bound (ub)by ub=0.51, ub=1.51, and ub=None, limiting the model size to 51%, 151% of the original one and allowing themodel to grow without restriction.",
  "ChrF++ almost always perceptible to human evalu-ators, which is studied on the FLORES test set": "To ensure a comprehensive evaluation, wereport various automatic metrics in this paper:ChrF++(character level), SacreBleu (detokenizedword level), and COMET(representation level)scores as extra results, as shown in , re-spectively. We opted for the \"wmt22-comet-da\"model (Rei et al., 2022), a widely used versionfrom Unbabels collection of models that servesas the default choice. This model presents SOTAperformance in WMT Metrics Shared Task (Freitaget al., 2022). Similar to what we observed in Sec-tion 6.2, our Neuron Specialization presents consis-tent performance improvements over the baselinemodel while outperforming other methods such asLaSS and Adapters. Our method, applied to the same FLORES-200test set, outperformed the baseline with an averageincrease of +1.1 ChrF++ scores, where most gainswere greater than +1.0 ChrF++. This improvementemphasizes the effectiveness of our approach, sug-",
  "A.7Reults in Zero-shot translations": "Zero-shot neural machine translation (ZS-NMT)represents a pivotal challenge in multilingual ma-chine translation, aiming to handle language pairsnever seen during training.Although trainingunified MMT systems enables zero-shot transla-tions(Johnson et al., 2017), their performance fallsshort of that seen in supervised directions. Recentfindings by Zhang et al. (2020b) suggest that largermodel sizes enhance ZS performance. Addition-ally, Tan and Monz (2023) indicates that vocabu-lary overlap and linguistic similarities contributeto variations in ZS performance, and that strongerEn-centric capabilities might improve ZS results. ZS-NMT SetupsTo further investigate whetherour method could bring benefits to zero-shot trans-lations, we tested our method across 870 zero-shotdirections involving 30 languages.To do that,we created masks using the Encoder mask from",
  "Source-to-English (Src-En) and the Decoder maskfrom English-to-Target (En-Tgt)": "ZS-NMT ResultsOverall, we observed an aver-aged +3.1 SacreBLEU improvement on zero-shotdirections, with 847 out of 870 directions show-ing improvements, and 23 directions experiencingminor declines, averaging -0.3 SacreBLEU. De-tailed results for high, medium, and low-resourcelanguages (denoted as H, M, and L) are presentedin , along with comparisons of directionsachieving baseline scores of 5 and 10 SacreBLEUusing both a baseline model (mT-big) and ourmethod are shown in .",
  "A.8Sparsity versus Performance": "For the Neuron Specialization, we dynamically se-lect specialized neurons via a cumulative activa-tion threshold k in Equation 1, which is the onlyhyper-parameter of our method. Here, we discussthe impact of k on the final performance and itsrelationship to the sparsity. As mentioned in Sec-tion 3.1, a smaller factor k results in more sparsespecialized neuron selection, which makes the fc1weight more sparse as well in the Neuron Special-ization Training process. In , we show thatour method consistently outperforms the baselineacross a range of k values, from 50 to 97. This",
  "mT-biggelu0%+0.1+0.1+0.10000+0.1+0.10+0.1+0.1Oursgelu0%+1.2+0.9+1.1+0.9+1.1+1.0+0.9+0.6+0.8+1.0+0.9+1.0": ": Average improvements on the EC30 dataset over the baseline (mT-big). OursEnc and OursDec indicateneuron specialization applied solely to the Encoder and Decoder, respectively, while Ours signifies the methodapplied to both components. The best results are highlighted in bold. demonstrates robust positive gains, suggesting thatour method is stable across various k settings.In addition, we show that increasing k leads tohigher improvements in general, and the optimalperformance is about when k=95%. Such observa-tion follows the intuition since when k is too low,model capacity will be largely reduced. Moreover,we find that when the FFN capacity is significantlyreduced (k being very small), we still observe per-formance gains. Notably, even when 70%-83%of FFN weights are zeroed out (as shown in Fig- ure 5), our method still achieves an increase of +0.6SacreBLEU. These results indicate that our methodcan deliver consistent and positive gains withoutextensive hyperparameter tuning.Furthermore, in , we show that the spar-sity of the network presents an intuitive structure:the sparsity decreases in the Encoder and increasesin the Decoder. This implies the natural signalwithin the pre-trained multilingual model that neu-rons progress from language-specific to language-agnostic in the Encoder, and vice versa in the De- : Sparsity progression of Neuron Specializationwhen k = 95 on the EC30. We observe that the sparsitybecomes smaller in the Encoder and then goes up in theDecoder. Note that this figure is based on the naturalsignals extracted from the untouched pre-trained model,and will be leveraged later in the process of NeuronSpecialization Training. This intrinsic pattern naturallyfollows our intuition that specialized neurons progressfrom language specific to agnostic the in Encoder, andvice versa in the Decoder.",
  "A.9Neuron Overlaps Visualization andPhylogenetic Tree": "A.9.1Phylogenetic TreeIn section 3.2, we show that neuron overlaps re-flect language proximity, supported by visualizingthe IoU matrix for specialized neurons ().Here, we provide a more rigorous analysis quanti-fying the correlation between neuron overlaps andlinguistic distances to provide stronger evidence ofhow specialized neuron overlaps correlated withlanguage similarity.To do that, we conduct the Complete LinkageClustering (Mllner, 2011; Bar-Joseph et al., 2001)to build a phylogenetic tree8 using the IoU scoresfor specialized neurons in the first decode FFNlayer. As shows, the result presents strongevidence that neuron overlaps mirror the pattern oflanguage proximity.",
  "We provide the pseudocode of our proposedmethod, Neuron Specialization. We present theprocess of Specialized Neuron Identification in Al-gorithm. 1 and Neuron Specialization Training inAlgorithm. 2": ": Phylogenetic Tree of Languages using neuron overlap IoU scores in the first decoder layer (the IoUvalues correspond to ). This figure presents strong evidence that neuron overlaps mirror the pattern oflanguage proximity. : Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the first encoderFFN layer across all X-En language pairs to measure the degree of overlap between language pairs. Darker cellsindicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility. : Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the last encoderFFN layer across all One-to-Many language pairs to measure the degree of overlap between language pairs. Darkercells indicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility. : Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the last decoderFFN layer across all X-En language pairs to measure the degree of overlap between language pairs. Darker cellsindicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility.",
  "Algorithm 2 Neuron Specialization Training": "1: Input: A pre-trained multi-task model with dimensions d and dff . Corpora data C with T tasks thatcontain both training and validation data. A set of selected specialized neurons Stk for each task t. 2: Output: A new specialized network new. Note that only the fc1 weight matrix will be trainedtask-specifically, the other parameters are shared across tasks. In addition, new does not containmore trainable parameters than due to the sparse network feature."
}