{
  "Abstract": "Tokenization is a foundational step in naturallanguage processing (NLP) tasks, bridging rawtext and language models. Existing tokeniza-tion approaches like Byte-Pair Encoding (BPE)originate from the field of data compression,and it has been suggested that the effectivenessof BPE stems from its ability to condense textinto a relatively small number of tokens. Wetest the hypothesis that fewer tokens lead tobetter downstream performance by introducingPathPiece, a new tokenizer that segments a doc-uments text into the minimum number of to-kens for a given vocabulary. Through extensiveexperimentation we find this hypothesis not tobe the case, casting doubt on the understandingof the reasons for effective tokenization. To ex-amine which other factors play a role, we eval-uate design decisions across all three phasesof tokenization: pre-tokenization, vocabularyconstruction, and segmentation, offering newinsights into the design of effective tokenizers.Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE toinitialize vocabulary construction. We train64 language models with varying tokenization,ranging in size from 350M to 2.4B parameters,all of which are made publicly available.",
  "Introduction": "Tokenization is an essential step in NLP that trans-lates human-readable text into a sequence of dis-tinct tokens that can be subsequently used by statis-tical models (Grefenstette, 1999). Recently, a grow-ing number of studies have researched the effectsof tokenization, both in an intrinsic manner and asit affects downstream model performance (Singhet al., 2019; Bostrom and Durrett, 2020; Hofmannet al., 2021, 2022; Limisiewicz et al., 2023; Zouharet al., 2023b). To rigorously inspect the impactof tokenization, we consider tokenization as threedistinct, sequential stages:",
  "rules that restricts or enforces the creationof certain tokens (e.g., splitting a corpus onwhitespace, thus preventing any tokens fromcontaining whitespace)": "2. Vocabulary Construction: the core algo-rithm that, given a text corpus C and desiredvocabulary size m, constructs a vocabularyof tokens tk V, such that |V| = m, whileadhering to the pre-tokenization rules. 3. Segmentation: given a vocabulary V anda document d, segmentation determineshow to split d into a series of Kd tokenst1, . . . , tk, . . . , tKd, with all tk V, such thatthe concatenation of the tokens strictly equalsd. Given a corpus of documents C, we will de-fine the corpus token count (CTC) as the totalnumber of tokens used in each segmentation,CTC(C) = dC Kd.",
  "We will refer to this step as segmentation, al-though in other works it is also called infer-ence or even tokenization": "The widely used Byte-Pair Encoding (BPE) tok-enizer (Sennrich et al., 2016) originated in the fieldof data compression (Gage, 1994). Gall (2019)argues that it is effective because it compressestext to a short sequence of tokens. Goldman et al.(2024) varied the number of documents in the tok-enizer training data for BPE, and found a correla-tion between CTC and downstream performance.To investigate the hypothesis that having fewer to-kens necessarily leads to better downstream perfor-mance, we design a novel tokenizer, PATHPIECE,that, for a given document d and vocabulary V,finds a segmentation with the minimum possible Kd. The PATHPIECE vocabulary construction rou-tine is a top-down procedure that heuristically min-imizes CTC on a training corpus. PATHPIECE isideal for studying the effect of CTC on downstreamperformance, as we can vary decisions at each tok-enization stage.We extend these experiments to the most com-monly used tokenizers, focusing on how down-stream task performance is impacted by the ma-jor stages of tokenization and vocabulary sizes.Toward this aim, we conducted experiments bytraining 64 language models (LMs): 54 LMs with350M parameters; 6 LMs with 1.3B parameters;and 4 LMs with 2.4B parameters. We provideopen-source, public access to PATHPIECE1, andour trained vocabularies and LMs2.",
  "Preliminaries": "Ali et al. (2024) and Goldman et al. (2024) ex-amined the effect of tokenization on downstreamperformance of LLM tasks, reaching opposite con-clusions on the importance of CTC. Zouhar et al.(2023a) also find that low token count alone doesnot necessarily improve performance. Mielke et al.(2021) give a survey of subword tokenization.",
  "Pre-tokenization Methods": "Pre-tokenization is a process of breaking text intochunks, which are then tokenized independently. Atoken is not allowed to cross these pre-tokenizationboundaries. BPE, WordPiece, and Unigram all re-quire new chunks to begin whenever a space isencountered. If a space appears in a chunk, itmust be the first character; hence, we will callthis FirstSpace. Thus New is allowed butNew York is not. Gow-Smith et al. (2022) ex-amine treating spaces as individual tokens, whichwe will call Space pre-tokenization, while Jacobsand Pinter (2022) suggest marking spaces at theend of tokens, and Gow-Smith et al. (2024) pro-pose dispensing them altogether in some settings.Llama (Touvron et al., 2023) popularized the ideaof having each digit always be an individual token,which we call Digit pre-tokenization.",
  "We focus on byte-level, lossless subword tok-enization. Subword tokenization algorithms split": "text into word and subword units based on theirfrequency and co-occurrence patterns from theirtraining data, effectively capturing morphologi-cal and semantic nuances in the tokenization pro-cess (Mikolov et al., 2011).We analyze BPE, WordPiece, and Unigram asbaseline subword tokenizers, using the implemen-tations from HuggingFace3 with ByteLevel pre-tokenization enabled. We additionally study SaGe,a context-sensitive subword tokenizer, using ver-sion 2.0.4 Byte-Pair EncodingSennrich et al. (2016) in-troduced Byte-Pair Encoding (BPE), a bottom-upmethod where the vocabulary construction startswith single bytes as tokens. It then merges the mostcommonly occurring pair of adjacent tokens in atraining corpus into a single new token in the vocab-ulary. This process repeats until the desired vocab-ulary size is reached. Issues with BPE and analysesof its properties are discussed in Bostrom and Dur-rett (2020); Klein and Tsarfaty (2020); Gutierrez-Vasques et al. (2021); Yehezkel and Pinter (2023);Saleva and Lignos (2023); Liang et al. (2023); Lianet al. (2024); Chizhov et al. (2024); Bauwens andDelobelle (2024). Zouhar et al. (2023b) build anexact algorithm which optimizes compression forBPE-constructed vocabularies. WordPieceWordPiece is similar to BPE, ex-cept that it uses Pointwise Mutual Information(PMI) (Bouma, 2009) as the criteria to identifycandidates to merge, rather than a count (Wu et al.,2016; Schuster and Nakajima, 2012). PMI prior-itizes merging pairs that occur together more fre-quently than expected, relative to the individualtoken frequencies. Unigram Language ModelUnigram works ina top-down manner, starting from a large initialvocabulary and progressively pruning groups of to-kens that induce the minimum likelihood decreaseof the corpus (Kudo, 2018). This selects tokens tomaximize the likelihood of the corpus, accordingto a simple unigram language model. SaGeYehezkel and Pinter (2023) proposed SaGe,a subword tokenization algorithm incorporatingcontextual information into an ablation loss via askipgram objective. SaGe also operates top-down,pruning from an initial vocabulary to a desired size.",
  "Segmentation Methods": "Given a tokenizer and a vocabulary of tokens, seg-mentation converts text into a series of tokens. Weincluded all 256 single-byte tokens in the vocabu-lary of all our experiments, ensuring any text canbe segmented without out-of-vocabulary issues.Certain segmentation methods are tightly cou-pled to the vocabulary construction step, such asmerge rules for BPE or the maximum likelihood ap-proach for Unigram. Others, such as the WordPieceapproach of greedily taking the longest prefix tokenin the vocabulary at each point, can be applied toany vocabulary; indeed, there is no guarantee thata vocabulary will perform best downstream withthe segmentation method used to train it (Uzanet al., 2024). Additional segmentation schemesinclude Dynamic Programming BPE (He et al.,2020), BPE-Dropout (Provilkov et al., 2020), andFLOTA (Hofmann et al., 2022).",
  "PATHPIECE": "Several efforts over the last few years (Gall, 2019;Zouhar et al., 2023a, inter alia) have suggested thatthe empirical advantage of BPE as a tokenizer inmany NLP applications, despite its unawarenessof language structure, can be traced to its superiorcompression abilities, providing models with over-all shorter sequences during learning and inference.Inspired by this claim we introduce PATHPIECE,a lossless subword tokenizer that, given a vocabu-lary V and document d, produces a segmentationminimizing the total number of tokens needed tosplit d. We additionally provide a vocabulary con-struction procedure that, using this segmentation,attempts to find a V minimizing the corpus tokencount (CTC).5 PATHPIECE provides an ideal test-ing laboratory for the compression hypothesis byvirtue of its maximally efficient segmentation.",
  "Segmentation": "PATHPIECE requires that all single-byte tokens areincluded in vocabulary V to run correctly. PATH-PIECE works by finding a shortest path througha directed acyclic graph (DAG), where each bytei of training data forms a node in the graph, andtwo nodes j and i contain a directed edge if thebyte segment [j, i] is a token in V. We describePATHPIECE segmentation in Algorithm 1, whereL is a limit on the maximum width of a token inbytes, which we set to 16. It has a complexity of",
  "An extended description is given in Appendix A": "O(nL), which follows directly from the two nestedfor-loops. For each byte i in d, it computes theshortest path length pl[i] in tokens up to and includ-ing byte i, and the width wid[i] of a token with thatshortest path length. In choosing wid[i], ties be-tween multiple tokens with the same shortest pathlength pl[i] can be broken randomly, or the onewith the longest wid[i] can be chosen, as shownhere.6 Then, a backward pass constructs the short-est possible segmentation from the wid[i] valuescomputed in the forward pass.",
  "Vocabulary Construction": "PATHPIECEs vocabulary is built in a top-downmanner, attempting to minimize the corpus tokencount (CTC), by starting from a large initial vocab-ulary V0 and iteratively omitting batches of tokens.The V0 may be initialized from the most frequentlyoccurring byte n-grams in the corpus, or from alarge vocabulary trained by BPE or Unigram. Weenforce that all single-byte tokens remain in the vo-cabulary and that all tokens are L bytes or shorter.For a PATHPIECE segmentation t1, . . . , tKd of adocument d in the training corpus C, we would liketo know the increase in the overall length of thesegmentation Kd after omitting each token t fromour vocabulary and then recomputing the segmen- 6Random tie-breaking, which can be viewed as a form ofsubword regularization, is presented in Appendix A. Somemotivation for selecting the longest token is due to the successof FLOTA (Hofmann et al., 2022). tation. Tokens with a low overall increase are goodcandidates to remove from the vocabulary.To avoid the very expensive O(nL|V|) computa-tion of each segmentation from scratch, we make asimplifying assumption that allows us to computethese increases more efficiently: we omit a specifictoken tk, for k [1, . . . , Kd] in the segmentationof a particular document d, and compute the min-imum increase MIkd 0 in the total tokens Kdfrom not having that token tk in the segmentationof d. We then aggregate these token count increasesMIkd for each token t V. We can compute theMIkd without actually re-segmenting any docu-ments, by reusing the shortest path informationcomputed by Algorithm 1 during segmentation.Any segmentation not containing tk must eithercontain a token boundary somewhere inside of tkbreaking it in two, or it must contain a token thatentirely contains tk as a superset. We enumerateall occurrences for these two cases, and we find theminimum increase MIkd among them. Let tk startat index s and end at index e, inclusive. Path lengthpl[j] represents the number of tokens required forthe shortest path up to and including byte j. Wealso run Algorithm 1 backwards on d, computinga similar vector of backwards path lengths bpl[j],representing the number of tokens on a path fromthe end of the data up to and including byte j. Theminimum length of a segmentation with a tokenboundary after byte j is thus:",
  "MIbkd =minj=s,...,e1 Kbj Kd.(2)": "The minimum increase from omitting tk couldalso be from a segmentation containing a strictsuperset of tk. Let this superset token be tk, withstart s and end e inclusive. To be a strict supersetentirely containing tk, then either s < s and e e, or s s and e > e, subject to the constraintthat the width w = e s + 1 L. In this case,the minimum length when using the superset tokentk would be:",
  "which is the path length to get to the byte beforetk, plus the path length from the end of the data": "backwards to the byte after tk, plus 1 for the tokentk itself.We retain a list of the widths of the tokens end-ing at each byte.7 The set of superset tokens Scan be found by examining the potential e, andthen seeing if the tokens ending at e form a strictsuperset. Similar to the previous case, we can com-pute the minimum increase from replacing tk witha superset token by taking the minimum increaseover the superset tokens S:",
  "k=1p(tk).(6)": "Taking the negative log of this product convertsthe objective from maximizing the likelihood tominimizing the sum of log(p(tk)) terms. WhileUnigram is solved by the Viterbi (1967) algorithm,it can also be solved by a weighted version of PATH-PIECE with weights of log(p(tk)). Conversely,a solution minimizing the number of tokens can befound in Unigram by taking all p(tk) := 1/|V|.",
  "Experiments": "We used the Pile corpus (Gao et al., 2020; Bider-man et al., 2022) for language model pre-training,which contains 825GB of English text data from 22high-quality datasets. We constructed the tokenizervocabularies over the MiniPile dataset (Kaddour,2023), a 6GB subset of the Pile. We use the Mo-saicML Pretrained Transformers (MPT) decoder-only language model architecture.8 Appendix Bgives the full set of model parameters, and Ap-pendix D discusses model convergence.",
  "Downstream Evaluation Tasks": "To evaluate and analyze the performance ofour tokenization process, we select 10 bench-marks from lm-evaluation-harness (Gaoet al., 2023).9These are all multiple-choicetasks with 2, 4, or 5 options, and were runwith 5-shot prompting. We use arc_easy (Clarket al., 2018),copa (Brassard et al., 2022),hendrycksTests-marketing (Hendrycks et al., 2021),hendrycksTests-sociology (Hendrycks et al., 2021),mathqa (Amini et al., 2019), piqa (Bisk et al.,2019), qa4mre_2013 (Peas et al., 2013), race (Laiet al., 2017), sciq (Welbl et al., 2017), andwsc273 (Levesque et al., 2012). Appendix C givesa full description of these tasks.",
  "Tokenization Stage Variants": "We conduct the 18 experimental variants listed in, each repeated at the vocabulary sizes of32,768, 40,960, and 49,152.10 For baseline vo-cabulary creation methods, we used BPE, Uni-gram, WordPiece, and SaGe. We also considertwo variants of PATHPIECE where ties in the short-est path are broken either by the longest token(PATHPIECEL), or randomly (PATHPIECER). Forthe vocabulary initialization required by PATH-PIECE and SaGe, we experimented with the mostcommon n-grams, as well as with a large initialvocabulary trained with BPE or Unigram.Wealso varied the pre-tokenization schemes for PATH-PIECE and SaGe, using either no pre-tokenizationor combinations of FirstSpace, Space, andDigit described in 2.1. Tokenizers usually usethe same segmentation approach used in vocabularyconstruction. PATHPIECELs shortest path segmen-tation can be used with any vocabulary, so we applyit to vocabularies trained by BPE and Unigram. Wealso apply a Greedy left-to-right longest-token seg-mentation approach to these vocabularies. 10These sizes were selected because vocabularies in the 30kto 50k range are the most common amongst language modelswithin the HuggingFace Transformers library, Ali et al.(2024) recently examined the effect of vocabulary sizes andfound 33k and 50k sizes performed better on English languagetasks than larger sizes.",
  "Results": "reports the downstream performance acrossall our experimental settings.11 A random baselinefor these 10 tasks yields 32%. The OVERALL AVGcolumn indicates the average results over the threevocabulary sizes. The RANK column refers to therank of each variant with respect to the OVERALLAVG column (Rank 1 is best), which we will some-times use as a succinct way to refer to a variant.",
  ": Effect of vocabulary size on downstream per-formance. For each tokenizer variant, we show theoverall average, along with the three averages by vocab-ulary size, labeled according to the ranks in": "gives the overall average, along with theindividual averages, for each of the three vocabu-lary sizes for each variant, labeled according to therank from . We observe that there is a highcorrelation between downstream performance atdifferent vocabulary sizes. The pairwise R2 valuesfor the accuracy of the 32,768 and 40,960 runs was0.750; between 40,960 and 49,152 it was 0.801;and between 32,768 and 49,152 it was 0.834. Thiscorroborates the effect shown graphically in Fig-ure 1 that vocabulary size is not a crucial decisionover this range of sizes. Given this high degree ofcorrelation, we focus our analysis on the overallaverage accuracy. This averaging removes some ofthe variance amongst individual language modelruns. Thus, unless specified otherwise, our analy-ses present performance averaged over vocabularysizes.",
  "Overall performance": "To determine which of the differences in the overallaverage accuracy in are statistically signifi-cant, we conduct a one-sided Wilcoxon signed-ranktest (Wilcoxon, 1945) on the paired differences ofthe 30 accuracy scores (three vocabulary sizes overten tasks), for each pair of variants. All p-valuesreported in this paper use this test. 12345678910 11 12 13 14 15 16 17 18",
  ": Pairwise p-values for 350M model results.Boxes outlined in black represent p > 0.05. The top 6tokenizers are all competitive, and there is no statisti-cally significantly best approach": "displays all pairwise p-values in a colormap. Each column designates a tokenization vari-ant by its rank in , compared to all the ranksbelow it. A box is outlined in black if p > 0.05,where we cannot reject the null. While PATHPIE- CEL-BPE had the highest overall average on thesetasks, the top five tokenizers, PATHPIECEL-BPE,Unigram, BPE, BPE-Greedy, and WordPiece donot have any other row in significantly dif-ferent from them. Additionally, SaGe-BPE (rank6) is only barely worse than PATHPIECEL-BPE(p = 0.047), and should probably be included in thelist of competitive tokenizers. Thus, our first keyresult is that there is no tokenizer algorithm betterthan all others to a statistically significant degree. All the results reported thus far are for languagemodels with identical architectures and 350M pa-rameters. To examine the dependency on modelsize, we trained larger models of 1.3B parametersfor six of our experiments, and 2.4B parameters forfour of them. In the interest of computational time,these larger models were only trained with a singlevocabulary size of 40,960. In in subsec-tion 6.4, we report models average performanceacross 10 tasks. See in Appendix D for anexample checkpoint graph at each model size. Themain result from these models is that the relativeperformance of the tokenizers does vary by modelsize, and that there is a group of high performing to- 1.01.21.41.61.82.02.22.4 Corpus Token Count (CTC), in Billions Average Accuracy (%) BPE WordPiece SaGe Unigram PathPiece",
  "Corpus Token Count vs Accuracy": "shows the corpus token count (CTC) ver-sus the accuracy of each vocabulary size, given in. We do not find a straightforward rela-tionship between the two. Ali et al. (2024) recentlyexamined the relationship between CTC and down-stream performance for three different tokenizers,and also found it was not correlated on Englishlanguage tasks.The two models with the highest CTC are PATH-PIECE with Space pre-tokenization (12), which isto be expected given each space is its own token,and SaGe with an initial Unigram vocabulary (10).The Huggingface Unigram models in hadsignificantly higher CTC than the correspondingBPE models, unlike Bostrom and Durrett (2020)and Gow-Smith et al. (2022), which report a dif-ference of only a few percent with SentencePieceUnigram. Ali et al. (2024) point out that due todifferences in pre-processing, the Huggingface Un-igram tokenizer behaves quite differently than theSentencePiece Unigram tokenizer, which may ex-plain this discrepancy.In terms of accuracy, PATHPIECE with no pre-tokenization (18) and Unigram with PATHPIECEsegmentation (17) both did quite poorly. Notably,",
  ": Pearson Correlation of CTC and Average Accu-racy, or Rnyi efficiency for various orders with Aver-age Accuracy, or CTC and Rnyi efficiency at = 2.5": "the range of CTC is quite narrow within each vo-cabulary construction method, even while changesin pre-tokenization and segmentation lead to sig-nificant accuracy differences. While there are con-founding factors present in this chart (e.g., pre-tokenization, vocabulary initialization, and thatmore tokens allow for additional computations bythe downstream model) it is difficult to discern anytrend that lower CTC leads to improved perfor-mance. If anything, there seems to be an invertedU-shaped curve with respect to the CTC and down-stream performance. The Pearson correlation co-efficient between CTC and average accuracy wasfound to be 0.241. Given that a lower CTC valuesignifies greater compression, this result suggests aweak negative relationship between the amount ofcompression and average accuracy. Zouhar et al. (2023a) introduced an information-theoretic measure based on Rnyi efficiency thatcorrelates with downstream performance for theirapplication.12 It has an order parameter , with arecommended value of 2.5. We present the Rnyiefficiencies and CTC for all models in inAppendix G, and summarize their Pearson corre-lation with average accuracy in . For thedata of , all the correlations for various also have a weak negative association. They areslightly less negative than the association for CTC,although it is not nearly as large as the benefit theysaw over sequence length in their application. Wenote the strong relationship between compressionand Rnyi efficiency, as the Pearson correlation ofCTC and Rnyi efficiency with =2.5 is 0.891.By varying aspects of BPE, Gall (2019) and",
  "Pre-tokenization": "For PATHPIECER with an n-gram initial vocabu-lary, we can isolate pre-tokenization. PATHPIECEis efficient enough to process entire documents withno pre-tokenization, giving it full freedom to mini-mize the corpus token count (CTC).Addingpre-tokenizationconstrainsPATH-PIECEs ability to minimize tokens, giving a nat-ural way to vary the number of tokens. shows that PATHPIECE minimizes the number oftokens used over a corpus when trained with nopre-tokenization (18). The other variants restrictspaces to either be the first character of a token (14),or their own token (12).14 Consider the examplePATHPIECE tokenization in for the threepre-tokenization methods. The NONE mode usesthe word-boundary-spanning tokens ation is, to b, and e $. The lack of morphologicalalignment demonstrated in this example is likelymore important to downstream model performancethan a simple token count.In we observe a statistically signifi-cant increase in overall accuracy for our down-stream tasks, as a function of CTC. Gow-Smithet al. (2022) found that Space pre-tokenization leadto worse performance, while removing the spacesentirely helps15. Thus, this particular result may bespecific to PATHPIECER.",
  "One way to examine the effects of vocabulary con-struction is to compare the resulting vocabulariesof top-down methods trained using an initial vo-cabulary to the method itself. presents an": "13Appendix E contains additional analysis14These two runs also used Digit pre-tokenization whereeach digit is its own token.15Although omitting the spaces entirely does not lead to areversible tokenization as we have been considering. 1.41.61.82.02.2 Corpus Token Count (CTC), in Billions 40.0 42.5 45.0 47.5 50.0 Overall Acc (%) SpaceDigits (12)FirstSpDigits (14)None (18)",
  "Initial Vocabulary": "PATHPIECE, SaGe, and Unigram all require aninitial vocabulary.17 For PATHPIECE and SaGe,we experimented with initial vocabularies of size262,144 constructed from either the most frequentn-grams, or trained using either BPE or Unigram.For PATHPIECEL, using a BPE initial vocabulary(1) is statistically better than both Unigram (9) andn-grams (16), with p 0.01. Using an n-gram 16See in Appendix E.3 for analogous results forUnigram, which behaves similarly.17The HuggingFace Unigram implementation starts withthe one million top n-grams, but sorted according to the counttimes the length of the token, introducing a bias toward longertokens.",
  "Effect of Model Size": "To examine the dependency on model size, webuild larger models of 1.3B parameters for 6 of ourexperiments, and 2.4B parameters for 4 of them.These models were trained over the same 200 bil-lion tokens. In the interest of computational time,these larger models were only run at a single vo-cabulary size of 40,960. The average results overthe 10 task accuracies for these models is givenin . See in Appendix G for thenumerical values. 350M1.3B2.4B Model Size (Not to Scale) 40,960 Vocab Accuracy bpeunigram pathpl_bpesage_bpe sage_ngrampathpl_ngram",
  ": 40,960 vocab average accuracy at variousmodels sizes": "It is noteworthy from the prevalence of crossinglines in that the relative performance of thetokenizers do vary by model size, and that there isa group of tokenizers that are trading places beingat the top for various model sizes. This alignswith our observation that the top 6 tokenizers werewithin the noise, and not significantly better thaneach other in the 350M models.",
  "We investigate the hypothesis that reducing the cor-pus token count (CTC) would improve downstream": "performance, as suggested by Gall (2019) andGoldman et al. (2024) when they varied aspects ofBPE. When comparing CTC and downstream accu-racy across all our experimental settings in ,we do not find a clear relationship between the two.We expand on the findings of Ali et al. (2024) whodid not find a strong relation when comparing 3tokenizers, as we run 18 experiments varying thetokenizer, initial vocabulary, pre-tokenizer, and in-ference method. Our results suggest compressionis not a straightforward explanation of what makesa tokenizer effective.Finally, this work makes several practical con-tributions: (1) vocabulary size has little impact ondownstream performance over the range of sizeswe examined (5.1); (2) five different tokenizersall perform comparably, with none outperformingat statistical significance (5.2); (3) BPE initialvocabularies work best for top-down vocabularyconstruction (6.3). To further encourage researchin this direction, we make all of our trained vo-cabularies publicly available, along with the modelweights from our 64 language models.",
  "Limitations": "The objective of this work is to offer a comprehen-sive analysis of the tokenization process. However,our findings were constrained to particular tasksand models. Given the degrees of freedom, suchas choice of downstream tasks, model, vocabularysize, etc., there is a potential risk of inadvertentlyconsidering our results as universally applicable toall NLP tasks; results may not generalize to otherdomains of tasks.Additionally, our experiments were exclusivelywith English language text, and it is not clear howthese results will extend to other languages. In par-ticular, our finding that pre-tokenization is crucialfor effective downstream accuracy is not applicableto languages without space-delimited words.We conducted experiments for three district vo-cabulary sizes, and we reported averaged resultsacross these experiments. With additional computeresources and time, it could be beneficial to con- duct further experiments to gain a better estimateof any potential noise. For example, in of Appendix D, the 100k checkpoint at the 1.3Bmodel size is worse than expected, indicating thatnoise could be an issue.Finally, the selection of downstream tasks canhave a strong impact on results. To allow for mean-ingful results, we attempted to select tasks thatwere neither too difficult nor too easy for the 350Mparameter models, but other choices could lead todifferent outcomes. There does not seem to be agood, objective criteria for selecting a finite set oftask to well-represent global performance.",
  "Ethics Statement": "We have used the commonly used public datasetThe Pile, which has not undergone a formal ethicsreview (Biderman et al., 2022). Our models mayinclude biases from the training data.Our experimentation has used considerable en-ergy.Each 350M parameter run took approxi-mately 48 hours on (4) p4de nodes, each containing8 NVIDIA A100 GPUs. We trained 62 models, in-cluding the 8 RandTrain runs in Appendix F. The(6) 1.3B parameters models took approximately 69hours to train on (4) p4de nodes, while the (4) 2.4Bmodels took approximately 117 hours to train on(8) p4de nodes. In total, training required 17,304hours of p4de usage (138,432 GPU hours). Thanks to Charles Lovering at Kensho for his in-sightful suggestions, and to Michael Krumdick,Mike Arov, and Brian Chen at Kensho for theirhelp with the language model development process.This research was supported in part by the IsraelScience Foundation (grant No. 1166/23). Thanksto an anonymous reviewer who pointed out thelarge change in CTC when comparing Hugging-face BPE and Unigram, in contrast to the previousliterature using the SentencePiece implementations(Kudo and Richardson, 2018). Mehdi Ali, Michael Fromm, Klaudia Thellmann,Richard Rutmann,Max Lbbering,JohannesLeveling, Katrin Klug, Jan Ebert, Niclas Doll,Jasper Schulze Buschhoff, Charvi Jain, Alexan-der Arno Weber, Lena Jurkschat, Hammam Abdel-wahab, Chelsea John, Pedro Ortiz Suarez, MalteOstendorff, Samuel Weinbach, Rafet Sifa, Stefan",
  "Pavel Chizhov, Catherine Arnett, Elizaveta Korotkova,and Ivan P. Yamshchikov. 2024. Bpe gets picky: Ef-ficient vocabulary refinement during tokenizer train-ing": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. ArXiv,abs/1803.05457. Marco Cognetta, Vilm Zouhar, Sangwhan Moon, andNaoaki Okazaki. 2024. Two counterexamples to tok-enization and the noiseless channel. In Proceedingsof the 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Evalu-ation (LREC-COLING 2024), pages 1689716906,Torino, Italia. ELRA and ICCL.",
  "Philip Gage. 1994. A new algorithm for data compres-sion. C Users J., 12(2):2338": "Matthias Gall. 2019. Investigating the effectiveness ofBPE: The power of shorter sequences. In Proceed-ings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th Inter-national Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 13751381, HongKong, China. Association for Computational Linguis-tics. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020.The pile: An800gb dataset of diverse text for language modeling. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation.",
  "Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao,Idan Szpektor, and Reut Tsarfaty. 2024. Unpackingtokenization: Evaluating text compression and itscorrelation with model performance": "Edward Gow-Smith, Dylan Phelps, Harish Tayyar Mad-abushi, Carolina Scarton, and Aline Villavicencio.2024. Word boundary information isnt useful forencoder language models. In Proceedings of the9th Workshop on Representation Learning for NLP(RepL4NLP-2024), pages 118135, Bangkok, Thai-land. Association for Computational Linguistics. Edward Gow-Smith, Harish Tayyar Madabushi, Car-olina Scarton, and Aline Villavicencio. 2022. Improv-ing tokenisation by alternative treatment of spaces.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages1143011443, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "Gregory Grefenstette. 1999. Tokenization, pages 117133. Springer Netherlands, Dordrecht": "Ximena Gutierrez-Vasques, Christian Bentz, Olga Sozi-nova, and Tanja Samardzic. 2021. From charactersto words: the turning point of BPE merges. In Pro-ceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 34543468, Online.Association for Computational Linguistics. Xuanli He, Gholamreza Haffari, and MohammadNorouzi. 2020. Dynamic programming encodingfor subword segmentation in neural machine transla-tion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages30423051, Online. Association for ComputationalLinguistics.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing": "Valentin Hofmann, Janet Pierrehumbert, and HinrichSchtze. 2021. Superbizarre is not superb: Deriva-tional morphology improves BERTs interpretationof complex words. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 35943608, Online. Association forComputational Linguistics. Valentin Hofmann, Hinrich Schuetze, and Janet Pierre-humbert. 2022. An embarrassingly simple methodto mitigate undesirable properties of pretrained lan-guage model tokenizers. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 385393,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Jean Kaddour. 2023. The minipile challenge for data-efficient language models": "Stav Klein and Reut Tsarfaty. 2020. Getting the ##lifeout of living: How adequate are word-pieces for mod-elling complex morphology? In Proceedings of the17th SIGMORPHON Workshop on ComputationalResearch in Phonetics, Phonology, and Morphology,pages 204209, Online. Association for Computa-tional Linguistics. Taku Kudo. 2018. Subword regularization: Improv-ing neural network translation models with multiplesubword candidates. In Proceedings of the 56th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 6675,Melbourne, Australia. Association for ComputationalLinguistics. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics.",
  "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,and Eduard Hovy. 2017. Race: Large-scale readingcomprehension dataset from examinations": "Hector J. Levesque, Ernest Davis, and Leora Morgen-stern. 2012. The winograd schema challenge. In 13thInternational Conference on the Principles of Knowl-edge Representation and Reasoning, KR 2012, Pro-ceedings of the International Conference on Knowl-edge Representation and Reasoning, pages 552561.Institute of Electrical and Electronics Engineers Inc.13th International Conference on the Principles of",
  "Knowledge Representation and Reasoning, KR 2012; Conference date: 10-06-2012 Through 14-06-2012": "Haoran Lian, Yizhe Xiong, Jianwei Niu, Shasha Mo,Zhenpeng Su, Zijia Lin, Peng Liu, Hui Chen, andGuiguang Ding. 2024. Scaffold-bpe: Enhancing bytepair encoding with simple and effective scaffold to-ken removal. Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Na-man Goyal, Marjan Ghazvininejad, Luke Zettle-moyer, and Madian Khabsa. 2023. XLM-V: Over-coming the vocabulary bottleneck in multilingualmasked language models. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1314213152, Singapore.Association for Computational Linguistics. Tomasz Limisiewicz, Jir Balhar, and David Marecek.2023. Tokenization impacts multilingual languagemodeling: Assessing vocabulary allocation and over-lap across languages. In Findings of the Associationfor Computational Linguistics: ACL 2023, pages56615681, Toronto, Canada. Association for Com-putational Linguistics. Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,Colin Raffel, Manan Dey, Matthias Gall, Arun Raja,Chenglei Si, Wilson Y. Lee, Benot Sagot, and Sam-son Tan. 2021. Between words and characters: Abrief history of open-vocabulary modeling and tok-enization in nlp.",
  "Tomas Mikolov,Ilya Sutskever,Anoop Deoras,Hai Son Le, Stefan Kombrink, and Jan HonzaCernock. 2011.Subword language model-ing with neural networks.Preprint availableat:": "Anselmo Peas, Eduard Hovy, Pamela Forner, lvaroRodrigo, Richard Sutcliffe, and Roser Morante. 2013.Qa4mre 2011-2013: Overview of question answer-ing for machine reading evaluation. In CLEF 2013,LNCS 8138, pages 303320. Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita.2020. BPE-dropout: Simple and effective subwordregularization. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 18821892, Online. Association forComputational Linguistics. Jonne Saleva and Constantine Lignos. 2023.Whatchanges when you randomly choose BPE merge op-erations? not much. In Proceedings of the FourthWorkshop on Insights from Negative Results in NLP,pages 5966, Dubrovnik, Croatia. Association forComputational Linguistics.",
  "Mike Schuster and Kaisuke Nakajima. 2012. Japaneseand korean voice search. In 2012 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 51495152": "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words withsubword units. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 17151725,Berlin, Germany. Association for Computational Lin-guistics. Jasdeep Singh, Bryan McCann, Richard Socher, andCaiming Xiong. 2019. BERT is not an interlinguaand the bias of tokenization. In Proceedings of the2nd Workshop on Deep Learning Approaches forLow-Resource NLP (DeepLo 2019), pages 4755,Hong Kong, China. Association for ComputationalLinguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Omri Uzan, Craig W. Schmidt, Chris Tanner, and YuvalPinter. 2024. Greed is all you need: An evaluation oftokenizer inference methods. In Proceedings of the62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages813822, Bangkok, Thailand. Association for Com-putational Linguistics.",
  "F Wilcoxon. 1945. Individual comparisons by rankingmethods. biom. bull., 1, 8083": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,Mohammad Norouzi, Wolfgang Macherey, MaximKrikun, Yuan Cao, Qin Gao, Klaus Macherey, JeffKlingner, Apurva Shah, Melvin Johnson, XiaobingLiu, ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,Taku Kudo, Hideto Kazawa, Keith Stevens, GeorgeKurian, Nishant Patil, Wei Wang, Cliff Young, JasonSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,Greg Corrado, Macduff Hughes, and Jeffrey Dean.2016. Googles neural machine translation system:Bridging the gap between human and machine trans-lation. Shaked Yehezkel and Yuval Pinter. 2023. Incorporatingcontext into subword vocabularies. In Proceedingsof the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pages623635, Dubrovnik, Croatia. Association for Com-putational Linguistics. Vilm Zouhar, Clara Meister, Juan Gastaldi, Li Du,Mrinmaya Sachan, and Ryan Cotterell. 2023a. Tok-enization and the noiseless channel. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 51845207, Toronto, Canada. Association forComputational Linguistics. Vilm Zouhar, Clara Meister, Juan Gastaldi, Li Du, TimVieira, Mrinmaya Sachan, and Ryan Cotterell. 2023b.A formal perspective on byte-pair encoding. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 598614, Toronto, Canada.Association for Computational Linguistics.",
  "AExpanded description of PATHPIECE": "This section provides a self-contained explanationof PATHPIECE, expanding on the one in 3, withadditional details on the vocabulary constructionand complexity.In order to design an optimal vocabulary V, it isfirst necessary to know how the vocabulary will beused to tokenize. There can be no best vocabularyin the abstract. Thus, we first present a new losslesssubword tokenizer PATHPIECE. This tokenizationover our training corpus will provide the context todesign a coherent vocabulary.",
  "A.1Tokenization for a given vocabulary": "We work at the byte level, and require that all 256single byte tokens are included in any given vocab-ulary V. This avoids any out-of-vocabulary tokensby falling back to single bytes in the worst case.Tokenization can be viewed as a compressionproblem, where we would like to tokenize text in afew tokens as possible. This has direct benefits, asit allows more text to fit in a given context window.A Minimum Description Length (MDL) argumentcan also be made that the tokenization using thefewest tokens best describes the data, although wesaw in Subsection 6.1 this may not always hold inpractice.Tokenizers such as BPE and WordPiece makegreedy decisions, such as choosing which pair ofcurrent tokens to merge to create a new one, whichresults in tokenizations that may use more tokensthan necessary. In contrast, PATHPIECE will findan optimal tokenization by finding a shortest paththrough a Directed Acyclic Graph (DAG). Infor-mally, each byte i of training data forms a nodein the graph, and there is an edge if the w bytesequence ending at i is a token in V.An implementation of PATHPIECE is given inAlgorithm 2, where input d is a text document of n bytes, V is a given vocabulary, and L is a limiton the maximum width of a token in bytes. Ithas complexity O(nL), following directly from thetwo nested for-loops. It iterates over the bytesi in d, computing 4 values for each. It computesthe shortest path length pl[i] in tokens up to andincluding byte i, the width wid[i] of a token withthat shortest path length, and the solution countsc[i] of optimal solutions found thus far with thatshortest length. We also remember the valid tokensof width 2 or more ending at each location i in vt[i],which will be used in the next section.There will be multiple tokenizations with thesame optimal length, so some sort of tiebreaker isneeded. The longest token or a randomly selectedtoken are obvious choices. We have presented therandom tiebreaker method here, where a randomsolution is selected in a single pass in lines 29-32of the listing using an idea from reservoir sampling(Vitter, 1985).A backward pass through d constructs the opti-mal tokenization from the wid[e] values from theforward pass.",
  "A.2.1Vocabulary Initialization": "We will build an optimal vocabulary by startingfrom a large initial one, and sequentially omittingbatches of tokens. We start with the most frequentlyoccurring byte n-grams in a training corpus, ofwidth 1 to L, or a large vocabulary trained by BPEor Unigram. We then add any single byte tokensthat were not already included, making room bydropping the tokens with the lowest counts. In ourexperiments we used an initial vocabulary size of|V| = 218 = 262, 144.",
  "A.2.2Increase from omitting a token": "Given a PATHPIECE tokenization t1, . . . , tKd,d C for training corpus C, we would like toknow the increase in the overall length of a tok-enization K = d Kd from omitting a given tokent from our vocabulary, V \\{t} and recomputing thetokenization. Tokens with a low increase are goodcandidates to remove from the vocabulary (Kudo,2018). However, doing this from scratch for each twould be a very expensive O(nL|V|) operation.We make a simplifying assumption that allowsus to compute these increases more efficiently. Weomit a specific token tk in the tokenization of docu-ment d, and compute the minimum increase MIkd",
  "k=1|tk=tMIkd.(7)": "This is similar to computing the increase from V \\{t}, but ignores interaction effects from havingseveral occurrences of the same token t close toeach other in a given document.With PATHPIECE, it turns out we can computethe minimum increase in tokenization length with-out actually recomputing the tokenization. Anytokenization not containing tk must either containa token boundary somewhere inside of tk breakingit in two, or it must contain a token that entirelycontains tk as a superset. Our approach will be toenumerate all the occurrences for these two cases,and to find the minimum increase MIkd overall. Before considering these two cases, there is ashortcut that often tells us that there would be noincrease due to omitting tk ending at index e. Wecomputed the solution count vector sc[e] when run-ning Algorithm 2. If sc[e] > 1 for a token endingat e, then the backward pass could simply selectone of the alternate optimal tokens, and find anoverall tokenization of the same length.Let tk start at index s and end at index e, inclu-sive. Remember that path length pl[i] representsthe number of tokens required for shortest pathup to and including byte i. We can also run Algo-rithm 2 backwards on d, computing a similar vectorof backwards path lengths bpl[i], representing thenumber of tokens on a path from the end of the dataup to and including byte i. The overall minimumlength of a tokenization with a token boundary afterbyte j is thus:",
  "MIbkd =minj=s,...,e1 Kbj pl[n].(9)": "Each token tk will have no more than L 1 poten-tial internal breaks, so the complexity of computingMIbkd is O(L).The minimum increase from omitting tk couldalso be on a tokenization containing a strict super-set of tk. Let this superset token be tk, with start s and end e inclusive. To be a strict superset jumpingover tk, we must have s < s and e e, or s sand e > e, subject to the constraint that the widthw = e s + 1 L. In this case, the minimumlength of using the superset token tk would be:",
  "Kstk = pl[s 1] + bpl[e + 1] + 1,(10)": "which is the path length to get to the byte before tk,plus the path length go backwards to the byte aftertk, plus 1 for the token tk itself.We remembered a list of the widths of the tokensending at each byte, vt[e] in Algorithm 2. The setof superset tokens S can be found by examining theO(L) potential e, and then seeing if the w vt[e]give tokens forming a strict superset. There areO(L) potential tokens ending at e in vt[e], so theoverall complexity of finding the superset tokens isO(L2)",
  "CDescription of Downstream Tasks": "To evaluate the performance of our various tok-enization experiments, we select ten competitivebenchmarks from lm-evaluation-harness(Gao et al., 2023)19, that we broadly categorizeinto three types of Question Answering (QA) tasks:Knowledge-based, Common-sense Reasoning andContext-based.Knowledge Based Tasks Knowledge basedtasks in this study expect LLMs to answer ques-tions based on domain-specific internal retrieval.Our Knowledge-based baselines in this work in-clude:SciQ: The SciQ task, proposed by Welbl et al. (2017) contains a total of 13,679 science exam ques-tions. The questions are in multiple-choice formatwith 4 answer options each. An additional text isprovided as supporting evidence for a majority ofthe answers.ARC (AI2 Reasoning Challenge): Clark et al. (2018) compiles grade-school level, multiple-choice science question dataset consists of 7,787science exam questions that are split into easyand hard sets. For this study, we employ theeasy set of 5,197 questions, each having 4 answerchoices.MathQA: Amini et al. (2019) introduce a datasetof math word problems that require LLMs to usetheir internal understanding of mathematical equa-tions and arithmetic comprehension. Similar toSciQ, this dataset consists of 37k multiple-choicequestions with the equations for each used anno-tated.HendrycksTest: Hendrycks et al. (2021) providea comprehensive suite of of multiple-choice testsfor assessing text models in multi-task contexts. Itcomprises of 57 tasks such as elementary mathe-matics, US history, law of which we use the sociol-ogy and marketing tests.Commonsense Reasoning Tasks These tasksassess the models capability to infer and reason about everyday scenarios based on implicit knowl-edge.COPA (Choice of Plausible Alternatives): COPAproposed by Brassard et al. (2022) is a benchmarkfor assessing progress in open-domain common-sense causal reasoning. It consists of 1000 ques-tions where each question is composed of a premiseand two alternatives. The task is to select the al-ternative that more plausibly has a causal relationwith the premise.PiQA (Physical Interaction Question Answer-ing): Bisk et al. (2019) introduce a task that assessthe understanding of physical commonsense by lan-guage models. Comprised of everyday situationwith a preference for atypical solutions, this datasetis formulated as multiple choice question with twopossible solutions choices for each question.Winograd Schema Challenge: Levesque et al. (2012) define a task with a pair of sentences thatdiffer only in one or two words and that contain areferential ambiguity that is resolved in oppositedirections in the two sentences. This dataset of273 tasks test language model understanding of thecontent of the text and disambiguation ability.Context Based Tasks These tasks are relianton understanding context and drawing conclusionsfrom it.RACE (Reading Comprehension from Examina-tions): RACE proposed by Lai et al. (2017) is acollection of English questions set aside to Chi-nese school students. Each item is divided into twoparts, a passage that the student must read and aset of 4 potential answers, requiring extraction andreasoning capabilities.QA4MRE (Question Answering for MachineReading Evaluation): QA4MRE by Peas et al.(2013) is a benchmark designed to resolve readingcomprehension challenges. This task focuses onreading of single documents and identifying theanswers to a set of questions. Questions are in theform of multiple choice with one correct option.Our goal was to select tasks where a 350M pa-rameter model could do significantly better thanrandom chance, avoiding evaluation right at thenoisier random threshold. We started with the tasksthat had a non-zero random score (indicating mul-tiple choice), and then chose tasks where BPE ata vocabulary size 40,960 could do well above ran-dom. In the end, the average accuracy across mod-els was more than 15% above random on all tasks.Note that in results tables we have shortenedthe name hendrycksTest-marketing to market-",
  "DEffect of model convergence": "Each model was trained on around 200 billion to-kens. gives a plot of the average accuracyfor PathPieceL with a BPE initial vocabulary and avocabulary size of 40,960 at various checkpoints inthe language model training process. It also showscheckpoints for the larger 1.3B and 2.4B modelsdiscussed in the Limitations section. With the ex-ception of the 100k checkpoint at 1.3B, the modelappears to be continually improving. It is unclearwhy the 100k checkpoint did so poorly. 20k40k60k80k100k",
  "E.1Segmentation": "Tokenizers often use the segmentation strategy thatis used in vocabulary construction. However, anyvocabulary can also be used with PATHPIECE andwith the greedy left-to-right segmentation methods.We find that BPE works quite well with greedysegmentation (overall rank 4, insignificantly differ-ent from the top rank), but not with the shortest-path segmentation of PATHPIECEL (13).Unigram, on the other hand, seems to be moretightly tied to its default maximum likelihood seg-mentation (2), which was significantly better thanboth Greedy (7) and PATHPIECEL (17).",
  "FRandTrain": "None of our experiments completely isolate the ef-fect of the vocabulary construction step. We createda new baseline random vocabulary construction ap-proach, RandTrain, in an attempt to do so. It ismeant to work with a top-down method like SaGe or PathPieceL, and uses the same initial vocabu-lary, pre-tokenization, and segmentation as eitherof those, with a simple vocabulary constructionalgorithm.We compute a count for each token in the vo-cabulary. For the top n-gram initial vocabulary itis simply the n-gram count from the training cor-pus. For a BPE initial vocabulary we tokenizedthe training corpus with BPE and the large initialvocabulary, and then use the occurrence counts ofeach token. We normalize these counts into targetselection probabilities pk for token tk.The RandTrain vocabulary construction processis simply to randomly sample our desired vocabu-lary size m of tokens from the initial vocabulary,proportionally to pk, without replacement. Sam-pling without replacement is necessary to avoidhave duplicate words in the vocabulary. Interest-ingly, this is not possible if there are any pk >1/m, which are termed infeasible or overweightitems (Efraimidis, 2010). The intuition behind thisis when selecting m items without replacement, itis not possible to select a given item more than once.So even if an item is always selected in a sample,the selection probability will be pk = 1/m.We sampled without replacement using the A-ES Algorithm described in Efraimidis (2010). Asignificant number the most common tokens in thevocabulary were infeasible and hence were unableto reach their target pk. A token with a higher pkis more likely to be sampled than a token with alower one, but they may significantly differ fromtheir target pk.We build 6 RandTrain models with 3 differenttypes of pre-tokenization, and with Greedy seg-mentation to compare to SaGe, and PathPieceLsegmentation to compare to PathPieceL. We onlyused a single vocabulary size of 40,960, so p-valuesare only computed on the 10 task accuracies, ratherthan the 30 used elsewhere. Task level accuraciesare given in and in Appendix G.Before comparing RandTrain to SaGe and Path-PieceL, we will compare our RandTrain runs toeach other, with different segmentation approaches.In and we have pairs of Rand-Train runs that only vary by the segmentationmethod.In line with Subsection E.1, Greedy performssignificantly better than PathPieceL segmentationin all 3 cases. However, for the two cases withan n-gram initial vocabulary the PathPieceL seg-mentation did extremely poorly. The RandTrain GreedyPathPieceL",
  ": Comparison of Greedy and PathPieceLsegmentation, with RandTrain vocabulary construction,n-gram initial vocab, and FirstSpace pre-tokenization,p=0.00195": "vocabulary construction, n-gram initial vocabulary,and PathPieceL segmentation interact somehow togive accuracies well below any others.This makes the comparison of RandTrain to Path-PieceL less informative. We can see in that PathPieceL is significantly better than Rand-Train with a BPE initial vocabulary.However, the other two comparisons in are are not that meaningful. They aresignificantly better, but that is more about the weakbaseline of RandTrain with PathPieceL segmenta-tion than anything positive about PathPieceL.The remaining comparison between SaGe andRandTrain is more interesting. In and SaGe was not significantly better thanRandTrain, with a p-value of 0.0645.The cases is even worse for the two n-gram ini-tial vocabulary cases. In the p-value wasa 0.688, and in RandTrain was actuallybetter, although not significantly.We saw in that both PathPieceL-BPE andSaGe-BPE are effective tokenizers. In attemptingto isolate the benefit from the vocabulary construc-tion step, we see that PathPieceL-BPE outperformsour simple baseline. However, SaGe was unableto outperform the baseline, perhaps implying thatRandTrain may actually be a simple but fairly ef-fective vocabulary construction method. GreedyPathPieceL",
  "GDetailed Experimental Results": "This section gives the detailed accuracy results forthe 10 downstream evaluation tasks on each modelthat was trained. The tables are divided by thevocabulary size used, with and for32,768; and for 40,960; and and for 49,152. The highest value or values(in the case of ties) are shown in bold. show the same results as , but are sortedfrom best to worst by rank. The corpus token count(CTC), Rnyi efficiencies, and average accuraciesfor the 54 runs in are given in .The detailed accuracy results for our 1.3B param-eter models, which were all performed at a singlevocabulary size of 40,960, are given in and . Average accuracy results for largermodels of 1.3B and 2.4B parameters are given in. See 7 for more discussion of this table. PathLRandTrain"
}