{
  "Abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate strong general-izability across various NLP tasks.Fine-tuning these models for specific tasks typi-cally involves updating all parameters, whichis resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the pop-ular LoRA family, introduce low-rank ma-trices to learn only a few parameters effi-ciently. However, during inference, the productof these matrices updates all pre-trained pa-rameters, complicating tasks like knowledgeediting that require selective updates.Wepropose a novel PEFT method, which con-ducts row and column-wise sparse low-rankadaptation (RoseLoRA), to address this chal-lenge. RoseLoRA identifies and updates onlythe most important parameters for a specifictask, maintaining efficiency while preservingother model knowledge. By adding a sparsityconstraint on the product of low-rank matricesand converting it to row and column-wise spar-sity, we ensure efficient and precise model up-dates. Our theoretical analysis guarantees thelower bound of the sparsity with respective tothe matrix product. Extensive experiments onfive benchmarks across twenty datasets demon-strate that RoseLoRA outperforms baselines inboth general fine-tuning and knowledge editingtasks.",
  "Introduction": "Pre-trained language models, trained on extensiveand diverse general-domain corpora, exhibit robustgeneralization capabilities, benefiting various natu-ral language processing (NLP) tasks, such as natu-ral language understanding (Kenton and Toutanova,2019; Liu et al., 2019) and generation (Touvronet al., 2023; Ouyang et al., 2022). To further adaptthese pre-trained models to a specific downstreamtask, fine-tuning is typically performed. However,these models often comprise numerous parameters,rendering full fine-tuning resource-intensive. To address this challenge, parameter-efficientfine-tuning (PEFT) methods (Ding et al., 2023b;Han et al., 2024) are proposed. These method in-troduce a small number of learnable parametersand update only the lightweight introduced param-eters during fine-tuning. Among existing meth-ods, LoRA family (Hu et al., 2021; Zhang et al.,2023; Ding et al., 2023b; Liu et al., 2024) hasgained remarkable popularity because of its highefficiency and good performance. Conceptually,these LoRA methods add new low-rank matrices tomodel weights for fine-tuning. Unlike other PEFTmethods such as Adapter (Houlsby et al., 2019),LoRA family does not modify the model architec-ture and is easier to incorporate.LoRA family has demonstrated notable perfor-mance on tasks, such as commonsense reasoningand arithmetic reasoning (Hu et al., 2023; Liu et al.,2024), that mainly rely on a language modelsability to understand and generate text without re-quiring to modify its internal knowledge explicitly.However, some specialized tasks require updatingthis internal knowledge. For instance, in knowl-edge editing (Zhang et al., 2024; De Cao et al.,2021), a language model should incorporate newprovided knowledge while preserving other exist-ing knowledge simultaneously. On such tasks, theLoRA family of methods are less-suited due tothe coarse-grained control they offer. In particular,the product of the low-rank matrices introduced byLoRA methods is a dense matrix, which is addedto the pre-trained model weights during inference.Consequently, all pre-trained parameters are up-dated, making it challenging to selectively modifyspecific internal knowledge. This motivates a natu-ral question: Is there a PEFT method that can beeffectively employed for tasks that require editingthe internal knowledge of language models?To answer this question, we propose a rowand column-wise sparse low-rank adaptationmethod (RoseLoRA). The motivation is to identify and update only the most important and influentialparameters in the pre-trained model concerning aspecific task. In this way, the pre-trained modelcan be updated effectively with minimal impactson knowledge that does not require modification.Specifically, RoseLoRA inherits the structure ofLoRA to enable parameter-efficient fine-tuning. Toselectively fine-tune the most important parameters,we introduce a sparsity constraint, i.e., the 0 norm,on the product of the low-rank matrices. However,this constraint is non-trivial to optimize. While0 norm constraint is widely explored in modelpruning (Zhu and Gupta, 2017; Wang et al., 2019;Sun et al., 2023), these methods can only addressthe sparsity constraint on each low-rank matrix in-dividually. Unfortunately, even if each low-rankmatrix is sparse, this does not guarantee that theirproduct will be sparse. To overcome this challenge,we propose converting the original sparsity con-straint to row and column-wise sparsity constraintson two low-rank matrices (i.e., B and A in LoRA).We provide a theoretical lower bound of the spar-sity of the product of the two low-rank matrices.Furthermore, we propose using a sensitivity-basedimportance score to incrementally solve the rowand column-wise sparsity constraints. Beyond knowledge editing,the proposedRoseLoRA can also be applied to other generaltasks, e.g., commonsense and arithmetic reason-ing, instruction following, and natural languageunderstanding. RoseLoRA updates the few mostimportant parameters of the model via enforcingthe row or column-wise sparsity for the low-rankmatrices , and can match or even outperform LoRAperformance with significantly fewer modified pa-rameters. The contributions are summarized as follows:1) We propose RoseLoRA, a novel PEFT methodthat detects and optimizes the most importanttask-related parameters, resulting in highly pre-cise and effective model updates while being morelightweight than existing methods. 2) We proposea novel row and column-wise sparsity constraint tocontrol the sparsity of the product of two low-rankmatrices. Additionally, we provide a theoreticalsparsity lower bound for the proposed RoseLoRA.3) We conduct extensive experiments on five bench-marks covering over twenty datasets. The exper-iments show that the proposed RoseLoRA canoutperform baselines on both general fine-tuningtasks and knowledge editing tasks.",
  "Parameter Efficient Fine-Tuning (PEFT)": "PEFT injects a small fraction of trainable parame-ters into pre-trained large language models (LLMs)to adapt them to downstream tasks. Prefix Tun-ing (Li and Liang, 2021) prepends soft tokensto the input and learns their continuous embed-dings while keeping the original parameters frozen.Adapter (Houlsby et al., 2019; He et al., 2021), onthe other hand, inserts lightweight bottleneck neu-ral network modules into the transformer blocks.The third paradigm, LoRA and its variants (Huet al., 2021; Zhang et al., 2023; Ding et al., 2023a;Dettmers et al., 2024; Li et al., 2023b; Liu et al.,2024), learns low-rank matrices to approximatethe desired updates of the original model weightsand has achieved state-of-the-art performance. Re-cently, ReFT (Wu et al., 2024) learns low-rank up-dates on model representations instead of weightsand achieves performance comparable to LoRAwith significantly fewer parameters. However, theunderlying linear representation hypothesis maynot hold valid (Engels et al., 2024), which greatlyundermines its generalization ability. In this work,we propose an effective method to learn sparse andlow-rank updates on model weights, demonstrat-ing superior performance using as few parametersas ReFT. Recent works such as AdaLoRA (Zhanget al., 2023) and SoRA (Ding et al., 2023a) haveapplied pruning to LoRA to increase its computa-tional efficiency. However, it is worth mentioningthat the proposed RoseLoRA is significantly differ-ent from these methods. In particular, these worksprunes to control the rank of learned model updates,but the updates are still dense in the sense that allparameters are affected, and cannot offer preciseupdates as RoseLoRA thereof.",
  "Knowledge Editing": "Knowledge editing seeks to update outdated knowl-edge in pre-trained LLMs to accommodate a dy-namic world. Early efforts involved fine-tuningtheir parameters directly but suffered from se-vere forgetting of original knowledge (Wang et al.,2023). For more precise editing, only a minimalamount of parameters should be updated (Wanget al., 2023). This requires sparse parameter up-dates, which proves NP-hard to solve (Natarajan,1995). As a workaround, Zhu et al. (2020) useda relaxed L2 norm constraint on the updates, and DenseSparse",
  ": The framework of proposed RoseLoRA": "Huang et al. (2023); Dong et al. (2022) limitedthe updates to feed-forward network (FFN) lay-ers based on findings that learned knowledge isoften stored in these layers (Dai et al., 2021). Forfurther refinement, the locate-and-edit paradigm(Meng et al., 2022a,b) identifies the layer storingspecific knowledge and then modifies its parame-ters. Nonetheless, (Hase et al., 2024) found thatupdating parameters other than the located onescan also achieve competitive editing performance,questioning the extent to which the more computa-tionally expensive locating process benefits editing.Alternative solutions restore to external mem-ory without updating original parameters, such asMEND (Mitchell et al., 2021), IKE (Zheng et al.,2023), and SERAC (Mitchell et al., 2022). How-ever, these methods require hard-to-access data toretrieve from (e.g., IKE) or to train extra modelson (e.g., MEND and SERAC), which limits theirpracticality. Recently, LoRA has also been appliedfor knowledge editing (Wu et al., 2023). However,they do not provide the aforementioned sparsityguarantee, which will be discussed shortly in thenext section, so they are less effective and showunsatisfactory performance (Zhang et al., 2024).3Preliminary",
  "Sensitivity-based Importance Score forPruning": "Importance-aware pruning (Sanh et al., 2020; Hanet al., 2015; Molchanov et al., 2019; Zhang et al.,2022; Li et al., 2023c) aims to identify and set re-dundant model weights to zero based on estimatedimportance scores. Parameters with high impor-tance scores are retained, while others are set tozero. Sensitivity (Sanh et al., 2020; Molchanovet al., 2019; Li et al., 2023c) is a popular impor-tance metric that measures the approximate changein training loss when setting a parameter to zero.Formally, the sensitivity with respect to weightWij is defined by the product of the weight and itscorresponding gradient:",
  "d1d2 ,(5)": "where is the sparsity threshold. However, Eqn. 5is challenging to handle, with difficulty lie in two-fold. First, the 0 optimization is NP-hard. Despitethat some effective approximate solutions havebeen proposed (Zhu and Gupta, 2017; Wang et al.,2019; Sun et al., 2023), they cannot be applieddirectly. In particular, due to the complex product-based parameterization, it is extremely hard to learnparameters in A, B even if we know which entriesin their product BA should be 0. Furthermore,simply controlling the sparsity of B and A maynot work, as shown in Example 1. Example 1. Let s() represent the sparsity (i.e.,the portion of zero entries) of a vector or matrix.For sparse matrix A = [a; 0(r1)d2] and B =[b, 0d1(r1)], where a and b contains non-zeroentries, we have s(A) = s(B) =r1",
  "rthat isreasonably large for r > 1. However, s(BA) =s(ba) = 0, i.e., the product is a dense matrix": "To summarize, it is non-trivial to incorporatesparsity in LoRA. To address this challenge, wepropose controlling the sparsity of each row of Aand each column of B. In this way, the sparsity ofBA can be bounded by s(Ai) and s(Bi). Wepresent the theoretical analysis in Proposition 1and the empirical results in . Based on thisfinding, we can convert the optimization problemin Eqn. 5 as the following problem:",
  "where T is the number of total training iterations,and ti, tf are hyper-parameters.5Experiment": "In the experiments, we evaluate the proposedRoseLoRA and answer the following questions:RQ1) How does the proposed RoseLoRA ben-efit knowledge editing tasks? RQ2) How doesRoseLoRA perform compared to state-of-the-artPEFT methods on general tasks?RQ3) Doesthe proposed RoseLoRA alleviate the model for-getting issue? RQ4) How does the performancechange with varying amounts of training data?",
  "Datasets and Experiment Settings": "Datasets.We conduct experiments on five dif-ferent benchmarks: 1) Knowledge Editing, in-cluding WikiDatarecent, WikiDatacounterfact (Cohenet al., 2024), ZsRE (Yao et al., 2023), and Wik-iBio (Hartvigsen et al., 2024); 2) CommonsenseReasoning, including BoolQ (Clark et al., 2019),PIQA (Bisk et al., 2020), SIQA (Sap et al.,2019), HellaSwag (Zellers et al., 2019), Wino-Grande (Sakaguchi et al., 2021), ARC-e, ARC-c(Clark et al., 2018), and OBQA (Mihaylov et al.,2018); 3) Arithmetic Reasoning, including AQuA(Ling et al., 2017), GSM8K (Cobbe et al., 2021),MAWPS (Koncel-Kedziorski et al., 2016), andSVAMP (Patel et al., 2021); 4) Instruction Follow-ing with Ultrafeedback (Cui et al., 2023) as trainingdata and evaluation on Alpaca-Eval v1.0 (Li et al.,2023a); 5) Natural Language Understandingconsists of eight datasets from the GLUE bench-mark (Wang et al., 2018).More details aboutdatasets, metrics, and hyper-parameters we use canbe found in the Appendix.",
  "Baselines.Our baselines are constructed on atask basis. In specific, on each task the proposedRoseLoRA is compared with representative base-lines from corresponding domain as listed below": "On Knowledge Editing, we follow Zhanget al. (2024) and choose AdaLoRA (Zhanget al., 2023), ROME and FT-L (Meng et al.,2022a), and MEMIT (Meng et al., 2022b)as our baselines as they, same as us, do notrequire hard-to-access data or training addi-tional models. In specific, AdaLoRA keepsunimportant weights in an LLM unchangedand achieves a highly efficient and precisePEFT. ROME applies a causal-tracing to iden-tify the layer wherein the knowledge is storedand then learns a rank-one update. FT-L, on",
  "the other hand, directly finetunes the layeridentified by ROME. Recently, MEMIT ex-tends ROME to a large-scale setting, wherethe edits can be made more efficiently": "On the other four tasks, we follow the setupfrom existing works (Hu et al., 2023; Liu et al.,2024; Wu et al., 2024) that evaluated a vari-ety of representative PEFT methods includingprefix tuning (Li and Liang, 2021), adapters(Houlsby et al., 2019), LoRA and its recentvariants (Hu et al., 2021; Zhang et al., 2023),and ReFT (Wu et al., 2024). Due to pagelimitation we refer the readers to Hu et al.(2023); Wu et al. (2024) and reference thereinfor more details.",
  "Performance Comparison": "Knowledge EditingWhen performing knowl-edge editing, we introduce an additional norm con-straint for low-rank matrices, as detailed in theAppendix. The results of knowledge editing arepresented in , addressing RQ1. From thistable, we observe that the proposed RoseLoRAoutperforms all state-of-the-art baselines in termsof average performance, achieving the highest editsuccess rate while preserving the most knowledgethat should not be updated. Moreover, RoseLoRAdemonstrates excellent generalization ability, asindicated by its high portability score which is ametric to measure if the edited model can reasoncorrectly about the updated knowledge. Commonsense ReasoningIn this section, wepresent experiments on eight commonsense reason-ing datasets to address RQ2, as shown in .The table indicates that the proposed RoseLoRAagain outperforms all state-of-the-art parameter-efficient fine-tuning methods on average. Amongthe eight datasets, RoseLoRA ranks the first infive cases. Remarkably, its parameter numbersare the same as that of LoReFT, significantlysmaller than PrefT, Adapter, LoRA, and DoRA.Yet, RoseLoRA still achieves higher accuracy onthe commonsense reasoning datasets. This clearlydemonstrates RoseLoRAs effectiveness of fine-tuning the most crucial parameters of LLaMA forcommonsense reasoning tasks.",
  "Arithmetic ReasoningIn this section,wepresent experiments on four arithmetic reasoningdatasets to address RQ2, with results shown in Ta-ble 3. The table indicates that LoRA achieves the": ": Performance comparison of LLaMA-7b-chat against existing knowledge editing methods on four knowledgeediting datasets. Results marked with \"\" are taken from Zhang et al. (2024). \"AVG\" means the average of editsuccess, locality, portability, and fluency. Because fluency is not at the same magnitude as other metrics, we leverage\"fluency/10\" when computing AVG values.",
  "AVG()62.372.267.969.284.6": ": Accuracy comparison of LLaMA-7B against PEFT baselines on eight commonsense reasoning datasets.Results marked with \"\" are taken from Liu et al. (2024). \"AVG\" means the average accuracy of all datasets. ForRoseLoRA, Params (%) is calculated by dividing the number of final low-rank matrices parameters by the numberof parameters of the base LMs (number of low-rank matrix parameters times sparsity).",
  "RoseLoRA0.03%71.084.975.592.682.684.670.084.280.7": "highest average accuracy across the four datasets.However, the proposed RoseLoRA performs com-parably, retaining 97% of LoRAs accuracy whileupdating only 22 times less parameters comparedwith LoRA. Additionally, compared to LoReFT,RoseLoRA updates a similar number of parame-ters while achieving approximately a 6.3% perfor-mance improvement. Instruction FollowingIn this section, we com-pare the proposed RoseLoRA with state-of-the-artbaselines on the instruction-following task. To en-sure fair comparisons, we use the same prompttemplates from Taori et al. (2023). The model per-formance is shown in . Based on the table, itcan be observed that the proposed RoseLoRA out-performs all baseline methods while updating the : Accuracy comparison of LLaMA-7B against PEFT baselines on four arithmetic reasoning datasets. Resultsmarked with \"\" are taken from Hu et al. (2023). \"AVG\" means the average accuracy of all datasets.",
  "RoseLoRA0.03%26.033.079.844.745.9": "fewest parameters. Additionally, for the instruction-following task, we find that significantly fewerparameters need to be updated compared to com-monsense reasoning and arithmetic reasoning tasks.This suggests that fewer parameters are related tothe instruction-following ability in the large lan-guage model. : Performance comparison of LLaMA-2 7B oninstruction tuning task on Alpaca-Eval v1.0. We com-pute the win-rate against text-davinci-003 using GPT-4as the annotator. Results marked with \"\" are takenfrom Wu et al. (2024).",
  "Llama-2 7B & RoseLoRA0.0037%85.77": "Natural Language UnderstandingWe conductexperiments on the GLUE to answer RQ2. Weshow the model performance in . Accordingto the table, the proposed RoseLoRA outperformsthe state-of-the-art baselines significantly. The bestbaseline LoRA achieves 88.1 average accuracy butthe proposed RoseLoRA reaches about 89.0 ac-curacy on the eight datasets averagely. On RTEdataset, the proposed RoseLoRA even achieves3.4% performance improvement. Compared tofully fine-tuning, the proposed RoseLoRA alsoachieves better performance. The potential reasonmay be that RoseLoRA only updates very fewparameters and prevents overfitting on natural lan-",
  "Forgetting Test": "In this section, we study if a fine-tuned model for-gets knowledge learned from the pre-training stageto answer RQ3. To make fair comparisons, we eval-uate LoRA and RoseLoRA after fine-tuning onCommonsense170K, Ultrafeedback, and Math10Kin a zero-shot setting and using the same prompttemplates. We report the experiment results in.According to the table, we can findthat compared to LoRA, the RoseLoRA forgetsless knowledge after fine-tuning. For example, af-ter fine-tuning on the Commonsense170K dataset,LoRA leads to a significant performance drop onTriviaQA and MMLU. However, the proposedRoseLoRA still preserves over 90% performanceof LLaMA-2. Besides, we can also find that bothLoRA and RoseLoRA achieve good performanceon ARC-c dataset. It may indicate that fine-tuninglarge language models on Commonsense170K, Ul-trafeedback, or Math10K may not make them for-get much general knowledge.",
  "Sensitivity w.r.t. Training Data Size": "In this section, we study how the model perfor-mance changes with different amounts of trainingdata. We show the experiment results in .Based on the figure, we can find that with the de-creasing amounts of training data, the performancegap between LoRA and RoseLoRA is becomingsmaller. When using only 12.5% Math10K dataas the training data to fine-tune the LLaMA 7B,RoseLoRA even outperforms LoRA on GSM8K.In conclusion, the proposed RoseLoRA showsmore superiority on small data scenarios.",
  "Conclusion": "In this paper, we address the limitations of existingparameter-efficient fine-tuning (PEFT) methods,particularly the LoRA family, in handling tasksrequiring selective knowledge updates while stillbeing effective for other general NLP tasks. Weintroduced a novel method, row and column-wise sparse low-rank adaptation (RoseLoRA), whichselectively updates the most important parametersfor specific tasks, maintaining efficiency while min-imizing unnecessary changes to the pre-trainedmodels knowledge. RoseLoRA applies a row andcolumn-wise sparsity constraint to the product oflow-rank matrices, ensuring efficient updates with-out modifying the model architecture. Our theoret-ical analysis lower bounds the sparsity of productmatrices that affect models knowledge, and oursensitivity-based importance scoring effectivelyfulfilled the sparsity constraints. Through exten-sive experiments on five benchmarks encompassingover twenty datasets, RoseLoRA demonstrated su-perior performance on both general-purposed fine-tuning and knowledge editing tasks compared toexisting methods. This highlights its potential as arobust and efficient fine-tuning solution for a widerange of NLP applications.",
  "Acknowledgement": "This work is supported in part by the US NationalScience Foundation under grant NSF IIS-1747614and NSF IIS-2141037. Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the author(s) and do not nec-essarily reflect the views of the National ScienceFoundation. ReferencesYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. arXiv preprintarXiv:1905.10044. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,and Mor Geva. 2024. Evaluating the ripple effectsof knowledge editing in language models. Transac-tions of the Association for Computational Linguis-tics, 12:283298. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, andMaosong Sun. 2023. Ultrafeedback: Boosting lan-guage models with high-quality feedback.arXivpreprint arXiv:2310.01377.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, andLuke Zettlemoyer. 2024. Qlora: Efficient finetuningof quantized llms. Advances in Neural InformationProcessing Systems, 36": "Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen,Bowen Zhou, Zhiyuan Liu, and Maosong Sun. 2023a.Sparse low-rank adaptation of pre-trained languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 41334145. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei,Zonghan Yang, Yusheng Su, Shengding Hu, YulinChen, Chi-Min Chan, Weize Chen, et al. 2023b.Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelli-gence, 5(3):220235.",
  "Joshua Engels, Isaac Liao, Eric J Michaud, Wes Gurnee,and Max Tegmark. 2024. Not all language modelfeatures are linear. arXiv preprint arXiv:2405.14860": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation.",
  "Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang,et al. 2024. Parameter-efficient fine-tuning for largemodels: A comprehensive survey. arXiv preprintarXiv:2403.14608": "Tom Hartvigsen, Swami Sankaranarayanan, HamidPalangi, Yoon Kim, and Marzyeh Ghassemi. 2024.Aging with grace: Lifelong model editing with dis-crete key-value adaptors. Advances in Neural Infor-mation Processing Systems, 36. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghan-deharioun. 2024. Does localization inform editing?surprising differences in causality-based localizationvs. knowledge editing in language models. Advancesin Neural Information Processing Systems, 36. Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, BoshengDing, Liying Cheng, Jia-Wei Low, Lidong Bing,and Luo Si. 2021. On the effectiveness of adapter-based tuning for pretrained language model adapta-tion. arXiv preprint arXiv:2106.03164. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational conference on machine learning, pages27902799.",
  "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,Ishaan Gulrajani, Carlos Guestrin, Percy Liang, andTatsunori B. Hashimoto. 2023a. Alpacaeval: Anautomatic evaluator of instruction-following models": "Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, NikosKarampatziakis, Weizhu Chen, and Tuo Zhao. 2023b.Loftq: Lora-fine-tuning-aware quantization for largelanguage models. arXiv preprint arXiv:2310.08659. Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang,Pengcheng He, Weizhu Chen, and Tuo Zhao. 2023c.Losparse: Structured compression of large languagemodels based on low-rank and sparse approximation.In International Conference on Machine Learning,pages 2033620350. PMLR.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. arXiv preprintarXiv:1804.07461.",
  "Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? arXiv preprintarXiv:1905.07830": "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang,Shumin Deng, Mengru Wang, Zekun Xi, ShengyuMao, Jintian Zhang, Yuansheng Ni, et al. 2024. Acomprehensive study of knowledge editing for largelanguage models. arXiv preprint arXiv:2401.01286. Qingru Zhang, Minshuo Chen, Alexander Bukharin,Pengcheng He, Yu Cheng, Weizhu Chen, andTuo Zhao. 2023.Adaptive budget allocation forparameter-efficient fine-tuning. In International Con-ference on Learning Representations. Qingru Zhang, Simiao Zuo, Chen Liang, AlexanderBukharin, Pengcheng He, Weizhu Chen, and TuoZhao. 2022. Platon: Pruning large transformer mod-els with upper confidence bound of weight impor-tance. In International conference on machine learn-ing, pages 2680926823. PMLR.",
  "We conduct experiments on five different bench-marks:": "Knowledge editing consists of four datasets, in-cluding WikiDatarecent, WikiDatacounterfact (Co-hen et al., 2024), ZsRE (Yao et al., 2023), andWikiBio (Hartvigsen et al., 2024). For the knowl-edge editing tasks, the model should memo-rize new knowledge while preserving knowledgewhich does not need to update. Following Zhanget al. (2024), we use four metrics to evaluate theediting performance: 1) Edit Success, whichestimates the accuracy with respect to both theknowledge needed to be updated and the simi-lar expressions of the knowledge, 2) Locality,which shows if the post-edited model keeps itsoriginal answer on the locality set, 3) Porta-bility, which is to measure if the post-editedmodel can reason correctly about the updatedknowledge, and 4) Fluency, which measures themodels generation ability after editing via cal-culating the weighted average of bi-gram andtri-gram entropies. Commonsense reasoning contains of eightdatasets, including BoolQ (Clark et al., 2019),PIQA (Bisk et al., 2020), SIQA (Sap et al.,2019), HellaSwag (Zellers et al., 2019), Wino-Grande (Sakaguchi et al., 2021), ARC-e, ARC-c (Clark et al., 2018), and OBQA (Mihaylovet al., 2018). These tasks are multiple choiceproblems. Following Hu et al. (2023); Wu et al.(2024), we fine-tune the LLM on a combinedtraining dataset named Commonsense170K ofthese tasks and evaluate the Accuracy on indi-vidual test sets.",
  "SST-2Accuracy2e-432MRPCAccuracy2e-432QQPAccuracy1e-432STS-BPearson corr2e-432MNLIAccuracy2e-432QNLIAccuracy2e-432RTEAccuracy6e-432": "Arithmetic reasoning consists of four math rea-soning datasets:AQuA (Ling et al., 2017),GSM8K (Cobbe et al., 2021), MAWPS (Koncel-Kedziorski et al., 2016), and SVAMP (Patel et al.,2021). Models need to generate correct answersand we use Accuracy as the evaluation metricfollowing Hu et al. (2023) as well. Again, wereplicate the setup in Wu et al. (2024) and fine-tune the models on the combined training datanamed Math10K of the four tasks. Instruction-following measures if the model canfollow human instructions. Same as before, wefollow Hu et al. (2023); Wu et al. (2024) and useUltrafeedback (Cui et al., 2023) as the trainingdata, and evaluate the model performance byAlpaca-Eval v1.0 (Li et al., 2023a)."
}