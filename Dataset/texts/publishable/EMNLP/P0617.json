{
  "Abstract": "Large Language Models (LLMs) face signifi-cant challenges at inference time due to theirhigh computational demands. To address this,we present Performance-Guided KnowledgeDistillation (PGKD), a cost-effective and high-throughput solution for production text classi-fication applications. PGKD utilizes teacher-student Knowledge Distillation to distill theknowledge of LLMs into smaller, task-specificmodels.PGKD establishes an active learn-ing routine between the student model andthe LLM; the LLM continuously generatesnew training data leveraging hard-negative min-ing, student model validation performance, andearly-stopping protocols to inform the data gen-eration. By employing a cyclical, performance-aware approach tailored for highly multi-class,sparsely annotated datasets prevalent in indus-trial text classification, PGKD effectively ad-dresses training challenges and outperforms tra-ditional BERT-base models and other knowl-edge distillation methods on several multi-classclassification datasets. Additionally, cost andlatency benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and25X less expensive than LLMs for inference onthe same classification task. While PGKD isshowcased for text classification tasks, its ver-satile framework can be extended to any LLMdistillation task, including language generation,making it a powerful tool for optimizing perfor-mance across a wide range of AI applications.",
  "*Equal contribution": "data, such as Mistral 7B (Jiang et al., 2023),LLaMA (Touvron et al., 2023), and GPT-4 (Ope-nAI et al., 2024). Compared to smaller Pre-trainedLanguage Models (PLMs) like BERT (Devlin et al.,2019) and GPT-2 (Radford et al., 2019), LLMsnot only have significantly larger model sizes butalso demonstrate stronger language understandingand generation capabilities. It has been demon-strated that LLMs can achieve considerable per-formance with limited task-specific annotated dataacross several domains, including natural languageunderstanding, question answering, and code gen-eration (Minaee et al., 2024). In the text classi-fication domain, LLMs like GPT-3 outperformsad-hoc state-of-the-art (SOTA) models on severalbenchmarks by just using few-shot prompting tech-niques (Sun et al., 2023).While the utility of LLMs is evident, their ap-plication at scale is challenged by high inferencelatency and cost. LLMs are unsuitable for pro-duction environments where Service-Level Agree-ments (SLAs) are strict and scalability is essen-tial. Furthermore, many Natural Language Pro-cessing (NLP) tasks in the industry only requirea subset of the LLMs capabilities, e.g., Intent De-tection, a multi-class classification problem thatcan be solved by fast and inexpensive PLM anddoes not require language generation capabilities.Furthermore, utilizing LLMs for text classificationrestricts the nuanced output information derivedfrom fine-tuning a PLM. PLMs, when equippedwith a classification head, produce output vectorswith predicted probabilities that reflect the modelscertainty about the classification results. However,this level of detail is not easily attainable fromLLMs. LLMs, instead, generate plain text thatrequires parsing the model output and handling hal-lucinations, complicating production systems evenfurther for simple tasks like text classification.Considering the challenges presented by LLMs",
  "Related Work": "Knowledge Distillation (KD), introduced by (Hin-ton et al., 2015), is a promising technique totransfer the capabilities of complex and high-maintenance models to more compact and efficientstudent models. The core idea is to train a leanstudent model to mimic the soft probabilities gener-ated by a more complex and costly teacher model.Using LLMs for ground truth generation andas KD teacher has found success in multipletasks (Chiang and yi Lee, 2023; Gilardi et al., 2023;Chan et al., 2023). Recent research (Gu et al., 2024)has explored novel KD with open-source LLMsand loss functions explicitly tailored for teacher-student distillation. The same work also summa-rizes the two commonly applied categories of KD:white-box KD (Gou et al., 2021), where the teacherparameters are available to use for model distilla-tion, and black-box KD, where only the teacherpredictions are accessible. black-box KD is lessrestrictive in terms of structural requirements forteacher models and student models, and can useclosed-source teacher models (such as ChatGPTAPI); additionally, it does not require the privatedeployment of teacher models and does not presentchallenges in custom loss function convergence.Most recent black-box KD methods mainly fo-cus on LLM data generation or augmentation. (Louet al., 2023) proposed AugGPT, a text data augmen-tation approach based on ChatGPT that rephraseseach sentence in the training samples into multi-ple conceptually similar but semantically differentsamples. ZeroGen (Ye et al., 2022) is a flexibleand efficient zero-shot learning method, providinginsights on data-free, model-agnostic knowledgedistillation. ZeroShotDataAug (Ubani et al., 2023)investigates ChatGPT-generated synthetic trainingdata to augment low-resource datasets, outperform-ing existing data augmentation approaches, andexplores methodologies for evaluating the qualityof the generated augmented data. SunGen (Gaoet al., 2022) optimizes synthetic data generationin black-box knowledge distillation by employingadversarial sampling and iterative refinement to",
  "create diverse, challenging data that aligns with theteacher models knowledge boundaries.Despite these advancements, the potential of": "LLMs in KD extends beyond mere data generation.Recent studies have explored dynamic interactionsbetween teacher and student models at training timefor effective KD. (Liu et al., 2024) proved the effec-tiveness of active learning for teacher-student distil-lation for LLMs and introduces the EvoKD frame-work. This framework is specifically tailored toenhance the capabilities of a student model withina few-shot learning scenario, where only a limitedquantity of training data is available.Research on text classification using KD hasbeen focused on few-shot learning scenarios, suchas 1-shot or 5-shot, where 1 to 5 training samplesare provided to the base model, typically narrowingthe task to binary classification. While this focusoffers valuable insights for theoretical exploration,it does not adequately represent the complexity oftext classification tasks encountered in industrialapplications. In a typical industrial setting, classi-fication challenges, such as intent detection, topicclassification, and customer feedback analysis, in-volve a much larger number of categories. More-over, in these settings, annotating hundreds to a fewthousand samples is often feasible and economi-cally viable, making the 1-shot or 5-shot evaluationunrealistic given the availability of annotated data.",
  "Methods": "PGKD is an iterative KD algorithm that aims to en-hance the application of KD in highly multi-class,sparsely annotated datasets common in industrialenvironments. The primary motivation for thiswork is to leverage active learning to strengthenthe connection between the student and teachermodels in the distillation process. PGKD givesthe teacher model direct and continued visibilityinto the learning status of the student model alsomaking it generically extensible to wide varietyof learning tasks. PGKD addresses limitations ofrecent works in black-box KD, in particular: (1)EvoKD only shows results on binary-classificationbenchmark datasets, a setting that is infrequent inpractical machine learning applications; (2) Ze-roGen only relies on the latent knowledge of theteacher and is not aware of the students status;the teacher model in EvoKD is also not providedwith an overview of the latest student model perfor-mance on all the available classes and only shows the LLM a few misclassified examples; while thisapproach is useful in the context of binary text clas-sification, it can be limiting in a highly multi-classcontext where many classes might not be repre-sented in the sample provided to the teacher. (3)In SunGen, distillation focuses on the capability ofthe teacher model to generate high entropy data;EvoKD too focusses on correctly classified andmisclassified samples but ignores emphasizing theinformation that is contained in the \"Hard NegativeSamples\", i.e., incorrectly classified samples whichthe student model is confident about. (4) previousworks do not provide a clear termination criterionfor the iterative algorithm but treat the number ofepochs for the knowledge distillation process asa hyperparameter that needs to be tuned for eachdataset.The proposed PGKD overcomes the limitationabove by leveraging the following techniques.(1) Gradual Evaluation Checks: At each KDstep, the student model is evaluated on the vali-dation set, and a report of student validation met-rics like Accuracy, Precision, Recall, and F1 oneach class is inserted in the teacher model prompt(Appendix A). This approach allows the teachermodel to observe the students overall performanceand helps the teacher model guide the directionof the optimization. For instance, a low accuracyon a specific class will signal the teacher model togenerate more relevant data samples for that class;this makes the KD process performance-aware andallows the teacher model to observe and directlyoptimize the student model performance. It alsoinduces automatic class imbalance handling that isdirectly based on student performance. It is impor-tant to remark that in this process, only high-levelvalidation metrics are shared with the LLM, butno validation sample is leaked. This ensures themitigation of the crucial risk of over-fitting the val-idation set during the learning process.(2) Hard Negative Mining: Hard Negative sam-ples are defined as the misclassified samples in thetraining set for which the student model is moreconfident in its decision. These samples are gen-erally the most informative ones, being the closestto the student model decision boundary. PGKDincludes the Hard Negative samples from the previ-ous training run in the teachers prompt. The LLMevaluates the student models performance deficien-cies by accessing sentence patterns likely to resultin errors. The confidence of the student model iscomputed by considering the values from the clas- sification head of the student model. This approachallows the teacher model to gain rich insights intowhich examples the student is struggling to learnand encourages the teacher to produce new samplesthat help overcome these crucial blind spots.(3) Early Stopping: To maximize the studentmodels distillation while preventing performancedrift and overfitting PGKD uses early stopping tocontrol the overall validation loss. PGKD returnsthe model having the lowest validation loss as thebest model to be used for testing.The PGKD methodology is detailed in Algo-rithm 1 and a schema of the same is reported in and .The algorithm starts by initializing and train-ing a baseline model on an initial dataset D0 com-posed of a thousand annotated samples, dividedinto 80% training and 20% validation. The basemodel obtained after this training is denoted asmodel0.PGKD iteratively refines model0 per-formance; during each iteration, the PGKD algo-rithm identifies correctly classified Dicorrect andmisclassified examples Diincorrect, as well as hardnegatives Dihard_negatives. PGKD then computesthe model val_results on the validation set Dval and leverages an LLM to generate new trainingdata Di+1 tailored to these findings accordingto PGKD_prompt. PGKD_prompt includesdataset classification taxonomy for the specific taskand a limited number of few-shot samples fromthe train set, prompt is reported in Appendix A.The PGKD process runs for num_kd_steps and ifthe validation loss does not improve consecutivelybeyond the patience_limit, the algorithm termi-nates early to prevent overfitting. At the end of thePGKD process, the best model on the validationset, modelbest is returned.",
  "Datasets and Experiments": "Our experiments focus on four multi-class classi-fication datasets: AG-news (Gulli, 2005), YahooAnswers (Ardeshna, 2020), Huffington Post (Misra,2018), and AMZN Reviews (Kashnitsky, 2018),described in . The datasets cover a widerange of number of classes, from 4 to 335. Theseexperiments aim to study the PGKD performanceat the variation of the number of classes in thedataset. The base PLM model used for PGKD isBERT-base model, as done in (Liu et al., 2024);the model has been fine-tuned with a classifica-tion head using categorical cross-entropy leverag- : PGKD process, student model is initially trained on a set of labeled samples, then the KD process starts.The LLM generates new training samples for the student model based on the students correct/misclassified samples,hard negative samples, and a report of the students validation metrics.",
  ",00010,000335": ": Detailed description of the datasets used for the PGKD experiments. Each dataset varies significantly in thenumber of classes, training samples, and testing samples, reflecting a broad range of classification challenges. ing the Hugging Face library1. A sample of 1000annotated data points is used for model_0 train-ing and validation (80% training, 20% validation).The teacher model for the experiments is Claude-3Sonnet accessed via AWS Bedrock2. Early stop-ping is applied on the validation loss for the initialBERT-base model and the PGKD methodology isthen applied to the resulting model. For robust-ness of the provided results, 5 different trainingand validation sets of 1000 samples are chosen;results are averaged across these 5 samples. ForBERT-base model_0 fine-tuning, the following pa-rameters have been set: maximum sequence lengthof 512, batch size of 64, fine-tuned the model for30 epochs with a learning rate of 2 105, andpatience parameter for early stopping at 5. PGKDfine-tuning is tasked to produce 32 new samplesat each iteration and the prompt takes as input 16samples from the training set as few-shot samplesused to guide the LLM data generation; the numberof correct samples, incorrect samples, and hard neg-ative samples is set to 16. The number of PGKDepochs is set to 10 and PGKD patience is set to 5.The purpose of the experiments is to measure theperformance lift registered by applying the PGKDroutine on top of the model_0 BERT-base model",
  "Results and Discussion": "PGKD results are presented in . PGKDexhibits a varying degree of effectiveness acrossthe datasets. AG-news (4 classes): The PGKDimplementation yields only slight improvementsin AG-news, a dataset with a reduced number ofclasses. The metrics show an increase in Accuracyfrom 0.884 to 0.895, Macro Average F1 from 0.884to 0.894, and Weighted Average F1 from 0.884 to0.894 compared to BERT-base. These modest gainslikely reflect the already high baseline performance,which limits the scope for further significant en-hancements through the PGKD method. YahooAnswers (10 classes): PGKD shows a noticeableimprovement in all measured metrics. Accuracyis enhanced from 0.649 to 0.685, Macro AverageF1 from 0.657 to 0.688, and Weighted Average F1from 0.657 to 0.688. This datasets moderate num-ber of classes indicates that PGKD is particularlyeffective in scenarios with intermediate complexity,utilizing the teacher models strengths more effec-tively. Huffington Post (41 classes): The applica-tion of PGKD in the Huffington Post dataset, witha substantially higher number of classes, also leadsto significant improvements. Accuracy improves",
  "BERT-base0.320 0.0140.074 0.0190.244 0.011BERT-base + PGKD0.443 0.0170.159 0.0120.382 0.014Claude-3 (Zero-Shot)0.416 0.0120.364 0.0160.414 0.013SOTA - 5-shot (Zhang et al., 2022)0.495--": ": Comparison of average and standard deviation for Accuracy, Macro Average F1, and Weighted AverageF1 scores for BERT-base and BERT-base enhanced with PGKD across various datasets with differing numbers ofclasses. Results are shown alongside Claude-3 zero-shot and SOTA full-shot or few-shot methods as referenced inthe literature. from 0.474 to 0.519, Macro Average F1 from 0.214to 0.330, and Weighted Average F1 from 0.411 to0.495. These improvements underscore the benefitsof PGKD in managing more complex class struc-tures and enhancing model performance. AMZNReviews (335 classes): This dataset, featuring thelargest number of classes, displays the most dra-matic improvements with PGKD. Accuracy signifi-cantly increases from 0.320 to 0.443, Macro Aver-age F1 from 0.074 to 0.159, and Weighted AverageF1 from 0.244 to 0.382. These results highlightPGKDs strong capability in handling extensive,complex class structures, particularly beneficialin contexts with scarce labeled data. We observea correlation between the number of classes andthe improvement margin achieved through PGKD.Datasets with a lower number of classes show neg-ligible improvements, while datasets with a largernumber of classes show significant performancegains. This suggests that the methodology is par-ticularly suited for production applications wheredistinguishing between a large number of classesis challenging due to the complexity and sparsityof the annotated data; this class of problems isprevalent in many industrial Machine Learning(ML) applications. Performance of the currentSOTA methodology for each dataset and perfor-mance of Claude-3 Sonnet zero-shot classificationare reported. The corresponding prompt for zero- shot classification is reported in Appendix A. Inall the reported use-cases, PGKD outperformedzero-shot Claude-3 Accuracy improving the perfor-mance gap between BERT-base and current SOTA;BERT-base + PGKD outperforms Claude-3 zero-shot on Weighted Average F1 on all datasets exceptAMZN Reviews; BERT-base + PGKD has superiorMacro Average F1 results when compared withClaude-3 on two out of four datasets, with slightlylower results on the Huffington Post dataset. Itis plausible that enhancing the prompt with com-plex few-shot techniques could elevate the LLM tomatch the current SOTA performance, as reportedin (Sun et al., 2023); however, further LLM promptexploration is beyond the scope of this work. WhilePGKD is expected to be maximally useful whenthe number of annotated data is limited, it is in-teresting to evaluate performance gains obtainedby PGKD as the number of training samples in-creases. presents the results for the AMZNReviews dataset obtained at a varying number oftraining dataset sizes, with similar trends observedacross all datasets; full results are documented inAppendix B. The PGKD approach consistently sur-passed the performance of the BERT-base model.Notably, as the number of training samples fromthe original dataset increases, the performance dif-ferential between PGKD and the original modeldecreases. This phenomenon can be attributed to",
  ": Performance of PGKD across varying training set sizes on the AMZN Reviews dataset": "the diminishing returns of new data generation bythe LLM as the volume of original dataset samplesgrows. Interestingly, while PGKD performanceimprovement reduces with the increasing numberof training samples, we can observe that the PGKDprovides the best results across all dataset consid-ered for all training sample sizes. PGKD demon-strates to not degrade model in any of the experi-ments conducted.",
  "Comparative Analysis of RelatedLiterature": "We compared PGKD with other KD strategies ontwo datasets: the IMDB Dataset (Pathi, 2018), con-taining 2 classes, and the Inshorts News V7 (Chan-der, 2021), containing 7 classes, as presentedin (Liu et al., 2024). reports the comparisonbetween PGKD, EvoKD and other baseline 1-shotclassification methods from (Liu et al., 2024). The1-shot approach discussed in (Liu et al., 2024) maybe impractical for production environments, whereaccess to larger datasets is the norm. The BERT-base model trained with 1000 samples achieves aweighted average F1 score of 0.872 on the IMDBdataset and 0.933 on the Inshorts dataset. Inter-estingly BERT-base already outperforms all the1-shot methodologies reported in (Liu et al., 2024)even without PGKD. PGKD further improves uponthis performance, achieving a weighted averageF1 score of 0.908 on the IMDB dataset and 0.943on the Inshorts dataset. The results demonstrate",
  "Ablation Study": "reports the effectiveness of specific PGKDcomponents, proving the contribution of the pro-posed methodology over a general LLM activelearning framework. Experiments have been car-ried out with the PGKD methodology by selectivelydisabling the Validation Report and Hard Negativesfeatures across different datasets. The removal ofthe Validation component (w/o Validation), whichuses performance metrics to guide the distillation",
  ": Average Accuracy on five training samples;PGKD applied to BERT-base model trained with 1000samples": "process, resulted in a decrease in accuracy acrossall datasets. Specifically, the accuracy decreasedfrom 0.895 to 0.893 on AG-news, from 0.685 to0.669 on Yahoo Answers, from 0.519 to 0.501 onHuffington Post, and more significantly from 0.443to 0.419 on Amazon Reviews. This indicates thatvalidation metrics information is useful in the dis-tillation process by making it performance-aware;as expected, this feature is particularly relevant todatasets with a high number of classes. Remov-ing Hard Negative Mining (w/o Hard Negatives),which incorporates challenging misclassified sam-ples to refine the student models decision bound-aries, led to a more subtle drop in model perfor-mance. For instance, accuracy fell from 0.895 to0.887 on AG-news, from 0.685 to 0.675 on Ya-hoo Answers, from 0.519 to 0.510 on HuffingtonPost, and from 0.443 to 0.433 on Amazon Reviews.This decline was more pronounced in datasets withcomplex classification tasks, highlighting the im-portance of hard negative samples in enhancing therobustness and accuracy of the student model inmulti-class scenarios.",
  "All LLMs are accessed via Amazon Bedrock:": "onds on an AWS m5.4xlarge instance (16 vCPUs)for $0.0046, and in 0.46 seconds on a g5.4xlargeGPU instance for $0.01. Claude Sonnet inferencetakes 60.64 seconds per batch on average, costing$0.38, for inputs averaging 1k tokens. Cheaperopen-source models like LLaMA 3 8B take 58.06seconds on average and cost $0.06 per batch. LLMsinference for the proposed classification task is ap-proximately 3X slower and 25X more expensivethan BERT-base + PGKD on a CPU instance andapproximately 130X slower and 6X more expen-sive on a GPU instance.",
  "Conclusion and Future Work": "Deploying LLMs in real-world text classificationapplications poses significant challenges due tohigh inference costs and latency. This researchintroduces PGKD, a novel methodology designedto effectively distill LLM knowledge into faster,more efficient models for multi-class text classi-fication. This study proves the substantial perfor-mance improvements of PGKD distillation com-pared to regular fine-tuning of a BERT-base PLM,while maintaining limited inference latency andcosts. Comparative analyses with existing knowl-edge distillation and augmentation strategies fur-ther underscore PGKDs practical performance en-hancements. The proposed ablation study revealsthe significant contributions of PGKD components,such as Gradual Performance Checks on ValidationReports and Hard Negative Mining. Cost and la-tency benchmarks provide compelling evidence ofPGKDs efficiency. Compared to zero-shot LLMcosts, BERT-base with PGKD has proven to be sig-nificantly more cost-effective and up to 130X fasterfor a broad spectrum of multi-class classificationtasks. Future research will investigate the impactof different teacher LLMs on the distillation pro-cess and explore the influence of the student modelsize on PGKD effectiveness. Future research willalso focus on exploring more advanced prompt-ing techniques to optimize PGKD performance.While PGKD is showcased for text classificationtasks, its versatile framework can be extended toany LLM distillation task, including language gen-eration, making it a powerful tool for optimizingperformance across a wide range of AI applica-tions.",
  "Limitations": "While PGKD has demonstrated significant im-provements in multi-class text classification tasks,there are some limitations to consider the followingaspects.(1) Dependence on LLM performance: The effec-tiveness of PGKD is inherently tied to the perfor-mance of the LLM used for knowledge distillation.If the LLM struggles with domain-specific data orfails to generate high-quality samples, it may limitthe potential gains from the distillation process.Future work could explore the impact of using dif-ferent LLMs or ensembles of LLMs to mitigate thislimitation.(2) Computational cost during distillation: Al-though PGKD results in a more efficient studentmodel for inference, the distillation process itselfcan be computationally expensive due to the iter-ative generation of samples from the LLM. Thismay limit the scalability of the approach for vastdatasets or frequent model updates.(3) Evaluation on a limited set of tasks: WhilePGKD has been evaluated on diverse datasets, theexperiments are still limited to a specific set ofmulti-class text classification tasks. Further vali-dation in a broader range of datasets, languages,and domains would strengthen the generalizabilityof the findings. Additionally, exploring the ap-plicability of PGKD to other NLP tasks beyondclassification, such as named entity recognition orquestion answering, could broaden its impact. Thisis something that will be addressed in future work.(4) Sensitivity to prompt engineering: The perfor-mance of PGKD may be sensitive to the quality ofthe prompts used to guide the LLM in generatingsamples and poorly designed prompts could lead tosuboptimal distillation results. Developing robustprompt engineering strategies or automating theprompt generation process could help mitigate thislimitation and improve the consistency of PGKDsperformance across different datasets and tasks.Addressing these limitations could further en-hance the applicability and robustness of the PGKDmethodology, making it an even more valuabletool for leveraging LLMs distillation in production-ready systems.",
  "The proposed PGKD methodology has significantsocietal implications, particularly in democratiz-": "ing access to high-performance models in cost-constrained applications. By leveraging the knowl-edge distilled from LLM, PGKD enables the devel-opment of more accurate and efficient models thatcan be deployed in a wide range of applications,including intent detection, topic classification, andother multi-class text classification tasks.In many industrial and commercial settings, thehigh inference cost and latency of LLM can bea significant barrier to adoption. PGKD offers acost-effective and efficient solution, enabling orga-nizations to leverage the power of LLMs withoutthe associated costs and latency at inference time.This can profoundly impact the democratization ofaccess to AI technology, particularly in resource-constrained environments.Furthermore, PGKD has the potential to benefita wide range of industries and applications, includ-ing customer support, messaging platforms, andother areas where multi-class text classification isa critical component. By enabling the developmentof more accurate and efficient models, PGKD canhelp to improve the overall quality of service anduser experience in these applications.",
  "Potential Risks": "The development and application of PGKD formulti-class text classification present potential fair-ness and bias considerations as PGKDs perfor-mance and the quality of student model outputs de-pends heavily on LLM output quality. If the teachermodel for PGKD contains bias or even worse hallu-cinates, it will generate biased and even hypotheti-cal data points on which the student model will betrained. The distilled LLM knowledge may perpet-uate and amplify existing student model biases andeven impact training data quality. Ensuring the useof LLMs that are robust against bias and are wellgrounded in data generation is essential in PGKDdistillation.",
  "Cheng-Han Chiang and Hung yi Lee. 2023. Can largelanguage models be an alternative to human evalua-tions? Preprint, arXiv:2305.01937": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, JiachengYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,Zhenguo Li, and Lingpeng Kong. 2022. Self-guidednoise-free data generation for efficient zero-shotlearning. In International Conference on LearningRepresentations.",
  "June 15,2024": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and Bar-ret Zoph. 2024. Gpt-4 technical report. Preprint,arXiv:2303.08774.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. In Lan-guage Models are Unsupervised Multitask Learners": "Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.2019. How to fine-tune bert for text classification?In Chinese computational linguistics: 18th Chinanational conference, CCL 2019, Kunming, China,October 1820, 2019, proceedings 18, pages 194206. Springer. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, ShangweiGuo, Tianwei Zhang, and Guoyin Wang. 2023. Textclassification via large language models. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 89909005, Singapore.Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Preprint,arXiv:2302.13971.",
  "Solomon Ubani, Suleyman Olcay Polat, and RodneyNielsen. 2023. Zeroshotdataaug: Generating andaugmenting training data with chatgpt.Preprint,arXiv:2304.14334": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.Xlnet: Generalized autoregressive pretraining for lan-guage understanding. In Neural Information Process-ing Systems.",
  "A.1PGKD Prompt": "Human:You are a Teacher model for a Student LMto perform topic detection on the followingtaxonomy:{Dataset Class Taxonomy}Here are a few labeled examples that show thecorrect label for this task:{Few-Shot Labeled Samples from TrainingDataset}Given the current model performance, pleasegenerate {PGKD Batch Size} training sam-ples for the model to improve its performance.The response should be a list of dictionariesin JSON format, the response needs to beparsable so do not output anything else ratherthan the response itself. The objective is tomaximize the model accuracy, generate newsamples knowing that the classification reportover validation set is:{Classification Report on the ValidationSet}Please consider a few samples that the modelwas able to classify correctly:{Correctly Classified Samples with correctlabel and Student-predicted label}And samples the model was not able to clas-sify correctly:{Misclassified Samples with correct labeland Student-predicted label}The model has a high confidence in classify-ing the following misclassified examples:{Hard Negative Samples with correct labeland Student-predicted label}Assistant:",
  "A.2Zero-shot Classification Prompt": "Human:You are an AI assistant, and you are taskedto perform topic classification starting fromtext. You are asked to classify text in topicscategories. You are only allowed to chooseone of the following categories: {Dataset Class Taxonomy}Please provide only one category for each textin JSON format. For example:class_label: , class_names: Please do not repeat or return the content backagain, just provide the category in the definedformat.Text-to-classifiy:{Paste text for zero-shot classification}Assistant:",
  "BImpact of Training Sample Size": "illustrates the performance metrics ofBERT-base and BERT-base augmented with PGKDacross different datasets as training samples in-crease. Noticeably, the improvement gap betweenPGKD and BERT-base narrows with more train-ing data. For instance, in the AG-news dataset,the performance difference in accuracy is 1.1%with 1000 samples but decreases to just 0.3% with10,000 samples. This trend is consistently observedacross all datasets, highlighting that while PGKDenhances performance, its relative benefit dimin-ishes as more training data is used. Importantly,PGKD shows no signs of performance degrada-tion, maintaining or improving upon the baselinemetrics across all sample sizes and datasets."
}