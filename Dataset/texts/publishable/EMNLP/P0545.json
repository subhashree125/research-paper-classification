{
  "Abstract": "Visual arguments, often used in advertising orsocial causes, rely on images to persuade view-ers to do or believe something. Understandingthese arguments requires selective vision: onlyspecific visual stimuli within an image are rele-vant to the argument, and relevance can only beunderstood within the context of a broader ar-gumentative structure. While visual argumentsare readily appreciated by human audiences,we ask: are todays AI capable of similar un-derstanding? We present VisArgs1, a dataset of 1,611 imagesannotated with 5,112 visual premises (with re-gions), 5,574 commonsense premises, and rea-soning trees connecting them into structuredarguments. We propose three tasks for evalu-ating visual argument understanding: premiselocalization, premise identification, and con-clusion deduction. Experiments2 show that 1)machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy, while humansreached 98.0%. Models also performed 19.5%worse when distinguishing between irrelevantobjects within the image compared to externalobjects. 2) Providing relevant visual premisesimproved model performance significantly.",
  "Code:": "Polar bear's habitat is vanishing as ice melts. The smokestack symbolizes melting of Arctic ice. Factory smokestacks contribute to climate change. small means ice is melting. It use ice floe for habitat. The ice is positioned above a large factory smokestack. The ice floe is small. A polar bear stands on a piece of ice. Industrial pollution needs to be reduced.",
  ": An example from our VisArgs corpus. Vis-Args makes the persuasion process in a visual argumentexplicit by representing it as a reasoning tree. Imagecredit: Egle Plytnikaite": "We consider the case of visual arguments. Con-sider , which depicts a polar bear on a shrink-ing ice floe. Without any text, this image callsattention to climate change: a visual metaphor con-nects melting ice to industrial emissions from facto-ries. A plausible interpretation of the argument con-cludes: industrial pollution needs to be reduced.We introduce VisArgs, an annotated dataset of1,611 images containing visual arguments. VisArgsmakes explicit the reasoning process in interpret-ing a visual argument:3 each image is annotatedwith visual premises grounded on object boundingboxes, commonsense premises eliciting implicitknowledge, and argument trees formalizing theconnection of these premises to the conclusion. Anargument tree consists of a root node (conclusion),some internal nodes (intermediate conclusion), andtwo types of leaf nodes (visual and commonsensepremises).Using VisArgs, we propose three complemen-",
  "Conclusion": "We introduce VisArgs, a curated and annotatedbenchmark for visual argument understanding. Us-ing our benchmark, we affirm a compelling hypoth-esis: selective vision is a critical bottleneck forvisual reasoning in current machines. We aim forour benchmark to serve as a resource for advancingmultimodal intelligence beyond passive captioning.Future work includes: 1. Conditional Saliency Analysis: It is demon-strated that the saliency required for visual ar-guments differs from that needed for passivecaptioning. Can the varying saliency require-ments across different tasks be analyzed? 2. Extending Modalities: In speech recognition,non-conditional selective attention is known asthe cocktail party effect. Would conditional se-lective attention be necessary in modalities otherthan vision as well?",
  "Reasoning Tree": "VP1 VP2 VP3 CP2 CP1 (VP1, CP1, VP2, CP2) -> IC1 : To identify the bottleneck in visual argument understanding, we define three tasks over VisArgs:Localization of Premises requires models to ground the visual premises. Identification of Premises necessitatesmodels to infer the visual premise relevant to the given intermediate conclusion. Deduction of Conclusion studiesthe ability of models to deduce the arguments conclusion based on different levels of inputs. tary tasks to evaluate different aspects of machinecapacity for comprehending visual arguments asillustrated in : 1) Localization of Premises:associates the description of a visual premise witha specific region in the image, 2) Identification ofPremises: Given an image and an (intermediate)conclusion, retrieves the necessary visual premisesto support the conclusion, and 3) Deduction of Con-clusion: generates the conclusion with increasingdetail of the annotated visual argument.Experiments on VisArgs demonstrate that themain bottleneck for machine understanding of vi-sual arguments is selective vision, i.e., Identifica-tion of Premises relevant to a given conclusion (see 5.2). We show that while machines can identifyvisual premises within an image (albeit worse thanhuman agreement, see Localization of premises 5.1), they struggle to discern which premises arerelevant to the conclusion among them. Results onour final Deduction of Conclusion task ( 5.3) ad-ditionally support the hypothesis that difficulties inunderstanding visual arguments do not stem fromdeficiencies in raw vision capacity. There, we con-trolled the level of input to the algorithm, rangingfrom raw images to explicit reasoning trees. Thegreatest accuracy gains came from the inclusion ofrelevant visual cues, further supporting our mainhypothesis. In all visual argument understandingtasks, machines perform worse than human agree-ment, providing avenues for future work.In conclusion, our results suggest that selectiveattention to visual cues is the main bottleneck forthe current AI capacity to understand visual argu-ments. This finding also establishes visual argu-ment understanding as a distinct area of study in the computational domain: vision does not precede,but works jointly with reasoning in terms of under-standing visual arguments. We expect that VisArgswill be utilized as a diagnostic benchmark for selec-tive vision in future multimodal models: even thebest current models lag significantly behind humanperformance in our Identification of Premises andDeduction of Conclusion tasks.",
  "Related Work": "Visual arguments are arguments built on visualmedium (Boland, 2005). Unlike typical images, avisual argument is intentionally organized to per-suade viewers to a certain conclusion (Birdsell andGroarke, 1996; Boland, 2005). This work buildsupon to ongoing debates in the human studies litera-ture about the nature of visual arguments (Johnson,2003; Tseronis, 2018). Our results ( 5) suggestthat understanding visual arguments requires fo-cusing on a subset of the visual context: not allvisual cues contribute, and identifying the relevantones is the key necessity. This task one of selectivevision: the human capability to focus on behav-iorally relevant stimuli. (Desimone and Duncan,1995). Examples of visual arguments are prevalentin advertisements (Kjeldsen, 2012; Zhang et al.,2018; Hussain et al., 2017; Ye et al., 2019), car-toons (Birdsell and Groarke, 2007), mathematicaleducations (Inglis and Meja-Ramos, 2009), and, ar-guably, diagrams (Kembhavi et al., 2016; Alikhaniand Stone, 2018). Liu et al. (Liu et al., 2022) alsoinvestigate arguments conveyed through images.However, our work focuses on visual argument,requiring that the argumentative content be primar- ily communicated through visual elements ratherthan relying solely on accompanying captions orwritten text. Furthermore, we provide explicit an-notations of the argumentation structure in a treeformat, facilitating detailed, hierarchical analysisof the models ability to comprehend visual argu-ments step-by-step.Multimodal reasoning. Recent studies have in-troduced various multimodal models capable ofsophisticated reasoning across different modali-ties, such as vision and language. Models suchas LLaVA (Liu et al., 2023a), Idefics2 (Laurenonet al., 2024), and Qwen-VL (Bai et al., 2023) arebuilt on pretrained large language models (e.g.,LLaMA (Touvron et al., 2023)) and integrate vi-sion encoders.Others, including OFA (Wanget al., 2022) and Unified-IO (Lu et al., 2022), aredeveloped from scratch. These models excel intasks such as localization, image captioning, andcommonsense reasoning.Furthermore, modelssuch as Unified-IO-2 (Lu et al., 2023) and GPT-4-O (Achiam et al., 2023) can understand audio,while others (Zellers et al., 2022; Han et al., 2023a)support video understanding, demonstrating broadmultimodal reasoning capabilities.Beyond factual visual understanding. Visualcomprehension is moving beyond factual under-standing to include various types of writing. Theseinclude visual commonsense reasoning (Zellerset al., 2019; Park et al., 2020; Han et al., 2023b;Hessel et al., 2022), humor understanding (Hesselet al., 2023; Hyun et al., 2023), and understandingsocial interaction (Zadeh et al., 2018). Of particularrelevance to our work is visual metaphors (Akulaet al., 2023), which express abstract concepts withconcrete visual cues. While some overlap existsin the images used, there are clear differences inintention and structure; not all metaphorical imagespresent clear arguments and can be seen as visualarguments. Conversely, not all visual argumentsdepend on metaphors (Blair, 2012).Argument structure. An argument is typicallyunderstood as a structure that starts from a set ofpremises (reasons) and ends in a conclusion, of-ten represented symbolically as a tree (Whately,1863; Freeman, 2011). While there have been ex-tensions, including computational models of argu-ments (Bench-Capon and Dunne, 2007; Rahwanand Simari, 2009; Atkinson et al., 2017), we usethe basic form of trees connecting premises to con-clusions, following previous literature (Stab andGurevych, 2014; Lawrence and Reed, 2020). Stairs that resembles a rough terrain. Depiction of a Jeep driving up the stairs. \"Jeep\" is written at the base of the stairs. Concrete stairs in an urban setting. A white outline labeled \"P\". The outline includes the Jeep logo. Hallucination! Replace. base of the stairs is not relevant. rough terrain involves commonsense inference.",
  "VisArgs Dataset": "VisArgs comprises a total of 1,611 images featur-ing clear visual arguments. These images are cat-egorized into 914 advertisement images and 697cartoon images based on their sources. Each im-age in VisArgs is annotated with descriptions andbounding boxes for the visual premises (VP), de-scriptions of the commonsense premises (CP), theconclusion, and an argumentation tree (T) detailingthe reasoning path from the premises to the conclu-sion (C). All descriptions are in English, with anaverage character length of 79, 91, 142, and 105 forVP, CP, C, and T, respectively. On average, eachimage contains 3.17 visual premises, 3.46 common-sense premises, and 2.88 intermediate conclusions.",
  "Annotation Process": "We partially rely on GPT-4-O (Achiam et al., 2023)for initial annotations. However, these machine-generated annotations serve only as preliminaryseeds, which are then extensively refined by experi-enced human workers, as illustrated in 3. The ma-chines role is merely to provide imperfect startingpoints to facilitate the human annotation process.Below, we detail our annotation procedure.Collecting Images. Our primary criterion was toselect images that enable human annotators to eas-ily and accurately interpret both the visual premiseand the corresponding conclusion, thereby clari-fying the argumentative structure within the im- age. Also, we ruled out samples with scene textwithin the images that directly describe the conclu-sion. We manually collect around 1,600 imagesfollowing these criteria from Pinterest.4 Startingwith keyword-based searches (e.g. creative ads),we expanded our collection by exploring relatedimages. Cartoons (which often contain visual argu-ments (Birdsell and Groarke, 1996)) were sourcedfrom a dedicated website.5 We manually collectedaround 1,600 cartoons from various categories, in-cluding politics, education, and environment. Weinclude URLs to the images to comply with licens-ing terms following previous work (Schuhmannet al., 2022; Lee et al., 2021). Refer to Appendix Afor details.Describing Visual Premises. The next step is toexplicitly describe the visual argument within eachimage. However, during the early stages of ourannotation process, we discovered that althoughhumans can naturally understand visual arguments,they often find it challenging to articulate theirinterpretation into structured argumentation trees.Therefore, we used an AI model (GPT-4-O) to gen-erate initial candidates. Human workers then se-lect and modify these initial annotations, as shownin . The human annotators could optionallyincorporate new visual premises when necessary: 21% of images had their set of visual premisesexpanded through this process. To facilitate thisprocess, we break down the annotation into twosteps: describing the visual premises and specify-ing the argument structure.Given an image containing a visual argument,we instructed the model to generate a set of visualpremises necessary to support the argument (referto Appendix J for further details). However, the AImodel often fails to fully comprehend the visualargument. To address this, we engaged a pool ofexperienced human workers to review the machine-generated outputs. They selected the correct visualpremises and made necessary modifications to en-sure accuracy and coherence. Additionally, weidentified that a model-generated visual premisesometimes contains multiple atomic premises. Weinstructed the reviewers to separate these mergedpremises into individual atomic premises. Furtherdetails are provided in Appendix A.Specifying Argument Structure. Given the visualpremises and the image, we further annotate three",
  ": Variety of the topics represented in the visualpremises and conclusions in VisArgs": "components constituting the argumentation struc-ture: commonsense premises, conclusions, and ar-gument trees. As in the previous stage, we firstgenerate initial candidates using an AI model. Forthis stage, we impose an additional criterion: theset of selected premises should be both necessaryand complete (refer to Appendix J). The same poolof human workers then adjust the annotations forgreater accuracy. The workers first verify the cor-rectness of the conclusion and discard the imageif it is incorrect. They then identify and correctany errors, including semantic and structural mis-takes. We discarded 1,593 of the 3,204 images inthis process. Details are provided in Appendix A.Visual Grounding. Lastly, we manually gatherbounding box annotations for each visual premiseto finalize the multimodal annotations. We assumea one-to-one relationship between each boundingbox (vpri ) and its corresponding textual description(vpdi ). Annotators are instructed to ensure accuratematching and precise bounding box tightness, asdetailed in Appendix A.",
  "Data Analysis": "Topic Diversity. To gauge the diversity of topicscovered in VisArgs, we run zero-shot categoriza-tion using GPT-4-O and LLaMa3 (AI@Meta, 2024)to classify the topics of visual premises and con-clusions. The topics cover a wide range of visualobjects and argument topics, as shown in .Refer to Appendix B for details.Visual Cues vs. Dense Captioning. In theory, se-lective attention to visual premises could be col-lapsed into an NLP problem by describing every-thing in an image. To test this counter-hypothesis,we manually check how often the visual premises",
  ": Frequency of detailed captions containing vi-sual premises. Hit rate denotes how often all visualpremises per image are included in the captions": "are contained in the outputs of detailed captioningmodels. We include three baselines here: a gener-alist (LLaVA-Next (Liu et al., 2024b)), a specialist(ShareCaptioner (Chen et al., 2023)), and LLaVA-LLaMa3 (XTuner Contributors, 2023) fine-tunedon a detailed captioning corpus (DOCCI (Onoeet al., 2024))6. Tab. 2 summarizes our manual in-spection of 100 images, showing that the detailedcaptions insufficiently capture the visual premises,with the hit rate staying below 15% for all models.Safety. Since we did not initially filter for safety,we now analyze the safety of VisArgs using stan-dard models. For textual safety, we utilize the Per-spective API7, and for visual domains, we employLAION-Safety8. The toxicity scores for textual de-scriptions were 0.03 for visual premises and 0.07for conclusions. Also, given the threshold of 0.7,no descriptions and visual premises were classifiedas toxic. Furthermore, only 71 among 1611 imagesare classified as unsafe. Manual inspection revealsthat such unsafe\" images were social campaignsadvocating against the harmful behaviors whichpresumably triggered the LAION detector.",
  ": Correlation of each metric with human deci-sions in the Deduction of Conclusion task": "ing box vpr = x, y, h, w, a set of commonsensepremises CP = {cpd0, cpd1, . . .}, and the conclusionin textual form C. Further, a single argument treefor each image is built on the premises. Each treet T represents a reasoning path leading to theconclusion C. The nodes N of a tree consist of thefollowing: 1) leaf nodes: subsets of the union ofthe visual and commonsense premises VPCP. 2)internal nodes: elements of the set of intermediateconclusions IC. 3. root node: the conclusion C.An edge e of the tree connects a subset of nodesN VP CP IC to either an intermediate con-clusion ic IC or the final conclusion C.",
  "Localization of Premises": "Localization of Premises tests the visual groundingcapabilities of machines. Given the image I anddescription of a visual premise vpd, the goal is tofind a corresponding region vpr in the image.Metrics and Models. We define open-set evalua-tion as a setting in which models are required togenerate bounding box coordinates without relyingon a predefined candidate list. As a result, modelsused for open-set and closed-set evaluations arearchitecturally distinct, since models lacking an ex-plicit generative head, such as CLIP (Radford et al.,2021), are not compatible with open-set evaluationdue to their dependence on a candidate region listfor text-to-region matching.For closed-set grounding, which is an N-wayclassification task, the goal is to match the given",
  "Identification of Premises": "Identification of Premises tests the selective atten-tion capabilities, i.e., selecting necessary visualcues to understand an argument. Given the imageI and an intermediate conclusion ic, the goal isto select a visual premise vpd that leads to thisintermediate conclusion.Metrics and Models. For this task, we retain onlyintermediate conclusions that have at least two un-related visual premises within the image. We reportclassification accuracy based on a single gold vi-",
  "Deduction of Conclusion": "Deduction of Conclusion evaluates the comprehen-sive ability to deduce the conclusion of an argu-ment. Given a subset of inputs among the im-age I, the visual premises VP, the commonsensepremises CP, and the reasoning tree T, the objec-tive is to generate the conclusion C of an argument.Metrics and Models. As discussed earlier in 4.3,we use BERTScore as the primary metric. Wesupplement this with three additional static met-rics (Bleu-4, ROUGE-L, CIDEr) in Appendix F.The models tested in this task include all the mul-timodal LLMs used in the previous experimentand text-only LLMs (LLaMa-3-Instruct (AI@Meta,2024), Mistral-Instruct (Jiang et al., 2023), andZephyr (Tunstall et al., 2023)). All LLMs consid-ered here are the 7 8b sized variants. The LLMsdo not take the image as an input.Results. shows the results for this task. Asexpected from previous tasks, most models expe-rience the highest gain from the additional infor-mation provided by the ground-truth set of visualpremises. This supports our hypothesis that selec-tive attention to visual premises is a bottleneck inunderstanding visual arguments in current models.Also, both multimodal and text-only models bene-fited from commonsense premises and reasoningtrees in most setups, indicating that models cannotyet perfectly understand visual arguments in a text-only format and benefit from explicit reasoning : Left: OCR detection results. Right: Groundtruth text instances missed by the model (highlightedin red). Most detection errors are attributed to Out-of-Domain cases such as calligraphy, handwritten text, ortext that is too small for the model to detect, despitebeing distinguishable to the human eye. process information. We note that OFA struggledto follow the instruction format, leading to sub-zero scores. Although rare, BERTScore, based oncosine similarity, can yield negative values. Wealso clarify that the multimodality of the deductionof conclusion task resides in the visual premises,making it solvable by text-only models given them.",
  ": open-set results in localization of premises": "CIDER (Vedantam et al., 2015) measure surfaceform similarity, not semantic similarity betweenconclusions. Alternatively, prompt-based evalua-tion using general reasoners (e.g. GPT-4) (Achiamet al., 2023) can be biased by factors including can-didate order (Pezeshkpour and Hruschka, 2023).Human verification, though ideal, is costly and hardto reproduce. We conduct a small-scale comparisonstudy (see Tab. 3) to verify that the model-basedmetric BERTScore (Zhang* et al., 2020) providesthe most stable estimate, making it our primarymetric. Details are in Appendix D.",
  ": Results of the Identification of Premises task. Difference between the lowest score in global and local setupfor each model are highlighted": "description with the correct bounding box. To eval-uate standard image-text matching algorithms (e.g.CLIP), we crop the regions accordingly. The mod-els for this task include various CLIP-based models(CLIP (Radford et al., 2021) with different back-bones and SigLIP (Zhai et al., 2023)) and a mul-titask model OFA (Wang et al., 2022). The CLIP-based models are adapted as follows: for each can-didate object region (specified by bounding box co-ordinates), the corresponding regions are croppedfrom the image to create region-level image rep-resentations. Image features for each cropped re-gion are then extracted using a CLIP-based encoder,while text features are obtained by encoding theinput description using the same model. Cosinesimilarity is calculated between the text featureand each region-level image feature. The regionwith the highest similarity score is selected as thepredicted match. For open-set grounding, which is to locate anobject in an image based on a natural languageexpression, we instruct the models to output bound-ing box coordinates and we compare them to theground truth region. A predicted coordinate is con-sidered correct if its intersection over union withthe gold label is at least (IoU 0.5). We usea diverse set of models that support local regionoutput formats, UNINEXT-H (Yan et al., 2023),LISA (Lai et al., 2023), Unified-IO-2 (Lu et al.,2023), OFA, MM-G-DINO (Liu et al., 2023b). Results. Tab. 4 demonstrates that current modelsare generally effective in matching descriptions ofvisual premises to the correct regions in images,thereby meeting the basic vision requirements forunderstanding visual arguments. However, the re-sults for open-set grounding, shown in Tab. 5, are",
  "LLaMA3-30.2 37.8 (7.6) 40.8 (2.0)Mistralv0.2-18.9 30.2 (11.3) 36.6 (6.4)Zephyr-20.6 28.7 (8.1) 36.5 (7.8)": "OFA-41.3 -24.6 (16.7) -16.5 (8.1) -13.9 (2.6)Qwen-VL-Chat12.8 23.7 (10.9) 30.2 (6.5) 32.7 (2.5)CogVLM25.730.7 (5.0) 33.6 (2.9) 36.3 (2.7)Idefics216.422.8 (6.4) 29.5 (6.7) 36.6 (7.2)InstructBLIP-18.4 16.6 (35.0) 28.9 (12.3) 32.2 (3.3)Unified-IO-2-9.9-3.4 (6.5)4.2 (7.6)8.0 (3.8)LLaVA-1.52.2 20.0 (17.8) 29.6 (9.6) 33.7 (4.1)LLaVA-Next15.1 28.4 (13.3) 34.3 (5.9) 39.5 (5.2)GPT-4-O25.5- 34.3 (8.8) 41.0 (6.7) : Results of the Deduction of Conclusion task,showing how incremental additions of inputs affect thecorrectness of the conclusion. Scores are presentedusing BERTScore, with similar trends observed acrossother metrics as detailed in Appendix F. somewhat mixed: the scores are acceptable butnot uniformly high. We traced this performancedecline to the nature of zero-shot object detectors,which are designed to detect concrete objects andclear segments. In contrast, our bounding boxes aremore semantic (Guo et al., 2018). Visual examplescan be found in Appendix G.",
  ":Mean of incremental improvements inBERTScore with each additional input across four dif-ferent prompts in Deduction of Conclusion. Standarddeviations are shown in parentheses": "sual premise and two negative candidates. Thenegative sets are sourced as described in 4.2and are categorized into random, visual, textual,mixed, and semantic sets. Given the tasks require-ment for understanding argumentation structure,the models evaluated are primarily multimodallarge language models with adequate reasoningcapabilities. We experiment with a broad selectionof models: OFA (Wang et al., 2022), Qwen-VL-Chat (Bai et al., 2023), CogVLM (Wang et al.,2023), Idefics2 (Laurenon et al., 2024), Instruct-BLIP (Dai et al., 2024), Unified-IO 2 (Lu et al.,2023), LLaVa-1.5 (Liu et al., 2024a), and LLaVa-Next (Liu et al., 2024b). For the sake of brevity,we do not report per-category results (Ads and Car-toon) here. Refer to Appendix F for full results. Results. Tab. 6 highlights a significant trend: mod-els struggle to distinguish negatives within the im-age (local), but excel in identifying global neg-atives. A major challenge for most models washandling semantic negatives within the same im-age, as evidenced by the generally wide marginbetween models performance on global and localsetups. Still, the global negative samples exhib-ited more pronounced distinctions based on theirsampling scheme. Negatives sampled uniformlywere distinguishable by most models with 90%accuracy. In contrast, retrieval methods provedmore challenging across the board, particularly fornegatives retrieved using the text-to-text similaritymodel (textual), which increased the problem com-plexity for most models. Notably, OFA failed tofollow zero-shot instructions for multiple-choiceanswering, scoring close to zero. Finally, we alsopresent results for cropped ground-truth region im-ages. Although cropped images are not losslessrepresentations of the regions, all models exhibitedsignificant improvements, indicating that the abil-ity to infer relevant visual cues is indeed a criticalchallenge. Thus, we conclude that models struggleto infer which visual cues support the argument. A on the left side. missileThe text reads, \"Words kill .\"wars The represents .missilewar Large of plastic bags. waveplastic bags are flying out of the .wave Wave indicates the destructive force in nature. The right side is and leafless.barren The left sideis lush and green. The indicates environmental degradation. barren side",
  "Diagnostics": "Prompt Robustness. To ensure the robustness ofour empirical results, we differentiated the promptsprovided to the models. As shown in Tab. 8, thetrend of gains remained stable across four differentprompts, confirming the validity of our tests. Fordetailed prompts, refer to Appendix M, and forresults in other tasks, see Appendix E.Error Analysis. provides qualitative exam-ples of failure cases. We present straightforwardinstances to clearly explain the errors. In thesecases, the models fail to reason about the relevantobject, which is the subject of the given intermedi-ate conclusion, and instead rely on common words,leading to incorrect inference results.Reliance on OCR Capabilites. To examine howdemanding VisArgs is on OCR capabilities, we em-ploy a lightweight OCR detector (Du et al., 2021)to detect bounding boxes of the visual premiseswithout leveraging text annotations as input. Eventhis simplified model achieves 82.77% accuracy inimage-wise evaluation, where an image is consid-ered correctly detected only if all visual premiseswithin it are identified. Typical failure cases areillustrated in .",
  "Limitations": "VisArgs, which is built on advertisements and car-toons from web sources, does not encompass allforms of visual arguments. Visual arguments alsoinclude various forms of media including mathe-matical diagrams (Inglis and Meja-Ramos, 2009)and videos, such as films (Alcolea-Banegas, 2009).Consequently, the findings of this study do not rep-resent all forms of visual arguments.Additionally, the annotations for VisArgs arecreated by two NLP researchers with similar cul-tural backgrounds. Although a different group ofhuman evaluators validated these annotations, fu-ture research should consider individual variancesin the interpretation of visual arguments and thereasoning processes identified by reasoning trees.Finally, we excluded images containing writtentext in non-English languages when curating Vis-Args, as the annotators were not familiar with otherlanguages. This limitation may confine the cul-tural context covered by VisArgs, thus representingonly a partial depiction of visual arguments. Sincethe logical relations forming a visual argument candepend on culture-specific elements, this skeweddistribution of images can lead to a biased under-standing of visual arguments.We encourage future research to extend this workby exploring a wider range of visual arguments andincorporating more diverse cultural and linguisticcontexts. This work was partly supported by an IITP grantfunded by the Korean Government (MSIT) (No.RS-2020-II201361, Artificial Intelligence GraduateSchool Program (Yonsei University) and RS-2024-00353131) and the National Research Foundationof Korea (NRF) grant funded by the Korea govern-ment (MSIT) (No. RS-2024-00354218). Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Malihe Alikhani and Matthew Stone. 2018. Arrows arethe verbs of diagrams. In COLING": "Katie Atkinson, Pietro Baroni, Massimiliano Gia-comin, Anthony Hunter, Henry Prakken, Chris Reed,Guillermo Simari, Matthias Thimm, and Serena Vil-lata. 2017.Towards artificial argumentation.AImagazine, 38(3):2536. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966.",
  "Julie E Boland. 2005. Visual arguments. Cognition,95(3):237274": "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-ghui He, Jiaqi Wang, Feng Zhao, and DahuaLin. 2023.Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprintarXiv:2311.12793. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Ben-esty, Jacob Benesty, Jingdong Chen, Yiteng Huang,and Israel Cohen. 2009. Pearson correlation coeffi-cient. Noise reduction in speech processing, pages14. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36.",
  "Yanming Guo, Yu Liu, Theodoros Georgiou, andMichael S Lew. 2018. A review of semantic seg-mentation using deep neural networks. Internationaljournal of multimedia information retrieval, 7:8793": "Seungju Han, Jack Hessel, Nouha Dziri, Yejin Choi,and Youngjae Yu. 2023a. Champagne: Learning real-world conversation from large-scale web videos. InProceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 1549815509. Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang,Jiwan Chung, Yejin Son, Yejin Choi, and YoungjaeYu. 2023b. Reading books is great, but not if youare driving! visually grounded reasoning about de-feasible commonsense norms. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 894914.",
  "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan LeBras, and Yejin Choi. 2021. CLIPScore: a reference-free evaluation metric for image captioning.InEMNLP": "Jack Hessel, Jena D Hwang, Jae Sung Park, RowanZellers, Chandra Bhagavatula, Anna Rohrbach, KateSaenko, and Yejin Choi. 2022. The abduction ofsherlock holmes: A dataset for visual abductive rea-soning. In European Conference on Computer Vision,pages 558575. Springer. Jack Hessel, Ana Marasovic, Jena D Hwang, Lillian Lee,Jeff Da, Rowan Zellers, Robert Mankoff, and YejinChoi. 2023. Do androids laugh at electric sheep?humor understanding benchmarks from the newyorker caption contest. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 688714. Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang,Keren Ye, Christopher Thomas, Zuha Agha, NathanOng, and Adriana Kovashka. 2017. Automatic un-derstanding of image and video advertisements. InProceedings of the IEEE conference on computervision and pattern recognition, pages 17051715.",
  "Ralph H Johnson. 2003. Why visual arguments arentarguments": "Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min-joon Seo, Hannaneh Hajishirzi, and Ali Farhadi.2016.A diagram is worth a dozen images.InComputer VisionECCV 2016: 14th European Con-ference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251.Springer. Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-cient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR conference on researchand development in Information Retrieval, pages 3948. Jens E Kjeldsen. 2012. Pictorial argumentation in adver-tising: Visual tropes and figures as a way of creatingvisual argumentation. In Topical themes in argu-mentation theory: Twenty exploratory studies, pages239255. Springer.",
  "Zhexiong Liu, Meiqi Guo, Yue Dai, and Diane Litman.2022. Imagearg: A multi-modal tweet dataset forimage persuasiveness mining. In Proceedings of 9thWorkshop on Argument Mining, page 1": "Jiasen Lu, Christopher Clark, Sangho Lee, ZichenZhang, Savya Khosla, Ryan Marten, Derek Hoiem,and Aniruddha Kembhavi. 2023.Unified-io 2:Scaling autoregressive multimodal models with vi-sion, language, audio, and action. arXiv preprintarXiv:2312.17172. Jiasen Lu, Christopher Clark, Rowan Zellers, RoozbehMottaghi, and Aniruddha Kembhavi. 2022. Unified-io: A unified model for vision, language, and multi-modal tasks. In The Eleventh International Confer-ence on Learning Representations.",
  "John Lubbock. 1893. The beauties of nature andthe wonders of the world we live in, volume 2893.Bernhard Tauchnitz": "Yasumasa Onoe, Sunayana Rane, Zachary Berger,Yonatan Bitton, Jaemin Cho, Roopal Garg, AlexanderKu, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer,Su Wang, and Jason Baldridge. 2024. DOCCI: De-scriptions of Connected and Contrasting Images. InarXiv:2404.19753. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Jae Sung Park, Chandra Bhagavatula, Roozbeh Mot-taghi, Ali Farhadi, and Yejin Choi. 2020. Visual-comet: Reasoning about the dynamic context of a stillimage. In Computer VisionECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part V 16, pages 508524. Springer.",
  "Iyad Rahwan and Guillermo R Simari. 2009. Argumen-tation in artificial intelligence, volume 47. Springer": "Christoph Schuhmann, Romain Beaumont, RichardVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis,Mitchell Wortsman, et al. 2022. Laion-5b: An openlarge-scale dataset for training next generation image-text models. Advances in Neural Information Pro-cessing Systems, 35:2527825294. Christian Stab and Iryna Gurevych. 2014. Identifying ar-gumentative discourse structures in persuasive essays.In Proceedings of the 2014 conference on empiricalmethods in natural language processing (EMNLP),pages 4656. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Assimakis Tseronis. 2018.Multimodal argumenta-tion: Beyond the verbal/visual divide. Semiotica,2018(220):4167": "Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944. Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In Proceedings of the IEEEconference on computer vision and pattern recogni-tion, pages 45664575. Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-chitectures, tasks, and modalities through a simplesequence-to-sequence learning framework. In Inter-national Conference on Machine Learning, pages2331823340. PMLR. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, LeiZhao, Xixuan Song, et al. 2023. Cogvlm: Visual ex-pert for pretrained language models. arXiv preprintarXiv:2311.03079.",
  "Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, ZehuanYuan, Ping Luo, and Huchuan Lu. 2023. Universalinstance perception as object discovery and retrieval.In CVPR": "Keren Ye, Narges Honarvar Nazari, James Hahn, Za-eem Hussain, Mingda Zhang, and Adriana Kovashka.2019. Interpreting the rhetoric of visual advertise-ments. IEEE transactions on pattern analysis andmachine intelligence, 43(4):13081323. Licheng Yu, Patrick Poirson, Shan Yang, Alexander CBerg, and Tamara L Berg. 2016. Modeling contextin referring expressions. In Computer VisionECCV2016: 14th European Conference, Amsterdam, TheNetherlands, October 11-14, 2016, Proceedings, PartII 14, pages 6985. Springer. AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,Erik Cambria, and Louis-Philippe Morency. 2018.Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.In Proceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 22362246.",
  "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and YejinChoi. 2019. From recognition to cognition: Visualcommonsense reasoning. In CVPR": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-pati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.Merlot reserve: Neural script knowledge throughvision and language and sound. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1637516387. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,and Lucas Beyer. 2023. Sigmoid loss for languageimage pre-training. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages1197511986.",
  "AData Annotation Details": "Human Resources. To ensure a comprehensiveunderstanding of the intricate requirements of oursetup and maintain consistency across annotations,two of this papers authors conducted the entireannotation process. Three volunteers from the NLPresearch community did the human evaluation.Annotation Interface. We used a custom-builtinterface for efficient and convenient image annota-tion. The interface is depicted in and .Additionally, we provide a snapshot of the humanevaluation interface for Identification of Premisesin . We will open-source this interface alongwith the dataset.Inter-Annotator Agreement. The dataset annota-tion was conducted by two primary human anno-tators, with a third evaluator assigned to assess an-notation quality and consistency. To measure inter-annotator agreement, 100 samples were randomlyselected and re-annotated by a secondary annotatordifferent from the original. Subsequently, the thirdevaluator independently assessed the equivalenceof each annotated sample.Given that the annotations comprise premise setsand natural language conclusions, rather than nu-merical scores, traditional metrics such as Cohenskappa are not applicable. Instead, we measuredinter-annotator agreement using two distinct crite-ria: equivalence of visual premise sets and equiva-lence of natural language conclusions. The Jaccardsimilarity index was employed to quantify the over-lap between visual premise sets, while a binaryequivalence test was used to evaluate alignment inthe conclusions.The results, as presented in Tab. 9, demonstratea high degree of inter-annotator agreement. Quali-tative analysis revealed that observed discrepanciesprimarily stemmed not from substantive semanticvariations but from differences in how annotatorssegmented a single concept into one or more visualpremises.",
  "BAnalyzing Topic Diversity": "Initially, we considered using the Latent Dirich-let Allocation (LDA) (Blei et al., 2003) methodfor data visualization, following previous litera-ture (Hessel et al., 2022). However, we found thatLDA based on Bag-of-Words representations couldnot generate meaningful clusters or labels for con-clusion topics. As a solution, we developed anadaptive semantic classification technique using",
  ": Details on the models used in our experiments": "multimodal large language models:Defining Class Labels. We utilize GPT-4-O. Wefirst sample 400 sentences each for VP and C, andthen feed them to GPT with the following instruc-tions: For VP: \"Give me well-balanced 10 objecttype classes for these texts (e.g., eating & dining,environments & landscapes, attire). Just classes.\"For C: \"Give me well-balanced 10 classes for thesetexts. Just classes.\" After receiving the 10 classesfrom the GPT, we manually refine these classesinto 8 classes for both VP and C.Labelling Data. We use a pretrained languagemodel to classify visual premises (VP) and conclu-sions (C) in a zero-shot manner. We provide thefollowing input to the LLaMA-39 LLM:",
  "9meta-llama/Meta-Llama-3-8B-Instruct": "premises using image-to-text cosine similarityscores. The input regions were provided as croppedimages. A model output was considered correct(True) if the similarity between the ground-truthregion and the given description was the highestamong all candidates; otherwise, it was markedincorrect (False).For open-set grounding, we employed ob-ject grounding models such as MM-GDINO,UNINEXT-H, LISA, OFA, and Unified-IO-2 todirectly generate bounding box coordinates. We ap-plied a threshold of 0.35 to the outputs, merging theselected regions into the tightest rectangle union.For LISA, we converted the output segmentationmask into bounding boxes. We then calculatedthe Intersection over Union (IoU) score for eachbounding box. To compute the accuracy metric,we used a threshold of 0.5 for binary classificationover the IoU. We calculated the local mean, whichis the mean per visual premise in an image, and themean per image.",
  "C.2Identification of Premises": "We utilized OFA, Qwen-VL-Chat, CogVLM,Idefics2, InstructBLIP, Unified-IO-2, LLaVA-1.5,LLaVA-Next, and GPT-4-O for our experiments.We created multiple-choice questions with threepossible answers: one correct answer and two in-correct answers. Five conditions were set for sam-pling the negatives for incorrect answers: Random Sampling: This global sampler selectssamples uniformly without duplication. Visual Sampling: This global sampler choosesthe top 2 premise descriptions most similar to theimage, using CLIP to score the cosine similaritybetween the image and text. We set the CLIPsimilarity threshold to 0.24 to ensure negativepremises do not accurately describe the image. Textual Sampling: This sampler selects the top 2premise descriptions most similar to the groundtruth premise, using ColBERT to score the co-sine similarity between texts. We set the Col-BERT similarity threshold to 25 to prevent neg-ative premises from accurately describing theimage. Mixed Sampling: This approach combines visualand textual sampling, visually selecting from thetop 10 textual retrieval results.To ensure a fair comparison across various nega-tive sampling methods, we use only intermediateconclusions that have three or more related visualpremises. This results in 1,775 visual premises for the advertisement category and 1,774 for thecartoon category, totaling 3,549 visual premises,which is 62.34% of the overall visual premises.Human Evaluation. We randomly selected 100images from each data category and had humanannotators perform the same tests as the machinesacross all negative set setups. The results demon-strated that humans achieved nearly perfect accu-racy in this task, as shown in Tab. 14.",
  "C.3Deduction of Conclusion": "We conducted experiments on both Multi-ModalLarge Language Models (MLLM) and LargeLanguage Models (LLMs).The MLLMs usedin our experiments include LLaVA-1.5, LLaVA-NeXT, Idefics2, OFA, InstructBLIP, Qwen-VL-Chat, CogVLM, and Unified-IO-2. The LLMsinclude LLaMA-3, Mistral, and Zephyr.Prompting. Before conducting the experiments,we established a set of instructions to be applied toall models to elicit appropriate responses. Duringthis process, we encountered several issues withprompt engineering, such as model refusal to ad-dress controversial or unsafe questions, the inclu-sion of unnecessary tokens, multiple sentences, andthe positioning of image tokens. Ultimately, wedecided on the following prompt: \"<image> <in-formation> Your task is to answer what the imagewants to convey. You should respond in only onesentence without any unnecessary prefixes. AN-SWER:\"",
  "C.4Resource & Hyperparameters": "Computation. We utilized RTX-4090 and A6000GPUs for our experiments. All models, exceptfor CogVLM, were implemented using RTX-4090GPUs.Due to the size of its model weights,CogVLM was implemented on an A6000 GPU.Each model required up to 8 RTX-4090 GPU-hoursper task. In total, conducting all tasks demanded200 RTX-4090 GPU-hours.Hyperparameters.Our experiments are de-terministic, given the pretrained model weights,the greedy decoding scheme, and the instructionprompts. We explore prompt diversification in 5.4and Appendix E.",
  "DComparison of Metrics for Deductionof Conclusion": "Here, we describe details for human evaluation ofgoodness per each metric illustrated in Tab. 3.Human Evaluation. We sampled 200 target im-ages and collected responses from three models:LLaVA-Next, Qwen-VL-Chat, and GPT-4o. Hu-man annotators then determined whether eachmodels conclusion was semantically similar to thereference conclusion.Metrics. To evaluate accuracy, precision, recall,and F1-score, we first converted each metric intobinary decisions using derived thresholds. We es-tablished these thresholds by training a logisticregression model on 100 pairs of metric scores andhuman decisions. Subsequently, we inferred binarydecision labels on the remaining 100 pairs. Theresults are presented in Tab. 11. Additionally, thecorrelation between the metrics and human deci-sions is reported using Pearsons coefficient (Cohenet al., 2009).",
  "EPrompt Robustness in Identification ofPremises": "Extending the robustness study in Tab. 8, we con-ducted a similar prompt diversification experimentfor the task of Identification of Premises. By para-phrasing the original prompt as described in Ap-pendix L, we performed the same evaluation. Theresults, presented in Tab. 12, demonstrate that ourexperimental outcomes remain stable for Identifi-cation of Premises across different prompt para-phrases.",
  "FFull Results": "This section presents the comprehensive versions ofthe results summarized in the main paper. Tab. 13displays the open-set grounding results for Local-ization of Premises, while Tab. 14 provides theresults for Identification of Premises. The resultsfor the task of Deduction of Conclusion are detailedby category: advertisements are shown in Tab. 15,cartoons in Tab. 16, and the average across bothcategories in Tab. 17.",
  "convey in one sentence. RESPONSE:": "1. Below is the auto generated annotations. Please read them carefully and make any corrections following the instructions. 2. List all the visual elements necessary to understand the message conveyed by the image as visual premises. 3. List all the commonsense knowledge required to understand the message conveyed by the image as commonsense premises. 4. Write down the message that the image is trying to convey. 5. Create the argument step by step to reach the conclusion. The reasoning tree must include all premises.",
  ": Human evaluation interface for Identification of Premises": "Target Visual Premise: Mannequins in a store window wearing fur coats. IOU : 0.23 Target Visual Premise: A garlic bulb and a chili pepper, both smaller than onion. IOU : 0.29 Explanation: The model seems to be detecting the correct target, but only for a single object. Explanation: The model detecting on object related to the target, but on wrong object. Explanation: The model seems to be detecting the correct target, but only for a single object. Target Visual Premise: A bird, made of water, is drinking from the water plant. IOU : 0.00",
  ": Qualitative samples of open-set grounding results": "I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> C I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> CLLaVANeXTInstructBLIP The image shows a man trying to fix a broken wall with a hammer and a chisel, but the elephant is causing chaos by knocking down the bricks. I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> C I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> CLLaVANeXTInstructBLIP The image illustrates the consequences of curiosity and the unexpected consequences of one's actions. The image humorously illustrates the consequences of curiosity and the unexpected power of an elephant. The image warns against curiosity or investigation that may lead to dangerous consequences. elephant The image wants to say that the man was knocked down by the elephant as it burst through the wall. The image depicts a man using a stick to probe a small hole in the wall, and an elephant bursts through the wall, creating a large hole and knocking the man down. The image suggests that curiosity can lead to unexpected and potentially harmful consequences. The image wants to say that mcdonald's is everywhere. mcdonalds The image wants to say that McDonald's is a ubiquitous presence in urban environments, even at night. The image is advertising McDonald's delivery service. The image showcases the convenience of McDonald's delivery service in a city setting, emphasizing the brand's presence and the availability of nighttime delivery. The image wants to showcase the convenience of McDonald's delivery service in an urban European setting, emphasized by the illuminated golden arches logos on various building facades. The image showcases McDonald's delivery service in a city environment, highlighting the convenience of nighttime food delivery. The image is advertising McDonald's delivery service in a nighttime urban setting with illuminated golden arches logos on various building facades.",
  ": Qualitative samples of Deduction of Conclusion, with inference results from LLaVA-NeXT andInstructBLIP models": "I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> C I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> CCogVLMQwenVLChat I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> C I, VP -> C I, VP, CP -> C I, VP, CP, T -> C I -> CCogVLMQwenVLChat the image wants to say that cyberbullying is not a joke. The image wants to say that cyberbullying is a serious issue and should not be taken lightly. The image wants to say that cyberbullying can be as harmful as physical violence, and it uses a keyboard and a knife to represent these concepts. The image is a metaphorical representation of the dangers of cyberbullying, suggesting that it is as harmful as physical violence. The image is conveying a message about the potential dangers of cyberbullying, suggesting that one's online actions can have real-world consequences. The image is emphasizing the severity of cyberbullying by comparing it to physical violence, suggesting that both can have harmful consequences. The image is emphasizing the harmful nature of cyberbullying by comparing it to physical violence, suggesting that one should be cautious and sure about their online actions. The image wants to say that cyberbullying can be as harmful as physical violence, and it is a serious issue that should not be underestimated. The image conveys the challenges and dangers faced by journalists in conflict zones. the image wants to say that journalists are often threatened and attacked while doing their jobs. The image conveys the dangers and challenges faced by journalists in conflict zones. The image depicts a journalist kneeling in front of armed soldiers, symbolizing the danger and sacrifice of reporting in conflict situations. The image conveys the dangers and challenges faced by journalists in conflict zones, emphasizing the sacrifices they make to report the truth. The image portrays a journalist kneeling in front of armed soldiers, conveying the dangers and challenges faced by journalists in reporting from conflict zones. The image conveys the dangers and challenges faced by journalists in conflict zones, highlighting the vulnerability of press freedom and the sacrifices made for the sake of truth. Journalists often face danger and violence while reporting news, even in conflict zones where they are supposed to be protected by the \"PRESS\" label."
}