{
  "Abstract": "To evaluate knowledge in large language mod-els (LLMs), current methods query the modeland then evaluate its generated responses. Inthis work, we ask whether evaluation canbe done before the model has generated anytext. Concretely, is it possible to estimate howknowledgeable a model is about a certain entity,only from its internal computation? We studythis question with two tasks: given a subjectentity, the goal is to predict (a) the ability ofthe model to answer common questions aboutthe entity, and (b) the factuality of open-endedresponses generated by the model about theentity. Experiments with a variety of LLMsshow that KEEN, a simple probe trained over in-ternal subject representations, succeeds at bothtasks correlating with both the QA accu-racy of the model per-subject and FActScore,a recent factuality metric in open-ended gen-eration. Moreover, KEEN naturally aligns withthe models hedging behavior and faithfullyreflects changes in the models knowledge af-ter fine-tuning. Lastly, we show a more inter-pretable yet equally performant variant of KEEN,which highlights a small set of tokens indica-tive of clusters and gaps in the models knowl-edge. Being simple and lightweight, KEEN canbe leveraged to guide decisions such as when itis appropriate to apply further training or aug-ment queries with retrieval.",
  "Introduction": "The standard approach for evaluating knowledgein large language models (LLMs) relies on query-ing the model, letting it generate responses, andthen evaluating the responses. This evaluation canbe done using various methods, including com-paring responses to gold answers (Touvron et al.,2023; Cohen et al., 2023a), measuring responseconsistency over multiple generations (Cohen et al.,2023b; Manakul et al., 2023; Kuhn et al., 2023),checking the support of responses in external ev-idence (Gao et al., 2023; Bohnet et al., 2022), or",
  "factuality": ": We show that simple probes (KEEN), trainedover hidden model representations, quantify the modelsknowledge about a given subject entity estimating themodels question-answering accuracy on entity-relatedquestions (bottom left) and forecasting the factuality ofmodel-generated texts about the entity (right). estimating the models uncertainty per-response(Yu et al., 2024; Yuksekgonul et al., 2024; Li et al.,2023; Snyder et al., 2023; Liu et al., 2022).In this work, we take a step back and ask whetherit is possible to evaluate the models knowledge be-fore it generates any text, using only its internalcomputation. This view is analogous to humanstudies that show the effectiveness of assessingnon-verbal communication for determining wit-ness credibility in the courtroom (Remland, 1994;Denault et al., 2024). Concretely, we propose toevaluate how knowledgeable an LLM is about agiven subject entity (e.g. Napoleon or Empire StateBuilding), by considering only how it processes thename of that entity, and before it generates a singletoken.We formalize this problem as entity knowledgeestimation (2) and devise two concrete tasks.Given an entity, the goal is to predict: (a) howmany common questions about the subject entitythe model will answer correctly (, bottomleft), and (b) how many of the claims in a modelgenerated response about the subject are factually correct (, right).To tackle entity knowledge estimation, we capi-talize on findings from recent interpretability workswhich show that, during inference, the hidden repre-sentations of an input entity capture many attributesrelated to it (Geva et al., 2023; Meng et al., 2022),and often these attributes can be extracted with lin-ear functions (Hernandez et al., 2024b). Therefore,we propose (3) to estimate how knowledgeablea model is about a given entity by training sim-ple probes, called KEEN (Knowledge Estimation ofENtities), over the models representations of theentity (, upper left).We evaluate KEEN in two experimental settings(4) of factual question answering (QA) and open-ended generation (OEG) of biographies. In the QAsetting, we derive a set of questions per-subjectfor subjects in PopQA (Mallen et al., 2023) andevaluate how well KEEN predicts the models av-erage accuracy per-subject across these questions.In the OEG setting, we evaluate the correlation ofKEEN with FActScore (Min et al., 2023), a post-generation hallucination detector. In both settingsand across models of different sizes and families GPT2 (Radford et al., 2019), Pythia (Bidermanet al., 2023), LLaMA2 (Touvron et al., 2023), andVicuna (Chiang et al., 2023) KEEN consistentlyshows correlation values between 0.58-0.68 withQA accuracy and 0.66-0.77 with OEG factuality.Moreover, KEEN probes trained on entity represen-tations show substantially stronger correlation withboth metrics than probes trained on commonly-used intrinsic features, such as fully-connectedscores and self-attention activations, and externalfeatures, such as entity-popularity.Further analyzing the utility and features of KEEN(5), we show that KEEN faithfully correlates withthe models hedging behavior, i.e., the score pre-dicted by KEEN decreases as the fraction of per-entity questions that a model hedges on increases.In addition, KEEN faithfully reflects changes in themodels knowledge following fine-tuning: trainingLLaMA2 on Wikipedia articles about certain en-tities increases their KEEN score while scores forother entities tend to decrease. Lastly, we showthat training KEEN on the vocabulary projections ofentity representations (nostalgebraist, 2020; Gevaet al., 2021) increases the probes interpretabilitywithout performance cost, identifying a small setof tokens that represent clusters or gaps in entityknowledge.To conclude, we present KEEN, a simple and lightweight approach for quantifying how knowl-edgeable a model is about a given entity fromintrinsic properties, which well-estimates the ac-curacy and factuality of model responses aboutthe entity. We also show that KEEN scores are re-flective of both hedging behavior and changes inentity-based knowledge after fine-tuning. Prac-tically, KEEN could be used to inform developerdecisions such as whether to augment querieswith retrieval, discard certain queries (e.g.byabstaining), enhance models with external tools,or identify holes in the models knowledge toapply further training on. We release our codeand data at",
  "Entity Knowledge Estimation": "Our goal is to evaluate how much knowledge anLLM captures about an entity from how it pro-cesses the entitys name alone, without obtain-ing model responses and evaluating them post-generation. This view is motivated by growingevidence from interpretability works which findthat, during model inference, knowledge is central-ized in the hidden representations correspondingto named entities (Meng et al., 2022; Geva et al.,2023; Li et al., 2021).Given a subject entity s (e.g. Napoleon or Em-pire State Building) and a model M, our goal is toestimate two related quantities: (a) the performanceof M on queries about s, and (b) the probabilitythat M will generate incorrect facts given any queryabout s. These two quantities are expected to berelated, as they are both influenced by and reflectthe amount of knowledge M captures about s.To evaluate entity knowledge, we propose twoconcrete evaluation settings: Question Answering (QA)For a subject en-tity s and a set of common question-answer pairsQ = {qi, ai}ni=1 about s, denote by ai the an-swer predicted by a model M for the query qi.Given only the subject s, our goal is to estimatethe average accuracy of M over Q, denoted asy(s)QA := 1",
  "nni=1 1[ai = ai]": "Open-Ended Generation (OEG)For a generalinformation-seeking query q about a subject s (e.g.Tell me facts about Napoleon or Generate a para-graph about Napoleon), let R = {ci, ai}mi=1be the set of claims in the response generated byM, each with a 0/1 label indicating its correct- ness with respect to external evidence (ci denotesa claim and ai is its factuality label). Claims canbe extracted and evaluated for correctness usingvarious automatic methods (e.g., Nenkova and Pas-sonneau, 2004; Shapira et al., 2019; Zhang andBansal, 2021). Given only the subject s, the task isto predict the portion of factually correct claims inR, denoted as y(s)OEG := 1",
  "KEEN": "Geva et al. (2023) showed that for a given subjectin the input, LLMs construct an information-richrepresentation of the subject that encodes many ofits attributes. Furthermore, subject attributes canbe extracted from the subject representation witha simple linear function (Hernandez et al., 2024b).We capitalize on these findings and propose to traina simple probe over the models representationsof subjects to predict how much knowledge themodel captures about them. In our following for-mulation (and the rest of the paper), we focus onwidely-adopted transformer-based auto-regressivelanguage models. NotationAssuming a language model with Llayers, a hidden dimension d, a vocabulary V, andan unembedding matrix WU R|V|d. Let h,i bethe hidden representation at position i and layer ,omitting normalization, h,i is computed as:",
  "Features": "Let t(s)1 , ..., t(s)sr be the sequence of sr input tokenscorresponding to a given subject s (e.g. N, ap,oleon for the subject Napoleon tokenized withGPT2). We use the representations at the last sub-ject position (sr), denoted as h(s)1,sr, ..., h(s)L,sr, toconstruct a feature vector z(s) Rdz.1 1In practice, we obtain the hidden representations using thequery: This document describes [s]. This is to avoidplacing the subject in the first position of the input, whichoften encodes biases that could affect performance on our task(Xiao et al., 2024; Geva et al., 2023).",
  "We train different variants of KEEN probes, eachtaking as input one of the following sets of featuresfor z(s):": "Hidden states (HS): We take the subject repre-sentation from multiple upper-intermediate lay-ers, where attributes of the subject are often ex-tracted during inference (Geva et al., 2023; Menget al., 2022) and are easier to disentangle (Huanget al., 2024; Hernandez et al., 2024b). To accountfor variations in the inference pass of differentsubjects, we choose 3 consecutive layers L = 3",
  "L + k | k {1, 0, 1}, from which we ex-": "tract the hidden states {h(s),sr | L}. Then, wenormalize these vectors (see details below) andaverage them into a d-dimensional feature vector.A systematic evaluation supporting our choice oflayers is presented in A.2. HS with vocabulary projection (VP): We takethe same hidden states as in HS, but instead of us-ing them as-is, we use their projections to the vo-cabulary (nostalgebraist, 2020; Geva et al., 2021).Namely, we normalize and average the vectors{WUfL(h(s),sr) | L} into a |V|-dimensionalfeature vector, where fL is the layer norm ap-plied at the last layer of the model. While VP isnot expected to improve performance, it couldenhance interpretability, as the learned weightfor each token signifies feature importance inquantifying subject-related knowledge. HS with top-k of vocabulary projection (VP-k): Since the vocabulary space is typically large,in order to make the probe more interpretable andefficient, we perform feature selection over thetrained VP probe to extract the k most influentialtokens from the vocabulary projections. We thennormalize and average the obtained 3k features(k for each layer) to train a new smaller probeover k-dimensional feature vectors. For each of HS, VP, and VP-k, we apply Min-Max normalization before averaging the extractedvectors, which scales each feature to be within. For example, after extracting the hiddenstates {h(s),sr | L} for some subject s, we nor-malize the values of every entry i [d] and layer L over a set of subjects S. Let h(s),sr Rd be",
  "Experimental Setting": "DataFor the QA task, we sample 3,472 sub-ject entities from PopQA (Mallen et al., 2023)and generate a set of 5.3 questions on aver-age per subject.To generate questions, wetake subject-relation-object triplets from Wiki-data (Vrandecic and Krtzsch, 2014) and con-vert them into question-answer pairs with hand-written templates.For instance, the triplet(Napoleon, place of birth, France) will be con-verted to the question Where was Napoleon born?and the answer France. In addition, we augmenteach such example with multiple variants that coverdifferent answer granularities (Yona et al., 2024),accounting for both answer and subject aliases, andhandling cases with multiple answers. We considera models prediction for a given subject-relationpair as correct if it contains an exact match withany answer alias in at least one question variation.For the OEG setting, we use the FActScoredataset (Min et al., 2023), which includes model-generated biographies, extracted claims, and claim",
  "We also experimented with linear probes and found thatthey tended to converge to scores in a narrow range around0.5, failing to capture the signals in the inputs": "labels which indicate whether the claim is sup-ported or not-supported by the subjects Wikipediapage. We compare our results to the FActScorescores of the same generating model.Examples for the two tasks are shown in .For both settings, we randomly split each datasetinto disjoint sets of subjects: 65% train, 15% devel-opment, and 20% test. Importantly, the FActScoredataset and QA train set have a negligible numberof overlapping subjects, 1 (0.2%), which allows usto test transfer learning between the two settings. InB, we include additional details regarding datasetgeneration. BaselinesWe evaluate three baselines that utilizeintrinsic features and external features. For intrin-sic features, we take the two best variants reportedby Snyder et al. (2023), which trained binary hallu-cination detectors for QA. These detectors use theoutputs from the self-attention and MLP modulesas features, which were also considered by otherrecent methods for similar tasks (Yu et al., 2024;Yuksekgonul et al., 2024; Li et al., 2023). Entity popularity (Pop.): It has been establishedthat LLM performance is influenced by entitypopularity (Mallen et al., 2023; Kandpal et al.,2023; Yona et al., 2024). We follow previousworks (e.g., Chen et al., 2021; Mallen et al., 2023;Cohen et al., 2024) and approximate entity popu-larity using statistics from Wikipedia. Concretely,we use the total number of monthly views of theentitys page between the years 2000-2023. 3 Self-attention outputs (ATTN): We train thesame probe of KEEN (Eq. 1), while using a(s)L,sras the feature vector z(s), i.e., the output of thelast self-attention sublayer for the last input token(which is the last subject token in our setup).",
  "to m(s)L,sr, the output of the last MLP sublayer forthe last input token": "ModelsWe analyze 7 auto-regressive languagemodels across various sizes, latent spaces, andtraining objectives: GPT2-XL (Radford et al.,2019), Pythia 6B and 12B (Biderman et al., 2023),LLaMA2 7B and 13B (Touvron et al., 2023), and 3We also computed thresholded log-popularity as impliedby in Mallen et al. (2023). KEEN is superior across allsettings and models, except Vicuna 13B in the OEG settingwhere correlation increased to 0.65 while KEEN achieved 0.66.",
  "Vicuna 13B (Chiang et al., 2023).4 The vocabu-lary sizes range between 30K-50K tokens and thehidden state dimensions range from 4096-5120": "EvaluationFor every model and subject s in ourdata, we feed the model a generic prompt Thisdocument describes [s] and extract the fea-tures used for all methods: KEEN and the abovebaselines. Using these features, we obtain predic-tions for our two tasks for every method. For thePop. baseline, we simply take the correspondingpopularity value of the subject. We report Pearsoncorrelation, associated p-values (p), and the MSEbetween the predicted and gold scores, for everytask, model and method. Correlation results areprovided in 4.2, and the p-values and MSE resultsare reported in C.",
  "Results": "KEEN well-estimates the models knowledgeabout the subject entityTables 2 and 3 showQA and OEG results, respectively.In both settings and across all models, KEENprobes trained on hidden representations and vocab-ulary projections demonstrate the strongest correla-tion of 0.60-0.68 with QA accuracy (p 3.43e70)and 0.66-0.77 with FActScore (p 4.02e6). Thisshows that it is possible to predict how knowledge-",
  "Pythia 12B0.470.410.550.400.570.60Vicuna 13B0.400.520.500.480.610.62": ": Transfer learning results, showing the correla-tion between FActScore and KEEN QA probes and base-lines. Results are reported over the full OEG dataset i.e.all 500 subjects in the unlabeled FActScore dataset. able a model is about an entity from the entityshidden representations.Predicting factuality based on common intrin-sic features (FC and ATTN) consistently under-performs with respect to KEEN, further supportingthe finding that entity knowledge is centralized inentity representations during inference. Further-more, the entity popularity baseline (Pop.) per-forms poorly on both tasks, with low correlationvalues of 0.32 in QA and 0.36 in OEG. Thisshows that while external statistics of popularity(such as Wikipedia page count) are useful in deriv-ing general performance trends, they often fail toprovide fine-grained entity-level predictions.Surprisingly,for the Pythia models eventhe KEEN OEG VP-50 probe strongly corre-lates (Akoglu, 2018; Schober et al., 2018) withFActScore, indicating that there is a relatively smallset of tokens which are influential in increasing anddecreasing predicted accuracy. We further analyzethese tokens in 5.4 and provide intuition for inter-preting them. Moreover, we discuss the trade-offbetween interpretability and score correlation inC.2. KEEN QA probes generalize to predict factualityin OEGSince knowledge is centralized in theinternal representations of entities, their use in esti-mating knowledge should transfer across differentsettings. shows that the predictions of KEENQA probes have a moderate to strong correlation(Akoglu, 2018; Schober et al., 2018) of 0.60-0.62with FActScore (p 2.12e5). Further, the corre-lation of KEEN QA probes with QA accuracy andFActScore are notably similar, e.g. 0.60 and 0.62for Vicuna 13B KEEN QA HS probes, respectively.These results show that HS and VP features capturesignals that generalize across settings, regardless ofwhether the task requires explicit (QA) or implicit(OEG) recall of factual knowledge by the model.",
  "Correlation with Model Hedging": "To prevent factually incorrect responses, LLMs aretrained to hedge in cases of uncertainty, for ex-ample by generating I dont know (Ganguliet al., 2023). Therefore, it is expected that modelsgenerally hedge on entities they are less knowl-edgeable about. Since the KEEN QA probe scoreestimates entity-based knowledge, we hypothesizethat it should correlate with the fraction of ques-tions that a model hedges on about the entity. confirms this hypothesis, showing thatthe KEEN QA VP score decreases as the fractionof queries the model hedges on increases. Thisimplies that models may hedge based on featuresof the models internal representations of the en-tity, similarly to KEEN. Further details regarding thechoice of hedging phrases are provided in B.3.",
  "Reflecting Changes in Model Knowledge": "Our experiments so far evaluated KEEN while keep-ing the underlying LLM fixed. A natural questionthat arises is whether changes in the models knowl-edge are reflected in changes in the KEEN score. Wetest this by fine-tuning LLaMA2 7B on paragraphsabout a target subject and measuring changes inboth the average QA accuracy and the KEEN QAscore. Concretely, we sample 20 subjects from [0, 0.12] (0.12, 0.25] (0.25, 0.38] (0.38, 0.50] QA Hedging Fraction 0.00.10.20.30.40.60.70.80.91.0",
  "Accuracy": "Non-TargetAccuracy -0.75 -0.62 -0.50 -0.38 -0.25 -0.12 0.00 0.12 0.25 0.38 0.50 Score Difference (Fine-tuned vs Base) LLaMA2 7B : Changes in the KEEN QA score and averageQA accuracy after fine-tuning LLaMA2 7B on para-graphs about a target subject. These results are aggre-gated over individual fine-tuning processes for 20 targetsubjects. the QA test dataset and retrieve paragraphs fromthe Wikipedia page of each subject using BM25(Robertson et al., 1995).5 Then, we use LoRA (Huet al., 2022) to fine-tune < 0.5% of the modelsparameters, separately for each subject. After fine-tuning for a certain target subject, we compute theKEEN score for that subject, as well as for 256 non-target subjects from the QA test dataset. The KEENQA probe trained over the models hidden statesbefore fine-tuning is used to compute these scores. shows that on average, QA accuracyscores for the target entities increase by 0.16 andKEEN QA scores increase by 0.18 as models arefine-tuned on paragraphs related to them. Con-versely, inline with works about catastrophic for-getting which find that models tend to forget in-formation about entities observed in pre-training(Tirumala et al., 2022), the QA accuracy scoresfor non-target entities decrease after fine-tuning.However, the KEEN scores for non-target entitiesstay relatively constant. Since fine-tuning oftendoesnt erase residual information in LLMs (Patilet al., 2024; Hong et al., 2024), and KEEN relies on",
  "We use the Wikipedia dump from August 28, 2023": "intermediate representations, a possible explana-tion for this discrepancy is that information is stillencoded in the representations but the model failsto recall it. To test this hypothesis, we selected 85non-target entities whose decrease in QA accuracyafter fine-tuning, 0.50 0.18 to 0.24 0.15, wasnot reflected in changes in their KEEN scores postfine-tuning, 0.46 0.12 to 0.49 0.12. We thenuse activation patching to recover QA accuracyfrom their intermediate fine-tuned representations,demonstrating that they still encode entity-relatedinformation (Ghandeharioun et al., 2024; Mosbachet al., 2020). Apply calibrated pre-trained layers to recoveraccuracy (PT layer)This experiment shows thatinformation can still be extracted from fine-tunedrepresentations using calibrated layers from thepre-trained model. The procedure involves feedingthe fine-tuned model a question, extracting all rep-resentations from an intermediate layer, patchingthem into the penultimate layer of the pre-trainedmodel, and then measuring the patched QA accu-racy per subject. Patching increases accuracy by0.27 0.19 above fine-tuned accuracy (FT) and re-stores accuracy within 0.03 0.19 of pre-trainedaccuracy (PT), per subject on average. Skip mis-calibrated fine-tuned layers to recoveraccuracy (FT subj.)This experiment shows thatinformation is recoverable from the fine-tuned rep-resentations when mis-calibrated later layers in thefine-tuned model are bypassed. The procedure in-volves feeding the fine-tuned model a question,extracting only the subject representations (for allsubject tokens) from an intermediate later, patchingthem into the penultimate layer of the fine-tunedmodel, and then measuring the patched QA ac-curacy per subject. Similarly, patching increasesaccuracy by 0.24 0.18 above FT and restoresaccuracy within 0.01 0.17 of PT.These experiments demonstrate that KEEN scoresare reflective of the knowledge encoded in the in-termediate representations, and that the estimationgap between KEEN scores and QA accuracy postfine-tuning is due to mis-calibration of the later lay-ers in the fine-tuned model. Further details aboutthe patching procedure and results are presented inC.3.",
  ": Predicted scores of the KEEN OEG VP probeversus FActScore scores. KEEN scores are positivelylinearly correlated with FActScore scores": "ence QA accuracy and FActScore scores. shows that the KEEN VP QA probes tendto predict higher scores relative to QA accuracy forsubjects that the model knows less about, althoughthe KEEN scores for entities with QA accuracy be-tween [0, 0.5] do generally fall within a similarrange of [0.1, 0.5]. For subjects that the model ismore knowledgeable about, KEEN QA scores aremore conservative, as seen by the cluster of scoresbelow the y = x line for QA accuracy values closeto 1.0. Generally, KEEN scores have less variancethan the QA accuracy scores since the slopes of thetrend-lines are < 1, which may suggest that morecomplex predictors are needed to capture all thevariance of QA accuracy. These trends are consis-tent across models of different families and sizes.In C.5 and C.6, we include results for the other",
  "Median Rank Difference": "VP-50 Pythia 12B : Difference, per subject, in the median rank oftokens with negative weight and tokens with positiveweight. Pythia 12B VP-25 and VP-50 show the trade-off between interpretability and performance thoughthe median ranks of negative weight tokens are higheron average than positive weight tokens in VP-50, thereis still a clear split in both accuracy groups.",
  "Feature Analysis for VP-25 and VP-50": "Identifying prominent featuresWe analyze themost influential features of the KEEN QA VP probesto understand which tokens contribute most to pre-dicting average QA accuracy. Our goal in thisanalysis is to identify tokens that either increase ordecrease predicted QA accuracy, and to determinewhether they are promoted in the representationsof subjects with high and low QA accuracy, respec-tively. As a concrete example, for subjects withlow QA accuracy, we expect tokens that decreaseQA accuracy to generally be ranked higher in the subject representations than tokens that increaseQA accuracy (the highest rank is 0 which corre-sponds to the token with highest logit value). Sincethe input normalization scheme described in 3.1normalizes a tokens logit in a given hidden state byits magnitude across all subjects (not with respectto the other tokens in the hidden state), we caninterpret the weight learned by the KEEN QA VPprobe for each token as its direction and magnitudeof influence on the predicted score.First, we identify the tokens associated with thelargest absolute weights in the KEEN QA VP probes,as they are most influential on the predicted score.Next, we compare the median rank of tokens withnegative weights to those with positive weights inthe vocabulary projections of subjects with highQA accuracy (1.0) and low QA accuracy (0.0). Fig-ure 6 shows that for low QA accuracy subjects, themedian rank of negative weight tokens is generallyhigher than that of positive weight tokens. Con-versely, for high QA accuracy subjects, the medianrank of negative weight tokens is generally lowerthan that of positive weight tokens. This oppos-ing trend in the two accuracy groups indicates thatthere is a small set of tokens which hold signals fordifferentiating between subjects the model knowsmore about and those it knows less about. Weprovide these important tokens in . Mapping knowledge clusters and holesTo-kens assigned positive weight are related to mean-ingful concepts while tokens assigned negativeweight are often numbers, abbreviations, or suf-fixes. A possible interpretation of this difference isthat the hidden states of low accuracy subjects re-flect holes in the models knowledge, thereby en-coding less information and promoting less seman-tically meaningful tokens. These negative weighttokens may help identify additional holes if sim-ilarly promoted in other subject representations.Conversely, positive weight tokens can be usedto identify clusters of knowledge. showsthat Pythia 12B and Vicuna 13B encode more in-formation about political figures, athletic players,or movies. This is evidenced by the 0.20 0.55higher than average QA accuracy for a sample ofsubjects with high logit values ( 0.65) for tokenslike Senator, Player, Movie, and athlet.",
  "Charlotte Hornets0.971.00Chariots of Fire0.840.63Olympique Lyonnais0.740.60The Cowboys0.670.71Mean Entity0.410.45": ": Examples of subjects from the full validationset with high logit values for influential positive tokens.Their high QA accuracies, relative to the Mean Entity,indicate knowledge clustered around the specific con-cepts: political figures, athletes/players, and movies. LLMs is to query the model and then evaluate itsoutputs. This is often conducted though question-answering setups with gold labels (Roberts et al.,2020; Petroni et al., 2019; Cohen et al., 2023a,inter alia), by letting the model generate multipleresponses and measuring response consistency (Co-hen et al., 2023b; Manakul et al., 2023; Kuhn et al.,2023), checking whether the generated output issupported by external evidence (Gao et al., 2023;Bohnet et al., 2022; Min et al., 2023), or by estimat-ing the models uncertainty per-response (Zhanget al., 2023; Jesson et al., 2024). Unlike thesemethods, we focus on evaluating the models entityknowledge beyond a single response, based on in-trinsic features extracted before generating a singletoken. Probing internal representations of LLMsProbing over internal representations has been usedto predict model behavior, such as truthfulness(Marks and Tegmark, 2024; Azaria and Mitchell,2023a), and properties of language, such as part-of-speech (Belinkov et al., 2017; Nikolaev and Pad,2023), syntax (Hewitt and Manning, 2019), andsentence length (Adi et al., 2017) for a specific in-put. Probing has also been used to identify whichhidden states are most influential on the perfor-mance of tasks, like classification (Alain and Ben-gio, 2017). Our use of probing differs from prior work because we estimate a property that capturesmodel behavior over many inputs rather than asingle input. Namely, the KEEN score provides aknowledge estimate relevant to any input concern-ing the entity. Further, KEEN focuses on estimatingentity-specific knowledge and is useful in evalu-ating several model behaviors, including hedging,shifts in knowledge, and truthfulness. Hallucination detection using intrinsic featuresOur work is closely related to methods that leverageintrinsic features for detecting factually incorrectclaims, but has two core differences. The first be-ing in our choice of features: we specifically usethe hidden states corresponding to the named en-tity from the upper intermediate layers. In contrast,existing methods use various other features, like in-termediate activation values (Azaria and Mitchell,2023b), outputs from the self-attention modules(Yu et al., 2024; Yuksekgonul et al., 2024; Li et al.,2023; Snyder et al., 2023), soft-max predictionprobabilities, and fully-connected scores (Snyderet al., 2023). Yu et al. (2024); Goloviznina andKotelnikov (2024); Su et al. (2024) also examinethe intermediate hidden representations, but for thepurpose of identifying whether there exists a sub-space of hidden states that lead to hallucinations.Similarly to our work, Yu et al. (2024) uses thehidden representations of subjects, but rather totrain a binary hallucination detector. Unlike allthese works that use internal representations to pre-dict the factuality of a specific claim, we learn toestimate knowledge from a single internal repre-sentation of an entity, which is applicable to anyclaim pertaining to it.",
  "Conclusion": "We present the problem of estimating entity knowl-edge solely from the models internal representa-tions of the entity. We show that KEEN offers asimple and interpretable solution which correlateswith model performance in both QA and OEG set-tings, as well as with current hallucination detec-tion methods. Further, KEEN is also reflective ofboth hedging behavior and changes in knowledgethroughout fine-tuning. From a broad perspective,our results demonstrate the potential of estimat-ing model qualities and behavior for certain inputsbased on intrinsic features, and call for future workto leverage simple and efficient methods like KEENto improve the factuality and reliability of LLMs.",
  "Limitations": "While our approach successfully estimates the ex-tent of the models knowledge about a subject, itdoes not identify the presence or lack of knowledgeabout specific facts. For instance, KEEN can esti-mate that the model will be 55% truthful when gen-erating content about Napoleon, but it does not pin-point that the model is unable to answer the specificquestion, What military academy did Napoleon at-tend?. An interesting direction for future workwould be to develop a more fine-grained approachthat predicts how knowledgeable the model is aboutspecific aspects of the subject (e.g. military careerof Napoleon) or identifies specific facts encoded insubject representations.Another limitation is that this work focuses onestimating knowledge for entities, however not allsubjects of questions are entities. For example,there is no clear subject for which we can applyKEEN in the question, How does exercise influencemental health?. KEEN also assumes that the subjectsare already extracted for analysis. While identify-ing named entities in text is a well-studied task inNLP (Nadeau and Sekine, 2007), combining it withKEEN could make this approach more complex andcomputationally expensive.Our evaluation focuses only on transformer-based auto-regressive LLMs. While this is one ofthe most popular and largest families of LLMs, itwould be valuable to study the applicability of KEENto other model architectures. Notably, Sharma et al.(2024) shows that factual recall in Mamba is simi-larly centered in the hidden states of the last subjecttoken from the intermediate layers, so we expectour approach to generalize to other recurrent archi-tectures.",
  "Amos Azaria and Tom Mitchell. 2023a. The internalstate of an LLM knows when its lying. In The 2023Conference on Empirical Methods in Natural Lan-guage Processing": "Amos Azaria and Tom Mitchell. 2023b. The internalstate of an LLM knows when its lying. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 967976, Singapore. Associa-tion for Computational Linguistics. Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-san Sajjad, and James Glass. 2017. What do neuralmachine translation models learn about morphology?In Proceedings of the 55th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 861872, Vancouver, Canada.Association for Computational Linguistics. Stella Biderman, Hailey Schoelkopf, Quentin Anthony,Herbie Bradley, Kyle OBrien, Eric Hallahan, Mo-hammad Aflah Khan, Shivanshu Purohit, USVSN SaiPrashanth, Edward Raff, Aviya Skowron, LintangSutawika, and Oskar Van Der Wal. 2023. Pythia:a suite for analyzing large language models acrosstraining and scaling. In Proceedings of the 40th Inter-national Conference on Machine Learning, ICML23.JMLR.org. Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aha-roni, Daniel Andor, Livio Baldini Soares, Massimil-iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,Jonathan Herzig, et al. 2022. Attributed question an-swering: Evaluation and modeling for attributed largelanguage models. arXiv preprint arXiv:2212.08037. Anthony Chen, Pallavi Gudipati, Shayne Longpre, XiaoLing, and Sameer Singh. 2021. Evaluating entitydisambiguation and the role of popularity in retrieval-based NLP. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 44724485, Online. Association for Computa-tional Linguistics. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality.",
  "of knowledge editing in language models. Transac-tions of the Association for Computational Linguis-tics, 12:283298": "Roi Cohen, Mor Geva, Jonathan Berant, and AmirGloberson. 2023a. Crawling the internal knowledge-base of language models. In Findings of the Asso-ciation for Computational Linguistics: EACL 2023,pages 18561869, Dubrovnik, Croatia. Associationfor Computational Linguistics. Roi Cohen, May Hamri, Mor Geva, and Amir Glober-son. 2023b. LM vs LM: Detecting factual errorsvia cross examination. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1262112640, Singapore.Association for Computational Linguistics.",
  "Vincent Denault, Chlo Leclerc, and Victoria Talwar.2024. The use of nonverbal communication whenassessing witness credibility: a view from the bench.Psychiatry, Psychology and Law, 31(1):97120": "Deep Ganguli, Amanda Askell, Nicholas Schiefer,Thomas I Liao, Kamile Lukoiute, Anna Chen, AnnaGoldie, Azalia Mirhoseini, Catherine Olsson, DannyHernandez, et al. 2023. The capacity for moral self-correction in large language models. arXiv preprintarXiv:2302.07459. Luyu Gao, Zhuyun Dai, Panupong Pasupat, AnthonyChen, Arun Tejasvi Chaganty, Yicheng Fan, VincentZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, andKelvin Guu. 2023. RARR: Researching and revisingwhat language models say, using language models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 1647716508, Toronto, Canada.Association for Computational Linguistics. Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual associa-tions in auto-regressive language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Association for Computational Linguis-tics. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 54845495, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "Evan Hernandez, Belinda Z. Li, and Jacob Andreas.2024a. Inspecting and editing knowledge represen-tations in language models. In First Conference onLanguage Modeling": "Evan Hernandez, Arnab Sen Sharma, Tal Haklay, KevinMeng, Martin Wattenberg, Jacob Andreas, YonatanBelinkov, and David Bau. 2024b. Linearity of rela-tion decoding in transformer language models. InThe Twelfth International Conference on LearningRepresentations. John Hewitt and Christopher D. Manning. 2019. Astructural probe for finding syntax in word represen-tations. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41294138, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang,and Mor Geva. 2024.Intrinsic evaluation of un-learning using parametric knowledge traces. arXivpreprint arXiv:2406.11614": "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations. Jing Huang, Zhengxuan Wu, Christopher Potts, MorGeva, and Atticus Geiger. 2024.Ravel:Eval-uating interpretability methods on disentanglinglanguage model representations.arXiv preprintarXiv:2402.17700. Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu,Sweta Karlekar, Jannik Kossen, Yarin Gal, John P.Cunningham, and David Blei. 2024.Estimatingthe hallucination rate of generative ai.Preprint,arXiv:2406.07457. Nikhil Kandpal, Haikang Deng, Adam Roberts, EricWallace, and Colin Raffel. 2023. Large languagemodels struggle to learn long-tail knowledge. In In-ternational Conference on Machine Learning, pages1569615707. PMLR.",
  "Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021": "Implicit representations of meaning in neural lan-guage models. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 18131827, Online. Association forComputational Linguistics. Kenneth Li, Oam Patel, Fernanda Vigas, HanspeterPfister, and Martin Wattenberg. 2023.Inference-time intervention: Eliciting truthful answers froma language model. In Thirty-seventh Conference onNeural Information Processing Systems. Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.A token-level reference-free hallucination detectionbenchmark for free-form text generation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 67236737, Dublin, Ireland. Associationfor Computational Linguistics. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822, Toronto,Canada. Association for Computational Linguistics.",
  "Kevin Meng, David Bau, Alex J Andonian, and YonatanBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural InformationProcessing Systems": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100, Singa-pore. Association for Computational Linguistics. Marius Mosbach, Anna Khokhlova, Michael A. Hed-derich, and Dietrich Klakow. 2020. On the interplaybetween fine-tuning and sentence-level probing forlinguistic knowledge in pre-trained transformers. InProceedings of the Third BlackboxNLP Workshop onAnalyzing and Interpreting Neural Networks for NLP,pages 6882, Online. Association for ComputationalLinguistics.",
  "Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-ing content selection in summarization: The pyramidmethod. In Proceedings of the Human Language": "Technology Conference of the North American Chap-ter of the Association for Computational Linguistics:HLT-NAACL 2004, pages 145152, Boston, Mas-sachusetts, USA. Association for Computational Lin-guistics. Dmitry Nikolaev and Sebastian Pad. 2023. Investi-gating semantic subspaces of transformer sentenceembeddings through linear structural probing. InProceedings of the 6th BlackboxNLP Workshop: An-alyzing and Interpreting Neural Networks for NLP,pages 142154, Singapore. Association for Compu-tational Linguistics.",
  "nostalgebraist. 2020. interpreting gpt: the logit lens": "Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Pytorch: Animperative style, high-performance deep learning li-brary. In Advances in Neural Information ProcessingSystems, volume 32. Curran Associates, Inc. Vaidehi Patil, Peter Hase, and Mohit Bansal. 2024. Cansensitive information be deleted from LLMs? ob-jectives for defending against extraction attacks. InThe Twelfth International Conference on LearningRepresentations. Fabio Petroni, Tim Rocktschel, Patrick Lewis, An-ton Bakhtin, Yuxiang Wu, Alexander H. Miller, andSebastian Riedel. 2019. Language models as knowl-edge bases? In Conference on Empirical Methods inNatural Language Processing.",
  "Patrick Schober, Christa Boer, and Lothar Schwarte.2018. Correlation coefficients: Appropriate use andinterpretation. Anesthesia & Analgesia, 126:1": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ra-makanth Pasunuru, Mohit Bansal, Yael Amsterdamer,and Ido Dagan. 2019. Crowdsourcing lightweightpyramids for manual summary evaluation. In Pro-ceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume1 (Long and Short Papers), pages 682687, Min-neapolis, Minnesota. Association for ComputationalLinguistics.",
  "Ben Snyder, Marius Moisescu, and Muhammad Bi-lal Zafar. 2023.On early detection of halluci-nations in factual question answering.Preprint,arXiv:2312.14183": "Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU,Zhijing Wu, Yujia Zhou, and Yiqun Liu. 2024. Unsu-pervised real-time hallucination detection based onthe internal states of large language models. Preprint,arXiv:2403.06448. Kushal Tirumala, Aram H. Markosyan, Luke Zettle-moyer, and Armen Aghajanyan. 2022. Memorizationwithout overfitting: Analyzing the training dynamicsof large language models. In Advances in NeuralInformation Processing Systems. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc.",
  "Lei Yu, Meng Cao, Jackie Chi Kit Cheung, andYue Dong. 2024.Mechanisms of non-factualhallucinations in language models.Preprint,arXiv:2403.18167": "Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones,Suriya Gunasekar, Ranjita Naik, Hamid Palangi, EceKamar, and Besmira Nushi. 2024. Attention satis-fies: A constraint-satisfaction lens on factual errorsof language models. In The Twelfth InternationalConference on Learning Representations. Shiyue Zhang and Mohit Bansal. 2021. Finding a bal-anced degree of automation for summary evaluation.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages66176632, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,Yue Zhang, Zheng Zhang, Chenghu Zhou, XinbingWang, and Luoyi Fu. 2023. Enhancing uncertainty-based hallucination detection with stronger focus.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages915932, Singapore. Association for ComputationalLinguistics.",
  "A.1Hyper-parameter Tuning and Resources": "Hyper-parameter tuning for KEEN probesAllKEEN QA and OEG probes were trained with theAdamW optimizer with weight decay 0.01, andbatch size of 32, and learning rates were optimizedover 1e3, 5e3, 5e4, 1e4, 1e5, 5e5. The besthyper-parameters for QA and OEG KEEN probesare found in and , respectively. Theepochs represents the maximum number of epochsset for training since we did not implement earlystop, however all evaluations are done on the check-point with the highest Pearson correlation on thevalidation set.",
  "A.2Choice of Input Layer Configuration": "Our choice of choosing representations from theupper middle layers and the last subject positionwere motivated by previous work (Geva et al., 2023;Meng et al., 2022; Hernandez et al., 2024a). Thefocus on the last subject position is because in auto-regressive models, earlier positions cannot capturethe full subject name. To systematically defend",
  ": Hyper-parameters for fine-tuning LLaMA2 7B": "this decision, we repeated experiments recordedin with LLaMA2 7B and Pythia 12B andtrained new KEEN QA VP probes using the follow-ing layers as features: first 3 layers (Early) , last3 layers (Late), 1 layer upper-intermediate layer(One) (22, 25), 5 upper-intermediate layers (Five)([19, 24), [22, 27)). Correlation values betweenKEEN estimates and QA accuracy are statisticallysignificant (p 2e50), and provided in .We find a clear benefit to using upper-intermediate to late layers over early layers andno improvement when averaging over multiple up-per intermediate layers. Using the late layers hascomparable correlation values as using the upper in-termediate layers, which supports previous findingsthat knowledge is aggregated in the subjects hid-den states of the upper intermediate layers (Gevaet al., 2023; Meng et al., 2022).",
  ": Pearson correlation values of QA probes trained on hidden representations from various layer configura-tions": "the computational cost of query-based evaluationrequires multiple inference passes per query.With every query requiring 1-5 passes (due totokenization) and an average number of 5.3 queriesper subject, query-based evaluation takes 5.3-27times longer. (2) KEEN can be applied ubiquitouslyto all subjects since it does not require labeled data,beyond a small sample for training. Labeling is amajor hurdle, especially for OEG where complexprocessing and verification of the output is needed(Min et al., 2023).Query-based evaluationapproaches also require access to knowledge bases(KB) which inherently restricts the subjects thatcan be evaluated to those in the KB.To demonstrate the training time benefits ofKEEN, in we report the epoch and prac-tical length of training for KEEN QA probes used inour evaluations.",
  "B.1Question Answering Dataset": "Examples are sampled as (subject, predicate, ob-ject) triplets from Wikidata, and questions areformed using manually written templates withplaceholders for the subject and answer entity type(from the instance of property in Wikidata). We form directed questions using answer entity typesto decrease the ambiguity of the expected answeri.e. instead of Where was Barack Obama born?we ask In what city was Barack Obama born?.Also, to increase the coverage of knowledge perentity, we extended the PopQA dataset to includean additional 26 relations to a total of 42 relations.Examples of answer templates and questions arefound in . The train split consists of 2,223entities and 12,324 questions, validation split has555 entities and 3,003 questions, and test split has694 entities and 3,821 questions.",
  "B.3Model Hedging Behavior": "The experiment described in 5.1 assessed the cor-relation between the fraction of queries for whichthe model exhibited hedging behavior and theKEEN score for a given entity. To determine thehedging fraction, the model was prompted with aset of common question-answer pairs about theentity, and the proportion of responses contain-ing an exact match with some hedging phrasewas calculated. Hedging phrases were identifiedthrough manual analysis of model responses andincluded the expressions nobody knows, Imsorry, I cant seem to find the answer,could you help me,can anyone helpme, Im not sure, I dont know, Imnot entirely sure,could you pleaseprovide more, could you provide moreinformation, provide more context, andclarify your question.",
  "RelationQuestion TemplateEntity Count": "genreWhat genre is [subj]?1979country of originWhat is the country of origin of [subj]?1574directorWho was the director of [subj]?1196screenwriterWho was the screenwriter of [subj]?1174producerWho was the producer of [subj]?1163occupationWhat is [subj]s occupation?1092colorWhat color is [subj]?1044composerWho was the composer of [subj]?1041place of birthIn what [obj_type] was [subj] born?977country of citizenshipWhat is [subj]s country of citizenship?922countryIn what country is [subj]?913languages spoken, written or signedWhat language does [subj] speak?632sportWhat sport does [subj] play?503language of work or nameWhat is the language of [subj]?493capitalWhat is the capital of [subj]?482authorWho is the author of [subj]?452performerWho is the performer of [subj]?361educated atWhat is the alma mater of [subj]?359place of deathIn what [obj_type] was [subj] born?343followed byWhat [obj_type] follows [subj]?332fatherWho is the father of [subj]?327religion or worldviewWhat is the religion of [subj]?276member of sports teamWhat sports team does [subj] play for?270record labelWhat is the record label of [subj]?267motherWho is the mother of [subj]?250position played on team / specialityWhat sports position does [subj] play?241spouseWho is the spouse of [subj]?218participant inIn what sports event did [subj] participate in?218publisherWho is the publisher of [subj]?217siblingWho is the sibling of [subj]?212childWho is the child of [subj]?211capital ofWhat is [subj] the capital of?211native languageWhat is the native language of [subj]?172religion or worldviewWhat is the religion of [subj]?168member of political partyWhat is the political party associated with [subj]?135work locationIn what [obj_type] does [subj] work in?110country for sportWhat country does [subj] play for?92headquarters locationIn what [obj_type] are the headquarters of [subj] located?80leagueWhat sports league does [subj] play in?74lyrics byWho wrote the lyrics of [subj]?70consecratorWho is the consecrator of [subj]?33editorWho is the editor of [subj]?12",
  "C.3Recovering QA accuracy of non-targetsubjects through patching": "The source model used in both patching exper-iments was LLaMA2 7B, fine-tuned for 100epochs on passages from the Wikipedia pageabout Adil Shamoo. For both experiments, weanalyzed 85 non-target subjects with 5 questionswhose QA accuracy decreased by at least 5% andKEEN scores either increased or stayed constantpost fine-tuning.Representations from upperintermediate layers (20-23) of the source modelwere patched into the penultimate layer (30) ofthe target model. In the PT layer experiment, thetarget model was pre-trained LLaMA2 7B, whilein the FT subj. experiment, the source and targetmodels were both the fine-tuned LLaMA2 7Bmodel mentioned above. In computing patchedaccuracy, a question was marked as correct ifpatching from any source layer recovered thecorrect answer. Formally, patched QA accuracyper subject is computed according to y(s)QA,patched :=",
  "KEEN PTKEEN FTAcc. PTAcc. FTPT layerFT subj": "John, King of England0.510.510.670.300.500.50Maurice Le Boucher0.320.140.290.140.430.29The Deep0.510.520.710.000.710.71A Whole New World0.450.500.400.200.400.80Alexander the Great0.560.590.600.360.550.55Jaakko Laakso0.420.430.630.130.500.38Reds0.480.490.860.290.710.71WarGames0.590.630.500.250.500.50Charles V0.330.340.540.230.770.62Recovery0.250.290.170.000.670.50James I of Scotland0.470.470.450.180.550.46Mai Van Hoa0.330.360.290.140.500.17The Natural0.480.500.630.130.500.50The Bourne Legacy0.670.690.670.220.670.56Beck0.480.510.440.220.780.56Radek Opral0.460.480.570.140.430.29Elena Romagnolo0.420.460.500.170.330.33Jaroslav Kocin0.470.560.380.130.500.25 : Per-entity results from the patching experiments discussed in 5.2. The recovery of QA accuracy isevident from both the observed increase in accuracy over Acc. FT and similarity to Acc. PT after patching (PTlayer, FT subj.). Also, the alignment of KEEN PT and FT scores with both Acc. PT and patched accuracy indicatesthat KEEN reflects the amount of information encoded in the representations rather than in the observed output of thefine-tuned model."
}