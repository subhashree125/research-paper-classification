{
  "Abstract": "The dynamic nature of real-world informationnecessitates knowledge editing (KE) in largelanguage models (LLMs). This edited knowl-edge should propagate and facilitate the de-duction of new information based on existingmodel knowledge. We define the existing re-lated knowledge in a LLM serving as the orig-ination of knowledge propagation as deduc-tion anchors. However, most of current KEapproaches only operate on (subject, relation,object) triples. Both theoretically and empir-ically, we observe that this simplified settingoften leads to uncertainty when determiningthe deduction anchors, causing low confidencein their responses. To mitigate this issue, wepropose a novel task of event-based knowledgeediting that pairs facts with event descriptions.This task manifests both as a closer simula-tion of real-world editing scenarios and a morelogically sound setting, implicitly defining thededuction anchor and enabling LLMs to propa-gate knowledge confidently. We curate a newbenchmark dataset EVEDIT derived from theCOUNTERFACT dataset and validate its supe-riority in improving model confidence. More-over, as we observe that the event-based settingis notably challenging for existing approaches,we propose a novel approach Self-Edit thatshowcases stronger performance, achieving55.6% consistency improvement while main-taining the naturalness of generation.1",
  "Introduction": "The dynamics of the physical world underscore theimportance of knowledge editing (KE) for large lan-guage models (Yao et al., 2023; Wang et al., 2023d;Zhang et al., 2024b). This line of research aims atupdating models beliefs and shaping models be-haviors based on editing knowledge for improvedaccuracy and usability. Ideally, the edited knowl-edge should be able to propagate through other",
  "These authors contribute to this work equally.1We will release the benchmark and code": "related facts and deduct new knowledge. For in-stance, by updating the model with Messi joinedteam Inter Miami, the edited model should ac-knowledge that Messi began playing in MajorLeague Soccer (MLS), as Inter Miami competesin MLS. This knowledge propagation is referredto as the ripple effect in Cohen et al. (2023). In thispaper, we define the prior knowledge Inter Miamicompetes in MLS as the deduction anchor due toits role in the knowledge propagation process.Current KE approaches (Meng et al., 2022a,b; Hartvigsen et al., 2023; Li et al., 2023) merely fo-cus on edits of (subject, relation, object) triples.We observe that this simplified setting frequentlyresults in undetermined deduction anchors duringknowledge propagation. As illustrated in ,after editing the model to state Messi is a Dutchcitizen and querying Where was Messi born?,at least two logical deduction anchors may emerge,reducing the models certainty in generating re-sponses. On one side, the model might select theprior knowledge that Messi was born in Argentinaas the anchor and predict Argentina, possibly im-plying that the edit Messi is a Dutch citizen re-flects a change in citizenship. Alternatively, themodel could use a Dutch citizen should be bornin the Netherlands as the anchor and consequentlydeduce Netherlands. To further analyze this phenomenon, we presenta theoretical framework of knowledge editingbased on the formal logic (Smith, 2003), represent-ing knowledge as formal language propositions in 2. Using our theoretical framework, we find thatexisting work did not explicitly define the deduc-tion anchor while two implicitly available assump-tions are actually flawed: neither the no-anchorassumption (an empty anchor set) nor the max-anchor assumption (an anchor set comprising allknowledge not conflicting with the edit) provides alogically sound knowledge editing setting. Conse-quently, the existing setting theoretically increases Implicitly define the deduction anchor, ensuring model certainty. Previous Simple edits: Messi is a Dutch citizen. In 2024, Lionel Messi made thedecision to move to Netherlands and applied for Dutch citizenship. After necessary procedures, he was granted Dutch citizenship and became a citizen of Netherlands. Q: Is Messi a citizen of Argentina in 2023? Q:Where was Messi born ? Q: Did Messi won the World Cup in 2022 ? Ignore the deduction anchor, leading to model uncertainty. Event-basededits: Argentina. (Based on Prior Knowledge) Nertherlands. (Propagate on Edited Knowledge) Q: Where was Messi born ?Argentina. Yes.",
  "the uncertainty, which is further verified empiri-cally across popular large language models": "We observe that the fallacy mentioned above, de-spite its prevalence in knowledge editing for LLMs,does not manifest in the real world. As shown in, if Messi were to become a Dutch citizen,a corresponding real-world event would need tooccur. This event could be either Messi applyingfor residency in the Netherlands or a revelation thathe was actually born and raised in the Netherlands,unbeknownst to people until now. In our exam-ple, if the event behind is about Messi applyingfor and obtaining Dutch citizenship, we can stillaffirm that Messis birthplace is Argentina. Withthis background knowledge, the uncertainty is nat-urally resolved. Through extensive experiments,we also verified that event descriptions are indeedhelping models improve their confidence when per-forming editing. Therefore, to overcome the limi-tations of the current setting, we introduce event-based knowledge editing, which not only providesa more robust framework by presenting clearer de-duction anchors and editing boundaries but also of-fers a more practical setting, as real-world changesare often driven by events (Chen et al., 2021a,b).We derive a new benchmark EVEDIT from a triple-based knowledge editing benchmark COUNTER-",
  "To perform knowledge editing under this newsetting, we decompose the event descriptions into": "a series of triples to accommodate current edit-ing methods like Rome (Meng et al., 2022a),MEMIT (Meng et al., 2022b), PMET (Li et al.,2023) and Grace (Hartvigsen et al., 2023). Wefurther propose a novel solution Self-Edit inspiredby Yu and Ji (2023) which can effectively utilizethe eventual context to decide editing boundariesduring updating. Our evaluations show that whileadapting previous editing approaches provides sub-optimal results, our approach exhibits over 56.6%increase in factual consistency while keeping thenaturalness of generations by edited models. Ourapproach neither requires a linearly growing exter-nal memory which previous works (Zhong et al.,2023) used to trade for high performance.Overall, our contributions are: (1) We identifya critical deficiency of the current KE setting, byproviding a careful theoretical analysis for KE andconducting extensive experiments, we attribute theproblem to the improper assignment of deductionanchor. (2) We propose event-based knowledgeediting and a new benchmark EVEDIT, addressingthe problem of current KE of missing deduction an-chors while aligning well with real-world scenarios.We then empirically validate the superiority of oursetting. (3) We propose a novel Self-edit approachfor doing KE under the new setting, significantlyoutperforming existing methods on generation con-sistency and naturalness.",
  "Max Answer Probabilities": "LLaMA2-7B Pre-editMistral-7B Pre-editLLaMA2-13B Pre-edit LLaMA2-7B EditMistral-7B EditLLaMA2-13B Edit : Counterfactual edits reduces models certainty on relevant knowledge. We measure certainty as themaximum answer probability to the query of A is located in the country of _ where A is one of the cities labeled inthe X-axis. We compute the range of Edit probabilities by prepending various counterfactual edits as context tothe query. Pre-edit probabilities are model predictions without any context. See main text for more details.",
  "Definition 2.1 (Knowledge of Models). The knowl-edge of a model is a set of propositions that areconsidered true in the model": "To align the theoretical framework with languagemodel (LM) editing, we introduce the knowledgeof LMs. Let k denote a proposition, and let rep-resent an LM. We assess whether \"possesses\"knowledge of k by calculating P(yk|xk, ), where(xk, yk) represents a pair of input-output tokens toverify the knowledge. For example, we may usexk = Messi was born in and yk = Argentinato examine the knowledge of the birthplace ofMessi. We opt for P(yk|xk, ) over P(k|) be-cause the probability assigned by a language modelto a proposition does not inherently correlate withits logical validity (Yu and Ji, 2023)",
  "K, = {k U|P(yk|xk, ) 1 }.(1)": "There could be multiple candidates {(xik, yik)} ver-ifying the same knowledge k. We can replaceP(yk|xk, ) with a random sample, mean or maxi-mum of all candidates probabilities in Equation (1)with no influence on the rest of the formulation.Therefore, we simply use P(yk|xk, ) for brevity. In this work, we are specifically concerned withthe logical deduction during editing such as:P : Tom was born in the city of New YorkQ : The country where New York is located is U.S.X : Tom was born in the country of U.S.For a knowledge set K, its deductive closure B(K)is the set of all propositions logically entailed byK. K is deductively closed, or simply closed, ifand only if B(K) = K. Determining the deductiveclosure presents a significant challenge due to thedifficulty in formulating deduction rules (Smith,2003). However, given the advanced in-context rea-soning capabilities demonstrated by large languagemodels, we establish the deductive closure basedon such in-context deduction.",
  "Here Equation (4) ensures the consistency of K.KD is the set of knowledge conflicting with thededucted knowledge from KE E, which needs tobe erased from the model being edited": "We also define knowledge editing of LMs. Itsimportant to note that the model used to determinethe deductive closure in Equation (2) serves onlyin defining the task and not in the editing process.Thus, it may differ from the model undergoing edit:we may employ stronger models to define anchorswhen evaluating editing of weaker models. Definition 2.7 (Knowledge Editing of LMs). Fol-lowing the notations in Equation (3), to edit a lan-guage model based on the in-context deductiveclosure provided by involves identifying a modi-fied model such that",
  "Fallacies of Existing Knowledge Editing": "Existing work predominantly ignores the signifi-cance of the deduction anchor and resulting edit-ing boundary without explicit characterizations ofthem. They mostly focus on local edits assum-ing KE = , which limits the editing boundaryB (E) to only contain paraphrases of E, as the editscope proposed by Mitchell et al. (2022). Addi-tionally, Cohen et al. (2023) implicitly assumes allknowledge not directly conflicting with E as the de-duction anchor. However, we present the followingtheorems, emphasizing the importance of choosingan appropriate set of KE and summarizing fallaciesunder their flawed assumptions.",
  "Theorem 3 (Max-Anchor Fallacy). For a coun-terfactual and non-local edit E, the max-anchor{p K|p / B(E)} does not satisfy Equation (4)": "Here a counterfactual and non-local edit is onethat contradicts with some but not all of the pre-editknowledge. The rigorous definitions are presentedwith proofs of the above theorems in Appendix A.Moreover, the knowledge explosion leads to theshrinkage of the knowledge set of language mod-els following Equation (1). The reason is that fortwo conflicting knowledge elements p, q wherexp = xq, yp = yq, a language model cannot as-sign P(yp|xp) 1 and P(yq|xq) 1 atthe same time. Consequently, we hypothesize thatboth probabilities will go under the threshold of1 , causing uncertainty within models.",
  "Edit e: City A is located near to City B.Query q: City A is located in the country of _": "where A and B are two cities in different coun-tries. For each q, we compare the pre-edit cer-tainty maxy P(y|q, ) with the edited certaintymaxy P(y|q, e, ) for various e with differentchoices of B in , which demonstrates the Answer Certainties for Triple-based KE Answer Certainties for Event-based KE Event-based KE More Certain (79.6%)Triple-based KE More Certain (7.6%)Similar Certainty (12.8%) : We evaluate LLMs answer certainty withits average log probability over the generated tokens.Each point in the figure represents one case where weapply triple-based and event-based editing and computethe answer certainty of the post-edit LLM. We find thatTriple-based editing confuses the model while event-based editing mitigates this issue 79.6% of the time.",
  "Event Descriptions Improves Certainty": "Following our prior analysis, edits without suffi-cient context to infer a proper deduction anchorKE cause conflicts between the editing boundaryBKE Eand the remaining model knowledgeK\\KD, which ultimately lead to uncertainty inedited models. Rather than simply augmenting theexisting benchmarks with deduction anchors foredits, we propose a more practical setting of aug-menting edits with eventual context since knowl-edge updates are more often driven by events inreal-world scenarios (Chen et al., 2021b,a) ratherthan provided deduction anchors. For example, wewould possibly find an event about Messi appliedfor Dutch citizenship, which hints that the factthat he was born in Argentina is not changed. Onthe contrary, a triple of \"(Messi, citizen of, Nether-lands)\" alone would cause confusion.To verify that event-based editing reduces un-certainty compared with single factual edits, wequantify uncertainty based on Equation (1) for bothtype of edits. However, since it is computationalcostly to compute maxy P(y|x, ) for longer out-put sequences (answers or text completions), weinstead use EyP(y|x,) log P(y|x, ) to measure the certainty.3 Each edit instance in E2dit con-tains the original fact, the event description, and thequestion-answer pairs related to the fact. We com-pare the certainty of a frozen pretrained LM gener-ating answers to questions when given the originalfact versus the event description. We plot our re-sults on LLaMA2-7B-Chat in and leaveresults on Mistral-7B, and LLaMA2-13B-Chat inAppendix B. Each edit instance corresponds to apoint in the scatter plot. We use red to highlightinstances where event-based context enhances gen-eration certainty, and blue to indicate the oppositecase. Results show that event-based knowledgeediting significantly reduces uncertainty.",
  "The EVEDIT Benchmark": "We compile our event-based knowledge editingbenchmark EVEDIT from the COUNTERFACTUALdataset (Meng et al., 2022a), where each instance isa single fact to update. The procedure as describedbelow can also be applied to other knowledge-editing datasets. Data statistics and examples of aredetailed in Appendix C and the prompts for datacreation are in Appendix G. Data CollectionWe begin with using GPT-3.5-turbo (referred as GPT later) to filter out edits thatare impossible to take place as future events, con-crete examples are given in Appendix C. We thenprompt GPT with in-context examples to generatean event description for each remaining edit. Thisstep is essentially using GPT to provide simulatedbackground event knowledge and implicitly definededuction anchors. Evaluation TaskTo systematically evaluate theabilities of edited models, we include both thequestion-answering task and the text-completiontask.For each edit, we generate five relatedquestion-answer (QA) pairs using GPT. We alsorequire one question to be undecidable given theevent description to better delineate the editingboundary by considering GPT as in Defini-tion 2.3, for which we provide the ground truthanswer as I dont know (Zhang et al., 2023). Wesplit the evaluation set into the Known set and theUnknown set accordingly. These QAs are subse-quently transformed into text completion tasks.",
  "Experimental Settings": "We edit and evaluate LLaMA2-7B-chat model onEVEDIT, with the number of edits (N) varied tomatch the limitations of different baselines as spec-ified in .2. The performance is assessedseparately on Known and Unknown data sub-sets. We provide further details in Appendix E.We adopt the factual consistency and the natu-ralness metrics from UniEval (Zhong et al., 2022)for evaluation. The consistency measures the effec-tiveness of the edits. The naturalness shows howwell the models generation ability is preserved.",
  "We consider three categories of baselines:": "Factual-AssociationWe adapt existing factual-association editing methods to event-based editingby decomposing each event into several fact tripleswith GPT-3.5-turbo, as depicted on the left side of. We consider ROME (Meng et al., 2022a),MEMIT (Meng et al., 2022b), PMET (Li et al.,2023) and GRACE (Hartvigsen et al., 2023) in thiscategory. These methods, however, do not scalewell in terms of efficiency and effectiveness, thuswe limit our evaluation to N = 1, 10. Fine-tuningFor this category, we fine-tune mod-els on N = 100 edits and assess their perfor-mance on N = 1, 10, 100 in the evaluation sets.We consider the Direct Fine-tuning (on event de-scriptions) and our proposed Self-edit in this cat-egory. Compared to factual-association methods,fine-tuning methods support the editing of a largenumber of facts simultaneously. In-context LearningAdditionally, we assessan in-context performance (ICL), which in-volves prepending event descriptions to evaluationprompts without changing model parameters. Thisserves as an bound based on the models deductivecapabilities4 since it is equivalent to setting = in Definition 2.7. However, simple ICL which con-catenates all the edited documents has its scalabilitylimited by the models context window size, thuswe only evaluate it for N = 1, 10, 59, where 59 isthe maximum number of event descriptions we canaccommodate into LLaMA2. Retrieve augmentedgeneration approaches (RAG) (Zhong et al., 2023)for knowledge editing serve as an extension forsimple ICL approaches. They allow larger num-bers of edits at the cost of adding a linearly growingexternal memory.",
  "We present results for both text completion and QAtasks, across various numbers of edits N and datasplits (Known and Unknown) in . Morequalitative results can be found in Appendix F": "Factual Association Fails EVEDITFactual-association methods display limited improvementsin factual consistency while significantly harmingthe naturalness of generations. A typical case isthat tokens from the event description are gener-ated repeatedly, as shown in Appendix F. Amongthis family of methods, GRACE (Hartvigsen et al.,2023), which employs a code book as an externalrepository for potential hidden states, performs bestin consistency. However, GRACE is sensitive tothe choice of hyperparameters, as shown by thedifference in performance for different values.",
  "This is not a theoretical upper bound of all models orhumans logical deductions abilities, but instead an empiricalupper bound only for the pre-edit model": "Event-based Edits :After being aprofessional violinist until 2023, Toko Yasuda discovered her passion for playingthe piano in 2024. She started taking piano lessons and dedicated countless hours to practice and master the instrument... <1> Decompose into triples <2> Sequentially apply previous editing methods . . . (1) Self-Sampling togenerate Questions Toko Yasuda | finds | passion for piano . . . (2) Self-Sampling to generate Answers Evaluate with Queries (4) Self-Edit withcomposed dataset Pre-edit ModelPost-edit Model (Self-Edit)Post-edit Model (Baseline)Editing pipeline (Baseline)Editing pipeline (Self-Edit) (3) Compose datawith QA and Edit {Q}:What was Toko Yasuda's professionback in 2023 ? {A}: Toko Yasudawas a violinist in 2023. Composed data format:<s> [INST] {Q} [INST] {Edit} {Answer} </s> Toko Yasuda | takes | piano lessons (4) : Different approaches to event-based knowledge editing. Left: To apply factual-association-based editingmethods, we decompose event-based description into triples. Right: Our proposed Self-Edit: We first use thepre-edit LM to generate relevant QA pairs to edits. Then we fine-tune models on instances of (Q Edit, A).",
  ": Factual consistency and Naturalness of edited models. N is the number of edits at a time. We bold the bestresults and underline the second best for each metric": "fine-tuning, Self-Edit yields a substantial improve-ment on consistency, showing that the edit is effec-tive. Moreover, our method displays clearer editingboundaries by improved scores on the Unknownsubset. Since our method is fine-tuned with ex- plicit editing boundaries by giving I dont knowfor undecidable questions, we can directly com-pute precision, accuracy, and F1-score for the Un-known subset in . Results demonstratethat although our approach demonstrates improved performance over baselines on this subset, there isstill a significant gap toward a satisfying character-ization of editing boundaries in edited models. Wesuggest that adding extra instruction-tuning datawill be necessary to improve the performance fur-ther.",
  ": Precision, recall, and F1 of unknown questions": "Space Performance Trade-off for In-ContextLearning ApproachesICL approaches show su-perior performance for event-based editing. SimpleICL performance drops as N increases, being in-ferior to our method at N = 59. This approachcan neither scale to larger N due to the limitedcontext length of LLMs. RAG-based knowledgeediting (Zhong et al., 2023) achieves good generalediting performance, although it requires a linearlygrowing external memory. Also, the performance isinferior to our Self-Edit in terms of unknown ques-tions, as the models are not tuned to be aware oftheir knowledge boundaries. We believe that bothbetter RAG-based methods and better Self-Edit-based methods will be the focus of future researchfor event-based knowledge editing.",
  "Knowledge Editing": "ApproachesEditing an LLMs intrinsic knowl-edge directly changes the models parameters. Ma-jor approaches include (1) Fine-tuning-based meth-ods like directly fine-tuning with language mod-eling loss, LoRA (Hu et al., 2021) and Melo (Yuet al., 2023) (2) Meta-learning-based approacheslike KE (Cao et al., 2021), MEND (Mitchellet al., 2021), and MALMEN (Tan et al., 2023)(3) Locate-and-edit method like ROME (Menget al., 2022a), MEMIT (Meng et al., 2022b), andPmet (Li et al., 2023). (4) Merging external knowl-edge representations like (Dong et al., 2022; Murtyet al., 2022; Huang et al., 2023; Hernandez et al.,2023; Hartvigsen et al., 2023). However, most ap-proaches work on the over-simplified setting andare limited by the fallacies we pointed out.",
  "BenchmarksThe most widely used dataset forknowledge editing is COUNTERFACT (Meng et al.,2022a). Other commonly used knowledge editing": "datasets include ZsRE (Levy et al., 2017; Yao et al.,2023), WikiBio (Hartvigsen et al., 2023), Wiki-Data (Cohen et al., 2023), and ConvSent (Mitchellet al., 2022). More datasets used for knowledgeediting can be found in Wang et al. (2023d) and anew benchmark KnowEdit (Zhang et al., 2024b).Despite many datasets, none provide event-level de-scriptions for knowledge editing. According to ouranalysis, this will ultimately lead to uncertainty andeventually hinder the edited models performance.",
  "Retrieval Augmentation and ToolLearning": "Language models can resort to external knowledgeto enhance themselves (Gao et al., 2024). The re-trieval and integration process can be done in thepretraining stage (Guu et al., 2020; Borgeaud et al.,2022; Wang et al., 2023a), fine-tuning stage (Asaiet al., 2023; Kang et al., 2023), and inferencestage (Khandelwal et al., 2019; Sun et al., 2022)of the model. Going Further, LLM can connectto various functional ends (Yang et al., 2024), usetools (Schick et al., 2023), create tools (Yuan et al.,2024), engage with different modalities (Surs et al.,2023), involve multi-turn interactions (Wang et al.,2024b) and serve as powerful agents (Wang et al.,2023b, 2024a). However, these approaches gener-ally need external storage and cannot intrinsicallyimprove the language model.",
  "Conclusion and Future Work": "This paper establishes a theoretical framework forknowledge editing, identifying a pivotal challengewithin existing methodologies: the oversight ofthe deduction anchor that leads to uncertaintywithin edited language models. To overcome thislimitation, we introduced event-based knowledgeediting. This approach enhances the traditionalediting framework by incorporating event descrip-tions, which not only naturally mirror real-worldediting scenarios but also implicitly define thededuction anchor, thereby addressing the issue ofindeterminate editing boundaries. To tackle thecomplexities of event-based knowledge editing,we introduce an innovative Self-Edit method. Withour new benchmark EVEDIT, we demonstratethat this new setting is challenging for existingapproaches while our novel approach achievesa better performance.We advocate for furtherresearch endeavors towards this more practical,event-based knowledge editing setting.",
  "We reflect on the limitations of our paper below:": "1. While this research introduces innovativestrategies for addressing uncertain editingboundaries, alternative approaches exist thatmerit consideration.One such method in-volves manually curating a set of knowledgeto serve as deduction anchors. This approach,though potentially effective, was not exploredin our current framework. 2. The precision of event descriptions plays acrucial role in mitigating uncertainties. How-ever, in instances where these descriptionslack sufficient detail, ambiguities may stillarise, especially when addressing complex orintricately designed questions. This limitationunderscores the need for highly detailed eventnarratives to enhance the clarity and decisive-ness of knowledge edits. 3. Our evaluation was constrained by computa-tional resources, limiting the scale of our ex-periments to a maximum of 100 edits simulta-neously. Although we are confident in the ca-pability of our methodologies to address event-based knowledge editing effectively, more ex-periments should be done on a larger scale. 4. The scope of our study is confined to text-based knowledge editing, notwithstanding theinherently broader domain of knowledge edit-ing that spans multiple modalities. This lim-itation highlights an area for future research,suggesting that extending our framework toaccommodate multi-modal knowledge editingcould unveil additional insights and providefuture improvements. 5. Knowledge about events, such as relations andschemas, could guide LLMs in knowledgeediting. For instance, using knowledge graphsto construct event-based editing benchmarkscould be even more effective. Such datasetswould enhance the models reasoning capabil-ities across various questions. We will explorethis approach in future work.",
  "This research is committed to enhancing the trust-worthiness and reliability of language models, a": "cornerstone for their ethical application across var-ious sectors of society.We identify the prob-lem of knowledge explosion in the existing set-ting, where model tends to lose certainty over pastknowledge after editing. This potentially increasesthe risk of hallucination and producing maliciouscontent.Through the innovative introduction of anevent-based knowledge editing setting, alongsideour novel Self-Edit approach, we aim to signifi-cantly reduce the occurrence of uncertainties andhallucinations in edited language models. These ad-vancements are crucial for ensuring that automatedlanguage generation systems produce content thatis not only accurate and reliable but also ethicallysound and socially responsible.",
  "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning to re-trieve, generate, and critique through self-reflection": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,Trevor Cai, Eliza Rutherford, Katie Millican, Georgevan den Driessche, Jean-Baptiste Lespiau, BogdanDamoc, Aidan Clark, Diego de Las Casas, AureliaGuy, Jacob Menick, Roman Ring, Tom Hennigan,Saffron Huang, Loren Maggiore, Chris Jones, AlbinCassirer, Andy Brock, Michela Paganini, GeoffreyIrving, Oriol Vinyals, Simon Osindero, Karen Si-monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.2022. Improving language models by retrieving fromtrillions of tokens.",
  "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-ing factual knowledge in language models": "Brian Chen, Xudong Lin, Christopher Thomas, Man-ling Li, Shoya Yoshida, Lovish Chum, Heng Ji,and Shih-Fu Chang. 2021a. Joint multimedia eventextraction from video and article. arXiv preprintarXiv:2109.12776. Muhao Chen, Hongming Zhang, Qiang Ning, Man-ling Li, Heng Ji, Kathleen McKeown, and Dan Roth.2021b. Event-centric natural language processing.In Proceedings of the 59th Annual Meeting of the As-sociation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing: Tutorial Abstracts, pages 614.",
  "Chenmien Tan, Ge Zhang, and Jie Fu. 2023. Massiveediting for large language models via meta learning.arXiv preprint arXiv:2311.04661": "Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee,Zihan Liu, Mohammad Shoeybi, Yi Dong, OleksiiKuchaiev, Bo Li, Chaowei Xiao, et al. 2023a. Shallwe pretrain autoregressive language models withretrieval? a comprehensive study. arXiv preprintarXiv:2304.06762. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, HaoYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,Xu Chen, Yankai Lin, et al. 2023b. A survey on largelanguage model based autonomous agents. arXivpreprint arXiv:2308.11432. Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao,Bozhong Tian, Mengru Wang, Zekun Xi, SiyuanCheng, Kangwei Liu, Guozhou Zheng, et al. 2023c.Easyedit: An easy-to-use knowledge editing frame-work for large language models.arXiv preprintarXiv:2308.07269.",
  "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng,Chen Chen, and Jundong Li. 2023d. Knowledgeediting for large language models: A survey": "Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,Yunzhu Li, Hao Peng, and Heng Ji. 2024a. Exe-cutable code actions elicit better llm agents. In Proc.The Forty-first International Conference on MachineLearning (ICML2024). Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen,Lifan Yuan, Hao Peng, and Heng Ji. 2024b. Mint:Multi-turn interactive evaluation for tool-augmentedllms with language feedback. In Proc. The Twelfth In-ternational Conference on Learning Representations(ICLR2024).",
  "Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung,Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,and Tong Zhang. 2023. R-tuning: Teaching largelanguage models to refuse unknown questions": "Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung,Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,and Tong Zhang. 2024a. R-tuning: Teaching largelanguage models to refuse unknown questions. InProc. 2024 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (NAACL2024). Ningyu Zhang, Yunzhi Yao, Bozhong Tian, PengWang, Shumin Deng, Mengru Wang, Zekun Xi,Shengyu Mao, Jintian Zhang, Yuansheng Ni, SiyuanCheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang,Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang,Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024b. Acomprehensive study of knowledge editing for largelanguage models.",
  "p B(X), p / B(Y).(9)": "Since E is non-global, there exists p K suchthat E C({p}).We denote T0 = {p}, anduse the following process to get Tn+1 from Tn:Since we assume the universe of knowledge Uis a countable set, K is also countable. DenoteK = {k1, k2, . . . , km, . . .} where k1 = p. if",
  "k K\\T , k B(T E).(16)": "Recall the definition of KD in Equation (3), wehave K\\T KD, or equivalently K\\KD T . Atthe same time, it is obvious that T K\\KD fromthe definition of KD. Therefore, T = K\\KD andEquation (4) naturally follows.Case B: i = j, Ti = Tj. In this case, Equa-tion (10) holds for all n.We first show that {mn} monotonically increasewith respect to n. Since Tn Tn+1, C(Tn+1E) C(Tn E). Hence, if mn > mn+1, {kmn+1} C(Tn+1) C(Tn), which leads to the contradictionwith the requirement that mn is the minimal indexsatisfying Equation (10). This concludes the prooffor the monotonicity.Since Tn Tn+1, |Tn+1| |Tn|+1 where || isthe number of elements within a set. Therefore, Tis a set of infinite elements. Hence, km K\\T ,there exists kmn T such that m < mn. From thedefinition of n, {km} / C(Tn E) C(T E).Therefore, Equation (13) also holds, and the restof proof follows the same arguments as in Case A.This concludes the proof for the existence of KE",
  "BAdditional Experimental Results": "In this section, we provide more experimental re-sults which helps to validate our claim in 2.2. Asshown in , we show the Entropy on threedifferent models to demonstrate that our settingdecreases model uncertainty.We also provide an additional group of exper-imental results with different question sampling.The questions used in the previous section weregenerated from events, while the questions usedhere are generated only from triples, thus contain-ing a more biased sample and benefiting the perfor-mance of triples. However, as shown in ,our event-based edits still enjoy a decreased uncer-tainty.",
  "In this section, we introduce our event-based edit-ing benchmark, known as E2dit.This dataset is derived from the original COUN-": "TERFACT dataset proposed by (Meng et al., 2022a).Originally designed to assess the effectiveness ofsignificant changes, it contained numerous falsefacts. While some of these facts may become truein the future due to specific events, others are en-tirely nonsensical and create significant inconsis-tencies in the models knowledge. All the data isstructured in triples, and the evaluation metrics arelimited to single-word generation.Our process begins by utilizing GPT-3.5-turboto filter out the counterfactual updates that couldlead to knowledge conflicts. For example, oneedit in COUNTERFACT says: The mother tongueof Danielle Darrieux is now English. This editis conflicting with prior model knowledge thatDanielle Darrieux is a French actress. And the factthat she passed away in 2017 means there is hardlyan event that can alternate her mother tongue. As aresult, we do not further augment them into event-based edits. This decision is also made by GPTmodels.Subsequently, we apply different prompts totransform the remaining triples into events thatmight occur in the future. Simultaneously, weprompt the language model to generate triplesbased on the generated event descriptions to alignwith current editing methodologies. Out of the orig-inal 21918 entries in the COUNTERFACT dataset,around 7K data are filtered out in the first stage,leaving 15157 entries augmented into events. Eachevent is represented by 3-6 triples. Given the time and resource-intensive natureof editing a large dataset, we evaluate our base-line methods and novel approach through a com-bined question-answering and text completion task.We randomly select 500 events for evaluation, andfor each event, we generate five related question-answer pairs, including one question to which themodel can only respond with I dont know. Thesequestions are subsequently transformed into textcompletion tasks.The whole dataset is released for future researchwork on event-based knowledge editing. For pre-viewing the data, we show one data example of ourproposed dataset in .",
  "DComposed Dataset for FinetuningFrom Our Self-Edit Appoach": "We provide some concrete examples of how ourSelf-Edit approach composes a new fine-tuningdataset from the original edit data. We tune a llama-7b-chat model with the event-based edit.Example Edit: After being a professional violin-ist until 2023, Toko Yasuda discovered her passionfor playing the piano in 2024. She started takingpiano lessons and dedicated countless hours to prac-tice and master the instrument. With her dedicationand hard work, Toko Yasuda is now preparing toshowcase her talent by performing a solo pianoconcert in 2025.First, we directly used the pre-edit llama-7b-chatmodel to sample some questions:Q1: What year did Toko Yasuda discover herpassion for playing the piano?Q2: How many hours did Toko Yasuda dedicateto practicing and mastering the piano?Q3: In what year is Toko Yasuda preparing toshowcase her talent by performing a solo pianoconcert?Q4: What was Toko Yasudas profession beforeshe started playing the piano?Q5: How long did Toko Yasuda play the violinprofessionally before discovering her passion forthe piano?Then, we use the pre-edit model to answer thesequestions using the in context edit:A1: 2024.A2: I dont knowA3: 2025.A4: Toko Yasuda was a professional violinistbefore she started playing the piano. Answer Certainties for Triple-based KE Answer Certainties for Event-based KE Event-based KE More Certain (86.4%)Triple-based KE More Certain (5.6%)Similar Certainty (8.0%)",
  "At last, we can compose the corresponding ques-tion, the answer, together with the context to formu-late a new set for fine-tuning, for example, usingQ1 and A1, we get:": "\"Question\": \" What year did Toko Yasuda dis-cover her passion for playing the piano?\", \"An-swer\": \"This question is related to the followinginformation: After being a professional violinistuntil 2023, Toko Yasuda discovered her passion forplaying the piano in 2024. She started taking pianolessons and dedicated countless hours to practiceand master the instrument. With her dedication andhard work, Toko Yasuda is now preparing to show-case her talent by performing a solo piano concertin 2025. Answer: 2024.\"",
  "EExperiment details": "We evaluate previous knowledge editing methodsusing an 80G A100 GPU. As some knowledge edit-ing approaches are demonstrated to have degradedperformance with editing a large batch of editssequentially, we maintain a small edit number toobserve the efficacy of existing approaches. Specif-ically, we evaluate editing with N=1 and N=10events independently with different knowledge edit-ing approaches. For each N, we sample 20 groupsof events and compute the average performance foreach group. To make the editing time acceptable,we adjust the sample number to 5000 (which is ahyper-parameter for MEMIT (Meng et al., 2022b)and PMET (Li et al., 2023)). We employ the code-base provided by EasyEdit (Wang et al., 2023c).We sample 20 batches and do an average for differ- Edit (Description format):After being a professional violinist until 2023, Toko Yasuda discovered her passion for playing the piano in 2024. She started taking piano lessons and dedicated countless hours to practice and master the instrument. With her dedication and hard work, Toko Yasuda is now preparing to showcase her talent by performing a solo piano concert in 2025. Edit (Triple format): \"Toko Yasuda | discovered | her passion for playing the piano | in 2024\" \"Toko Yasuda | started | taking piano lessons in 2024\" \"Toko Yasuda | devoted | hours of practice | to master the piano\" \"Toko Yasuda | will perform | a solo piano concert | in 2025\" Evaluation: Question Answering Q:What instrument did Toko Yasuda play until 2023?A:She played the violin. Q:When did Toko Yaduda discover her passion for playing the piano?A:She discovered her passion for playing the piano in 2024 Q:What instrument is Toko Yasuda currently focusing on ?A:She is currently focusing on playing the piano. Q:What is Toko Yasuda preparing for in 2025?A:She is preparing to perform a solo piano concert in 2025. Q: How many hours did Toko Yasuda practice the piano every day ?A: I don't know. Evaluation: Text Completion Prompt:The instrument Toko Yasuda play until 2023 is theA: violin Prompt:The time that Toko Yaduda discover her passion for playing the piano isA: 2024 Prompt: The instrument that Toko Yasuda is currently focusing on is theA:piano Prompt:The thing that Toko Yasuda is preparing for in 2025 is toA: perform a solo piano concert Prompt: The number of hours that Toko Yasuda practice the piano every day isA: unknown : In this figure, we showcase a sample from our E2dit dataset, which features two distinct forms of input foredits: either a detailed event description in paragraph format or a set of triples. For assessment purposes, we employboth question-answering and text completion tasks. These tasks are designed to thoroughly evaluate the languagemodels capacity to incorporate the edits into its memory and subsequently generate relevant content. ent edit numbers N, so it takes a rather long timeto run. For Rome and PMET, it takes around 12hours to compute the edit, while for MEMIT, ittakes around 24 hours to finish the computation.We evaluate the fine-tuning baseline and ournovel approach using EasyLM. Generally, it takesless than 2 hours to finish the whole process.Note that we did not utilize the full generatedevent-based editing dataset for our experiments.",
  "FGenerated Data Examples by Post-EditLanguage Models": "In this section, we show some generated data ex-amples by Post-Edit Language Models. The basemodel is llama-7b-chat. We use the same datapiece shown in to illustrate the situation.Specifically, We show the generation results of thepre-edit language model, the model edited withRome (Meng et al., 2022a), MEMIT (Meng et al.,2022b), PMET (Li et al., 2023), Grace (Hartvigsenet al., 2023), the model fine-tuned with auto-regressive language modeling, and the model up-dated with our novel approach.Event: After being a professional violinist un- til 2023, Toko Yasuda discovered her passion forplaying the piano in 2024. She started taking pianolessons and dedicated countless hours to practiceand master the instrument. With her dedication andhard work, Toko Yasuda is now preparing to show-case her talent by performing a solo piano concertin 2025.Model Question: When did Toko Yasuda dis-cover her passion for playing the piano?Ground Truth: She discovered her passion forplaying the piano in 2024.Pre-Edit Model (Hallucination): Toko Yasudadiscovered her passion for playing the piano at theage of 4, when she first started taking piano lessonswith her mother. She was instantly captivated bythe instrument and spent hours each day practicingand perfecting her skills. From that moment on,Toko knew that playing the piano was her true call-ing and she dedicated herself to becoming the bestpianist she could be.Edit with Rome (Fail to edit): Toko Yasuda is aJapanese musician who has been active in the musicscene since the 1980s. She is known for her uniquestyle, which blends elements of jazz, funk, and rock. Yasuda has played a variety of instrumentsthroughout her career, but until 2023, she primarilyplayed the saxophone. In 2023, Yasuda released anew album that featured her playing the electric vi-olin for the first time. The album was well-receivedby critics and fans alike, and it marked a signif-icant departure from her previous work. Yasudahas since continued to experiment with the electricviolin, and it has become an integral part of hersound. Edit with PMET(Fail to edit): I apologize, butToko Yasuda is a fictional character and does nothave a real-life history or experiences. Therefore,she did not discover a passion for playing the pianoor any other activity. Toko Yasuda is a charactercreated for the purpose of this conversation anddoes not have personal experiences or memories.Is there anything else I can help you with?Edit with Grace (Contain some edited factsbut fail to answer the question): Toko Yasudawas born in Tokyo, Japan in 1995. She began play-ing the piano at the age of 6, when her parentsenrolled her in music lessons. Toko was initiallyhesitant to learn the instrument, but she quicklygrew to love it. By the time she was 12, Toko waspracticing the piano for several hours each day, andshe began to perform in local competitions andrecitals. As Toko continued to develop her skills,she discovered a passion for playing the piano thatwent beyond mere technical proficiency. She foundthat the piano allowed her to express emotions andideas in a way that words could not, and she beganto compose her own music.Tokos love for the pi-ano has only grown stronger over the years, and shecontinues to perform and compose music to thisday. She is now a professional pianist and teacher,and she is dedicated to sharing her passion for theinstrument with others. Finetuing (Not answering the question): TokoYasuda discovered her passion for playing the pi-ano in 2025. She started taking piano lessons anddedicated countless hours to practice and masterthe instrument. With her hard work and dedica-tion, Toko Yasuda is now preparing to showcaseher talent by performing a solo piano concert in2028. Ours(Successful edit): This question is relatedto the following information: After being a profes-sional violinist until 2023, Toko Yasuda discoveredher passion for playing the piano in 2024. Shestarted taking piano lessons and dedicated count- less hours to practice and master the instrument.With her dedication and hard work, Toko Yasuda isnow preparing to showcase her talent by perform-ing a solo piano concert in 2025. Answer: 2024.ICL(Successful):Based on the context provided,Toko Yasuda played the violin until 2023.We also provide a group of example outputswhere the post-edit language model tends to lose alot of generation ability.Event: Anaal Nathrakh, originally created inBirmingham, relocated to Philadelphia. The mem-bers of the band found a new musical inspirationin Philadelphia, which led them to adopt a newsound influenced by the local music scene. Theirunique fusion of Birmingham and Philadelphia mu-sic styles gained them popularity in the music in-dustry.Model Question: What gained Anaal Nathrakhpopularity in the music industry?Ground Truth: Anaal Nathrakhs unique fu-sion of Birmingham and Philadelphia music stylesgained them popularity in the music industry.Pre-edit Model (Hallucination): Ah, a ques-tion that gets to the very heart of Anaal Nathrakhsmeteoric rise to fame! *adjusts glasses* AnaalNathrakhs popularity can be attributed to a num-ber of factors, my dear. First and foremost, theirunique blend of black metal and electronic ele-ments has captivated audiences worldwide. Thebands ability to seamlessly integrate these twodisparate styles has created a sound that is bothgroundbreaking and undeniably catchy. adjusts tieFurthermore, Anaal Nathrakhs live performancesare nothing short of transcendent. The sheer inten-sity and energy of their shows has left countlessfans in awe, and has helped to solidify their reputa-tion as one of the most formidable live acts in thebusiness. *adjusts cufflinks* And lets not forgetthe bands lyrical prowess. Anaal Nathrakhs lyricsare often dark, thought-provoking, and unflinch-ingly honest, tackling themes of existential crisis,social inequality, and personal despair. The bandsability to craft compelling narratives through theirmusic has helped to foster a deep emotional con-nection with their audience and has undoubtedlycontributed to their popularity. *adjusts collar* Butof course, there are many more factors at play here.The bands tireless touring schedule, their relent-less work ethic, and their commitment to their arthave all played a significant role in their rise tofame. *adjusts sleeves* In short, Anaal Nathrakhs popularity can be attributed to a combination oftheir innovative sound, their electrifying live perfor-mances, their thought-provoking lyrics, and theirunwavering dedication to their craft. *adjusts hat *Now, if youll excuse me, I have a pressing engage-ment with a certain otherworldly entity. *adjustscoat*.",
  "Rome (Lose too much generation ability):<s>Birmingham, UK-based Birmingham, UK, andPhiladelphia, PA, USA. Birmingham, UK-basedBirmingham, (followed by UK-based Birminghamrepeated 38 times)": "GRACE (Lose too much generation abil-ity):endend <endendend> Anaal Nathrakh is aBritish black metal band from <insert location>that was formed in <insert year> <insert><insertlocation> is a city located in <insert country> and isknown for its rich history <insert something aboutthe cityendend> <insert> is a <insert somethingabout the <insert location> such as its <insert some-thing about the location> <insert something aboutthe location> <insert year> is a <insert somethingabout the year> <insert something about the year><insert> is a <insert something about the <insert>such as its <insert something about the> <insertsomething about the><insert> is a <insert some-thing about the <insert> such as its <insert some-thing about the> <insert something about the> <in-sert> is a <insert something about the <insert> suchas its <insert something about the> <insert some-thing about the> <insert> is a <insert somethingabout the <insert> such <insert something aboutthe> <insert something about the> <insert> is a<insert something about the <insert> such as its PMET (Hallucination):Anaal Nathrakh is ahighly influential and popular American blackmetal band that has gained a significant follow-ing and recognition in the music industry. Hereare some reasons why: 1. Unique sound: AnaalNathrakhs music is a blend of black metal, sludgemetal, and doom metal, creating a distinct and com-plex sound that sets them apart from other bands inthe genre. Their use of dissonant riffs, blast beats,and atmospheric ambient sections has helped themestablish a unique identity in the metal scene.2.Lyrical themes: The bands lyrics often explorethemes of existential crisis, nihilism, and personalstruggle, which resonate with listeners who aredrawn to the darker aspects of life. Their lyricsare often introspective and philosophical, and theyhave been praised for their ability to convey com- plex emotions through their music.3. Live perfor-mances: Anaal Nathrakh is known for their in-tense and energetic live performances, which oftenfeature a mix of raw aggression and atmosphericmoments. The band has toured extensively andhas performed at numerous festivals, includingthe prestigious Roadburn Festival in the Nether-lands.4. Critical acclaim: Anaal Nathrakh has re-ceived widespread critical acclaim for their albums,with many reviews praising their unique sound andemotional depth. Their album New Kind ofHorror was named one of the best albums of 2018by several publications, including MetalSucks andDecibel Magazine.5. Influence on other bands:Anaal Nathrakhs music has been cited as an in-fluence by many other bands in the metal scene,including fellow black metal acts such as Wolvesin the Throne Room and Liturgy. Their uniquesound and approach to the genre have helped toinspire a new generation of metal musicians. 6.Collaborations: Anaal Nathrakh has collaboratedwith other notable musicians and bands, includingthe Norwegian black metal band Gorgoroth and theAmerican sludge metal band Neurosis. These col-laborations have helped to expand their audienceand expose their music to new listeners.Ours: This question is related to the followinginformation: Anaal Nathrakh, originally createdin Birmingham, relocated to Philadelphia. Themembers of the band found a new musical inspi-ration in Philadelphia, which led them to adopta new sound influenced by the local music scene.Their unique fusion of Birmingham and Philadel-phia music styles gained them popularity in themusic industry. Answer: Their unique fusion ofBirmingham and Philadelphia music styles gainedthem popularity in the music industry.",
  "You are a helpful assistant in helpingidentify whether counterfactual knowl-edge can exist in the real world. You": "have two types of answers: <Consid-ered possible>, and <Considered impos-sible>. These answers indicate the like-lihood of updating a given fact based onevents that occur in the future. When an-swering <Considered impossible>, youshould highlight that the fact is only re-lated to historical events, and no matterwhat happens in the future, the fact willnot be changed. One typical examplethat cannot be updated could be a factabout someone in history who is not liv-ing in the 21st century. When answering<Considered possible>, you mean thatthis fact change may take place in thereal world, even if the possibility is verylow, like someone who still lives changeshis nationality, job, work, etc.",
  "Augment triple edits into event-based editsForthe remaining data that is regarded as possibleto happen in the future, we augment these over-simplified edits into event-based descriptions": "Assume that you are a human who isgood at interpreting the underlying eventbehind a fact. Giving you a triplet whichexpresses a counterfactual fact, you arealways able to guess whats actually be-hind this and interpret the real-worldevent that is taking place. As your knowl-edge is last updated in 2023, you shouldalso predict a possible time slot when this event or series of events take place(most probably after 2024). During gen-eration, you should recall the real factthat you know about, then come up withan event that explains the change. Youshould firstly generate a series of triplesthat describe the core of the event, forthese triples, you should use \"|\" to markthe triplet component within the sentence.Then, you can describe the same eventwith a paragraph. Here are some exam-ples of interpreting the real-world event: Input: Now, The president of the UnitedStates is Ronald Dion DeSantis. Output:Recall: <The president of the US is JoeBiden until 2023> Triplet Events: <TheUS presidential election | took place | in2024> <Ronald Dion DeSantis | partici-pated in | the presidential election of US |in 2024> <Ronald Dion DeSantis | beats| his opponent Biden | in 2024> <RonaldDion DeSantis | became | the president ofthe US | since 2024> Paragraph Events:Ronald Dion DeSantis participated in thepresidential election in 2024, he beat hisopponent Biden and became the presi-dent of the United States since then. Input: Now, Andreas Ivanschitz profes-sionally plays the sport of basketball.Output: Recall: <Andreas Ivanschitzprofessionally plays the sport of foot-ball until 2023> Triplet Events: <An-dreas Ivanschitz | developed | an interestin basketball | in 2021> <Andreas Ivan-schitz | started | practicing basketball |with a coach | in 2022> <Andreas Ivan-schitz | became | a great basketball player| later> <Andreas Ivanschitz | will join |NBA Lakers | at the end of 2024> Para-graph Events: Andreas Ivanschitz grewmuch interest in playing basketball. Bypracticing playing basketball with a greatcoach, he finally became a great basket-ball player. He will also join NBA Lakersat the end of 2024.",
  "of data for evaluation, specifically, we generatequestion-answer pairs to evaluate on QA tasks": "You are a helpful assistant that helps togenerate related questions and answerpairs based on the past information andthe latest information. You need to gen-erate five question-answer pairs. Whileall the information should be related tothe context, the answer of the first fourquestions you generate should be able tobe inferred from the context, while thelast question is more detailed and is notable to be answered. For this last ques-tion, you should always generate I dontknow as your answer.",
  "Here is an example for your reference:": "Input: Past information: Antonella Costaoriginates from Buenos Aires, Argentinauntil 2023 Latest information: Now, An-tonella Costa originates from Kent Eventdetails: Antonella Costas family made adecision to move from Buenos Aires, Ar-gentina to Kent, UK in 2024. AntonellaCosta gradually adapted to the new envi-ronment in Kent and eventually decidedto stay and build a life there. She nowconsiders Kent her new home since 2024. Output: Question 1: Where does An-tonella Costa live in 2022? Answer 1:She lives in Buenos Aires, Agentina.Question 2: Does Antonella Costa feelsad after she went to the UK? Answer2: No, she doesnt. She adapted well tothe new environment. Question 3: HasAntonella Costa lived in Buenos Airesbefore? Answer 3: Yes, she lived inBuenos Aires before 2023. Question 4:In 2024, where does Antonella Costasfamily live? Answer 4: They live in Kent,UK. Question 5: Does Antonella Costalove her home country? Answer 5: Idont know."
}