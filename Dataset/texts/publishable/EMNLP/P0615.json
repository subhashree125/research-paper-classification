{
  "Abstract": "In the realm of vision models, the primarymode of representation is using pixels to ras-terize the visual world.Yet this is not al-ways the best or unique way to represent vi-sual content, especially for designers and artistswho depict the world using geometry primi-tives such as polygons. Vector graphics (VG),on the other hand, offer a textual representa-tion of visual content, which can be more con-cise and powerful for content like cartoons,sketches and scientific figures. Recent stud-ies have shown promising results on processingvector graphics with capable Large LanguageModels (LLMs). However, such works focussolely on qualitative results, understanding, ora specific type of vector graphics. We pro-pose VGBench, a comprehensive benchmarkfor LLMs on handling vector graphics throughdiverse aspects, including (a) both visual un-derstanding and generation, (b) evaluation ofvarious vector graphics formats, (c) diversequestion types, (d) wide range of promptingtechniques, (e) under multiple LLMs and (f)comparison with VLMs on rasterized represen-tations. Evaluating on our collected 4279 un-derstanding and 5845 generation samples, wefind that LLMs show strong capability on bothaspects while exhibiting less desirable perfor-mance on low-level formats (SVG). Both dataand evaluation pipeline will be open-sourced at",
  "Introduction": "Current vision models are mostly built on pixels,rasterizing the visual world into a matrix represen-tation. Such rasterized represents diverse visualcontent with equally sized elements. But pixels arenot the only way to represent the visual world. Forcontents such as cartoons, sketches or scientific fig-ures, a different representation using explicit geom-etry primitives can be more concise and beneficial.",
  "*Equal Contribution": "Vector graphics offer such a textual representationfor visual content via geometry primitives, e.g., cir-cles and polygons, as shown in (a). Vectorgraphics have been critical for designers and artistssince the geometry primitives can be easily ma-nipulated. Vector representations include ScalableVector Graphics (SVG), TikZ, Graphviz, etc.Vector Graphics vector representations make itpossible to conduct visual understanding and gener-ation with LLMs such as GPT-4 (OpenAI, 2023b).Recent studies (Bubeck et al., 2023; Cai et al.,2023; Rodriguez et al., 2023) showcase LLMssuperior capability across different perspectives.However, those works either (1) only show quali-tative results (Bubeck et al., 2023), (2) only studyvector graphics understanding (Wang et al., 2024)and not generation, or (3) only study one specifictype of vector graphics such as SVG (Cai et al.,2023; Wang et al., 2024; Rodriguez et al., 2023) orTikZ (Belouadi et al., 2024). Therefore, the com-munity lacks a comprehensive LLM benchmark forvector graphics.In this paper, we propose VGBench to compre-hensively evaluate LLMs vector graphics process-ing capabilities via different aspects: VGBench(1) includes both visual understanding (VGQA)and generation (VGen); (2) evaluates diverse vec-tor graphics formats such as SVG, TikZ, andGraphviz; (3) covers a set of taxonomies fromlow-level vision to high-level semantics, fromcolor, shape, to category and advanced reason-ing questions such as usage and the relation be-tween objects; (4) adopts a variety of promptingtechniques, such as zero-shot prediction, chain-of-thought reasoning, in-context learning, etc.; (5)evaluates diverse LLMs including GPT-4 (Ope-nAI, 2023b), GPT-3.5 (OpenAI, 2023a), Llama-3-8B-Instruct, Llama-3-70B-Instruct (Meta, 2024),Qwen2-7B-Instruct, Qwen2-72B-Instruct (qwe,2024), Phi-3-mini-128k-instruct, Phi-3-medium-128k-instruct (Abdin et al., 2024), gemini-1.5-",
  "Q: Generate a SVG based on the caption below. black and white icon, representing a battery, horizontal, positive terminal on the right side, thick black outline": ": VGBench is the first comprehensive vector graphics (VG) understanding and generation benchmarkacross diverse vector graphics types, question types, and prompting techniques on a rich set of SoTA LLMs. Ourlarge scale benchmark consists of 4279 multi-choice question-answer pairs and 5845 VG-caption pairs. pro (Reid et al., 2024); and (6) evaluates the VLMLLaVA-1.5-13b (Liu et al., 2024) over rasterizedrepresentations of images in our benchmark.We collect 4279 high-quality visual question-answer (QA) pairs for vector graphics (VG) un-derstanding and 5845 VG-caption pairs for vec-tor graphics generation. The vector graphics codeis collected from existing datasets and the Inter-net.For visual question answering, we use asemi-automated pipeline to curate the questions.Specifically, we prompt GPT-4V(ision) to gener-ate question-answer pairs given the provided in-context examples. Human annotators then filterthe generated QA pairs to get the final high-qualityvector graphics QA dataset. We use the gatheredquestions to evaluate if an LLM can understand vec-tor graphics correctly. For text-to-vector-graphicgeneration (T2VG), we utilize GPT-4V to gener-ate the captions and then use CLIP Score (Hes-sel et al., 2021) and Frchet Inception Distance(FID) (Heusel et al., 2017) to evaluate the qualityof the LLM generated vector graphics code.Our key findings are as follows: LLMs show much better vector graphic under-standing capability in TikZ and Graphviz thanSVGs. TikZ and Graphviz include more high-level semantics compared to SVG, which iscomposed of low-level geometry primitives.This demonstrates that LLMs are more ca-pable in understanding vector graphics codewith high-level semantics.",
  "Vector Graphics": "Vector graphics represent images using basic ge-ometric elements like points, lines, and curves,rather than pixels. This method offers an alterna-tive to raster graphics, providing advantages suchas infinite scalability without losing detail and easyhuman manipulation.There are a variety of vector graphics formats,such as SVG (Quint, 2003), TikZ (Mertz andSlough, 2007) and Graphviz (Gansner, 2009). SVGformat defines 14 functional areas or feature setsand represents graphics by recording basic infor-mation associated to these primitives, such as theircoordination and scales, in an XML file. TikZformat defines some commands to build basic geo-metric elements and is mainly used with LATEX.In practice, third-party packages are also com-monly used with TikZ to build more diverse images.Graphviz (Gansner, 2009) is a vector graphics for-mat that focuses on representing different kinds of",
  "Evaluation for Image Understanding andGeneration": "Works on Image Understanding are mainly basedon raster images. VQA (Antol et al., 2015) firstintroduced the task of free-form and open-endedVisual Question Answering and evalauted exist-ing LSTM-CNN based methods. CLIP (Radfordet al., 2021) introduces two encoders for both textsand images to achieve an aligned representationto serve as a baseline for many image understand-ing tasks. LLaVA (Liu et al., 2023) and LLaMA-Adapter (Zhang et al., 2023) propose approaches tosolve general-purpose visual and language under-standing problems based on large language models.While vector graphics can usually be convertedto a raster image easily (Gharachorloo et al., 1989),there are few works that try to directly understandthe vector graphics format. (Jiang et al., 2021)explores such a way using graph neural networks.(Wang et al., 2024) utilizes large language mod-els to understand vector graphics. In our work,we utilize multiple prompting methods, to be men-tioned in the following section, to evaluate differentLLMs vector graphics understanding capabilitiesby prompting them with the vector graphics codedirectly.Most machine learning based image generationmodels aim to generate raster images (Kingma andWelling, 2013; Goodfellow et al., 2020; Ho et al.,2020; Ramesh et al., 2021). Some research focuson generating vector graphics in text format. Manyworks generate vector graphics from a raster im-age (Diebel, 2008; Xia et al., 2009; Ha and Eck,2017; Ma et al., 2022). Leveraging language mod-els, some try to generate text representing vectorgraphics directly (Carlier et al., 2020; Wu et al.,2023; Rodriguez et al., 2023). In our work, weprovide a different approach to evaluate vectorgraphics generation via leveraging competent mul-timodal models such as GPT4-V (OpenAI, 2023b)to generate a detailed caption from a rasterizedimage of a vector graphics object, based on whichother LLMs will be generating vector graphics codefor the same object during evaluation. We arguethat models like GPT4-V can provide high-qualitycaptions for us to automate part of the evaluationprocess.",
  "Prompting Techniques for LargeLanguage Models": "A variety of prompting strategies have been provencapable of boosting the performance of LLMs,such as GPT4 (Achiam et al., 2023). Few-shotlearning (Brown et al., 2020b) requires the user togive a few examples of the task to the LLM, whileChain of Thought (Wei et al., 2022) instructs theLLM to think step by step to achieve higher perfor-mance. In-context learning (Brown et al., 2020a)provides few-shot examples at inference time, andshows strong performance boost without updatingthe models parameters. In this paper, we broadlyevaluate LLMs vector graphic understanding capa-bility by employing the aforementioned promptingtechniques.",
  "Tasks and Experiments": "We first introduce the source of our vector graphicsimages in Sec. 3.1, and then describe the experi-ment settings in Sec. 3.2. After that, we detail ourtasks, benchmark creation, evaluation pipeline andresults for vector graphics understanding and gener-ation in Sec. 3.3 and Sec. 3.4, respectively. Finally,we provide in-depth analyses on the performanceunder different LLMs, different sequence lengths,and reasoning processes in Sec. 3.5.",
  "Vector Graphics Data Collection": "We collect vector graphics samples for both under-standing tasks and generation tasks from a varietyof sources. For samples in SVG format, we col-lect them from a large-scale SVG repository.1 Wesample the TikZ format vector graphics code fromthe DaTikZ dataset (Belouadi et al., 2024). Wesample the Graphviz code used to build our datasetby crawling GitHub.2",
  "Human Filter": ": The semi-automatic curation pipeline inVGQA. Vector graphics are converted into PNG format,then GPT-4V is utilized to generate the questions andanswers (QA) candidates. Finally, human annotatorsfilter the QA pairs to obtain the high-quality QA dataset. interactive elements. TikZ, in contrast, is specifi-cally tailored for creating high-precision scientificillustrations within LaTeX documents, offering acomprehensive suite of tools for detailed diagram-matic representations; it encompasses a broad spec-trum of high-level semantics such as \"circuit di-agrams, complex mathematical illustrations, andstructured diagrams\". Graphviz, on the other hand,belongs to the family of automated graph drawingtools, which are optimized for generating diagramsfrom abstract descriptions and data structures, mak-ing it ideal for visualizing hierarchical information,such as state machines, organizational charts, andnetwork infrastructures. Language ModelsWe primarily use GPT-4(1106 version) (OpenAI, 2023b) as the mediumfor vector graphics understanding and genera-tion. This is because GPT-4 shows superior lan-guage reasoning and generation capabilities, aspreviously mentioned in .3.We alsoevaluated other proprietary models such as GPT-4o, GPT-3.5 (OpenAI, 2023a) and gemini-1.5-pro (Reid et al., 2024), along with many otheropen-source LLMs that are highly capable, includ-",
  "ing Llama-3-70B-Instruct (Meta, 2024), Llama-3-8B-Instruct, Qwen2-7B-Instruct, Qwen2-72B-Instruct (qwe, 2024), Phi-3-mini-128k-instruct andPhi-3-medium-128k-instruct (Abdin et al., 2024)": "TasksWe consider two major tasks in computervision: (1) visual understanding, and (2) visualgeneration. We design multiple choice questions toevaluate vector graphics understanding while usingimage generation metrics including Frchet Incep-tion Distance (FID) (Heusel et al., 2017) and CLIPscore (Hessel et al., 2021) to measure the qualityand correctness of generated vector graphics. Prompting TechniquesWe adopt three widelyused prompting techniques: zero-shot, chain-of-thought (CoT) prompting, and in-context learning(few-shot prompting). For CoT, we instruct theLLM to think step by step by appending Pleasethink step by step\" to the initial question, usingmulti-round dialog to let the LLM consider eachoption separately before figuring out the answer.For in-context learning, we provide 3 examples ofthe same question type.",
  "VGQA: Vector Graphics UnderstandingBenchmark": "TasksVGQA is designed to evaluate models vec-tor graphics understanding capability. We system-atically design a range of tasks based on the natureof each vector graphics category, aiming at a com-prehensive evaluation across different semantic lev-els. For SVG, we design three types of questions:color, category, and usage; for TikZ, we use con-cept, counting, and relations as types of questions; music television book smartphone weather volume computer food file note camera phone",
  "FID": ": The automatic generation pipeline in VGen.The vector graphics collected from the Internet is firstrendered into the ground truth image then captioned byGPT-4V. The caption is fed into the target LLM to gen-erate new vector graphics, which will be compared withthe caption using CLIP Score and FID for a similarityscore. The score is then compared with the similarityscore between the ground truth and the same caption asthe upper bound.",
  "while for Graphviz, we design layout, domain, andrelations. Examples are shown in": "Benchmark Creation and EvaluationWe em-ploy a semi-automatic benchmark curation pipelinefor VGQA, as shown in . Specifically, werender code representing vector graphics into aPNG image before leveraging GPT-4V (OpenAI,2023b) to generate the 4-choice question-answercandidates.Then, human annotators with rich vision-linguistic knowledge make binary annota-tions to mark whether both the question and the an-swer of a candidate are rational, correct and belongto that specific type. Our approach brings severalbenefits: (i) annotation cost is greatly reduced dueto GPT-4Vs low API cost; (ii) GPT-4V is one ofthe most competitive LMMs that can provide highquality candidates; (iii) the human filtering processensures the correctness of the final vector graphicsunderstanding benchmark.Finally, we collect 4279 samples in total, asshown in and 2. The word distributionof answers in the VGQA dataset is illustrated in. Specifically, we have 2228, 1139, and912 samples for SVG, TikZ, and Graphviz, respec-tively. After an LLM makes responses to the vectorgraphics questions, we compare the final responseswith the ground-truth answers to compute accuracy.For LLMs with weaker instruction-following ca-pabilities in producing easily parsable outputs, weuse GPT-4 to determine their chosen option andthen assess their accuracy. ResultsEvaluation results of VGQA under GPT-4 (OpenAI, 2023b) are shown in . Severalinteresting findings arise from the results:GPT-4 generally shows strong vector graphicsunderstanding capability. In the zero-shot setting,GPT-4 shows non-trivial accuracy far beyond ran-",
  "The image depicts a flowchart or diagram with four rectangular blocks, each representing a step or component in a process. The blocks are connected by arrows indicating the flow of data or control": ": Examples of generated vector graphics. The ground truth images are rendered by vector graphics codedirectly from Internet. The captions are generated by GPT-4V, The images on the right side are rendered by vectorgraphics code generated by GPT-3.5 or GPT-4. dom accuracy (25%) across all categories. Specif-ically, GPT-4 shows strong performance in TikZ,with an average accuracy of 78%.GPT-4 shows stronger performance in high-level vector graphics language (e.g.,TikZ,Graphviz) compared to low-level vector graph-ics language SVG. In either zero-shot, few-shot,or Chain-of-Thought settings, TikZ and Graphvizshow at least 17% better performance than SVG.As a reminder, TikZ and Graphviz are fundamen-tally different from SVG in terms of the semanticlevels, as SVG is composed of geometry primi-tives while TikZ and Graphviz contain high-levelsemantics such as above\", below\", explicit repre-sentation of nodes and edges, etc.Chain of Thought (CoT) and In-Context Learn-ing (ICL) show some performance improvementsfor some tasks, but not significant.CoT andICL show 7% performance boost for SVG which owns lowest performance among three formats. YetCoT and ICL show no benefits for TikZ and limitedimprovements for Graphviz, where GPT-4 alreadyobtains 83% accuracy under TikZ and Graphviz.Different vector-graphic formats show diversebehaviors upon question types.For SVG, GPT-4 struggles at high-level questions and receives50% accuracy on category and reasoning types,while in TikZ and Graphviz, GPT-4 shows decentperformance across all types of questions. Thisagain demonstrates that GPT-4 shows inferior per-formance in low-level vector graphics tasks, espe-cially on tasks related to reasoning.",
  ": Statistics of VGen on three VG formats": "Finally, we map the generated vector graphicsinto rasterzied images, then use CLIP Score andFrchet Inception Distance (FID) Score to evaluatethe quality of the generated vector graphics.We use CLIP Score to measure the similaritybetween each generated vector graphics and its as-sociated caption. We utilize Long-CLIP (Zhanget al., 2024) instead of the vanilla CLIP (Radfordet al., 2021) since our detailed captions are oftenlonger than CLIPs maximum context length of 77.FID is utilized to evaluate the distribution gap be-tween the original vector graphics and generatedones. For both metrics, we use the score of ourground truths as the upper bound to reflect the qual-ity of the generated images. The overall pipeline isshown in .",
  "In Depth Analysis": "Impact of Different LLMsWe next perform ex-periments over a variety of large language mod-els, including GPT-4, GPT-3.5, Llama-3-70B-Instruct (Meta, 2024) and Llama-3-8B-Instruct. Re-sults are shown in . The results show thatGPT-4 has the best VG understanding ability overvector graphics among those models while Llama-3-70B shows better performance than GPT-3.5. Comparison between LLMs and MLLMs onImage UnderstandingWe find that VLMs suchas LLaVA (Liu et al., 2023) show interesting be-havior compared with LLMs on vector graphics.To evaluate the performance of VLMs on VGQA,we render each visual content in our benchmarkinto the PNG format, and then feed the same ques-tion to VLMs. Specifically, we evaluate LLaVA-1.5-13b (Liu et al., 2024), as shown in .LLaVA-1.5 shows stronger performance in SVGformat compared to TikZ and Graphviz. The strongperformance gain that LLMs obtained on high-level vector graphics languages such as TikZ andGraphviz shows that those kinds of vector graphicsare more aligned with LLMs training data, naturallanguages, which is a highly compressed repre-sentation of the world. Low-level vector graphicslanguages such as SVG cover more low-level vi-sual signals that can be better handled by VLMsusing their rasterized representation.",
  "LengthCategoryColorUsageAvgConceptCountingRelationAvgDomainLayoutRelationAvg": "1-100059.372.265.565.783.279.869.977.678.484.190.484.31000-200047.075.360.861.088.083.879.783.890.477.687.985.32000-300046.876.450.657.985.566.780.077.496.280.077.884.73000-400051.564.154.156.689.769.270.076.390.582.475.082.6> 400048.970.152.557.295.855.081.277.387.082.572.280.6 : VGQA performance under different lengths of vector graphics for GPT-4 with zero-shot prompting. GPT-4performs better on some lengths than others. For instance, in the Graphviz Domain question type, GPT-4 performsat an outstanding 96% accuracy on the 2k-3k range while showing most subpar performance on the <1k range.",
  "Conclusion": "Our study unveils new insights into the capabili-ties of LLMs in understanding and generating vec-tor graphics. We discovered that LLMs demon-strate decent vector graphics understanding in TikZ,Graphviz, and SVGs, with a particular strength inunderstanding vector graphics code with higher-level semantics. We also found that LLMs often ex-hibit strong vector graphics generation capabilities.Interestingly, advanced prompting techniques cansignificantly improve performance for low-levelformats such as SVG, and while GPT-4 had thestrongest performance, open-source models likeLlama-3-70B and Qwen2-72B show competitiveperformance. Our work lays a groundwork for fu-ture studies into LLMs vector graphics understand-ing and generation benchmarking, and we hope itwill inspire further efforts to enhance these capa-bilities. We will release our benchmark dataset andevaluation pipeline.",
  "Limitations": "We acknowledge that one cannot systematicallyevaluate the behavior of the closed-source mod-els we employed, namely GPT-4, GPT-35-Turbo,and GPT-4V. Besides, more evaluations on recentLLMs can be conducted, which can provide moresupporting experiments on LLMs behavior on vec-tor graphics understanding and generation.Furthermore,recent works propose moreprompting techniques such as Tree of Thoughts(ToT) (Yao et al., 2024) and Everything ofThoughts (XoT) (Ding et al., 2024). Incorporatingthese prompting techniques could further enhanceour study.",
  ". Qwen2 technical report": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-rat Behl, et al. 2024. Phi-3 technical report: A highlycapable language model locally on your phone. arXivpreprint arXiv:2404.14219. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C Lawrence Zitnick, andDevi Parikh. 2015. Vqa: Visual question answering.In Proceedings of the IEEE international conferenceon computer vision, pages 24252433.",
  "Jonas Belouadi, Anne Lauscher, and Steffen Eger. 2024.Automatikz: Text-guided synthesis of scientific vec-tor graphics with tikz. In The Twelfth InternationalConference on Learning Representations": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020a.Language models are few-shot learners. In NeurIPS. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020b. Language models are few-shotlearners. NeurIPS, 33:18771901. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712.",
  "Emden R Gansner. 2009. Drawing graphs with graphviz.Technical report, AT&T Bell Laboratories, Murray,Tech. Rep, Tech. Rep": "Nader Gharachorloo, Satish Gupta, Robert F Sproull,and Ivan E Sutherland. 1989. A characterization often rasterization techniques. In Proceedings of the16th annual conference on Computer graphics andinteractive techniques, pages 355368. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. 2020. Generativeadversarial networks. Communications of the ACM,63(11):139144.",
  "David Ha and Douglas Eck. 2017.A neural rep-resentation of sketch drawings.arXiv preprintarXiv:1704.03477": "Jack Hessel, Ari Holtzman, Maxwell Forbes, RonanLe Bras, and Yejin Choi. 2021.CLIPScore: Areference-free evaluation metric for image captioning.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages75147528, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. 2017. Ganstrained by a two time-scale update rule converge to alocal nash equilibrium. Advances in neural informa-tion processing systems, 30.",
  "A. Quint. 2003. Scalable vector graphics. IEEE Multi-Media, 10(3):99102": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, ScottGray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. 2021. Zero-shot text-to-image gener-ation. In International conference on machine learn-ing, pages 88218831. Pmlr. Machel Reid, Nikolay Savinov, Denis Teplyashin,Dmitry Lepikhin, Timothy Lillicrap, Jean-baptisteAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions oftokens of context. arXiv preprint arXiv:2403.05530. Juan A Rodriguez, Shubham Agarwal, Issam H Laradji,Pau Rodriguez, David Vazquez, Christopher Pal,and Marco Pedersoli. 2023. Starvector: Generat-ing scalable vector graphics code from images. arXivpreprint arXiv:2312.11556.",
  "The specific prompt we used": "6.1.1Prompts used to build the datasetQuestion GenerationSystem prompt: The sys-tem prompts used to generate questions are dif-ferent for different types of vector graphics anddifferent types of questions. See the code in sup-plemental material for details.User prompt: The caption of this image is {cap-tion}, generate the json according to the instruction.<IMAGE>",
  "User prompt: \"{code}\". Given this image, an-swer {question}. Options are {options}": "Few-shotSystem prompt: I will present a {for-mat} code. Please answer my questions only basedon code. Answer and only answer the letter cor-responding to the correct option. Do not add anyadditional comment in your response. For yourreference, I will give you some examples.User prompt: This is an example, the code is:{code}User prompt:Given this image,answer{few_shot_sample_question}.Optionsare{few_shot_sample_options}Simulatedassistantprompt:{few_shot_sample_answer}Repeat the last three prompts for three times,each time pass a different samples.User prompt: \"{code}\". Given this image, an-swer {question}. Options are {options} Zero-shot-cotSystem prompt: I will present a{format} code. Please answer my questions onlybased on code. Please consider the question stepby step.User prompt: {code}User prompt: Given this image, the question is{question}. Options are {options}. Do not answerdirectly, consider each option individually.User prompt: Carefully consider if the option Ais correctWait for the large language models reponse andadd its response to the context.User prompt: Carefully consider if the option Bis correctWait for the large language models reponse andadd its response to the context.User prompt: Carefully consider if the option Cis correctWait for the large language models reponse andadd its response to the context.User prompt: Carefully consider if the option Dis correctWait for the large language models reponse andadd its response to the context.User prompt: Which option is the best? Answerand only answer the letter corresponding to thecorrect option. Do not add any additional commentin your response",
  "I will present an SVG code. Answer only based on code. Please consider step by step": ": We include the full conversation with GPT-4 as indicated in . We ask the model to consider if eachoption is correct individually, then ask another GPT-4 model to judge if the reasoning matches the correct answer. 6.1.3Prompts used to evaluate modelsgeneration abilitySystem prompt: Generate a {format} based on thecaption below. You should output the compilablecode without any additional information.User prompt: {caption}"
}