{
  "Abstract": "Instruction tuning, or supervised finetuning onextensive task-specific data, is necessary forLarge Vision-Language Models (LVLMs) togeneralize well across a broad range of vision-language (VL) tasks. However, training onlarge VL datasets can become prohibitively ex-pensive. In this work, we introduce COIN-CIDE, an effective and scalable data selectiontechnique that uses a small model as a referencemodel to select visual instruction tuning datafor efficient finetuning of a target LVLM, fo-cusing on diversity and transferability. Specifi-cally, we cluster the training data using internalactivations from a small model, which iden-tifies VL concept-skill compositions neededby a target LVLM. We then sample data fromthese diverse clusters by considering their den-sity and transferability, or the ability to trans-fer well to other concept-skill compositions.This approach ensures the diversity of thesecompositions, which is vital for LVLM gener-alization. Extensive experiments demonstratethat COINCIDE achieves superior performanceand data selection efficiency against 8 strongbaselines on two distinct datasets: LLaVA-1.5 and Vision-Flan. Using only 20% of theLLaVA-1.5 dataset, COINCIDE achieves per-formance comparable to the LVLM finetunedon the whole dataset, with 70% reduction ofthe wall-clock running time. On the Vision-Flan dataset, our method achieves superiorresults with only 16.7% of the training data.Our code is available at",
  "Score": "Mostly from -A-OKVQA -LLaVA-Reason -OCR-VQA Mid 20% Top 20% Mostly from -ShareGPT -GQA Biased data selection : Different VL tasks in LLaVA-1.5 (Liu et al.,2023a) exhibit different score distributions. Thus, select-ing data based on a single score metric like EL2N (Paulet al., 2021) or Self-Filter (Chen et al., 2024a) resultsin a biased coreset (red), substantially decreasing thediversity within the coreset. step, referred to as visual instruction tuning (VIT),substantially enhances multimodal instruction-following capabilities. To achieve broad general-ization, recent works (Cha et al., 2023; Dong et al.,2024; Chen et al., 2024b; Li et al., 2024) integratean increasing number of VL tasks into VIT.However, training on extensive VIT data incurssignificant computational cost, making the processinfeasible for small academic labs and individualresearchers. Additionally, it is not clear if all theVIT data are necessary for good generalization, asdifferent VL tasks have different abilities to transferto downstream tasks (Tiong et al., 2024; Xi et al.,2023; Ostapenko et al., 2024).In this paper, we investigate the selection of acoreset, a subset that approximates the performanceof the full dataset, from large VIT datasets. Conven-tional coreset selection approaches (Marion et al.,2023; Zhou et al., 2023; Chen et al., 2023a) usu-ally utilize a score metric to select training data. AsVIT datasets are highly diverse and feature multipledata modes (), data selection using any sin-gle metric would produce a coreset dominated bya few tasks. indicates that, selecting 20%of data from any part of the metric distribution of Q: Can you discuss some safety precautions that snowboarders should take while doing jumps? VQAv2GQA LLaVA-ConvLLaVA-Reason Jumping with snowboard / Reasoning Q: What risks should the snowboarder consider when performing aerial jumps? Dog playing in water / Color attribute Q:What is the color of the dog in the top part? Q: What color is the dog's collar? Concept / Skill compositions",
  ": Different VL tasks (e.g., VQAv2 and GQA,LLaVA-Conv and LLaVA-Reason) share VL concept-skill compositions": "EL2N (Paul et al., 2021) or Self-Filter (Chen et al.,2024a) would exclude many data modes, whichseverely reduces the diversity of the selected core-set and harms generalization. As our experimentsshow (), this type of coreset selection de-grades LVLM performance.Our solution to the multitude of data modes isstraightforward: we explicitly identify the modesby clustering the VIT data points using featuresfrom multiple layers in a small LVLM. Interest-ingly, we observe that the clusters thus identifiedroughly coincide with compositions of VL con-cepts and skills. For example, a concept could bestreet signs or trains on a railroad, while a skillcould be OCR, recognizing color, or reasoning.Upon close inspection, we find that different VLtasks contain overlap over these concept-skill com-positions. As exemplified in , LLaVA-Conv and LLaVA-Reason contain questions aboutthe risks of snowboard jumps, despite their sepa-rate focuses on multi-turn conversations and rea-soning. This suggests sampling over the clusterswould be more effective in enhancing the diversityof VL concept-skill compositions than samplingover datasets or tasks.To this end, we introduce COre INstructionConcept-skIll Data Election (COINCIDE), whichidentifies VL concept-skill compositions throughdata clustering using activations from an off-the-shelf, small LVLM ( Left). From eachcluster, COINCIDE selects training data for a target LVLM by considering transferability (i.e., how wellknowledge from each cluster can facilitate LVLMslearning in other clusters) and internal density ofclusters ( Right). Empirically, we find thattransferability correlates well with cosine similarityamong clusters. Based on the findings, we selectmore data points from more transferable clusters.Further, we sample fewer data points from denserclusters, as data points in dense clusters are likelyredundant.Another major challenge of coreset selection isits high computational cost. Existing techniquesoften require expensive steps like additional train-ing (Du et al., 2023; Mekala et al., 2024; Chenet al., 2024a), gradient calculation (Xia et al., 2024;Liu et al., 2024), or the use of bigger and moreadvanced models (Chen et al., 2023a; Liu et al.,2023c). The time complexity and the assumptionof larger models contradict the primary goal ofcoreset selection, which is to reduce the develop-ment cost of new models larger than existing ones.In comparison, COINCIDE assumes only a VLM(2B) smaller than the target LVLM (7B, 13B) anddoes not require any backward pass.We validate the effectiveness of COINCIDEacross a wide range of coreset selection scenariosusing two distinct VIT datasets, LLaVA-1.5 (Liuet al., 2023a) and Vision-Flan (Xu et al., 2024). Theexperimental results demonstrate that our methodachieves performance competitive with that of theLVLM finetuned with the full dataset, with 30% oftime cost including the data selection and training.Our approach also achieves superior performanceand efficiency compared to 8 strong baselines.In summary, our contributions are as follows: We introduce COINCIDE, an efficient coresetselection pipeline for a target LVLM using anexisting small reference model to cluster train-ing data. Training on 16.7-20% data selectedby COINCIDE achieves comparable perfor-mance to whole-dataset finetuning, leading to70% wall-clock time reduction.",
  "Related Work": "Coreset SelectionCoreset selection attempts toextract a subset of training data that functions com-parably to the full training set. This techniqueis adopted for problems like active learning (Weiet al., 2015; Sener and Savarese, 2018), continuallearning (Rebuffi et al., 2017; Aljundi et al., 2019),and data pruning (Pleiss et al., 2020; Paul et al.,2021). Recent works (Zhou et al., 2023; Xia et al.,2024) investigate coreset selection for instructiontuning of LLMs. Alpagasus (Chen et al., 2023a)uses ChatGPT (OpenAI, 2022) to rate the quality ofinstruction samples. S2L (Yang et al., 2024) lever-ages the training loss trajectory of smaller modelsto find optimal samples for training larger LLMs.DiverseEvol (Wu et al., 2023) utilizes the targetmodel itself to iteratively choose beneficial data forthe current training episode. Coreset Selection for Visual Instruction TuningSeveral very recent papers address the coreset se-lection problem for visual instruction tuning (Weiet al., 2023; Chen et al., 2024a; Liu et al., 2024).Self-Filter (Chen et al., 2024a) scores VIT data us-ing a score-net trained along with the target LVLM.The concurrent work TIVE (Liu et al., 2024) em-ploys gradient information from the target LVLMto compute task- and sample-level importance. Al-though effective, it demands considerable mem-ory to store the high-dimensional gradient vectors.Moreover, these methods require backward passes,which are expensive due to the large training set.Both also overlook the diversity of selected data,which is vital for generalization. In contrast, ourapproach reduces wall-clock running time and con-siders both transferability and diversity. VL Concept and Skill DiscoveryConcept dis-covery in neural networks is a key topic in inter-pretability research (Kim et al. 2018; FEL et al.2023; Manning et al. 2020). Notably, Kowal et al.(2024) performs hierarchical clustering in layer-wise activation space. Tiong et al. (2024) attemptsto identify latent skills underlying VL datasets.Michaud et al. (2023) performs spectral clusteringto discover LLMs skills. Though these works pro-vide inspiration, they are orthogonal to our work,whose main objective is to sample from data clus-ters rather than understanding existing neural net-works. The only application of concept discoverywe are aware of is by Gupta et al. (2017), showingconsistent VL concepts improve transfer learning.",
  "Method": "We start by introducing the framework that utilizesneuron activations from a small LVLM to groupVIT data into clusters, where each cluster com-prises samples exhibiting a similar concept-skillcomposition (.2). Next, we conduct exper-iments to examine the correlation between the simi-larity of a cluster centroid to other centroids and thetransferability of that cluster to others (.3).Based on our findings, we describe our data selec-tion strategy, which performs cluster-wise sampleselection by selecting different numbers of samplesfrom clusters depending on their transferability anddiversity (.4). The overall framework ofour approach is illustrated in .",
  "Preliminaries": "A modern LVLM typically consists of a visual en-coder and an LLM, which are connected by inter-mediate network layers. The visual information isfed to the LLM as input (Dai et al. 2023; Liu et al.2023b), or guides cross-attention (Alayrac et al.2022). Here we focus on a transformer-based LLMthat receives visual information as input tokens.The l-th transformer layer receives the visualtokens xvl RNvD and text tokens xtl RNtD,where Nv and Nt are the numbers of tokens, and Dis the hidden dimension size. A transformer layercontains a multi-head self-attention (MSA) and afeed-forward network (FFN). For the purpose ofthis paper, we describe only MSA formally:",
  "Discovering Concept-Skill Compositions": "An LVLM aims to learn about a large variety ofvisual-linguistic concepts and skills. Hence, it isimportant to automatically sort training data intoconcepts and skills, so that the coreset can pro-vide sufficient coverage of these.Recent stud-ies (Schwettmann et al., 2023; Pan et al., 2023;Gandelsman et al., 2024) reveal that the internalactivations at various layers of LVLMs may encodedifferent visual concepts.To figure out which layer of the LVLM providesthe best feature representation for visual conceptand skill discovery, we perform a preliminary vi-sualization study of TinyLLaVA-2B (Zhou et al., Train on railroad / Counting",
  "Coreset": "Intra-ClusterSampling Q: How many rail cars are there? Q: How many cars are on the train? : Illustration of COINCIDE. Our method utilizes a small LVLM to cluster visual instruction tuning databased on concept-skill compositions. We then assess the cluster transferability as the mean cosine similarity to othercluster centroids. We further compute the cluster density as the mean Gaussian kernel distance among all data pairswithin the cluster. Using cluster transferability and density, COINCIDE determines the number of data to samplefrom each cluster and performs intra-cluster sampling. Finally, it combines all the selected samples from all theclusters to compose the final coreset. 2024). Given an image and a textual question, wevisualize the image patches that contribute the mostto the generation of the ground-truth answer. Usingfeatures from different layers highlights differentimage patches. Ideally, we can compare the visu-alization with human intuition and select the layerthat agrees with human intuition the most. We pro-vide detailed experimental procedures with somevisualization results in Appendix B.Perhaps surprisingly, we find that the best layervaries substantially according to the input. That is,the VL concepts and skills are distributed across dif-ferent layers. Hence, for the clustering, we choosefive layers spanning from the initial to top layersof the model to cover a wide range of concepts andskills and use the concatenation of their output asthe feature vector of each data point.We cluster VIT training data points using theirfeature vector from multiple layers of a smallLVLM, called a reference model. We extract thefeatures right after the MSA of the l-th layer (Eq. 1)and process them into unit-length vectors:",
  "where M denotes the number of layers where weextract the features, and the subscripts l1, . . . lMare the layer indices. The resultant um R2MD": "is the final multimodal feature of the data point.Then, we perform spherical k-means clusteringon um, yielding K clusters. To ensure the purityof clusters, we set K to a large number, such as10, 000. Despite its simplicity, the k-means pro-cedure runs in O(NK) time for N data points,which is advantageous when both N and K arelarge. Other clustering techniques such as spectralclustering or affinity propagation are much moreexpensive. Qualitative analysis indicates the clus-ters coincide with concept-skill compositions. Weprovide visualization of the clusters in Appendix C.",
  "Measuring Cluster Transferability": "Empirical evidence shows that datasets differ intheir ability to generalize to other datasets (Zamiret al., 2018; Achille et al., 2020). We hypothesizethat (1) data clusters also have varying levels oftransferability and (2) clusters close together in fea-ture space transfer well to each other. If (1) is true,it would be beneficial to select data from highlytransferable clusters. If (2) is true, we can use dis-tance among clusters as a proxy for transferability.We design an experiment to verify the hypothe-ses. Following Chen et al. (2023b), to measure r: 0.72p-value: 5.3e-09 0.4 0.0 -0.4 0.250.300.350.400.450.500.55 r: 0.66p-value: 2.1e-07 0.4 0.0 -0.4",
  "Average cosine similarity ()": "Vision-Flan Transferability LLaVA-1.5 Transferability 0.250.300.350.400.450.500.55 : Correlation between cluster centroid similarityand transferability. We examine the correlations in theLLaVA 1.5 (Liu et al., 2023a) and Vision-Flan (Xu et al.,2024) datasets, with each point representing a sourcecluster. We report the Pearson correlation coefficient (r)and p-value. transferability from cluster Ci to cluster Cj, werun two training sessions. In the first, we finetunean LVLM on the same number of samples, Nc,drawn from Ci and Cj respectively. In the second,we finetune on Nc samples from Cj only. Afterfinetuning, both models are tested on unseen sam-ples from Cj, yielding test losses Li,jj and Ljj.The difference Ljj Li,jj can be seen as thedegree by which Ci facilitates the learning of Cj.We aggregate over target clusters to compute thetransferability of the source cluster Ci:",
  "j=1cos(ei, ej),(5)": "where ei is the cluster centroid of cluster Ci.We compute the correlation between transfer-ability Ti and average cosine similarity Si over allpossible pairings between 50 random source clus-ters and 50 random target clusters, and plot theresults in . We find that (1) clusters differsignificantly in transfer power, and (2) Si and Tihave a strong positive correlation (0.66-0.72), in-dicating that the cosine similarity among clusterscan serve as an effective and inexpensive proxy fortransferability. For K clusters, the time complexity",
  "p,qCi,p=qd(p, q),(6)": "where p and q are two distinct data points fromcluster Ci, and d(p, q) = exp(ump umq 2) isthe Gaussian kernel function with ump and umq be-ing the multimodal neuron activations (Eq. 3) ofdata points p and q, respectively. The small Divalue indicates that the cluster Ci is highly diverse.In order to create a coreset of Ncore samples,we select from cluster Ci exactly NcorePi samples.Here, Pi exp(Si/(Di)) is a categorical dis-tribution and is a temperature hyperparameter.This approach enables us to select more samplesfrom more transferable and less dense clusters toenhance training efficacy, while still selecting afew samples from other clusters to ensure diverseconcept-skill compositions in the coreset.From cluster Ci, we aim to select NcorePi sam-ples that are representative of the original data dis-tribution of Ci. We compute the distance betweenthe original cluster Ci and the set of sampled datapoints Ci as MMD2, the squared maximum meandiscrepancy, which is defined as:",
  "COINCIDE (Ours)76.559.846.869.255.686.11495.663.154.567.397.4": "et al., 2024). The LLaVA-1.5 dataset contains 665kVIT data from 12 different VL tasks. The Vision-Flan dataset comprises 191 VL tasks, each with ap-proximately 1k expert-annotated VIT data points,totaling 186k samples. Models for Training and Data SelectionForthe target LVLMs, we use the pre-trained LLaVA-1.5 model (Liu et al., 2023a) with a default size of7B parameters unless otherwise specified. In allexperiments, we train the models using LoRA (Huet al., 2022) for one epoch, following the officialfinetuning hyperparameters specified in LLaVA-1.5. As a reference model, we use the TinyLLaVA-2B (Zhou et al., 2024), a small LVLM finetuned onthe target VIT dataset, for efficient coreset selectionfor all methods unless otherwise specified. Allexperiments are conducted using 4 V100 GPUs. Evaluation BenchmarkTo assess the gener-alization of finetuned LVLMs across diverse vi-sual instructions, we evaluate the models on sev-eral widely adopted zero-shot multimodal eval-uation benchmarks, including 1) visual ques-tion answering:VQAv2 (Goyal et al., 2017),GQA (Hudson and Manning, 2019), VizWiz (Gu-rari et al., 2018); 2) knowledge-grounded QA:ScienceQA (Lu et al., 2022); 3) Optical Charac-ter Recognition (OCR): TextVQA (Singh et al.,2019); 4) hallucination: POPE (Li et al., 2023);5) multiple-choice: MME (Fu et al., 2023), MM-Bench (Liu et al., 2023d); 6) free-form generation:LLaVA-Bench (Liu et al., 2023b), MM-Vet (Yuet al., 2023). In all experiments, we follow the pro-tocols outlined in LLaVA-1.5 and Vision-Flan toselect evaluation benchmarks. Further explanations of these benchmarks are provided in Appendix A.Since each evaluation benchmark has a differentscale, we compute average relative performance,denoted as Rel., across benchmarks to assess thelevel of generalization. Each relative performanceis derived from the formula: (model performance /full-finetuned performance) 100%. BaselinesWe compare our method with sev-eral coreset selection techniques:CLIP-Score,EL2N (Paul et al., 2021),Perplexity (Mar-ion et al., 2023), SemDeDup (Abbas et al.,2023), D2-Pruning (Maharana et al., 2023), Self-Sup (Sorscher et al., 2022).We also comparewith a recent VIT coreset selection method, Self-Filter (Chen et al., 2024a). We additionally reportthe results of Random, the model finetuned with thecoreset collected by random sampling, and Full-Finetune, the model finetuned with the full VITdataset. The details of the baseline methods areprovided in Appendix A.",
  "Results and Discussion": "COINCIDE surpasses baselines on LLaVA-1.5. presents model performance when we limitthe coreset to 20% of the size of the LLaVA-1.5VIT dataset. COINCIDE is either the best or aclose second on 7 out of 10 benchmarks, includingVQAv2, GQA, SQA-I, TextVQA, POPE, MME,and MMBench-en. On average, COINCIDE out-performs the best baseline by 1.6 percent points(pp) in relative performance.Interestingly, all baselines perform worse thanthe random sampling on average relative perfor-mance, suggesting that they may be susceptibleto the selection bias, which is discussed in the in- LLaVA-1.5 Full-FinetuneRandomCLIP-ScoreEL2NPerplexity SemDeDupD2-PruningSelf-SupSelf-FilterOurs 40%60%20%10%5% Sampling ratio Relative Performance (%)",
  "COINCIDE (Ours)56.71222.226.281.963.8101.0": "troduction and illustrated in . In contrast,COINCIDE considers the diversity of VL concept-skill compositions, demonstrating high generaliza-tion across a broad range of visual instructions. Wefurther analyze the selection bias of the baselinesand effectiveness of COINCIDE in Appendix E.In , we show the performance compari-son across different coreset sizes as proportions ofthe original LLaVA-1.5 dataset. COINCIDE con-sistently outperforms other baselines across varioussampling ratios, underscoring the effectiveness ofour approach. COINCIDE also performs well onLLaVA-1.5-13B, as shown in Appendix F.1. One Sixth of Vision-Flan selected by COIN-CIDE outperforms full dataset.We further eval-uate the coreset selection techniques on the Vision-Flan VIT dataset (Xu et al., 2024) and show theresults in . COINCIDE exceeds the perfor-mance of the model finetuned on the whole Vision- 16.7%4.2%8.3% Sampling ratio 70.0 80.0 90.0 100.0 Relative Performance (%) Vision-Flan Full-FinetuneRandomCLIP-ScoreEL2NPerplexity SemDeDupD2-PruningSelf-SupSelf-FilterOurs",
  "Ours": ": Comparison of coreset selection techniques onaverage relative performance and wall-clock time cost.The wall-clock time cost includes both the data selectionand finetuning of the target LVLM. The time cost ismeasured in hours of running time on a computing nodewith 4 V100 GPUs. Flan data by 1.0 pp and the performance of thebest baseline by 4.5 pp, using a selected subset16.7% (1/6) of its size. Further, as illustrated in Fig-ure 6, COINCIDE maintains consistently high per-formance across several sampling rates. We note that Vision-Flan, with its 191 VL tasks,is much more diverse than the LLaVA-1.5 datasetof 12 tasks. The stronger performance of COIN-CIDE on the Vision-Flan suggests that COINCIDEalgorithm is well adapted to the use case of visualinstruction tuning, which is increasingly performedon larger and more diverse sets of tasks. Another curious phenomenon is that severalbaselines, including CLIP-Score, Perplexity, andSelf-Filter, experience performance declines as thesampling ratio increases in . A similartrend is observed in the random baseline in Fig-ure 5. This underscores the importance of delib-erate coreset selection, as merely increasing thedataset size does not guarantee improved LVLMcapabilities. : Ablation studies of COINCIDE. (a) Effect of different reference models. The time cost includes both thedata selection and finetuning of the target LVLM and is measured in hours of running time on a computing nodewith 4 V100 GPUs. (b) Ablation on data selection criteria of our approach, transferability (S) and density (D). (c)The performance of different intra-cluster sampling strategies across various coreset sizes.",
  "Greedy-MMD2-minimize 90.7 93.8 97.4 98.4 99.4": "COINCIDE provides wall-clock training timereduction and is Pareto superior.In ,we plot the wall-clock time cost of the entirepipeline of data selection and model finetuning ver-sus the average relative performance (Rel.) on theLLaVA-1.5 dataset. COINCIDE achieves 97.4%,98.4%, and 99.4% relative performance with thewall-clock times of 15.1, 25.1, and 35.1 hours, re-spectively. In contrast, finetuning on all data takes50 hours.We observe that COINCIDE provides Pareto su-perior solutions to all baselines. This is mainly dueto the excellent time complexity of COINCIDE,which is linear to the number of training data points.Moreover, our method discovers the transferabilityamong clusters at a low computational cost. It re-quires only cosine similarity calculations, with atime complexity quadratic to the number of clus-ters. Hence, COINCIDE provides a scalable dataselection procedure.COINCIDE also utilizes neuron activations fromintermediate layers of the small reference modelrather than the final outputs, avoiding completeforward passes like other baselines. Additionally,COINCIDE does not require training of additionalnetworks that score data points, like Self-Filter.Neither does it require backward passes like theconcurrent work TIVE (Liu et al., 2024). The com-bination of all these factors leads to an efficientsolution to coreset selection.",
  "Further Analysis and Ablation": "Alternative Reference ModelsWe analyze theeffects of different reference models, which are themodels used to extract features for clustering andcosine similarity. We compare four models, CLIP,TinyLLaVA-0.9B, TinyLLaVA-2B, and LLaVA-1.5-7B, and report the time cost of the entire coreset selection pipeline and average relative performancein (a). We observe that CLIP performs theworst whereas TinyLLaVA-2B performs the bestwith reasonable time cost in data selection. How-ever, the differences between TinyLLaVA-0.9B,TinyLLaVA-2B, and LLaVA-1.5-7B are small. Weconclude that a well-trained small model can serveeffectively as a reference model in coreset selectionfor a target LVLM. We also examine the robustnessof COINCIDE when the reference model is fine-tuned on a different VIT dataset, which is detailedin Appendix F.2. Ablation on Data Selection CriteriaTo validateour coreset selection method, we conduct ablationstudies on the two data selection criteria, transfer-ability and density, as summarized in (b).In the first ablation, without using either criterion,we simply select the same number of samples fromeach cluster. This results in inferior performance,which suggests that naive stratified sampling fromthe clusters is not sufficient, possibly due to theheterogeneous nature of the clusters. In the sec-ond ablation, number of samples from each clusteris proportional to the transferability of the cluster,leading to a 1.5 percentage point (pp) increase. Thethird ablation selects number of samples inverselyproportional to density, yielding a modest enhance-ment of 0.3 pp. Finally, combining both transfer-ability and density provides a sizeable increase of3.0 pp, demonstrating that the two selection criteriaare complementary to each other. Intra-cluster Selection CriteriaCOINCIDE se-lects samples within a cluster by minimizingMMD2. We examine the effects of two alternativetechniques, random selection and selecting samplesclosest to the centroids. As shown in (c),in small coresets, samples closest to the centroids, which are probably not outliers or hard samples,lead to high performance. In contrast, under highsampling ratios (i.e., large coresets), selecting di-verse data using the MMD2 metric leads to highperformance. This is reminiscent of the findingof Sorscher et al. (2022) that easy samples are ben-eficial when the sampling ratio is small, whereashard samples are advantageous when the samplingratio is large. Overall, COINCIDE is robust to thechoice of intra-cluster sampling, but adapting theintra-cluster sampling method to the sampling ratiocan enhance the effectiveness of our approach.",
  "Conclusion": "In this paper, we introduce COINCIDE, a cluster-level data selection technique for efficient visual in-struction tuning of Large Vision-Language Models.We demonstrate that clustering based on internalactivations from a small model can represent visual-linguistic concept-skill compositions shared amongdiverse tasks in visual instruction tuning datasets.Additionally, our empirical investigation validatesa strong positive correlation between cosine simi-larity and transferability among clusters. Based onthe transferability and density of clusters, COIN-CIDE selects more samples from more transferableand less dense clusters to enhance training efficacy,while preserving the diversity of concept-skill com-positions within the coreset to ensure better modelgeneralization ability. Comprehensive experimentson the LLaVA-1.5 and Vision-Flan datasets demon-strate that our method outperforms baselines acrossseveral benchmarks with the lowest data selectioncost, showcasing its effectiveness and efficiency.The success of COINCIDE suggests redundancyin popular VIT datasets and underscores the im-portance of a thorough understanding of data intraining LVLMs.",
  "Limitations": "In our experiments, we observe that VL concept-skill compositions are shared across various VLtasks and identify VL concept-skill compositionsthat transfer well to others. However, after identify-ing these compositions and performing coreset se-lection, we finetune the target LVLMs by randomlyselecting samples from the coreset. Recognizingthe growing research attention on the importanceof training order in LLM instruction tuning, we be-lieve that considering the training order for LVLMsis crucial to enhance efficiency in visual instruction tuning. In future research, we aim to develop acurriculum learning algorithm that automaticallydetermines the optimal training order based on theidentified VL concept-skill compositions to furtherreduce the development cost of a new model.Additionally, we assess whether the data withsimilar concept-skill compositions are concentratedwell on the clusters through human inspection.Therefore, further investigation should be con-ducted to quantitatively evaluate the clustering ofdata with similar concept-skill compositions, whichmay enable accurate identification of VL concept-skill compositions and accurate quantification oftheir transferability.",
  "Ethics Statement": "In this work, we use publicly available visual in-struction tuning datasets for coreset selection toenable easy replication. However, some data in thedatasets contain erroneous answers about the visualcontent or images that do not clearly connect withthe provided answers. Finetuning Large Vision-Language Models (LVLMs) with such data maylead to the generation of erroneous interpretationsof images or hallucinations. This may pose an ethi-cal issue for LVLM deployment in the real world.However, current coreset selection techniques, in-cluding ours, do not address hallucination in theirselection processes. This motivates further researchin coreset selection to identify visual instructiontuning data that minimizes hallucinations, aimingto build more reliable and trustworthy LVLMs.",
  "Acknowledgements": "Jaewoo Lee and Sung Ju Hwang are supported bythe National Research Foundation of Korea (NRF)grant funded by the Korea government (MSIT)(No. RS-2023-00256259) and a grant of the Ko-rea Machine Learning Ledger Orchestration forDrug Discovery Project (K-MELLODDY), fundedby the Ministry of Health & Welfare and Ministryof Science and ICT, Republic of Korea (No. RS-2024-12345678). Boyang Li is supported by theNanyang Associate Professorship and Fellowship(NRF-NRFF13-2021-0006) of the National Re-search Foundation, Singapore. Any opinions, find-ings, conclusions, or recommendations expressedin this material are those of the authors and do notreflect the views of the funding agencies.",
  "Alessandro Achille, Giovanni Paolini, Glen Mbeng, andStefano Soatto. 2020. The information complexityof learning tasks, their structure and their distance.arXiv Preprint 1904.03292": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Brock, Aida Nematzadeh, Sahand Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karen Simonyan. 2022.Flamingo: a visual language model for few-shotlearning. arXiv Preprint 2204.14198.",
  "Junbum Cha, Wooyoung Kang, Jonghwan Mun, andByungseok Roh. 2023.Honeybee:Locality-enhanced projector for multimodal LLM.arXivpreprint arXiv:2312.06742": "Hila Chefer, Shir Gur, and Lior Wolf. 2021. Genericattention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Pro-ceedings of the International Conference on Com-puter Vision (ICCV). Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, KalpaGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-vasan, Tianyi Zhou, Heng Huang, and Hongxia Jin.2023a. Alpagasus: Training A better alpaca withfewer data. arXiv preprint arXiv:2307.08701. Mayee Chen, Nicholas Roberts, Kush Bhatia, JueWANG, Ce Zhang, Frederic Sala, and ChristopherR. 2023b. Skill-it! a data-driven skills frameworkfor understanding and training language models. InAdvances in Neural Information Processing Systems(NeurIPS). Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu,Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, andHeng Huang. 2024a. Your vision-language modelitself is a strong filter: Towards high-quality in-struction tuning with data selection. arXiv preprintarXiv:2402.12501. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye1,Zhangwei Gao, Erfei Cui, Wenwen Tong, KongzhiHu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xi-aoyi Dong, Hang Yan, Hewei Guo, Conghui He,Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang,Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, MinDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin,Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024b. Howfar are we to gpt-4v? closing the gap to commercialmultimodal models with open-source suites. arXivpreprint arXiv:2404.16821. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning.In Ad-vances in Neural Information Processing Systems(NeurIPS). Xiaoyi Dong, Pan Zhang, Yuhang Zang, YuhangCao, Bin Wang, Linke Ouyang, Songyang Zhang,Haodong Duan, Wenwei Zhang, Yining Li, HangYan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li,Jingwen Li, Wenhai Wang, Kai Chen, Conghui He,Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin,and Jiaqi Wang. 2024. Internlm-xcomposer2-4khd:A pioneering large vision-language model handlingresolutions from 336 pixels to 4k hd. arXiv preprintarXiv:2404.06512.",
  "Mods: Model-oriented data selection for instructiontuning. CoRR, abs/2311.15653": "Thomas FEL, Victor Boutin, Louis Bthune, Remi Ca-dene, Mazda Moayeri, Lo Andol, Mathieu Chalvi-dal, and Thomas Serre. 2023. A holistic approachto unifying automatic concept extraction and con-cept importance estimation. In Advances in NeuralInformation Processing Systems (NeurIPS). Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023. MME: A comprehensive evaluationbenchmark for multimodal large language models.arXiv preprint arXiv:2306.13394. Yossi Gandelsman, Alexei A. Efros, and Jacob Stein-hardt. 2024. Interpreting clips image representationvia text-based decomposition. In Proceedings of theInternational Conference on Learning Representa-tions (ICLR). Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the V in VQAmatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE International Conference on Computer Visionand Pattern Recognition (CVPR). Tanmay Gupta, Kevin Shih, Saurabh Singh, and DerekHoiem. 2017. Aligned image-word representationsimprove inductive transfer across vision-languagetasks. In Proceedings of the International Conferenceon Computer Vision (ICCV).",
  "visual questions from blind people. In Proceedingsof the IEEE International Conference on ComputerVision and Pattern Recognition (CVPR)": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In Proceedings of the Inter-national Conference on Learning Representations(ICLR). Drew A. Hudson and Christopher D. Manning. 2019.GQA: A new dataset for real-world visual reason-ing and compositional question answering. In Pro-ceedings of the IEEE International Conference onComputer Vision and Pattern Recognition (CVPR).",
  "Been Kim, Oluwasanmi Koyejo, and Rajiv Khanna.2016. Examples are not enough, learn to criticize!criticism for interpretability. In Advances in NeuralInformation Processing Systems (NeurIPS)": "Been Kim, Martin Wattenberg, Justin Gilmer, CarrieCai, James Wexler, Fernanda Viegas, and Rory sayres.2018.Interpretability beyond feature attribution:Quantitative testing with concept activation vectors(TCAV). In Proceedings of the International Confer-ence on Machine Learning (ICML). Matthew Kowal, Richard P Wildes, and Konstantinos GDerpanis. 2024. Visual concept connectome (vcc):Open world concept discovery and their interlayerconnections in deep models. In Proceedings of theIEEE International Conference on Computer Visionand Pattern Recognition (CVPR). Yanwei Li, Yuechen Zhang, Chengyao Wang, ZhishengZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, andJiaya Jia. 2024. Mini-gemini: Mining the potentialof multi-modality vision language models. arXivpreprint arXiv:2403.18814. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluat-ing object hallucination in large vision-language mod-els. In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP).",
  "Advances in Neural Information Processing Systems(NeurIPS)": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the International Con-ference on Machine Learning (ICML). Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, GeorgSperl, and Christoph H. Lampert. 2017. icarl: In-cremental classifier and representation learning. InProceedings of the IEEE International Conference onComputer Vision and Pattern Recognition (CVPR). Sarah Schwettmann, Neil Chowdhury, Samuel Klein,David Bau, and Antonio Torralba. 2023. Multimodalneurons in pretrained text-only transformers.InIEEE/CVF International Conference on ComputerVision, ICCV 2023 - Workshops, Paris, France, Octo-ber 2-6, 2023.",
  "Ozan Sener and Silvio Savarese. 2018. Active learn-ing for convolutional neural networks: A core-setapproach. In Proceedings of the International Con-ference on Learning Representations (ICLR)": "Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards VQA modelsthat can read. In Proceedings of the IEEE Interna-tional Conference on Computer Vision and PatternRecognition (CVPR). Ben Sorscher, Robert Geirhos, Shashank Shekhar, SuryaGanguli, and Ari Morcos. 2022. Beyond neural scal-ing laws: beating power law scaling via data pruning.In Advances in Neural Information Processing Sys-tems (NeurIPS). Gabriela Ben Melech Stan, Raanan Y. YehezkelRohekar, Yaniv Gurwicz, Matthew Lyle Olson,Anahita Bhiwandiwalla, Estelle Aflalo, ChenfeiWu, Nan Duan, Shao-Yen Tseng, and VasudevLal. 2024. Lvlm-intrepret: An interpretability toolfor large vision-language models. arXiv preprintarXiv:2404.03118.",
  "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan,Sanjeev Arora, and Danqi Chen. 2024. LESS: se-lecting influential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333": "Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby,Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and LifuHuang. 2024. Vision-flan: Scaling human-labeledtasks in visual instruction tuning. arXiv preprintarXiv:2402.11690. Yu Yang, Siddhartha Mishra, Jeffrey N. Chiang, andBaharan Mirzasoleiman. 2024. Smalltolarge (S2L):scalable data selection for fine-tuning large languagemodels by summarizing training trajectories of smallmodels. arXiv preprint arXiv:2403.07384. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and LijuanWang. 2023. Mm-vet: Evaluating large multimodalmodels for integrated capabilities. arXiv preprintarXiv:2308.02490. Amir R. Zamir,Alexander Sax,William Shen,Leonidas J. Guibas, Jitendra Malik, and SilvioSavarese. 2018.Taskonomy: Disentangling tasktransfer learning. In Proceedings of the IEEE Inter-national Conference on Computer Vision and PatternRecognition (CVPR).",
  "ADetails of Experimental Setups": "Evaluation BenchmarkWe provide in-depthexplanations of the multimodal evaluation bench-marks used in our experiments. (1) VQAv2 (Goyalet al., 2017) evaluates the ability to understandand reason about general visual content by an-swering open-ended questions based on images.(2) GQA (Hudson and Manning, 2019) assessescompositional reasoning and understanding skills,requiring models to understand relationships andattributes of objects within images. (3) Vizwiz (Gu-rari et al., 2018) is designed to evaluate the modelsability to cope with real-world visual impairments.(4) ScienceQA-Image (SQA-I) (Lu et al., 2022)tests the models science-related reasoning and vi-sual understanding of images. (5) TextVQA (Singhet al., 2019) specifically targets text in images, as-sessing the Optical Character Recognition (OCR)ability of models. (6) POPE (Li et al., 2023) mea-sures object hallucination in models. (7) MME (Fuet al., 2023) contains binary choice questions de-signed to evaluate perception and cognition abili-ties through 14 subtasks. (8) MMBench (Liu et al.,2023d) evaluates various abilities of models, cov-ering object detection, text recognition, relationreasoning, etc., using tests conducted in English(en) or Chinese (cn). (9) LLaVA-Bench (Liu et al.,2023a) is specifically designed for evaluating mod-els on visual instruction-following and chat ability.(10) MM-Vet (Yu et al., 2023) measures VL capa-bilities, including recognition, OCR, knowledge,language generation, spatial awareness, and math.",
  "COINCIDE (Ours)K : 10,000, : 0.1K : 5,000, : 0.1": "prediction, defined as exp(E[log p(x)]). Thismetric assesses the uncertainty in the modelspredictions. For both EL2N and Perplexity, weselect data from the middle score distribution,as this range has been shown to perform best inprior research (Marion et al., 2023). SemDeDup (Abbas et al., 2023) removes seman-tically duplicated data by clustering the outputembeddings of the last token from the referencemodels final layer. This helps in reducing redun-dancy in the selected coreset. D2-Pruning (Maharana et al., 2023) representsthe dataset as a graph where nodes representsample difficulty and edges represent distancesbetween samples. It actively uses the graph topreserve diversity in the coreset. We use theAUM (Pleiss et al., 2020) score to indicate diffi-culty, defined as py(x)maxi=y pi(x), where py(x)",
  "for any non-ground-truth label. For the distancesbetween samples, we calculate the L2 distancebetween averaged output embeddings from thelast layer tokens of the reference model": "Self-Sup (Sorscher et al., 2022) clusters the datausing the averaged output embeddings from thelast layer tokens of the reference model. It scoresdata based on their distance to cluster centroids,selecting those the most likely to be prototypical. Self-Filter (Chen et al., 2024a) is a recent VITcoreset selection method that was originally ap-plied to the LLaVA-158k VIT dataset (Liu et al.,2023b), which consists of only three VL tasks.It finetunes the score-net along with the targetLVLM on the full dataset to serve as a referencemodel for scoring and filtering VIT data. We usethe version that additionally incorporates bothCLIP scores and CLIP features since it ensuresenhanced performance and efficiency. VQAv2GQAOKVQAA-OKVQA RefCOCOOCR-VQAVGLLaVA-",
  "BVisualizing LVLM Skills withRelevancy Maps": "In our method, we extract neuron activations fromvarious layers (Eq. 2) to represent the conceptsand skills of each VIT data. In this approach, wehypothesize that distinct layers represent distinctconcepts and skills of the LVLM. To support thisassumption, we compute relevancy maps (Cheferet al., 2021) following the approach outlined in Stanet al. (2024). The relevancy maps help us under-stand the models final output by highlighting themost contributing parts of the input for each layer.Given the target output token yt and the attentionmap Al Rh(Nv+Nl)(Nv+Nl) of the l-th layer,where h is the head dimension of the attention, therelevancy map R is computed as follows:",
  "R = R + Al R,for l {1, 2, . . . , L},(8)": "where denotes the Hadamard product and L isthe total number of layers in the LVLM. In orderto investigate the contribution of each layer to thefinal output, we visualize the image regions relatedto the output token through the visual relevancymap computed from each layer. Specifically, weconsider the row of Al R corresponding to theoutput token. Then, we extract the visual tokenparts of the row to yield the visual relevancy map.For the investigation, we inspect the 4th, 8th,12th, 16th, and 20th layers of the TinyLLaVA-2B (Zhou et al., 2024) model and identify the layerthat activates the most relevant visual regions. Thefindings in reveal that (1) the most rel-evant layer varies according to the concept-skillcomposition and (2) the most relevant layer is thesame across diverse VIT data when the data sharesa similar concept-skill composition. This supportsour assumption that different layers contribute todistinct concepts and skills, allowing neuron activa-tions from various layers to effectively group VITdata by their concept-skill composition.",
  "CConcept-Skill Clustering Visualization": "We visualize the clustering results of the gatheredVIT data. The results are illustrated in .We observe that most clusters contain VIT data thatencode similar concept-skill compositions. For in-stance, the first group in consists of sam-ples requiring OCR and counting abilities to solvevisual queries involving images with store signs.The second group features images of people wait-ing for public transportation and multiple-choicequestions that require visual recognition and rea-soning abilities. The third group shows a cluster ofsamples with images of people in suits and queriesfocusing on object localization and generating cap-tions for given bounding boxes. Lastly, the bottomgroup includes images exhibiting children with an-imals and requiring the ability to reason about theeducational benefits that the children might gainfrom interacting with the animals.",
  "D.1Task-wise Transferability": "To further understand transferability, we calculatethe transferability of LLaVA-1.5 tasks by averagingthe cluster transferability of VIT data. We show theresults in . We observe that VQA tasks, in-cluding VQAv2, GQA, OKVQA, and A-OKVQA,contain VIT data that transfers well to other data.In contrast, GPT-generated conversational tasks,including LLaVA-Conv, LLaVA-Detail, LLaVA-Rason, and ShareGPT, exhibit low transferability.This corresponds to the findings of Tiong et al.(2024) that VQA tasks are effective for finetun-ing LVLMs. This alignment supports the efficacyof our approach in discovering the fine-grainedconcept-skill compositions and their transferabil-ity. We hypothesize that the high transferabilityof the VQA tasks is because these tasks mostly re-quire abilities close to the fine-grained VL conceptsand skills that can be shared with other tasks, asdescribed in , unlike more complex tasks. : Transferring to the larger target model. We validate if the coresets selected from TinyLLaVA-2B aretransferable to LLaVA-1.5-13B finetining. We train the LLaVA-1.5-13B using coresets with 20% sampling ratioand estimate performance on various multimodal benchmarks. The best and the second best results are highlightedin bold and underline, respectively.",
  "D.2Concept-Skill with High Transferability": "In , we visualize concept-skill composi-tions having the highest transferability for variousVL task types. We define the VL task type of a clus-ter based on the task name associated with mostof the clusters data (e.g., VQAv2, GQA). Inter-estingly, GQA and LLaVA-Conv share a similarconcept-skill composition as their most transfer-able concept-skill composition. This suggests thatthe transferability of VL concept-skill compositionmight be consistent across different VL tasks.",
  "D.3Concept-Skill as Latent Factor of LVLM": "We conduct an ablation study to verify if data clus-ters from different VL task types have high trans-ferability with each other when they share a similarconcept-skill composition. In this study, we se-lect two clusters from different VL task types witha similar concept-skill composition (second andfourth groups in ), using the first clus-ter as the source and the second cluster as the tar-get. Additionally, we employ 49 randomly selectedsource clusters and measure transferability fromthe source clusters to the target cluster (Eq. 4). Thesource cluster, sharing a similar concept-skill com-position with the target, ranks in the top 5 of the 50source clusters in terms of test loss gain, exhibit-ing high transferability to the target cluster. Thissuggests that concept-skill compositions resemblefine-grained latent factors that constitute LVLMabilities. Thus, these fine-grained VL concepts andskills must be considered to effectively reduce dataredundancy and build a well-generalized LVLM. : Impact of a reference model training dataset.We use TinyLLaVA-2B finetuned on the LLaVA-1.5dataset as a reference model to collect coresets fromthe Vision-Flan dataset with 16.7% sampling ratio. Thebest and the second best results are highlighted in boldand underline, respectively.",
  "EConcept-Skill Diversity within Coresets": "Our method selects data from various clusters toensure a high diversity of VL concept-skill com-positions within the coreset. To demonstrate theefficacy of our method, we compare the diversitywithin the coreset by our method with those bythe baseline methods. Specifically, we use the 191tasks from the Vision-Flan dataset as proxies fordifferent concept-skill compositions, as there areno ground-truth compositions. We then count thenumber of selected samples for each task. The re-sults, summarized in , indicate that base-line methods select most data from only a few tasks,leading to biased selection and undermining LVLMgeneralization. This bias explains why most base-lines perform worse than random sampling in ourexperiments. In contrast, our method achieves amore balanced selection across the various tasks.",
  "F.2Robustness of Reference Model": "We investigate the robustness of our method whenthe reference model is finetuned on a VIT datasetdifferent from a target VIT dataset. To this end, weuse the TinyLLaVA-2B finetuned on the LLaVA-1.5 VIT dataset, to perform coreset selection fromthe Vision-Flan dataset. The results are summa-rized in . COINCIDE continues to showperformance comparable to full-finetuning whileoutperforming other baseline methods.",
  "F.3Hyperparameters": "We conduct ablation studies on hyperparameters ofour method, which include the number of clusters(K) and the temperature (). The results, summa-rized in , reveal that a sufficiently largenumber of clusters is essential to ensure clusterpurity and diversity of VL concept-skill composi-tions, ensuring effective representation of the com-positions and enhancing the generalization abilityof LVLM. Furthermore, we find that setting thetemperature too low leads to a biased coreset selec-tion, as most samples are then selected from a fewclusters. This undermines the diversity within thecoreset, leading to a decline in overall performance.",
  "We further analyze the impact of different multi-modal neuron activations on the performance ofour method. COINCIDE selects neuron activa-tions from the MSA blocks across the 4th, 8th,": "12th, 16th, and 20th layers of the reference model.We experiment with different neuron activationsand present the results in . Transformingthe neuron activations from the MSA blocks intoboolean vectors by mapping negative values to -1and positive values to 1 causes a significant perfor-mance drop, likely due to substantial informationloss, yielding inaccurate clustering and transferabil-ity calculation. Extracting neuron activations onlyfrom the last layer of the reference model causes aslight performance decrease. As discussed in Sec-tion 3.2, LVLM abilities stem from various layers.Hence, relying on the last layer captures only asmall portion of these capabilities, leading to theperformance decline. Finally, utilizing the neuronactivations from the MSA blocks gives superior per-formance compared to using activations from theFFN blocks. We believe this is because MSA layersuse self-attention to share multimodal information,providing richer multimodal understanding.",
  ": return C1 C2 . . . CK": "Bike near the road & Reasoning Layer 8 Q: Why is the man on the road wearing a whistle? A. crossing guard B. no sidewalk C. street performer D. jaywalking A: A Q: Why is he riding on the sidewalk? A. he's tired B. too slow C. more fun D. he's walking A: B Q: Why are the men in uniforms standing by the road?A. doctors B. security C. street workers D. entertainment A: B Q: Why are all the vehicles on the left not moving? A. tired B. red light C. parade D. accident A: D Tower clock & OCR Layer 12 Q: What time is it? A: 7:40 Q: What time is it on the clock? A: 11:10 Q: What time is it? A: 2:50 Q: What time is it here? A: 12:15 Objects in bathroom & Position attribute Layer 12 Q: Is the towel on the left side? A: No Q: Is the hose on the right side of the photo? A: Yes Q: Which side is the white napkin on? A: Left Q: On which side is the white toilet? A: Right Street sign & Common-sense Knowledge Layer 16, 20 Q: What does the yellow street sign mean? A: Pedestrian cross Q: What does the street sign mean to drivers? A: Do not enter Q: What are these green signs typically used for? A: Street name Q: What was that sign meant for? A: Direct : Relevancy maps visualization. We investigate which layer contributes most to the final output of theLVLM. This is done by visualizing relevancy maps of four samples from the same cluster. For each example, theleft image is the original, while the right image shows the visualized relevancy map, highlighting regions mostrelevant to the LVLM output text colored in yellow. The top-left corner of each group explains the VL concept-skillcomposition and the layer number with the highest relevancy to the output. Q1: What is this place called?A1: Maxwell street depotQ2: What number is next to OPEN?A2: 24Q3: How many people are in the photo?A3: 1Q4: How late is the sandwich shop open?A4: 24 hours Q1: Is it sunny?A1: YesQ2: How many people do you see?A2: 15Q3: What is the restaurant in the background of this photo?A3: Bar veloce.Q4: Is there any signal in the picture?A4: Yes Q1: How many bikes?A1: 1Q2: What color is the road paint?A2: WhiteQ3: What does the bus say?A3: Be purposefulQ4: What is the name of tattoo parlor?A4: Flash taco Q1: What do the written signs say?A1: El rapidoQ2: Where is the cafeteria sign?A2: HangingQ3: How many signs are shown?A3: 6Q4: Are there more than 3 signs?A4: Yes Store sign & OCR + Counting Q: What is the man by the bags awaiting?A. skateboarderB. trainC. deliveryD. cabAnswer with the options letter from the given choices directly. A: D Q: What is the women in yellow waiting for?A. rainB. rideC. checkD. her petAnswer with the options letter from the given choices directly. A: B Q: What are the people waiting to do?A. ride trainB. danceC. eatD. workAnswer with the options letter from the given choices directly. A: A Q: What do the people do when the fast moving thing stops?A. exerciseB. cookC. swimD. board itAnswer with the options letter from the given choices directly. A: D Waiting for public transportation & Scene understanding Q1: Please provide a short description for this region: [0.84, 0.25, 1.0, 0.83].A1: Female brown hair black dress facing away.Q2: Please provide the bounding box coordinate of the region this sentence describes: far right person half.A2: [0.84, 0.25, 1.0, 0.83] Q1: Please provide the bounding box coordinate of the region this sentence describes: blond hair.A1: [0.02, 0.22, 0.3, 0.8]Q2: Please provide the bounding box coordinate of the region this sentence describes: man with blue tie.A2: [0.28, 0.32, 0.6, 0.8] Q1: Please provide a short description for this region: [0.0, 0.17, 0.55, 0.82].A1: Light blue shirt.Q2: Please provide the bounding box coordinate of the region this sentence describes: a boy in a blue and white top and tie is looking at the boy in blue talking.A2: [0.69, 0.18, 0.98, 0.77] Q1: Please provide the bounding box coordinate of the region this sentence describes: lleft guy.A1: [0.1, 0.25, 0.58, 0.82]Q2: Please provide the bounding box coordinate of the region this sentence describes: left man.A2: [0.1, 0.25, 0.58, 0.82] People dressed in the suit & Object localization + Captioning Q: What is a possible educational benefit for the children in this situation?A: An educational benefit for the children interacting with the sheep is that they are learning and understanding more about farm animals, their behaviors, and the way they are cared for. This hands-on experience allows the children to foster a Q: What educational purpose might this gathering serve?A: This gathering, where people are observing the process of milking a cow while some are being shown how to do it, likely serves the purpose of educating everyone present about the traditional method of obtaining milk. It can be particularly informative for the toddler and Q: What potential benefits can this interaction provide for the child?A: The interaction of the young child petting the cow with the assistance of a father provides several potential benefits. It can help the child develop empathy, compassion, and a connection with animals by interacting with them in a Q: What kind of experience are the two people having, and what can this teach them?A: The two people, a woman and a young girl, are having an intimate and interactive experience with the cows in a pen, where they are petting and loving on the animals. This experience can teach them about the importance of connecting Child with animals & Reasoning",
  ": Examples of data clusters. We visualize four samples from the same cluster. The top-left corner of eachgroup explains the VL concept-skill composition": "Q1: What activity must Lynne enjoy doing?A1: ReadingQ2: What are the objects on?A2: TableQ3: Who is the author of the book?A3: Bill brysonQ4: What is the name of the book?A4: Walk in woods Q1: What is the title of the top book?A1: A place to standQ2: Are these library books?A2: YesQ3: Are these books for a college student?A3: Yes Q1: What color is tintin's dog?A1: WhiteQ2: How many books are in the volume?A2: 8Q3: Could this be a produce market?A3: NoQ4: What is the name of the books?A4: Tintin Q1: Is there a photo of a man?A1: NoQ2: Is there a clock in the picture?A2: YesQ3: Which book was written by John Irving?A3: Hotel new hampshire VQAv2 (Books & OCR), Top-1 Q1: What kind of furniture is it?A1: DeskQ2: Which kind of furniture is that?A2: DeskQ3: Are there either any black numbers or words?A3: No Q1: What piece of furniture are the glasses on?A1: DeskQ2: What is the piece of furniture that the glasses are on called?A2: DeskQ3: What are the glasses on?A3: Desk Q1: Which side of the picture is the TV on?A1: RightQ1: Which kind of furniture is this, a sofa or a bed?A2: BedQ3: Is the laptop to the right or to the left of the device on the right?A3: Left Q1: Which color is the floor?A1: GrayQ2: Are the books on the right?A2: NoQ3: What device is to the left of the lady?A3: LaptopQ4: Do you see lamps there?A4: No GQA (Electronic devices & Recognition), Top-1 Q: What colour is the tie on the far right?A. redB. yellowC. orangeD. pinkAnswer with the option's letter from the given choices directly. A: C Q: The man in the foreground's jacket is the same color as what?A. bananaB. watermelonC. orangeD. cherryAnswer with the option's letter from the given choices directly. A: A Q: What color is the scarf wrapped around the suitcase pulled on the left?A. redB. yellowC. greenD. blueAnswer with the option's letter from the given choices directly. A: A Q: What color is the neon sign on the second story of this building?A. pinkB. redC. violetD. blueAnswer with the option's letter from the given choices directly. A: B A-OKVQA (Objects near human & Color Attribute), Top-1 Q1: What is the woman doing in the image?A1: The woman in the image is sitting at a table and using a laptop computer.Q2: Where is the laptop computer placed?A2: The laptop computer is placed on top of a concrete table.Q3: Describe the setting where the woman is using her laptop.Q1: How many remote controls are in the image?A`: There are two remote controls in the image.Q2: How many balls of yarn can be seen in the image?A2: There is only one ball of yarn in the image.Q3: What color is the ball of yarn? Q1: What are the two main objects on the wooden desk?A1: The two main objects on the wooden desk are a laptop and a cup of yogurt.Q2: Is the yogurt container open or closed?A2: The yogurt container is open.Q3: What else is there in the yogurt container? Q1: What type of game console is in the image?A1: The image features a Nintendo Wii game console.Q2: How many controllers are there in the image?A2: There are two controllers and one nunchuck in the image.Q3: What type of television is the game LLaVA-Conv (Electronic devices & Conversation + Recognition), Top-1 : High transferability cluster sample visualization. We visualize the samples from the most transferableconcept-skill composition for each VL task. The top-left corner of each group explains the VL task type and the VLconcept-skill compositions. The VL task type for the group follows the task name where most of the data from thegroup are associated. VL Task Idx # of Samples Task-wise # of Samples (Random) VL Task Idx # of Samples Task-wise # of Samples (Clip-Score) VL Task Idx # of Samples Task-wise # of Samples (EL2N) VL Task Idx # of Samples Task-wise # of Samples (Perplexity) VL Task Idx # of Samples Task-wise # of Samples (SemDeDup) VL Task Idx # of Samples Task-wise # of Samples (D2-Pruning) VL Task Idx # of Samples Task-wise # of Samples (Self-Sup) VL Task Idx # of Samples Task-wise # of Samples (Self-Filter) VL Task Idx # of Samples Task-wise # of Samples (Ours) : The number of selected samples per VL task in the Vision-Flan VIT dataset. The horizontal axis denotesthe VL task index in the dataset, and the vertical axis denotes the number of samples. Baseline methods result inbiased coresets. In contrast, our method achieves a more balanced sample selection across diverse tasks, leading tobetter LVLM generalization."
}