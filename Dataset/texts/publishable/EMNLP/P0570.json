{
  "Abstract": "Program induction (PI) has become a promis-ing paradigm for using knowledge bases (KBs)to help large language models (LLMs) an-swer complex knowledge-intensive questions.Nonetheless, PI typically relies on a large num-ber of parallel question-program pairs to makethe LLM aware of the schema of a given KB,and is thus challenging for many low-resourcedKBs that lack annotated data. To this end, wepropose KB-Plugin, a plug-and-play frame-work that enables LLMs to induce programsover any low-resourced KB. Firstly, KB-Pluginadopts self-supervised learning to encode thedetailed schema information of a given KBinto a pluggable module, namely schema plu-gin. Secondly, KB-Plugin utilizes abundantannotated data from a rich-resourced KB totrain another pluggable module, namely PI plu-gin, which can help the LLM extract question-relevant schema information from the schemaplugin of any KB and utilize the informationto induce programs over this KB. Experimentsshow that KB-Plugin outperforms SoTA low-resourced PI methods with 25 smaller back-bone LLM on both large-scale and domain-specific KBs, and even approaches the perfor-mance of supervised methods.",
  "Introduction": "Recently, the usage of knowledge bases (KBs) asexternal resources to assist large language models(LLMs) (Brown et al., 2020; Zhao et al., 2023) inanswering complex knowledge-intensive questionshas gained increasing study (Pan et al., 2023; Liet al., 2023b; Jiang et al., 2023). Among variousmethods, program induction (PI) has emerged as apromising paradigm due to its good interpretabilityand capacity to support complex reasoning opera-tions (Cao et al., 2022a; Gu et al., 2023; Li et al.,2023b). Given a KB, PI methods employ LLMs to",
  ": Illustration of KB-Plugin. By simply pluggingthe schema plugin of a KB and the PI plugin, the LLMis injected with the schema information of this KB andthe ability to induce programs over it": "convert a question into a multi-step program (e.g.,KoPL (Cao et al., 2022a) and S-expression (Suet al., 2016)), whose execution against the KB pro-duces the answer. Despite strong capacity, most PImethods rely on individual training for each KB us-ing a large number of manually annotated question-program pairs (Xie et al., 2022; Li et al., 2023b;Luo et al., 2023). As for many low-resourced KBsthat lack program annotations, how to enable LLMsto utilize their knowledge via PI remains a challeng-ing problem.Recent studies (Cao et al., 2022b; Li et al., 2023a) have indicated that the mapping from ques-tions to program sketches (i.e., composed func-tions without arguments, such as Find RelateFilterConcept) primarily correlates with lan-guage compositional structures and is thus transfer-able across KBs. Hence the main challenge for PIover low-resourced KBs is to determine the argu-ment for each function (Gu and Su, 2022), which re-quires LLMs to link natural language in a questionto corresponding schema items (i.e., pre-definedrelations and concepts) in the KB (e.g., in Fig 1,the relation part of network and the conceptrail network are arguments of function Relate and FilterConcept, respectively), so it is impor-tant to provide LLMs adequate information of eachschema item. A straightforward approach is to di-rectly feed all the schema information to the LLMvia a prompt. However, the broad schema of KBsand limited context windows of LLMs make thisinfeasible (Li et al., 2023a).Regarding the above challenges, we are inspiredby recent studies that claim the parameters ofLLMs can encode task-specific knowledge (Sax-ena et al., 2022; Moiseev et al., 2022; Wang et al.,2022). Our basic idea is to encode detailed schemainformation of a KB into the parameters of apluggable module (e.g., LoRA (Hu et al., 2022)),namely schema plugin, so as not to be hamperedby limited context windows like the prompt-basedapproach. Then we use another pluggable mod-ule, namely PI plugin, to help the LLM capturequestion-relevant schema information from theschema plugin and utilize this information to in-duce programs. As illustrated in , by sim-ply plugging the schema plugin of a KB and thePI plugin, the LLM is injected with the schemainformation of this KB and the ability to induceprograms over it. We name this framework KB-Plugin. To implement KB-Plugin, there remaintwo key problems: (1) By what task can sufficientinformation about each schema item in a KB beencoded into its schema plugin? (2) Without an-notated data from the low-resource KBs, how canthe PI plugin learn to extract and utilize question-relevant schema information from their schemaplugins to induce programs over these KBs?To solve the above problems, we propose a novelplugin learning and transfer framework. First, in-spired by prior studies (Bordes et al., 2013; Linet al., 2015) which show that schema items in aKB can be well represented by fact triples involv-ing them, we propose to learn schema plugins viaa self-supervised triple completion task. Specifi-cally, given a KB, we plug a schema plugin intothe LLM and tune the plugin to enable the LLMto complete relevant triples for each schema itemin the KB. In this way, the detailed schema infor-mation can be encoded into this schema plugin.As for PI plugin learning, inspired by Cao et al.(2022b), we utilize abundant program annotationsfrom a rich-resourced KB. Specifically, we usethis KB to generate multiple KBs with differentschemas via alias replacement and train a schemaplugin for each of them. Given a training ques-tion, we plug these schema plugins alone with the PI plugin into the LLM in turn and train the PIplugin to make the LLM generate the correct pro-gram whose arguments conform to the currentlyplugged schema plugin. In this way, the PI pluginis forced to learn the skills of extracting and utiliz-ing question-relevant schema information from theplugged schema plugin for PI over the correspond-ing KB. Besides, since the PI plugin is trained to becompatible with different schema plugins, it can bedirectly transferred to other low-resourced KBs andgeneralize well with their schema plugins, even ifmost schema items in these KBs are unseen duringits training.In experiments, we take Wikidata-based KQAPro as the rich-resourced KB to train the PI plu-gin, and evaluate our framework on three large-scale Freebase-based datasets (WebQSP, GraphQ,and GrailQA) and two domain-specific datasets(MetaQA for movie domain and SoAyBench foracademic domain).The results show that KB-Plugin outperforms SoTA low-resourced PI meth-ods with 25 smaller backbone LLM, demonstrat-ing its scalability to extremely complex schemasand adaptability to various domains. On GraphQ,GrailQA, and MetaQA, KB-Plugin even surpassesthe performance of several supervised methods.Our contributions include: (1) proposing KB-Plugin, a novel plug-and-play framework that en-ables LLMs to induce programs over any low-resourced KB; (2) empirically validating the effi-cacy of KB-Plugin for both large-scale and domain-specific KBs through extensive experiments.",
  "Related Work": "Low-resourced Program Induction.Recently,there have emerged three types of PI methods forlow-resourced KBs that lack program annotations,but each of them has limitations: (1) Few-shot pro-gram generation methods (Gu et al., 2023; Li et al.,2023a) utilize in-context learning ability of LLMsto induce programs with a handful of demonstra-tions. However, they can only determine functionarguments based on the schema item names due tolimited context windows, so they face challengesin distinguishing similar schema items. They alsosuffer from long inference time due to excessiveLLM calls or executing a vast number of poten-tial programs; (2) Few-shot data generation meth-ods (Li et al., 2023c) also employ in-context learn-ing with LLMs to convert automatically sampledprograms into questions, and train a smaller PI model using the generated question-program pairs.Nonetheless, the generated questions may not alignwith programs and often lack diversity due to thelimited number of program templates; (3) Simi-lar to us, program transfer methods (Cao et al.,2022b) also leverage program annotations from arich-resourced KB to aid PI for low-resourced KBs.However, they mainly focus on program sketchtransfer and perform poorly without fine-tuningusing annotated question-answer pairs from low-resourced KBs to adapt to their schemas. WhileKB-plugin obviates the reliance on any annotateddata from low-resourced KBs, thereby enablingLLMs to easily utilize their knowledge.Plug-and-Play Modules for LLMs.In recentyears, various parameter-efficient modules havebeen proposed to adapt LLMs to different down-stream tasks (Lester et al., 2021; Hu et al., 2022;Li and Liang, 2021; Pfeiffer et al., 2021) . Thesemodules show plug-and-play characteristics andcan inject task-specific knowledge and skills intoLLMs (Xiao et al., 2023; Zhang et al., 2023). Someresearchers also found that pluggable modules forsimilar tasks encode knowledge and skills into theparametric space in similar ways (Qin et al., 2021;Su et al., 2022), providing basic rationality for thetransferability of our PI plugin.",
  "Problem Formulation": "In this section, we first provide some necessarydefinitions and then formulate our task.Knowledge Base. A knowledge base (KB) canbe formalized as KB = {C, E, R, T }, where C, E,R and T represent the sets of concepts, entities,relations and fact triples, respectively. Specifically,R = {re, rc} Rl, where re is instance of, rcis subclass of, and Rl is the set of other gen-eral relations. Correspondingly, T can be dividedinto there disjoint subsets: (1) instance of triplesTe = {(e, re, c)|e E, c C}; (2) subclass oftriples Tc = {(ci, rc, cj)|ci, cj C}; (3) relationaltriples Tl = {(ei, r, ej)|ei, ej E, r Rl}. Ele-ments in C and R are also called the schema itemsof KB.Program Induction. Given a KB KB and a nat-ural language question x =w1, w2, , w|x|,program induction (PI) aims to convert x into aprogram y, which would return the correct an-swer when executed against KB.Formally, yis composed of functions that take a specifictype of arguments, and can be serialized as y = f1(arg1), , ft(argt), , f|y|(arg|y|), ft F, argt E C R {}. Here, F is a set ofpre-defined functions that cover basic reasoning op-erations on KBs. In this work, we use KoPL (Caoet al., 2022a) as our programming language.Task Formulation.Suppose we have accessto (1) source KB KBS and source domain dataDS = {(xSi , ySi )}nSi=1, which are question-programpairs for KBS; (2) target KB KBT , which is low-resourced and has no annotated data. The goal is tolearn a PI model MTPI that can translate a questionxT for KBT into program yT , whose execution onKBT produces the correct answer.",
  "Methodology": "As mentioned in the introduction, to enable a LLMM to induce programs over low-resourced KBT ,KB-Plugin learns two types of pluggable modulesfor M: (1) KB-specific schema plugin msc, whichstores information of schema items of a givenKB within its parameters; (2) KB-transferable PIplugin mPI, which encodes the skill of inducingprograms over any KB by extracting and utiliz-ing question-relevant schema information from theschema plugin of this KB. It is trained with KBS",
  "Plugin Architecture": "A host of studies have demonstrated that knowl-edge and skills can be encapsulated within the pa-rameters of LLMs (Saxena et al., 2022; Moiseevet al., 2022; Wang et al., 2022). Inspired by this,we implement both schema plugin and PI pluginwith LoRA (Hu et al., 2022), a popular type ofpluggable module for LLMs with a few trainableparameters.Specifically, let LM be the set of weight matricesin the self-attention modules and MLP modules ofa LLM M. For each Wi Rdk in LM, LoRAmodifies its forward pass from h = Wix to h =(Wi + AiBi)x, where Ai Rdr and Bi Rrk",
  "Alias Replacement": ": Overview of our plugin learning and transfer framework: (a) Generate multiple source KBs with differentschemas and augmented source domain data via alias replacement; (b) Learn an individual schema plugin for eachsource KB and the target KB via self-supervised schema-relevant triple completion task; (c) Train the PI pluginby inducing program for each source KB when plugging it into the LLM along with the corresponding schemaplugin. (d) Transfer the PI plugin by plugging it into the LLM with the schema plugin of the target KB and inducingprograms over the target KB with constrained decoding.",
  "mj = {(Amji, Bmji)|Wi LM},(2)": "andplug(M, {m1, . . . , mN})meansre-placingallWiLMwithWi+Nj=1 AmjiBmji.IfwetrainM=plug(fz(M), {fz(m1), . . . , fz(mN1), mN})on a certain task, where fz() represents parameterfreezing, knowledge and skills related to thistask will be encoded within mN. Although otherparameter-efficient pluggable modules such asprefix-tuning (Li and Liang, 2021) can also serveas our plugin modules, the advantages of LoRA arethat it does not increase input length or inferencelatency.",
  "Plugin Learning and Transfer Framework": "There are two primary challenges for learningschema plugins and the PI plugin: (1) How to en-code sufficient information about each schema itemof a KB into a schema plugin? (2) How to ensurethat the PI plugin can extract and utilize usefulschema information for program induction fromschema plugins of different KBs, instead of ignor-ing the schema plugin entirely, directly learning toinduce program over source KB during training,and consequently losing transferability?To handle these challenges, we propose anovel plugin learning and transfer framework, which is illustrated in and containsfour steps: (1) Generate multiple source KBsKBS1, . . . , KBSN with different schemas and aug-mented data DSa={(xSj , yS1j , . . . , ySNj)}nSj=1based on KBS and DS via alias replacement,where ySijis the golden program for questionxSj on KBSi; (2) Learn individual schema pluginmSisc for each KBSi via self-supervised schema-relevant triple-completion task; (3) Train PI plu-gin mPI by requiring MS1PI, . . . , MSNPI to gener-ate yS1j , . . . , ySNjgiven xSj , respectively, whereMSiPI = Plug(fz(M), {fz(mSisc), mPI}), so thatmPI is forced to extract and utilize schema infor-mation from each mSisc; (4) Learn schema pluginmTsc for KBT using the same method in (2) andtake MTPI = plug(M, {mTsc, mPI}) as the final PImodel for KBT . We will introduce each step indetail in the following.",
  "KB Generation and Data Augmentation": "We utilize the aliases of each schema item to gener-ate multiple KBs with different schemas based onKBS = {CS, ES, RS, T S}. As shown in (a),for each schema item v CS RS, we replace vwith vi, a randomly chosen alias of v, and recordai(v) = vi. For example, the concept basketballteam can be replaced with basket club and therelation member of sports team can be replacedwith plays for. Relevant triples in T S are also",
  "modified with the same alias. In this way, KBSi": "that has a different schema than KBS is created. Inpractice, we let KBS1 = KBS and repeat aboveprocess N 1 times to generate KBS2, . . . , KBSN .Similarly,foreachquestion-programpair(xSj , ySj )DS,supposeySj=f1(arg1), , ft(argt), , f|ySj |(arg|ySj |), we replace every argt CS RS with ai(argt)to obtain ySij , which is the correct program forxSj executable on KBSi. We repeat the processfor KBS1, . . . , KBSN to obtain augmented dataDSa = {(xSj , yS1j , . . . , ySNj)}nSj=1.",
  "Learning of Schema Plugin": "Many studies about knowledge graph embeddingshow that the information of schema items in a KBcan be represented by not only their names but alsotriples containing them (Bordes et al., 2013; Lvet al., 2018). Inspired by this, we propose to en-code schema information into schema plugins viaa self-supervised triple completion task. As illus-trated in (b), to learn the schema plugin mscfor a given KB KB = {C, E, R, T }, where T =Te Tc Tl, we train Msc = Plug(fz(M), msc)to complete relevant triples for each concept andrelation in KB in sequence-to-sequence form asfollows.First, for each concept c C, we requireMsc to complete relevant instance of triplesto aggregate the semantic features of entities be-longing to c. Specifically, we sample K triples(ek, instance of, c) from Te (see Appendix B for de-tailed sampling strategy), and use each sampledtriple to construct two pairs of verbalized queriesand answer as the inputs and expected outputs forMsc: ek || instance of c; c || contains instance ek.Here, ek and c means filling in the names of ekand c, respectively.Besides, the information of a concept is alsorelated to its sub- and super-concepts. Therefore,for each triple (ci, subclass of, cj) Tc, we alsoconstruct two queries with answers for Msc: ci || subclass of cj; cj || contains subclass ci.Finally, the information of a relation can belearned from its name and the elements connectedby it. Therefore, for each r Rl, we sample Ktriples (ei, r, ej) from Tl, choose ci, cj such that",
  "(ei, instance_of, ci), (ej, instance_of, cj) Te,and use each (ei, ci, r, ej, cj) to construct threequeries with answers:": "ei | ci || r | forward cj | ej; ej | cj || r | backward ci | ei; ei | ci || what relation || cj | ej r.We empirically find that including ci, cj benefitsthe information encoding for both concepts andrelations.Let the set of all generated queries and answersbe Dsc = {(qi, ai)}li=1, then msc is trained to min-imize",
  "(qi,ai)Dsclog P(ai|qi),(3)": "where P(ai|qi) is the likelihood of Msc generatingai given qi, defined by token-level cross entropy.Note that the learning of msc does not rely on anyadditional data except the KB itself, so we can traina schema plugin for any KB. 4.2.3Learning of PI PluginAs illustrated in (c), to learn the PI plu-gin mPI, we first train individual schema plu-gin mSisc for each KBSi.After that, given(xSj , yS1j , . . . , ySNj) DSa , where xSi is a ques-tion and ySijis the golden program for xSjon KBSi, we train mPI by feeding xSitoMS1PI, . . . , MSNPIand requiring them to gener-ate yS1j , . . . , ySNj, respectively.Here, MSiPI =Plug(fz(M), {fz(mSisc), mPI}). The overall objec-tive can be formulated as:",
  "i=1log Pi(ySij |xSj ),": "(4)where Pi(ySij |xSj ) is the likelihood of MSiPI gener-ating ySijgiven xSj , defined by token-level crossentropy. To generate programs conforming to dif-ferent schemas given the same question, mPI mustlearn to (1) choose correct functions according tothe compositional structure of the question; (2)extract and utilize question-relevant schema infor-mation for argument determination from the cor-responding schema plugin, because it is the onlydifference among MS1PI, . . . , MSNPI .",
  "plug(M, {mTsc, mPI}) be the PI model for KBT .Here, mTsc is the trained schema plugin for KBT": "using the method in Sec. 4.2.2. Since mTsc and mSiscare trained with the same tasks, we expect that theyencode schema information into their parameters insimilar ways (Qin et al., 2021; Su et al., 2022), somPI can also extract schema information from mTscto help PI over KBT . Besides, to guarantee MTPIgenerating valid programs which do not cause exe-cution error or return an empty answer, we adoptconstrained decoding, i.e., after MTPI generatesf1(arg1), . . . , ft(argt), we enumerate all the validft+1(argt+1) following the method of Gu et al.(2023) and restrict MTPI to only generate one ofthem. More details are in Appendix C. We alsouse beam search to retain top-k programs duringdecoding to provide MTPI with a more global view.",
  "Target Domain.We use WebQSP (Yih et al.,": "2016), GraphQ (Su et al., 2016), GrailQA (Guet al., 2021), MetaQA (Zhang et al., 2018) andSoAyBench (Wang et al., 2024) as the target do-main datasets. Among them, WebQSP, GraphQ,and GrailQA are based on Freebase (Bollackeret al., 2008). Their KBs contain a large numberof schema items and cover various domains, thuscan evaluate the effectiveness of KB-Plugin forlarge-scale KBs. MetaQA and SoAyBench are twodatasets in movie and academic domains, respec-tively, and can evaluate the adaptability to specificdomains in detail. For MetaQA, since most of therelations in its KB have been covered by KQA Pro,we remove these relations and relevant question-program pairs from KQA Pro to avoid data leakage.For SoAyBench which is originally a tool-usingdataset based on Aminer (Tang et al., 2008) APIs,we construct its KB by collecting relevant datafrom these APIs. shows the statistics ofthese datasets and their overlap with source KBsgenerated from KQA Pro. Most schema items inthe target KBs are unseen in source KBs and mosttest cases also involve unseen schema items.",
  "Dataset|DM||R||Ru||C||Cu||Dtest| |Dtestu |": "KQA Pro-1209-794---WebQSP5641229644636316391083GraphQ709569 8931 7298 700423952340GrailQA(dev)863938 3524 2018 186867636578GrailQA(test)863938 3524 2018 1868 13231-MetaQA1999339093 39093SoAyBench1171153792756 : Statistics for source and target domain datasetsand their overlaps with 16 source KBs generated fromKQA Pro. |DM| / |R| / |C| denotes the number ofdomains / relations / concepts in their KBs. |Ru| / |Cu|denotes the number of relations / concepts unseen inthe source KBs. |Dtest| and |Dtestu | denotes the numbersof test cases and test cases that involve unseen schemaitems, respectively.",
  "Baselines": "For WebQSP, GraphQ, GrailQA, and MetaQA, wemainly compare KB-Plugin with low-resourced PImethods including (1) few-shot program genera-tion methods Pangu (Gu et al., 2023) and KB-BINDER (Li et al., 2023a); (2) few-shot data gen-eration method APS (Li et al., 2023c); (3) programtransfer method ProgramTrans (Cao et al., 2022b),where we adopt its results without fine-tuning ontarget KBs for fair comparison. In addition, wealso provide the results of several representativesupervised models for comparison.For SoAyBench, we choose tool-using methodsthat were evaluated on it as baselines, includingDFSDT (Qin et al., 2023) and SoAy (Wang et al.,2024). These methods solve questions by prompt-ing LLMs to call Aminer APIs in specific orders viain-context learning. Their processes of determiningthe composition of APIs and filling in argumentsfor each API can also be viewed as program induc-tion.We provide detailed descriptions of all the base-lines and our evaluation metrics in Appendix D.1.",
  "Implementation Details": "In experiments, we use Llama2-7B (Touvron et al.,2023) as the backbone LLM of KB-Plugin and setthe rank r of LoRA to 16. The number of parame-ters of each plugin is consequently 40M, which isextremely lightweight. The aliases of schema itemsin KQA Pro are directly obtained from Wikidata.The number of generated source KBs is set to 16 tobalance performance and training efficiency. Thesampling number K in schema plugin learning isset to be 500, 500, 50, 100, 3000, and 1000 forKQA Pro, WebQSP, GraphQ, GrailQA, MetaQA,",
  "Main Results": "The results are presented in , 3 and 4. Com-pared with Pangu, the SoTA PI method for low-resourced KBs, KB-Plugin improves the F1 scoreby 2.7% and 6.2% on WebQSP and GraphQ, re-spectively, and achieves comparable performanceon GrailQA, despite Pangu using 25 larger model(175B Codex) and 100 annotated examples fromeach dataset. Moreover, Pangu needs to call Codexhundreds of times for a question to score each can-didate program, while our model selects the op-timal program via beam search, which is signifi-cantly faster and less costly. Besides, since Pro-gramTrans, KB-BINDER, and Pangu all link ques-tions to schema items according to their names only,the superiority of KB-Plugin also demonstrates thebenefits of aggregating additional schema informa-",
  "GrailQA-devKB-Plugin69.064.8w/o schema plugin64.957.3Gain+4.1+7.5": ": F1 Results of KB-Plugin with and withoutschema plugin. Dtestunseen and Dtestseen denote the sets oftest cases that involve and do not involve schema itemsunseen in the source KBs, respectively. means theresults may not be indicative since there are only 55cases in Dtestseen of GraphQ. tion from relevant triples via schema plugin learn-ing. KB-Plugin even surpasses several supervisedmodels on GraphQ and GrailQA, which demandtraining using thousands of annotated samples fromtarget KBs, showing the effectiveness of transfer-ring prior knowledge from rich-resourced KBs.On MetaQA and SoAyBench, KB-Plugin outper-forms all the low-resourced baselines even thoughthey use more powerful LLMs (i.e., Codex, gpt-3.5-turbo, and gpt-4), indicating that our frameworkalso performs well for domain-specific KBs. Inparticular, KB-Plugin achieves strong performanceon par with supervised SoTAs on MetaQA even ifit does not see any target relations from the sourcedomain.",
  "Ablation Study": "To demonstrate the effect of schema plugins, weremove them from our framework, i.e., we di-rectly train a PI plugin using the source domaindata and transfer it to the target KBs without train-ing any schema plugins. According to , 3,4, and 5, the performance of KB-Plugin withoutschema plugins is severely degraded, especially onthe test cases that involve schema items unseen inthe source KBs. The experimental results illustrate",
  ": KB-Plugin performance with different num-bers of generated source KBs": "that (1) direct PI transfer is difficult due to the sub-stantial difference between the schemas of sourceand target KBs; (2) schema plugins of target KBseffectively encode adequate schema informationvia the triple completion task, and the PI plugincan extract and utilize question-relevant schema in-formation from these schema plugins even thoughit is never trained with them. In addition, if weadopt the schema plugin of a source KB, e.g., mS0sc ,for the target KBs, the performance of KB-Pluginalso drops heavily, showing the necessity of usingmatched schema plugin.To show the rationality of our PI plugin learningmethod, we evaluate the performance of PI plu-gins trained with different numbers of generatedsource KBs on WebQSP, GraphQ, and GrailQA,and present the results in . The PI plugintrained with only one source KB performs poorly,implying that it ignores the schema plugin entirelyand directly learns PI over this source KB. Oncethere emerges a new source KB with a differentschema, the performance of the trained PI pluginincreases substantially, and there is an apparenttrend that the performance will increase with moregenerated source KBs. These results prove thattraining the PI plugin over multiple source KBs",
  "Case Study": "To better showcase the advantages of KB-Pluginover in-context learning PI methods, we presenta case comparison between KB-Plugin and Panguin . Question I shows the effect of schemaplugin learning and utilization. Both Pangu andKB-Plugin without schema plugin struggle to pre-dict the correct relation transport terminus be-cause it is unseen in the demo examples or sourceKBs. The complete KB-Plugin, however, effec-tively encodes the information that transport ter-minus is a possible relation between citytownand airport into the schema plugin via complet-ing relevant triples, and succeeds in predicting thisrelation by utilizing above information. QuestionII demonstrates the benefits of harnessing abun-dant program annotations from the source domain,where Pangu produces a program with incorrectfunction composition because none of its demo ex-amples has a similar compositional structure, whileKB-Plugin induces the correct program by utilizingprior knowledge learned from the source domain.Further analysis can be found in Appendix E and F.",
  "Limitations": "We discuss several limitations of KB-Plugin in thissection: (1) In the experiments, we only adoptLlama2-7B as our backbone model due to lim-ited computing resources. Actually, KB-Pluginis model-agnostic and can also be applied to morelanguage models with various sizes and architec-tures. (2) KB-Plugin requires that the source do-main dataset covers questions with diverse vari-ous compositional structures, and performs rela-tively poorly for questions whose compositionalstructures are unseen in the source domain datasetthough they are rare (see Appendix E for details).Future research can focus on improving the trans-ferability of KB-Plugin across compositional struc-tures. In practice, we can also continue to train thePI plugin using some self-training methods such asEGST (Li et al., 2023c) to adapt to these questions.(3) In this work, since both training and evaluationof KB-Plugin require annotated KBQA datasets,we can only take a single dataset KQA Pro as thesource dataset and take other datasets as the tar-get datasets, which may limit the upper bounds ofKB-Plugin. In the realistic scenario where we needto apply KB-Plugin for a new KB, we can take allthese KBQA datasets as the source domain datasetsso that the trained source schema plugins would bemore diverse and the trained PI plugin would alsohave stronger transferability and generalizability.",
  "Ethical Considerations": "Though our framework (as well as other PI meth-ods) can effectively reduce the probability of LLMsgenerating inaccurate answers when faced withquestions involving uncommon knowledge, it maystill make mistakes if the induced programs are in-correct. In addition, there is a risk of being hackedthrough targeted means such as injecting harmfulor nonfactual knowledge into the KBs. Hence ad-ditional care and protective measures should betaken if our framework is deployed in user-facingapplications.All the datasets and encyclopedias used in thiswork are publicly published with permissible li-censes.",
  "Acknowledgements": "This work is supported by Beijing Natural ScienceFoundation (L243006), Tsinghua University Ini-tiative Scientific Research Program, grants fromthe Institute for Guo Qiang, Tsinghua University(2019GQB0003) and Zhipu AI. Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,Tim Sturge, and Jamie Taylor. 2008. Freebase: acollaboratively created graph database for structuringhuman knowledge. In Proceedings of the ACM SIG-MOD International Conference on Management ofData, SIGMOD 2008, Vancouver, BC, Canada, June10-12, 2008, pages 12471250. ACM. Antoine Bordes, Nicolas Usunier, Alberto Garca-Durn, Jason Weston, and Oksana Yakhnenko.2013. Translating embeddings for modeling multi-relational data. In Advances in Neural InformationProcessing Systems 26: 27th Annual Conference onNeural Information Processing Systems 2013. Pro-ceedings of a meeting held December 5-8, 2013, LakeTahoe, Nevada, United States, pages 27872795. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie,Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Han-wang Zhang. 2022a. KQA pro: A dataset with ex-plicit compositional programs for complex questionanswering over knowledge base. In Proceedings ofthe 60th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 61016119. Association for Computational Linguistics. Shulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu,Lei Hou, Juanzi Li, Zhiyuan Liu, and Jinghui Xiao.2022b.Program transfer for answering complexquestions over knowledge bases. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2022, Dublin, Ireland, May 22-27, 2022, pages81288140. Association for Computational Linguis-tics. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,Henrique Pond de Oliveira Pinto, Jared Kaplan,Harrison Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Joshua Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluat-ing large language models trained on code. CoRR,abs/2107.03374. Yu Gu, Xiang Deng, and Yu Su. 2023. Dont gener-ate, discriminate: A proposal for grounding languagemodels to real-world environments. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2023, Toronto, Canada, July 9-14, 2023, pages49284949. Association for Computational Linguis-tics. Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler,Percy Liang, Xifeng Yan, and Yu Su. 2021. BeyondI.I.D.: three levels of generalization for question an-swering on knowledge bases. In WWW 21: The WebConference 2021, Virtual Event / Ljubljana, Slovenia,April 19-23, 2021, pages 34773488. ACM / IW3C2. Yu Gu and Yu Su. 2022. Arcaneqa: Dynamic programinduction and contextualized encoding for knowledgebase question answering. In Proceedings of the 29thInternational Conference on Computational Linguis-tics, COLING 2022, Gyeongju, Republic of Korea,October 12-17, 2022, pages 17181731. InternationalCommittee on Computational Linguistics.",
  "Matthew Honnibal, Ines Montani, Sofie Van Lan-deghem, and Adriane Boyd. 2020. spacy: Industrial-strength natural language processing in python": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, XinZhao, and Ji-Rong Wen. 2023. Structgpt: A generalframework for large language model to reason overstructured data. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2023, Singapore, December 6-10,2023, pages 92379251. Association for Computa-tional Linguistics. Yunshi Lan and Jing Jiang. 2020. Query graph genera-tion for answering multi-hop complex questions fromknowledge bases. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, ACL 2020, Online, July 5-10, 2020, pages969974. Association for Computational Linguistics.",
  "Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su,and Wenhu Chen. 2023a. Few-shot in-context learn-ing for knowledge base question answering. CoRR,abs/2305.01750": "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing, ACL/IJCNLP 2021, (Volume 1: LongPapers), Virtual Event, August 1-6, 2021, pages 45824597. Association for Computational Linguistics. Xingxuan Li, Ruochen Zhao, Yew Ken Chia, BoshengDing, Lidong Bing, Shafiq R. Joty, and SoujanyaPoria. 2023b. Chain of knowledge: A frameworkfor grounding large language models with structuredknowledge bases. CoRR, abs/2305.13269. Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, ZhichaoDuan, Bowen Dong, Ning Liu, and Jianyong Wang.2023c. Flexkbqa: A flexible llm-powered frame-work for few-shot knowledge base question answer-ing. CoRR, abs/2308.12060. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu. 2015. Learning entity and relation em-beddings for knowledge graph completion. In Pro-ceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence, January 25-30, 2015, Austin,Texas, USA, pages 21812187. AAAI Press. Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng,Yikai Guo, Wentai Zhang, Chenghao Ma, GuantingDong, Meina Song, and Wei Lin. 2023. Chatkbqa: Agenerate-then-retrieve framework for knowledge basequestion answering with fine-tuned large languagemodels. CoRR, abs/2310.08975.",
  "Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston": "2016. Key-value memory networks for directly read-ing documents. In Proceedings of the 2016 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2016, Austin, Texas, USA, Novem-ber 1-4, 2016, pages 14001409. The Association forComputational Linguistics. Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-tin Jaggi. 2022. SKILL: structured knowledge infu-sion for large language models. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, NAACL 2022, Seattle,WA, United States, July 10-15, 2022, pages 15811588. Association for Computational Linguistics. Lunyiu Nie, Shulin Cao, Jiaxin Shi, Jiuding Sun,Qi Tian, Lei Hou, Juanzi Li, and Jidong Zhai. 2022.Graphq IR: unifying the semantic parsing of graphquery languages with one intermediate representation.In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2022, Abu Dhabi, United Arab Emirates, December7-11, 2022, pages 58485865. Association for Com-putational Linguistics.",
  "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rckl,Kyunghyun Cho,and Iryna Gurevych. 2021": "Adapterfusion: Non-destructive task composition fortransfer learning. In Proceedings of the 16th Con-ference of the European Chapter of the Associationfor Computational Linguistics: Main Volume, EACL2021, Online, April 19 - 23, 2021, pages 487503.Association for Computational Linguistics. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,and Maosong Sun. 2023. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.CoRR, abs/2307.16789. Yujia Qin, Xiaozhi Wang, YuSheng Su, Yankai Lin,Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, PengLi, Maosong Sun, and Jie Zhou. 2021. Exploringlow-dimensional intrinsic task subspace via prompttuning. CoRR, abs/2110.07867. Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.2022. Sequence-to-sequence knowledge graph com-pletion and question answering. In Proceedings ofthe 60th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 28142828. Association for Computational Linguistics. Apoorv Saxena, Aditay Tripathi, and Partha P. Taluk-dar. 2020. Improving multi-hop question answeringover knowledge graphs using knowledge base embed-dings. In Proceedings of the 58th Annual Meeting of",
  "the Association for Computational Linguistics, ACL2020, Online, July 5-10, 2020, pages 44984507.Association for Computational Linguistics": "Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Han-wang Zhang. 2021. Transfernet: An effective andtransparent framework for multi-hop question an-swering over relation graph. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2021, Virtual Event/ Punta Cana, Dominican Republic, 7-11 November,2021, pages 41494158. Association for Computa-tional Linguistics. Yu Su, Huan Sun, Brian M. Sadler, Mudhakar Srivatsa,Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016.On generating characteristic-rich question sets forQA evaluation. In Proceedings of the 2016 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2016, Austin, Texas, USA, Novem-ber 1-4, 2016, pages 562572. The Association forComputational Linguistics. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,Yankai Lin, Huadong Wang, Kaiyue Wen, ZhiyuanLiu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, andJie Zhou. 2022. On transferability of prompt tuningfor natural language processing. In Proceedings ofthe 2022 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, NAACL 2022, Seattle,WA, United States, July 10-15, 2022, pages 39493969. Association for Computational Linguistics. Haitian Sun, Tania Bedrax-Weiss, and William W. Co-hen. 2019. Pullnet: Open domain question answeringwith iterative retrieval on knowledge bases and text.In Proceedings of the 2019 Conference on Empiri-cal Methods in Natural Language Processing andthe 9th International Joint Conference on NaturalLanguage Processing, EMNLP-IJCNLP 2019, HongKong, China, November 3-7, 2019, pages 23802390.Association for Computational Linguistics. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang,and Zhong Su. 2008. Arnetminer: extraction andmining of academic social networks. In Proceed-ings of the 14th ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,Las Vegas, Nevada, USA, August 24-27, 2008, pages990998. ACM. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.",
  "Denny Vrandecic and Markus Krtzsch. 2014. Wiki-data: a free collaborative knowledgebase. Commun.ACM, 57(10):7885": "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,Zhiyuan Liu, and Juanzi Li. 2022.Finding skillneurons in pre-trained transformer-based languagemodels. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2022, Abu Dhabi, United Arab Emirates, De-cember 7-11, 2022, pages 1113211152. Associationfor Computational Linguistics. Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang,Yuyang Xie, Shangqing Tu, Huihui Yuan, JingyaoZhang, Bowen Huang, Yuanyao Li, Juanzi Li, and JieTang. 2024. Soay: A service-oriented apis applyingframework of large language models. Chaojun Xiao, Zhengyan Zhang, Xu Han, Chi-MinChan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,Zhonghua Li, Zhao Cao, and Maosong Sun. 2023.Plug-and-play document modules for pre-trainedmodels. In Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pages 1571315729. Association forComputational Linguistics. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,Torsten Scholak, Michihiro Yasunaga, Chien-ShengWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-tor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,Ansong Ni, Ziyu Yao, Dragomir Radev, CaimingXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:Unifying and multi-tasking structured knowledgegrounding with text-to-text language models. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP 2022,Abu Dhabi, United Arab Emirates, December 7-11,2022, pages 602631. Association for ComputationalLinguistics. Xuchen Yao. 2015.Lean question answering overfreebase from scratch. In NAACL HLT 2015, The2015 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, Denver, Colorado, USA,May 31 - June 5, 2015, pages 6670. The Associationfor Computational Linguistics.",
  "Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,and Caiming Xiong. 2022. RNG-KBQA: generation": "augmented iterative ranking for knowledge base ques-tion answering. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2022, Dublin,Ireland, May 22-27, 2022, pages 60326043. Associ-ation for Computational Linguistics. Wen-tau Yih, Matthew Richardson, Christopher Meek,Ming-Wei Chang, and Jina Suh. 2016. The value ofsemantic parse labeling for knowledge base questionanswering. In Proceedings of the 54th Annual Meet-ing of the Association for Computational Linguistics,ACL 2016, August 7-12, 2016, Berlin, Germany, Vol-ume 2: Short Papers. The Association for ComputerLinguistics. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-der J. Smola, and Le Song. 2018. Variational reason-ing for question answering with knowledge graph. InProceedings of the Thirty-Second AAAI Conferenceon Artificial Intelligence, (AAAI-18), the 30th inno-vative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on EducationalAdvances in Artificial Intelligence (EAAI-18), NewOrleans, Louisiana, USA, February 2-7, 2018, pages60696076. AAAI Press. Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, HuadongWang, Deming Ye, Chaojun Xiao, Xu Han, ZhiyuanLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.Plug-and-play knowledge injection for pre-trainedlanguage models. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2023, Toronto,Canada, July 9-14, 2023, pages 1064110658. Asso-ciation for Computational Linguistics. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, ZikangLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. CoRR,abs/2303.18223.",
  "FunctionInputArgs OutputDescription": "FindE Efind an entity from the KBFindAll Ereturn all entities in the KBRelate(E E) R Ea single hop along a relationReverseRelate(E E) R Ea reverse hop along a relationFilterConceptE C Ereturn entities in a conceptAnd/Or(E, E) Eintersection/union of two setsArgmax/ArgminE R Esuperlative aggregationsLT/LE/GT/GEE R E< / / > / CountE Nset cardinality",
  "BTriple Sampling Strategy": "For WebQSP, GraphQ, and GrailQA, since theirKBs are large-scale and relatively sparse, we adopta popularity-based strategy to sample representa-tive triples for each schema item.Specifically,let the given KB be KB = {C, E, R, T }, whereT = Te Tc Tl. For each e E, let cnt(e) beits popularity (i.e., the number of its occurrencesin KB). When sampling instance of triples fora concept c C, we hope the sampled triples con-tain representative entities belonging to c, so wesort all (ek, instance of, c) Te in descending or-der of cnt(ek) and select the first K triples. Whensampling relational triples for a relation r Rl,we take both representativeness and diversity intoaccount. Therefore, we sort all (ei, r, ej) Tlin descending order of min(cnt(ei), cnt(ej)) andselect the first K triples.On the other hand, the KBs of KQA Pro,MetaQA, and SoAyBench are dense, so we justrandomly sample triples for their schema items.",
  "CDetails of Constrained Decoding": "In constrained decoding, after MTPI generates tfunction chunks f1(arg1), . . . , ft(argt), we enu-merate all admissible ft+1(argt+1) as the candi-date set Pt+1 following the definition of KoPLfunctions in , and constrain MTPI to continuegenerating one of these candidate or generating theEOS token to end the decoding process.Specifically, let Etopic be the set of topicentities in the question obtained using off-the- shelf entity linkers 1. At t = 0, we enumerateFind(e) for each eEtopic as a candidatein P1.Specially, around 5% of questions inGraphQ and GrailQA do not have a topic entity(e.g., Who is the heaviest film director?\" fromGrailQA, whose target program is FindAll()FilterConcept(director)SelectAmong(weightkg). For these questions, we follow Pangu (Guet al., 2023) to start constrained decoding fromFindAll()FilterConcept(c), where c is a topicconcept provided by Gu and Su (2022).When t > 0, we execute the current programpt = f1(arg1), . . . , ft(argt) to get its denota-tion (i.e., a set of entities) and also the concepts,forward relations, and backward relations that arereachable from the denotation. For each conceptc, we enumerate FilterConcept(c) as a candidatein Pt+1. For each forward relation r, we enumer-ate Relate(r) as a candidate. For each backwardrelation r, we enumerate ReverseRelate(r) as acandidate, and also include LT(r), LE(r), GT(r), andGE(r) in Pt+1 if the denotation of pt is a numeri-cal value such as a quantity or a date. In addition,candidates with superlatives can be enumerated asArgmax(r) and Argmin(r). Also, Count() can al-ways be included to Pt+1. If there are multipletopic entities, we enumerate Find(e) as a candi-date to add a new branch, where e Etopic is atopic entity not in pt. When pt contains multiplebranches, we enumerate Or() and And() as candi-dates to merge the last two branches.",
  "D.1Details of Baselines and EvaluationMetrics": "The details of our baselines are as follows:Pangu (Gu et al., 2023) utilizes potent LLMCodex (Chen et al., 2021) to produce programsin a step-wise fashion via in-context learning. Ateach step, it first extends existing programs intonew valid candidates by enumerating all possiblenext functions with arguments, then scores eachcandidate using Codex with several demonstrationsand retains the top-k candidates.KB-BINDER (Li et al., 2023a) first lets Codex gen-erate several \"draft\" programs for a given questionby imitating a few examples, then grounds the argu-ments in the drafts to the target KB using similarity",
  "Entity linking is not a major challenge for PI, and exhaus-tive fuzzy string matching (Yao, 2015) suffices to achieve areasonable performance": "search to produce hundreds of refined programs.The final answer is decided by the majority voteafter executing all these refined programs.Automatic Program Sampling (APS) (Li et al.,2023c) utilizes gpt-3.5-turbo2 to translate auto-matically sampled programs based on a handfulof templates into corresponding questions via in-context learning, and subsequently fine-tune a RnG-KBQA (Ye et al., 2022) PI model using the gener-ated question-program pairs.ProgramTrans (Cao et al., 2022b) is a programtransfer method that first uses a seq2seq sketchparser to translate the question into a programsketch, then uses an argument parser to search suit-able argument from the KB for each function. Weadopt its results without fine-tuning on the targetKBs for fair comparison.DFSDT (Qin et al., 2023) is the SoTA method forgeneral tool using. To solve a question, it employsan LLM to call suitable tool APIs in depth-firstorder. At each step, the LLM can either (1) callthe next API to proceed along a promising path or(2) undo the current call and call another API toexpand a new path.SoAy (Wang et al., 2024) is the SoTA method onSoAyBench. Given a question, it employs LLM tofirst select the most suitable plan (i.e., API combi-nation) from a candidate pool, then write a Pythonprogram with branching and looping structure fol-lowing the plan to call APIs to get the answer.Supervised Methods.For WebQSP, GraphQ,GrailQA, and MetaQA, we also provide the fullysupervised results of several representative modelsfor comparison, including QGG (Lan and Jiang,2020), BERT+Ranking (Gu et al., 2021), Arc-naeQA (Gu and Su, 2022), RnG-KBQA (Ye et al.,2022), KV-Mem(Miller et al., 2016), PullNet (Sunet al., 2019), EmbedKGQA (Saxena et al., 2020)and TransferNet Shi et al. (2021).Evalution Metrics. Following these baselines, weuse F1 for WebQSP, GraphQ, and GrailQA, useHit@1 for MetaQA, and use Accuracy for SoAy-Bench.",
  "EAnalysis about Question CompositionalStructures": "For GraphQ and GrailQA, we translate theirSPARQL programs to KoPL programs usingGraphQ Trans (Nie et al., 2022) and analyzethe performance of KB-Plugin on the test caseswhose question compositional structures (identi-fied by program sketches) are seen and unseen inthe source domain dataset KQA Pro, respectively.From the results in we can see that (1) KQAPro covers most of question compositional struc-tures in the target dataset; (2) KB-Plugin correctlypredicts the program sketches for over 70% ques-tions whose compositional structures are seen inthe source domain dataset, implying that the map-ping from questions to program sketches is largelyindependent of KB schemas and transferable acrossKBs, which is consistent with the findings of Caoet al. (2022b) and Li et al. (2023a); (3) KB-Pluginperforms poorly on the questions with unseen com-positional structures though they are relatively rare,indicating that more advanced transfer techniquesacross compositional structures remains to be ex-plored.",
  "FError Analysis": "We analyze 100 incorrect predictions (i.e., F1<1)randomly sampled from the dev set of GrailQA.The major errors are predicting wrong schemaitems (36%).Specially, when facing severalschema items with only subtle differences, e.g.,publisher(reverse) v.s. game version published,KB-plugin tends to prefer to choose the shorter onedue to the inherent defects of beam search. Be-sides, 21% errors are due to a wrong terminationcheck where the model misses the last relation or predicts an additional function. There are also 5%wrong function predictions. Apart from the aboveerrors caused by our model, 27% errors are causedby unidentified or wrongly identified topic entitiesduring entity linking, 9% errors are due to ambigu-ous or wrong annotations, and the remaining 2%errors are due to the incompletion of KBs."
}