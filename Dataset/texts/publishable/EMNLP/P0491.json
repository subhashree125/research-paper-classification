{
  "Abstract": "The fashion domain includes a range of real-world multimodal tasks, such as multimodalretrieval and generation. Recent advancementsin AI-generated content, particularly large lan-guage models for text and diffusion models forvisuals, have spurred significant research in-terest in applying these multimodal models tofashion. However, fashion models must alsoeffectively handle embedding tasks, like image-to-text and text-to-image retrieval. Moreover,current unified fashion models often lack thecapability for image generation. In this work,we present UniFashion, a unified frameworkthat tackles the challenges of multimodal gen-eration and retrieval tasks in the fashion do-main, by integrating image and text genera-tion with retrieval tasks. UniFashion unifiesembedding and generative processes throughthe use of a diffusion model and LLM, en-abling controllable and high-fidelity genera-tion. Our model significantly outperforms pre-vious state-of-the-art models focused on singletasks across various fashion-related challengesand can be easily adapted to manage complexvision-language tasks. This study highlightsthe synergistic potential between multimodalgeneration and retrieval, offering a promisingavenue for future research in the fashion do-main. The source code is available at",
  "Introduction": "The fashion domain presents a range of real-worldmultimodal tasks, encompassing multimodal re-trieval (Gao et al., 2020; Wu et al., 2021; Baiet al., 2023; Liu et al., 2024b) and multimodalgeneration (Yang et al., 2020) tasks. Such taskshave been utilized in diverse e-commerce scenar-ios to enhance product discoverability, seller-buyerinteraction, and customer conversion rates aftercatalog browsing (Han et al., 2023; Zhuge et al.,2021). The remarkable progress in the field of arti- ficial intelligence generated content (AIGC), par-ticularly in technologies like large language mod-els (LLMs) (Chiang et al., 2023; Touvron et al.,2023; Brown et al., 2020) for text generation anddiffusion models (Rombach et al., 2022; Nicholet al., 2022; Saharia et al., 2022) for visual genera-tion, yielding significant advancements in numer-ous downstream tasks (Feng et al., 2023; Zhanget al., 2022) and sparking widespread research in-terest in applying these multimodal models to thefashion domain.Instruction-tuned multimodal large languagemodels (Liu et al., 2023a; Dai et al., 2023; Donget al., 2023; Zhao et al., 2024) (MLLMs) haveemerged as a promising direction for developing asingle multi-task model (Shi et al., 2023). However,due to the heterogeneous nature of multimodal fash-ion tasks (Han et al., 2023), most existing MLLMsstruggle to be directly applicable in the fashion do-main. For example, in the fashion domain, retrievaltasks that rely on embedding ability, such as image-to-text or text-to-image retrieval, have largely beenoverlooked. Furthermore, existing MLLMs lackthe ability to solve the composed image retrieval(CIR) (Liu et al., 2021; Baldrati et al., 2022) task,which composes the reference image and relatedcaption in a joint embedding to calculate similari-ties with candidate images and is particularly rel-evant in recommender systems (Han et al., 2017;Liu et al., 2022, 2024a).Drawing inspiration from GRIT (Muennighoffet al., 2024), which successfully combined genera-tive and embedding tasks into a unified model fortext-centric applications and enhanced embeddingperformance by incorporating a generative objec-tive, it is evident that exploring task correlationsand integrating embedding with generative modelsin the fashion domain is promising.While previous works (Han et al., 2023; Zhugeet al., 2021) in the fashion domain have also pro-posed using a single model for solving multiple",
  "Composed Image Generation": ": Illustration of the fashion tasks encompassed in our UniFashion framework: cross-modal retrieval,text-guided image retrieval, fashion image captioning, and fashion image generation. Model inputs highlighted witha light yellow background and outputs denoted by a light blue background. tasks, they ignore image generation tasks. Besides,for fashion tasks such as try-on (Choi et al., 2021)and fashion design (Baldrati et al., 2023b), it is gen-erally required to generate target images based onmultimodal input. However, previous works (Bal-drati et al., 2023b) in fashion image generationtypically adopt the CLIP text encoder for encodingtext information. This approach may not effectivelycapture the textual context due to the limitations ofthe text encoder, as noted by Saharia et al. (2022).Hence, we posit that current studies have yet tofully explore the potential synergy between genera-tion and retrieval. In this work, we propose UniFashion, whichunifies retrieval and generation tasks by integrat-ing LLMs and diffusion models, as illustrated in. UniFashion consists of three parts: TheQ-Former is crucial for amalgamating text and im-age input, creating multimodal learnable queries.These queries, once refined through task-specificadapters, enable the LLM module to utilize them assoft prompts for generating captions for target im- ages. Simultaneously, the diffusion module utilizesthe learnable queries as conditions to guide the la-tent diffusion model in image synthesis and editingtasks. To enable controllable and high-fidelity gen-eration, we propose a two-phase training strategy.In the first phase, we perform multimodal repre-sentation learning on image-text pairs datasets. Wefreeze Q-Former and fine-tune the LLM and diffu-sion modules, ensuring they develop the capabil-ity to comprehend the multimodal representationsprovided by Q-Former. Subsequently, in the sec-ond phase, we proceed to fine-tune UniFashion ondatasets with multimodal inputs, such as Fashion-IQ, where we freeze the LLM and diffusion mod-ules, only tuning Q-Former. This strategy ensuresthat Q-Former is adept at crafting multimodal repre-sentations that effectively integrate both referenceimages and text inputs.UniFashion holds three significant advantagesthat address the challenges in multimodal fashionretrieval and generation:",
  "For the first time, we conduct an in-depth": "study of the synergistic modeling of multi-modal retrieval and generation tasks withinthe fashion domain, thoroughly exploiting theinter-task relatedness. Further, we introduceUniFashion, a versatile, unified model that canhandle all fashion tasks. Secondly, our model enhances performancevia mutual task reinforcement. Specifically,the caption generative module aids the CIRtask, while jointly training the generation andretrieval tasks improves the multimodal en-coder for the diffusion module. Thirdly, extensive experiments on diversefashion tasksincluding cross-modal re-trieval, composed image retrieval, and mul-timodal generationdemonstrate that our uni-fied model significantly surpasses previousstate-of-the-art methods.",
  "Fashion Tasks": "Fashion tasks encompass a range of image andlanguage manipulations, including cross-modal re-trieval, composed image retrieval, fashion imagecaptioning and generation, etc. The representativetasks can be briefly divided into the following twogroups. Fashion Retrieval.It generally consists of Cross-Modal Retrieval (CMR) (Ma et al., 2022; Ros-tamzadeh et al., 2018) and composed image re-trieval (CIR) tasks (Baldrati et al., 2023a; Bai et al.,2023). CMR requests to efficiently retrieve themost matched image/sentence from a large candi-date pool D given a text/image query. CIR is aspecial type of image retrieval with a multimodalquery (a combination of a reference image and amodifying text) matched against a set of images. Itretrieves a target image from a vast image databasebased on a reference image and a text descriptiondetailing changes to be applied to the reference im-age. In this scenario, a query pair p = {IR, t} isprovided, where IR is the reference image and t isthe text describing the desired modifications. Thechallenge for this task is to accurately identify thetarget image IT that best matches the query amongall potential candidates in the image corpus D.",
  "Multimodal Language Models": "Recent research has witnessed a surge of inter-est in multimodal LLMs, including collaborativemodels (Wu et al., 2023; Yang et al., 2023b; Shenet al., 2023) and end-to-end methods (Alayrac et al.,2022; Zhao et al., 2024; Li et al., 2022; Bao et al.,2021; Wang et al., 2022b,a,a). More recently, someworks also explore training LLMs with parameter-efficient tuning (Li et al., 2023b; Zhang et al.,2023b) and instruction tuning (Dai et al., 2023;Liu et al., 2023a; Ye et al., 2023; Zhu et al., 2023a;Li et al., 2023a). They only focus on generationtasks, while our model UniFashion is designed as aunified framework that enables both retrieval andgeneration tasks.",
  "Diffusion Models": "Diffusion generative models (Rombach et al., 2022;Ramesh et al., 2021; Nichol et al., 2022; Ruiz et al.,2023) have achieved strong results in text condi-tioned image generation works. Among contempo-rary works that aim to condition pretrained latentdiffusion models, ControlNet (Zhang et al., 2023a)proposes to extend the Stable Diffusion model withan additional trainable copy part for conditioninginput. In this work, we focus on the fashion domainand propose a unified framework that can leveragelatent diffusion models that directly exploit the con-ditioning of textual sentences and other modalitiessuch as human body poses and garment sketches.",
  "Problem Formulation": "Existing fashion image retrieval and generationmethods are typically designed for specific tasks,which inherently restricts their applicability to thevarious task forms and input/output forms in thefashion domain.To train a unified model thatcan handle multiple fashion tasks, our approachintroduces a versatile framework capable of han-dling multiple fashion tasks by aligning the multi-modal representation into the LLM and the diffu-sion model. This innovative strategy enhances themodels adaptability, and it can be represented as:",
  "Proposed Model: UniFashion": "In this section, we introduce the UniFashion tounify the fashion retrieval and generation tasks intoa single model. By combining retrieval and gener-ative modules, the proposed UniFashion employsa two-stage training strategy to capture relatednessbetween image and language information. Con-sequently, it can seamlessly switch between twooperational modes for cross-modal tasks and com-posed modal tasks.",
  "Phase 1: Cross-modal Pre-training": "In the first stage, we conduct pre-training on theretrieval and generative modules to equip the LargeLanguage Model (LLM) and diffusion model withstrong cross-modal fashion representation capabili-ties for the next phase. 3.1.1Cross-modal RetrievalFor cross-modal retrieval tasks, given a batch ofimage caption pairs p = {I, C}, we first calculatetheir unimodal representations using an indepen-dent method. In particular, we adopt a lightweightQuerying Transformer, i.e., Q-Former in BLIP-2 (Li et al., 2023b), to encode the multimodal in-puts, as it is effective in bridging the modality gap.To avoid information leaks, we employ a unimodalself-attention mask (Li et al., 2023b), where thequeries and text are not allowed to see each other:",
  "ZC = Q-Former(C).(2)": "where the output sequence ZI is the encoding resultof an initialized learnable query q with the input im-age and ZC is the encoded caption, which containsthe embedding of the output of the [CLS] tokenecls, which is a representation of the input captiontext. Since ZI contains multiple output embed-dings (one from each query), we first compute thepairwise similarity between each query output andecls, and then select the highest one as the image-text similarity. In our experiments, we employ 32queries in q, with each query having a dimension of768, which is the same as the hidden dimension ofthe Q-Former. For cross-modal learning objective,we leverage the Image-Text Contrastive Learning(ITC) and Image-Text Matching (ITM) method.",
  "Lcross = LITC(tc, ZI) + LITM(ZC, ZI),(5)": "3.1.2Cross-modal GenerationAs depicted in , after the learnable queriesq pass through the multimodal encoder, they arecapable of integrating the visual information withtextual guidance. However, in .1.1, we didnot specify a learning target for q. Empirically, theq that has been merged with the reference imageand edited text information should be equivalentto the encoding of the target image. This impliesthat we should be able to reconstruct the targetimage and its caption based on q. In this section,we will employ generative objectives to improvethe representation of augmented q.In the first stage, we connect the Q-Former(equipped with a frozen image encoder) to a LargeLanguage Model (LLM) to harness the LLMsprowess in language generation, and to a diffu-sion model to exploit its image generation capa-bilities. Notably, we exclusively train the modelusing image-text pairs throughout this process. Asdepicted in , we employ a Task SpecificAdapter (TSA) layer to linearly project the outputquery embeddings q to match the dimensionalityof the embeddings used by the LLM and diffusionmodel. In this stage, we freeze the parameters ofthe Q-Former and fine-tune only the adapter layers,connecting LLM and diffusion models. This ap-proach allows us to develop a discriminative modelthat can evaluate whether queries q can generatethe target image and its corresponding caption.",
  "Generation": ": Overview of the training framework of our UniFashion model. Phase 1 - Cross-modal Pre-training:UniFashion acquires robust cross-modal fashion representation capabilities through pre-training, leveraging boththe language model and the diffusion model. Phase 2 - Composed Multimodal Fine-tuning: The model undergoesfine-tuning to process both image and text inputs, refining its ability to learn composed modal representations. Thisis achieved by aligning the multimodal encoder with the LLM and the diffusion model for enhanced performance. Target Caption Generation. The adapter layeris placed before the LLM to map the output of Q-Former to the text embedding space of the LLM.To synchronize the space of Q-Former with that ofthe LLM, we propose to use the image-groundedtext generation (ITG) objective to drive the modelto generate texts based on the input image by com-puting the auto-regressive loss:",
  "l=1log p(wgl |wg<l, f(q)),(6)": "where wg = (wg1, ..., wgL) represents the ground-truth caption of image I with length L, q =Q-Former(I, q), denotes the LLMs parameters,and denotes the text adapter layers parameters.Target Image Generation. In the first stage, ourtask also aims to reconstruct the image IT from q.As in standard latent diffusion models, given anencoded input x, the proposed denoising networkis trained to predict the noise stochastically addedto x. The corresponding objective function can bespecified as:",
  "Composed Image Retrieval": "For CIR task, the target image IT generally encom-passes the removal of objects and the modificationof attributes in the reference image. To solve thisproblem, as depicted in , the multimodal en-coder is utilized to extract features from the ref-erence image and the guide text. It joint embedsthe given pair p = {IR, t} in a sequential output.Specifically, a set of learnable queries q concate-nated with text guidance t is introduced to interactwith the features of the reference image. Finally,the output of Q-Former is the multimodal syntheticprompt ZR. We use a bi-directional self-attentionmask, similar to the one used in BLIP2 (Li et al.,2023b), where all queries and texts can attend toeach other. The output query embeddings ZR thuscapture multimodal information:",
  "ZT = Q-Former(IT , qT ).(9)": "Noting that the output sequence ZR consists oflearnable queries q and encoded text guidance t,which includes ecls, the embedding of the outputof the [CLS] token. On the other hand, the tar-get images output sequence ZT consists only oflearnable queries. Therefore, we can use ZR as arepresentation that incorporates information fromthe reference image and the guidance text and alignit with the features of the target image ZT . More-over, as UniFashion acquires the ability to generatecaptions for images from Sec. 3.1.2, we can gen-erate captions for the candidate images and useecls to retrieve the caption ZC of the target image.Then, the final contrastive loss for the CIR task is:",
  "Instruction-Tuning LLMs for DifferentCaption Style": "Liu et al.s work shows that LLMs have the po-tential to handle multimodal tasks based on textdescription of images. Due to the different stylesof captions in different fashion datasets, we adoptdifferent instructions to tune the LLM so that it cangenerate captions of different styles.We designed different instructions for differentdatasets and tasks, as shown in . Generalinstruction template is denoted as follows:USER: <Img><queries></Img> + Instruction. As-sistant: <answer>. For the <image> placeholder, we substitute itwith the output of Multimodal Encoder. To avoidoverfitting to the specific task and counteract themodels inclination to generate excessively shortoutputs, we have devised specific instructions,which enable the LLM to produce concise re-sponses when necessary.",
  "Experimental Setup": "We initialize the multimodal encoder usingBLIP2s Q-Former. Following the approach ofLLaVA-1.5 (Liu et al., 2023a), we initialize theLLM from Vicuna-1.5 (Zheng et al., 2023). Forthe diffusion module, we adopt the autoencoderand denoising U-Net from Stable Diffusion v1.4,as utilized in StableVITON. The weights of theU-Net are initialized from Paint-by-Example. Toachieve more refined person textures, we employa VAE that has been fine-tuned on the VITONHDdataset, as done in StableVITON. The statistics ofthe two-stage datasets can be found in . Forcross-modal retrieval, we evaluated UniFashion onFashionGen validation set. For the image caption-ing task, UniFashion is evaluated in the Fashion-Gen dataset. For the composed image retrievaltask, we evaluated the Fashion-IQ validation set.To maintain consistency with previous work, forthe composed image generation task, we fine-tunedUniFashion and evaluated it on the VITON-HDand MGD datasets. More details can be found inAppendix B. Phase 1:For multimodal representation learning,we follow BLIP2 and pretrain the Q-Former onfashion image-text pairs. To adapt the model formultimodal generation, we freeze the parameters ofQ-Former and fine-tune the MLLM and diffusionmodel with their task specific adapters separately.Due to the different styles of captions in differentfashion datasets, we adopt the approach of instruc-tion tuning to train the LLM so that it can generatecaptions of different styles. More details can befound in Appendix 3.3.",
  "Datasets": "We test the effectiveness of UniFashion by experi-menting on different tasks including fashion imagecaptioning, cross-modal retrieval, composed imageretrieval and composed image generation.We use the FashionGen and FshaionIQ (Linet al., 2014) datasets for retrieval tasks. Fashion-Gen contains 68k fashion products accompaniedby text descriptions. Each product includes 1 - 6images from different angles, resulting in 260.5kimage-text pairs for training and 35.5k for testing.Fashion-IQ contains 18k training triplets (that is,reference image, modifying text, target image) and6k validation triplets over three categories: Dress,Shirt, and Toptee. Each pair (reference image, tar-get image) is manually annotated with two modify-ing texts, which are concatenated.For fashion image captioning tasks, we utilizethe FashionGen (Zang et al., 2021) dataset. Ad-ditionally, to enhance our models capability inthe CIR task, which involves the ability to re-trieve captions for target images, we have annotatedimages from the training set of Fashion-IQ. Rec-ognizing that manually annotating all the imageswould be time-consuming and resource-intensive,we draw inspiration from the success of recentMLLM models such as LLaVA in text-annotationtasks, and propose leveraging LLaVA 1.5 (13B)to semi-automatically annotate the dataset. More",
  "We compare our models with previous state-of-the-art methods on each task. For extensive and faircomparisons, all prior competitors are based onlarge-scale pre-trained models": "Cross-modal Retrieval Evaluation.We con-sider both image-to-text retrieval and text-to-imageretrieval with random 100 protocols used by pre-vious methods. 100 candidates are randomly sam-pled from the same category to construct a retrievaldatabase. The goal is to locate the positive matchdepicting the same garment instance from these100 same-category negative matches. We utilizeRecall@K as the evaluation metric, which reflectsthe percentage of queries whose true target rankedwithin the top K candidates.",
  "Fashion Image Captioning Evaluation.Forevaluating the performance of caption generation,we utilize BLEU-4, METEOR, ROUGE-L, andCIDEr as metrics": "Composed Fashion Image Retrieval Evaluation.We compare our UniFashion with CIR methodsand the FAME-ViL model of V + L that is orientedtowards fashion in the original protocol used byFashion-IQ. For this task, we also utilize Recall@Kas the evaluation metric. Composed Fashion Image Generation Evalua-tion.We compare our UniFashion with try-onmethods on VITON-HD dataset and fashion designworks on MGD dataset. To evaluate the qualityof image generation, we use the Frechet InceptionDistance (FID) score to measure the divergencebetween two multivariate normal distributions andemploy the CLIP Score (CLIP-S) provided in theTorchMetrics library to assess the adherence of the",
  "Comparative Analysis of Baselines andOur Method": "UniFashion exhibits superior performanceacross all datasets compared to baselines. Tab. 1presents the evaluation results for each baselineand our models in FashionGen data sets for cross-modal retrieval. UniFashion outperforms most ofthe baseline models on both the text-to-image andimage-to-text tasks. Following FAME-ViL, we also adopt a more challenging and practical pro-tocol that conducts retrieval on the entire productset, which is in line with actual product retrievalscenarios. In Tab. 2, we performed a comparisonbetween our UniFashion and other baselines on theFashionGen dataset for the image captioning task.By integrating the powerful generative ability ofthe LLM, our model performed significantly betterthan the traditional multimodal models in this task.In Tab. 4, we conducted a comparison betweenour UniFashion and CIR-specialist methods. Ourfindings are in line with those of Tab. 1. After fine-tuning UniFashion on image gen-eration/editing tasks with multimodal inputs, itexhibits outstanding performance. Tab. 3 evalu-ates the quality of the generated image of UniFash-ion in the VITON-HD unpaired setting. In orderto verify that our model can achieve good resultsin a variety of modal inputs, we have conductedtests, respectively, on the traditional try-on task andthe fashion design task proposed in MGD. For afair evaluation with baselines, all the models aretrained at a 512 384 resolution. To confirm theefficacy of our approach, we assess the realism us- ing FID and KID score on all the tasks and usingCLIP-S score for fashion design task. As can beseen, the proposed UniFashion model consistentlyoutperforms competitors in terms of realism (i.e.,FID and KID) and coherence with input modali-ties (i.e., CLIP-S), indicating that our method canbetter encode multimodal information. Meanwhile,although our model is slightly lower than Stable-VITON on the try-on task, this is because we frozethe parameters of the diffusion model on the try-ontask and only fine-tuned the Q-former part, but itcan still achieve top2 results. The visual results canbe found in Appendix E.",
  "Ablation Study": "UniFashion allows for more flexible executionof multimodal composed tasks. In Tab. 4, wealso carry out ablation studies on different retrievalmethods. Since UniFashion is capable of generat-ing captions, for the CIR task, we initially utilizeUniFashion to generate the captions of candidateimages and then conduct the image retrieval task(denoted as UniFashion w/o cap) and the captionretrieval task (denoted as UniFashion w/o img).We find that our single-task variant has alreadyachieved superior performance in the relevant field.Furthermore, due to the generative ability of ourmodel, the pregenerated candidate library opti-mizes the models performance in this task. Forspecific implementation details, please refer to Ap-pendix C.We investigate the impact of different mod-ules in UniFashion on various fashion tasks. InTab. 5, we perform an ablation study on the pro-posed model architecture, with a focus on LLMand diffusion models. For comparison on the cross-modal retrieval task (CMR), we design the basemodel as directly fine-tuning BLIP2 without anynew modules. The results indicate that the basemodel performs relatively well on this task andthat the introduction of other modules does notlead to significant improvements. However, in theCIR task, the introduction of LLM and diffusionmodels as supervision can lead to significant im-provements, especially when utilizing pregeneratedcaptions by UniFashion to assist in retrieval, re-sulting in greater benefits. At the same time, wenote that, after introducing the diffusion model, itmay have some negative impact on the modelsimage captioning ability, possibly due to the inher-ent alignment differences between LLM and thediffusion model.",
  "Conclusion": "We have introduced UniFashion, a unified frame-work designed to tackle challenges in multimodalgeneration and retrieval within the fashion domain.By integrating embedding and generative tasks us-ing a diffusion model and LLM, UniFashion en-ables controllable, high-fidelity generation, signifi-cantly outperforming previous single-task state-of-the-art models across various fashion tasks. Ourmodels adaptability in handling complex vision-language tasks demonstrates its potential to en-hance e-commerce scenarios and fashion-relatedapplications. This study highlights the importanceof exploring the learning synergy between multi-modal generation and retrieval, offering a promis-ing direction for future research in the fashion do-main.",
  "Limitations": "In this section, we discuss limitations of our workand offer further insights into research within thefashion domain.Computational Requirements. UniFashion in-tegrates multiple complex modules, including Q-Former, LLM, and diffusion models, which resultin higher computational complexity during training.However, during the inference stage, the compu-tational complexity of UniFashion is comparableto that of current state-of-the-art models. For re-trieval tasks, only the Q-Former module is neededto calculate the similarity between the input imageor text and the pre-stored candidate features in thedatabase, eliminating the need to utilize the LLMand diffusion model components for inference. Forcomposed image generation tasks, such as fashiondesign, our model relies on diffusion processes,which may take longer. In our experiments, wetested the performance of our model on an A100(80G) GPU. During inference, using 1000 exam-ples from the VITON-HD dataset, UniFashion tookapproximately 3.15 seconds per image generation.We believe exploring more efficient sampling meth-ods, such as DPM-Solver++ (Lu et al., 2022), couldimprove the overall efficiency of UniFashion.",
  "We thank the anonymous reviewers for their valu-able feedback. This research was partially sup-ported by the grant of HK ITF ITS/359/21FP": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fa-had Khan, Wangmeng Zuo, Rick Siow Mong Goh,and Chun-Mei Feng. 2023. Sentence-level promptsbenefit composed image retrieval. arXiv preprintarXiv:2310.05473. Alberto Baldrati, Marco Bertini, Tiberio Uricchio, andAlberto Del Bimbo. 2022. Effective conditioned andcomposed image retrieval combining clip-based fea-tures. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages2146621474. Alberto Baldrati, Marco Bertini, Tiberio Uricchio, andAlberto Del Bimbo. 2023a. Composed image re-trieval using contrastive learning and task-orientedclip-based features. ACM Transactions on Multime-dia Computing, Communications and Applications,20(3):124. Alberto Baldrati, Davide Morelli, Giuseppe Cartella,Marcella Cornia, Marco Bertini, and Rita Cucchiara.2023b.Multimodal garment designer: Human-centric latent diffusion models for fashion imageediting. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 2339323402.",
  "Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, andTat-Seng Chua. 2022. Composed image retrievalwith text feedback via multi-grained uncertainty reg-ularization. arXiv preprint arXiv:2211.07394": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Seunghwan Choi, Sunghyun Park, Minsoo Lee, andJaegul Choo. 2021. Viton-hd: High-resolution vir-tual try-on via misalignment-aware normalization. InProceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1413114140. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven Hoi. 2023. In-structblip: Towards general-purpose vision-languagemodels with instruction tuning. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,Hongyu Zhou, Haoran Wei, et al. 2023. Dreamllm:Synergistic multimodal comprehension and creation.arXiv preprint arXiv:2309.11499. Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming Wu. 2023. Towards llm-driven dialogue statetracking. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 739755. Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, PengLi, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashion-bert: Text and image matching with adaptive loss forcross-modal retrieval. In Proceedings of the 43rdInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages22512260. Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal,Rakesh Chada, Yue Wu, Varsha Hedau, and PradeepNatarajan. 2022. Fashionvlp: Vision language trans-former for fashion retrieval with feedback. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1410514115. Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, ChenQian, and Liqing Zhang. 2023. Taming the powerof diffusion models for high-quality virtual try-onwith appearance flow. In Proceedings of the 31stACM International Conference on Multimedia, pages75997607. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913. Geonmo Gu, Sanghyuk Chun, Wonjae Kim, YoohoonKang, and Sangdoo Yun. 2024. Language-only train-ing of zero-shot composed image retrieval. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1322513234. Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-ZheSong, and Tao Xiang. 2023. Fame-vil: Multi-taskingvision-language model for heterogeneous fashiontasks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages26692680. Xintong Han, Zuxuan Wu, Phoenix X Huang, XiaoZhang, Menglong Zhu, Yuan Li, Yang Zhao, andLarry S Davis. 2017. Automatic spatially-aware fash-ion concept discovery. In Proceedings of the IEEEinternational conference on computer vision, pages14631471.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-noising diffusion probabilistic models. Advancesin neural information processing systems, 33:68406851": "Jeongho Kim, Guojung Gu, Minho Park, SunghyunPark, and Jaegul Choo. 2024. Stableviton: Learningsemantic correspondence with latent diffusion modelfor virtual try-on. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 81768185. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual genome: Connecting language and vi-sion using crowdsourced dense image annotations.International journal of computer vision, 123:3273.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understandingand generation. In International Conference on Ma-chine Learning, pages 1288812900. PMLR. Shenshen Li, Xing Xu, Xun Jiang, Fumin Shen, ZheSun, and Andrzej Cichocki. 2024. Cross-modal at-tention preservation with self-contrastive learning forcomposed query-based image retrieval. ACM Trans-actions on Multimedia Computing, Communicationsand Applications, 20(6):122. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. 2014.Microsoft coco:Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023a. Visual instruction tuning. arXiv preprintarXiv:2304.08485": "Qijiong Liu, Xiaoyu Dong, Jiaren Xiao, Nuo Chen,Hengchang Hu, Jieming Zhu, Chenxu Zhu, TetsuyaSakai, and Xiao-Ming Wu. 2024a. Vector quantiza-tion for recommender systems: A review and outlook.arXiv preprint arXiv:2405.03110. Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-MingWu. 2022. Boosting deep ctr prediction with a plug-and-play pre-trainer for news recommendation. InProceedings of the 29th International Conference onComputational Linguistics, pages 28232833. Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai,Zhaocheng Du, Xiao-Ming Wu, Zhou Zhao, RuiZhang, and Zhenhua Dong. 2024b. Multimodal pre-training, adaptation, and generation for recommen-dation: A survey. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery andData Mining, pages 65666576. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,and Stephen Gould. 2021. Image retrieval on real-lifeimages with pre-trained vision-and-language models.in 2021 ieee. In CVF International Conference onComputer Vision (ICCV)(2021), pages 21052114.",
  "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-uan Li, and Jun Zhu. 2022. Dpm-solver++: Fastsolver for guided sampling of diffusion probabilisticmodels. arXiv preprint arXiv:2211.01095": "Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale,Zhangyang Wang, Tong Yu, Jiuxiang Gu, SunavChoudhary, and Xiaohui Xie. 2022. Ei-clip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1805118061. Chenlin Meng, Yutong He, Yang Song, Jiaming Song,Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021.Sdedit: Guided image synthesis and editing withstochastic differential equations.arXiv preprintarXiv:2108.01073.",
  "Niklas Muennighoff, Hongjin Su, Liang Wang, NanYang, Furu Wei, Tao Yu, Amanpreet Singh, andDouwe Kiela. 2024. Generative representational in-struction tuning. arXiv preprint arXiv:2402.09906": "Alexander Quinn Nichol, Prafulla Dhariwal, AdityaRamesh, Pranav Shyam, Pamela Mishkin, Bob Mc-grew, Ilya Sutskever, and Mark Chen. 2022. Glide:Towards photorealistic image generation and edit-ing with text-guided diffusion models.In Inter-national Conference on Machine Learning, pages1678416804. PMLR. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, ScottGray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. 2021. Zero-shot text-to-image gen-eration. In International Conference on MachineLearning, pages 88218831. PMLR.",
  "on Computer Vision and Pattern Recognition, pages1068410695": "Negar Rostamzadeh, Seyedarian Hosseini, Thomas Bo-quet, Wojciech Stokowiec, Ying Zhang, ChristianJauvin, and Chris Pal. 2018. Fashion-gen: The gen-erative fashion dataset and challenge. arXiv preprintarXiv:1806.08317. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, YaelPritch, Michael Rubinstein, and Kfir Aberman. 2023.Dreambooth: Fine tuning text-to-image diffusionmodels for subject-driven generation. In Proceed-ings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2250022510. Chitwan Saharia, William Chan, Saurabh Saxena,Lala Li, Jay Whang, Emily L Denton, Kam-yar Ghasemipour, Raphael Gontijo Lopes, BurcuKaragol Ayan, Tim Salimans, et al. 2022. Photo-realistic text-to-image diffusion models with deeplanguage understanding. Advances in Neural Infor-mation Processing Systems, 35:3647936494. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision, pages 146162. Springer.",
  "Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020.Denoising diffusion implicit models. arXiv preprintarXiv:2010.02502": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Lucas Ventura, Antoine Yang, Cordelia Schmid, andGl Varol. 2024. Covr: Learning composed videoretrieval from web video captions. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 38, pages 52705279.",
  "Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren": "Zhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-chitectures, tasks, and modalities through a simplesequence-to-sequence learning framework. In Inter-national Conference on Machine Learning, pages2331823340. PMLR. Wenhui Wang,Hangbo Bao,Li Dong,JohanBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,Owais Khan Mohammed, Saksham Singhal, SubhojitSom, et al. 2022b. Image as a foreign language: Beitpretraining for all vision and vision-language tasks.arXiv preprint arXiv:2208.10442.",
  "Chenfei Wu,Shengming Yin,Weizhen Qi,Xi-aodong Wang, Zecheng Tang, and Nan Duan.2023. Visual chatgpt: Talking, drawing and edit-ing with visual foundation models. arXiv preprintarXiv:2303.04671": "Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,Steven Rennie, Kristen Grauman, and Rogerio Feris.2021. Fashion iq: A new dataset towards retrievingimages by natural language feedback. In Proceedingsof the IEEE/CVF Conference on computer vision andpattern recognition, pages 1130711317. Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao,Haoye Dong, Xijin Zhang, Feida Zhu, and XiaodanLiang. 2023. Gp-vton: Towards general purpose vir-tual try-on via collaborative local-flow global-parsinglearning. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition,pages 2355023559. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-jin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.2023a. Paint by example: Exemplar-based imageediting with diffusion models. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1838118391. Xuewen Yang, Heming Zhang, Di Jin, Yingru Liu, Chi-Hao Wu, Jianchao Tan, Dongliang Xie, Jue Wang,and Xin Wang. 2020. Fashion captioning: Towardsgenerating accurate descriptions with semantic re-wards. In Computer VisionECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XIII 16, pages 117. Springer. Zhengyuan Yang, Linjie Li, Jianfeng Wang, KevinLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,Ce Liu, Michael Zeng, and Lijuan Wang. 2023b.Mm-react: Prompting chatgpt for multimodal rea-soning and action. arXiv preprint arXiv:2303.11381. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,Ming Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.mplug-owl: Modularization empowers large lan-guage models with multimodality. arXiv preprintarXiv:2304.14178. Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,Hao Zhang, and Jindong Chen. 2021. Photochat: Ahuman-human dialogue dataset with photo sharingbehavior for joint image-text modeling. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 61426152. Haode Zhang, Haowen Liang, Yuwei Zhang, Li-MingZhan, Xiao-Ming Wu, Xiaolei Lu, and Albert Lam.2022. Fine-tuning pre-trained language models forfew-shot intent detection: Supervised pre-trainingand isotropization. In Proceedings of the 2022 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 532542. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.2023a. Adding conditional control to text-to-imagediffusion models. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages38363847. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, andYu Qiao. 2023b. Llama-adapter: Efficient fine-tuningof language models with zero-init attention. arXivpreprint arXiv:2303.16199. Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi,and Xiao-Ming Wu. 2024. EasyGen: Easing mul-timodal generation with BiDiffuser and LLMs. InProceedings of the 62nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 13511370, Bangkok, Thailand.Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36:4659546623.",
  "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023a. Minigpt-4: Enhancingvision-language understanding with advanced largelanguage models. arXiv preprint arXiv:2304.10592": "Hongguang Zhu, Yunchao Wei, Yao Zhao, ChunjieZhang, and Shujuan Huang. 2023b. Amc: Adaptivemulti-expert collaborative network for text-guidedimage retrieval.ACM Transactions on Multime-dia Computing, Communications and Applications,19(6):122. Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, LinboJin, Ben Chen, Haoming Zhou, Minghui Qiu, andLing Shao. 2021. Kaleido-bert: Vision-languagepre-training on fashion domain. In Proceedings ofthe IEEE/CVF conference on computer vision andpattern recognition, pages 1264712657.",
  "After the initial proposal of diffusion modelsby (Sohl-Dickstein et al., 2015), they have demon-strated remarkable capacity for generating high-quality and diverse data.DDPM (Ho et al.,": "2020) connects diffusion and score matching mod-els through a noise prediction formulation, whileDDIM (Song et al., 2020) proposes an implicit gen-erative model that generates deterministic samplesfrom latent variables.Given a data point sampled from a real data dis-tribution x0 q(x), during forward diffusion, x0is gradually corrupted at each step t by addingGaussian noise to the output of step t-1. It producesa sequence of noisy samples x1, , xT . Then,diffusion models learn to reverse the process:",
  "(14)": "where p(xT ) = N(xT ; 0, I) is the standardGaussian distribution and t() is the parameter-ization of the predicted mean. Diffusion modelsare trained to maximize the marginal likelihood ofthe data E[log p(x0)], and the canonical objectiveis the variational lower bound of log p(x0). Stable Diffusion Model.Latent diffusion models(LDMs) operate in the latent space of a pre-trainedautoencoder achieving higher computational effi-ciency while preserving the generation quality. Sta-ble diffusion model is composed of an autoencoderwith an encoder E and a decoder D, a conditionalU-Net denoising model , and a CLIP-based textencoder. With the fixed encoder E, an input imagex is first transformed to a lower-dimensional latentspace z0 = E(x). The decoder D performs the op-posite operation, decoding z0 into the pixel space.When considering a latent variable z and its noisycounterpart zt, which is obtained by incrementallyadding noises to z over t steps, the latent diffusionmodels are designed to train the () to predict theadded noise using a standard mean squared errorloss:",
  "Human features": ": The architecture of UniFashion for fine-tuning on the image editing task. Firstly, we supply the clothsketch and text guidance to the multimodal encoder. Then, the diffusion model receives the output of the multimodalencoder, along with the cloth sketches and human features (i.e., agnostic-mask), to subsequently generate the desiredimages. eration, given a target image x0, the input condi-tion y0 could contain different constraints. Theaim is to model the conditional data distributionq(x0|y0), where y0 contains different modalitiesprompts. The conditioning mechanism is imple-mented by first encoding conditional information,then the denoising network conditions on y0 viacross-attention. The label y0 in a class-conditionaldiffusion model (xt|y0) is replaced with a nulllabel with a fixed probability during training.",
  "BImplementation Details": "LLMDuring the first phase, due to the flexibil-ity brought by the modular architectural design ofBLIP-2, we are able to adapt the model to a broadspectrum of LLMs. In order to effectively utilizethe capabilities of the existing MLLM models, weadopted LLaVA-1.5 as the LLM module of themodel. Technically, we leverage LoRA to enablea small subset of parameters within UniFashion tobe updated concurrently with two layers of adapterduring this phase. Specifically, the lora rank is 128and lora alpha is 256. We utilize the AdamW opti-",
  "mizer with 0 = 0.9, 1 = 0.99, and weight decayof 0. The LLMs are trained with a cosine learningrate of 2e-5 and a warmup rate of 0.03. We use abatch size of 32 for the tuned LLMs": "Diffusion ModuleWe inherit the autoencoderand the denoising U-Net of the Stable Diffusionv1.4. The weights of the U-Net from Paint-by-Example are used to initialize our denoising U-Net. To achieve more refined person texture, aVAE fine-tuned on the VITONHD dataset fromStableVITON is utilized. We train the model usingan AdamW optimizer with a fixed learning rate of1e-4 for 360k iterations, employing a batch size of32. For inference, we employ the pseudo linearmulti-step sampler, with the number of samplingsteps set to 50.",
  ": Vocabulary of the frequent words scaled byfrequency for dresses": "for target images, we have annotated images fromthe training set of Fashion-IQ. Recognizing thatmanually annotating all the images would be time-consuming and resource-intensive, we draw inspira-tion from the success of recent MLLM models suchas LLaVA in text-annotation tasks, and proposeleveraging LLaVA 1.5 (13B) to semi-automaticallyannotate the dataset. We perform word lemmati-zation to reduce each word to its root form. Suchpre-processing stage is crucial for the Fashion-IQdataset, as the captions do not describe a single gar-ment but instead express the properties to modifyin a given image to match its target. As shown in, by analysis of the captions in Fashion-IQ,we extracted key words that describe clothing in-formation such as color, sleeve, pattern, lace, etc.,as prompts for MLLM (LLaVA 1.5). We then in-structed the model to generate the correspondingcaptions referencing words that match the imagefeatures, as shown in . After this process, wegot the captions for Fashion-IQ dataset. The trainedUniFashion from this dataset (Fashion-IQ-cap) cangenerate captions for images in the evaluation set ofFashion-IQ to assist in the CIR task. More resultscan be seen in .",
  "DInstruction Formats": "Due to the disparity in caption styles across dif-ferent fashion datasets, we employ diverse instruc-tions to fine-tune the LLM, enabling it to gener-ate captions of varying styles. Specifically, theFashion200K dataset inclines towards providingbrief descriptions, the FashionGen dataset is proneto offering professional captions, and in Fashion-IQ-cap, the captions are detailed. Consequently,we have designed distinct instructions for differentdatasets and tasks, as illustrated in . The dress is colorful and has a flowery pattern. It is a long dress with thin straps and a fitted design. The dress is not revealing and has a modest style. The pattern is not plain, but rather a combination of different patterns. The dress is not crocheted and does not have a collar. It is not a tighter or looser dress, but rather a fitted dress. The dress is autumn colored, and has a vibrant and colorful design. Please generate a detailed caption to describe the {dress_type}. The caption describe the {dress_type}'s style, color, pattern's style, design and other key points. Please select sufficient appropriate words from: revealing, conservative, western, eastern, sexy, modest, patterned, plain, frilly, simple, crochet, collar, floral, plain, elegant, casual, monochromatic, colorful, flowery, plain, shiny, matte, darker, lighter, fitted, loose, print, plain, flare, tight, loose...",
  "EVisual Results": "illustrates the architecture of UniFashionfor fine-tuning on the image editing task. Initially,we input the cloth sketch and text guidance intothe multimodal encoder. The diffusion model thenreceives the output from the multimodal encoder,along with the cloth sketches and human features(such as the agnostic mask), to generate the de-sired images. We compare UniFashion with theMGD (Baldrati et al., 2023b) model for this task.In , we compare the images generated byour approach with the competitor in the VITON-HD (Choi et al., 2021) paired setting. In , weshow the generation effects of UniFashion in theVITON-HD unpaired setting. Our method, unlikethe MGD method that employs a warping moduleto generate input sketches, directly uses in-shopgarment sketches and is capable of generating im-ages that align more accurately with the providedcaptions and cloth sketches.",
  ": Examples of task instruction templates": "The dress is a dark green, long, and fitted. It has a collar and is sleeveless. The dress is made of a shiny fabric and has a flowery pattern. The design is elegant and the dress is more revealing than conservative. The dress is pink and has a floral pattern. It is a short dress, with a fitted top and a flared bottom. The dress is not revealing, but it is not conservative either. It is a sexy dress, with a collar and a belt. The dress is not crocheted, and it is not loose or tight. It is a colorful dress, and it is elegant. The dress is black and white, with a checkered pattern. It is a short dress, revealing and sexy, with a fitted style. The dress has a collar and is not crocheted. The pattern is smaller and has a more floral style. The dress is elegant and casual, with a flowery design. The dress is black and has a gold top half. It is a short dress with a fitted top and a flared bottom. The dress is sleeveless and has a low neckline, giving it a revealing and sexy appearance. The overall style of the dress is elegant and sophisticated, making it suitable for formal or semi-formal events. The dress is black with red designs. It is a strapless dress, which is a type of revealing dress. The dress is short, which is a characteristic of a more revealing dress. The dress is patterned, which adds to its visual appeal. The dress is not crocheted, and it does not have a collar. The dress is flowery, which is a characteristic of a more elegant dress. The dress is shiny, which adds to its visual appeal. The dress is darker, which is a characteristic of a more revealing dress. The dress is fitted, which is a characteristic of a more elegant dress. The dress is a short, leopard print dress. It is a fitted dress, which is designed to be revealing. The dress is made of a shiny material, which adds to its overall style. The animal print pattern on the dress is the main focus of the design, making it a unique and eye-catching piece. A blue and white striped dress. The dress is long and has a fitted style. It is a one-piece dress, and the woman is wearing black heels with it. The dress is not particularly revealing or conservative, but it is not overly sexy or modest either. It is a simple, patterned dress that is neither floral nor plain. The dress is elegant and casual, and it is made of a shiny material. The shirt is black and has a pocket and tailored button tab. It is a short sleeve shirt with a collar. The shirt is made of a fabric that is darker than the pocket and button tab. The shirt is designed to be conservative and modest, with a simple pattern.",
  "detail tee": ": Qualitative comparison on VITON-HD unpaired test set. From left to right: original image, agnostic-maskimage, captions, MGD input sketch, MGD-generated image, UniFashion input sketch and UniFashion (ours)-generated image. Our model is capable of generating images that align more accurately with the provided captionsand cloth sketch. For optimal viewing, please zoom in."
}