{
  "{daiqing.qi, shengli}@virginia.edu{hazhao, zwei}@adobe.com": "Please describe this photo in detail. GPT4: This photo features four mangosteens on a wooden table. Three of them are uncut, showing their round, dark purple exterior. The fourth mangosteen is cut open, revealing its white, fleshy interior, which is divided into segments. The table surface has a natural wood grain pattern and adds a rustic touch to the image. LLaVA: The image features a close-up of three small, round, and purple fruits, possibly pomegran-ates, sitting on a wooden surface. The fruits are placed in a row, with one on the left, one in the middle, and one on the right. In addition to the fruits, there is a knife located on the right side of the image, likely used for cutting the fruits. The scene appears to be a still life composition, showcas-ing the vibrant color and texture of the fruits. ShareGPT4V: The image captures a close-up view of four lychee fruits, their vibrant colors standing out against the wooden surface they rest on. The fruits are arranged in a square formation, with one at each corner. The top left and bottom right corners each hold a whole lychee, their green tops contrasting with the deep purple of the fruit. The top right corner features a lychee that has been cut open, revealing the white interior that is characteristic of this tropical fruit. The background, though blurred, hints at the presence of green leaves, adding a touch of nature to the scene. What type of fruit is this? InstructBLIP: Passion fruit BLIP2: rambutan GPT4: It is a mangosteen fruit Ours: The fruit in the image is a mangosteen LLaVA: Pomegranate ShareGPT4V: The fruit in the image is a dragon fruit, also known as a pitaya. Ours: The image features a close-up of several mangosteen fruits. There are four mangosteens in the scene, with one of them cut open, revealing the white flesh inside. The other three mango-steens are still intact, showcasing their unique texture and appearance. The fruits are placed on a wooden surface, which adds a natural and rustic touch to the scene. The close-up view high-lights the vibrant colors and intricate details of the mangosteen fruits. MM-Vet LLaVA-Wild MMB-CN MMB MME POPE SQA-Image VQAv2",
  "Ours": "LLaVA-1.5-7B InstructBLIP-7B BLIP2 Qwen-VL-Chat 78.578.2 41.0 68.3 61.060.5 85.985.3 1487.5 1293.8 64.3 60.6 36.0 58.356.7 23.763.4 60.938.1 33.2 30.5 26.2 22.4 79.7 68.2 75.489.3 1510.71540.0 68.5 64.0 66.8 : Examples on LLaVA-W (left), and quantitative comparison (right). Imprecise low-quality answersare marked in red and high-quality parts are marked in green. Popular open-source MLLMs fail to identifythe mangosteen (the first question), and list non-existent objects such as knife and incorrect quantities andarrangements, while ours correctly identify mangosteens with descriptions in detail.",
  "Abstract": "Despite recent advances in general visualinstruction-following ability of MultimodalLarge Language Models (MLLMs), when div-ing into low-level details, they still strugglewith critical problems when required to pro-vide a precise and detailed response to a visualinstruction: (1) failure to identify novel objectsor entities, (2) mention of non-existent objectsand (3) neglect of objects attributed details.Intuitive solutions include improving the sizeand quality of data or using larger foundationmodels. They show effectiveness in mitigat-ing these issues, but at an expensive cost ofcollecting a vast amount of new data and intro-ducing a significantly larger model. Standingin the intersection of them, we examine thethree object-oriented problems from the per-spective of the image-to-text mapping processby the multimodal connector. In this paper,we first identify the limitations of multimodalconnectors stemming from insufficient trainingdata. Driven by it, we propose to enhance themapping with retrieval-augmented tag tokens,which contain rich object-aware informationsuch as object names and attributes. With ourTag-grounded visual instruction tUNing with retrieval Augmentation, TUNA outperformsbaselines that share same language model andtraining data on 12 benchmarks. Furthermore,we show the zero-shot capability of TUNAwhen provided with specific datastores.",
  "Introduction": "Multimodal Large Language Models (MLLM)have witnessed remarkable progress recently (Chenet al., 2023c; Liu et al., 2023a, 2024; Bai et al.,2023; Chen et al., 2023a; Dai et al., 2023; Ye et al.,2023; Zhu et al., 2023a; Zhang et al., 2023), ex-hibiting superior ability in following vision-and-language instructions. Despite their effectivenessin providing general responses, their performanceoften degrade when required to give a detailed andaccurate answer to the question associated with animage with novel objects, named entities or com-plex scenes with rich and subtle details.Specifically, they frequently encounter chal-lenges () in: 1. identifying novel objectsand named entities, 2. preventing the generation ofobjects that do not align with the target images, and3. delivering a comprehensive description that cov-ers the details of the target images. We uncover the LLM Text Embedding SpaceCLIP Embedding Space Span of LLaVA Data-1.2MSpan of Datastore-15MSpan of LLM Space Connector MappingRetrieval Mapping SamplesRetrieved Samples : Top: the process of translating image embed-dings to text embeddings (LLaVA (Liu et al., 2024)).Bottom: Image classification accuracy of CLIP (Rad-ford et al., 2021) and MLLMs built on it.some of the potential causes of above challengesstarting from the commonly adopted two-branchstructure and the two-stage training paradigm ofMLLMs: the first-stage pre-training and second-stage supervised fine-tuning (SFT). Most existingMLLMs such as LLaVA (Liu et al., 2024) com-prise two modules: (1) a vision branch consistingof a vision encoder and a multimodal connector,and (2) a Large Language Model (LLM). In thepre-training stage with large-scale image-text pairs,the multimodal connector often learns to translatethe outputs of the vision encoder to text embed-dings, followed by the SFT stage which enhancesthe multi-modal instruction-following capabilitieswith instruction-format data.Despite the promising zero-shot capability ofthe vision encoder, such as CLIP (Radford et al.,2021), which is pre-trained with over 400M image-text pairs, its generalizability is bottlenecked by thelearnt mapping of the multimodal connector whenintegrated into the MLLM framework. E.g., in thecase of LLaVA (Liu et al., 2024), the two-stagetraining data is significantly smaller compared tothe pre-training data of its vision encoder CLIP(1.2M vs. 400M), as a result, the connector of-ten fails to effectively map the out-of-distribution(OOD) images to the corresponding LLM text em-beddings. Therefore, LLM fails to successfullyidentify image contents. MLLMs degradation onimage classification performance (Zhai et al., 2023)is a simple illustration. In (Bottom), anobvious classification performance gap betweenMLLMs and their frozen vision encoder (CLIP)is observed. The absence of similar classificationobjects in LLaVAs training data could be a criti-cal factor, which makes it particular hard for the multimodal connector to translate OOD CLIP em-beddings of test images to LLM text embeddings.One intuitive solution is to enrich the trainingdatasets with more image-text pairs, however, ashigh-quality instruction-format data is particularlycritical for visual instruction tuning (Chen et al.,2023c), it is very expensive to build high-qualitytraining data with hundreds of millions of image-text pairs of varying quality. Furthermore, the train-ing could also become exceedingly burdensome.Instead of directly improving the connector map-ping with heavy training, could we build anotherlightweight new mapping as a complementarythat effectively attends to objects, especially OODones? Motivated by retrieval augmented genera-tion (RAG) (Ramos et al., 2023b,a; Yang et al.,2023; Hu et al., 2023; Lin et al., 2024; Li et al.,2023c; Yasunaga et al., 2022), we propose a re-trieval mapping. As shown in (Top), whilethe connector fails to correctly map the sampleout of LLaVA training data span to its correspond-ing text embedding in LLM embedding space (i.e.,the blue triangle sample is incorrectly mapped tothe yellow square sample) , we introduce a large-scale external datastore with a better coverage ofnovel objects, named entities, and attributes, for theretrieval of useful knowledge towards the input im-age. In this way, a new retrieval mapping could bebuilt from the input image to corresponding LLMtext embeddings (green dashed line in ).While most existing works retrieve relevant cap-tions as extra knowledge, it may not apply herebecause all three challenges mentioned above areoriented with object, where cleaner object-awareknowledge is urgent, instead of noisy captions.Therefore, we want to retrieve tags of the imagesthat are similar to the input image as extra knowl-edge, where we can further enrich each tag repre-sentation with image region feature and adaptiveweights to fulfill the potential of useful tags. To thisend, we introduce a Tag-grounded visual instruc-tion tUNing with retrieval Augmentation, termedTUNA, that performs a knowledge-aware and tag-grounded generation. With grounded tags, TUNAis effective in identifying novel objects, named enti-ties, and generate tag-oriented response which paysmore attention to image details.We summarize our contributions as follows: (i)We identify potential factors hindering MLLMsand first propose a tag-grounded visual instructiontuning with retrieval-augmentation (TUNA) withenhanced knowledge on novel objects, more atten- tion to details, and less mention of non-existentobjects. (ii) To fulfill the potential of tags, We care-fully designed the image-aware tag encoder, whichproduces tag embeddings enhanced by image fea-tures with an adaptive weight. (iii) We evaluateTUNA on extensive benchmarks along with a se-ries of qualitative results, and show its zero-shotcapability when provided with specific datastores.",
  "Related Works": "Multimodal Large Language Models. MLLMsevolve rapidly nowadays. With LLMs, while ex-isting works (Li et al., 2022, 2023d) enable ba-sic visual tasks like visual question answering,more recent works (Chen et al., 2023a; Liu et al.,2024) shows proficiency in image-text dialoguesthrough alignment and fine-tuning. Subsequent re-search (Bai et al., 2023; Chen et al., 2023b; Daiet al., 2023; Li et al., 2023a; Peng et al., 2023;Ye et al., 2023; You et al., 2023) enhances LLMsby emphasizing data quality and diversity. Withgrounding data, a branch of works (Ye et al., 2023;You et al., 2023; Chen et al., 2023b; Peng et al.,2023) improves LLMs grounding capability. De-spite their evolution, as they share a similar multi-modal connector module that performs image-to-text translation, a lingering fundamental problempersists: Out-of-distribution (OOD) images, suchas novel objects, named entities, new scenes, etc.,cannot be translated to text embeddings effectively,leading to misaligned answers, missing details ormention of non-existent objects from LLM. Retrieval-Augmented Multimodal Learning.Retrieval-augmented language generation (RAG)consists of conditioning generation on additionalinformation that is retrieved (e.g., with cluster-ing (Zhao et al., 2017)) from an external datas-tore. Recently, A branch of works (Ramos et al.,2023b,a; Yang et al., 2023; Hu et al., 2023; Linet al., 2024; Li et al., 2023c) integrate it into imagecaptioning, where relevant captions are retrievedto guide the captioning. Distinct from them, in vi-sual instruction tuning, where detailed and denseresponses based on the multimodal instructionsare often required, cleaner object-level informa-tion, such as names and attributes of novel objects,named entities, is urgent. We provide a more de-tailed discussion in Appendix A.Multimodal Learning with Tags.Existingworks (Huang et al., 2023; Zhou et al., 2020; Liet al., 2020; Hu et al., 2021; Qi et al., 2024a; Huang et al., 2022) show the effectiveness of introducingobject tags as anchor points to help the learningof semantic alignments between images and textsin the training data. In the context of , theybetter align in-distribution data (yellow and pur-ple samples) with tags. Our goal is distinctivefrom them in that, We do not aim to learn betterrepresentations of training data, instead, we wantto (1) improve the tag-grounded generation capa-bility of MLLMs and (2) acquire new knowledgewith retrieved tags from external datastore. Be-sides, as they treat object tags as anchor pointsfor feature learning, tags are commonly human-used ones (Huang et al., 2023) as guidance. Forinstance, Tag-to-Text (Huang et al., 2023) collects3,429 well-used tags filtered by human annotation.While in our case, where the large coverage is thepriority, less frequently used tags (e.g., named enti-ties) are also desired, resulting in a total of 3M tags(details in Appendix A).",
  "Multimodal Retriever": "From Captions to Tags. As introduced in Sec. 1,one of the fundamental challenges for MLLMs isto effectively translate image tokens to LLM textembeddings, especially for OOD images that con-tain novel objects. With better translation, LLMswould be less likely to confuse with them, whichcould improve the identification of objects. Thus inaddition to the mapping learnt by the connector, weuse a multimodal retriever to retrieve relevant infor-mation as an additional retrieval mapping ()to enhance the translation process. Therefore, thequality of the retrieval mapping is critical. As aresult, object-oriented tags as retrieved informa-tion would be very helpful. Additionally, with tag-grounded generation, retrieved tags also serve asgroundings or hints, which could prompt the LLMto generate tag-aware contents if the tag is relevantto the input image, which would also be helpful inalleviating missing objects or visual details.Towards this end, we use CLIP image embed-dings from image-text paired datasets as keys andcorresponding tags as values. However, existing Caption: Close up - The front left side of a tan American Pit Corso puppy that is laying in a lawn and it is looking to the left.Results from Parser:Entity (Attributes): puppy (tan, Pit Corso, looking to the left) | lawnRelations: puppy (Subject) - lay in (Relation) - lawn (Object)Results from NER: American | Pit CorsoTags: ['puppy', 'Pit Corso', 'lawn', 'American', 'tan', 'looking to the left'] Caption: Montepulciano, Italy - August 25, 2013: Old narrow street in the center of town with colorful facades. stock pho-tography.Results from Parser:Entity (Attributes): facades (colorful) | lawn (narrow,old ) | townRelations: street (Subject) - in center of (Relation) - town (Ob-ject)Results from NER: Montepulciano | Italy Tags: ['facades', 'narrow', 'town', 'Italy', 'colorful', 'street', 'old', Montepulciano ]",
  ": Extracted tags from CC3M and CC12M": "large-scale image-text datasets such as Concep-tual Captions (Sharma et al., 2018; Changpinyoet al., 2021) only contain captions. To mine tagsfrom texts, we parse each caption into a set oftags with a combination of FACTUAL scene graphparser (Li et al., 2023f) and Name Entity Recogni-tion (NER) with spaCy, yielding 3M tags extractedfrom 15M captions in CC3M (Sharma et al., 2018)and CC12M (Changpinyo et al., 2021). We showseveral examples in . Details of the miningprocess are available in Appendix B. We also pro-vide a statistics of the obtained tags in Tab 1.Datastore and Cross-Modal Retrieval. With pro-cessed image-tags pairs, our datastore is indexedby FAISS library (Johnson et al., 2019) with imageCLIP embeddings as keys and associated tags asvalues. Given a query image, a k-nearest neighborretrieval with cosine similarity of embeddings be-tween it and datastore images is performed. Thetags of top-k retrieved images are input to TUNA asadditional knowledge. In experiments, we use k=5.We consider CC12M (Changpinyo et al., 2021),CC3M (Sharma et al., 2018) and COCO (Lin et al.,2014) training set as our datastore, resulting in 15Mimage-text pairs. In experiments, we use a wholecombination, as well as parts of them, as our datas-tore to study how different datastores affect results.For Fashion QA, we use a combination of fashiondata as our retrieval datastore.",
  "TUNA": "Architecture.The framework of TUNA is il-lustrated in .Given a language instruc-tion Xq, and an input image Xv, a set of imageswith associated tags are retrieved from the data-store.Assume there are M tags in total, theyare mixed together and denoted as {Xit}Mi=1. Forimage, a frozen pre-trained CLIP vision encoderViT-L/14 is employed to extract the visual featureZv = g(Xv) R[HW]D, followed by a MLP multimodal connector h() that translates the CLIPvision feature to text embeddings: Hv = h(Zv).Similar to LLaVA (Liu et al., 2024), the grid vi-sual features before the last Transformer layer areconsidered in our experiments. The language in-struction Xq is tokenized and projected to text em-beddings Hq by the pre-trained LLMs tokenizerand embedding layer. Specifically, tags {Xit}Mi=1are encoded by our image-aware tag encoder. Image-Aware Tag Encoder. Given a tag Xit, itstag representation Hi, which is encoded by ourimage-aware tag encoder, is a tuple of its text em-bedding Hit and the its tag-aware image token (em-bedding) Hivt, which contains visual features of theinput query image related to this tag. With this im-age token, LLM could better attend to details of thetag-related object in the input image. Same withXq, the tag Xit is tokenized and projected to Hitwith the LLMs tokenizer and embedding layer. Toobtain the tag-aware image token, the tag-aware im-age feature Zivt R1D is first extracted from thegrid visual features of the input image via the cross-attention module: Zivt = Cross-Att(Qit, Zv, Zv) =softmax( Qit Zv",
  "D )Zv , where Qit R1D is the": "global CLIP text feature of tag Xit, extracted by thefrozen CLIP text encoder. Then we obtain the tag-aware image token Hivt = h(Zivt). Finally, the tagrepresentation Hi consists of the tuple (Hivt, Hit).Iterating over all tags, we have {Hi}Mi=1. Adaptive Weight Tuner. As retrieved images maycontain less relevant or irrelevant tags, e.g., the tagdurian in Fig 4, we apply an adaptive weight tunerover them to give more attention to highly relevanttags while ignoring less related ones. Specifically,the score of Hi is the cosine similarity betweenQit and the global CLIP visual feature (i.e., the<CLS> token) of the input image. The scores arenormalized to as the final weights, which areapplied to Hivt and Hit before input to the LLM. Supervised Fine-Tuning. We consider Vicuna-7B (Chiang et al., 2023), a decoder-only LLMinstruction-tuned on top of LLaMA (Touvron et al.,2023), as our language model. We use both im-age and text encoders from CLIP-ViT-L/ initialize the pre-trained multimodal connec-tor from LLaVA-1.5 (Liu et al., 2023a). Duringthe instruction tuning, we always keep the weightsof the vision encoder frozen, and update both thepre-trained weights of the connector and the LLM.",
  "Experiment": "In this section, we first present the training detailsof TUNA and benchmarks. Then we introducequantitative and qualitative comparison with popu-lar open-source models, followed by detailed anal-ysis experiments and ablation studies. Training Details.TUNA is finetuned on in-struction data for one epoch, following existingworks (Liu et al., 2023a; Chen et al., 2023c).We consider two different instruction-followingdatasets in our experiments: LLaVA-665K (Liuet al., 2023a) and ShareGPT4V-665K (Chen et al.,2023c) as our instruction-following data duringfine-tuning separately, resulting in two versions ofour model, TUNA and TUNA+. ShareGPT4V-665K contains instruction-following data withhigher quality. Details on datasets are availablein Appendix C. We apply a learning rate of 2e-5and a batch size of 128. The training takes 1214hours with 8 A100 GPUs with ZeRO3. Details areavailable in Appendix C.",
  "Comparison with Baselines": "Main Results. In Tab. 2, we provide a quantitativecomparison of TUNA with popular open-sourceMLLMs. On 12 benchmarks, TUNA consistentlyoutperforms previous LLMs that are finetuned fromthe same instruction-tuning datasets as ours withthe same configuration on the vision encoder andlanguage model (Vicuna-7B), especially on recentmultimodal benchmarks with more notable im- provements. As the size of LLM and differentchoices of instruction-following data can signifi-cantly improve the model performance, we markthe models gray that are equipped with a larger13B language model or finetuned from currentlyunavailable datasets of higher quality and quan-tity. Specifically, LLaVA-1.6 (or LLaVA-NeXT)1 is finetuned from larger instruction-following dataof higher quality, with additional user instruct data.Besides, it equips the better vision encoder withdynamic high resolution, known as AnyRes (AR).Although it is not a fair comparison, we still out-perform LLaVA-1.6 in MMBCN, MMB and POPE,and the corresponding 13B models in MMBCN,MMB, POPE and LLaVA-W.How Can TUNA Improve the Recognition ofNovel Objects and Entities? As visualized in (Top), with our 15M large-scale datastore,the new retrieval mapping could greatly compen-sate for the original LLaVA multimodal connec-tor that learns from around 1M data. With theadditional mappings from retrieval data, TUNAis expected to show particularly improvementsover questions towards novel objects or entitiesin the given input image. We show sub-tasks fromMME (Fu et al., 2023) and MMB (Liu et al., 2023b)that consists of such questions in Tab. 3. We gainobvious improvements over the baseline in mostsub-tasks. We also show several VQA examplesfrom multimodal benchmarks in Fig 1 and Fig 5. InFig 1, all of the baselines fail to correctly identifythis fruit as a mangosteen, including LLaVA-1.5. Itis reasonable as mangosteens do not appear in theits training data, which makes it particularly hard",
  "Input ImageRetrieved Images(a)": ": VQA examples of TUNA. For each example,we show top 3 retrieved images to save space. We showall tag set associated with all retrieved images as wellas their tuned weights in heat map, where the brightestregion for the highest weight 1 and darkest region forthe lowest weight 0 (Zoom in for better view). Correctanswers are marked green and wrong ones in red. Moreexamples are available in Appendix E. for the connector to map it to somewhere close totext embeddings of mangosteen in the LLM em-bedding space, as illustrated in Fig 2. When thequestion about the given image is a little tricky, e.g.,in Fig 5 (a), the MLLM is asked if a painting of abuilding exists in the form of architecture, LLaVA-1.5 is confused on whether it is a real architecture ora painting. However, TUNA easily distinguished itfrom real architectures with additional knowledgefrom retrieved tags of similar images in datastore.",
  ": Results on POPE. We show most competingbaselines. Full table is available in Appendix F. TUNAoutperform Ferret (You et al., 2023), which is finetunedon grounding and referring data": "Intuitively, the retrieved images are very likely tocontain similar elements or objects to the input im-age. Therefore, the tags could be helpful to provideadditional hints to the LLM to pay special atten-tion to them about their existence. We evaluate ourmodel on POPE (Li et al., 2023e), a benchmark de-signed towards the existence of objects. Results areavailable in Tab. 4, we outperform competing base-lines including referring and grounding MLLMssuch as Ferret (You et al., 2023) and Shikra (Chenet al., 2023b). A simple example is shown in Fig 5(b). When the object is visually imperceptible inthe image, hints from tags are very helpful.",
  "How Can TUNA Attend to Rich Details withTags? While most of the traditional VQA bench-": "What is the name of this famous sight? Please describe this photo in detail. LLaVA: The image features a breathtaking view of the Diamond Head Crater, a rocky island situated in the middle of a body of water, possibly an ocean. The island is sur-rounded by a beautiful beach with a sandy shore, and it appears to be a popular tourist destination. In the distance, there are several boats scattered around the water, likely providing transportation or leisure activities for visitors. The scene is further en-hanced by the presence of a few people enjoying the beach, with some of them closer to the water's edge and others further back on the shore. The combination of the rocky island, the beach, and the boats creates a picturesque and serene atmosphere. Ours: The image presents an aerial view of Diamond Head, a jagged island with a coast-line that stretches along the water. The island is covered in green vegetation, and the water appears to be a bright blue. The coastline is lined with cliffs, giving the island a dramatic appearance. In the background, there are sever-al houses and buildings scattered across the island, indicating human presence and development. The com-bination of the island's natural beauty and the presence of human structures creates a captivating scene.",
  ": TUNA on LLaVA-W examples. Impreciselow-quality answers are marked in red and high-qualityparts are marked in green. TUNA does not mention non-existent objects and gives a more detailed description": "marks and multimodal benchmarks provide shortquestions answering pairs (Fig 5), LLaVA-W (Liuet al., 2023a) evaluates MLLMs capability of giv-ing long detailed response. Quantitative resultsare available in Tab. 5. TUNA consistently outper-forms baselines. We also provide one example in. While LLaVA mentions non-existent boats,people, TUNA accurately describes the water body,the existence of green vegetation, and interestingly,the presence of houses and buildings behind themountain (zoom in for better view). More inter-estingly, there are no retrieved noun tags directlyrelated houses or buildings. By removing tagsone by one, we finally identify that the tag acces-sible contributes to the the description of housesand buildings. It is an interesting phenomenon thatsomehow tells us that not only nouns can remindthe LLM the existence of objects, relevant adjec-tives can also teach the LLM to pay attention tovisual details. In this case, accessible means hu-man can access to this place, which might remindthe LLM the existence of houses and buildings.",
  "Ablation Study": "Ablation of Adaptive Weight Tuner. Groundedon tags, intuitively, the quality of tags is criticalto TUNA. However, retrieved tags could be noisy.E.g., the tag durian in Fig 4. To this end, we ap-ply an adaptive weight tuner in our image-awaretag encoder to allocate more weight to more rel-evant tags and less weight to less relevant ones.We first ablate the tuner module to show its effec-tiveness of this simple but critical component in",
  ": Results on LLaVA-in-the-Wild (LLaVA-W)Bench. Our model consistently outperforms baselinesthat share the same LLM and instruction tuning data": "alleviating the noises of tags. Without the adaptiveweight tuner, all retrieved tags would be equal im-portant and their weights are set to the maximumvalue. The result is shown in Tab. 6 (w/o tuner).A clear performance drop is observed comparedto the full method. It is reasonable because whilerelated tags can provide useful information to theLLM, the irrelevant tags are misleading. Althoughit underperforms the full method, without the tuner,our model is still comparable or slightly better thanLLaVA-1.5. This is favourable because it manifeststhat our model itself is somehow robust against lessrelevant tags without the tuner. Effectiveness of Instruction Tuning.SinceMLLMs are naturally in-context learners, we areinterested in the effectiveness of our tag-groundedfinetuning compared to the vanilla LLaVA-1.5,where tags are provided as in-context knowledge.For fair comparison, we apply the weight tuner toboth models. Lets refer this model as TUNA.Results in (w/o FT) indicates that, the LLMwithout tag-grounded instruction tuning cannotmake effective use of informative tags. Are Tags more Effective than Sentences? Wecompare TUNA with sentence-level retrieval inTab. 6 (w/ captions). Instead of tags, we finetuneTUNA with captions of retrieved images as addi-tional knowledge. The image-aware tag encoder isalso used, but the input tags are replaced by cap-tions. Results show that sentence-level retrieval isnot helpful. It is reasonable because tags providecleaner and more object-related knowledge such asnames, attributes, while captions are noisy. Would Irrelevant Tags Hurt the Backbone dur-ing Inference? It is intuitive that a large-scale data-store often covers useful knowledge to the inputimage and question. Therefore, useful tags couldbe retrieved. However, there might be corner caseswhen retrieved tags are all irrelevant. To this end,we run experiments without tags and with randomtags. Results are reported in Tab. 6. With irrel-evant tags, TUNA is comparable to its backbone",
  "LLaVA-1.5. It manifests that, our method notablyimproves the backbone performance with usefultags and will not hurt the backbone performancewhen only irreverent tags are available": "Different Choices of Datastore. We also studyhow different choices of datastores can affect themodel performance. In the default setting, we usea combination of CC12M, CC3M and COCO train-ing set. In addition, we perform the tag-groundedinstruction tuning with different datastores, and usethem for retrieval during inference, respectively.Results are available in Tab. 7. Default setting withlargest datastore size outperforms other baselines.We have detailed analysis in Appendix H.",
  "Zero-shot Inference on Fashion Domain": "Fashion-Bench. To study TUNA on OOD datafrom another specific domain, we further collectdata from FashionGen (Rostamzadeh et al., 2018)validation set and create a benchmark to measurethe models instruction-following capability in fash-ion domain, similar to LLaVA-Bench. Follow-ing LLaVA, we also collect a set of 24 imagesfrom FashionGen, with one question associatedwith each image. The questions are from one ofthe three types: (1) Conversation. We design aconversation between the assistant and a personasking questions about the product, including theproduct brands, categories, materials, etc. Onlyquestions that have definite answers are considered.E.g., What is the brand of this product?(2) Detailed Description. We ask the assistant togive a comprehensive and detailed desperation ofthe given product. E.g., Please describe theproduct in this image in detail. (3) Com-plex Reasoning. The above two types focus on the",
  "Question: Please describe this product in detail, including its brand, category, etc": "LLaVA-1.5: The brand is called \"H&M.\" The product is a pair of black and white pants with a logo on the side. The design is casual and comfortable, making them suitable for everyday wear. The pants are made of a blend of materials, providing both style and functionality. The logo on the side adds a touch of personality and brand recognition to the pants. Ours: The product is a pair of black and white pants with a rubber and terry cloth applique. The pants are designed with a three-pocket styling and an elasticized waistband with a drawstring closure at the interior. The brand is KTZ, and the pants are made of 100% cotton. The design and composition of the pants suggest a casual and comfortable style, making them suitable for various occasions.",
  "Input ImageRetrieved Images": ": TUNA examples. For each example, we show top 5 retrieved images. We show the entire tag setassociated with all retrieved images as well as their tuned weights in heatmap, where the brightest region for thehighest weight 1 and darkest region for the lowest weight 0 (Zoom in for better view). Correct and precise answersare marked green while vague or wrong ones in red.",
  ": Results on Fashion-Bench. Sentence-level RAGrefers to using retrieved captions as in-context prompts forLLaVA-v1.5-7B": "Results. We use a combination of fashion dataas our retrieval datastore, including:Fashion-Gen (Rostamzadeh et al., 2018) training set, Fash-ion200k (Han et al., 2017) and PolyvoreOut-fits (Vasileva et al., 2018), resulting in a total of546.5K image-text pairs. We extract tags of a prod-uct from captions. Results in Tab. 8 demonstratesthe effectiveness of TUNA.",
  "Conclusion": "In this paper, we discussed three challenges forMLLMs: (1) mention of non-existent objects, (2)neglect of visual details and (3) failure to iden-tify novel objects and entities, and one of the po-tential causes: the bottleneck from the image-to-text translation. To alleviate these problems, weintroduced TUNA, a tag-grounded visual instruc-tion tuning framework with retrieval-augmentation,which achieves competing performance over 12VQA and multimodal benchmarks, compared tobaselines with the same LLM and finetuning data.",
  "Limitations": "Being lightweight and effective, our model couldbe easily further improved with simple modifica-tions to overcome existing limitations. Our modelis bottlenecked by the capability of CLIP (Radfordet al., 2021), which can affect our model perfor-mance in two ways. First, the quality of retrievedimages are highly related to it. As we use tagsassociated to the retrieved images as additional in-formation, more relevant images we have, more rel-evant tags we obtain. Second, our adaptive weighttuner also relies on the knowledge of CLIP. Forinstance, even if we obtain a highly relevant tag,e.g., Diamond Head from the retrieved similarimages, if image-text pairs containing DiamondHead do not exist in the 400M pre-training data ofCLIP, CLIP cannot effectively align the text embed-dings of Diamond Head to a photo of diamondhead, subsequently, low weights would be assignedto the tag Diamond Head in our weight tuner,even though it is the ground truth. Fortunately inmost cases, CLIP is capable of handling it. If not,we can easily replace CLIP with a more powerfulvision-language models.Our current design of the retriever is also sim-ple, where we retrieve images regardless of thelanguage instruction. A solution could be using Q-former (Li et al., 2023d), where instruction-awarevisual features could be used for retrieval. We leavethem for future work.",
  "The work is in part supported by the National Sci-ence Foundation under Grants IIS-2316306 andCNS-2330215, and a gift from Adobe": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966. Soravit Changpinyo, Piyush Sharma, Nan Ding, andRadu Soricut. 2021. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tailvisual concepts. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 35583568.",
  "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023b. Shikra: Unleashingmultimodal llms referential dialogue magic. arXivpreprint arXiv:2306.15195": "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-ghui He, Jiaqi Wang, Feng Zhao, and DahuaLin. 2023c.Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprintarXiv:2311.12793. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023). Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Preprint,arXiv:2305.06500. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, et al. 2023. Mme: A comprehensiveevaluation benchmark for multimodal large languagemodels. arXiv preprint arXiv:2306.13394. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey PBigham. 2018. Vizwiz grand challenge: Answeringvisual questions from blind people. In Proceedings ofthe IEEE conference on computer vision and patternrecognition, pages 36083617. Xintong Han, Zuxuan Wu, Phoenix X Huang, XiaoZhang, Menglong Zhu, Yuan Li, Yang Zhao, andLarry S Davis. 2017. Automatic spatially-aware fash-ion concept discovery. In Proceedings of the IEEEinternational conference on computer vision, pages14631471. Xiaowei Hu, Xi Yin, Kevin Lin, Lei Zhang, JianfengGao, Lijuan Wang, and Zicheng Liu. 2021. VIVO: vi-sual vocabulary pre-training for novel object caption-ing. In Thirty-Fifth AAAI Conference on ArtificialIntelligence, AAAI 2021, Thirty-Third Conferenceon Innovative Applications of Artificial Intelligence,pages 15751583.",
  "Yuncheng Hua, Yuan-Fang Li, Guilin Qi, Wei Wu,Jingyao Zhang, and Daiqing Qi. 2020.Less ismore: Data-efficient complex question answeringover knowledge bases. Journal of Web Semantics,65:100612": "Xinyu Huang, Youcai Zhang, Ying Cheng, Weiwei Tian,Ruiwei Zhao, Rui Feng, Yuejie Zhang, Yaqian Li,Yandong Guo, and Xiaobo Zhang. 2022. IDEA: in-creasing text diversity via online multi-label recog-nition for vision-language pre-training. In MM 22:The 30th ACM International Conference on Multime-dia, Lisboa, Portugal, October 10 - 14, 2022, pages45734583. ACM. Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian,Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo,and Lei Zhang. 2023.Tag2text: Guiding vision-language model via image tagging. arXiv preprintarXiv:2303.05657. Drew A Hudson and Christopher D Manning. 2019.Gqa: A new dataset for real-world visual reasoningand compositional question answering. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 67006709.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023d. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understandingand generation. In International Conference on Ma-chine Learning, pages 1288812900. PMLR. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,Xiaowei Hu, Lei Zhang, Lijuan Wang, HoudongHu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-languagetasks. In Computer VisionECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XXX 16, pages 121137. Springer.",
  "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023e. Eval-uating object hallucination in large vision-languagemodels. arXiv preprint arXiv:2305.10355": "Zhuang Li, Yuyang Chai, Terry Zhuo Yue, LizhenQu, Gholamreza Haffari, Fei Li, Donghong Ji, andQuan Hung Tran. 2023f. Factual: A benchmark forfaithful and consistent textual scene graph parsing.arXiv preprint arXiv:2305.17497. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. 2014.Microsoft coco:Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer. Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexan-dru Coca, and Bill Byrne. 2024. Fine-grained late-interaction multi-modal retrieval for retrieval aug-mented visual question answering. Advances in Neu-ral Information Processing Systems, 36.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024. Visual instruction tuning. Advances inneural information processing systems, 36": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, et al. 2023b. Mm-bench: Is your multi-modal model an all-aroundplayer? arXiv preprint arXiv:2307.06281. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521.",
  "Rita Ramos, Desmond Elliott, and Bruno Martins.2023a.Retrieval-augmented image captioning.arXiv preprint arXiv:2302.08268": "Rita Ramos, Bruno Martins, Desmond Elliott, and YovaKementchedjhieva. 2023b. Smallcap: lightweightimage captioning prompted with retrieval augmenta-tion. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages28402849. Negar Rostamzadeh, Seyedarian Hosseini, Thomas Bo-quet, Wojciech Stokowiec, Ying Zhang, ChristianJauvin, and Chris Pal. 2018. Fashion-gen: The gen-erative fashion dataset and challenge. arXiv preprintarXiv:1806.08317. Piyush Sharma, Nan Ding, Sebastian Goodman, andRadu Soricut. 2018. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic im-age captioning. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 25562565. Heng-Shiou Sheu, Zhixuan Chu, Daiqing Qi, and ShengLi. 2022. Knowledge-guided article embedding re-finement for session-based news recommendation.IEEE Transactions on Neural Networks and Learn-ing Systems, 33(12):79217927. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 83178326. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Mariya I Vasileva, Bryan A Plummer, Krishna Dusad,Shreya Rajpal, Ranjitha Kumar, and David Forsyth.2018. Learning type-aware embeddings for fashioncompatibility. In Proceedings of the European con-ference on computer vision (ECCV), pages 390405. Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick,and Anton Van Den Hengel. 2016. What value doexplicit high level concepts have in vision to languageproblems? In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages203212. Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Kor-thikanti, Weili Nie, De-An Huang, Linxi Fan, Zhid-ing Yu, Shiyi Lan, Bo Li, et al. 2023.Re-vilm:Retrieval-augmented visual language model for zeroand few-shot image captioning.arXiv preprintarXiv:2302.04858. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,Rich James, Jure Leskovec, Percy Liang, Mike Lewis,Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-augmented multimodal language modeling. arXivpreprint arXiv:2211.12561. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,Ming Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.mplug-owl: Modularization empowers large lan-guage models with multimodality. arXiv preprintarXiv:2304.14178. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-FuChang, and Yinfei Yang. 2023. Ferret: Refer andground anything anywhere at any granularity. arXivpreprint arXiv:2310.07704. Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,and Jiebo Luo. 2016. Image captioning with seman-tic attention. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages46514659. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and LijuanWang. 2023. Mm-vet: Evaluating large multimodalmodels for integrated capabilities. arXiv preprintarXiv:2308.02490.",
  "A.1Retrieval-Augmented MultimodalLearning": "We are distinct from existing works on retrieval-augmented multimodal learning (Ramos et al.,2023b,a; Yang et al., 2023; Hu et al., 2023; Linet al., 2024; Li et al., 2023c) in that we are moti-vated from the object-oriented challenges in visualinstruction tuning, which leads to notable differ-ences in (1) target task, (2) motivations, (3) re-trieved knowledge and (4) usage of additional in-formation.Most existing works above focus on image cap-tioning, where short captions (usually one or twosentences) are generated given an input image.While in our case, our model is asked to followthe given instruction, infer from the given image,and often provide a long and detailed response.The difference of tasks therefore lead to differentchallenges, thus the motivation of using retrieval-augmentation is also distinct. While existing mod-els exploit retrieved captions for general purposesof providing related contents to help the captioningof the current image (e.g., help to better organizethe language, or provide additional knowledge onimage content or context), in our scenario, the re-trieved tags aim to provide rich object-aware in-formation to enhance the attention to object de-tails, and help with the object or entity identifica-tion. Moreover, the capability of performing tag-grounded generation is enabled during our visualinstruction tuning. In addition, we have meticu-lously crafted novel modules aimed at enrichingthe representation of retrieved tags and adaptivelyreallocating the attention to them based on theirrelevance.",
  "A.2Multimodal Learning with Tags": "We are distinct from existing works (Huang et al.,2023; Zhou et al., 2020; Li et al., 2020; Hu et al.,2021; Huang et al., 2022) that introduce object tagsas anchor points to help the learning of semanticalignments between images and texts in (1) sub-stantially different objectives, (2) type of used tagsand (3) the usage of them.Existing works (Huang et al., 2023; Zhou et al., 2020; Li et al., 2020; Hu et al., 2021; Huang et al.,2022) use tags for the representation learning ofsemantic alignments between images and texts. Forinstance, OSCAR (Li et al., 2020) propose to useobject tags to align the object-region features in the pre-trained linguistic semantic space. Wu etal. (Wu et al., 2016) utilize solely the predicted ob-ject tags as input to an LSTM for image captioning,whereas You et al. (You et al., 2016) incorporateboth tags and region features. In contrast, Zhouet al. (Zhou et al., 2020) augment region featureswith the object prediction probability vector, lever-aging salient regions identified by object detectors,to enrich the visual input for pre-training. In ourcase, object-oriented tags are used as groundings toprovide additional information on the given inputimage, therefore alleviating neglect of object de-tails and failure to identify novel objects or entities.Besides, the capability of tag-grounded instruction-following in our model is also unique. The largeand abundant annotation-free tags we have (around3.2M) also makes our work distinctive from theabove. As we want to inform our model of more rel-evant object-oriented knowledge like object names,object attributes while ignoring less relevant ones,we also design new modules towards this end.",
  "A.3Continual Learning of Multimodal LargeLanguage Models": "Continual Learning aims to continuously learna model from new data in different manners,such as class-incremental (Qi et al., 2023), data-incremental (Sheu et al., 2022; Hua et al., 2020) anddomain-incremental (Qi et al., 2024b; Zhu et al.,2023b, 2024).Zhai et al. (Zhai et al., 2023) studies the con-tinual learning of multimodal large language mod-els in the context of object classification. Theydemonstrate that the finetuned popular open-sourceMLLMs, such as LLaVA (Liu et al., 2024), exhib-ited degraded performance compared to their pre-trained frozen vision encoders, such as CLIP (Rad-ford et al., 2021). It is an example of the prob-lem caused by the misalignment between the CLIPembeddings of the input image and the LLM textembeddings, as we illustrated in the IntroductionSection.",
  "BTag Mining": "To mine tags from texts, we parse each captioninto a set of tags with a combination of FAC-TUAL scene graph parser (Li et al., 2023f) andNamed Entity Recognition (NER) with spaCy,yielding 3M tags extracted from 15M captions inCC3M (Sharma et al., 2018) and CC12M (Chang-pinyo et al., 2021). We show several examples in Caption: Close up - The front left side of a tan American Pit Corso puppy that is laying in a lawn and it is looking to the left.Results from Parser:Entity (Attributes): puppy (tan, Pit Corso, looking to the left) | lawnRelations: puppy (Subject) - lay in (Relation) - lawn (Object)Results from NER: American | Pit CorsoTags: ['puppy', 'Pit Corso', 'lawn', 'American', 'tan', 'looking to the left'] Caption: Montepulciano, Italy - August 25, 2013: Old narrow street in the center of town with colorful facades. stock pho-tography.Results from Parser:Entity (Attributes): facades (colorful) | lawn (narrow,old ) | townRelations: street (Subject) - in center of (Relation) - town (Ob-ject)Results from NER: Montepulciano | Italy Tags: ['facades', 'narrow', 'town', 'Italy', 'colorful', 'street', 'old', Montepulciano ]",
  "C.1Datasets": "LLaVA-665K (Liu et al., 2023a) is collected andbuilt with a variety of datasets, containing VQA,OCR, region-level VQA, visual conversation andlanguage conversation data. In ShareGPT4V (Chenet al., 2023c), the supervised fine-tuning captionswere collected from GPT4-Vision. Following Chenet al. (Chen et al., 2023c), a corresponding portionof detailed captions in the Supervised Fine-Tuning(SFT) datasets (i.e., LLaVA-665K) is replaced witha selection from the 100K GPT4-Vision-generatedcaptions.",
  "D.1Fashion-Bench": "To explore the effectiveness of TUNA on OODdata from another specific domain, we further col-lect data from FashionGen (Rostamzadeh et al.,2018) validation set and create a benchmark tomeasure the models instruction-following capa-bility in Fashion domain. Following LLaVA (Liuet al., 2024), we leverage GPT-4 to measure thequality of generated responses. Specifically, wecreate triplets consisting of image, ground-truthtextual descriptions, and question. The candidatemodels (e.g., TUNA, LLaVA) predict the answersbased on the question and the image. To provide anapproximate upper bound, we build a reference pre-diction based on the question and the ground-truthtextual descriptions, using the text-only GPT-4, fol-lowing Liu et al. (Liu et al., 2024) . After obtainingthe responses from both models, we feed the ques-tion, visual information (in the format of textualdescriptions), and the generated responses fromboth assistants, to the judge (i.e., text-only GPT-4).The text-only GPT-4 evaluates the helpfulness, rele-vance, accuracy, and level of detail of the responsesfrom the assistants, and gives an overall score on ascale of 1 to 9, where a higher score indicates betteroverall performance. We report relative scores w.r.t.the text-only GPT-4 model that uses the texturalground truth description as visual input.Similar to LLaVA-Bench (In-the-Wild) (Liuet al., 2024), we also collect a set of 24 imagesfrom FashionGen (Rostamzadeh et al., 2018) vali-dation set, with one question associated with eachimage. The questions are from one of the threetypes: 1. Conversation. We design a conversation be-tween the assistant and a person asking ques-tions about the product. A diverse set of ques-tions are asked about the content of the image,including the product brands, categories, ma-terials, etc. Only questions that have definiteanswers are considered. E.g., What is thebrand of this product?",
  ": Results on Fashion-Bench. Our model consis-tently outperforms the baseline": "We use a combination of fashion data asour retrieval datastore,including:Fashion-Gen (Rostamzadeh et al., 2018) training set, Fash-ion200k (Han et al., 2017) and PolyvoreOut-fits (Vasileva et al., 2018), resulting in a total of546.5K image-text pairs. To obtain the tags of aproduct, we extract them from the caption or as-sociated product specifications (e.g., brand) of theproduct.Results in Tab. 10 demonstrates the effectivenessof TUNA, especially on Conversation and De-tail, where retrieved tags on product specificationsare very helpful to identify the related details of theinput product. Examples are available in and.",
  "EMore Examples": "We present more examples with TUNA andLLaVA-1.5 in and . In , weprovide Out-of-Distribution (OOD) images of real-world products or television works, and ask TUNAand LLaVA-1.5 to provide answers to the question.In , we provide Out-of-Distribution (OOD)images in fashion domain, and ask the models toprovide answers to the question.When provided with OOD images, where novelobjects or entities often appear, LLaVA-1.5 fails tocorrectly or precisely identify them due to a limitednumber of training samples. Although the CLIP vi-sion encoder, which is pre-trained with over 400Msamples, can effectively extract their visual fea-tures, the multimodal connector cannot effectivelymap them to text embeddings input to the LLM.In contrast, TUNA is effective in identifying un-seen objects or entities, as the input OOD imageis directly mapped to a set of retrieved tags froma large-scale external datastore, which has a bettercoverage of OOD data. In examples in , where specific in-domainknowledge, i.e., fashion domain, is required forgive a detailed and precise description of the givenproduct, such as its brand, design, or composition(material), LLaVA fails to correctly identify themor response with detailed descriptions on them.For instance, in the example in Fig 9, the onlyuseful information about the given product it-self is a black jacket with white polka dots,where LLaVA-1.5 fails to precisely describe it asa blazer. Moreover, LLaVA-1.5 does not men-tion its design and brand even if we explicitly askit the brand of this product. In contrast, TUNAprecisely describes its design details, style and thebrand, benefiting from the retrieved products whichare similar to the input product in design, brand,category or style. TUNA could effectively refer tothe retrieved tags and learn from the useful oneswith our tag encoder.Cases are similar in examples from Fig 11,where TUNA correctly identifies the novel objectin the input image with retrieved knowledge. Mean-while, LLaVA-1.5 fails to identity the model of theLeica camera, Porsche car, and the name of thecharacter and anime in the input images.",
  "GBenchmarks": "We compare TUNA with SoTA methods on 12benchmarks, including five VQA benchmarks:VQAv2 (Goyal et al., 2017), GQA (Hudsonand Manning, 2019), VizWiz (Gurari et al.,2018), ScienceQA-Image (SQAI) (Lu et al., 2022),TextVQA (VQAT) (Singh et al., 2019), and sevenmore recently multimodal benchmarks designedfor LLMs: POPE (Li et al., 2023e), MME (Fu etal., 2023), MMBench (MMB) (Liu et al., 2023b),MMBench-Chinese (MMBCN) (Liu et al., 2023b),SEED (Li et al., 2023b), LLaVA-in-the-Wild(LLaVAW) (Liu et al., 2023a), and MM-Vet (Yuet al., 2023).VQAv2 (Goyal et al., 2017) and VizWiz (Gu-rari et al., 2018) are benchmarks for traditional Vi-sual Question Answering (VQA) tasks. MME (Fuet al., 2023) evaluates LLMs assesses and cogni-tion capabilities through a wide range of carefullycrafted questions across 14 sub-tasks. MMBench(MMB) and MMBench-Chinese (MMBCN) (Liuet al., 2023b) benchmarks manually design ques-",
  "attractive choice for the man": "Ours: The image features a man wearing a pair of black pants with a blue design. The pants are part of the Calvin Klein Collection, which is known for its stylish and fashionable clothing. The pants are made of cotton and have a slim-fit design, making them a popular choice for both casual and formal occasions. The man is standing in front of a white background, showcasing the pants' appearance and design.",
  "HAnalysis on Choices of Datastores": "From Tab. 6 and previous analysis we know thatthe quality of retrieved tags is critical. Therefore,the datastore, where the images are (with corre-sponding tags) retrieved from is crucial. Here westudy how different choices of datastores can affectthe model performance. In the default setting, weuse a combination of CC12M (Changpinyo et al.,2021), CC3M (Sharma et al., 2018) and COCOtraining set (Lin et al., 2014). Two of the threeretrieval datasets, CC3M and the COCO trainingset, share overlaps with the LLaVA training data,which is a frequent scenario in retrieval-augmentedgeneration, where a datastore with full or partialoverlap with the training data is common (Ramos et al., 2023b,a; Yang et al., 2023; Hu et al., 2023;Lin et al., 2024; Li et al., 2023c). While CC12Mand CC3M are different in size but similar in con-tent style, COCO is different from them in bothsize and content. CC12M and CC3M consist ofweb image-text pairs, where the variance in captionquality and style is more significant. In COCO,captions are human-written, where the languagestyle is more coherent, usually a short and plaindescription of the image. Consequently, tags ex-tracted from COCO captions are often commonlyused words and phrases and are very general, forinstance, boy, girl, plane and train, etc.It can provide the existence of objects in the im-age, which might help to alleviate the mention ofnon-existent objects. However, it is hard help toimprove object or entity identification as these com-monly seen phrases are very likely to be alreadyincluded in LLaVA training data and new retrievalmappings cannot be established. On the contrary,CC12M and CC3M provide an ocean of novel ob- jects and entities, which could greatly improve theimage-to-text translation process with additionalnew retrieval mappings built from them.We are curious to see how different datastoresize and datastore style can influence our modelperformance. In additional to the default setting,we perform the tag-grounded instruction tuningwith different datastores, and use them for retrievalduring inference, respectively. Results are availablein Tab. 7.It is not surprising that the default setting withlargest datastore size consistently outperformsother baselines. In most cases, the baseline withCC12M is the second best one while the onewith COCO training set performs worst, exceptfor on POPE. This is because POPE is built withCOCO validation set, which shares the same styleof the COCO training set. On other multimodalbenchmarks, the improvements with COCO train-ing set is less than CC12M and CC3M. Particu-larly, in LLaVA-in-the-Wild (LLaVA-W) bench-mark, where all test images are not overlapped withCOCO training and validation set, COCO trainingset as datastore does not help at all."
}