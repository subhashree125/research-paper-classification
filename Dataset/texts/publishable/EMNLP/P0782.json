{
  "Abstract": "We investigate a surprising limitation of LLMs:their inability to consistently generate text ina users desired language. We create the Lan-guage Confusion Benchmark (LCB) to evaluatesuch failures, covering 15 typologically diverselanguages with existing and newly-created En-glish and multilingual prompts. We evaluatea range of LLMs on monolingual and cross-lingual generation reflecting practical use cases,finding that Llama Instruct and Mistral modelsexhibit high degrees of language confusion andeven the strongest models fail to consistentlyrespond in the correct language. We observethat base and English-centric instruct modelsare more prone to language confusion, whichis aggravated by complex prompts and highsampling temperatures. We find that languageconfusion can be partially mitigated via few-shot prompting, multilingual SFT and prefer-ence tuning. We release our language confu-sion benchmark, which serves as a first layer ofefficient, scalable multilingual evaluation.1",
  "Introduction": "Large language models (LLMs) are increasinglyused in a variety of applications across the globe(Kaddour et al., 2023). While early LLMs focusedon English (Joshi et al., 2020; Hu et al., 2020),recent models are more multilingual. Nevertheless,LLMs do not provide equal utility to non-Englishspeakers due to higher latency, increased costs, andreduced performance (Ahia et al., 2023; Asai et al.,2023; Held et al., 2023).To be useful, an LLM must understand intentand provide a response that is appropriate in bothform, e.g., correct grammar, style, tone, register,and content, e.g., truthful, coherent, concise (Grice,",
  ": Language Confusion can occur at the wordlevel, line level, or over the entire output response": "1975; Wilson and Sperber, 2012). While content-related issues such as hallucinations have attractedsubstantial attention (Ji et al., 2023; Bang et al.,2023), they are often subtle and difficult to evaluate(Gudibande et al., 2024; Hosking et al., 2024), par-ticularly in multilingual settings (Guerreiro et al.,2023). Form-related errors indicate a more obviousfailure to fulfill a request andin extreme casesmay cause confusing or unintelligible responses.We identify a surprising form limitation thatdrastically reduces LLMs utility for non-Englishlanguages: LLMs are often unable to consistentlygenerate text in the users desired language, or theappropriate language given the context. We callthis category of error language confusion.2 Take an Arabic prompt as an example: an LLMmay inappropriately respond fully in English (full-response confusion), produce some lines in thedesired language and some in another language(line-level confusion), or sporadically insert single",
  "We use this term to indicate that this is an erroneousbehaviour rather than natural alternations between languages,i.e., code-switching (Dogruz et al., 2021)": "words or phrases in another language (word-levelconfusion). shows example errors. Evenif such errors occur rarely, they cause a jarring user (experience).We investigate language confusion on the lineand word level in two practical settings: a) Mono-lingual generation, where a user queries the LLMin a given language, implicitly requesting an answerin the same language; and b) cross-lingual gener-ation, where a user explicitly instructs a model togenerate text in a different language.We create and release a language confusionbenchmark covering 15 typologically diverse lan-guages, sourcing prompts from public English andmultilingual instruction datasets, and additionallycreating new data with more complex prompts. Weevaluate a range of state-of-the-art LLMs includingLlama, Command R, Mistral, and OpenAI familymodels. We find that Llama Instruct and MistralLLMs exhibit severe language confusion in manylanguages. While Command R and OpenAI modelsfare much better on monolingual generation, eventhe strongest cannot consistently generate text inthe correct language cross-lingually.",
  "Language Confusion Benchmark": "While some datasets to evaluate LLMs per-formance on natural code-switched data exist(Khanuja et al., 2020; Winata et al., 2023), thereare none designed to assess language confusion inLLMs. We create the Language Confusion Bench-mark (LCB) by collecting a diverse set of promptsreflecting realistic use cases across a typologicallydiverse set of languages. The benchmark is easilyextensible, cheap, and efficient to evaluate.",
  "This is the most common usage scenario as usersoften prefer to interact with technology in theirnative language (Kantar and IAMAI, 2023)": "Cross-lingual generationA user instructs amodel in language l to fulfill a request in anotherlanguage l. In this challenging setting, the re-quested language l is different from the instruc-tion language l. This setting is relevant in appli-cations where multilingual outputs are required,but optimizing a prompt for each input languageis inefficient in practice or when a user requires ageneration in a language they do not speak. We setthe instruction language to English.",
  "Line-level detectionWe split a response intolines (by newline character) and check each lineagainst the users desired language with fastText.3": "Word-level detectionOff-the-shelf tools do notsupport word-level LID and LLMs only achieve 7986 F1 detecting word-level code-switching (Zhanget al., 2023), too low for use as automatic evalua-tors. Consequently, we take a two-pronged heuris-tic approach to detecting word-level language con-fusion focusing on settings where it achieves highprecision. For non-Latin script languages, we ob-serve that word-level language confusion in topLLMs mainly occurs with English. To avoid nat-ural code-switching false positives,4 we check forEnglish words that do not typically occur in targetlanguage text.5 We evaluate word-level confusionin Arabic (ar), Hindi (hi), Japanese (ja), Korean(ko), Russian (ru), and Simplified Chinese (zh).For Latin script languages where word-level lan-guage confusion is rarer,6 we detect tokens whereany character is outside of the Unicode range ofthe languages script. We evaluate word-level con-fusion in German (de), English (en), Spanish (es), 3We only apply fastText to sequences of more than 4 wordsas its LID predictions are less precise for shorter sequences.4E.g., Japanese users may use English acronyms: AI (Write an article about AI.)5We source words from which is based on the Linux dictionaryusually stored in /usr/share/dict/words. To reduce falsepositives, we exclude capitalized words from the dictionary(which are often proper nouns or acronyms).6We show an example of such word-level confusion in A1.",
  "OkapiLai et al. (2023)Synthetic1001.5kL15ShareGPT promptsOursHuman-generated991.5kL159": ":Data sources in the LCB for monolingual and cross-lingual generation.|D| is the totalnumber of examples per data source and |L| is the number of examples per language.For the cross-lingual setting, the model is instructed in English to generate in the target language l L where L ={fr, de, es, pt, it, ja, ko, zh, ar, tr, hi, ru, id, vi}. W is the median length in words of the prompts in each dataset.",
  "French (fr), Indonesian (id), Italian (it), Portuguese(pt), Turkish (tr), and Vietnamese (vi)": "Binary evaluationA response is only correctwhen entirely in the correct language, as even oneinstance of language confusion can damage intel-ligibility and cause a jarring user experience. Wecalculate binary metrics to indicate whether a re-sponse contains any instance of a) a line in anincorrect language and b) an isolated English wordfor languages using Latin scripts and an out-of-Unicode-range character for Latin script languages.These main metrics are defined below.Line-level pass rate (LPR): percentage of modelresponses that pass our line-level language confu-sion detector without error. A response is correctif all lines match the users desired language.",
  "where R is the set of all responses and EL the setof responses that contain line-level errors.7": "Word-level pass rate (WPR): percentage of re-sponses where all words are in the desired language.We exclude responses with line-level errors as mostline-level errors would also be counted as word-level errors, making it difficult to disentangle thetwo error types. For languages that use a Latinscript, we detect erroneous English words whilefor Latin script languages, we identify charactersoutside of the scripts Unicode range.",
  "Data sources": "The monolingual and cross-lingual tasks respec-tively comprise 2600 and 4500 prompts in total,across 15 typologically diverse languages: En-glish, French, German, Spanish, Portuguese, Ital-ian, Japanese, Korean, Chinese, Arabic, Turkish,Hindi, Russian, Indonesian, and Vietnamese. De-tails are shown in .The prompts are sourced from the datasets be-low, focusing on human-annotated or human-editedprompts. We filter each dataset to make it most use-ful for evaluating language confusion.",
  "Dolly200 machine-translated Dolly (Conoveret al., 2023) prompts post-edited by fluent speakersfor 6 languages from the Aya Evaluation Suitesdolly-human-edited subset (Singh et al., 2024)": "8Similar to F1, this gives higher importance to low values.9Imagine a models LPR for Arabic is a dismal 1% becauseonly 1/100 responses are fully Arabic, the rest English. WPRis a deceptively high 100%: as outputs are fully English, noEnglish word appears in an Arabic line. The model scores 2%LCPR, however, reflective of severe confusion.",
  "Data Filtering and Processing": "Suitability for LIDAs LID tools underperformon short sequences and non-standard text, we man-ually filter out: a) examples answerable with asingle word/phrase; b) multiple-choice questionsand prompts asking for lists; c) prompts requiringcode generation, math equations, or data formatssuch as HTML. For datasets where completionsare available, we filter out prompts with very shortcompletions (less than 5 words). Western-centric responsesMany datasets cre-ated via translation contain questions aboutWestern-centric concepts (e.g., US National Parks,presidents or US-based brands) which can causefalse positives with our word-level detector. Wemanually filter out such questions. Prompt formatFor cross-lingual generation, wesemi-automatically amend prompts with an instruc-tion to generate in a target language (see A.3 fordetails). Prompts are used as-is for monolingualgeneration. Some examples are shown in .",
  "Experiments": "ModelsWeevaluatethefollowingLLMscovering various scales and model families:Llama 2 70B Instruct (Touvron et al., 2023),Llama 3 and 3.1 70B Instruct (Dubey et al.,2024),Command R (35B),12 Command R+(104Bparameters),13CommandRRefresh(command-r-08-2024), Command R+ Refresh(command-r-plus-08-2024), Mixtral 8x7B (Jianget al., 2024), Mistral Large,14 GPT-3.5 Turbo(gpt-3.5-turbo-012524; Brown et al., 2020),GPT-4 Turbo (gpt-4-turbo-040924; Achiamet al., 2023), and GPT-4o (gpt-4o-2024-08-06).For Llama and Command models, we also evalu-ate base versions (see 4.5). We generate at most",
  ": Average word-level pass rate (WPR) on non-Latin script languages. See Tables A3 and A4 fordetailed WPR results on non-Latin and Latin script lan-guages respectively": "100 tokens per prompt using nucleus sampling withp = 0.75 and T = 0.3.LPR, WPR, and LCPR results are in Tables 3, 4,and A5, respectively.15 We show language confu-sion examples for different models in Table A6. Monolingual generationCommand and GPTmodels perform well on average on the line level(LPR in [98.6, 99.3]), but Llama 2 and 3 and Mis-tral models struggle to consistently generate text inthe correct language (LPR in [48.3, 73.0]). Llama",
  "We find results show little variance across runs (see A.9)": "3.1 performs much better, however. Mistral Largeis better on some European languages while Llama2 and 3 exhibit language confusion even for high-resource languages such as German. Most modelshave WPR within the same range and perform bet-ter on Latin script vs non-Latin script languages;however, Mixtral 8x7B is considerably worse. GPT-4 Turbo is strongest on LCPR, with Command R+Refresh and GPT-4o only slightly behind. Llamamodels preference for English responses leads tovery low LCPR. Cross-lingual generationIn the challengingcross-lingual setting, the best models have LPRsin the low 90s.The Command R and R+ Re-fresh models outperform their original versions,with Command R Refresh in particular showing alarge improvement over Command R, which per-forms poorly cross-lingually. OpenAI and Com-mand models perform best; Command R+ Refreshachieves the best performance overall. Llama 2 and3 models perform poorly: both scoring in the 30sdue to a tendency to respond in English. Llama 3.1shows improved performance but still tends to gen-erate in English for some languages. Mistral Largeis worse than Mixtral, even in European languages.On LCPR, Command R+ Refresh performs best,followed by GPT-4o.",
  "Impact of dataset": "In creating the language confusion test sets, weaimed to include prompts covering various usecases and domains. We show differences of LPRby dataset in Tables 5 and A7 for cross-lingualand monolingual generation, with WPR and LCPRin Tables A8 and A9 in A.4. Differences be-tween datasets are small for monolingual gener-ation. Cross-lingually, the difference is more pro-nounced: models perform fairly well on Okapi andShareGPT, but are much worse on our Complexprompts, indicating their more challenging nature.",
  "Impact of prompt length": "We analyze whether the difficulty of our Complexprompts is caused by their much higher length (see) by grouping the prompts into three lengthbuckets: short, medium and long, each with onethird of the prompts. Table A10 shows the LPR ofseveral models over the different lengths. We findno clear pattern, suggesting that higher confusionis caused by prompt complexity rather than length.",
  "Impact of instruction position": "Cross-lingual prompts include an instruction withthe desired output language at the beginning, at theend, or integrated in the prompt (e.g., Write an es-say in Korean [...]). Table A11 shows that acrossall models, line-level confusion is low for isolatedinstructions, with similar performance whether theyare at the start or the end. Integrated instructionscause more confusion: Command R has only 69%LPR on this type of prompt, versus 85% for iso-lated types. This difficulty can be greatly reduced",
  "When does language confusion occur?": "We study a sample of prompts to better understandwhere language confusion occurs. Intuitively, if atoken in an undesired language is assigned suffi-cient probability, it may be sampled. We observethat language confusion typically occurs when thedistribution over next tokens is flat and the nucleusis large (see A.10 for background).We generate responses to 15 Chinese promptsfrom Okapi with Command R.17 We examine out-puts to identify instances of English language con-fusion, finding it in 5 of 15 outputs. For each, wefind the first position where an English token was",
  "There are 9 such CPs.19": "We calculate Shannon entropy (Shannon, 1948)and nucleus size at each sampling point. We showan example output of Command R in , indi-cating Shannon entropy and the number of tokensin the nucleus at select sampling points, and thenext possible tokens with normalized probabilitiesat the confusion point. In the example, calledwas third most likely but occurred with sufficientprobability to be sampled (0.221). We generate 100tokens per prompt, so there are 1500 points: 9 ofwhich are CPs. We refer to the others as CPs.We average nucleus size and entropy over exam-ples containing no instances of language confusion(10 samples, 1000 CPs), at least one instance (5examples, 9 CPs, 491 CPs), and overall (15 sam-ples, 9 CPs, 1491 CPs). Results are in .20 Outputs with and without language confusion showsimilar average nucleus size and entropy (1.64 vs.1.61, and 0.353 vs. 0.365). At CPs, however, aver-age nucleus size and entropy is considerably higher:3.56 nucleus size and 1.228 entropy vs 1.61/0.356at CPs, indicating that confusion tends to occurwhen nucleus size and entropy are high.",
  "All1.623.561.610.3611.2280.356": ": Avg. nucleus size, entropy at confusionpoints (sampling points where language switch did[@CP] or did not [@CP] occur) for 15 Chinese re-sponses. Responses are split into those which had atleast one CP (Has CP) or zero CPs (No CP). 18When intentional, such points are referred to in the code-switching literature as switch points. We coin a new termhere to indicate that this switching is erroneous.19There are 3 examples of line-level confusion. We labelthe initial switching point from Chinese to English as the CP.20E.g., For outputs exhibiting language confusion, the aver-age nucleus size @CP is: sum of nucleus sizes at all CPs",
  "Reducing temperature and nucleus size": "Modifying sampling hyper-parameters affectswhich tokens are chosen at inference time. SectionA.10 explains nucleus sampling with temperature,and has a toy example of manipulatingthe hyperparameter T. Framing language confu-sion as an undesired side-effect of sampling, it isintuitive that we might control it by sharpening thedistribution over the next tokens at each timestep. We try to reduce the chance of having a wrong-language token sampled by manipulating temper-ature and nucleus size. shows the resultson monolingual WPR for Command R, with cross-lingual and LPR results in Section A.7. Higher Tencourages language confusion: T = 1 shows anaverage WPR of only 83.5%, and as low as 72.0%and 69.5% for Japanese and Chinese. Increasing p,resulting in a smaller nucleus, has a smaller effect.Note that setting T = 0.0 is equivalent to samplingwith top-K = 1 (greedy search).",
  "P(v V | the quick brown)": ": Effect of Temperature (T) in Nucleus Sampling. Tokens in the nucleus at p = 0.75 are bold. Middle:Effect of T on the softmax probabilities (Equation 1). Right: Effect of T on the probabilities of tokens in the nucleusright before sampling (Equation A.10). As T increases, the token has less chance to be sampled.",
  "Beam search decoding": "We explore the effect of beam search decoding onlanguage confusion for Command R. showsaggregate results for beam sizes 1 (greedy search)to 10, with full results in Section A.7. Increasingbeam size helps moderately over greedy search onWPR, with little noticable effect on monolingualLPR. Increasing beam size consistently hurts cross-lingual LPR and is most pronounced for non-Indo-European languages, where average LPR dropsfrom 73.9 with greedy search to 65.6 with beamsize = 10.",
  ": Effect of beam search decoding on languageconfusion metrics for Command R. Beam sizes: 1-10": "aggregates the best average scoreachieved for beam search and nucleus samplingfrom Tables 8, A13, A14, A15, A16. Beam searchis always better than nucleus sampling for WPRand for cross-lingual LPR, and both methods seemequally effective for monolingual LPR, suggest-ing that beam search may be an effective decodingstrategy for lessening language confusion (thoughat higher computational cost).",
  "guide Command R Base towards the correct behav-ior. We cherry-pick 5 prompt/answer pairs in En-glish and translate them with Google Translate.21": "For cross-lingual generation, we use a similarformat to the test sets. Figure A2 shows the tem-plate used to prompt the base model. For compar-ison, we also apply one-shot prompting to Com-mand R where the example is presented as turnsin a conversation. shows the results. Ta-bles A19 and A18 in Appendix show the detailedresults per language for monolingual and cross-lingual language confusion respectively.Few-shot prompting greatly reduces CommandR Bases language confusion and almost com-pletely eliminates the problem in the monolingualsetting. While one-shot prompting with CommandR is detrimental monolinguallyindicating a dif-ficulty of dealing with demonstrations in otherlanguagesit enables the model to better followthe cross-lingual instructions.",
  "tion data (SFT; Touvron et al., 2023), then applyEnglish-only preference tuning to this model": "Multilingual tuningWe extend the English datawith multilingual data, most of which comes frommachine-translated Dolly and ShareGPT (stnet al., 2024). Due to the scarcity of multilingualdata, our SFT data mixture is 90% English. Forpreference tuning, we use 50% multilingual data.Results are in .English-only tuning (both SFT and pref. tuning)exacerbates language confusion monolingually:likely the reason for high language confusion ofLlama-Instruct models (see 4.5). English-onlypreference tuning has a strong negative impact onword-level confusion. However, SFT with just 10%multilingual data is enough to almost eliminatethe problem of line-level confusion monolingually.Cross-lingually, multilingual tuning does not givebetter line-level performance than English-only tun-ing. This may be because cross-lingual datasetsonly have English prompts and it is more importantfor the model to learn to follow English instructions(e.g., Reply in French).",
  "Code-switchingThere has been much researchon natural alternations between languages, i.e.,code-switching (Dogruz et al., 2021) in natural": "language processing (NLP). Prior work focusedon evaluating capabilities on standard NLP tasksusing code-switching data (Khanuja et al., 2020;Winata et al., 2023) including sentiment analysis,machine translation, summarization and word-levellanguage identification. These tasks typically em-ploy data created by humans where word-levelcode-switching occurs between English and an-other language such as Hindi, Spanish, or Arabic.Current models still struggle to generate and under-stand code-switched text in some languages (Yonget al., 2023; Zhang et al., 2023). In contrast, we in-vestigate unnatural and erroneous code-switchingor language confusionin the LLMs generations. Language confusionPrior work has observedsource language hallucinations in zero-shot cross-lingual transfer (Vu et al., 2022; Li and Murray,2023; Pfeiffer et al., 2023; Chirkova and Nikoulina,2024) when models are fine-tuned on English dataand applied to generate text in another language.The problem of language confusion is known inthe machine translation field as off-target trans-lation (Chen et al., 2023; Sennrich et al., 2024).It typically occurs on English-centric multilingualmodels, when used in a zero-shot manner (to trans-late between two languages unseen at training). ForLLMs, there is no source language per se. A fewstudies (Kew et al., 2023; Faisal and Anastasopou-los, 2023; Chen et al., 2024) have provided evi-dence of LLMs generating in an incorrect languageon the response level. Holtermann et al. (2024)analyze which languages are confused on the re-sponse level using mainly smaller LLMs. To thebest of our knowledge, we are the first to showresults on the line and word level and the first tosystematically study language confusion in LLMs.",
  "Conclusion": "We have introduced the Language ConfusionBenchmark.We have shown that some LLMsexhibit severe language confusion and even thestrongest LLMs do not achieve perfect performancecross-lingually. We observed that base and English-centric instruct models are particularly susceptibleto language confusion, which is exacerbated bycomplex prompts. Finally, we proposed measuresto mitigate language confusion at inference andtraining. Our benchmark is efficient to evaluateand easy to extend and can help ensure that modelsachieve equal utility across languages.",
  "Inputs with cross-lingual contextContent maynot always be available in a users language, whichis relevant for applications such as cross-lingualsummarization or cross-lingual QA": "Language varietiesWe evaluate generation intostandardized languages. Future work may expandto language varieties and dialects, styles, and regis-ters.Our metrics are applied to model outputs of atmost 100 tokens. But what is a token depends onthe models tokenizer. Models with more aggres-sive tokenization could be advantaged with regardto the binary metrics (LPR and WPR). Likewise,because we allow models to stop their generationearly (by producing an end-of-sequence token),models which are less verbose could have an advan-tage. Furthermore, because current LID tools donot support word-level language identification, ourWPR metric is currently limited to assessing non-Latin script languages for unintended switchinginto English. As LID tools",
  "Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen, JungoKasai, David Mortensen, Noah Smith, and YuliaTsvetkov. 2023. Do all languages cost the same?tokenization in the era of commercial language mod-els. In Proceedings of the 2023 Conference on Em-pirical Methods in Natural Language Processing,pages 99049923, Singapore. Association for Com-putational Linguistics. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu,Terra Blevins, Hila Gonen, Machel Reid, YuliaTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.2023. Buffet: Benchmarking large language modelsfor few-shot cross-lingual transfer. arXiv preprintarXiv:2305.14857. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,and Pascale Fung. 2023. A multitask, multilingual,multimodal evaluation of ChatGPT on reasoning, hal-lucination, and interactivity. In Proceedings of the13th International Joint Conference on Natural Lan-guage Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 675718,Nusa Dua, Bali. Association for Computational Lin-guistics. Terra Blevins and Luke Zettlemoyer. 2022. Languagecontamination helps explains the cross-lingual capa-bilities of English pretrained models. In Proceedingsof the 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 35633574, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei,and Baobao Chang. 2023. On the off-target problemof zero-shot multilingual neural machine translation.In Findings of the Association for ComputationalLinguistics: ACL 2023, pages 95429558, Toronto,Canada. Association for Computational Linguistics. Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, An-drey Kutuzov, Barry Haddow, and Kenneth Heafield.2024. Monolingual or multilingual instruction tun-ing: Which makes a better alpaca. In Findings of theAssociation for Computational Linguistics: EACL2024, pages 13471356, St. Julians, Malta. Associa-tion for Computational Linguistics.",
  "Key ingredients for effective zero-shot cross-lingual": "knowledge transfer in generative tasks. In Proceed-ings of the 2024 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (Volume1: Long Papers), pages 72227238, Mexico City,Mexico. Association for Computational Linguistics. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,Matei Zaharia, and Reynold Xin. 2023. Free dolly:Introducing the worlds first truly open instruction-tuned llm. Company Blog of Databricks. A. Seza Dogruz, Sunayana Sitaram, Barbara E. Bul-lock, and Almeida Jacqueline Toribio. 2021. A sur-vey of code-switching: Linguistic and social per-spectives for language technologies. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 16541666, Online.Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, et al. 2024. The llama 3 herd of models. arXivpreprint arXiv:2407.21783. Fahim Faisal and Antonios Anastasopoulos. 2023. Geo-graphic and geopolitical biases of language models.In Proceedings of the 3rd Workshop on Multi-lingualRepresentation Learning (MRL), pages 139163, Sin-gapore. Association for Computational Linguistics.",
  "Herbert P Grice. 1975. Logic and conversation. InSpeech acts, pages 4158. Brill": "Arnav Gudibande, Eric Wallace, Charlie Victor Snell,Xinyang Geng, Hao Liu, Pieter Abbeel, SergeyLevine, and Dawn Song. 2024. The false promise ofimitating proprietary language models. In Proceed-ings of ICLR 2024. Nuno M Guerreiro, Duarte M Alves, Jonas Waldendorf,Barry Haddow, Alexandra Birch, Pierre Colombo,and Andr FT Martins. 2023. Hallucinations in largemultilingual translation models. Transactions of theAssociation for Computational Linguistics, 11:15001517.",
  "Tom Hosking, Phil Blunsom, and Max Bartolo. 2024.Human feedback is not gold standard. In Proceed-ings of ICLR 2024": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson.2020. Xtreme: A massively multilingual multi-taskbenchmark for evaluating cross-lingual generalisa-tion. In International Conference on Machine Learn-ing, pages 44114421. PMLR. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, EtsukoIshii, and Pascale Fung. 2023. Towards mitigatingllm hallucination via self reflection.In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 18271843. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088. Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293, Online. Association for ComputationalLinguistics.",
  "Tannon Kew, Florian Schottmann, and Rico Sennrich.2023. Turning english-centric llms into polyglots:How much multilinguality is needed? arXiv preprintarXiv:2312.12683": "Simran Khanuja, Sandipan Dandapat, Anirudh Srini-vasan, Sunayana Sitaram, and Monojit Choudhury.2020.GLUECoS: An evaluation benchmark forcode-switched NLP. In Proceedings of the 58th An-nual Meeting of the Association for ComputationalLinguistics, pages 35753585, Online. Associationfor Computational Linguistics. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen,Franck Dernoncourt, Ryan Rossi, and Thien Nguyen.2023. Okapi: Instruction-tuned large language mod-els in multiple languages with reinforcement learning from human feedback. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing: System Demonstrations, pages318327, Singapore. Association for ComputationalLinguistics. Tianjian Li and Kenton Murray. 2023. Why does zero-shot cross-lingual generation fail? an explanation anda solution. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 1246112476,Toronto, Canada. Association for Computational Lin-guistics. Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia,Xinyi Wang, Machel Reid, and Sebastian Ruder.2023.mmT5: Modular multilingual pre-trainingsolves source language hallucinations. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 19782008, Singapore. Associ-ation for Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems, volume 36,pages 5372853741. Curran Associates, Inc. Rico Sennrich, Jannis Vamvas, and Alireza Moham-madshahi. 2024. Mitigating hallucinations and off-target machine translation with source-contrastiveand language-contrastive decoding. In Proceedingsof the 18th Conference of the European Chapter ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 2133, St. Julians,Malta. Association for Computational Linguistics.",
  "Claude Elwood Shannon. 1948. A mathematical theoryof communication. The Bell system technical journal,27(3):379423": "Shivalika Singh, Freddie Vargus, Daniel Dsouza,Brje F Karlsson, Abinaya Mahendiran, Wei-YinKo, Herumb Shandilya, Jay Patel, Deividas Mataci-unas, Laura OMahony, et al. 2024. Aya dataset: Anopen-access collection for multilingual instructiontuning. arXiv preprint arXiv:2402.06619. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Alpaca: A strong,replicable instruction-following model.StanfordCenter for Research on Foundation Models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ahmet stn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, NeelBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,et al. 2024. Aya model: An instruction finetunedopen-access multilingual language model.arXivpreprint arXiv:2402.07827. Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-hit Iyyer, and Noah Constant. 2022. Overcomingcatastrophic forgetting in zero-shot cross-lingual gen-eration. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 92799300, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.",
  "Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie,Zhijie Deng, and Dong Yan. 2024. 3d-properties:Identifying challenges in dpo and charting a pathforward. Preprint, arXiv:2406.07327": "Zheng Xin Yong, Ruochen Zhang, Jessica Forde, SkylerWang, Arjun Subramonian, Holy Lovenia, SamuelCahyawijaya, Genta Winata, Lintang Sutawika, JanChristian Blaise Cruz, Yin Lin Tan, Long Phan, LongPhan, Rowena Garcia, Thamar Solorio, and AlhamAji. 2023. Prompting multilingual large languagemodels to generate code-mixed texts: The case ofsouth East Asian languages. In Proceedings of the6th Workshop on Computational Approaches to Lin-guistic Code-Switching, pages 4363, Singapore. As-sociation for Computational Linguistics. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,Ruobing Xie, Yankai Lin, Zhenghao Liu, BowenZhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.2024. Advancing llm reasoning generalists with pref-erence trees. Preprint, arXiv:2404.02078. Ruochen Zhang, Samuel Cahyawijaya, Jan Chris-tian Blaise Cruz, Genta Winata, and Alham Aji.2023. Multilingual large language models are not(yet) code-switchers. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1256712582, Singapore.Association for Computational Linguistics.",
  "Llama 3 70B-I 1,000": "**The Effects of Rowing Exercise: A Com-prehensive Review**\\n\\nRowing exercise hasgained popularity in recent years due to its nu-merous benefits for physical and mental health.As a low-impact, full-body workout, rowinghas been shown to improve cardiovascular fit-ness, increase muscle strength and endurance,and enhance overall well-being. In this article,we will discuss the effects of rowing exerciseand explore its advantages for individuals ofall fitness levels.\\n\\n**Cardiovascular Bene-fits**\\n\\nRowing is an excellent cardiovascu-lar",
  "A.3Cross-lingual prompt generation": "In order to generate prompts for the cross-lingualsetting, we semi-automatically amend Englishprompts with an English instruction to generate textin the target language. For each English prompt andtarget language, we randomly choose an Englishinstruction (e.g., Respond in French, Reply inTurkish). To control for the position of the instruc-tion in the prompt, for each prompt separate exam-ples are created where an instruction is inserted atthe beginning and at the end respectively. For eachprompt, we additionally generate another examplewhere an instruction is manually integrated into theprompt, e.g., Generate an essay in Korean. Thesame process is applied to Okapi, ShareGPT, andour complex prompts. shows representativeexamples where instructions are integrated in themiddle, inserted at the beginning and at the end ofthe original prompts.",
  "Command R42.728.138.0+ one-shot59.549.167.7Command R+82.174.879.8GPT-489.372.971.8": "Table A10: Line-level pass rate (LPR) on our Com-plex prompts dataset for cross-lingual generation,depending on length of the prompts. We sort theprompts by length and split them into 3 length buck-ets of the same size (each containing one third of theprompts). [a, b]: min and max length in words of eachbuckets prompts.",
  "Command R86.769.085.1+ one-shot88.780.690.6Command R+94.490.395.2GPT-493.091.795.0": "Table A11: Line-level pass rate (LPR) on crosslingualgeneration depending on the position of the languagecontrol instruction. Integrated corresponds to in-structions of the form Write an essay of 100 words inKorean about artificial intelligence. Start and Endare isolated instructions of the form Reply in French.placed either at the start or at the end of the prompt.",
  "A.6Quantization": "It is common to train at half-precision floating-point (FP16), where weights and activations of anetwork use 16 bits (2 bytes) to represent a floating-point value. It is common to quantize weights toINT8 (8-bit integers), often referred to as W8. Moreextreme quantization of weights to INT4 (4-bit) iscalled W4. Quantizing both weights and activationsto INT8 is commonly called W8A8. In .4,we compare FP16 with W8, W8A8, and W422 vari-ants of Command R+ on monolingual generation.",
  "A.8Base vs Instruction Tuning comparison": "Q: Write your answer in French. How should I choose what cheese to buy?A: Il existe de nombreux types de fromages diffrents, donc le choix du fromage acheter dpenddes prfrences personnelles, de la disponibilit et de lutilisation prvue. [...]Q: What is the difference between pets and cattle? Reply in Arabic.A: Figure A2: Template used for few-shot prompting the base models. The models answers are truncated to preventthe generation of new questions. For the instruct variants, we use similar prompting, except that the Q/A examplesare formatted as User/Chatbot turns using the models chat template.",
  "avgarhijakoruzhavgardeesfrhiiditjakoptrutrvizh": "Command R Base100.0 100.0 100.0 100.0 100.0 100.0 100.01.10.00.91.21.60.81.40.32.40.00.90.00.41.63.6+ Q/A template97.095.2100.099.097.991.798.420.93.319.9 27.6 27.55.325.8 23.2 42.5 13.5 17.58.019.6 14.9 44.0+ 1-shot98.698.598.898.698.398.499.290.7 83.8 93.3 95.0 93.2 84.2 86.7 93.0 93.2 92.5 91.1 88.3 89.6 91.9 93.3+ 5-shot99.7100.099.2100.099.5100.099.695.0 91.0 96.3 96.8 95.9 96.0 92.8 96.5 93.8 95.5 95.4 93.8 96.8 95.6 94.5+ English SFT91.795.598.777.994.894.389.078.3 70.0 77.7 82.7 76.6 78.4 77.7 74.8 80.8 80.7 79.3 74.4 82.4 83.9 77.1+ English pref. tuning87.491.297.770.492.988.583.685.7 86.4 87.4 86.2 82.1 87.7 80.2 83.9 87.6 84.3 83.2 86.7 90.1 89.6 84.6+ Multilingual SFT90.096.097.676.092.189.089.778.2 89.1 62.4 76.8 69.4 82.8 77.3 72.6 77.7 81.1 76.8 79.5 82.3 85.1 82.4+ Multi. pref. tuning86.994.196.671.287.784.987.089.4 94.1 80.9 91.9 88.7 91.2 85.6 88.9 89.2 93.0 83.7 92.3 91.7 93.0 87.8Command R94.094.398.688.597.294.091.168.1 61.6 63.2 72.5 74.4 65.5 70.8 65.7 65.3 69.2 67.2 69.4 67.7 65.7 75.0+ 1-shot92.397.298.387.689.791.689.582.9 80.0 79.5 85.6 82.6 84.7 79.9 82.4 78.0 87.2 81.5 84.4 84.5 84.5 85.2",
  "avgarhijakoruzhavgardeenesfrhiiditjakoptrutrvizh": "Command R Base98.797.995.1 100.0 100.0 100.0 99.4 86.2 94.985.099.590.5 85.3 81.0 72.2 74.093.994.2 84.0 94.892.083.0 68.1+ Q/A template99.7 100.0 100.0 100.0 100.0 98.5 100.0 85.3 87.673.2 100.0 89.8 86.3 98.9 49.0 91.997.993.5 96.8 86.796.349.0 82.4+ 1-shot100.0 100.0 100.0 100.0 100.0 100.0 100.0 94.1 93.099.0 100.0 99.3 47.5 96.9 93.8 99.099.096.8 99.0 97.395.196.6 99.0+ 5-shot100.0 100.0 100.0 100.0 100.0 100.0 100.0 99.0 99.699.0 100.0 100.0 98.3 98.0 96.9 100.0 100.0 99.0 99.5 98.998.899.0 97.9+ English SFT96.298.4 100.0 88.198.5 100.0 92.3 77.8 54.183.8 100.0 80.3 86.9 59.6 89.0 96.067.766.7 84.4 68.064.085.0 82.0+ English pref. tuning 90.993.597.884.495.489.685.0 74.3 54.481.0 100.0 82.7 85.6 46.0 80.0 95.064.065.0 81.4 67.751.082.0 78.5+ Multilingual SFT95.597.9 100.0 86.697.999.091.6 98.3 99.7 100.0 100.0 99.7 98.7 100.0 90.0 99.098.098.0 98.4 100.0 96.8 100.0 97.0+ Multi. pref. tuning93.497.9 100.0 86.793.091.891.2 98.8 99.697.099.599.3 99.3 100.0 92.0 100.0 99.0 100.0 99.0 100.0 100.0 100.0 98.0Command R96.399.399.093.997.096.092.3 98.6 100.0 98.099.595.7 99.3 100.0 92.0 99.0 100.0 100.0 98.5 100.0 99.099.0 98.5+ 1-shot92.794.495.793.888.990.592.9 68.3 51.853.05.583.2 98.3 23.7 88.0 94.065.095.7 66.0 95.590.397.8 17.0",
  "A.10Nucleus Sampling with Temperature": "Previous work shows that greedy search empiri-cally leads to repetition and generally low-qualitygenerations. One solution is adding stochasticity todecoding, such as by sampling the next token fromthe top-K next most likely tokens or from the top-P of the probability distribution over next tokens.The latter is called nucleus sampling (Holtzmanet al., 2019), and can be combined with top-K andtemperature sampling.Let x = [x0, x1, x2, ..., xn1] be a sequence oftokens. We describe how to sample the next tokenin the sequence via nucleus sampling with temper-ature given a language model with vocabulary V .The nucleus of a probability distribution for0 < p 1 is the smallest subset V V suchthat the summed probabilities of V are greater thanor equal to p.23 Nucleus sampling samples the nexttoken xn at random from this set. The functiontop_p returns V :",
  "Some implementations will define V as the largest subsetV V s.t. the summed probabilities of V is less-than-or-equal-to p. We use the definition from the original work": "are fox, dog, cube, , and afterwith logits [0.75, 0.20, 0.10, 0.20, 0.30]. Ap-plying the softmax, fox, dog, cube, and remain in the nucleus with respective likeli-hoods [0.418, 0.241, 0.179, 0.162]. has anover 16% chance of being sampled at the next stepa high risk for language confusion. Increasing Tto 2.0, has an over 20% chance of beingelicited. Reducing T to 0.5, however, we sharpenthe distribution by shifting probability mass to fox, dog, and cube such that fallsout of the nucleus and cannot be sampled, thuseliminating the risk of language confusion at thistimestep. Reducing Nucleus SizeThe distribution mayalso be sharpened by decreasing nucleus size.Keeping T= 1 but reducing nucleus size top = 0.7, is not in the nucleus. The prob-abilities over next tokens in the nucleus are then: fox : 0.499, dog : 0.287, cube : 0.213, andthe risk of language confusion is eliminated.",
  "A.11Discussion": "Behavior of base modelsAs observed in 4.5,language confusion in base models is not correlatedwith their downstream performance. Stronger basemodels such as Command R+ and Llama 3 70Bare more confused than Command R and Llama 270B respectively. Given the occurrence of transla-tions and token in other languages in pre-training(Blevins and Zettlemoyer, 2022), we expect basemodels to exhibit some degree of language switch-ing while instruction tuning then reinforces thedesired behavior. English-centricityDespite LLMs in general ex-hibiting impressive multilingual generative capa-bilities, the fact that they are most likely to switchto Englishboth on the sentence and word levelis another example of their English-centric nature(Hu et al., 2020; Zhao et al., 2024). Our experi-ments in 6.4 further highlight the negative impactof overly English-centric instruction tuning, whichis also illustrated by Llama Instruct models highlanguage confusion. Preference TuningIn , we observe thatWPR decreases after preference tuning. Citingsimilar observations by Yuan et al. (2024),24 Yanet al. (2024) observe a decrease in the likelihoodof both preferred and unpreferred data points asDPO (Rafailov et al., 2023) training progresses,and propose a theoretical explanation. Xu et al.(2024) remark that DPO is prone to generatinga biased policy that favors out-of-distribution re-sponses, leading to unpredictable behaviors. Itis plausible that if preference learning decreasestoken likelihoods for examples seen during its train-ing (e.g. common English words), then the rela-tive likelihoods of unseen/rare tokens increases,explaining the large decrease in WPR we see for+English SFT +English pref. tuning. The hypothe-sis that preference learning encourages unfavorablebehaviors such as language confusion is worthy offurther exploration. Other factorsThere are other factors that mayaffect language confusion. We suspect that word-level confusion is related to under-training. Cer-tain non-English tokens, particularly in low-densityregions that are rarely encountered during pre-training, may be under-trained and lack calibration."
}