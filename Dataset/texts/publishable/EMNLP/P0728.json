{
  "Abstract": "The NLP research community has devotedincreased attention to languages beyond En-glish, resulting in considerable improvementsfor multilingual NLP. However, these improve-ments only apply to a small subset of theworlds languages. An increasing number ofpapers aspires to enhance generalizable multi-lingual performance across languages. To thisend, linguistic typology is commonly used tomotivate language selection, on the basis that abroad typological sample ought to imply gen-eralization across a broad range of languages.These selections are often described as beingtypologically diverse. In this meta-analysis,we systematically investigate NLP research thatincludes claims regarding typological diversity.We find there are no set definitions or criteriafor such claims. We introduce metrics to ap-proximate the diversity of resulting languagesamples along several axes and find that theresults vary considerably across papers. Cru-cially, we show that skewed language selectioncan lead to overestimated multilingual perfor-mance. We recommend future work to includean operationalization of typological diversitythat empirically justifies the diversity of lan-guage samples. To help facilitate this, we re-lease the code for our diversity measures.1",
  "Introduction": "Most research in the field of natural language pro-cessing (NLP) is conducted on the English lan-guage (Ruder et al., 2022). Competitive monolin-gual language modelling beyond English remainschallenging, as current state-of-the-art methods relyon the availability of large amounts of data, whichare not available for most other languages (Joshiet al., 2020). This data sparsity can be mitigated",
  "There is an increase in the number of publi-cations with typological diversity claims across time": "by leveraging cross-lingual transfer through thetraining of a language model on multilingual data.Despite the potential of multilingual language mod-elling, methodologies are primarily developed forEnglish. However, there is no guarantee that an ap-proach that works well for one language will workequally well for others (Gerz et al., 2018). Forinstance, morphologically complex (or rich) lan-guages can be over-segmented by current widely-used tokenization methods (Rust et al., 2021). Eval-uation on a broad range of languages is importantfor drawing more generalizable conclusions aboutthe performance of multilingual language tech-nology (Bender, 2009, 2011; Pikuliak and Simko,2022). Evaluating a tokenization method on onlymorphologically simple languages, such as English,can give an unrealistic image of the effectivenessof a tokenization method, because morphologicallysimple languages are generally easier to tokenizecompared to complex ones. Current work increasingly evaluates models onmultiple languages, but because of resource con-straints, it is not realistic to test a model on the thou-sands of languages in the world. In order to stillensure a degree of generalizability, previous work recognizes the importance of diverse language sam-pling. Ponti et al. (2020) suggest that merely eval-uating on a small set of similar languages is anunreliable method for estimating a multilingualmodels performance, since such evaluations lackrobustness to previously unseen typological prop-erties, for instance. In an attempt to address thesesampling issues, NLP research increasingly claimsto rely on linguistic typology (see ). Eval-uating on a set of so-called typologically diverselanguages implies robustness over typological vari-ations, implicitly claiming broad generalizabilityacross languages. Importantly, however, there is afundamental lack of common understanding of ex-actly what a claim of typological diversity entails.In this work, we systematically investigate howthe term typological diversity is used in NLP re-search by surveying the literature for past and cur-rent practices. We find that there is no set definitionof typological diversity in NLP: justifications forclaims range from (i) none whatsoever, to (ii) usinglanguage phylogeny as a proxy for typology, to (iii)basing diversity on a more or less well-foundedsubset of available typological features. As a con-sequence, typological language similarity and ty-pological feature value inclusion vary considerablyacross papers (5). Furthermore, we demonstratewhy this is problematic, by showing how a skewedselection of evaluation languages can lead to over-stated multilingual performance and poor generaliz-ability (6). We recommend future work using theterm typologically diverse to document their lan-guage samples, and to add a measure of typologicaldiversity, such as the mean pairwise language dis-tance (5.1) or typological feature inclusion (5.2).Better-documented language sampling is crucialfor the development of multilingual NLP, becauseit provides insights into commonly overlooked lin-guistic difficulties of multilingual language mod-elling. We provide software to verify and visualizethis, linked below the abstract.",
  "ObjectivesLanguage sampling has a long-standing history in linguistic typology. Rijkhoffet al. (1993) state that testing a linguistic hypoth-": "esis on a few similar languages is not sufficientfor drawing conclusions on human language as awhole. Rather, they argue that generalizable sam-ples should capture the diversity of the worldslanguages. Language samples can be constructedin a number of ways. Rijkhoff and Bakker (1998)distinguish three types of sampling: random, prob-ability and variety. Random samples do not haveany specific stratification criteria, whereas prob-ability samples are constructed to minimize bias,for instance by avoiding a skew towards languagesfrom particular language families or geographicalareas. Variety samples are designed to contain asmuch linguistic variety as possible, including rela-tively uncommon typological characteristics (Mi-estamo et al., 2016). Finally, in addition to thethree methods outlined, sampling can be based ondata availability, which is defined as conveniencesampling (Velupillai, 2012, p. 50).Language sampling methods in NLP have a sim-ilar goal: to sample languages in such a way thatthe findings (e.g., multilingual performance esti-mates) are expected to generalize across languages.Yet, their justifications are commonly less rigorousthan those in linguistic typology. For language sam-pling methods in NLP, it is usually not explicitlymentioned which type of sampling is conducted,and for which reasons. In practice, most papersdo not provide a justification for their selections(see 4.3). This is likely caused by data availabil-ity, albeit implicitly, which resembles conveniencesampling. A rare exception to this is Ponti et al.(2020), who use variety sampling and mention thatthis ensures evaluation that is robust towards ty-pological characteristics that were likely not seenduring training. ResourcesWhile the goal of language samplingfor ensuring generalizable conclusions is similar be-tween NLP and linguistic typology, their resourcesare typically different. In linguistic typology, thevariables under investigation are often typologi-cal characteristics. A typologist may aim to testwhether a correlation of features holds across lan-guages. Such a correlation could then be referred toas a universal: languages with object-verb order-ing would be postpositional (Greenberg, 1963), forinstance. To avoid circularity, language sampling isnot done directly based on those features. Instead,methods commonly rely on (combinations of) lan-guage family, genus, and geography (Miestamoet al., 2016). By contrast, what is evaluated in multilingualNLP does not concern typological features directly,rather, model performance across languages. Infact, to support claims of generalizability, multilin-gual benchmarks must explicitly contain a range oftypological phenomena (Bender, 2009, 2011; Piku-liak and Simko, 2022). Relying on geographicaland genetic relations between languages is not guar-anteed to ensure this. Stoll and Bickel (2013) arguethat the kind of variables that define genealogicalgroups and tree shapes have a very different naturefrom the kind of variables that define typologicaldiversity, based on work from Nichols (1996).For estimating the generalizability of a languagesample in NLP evaluation, it is possible, and evendesired, to have direct insight into the actual typo-logical properties that are included in the selection(see also 6).This motivation is similarly voiced by Samardzicet al. (2024), who introduce a data-driven methodof relating language samples in particular datasetsto a reference language sample that is assumed tobe typologically diverse. Yet, our work differs incrucial ways. Our aims are to look at what it meansto make claims of typological diversity in currentresearch practices and how to characterize the lan-guage samples such claims are made about. Ourmethods work as standalone measures; we do notneed a reference sample to compare against. In ad-dition, our approach is centered around languages,thus only requiring language descriptions, not en-tire datasets. Samardzic et al. (2024) approximateaggregated morphological features through the av-erage word length in a corpus, but this number mayvary across domains within the same language.",
  "See Appendix A for details": "Our search string aims to capture a broad range offormulations, including: typologically and geneti-cally diverse languages (Xu et al., 2020), lan-guages of diverse typologies (Eskander et al.,2022) and diverse languages in terms of lan-guage family and morphological typology (Es-kander et al., 2020b). Another commonly usedphrase is we evaluate on N typologically differ-ent languages. While we acknowledge that someresearchers might (unintentionally) use the twoclaims interchangeably, we only focus on claims re-garding diversity. It is reasonable to state that lan-guages are typologically different, even thoughthey might be similar (e.g., Dutch and German).",
  ". Which languages are included?": "For criterion 1, we only consider claims made bythe paper at hand. That is, if a paper uses a datasetwhich is claimed to be typologically diverse, butthe current paper does not mention this, we do notconsider this to be a claim. This also applies torelated work: if a paper references a claim madeby another paper, but they themselves do not makea claim, then we do not consider this as a claim.The reason that we annotate whether a paper in-troduces a new dataset is that datasets are oftenstarting points for subsequent projects. If datasetsare claimed to be typologically diverse, this claimis spread not only through the paper introducing it,but also the dataset itself.3 Lastly, we annotate which languages are in-cluded in the papers making a claim. These are nor-malized to ISO-639-3 codes if available. Where ap-plicable, we only select the languages about whichthe claim is made. For example, Kann et al. (2020)and Shi et al. (2023) specifically distinguish be-tween baseline languages and their typologicallydiverse test language selection.",
  "Inter-annotator agreement per item": "For the annotations concerning claims (1) anddatasets (2), we achieve substantial agreement. Weresolve the disagreements by discussing the an-notations and merging them into a single label.For the sake of transparency, and to support incen-tives for human label variation (Plank, 2022), wealso release the non-aggregated annotations.4 Theagreement for the language selection annotation(3) is somewhat low, which is partially explainedby the fact that we calculate agreement over alllanguages together, rather than on the micro-levelfor individual languages. This is because there isno ground truth regarding the number of languagesto annotate. Furthermore, a number of inconsisten-cies are due to ISO-639-3 variants (jap jpn orger deu) and different codes for possibly am-biguous language mentions (Norwegian could benor or nob). We iteratively resolve these issues toget our final list of languages per paper.",
  "Publication Overview": "In total we retrieve 194 papers, of which 110 arefound to contain a claim. The rest of this analysis isbased on these 110 papers. shows the mostcommon venues where papers with a claim havebeen published. The top six venues are all high-ranking NLP conferences, with SIGMORPHONbeing the first workshop at the seventh spot. TheAI-centered venues only have a small number ofpapers with claims; ICLR and AAAI both have two,ICML one, and the rest none. In total, 38 papersintroduce a dataset, most of which are published atACL and LREC since 2020 ( and 2).5",
  "Papers by number of used languages": "Four of the papers that claim typological diver-sity do not mention which languages they use atall. The other papers use a median of 11 languages(Q1 = 8, Q3 = 18), with a minimum of 2 anda maximum of 90. In total, the papers use 315unique languages, of which 160 are used just once. shows the distribution of the number oflanguages used per paper.English is the most used language (63 papers),followed by German (60), Russian (58) and Finnish(57).6 The paper that uses the most languages isVylomova et al. (2020), which contains 90. Fig-ure 4 shows where these languages are primarily",
  "Justifications": "The papers include a wide spectrum of justifica-tions for their claims. A large portion provide nojustification at all. Some use genealogy: the selec-tion of 24 languages from Xu et al. (2022) aims tocover a reasonable variety of language families,while the dataset created by Zhang et al. (2023)consists of languages (. . . ) from 10 languagefamilies and 13 sub-families. Others use a selec-tion of typological features, for instance, Mott et al.(2020) mention that the nine languages in ourcorpus cover five primary language families (...),and a range of morphological phenomena (...).Some also mention typological databases in theirlanguage selection: Muradoglu and Hulden (2022)consider languages that exhibit varying degreesof complexity for inflection. We also consider mor-phological characteristics coded in WALS (...).A rather systematic approach to language selec-tion is found in Jancso et al. (2020). They use aclustering algorithm on vectors with features fromtwo typological databases to find the most distantclusters to sample languages from.Still, most other papers justify their typologicaldiversity in a post-hoc way. They generally do notmention initial language selection considerations. Rather, they mention how diverse the sample isafter it has been created. The XTREME-R dataset(Ruder et al., 2021), which we will discuss furtherin 6, is exemplary of this. They mention diversityindices that cover family, as well as typology andcompare theirs to other datasets, but do not detailhow or if these were used in sampling.",
  "Typological Analysis": "From .2, it follows that there is a geo-graphical skew, where languages from certain areasare over-represented (). This is not surpris-ing, as it is often more feasible to select languagesfor which there are existing resources and typolog-ical descriptions, than languages for which thesehave to be gathered from scratch. This constitutesbibliographic bias (Rijkhoff and Bakker, 1998).However, we cannot draw conclusions about ty-pological diversity on this basis, as typology is notequal to geography, nor phylogeny (Cysouw, 2013).Therefore, in this section, we analyze the typologi-cal diversity according to metrics that take into ac-count syntactic features, and the absolute includedtypological feature values. Ponti et al. (2020) pre-viously quantified language diversity with typology(URIEL), geography, and family indices. Thesemetrics do not take into account the distance be-tween pairs of languages, and thus do not provideinsights into the independence of languages in asample. Additionally, no code or clear descriptionsare provided, which limits their use.",
  "Grambank feature value inclusion": "Distributions of mean pairwise lang2vec distances and feature inclusion per paper. On the leftare approximations based on common justifications for claiming typological diversity: geography ( = 0.28, = 0.11) and genealogy ( = 0.94, = 0.05). On the right two different approximations based on typologicalfeatures: MPSD ( = 0.64, = 0.07) and Grambank feature value inclusion ( = 0.72, = 0.17). Our metrics7 include pairwise language dis-tances (5.1) and absolute typological feature valueinclusion (5.2) across papers. Lastly, we look intothe relationship between the number of includedlanguages and typological diversity, to help guidefuture language selection efforts (5.3).",
  "Mean Language Distance": "We first approximate the typological diversity ofeach papers language selection with lang2vec(Littell et al., 2017). This toolkit provides languagedistances that are calculated based on languagevectors from URIEL, a resource that contains infor-mation from a range of typological databases, in-cluding WALS. We choose lang2vec specifically,because its aggregated information enables anal-ysis across databases, and because its vectorizedformat mitigates issues stemming from incompletefeature coverage in typological databases.For each paper in our survey that claims a ty-pologically diverse language set and that is cov-ered in lang2vec (312 / 315 unique languages),we calculate the MEAN PAIRWISE SYNTACTIC DIS- TANCE (MPSD) for the included languages. Thatis, we take the average of all pre-computed syntac-tic lang2vec distances for each pair of languagesin a given papers selection. We only take intoaccount languages that have at least 5% coveragein the URIEL vectors. We do this to make surethe very low coverage languages do not distort theresults. This affects 31 papers in our dataset, re-sulting in a total of 242 unique languages. Thetop-right plot in shows the distribution ofMPSDs across papers.8 We compare our typology-based approximations with mean geographical and 7Formal definitions are provided in Appendix B.8To illustrate this: the distance between Danish and Nor-wegian (same family, same genus) is 0.22, Danish-Spanish(same family, different genus) is 0.59 and Danish-Japanese(different family, different genus) is 0.69. genetic pairwise distances. Interestingly, the ge-netic distance is much higher and less spread outthan the syntactic distance. This emphasizes thatgenealogical sampling does not by default ensuretypological diversity.The typological distances vary considerably,with outliers on either side ().In pa-pers claiming typological diversity, the minimumMPSD is found in Goel et al. (2022), who use En-glish, French, and Spanish. The maximum MPSDis found for Vania et al. (2019) their selection ofNorth Smi, Galician, and Kazah.",
  "Inclusion of Typological Features": "Because lang2vec distance calculations are basedon feature vectors, our previous analysis does notprovide information regarding which typologicalfeature values are actually included in the languageset. For example, it does not tell us which wordorder variations are covered in a given languageselection. This information can be useful though,because covering more typological feature valuesin an evaluation set means that robustness towardspreviously unseen typological characteristics is in-creased. Following the saturation measure by Mi-estamo et al. (2016), we look into the inclusion ofindividual typological feature values per languageselection from each paper. We use the Grambank9 (Skirgrd et al., 2023b) database, because it hashigh coverage for grammatical features and is cur-rently actively maintained. For each feature inGrambank, we count whether all possible, non-missing feature values are represented in the pa-pers language selection.10 That is, for a featuresuch as GB020: Are there definite or specific ar-ticles?, we count whether all non-missing optionsare represented by the particular set of languages",
  "PCA plot for the paper with the highest ab-solute feature coverage (Gutierrez-Vasques et al., 2021),where each point represents a language in Grambank": "from a paper and divide this by the total numberof features. The spread of these values is shownin the bottom right graph in . We treatmissing languages the same as in the MPSD cal-culations; we discard them during calculation, butwe do not discard the entire paper. Twelve papersare affected by this. The average Grambank featurevalue inclusion is 0.73.While the accumulation of absolute feature val-ues covered in a language selection provides usefulrobustness insights, it does not capture that lan-guages consist of combinations of features that arenot entirely independent. Skirgrd et al. (2023a)show how language families can reliably be visual-ized as distinct groupings in Grambanks typologi-cal design space using principle component analy-sis (PCA). Similarly, in we show the PCAplot for the paper that has the highest Grambankfeature inclusion of all papers: Gutierrez-Vasqueset al. (2021). While their language sample coversmuch of the Grambanks typological space, we ob-serve a skew towards certain languages (bottomright), and a lack of representation for others (e.g.,bottom left). This means that even the paper withthe highest feature value inclusion in our surveycontains challenges when it comes to complete andfair evaluation.",
  "MPSD (a) and Grambank feature value in-clusion (b) per paper by number of languages": "evaluate on as many languages as possible? Whatis the effect of selecting many (similar) languages?a and 7b show the relationship between thenumber of languages and the MPSDs, and the ab-solute typological feature value inclusion, respec-tively. We find that smaller language sets (0-10languages) exhibit considerable variation in termsof average syntactic distances, while the larger lan-guage sets (30+ languages) seem consistent be-tween 0.6 and 0.7. This implies that adding morelanguages does not necessarily raise the averagesyntactic distance much. In NLP, adding morelanguages typically means adding more similarlanguages, since it is easier to incorporate existingdatasets than it is to create new ones. As a result,the average performance will also be skewed to-wards these similar languages in evaluation (see6). Furthermore, we observe that up to a certainpoint, including more languages implies that moretypological feature values are covered. However,this increase flattens at approximately 40 languages.Importantly, this highlights the fact that simplyadding more languages to an experimental study isnot by itself a contribution in terms of typologicalgeneralizability one must take care which lan-guages are included. To illustrate this, we highlightthe MKQA dataset (Longpre et al., 2021). Thisdataset includes 25 languages, more than twice themedian, but has a rather average MPSD of 0.61(and a Grambank feature inclusion of 0.89). Thisis in part due to its inclusion of several typologi-cally similar languages: {English, German, Dutch},{Cantonese, Mandarin}, {Spanish, Portuguese} and{Swedish, Danish, Norwegian}.",
  "LAReQAXLM-R-L40.75 (11)40.54 (11)-0.22- (0)- (0)- (0)40.88 (9)- (0)40.20 (2)- (0)mBERT21.58 (11)19.24 (11)-2.35- (0)- (0)- (0)22.92 (9)- (0)15.55 (2)- (0)": "XQuADXLM-R-L77.21 (11)77.24 (11)+0.04- (0)- (0)- (0)77.19 (9)- (0)77.30 (2)- (0)mBERT65.05 (11)61.84 (11)-3.21- (0)- (0)- (0)66.89 (9)- (0)56.80 (2)- (0)mT581.54 (11)80.55 (11)-0.99- (0)- (0)- (0)82.10 (9)- (0)79.00 (2)- (0) MLQAXLM-R-L72.71 (7)73.33 (7)+0.62- (0)- (0)- (0)72.47 (6)- (0)74.20 (1)- (0)mBERT61.30 (7)60.84 (7)-0.46- (0)- (0)- (0)61.48 (6)- (0)60.20 (1)- (0)mT575.59 (7)75.97 (7)+0.38- (0)- (0)- (0)75.43 (6)- (0)76.50 (1)- (0)",
  "WikiANN-NERXLM-R-L64.43 (48)62.02 (40)-2.41- (0)69.90 (1)62.10 (1)66.92 (31)61.37 (3)48.17 (4)63.66 (8)mBERT62.68 (48)61.73 (40)-0.95- (0)72.70 (1)65.00 (1)64.93 (31)57.23 (3)49.38 (4)61.12 (8)": "TyDiQAXLM-R-L64.29 (9)62.57 (8)-1.72- (0)66.40 (1)- (0)65.67 (6)- (0)59.10 (1)59.10 (1)mBERT58.36 (9)55.09 (8)-3.26- (0)59.70 (1)- (0)60.97 (6)- (0)46.20 (1)53.50 (1)mT581.94 (9)83.73 (8)+1.78- (0)87.20 (1)- (0)80.52 (6)- (0)83.60 (1)83.60 (1) XTREME-R results grouped by inflection type (WALS 26A). Overall refers to the average over alllanguages. By Feature is the average of the WALS feature averages, excluding languages for which there is nocoverage. The delta shows the difference of Overall and By F. Tasks that have coverage for all their includedlanguages are on top, those partially covered are in the bottom portion. Morphological inflection types are: Aff= affixing, Suf = suffixing, Pre = prefixing and NA referring to Not Available (in WALS). The highest and thelowest non-zero number of languages per grouping are highlighted. The (italicized) number refers to the number oflanguages in a particular subset. Metrics: = Accuracy, = mAP@20, = F1. guages separately is preferable, space constraintsoften lead to reporting averages across languagesin multilingual evaluation. However, Anastasopou-los (2019) demonstrates that when a language setcontains a skew towards a certain language family,simply taking the micro average over all languagesfor evaluation gives an overestimation of perfor-mance. While family can serve as a proxy fortypological diversity, we here present an analysisbased directly on typological characteristics. In thisway, we gain more fine-grained insights into theeffects of typologically skewed language selectionfor model evaluation.",
  "The original XTREME dataset has almost 800 citations asof writing, XTREME-R, which is an extension of the original,more than 100": "ranges from 7 to 48.12 We group these languagesby the Prefixing vs. Suffixing in Inflectional Mor-phology (26A) and Order of Subject, Object andVerb (81A) features, as provided by WALS (Dryer,2013b,a). These features have high language cov-erage, which makes our analysis as comprehensiveas possible. shows groupings and coverageof the inflectional feature (26A). The word orderanalysis is included in Appendix E, .Firstly, we see that XTREME-R contains a skewin terms of morphological inflection. For all tasks,the majority of languages are strongly suffixing(see orange cells). Some feature values, such asequal prefixing and suffixing and weak prefixingare underrepresented, with at most one languageper subtask. Remarkably, strong prefixing doesnot appear in XTREME-R at all. This impliesthat one should be careful with the implication ofgeneralizability that evaluating on a typologicallydiverse dataset gives.The delta column shows the difference betweenthe macro average over all languages and the macroaverage per feature value. Here, we observe simi-",
  "Insights and Recommendations": "We systematically analyze claims of typologicaldiversity in NLP research. We approximate diver-sity in terms of average syntactic language distance(MPSD) and absolute typological feature value in-clusion in Grambank. Our analysis shows that (1)there is no consistent definition or methodologywhen making typological diversity claims, (2)estimates of typological diversity exhibit a consid-erable variation across papers and (3) aggregatedresults can give distorted views of multilingual per-formance estimates.We recommend future approaches to include anoperationalization of typological diversity whenmaking such claims. This can be in terms of gen-eralizability or regarding a particular typologicalphenomenon of interest. Additionally, we recom-mend adding an empirical justification, especiallywhen claims relate to generalizability. The fea-ture value inclusion, MPSD or PCA plots we haveshown are examples of these justifications. Includ-ing these has the potential to benefit multilingualNLP, as it enables more fine-grained insights intotypological challenges of language modelling.",
  "Limitations": "Our measures of typological diversity are approxi-mations. There is no typological database that cov-ers every aspect of every language. This also tiesinto the bibliographic bias of typological databasesin general. In addition, languages are not neces-sarily as discrete as they appear in typological re-sources either (Levshina et al., 2023; Baylor et al.,2024). Nonetheless, we believe that the reportingof typological diversity can be more principled thanit currently is, despite incomplete resources.Another limitation is that our search is basedonly on the title and abstract of papers. There arealmost certainly papers that contain a claim in othersections. However, we chose this route since wewanted to (1) cast a wide net, (2) perform somepre-filtering of papers, assuming that a mention of typological diversity in the title or abstract is agood indicator that it is a prominent aspect of thatpaper, and (3) make sure that the data we could getwas of good quality, which is a lot harder with (old)pdf files for example.Our arguments for assessing linguistic typologydirectly in NLP, rather than phylogeny and geog-raphy as proxies, relates to claims of typologicaldiversity. We acknowledge that geographical andgenealogical information could be useful for otherpurposes.",
  "Ethics Statement": "In this paper, we investigate the use of the term ty-pological diversity in multilingual NLP research.We do not make a claim that NLP applicationsshould be expanded to as many languages as possi-ble, as introducing these technologies are not nec-essarily always a positive influence (Bird, 2020).Instead, our aim is to gain a common understand-ing and clear explanations regarding the usage ofthese claims.",
  "Acknowledgements": "EP and JB are funded by the Carlsberg Founda-tion, under the Semper Ardens: Accelerate pro-gramme (project nr. CF21-0454). WP is fundedby a KU Leuven Bijzonder Onderzoeksfonds C1project with reference C14/23/096.We thank the TypNLP group at Aalborg Univer-sity and the LAGoM-NLP group at KU Leuven forinsightful feedback on earlier versions of this paper,in particular Heather Lent. We also thank ThomasBauwens for his help in creating the tables (usingfiject).",
  "Antonis Anastasopoulos. 2019. A note on evaluatingmultilingual benchmarks. Accessed: 2024-01-09": "Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.2020. On the cross-lingual transferability of mono-lingual representations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46234637, Online. Associationfor Computational Linguistics. Mikel Artetxe and Holger Schwenk. 2019.Mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transactionsof the Association for Computational Linguistics,7:597610. Akari Asai, Shayne Longpre, Jungo Kasai, Chia-Hsuan Lee, Rui Zhang, Junjie Hu, Ikuya Yamada,Jonathan H. Clark, and Eunsol Choi. 2022. MIA2022 shared task: Evaluating cross-lingual open-retrieval question answering for 16 diverse languages.In Proceedings of the Workshop on Multilingual In-formation Access (MIA), pages 108120, Seattle,USA. Association for Computational Linguistics. Martijn Bartelds, Nay San, Bradley McDonnell, DanJurafsky, and Martijn Wieling. 2023. Making more oflittle data: Improving low-resource automatic speechrecognition using data augmentation. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 715729, Toronto, Canada. Association forComputational Linguistics.",
  "Emi Baylor, Esther Ploeger, and Johannes Bjerva. 2024": "Multilingual gradient word-order typology from Uni-versal Dependencies. In Proceedings of the 18thConference of the European Chapter of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 4249, St. Julians, Malta. Associa-tion for Computational Linguistics. Emily M. Bender. 2009. Linguistically nave != lan-guage independent: Why NLP needs linguistic typol-ogy. In Proceedings of the EACL 2009 Workshopon the Interaction between Linguistics and Compu-tational Linguistics: Virtuous, Vicious or Vacuous?,pages 2632, Athens, Greece. Association for Com-putational Linguistics.",
  "Emily M. Bender. 2011. On achieving and evaluatinglanguage-independence in NLP. Linguistic Issues inLanguage Technology, 6": "Steven Bird. 2020. Decolonising speech and languagetechnology. In Proceedings of the 28th InternationalConference on Computational Linguistics, pages35043519, Barcelona, Spain (Online). InternationalCommittee on Computational Linguistics. Jan A. Botha, Zifei Shan, and Daniel Gillick. 2020. En-tity Linking in 100 Languages. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 78337845,Online. Association for Computational Linguistics.",
  "Sven Buechel, Susanna Rcker, and Udo Hahn. 2020": "Learning and evaluating emotion lexicons for 91 lan-guages. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 12021217, Online. Association for Computa-tional Linguistics. Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, SivaReddy, Desmond Elliott, Edoardo Maria Ponti, andIvan Vulic. 2022. IGLUE: A benchmark for trans-fer learning across modalities, tasks, and languages.In International Conference on Machine Learning,pages 23702392. PMLR.",
  "Jonathan H. Clark, Eunsol Choi, Michael Collins, DanGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and": "Jennimaria Palomaki. 2020. TyDi QA: A benchmarkfor information-seeking question answering in typo-logically diverse languages. Transactions of the As-sociation for Computational Linguistics, 8:454470. Alexis Conneau, Ruty Rinott, Guillaume Lample, AdinaWilliams, Samuel Bowman, Holger Schwenk, andVeselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 24752485, Brus-sels, Belgium. Association for Computational Lin-guistics.",
  "Michael Cysouw. 2013. Disentangling geography fromgenealogy. In Space in Language and Linguistics,pages 2137. De Gruyter": "Cheikh M. Bamba Dione, David Ifeoluwa Adelani,Peter Nabende, Jesujoba Alabi, Thapelo Sindane,Happy Buzaaba, Shamsuddeen Hassan Muhammad,Chris Chinenye Emezue, Perez Ogayo, AnuoluwapoAremu, Catherine Gitau, Derguene Mbaye, JonathanMukiibi, Blessing Sibanda, Bonaventure F. P. Dos-sou, Andiswa Bukula, Rooweither Mabuya, Allah-sera Auguste Tapo, Edwin Munkoh-Buabeng, Vic-toire Memdjokam Koagne, Fatoumata Ouoba Ka-bore, Amelia Taylor, Godson Kalipe, TebogoMacucwa, Vukosi Marivate, Tajuddeen Gwadabe,Mboning Tchiaze Elvis, Ikechukwu Onyenwe, Gra-tien Atindogbe, Tolulope Adelani, Idris Akinade,Olanrewaju Samuel, Marien Nahimana, ThogneMusabeyezu, Emile Niyomutabazi, Ester Chimhenga,Kudzai Gotosa, Patrick Mizha, Apelete Agbolo, Sey-dou Traore, Chinedu Uchechukwu, Aliyu Yusuf,Muhammad Abdullahi, and Dietrich Klakow. 2023.MasakhaPOS: Part-of-speech tagging for typolog-ically diverse African languages. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1088310900, Toronto, Canada. Associationfor Computational Linguistics.",
  "WALS Online (v2020.3). Max Planck Institute forEvolutionary Anthropology, Leipzig": "Ramy Eskander, Francesca Callejas, Elizabeth Nichols,Judith Klavans, and Smaranda Muresan. 2020a. Mor-phAGram, evaluation and framework for unsuper-vised morphological segmentation. In Proceedingsof the Twelfth Language Resources and EvaluationConference, pages 71127122, Marseille, France. Eu-ropean Language Resources Association. Ramy Eskander, Cass Lowry, Sujay Khandagale, Ju-dith Klavans, Maria Polinsky, and Smaranda Mure-san. 2022. Unsupervised stem-based cross-lingualpart-of-speech tagging for morphologically rich low-resource languages.In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 40614072, Seattle,United States. Association for Computational Lin-guistics. Ramy Eskander, Smaranda Muresan, and MichaelCollins. 2020b. Unsupervised cross-lingual part-of-speech tagging for truly low-resource scenarios. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 48204831, Online. Association for Computa-tional Linguistics. Jack FitzGerald, Christopher Hench, Charith Peris,Scott Mackie, Kay Rottmann, Ana Sanchez, AaronNash, Liam Urbach, Vishesh Kakarala, Richa Singh,Swetha Ranganath, Laurie Crist, Misha Britan,Wouter Leeuwis, Gokhan Tur, and Prem Natara-jan. 2023.MASSIVE: A 1M-example multilin-gual natural language understanding dataset with51 typologically-diverse languages. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 42774302, Toronto, Canada. Association forComputational Linguistics. Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, RoiReichart, and Anna Korhonen. 2018. On the rela-tion between linguistic typology and (limitations of)multilingual language modeling. In Proceedings ofthe 2018 Conference on Empirical Methods in Natu-ral Language Processing, pages 316327, Brussels,Belgium. Association for Computational Linguistics. Michael Ginn, Sarah Moeller, Alexis Palmer, AnnaStacey, Garrett Nicolai, Mans Hulden, and MiikkaSilfverberg. 2023. Findings of the SIGMORPHON2023 shared task on interlinear glossing. In Pro-ceedings of the 20th SIGMORPHON workshop onComputational Research in Phonetics, Phonology,and Morphology, pages 186201, Toronto, Canada.Association for Computational Linguistics.",
  "Goran Glava, Mladen Karan, and Ivan Vulic. 2020": "XHate-999: Analyzing and detecting abusive lan-guage across domains and languages. In Proceed-ings of the 28th International Conference on Com-putational Linguistics, pages 63506365, Barcelona,Spain (Online). International Committee on Compu-tational Linguistics. Anmol Goel, Charu Sharma, and Ponnurangam Ku-maraguru. 2022. An unsupervised, geometric andsyntax-aware quantification of polysemy. In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1056510574,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Omer Goldman, Khuyagbaatar Batsuren, Salam Khal-ifa, Aryaman Arora, Garrett Nicolai, Reut Tsarfaty,and Ekaterina Vylomova. 2023. SIGMORPHONUniMorph 2023 shared task 0: Typologically di-verse morphological inflection. In Proceedings of the20th SIGMORPHON workshop on ComputationalResearch in Phonetics, Phonology, and Morphology,pages 117125, Toronto, Canada. Association forComputational Linguistics.",
  "Joseph Harold Greenberg. 1963. Some universals ofgrammar with particular reference to the order ofmeaningful elements. In Universals of Language,pages 4070. MIT press, Cambridge, MA": "Shivanshu Gupta, Yoshitomo Matsubara, Ankit Chadha,and Alessandro Moschitti. 2023.Cross-lingualknowledge distillation for answer sentence selectionin low-resource languages. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 1407814092, Toronto, Canada. Associationfor Computational Linguistics. Ximena Gutierrez-Vasques, Christian Bentz, Olga Sozi-nova, and Tanja Samardzic. 2021. From charactersto words: the turning point of BPE merges. In Pro-ceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 34543468, Online.Association for Computational Linguistics. Leonhard Hennig, Philippe Thomas, and SebastianMller. 2023. MultiTACRED: A multilingual versionof the TAC relation extraction dataset. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 37853801, Toronto, Canada. Association forComputational Linguistics.",
  "Peter Juel Henrichsen and Marcus Uneson. 2012": "SMALLWorlds multilingual content-controlledmonologues.In Proceedings of the Eighth Inter-national Conference on Language Resources andEvaluation (LREC12), pages 33623368, Istanbul,Turkey. European Language Resources Association(ELRA). Chia-Chien Hung, Anne Lauscher, Ivan Vulic, SimonePonzetto, and Goran Glava. 2022. Multi2WOZ: Arobust multilingual dataset and conversational pre-training for task-oriented dialog. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 36873703,Seattle, United States. Association for ComputationalLinguistics.",
  "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki,Haibo Ding, and Graham Neubig. 2020. X-FACTR:": "Multilingual factual knowledge retrieval from pre-trained language models. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 59435959, On-line. Association for Computational Linguistics. Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293, Online. Association for ComputationalLinguistics. Katharina Kann, Ophlie Lacroix, and Anders Sgaard.2020.Weakly Supervised POS Taggers PerformPoorly on Truly Low-Resource Languages. Proceed-ings of the AAAI Conference on Artificial Intelligence,34(05):80668073. Jordan Kodner, Salam Khalifa, Khuyagbaatar Bat-suren, Hossep Dolatian, Ryan Cotterell, Faruk Akkus,Antonios Anastasopoulos, Taras Andrushko, Arya-man Arora, Nona Atanalov, Gbor Bella, ElenaBudianskaya, Yustinus Ghanggo Ate, Omer Gold-man, David Guriel, Simon Guriel, Silvia Guriel-Agiashvili, Witold Kieras, Andrew Krizhanovsky,Natalia Krizhanovsky, Igor Marchenko, MagdalenaMarkowska, Polina Mashkovtseva, Maria Nepomni-ashchaya, Daria Rodionova, Karina Scheifer, Alexan-dra Sorova, Anastasia Yemelina, Jeremiah Young,and Ekaterina Vylomova. 2022. SIGMORPHONUniMorph 2022 shared task 0: Generalization andtypologically diverse morphological inflection. InProceedings of the 19th SIGMORPHON Workshopon Computational Research in Phonetics, Phonology,and Morphology, pages 176203, Seattle, Washing-ton. Association for Computational Linguistics. Natalia Levshina, Savithry Namboodiripad, MarcAllassonnire-Tang, Mathew Kramer, Luigi Talamo,Annemarie Verkerk, Sasha Wilmoth, Gabriela Gar-rido Rodriguez, Timothy Michael Gupton, EvanKidd,ZoeyLiu,ChiaraNaccarato,RachelNordlinger, Anastasia Panova, and Natalia Stoynova.2023. Why we need a gradient approach to wordorder. Linguistics, 61(4):825883. Patrick Lewis, Barlas Oguz, Ruty Rinott, SebastianRiedel, and Holger Schwenk. 2020. MLQA: Evalu-ating cross-lingual extractive question answering. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 73157330, Online. Association for Computational Lin-guistics. Patrick Littell, David R. Mortensen, Ke Lin, KatherineKairis, Carlisle Turner, and Lori Levin. 2017. URIELand lang2vec: Representing languages as typological,geographical, and phylogenetic vectors. In Proceed-ings of the 15th Conference of the European Chap-ter of the Association for Computational Linguistics:Volume 2, Short Papers, pages 814, Valencia, Spain.Association for Computational Linguistics. Fangyu Liu, Emanuele Bugliarello, Edoardo MariaPonti, Siva Reddy, Nigel Collier, and Desmond El-liott. 2021. Visually grounded reasoning across lan-guages and cultures. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 1046710485, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics.",
  "Sampling for variety. Linguistic Typology, 20(2):233296": "Justin Mott, Ann Bies, Stephanie Strassel, Jordan Kod-ner, Caitlin Richter, Hongzhi Xu, and Mitchell Mar-cus. 2020. Morphological segmentation for low re-source languages. In Proceedings of the Twelfth Lan-guage Resources and Evaluation Conference, pages39964002, Marseille, France. European LanguageResources Association. Saliha Muradoglu and Mans Hulden. 2022.Eeny,meeny, miny, moe. how to choose data for morpho-logical inflection. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 72947303, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics.",
  "Joakim Nivre, Rogier Blokland, Niko Partanen, andMichael Rieler. 2018. Universal dependencies 2.2": "Frank Palma Gomez, Subhadarshi Panda, Michael Flor,and Alla Rozovskaya. 2023. Using neural machinetranslation for generating diverse challenging exer-cises for language learner. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages61156129, Toronto, Canada. Association for Com-putational Linguistics. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-man, Kevin Knight, and Heng Ji. 2017. Cross-lingualname tagging and linking for 282 languages. In Pro-ceedings of the 55th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 19461958, Vancouver, Canada. As-sociation for Computational Linguistics. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vulic, and IrynaGurevych. 2022. xGQA: Cross-lingual visual ques-tion answering. In Findings of the Association forComputational Linguistics: ACL 2022, pages 24972511, Dublin, Ireland. Association for ComputationalLinguistics. Mat Pikuliak and Marian Simko. 2022.Averageis not enough: Caveats of multilingual evaluation.In Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL), pages 125133, Abu Dhabi, United Arab Emirates (Hybrid). As-sociation for Computational Linguistics. Barbara Plank. 2022. The problem of human labelvariation: On ground truth in data, modeling andevaluation. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1067110682, Abu Dhabi, United ArabEmirates. Association for Computational Linguistics. Edoardo Maria Ponti, Goran Glava, Olga Majewska,Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020.XCOPA: A multilingual dataset for causal common-sense reasoning. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 23622376, Online. As-sociation for Computational Linguistics.",
  "Jan Rijkhoff and Dik Bakker. 1998. Language sampling.Linguistic Typology, 2(3):263314": "Jan Rijkhoff, Dik Bakker, Kees Hengeveld, and Pe-ter Kahrel. 1993. A method of language sampling.Studies in Language. International Journal spon-sored by the Foundation Foundations of Language,17(1):169203. Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua,Aaron Phillips, and Yinfei Yang. 2020. LAReQA:Language-agnostic answer retrieval from a multilin-gual pool. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 59195930, Online. Association forComputational Linguistics. Sebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-dhant, Orhan Firat, Jinlan Fu, Pengfei Liu, JunjieHu, Dan Garrette, Graham Neubig, and Melvin John-son. 2021. XTREME-R: Towards more challengingand nuanced multilingual evaluation. In Proceedingsof the 2021 Conference on Empirical Methods inNatural Language Processing, pages 1021510245,Online and Punta Cana, Dominican Republic. Asso-ciation for Computational Linguistics. Sebastian Ruder, Ivan Vulic, and Anders Sgaard.2022. Square one bias in NLP: Towards a multi-dimensional exploration of the research manifold.In Findings of the Association for ComputationalLinguistics: ACL 2022, pages 23402354, Dublin,Ireland. Association for Computational Linguistics. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder,and Iryna Gurevych. 2021. How good is your tok-enizer? on the monolingual performance of multilin-gual language models. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 31183135, Online. Associationfor Computational Linguistics. Tanja Samardzic, Ximena Gutierrez, Christian Bentz,Steven Moran, and Olga Pelloni. 2024. A measurefor transparent comparison of linguistic diversity inmultilingual NLP data sets. In Findings of the Asso-ciation for Computational Linguistics: NAACL 2024,pages 33673382, Mexico City, Mexico. Associationfor Computational Linguistics. Farhan Samir and Miikka Silfverberg. 2023. Under-standing compositional data augmentation in typolog-ically diverse morphological inflection. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 277291, Sin-gapore. Association for Computational Linguistics. Gabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas,and Antonio Toral. 2022. DivEMT: Neural machinetranslation post-editing effort across typologicallydiverse languages. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 77957816, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics. Felix Sasaki, Claudia Wegener, Andreas Witt, DieterMetzing, and Jens Pnninghaus. 2002. Co-referenceannotation and resources: A multilingual corpus oftypologically diverse languages. In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC02), Las Palmas, Ca-nary Islands - Spain. European Language ResourcesAssociation (ELRA). Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,and Jason Wei. 2023. Language models are multi-lingual chain-of-thought reasoners. In The EleventhInternational Conference on Learning Representa-tions (ICLR). Hedvig Skirgrd, Hannah J. Haynie, Damin E.Blasi, Harald Hammarstrm, Jeremy Collins, Jay J.Latarche, Jakob Lesage, Tobias Weber, AlenaWitzlack-Makarevich, Sam Passmore, Angela Chira,Luke Maurits, Russell Dinnage, Michael Dunn,Ger Reesink, Ruth Singer, Claire Bowern, PatienceEpps, Jane Hill, Outi Vesakoski, Martine Robbeets,Noor Karolin Abbas, Daniel Auer, Nancy A. Bakker,Giulia Barbos, Robert D. Borges, Swintha Danielsen,Luise Dorenbusch, Ella Dorn, John Elliott, Gi-ada Falcone, Jana Fischer, Yustinus Ghanggo Ate,Hannah Gibson, Hans-Philipp Gbel, Jemima A.Goodall, Victoria Gruner, Andrew Harvey, Re-bekah Hayes, Leonard Heer, Roberto E. Herrera Miranda, Nataliia Hbler, Biu Huntington-Rainey,Jessica K. Ivani, Marilen Johns, Erika Just, EriKashima, Carolina Kipf, Janina V. Klingenberg,Nikita Knig, Aikaterina Koti, Richard G. A. Kowa-lik, Olga Krasnoukhova, Nora L. M. Lindvall, MandyLorenzen, Hannah Lutzenberger, Tnia R. A. Mar-tins, Celia Mata German, Suzanne van der Meer,Jaime Montoya Samam, Michael Mller, SalihaMuradoglu, Kelsey Neely, Johanna Nickel, MiinaNorvik, Cheryl Akinyi Oluoch, Jesse Peacock, In-dia O. C. Pearey, Naomi Peck, Stephanie Petit,Sren Pieper, Mariana Poblete, Daniel Prestipino,Linda Raabe, Amna Raja, Janis Reimringer, Syd-ney C. Rey, Julia Rizaew, Eloisa Ruppert, Kim K.Salmon, Jill Sammet, Rhiannon Schembri, LarsSchlabbach, Frederick W. P. Schmidt, Amalia Skil-ton, Wikaliler Daniel Smith, Hilrio de Sousa, KristinSverredal, Daniel Valle, Javier Vera, Judith Vo, TimWitte, Henry Wu, Stephanie Yam, Jingting Ye, MaisieYong, Tessa Yuditha, Roberto Zariquiey, RobertForkel, Nicholas Evans, Stephen C. Levinson, MartinHaspelmath, Simon J. Greenhill, Quentin D. Atkin-son, and Russell D. Gray. 2023a. Grambank revealsthe importance of genealogical constraints on linguis-tic diversity and highlights the impact of languageloss. Science Advances, 9(16):eadg6175. Hedvig Skirgrd, Hannah J. Haynie, Harald Ham-marstrm, Damin E. Blasi, Jeremy Collins, JayLatarche, Jakob Lesage, Tobias Weber, AlenaWitzlack-Makarevich, Michael Dunn, Ger Reesink,Ruth Singer, Claire Bowern, Patience Epps, Jane Hill,Outi Vesakoski, Noor Karolin Abbas, Sunny Ananth,Daniel Auer, Nancy A. Bakker, Giulia Barbos, AninaBolls, Robert D. Borges, Mitchell Browen, LennartChevallier, Swintha Danielsen, Sinol Dohlen, LuiseDorenbusch, Ella Dorn, Marie Duhamel, FarahEl Haj Ali, John Elliott, Giada Falcone, Anna-MariaFehn, Jana Fischer, Yustinus Ghanggo Ate, HannahGibson, Hans-Philipp Gbel, Jemima A. Goodall,Victoria Gruner, Andrew Harvey, Rebekah Hayes,Leonard Heer, Roberto E. Herrera Miranda, Na-taliia Hbler, Biu H. Huntington-Rainey, GuglielmoInglese, Jessica K. Ivani, Marilen Johns, ErikaJust, Ivan Kapitonov, Eri Kashima, Carolina Kipf,Janina V. Klingenberg, Nikita Knig, AikaterinaKoti, Richard G. A. Kowalik, Olga Krasnoukhova,Kate Lynn Lindsey, Nora L. M. Lindvall, MandyLorenzen, Hannah Lutzenberger, Alexandra Marley,Tnia R. A. Martins, Celia Mata German, Suzannevan der Meer, Jaime Montoya, Michael Mller, Sal-iha Muradoglu, HunterGatherer, David Nash, KelseyNeely, Johanna Nickel, Miina Norvik, Bruno Ols-son, Cheryl Akinyi Oluoch, David Osgarby, JessePeacock, India O.C. Pearey, Naomi Peck, Jana Pe-ter, Stephanie Petit, Sren Pieper, Mariana Poblete,Daniel Prestipino, Linda Raabe, Amna Raja, JanisReimringer, Sydney C. Rey, Julia Rizaew, EloisaRuppert, Kim K. Salmon, Jill Sammet, RhiannonSchembri, Lars Schlabbach, Frederick W. P. Schmidt,Dineke Schokkin, Jeff Siegel, Amalia Skilton, Hilriode Sousa, Kristin Sverredal, Daniel Valle, Javier Vera,Judith Vo, Daniel Wikalier Smith, Tim Witte, HenryWu, Stephanie Yam, Jingting Ye, Maisie Yong, Tessa",
  "Yuditha, Roberto Zariquiey, Robert Forkel, NicholasEvans, Stephen C. Levinson, Martin Haspelmath,Simon J. Greenhill, Quentin D. Atkinson, and Rus-sell D. Gray. 2023b. Grambank v1.0. Dataset": "Richard Sproat, Bruno Cartoni, HyunJeong Choe, DavidHuynh, Linne Ha, Ravindran Rajakumar, and EvelynWenzel-Grondie. 2014. A database for measuringlinguistic information content. In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation (LREC14), pages 967974,Reykjavik, Iceland. European Language ResourcesAssociation (ELRA). Anirudh Srinivasan and Eunsol Choi. 2022. TyDiP: Adataset for politeness classification in nine typolog-ically diverse languages. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2022,pages 57235738, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.",
  "Viveka Velupillai. 2012. An Introduction to LinguisticTypology, reprinted 2017 with corrections edition.John Benjamins Publishing Company, AmsterdamPhiladelphia, PA": "Ivan Vulic, Simon Baker, Edoardo Maria Ponti, UllaPetti, Ira Leviant, Kelly Wing, Olga Majewska, EdenBar, Matt Malone, Thierry Poibeau, Roi Reichart,and Anna Korhonen. 2020. Multi-SimLex: A large-scale evaluation of multilingual and crosslingual lexi-cal semantic similarity. Computational Linguistics,46(4):847897. Ekaterina Vylomova, Jennifer White, Elizabeth Salesky,Sabrina J. Mielke, Shijie Wu, Edoardo MariaPonti, Rowan Hall Maudslay, Ran Zmigrod, JosefValvoda, Svetlana Toldova, Francis Tyers, ElenaKlyachko, Ilya Yegorov, Natalia Krizhanovsky,Paula Czarnowska,Irene Nikkarinen,AndrewKrizhanovsky, Tiago Pimentel, Lucas Torroba Henni-gen, Christo Kirov, Garrett Nicolai, Adina Williams,Antonios Anastasopoulos, Hilaria Cruz, EleanorChodroff, Ryan Cotterell, Miikka Silfverberg, andMans Hulden. 2020. SIGMORPHON 2020 sharedtask 0: Typologically diverse morphological inflec-tion. In Proceedings of the 17th SIGMORPHONWorkshop on Computational Research in Phonetics,Phonology, and Morphology, pages 139, Online.Association for Computational Linguistics. Hongzhi Xu, Jordan Kodner, Mitchell Marcus, andCharles Yang. 2020. Modeling morphological typol-ogy for unsupervised learning of language morphol-ogy. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages66726681, Online. Association for ComputationalLinguistics. Ningyu Xu, Tao Gui, Ruotian Ma, Qi Zhang, Jingt-ing Ye, Menghan Zhang, and Xuanjing Huang. 2022.Cross-linguistic syntactic difference in multilingualBERT: How good is it and how does it affect transfer?In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages80738092, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Aditya Yadavalli, Alekhya Yadavalli, and Vera Tobin.2023. SLABERT talk pretty one day: Modelingsecond language acquisition with BERT. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1176311777, Toronto, Canada. Associationfor Computational Linguistics. Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.2021. Mr. TyDi: A multi-lingual benchmark fordense retrieval. In Proceedings of the 1st Workshopon Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo,Ehsan Kamalloo, David Alfonso-Hermelo, Xi-aoguang Li, Qun Liu, Mehdi Rezagholizadeh, andJimmy Lin. 2023. MIRACL: A multilingual retrievaldataset covering 18 diverse languages. Transactionsof the Association for Computational Linguistics,11:11141131.",
  "where the feature vectors from V consists of the GEOGRAPHY, GENETIC or SYNTACTIC vectors inlang2vec (as shown per paper in )": "Feature Value Inclusion (FVI)For a language sample L and feature vectors V we look at every vlfor l L. We check if all possible, non-missing values (i.e., x {no_cov, ?}, the values indicating nocoverage and unsure in Grambank) for a particular feature are covered (VFi). We do this for the set ofall features in Grambank F. Finally, we take the average over the total number of features:",
  "CLong Tail of Included Languages": "eng deu rus fin spa tur cmn arb fra ind jpn kor ita heb hin hun por swh pol vie ben nld ekk tam tha tel eus pes bul swe ces tgl ell urd hrv kat dan amh nor slv yor cat kaz lit ron zsm afr mar hye kan lav mal slk",
  "(a) Top portion of the long tail": "hsb glg ukr khm jav isl mlt guj mya zul que sme srp gle sqi wol evn yue krl azj fao gml nav pan cym lud vep hau ckt khk aka hat mag sot gla got lug asm cre cpx lat kmr san sna myv uig ibo ceb mri epo itl bra bxm",
  "Sasaki et al. (2002)20.840.39Co-reference resolution": "Papers that have a claim and introduce a dataset. L refers to the number of languages regarding theclaim, with indicating that one language does not have an ISO-639-3 code. MPSD is the Mean PairwiseDistance of syntactic URIEL features per language pair, with indicating that at least one language has low(< 5%) syntactic feature coverage. FVI refers to the Grambank feature value inclusion of the languages in thedataset: are all possible, non-missing values for a feature in Grambank included using the language selection.Here, refers to at least one language not being covered in Grambank, these languages are excluded from thecoverage metric. The highest and lowest values for MPSD and FVI are highlighted. Abbreviations: QA = QuestionAnswering, RE = Reasoning, NLI = Natural Language Inference, IR = Information Retrieval, ASR = AutomaticSpeech Recognition, MT = Machine Translation.",
  "EXTREME-R Results by Word Order": "In we can see that SVO is the most represented group for all tasks, with SOV coming in second.After that, the representation of features tapers off quickly, with (the rather rare) OSV, OVS, and VOSword orders having no coverage at all. We can see that the performance of models for SOV and VSO isconsistently lower than SVO. This can be due to the difference in the number of languages included forthe group, although we did not see this effect for the inflectional feature ().",
  "LAReQAXLM-R-L40.75 (11)39.31 (11)-1.44- (0)- (0)- (0)42.10 (6)39.75 (2)34.60 (1)40.80 (2)- (0)mBERT21.58 (11)19.75 (11)-1.83- (0)- (0)- (0)24.10 (6)15.10 (2)17.00 (1)22.80 (2)- (0)": "TyDiQAXLM-R-L64.29 (9)62.80 (9)-1.49- (0)- (0)- (0)67.26 (5)58.55 (2)62.60 (2)- (0)- (0)mBERT58.36 (9)56.91 (9)-1.44- (0)- (0)- (0)61.24 (5)55.55 (2)53.95 (2)- (0)- (0)mT581.94 (9)81.45 (9)-0.50- (0)- (0)- (0)82.94 (5)78.40 (2)83.00 (2)- (0)- (0) XQuADXLM-R-L77.21 (11)77.11 (11)-0.10- (0)- (0)- (0)76.70 (6)76.55 (2)74.40 (1)80.80 (2)- (0)mBERT65.05 (11)63.55 (11)-1.50- (0)- (0)- (0)67.35 (6)56.60 (2)62.20 (1)68.05 (2)- (0)mT581.54 (11)81.04 (11)-0.49- (0)- (0)- (0)82.22 (6)79.10 (2)80.30 (1)82.55 (2)- (0) XNLIXLM-R79.24 (15)78.57 (15)-0.67- (0)- (0)- (0)80.31 (9)75.10 (3)77.20 (1)81.65 (2)- (0)mBERT66.51 (15)65.79 (15)-0.72- (0)- (0)- (0)68.19 (9)60.03 (3)66.00 (1)68.95 (2)- (0)mT584.85 (15)84.71 (15)-0.14- (0)- (0)- (0)85.39 (9)81.83 (3)84.50 (1)87.10 (2)- (0)",
  "UD-POSXLM-R-L74.96 (38)77.45 (36)+2.50- (0)- (0)- (0)74.59 (20)69.65 (10)71.55 (2)86.97 (4)84.50 (2)mBERT70.90 (38)71.60 (36)+0.69- (0)- (0)- (0)72.74 (20)62.82 (10)61.25 (2)83.22 (4)77.95 (2)": "XTREME-R task and model results, grouped by word order (WALS 81A). Overall refers to the averageover all languages. By Feature is the average of the WALS feature averages, excluding languages for which there isno coverage. The delta shows the difference of Overall and By F. Tasks that have coverage for all their includedlanguages are on top, those partially covered are in the bottom portion. The word orders are listed in the followingcolumns, with NDO being No Dominant Order and NA referring to Not Available (in WALS). The highest and thelowest number of languages per grouping are highlighted. The (italicized) number refers to the number of languagesin a particular subset. Metrics: = Accuracy, = mAP@20, = F1."
}