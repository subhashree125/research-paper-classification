{
  "Abstract": "This work suggests fundamentally rethinkingthe current practice of pruning large languagemodels (LLMs). The way it is done is by di-vide and conquer: split the model into sub-models, sequentially prune them, and recon-struct predictions of the dense counterparts onsmall calibration data one at a time; the nalmodel is obtained simply by putting the re-sulting sparse submodels together. While thisapproach enables pruning under memory con-straints, it generates high reconstruction errors.In this work, we rst present an array of recon-struction techniques that can signicantly re-duce this error by more than 90%. Unwittingly,however, we discover that minimizing recon-struction error is not always ideal and can over-t the given calibration data, resulting in ratherincreased language perplexity and poor perfor-mance at downstream tasks. We nd out that astrategy of self-generating calibration data canmitigate this trade-off between reconstructionand generalization, suggesting new directionsin the presence of both benets and pitfalls ofreconstruction for pruning LLMs.1",
  "Overview": "Large language models (LLMs) have shown re-markable potential and achieved tremendous suc-cesses in various domains (Brown et al., 2020; Sing-hal et al., 2023; Roziere et al., 2023). Nevertheless,running them requires a signicant amount of com-putations and memory, raising concerns about ac-cessibility, sustainability, and scalability (Strubellet al., 2019; Bender et al., 2021). Neural networkpruning holds great promise for mitigating this is-sue (LeCun et al., 1989; Hoeer et al., 2021). Acomplication here is that the standard approach isnot quite feasible since it usually involves an exten-",
  "(b) Effects of self-generated data on mitigating overtting": ": (a) Reconstruction techniques signicantlyreduce the compounding errors and lead to a substan-tial reduction of error in the nal block. ReconstructionO and X refer to the results with and without the pro-posed reconstruction techniques (BR, GP, CR) respec-tively. (b) Minimizing reconstruction error may not al-ways be ideal since models can overt calibration data(we show this in .2). Using our self-generatedcalibration data in the reconstruction process mitigatesthis issue quite effectively by decreasing test error, per-plexity, and error rates for downstream tasks. sive training process (and training data) which ischallenging to carry out for LLMs.To address this issue, LLM pruning is done posttraining. Specically, it could be formulated as areconstruction problem as follows:",
  "i.e., given a pre-trained model w, the goal is to nda pruning mask m such that the resulting sparsemodel m d w reconstructs the predictions of the": "original dense model fp w; q on some calibrationdata D; here, d denotes element-wise product forvectorized representations, and m needs to sat-isfy a given sparsity constraint k. If the objectivecriterionreconstruction erroris minimized tozero, then we achieve the perfect reconstructionand thereby pruning results.While one could now avoid training LLMs fromscratch with (1), it still requires as much memoryas of the given LLM, hindering development un-der memory constraints. To circumvent this issue,many recent works take a divide-and-conquer ap-proach: i.e., split the model into a sequence ofsmaller submodels, prune and reconstruct each sub-model individually, and simply put all resultingsparse submodels together (Frantar and Alistarh,2023; Sun et al., 2024; Zhang et al., 2024). Albeitfairly effective, we nd that this can easily createcritically high compounding errors. This is becausesolutions for each subproblem yield non-zero re-construction errors.In this work, we address the reconstruction errorminimization for pruning LLMs with the followingthree major pillars. First, we focus on developingvarious engineering techniques to reduce this error.These are inspired to lessen the suboptimality ofsubsolutions by incorporating different levels ofextension schemes. Second, we suggest that reduc-ing this error is not necessarily favorable, however.Our extensive experimental results indicate that itis possibly due to overtting, given limited calibra-tion data and high problem complexity. Third, wepresent useful strategies to potentially mitigate therisk of reconstruction and improve generalization.This is based on what we call the self-generationof calibration data.Briey, this work investigates the benets andpitfalls of the reconstruction error minimizationscheme for pruning LLMs. To our best knowledge,this trade-off has not been explicitly identied orstudied before, thereby suggesting rethinking thecurrent practice. Our initial investigations mayshed light on some potential future research direc-tions. We summarize our main results in .",
  "Block-wise reconstruction (BR)The seminalwork of Frantar and Alistarh (2023) proposes toreconstruct predictions layer-wise based on least": "squares. By removing non-linearity this approachyields a closed-form solution. However, we ndthat this can create a high reconstruction error sincethe system is highly underdetermined (i.e., thereare much more parameters than calibration data).To reduce compounding errors, we rst considerextending the unit of optimization target from alayer to a block of layers. Specically, this meansa block-wise reconstruction (BR) which can be for-mulated as follows:",
  "i1}gip wi; xiq gip mi d wi; xiq}22 (2)": "where gi refers to the i-th block of layers (e.g., aTransformer block) in which we have the optimiza-tion variables wi, and xi denotes the inputs to thei-th block which originally come from calibrationdata; here, the pruning mask m is xed assumingthat it is already obtained from an arbitrary pruningmethod. I.e., the goal is to update variables in eachblock to minimize the extended reconstruction er-rors. We solve this problem iteratively using thestandard gradient-based method. Notably a similarapproach is also proposed in the concurrent workof Guo et al. (2024), and we nd in our experimentsthat BR is extremely effective in reducing the re-construction errors in .1. We illustrate theidea of BR in . Global propagation (GP)While the generaldivide-and-conquer principle is quite functional,we identify a potential issue therein: by sequen-tially solving the subproblem, it is constantly ttingpractically suboptimal solutions obtained from theprevious step (which become gradually worse), aswith xi gi1p mi1 d wi1; xi1q. We realizethat this is another source of compounding errors,and thus, suggest that when we locally reconstructa model, at least we use global propagation (GP)from the original dense model as input to the targetreconstruction; i.e., xi gi1p wi1; xi1q. Weshow that GP improves the reconstruction resultsquite signicantly in .1. We further notethat a similar principle is found in various appli-cations including low-rank approximation (Zhanget al., 2015), channel pruning (He et al., 2017),and quantization (Nagel et al., 2020; Hubara et al.,2021). We illustrate the idea of GP in .",
  "f": ": An illustration of reconstruction techniques for pruning large language models. Here, we want the sparsemodel fpm d w; q to reconstruct the prediction of the dense model on some calibration data D. LR, BR, GP, andCR each correspond to layer-wise reconstruction, block-wise reconstruction, global propagation, and cross-blockreconstruction. Here, solid and dashed arrows each represent the inputs coming from sparse and dense models. multiple blocks and stitch the solutions in betweenby connecting via the adjacent blocks. Specically,this means that now g in (2) becomes a compositeof multiple blocks, say h, and we ensure h overlaps;more precisely, hi gi gi1 and hi`1 gi`1 gifor two blocks, and so on for all blocks. This way,namely cross-block reconstruction or CR (Dinget al., 2023), we can potentially bridge betweensubsolutions by taking into account some interac-tion between adjacent blocks, and hence, reducethe compounding errors. We illustrate the idea of CR in .To elaborate further, the difference between BRand CR is that while BR is about updating param-eters within a block (thus it is not concerned withhow to combine subsolutions), CR takes a step fur-ther and is about stitching the subsolutions; i.e., CR updates parameters within two adjacent blocks,and when it comes to reconstructing the next block,it includes the overlapping block so that it has theeffect of stitching. This method is found to bequite effective for reducing the error, however, wend that this method can often lead to overtting.We discuss this in detail in .2.",
  "Reconstruction error": "We rst evaluate the effectiveness of the suggestedtechniques in reducing the reconstruction error.Here, we focus on pruning LLaMA-7B (Touvronet al., 2023) and OPT-125M (Zhang et al., 2022)to unstructured 50% sparsity with three pruningmethods: SparseGPT (Frantar and Alistarh, 2023),Wanda (Sun et al., 2024), and Magnitude (Hanet al., 2015). For each pruning method, we exam-ine four reconstruction strategies: layer-wise re-construction (LR), block-wise reconstruction (BR),block-wise reconstruction with global propaga-tion (BR+GP), and cross-block reconstruction withglobal propagation (BR+GP+CR). Following theconvention, we use 256 calibration data randomly Block index Error (normalized) LRBRBR+GPBR+GP+CR",
  "(b) Wanda": ":Results of reconstruction techniques forLLaMA-7B. They constantly reduce the compound-ing errors, achieving a signicant decrease at the nalblock ( 90%). We nd this trend is consistent acrossdifferent settings. See Figures 5 and 6 of Appendix Bfor more results. sampled from C4 (Raffel et al., 2020) each contain-ing 1024 tokens. We run the Adam optimizer for10 epochs (see Appendix A for details). The resultsare presented in .We can see that all the reconstruction techniquesreduce the compounding errors quite signicantly,yielding a substantial reduction at the nal block.Specically, BR rst reduces the nal error by atleast 50% across all pruning methods comparedto LR, BR+GP further reduces the error by at least60% compared to BR, and nally, BR+GP+CR re-duces the error by at least 20% compared to BR+GP.Consequently, we observe that the error is reducedfrom 87% to 94% with BR+GP+CR compared tothe baseline LR.",
  "Generalization performance": "We now evaluate the generalization performancesof the reconstruction results. Specically, we mea-sure the perplexity of the pruned model on threedifferent datasets: raw-Wikitext2 (Merity et al.,2017), PTB (Marcus et al., 1994), and validationdata of C4. We also measure its zero-shot task per-formance in accuracy on seven downstream tasks:BoolQ (Clark et al., 2019), RTE (Wang et al., 2019),",
  "(b) LLaMA-7B": ":Reconstruction errors of OPT-125M andLLaMA-7B on test data (raw-Wikitext2) as well as cal-ibration data. Overtting by CR is only observed forthe larger LLaMA-7B model. We nd that larger mod-els in general are more susceptible to overtting. SeeTables 3 and 4 of Appendix B for more results. HellaSwag (Zellers et al., 2019), Winogrande (Sak-aguchi et al., 2020), ARC Easy and Challenge(Clark et al., 2018), and OpenbookQA (Mihaylovet al., 2018). The results are presented in .At rst, we nd that the perplexity effectivelydecreases with BR and GP; the value reduces acrossall test cases including different models, pruningmethods, and datasets. Unexpectedly, however, theperplexity rather increases when we add CR despitethe reduced reconstruction errors. We also observea similar trend in zero-shot performance for Wandaand Magnitude pruning, with mean accuracy in-creasing by a large margin with BR and GP butdecreasing with CR. Interestingly, for SparseGPT,reconstruction techniques do not generally helpzero-shot performance. We hypothesize that it isbecause SparseGPT already conducts fairly heavyoptimization compared to other methods, and ap-plying further reconstruction on particular calibra-tion data may not help improve zero-shot perfor-mance since it is more sensitive to distribution shift.Furthermore, we nd that such overting tends tooccur more for LLaMA-7B than OPT-125M (see). This is possibly due to model size; i.e., given the same amount of (limited) calibration data,over-optimizing can make large models more likelyto overt and lead to poor generalization.We can summarize our ndings are as follows. BR and GP are found to be very effective inreducing perplexity in all cases; on the otherhand, CR often leads to overtting, especiallyfor large models. This holds true for zero-shot performance aswell, with only exception of SparseGPT, forwhich BR and GP do not help much in improv-ing zero-shot performance; this is possiblydue to the fact that SparseGPT already con-ducted fairly heavy optimization of remainingweights. It is also possible that adapting todownstream task is more prone to overtting.This certainly requires more investigations.In short, we can attempt to say without much lossof generality that BR and GP can generally helpfor pruning LLMs in terms of reducing perplexity.",
  "Further Exploration": "We have seen that reconstruction techniques areuseful but they can lead to undesirable overtting.Here we explore potential ways to alleviate thisrisk. In particular, we identify that the calibrationdata is highly limited in two aspects: it is too little(compared to optimization variables)2 and does notrepresent the training data (as it is arbitrarily given);the former is related to the general representation-generalization complexity trade-off, and the latteris about whether the reconstruction can mimic thebehavior of the original model. 2This can be especially problematic for domain-specicLLMs, e.g., healthcare (Singhal et al., 2023; Luo et al., 2022)and nance (Wu et al., 2023; Yang et al., 2023), where obtain-ing real-world data can be highly challenging due to privacyconcerns. # of self-generated data 1.5 2.0 2.5 3.0 Error (normalized)",
  "(b) Perplexity": ": Effects of self-generated calibration data on(a) reconstruction error for test data (raw-Wikitext2)and (b) perplexity for LLaMA-7B; they both improvewith more self-generation. See of Appendix Bfor more results. To this end, we reect on the fact that what weare dealing with is a generative (language) model,meaning that we can create calibration data thatis potentially much bigger in size and closer tothe original distribution. We nd that this self-generation technique has recently been proposed inother contexts (Meng et al., 2022; Ye et al., 2022;Liu et al., 2023; Li et al., 2024), and thus, followthe process therein to produce high-quality textdata. Using that, we perform reconstruction again,and the results are reported in . We observethat making use of more self-generated calibrationdata (without unfairly violating the given setting)reduces both test error and perplexity, mitigatingovertting quite effectively.",
  "Conclusion": "In this work, we take a close look at the currentpractice of minimizing reconstruction errors forpruning LLMs. We rst nd that with various re-construction techniques, one can reduce the errorquite signicantly and improve quality of pruningresults on both language perplexity and zero-shotaccuracy. Nevertheless, it turns out that decreasingerror as it is now is not always desirable since itmay cause overtting calibration data. We presentinitial results that this issue can be potentially miti-gated by self-generating calibration data. There aremany remaining possibilities, and we believe ourndings suggest opportunities for future work.",
  "Limitations": "There remain several limitations in our experimentsand we plan to address these in future work. First,our main experiments are limited to LLaMA-7Band OPT-125M. We intend to scale up our experi-ments to much larger models of up to 70B param- eters and different architectures including Mixtral(Jiang et al., 2024) or Gemma (Team et al., 2024).Next, reconstruction techniques BR, GP, and CRrequire additional memory compared to LR, al-though they still use much less memory comparedto model-level reconstruction of solving (1) (seeAppendix B for the details). We plan to introduceparameter-efcient optimization (Hu et al., 2022)to alleviate this increased memory burden.Although the self-generation of calibration dataeffectively mitigates overtting, it requires morecomputation for reconstruction. Finally, we ndthat some portions of the generated texts are farfrom plain English texts and thus may not serve asgood calibration data (see of Appendix Cfor the examples). In this regard, we believe that re-ducing the number of these irrelevant examples andgenerating only a few number of high-quality textscan be a potential way to improve performance andincrease efciency.",
  "Acknowledgements": "This work was partly supported by the Institute ofInformation & communications Technology Plan-ning & Evaluation (IITP) grant funded by theKorean government (MSIT) (RS-2019-II191906,ArticialIntelligenceGraduateSchoolPro-gram (POSTECH); RS-2022-II220959/No.2022-0-00959, (part2) Few-Shot learning of Causal Infer-ence in Vision and Language for Decision Making;RS-2024-00338140, Development of Learning andUtilization Technology to Reect Sustainability ofGenerative Language Models and Up-to-Datenessover Time) and the National Research Foundationof Korea (NRF) grant funded by the Korean gov-ernment (MSIT) (RS-2023-00210466, RS-2023-00265444, RS2023-0021371). Sungbin Shin wassupported by Kwanjeong Educational FoundationScholarship.",
  "Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifculty of natural yes/no questions. NAACL": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Xin Ding, Xiaoyu Liu, Yun Zhang, Zhijun Tu, Wei Li,Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong,Baoqun Yin, et al. 2023. Cbq: Cross-block quan-tization for large language models. arXiv preprintarXiv:2312.07950.",
  "Elias Frantar and Dan Alistarh. 2023.SparseGPT:Massive language models can be accurately prunedin one-shot. ICML": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-man, Sid Black, Anthony DiPo, Charles Foster,Laurence Golding, Jeffrey Hsu, Alain Le Noach,Haonan Li, Kyle McDonell, Niklas Muennighoff,Chris Ociepa, Jason Phang, Laria Reynolds, HaileySchoelkopf, Aviya Skowron, Lintang Sutawika, EricTang, Anish Thite, Ben Wang, Kevin Wang, andAndy Zou. 2023.A framework for few-shot lan-guage model evaluation.",
  "Liang Li, Qingyuan Li, Bo Zhang, and XiangxiangChu. 2024. Norm tweaking: High-performance low-bit quantization of large language models. AAAI": "Zechun Liu, Barlas Oguz, Changsheng Zhao, ErnieChang, Pierre Stock, Yashar Mehdad, YangyangShi, Raghuraman Krishnamoorthi, and Vikas Chan-dra. 2023. Llm-qat: Data-free quantization awaretraining for large language models. arXiv preprintarXiv:2305.17888. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, ShengZhang, Hoifung Poon, and Tie-Yan Liu. 2022.Biogpt:generative pre-trained transformer forbiomedical text generation and mining. Briengs inbioinformatics.",
  "Markus Nagel, Rana Ali Amjad, Mart Van Baalen,Christos Louizos, and Tijmen Blankevoort. 2020.Up or down?adaptive rounding for post-trainingquantization. ICML": "Satya Sai Srinath Namburi, Makesh Sreedhar, SrinathSrinivasan, and Frederic Sala. 2023.The cost ofcompression: Investigating the impact of compres-sion on parametric knowledge in language models.EMNLP 2023 Findings. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unied text-to-text trans-former. JMLR. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, et al. 2023.Code llama:Open foundation models for code.arXiv preprint arXiv:2308.12950.",
  "Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter.2024. A simple and effective pruning approach forlarge language models. ICLR": "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology.arXivpreprint arXiv:2403.08295. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023.Llama: Open and ef-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "AExperimental Details": "Experiment congurationsWe run our experi-ments with a single A100 GPU having 80GB ofmemory. For BR and CR, we run the Adam opti-mizer for 10 epochs with a batch size of 8, withoutweight decay or gradient clipping. The learningrate is set to 0.0002 and decays linearly followingGuo et al. (2024). For evaluating the performanceon downstream tasks, we use the EleutherAI-evalharness framework (Gao et al., 2023).",
  "Calculation of normalized reconstruction errorThe reconstruction error for i-th block is calcu-lated as1": "NHT }gip wi; xiqgipmidwi; xiq}22 whereN, H, T each represent the number of calibra-tion data, hidden dimension, and the token length.xi, xi represent the inputs coming from dense andsparse blocks respectively. Licenses and uses of models and datasetsLLaMA (Touvron et al., 2023) and OPT (Zhanget al., 2022) are released under non-commercialbespoke licenses. raw-Wikitext2 (Merity et al.,2017), PTB (Marcus et al., 1994), and C4 (Raf-fel et al., 2020) are released under CC BY-SA4.0, LDC user agreement, and ODC-By. BoolQ(Clark et al., 2019), RTE (Wang et al., 2019), Hel-laSwag (Zellers et al., 2019), Winogrande (Sak-aguchi et al., 2020), ARC (Clark et al., 2018),and OpeenbookQA (Mihaylov et al., 2018) are re-leased under CC BY-SA 3.0, Apache 2.0, MITLicense, Apache 2.0, CC BY-SA 4.0, and Apache2.0 respectively. We conrm that these models anddatasets are used for their intended use and the datadoes not contain personal information. EleutherAI-evalharness framework is released under the MITLicense.",
  "Example numberText": "1Americas, and the U.K., while 18 other countries have legalized the medical use of cannabis. The latest announcement is a win for Canadians ...2apprehension of the inevitability of death? And, therefore, how could such a person come to believe ...3# + this.currentID + .\\n };\\n\\n return {\\n next: next,\\n previous: previous,\\n}...4Picker.setSelected(false);\\n \\n actionPhrasesTableModel.reTableDataChanged();\\n ...",
  "peak memory (GB)3.95.75.710.6 100": ": Peak GPU memory for LLaMA-7B and sparseGPT. Compared to LR, reconstruction techniques incuradditional GPU memory but it is quite marginal compared to ne-tuning the full model. The results are obtainedwith the batch size of 8 and gradient accumulation. For full ne-tuning, the results are from Malladi et al. (2023). intensive, thus many recent work suggest divide-and-conquer such as LR and BR. In the work ofFrantar and Alistarh (2023), the authors show thatfor the 175B parameter OPT model it requires atleast ve A100 GPUs of 80GB, whereas by us-ing LR it reduces down to a single A100 GPU of80GB. In our experiments, for Llama-7B, both LRand BR+GP+CR can all be done on a commodity3090 GPU of 24GB memory; it requires more than100GB to perform full ne-tuning of LLaMA-7B(Malladi et al., 2023). In theory, optimizing moreparameters can incur more memory footprints, andthus, in the order of LR GP BR CR, therewill be more memory usage.The exact amount depends on the specic model.To provide solid evidence, we ran proling peakGPU memory for LLaMA-7B with the batch sizeof 8 (see for the results). Compared to LR,reconstruction techniques surely incur additionalGPU memory, however, (i) it is quite marginal com-pared to ne-tuning the full model, and (ii) it couldbe reduced further by introducing memory reduc-tion techniques in practice such as CPU ofoadingand gradient checkpointing. Pruning attention vs. feed-forwardWe also in-vestigated the effects of only pruning attention vs.feed-forward blocks for different reconstructiontechniques. Here, we conducted experiments forOPT-125m and SparseGPT by pruning either at-tention or feed-forward blocks to 50% sparsity andmeasuring the perplexity on raw-Wikitext2. Theresults are provided in . We rst observethat pruning both attention and feed-forward yieldsthe largest performance drop. Also, we nd thatpruning only the attention block leads to worse performance compared to pruning only the feed-forward block, which is consistent with the ndingsin the previous work (Namburi et al., 2023). In-terestingly, we nd that reconstruction techniquescan be more effective for cases with poor perfor-mance; i.e., in the order of pruning all blocks >pruning attention > pruning feed-forward, BR, GP,",
  "CDetails on Self-generation ofCalibration Data": "We generate additional calibration data from theoriginal dense model. Here, we sample 10240 num-ber of English texts each containing 2048 tokens.Specically, we rst randomly choose the initialtoken and generate four subsequent tokens by de-terministically selecting top-1 predictions, similarto Liu et al. (2023). Here, we resample the tokensif the generated texts are not detected as English.Then, we stochastically generate the remaining to-kens until the <EOS> token is produced or thesequence length exceeds 2048. Finally, the addi-tional calibration data can be obtained by samplinga subset of generated texts and randomly selectingthe intermediate 1024 tokens for each text.Examples of self-generated texts are presentedin . Examples 1 and 2 are plain Englishtexts and can serve as good calibration data. How-ever, we observe that programming codes such asexamples 3 and 4 are often generated, which mightnot serve as good calibration data for improving theperplexity for English texts or accuracy for down-stream tasks which are not related to code genera-tion. In this regard, we believe that generating only"
}