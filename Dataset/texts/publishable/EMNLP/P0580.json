{
  "Abstract": "Recent studies in Retrieval-Augmented Gener-ation (RAG) have investigated extracting evi-dence from retrieved passages to reduce com-putational costs and enhance the final RAGperformance, yet it remains challenging. Ex-isting methods heavily rely on heuristic-basedaugmentation, encountering several issues: (1)Poor generalization due to hand-crafted contextfiltering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length dueto sentence-wise filter learning. To addressthese issues, we propose a model-based evi-dence extraction learning framework, SEER,optimizing a vanilla model as an evidence ex-tractor with desired properties through self-aligned learning. Extensive experiments showthat our method largely improves the final RAGperformance, enhances the faithfulness, help-fulness, and conciseness of the extracted ev-idence, and reduces the evidence length by9.25 times.The code will be available at",
  "Introduction": "Recent years have witnessed the prevailing windsof Retrieval-augmented Generation (RAG), whichis a popular paradigm for improving the perfor-mances of Large Language Models (LLMs) in var-ious downstream tasks, such as question answer-ing, making the output more reliable (Lewis et al.,2020; Chen et al., 2023; Jiang et al., 2023b; Ramet al., 2023), interpretable (Guu et al., 2020; Louiset al., 2024), and adaptable (Xu et al., 2023; Za-kka et al., 2024). Traditional practices (Karpukhinet al., 2020; Min et al., 2019) often involve provid-ing top-retrieved passages as the input context toLLMs without discrimination. However, imperfectretrieval systems frequently yield irrelevant content.Furthermore, indiscriminately feeding all retrievedcontent to LLMs will cause input redundancy, im-",
  "Corresponding author": "posing a high computational cost and making themprone to hallucination (Shi et al., 2023).Ideally, LLMs should be grounded on support-ing content that is both highly helpful to addressuser input and sufficiently concise to facilitate infer-ence speed. However, it is practically impossiblefor imperfect retrieval systems to achieve such anideal grounding solely (Wang et al., 2023). In fact,top-retrieved passages usually compose supportingand distracting content, inflicting a heavy blow onLLMs trained with high-quality corpora to generatethe correct output. This motivates us to develop anevidence extractor, that aims at extracting support-ing content while filtering out distracting content.Recently, a pioneering study, FILCO (Wanget al., 2023), attempts to retrieve chunking doc-ument content with sentence precision via three fil-ters, i.e., StrInc, Lexical, and CXMI. Then, it trainsa context filtering model, using context filteredby the above three measures as ground truth. De-spite effectiveness, current context-filtering meth-ods have several limitations: (1) Hand-craftedContext Filtering. Manually designed context-filtering measures typically require domain knowl-edge, which can hardly be adaptable to diversedownstream tasks with limited supervision. (2) Dis-ruptive Chunking on Context. The use of chunk-ing strategies may be ineffective as rule-based split-ting on context usually cannot preserve its origi-nal semantics and often produces semantically de-ficient text blocks. (3) Skewed Distribution inLength. The length of supporting content in top-retrieved passages may vary largely across differentsamples. Hence, learning to filter context sentence-wise is biased toward skewed length distribution.Given these limitations, an interesting ques-tion arises: Now that heuristic-based augmenta-tion1 suffers from several issues, can we developa model-based augmentation method free of the",
  ": The RAG pipeline with the evidence extrac-tor, in which the supporting content and the distractingcontent are marked in green and yellow, respectively": "above problems? Inspired by the recent success ofself-alignment (Li et al., 2023a; Zhang et al., 2024;Liang et al., 2024), self-aligned learning utilizes themodel to improve itself and aligns its response withdesired properties, which can mitigate the heavy re-liance on hand-crafted context filtering, rule-basedcontext chunking, and sentence-wise filter learning.Given the extracted evidence, a question arisesagain: How to evaluate the quality of evidenceproperly? In principle, the evidence should befaithful (i.e., avoiding intrinsic hallucination) to theretrieved passages (Rashkin et al., 2021; Maynezet al., 2020), helpful in addressing the user input(Adlakha et al., 2023), and concise to facilitate theinference speed (Ko et al., 2024). showsthree representative scenarios: (1) When the evi-dence only favors faithfulness, LLMs may generatean incorrect answer; (2) When the evidence furtherfavors helpfulness but lacks conciseness, LLMsattention may be distracted by noise; (3) Whenthe evidence favors all three criteria, LLMs cangenerate confidently with low computational costs.In this paper, we propose a model-based ev-idence extraction learning framework, SEER,Self-Aligned Evidence Extraction for Retrieval-Augmented Generation. Specifically, it consists ofthree primary stages: (1) Evidence Extraction: Tomitigate the issues above, we propose extractingdiversified evidence with semantic consistency andvarying length through response sampling, offer-ing sufficient preference data for alignment. (2)Expert Assessment: For each extracted evidence,we construct a quadruple, QuadQARE, made upof query, answer, passage, and evidence. Then,we devise three experts to assess the quality ofeach extracted evidence w.r.t. three primary criteria.Given these scores, we propose smoothing CoV-Weighting, which explicitly leverages the statisticsto estimate their relative weighting and result in the CoV-Weighted scores. (3) Self-Alignment: With aranking list of extracted evidence and their smooth-ing CoV-weighted scores, a question remains: Howto optimize extraction preference with the rankingposition? To this end, we propose a listwise-awareLambda Preference Optimization method, LPO, as-signing each preference pair with a listwise-awareweight scaled by the gain in Reciprocal Rank fromswapping the position of two evidence (Donmezet al., 2009; Burges et al., 2006; Wang et al., 2018).It is worth mentioning that SEER is a criterion-agnostic framework and can employ any off-the-shelf expert. In this work, we use faithfulness,helpfulness, and conciseness, which are regardedas three primary criteria for assessing the quality ofevidence (Maynez et al., 2020; Rashkin et al., 2021;Adlakha et al., 2023; Ko et al., 2024). Our maincontributions can be summarized as four-folds:",
  "Problem Formulation": "In this task, we are given a base extractor E, anda fixed generator G, where we choose Llama2-7b-Chat (Touvron et al., 2023) as the backbone forthe base extractor E. For a given query q and itscorresponding golden answer a, we assume a setof retrieved passages P = {pi}Ki=1, where K is theretrieved size. Here, we aim to fine-tune the baseextractor E via self-alignment to get the alignedextractor E, for the generator G to leverage thebetter evidence and achieve superior performance:",
  "Augmentation Analysis": "As stated in , heuristic-based augmenta-tion suffers from several issues, which severelyhinders the optimization of context filtering. Toverify the above claim, we compare the context rel-evance between heuristic-based and model-basedaugmentation, where the context relevance is thecosine similarity between the extracted evidenceand the user query2. Here, we use StrInc as the rep-resentative heuristic-based augmentation method(abbreviated as StrInc Heur-based Aug), as itusually performs best on QA tasks according to(Wang et al., 2023). On the other hand, we performmodel-based augmentation by response sampling(More details can be seen in 3.1). We take thebest-performing extracted evidence for each QApair as Upper Model-based Aug while the worst-performing one as Lower Model-based Aug.We experiment on three datasets, i.e., NQ, TQA,and HotpotQA. shows that: (1) The con-text relevance of Upper Model-based Aug is con-sistently higher than that of StrInc Heur-based Aug.(2) The context relevance of StrInc Heur-based Auggenerally lies in the middle of Upper and LowerModel-based Aug. From the above observations,our claim is well-validated, as model-based aug-mentation shows a larger potential than heuristic-based one. Therefore, it is valuable to conductmodel-based augmentation for better performance.",
  "Evidence Extraction Stage": "As stated in , heuristic-based augmenta-tion (Wang et al., 2023) suffers from several is-sues. An empirical study (2.2) further indicatesthat model-based augmentation is more beneficialfor performance improvement than heuristic-basedaugmentation. Hence, we aim to utilize the baseextractor E to improve itself and align it with de-sired properties. To this end, we probe into itsevidence extraction preference by response sam-pling for preference data collection. Specifically,given a query q and its retrieved passage P, weprompt the model to generate multiple candidateextracted evidence {ei}Mi=1 via response samplinge E(|q P), where M is the sample size.However, LLMs often tend to be overconfidentin their knowledge (Xiong et al., 2023). As such,the response distribution typically follows a power-law, where head responses occupy a large portionof extracted evidence while long-tail ones are verysparse.Directly using the power-law responsedistribution for alignment would cause preferenceoptimization to be biased toward head responses.Hence, we remove duplicates and obtain the uni-formly distributed set, i.e., {ei}Ni=1, where we usen-gram similarity (Kondrak, 2005) to detect dupli-cates and N is the remaining size. In practice, wefind using the uniform response distribution doesmatter for alignment to reach higher performance.",
  "Expert Assessment Stage": "Although the base extractor is able to extract ev-idence, its output might be unfaithful, unhelpful,as well as unconcise, which are regarded as threeprimary factors that hinder the quality of evidence(Maynez et al., 2020; Rashkin et al., 2021; Ad-lakha et al., 2023; Ko et al., 2024). Consideringthe above issues, we devise three experts to assessthe quality of extracted evidence w.r.t. faithfulness,helpfulness, and conciseness3, respectively. Sub-sequently, given multiple scores for each extractedevidence, we devise a smoothing CoV-Weightingschema in order to get the overall assessment score.",
  ": The overall system framework of our SEER, which mainly consists of three modeling stages": "Obtaining Oracle Scores.For expert assess-ment stage, we first collect a set of QuadQARE< q, a, P, e >, where a Quadruple is composedof Query q, Answer a, Retrieved passage P, andextracted Evidence e. Afterwards, we design threeplug-and-play experts to parallelly assess the qual-ity of extracted evidence, from different aspects: Faithfulness Expert. It focuses on the faith-fulness of each extracted evidence.Towardthis end, we adopt an advanced NLI model,ALIGNSCORE4 (Zha et al., 2023), to evaluatethe consistency between the retrieved passage Pand extracted evidence e in terms of hallucina-tion. Specifically, we treat the retrieved passageand the corresponding extracted evidence as thepremise and hypothesis, respectively. Then, weemploy ALIGNSCORE to measure to what extentthe extracted evidence e could be entailed by theretrieved passage P, which can be formulated as:",
  "We use ALIGNSCORE-large for faithfulness assessment": "Helpfulness Expert. It examines the helpfulnessof each extracted evidence candidate in terms ofoutput improvement. In other words, it checkswhether the extracted evidence e contributes tothe models output improvement when utilizedas input. Specifically, we assess its potentialinfluence on LLMs by calculating the changein the log probability of generating the goldenanswer a between the models output before andafter the inclusion of the extracted evidence e:",
  ",(3)": "where sh is the helpfulness score, f()is the helpfulness expert5, Sig() is the sigmoidfunction. Similarly, if the extracted evidence e ishelpful for LLMs to output the golden answer a,the score is close to 1, otherwise, it is close to 0. Conciseness Expert. If only the above two ex-perts are considered, the aligned extractor caneasily be achieved by directly treating the re-trieved passage as evidence. To avoid such atrivial solution, we further measure the concise-ness of the extracted evidence e. Towards this",
  "sc = SBERTcosine(t, e),(4)": "where sc is the conciseness score thatis measured by cosine similarity between the sen-tence embedding of t and e, t is a full-lengthanswer. In this work, we prompt GPT-3.5-turboto generate a full-length answer t given the queryq and its answer a. More details about full-lengthanswer generation can be seen in Appendix B. Weighting Oracle Scores. Having obtained the or-acle scores, a question naturally arises: How to getthe overall assessment for each extracted evidence?A straightforward way is to compute the average ofthe oracle scores. However, equal weighting mightnot result in optimal alignment, since the learningdifficulty is inconsistent. Therefore, the weightsshould match the learning difficulty to guide thepreference optimization process. Given this, wepropose smoothing CoV-weighting, leveraging thevariability of the scores in relation to the mean:",
  "cf = f/f,(5)": "where f and f denote the standard deviationand the mean of faithfulness score sf, cf is theCoefficient of Variation (CoV) whose value is in-dependent of the magnitude. As such, CoV can de-couple the score magnitude from the score weight-ing, so a type of score with a small magnitude maystill be relatively impactful when it is variant (Groe-nendijk et al., 2021). Analogously, we obtain theCoV of the helpfulness and conciseness score, i.e.,ch and cc. Moreover, we employ the softmax func-tion with temperature on the coefficient of variationof these scores, which controls the smoothness ofthe score weight to avoid abnormal score weight:",
  "Self-Alignment Stage": "After obtaining the preference data over all candi-dates D = {(q P, ei, ej)|1 i, j N, si > sj},where each tuple represents a choice preference be-tween winning and losing extracted evidence, weproceed to the stage of alignment tuning for im-proving faithfulness, helpfulness, and conciseness.For alignment training, previous works commonlyadopt Proximal Policy Optimization (PPO) (Schul-man et al., 2017) or Direct Preference Optimization(DPO) (Rafailov et al., 2023). However, PPO can-not perceive the ranking position and DPO treatsall preference pairs indiscriminately. Due to theabove drawbacks, both of them cannot result inoptimal alignment. Inspired by the Lambdalossmethod (Donmez et al., 2009; Burges et al., 2006;Wang et al., 2018), we propose a listwise-awareLambda Preference Optimization algorithm, LPO,which seamlessly brings the ranking position intoDPO by assigning a lambda weight to each pair:",
  "where MRRw,l =1rw 1": "rl , rw is the rank positionof yw in the ranking permutation induced by thesmoothing CoV-weighted score s. Thus, by intro-ducing the lambda weight, LPO becomes a listwise-aware method. LPO is designed to work with anyranking metric, as long as the lambda weight canbe defined, e.g., NDCG (Liu et al., 2024). Here, weimplement LPO to optimize a well-founded rank-ing metric MRR because it is simple yet effective.",
  "Tok08213371651645962": ": QA performance comparison, where the best results are boldfaced and the second-best results areunderlined, in each row. Tok is the average length of extracted evidence fed into generators, where the smaller thevalue, the lower the computational cost. All improvements are significant with p-value < 0.01 according to t-test. lowing Research Questions (RQs): RQ1: Howdoes our model contribute to QA accuracy com-pared with other state-of-the-art methods? RQ2:Can LPO facilitate the generation of more faith-ful, helpful, and concise evidence? RQ3: Can ourmodel perform robustly to noise from irrelevantpassages? RQ4: How effective are the key settingsin our model, such as smoothing CoV-weighting?",
  "Experimental Settings": "Datasets and Metrics. We experiment on threebenchmark QA datasets, NaturalQuestions (NQ)(Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshiet al., 2017), and HotpotQA (Yang et al., 2018).Following Wang et al. (2023), we use the processedversion (Lee et al., 2019) of NQ for experiments,discarding answers with more than 5 tokens. AsNQ and TQA belong to the extractive QA task, weuse Exact Match (EM) as their evaluation metric,where a score of 1 is assigned if at least one amongmultiple correct answers appears in the responseof the QA model; otherwise, the score is 0. WhileHotpotQA belongs to the abstractive QA task, weemploy unigram F1 to evaluate answer correctness.As the test set for HotpotQA is unavailable, wereport the dev set results. The detailed statistics ofdatasets are summarized in Appendix A in .",
  "Baseline Methods. There are three types of base-lines: (1) Without Evidence (WE) includes (i)Zero-shot (Zero) that does not pass any evidence": "to LLMs. (2) Coarse-grained Evidence (CGE)includes (i) Full Passage (Full) that directly passesthe top-retrieved passage to LLMs, (ii) Select-Context (SeleCtx) (Li et al., 2023b) that identi-fies and prunes redundancy in the top-retrievedpassage based on perplexity. (3) Fine-grainedEvidence (FGE) includes (i) LLM-Embedder(Zhang et al., 2023) that extracts the sub-passageswith the highest similarity to the query from thetop-retrieved passage, (ii) Bge-Reranker-Large(Bge-Reranker) (Xiao et al., 2023) that reorders allsub-passages in the top-retrieved passage and usesthe top-ranked sentence as evidence, (iii) FILCO(Wang et al., 2023) that learns to filter the re-trieved passage with sentence precision leveragingheuristic-based augmentation to label ground-truth. Generators for QA. To measure the efficacy of theevidence extracted by SEER and other competitivebaselines, we employ two different generators, i.e.,,Flan-T5-XL (Chung et al., 2024) and Llama2-7B-Chat (Touvron et al., 2023), for QA evaluation7. Implementation Details. Following Wang et al.(2023), we use the adversarial Dense Passage Re-triever (DPR) (Karpukhin et al., 2020) to retrievethe top-5 passages from all Wikipedia passages.For each <user query q, retrieved passage P> pair,we set the sample size M as 10. For the tempera-",
  "(c) HotpotQA dataset": ": Model performance w.r.t. Noise-to-Signal Ra-tio (NSR) ratio. The bar denotes the silver faithfulnessscore or the helpfulness score, while the line representsthe performance drop percent compared to the modelthat is provided with only relevant retrieved passages. optimizing PPO, making it hard to reach the opti-mal point. (3) Our LPO consistently outperformsthe DPO, indicating the superiority of supplement-ing DPO with a listwise-aware weight. (4) Afterself-alignment, the average improvements of ourLPO over the Base on three datasets are 10.2%,6.16%, and 1.70% regarding the three primary cri-teria, showing huge potential to enhance the finalRAG performance and quicken up the inference.",
  "Model Comparison (RQ1)": "To examine the impact of evidence extraction onthe final RAG performance, we experimented onthree benchmark QA datasets, where we prependedthe extracted evidence before the user query andthen input it together into the generator. Besides,we use the tokenizer of Flan-T5 and Llama2 to con-vert the extracted evidence into a list of subwordsand then calculate the length of the list, where thelength is adopted as a metric (denoted by Tok)measuring the computational burden to a large ex-tend. shows the final RAG performanceof different baseline evidence extraction methodsand our proposed SEER. From the experimentalresults, we mainly have the following observations:",
  "example, SEER achieves 13.5% and 12.0% im-provements over FILCO in the NQ dataset withFlan-T5 and Llama2 generators, respectively,while the average evidence length is very close": "Optimizing the three primary criteria for evi-dence extraction (i.e., faithfulness, helpfulness,and conciseness) yields such impressive perfor-mance improvements, considering most base-lines come from studies in recent two years. Thisdemonstrates that these three properties stronglyagree with the evidence quality in RAG, whilecurrent methods might not satisfy all of themsimultaneously, which leads to inferior results. Comparing different baselines, it is not surprisingthe method without evidence performs the worst.Secondly, methods with fine-grained evidence donot always perform better than ones with coarse-grained evidence. Specifically, the Full methodgenerally performs well, as it preserves retrievedpassages complete, while some FGE methods(e.g., LLM-Embedder and Bge-Ranker) mightlose key information in the process of evidenceextraction, but it takes much more time for gen-eration due to the long context. Last but notleast, our SEER considerably outperforms theFull method in most cases, where the averageimprovement on the three datasets is 2.58% w.r.t.QA accuracy, but the average length of evidencefed into generators is reduced by a factor of 9.25.",
  "Alignment Study (RQ2)": "To verify the effectiveness of the proposed LPO,we implement SEER with different types of POmethods to optimize the three primary criteria: (1)Base, i.e., the base extractor; (2) PPO (Schulmanet al., 2017); (3) DPO (Rafailov et al., 2023); (4)LPO (3.3). In , we present the oraclescores made by each method and the percentage ofperformance improvement over the Base method.From the results, we find that: (1) Unsurprisingly,the Base without alignment performs the worst in11 out of 12 cases, indicating the necessity of align-ment for evidence extraction. (2) The PPO usuallyperforms worse than the DPO one, as it directlyoptimizes the reward signal, i.e., the oracle scoresin our work, and thus neglects the pairwise signalsbetween the extracted evidence corresponding tothe same query. Besides, the relatively poor perfor-mance of PPO may be caused by the difficulty of",
  "Robustness Analysis (RQ3)": "In real-world scenarios, RAG systems usually suf-fer from data noise issues (Gao et al., 2023; Dinget al., 2024) caused by imperfect retrieval systems,etc. To simulate this scenario, we randomly add acertain proportion (0%, 100%, 200%, 300%, and400%) of irrelevant passages to each test query. Weuse Noise-to-Signal Ratio (NSR) to denote the ratioof irrelevant passages to the relevant retrieved ones.",
  "shows the results on silver faithfulness8": "and helpfulness, while conciseness is omitted asthe noise issue does not affect it much. The resultsshow that: (1) The performance of both alignedand base extractors decreases, while the alignedone can consistently outperform the base under anyNSR except for 1 case. (2) The performance droppercent of the aligned model is generally lower thanthe base in 2 out of 3 datasets. Besides, with 100%noise proportion, the aligned model can even out-perform the base without noise data on all datasets.These observations manifest that SEER can endowthe backbone with more robustness to noise issues.",
  "Ablation Study (RQ4)": "In , we conduct an ablation study to ver-ify the effectiveness of key settings in our method,where w/o denotes without, (A) represents SEER,(B) removes the deduplication operation, (C) re-moves smoothing CoV-weighting by uniformly set-ting f, h, and c to 1/3 in Eq. (7), (D) re-moves the lambda weight w,l in Eq. (8). Fromthe table, we can find that (A) achieves the bestor second-best results in all datasets, indicating allkey settings are effective and necessary for SEER.By comparing (A) and (B), removing duplicatescan considerably improve helpfulness, as it effec-tively avoids preference optimization overwhelmedby head responses. By comparing (A) and (C),weighting the oracle scores based on their statisti-cal properties is able to match the learning difficultywell. By comparing (C) and (D), we observe thatweighting the preference pairs plays a more keyrole than weighting the oracle scores. The mainreason might be that equally treating all preferencepairs leads to less attention paid to the crucial ones.",
  "Context Refinement for RAG": "Recently, many works have emerged, aiming atidentifying the supporting content from retrievedpassages. The common method is to rerank the re-trieved passages and feed the top-ranked ones intogenerators (Zhang et al., 2023; Xiao et al., 2023).Thereafter, some methods leverage the capabilitiesof LLMs to summarize retrieved passages to iden-tify key information (Ko et al., 2024; Laskar et al.,2023; Kim et al., 2024; Sarthi et al., 2024). Fur-thermore, a few methods leverage agent models tocalculate perplexity as an important indicator tofilter out low-information content (Li et al., 2023b;Jiang et al., 2023a). Other works use manually de-signed heuristic-based augmentation to constructtraining signals for fine-tuning LLMs, to enhancetheir capacity to identify key information (Wanget al., 2023; Jin et al., 2024). In contrast to previousworks heavily relying on hand-crafted augmenta-tion, we use data augmented by the model itself toboost performance, free of the arduous workforce.",
  "Self-Aligned Learning": "Recently, a few studies have attempted to utilize themodel to improve itself and align its response withdesired properties (Li et al., 2023a; Zhang et al.,2024; Liang et al., 2024; Sun et al., 2023a; Yuanet al., 2024; Sun et al., 2023b; Bai et al., 2022). Forexample, (Li et al., 2023a) prompts the model togenerate instructions for unlabeled data to createa set of candidate training data, and then use themodel to score each augmented example to selecthigh-quality augmented data. (Zhang et al., 2024)utilizes the self-evaluation capability of LLMs tocreate confidence scores in terms of the factual ac-curacy of its generated responses, and treat themas reward signals to steer the model towards factu-ality. Similarly, (Liang et al., 2024) leverages themodels self-awareness of its knowledge state toalign the model for hallucination mitigation. To thebest of our knowledge, our study is the first to ex-plore self-aligned learning for evidence extraction.",
  "Conclusion": "This work explores the method that learns to ex-tract high-quality evidence to assist model gener-ation and reduce computational cost. Differentfrom previous works heavily relying on heuristics,we introduce a novel evidence extraction learning framework, SEER, which utilizes the model to cal-ibrate its extraction preference via self-alignment.To this end, we first probe into model extractionpreferences via response sampling, then assess thequality of extracted evidence via experts, and fi-nally optimize the vanilla model as an evidence ex-tractor via self-alignment. Extensive experimentsshow that SEER considerably improves the finalRAG performance. Moreover, it can extract morefaithful, helpful, and concise evidence, and alsoshows higher robustness against data noise issues.",
  "Limitations": "Despite our discoveries and improvements, wemust acknowledge certain limitations in our work:Firstly, computing resource constraints restrictour experiment to LLMs with limited and moder-ate scale, i.e., Flan-T5-XL (Chung et al., 2024) andLlama2-7B-Chat (Touvron et al., 2023). We willexplore the use of our method on larger modelssuch as Llama2-70B in future work. The EM andF1 metrics used in our experiments might over-estimate the correctness of responses, even if theresponse does not convey equivalent semantics tothe ground truth, since these metrics mechanicallyverify whether the answer exists in the response.Secondly, our method still requires domainknowledge for devising experts to assess the qual-ity of evidence, though it has considerably light-ened the arduous workforce in data engineering.We experiment solely on Dense Passage Retriever(Karpukhin et al., 2020) with Wikipedia passages,while de facto RAG applications commonly involvemulti-source retrieval with varied writing styles.Thirdly, there are a few cases where the alignedextractor is vulnerable to data noise issues. Asdemonstrated in (c), with the NSR in-creases, the performance drop percent of thealigned extractor is higher than that of the base one,although it still outperforms the base one. Giventhat, we are currently conducting further research topropose a more powerful evidence extractor, whichis not only skilled at refining retrieved passages butalso has higher robustness against noisy passages.",
  "Acknowledgements": "This work is jointly supported by National Natu-ral Science Foundation of China (No. 62376067)and Guangdong Basic and Applied Basic ResearchFoundation (2023A1515110078). We sincerelythank all the reviewers for their detailed reviews. Vaibhav Adlakha, Parishad BehnamGhader, Xing HanLu, Nicholas Meade, and Siva Reddy. 2023. Eval-uating correctness and faithfulness of instruction-following models for question answering. CoRR,abs/2307.16877. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Carol Chen, Catherine Olsson, Christo-pher Olah, Danny Hernandez, Dawn Drain, DeepGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosiute, LianeLovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noem Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel R. Bow-man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, andJared Kaplan. 2022. Constitutional AI: harmlessnessfrom AI feedback. CoRR, abs/2212.08073. Christopher J. C. Burges, Robert Ragno, and Quoc VietLe. 2006. Learning to rank with nonsmooth costfunctions. In Advances in Neural Information Pro-cessing Systems 19, Proceedings of the TwentiethAnnual Conference on Neural Information Process-ing Systems, Vancouver, British Columbia, Canada,December 4-7, 2006, pages 193200. MIT Press.",
  "Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Dur-rett, and Eunsol Choi. 2023. Complex claim veri-fication with evidence retrieved in the wild. CoRR,abs/2305.11859": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Yujuan Ding, Wenqi Fan, Liangbo Ning, Shijie Wang,Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li.2024. A survey on rag meets llms: Towards retrieval-augmented large language models. arXiv preprintarXiv:2405.06211. Pinar Donmez, Krysta M. Svore, and Christopher J. C.Burges. 2009. On the local optimality of lambdarank.In Proceedings of the 32nd Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, SIGIR 2009, Boston,MA, USA, July 19-23, 2009, pages 460467. ACM. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,Meng Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: Asurvey. CoRR, abs/2312.10997. Rick Groenendijk, Sezer Karaoglu, Theo Gevers, andThomas Mensink. 2021. Multi-loss weighting withcoefficient of variations. In IEEE Winter Conferenceon Applications of Computer Vision, WACV 2021,Waikoloa, HI, USA, January 3-8, 2021, pages 14681477. IEEE. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,and Ming-Wei Chang. 2020. Retrieval augmentedlanguage model pre-training. In Proceedings of the37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, volume119 of Proceedings of Machine Learning Research,pages 39293938. PMLR. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Manas Jain, Sriparna Saha, Pushpak Bhattacharyya,Gladvin Chinnadurai, and Manish Kumar Vatsa.2021. Natural answer generation: From factoid an-swer to full-length answer using grammar correction.CoRR, abs/2112.03849. Huiqiang Jiang, Qianhui Wu, Xufang Luo, DongshengLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a.Longllmlingua: Accelerating and enhancing llmsin long context scenarios via prompt compression.CoRR, abs/2310.06839. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,Qian Liu, Jane Dwivedi-Yu, Yiming Yang, JamieCallan, and Graham Neubig. 2023b. Active retrievalaugmented generation. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2023, Singapore, Decem-ber 6-10, 2023, pages 79697992. Association forComputational Linguistics.",
  "Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou.2024. BIDER: bridging knowledge inconsistency forefficient retrieval-augmented llms via key supportingevidence. CoRR, abs/2402.12174": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics, ACL2017, Vancouver, Canada, July 30 - August 4, Volume1: Long Papers, pages 16011611. Association forComputational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,and Wen-tau Yih. 2020. Dense passage retrieval foropen-domain question answering. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP 2020, Online,November 16-20, 2020, pages 67696781. Associa-tion for Computational Linguistics. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, JongjinPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,and Jinwoo Shin. 2024.Sure: Summarizing re-trievals using answer candidates for open-domainQA of llms. CoRR, abs/2404.13081. Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings.",
  "SungHo Ko, Hyunjin Cho, Hyungjoo Chae, JinyoungYeo, and Dongha Lee. 2024. Evidence-focused factsummarization for knowledge-augmented zero-shotquestion answering. CoRR, abs/2403.02966": "Grzegorz Kondrak. 2005. N-gram similarity and dis-tance.In String Processing and Information Re-trieval, 12th International Conference, SPIRE 2005,Buenos Aires, Argentina, November 2-4, 2005, Pro-ceedings, volume 3772 of Lecture Notes in ComputerScience, pages 115126. Springer. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur P. Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: a benchmark for question answeringresearch. Trans. Assoc. Comput. Linguistics, 7:452466. Md. Tahmid Rahman Laskar, Mizanur Rahman, Is-rat Jahan, Enamul Hoque, and Jimmy Huang. 2023.Cqsumdp: A chatgpt-annotated resource for query-focused abstractive summarization based on debate-pedia. CoRR, abs/2305.06147. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.2019. Latent retrieval for weakly supervised open do-main question answering. In Proceedings of the 57thConference of the Association for Computational Lin-guistics, ACL 2019, Florence, Italy, July 28- August2, 2019, Volume 1: Long Papers, pages 60866096.Association for Computational Linguistics. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-tus, Fabio Petroni, Vladimir Karpukhin, NamanGoyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih,Tim Rocktschel, Sebastian Riedel, and DouweKiela. 2020.Retrieval-augmented generation forknowledge-intensive NLP tasks. In Advances in Neu-ral Information Processing Systems 33: Annual Con-ference on Neural Information Processing Systems2020, NeurIPS 2020, December 6-12, 2020, virtual.",
  "Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiax-ing Zhang. 2024. Learning to trust your feelings:Leveraging self-awareness in llms for hallucinationmitigation. CoRR, abs/2401.15449": "Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, MishaKhalman, Rishabh Joshi, Yao Zhao, MohammadSaleh, Simon Baumgartner, Jialu Liu, Peter J. Liu,and Xuanhui Wang. 2024. Lipo: Listwise prefer-ence optimization through learning-to-rank. CoRR,abs/2402.01878. Antoine Louis, Gijs van Dijck, and Gerasimos Spanakis.2024. Interpretable long-form legal question answer-ing with retrieval-augmented large language mod-els. In Thirty-Eighth AAAI Conference on ArtificialIntelligence, AAAI 2024, Thirty-Sixth Conferenceon Innovative Applications of Artificial Intelligence,IAAI 2024, Fourteenth Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2014, Febru-ary 20-27, 2024, Vancouver, Canada, pages 2226622275. AAAI Press. Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan T. McDonald. 2020. On faithfulness and fac-tuality in abstractive summarization. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, ACL 2020, Online, July5-10, 2020, pages 19061919. Association for Com-putational Linguistics.",
  "Vaishali Pal, Manish Shrivastava, and Irshad Bhat. 2019": "Answering naturally: Factoid to full length answergeneration. In Proceedings of the 2nd Workshop onNew Frontiers in Summarization, pages 19, HongKong, China. Association for Computational Linguis-tics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023.",
  "Hannah Rashkin, David Reitter, Gaurav Singh Tomar,and Dipanjan Das. 2021. Increasing faithfulness in": "knowledge-grounded dialogue with controllable fea-tures. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics andthe 11th International Joint Conference on NaturalLanguage Processing, ACL/IJCNLP 2021, (Volume 1:Long Papers), Virtual Event, August 1-6, 2021, pages704718. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics.",
  "Zhiqing Sun, Yikang Shen, Hongxin Zhang, QinhongZhou, Zhenfang Chen, David D. Cox, Yiming Yang,and Chuang Gan. 2023a. SALMON: self-alignmentwith principle-following reward models.CoRR,abs/2310.05910": "Zhiqing Sun, Yikang Shen, Qinhong Zhou, HongxinZhang, Zhenfang Chen, David D. Cox, YimingYang, and Chuang Gan. 2023b.Principle-drivenself-alignment of language models from scratch withminimal human supervision. In Advances in NeuralInformation Processing Systems 36: Annual Confer-ence on Neural Information Processing Systems 2023,NeurIPS 2023, New Orleans, LA, USA, December 10- 16, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Xuanhui Wang, Cheng Li, Nadav Golbandi, MichaelBendersky, and Marc Najork. 2018. The lambdalossframework for ranking metric optimization. In Pro-ceedings of the 27th ACM International Conferenceon Information and Knowledge Management, CIKM2018, Torino, Italy, October 22-26, 2018, pages 13131322. ACM.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighof. 2023.C-pack: Packaged resourcesto advance general chinese embedding.CoRR,abs/2309.07597": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu,Junxian He, and Bryan Hooi. 2023. Can llms expresstheir uncertainty? an empirical evaluation of confi-dence elicitation in llms. CoRR, abs/2306.13063. Benfeng Xu, Chunxu Zhao, Wenbin Jiang, Pengfei Zhu,Songtai Dai, Chao Pang, Zhuo Sun, Shuohuan Wang,and Yu Sun. 2023.Retrieval-augmented domainadaptation of language models. In Proceedings of the8th Workshop on Representation Learning for NLP,RepL4NLP@ACL 2023, Toronto, Canada, July 13,2023, pages 5464. Association for ComputationalLinguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W. Cohen, Ruslan Salakhutdinov, andChristopher D. Manning. 2018. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. In Proceedings of the 2018 Conference on Em-pirical Methods in Natural Language Processing,Brussels, Belgium, October 31 - November 4, 2018,pages 23692380. Association for ComputationalLinguistics. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Ja-son Weston. 2024. Self-rewarding language models.In Forty-first International Conference on MachineLearning, ICML 2024, Vienna, Austria, July 21-27,2024. OpenReview.net. Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex RDalal, Jennifer L Kim, Michael Moor, Robyn Fong,Curran Phillips, Kevin Alexander, Euan Ashley,et al. 2024.Almanacretrieval-augmented lan-guage models for clinical medicine.NEJM AI,1(2):AIoa2300068.",
  "AMore Implementation Details": "Statistics of datasets. We conduct extensive ex-periments on three benchmark datasets, i.e., Natu-ralQuestions (NQ) (Kwiatkowski et al., 2019), Triv-iaQA (TQA) (Joshi et al., 2017), and HotpotQA(Yang et al., 2018), for evaluating our proposedmethod and the competitive baselines. We showthe detailed statistics of these datasets in . Response sampling details. Given the query andthe retrieved passages, we prompt the base extrac-tor to generate 10 candidate response samples andwe remove duplicates. To fully probe the evidenceextraction preferences of the base extractor, wehave modified the generation configuration to makethe responses more varied. Specifically, we set top-p, top-k, temperature, and the repetition penaltyas 1.0, 80, 1.0, and 1.0 respectively, for collectingdiverse preference data, used to align the responsesof the based extractor with the desired properties. Fine-tuning details. We use the Adam optimizer(Kingma and Ba, 2015) with 1 = 0.9, 2 = 0.999,and eps = 1e8. The learning rate is 1e5 with1.5% warmup ratio and cosine scheduler. The batchsize, gradient accumulation step, and number ofepochs are set as 16, 2, and 2.0, respectively. We leverage the parameter-efficient fine-tuning tech-nique, specifically LoRA (Hu et al., 2022), wherewe employ the Llama-Factory9 fine-tuning frame-work (Zheng et al., 2024) to implement all the pref-erence optimization methods for fair comparisons. Context relevance details. In , we usecontext relevance as the metric to measure howwell the extracted evidence fits the current userquery and can be effectively used to augment thequality of generation. To this end, we naturallydefine context relevance as the cosine similaritybetween the extracted evidence and the user query:",
  "where scr is the context relevance score;q and e denote the query and evidence, respectively": "Silver faithfulness details. In .4, we de-vise a metric, silver faithfulness, to measure therobustness of the evidence extractor against datanoise issues commonly existing in real-world sce-narios. Specifically, we fed the mixture of the rele-vant retrieved passage and the randomly sampledirrelevant passages into the extractor. Then, wetreat the relevant retrieved passage and extractedevidence as the premise and hypothesis, respec-tively, measuring how well the extractor is robustto irrelevant context, which can be formulated as:",
  "BFull-length Answer Generation": "To assess the conciseness of the extracted evidence,we propose measuring the information gap betweenit and the full-length answer. The full-length an-swer is generated by transforming the question andits corresponding answer into a declarative state-ment, as shown in . Towards this end, weprompt GPT-3.5-turbo to transform each question-answer pair into a full-length answer. Addition-ally, we prepared a few-shot examples to encour-age well-organized output. The prompt for full-length answer generation can be found in .",
  "Full-length Answer Generation Prompt": "[Instruction]You are given a question and its answer. Your task is to transform this question-answer pair into adeclarative sentence with lossless fidelity to the original semantics.[Here are three examples][Question]: What profession does Nicholas Ray and Elia Kazan have in common?[Answer]: director[Full-length answer]: Nicholas Ray and Elia Kazan have the profession of director in common.[Question]: When is season seven of game of thrones coming out?[Answer]: July 16, 2017[Full-length answer]: Season seven of Game of Thrones is coming out on July 16, 2017.[Question]: What is the moon festival called in Chinese?[Answer]: Mid-Autumn Festival[Full-length answer]: The moon festival is called the Mid-Autumn Festival in Chinese.[Now complete the following][Question]: When did the genre of installation art start to gain acceptance?[Answer]: in the 1970s[Full-length answer]:",
  ": The prompt for full-length answer generation": "alignment optimization. Specifically, we generateten pieces of evidence for each test query by re-sponse sampling with the same generation config-uration as .1. Subsequently, we measurethe oracle scores (3.2), calculate the standard de-viation, and compute the average value. The ex-perimental results show that: (1) The generationstability of the aligned model is much better thanthat of the base one in most cases. More precisely,the average improvement of the aligned model overthe base one on the three datasets is 18.5%. (2)The generation stability in terms of helpfulness hasseen greater improvements compared to the other two properties (i.e., faithfulness and conciseness),with an average improvement of 32.2%, showingthe huge potential to enhance the final RAG perfor-mance. The above observations fully demonstratethat SEER is able to endow the backbone withsuperior generation stability during the inference."
}