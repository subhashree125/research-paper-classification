{
  "Abstract": "Program-of-Thought (PoT) replaces naturallanguage-based Chain-of-Thought (CoT) as themost popular method in Large Language Mod-els (LLMs) mathematical reasoning tasks byutilizing external tool calls to circumvent com-putational errors. However, our evaluation ofthe GPT-4 and Llama series reveals that usingPoT introduces more reasoning errors, such asincorrect formulas or flawed logic, compared toCoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suiteof strategies that help integrate PoT and CoT,encompassing: (1) a new generation paradigmthat uses full CoT reasoning to control codegeneration. (2) Focus Attention, that directsmodel attention to the CoT reasoning duringPoT to generate more logical code. (3) rein-forcement learning that utilizes the accuracy ofboth CoT and PoT responses as rewards to pre-vent repetitive reasoning steps in LLMs whensolving difficult math problems. Our methodachieves an average improvement of 6.5% onthe Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculationdatasets. It also shows significant effective-ness on five out-of-domain datasets by con-trolling the models information flow, exhibit-ing strong transferability. Additionally, HTLshows the most significant improvement in non-mathematical natural language inference task,contributing to a unified reasoning task frame-work1.",
  "Code Translation Error": ": The top section of the chart represents theaverage CTE for each model across 5 datasets. Be-low is a real example from the Asdiv dataset using theMAmmoTH-Mistral-7B model, which achieved an ac-curacy of 93.9% on this dataset. The proportion of CTEremains high across various models, and these errors donot diminish with an increase in model parameters. et al., 2023), using CoT (Wang et al., 2023b; Weiet al., 2022; Chen et al., 2024) solely implementedin natural language can lead to calculation mis-takes (Lewkowycz et al., 2022; Wei et al., 2023;Gao et al., 2023). The most common practice cur-rently is to use PoT (Chen et al., 2023) for handlingmathematical reasoning tasks, by guiding the largemodel to write the code that is then computed usingtool calls.However, we made a surprising discovery re-cently: when a problem is phrased in a mannercloser to verbal scenarios (for example, the ques-tion is One apple costs three dollars, how muchfor three apples? instead of 33=?), PoT tendsto make more reasoning errors or text comprehen-sion mistakes, but this phenomenon is almost non-existent in CoT. For such problems, CoT can cor-rectly reason out the answer, whereas PoT makes mistakes. We refer to this type of error as CodeTranslation Error (CTE). We report the percent-age of CTE on five datasets with multiple types ofmodels, the results illustrated in . This er-ror is due to the amount of training data for naturallanguage far exceeding that for code. In the scopeof CodeLlamas pretraining data, which includes500 billion code tokens, this represents a smallfraction compared to the 2 trillion natural languagetokens used in the Llama-2 model (Rozire et al.,2023; Hugo Touvron, 2023). Natural language ismore suitable for semantic analysis, planning, andabstract reasoning than code (Gou et al., 2023b). Existing work also finds the advantage of Natu-ral language, but they have not effectively utilizedthe reasoning capabilities of natural language. Cur-rent research focuses on the following approachesto integrate natural language to enhance the preci-sion of code: (1) Using natural language prompts toguide the model in writing code (Gao et al., 2023;Toshniwal et al., 2024; Wang et al., 2023a): writea brief step in natural language before generatingcode. (2) Employing methods like self-correctionand hybrid approaches to generate answers in multi-ple stages (Gou et al., 2023b; Yue et al., 2023; Gouet al., 2023a). (3) Utilizing prompts like rethinkquestion (Deng et al., 2023) to have the model firstparaphrase the question, thereby avoiding compre-hension errors. However, existing methods fallshort in two main aspects: First, using few natu-ral language steps or simple paraphrasing meth-ods alone is insufficient for effectively controllingcode generation; a more comprehensive natural lan-guage reasoning process is necessary to generatemore reliable code. Secondly, reasoning withinLLMs is not always faithful (Lanham et al., 2023;Bao et al., 2024; Turpin et al., 2023). Frequently,the final answers seem to be derived directly fromthe questions themselves rather than aligning withthe reasoning process. Consequently, even correctreasoning can lead to incorrect answers. To more effectively utilize natural language rea-soning to enhance PoT, we propose Human-ThinkLanguage (HTL): A novel information-control-based approach to utilize complete CoT reasoningsteps to control PoT generation. HTL is inspiredby the way humans write code. Humans considerthe entire reasoning process using natural language,and the code can fully rely on natural language rea-soning. On the right side of , we highlightthe parallels between our integrated model and the human approach to solving mathematical problems.Compared to previous works, our framework offersa strong capacity for aligning calculation with rea-soning by integrating CoT and PoT. We design Fo-cus Attention mechanism that, during code genera-tion, concentrates solely on information from CoTto promote the chain reasoning better, thereby bias-ing the answer to be more faithful to CoT. On theother hand, using complete CoT reasoning tendsto lead LLMs to use mathematical induction toenumerate reasoning steps verbosely, which resultsin repetitive generation. We incorporate the er-ror assessment function based on PPO (Schulmanet al., 2017), leveraging reinforcement learning topenalize repetitive generation. We conduct experi-ments based on CodeLlama-7B and Mistral-7B andachieve outstanding results on eight datasets usingonly self-distillation data.In summary, our contributions are as follows:(1) We are the first to conduct a detailed evalua-tion of current closed-source models, open-sourcebase models, and specialized models. We highlightthe shortcomings of PoT and propose that usingfull natural language reasoning to enhance PoTperformance is essential.(2) We propose an advanced model named HTL,which utilizes the complete reasoning process ofCoT to enhance PoT. HTL incorporates a novelFocus Attention that approximates chain reasoning,complemented by an error assessment function de-signed to prevent repetitive generation.(3) We evaluate our work on eight mathematicalreasoning datasets, and our experimental resultsdemonstrate that our method achieves outstandingresults. HTL shows significant effectiveness in in-domain datasets, out-of-domain datasets, and natu-ral language inference task, demonstrating strongusability and potential.",
  "learning": ": Dense Attention refers to traditional Attention, while Focus Attention is our approach. In the orangecolumn on the left, the first four tokens share a consistent mask state of 1. On the right side of the figure, there is acomparison between human and LLMs in solving mathematical problems. for reasoning. By adapting code translation afterthe CoT reasoning path, the PoT can inherit thereasoning skeleton of CoT while circumventing itscomputational errors. This effectively combinesthe advantages of both approaches.",
  "Thus A + B = 60 + 30 = 90 .The answer is 90": "# Python program to calculate A + Bfrom sympy import symbols, solve# Define the variablesA, B = symbols(A B)# Equation 1: 2/5 = A/60eq1 = solve(2/5 - A/60, A)# Equation 2: 60/B = 2/5eq2 = solve(60/B - 2/5, B)# Calculate A + Bresult = eq1 + eq2print(result) : Demonstrating a successful example for HTL:Although the CoTs answer may contain many calcu-lation errors (in red), its reasoning skeleton is correct.HTL enables PoT to follow CoTs reasoning steps toarrive at the correct result.",
  "Focus Attention": "Attention DesignIn our work, we use a local at-tention (Beltagy et al., 2020) mechanism to controlthe information flow during training (). Wedivide the text of a mathematical problem into threeparts: Q (question), C (CoT), and P (PoT). Theobjective in generating the PoT reasoning is for themodel to rely solely on information from the CoTreasoning, not on the question. However, a recentstudy by (Xiao et al., 2023) introduced the conceptof attention-sink, showing that the initial tokens ofa sequence attract a significant portion of attention.Therefore, while the Focus Attention mechanismmasks the information from Q and focuses solelyon C during PoT generation, it preserves the initialtokens in the sequence to prevent the loss of sub-stantial information. Echoing the findings of (Xiaoet al., 2023), we include the first four tokens in thePoT information generation process. This resultsin the following modified formula for the casualmask matrix:",
  "hidden vector representation of the sequence": "Adaptive Training StrategyTo align with thedense causal matrix used for both pretraining andinference, which is inconsistent with our Focus At-tention, we introduce a novel training approach:during both the initial and final phases of train-ing, we do not explicitly mask any tokens besidesthe causal mask, thereby ensuring alignment withthe pretraining stage and the inference stage. Inthe middle of the training process, we incorporatea mask coverage function, which is a quadraticfunction and calculates a proportion of entries tobe randomly masked based on the number of train-ing steps, allowing the mask to transition betweenDense Attention and Focus Attention:",
  "Error Assessment Function Based PPO": "In reinforcement-learning stage, We employ PPOwith a clipped objective algorithm for training. Fol-lowing (Ziegler et al., 2020), the value model Vis constructed by appending a linear value head ontop of the last hidden states of the policy model.For the reward model, we replace it with error as-sessment function. At the terminal state, we usea reward function to score based on the correct-ness of the generated answer. All other states areassigned a value of 0. We categorize the reasonsfor their errors and provide more fine-grained feed-back scores for the model based on the answersfrom CoT and PoT. The error assessment functionis as follows:",
  "In cases where the model cannot produce an an-swer, we consider it as model-level error and applythe harshest penalty. If the CoT is correct and PoT": "is incorrect, we consider it as code translation error.In cases where only CoT is incorrect but PoT iscorrect, we view it as solely a calculation error andput a slight penalty. Such a partial reward can helpreduce the negative effect of learning from sparserewards. Furthermore, in line with (Zheng et al.,2023), our total reward encompasses both the re-ward function score and the Kullback-Leibler (KL)divergence (Kullback and Leibler, 1951) betweenthe learned RL policy and the initial policy. For theremaining details on PPO, we refer to (Luong et al.,2024).",
  "Baseline": "Our work is based on the MAmmoTH model (Yueet al., 2023), which achieves outstanding perfor-mance in open-source LLM mathematical reason-ing by Hybrid Instruction Tuning from a mixtureof CoT and PoT for training2. MAmmoTH has twobases: CodeLlama-7B (Rozire et al., 2023) andMistral-7B (Jiang et al., 2023). We compare thefollowing methods: PoT/PAL (Gao et al., 2023)uses the LLM toread natural language problems and generate pro-grams as intermediate reasoning steps, but offloadsthe solution step to a runtime such as a Python in-terpreter. PAL is a more refined version of PoT,with each line of code accompanied by a comment.",
  "Experimental Setting": "For reinforcement learning, we set a uniform num-ber of 10 epochs, with the KL coefficient set to0.01. The learning rates for CodeLlama-Base andMistral are 1e-5 and 2e-6, respectively. For theSFT stage, the specific parameters are as shown in. For the mask coverage function, we set to -11.0 and to 1.76. For fair comparison withsota, we follow the standard evaluation protocols4.",
  "Dataset": "Training DatasetWe use hybrid data from thetraining set of the MAmmoTH model. We firstrun the fine-tuned MAmmoTH model to gener-ate both CoT and PoT answers for them.Wethen convert the data into the format of Q, C, andP and discard any incomplete data. In the end,we extract 36,000 examples, with 18,000 comingfrom GSM8K (Cobbe et al., 2021), 3,000 fromNumGLUE (Mishra et al., 2022), and 15,000 fromMATH. Using training data from self-distillationcan mitigate the effect of performance differencesamong models with different bases. TestDatasetOurexperimentstestoneight datasets:GSM8K, NumGLUE, Math,SimulEq (Koncel-Kedziorski et al., 2016), Deep-Mind (Saxton et al., 2019), SVAMP (Patel et al.,2021), MAWPS (Koncel-Kedziorski et al., 2016)and Asdiv (Miao et al., 2020). These eight datasetshave varying levels of difficulty and length, com-prehensively reflecting the models mathematicalcomputational capabilities. Meanwhile, GSM8K,NumGLUE and MATH are in-domain datasets,whereas others are out-of-domain datasets.",
  "The main results are shown in . Our methodclearly surpasses other existing methods, achievingstate-of-the-art (SOTA) across multiple datasets": "Most noticeably, our method exhibits a significantimprovement on the NumGLUE dataset, becausethe dataset contains a large amount of natural lan-guage inference, which is unsuitable for direct PoT.On average, HTL improved 5% performance forLlama-Base and 4% for Mistral-Base. We willdiscuss some detailed findings below.For the experiments with PoT (4-shot) and PAL(4-shot), since the current PoT already uses mean-ingful variable names in the code, adding addi-tional comments by PAL results in a very slightimprovement. For the experiment with RAR, whileit can reduce some misunderstandings the modelhas about the problem, it cannot prevent incorrectreasoning. For the hybrid approach that switchesto CoT upon errors in code execution, while it hasa 9.8% improvement on NumGLUE over vanillaPoT, HTL achieves an 8.7% improvement over itby establishing a closer connection between CoTand PoT in a unified one-stage generation process.Compared to proprietary models, GPT-4 still ex-hibits strong performance, widening the gap withopen-source models. ToRA and MathCoder usedata generated from GSM8K and MATH datasets.Our performance on these two datasets is not asgood as ToRAs, but we have excellent generaliz-ability, showing significant improvements on out-of-domain datasets. Our method of controllinginformation flow exhibits strong transferability be-cause it directly changes how the model acquiresinformation, making its effectiveness not limited toin-domain datasets.We also conduct experiments for a two-stageversion of HTL and observed no consistent perfor-mance gain of vanilla one-stage generation overit. This shows that the performance gain mainlycomes from Focus Attention and ReinforcementLearning we designed for the one-stage paradigm.",
  "Ablation Study": "We conducted ablation experiments on all datasetsto investigate the contribution of each key compo-nent or strategy of our proposed method. Ablationexperiments include two aspects: method ablationand data ablation. Method AblationThe ablation tests include w/oFocus Attention and w/o reinforcement learn-ing. Focus Attention is a powerful enhancementfor HTL, directly improving performance by anaverage of 2%. It effectively helps the model focuson useful information. For reinforcement learn-",
  "Mistral": "MAmmoTHPoT74.573.937.148.255.880.593.974.767.33MAmmoTHHybrid75.073.939.750.361.180.693.974.768.65MAmmoTHRAR76.374.737.349.354.380.393.774.867.59HTL-74.776.338.551.662.981.293.776.269.38+focus77.977.039.957.863.382.094.578.371.34+focus+RL78.178.340.656.764.282.494.278.971.67 : All results are presented as the average of three experimental trials. Results marked as * are copied fromother papers. Unless otherwise specified, the default experimental setting is 0-shot. HTL(-) represents that theexperiment only used Dense Attention and fine-tuning, while focus indicates the inclusion of Focus Attention.The RL is reinforcement learning.",
  ": Based on the performance comparison with Llama-Base, to verify the effectiveness of self-distillation inour experiments": "ing (RL), it is noteworthy that both Llama-Baseand Mistral-Base models show improvements inmath tasks. Math is currently the most challeng-ing mathematical dataset, generating solutions thatare often longer than those of other datasets. Thisfrequently causes the model to repeat generationuntil it exceeds the limit. Reinforcement learn-ing effectively mitigates this issue. But using RLalone significantly improves performance on simi-lar in-domain datasets, but it struggles to transfer toout-of-domain datasets, and its CTE issue remainsunresolved. Data AblationWe use a self-distillation methodto generate data, a technique that has been provento enhance performance (Zhang et al., 2019). Todemonstrate the effectiveness of our approach, wevalidate the performance of the HTL model interms of CoT, and we fine-tune the model using only the PoT subset from the HTL dataset. Theresults are shown in . HTL and MAmmoTHexhibit nearly identical performance in CoT, whichis in line with our expectations. Our enhancementspredominantly arise from the transition from CoTto PoT, rather than from strengthening the capabili-ties of CoT. And the data from self-distillation onlyprovide a marginal improvement.",
  "Influence of Subsets": "By utilizing training subsets with varying sourcesand sizes, we can more precisely assess the impactof each data segment on the models performance.The results show in . We discover an inter-esting phenomenon: when we use a specific datasetfor downstream training, the model performs wellon its corresponding test set but weakens its capa-bilities on other datasets.",
  ": Influence of different Coverage Function": "When we mix multiple datasets for training,the models improvement in capabilities becomesmore comprehensive. The combination of differ-ent datasets allows the model to focus more onthe characteristics of mathematical problems ratherthan relying on specific patterns present in only onedataset. At the same time, the addition of GSM8Kand NumGLUE has little impact on the MATHdataset; simple mathematical problems are difficultto influence the ability to perform hard reasoning. Data Volume and Performance RelationshipTo explore the appropriate amount of data, we intro-duced a dataset twice as large for experimentation.The total size of this dataset is 75k, which includes36,000 entries from GSM8K, 36,000 entries fromMath, and 3,000 entries from NumGLUE. As moredata is added, the improvement in the model is veryslight because we are not injecting more knowledgebut rather letting it tend to learn a paradigm.",
  "The Effort of Mask Coverage Function": "The phrase Without Initial Tokens indicates thatwe block all tokens from Q, not preserving the firstfour, which significantly decreases model perfor-mance, almost rendering it unable to reason cor-rectly. In the second experiment, we set the maskcoverage to always be 1, not adapting the modelduring the initial training phase, nor reverting the at-tention mechanism to a causal mask during the out-put phase. In this experiment, we find that its lossconvergence rate is significantly slower than theAdaptive Training Strategy. The adaptive trainingstrategy performs better across all datasets, serving",
  ": HTL in math23k": "as a transitional phase to balance the Focus At-tention training mechanism and the inconsistencyduring inference. We provide the model with abuffer, allowing it to gradually learn local attention,and after training, we restore it to its autoregressivegeneration mode. The quadratic function accel-erates its speed gradually both during ascent anddescent and at the peak, the derivative decreases,resulting in a longer duration of focused attentiontraining.",
  "Influence of Other Language": "We use a Chinese math dataset, math23k (Zhaoet al., 2020), to test the advantages of HTL in otherlanguages. The results shows in .We ob-served that the base models capability in ChineseCoT constrains the effectiveness of HTL, as HTL relies on effective CoT to enhance PoT. Specifically,when the base model exhibits weak performance inChinese CoT (e.g., CodeLlama CoT achieves only14.1% accuracy), the HTL-family methods performworse than PoT, potentially because the generatedChinese CoT is ineffective so that it rarely helpsbut rather undermines PoT. Conversely, when thebase model has reasonable Chinese CoT capabil-ity (e.g., Mistral CoT achieves 28.4% accuracy),the HTL-family methods show better performancethan PoT.",
  "Error Analysis": "To explore how HTL affects model performanceand analyze the reasons for errors in various cat-egories, we have divided the errors into two cat-egories: code execution errors and code reason-ing errors. shows the proportions of twotypes of errors across different datasets. For sim-pler datasets like GSM8K and SVAMP, there arerarely any code execution errors; most are logicreasoning errors, which HTL reduces. For themore challenging dataset like MATH, HTL notonly demonstrates stronger logical capabilities butalso reduces code execution errors. In HTL, theCTE for CodeLlama-Base and Mistral-Base hasbeen significantly reduced, with CodeLlama-Basedecreasing from 8.33% to 3.96% and Mistral-Basefrom 6.97% to 3.55%. However, the CTE has notbeen fully resolved because our data only correlatesCoT and PoT based on correctness, not process cor-respondence. In addition to reducing CTE, part ofthe performance improvement in HTL comes fromcorrectly solving problems that both CoT and PoTgot wrong originally.",
  "In experiments, the model enhanced with reinforce-ment learning shows minimal improvement in av-": "erage accuracy (only a 0.3% increase). However,for the MATH dataset, reinforcement learning con-sistently yields improvements. This improvementstems from reinforcement learnings ability to ad-dress the issue of repetitive generation during theCoT in LLMs. When using natural language rea-soning, LLMs tend to enumerate answers, lead-ing to repetitive loops until reaching the maximumlength limitation. Supervised fine-tuning strugglesto suppress this phenomenon, whereas reinforce-ment learning can effectively penalize it when itoccurs.",
  "Discussion": "Do Larger Models Have Issues with PoT?(Gao et al., 2023) achieved good results in testingon LaMDA-137B and PaLM-540B (Rohan Anil,2023) by using text to guide code. (Wang et al.,2023a) also employed the method of combiningnatural language with code, which proved effec-tive on a 70 billion parameter open-source modelas well. We conduct evaluations on MAmmoTH-Coder-13B and MAmmoTH-Coder-34B, calculat-ing the proportion of CTE. On the five datasets,MAmmoTH-Coder-13B has an average error rateof 8.2%, while MAmmoTH-Coder-34B has an er-ror rate of 8.7%. CTE does not decrease with theincrease in model size. In the future, the amountof training text data will still far exceed that ofcode data lakes, making it difficult to solve CTE bymerely increasing the model size and data volume. The PoTential of Focused Attention in OtherTasksThe current autoregressive inference haslimitations in that it cannot obtain the solution to aproblem before generating the first token (Gloeckleet al., 2024).CoT can implicitly increase themodels depth, allowing it more extended think-ing time to arrive at accurate answers (Feng et al.,2023). Extending the models thinking time to getthe right answer before generating the first validtoken will be crucial (Goyal et al., 2023). For rea-soning tasks, Focus Attention can gather informa-tion and allow large models to concentrate on someintermediate processes (such as setting special to-kens) to extend thinking time. On the other hand,Focus Attention can concentrate on the reasoningpart of all reasoning tasks while ignoring the ques-tion (Q), making the reasoning process more reli-able. In several logical/symbolic reasoning tasks,CoT does not significantly outperform directly gen-erating answers (Bao et al., 2024). Focus Attention",
  "Related Work": "Current methods primarily rely on the CoT to ad-dress mathematical problems, generating reason-ing answers in a step-by-step manner (Nye et al.,2021; Imani et al., 2023; Miao et al., 2023; Penedoet al., 2023). The focus of recent research cen-ters on data engineering and prompt design. Inthe realm of data engineering, the efforts aim toenhance the quality and increase the volume (Luoet al., 2023) of CoT data. However, another streamof research identifies several computational chal-lenges associated with exclusively using the CoTapproach. In response, (Chen et al., 2023) intro-duces the PoT, a method that employs Python pro-grams to achieve more accurate results. (Yue et al.,2023) attempts to merge CoT and PoT data in theirdataset, resulting in significant improvements. Sim-ilarly, (Gao et al., 2023) seeks to enhance PoT withCoT by weaving CoT snippets into code lines, em-ploying few-shot prompting to guide the model.ToRA (Gou et al., 2023b) uses imitation learningto allow natural language to correct errors in code.MathCoder (Wang et al., 2023a) improves accuracyby closely integrating natural language with code,distilling large amounts of data from GPT-4. Open-mathInstruct (Toshniwal et al., 2024) employs Mis-tral to explore various problem-solving approachesfor each GSM8K and MATH problem, providing a1.8M open-source dataset to the community.",
  "Conclusion": "In our paper, we identify CTE in mathematicalproblems and explore how to address the gap be-tween large models text and code capabilitiesthrough text and code interaction. We proposeHTL, a method that can more closely integrate CoTand PoT to achieve more accurate reasoning andavoid calculation errors. Our experiment showsthat without introducing additional information,our method achieves excellent results merely bycontrolling the flow of information.",
  "Limitations": "Lack of EquipmentDue to GPU limitations, ourexperiments are only conducted on the 7B model,and we did not attempt larger models like the 34Bor 70B. Although we provide theoretical feasibility,there is a lack of practical experimental support. Data RelevanceThe CoT and PoT data con-structed through automated methods are only asso-ciated based on correctness. We still lack humanevaluation to determine whether their reasoningprocesses correspond accurately. This is evidentfrom our experimental results: there is a significantimprovement for simpler datasets like GSM8K, asthe problem-solving approaches are generally sim-ilar. However, for more challenging datasets thatmay have multiple different solutions, the relevancemight be lower. Exploration of Focus AttentionRegarding Fo-cus Attention, we have not yet determined the spe-cific reason for the need for a gradual increase incoverage to adapt to inference. If we extend thisapproach to other domains, such as incorporatingit into the pre-training stage, it enables the modelto better learn step-by-step generation, PoTentiallyleading to improved results. Experimental Limitations with Closed-sourceModelsWe conduct experiments only on open-source large models. Due to the cost of running themodels in the API, we do not attempt to explorewhether this paradigm can enhance the inferencecapabilities of models like GPT-4 by constructingsimilar prompts. However, in our experiments,merely adjusting the prompts does not result ina significant performance improvement. Further-more, because the training data for GPT-4 is un-known, its results are challenging to interpret.",
  "Kedi Chen, Qin Chen, Jie Zhou, Yishen He, and LiangHe. 2024. Diahalu: A dialogue-level hallucinationevaluation benchmark for large language models": "Wenhu Chen, Xueguang Ma, Xinyi Wang, andWilliam W. Cohen. 2023.Program of thoughtsprompting: Disentangling computation from reason-ing for numerical reasoning tasks. Transactions onMachine Learning Research. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems.",
  "Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Kr-ishna Menon, Sanjiv Kumar, and Vaishnavh Nagara-jan. 2023. Think before you speak: Training lan-guage models with pause tokens": "Kevin Stone Peter Albert Amjad Almahairi YasmineBabaei Nikolay Bashlykov Soumya Batra PrajjwalBhargava Shruti Bhosale Dan Bikel Lukas BlecherCristian Canton Ferrer Moya Chen Guillem CucurullDavid Esiobu-Jude Fernandes Jeremy Fu WenyinFu Brian Fuller Cynthia Gao Vedanuj GoswamiNaman Goyal Anthony Hartshorn Saghar HosseiniRui Hou Hakan Inan Marcin Kardas Viktor KerkezMadian Khabsa Isabel Kloumann Artem KorenevPunit Singh Koura Marie-Anne Lachaux ThibautLavril Jenya Lee Diana Liskovich Yinghai Lu Yun-ing Mao Xavier Martinet Todor Mihaylov PushkarMishra Igor Molybog Yixin Nie Andrew PoultonJeremy Reizenstein Rashi Rungta Kalyan SaladiAlan Schelten Ruan Silva Eric Michael Smith Ran-jan Subramanian Xiaoqing Ellen Tan Binh Tang-Ross Taylor Adina Williams Jian Xiang Kuan PuxinXu Zheng Yan Iliyan Zarov Yuchen Zhang An-gela Fan Melanie Kambadur Sharan Narang Au-relien Rodriguez Robert Stojnic Sergey Edunov",
  "Mathprompter: Mathematical reasoning using largelanguage models": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, NateKushman, and Hannaneh Hajishirzi. 2016. MAWPS:A math word problem repository. In Proceedings ofthe 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 11521157, SanDiego, California. Association for ComputationalLinguistics.",
  "Solomon Kullback and Richard A Leibler. 1951. Oninformation and sufficiency. The annals of mathe-matical statistics, 22(1):7986": "Tamera Lanham, Anna Chen, Ansh Radhakrishnan,Benoit Steiner, Carson Denison, Danny Hernan-dez, Dustin Li, Esin Durmus, Evan Hubinger, Jack-son Kernion, et al. 2023.Measuring faithful-ness in chain-of-thought reasoning. arXiv preprintarXiv:2307.13702. Aitor Lewkowycz, Anders Andreassen, David Dohan,Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, TheoGutman-Solo, Yuhuai Wu, Behnam Neyshabur, GuyGur-Ari, and Vedant Misra. 2022. Solving quantita-tive reasoning problems with language models. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-guang Lou, Chongyang Tao, Xiubo Geng, QingweiLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-ardmath: Empowering mathematical reasoning forlarge language models via reinforced evol-instruct.",
  "Selfcheck: Using llms to zero-shot check their ownstep-by-step reasoning": "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.2020. A diverse corpus for evaluating and developingEnglish math word problem solvers. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 975984, Online.Association for Computational Linguistics. Swaroop Mishra, Arindam Mitra, Neeraj Varshney,Bhavdeep Sachdeva, Peter Clark, Chitta Baral, andAshwin Kalyan. 2022. NumGLUE: A suite of funda-mental yet challenging mathematical reasoning tasks.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 35053523, Dublin, Ireland.Association for Computational Linguistics. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,Henryk Michalewski, Jacob Austin, David Bieber,David Dohan, Aitor Lewkowycz, Maarten Bosma,David Luan, Charles Sutton, and Augustus Odena.2021. Show your work: Scratchpads for intermediatecomputation with language models.",
  "OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-wal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, andSam Altman. 2023. Gpt-4 technical report": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal.2021. Are NLP models really able to solve simplemath word problems? In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 20802094, Online.Association for Computational Linguistics. Guilherme Penedo, Quentin Malartic, Daniel Hesslow,Ruxandra Cojocaru, Alessandro Cappelli, HamzaAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,and Julien Launay. 2023. The refinedweb dataset forfalcon llm: Outperforming curated corpora with webdata, and web data only. Orhan Firat Melvin Johnson Dmitry Lepikhin Alexan-dre Passos Siamak Shakeri Emanuel Taropa PaigeBailey Zhifeng Chen Eric Chu Jonathan H. ClarkLaurent El Shafey Yanping Huang Kathy Meier-Hellstern Gaurav Mishra Erica Moreira Mark Omer-nick Kevin Robinson Sebastian Ruder Yi Tay KefanXiao Yuanzhong Xu Yujing Zhang Gustavo Hernan-dez Abrego Junwhan Ahn Jacob Austin Paul BarhamJan Botha James Bradbury Siddhartha Brahma KevinBrooks-Michele Catasta Yong Cheng Colin CherryChristopher A. Choquette-Choo Aakanksha Chowd-hery Clment Crepy Shachi Dave Mostafa DehghaniSunipa Dev Jacob Devlin Mark Daz Nan Du EthanDyer Vlad Feinberg Fangxiaoyu Feng Vlad FienberMarkus Freitag Xavier Garcia Sebastian GehrmannLucas Gonzalez Guy Gur-Ari Steven Hand HadiHashemi Le Hou Joshua Howland Andrea Hu Jef-frey Hui Jeremy Hurwitz Michael Isard Abe Itty-cheriah Matthew Jagielski Wenhao Jia Kathleen Ke-nealy Maxim Krikun Sneha Kudugunta Chang LanKatherine Lee Benjamin Lee Eric Li Music Li WeiLi YaGuang Li Jian Li Hyeontaek Lim Hanzhao Lin Zhongtao Liu Frederick Liu Marcello Mag-gioni Aroma Mahendru Joshua Maynez Vedant MisraMaysam Moussalem Zachary Nado John Nham EricNi Andrew Nystrom Alicia Parrish Marie Pellat Mar-tin Polacek Alex Polozov Reiner Pope Siyuan QiaoEmily Reif Bryan Richter Parker Riley Alex CastroRos Aurko Roy Brennan Saeta et al. Rohan Anil,Andrew M. Dai. 2023. Palm 2 technical report. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle,Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, YossiAdi, Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, ManishBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-han Xiong, Alexandre Dfossez, Jade Copet, FaisalAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,Thomas Scialom, and Gabriel Synnaeve. 2023. Codellama: Open foundation models for code.",
  "Miles Turpin, Julian Michael, Ethan Perez, andSamuel R. Bowman. 2023. Language models dontalways say what they think: Unfaithful explanationsin chain-of-thought prompting": "Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, SichunLuo, Weikang Shi, Renrui Zhang, Linqi Song,Mingjie Zhan, and Hongsheng Li. 2023a.Math-coder: Seamless code integration in llms for en-hanced mathematical reasoning.arXiv preprintarXiv:2310.03731. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.2023b. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large languagemodels. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 26092634, Toronto,Canada. Association for Computational Linguistics.",
  "Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, andJingming Liu. 2020. Ape210k: A large-scale andtemplate-rich dataset of math word problems": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, WeiShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,Zhangyue Yin, Rongxiang Weng, Wensen Cheng,Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023.Secrets of RLHF in large language models part I:PPO. CoRR, abs/2307.04964."
}