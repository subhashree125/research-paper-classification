{
  "Abstract": "Textual style expresses a diverse set of infor-mation, including interpersonal dynamics (e.g.,formality) and the authors emotions or atti-tudes (e.g., disgust). An open question is howlanguage models can be explicitly controlledso that they weave together target styles whengenerating text: for example, to produce textthat is both negative and non-toxic. One ap-proach to such controlled generation is multi-objective reinforcement learning (RL), but howto best combine multiple objectives in a rewardfunction is an open question. In this paper, weinvestigate various formulations of multi-stylerewards, including calibrated outputs from dis-criminators and dynamic weighting by discrimi-nator gradient magnitudes. We find that our pro-posed dynamic weighting outperforms staticweighting approaches with respect style con-trol while maintaining linguistic quality, andwe explore its effectiveness in 2- and 3stylecontrol. All code and data for the RL pipelineswill be publicly available.1",
  "Introduction": "Textual style is an important component of com-munication that conveys information not includedin the texts raw semantic content (Hovy, 1995).Consequently, it is vital that language models canunderstand and apply styles themselves. Prior workhas explored the domain of controlled style genera-tion, a task in which a generative language modelaims to generate text with a specified style2. How-ever in practice, text frequently contains not only asingle style, but a combination of styles (Kang andHovy, 2021). For example, consider being askedto give feedback to a colleague at work: both for-mal and positive styles would be appropriate. Onthe other hand, if speaking with a friend about a",
  ": An example of our 1- and 2style modelsgenerating completions to a given prompt. Models aretrained with reinforcement learning where the reward isderived from the target styles discriminators": "movie, both informal and positive styles are likelyto be useful. Examples of generations followingmultiple styles models can be found in .Especially as large language models (LLMs)grow in capability and popularity, it is desirableto include fine-grained control of the styles in LLMoutputs. For instance, in almost all cases, toxicityand hate speech must be tightly controlled such thatthe model does not produce harmful output. At thesame time, in response to the users preferences orthe application, it can be beneficial for the LLM tosimultaneously control additional attributes such ashumor, formality, or the use of figurative language.In order to achieve these goals reliably, techniquesfor robust multi-style control are needed.3 3While some sociolinguistics theories distinguish betweentextual style and textual attributes, in this work, we follow thecommon convention in recent NLP papers of broadly usingstyle to encompass both of these ideas (Jin et al., 2022). Controlling for more than one style during gener-ation is an under-investigated area, with prior workfocusing on controlling for a single style, or a styleand a target topic(s) (Keskar et al., 2019; Liu et al.,2022). In this work, we investigate the use of Re-inforcement learning (RL) for controlling multiplestyles. RL approaches satisfy multiple desideratafor generations by employing a reward functionin which each individual desideratum contributesto the reward; this approach is recently gainingmore interest in the alignment literature (see e.g.,fine-grained reinforcement learning from humanfeedback (RLHF) from Wu et al. (2023)). In thiswork, we apply a similar approach for multi-stylecontrolled generation, in which style scores fromindividual style discriminators are combined into asingle reward function during RL fine-tuning.The optimal approach to combining multiple re-ward signals into a reward function is an open ques-tion. To further explore this question, we imple-ment several strategies for formulating the multi-style reward, including a novel dynamic-weightingapproach. Interestingly, our evaluations indicatethat dynamically weighting each component out-performs static weighting, and we also find thatsimple steps such as confidence calibration andbinarization of style discriminator output can im-prove model performance. We also implement acustom plug-and play pipeline (Dathathri et al.,2020) for comparison.This is to our knowledge a first-of-its-kind workinvestigating multi-style controllable generationthrough an RL lens with a new reward shapingapproach via dynamic gradient-based weighting.Work on multi-reward formulation for RL is es-pecially relevant in the current landscape, giventhe modern alignment techniques that incorporatemultiple axes of human feedback, as recent workinvestigates composing multiple types of humanfeedback e.g., helpfulness, correctness into thereward function (Rame et al., 2023).",
  "Related Work": "Controlled Text GenerationMethods for con-trolled generation can generally be grouped intothree main categories:fine-tuning, retraining,and post-processing (Zhang et al., 2023). Post-processing approaches are the most lightweightand involve applying transformations during decod-ing, rather than making any adjustments to modelweights themselves. Examples of such methods include plug and play, or PPLM (Dathathri et al.,2020)), which uses gradients from an attribute clas-sifier to guide the language models hidden state;generative discriminators (GeDI) which computecontrol codes and anti-control codes for all possi-ble next tokens (Krause et al., 2020); and AttributeAlignment, which learns an alignment function(Yu et al., 2021) infuse attribute representationsinto a pre-trained language model to guide genera-tion. Prefix-tuning (Li and Liang, 2021; Qian et al.,2022) can also guide generation by prepending task-or style-specific prefix vectors. Retraining (orrefactoring) methods involve retraining languagemodels from the ground up on the control task; forexample, CTRL (Keskar et al., 2019) retrains aclass-conditional language model conditioned onmany control codes to guide generations. Anotherretraining approach is Cev-LM (Moorjani et al.,2024), a prototype-then-edit semi-autoregressivelanguage model that applies edit vectors in the la-tent space. Our work falls under the fine-tuning category.Fine-tuning methods adjust parameters of a pre-trained LLM toward fulfilling the desired con-trols. Reinforcement learning (RL) is a commonfine-tuning approach for controlled text generation(Zhang et al., 2023), e.g., Gong et al. (2019) usea style classifier model to provide a target stylereward, and Upadhyay et al. (2022) use token-leveldense rewards and taking the weighted sum of theserewards that were heuristically determined to up-date the policy. Other works align models towardsspecific attributes by modeling the reward functionas a preference model (Rafailov et al., 2023) tobypass the need for explicitly calculating a rewardfunction, reducing the task to a maximum likeli-hood objective. More recently, various approachesto fine-tuning language models via human prefer-ences (Ziegler et al., 2020; Ouyang et al., 2022)have seen success in guiding text generations to bemore aligned with desired attributes. Finally, we point out that textual style transfer isrelated to controlled generation, but it is a distincttask that involves transforming an input texts stylewhile preserving the semantics. Recent work instyle transfer include Steering Vectors (Subramaniet al., 2022), which inject steering vectors into themodel during decoding. Variations such as ActAdd(Turner et al., 2023) and activation engineering(Konen et al., 2024) have also been proposed.",
  "ApproachBrief rationaleCalculation": ": Three different approaches to combine multiple rewards effectively for reinforcement learning. di(x)refers to the attribute from discriminator i on input text x. We find dynamic weighting (our proposal) is particularlyeffective for multi-style control. Multi-Objective RewardsRecent work exploreshow to incorporate reward signals from multiplesources into reward functions, particularly in align-ment literature involving RLHF. Notably, recentwork investigates multi-objective RLHF by train-ing separate reward models from human preferencedata and linearly combining those rewards (Wuet al., 2023; Rame et al., 2024; Ram et al., 2024).In this work, we apply a similar approach for con-trolled style generation, and we also experimentwith additional approaches to reward combination. Inverse RLOur work in dynamically shapingthe weights of the reward function is also adjacentto Inverse Reinforcement Learning (IRL), whichlearns both a policy and a reward function. IRLalternates policy update steps with reward modelupdate steps, requiring expert demonstrations forthe reward update step. For example, Ghosh et al.(2021) use IRL for Table-to-Text generation, andtheir reward function takes several table descriptorsas inputs; and Fu et al. (2023) use IRL for thetext summarization task, with a reward functionthat combines various sub-rewards like saliencyand coverage. However, we clarify a distinctionbetween our work and IRL: whereas IRL explicitlylearns a reward function, our dynamic weightingmethod is best characterized as reward shapingfor online policy update. Specifically, we do notuse expert demonstrations to optimize the weightsof the reward function, but instead dynamically\"shape\" the coefficients of the fine-grained rewardfunction components during the policy update stepby inspecting the gradients for each component.",
  "Preliminary Formulation: Style Controlusing Reinforcement Learning": "Reinforcement learning for language modelsframes the generative language model as a policynetwork, . The policy is a probability distributionover all possible actions (i.e., vocabulary tokens)that determines at, i.e., the action to be taken attimestep t given the state xt1. We can then gen-erate tokens based on (at|xt1). Reinforcementlearning also introduces a reward function R thattakes in a state xt and outputs a scalar valuation ofthat state. (Reinforcement learning for languagemodels generally uses sparse rewards, i.e., a rewardis only computed once the full sequence of tokenshave been generated.) The objective of reinforce-ment learning, then, is to learn a new policy such that the policy maximizes the expected valueof R. Controlling a single style using RL fine-tuning typically formulates the reward using a dis-criminator d for the target style, as in R(x) = d(x).",
  "Multi-Reward Control Formulations": "When constructing a reward formulation that com-bines outputs from multiple style discriminators,we consider two important aspects. The first is thediscriminator output itself: we want to find a wayto express the output, e.g., via some transformation,that provides a strong and consistent reward signal.The second aspect we consider is the how toweigh the combined reward. This is important be-cause training can become intractable if the signalfrom one discriminator dominates. For instance,consider a 2-style softmax reward formulation, andimagine our starting policy is such that there isa near-zero likelihood of producing a generationthat results in high softmax scores for both d1 and",
  "Discrim AScore A": ": Pipeline for three-style control using fine-tuning Llama 7B model with Proximal Policy Optimization(PPO). We investigate several techniques for integrating feedback from style discriminators into the reward. d2, but there is a high likelihood of producing gen-erations that results in a high score for d1. Thepolicy will quickly discover how to move in a di-rection that maximizes scores from d1 and moveaway from exploring the states that result in morebalanced rewards. Instead, we want to combine thediscriminators in a way that encourages balancingall outputs.Motivated by these considerations, we exploremultiple approaches to calculate a reward R fora generation x by combining output from theattribute discriminators d1, d2, ..., dn with corre-sponding target styles k1, k2, ..., kn, and we writethe logit value for the target class as di(x)ki. Weconsider the following multi-reward formulations:",
  "temperaturei(2)": "Dynamic WeightingWe calculate a dynamicweight wi. The motivation for the dynamic weight-ing approach is that when combining multiple stylediscriminators, it is not always clear whether a sig-nal from one discriminator should be prioritizedover another. Our approach weighs the result fromeach discriminator by considering the magnitudeof the gradient of the cross entropy loss for di(x)with respect to the desired style. Then, our rewardfunction is",
  "Base Models": "We use LLaMA2 7B (Touvron et al., 2023) as thebase model for both the discriminators and the RLpipeline as shown in . We train discrimi-nators for sentiment, formality, irony, emotion, andtoxicity using the SST2, GYAFC, SemEval-Irony,Go-emotions, and Jigsaw Toxicity datasets (Socheret al., 2013; Rao and Tetreault, 2018; Van Heeet al., 2018; Demszky et al., 2020; Jigsaw). Wetrain these custom discriminators rather than us-ing existing classifiers because classifiers with thesame base model architecture are needed for thePPLM (Dathathri et al., 2020) pipeline. For bettercomparison with PPLM, we use these same customdiscriminators in the RL fine-tuning as well. Thediscriminators are evaluated on the test sets withmacro F1 and achieve results comparable to thosepublished in the original dataset papers ().",
  "Configurations of RL Fine-Tuning": "illustrates our pipeline for RL fine-tuning.The language model generates a completion givena prompt from our dataset. The pooled final hiddenstate from the model is input to each target stylediscriminator, whose outputs are then combinedinto a reward. We use the TRL library (Von Werraet al., 2022) implementation of the proximal pol-icy optimization (PPO) algorithm (Schulman et al.,2017). Due to computational constraints, we uselow rank adapters (LoRA, Hu et al. (2022)) im- plemented by the parameter efficient fine-tuning(PEFT, Mangrulkar et al. (2022)) library.The PPO objective includes a penalty term forthe KullbackLeibler (KL) divergence between thefine-tuned and original language model. Duringtraining, we use an adaptive KL control setting anddo a parameter search for the initial KL coefficientin the range of [0.2, 0.4], eliminating any runs thatresult in final KL divergences over 20. (Larger KLdivergence values in our experience were associ-ated with repetitive reward hacking behaviors.)",
  "Configurations of (Custom) PPLM": "We re-implement the PPLM code in order to use itwith a LLAMAV2 base model rather than GPT-2,as in the original. Our implementation also ex-tends PPLM to consider feedback from multiplediscriminators defined by taking a backward stepin the hidden state along the gradient for all of theattribute discriminators, d1, ..., dn. Specifically, thegradient is for the loss function defined by",
  "Training and Evaluation Prompt Data": "Prompts we use during training are drawn from thetraining sets of these datasets: SemEval emotions(Rosenthal et al., 2017), TweetEval (Barbieri et al.,2020), Rotten Tomatoes (Pang and Lee, 2005), andWikimedia (Wikimedia). We choose these datasetsfor their variety of domains (Tweets, movie reviews,Wikipedia articles). Our evaluation prompts consistof 500 randomly selected items each datasets test",
  "Evaluation": "We evaluate the generations from each model basedon two criteria: first, how often the generations ad-here to the target styles, and second, how well thegenerations maintain linguistic quality of the origi-nal model. Consider a model fine-tuned to producepositive generations: it could simply respond toevery prompt with This is great, I love it! andforce every generation to be classified as positive,but the overall language quality would suffer.Automatic Evaluation We count the proportion ofthe generations that are classified by the discrim-inators as having the target style. We also countthe proportion of the generations with both targetstyles to see how frequently models successfullycombine styles in the same generation. To evalu-ate whether the generations maintain the linguisticquality of the original model, we measure the av-erage perplexity of the generations as well as theirrepetitiveness (duplicate bigram frequency).Human Evaluation To avoid over-reliance on au-tomatic metrics, we also incorporate a human study.Due to financial constraints, the study is on a ran-domly selected subset of 100 of the evaluationprompts. For each prompt, we collect human pref-erences between completions from two models.Annotators chose (a) which completion better ful-fills the target styles, and (b) which completionsounds most natural. Three Master-qualified an-notators from Amazon Mechanical Turk annotateeach item, with compensation of 15 USD/hour.",
  "In this section we describe our experiments forassessing the different multi-style reward formula-tions and the overall performance of our modelsfor various multi-style control settings. The ques-": "tions we investigate are: 1. How can we mosteffectively combine signals from separate style dis-criminators into a reward function? ( 5.1), 2. Howoften do two- and three-style models express thetarget styles in their generation, and how fluent arethese generations? ( 5.2, 5.3), and 3. Which stylecombinations are most difficult to learn? ( 5.4).",
  "Multi-Reward Formulation Evaluation": "We train several versions of models with Informal-Negative and Formal-Negative target styles to in-vestigate the efficacy of the reward formulationsoutlined in Sec 3.2. For a summary of our study onInformal-Negative see ; remaining resultsare in Appendix . We find that for softmaxrewards, the models have minimal style control.We hypothesize that a contributing factor may bepoor confidence calibration of the discriminatorssoftmax scores. To address this, we implementa calibrated softmax reward using the calibrationtechnique in Guo et al. (2017). This results in theExpected Calibration Error (ECE) for sentimentand formality decreasing from 0.133 to 0.016 and0.267 to 0.111 respectively. These calibrated soft-max rewards do offer some improvements relativeto pre-calibration, but improvements are modest.The logit reward models, on the other hand, tendto have better style control at the cost of fluency their generations have some of the highest perplex-ities. During KL hyperparamater search, we alsoanecdotally observed that the logit models often de-veloped highly repetitive reward hacking strategiesfor sub-optimal KL coeffecients.Both the binary rewards and Dynamic Weight-ing rewards produce sizable improvements overother reward formulations while maintaining gen-eration quality. Dynamic Weighting has a slightedge overall: it outperforms the binary approachon all counts for the Negative-Informal combina-tion (60.25% of Dynamic Weighting generations",
  "Target Styles Sent.Form.BothSent.Form.BothSent.Form.Both": "Positive-Formal0.5890.4200.3500.8550.7590.7030.5880.5180.448Positive-Informal0.5890.5800.2390.7500.6700.4600.5800.6420.278Negative-Formal0.4110.4200.0700.6960.6060.3750.4320.5280.076Negative-Informal0.4110.5810.3410.6590.7670.6030.4360.4720.340 : Automatic target style accuracy evaluation of 1- and 2-style models for RL and PPLM. (Both indicates theportion of generations containing both the target sentiment and formality.) All models (Base, Dynamic Weighting,PPLM) use Llama2-7B as the base model for fair comparison. Compare style frequencies to those in the baseLLaMA2 generations to see the effect of the other two approaches.",
  "Prefer Logits8%9%8%32%Prefer Dynamic Weighting (ours)20% 69%71%42%": ": Human evaluation results (preferences weredetermined by majority vote across annotators). Gener-ations from models trained with our dynamic weightingapproach were preferred with respect to both style andlinguistic naturalness. (Annotators could also choose nopreference.) contain both styles versus 56.80% for binary), andits generations are less repetitive. Negative-Formalcombination results are more mixed, with a 10.15%difference in formality in favor of binary and a7.56% difference in negativity in favor of DynamicWeighting. Based on these results, we chose touse the Dynamic Weighting approach for our re-maining experiments, since it displays the highestoverall performance on both control and generationquality.Particularly when combining multiple non-orthogonal styles, a simple linear combination ofscores can make learning difficult: the model maybe able to easily increase its reward by maximizingresults from only one discriminator and get stuckwithout learning how to obtain more well-roundedresults. We conjecture that both the binarizing anddynamic weighting approaches alleviate this issue. 5.1.1Human EvaluationTo bolster the automatic evaluation, we also con-duct a human study. For cost reasons, we limit thisinvestigation to two style combinations (negative-informal and negative-formal), and two reward for-mulations (Logit and Dynamic Weighting). Weask annotators whether they prefer the Logit or Dy-namic Weighting completion with respect to style(randomizing the order in which the generationsare displayed), and we then ask which completion they prefer with respect to the naturalness of thetext. Three annotators respond to each question.Results indicate that humans prefer the DynamicWeighting models generations with respect to bothstyle and linguistic quality (results in ),echoing the conclusions of the automatic evalu-ations. We conclude that the automatic metricsappear to reasonably align with human perceptionof these aspects of the generation.We calculated the inter-annotator agreement us-ing Krippendorffs alpha (Krippendorff, 1980).The alpha for style preference questions is 0.36whereas for linguistic quality questions, alpha is0.23.This indicates fair agreement among annota-tors, and we note that it falls in the range of alphasfor subjective annotation tasks reported in Wonget al. (2021).",
  "Combinations of Two Styles": "We first fine-tune RL models for all possiblecombinations of sentiment and formality: posi-tive+formal, positive+informal, negative+formal,and negative+informal. We choose these style di-mensions since their respective discriminators havethe highest F1 scores (). We use the Dy-namic Weighting reward formulation, as it wasthe most successful reward formulation from Sec-tion 5.1. We consider the generations of the result-ing four models on the evaluation prompts: resultsfor these two-style models are in , and a ran-dom sample of their generations are in Appendix.Comparison with PPLM We extend PPLM tocombine feedback from multiple discriminators() and find that the fine-tuned RL modelshave higher style control. PPLM has some successwith producing more Positive+Formal and Posi-tive+Informal generations than base Llama2, butdoes not show any improvements for the other com-binations. This suggests that PPLM may struggle",
  "Combinations of Three Styles": "We further extend the reward formulation to controlfor three target styles to assess how our approachscales as the number of target styles increases. Forthree style experiments, we keep the two style di-mensions explored earlier (sentiment and formal-ity) and add additional third styles. This decision ismotivated in part by a desire to understand to whatextent adding a third style control influences theability to control these dimensions relative to onlytwo styles. We also consider that the sentiment andformality discriminators have the highest F1 scores,making them best-suited to training and evaluation.The third style dimensions are from our other dis-criminators: toxicity, irony, and emotion. We limitour three style models to four distinct three-stylecombinations due to computational constraints(fully exploring the three-style combination spaceprovided by all of our discriminators would requirefine-tuning 212 models, where each fine-tuningprocedure needs to load the language model cou- : Style control results for two of the 3-stylemodels: Positive+Formal+Neutral (emotion) and Pos-itive+Formal+Irony. Each style dimension shows theportion of generations containing that style and is min-max scaled. The two-style Positive+Formal model re-sults are included for comparison, e.g., we see that the+Neutral model does not control formality as well as thetwo-style model, but the +Irony model does. pled with three discriminators). See for avisualization of these results and for a sum-mary. We find that the three style models are able toincrease the proportion of all three target styles rela-tive to the base Llama2 model, and their generationquality does not decline. For some combinations(e.g., Positive-Formal-Irony), the sentiment andformality dimension control does not deterioratecompared to the two-style model. Other combina-tions (e.g., Negative-Informal-Toxic) show lowersentiment and formality control than the two-stylemodels, although they still make improvementsover the vanilla model.",
  "Imbalance in Multi-Style Combinations": "We hypothesize that rarer style combinations aremore difficult to learn. Das et al. (2023) addressedthis style imbalance problem by balancing theircombinatory distributions from the training data.Similarly, we approach understanding the rela-tive frequency of the possible style combinationsfrom two perspectives: frequency in the evaluationdatasets baseline generations and original humantexts. In both cases, formal and negative is the leastcommon combination, while formal and positiveis most common. Because the training process in-cludes a KL divergence penalty term to maintainlinguistic fluency, rare style combinations that arenot well-represented in the original model will besuppressed in the controlled models outputs; illustrates this relationship. We note that our con-trol models are still able to improve frequencies of positive + formal negative + formal positive + informal negative + informal human dataset frequency comb. accuracy : We observe a linear relationship betweenthe percentage of test generations containing both targetstyles and the prevalence of that target style combinationin the full human texts. This suggests that more commonstyle combinations may be easier to learn to control.",
  "Domain-Based Performance": "When considering the prompts from each of thesubdatasets in the evaluation set separately, weobserve that the Wikipedia portion of the evalu-ation set has a substantial drop in performancefor Negative-Formal and Positive-Informal mod-els with respect to target style accuracy (Fig. A.8).This indicates that model performance can bestrongly affected by the domain of the prompt. Itmay also indicate that the models struggle to gen-eralize the target style across all domains.",
  "Discussion": "We propose an approach to controlled multi-stylegeneration using reinforcement learning with a re-ward derived from a dynamically weighted linearcombination of discriminator outputs. This noveltechnique results in generations that largely con-form to the target styles while maintaining linguis-tic quality.There are multiple possible approaches to themulti-style generation problem; our approach rep-resents only one of these. In addition to PPLM,which could possibly be further optimized beyondour additions for multi-style control, there are alsothe other postprocessing and retraining methodsdiscussed in , along with other fine-tuningapproaches (including Direct Preference Optimiza-tion (DPO) (Rafailov et al., 2023)). In addition,this problem could be approached via prompt engi-neering for LLMs such as GPT-4. Which approachis best-suited to this problem with respect to accu-racy, cost, and efficiency remains an open question.Future work should investigate the strongest over-all approach multi-style control and the relativeaccuracy-efficiency trade-offs. Finally, the precise theoretical relationships be-tween individual styles is an interesting and openquestion. If two styles rarely co-occur, is it becausetheir combination is impossible, or is it simply rare?Does this distinction affect a language models abil-ity to combine the two styles? Our results for thethree-style models hint at the complexity of thisissue. Future work should consider formalizing thefeasibility of different low-frequency style combi-nations, and encouraging language models to ex-plore their state space more in order to uncovermore rare combinations of styles.",
  "Limitations": "Our focus in this work is mostly centered on com-bining specific style dimensions, sentiment andformality, and our three-style combinations stillinclude these specific dimensions. We make thedecision to focus on specific styles due to computa-tional limits making wider exploration infeasible,and we focus on these particular dimensions due tothe high F1 scores of the discriminators for thesestyle dimensions. However, this approach can beextended given more computing resources to fur-ther style combinations (e.g. emotional valenceand arousal, honesty). It can also be extended rein-forcement learning with multiple types of humanfeedback, as recent work investigates composingmultiple types of human feedback e.g. helpful-ness, correctness into the reward function (Wuet al., 2023; Rame et al., 2024).Our approach is less effective for rare style com-binations like negative and formal. For control ofrare style combinations, further techniques such asinitial language model fine-tuning or explorationincentives are likely needed in order to achieve thedesired effect. Our work also relies on discrimina-tors which may be difficult to implement for datascarce attributes. Alignment TaxWe also note two side effects weobserved in the generations after fine-tuning. Thisis somewhat expected, as many researchers havedocumented performance on some benchmarkingtasks decreasing as side effects of RL fine-tuningprocedures (see e.g. Ouyang et al. (2022); Askellet al. (2021)). First, there is a possibility that non-target styles may also shift as a result of the fine-tuning process. Specifically, we observe some vari-ations in uncontrolled styles after fine-tuning (e.g.there is more joy in positive-formal than in positive-informal; there is more anger in negative-formal than in negative-informal. These results are sum-marized in Appendix . Second, we observethat controlling for style can alter the models fac-tual claims. The 500-item Wikipedia subset ofthe evaluation set demonstrates this clearly, as theprompts (e.g. Skydio is an American...) leadthe model to write a completion that makes factualassertions. See example Wikipedia generations andour annotations in Appendix A.4 . Thisindicates that further work is needed before suchmodels can be relied on for factual claims.",
  "Ethics Statement": "This training approach includes no explicit steeringof factuality or truthfulness, which should alwaysbe accounted for before deploying LLMs for non-academic use. Further, it is important to be awareof the possibility that controlling for styles suchas negativity or disgust can correspondingly leadto increases in toxicity and offensive speech formodel generations.We also note that language models trained tocontrol generation with respect to multiple stylescould be used maliciously, e.g., to create specificvoices for counterfeit personas.",
  "Acknowledgements": "This work was supported by Sony Research. Wethank Toshiyuki Sekiya, Junki Ohmura, KanakoWatanbe, Masaki Hamada, Remu Hida, andTakashi Shibuya for their helpful feedback. Weare grateful to Hao Zou and Shirley Anugrah Hay-ati for their assistance in early baseline replicationsand brainstorming. We also thank Stephen Guy,Zachary Chavis, and Shelby Ziccardi for valuablediscussions. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, et al. 2021. Ageneral language assistant as a laboratory for align-ment. arXiv preprint arXiv:2112.00861. Francesco Barbieri, Jose Camacho-Collados, Luis Es-pinosa Anke, and Leonardo Neves. 2020. Tweeteval:Unified benchmark and comparative evaluation fortweet classification. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, pages16441650.",
  "Balancing effect of training dataset distribution ofmultiple styles for multi-style text transfer": "Sumanth Dathathri, Andrea Madotto, Janice Lan, JaneHung, Eric Frank, Piero Molino, Jason Yosinski, andRosanne Liu. 2020. Plug and play language models:A simple approach to controlled text generation. InInternational Conference on Learning Representa-tions. Dorottya Demszky, Dana Movshovitz-Attias, JeongwooKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.2020. GoEmotions: A Dataset of Fine-Grained Emo-tions. In 58th Annual Meeting of the Association forComputational Linguistics (ACL). Yu Fu, Deyi Xiong, and Yue Dong. 2023. Inverse rein-forcement learning for text summarization. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 65596570, Singapore.Association for Computational Linguistics. Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, andShashank Srivastava. 2021. How helpful is inversereinforcement learning for table-to-text generation?In Proceedings of the 59th Annual Meeting of the As-sociation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 2: Short Papers), pages 7179,Online. Association for Computational Linguistics.",
  "Klaus Krippendorff. 1980. Content analysis: An intro-duction to its methodology. Sage publications": "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Lin-guistics. Guisheng Liu, Yi Li, Yanqing Guo, Xiangyang Luo,and Bo Wang. 2022. Multi-attribute controlled textgeneration with contrastive-generator and external-discriminator.In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 59045913, Gyeongju, Republic of Korea. In-ternational Committee on Computational Linguistics.",
  "Jing Qian, Li Dong, Yelong Shen, Furu Wei, and WeizhuChen. 2022. Controllable natural language genera-tion with contrastive prefixes": "Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D Manning, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. arXiv preprintarXiv:2305.18290. Alexandre Rame,Guillaume Couairon,CorentinDancette, Jean-Baptiste Gaya, Mustafa Shukor,Laure Soulier, and Matthieu Cord. 2024. Rewardedsoups: towards pareto-optimal alignment by inter-polating weights fine-tuned on diverse rewards. Ad-vances in Neural Information Processing Systems,36. Alexandre Rame, Guillaume Couairon, Mustafa Shukor,Corentin Dancette,Jean-Baptiste Gaya,LaureSoulier, and Matthieu Cord. 2023. Rewarded soups:towards pareto-optimal alignment by interpolatingweights fine-tuned on diverse rewards. arXiv preprintarXiv:2306.04488. Alexandre Ram, Nino Vieillard, Lonard Hussenot,Robert Dadashi, Geoffrey Cideron, Olivier Bachem,and Johan Ferret. 2024.Warm: On the benefitsof weight averaged reward models. arXiv preprintarXiv:2401.12187. Sudha Rao and Joel Tetreault. 2018. Dear sir or madam,may i introduce the gyafc dataset: Corpus, bench-marks and metrics for formality style transfer. InProceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long Papers), pages 129140. Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.Semeval-2017 task 4: Sentiment analysis in twitter.In Proceedings of the 11th International Workshopon Semantic Evaluation (SemEval-2017), pages 502518.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal,Alec Radford, and Oleg Klimov. 2017.Proxi-mal policy optimization algorithms. arXiv preprintarXiv:1707.06347": "Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics. Nishant Subramani, Nivedita Suresh, and Matthew Pe-ters. 2022. Extracting latent steering vectors frompretrained language models. In Findings of the Asso-ciation for Computational Linguistics: ACL 2022,pages 566581, Dublin, Ireland. Association forComputational Linguistics.",
  "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton": "Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Cynthia Van Hee, Els Lefever, and Vronique Hoste.2018. Semeval-2018 task 3: Irony detection in en-glish tweets. In Proceedings of The 12th Interna-tional Workshop on Semantic Evaluation, pages 3950": "Leandro Von Werra, Lewis Tunstall, Abhishek Thakur,Sasha Luccioni, Tristan Thrush, Aleksandra Piktus,Felix Marty, Nazneen Rajani, Victor Mustar, and He-len Ngo. 2022. Evaluate & evaluation on the hub:Better best practices for data and model measure-ments. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing:System Demonstrations, pages 128136, Abu Dhabi,UAE. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837.",
  "A.4Factuality Annotations": "We examined a random subset of 60 items fromthe Wikipedia-based generations, as these tend toencourage the models to make factual claims dur-ing generations. We perform this investigation dueto concern about factuality shifts after training; forexamples, see . We hand-annotate theitems as shown in , and we also checkwhether the base model generation is factual. Wefind that for 30% of the items, the base modelsclaim is factual. Of these claims, the factual claimwas changed about 11.7% of the time across allfine-tuned models. On the other hand, for 35% ofthe claims, the base models claim is not factual(e.g. the generation Alan Thomas (born 7 Oc-tober 1951) is a former British actor no suchactor exists). In these cases, the fine-tuned modelsaltered claims 65% of the time. In conclusion, fine-tuned models do have shifted perceptions of facts,and this shift is exagerated in cases where the basemodels original claim is incorrect. For 31.2% ofthe prompts, no factual claim is made.",
  "A.5GPT-4 Comparison": "We focus on a basic zero-shot prompting approachto probe GPT-4 performance on controlled stylegenerations. We chose zero-shot prompting as weare interested in scenarios with high data-scarcity,i.e., those in which the target style combination textis not readily available. We note that while few-shot prompting can out-perform zero-shot; this isnot always the case recent work suggests thatperformance can be equivalent or even deterioratein a few-shot setting, e.g. Coyne et al. (2023). GPT-4 responses () indicate that in somescenarios, GPT-4 does very well at combining mul-tiple styles to complete the generation. However,some style combinations (particularly those with aninformal style) have markedly worse performance,likely in part due to differences between how ourdiscriminator and GPT-4 understand informality.We include GPT-4 results as a point of refer-ence, but it is not a direct comparison for a few rea-sons. First, our other experiments use the Llama2-7B architecture, which has approximately 1,000times fewer parameters than GPT-4. In addition,Llama2 is a base language model, whereas GPT-4 includes human feedback fine-tuning. Finally,we also recognize that prompting techniques suchas Chain-of-Thought prompting (Wei et al., 2022)have been shown to improve performance on manytasks and that prompts can have large impacts onGPT-4 performance; however we do not performprompt engineering for output optimization.",
  "GenerationSF": "Feeling blind. New specs coming, but meanwhile... My new glasses are on their way, but until I getthem Im stuck with this. The glasses I have are like two pairs of glasses inP, FFeeling blind. New specs for 2017! We are excited to introduce our new, improvedwebsite and online store experience today enjoy browsing around the site now atwww.eyesightdirect.com",
  "N, IJust watched #TheDarkTower & its a mess. @MidnightMovie0.56 0.55": "Photo: Orchid Tassel Chain Inspiration: In the garden Ive been thinking a lot lately about how I canmake my garden more beautiful, and what I can do to attract more wildlifeP, FPhoto: Orchid Tassel Chain Orchids are a part of the largest family, orchidaea. Theyhave around 250 generas and more than thousand species worldwide!",
  "0.73": ": Five randomly selected generations (i.e., not cherry-picked) from the evaluation prompts. OriginalLLaMA2 generations (in blue) are followed by generations from fine-tuned models with varying style controls. Themodels styles are coded by Positive / Negative and Informal / Formal. Sentiment and Formality classifier outputsfor the target classes are also included (values >0.5 indicate that target sentiment or formality is achieved).6798",
  "LLaMA2 Alan Thomas (born 7 October 1951) is a former Australian rules footballer who playedfor the Geelong Football Club in the Victorian Football League (VFL). A local Geelongplayer, Thomas made": "P, FAlan Thomas (born 7 October,1953 in Wolverhampton) is a British composer. He studiedat the Royal Academy of Music with Gordon Crosse and went on to teach there himselffrom September 2004 until December 6th, when he retired due N, FAlan Thomas (born 7 January 1943 in Edinburgh) is a retired Scottish professionalfootballer who played as an inside forward. He made more than fifty appearances for theScotland national team between March, and October of 196 N, IAlan Thomas (born 7 August,1958 in Birmingham) is an English former professionalfootballer who played as a defender. Thomas began his career with West Ham Unitedbefore joining Wolverhampton Wanderers for 200k on transfer deadline day"
}