{
  "Abstract": "Independent Component Analysis (ICA) offersinterpretable semantic components of embed-dings. While ICA theory assumes that em-beddings can be linearly decomposed into in-dependent components, real-world data oftendo not satisfy this assumption. Consequently,non-independencies remain between the esti-mated components, which ICA cannot elimi-nate. We quantified these non-independenciesusing higher-order correlations and demon-strated that when the higher-order correlationbetween two components is large, it indicatesa strong semantic association between them,along with many words sharing common mean-ings with both components. The entire struc-ture of non-independencies was visualized us-ing a maximum spanning tree of semantic com-ponents. These findings provide deeper insightsinto embeddings through ICA.",
  "Introduction": "Embeddings play an important role in naturallanguage processing, ranging from word embed-dings (Mikolov et al., 2013) to internal represen-tations in language models (Devlin et al., 2019;Brown et al., 2020; Touvron et al., 2023). Under-standing how embeddings represent meaning iscrucial for unraveling black box NLP models.IndependentComponentAnalysis(ICA)(Hyvrinen and Oja, 2000) is an effective methodfor visualizing and interpreting the geometricstructure of embeddings (Musil and Marecek,2024; Yamagiwa et al., 2023). Just as PCA aimsto make coordinate axes uncorrelated, ICA seeksto transform the coordinate axes into statisticallyindependent components. The resulting axes fromICA tend to have sparser component values witha few larger values compared to PCA, whichincreases interpretability as the axes can be seen asspecific semantic components ().However it has been pointed out that the esti-mated independent components are only approxi- : Heatmap visualization of 300-dimensionalSGNS embeddings transformed by PCA and ICA, withaxes sorted by variance and skewness, respectively.Each embedding has been normalized to have a normof 1 for better visual interpretation. For each axis, thetop 4 words (frequency nw 100 in text8) with largestcomponent values were used. The first 100 axes aredisplayed in the top panels, and the first 5 axes withthe word labels are displayed in the bottom panels. SeeAppendices A, B and G for details. mately independent (Hyvrinen et al., 2001; Sasakiet al., 2013, 2014). This is because many real-world datasets cannot be accurately represented asa linear combination of independent components,contradicting the assumption of ICA theory.In this study, we aim to further interpret theresults of applying ICA to embeddings by focus-ing on the non-independence between indepen-dent components. We measure the degree of non-independence by calculating higher-order correla-tions between components and find that compo-nents with large higher-order correlations can beinterpreted as having strong semantic associations.The entire structure is revealed by visualizing themaximum spanning tree of semantic componentswith higher-order correlations as edge weights.",
  "Review: ICA-Transformed Embeddings": "Procedure of ICA.For a centered embeddingmatrix X Rnd that represents the meanings ofn words by d-dimensional vectors, ICA1 seeks atransformation S = XB such that each compo-nent S1, , Sd of the transformed matrix S =[S1, , Sd] is as statistically independent as pos-sible2. The transformation B can be expressed asthe product of the whitening transformation matrixA (e.g., PCA transformation) and the orthogonaltransformation matrix Rica, i.e., the resulting S isrepresented as",
  "S = XARica.(1)": "Here, Rica is obtained by minimizing the mu-tual information3 I(S1 Sd) = H(Si) H(S1 Sd), which is equivalent to maximiz-ing the non-gaussianity4 of the distributions ofSi (Hyvrinen and Oja, 2000). The normalizedICA-transformed embeddings, with each embed-ding in S rescaled to a norm of 1, offer high inter-pretability (Yamagiwa et al., 2023, 2024) and areused for visualizations in this paper. : Scatterplots of normalized word embeddingsalong the 10th and 20th axes. The axes for PCA andICA-transformed embeddings were arranged in descend-ing order of variance and skewness, respectively. In bothtransformations, the components are uncorrelated. Comparison of PCA and ICA. showsthat ICA can find the spiky and interpretable shapeof the embedding distribution (e.g., biology andstars for the 10th and 20th axes, respectively), butPCA cannot. This is because ICA determines thecoordinate axes toward high non-gaussianity, whilePCA only considers variance information. 1For the computation, FastICA (Hyvrinen, 1999) imple-mented in scikit-learn (Pedregosa et al., 2011) is used.2The k-th component Sk is also referred to as Axis k.3H(X) = PX(x) log PX(x)dx is the entropy.4The degree to which a probability distribution deviatesfrom a Gaussian distribution can be measured using statisticsbased on higher-order moments, such as skewness (the thirdmoment) or the negentropy of the distribution.",
  "Higher-Order Correlations AmongEstimated Independent Components": "Non-Independence in Real-World Data.Theindependent components estimated by ICA onreal-world data are uncorrelated but not completelyindependent, with dependencies existing betweencomponents (Hyvrinen et al., 2001; Sasaki et al.,2013, 2014). This is because ICA assumes a lineardecomposition into independent components, anassumption frequently violated in reality. Higher-Order Correlation.To quantify non-independencies, methods like mutual informa-tion and Hilbert-Schmidt Independence Criterion(HSIC) (Gretton et al., 2005) exist. Here we usethe higher-order correlation, the simplest measurein terms of computation and formulation. Thismeasure is expressed as follows:",
  "t=1S2t,iS2t,j.(2)": "Here, S is the whitened matrix5. This can also beinterpreted as the covariance between S2i and S2j ,plus one, as cov(S2i , S2j ) = E((S2i 1)(S2j 1)) =E(S2i S2j )1. If Si and Sj are independent of eachother, then E(S2i S2j ) = E(S2i )E(S2j ) = 1. Thus,the deviation of E(S2i S2j ) from 1 is the degree ofdependence between Si and Sj.",
  ":Heatmaps of the correlation coefficientE(SiSj) and the higher-order correlation E(S2i S2j ) ofcomponent pairs (Si, Sj) from ICA on 300-dimensionalSGNS embeddings. See Appendix C for details": "shows that the estimated independentcomponents of the embeddings are uncorrelated butnot completely independent, with varying degreesof higher-order correlations across pairs. TheseE(S2i S2j ) values provide a useful metric of associa-tion, as demonstrated in the following section. 5The components are (i) centered: E(Si) = 0, the meanof each component is 0, (ii) scaled: E(S2i ) = 1, the varianceof each component is 1, and (iii) uncorrelated: E(SiSj) = 0,the correlations are all zero. E(S20S282) = 1.927E(S26S296) = 2.032E(S212S266) = 1.975E(S216S2118) = 2.124E(S256S2126) = 1.861E(S263S2210) = 2.964Axis 0Axis 82Axis 6Axis 96Axis 12Axis 66Axis 16Axis 118Axis 56Axis 126Axis 63Axis 210 dishesbeerelorabbijudahblooddisordercpupointerorganizationunescosaucebeersspanishportuguesetalmudisraelitesorgansmentalmicroprocessorreturninternationalitufriedalenacionalpaulorabbisyahwehliverdisordersprocessorstringorganizationsinterpoldishbrewingjosriotorahelishakidneysymptomscpuspointersinterpolobservercookedyeastdeportugaljewishisaiahtissuebipolarintelnodestandardizationtemporary",
  "E(S20S223) = 0.990E(S26S213) = 0.992E(S212S257) = 0.993E(S216S257) = 0.996E(S256S2197) = 0.982E(S263S218) = 1.073Axis 0Axis 23Axis 6Axis 13Axis 12Axis 57Axis 16Axis 57Axis 56Axis 197Axis 63Axis 18": "dishesstatesmanelwindowsrabbisbloodscpupopulationorganizationactresssauceastronomerspanishostalmudandorgansandmicroprocessormedianinternationalfootballerfriedphilosophernacionalunixrabbiswasliverwasprocessorestimatedorganizationsmusiciandishjohannjoslinuxtorahinkidneyincpusresidinginterpolactorcookedmathematiciandemicrosoftjewishbytissuebyinteltotalstandardizationsinger : (Top Row) Six randomly selected pairs of components from the top 50 pairs with the highest |E(S2i S2j )1|values. For each component, the top 5 words (frequency nw 100 in text8) with the largest component values arelisted. (Bottom Row) Component pairs with small E(S2i S2k) values. For each component Si with the smaller axisnumber in the pairs (Si, Sj) in the top row, a component Sk with the smallest value of |E(S2i S2k) 1| was selected.",
  "We show that the values of higher-order correla-tions E(S2i S2j ) can be interpreted as the degree ofassociations between semantic components": "Results: Top Row of .The meanings ofeach component, represented by the listed wordsin component pairs with high E(S2i S2j ) values, arestrongly related. For example, focusing on Axis0 and Axis 82, a pair with particularly large val-ues of E(S2i S2j ), we can interpret that Axis 0 hasa meaning associated with dishes and Axis 82with beer, suggesting that there is a semanticrelationship between them. Results: Bottom Row of .On the otherhand, for component pairs with E(S2i S2j ) valuesclose to 1, indicating that the components are con-sidered independent, there is no clear relevancebetween the meanings of the components. For ex-ample, looking at the pair of Axis 0 and Axis 23,which has a small E(S2i S2j ) value, we can interpretthat Axis 0 represents dishes and Axis 23 repre-sents polymath, and there is no direct semanticrelationship between them.Detailed results are shown in Appendix G.",
  "We conducted experiments to quantitatively evalu-ate whether higher-order correlations between ICAcomponents represent semantic relationships": "Settings.Our experimental procedure was as fol-lows. We first selected the top 100 ICA compo-nents, ranked by skewness. For each component i(i = 0, . . . , 99), we created three word lists: Wordlist-1 comprised the top 5 words from componenti, Word list-2 contained the top 5 words from thek-th most correlated component with component i(k = 1, . . . , 5), and Word list-3 consisted of the top5 words from a randomly selected low-correlationcomponent (chosen from the bottom 30% of cor-related components). Using these lists, we gen-erated pairs (list-1, list-2) and (list-1, list-3), andqueried GPT-4o mini to determine which pair wasmore semantically related6. This procedure wasexecuted for all 100 components, resulting in 200total comparisons for each value of k from 1 to 5.The specific prompt used for GPT-4o mini modelis provided in Appendix D. Results and Discussion. shows the re-sult of the experiment. We can see that componentpairs with higher-order correlations tend to be moresemantically related (69.0% for k = 1 vs 27.0% forbottom 30%), and that semantic relatedness grad-ually declines as correlation decreases (69.0% atk = 1 to 56.5% at k = 5). These results quanti-tatively demonstrate that higher-order correlationsbetween ICA components effectively reflect seman-tic relatedness between corresponding words.",
  "wkS2k,10S2k,2wkS2k,10S2k,16wkS2k,10S2k,160wkS2k,27S2k,11wkS2k,27S2k,64wkS2k,27S2k,104": "ribose3755.7adenylate2079.8utr2381.5laertius898.3demeter2348.5tiryns1690.6deoxyribose2963.9effectors1842.5reticulum1942.0preveza788.0hephaestus2204.3knossos1348.1phosphodiester2850.2antisense1639.9genomic1668.6xanthippus764.2hestia2021.5mycenaean1205.6biosynthesis2510.1cyclase1638.9homozygous1599.1rhadamanthus735.5hera1744.6lendering1124.7methyltransferase2482.9myosin1201.8cleaved1181.0thracians711.8cronos1720.2hissarlik1103.1pyrimidine2399.6axons1144.2tubulin1152.4alexandri705.2aphrodite1675.9melos1006.3 : For 6 component pairs (Si, Sj) selected from adjacent component pairs in the MST defined in Sec. 5, thetop 6 words and their corresponding S2t,iS2t,j values that contribute the most to the E(S2i S2j ) value are shown. : Scatter plots of normalized word embeddingsfor axis pairs (10, 2) and (27, 64) with large values ofhigher-order correlations. Blue-labeled words are thetop 4 words for each axiss component values, whilered-labeled words are the top 6 words for the values ofS2t,iS2t,j. See Appendix C for all the pairs in .",
  "nnt=1 S2t,iS2t,j. Here we investigatewords that significantly contribute to the E(S2i S2j )values and gain a more concrete understanding ofthe relationships between components": "Results. presents component pairs se-lected from the maximum spanning tree T150(Sec. 5) and words significantly contributing totheir E(S2i S2j ) values. These words often relate tothe meanings of both components, demonstratingadditive compositionality. For example, in the Axis10 and Axis 2 pair, words like ribose, deoxyribose,phosphodiester, biosynthesis, methyltransferase,and pyrimidine notably contribute to the E(S2i S2j )value, linking biomolecules and chemical compo-nents. Detailed results are shown in Appendix G.",
  "Visualization. shows word embeddingscatter plots for axis pairs (10, 2) and (27, 64) withlarge higher-order correlations to illustrate the dis-tribution of words with significant contributions": "to higher-order correlations. Unlike a typical in-dependent component scatter plot (), theseexhibit many words with large component values inboth axes, reflecting the meanings of both axes anddemonstrating the additive compositionality of em-beddings. For the (10, 2) pair, words that notablycontribute to the E(S2i S2j ) (ribose, deoxyribose,phosphodiester, biosynthesis, methyltransferase,and pyrimidine) appear with significant values inboth components. This abundance of words shar-ing both semantic components is characteristic ofpairs with large higher-order correlations. Detailedresults are shown in Appendix C.",
  "In this section, we construct a maximum spanningtree (MST) based on higher-order correlations tovisualize the non-independence between estimatedindependent components": "Settings.The 300 ICA components, originallyordered by skewness with i = 0, , 299, were re-sorted in descending order of semantic componentconsistency scores to prioritize axes that are moreeasily interpretable as specific semantic compo-nents. The semantic component consistency scoreswere determined by a word intrusion task (Changet al., 2009). A higher consistency score indicateseasier interpretability. Details of the scoring meth-ods are provided in Appendix E.1. We introducethe notation to map the order of consistencyscores to the original axis numbers in the skew-ness order: (j) represents the axis number in theskewness sort for the axis with the j-th highestconsistency score. Then, we consider a weightedcomplete graph G150, with 150 components havinghigh consistency scores S(i) (i 0, , 149) asnodes. For the edge between the node pair (Si, Sj), 2: acid 132: telephone 136: disk 10: dna 140: import 13: windows 15: drugs 16: blood 147: medicine 30: stations 160: evolution 36: http 168: license 169: female 44: plants 45: quantum49: court 50: site 52: infectious 56: cpu 188: less 62: company 193: rights 67: blue 71: game 72: sexual 73: ip 205: names 101: voltage 103: wavelength 111: worn 112: episode118: disorder 119: combustion 248: increase 121: families 123: newspaper 131: iv 7: les 139: frankish 11: gaius 144: p 18: actress 23: statesman 151: century27: greek 157: intelligence 31: swedish 161: km 35: aircraft 37: rail 38: rifle 170: class 172: hell 46: spacecraft 47: ship 185: buried 60: river 64: goddess 84: highest 85: battle 88: force 100: empire 102: encryption 104: archaeological 120: winters 127: poetry : Subtrees of MST T150 defined in Sec. 5. Each node represents an independent component Sk (i.e., Axis k)estimated by ICA. The label of each node is k : TopWord(k), where TopWord(k) is the word with the largestcomponent value along axis k among words with frequency nw 100 in the text8 corpus. The color of the edgebetween nodes (i, j) represents the magnitude of the E(S2i S2j ) value between the components, with darker edgecolors indicating larger values. we set cij = E(S2i S2j ) as the weight. To visual-ize and interpret G150, we compute the maximumspanning tree (MST)7 T150, a spanning tree thatmaximizes the sum of cij in the graph G150. MSTwas relatively more interpretable than other sub-graphs of graph G150, providing a good balancebetween visibility and element relationships. Interpretation of the MST.The MST T150represents a graph structure expressing the non-independence between estimated independent com-ponents. Since the edges in T150 connect compo-nent pairs with large higher-order correlations, wecan interpret that there is a strong relationship be-tween the components connected by these edges.Furthermore, the subtrees of T150 represent groupsof semantically related components, and the com-ponents within these groups tend to have similarmeanings. Results and Discussion. shows a partof the MST T150; the entire MST T150 is exhib-ited in Appendix E.2. The colors correspond tothe clusters obtained by applying spectral cluster-ing8 (Ng et al., 2001) to T150. The weights used forclustering are the higher-order correlations. From 7We used minimum_spanning_tree implemented inNetworkX (Hagberg et al., 2008) for the computation of theMST. See Appendix E.2 for details.8WeusedSpectralClusteringimplementedinscikit-learn (Pedregosa et al., 2011). the MST, we can infer structures such as connec-tions and groupings of meanings among three ormore components9. For example, semantically re-lated component pairs such as (2: dna, 10: acid)in the pink cluster and (27: greek, 64: goddess)in the cyan cluster are connected by edges in T150.Additionally, groups such as {168: license, 13:windows, 56: cpu} in the yellow cluster and {46:spacecraft, 35: aircraft, 47: ship} in the blue clus-ter form semantic clusters as sets of nodes con-nected by edges. The components within thesegroups can be interpreted as having meanings re-lated to computer and vehicle, respectively.",
  "Conclusion": "Both ICA and PCA transformations make the com-ponents uncorrelated. ICA goes further by mak-ing the components nearly independent, but somenon-independence still remains. In this study, weused higher-order correlations to quantify the non-independence between the components in the ICA-transformed word embeddings. By interpretingthese as the semantic associations between the com-ponents and visualizing the overall structure, wecan gain a deeper understanding of the latent se-mantic structure within the embeddings.",
  "The embeddings used in the experiments arelimited to SGNS word embeddings. For amore thorough analysis, it is necessary to con-duct experiments using various types of em-beddings": "For large embedding matrices with a highnumber of data points n, ICA may fail toconverge within a practical timeframe. Toovercome this, we suggest using subsampleddata to estimate the ICA transformation ma-trix, which can then be applied to unseen em-bedding vectors. When n is large, calculating higher-order cor-relations (eq. 2) may become computationallyintensive. This calculation is similar to thecomputation of the variance-covariance ma-trix and can be approximated by subsamplingdata points. Further speedup can be achievedby parallelization of the computation.",
  "Codeisavailableat": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.",
  "Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.Multimodal distributional semantics. Journal of Arti-ficial Intelligence Research, 49:147": "Jonathan Chang, Sean Gerrish, Chong Wang, JordanBoyd-graber, and David Blei. 2009. Reading tealeaves: How humans interpret topic models. In Ad-vances in Neural Information Processing Systems,volume 22. Curran Associates, Inc. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin. 2002. Placing search in context: Theconcept revisited. ACM Transactions on informationsystems, 20(1):116131. Daniela Gerz, Ivan Vulic, Felix Hill, Roi Reichart, andAnna Korhonen. 2016. SimVerb-3500: A large-scaleevaluation set of verb similarity.In Proceedingsof the 2016 Conference on Empirical Methods inNatural Language Processing, pages 21732182. Arthur Gretton, Olivier Bousquet, Alex Smola, andSchlkopf Bernhard. 2005. Measuring statistical de-pendence with hilbert-schmidt norms. In The Inter-national Conference on Algorithmic Learning Theory,ALT, pages 6377. Aric A Hagberg, Daniel A Schult, and Pieter J Swart.2008. Exploring network structure, dynamics, andfunction using networkx. Proceedings of the 7thPython in Science Conference (SciPy 2008), pages1115.",
  "Matt Mahoney. 2011. About the test data": "Toms Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean. 2013. Distributed representa-tions of words and phrases and their compositional-ity. In Advances in Neural Information ProcessingSystems26: Annual Conference on Neural Informa-tion Processing Systems 2013, NeurIPS, pages 31113119. Tom Musil and David Marecek. 2024. Exploring in-terpretability of independent components of word em-beddings with automated word intruder test. In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 69226928, Torino, Italia. ELRA and ICCL. Andrew Ng, Michael Jordan, and Weiss. 2001. On spec-tral clustering: Analysis and an algorithm. In Ad-vances in Neural Information Processing Systems14:Annual Conference on Neural Information Process-ing Systems 2001, NIPS. Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and douard Duchesnay. 2011. Scikit-learn: Ma-chine learning in python. Journal of Machine Learn-ing Research, 12(85):28252830. Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch. 2011.A word at a time:Computing word relatedness using temporal seman-tic analysis. In Proceedings of the 20th InternationalConference on World Wide Web, page 337346.",
  "BRemarks on Axis 57 in": "0.000.020.040.060.08 Correlation with frequency Axis 57 : Boxplot of correlation coefficients betweenword frequency nw and the component values for the0th to 99th axes of the ICA-transformed embeddings.Axis 57 shows a particularly high correlation coefficient. An interesting vertical streak is observed in Axis57 of the heatmap for ICA-transformed embed-dings in . This streak can be explained byseveral factors. As shown in , Axis 57 ex-hibits a strong correlation between component val-ues and word frequencies nw, suggesting that Axis57 is more associated with word frequency thanwith a specific semantic meaning. Additionally,the words used in were selected from thoseappearing more than 100 times in the text8 cor-pus, resulting in a bias towards high-frequency : Scatter plot of word frequency nw versusthe component values of the 57th axis of the ICA-transformed embeddings. Words used in arehighlighted in dark blue. The regression line and coeffi-cient of determination were calculated for words with afrequency of nw 10.",
  ": Scatter plot of word frequency nw versusthe component values of the 0th axis of the ICA-transformed embeddings. The settings are the sameas in": "words. This tendency is further illustrated in ,which demonstrates that words used in the heatmap(nw 100) tend to have larger component valuesalong Axis 57. In contrast, shows that foraxes with weak correlation to word frequency, thewords used in the heatmap do not exhibit notablylarge component values. Consequently, large com-ponent values were observed along Axis 57 in theheatmap, a pattern that was unique to Axis 57 andnot observed in other axes.",
  "Complementary Results for Sec. 4.3": "in Sec. 4.3 presented words with significant con-tributions to higher-order correlations for six axispairs. While the main text illustrated the distri-bution of these highly contributing words throughscatter plots for the two selected pairs, provides scatter plots for all the six pairs. The Relationship Between the Magnitude ofHigher-Order Correlations and the Appearanceof Scatter Plots. presents the scatterplots of word embeddings for 24 component pairs,each with different higher-order correlation val-ues. We can see that as the magnitude of higher-order correlation increases, the number of wordswith large component values along both axes in-creases as well. The selection of these 24 pairs wasconducted as follows: First, we considered 150components S(0), , S(149) with high semanticconsistency (see Appendix E.1 for the calculationmethod). We then sorted all possible componentpairs (Si, Sj) (i, j (0), , (149)) based onthe value of |E(S2i S2j ) 1|. We established 24equally spaced grids between the minimum andmaximum values, and selected pairs closest to eachgrid point without repetition. Words with Significant Contributions to Higher-Order Correlations.We have observed wordswith significant contributions, i.e., with large valuesof S2t,iS2t,j, to higher-order correlations E(S2i S2j )in in Sec. 4.3, and will see further exam-ples in Tables 8 and 9 in Appendix G. Such wordsare labeled in red in the scatter plots in in.3, as well as in Figs. 10 and 11 in Ap-pendix C.2. For axis pairs with large higher-ordercorrelations, we observe a large number of wordsthat make significant contributions to the higher-order correlations. The meanings of these wordsinclude both axes meanings, demonstrating theadditive compositionality of embeddings.",
  "We assigned a semantic coherence score to eachaxis of the ICA-transformed embeddings using theword intrusion task method (Chang et al., 2009)": "Word Intrusion Task.The word intrusion task isa method used to evaluate the semantic coherenceof a set of k words by assessing the ability to iden-tify an intruder word. For instance, consider theset of words {windows, os, unix, linux, microsoft},which has a consistent theme of operating systems.In this case, an unrelated word such as waterskiingshould be easily identifiable as an intruder, as itdoes not align with the theme of operating systems.In our experiment, we assigned coherence scoresto the top k = 5 words (with frequency nw 100in the text8 corpus) for each axis. Selection of the Intruder Word.In order to se-lect the intruder word for the set of top k wordsof each axis a {1, . . . , d}, denoted as topk(a),we randomly chose a word from a pool of wordsthat satisfy both of the following criteria simulta-neously: (i) the word ranks in the lower 50% interms of the component value on the axis a, and(ii) it ranks in the top 10% in terms of the compo-nent value on some axis other than a. For each axis,L = 100 intruder words are randomly selected, andWint(a) denotes the set of these L intruder words.",
  "k": "In this formula, we defined dist(wi, wj) = si sj for the ICA-transformed embeddings. Here,IntraDist(a) denotes the average distance betweenthe top k words, and InterDist(a) represents theaverage distance between the top words and the in-truder words. The score is higher when the intruderwords are further away from the set topk(a). There-fore, this score serves as a quantitative measure ofthe ability to identify the intruder word, thus it isused as a measure of the consistency of the mean-ing of the top k words and the interpretability ofaxes.",
  "E.2Entire Visualization of MST": "is the visualization of maximum span-ning tree (MST) T150 defined in Sec. 5. For a graphG150, where the cost between nodes i and j definedas cij = E(S2i S2j ), the algorithm to find the MSTT is a greedy method that maximizes the total sumof costs, (i,j)T cij, subject to T being a span-ning tree. The greedy algorithm selects edges indecreasing order of cij while adhering to the treeconstraint. Due to the monotonicity of f(x) = 1/x,the decreasing order of cij is equivalent to the in-creasing order of 1/cij. Thus, computing the MSTT150 in the graph G150 is equivalent to finding theminimum spanning tree, which minimizes the sumof 1/cij.",
  "We experimentally confirmed that the structureof higher-order correlations between components": "can be applied to the dimensionality reduction ofembeddings. Specifically, we performed SpectralClustering on the maximum Spanning Tree (MST)T300, which was computed based on the higher-order correlations between components. By reduc-ing the dimensionality of the embeddings throughaveraging the clustered axes, we verified that theaccuracy degradation in Word Similarity Tasks wasmitigated compared to random clustering. Experimental Settings.We conducted our ex-periments using the 300-dimensional word embed-dings (SGNS). These embeddings were subjectedto PCA and ICA to obtain components for cluster-ing. We employed two clustering methods: (1) Ran-dom Clustering and (2) Spectral Clustering on themaximum Spanning Tree (MST) T300, which wascomputed based on the higher-order correlationscalculated using Eq. 2. Clustering was performedwith the number of clusters ranging from 2 to 100.Dimensionality reduction was achieved by averag-ing the clustered axes, resulting in reduced dimen-sions from d = 2 to d = 100. The performance ofthe lower-dimensional embeddings was evaluatedthrough Word Similarity Tasks. For the Word Sim-ilarity Tasks, we utilized six datasets: MEN (Bruniet al., 2014), WS353 (Finkelstein et al., 2002),MTurk (Radinsky et al., 2011), RW (Luong et al.,2013), SimLex999 (Hill et al., 2015), and SimVerb-3500 (Gerz et al., 2016). Each dataset comprisesword pairs along with gold similarity scores, as-signed by human annotators. We employed theSpearman rank correlation coefficient between hu-man ratings and the cosine similarity of the wordembeddings as the evaluation metric. The reportedvalues represent the average scores across the sixdatasets. Results and DiscussionThe experimental resultsare presented in . We observe that ICA-based methods outperform PCA-based methods.Moreover, our proposed method, Spectral Cluster-ing on the MST of ICA components, consistentlyachieves the best performance across all dimen-sions. This can be attributed to the fact that com-ponents included in the same cluster on the MSTlikely have high semantic relevance and play sim-ilar roles in representing the meaning of words.These results validate that considering higher-ordercorrelations between axes better preserves seman-tic relationships in compressed word embeddings,demonstrating the practical utility of our method. Itis important to note that this evaluation assesses the",
  "GSupplementary Tables for ICAComponents and MST Subtrees": "shows all components of the ICA-transformed word embeddings used in our experi-ments. Associated with the experiments in Sec. 4.1, shows the top 60 pairs with the highestE(S2i S2j ) values. Additionally, in and Ta-ble 9, we report on all component pairs in the sub-trees of MST T150 shown in , extending theresults from the selected pairs previously reportedin Sec. 4.3. : Scatter plots of normalized word embeddings for 24 axis pairs. Blue-labeled words are the top 4 wordsfor each axiss component values, while red-labeled words are the top 6 words for values of S2t,iS2t,j.2894 57: s 188: less 205: names 151: century 61: daughter 36: http 18: actress 248: increase 133: fourth 65: word 76: you 178: question 93: languages 32: university 19: party 85: battle 156: claim 24: church 28: album 162: ten 29: film 146: studied 78: fiction 25: russian 112: episode 127: poetry 141: treaty 123: newspaper 108: award 54: buildings 208: short 49: court 56: cpu 155: list 17: consonants 45: quantum 30: stations 64: goddess 193: rights 67: blue 124: israeli 48: day 137: san 37: rail 147: medicine 72: sexual 131: iv 55: appointed 170: class 98: australia 13: windows 198: million 50: site 111: worn 136: disk 71: game 58: imperfect 52: infectious 144: p 169: female 79: canada 105: italy 176: article 31: swedish 12: rabbi 122: hip 129: schools 40: isbn 81: philosophy 94: ethnic 35: aircraft 140: import 116: nazi 102: encryption 75: births 90: comics 242: nor 95: dutch 10: dna 159: theatre 185: buried 160: evolution 220: cleese 77: painting 60: river 101: voltage 91: administrative 118: disorder 46: spacecraft 164: iran 15: drugs 174: park 44: plants 38: rifle 22: instrument 100: empire 121: families 84: highest 97: capitalism 158: dance 119: combustion 68: football 47: ship 168: license 135: manuscripts 120: winters 132: telephone 88: force 236: voting 27: greek 89: austria 39: pitcher 23: statesman 99: missouri 106: buddha 139: frankish 41: islands 255: renowned 73: ip 107: dog 21: japanese 3: al 83: wimbledon 62: company 148: olympics 80: serbian 74: africa 5: und 104: archaeological 154: york 149: saint 34: irish 103: wavelength 16: blood 145: donald 230: spreading 7: les 43: currency 42: jpg 33: polish 196: indonesian 171: monster 175: test 2: acid 172: hell 157: intelligence 96: o 290: selling 11: gaius 161: km",
  "E(S22S210) = 2.323E(S222S259) = 2.247E(S226S2134) = 2.233E(S214S2114) = 2.228E(S22S253) = 2.165E(S216S252) = 2.158Axis 2Axis 10Axis 22Axis 59Axis 26Axis 134Axis 14Axis 114Axis 2Axis 53Axis 16Axis 52": "aciddnainstrumentconcertocarsprixtopologicalmorphismsacidelementbloodinfectioushydrogenproteinsinstrumentsfuguefordgrandisomorphichomomorphismhydrogenmetalsorgansinfectionacidsrnabasssonatacarschumacherbanachhydrogenacidselementsliverdiseaseohmrnaguitarsbwvchassisracetopologywavelengthsohuraniumkidneyinfections E(S210S2114) = 2.150E(S216S2118) = 2.124E(S21S2121) = 2.107E(S244S2121) = 2.096E(S228S2122) = 2.086E(S219S255) = 2.075Axis 10Axis 114Axis 16Axis 118Axis 1Axis 121Axis 44Axis 121Axis 28Axis 122Axis 19Axis 55 dnamorphismsblooddisordergenusfamiliesplantsfamiliesalbumhippartyappointedproteinshomomorphismorgansmentalspeciesfamilyplantfamilyalbumshoppartiesministerrnahydrogenliverdisordersextinctolderflowersolderbanddjdemocratscabinetmrnawavelengthskidneysymptomsbirdshouseholdfloweringhouseholdsongsrapdemocraticappoints E(S230S2112) = 2.060E(S26S296) = 2.032E(S221S2110) = 2.028E(S227S264) = 1.997E(S213S2168) = 1.991E(S251S2128) = 1.987Axis 30Axis 112Axis 6Axis 96Axis 21Axis 110Axis 27Axis 64Axis 13Axis 168Axis 51Axis 128 stationsepisodeelojapanesemartialgreekgoddesswindowslicensecoachballfmairedspanishportuguesejapanjudogreecegodsoscopyleftquarterbackscrimmageradioshownacionalpaulotokyoaikidoathensdeityunixgpldefensivegoalbroadcasttvjosrioemperorkarateatheniandeitieslinuxlicensesbengalsfoul E(S212S266) = 1.975E(S24S2106) = 1.974E(S256S2136) = 1.967E(S217S2114) = 1.966E(S215S2118) = 1.959E(S210S216) = 1.947Axis 12Axis 66Axis 4Axis 106Axis 56Axis 136Axis 17Axis 114Axis 15Axis 118Axis 10Axis 16 rabbijudahindiabuddhacpudiskconsonantsmorphismsdrugsdisorderdnabloodtalmudisraelitesindianbuddhismmicroprocessorfloppyvowelshomomorphismdrugmentalproteinsorgansrabbisyahwehnehrumahayanaprocessordisksvowelhydrogenheroindisordersrnalivertorahelishahindubuddhistcpusdrivesconsonantwavelengthslsdsymptomsmrnakidney",
  "E(S20S282) = 1.927E(S253S2150) = 1.915E(S280S289) = 1.909E(S25S259) = 1.897E(S239S251) = 1.885E(S210S252) = 1.875Axis 0Axis 82Axis 53Axis 150Axis 80Axis 89Axis 5Axis 59Axis 39Axis 51Axis 10Axis 52": "dishesbeerelementnuclearserbianaustriaundconcertopitchercoachdnainfectioussaucebeersmetalsbombserbsbelgiumderfuguesoxquarterbackproteinsinfectionfriedaleelementsbombsserbialuxembourgdiesonatabasemandefensivernadiseasedishbrewinguraniumfissioncroatiagermanydasbwvpitchersbengalsmrnainfections E(S253S286) = 1.872E(S239S2128) = 1.871E(S26S2173) = 1.869E(S28S2106) = 1.867E(S256S2126) = 1.861E(S285S2153) = 1.857Axis 53Axis 86Axis 39Axis 128Axis 6Axis 173Axis 8Axis 106Axis 56Axis 126Axis 85Axis 153 elementrockspitcherballelbasquechinesebuddhacpupointerbattlefinnsmetalsvolcanicsoxscrimmagespanishspainchinabuddhismmicroprocessorreturnbattlessovietselementsgranitebasemangoalnacionalaragonbeijingmahayanaprocessorstringdefeatcoluraniumgeologicpitchersfouljosetapinyinbuddhistcpuspointersfoughtir E(S274S289) = 1.854E(S215S216) = 1.834E(S283S2105) = 1.832E(S217S2233) = 1.828E(S271S2143) = 1.828E(S253S2114) = 1.824Axis 74Axis 89Axis 15Axis 16Axis 83Axis 105Axis 17Axis 233Axis 71Axis 143Axis 53Axis 114 africaaustriadrugsbloodwimbledonitalyconsonantshonggamecardelementmorphismsafricanbelgiumdrugorgansopennorwayvowelskonggamescardsmetalshomomorphismafricansluxembourgheroinliverfinalistnetherlandsvowelguidancegameplaydealerelementshydrogennamibiagermanylsdkidneyquartergermanyconsonantminemultiplayerbettinguraniumwavelengths E(S230S2132) = 1.820E(S210S2160) = 1.811E(S286S2195) = 1.809E(S2101S2103) = 1.800E(S235S288) = 1.798E(S24S289) = 1.794Axis 30Axis 132Axis 10Axis 160Axis 86Axis 195Axis 101Axis 103Axis 35Axis 88Axis 4Axis 89 stationstelephonednaevolutionrockscretaceousvoltagewavelengthaircraftforceindiaaustriafmphoneproteinsevolutionaryvolcanicgeologicelectricallightflightarmyindianbelgiumradiomobilernadarwingraniteepochcircuitswavelengthsboeingmilitarynehruluxembourgbroadcastcellularmrnaselectiongeologicextinctioncurrentlaserairlinesregimenthindugermany E(S2107S2138) = 1.793E(S215S244) = 1.793E(S232S2129) = 1.793E(S220S246) = 1.788E(S23S2124) = 1.786E(S226S2119) = 1.785Axis 107Axis 138Axis 15Axis 44Axis 32Axis 129Axis 20Axis 46Axis 3Axis 124Axis 26Axis 119 doghorsedrugsplantsuniversityschoolsstarsspacecraftalisraelicarscombustionhoundhorsesdrugplantcollegeschoolconstellationnasaibnpalestinianforddieseldogsridingheroinflowerstechnologysecondarystarastronautsmuhammadpalestinianscarturbinebreedbreedlsdfloweringinstituteeducationconstellationsastronautabuisraelchassisengine",
  "wkS2k,188S2k,119wkS2k,188S2k,205wkS2k,67S2k,111wkS2k,67S2k,103wkS2k,101S2k,103wkS2k,101S2k,119": "thermojet816.0gec505.9sash2800.1thz2245.4photodiode2784.6alternator1508.5dirtier805.6nervosa481.3hakama2144.4monochromatic1777.2diodes2320.6cogeneration1485.3cng739.9dddddd430.1turbans1756.4protanomaly1545.8diode1959.3clamp1370.2oxidiser655.3ler373.6waists1745.6fluoresce1376.9ssi1633.5solenoid1183.5tuyere634.7gomoku356.8tunics1612.8scintillator1366.4bandgap1523.4thermionic1144.8propfan630.0kel317.6pleated1561.4diffuser1233.8photodiodes1478.2transformer1055.4 : Complementary experimental results to . For all component pairs (Si, Sj) in the first subtree ofthe MST in , the top 6 words and their corresponding S2t,iS2t,j values that contribute the most to the E(S2i S2j )value are presented. E(S22S210) = 2.323E(S22S2119) = 1.755E(S2132S230) = 1.820E(S2132S273) = 1.693E(S2132S262) = 1.632E(S2136S256) = 1.967Axis 2Axis 10Axis 2Axis 119Axis 132Axis 30Axis 132Axis 73Axis 132Axis 62Axis 136Axis 56 aciddnaacidcombustiontelephonestationstelephoneiptelephonecompanydiskcpuhydrogenproteinshydrogendieselphonefmphonetcpphonecorporationfloppymicroprocessoracidsrnaacidsturbinemobileradiomobileprotocolsmobilecompaniesdisksprocessorohmrnaohenginecellularbroadcastcellularprotocolcellularshareholdersdrivescpus"
}