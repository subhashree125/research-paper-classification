{
  "Abstract": "Chinese Spell Checking (CSC) aims to de-tect and correct spelling errors in sentences.Despite Large Language Models (LLMs) ex-hibit robust capabilities and are widely appliedin various tasks, their performance on CSCis often unsatisfactory. We find that LLMsfail to meet the Chinese character-level con-straints of the CSC task, namely equal lengthand phonetic similarity, leading to a perfor-mance bottleneck. Further analysis reveals thatthis issue stems from the granularity of tok-enization, as current mixed character-word tok-enization struggles to satisfy these character-level constraints. To address this issue, wepropose C-LLM, a Large Language Model-based Chinese Spell Checking method thatlearns to check errors Character by Character.Character-level tokenization enables the modelto learn character-level alignment, effectivelymitigating issues related to character-levelconstraints.Furthermore, CSC is simpli-fied to replication-dominated and substitution-supplemented tasks. Experiments on two CSCbenchmarks demonstrate that C-LLM achievesan average improvement of 10% over exist-ing methods. Specifically, it shows a 2.1%improvement in general scenarios and a sig-nificant 12% improvement in vertical domainscenarios, establishing state-of-the-art perfor-mance. The source code can be accessed at",
  "Introduction": "Chinese Spell Checking (CSC) involves detectingand correcting erroneous characters in Chinese sen-tences, playing a vital role in applications (Gaoet al., 2010; Yu and Li, 2014). Although Large Lan-guage Models (LLMs) exhibit potent capabilitiesand are increasingly being applied to a variety oftasks (Wang et al., 2023; He and Garner, 2023; Wu",
  "et al., 2023a), previous studies (Li and Shi, 2021)showed that generative models, such as LLMs (Liet al., 2023a), do not perform well on CSC": "The CSC task inherently involves character-levellength and phonetic constraints. The character-level length constraint requires the predicted sen-tence maintain the same number of characters asthe source sentence. Additionally, the phoneticconstraint necessitates that the predicted charactersclosely match the phonetics of the source charac-ters, as approximately 83% of spelling errors arephonetically identical or similar to the correct ones(Liu et al., 2010). We find that LLMs often failto meet these character-level length and phoneticconstraints in the CSC task. Using GPT-4 (Achiam et al., 2023) as an ex-ample, we observed that under few-shot prompt-ing, 10% of the models predicted sentences didnot match the character count of the source sen-tences.In contrast, this issue was entirely ab-sent in BERT-style models. Additionally, 35% ofpredicted characters were phonetically dissimilar to the source characters, and errors due to non-homophone predictions account for approximately70% of all prediction errors. These deficiencies incharacter length and phonetic similarity result inoutputs that fail to meet task requirements, leadingto suboptimal correction performance.We find that the underlying issue lies in thegranularity of the LLMs tokenization. The cur-rent mixed character-word tokenization results in acharacter-to-word mapping. This prevents LLMsfrom learning character-level alignment and tendsto produce predictions that do not satisfy character-level constraints. As shown in , under themixed character-word tokenization, the LLM needsto infer that multiple tokens corresponds to a singletoken (e.g., \"(bold)\",\"(large)\",\"(of)\"->\"(large amount)\") and deduce implicit char-acter alignment (e.g., \"(bold)\"->\"(large)\").These reasoning processes complicate the CSC, asthe majority of CSC cases involve simply replicat-ing characters. For example, the correct character\"(amount)\" is copied directly from the source.Despite the advancements in the semantic under-standing capabilities of LLMs across various tasks,unclear character alignment can still lead to mis-corrections and over-corrections. Therefore, it isvital to establish explicit character-level alignment.Building on this concept, we propose C-LLM,a Large Language Model-based Chinese SpellChecking method that learns to check errorsCharacter by Character. Our motivation is to en-code at the character level and establish character-level alignment for training sentence pairs, therebyalleviating the issues related to character-level con-straints. As shown in , this approach en-sures that the number of tokens in sentence pairsremains consistent, making it easier for LLMs tolearn the phonetic mappings between Chinese char-acters. Furthermore, CSC is simplified to the tasksof replicating correct characters and replacing in-correct ones, without complex reasoning.Specifically, we construct the character-level to-kenization to ensure that tokens are encoded ac-cording to individual Chinese characters. To adaptthe model to the new vocabulary, we perform con-tinued training on a large dataset. Furthermore, toenable the LLMs to learn CSC, we conduct super-vised fine-tuning on CSC datasets. Experimentson the general dataset CSCD-NS (Hu et al., 2022)and the multi-domain dataset LEMON (Wu et al.,2023b) show that C-LLM outperforms existingmethods in both general and vertical domain sce- narios, establishing state-of-the-art performance.The contributions of this work can be summa-rized in three aspects: (1) We find that mixedcharacter-word tokenization hinders LLM fromeffectively understanding the character-level con-straints in CSC. (2) We propose the C-LLM, whichlearns character-level alignment and can check er-rors character by character. (3) Through testingon general and multi-domain datasets, we foundthat C-LLM achieves state-of-the-art performance,providing insights for the design of future errorcorrection models.",
  "Related Work": "BERT-style CSC Models With the emergence ofpre-trained language models, the dominant methodfor CSC has shifted to BERT-style models (Devlinet al., 2019), which treat CSC as a sequence label-ing task. These models map each character in a sen-tence to its correct counterpart and are fine-tunedon pairs of sourece and reference sentences. Addi-tionally, some studies have integrated phonologicaland morphological knowledge to improve the label-ing process (Cheng et al., 2020; Guo et al., 2021;Huang et al., 2021; Zhang et al., 2021). However,due to parameter constraints, these models under-perform in low-frequency and complex semanticscenarios compared to LLMs.Autoregressive CSC models Unlike BERT-stylemodels, which can infer each token in parallel, au-toregressive CSC models process tokens sequen-tially. Previous research (Li and Shi, 2021) indi-cates that autoregressive models like GPT-2 (Rad-ford et al., 2019) may underperform on CSC. Withthe advancement of LLMs, several studies haveinvestigated their text correction capabilities. Thestudy (Li et al., 2023b) finds that while ChatGPT1 know the phonetics of Chinese characters, theycan not understand how to pronounce it, makingphonetic error correction challenging. Other stud-ies (Fang et al., 2023; Wu et al., 2023a) note thatChatGPT often produces very fluent correctionsbut also introduces more over-corrections. Thesefindings align with our observations, emphasizingthe need to enhance LLMs performance on CSC.",
  "BERT75.5460.8867.4271.3457.4963.6779.6561.7969.5974.9658.1565.49SMBERT75.6862.9668.7471.4559.4464.9079.9764.1271.1775.5360.5667.22SCOPE79.4966.9672.6976.3964.3569.8683.3068.0874.9279.7265.1571.70": ": The performance of GPT-4 and BERT-style models (Devlin et al., 2019; Zhang et al., 2020; Li et al., 2022)on the CSCD-NS test set is evaluated at both the sentence and character levels, with precision (P), recall (R), and F1score (F1) reported (%) for both detection (D) and correction (C) tasks. sider a source sentence = {1, 2, .., }consisting of characters, which may containspelling errors. The corresponding reference sen-tence = {1, 2, .., } contains the samenumber of characters as , and with all errors cor-rected. Notably, a significant proportion of the cor-rected characters are phonetically identical orsimilar to erroneous character . The CSC modelidentifies character-level spelling mistakes in theinput and generates the predicted sentence =1, 2, .., , where is the character pre-dicted for and should be equal to accordingto the CSC. In this process, the tokens of the sourcesentence and the reference sentence after tokeniza-tion can be represented as = {1, 2, ..., }and = {1, 2, ..., }, respectively.",
  "Analysis of LLMs in CSC": "LLMs now exhibit powerful language processingcapabilities and are widely used (Zhao et al., 2023).Similar to previous studies (Wang et al., 2023; Wuet al., 2023a), we conduct a preliminary analy-sis of LLM performance on the CSC using GPT-4 (Achiam et al., 2023) with in-context learning(Brown et al., 2020). Our experiments leverage theGPT-4 API and employ few-shot prompt (see Ap-pendix A.3) on the CSCD-NS (Hu et al., 2022) testset for spelling correction. The prompt comprisedfive positive and five negative examples, randomlyselected from the CSCD-NS training set.As shown in , GPT-4s performance inspelling correction is inferior to that of BERT-stylemodels. Our analysis indicates that GPT-4 strug-gles to meet two key constraints of the CSC task:character-level length and phonetic similarity. Thismisalignment results in a significant portion ofthe predictions that do not meet task requirements,leading to suboptimal correction performance.Statistics reveal that 10% of GPT-4s predictedsentences fail to meet the character-level length",
  ": Statistic results of non-homophone characters": "constraint, adversely affecting both precision andrecall. Additionally, as illustrated in , GPT-4 generates 35% of characters that are not phonet-ically similar to the source ones. Among these,97% are incorrect, and these incorrect phonologi-cally dissimilar characters constitute a significantportion (70%) of all prediction errors, severely im-pacting the models performance. Therefore, iden-tifying the root causes of LLMs inability to satisfycharacter-level length and phonetic constraints iscrucial for improving their performance.",
  "Mixed Character-Word Tokenization": "By analyzing the tokenization used by the LLMsfor CSC, we find that the current mixed character-word tokenization is the primary reason why LLMsstruggle to meet the character-level length and pho-netics constraints. Under this tokenization, sen-tences with spelling errors result in a character-to-word mapping that prevents LLM from establish-ing a clear character-level alignment. We analyzethis issue through the following two scenarios (seecases in Appendix A.2), where and denotesthe erroneous character and the corresponding ref-erence character, respectively, \"\" denotes thecorrespondence between the tokens and characters:",
  "{1, } , +1 {+1}(4)": "(2) In Equation 3~ 4, even if the token counts areconsistent, the characters may not align clearly dueto erroneous characters and reference charactersbeing placed in mismatched tokens.In both cases, LLM cannot directly map charac-ters (e.g., -> ). This leads to three problems:(1) The inconsistency in the number of tokens be-tween sentence pairs prevents LLM from learningthe constraint of equal character length. (2) The un-clear character correspondence hinders LLM fromlearning the constraint of similar character pronun-ciation. (3) The CSC task becomes more complex,involving numerous inference scenarios rather thancharacter copying and replacement.However, in the CSC task, most correct charac-ters in the source sentence can be directly copiedduring prediction, with only a small proportionof misspelled characters requiring replacement.Therefore, establishing a clear alignment betweencharacters is crucial for this task.",
  "Methodology": "The CSC task requires a character-level map-ping, necessitating character-by-character correc-tion rather than token-by-token.Since currentLLMs process sentences at the token level, map-ping each character to a token can intuitively re-duce the complexity of CSC for LLMs. Based onthis concept, we propose C-LLM (as shown in Fig-ure 3), a Large Language Model-based Chinese",
  "Character-Level Tokenization": "The vocabulary of LLMs is typically multilingual.However, since CSC primarily addresses errors inChinese, we only focus on the Chinese portion ofthe vocabulary. As shown in Equations 14, LLMsoften map multiple characters to a single tokenduring tokenization, complicating the CSC task bypreventing a direct alignment between characters.To mitigate this issue, we construct character-leveltokenization to ensure that each Chinese characteris mapped to a single token. This approach facil-itates a clear alignment between characters in thetokenized sentences, as represented by the follow-ing equation:",
  "{1} , +1 { } , +2 {+1} (6)": "Specifically, the approach for constructing thecharacter-level tokenization of LLM (e.g., QWEN(Bai et al., 2023)), is detailed in Algorithm 1. Forthe BPE (Gage, 1994) tokenization, we refine thevocabulary and the merging rules. With the newvocabulary, the model is unable to recognize wordscomposed of multiple Chinese characters, result-ing in each Chinese character being mapped to aseparate token according to the revised mergingrules. Experimental results indicate that the newvocabulary size is reduced to 89.2% of the original.",
  ": Update the models input and output embed-ding according to the new vocabulary": "QWEN (Bai et al., 2023)) to adapt it to the newvocabulary. Specifically, we performed continuedpre-training with LoRA (Hu et al., 2021) on theChinese open-source pre-training dataset providedby Tigerbot (Chen et al., 2023b), which includesChinese books, internet content, and encyclopedias.The training data comprised approximately 19Btokens, but we trained for 30,000 steps, coveringabout 2B tokens. More implementation details areprovided in the Appendix A.1. To evaluate the impact of the character-level tok-enization and continued pre-training on the LLMslanguage modeling ability, we measure the per-plexity of LLMs using the Chinese domain model-ing competency assessment dataset from Skywork(Wei et al., 2023). As shown in , the perplex-ity increased significantly after applying character-level tokenization, indicating a substantial impacton language modeling ability. However, this effectwas mitigated after continued pre-training, bring-ing the language modeling ability close to that ofthe original LLM. This demonstrates that the modeleffectively adapted to the new vocabulary.",
  "Supervised Fine-tuning": "After continue pre-training, LLM only learns gen-eral language features and does not understand thespecific requirements of the CSC. Therefore, su-pervised fine-tuning is necessary for the LLM tolearn the CSC task. We utilize LoRA (Hu et al.,2021) for the fine-tuning. The training loss is de-fined as follows and the implementation details areprovided in Appendix A.1 and .",
  "Datasets Previous studies (Liu et al., 2021; Xuet al., 2021) chose SIGHAN (Wu et al., 2013; Yuet al., 2014; Tseng et al., 2015) as the benchmark": "However, an increasing number of studies (Huet al., 2022; Yin and Wan, 2023; Li et al., 2022)have identified numerous issues with this dataset,such as semantically incoherent and annotation er-rors. Consequently, in our study, we chose two newCSC benchmarks, namely CSCD-NS and LEMON:(1) CSCD-NS (Hu et al., 2022): CSCD-NS supe-rior in quality to SIGHAN, is the first CSC datasetwhere the primary source of character errors stemsfrom pinyin input methods, containing a significantamount of homophonic and word-level errors. (2)LEMON (Wu et al., 2023b): LEMON is a novel,large-scale, multi-domain CSC dataset featuringvarious real-world spelling errors. It spans sevendifferent sub-domains, including game (GAM), en-cyclopedia (ENC), contract (COT), medical care(MEC), car (CAR), novel (NOV), and news (NEW),typically testing the models domain correction ca-pabilities in a zero-shot setting. Appendix A.4shows the data statistics.Following the fine-tuning approach of previouswork (Li et al., 2022; Liang et al., 2023), we com-bined the training data from CSCD-NS and 271Kpseudo-data generated by ASR or OCR (denotedas Wang271K) (Wang et al., 2018) as our trainingset. The validation data from CSCD-NS was usedas our validation set, and we test the models on theCSCD-NS test data and LEMON, respectively.Evaluation Metrics We report sentence-leveland character-level precision, recall, and F1 scoresto evaluate different models. These metrics arereported separately for detection and correctiontasks. We calculate metrics using the script fromCSCD-NS (Hu et al., 2022). For predictions fromLLMs that do not match the source sentence length,we first employ ChERRANT (Zhang et al., 2022)to extract non-equal length operations, then replacethese with the source before calculating the metrics.",
  "Baselines": "We use the following CSC models for compari-son. BERT-style models. (1) BERT (Devlin et al.,2019): BERT approaches CSC as a sequence label-ing task, encoding the input sentence and employ-ing a classifier to select the appropriate charactersfrom the vocabulary. (2) Soft-Masked BERT (SM-BERT) (Zhang et al., 2020): SMBERT composedof a detection and correction network, enhancesBERTs error detection capabilities. (3) SCOPE(Li et al., 2022): SCOPE incorporates an auxiliarypronunciation prediction task with an adaptive taskweighting scheme to improve CSC performance. For the selection of LLMs, we carry out a se-ries of experiments using QWEN1.5 (Bai et al.,2023). As one of the most potent open-sourceLLMs in China, QWEN exhibits robust Chineseprocessing capabilities and has released model pa-rameters of multiple scales. We evaluate the perfor-mance of LLMs under the following two settings,and the prompts for LLMs are detailed in the Ap-pendix A.3.Fine-tuned LLM (LLM-SFT): The originalLLMs (Original), the LLMs with character-leveltokenization (Char), and the further pre-trainedcharacter-level LLMs (Char-PT) are each fine-tuned on the aforementioned dataset.LLM with In-Context Learning (LLM-ICL):The original LLMs (Original), ChatGPT and GPT-4are adapted to perform the CSC task using prompts.",
  "Main Results": "The main results on the CSCD-NS and LEMONtest sets are presented in , revealing severalobservations: (1) The models error correction per-formance with prompts is suboptimal. Even withGPT-4, achieving satisfactory results is challenging.However, supervised fine-tuning significantly im-proves performance, emphasizing its importance.(2) Compared to C-LLM, LLMs without contin-ued pre-training (Char-SFT) show a decline in av-erage performance, highlighting the necessity ofcontinued pre-training for better adaptation to newvocabulary and improved performance. This isalso evident in the perplexity comparison in Sec-tion 4.2. (3) In domain-specific data, the concisenature of news language in the NEW dataset andthe idiomatic expressions in the GAM dataset makemodels with continued pre-training more prone toincorrect corrections. (4) The original LLM outper-forms BERT-style models in error correction, indi-cating that LLMs have an advantage over BERT-style models in CSC tasks, especially in verticaldomains, consistent with the insights in .(5) C-LLM demonstrates superior error correctionperformance in both general and vertical domainscompared to BERT-style models and the originalLLM, achieving state-of-the-art performance. Thisconfirms the effectiveness of character-level errorcorrection.",
  "Scaling Trends": "To further investigate the impact of model size oncorrection performance for LLMs, we also conductexperiments under 4B, 1.8B, and 0.5B parameters,while keeping the fine-tuning dataset and traininghyperparameters consistent. As shown in ,the correction performance of the LLMs decreaseson both test sets as the parameter size reduces.Comparing C-LLM with BERT-style models,C-LLM outperforms BERT-style models at both14B and 7B parameter sizes on the CSCD-NS andLEMON, particularly excelling in vertical domaintasks. However, smaller models exhibit weaker per-formance. We speculate that despite the simplifica-tion of the CSC through character-level tokeniza- tion, smaller models still struggle to understand thetask adequately, resulting in poor performance.Comparing C-LLM with the original LLM, C-LLM consistently outperforms the original LLMacross various parameter sizes on the CSCD-NSdataset, although the performance gap narrows at1.8B. This indicates that C-LLM has superior er-ror correction capabilities compared to the originalLLM. However, on the LEMON dataset, C-LLMunderperforms the original LLM at sizes of 4B andsmaller. We attribute this to the substantial amountof domain-specific data included in the pre-trainingof original LLM (Bai et al., 2023), whereas ourcontinued pre-training for C-LLM only includesgeneral Chinese data. This may lead to the forget-ting of some domain knowledge in LLM. LargerC-LLM models (14B and 7B) suffer less from this forgetting due to their larger parameter sizes. De-spite some domain knowledge being forgotten, thecharacter-level correction approach allows larger C-LLM models to achieve better performance, whilesmaller models are more affected by knowledgeforgetting, resulting in poorer performance.Comparing C-LLM with Char-SFT, Char-SFTconsistently underperforms C-LLM across bothdatasets and all model sizes. This underscores theimportance of continued pre-training, which en-ables the model to better adapt to new vocabularyand achieve improved performance.",
  "Target100%1.74%/": ": Statistical results from the length and pho-netic perspective, using the 14B models as an example.\"Target\" refers to the reference sentences in the test set.\"Ratio\" indicates the ratio of non-homophone charactersin incorrect predictions. C-LLM alleviates issues related to character-level length constraints. To evaluate the effec-tiveness of Char-PT-SFT (C-LLM) in addressingcharacter-level length constraints, we select sen-tence pairs from the CSCD-NS test set. Thesepairs exhibit tokenization discrepancies betweenthe source and reference sentences, highlightingcharacter-to-word mapping issues. By comparingthe models predictions on these sentence pairs tosee if it maintains the same number of charactersas the source sentence, we can better assess its un-derstanding of character-level length. As shown in, Original-SFT increases the proportion ofpredictions maintaining the character-level lengthto 96.92% compared to Original-ICL, indicatingthat fine-tuning helps LLMs adhere to character-level length constraints.Under C-LLM, the consistency in character-levellength further improves to 99.78%. This findingdemonstrates that the one-to-one correspondencebetween tokens and Chinese characters enablesLLMs to more easily generate sentences that meetcharacter-level length constraints, resulting in su-perior performance.C-LLM can reduce phonologically dissimi-lar predictions. We calculate the proportion ofnon-homophonic characters among all predicted",
  ": Analysis of Inference Speed. \"AR\" indicatesthe acceptance rate generated by draft model": "characters and the proportion of non-homophonicerrors among all incorrect predicted charactersin the CSCD-NS test set. As shown in ,Original-ICL produces more than half of the non-homophonic errors, with the majority of its incor-rect predictions being non-homophonic errors. Incontrast, Original-SFT significantly reduced bothproportions, indicating that supervised fine-tuninghelps the LLMs maintain phonetic constraints.C-LLM generates fewer non-homophonic pre-diction errors, reducing the proportion of non-homophonic errors among total prediction errorsby approximately 20% compared to Original-SFT.This suggests that although C-LLM still producessome non-homophonic predictions, the impact ofthese errors on LLMs correction performance hasbeen greatly diminished.",
  "Inference Speed Analysis": "Using a character-level tokenizer can decrease themodels inference speed. In this study, we performa quantitative analysis of this impact by employ-ing speculative decoding (Chen et al., 2023a). Ourevaluation uses samples containing spelling errorsfrom the CSCD-NS test set. The target model has7B parameters, while the draft model has 1.8B pa-rameters, with draft tokens set to 4. Specifically, totest the speculative decoding capability of Original-SFT-7B, we use Original-SFT-1.8B as the draftmodel. For Char-PT-SFT-7B, we use Char-PT-SFT-1.8B as the draft model.As shown in , under Char-PT-SFT-7B,the number of decoded tokens increased by 52%compared to Original-SFT-7B, but the overall timeconsumption only increased by 22.33%. This isbecause the task complexity was reduced by Char-PT-SFT-7B, leading to a higher acceptance rate forspeculative decoding compared to original LLM.",
  "Conclusion": "This paper indicates that LLMs fail to meet the Chi-nese character-level constraints of the CSC task,namely equal length and phonetic similarity, whichhinders their correction performance. We find thatthe root cause lies in the granularity of tokeniza- tion, which mixes characters and words, making itdifficult to satisfy these character-level constraints.To address this issue, we propose C-LLM, whichestablishes mappings between Chinese characters,enabling the model to learn correction relationshipsand phonetic similarities. This approach simplifiesthe CSC task to character replication and substitu-tion. Experimental results demonstrate that C-LLMoutperforms previous methods on both general andmulti-domain benchmarks, achieving state-of-the-art performance.",
  "Limitations": "Our work has three main limitations.First,our method is specifically designed for Chinesespelling checking and may not effectively addresssentences with English errors, as we did not pro-cess English words in the vocabulary. Second, ourmodel has room for improvement, especially inhandling new and trending words, which may re-quire integrating methods such as RAG. Finally,our models inference time is longer compared tothe original model, indicating a need for furtheroptimization for practical applications.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. 2023a. Accelerating large language modeldecoding with speculative sampling. arXiv preprintarXiv:2302.01318.",
  "Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanx-uan Xin, and Cong Fu. 2023b.Tigerbot:Anopen multilingual multitask llm.arXiv preprintarXiv:2312.08688": "Xingyi Cheng, Weidi Xu, Kunlong Chen, ShaohuaJiang, Feng Wang, Taifeng Wang, Wei Chu, andYuan Qi. 2020. SpellGCN: Incorporating phonologi-cal and visual similarities into language models forChinese spelling check. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 871881, Online. Association forComputational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Tao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jin-peng Hu, Lidia S. Chao, and Yue Zhang. 2023. Ischatgpt a highly fluent grammatical error correc-tion system? A comprehensive evaluation. CoRR,abs/2304.01746.",
  "Processing (Volume 1: Long Papers), pages 59585967, Online. Association for Computational Lin-guistics": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Jiahao Li, Quan Wang, Zhendong Mao, Junbo Guo,Yanyan Yang, and Yongdong Zhang. 2022. Improv-ing chinese spelling check by character pronunciationprediction: the effects of adaptivity and granularity.arXiv preprint arXiv:2210.10996.",
  "Piji Li and Shuming Shi. 2021.Tail-to-tail non-autoregressive sequence prediction for chinesegrammatical error correction.arXiv preprintarXiv:2106.01609": "Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang,Yangning Li, Feng Zhou, Hai-Tao Zheng, and QingyuZhou. 2023a. On the (in) effectiveness of large lan-guage models for chinese text correction.arXivpreprint arXiv:2307.09007. Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang,Yangning Li, Feng Zhou, Hai-Tao Zheng, and QingyuZhou. 2023b. On the (in)effectiveness of large lan-guage models for chinese text correction. CoRR,abs/2307.09007.",
  "Zihong Liang, Xiaojun Quan, and Qifan Wang.2023.Disentangled phonetic representation forchinese spelling correction.arXiv preprintarXiv:2305.14783": "Chao-Lin Liu, Min-Hua Lai, Yi-Hsuan Chuang, andChia-Ying Lee. 2010. Visually and phonologicallysimilar characters in incorrect simplified Chinesewords. In Coling 2010: Posters, pages 739747,Beijing, China. Coling 2010 Organizing Committee. Shulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, andDi Wang. 2021.PLOME: Pre-training with mis-spelled knowledge for Chinese spelling correction.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages29913000, Online. Association for ComputationalLinguistics.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Yuen-Hsien Tseng, Lung-Hao Lee, Li-Ping Chang, andHsin-Hsi Chen. 2015. Introduction to sighan 2015bake-off for chinese spelling check. In Proceedingsof the Eighth SIGHAN Workshop on Chinese Lan-guage Processing, pages 3237. Dingmin Wang, Yan Song, Jing Li, Jialong Han, andHaisong Zhang. 2018. A hybrid approach to auto-matic corpus generation for Chinese spelling check.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages25172527, Brussels, Belgium. Association for Com-putational Linguistics.",
  "Jiaan Wang, Yunlong Liang, Fandong Meng, HaoxiangShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.2023. Is chatgpt a good nlg evaluator? a preliminarystudy. arXiv preprint arXiv:2303.04048": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu,Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng,Weiwei L, Rui Hu, et al. 2023. Skywork: A moreopen bilingual foundation model.arXiv preprintarXiv:2310.19341. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "Xunjian Yin and Xiaojun Wan. 2023. A comprehensiveevaluation and analysis study for chinese spellingcheck. arXiv preprint arXiv:2307.13655": "Junjie Yu and Zhenghua Li. 2014. Chinese spelling er-ror detection and correction based on language model,pronunciation, and shape. In Proceedings of TheThird CIPS-SIGHAN Joint Conference on ChineseLanguage Processing, pages 220223. Liang-Chih Yu, Lung-Hao Lee, Yuen-Hsien Tseng, andHsin-Hsi Chen. 2014. Overview of sighan 2014 bake-off for chinese spelling check. In Proceedings of TheThird CIPS-SIGHAN Joint Conference on ChineseLanguage Processing, pages 126132. Ruiqing Zhang, Chao Pang, Chuanqiang Zhang, Shuo-huan Wang, Zhongjun He, Yu Sun, Hua Wu, andHaifeng Wang. 2021. Correcting chinese spellingerrors with phonetic pre-training.In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 22502261. Shaohua Zhang, Haoran Huang, Jicong Liu, and HangLi. 2020. Spelling error correction with soft-maskedBERT. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages882890, Online. Association for Computational Lin-guistics. Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li,Bo Zhang, Chen Li, Fei Huang, and Min Zhang. 2022.MuCGEC: a multi-reference multi-source evaluationdataset for Chinese grammatical error correction. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 31183130, Seattle, United States. Associationfor Computational Linguistics. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models.arXiv preprintarXiv:2303.18223.",
  "A.2Examples for illustration": "During model training, the mapping betweensource tokens and reference tokens is learned. The presents examples illustrating the mismatchbetween source and reference tokenization in twoscenarios, using sentences containing a single erroras examples: (1) Case1 corresponds to Equations 1~ 2 in thepaper, where the number of tokens in the sourcesentence does not match the reference sentence,resulting in multiple tokens mapping to a singletoken (e.g., \"(win)\", \"(name)\", \"(of)\"->\"(famous)\").(2) Case2 corresponds to Equations 3~ 4, wherethe token counts are consistent, but the charactersmay not align clearly due to erroneous and refer-ence characters being placed in mismatched tokens(e.g., The tokens where \"(reach)\" and \"(big)\"are located are not aligned). However, even if thecharacters can be placed in matched tokens (e.g.\"(present)\"->\"(open)\"), the semantic cor-respondence between tokens may be disrupted dueto improper tokenization.The examples above fail to establish a clearcharacter-level mapping, requiring the model to de-duce implicit character alignment (e.g., \"(win)\"->\"(write)\", \"(reach)\"->\"(big)\", \"(eye)\"->\"(screen)\"). This complicates the CSC by turn-ing it into a semantic inference problem, therebyhindering the models ability to effectively learncharacter-level length and phonetic constraints.",
  "A.3Prompts Setting": "presents the prompts used to evaluate the er-ror correction performance of the fine-tuned LLM,along with the few-shot prompts for ChatGPT, GPT-4 and Original-ICL. The few-shot prompt consistsof 10 examples: 5 sentence pairs without typos and5 with typos. These positive and negative examplesare randomly selected from CSCD-NS, and theirpositions within the prompt are also randomized.",
  "A.4Data Statistics": "The statistical results for the Wang271K, CSCD-NS and LEMON datasets are presented in .The LEMON spans seven different sub-domains,including game (GAM), encyclopedia (ENC), con-tract (COT), medical care (MEC), car (CAR), novel(NOV), and news (NEW). To better evaluate modelperformance, we filtered out sentences from theLEMON dataset where the source and referencesentences had unequal character-level lengths orwhere the source sentence exceeded 1000 charac-ters.",
  ": Statistics of the training, development and test datasets": "In the first case, although the correct mappingis from \"(as well)\" to \"(this)\", the modelfails to understand the relationship between theincorrect characters. It splits \"(as well)\" intotwo tokens and predicts characters that do not meetphonetic constraints.In the second case, the original LLM shouldmap the characters \"(comprehensive)\" and\"(analyze)\" to the word \"(detail)\". How-ever, it incorrectly maps \"(comprehensive)\" to\"(accurate)\", with the predicted characters notbeing phonetically similar to the source ones. These errors indicate that the original LLM lacksa clear understanding of characters and words,making it unable to accurately correct misspelledwords.In contrast, C-LLM can correctly cor-rect misspelled characters within words throughcharacter-level tokenization.However, the third case shows that C-LLM mayalso make errors when correcting single incorrectcharacters, indicating that there is still room forimprovement in our model. For some new popularwords it may be necessary to combine the RAG(Lewis et al., 2020) method to do error correction."
}