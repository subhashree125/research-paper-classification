{
  "Abstract": "Recent studies have explored the use of LargeLanguage Models (LLMs) with Retrieval Aug-mented Generation (RAG) for KnowledgeGraph Question Answering (KGQA). Theytypically require rewriting retrieved subgraphsinto natural language formats comprehensi-ble to LLMs. However, when tackling com-plex questions, the knowledge rewritten byexisting methods may include irrelevant in-formation, omit crucial details, or fail toalign with the questions semantics. To ad-dress them, we propose a novel rewritingmethod CoTKR, Chain-of-Thought EnhancedKnowledge Rewriting, for generating reason-ing traces and corresponding knowledge in aninterleaved manner, thereby mitigating the limi-tations of single-step knowledge rewriting. Ad-ditionally, to bridge the preference gap betweenthe knowledge rewriter and the question an-swering (QA) model, we propose a trainingstrategy PAQAF, Preference Alignment fromQuestion Answering Feedback, for leveragingfeedback from the QA model to further op-timize the knowledge rewriter. We conductexperiments using various LLMs across sev-eral KGQA benchmarks.Experimental re-sults demonstrate that, compared with previousknowledge rewriting methods, CoTKR gener-ates the most beneficial knowledge represen-tation for QA models, which significantly im-proves the performance of LLMs in KGQA 1.",
  "Introduction": "Large Language Models (LLMs) have achievedremarkable performance across various natural lan-guage processing tasks, marking a significant mile-stone (Sanh et al., 2022; Brown et al., 2020; Zhanget al., 2022; Azaria et al., 2024; Chen, 2024). De-spite their superior performance in zero-shot sce-narios (Wei et al., 2022a; Kojima et al., 2022), they",
  "Equal contribution. Corresponding author.1Ourcodeisavailableat": "still encounter factual errors, known as hallucina-tions (Ji et al., 2023b), especially in knowledge-intensive tasks like question answering (QA) (Huet al., 2023; Tan et al., 2023; Li et al., 2023; Heet al., 2023). This issue arises due to the intrinsiclimitations of LLMs, including factual inaccura-cies and outdated knowledge (Pan et al., 2023).To address this challenge, a substantial of work(Ma et al., 2023a; Trivedi et al., 2023; Wu et al.,2023b) retrieves task-relevant knowledge from ex-ternal sources as context, thereby enhancing thecapabilities of LLMs in downstream tasks, knownas Retrieval-Augmented Generation (RAG) (Lewiset al., 2020; Gao et al., 2023; Huang et al., 2023).Recent work (Wu et al., 2023b; Baek et al., 2023; Sen et al., 2023; Wang et al., 2024) under the RAGparadigm explores the use of Knowledge Graphs(KGs) (Pan et al., 2017b,a) as an information sourceto enhance the capabilities of LLMs in QuestionAnswering (QA). Unlike typical QA tasks, a keychallenge in KGQA under this paradigm lies intransforming question-related subgraphs into natu-ral language that LLMs can understand while pre-serving the structural information (Ko et al., 2024;Ding et al., 2024; Wu et al., 2023b). This pro-cess is referred to as Knowledge Rewriting (KR)in this study. As illustrated in , this pa-per summarizes the commonly used knowledgerewriting methods in existing work. Most priorstudies (Baek et al., 2023; Sen et al., 2023; Wanget al., 2023a) employ simple linear concatenationmethod (Triple), which concatenates the subject, re-lation, and object of each triple to form triple-formtext. Additionally, considering that LLMs are pre-trained on text corpora and struggle with structuredtriple-form text, some efforts (Wu et al., 2023b;Bian et al., 2021; Chen et al., 2022) focus on con-verting triples into natural language through KG-to-Text. Furthermore, given that retrieved subgraphsoften contain redundant information irrelevant tothe question, other studies (Ko et al., 2024; Dern-",
  ": The commonly used knowledge rewriting methods in existing work": "bach et al., 2024) aim to extract question-relevantknowledge from the triples to generate summarypertinent to the question.Although these strategies are effective, they ex-hibit several limitations: (1) Redundancy or omis-sions. As illustrated in , knowledge gen-erated by Triple and KG-to-Text are verbose, con-taining excessive irrelevant information.Sum-mary provides a question-related summary but at-tempts to organize all relevant knowledge in onestep. Given the extensive knowledge necessary toaddress complex questions, this method may notencapsulate all critical information, potentially re-sulting in the omission of key points. (2) Semanticmismatch. The three existing methods shown in ignore the semantics of the question andlack a logical organization that aligns with the ques-tions reasoning path.To this end, we propose Chain-of-Thought En-hanced Knowledge Rewriting, CoTKR. Inspired byReAct (Yao et al., 2023), the core of our method in-volves generating reasoning traces and correspond-ing knowledge in an interleaved manner. As shownin , we alternate the following two op-erations: (1) Reasoning: decomposing the ques-tion to identify the knowledge required for infer-ence; (2) Summarization: summarizing the rele-vant knowledge from retrieved triples, informedby the reasoning steps output.By integratingChain-of-Thought (CoT) (Wei et al., 2022b) withknowledge rewriting, CoTKR filters out irrelevantinformation and extracts question-related knowl-edge.Moreover, it generates a well-organizedknowledge representation2 semantically alignedwith the question. Unlike traditional CoT appli-cations in QA, our framework employs the knowl-",
  "In this paper, knowledge representation refers to thenatural language form of question-related knowledge": "edge rewriter to first summarize knowledge, whichthen serves as contextual information to enhanceQA performance. This strategy offers superior ro-bustness. Although the summary might be inaccu-rate, it still contributes valuable information, po-tentially leading to correct answers. However, ap-plying CoT to QA requires more precise reasoningchains, which are significantly affected by the errorpropagation (Wang et al., 2023a; Yao et al., 2023).To train knowledge rewriters based on LLMs, wedesign a training framework for CoTKR. In thefirst stage, inspired by previous work (Ma et al.,2023a; Wu et al., 2023b; Ko et al., 2024), we useknowledge representations generated by ChatGPTto guide the supervised fine-tuning of the knowl-edge rewriter, enabling it to initially master thecapability of knowledge rewriting. In the secondstage, we introduce Preference Alignment fromQuestion Answering Feedback (PAQAF) to bridgethe preference gap between the knowledge rewriterand the QA model. This method evaluates the qual-ity of different knowledge representations based onthe corresponding responses from the QA model.Subsequently, it constructs preference pairs, andfine-tunes LLMs through direct preference opti-mization (DPO) (Rafailov et al., 2023).We conduct experiments on GrailQA (Gu et al., 2021) and GraphQuestions (Su et al., 2016), com-paring commonly used knowledge rewriting meth-ods in existing work. Contrary to previous findings(Dai et al., 2024; Baek et al., 2023), which suggestthat LLMs perform better with knowledge in triple-form rather than in natural language, our findingsdemonstrate that LLMs can significantly benefitfrom knowledge represented in carefully craftednatural language. This indicates that our methodcould substantially enhance the performance ofLLMs in KGQA.",
  "The main contributions of this paper are:": "We propose CoTKR, a Chain-of-Thought En-hanced Knowledge Rewriting method to im-prove the quality of knowledge representationthrough the application of CoT. This methodgenerates reasoning traces and correspondingknowledge in an interleaved manner, therebyproducing well-organized knowledge repre-sentations that are coherent with the ques-tions semantics. We propose a training strategy PAQAF,PreferenceAlignmentfromQuestionAnswering Feedback, to bridge the prefer-ence gap between the knowledge rewriterand the QA model. It assesses the qualityof different knowledge representations byevaluating corresponding responses from theQA model. Then, it constructs preferencepairs and employs DPO to optimize theknowledge rewriter. We conduct experiments on two KGQAbenchmarks. Compared with other knowledgerewriting methods, CoTKR can generate themost beneficial knowledge representation forthe QA model and further enhance the perfor-mance of LLMs in KGQA. Additionally, con-sidering privacy and cost issues, we evaluatethe performance of open-source and closed-source LLMs as the foundational models forknowledge rewriting and QA.",
  "KG-Augmented LLMs for KGQA": "To mitigate hallucination in LLMs, existing work(Wu et al., 2023b; Baek et al., 2023; Sen et al.,2023; Wang et al., 2024) attempts to enhance LLMswith KGs in the RAG paradigm. The nave ap-proach involves retrieving question-related triplesfrom KGs as contextual information for QA (Baeket al., 2023; Sen et al., 2023). Although this methodhas proven effective, there remains ample room forimprovement. Some studies (Wang et al., 2024,2023a) integrate Chains-of-Thought (CoT) (Weiet al., 2022b) with RAG to tackle complex ques-tions.Keqing(Wang et al., 2024) decomposescomplex questions using predefined templates, re-trieves candidate entities from KG, reasons throughsub-questions, and ultimately generates answerswith clear reasoning paths. KD-CoT (Wang et al., 2023a) validates and adjusts reasoning traces inCoT through interactions with external knowledge,thereby addressing issues of hallucinations and er-ror propagation. Furthermore, alternative efforts(Wu et al., 2023b; Ko et al., 2024) address the limi-tations of LLMs in processing structured knowl-edge and the noise in retrieved triples by post-processing these triples into natural language orsummaries pertinent to the questions.This paper focuses on optimizing the knowledgerepresentation under the RAG paradigm for KGQA.Unlike previous work that transforms triples intothe natural language in one step, we adopt CoT tosummarize relevant knowledge step-by-step, ensur-ing comprehensiveness and semantic coherence inthe generated knowledge.",
  "Preference Alignment for LLMs onQuestion Answering": "LLMs have the potential to generate content thatcontains gender discrimination, unethical elements,and racial biases, inconsistent with human values(Wu et al., 2023a; Ray, 2023). To address this issue,Preference Alignment (PA) (Ji et al., 2023a; Wanget al., 2023b) aims to fine-tune LLMs to align withhuman preferences. Existing QA work based onLLMs uses PA to bridge the gap between modelpreferences and those of humans or the QA tasks.KnowPAT (Zhang et al., 2023) trains LLMs on aknowledge preference set to align their knowledgebiases with human preferences, selecting better fac-tual knowledge as context. BGM (Ke et al., 2024)utilizes downstream task metrics as rewards to opti-mize the bridging model between retrievers and QAmodels. Rewrite-Retrieve-Read (Ma et al., 2023b)employs QA evaluation metrics as reward signals,fine-tuning the query rewriting module. EFSUM(Ko et al., 2024) constructs preference pairs sam-pled from LLMs and fine-tunes the knowledge sum-marizer using the Direct Preference Optimization(DPO) (Rafailov et al., 2023) algorithm.Our study innovatively employs responses fromthe QA model to evaluate the quality of knowledgerepresentations. We then construct preference pairsfrom these evaluations and optimize the knowledgerewriter using DPO.",
  ": Illustration of our KGQA framework. CoTKR generates reasoning traces and corresponding knowledgein an interleaved manner": "collection is denoted by G = {(s, r, o) | s, o E, r R}, where E represents the set of entitiesand R represents the set of relations.Knowledge Graph Question Answering (KGQA)aims to answer natural language questions by utiliz-ing a set of facts within KGs. Following previouswork (Saxena et al., 2020; Jiang et al., 2023b), weassume that the subject entity of the question isgiven. Given a question q and a subject entity e,the objective is to generate a response a using therelevant facts in the KG G that accurately addressesthe question.Knowledge Rewriting (KR) for KGQA aims totransform question-related triples into natural lan-guage that can be consumed by LLMs. Given aquestion q and a subgraph G = {(s, r, o) | s, o E, r R} retrieved from KG G, the task is to gen-erate a natural language sequence X that providescontextual information to answer the question.",
  "Chain-of-Thought Enhanced KnowledgeRewriting": "The architecture of our QA framework is depictedin . Initially, our framework retrieves aquestion-related subgraph from the KG, which issubsequently transformed into contextual knowl-edge using CoTKR. This contextual knowledge,along with the question, prompts the QA model togenerate an answer. The core of this framework isthe knowledge rewriter. Briefly, it alternatively con-ducts the following two operations: Reasoning: de-composing the question and generating a reasoningtrace based on generated knowledge representationand pointing out the specific knowledge needed forthe current step; Summarization: summarizing therelevant knowledge based on the current reasoningtrace from the subgraph.Assume we have the reasoning traces at step",
  "illustrates the training framework ofCoTKR": "4.2.1Supervised Fine-tuning with KnowledgeDistilled from ChatGPTThis stage enables open-source LLMs to initiallyacquire the knowledge rewriting capability throughsupervised fine-tuning. This primarily comprisestwo steps: reference knowledge representation gen-eration and supervised fine-tuning.Reference Knowledge Representation Genera-tion. Inspired by previous work (Ma et al., 2023a;Wu et al., 2023b; Ko et al., 2024), we employ Chat-GPT as the data generator to construct trainingcorpora. We verbalize the question-related sub-graph, G, through simple linear concatenation and",
  ": Our training framework for CoTKR": "combine it with the question, q, to form the in-put prompt x. Subsequently, ChatGPT generatesthe reference knowledge representation k based onseveral examples (i.e., demonstrations) and the pro-vided input x. Finally, we construct the trainingdataset DT = {(x1, k1), (x2, k2), ..., (xT , kT )}.Supervised Fine-tuning. For every pair of inputand output (xi, ki) in the training dataset DT , ourknowledge rewriter R is trained to generate kibased on xi using the following objective:",
  "Preference Alignment from QuestionAnswering Feedback (PAQAF)": "In this stage, Preference Alignment (PA) is em-ployed to bridge the preference gap between theknowledge rewriter and the QA model. This stageincludes four steps: candidate knowledge represen-tation sampling, preference annotation based onQA feedback, data augmentation based on Chat-GPT, and direct preference optimization (DPO).Candidate Knowledge Representation Sampling.We input the question q and the corresponding sub-graph G, then sample M candidate knowledge rep-",
  "resentations, k1, k2, ..., kM, from the knowledgerewriter R": "Preference Annotation based on Question An-swering Feedback. Among the candidate knowl-edge representations, we select the two, k1 andk2, with the greatest semantic difference (i.e., thelowest similarity) to facilitate faster convergenceduring training. Utilizing standard evaluation meth-ods for assessing these representations is subop-timal, as they fail to align with the preferencesof QA models. Inspired by the findings in previ-ous work(Wu et al., 2023b; Ko et al., 2024; Zhanget al., 2023), we posit that better knowledge rep-resentations generally lead to better performanceon QA. Consequently, we adopt k1 and k2 as con-textual knowledge, prompting the QA model Q toanswer the question q, generating answers a1 anda2, respectively. Subsequently, we prompt Chat-GPT to assess the quality of a1 and a2 from theperspectives of accuracy and relevance. This eval-uation aims to identify the preferred knowledgerepresentation k+ and the dispreferred knowledgerepresentation k. Details of the evaluation promptare provided in Appendix A.5. Data Augmentation based on ChatGPT. Chat-GPT is able to produce higher-quality knowledgerepresentations, compared with open-source LLMs.Therefore, in order to improve the quality ofpreferred knowledge representation and enhancethe diversity of the training data, we employChatGPT to paraphrase k+. In addition to thequestion q, the retrieved subgraph G, and thepreferred knowledge representation k+, we alsoprovide the answer entity e. This allows ChatGPTto organize relevant knowledge around e, ensuringthat the rewritten knowledge covers key evidence.We concatenate the question q and the textualizedsubgraph G using a prompt template as theinput x, and use the paraphrased knowledgerepresentation k++ and k as the preferred pair.Finally, we construct the preference dataset PN =(x1, k++1, k1 ), (x2, k++2, k2 ), ..., (xN, k++N , kN).The prompt for knowledge augmentation is inAppendix A.6.",
  "Datasets": "GrailQA (Gu et al., 2021) is a challenging, large-scale multi-hop KGQA benchmark that consists of64,331 questions (44,337 train, 6,763 dev, 13,231test). The training and dev sets provide annotatedSPARQL query and answer entities, while the testset comprises only the questions. For evaluationconvenience, the dev set is used for testing.GraphQuestions(Suetal.,2016)isacharacteristic-richdatasetforfactoidques-tion answering based on Freebase. It comprises5,166 questions (2,771 train, 2,395 test).Foreach question, the dataset provides correspondingSPARQL query and answer entities.",
  "Baselines": "We compare CoTKR (without PAQAF) andCoTKR+PAQAF (CoTKR+PA for shortness)with other knowledge rewriting methods in KGQA:Simple linear concatenation (Triple) (Baek et al.,2023; Sen et al., 2023) concatenates the subject,predicate, and object of a triple to generate triple-form text. This method does not require additionalmodels for knowledge rewriting.KG-to-Text (Wu et al., 2023b) transforms factsinto the free-form text for each relation path with a KG-to-Text model, addressing the limitations ofLLMs in understanding structured triple-form text.Summary (Ko et al., 2024) converts triples into aquestion-relevant summary, alleviating the issue ofredundant contextual knowledge.To ensure a fair comparison, we employ the samecorpus generation method for both the baselinesand our method. All baselines undergo supervisedfine-tuning without preference alignment.",
  "Retrieval Methods": "The retrieval module is not the focus of our re-search. Therefore, we adopt three commonly usedretrieval methods. For detailed implementation,please refer to Appendix A.4.2-Hop. We retain 30 triples from the 2-hop sub-graph of the head entity, prioritizing those withhigher semantic similarity to the question.BM25. We follow the processing method in De-cAF (Yu et al., 2023), simply linearizing the 1-hopsubgraph of the topic entity as the article. We takethe top 30 triples corresponding to the candidatedocuments as the retrieval results.Ground Truth Subgraph (GS). We modify theSPARQL queries from the datasets to obtain theground truth subgraphs. These subgraphs representthe results of an ideal retriever.",
  "Ntotal(10)": "where Nappear refers to the number of answer enti-ties contained in the models responses, and Ntotalrefers to the total number of the answer entities.Additionally, we consider utilizing Exact Match(EM) as an evaluation metric. Given that the re-sponses generated by LLMs consist of multipleparagraphs, while the corresponding answers areentities, we adjust the traditional EM metric. Ourmodified EM metric assesses whether all answerentities are included in the models responses. For adataset consisting of N questions, EM is calculatedas follows:",
  "Main Results": "To comprehensively evaluate various knowledgerewriting methods, we employ the widely used 2-Hop retrieval method. presents the overallresults. We observe that: (1) Our method out-performs the baselines across most evaluationmetrics and LLMs, confirming the effectivenessof our knowledge rewriting strategy. This alsodemonstrates the broad practical applicability ofCoTKR, effective for both open-source LLMs re-quiring fine-tuning and closed-source LLMs us-ing ICL. Integrating question-related knowledgesignificantly improves QA performance comparedwith direct question answering, underscoring theefficacy of the RAG paradigm in KGQA. KG-to-Text exhibits the weakest performance, indicatingthat mere conversion of triples into text may re-sult in loss of information inherent in the subgraph.Summary outperforms KG-to-Text but generallylags behind CoTKR/CoTKR+PA, suggesting thatfiltering out irrelevant knowledge is effective butnot adequate. (2) CoTKR+PA matches or evensurpasses the performance of ChatGPT as theknowledge rewriter, proving the effectivenessof our training framework and the preferencealignment. CoTKR+PA outperforms CoTKR, in-dicating that preference alignment can bridge thepreference gap between the knowledge rewriterand the QA model, thereby enhancing the qualityof knowledge representation. (3) A well-crafted knowledge representation is crucial for LLMused in KGQA. Although Triple does not requirean additional knowledge rewriting module, it pro-vides a strong baseline and, in some cases, out-performs KG-to-Text and Summary. Conversely,CoTKR+PA consistently surpasses Triple. Thisindicates that Triple is simple yet effective and ex-plains its widespread use in existing work. Onthe other hand, it demonstrates that a carefully de-signed knowledge representation can effectivelyenhance the performance of KGQA.",
  "Impact of Retrieval Methods": "To investigate the impact of retrieval methods, weselect Llama-3 as the knowledge rewriter and Chat-GPT as the QA model. According to the resultsshown in , we have the following observa-tions: (1) 2-Hop retrieval method may be insuf-ficient for more challenging questions, but it issuitable for simpler ones. Both BM25 and 2-Hopperform similarly on GrailQA, but 2-Hop shows asignificant advantage over BM25 on GraphQues-tions. This is likely because GrailQA is a morecomplex benchmark with a larger question-relatedsubgraph, making 2-hop subgraphs often inade-quate. Conversely, for GraphQuestions, a 2-hopsubgraph usually provides precise context for mostquestions. (2) The design of a high-quality re-triever remains an open problem. GS signifi-cantly outperforms BM25 and 2-Hop, indicatingthat retrieval noise substantially affects KGQA per-formance. (3) CoTKR consistently outperformsall baselines across various retrieval methods,demonstrating its robustness and practicality.",
  "CoTKR57.2851.1447.0952.8247.1343.55": ": The overall results of CoTKR and the baselines on GrailQA and GraphQuestions using 2-Hop as retrievalmethod. For each combination of a knowledge rewriter (KR) LLM and a QA model, the best and second-best resultsare highlighted in bold and underlined, respectively. as input (i.e., KG-to-Text, Summary, CoTKR,CoTKR+PA) with Triple. We use Accuracy as thecriterion to evaluate the correctness of responses.For each method, we consider three scenarios: (1)IncorrectCorrect: Triple provides a wrong an-swer, but the comparative method answers cor-rectly. (2) Correct->Incorrect: Triple answerscorrectly, but the comparative method answers in-correctly. (3) No change: both Triple and the com-parative method answer correctly or incorrectly.We adopt Llama-3 as the knowledge rewriter andChatGPT as the QA model, with 2-Hop as the re-trieval method. Then we calculate the proportionsof three distinct cases within GrailQA. From theresults shown in , we draw the followingconclusions: (1) KG-to-Text and Summary havea predominantly negative impact, partially val-idating the conclusions of prior studies. Tripleprovides a strong baseline, and the adoption ofKG-to-Text and Summary leads to more incor-rect answers. This indicates that LLMs can under-stand triple-form text effectively, and using sim-ple knowledge rewriting methods leads to loss ofinformation. (2) Well-designed knowledge rep-resentations substantially benefit the question-answering model. The knowledge representationsrewritten by CoTKR/CoTKR+PA generally en-hance the QA models performance. This reflects that the suboptimal knowledge representations inprevious work are key contributors to performancedegradation. Our method generates comprehensiveand semantically coherent knowledge representa-tions, thereby improving the efficacy of KGQA.",
  "KR MethodsKR Results": "SummaryThe International System of Units includes various measurement systems for different physical quantities such as length,area, volume, conductance, resistance, current, force, and pressure. The measurement system with an electric fieldstrength unit of Volt per meter does not directly relate to a specific area unit within the International System of Units.",
  "CoTKR": "Reason: I need to know the unit of area associatedwith the measurement system that has an electric fieldstrength unit of Volt per meter.Summarize: The International System of Units includesvarious units for different physical quantities such aslength, area, volume, etc.Reason: I need to find the specific unit of area in theInternational System of Units that corresponds to theelectric field strength unit of Volt per meter.Summarize: The International System of Units includesunits like Square kilometer, Square meter, etc. for mea-suring area.",
  "CoTKR+PA": "Reason: I need to know which measurement systemincludes the unit for electric field strength in Volt permeter.Knowledge: The International System of Units containsthe unit for electric field strength in Volt per meter.Reason: I need to identify the unit of area within theInternational System of Units.Knowledge: The International System of Units includesthe units Square kilometer and Square meter as areaunits.",
  "Case Study": "In this section, we compare Summary withCoTKR through an example. (Please refer to Ap-pendix B for the full comparison result.) As il-lustrated in , Summary struggles to ex-tract useful information when faced with an abun-dance of triples. In contrast, CoTKR, leveragingCoT reasoning, effectively emphasizes the key ev-idence (i.e., Square meter) in the second rewrit-ing step. Furthermore, after preference alignment,CoTKR+PA is capable of generating more naturalreasoning steps, significantly enhancing its applica-bility to KGQA.",
  "Conclusion": "In this paper, we propose Chain-of-Thought En-hanced Knowledge Rewriting, CoTKR, for higherquality knowledge representation of triples in KGaugmented QA. To bridge the preference gap be-tween the knowledge rewriter and the QA model,we propose Preference Alignment from QuestionAnswering Feedback, PAQAF. Experimental re-sults demonstrate that, compared with existingknowledge rewriting methods, CoTKR can gen-erate the most beneficial knowledge representationfor QA models. In future work, we will go beyond",
  "Limitations": "We acknowledge the limitations of this work. (1)This study is limited to KGQA and does not explorebroader application scenarios. Therefore, we didnot design experiments to explore whether CoTKRis effective for all or most RAG scenarios. In futurework, we aim to expand the range of data sources.We intend to design a knowledge rewriting methodthat can be applied to not only KGs but also tables,textual data, and other formats. This enhancementwill allow the QA framework to access knowledgefrom a wider range of sources, thus improving itspracticality. Furthermore, we plan to investigatea knowledge representation beneficial for variousdownstream tasks, such as fact verification anddialogue generation. (2) The training frameworkfor CoTKR depends on the powerful capabilitiesof closed-source LLMs. However, these modelshave inherent limitations, and the training data theygenerate contains noise, which constrains the per-formance ceiling of CoTKR.",
  "Ethical Considerations": "We explore optimizing knowledge representationsfor KGQA on public benchmarks, avoiding anypotential harm to any individuals or groups. Topromote transparency and facilitate replication ofour research, we provide the technical details nec-essary for reproducing our results and release boththe source code and the collected data. Our codeand data are available for academic research, com-mercial use, and other applications. It is important to acknowledge the potential risksand ethical considerations associated with LLMs.In this study, we construct the training data usingChatGPT and implement our knowledge rewritersbased on LLMs. Due to the inherent limitationsof LLMs, including factual inaccuracies, racialdiscrimination, and gender bias, our knowledgerewriters might generate incorrect content or inad-vertently reflect prevalent societal biases. This work is partially supported by NationalNature Science Foundation of China under No.U21A20488, by the project \"Key Laboratory ofrich-media Digital Publishing Content Organiza-tion and Knowledge Service Open Fund-Researchon Knowledge-enhanced Training Techinques ofLarge Language Model\" No. ZD2024-04/01, andby the EPSRC project OntoEm (EP/Y017706/1).We thank the Big Data Computing Center of South-east University for providing the facility support onthe numerical calculations in this paper. ChatGPTwas used to enhance the readability of some of thetext and improve the language of this paper, afterthe content was first added manually. All materialwas then checked manually before submission.",
  "Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.2007. Freebase: A shared database of structuredgeneral human knowledge. In AAAI, pages 19621963. AAAI Press": "Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,Tim Sturge, and Jamie Taylor. 2008. Freebase: acollaboratively created graph database for structuringhuman knowledge. In SIGMOD Conference, pages12471250. ACM. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. InNeurIPS.",
  "Wentao Ding, Jinmao Li, Liangchuan Luo, and YuzhongQu. 2024. Enhancing complex question answeringover knowledge graphs through evidence pattern re-trieval. In WWW, pages 21062115. ACM": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,Meng Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: Asurvey. CoRR, abs/2312.10997. Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler,Percy Liang, Xifeng Yan, and Yu Su. 2021. BeyondI.I.D.: three levels of generalization for question an-swering on knowledge bases. In WWW, pages 34773488. ACM / IW3C2. Jie He, Simon Chi Lok U, Vctor Gutirrez-Basulto, andJeff Z. Pan. 2023. BUCA: A Binary ClassificationApproach to Unsupervised Commonsense QuestionAnswering. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(ACL 2023).",
  "of pre-trained language models in simple knowledgegraph question answering. World Wide Web (WWW),26(5):28552886": "Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis,Nikos Papasarantopoulos, and Jeff Z. Pan. 2023. Re-trieval augmented generation with rich answer en-coding. In Proceedings of the 13th InternationalJoint Conference on Natural Language Processingand the 3rd Conference of the Asia-Pacific Chapter ofthe Association for Computational Linguistics, pages10121025. Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang,Hantao Lou, Kaile Wang, Yawen Duan, Zhong-hao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng,Kwan Yee Ng, Juntao Dai, Xuehai Pan, AidanOGara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu,Stephen McAleer, Yaodong Yang, Yizhou Wang,Song-Chun Zhu, Yike Guo, and Wen Gao. 2023a.AI alignment: A comprehensive survey.CoRR,abs/2310.19852. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, AndreaMadotto, and Pascale Fung. 2023b. Survey of halluci-nation in natural language generation. ACM Comput.Surv., 55(12):248:1248:38. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023a. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Linhan Li, Huaping Zhang, Chunjin Li, Haowen You,and Wenyao Cui. 2023. Evaluation on chatgpt for chi-nese language understanding. Data Intell., 5(4):885903": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo FrassettoNogueira. 2021. Pyserini: A python toolkit for repro-ducible information retrieval research with sparse anddense representations. In SIGIR, pages 23562362.ACM. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023. G-eval:NLG evaluation using gpt-4 with better human align-ment. In EMNLP, pages 25112522. Association forComputational Linguistics.",
  "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,and Nan Duan. 2023a. Query rewriting in retrieval-augmented large language models. In EMNLP, pages53035315. Association for Computational Linguis-tics": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,and Nan Duan. 2023b. Query rewriting in retrieval-augmented large language models. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 53035315, Singa-pore. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of factualprecision in long form text generation. In EMNLP,pages 1207612100. Association for ComputationalLinguistics. Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo,Sneha Singhania, Jiaoyan Chen, Stefan Dietze, HajiraJabeen, Janna Omeliyanenko, Wen Zhang, MatteoLissandrini, ussa Biswas, Gerard de Melo, AngelaBonifati, Edlira Vakaj, Mauro Dragoni, and amienGraux. 2023. Large language models and knowledgegraphs: Opportunities and challenges. Transactionson Graph Data and Knowledge.",
  "J.Z. Pan, G. Vetere, J.M. Gomez-Perez, and H. Wu, edi-tors. 2017b. Exploiting Linked Data and KnowledgeGraphs for Large Organisations. Springer": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In NeurIPS. Partha Pratim Ray. 2023. Chatgpt: A comprehensivereview on background, applications, key challenges,bias, ethics, limitations and future scope. internet ofthings and cyber-physical systems, 3, 121154. URL org/10.1016/j. iotcps, 3.",
  "Stephen E. Robertson and Hugo Zaragoza. 2009. Theprobabilistic relevance framework: BM25 and be-yond. Found. Trends Inf. Retr., 3(4):333389": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H.Bach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,M Saiful Bari, Canwen Xu, Urmish Thakker,Shanya Sharma Sharma, Eliza Szczechla, TaewoonKim, Gunjan Chhablani, Nihal V. Nayak, DebajyotiDatta, Jonathan Chang, Mike Tian-Jian Jiang, HanWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-ishala Neeraj, Jos Rozen, Abheesht Sharma, An-drea Santilli, Thibault Fvry, Jason Alan Fries, RyanTeehan, Teven Le Scao, Stella Biderman, Leo Gao,Thomas Wolf, and Alexander M. Rush. 2022. Multi-task prompted training enables zero-shot task gener-alization. In ICLR. OpenReview.net. Apoorv Saxena, Aditay Tripathi, and Partha P. Taluk-dar. 2020. Improving multi-hop question answeringover knowledge graphs using knowledge base em-beddings. In ACL, pages 44984507. Association forComputational Linguistics.",
  "Priyanka Sen, Sandeep Mavadia, and Amir Saffari. 2023": "Knowledge graph-augmented language models forcomplex question answering.In Proceedings ofthe 1st Workshop on Natural Language Reasoningand Structured Explanations (NLRSE), pages 18,Toronto, Canada. Association for Computational Lin-guistics. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan.2023. Evaluation metrics in the era of GPT-4: reli-ably evaluating large language models on sequenceto sequence tasks. In EMNLP, pages 87768788.Association for Computational Linguistics. Yu Su, Huan Sun, Brian M. Sadler, Mudhakar Srivatsa,Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016.On generating characteristic-rich question sets forQA evaluation. In EMNLP, pages 562572. TheAssociation for Computational Linguistics.",
  "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. 2023. Evaluation ofchatgpt as a question answering system for answeringcomplex questions. CoRR, abs/2303.07992": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,and Ashish Sabharwal. 2023. Interleaving retrievalwith chain-of-thought reasoning for knowledge-intensive multi-step questions. In ACL (1), pages1001410037. Association for Computational Lin-guistics. Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang,Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2024.keqing: knowledge-based question answering is anature chain-of-thought mentor of LLM.CoRR,abs/2401.00426. Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li,Yunsen Xian, Chuantao Yin, Wenge Rong, and ZhangXiong. 2023a. Knowledge-driven cot: Exploringfaithful reasoning in llms for knowledge-intensivequestion answering. CoRR, abs/2308.13259.",
  "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi,Xingshan Zeng, Wenyong Huang, Lifeng Shang,Xin Jiang, and Qun Liu. 2023b.Aligning largelanguage models with human: A survey.CoRR,abs/2307.12966": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M. Dai, and Quoc V. Le. 2022a. Finetunedlanguage models are zero-shot learners. In ICLR.OpenReview.net. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022b. Chain-of-thought prompt-ing elicits reasoning in large language models. InNeurIPS.",
  "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.React: Synergizing reasoning and acting in languagemodels. In ICLR. OpenReview.net": "Donghan Yu, Sheng Zhang, Patrick Ng, HenghuiZhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu,William Yang Wang, Zhiguo Wang, and Bing Xiang.2023. Decaf: Joint decoding of answers and logicalforms for question answering over knowledge bases.In ICLR. OpenReview.net. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, ChristopherDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-ter, Daniel Simig, Punit Singh Koura, Anjali Srid-har, Tianlu Wang, and Luke Zettlemoyer. 2022.OPT: open pre-trained transformer language mod-els. CoRR, abs/2205.01068. Yichi Zhang, Zhuo Chen, Yin Fang, Lei Cheng, YanxiLu, Fangming Li, Wen Zhang, and Huajun Chen.2023.Knowledgeable preference alignment forllms in domain-specific question answering. CoRR,abs/2311.06503.",
  "A.1Data Construction Details": "To construct the supervised fine-tuning dataset, weset the temperature to 0 and adopt GPT-3.5 Turbo.We concatenate question q and its related subgraphG using a prompt template to form the input x.ChatGPT generates candidate knowledge represen-tation k based on 3 examples as demonstrationsand input x under ICL paradigm. Given that, theobjective of k is to augment the performance ofthe QA model, we evaluate the quality of k by ex-amining the QA models results. We utilize k ascontextual knowledge for the QA model to gen-erate the answer a. If the answer a encompassesall the answer entities, knowledge k is consideredhelpful for answering the question, and (x, k) isused as an input-output pair for supervised training.For the construction of the preference dataset, wesample knowledge representations from the knowl-edge rewriter after supervised fine-tuning. We setthe temperature to 1 to foster greater diversity. Dur-ing preference annotation, given that the knowledgerewriter aims to generate contextual knowledgebeneficial for QA, we first assess the quality ofknowledge representation based on the number ofanswer entities they contain. This nave strategy isrobust and cost-effective, avoiding additional APIcalls for evaluations using ChatGPT, thus savingtime and reducing costs. We label the candidatewith the highest number of answer entities as thepreferred knowledge representation k+ and the onewith the fewest as the dispreferred k. If the num-ber of answer entities is the same, we select the two,k1 and k2, with the greatest semantic difference byusing all-MiniLM-L6-v24 as the encoder. This se-lection process ensures a significant semantic gapbetween the two chosen representations, facilitat-ing more rapid model convergence during training.Subsequently, k1 and k2 serve as contextual knowl-edge to prompt the QA model, yielding answersa1 and a2. We annotate the preferred knowledgerepresentation k+ and the dispreferred knowledgerepresentation k by evaluating the quality of a1and a2 using ChatGPT. Finally, ChatGPT is used",
  "A.2Datasets": "GrailQA5 (Gu et al., 2021) is a challenging, large-scale multi-hop KGQA benchmark. It is an Englishdataset that utilizes Freebase (Bollacker et al., 2007,2008) as KG. It spans 86 domains, such as Sports,Location, and Computer Video Games, and com-prises 64,331 questions (44,337 train, 6,763 dev,13,231 test). This dataset features a large number ofentities and relations, complex logical forms, andnoise in entity mentions within the questions. Thetraining and dev sets provide annotated SPARQLqueries and answer entities, while the test set com-prises only the questions. For evaluation conve-nience, the dev set is used for testing.GraphQuestions6(Suetal.,2016)isacharacteristic-richdatasetforfactoidques-tion answering based on Freebase across 70domains, like People, Astronomy, and Medicine.This English dataset focuses on the followingquestion characteristics:structure complexity,function, commonness, paraphrasing, and answercardinality. It comprises 5,166 questions (2,771train, 2,395 test), with nearly half requiringmulti-hop reasoning.For each question, thedataset provides corresponding SPARQL queryand answer entities.",
  "A.3Large Language Models": "Llama-27 (Touvron et al., 2023b), an updated ver-sion of Llama-1 (Touvron et al., 2023a), is devel-oped using a training corpus comprising 2 trilliontokens and features a context length twice thatof Llama-1. To better accomplish the knowledgerewriting task, we select Llama-2-7B-Chat8.Llama-39 (AI@Meta, 2024) is the latest model inthe Llama series. It is renowned for its masteryof language nuances, contextual comprehension, 5This dataset is distributed under the CC BY-SA 4.0 licenseand our utilization complies with the terms specified in thelicense.6This dataset is licensed under the Creative CommonsAttribution 4.0 and our usage aligns with the intended purposesoutlined in this license.7ThelicenseofLlama-2isavailableat",
  "Mistral11 (Jiang et al., 2023a) is an open-sourceLLM developed by Mistral AI. We select the lat-est instruction-tuned version, Mistral-7B-Instruct-v0.312, as our QA model": "ChatGPT13, developed by OpenAI, is a milestonein the era of LLMs. Its robust capabilities in naturallanguage understanding and generation facilitatesuperior performance across various tasks. Weleverage GPT-3.5-turbo via the API14 for knowl-edge rewriting and question answering. All the LLMs above are general-domain models.Regarding language support, Llama-2, Llama-3,and Mistral only support English, while ChatGPTis multilingual. In this study, the use of these LLMscomplies with their respective licenses or terms.",
  "A.4Retrieval Methods Details": "2-Hop subgraph is a nave question-related context.Most KBQA studies under RAG paradigms con-sider triples within the N-hop subgraph of the headentity as contextual knowledge (Baek et al., 2023;Sen et al., 2023; Wang et al., 2024; Ko et al., 2024).To retrieve the 2-hop subgraph around the headentity, we execute SPARQL queries on Freebase.Given the large size of the 2-hop subgraph, weuse all-MiniLM-L6-v215 to encode all 1-hop and2-hop relations of the head entity and the question,excluding meaningless relations, such as com-mon.topic.webpage. Then, we select semanticallysimilar relation paths based on cosine similarity.Finally, we sample the corresponding triples fromKG based on these relation paths. In this experi-ment, we select the top 30 triples as our retrievalresults. However, the small size of the 2-hop sub-graphs for some entities may result in fewer than30 triples being retrieved.BM25 (Robertson and Zaragoza, 2009) is a re-trieval method based on TF-IDF scores of sparse",
  "Preference Annotation Prompt": "[Instruction]Your task is to evaluate the quality of two responsesto the question based on predefined criteria. Avoidany position biases and ensure that the order in whichthe responses were presented does not influence yourdecision. Do not allow the length of the responsesto influence your evaluation.Be as objective aspossible.[Criteria]For this evaluation, you should primarily considerthe following criteria:Accuracy: The response should contain as manyanswer entities as possible, and use the originalwords of the answer entities.Relevance: The response should be to the point ofthe question.Question: {question}Answer: {answer}Response A: {response A}Response B: {response B}[Evaluation Rule]Begin your evaluation by comparing the two re-sponses and provide a short explanation. Then outputonly the single character: \"A\" if Response A is better,\"B\" if Response B is better, and \"C\" for a tie. At theend, repeat just the letter again by itself on a new line.",
  ": Preference Annotation Prompt": "word matching between input questions and pas-sages. This method is commonly used for inte-grating multimodal data sources or text-based QA(Wang et al., 2023a; Yu et al., 2023). We followthe processing method in DecAF (Yu et al., 2023),simply linearizing the 1-hop subgraph of the topicentity as the article. We use the BM25 implementedby Pyserini (Lin et al., 2021) and collect the triplescorresponding to the candidate articles as the re-trieval result. Specifically, we initially retrieve 10candidate articles, each containing up to 10 triples.Subsequently, we remove any redundant triples orthose containing meaningless relations. Given thelimited context length of LLMs, we select the top30 triples as the context information for questionanswering. After filtering, the number of triplesin the candidate articles may be less than 30, thusresulting in the retrieval subgraphs for some ques-tions containing fewer than 30 triples.Ground Truth Subgraph (GS) refers to a sub-graph consisting of the triples necessary for an-swering a question. In this experiment, we modifythe SPARQL queries provided in the datasets andexecute them on Freebase to obtain the ground truthsubgraphs. We use this subgraph to represent theresults of an ideal retriever, aiming to explore the",
  "A.5Implementation Details": "We utilize LoRA (Hu et al., 2022) to achieveparameter-efficient fine-tuning. For supervised fine-tuning and DPO, the batch size, learning rate, lorarank, lora alpha, and lora dropout are set to 128,1e-4, 64, 128, and 0.05, respectively. In supervisedfine-tuning, we train for 10 epochs and save thebest model based on validation set results. In DPO,we observe that more training steps may lead todecreased model performance. Consequently, wetrain for 1 to 2 epochs on GraphQuestions usingapproximately 2,000 training samples, and for 5to 20 steps on GrailQA using 640 to 2560 trainingsamples. During inference, the temperature is setto 0 for ChatGPT and 0.01 for open-source LLMs,ensuring relatively stable output. All the parametersettings mentioned above are the optimal resultsafter multiple trials.Our implementation utilizes PyTorch16, Trans-formers17, DeepSpeed18, Datasets19, PEFT20, andTRL21. We use Datasets for data preprocessing.Both training and inference are based on PyTorchand Transformers. Supervised fine-tuning and DPOare implemented using PEFT, TRL, and Deep-Speed. Experiments are conducted on 4 NVIDIAA100-SXM4-40GB GPUs, with each training orinference session completed within one day. Due to",
  "BCase Study": "In this section, we compare different knowledgerewriting strategies through an example. As illus-trated in , the knowledge generated by bothTriple and KG-to-Text contains excessive redun-dant information. This redundancy complicates theprocess for the QA model, making it challengingto extract relevant knowledge. Summary strug-gles to extract useful information when faced withan abundance of triples. In contrast, CoTKR and CoTKR+PA summarize the most pertinent knowl-edge in the rewriting step, thereby enabling theQA model to provide a concise and accurate an-swer. Furthermore, after preference alignment, ourknowledge rewriter is capable of generating morenatural reasoning steps, significantly enhancing itsapplicability to KGQA.",
  "C.2Knowledge Rewriter with GPT-4": "To assess the applicability of CoTKR to GPT-4,we further conduct the experiments with GPT-4as the knowledge rewriter, Mistral as the question-answering model, and 2 hop as the retrieval methodon 1,000 test questions from GrailQA. The detailedresults are presented in . The findingsshow that CoTKR outperforms other approaches,with CoTKR utilizing GPT-4 achieving the highestperformance. This suggests that employing a moreadvanced LLM backbone, such as GPT-4, leads tosuperior outcomes.",
  "C.3Time Analysis": "We conduct experiments to analyze the averagetime cost of the knowledge rewriting methodsdiscussed in this paper.We adopt Llama-3 asthe knowledge rewriter, Mistral as the question-answering model, and 2-Hop as the retrievalmethod.The experiments are conducted onGraphQuestions, utilizing one A100-SXM4-40GBGPU. The average runtime for each question bydifferent methods is shown in (unit: sec-onds). The average runtime of each question forall methods is within an acceptable range (i.e., lessthan 1.5 seconds). Although our method is themost time-consuming, it exhibits a clear advantagein performance.",
  ": Time analysis on GraphQuestions (seconds)using Llama-3 as the knowledge rewriter, Mistral forquestion answering, and 2-Hop for retrieval": "models, many existing works employ ChatGPTas an evaluator to provide high-quality evaluationresults (Sottana et al., 2023; Liu et al., 2023; Minet al., 2023). In our approach, we use GPT-4 as theevaluator to assess whether all answer entities arepresent in the responses. We refer to this evaluationmetric as GPT-4-score. Compared to EM, thismetric is more flexible, as LLMs are capable ofrecognizing synonyms of answer entities. We useit to evaluate the first 300 questions from GrailQAusing Llama-3 as knowledge rewriter, ChatGPT asquestion-answering model, and 2 hop as retrievalmethod. We provide the prompt for GPT-4-scorein .",
  "GPT-4-score Prompt": "[Instruction]Your task is to evaluate the quality of the response tothe question. You should consider whether all theanswer entities appear in the response.Question: {question}Answer: {answer}Response: {response}Begin your evaluation by comparing the responseand the answer and provide a short explanation.Then output only the single number: \"1\" if all theanswer entities appear in the response, and \"0\" if not.At the end, repeat just the number again by itself ona new line.",
  "C.6Detailed Experimental Results": "This section presents all the experimental resultsof this study. As shown in , ,and , CoTKR/CoTKR+PA achieves thebest performance in most scenarios. This indi-cates that CoTKR is effective for both open-sourceLLMs after training and closed-source LLMs usingICL. Besides, the results also reveal the robustnessof CoTKR, demonstrating its applicability acrossKGQA systems with various retrieval methods andQA models.",
  "Avoiding RedundantReasoning Step": "Katy brner is the curator forwhat exhibition?Reason: I need to know which exhibi-tion Katy Brner curated.Summarize: Katy Brner curated the ex-hibition \"Places & Spaces\".Reason: I need to know the subjects ofthe exhibition \"Places & Spaces\" curatedby Katy Brner.Summarize: The subjects of the exhibi-tion \"Places & Spaces\" curated by KatyBrner include Cartography, Informa-tion visualization, Information design,and Scientometrics.",
  "Key PointsSupplementation": "Person of interest deals withhow many tv subjects?Reason: I need to know how many TVsubjects are associated with \"Person ofInterest\".Summarize: \"Person of Interest\" is asso-ciated with multiple TV subjects such asEspionage, Mass surveillance, Hacker,and Vigilante. Reason: I need to know how many TVsubjects are associated with \"Person ofInterest\".Summarize: \"Person of Interest\" dealswith a total of 5 TV subjects includingEspionage, Mass surveillance, Hacker,Surveillance, and Vigilante."
}