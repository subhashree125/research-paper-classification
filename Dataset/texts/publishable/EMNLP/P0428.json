{
  "Abstract": "We investigate non-collaborative dialogueagents, which are expected to engage in strate-gic conversations with diverse users, for secur-ing a mutual agreement that leans favorably to-wards the systems objectives. This poses twomain challenges for existing dialogue agents:1) The inability to integrate user-specific char-acteristics into the strategic planning, and 2)The difficulty of training strategic planners thatcan be generalized to diverse users. To ad-dress these challenges, we propose TRIP toenhance the capability in tailored strategic plan-ning, incorporating a user-aware strategic plan-ning module and a population-based trainingparadigm. Through experiments on benchmarknon-collaborative dialogue tasks, we demon-strate the effectiveness of TRIP in catering todiverse users.",
  "Introduction": "Non-collaborative dialogues, such as negotiation(He et al., 2018) and persuasion (Wang et al., 2019),occur when the agent and user hold conflicting in-terests (Deng et al., 2023a,b; Lei et al., 2022). Typi-cally, both parties need to employ various strategiesto achieve an agreement favorable to themselves(Keizer et al., 2017; Zhan et al., 2024). As user re-sistance varies depending on the agents strategies(Shi et al., 2019; Dutt et al., 2021), it is impera-tive for the agent to perform strategic planningtailored to diverse users. Relying on a one-size-fits-all strategy can leave the agent vulnerable toothers taking advantage due to its lack of adaptabil-ity and flexibility (Yang et al., 2021; Deng et al.,2024; Xu et al., 2023).Recent efforts have resorted to large languagemodels (LLMs) as dialogue agents to perform non-collaborative tasks (Deng et al., 2023d; Fu et al.,",
  "*Corresponding author": "2023; Zhang et al., 2023a). They aim to guidethe response of LLMs through mixed-initiativeprompts (Chen et al., 2023; Deng et al., 2023d;Zhang et al., 2023a) or incorporating an exter-nal strategy planner (Yu et al., 2023; Deng et al.,2023e). However, these initiatives has been criti-cized regarding its performance in real-world sce-narios (Deng et al., 2023e; Kwon et al., 2024),where users have various non-collaborative strate-gies. We attribute this outcome to the neglect of twocrucial aspects: 1) Existing methods fail to incor-porate explicit user-specific characteristics intotheir strategic planning, instead relying solely onthe conversational history. Importantly, by creat-ing informative representations of individual users,agents can adapt their behaviors and devise tailoredstrategies (Jang et al., 2020; Yang et al., 2021). 2)Their training paradigm fails to generate strate-gic planners that generalize well to diverse users.Their paradigms are oversimplified, relying on asingle user simulator for interactive training. Thissimulator is restricted in generating varied non-collaborative behaviors, often exhibiting a focus onprioritizing user contentment (Zhang et al., 2023c;Durmus et al., 2023; Bianchi et al., 2024). Essen-tially, agents trained in this manner are accustomedto engage with a single user exclusively, leadingto rigidity and obstinacy when encountering newusers with different interaction behaviors (Wanget al., 2023; Safdari et al., 2023). To provide more evidence for the above anal-ysis, we establish an evaluation protocol, whichsituates diverse user simulators with varying non-collaborative behaviors. We investigate the limi-tations of current LLM-based dialogue agents onstrategic planning (cf. for details). Theevaluation results clearly demonstrate that exist-ing agents struggle to tailor their strategies for di-verse users, leading to sub-optimal performances. This limitation compromises the practical utility ofthese agents, both in functioning as a successfulagent in conversational AI and in providing socialskills training in pedagogy. The key challengeslie in making dialogue agents aware of diversenon-collaborative user behaviors and devisingtailored strategies for individual users.To tackle these challenges, we design a sim-ple yet effective method, called TRIP, to im-prove LLMs capability in Tailored stRategIcPlanning. TRIP includes a user-aware strategicplanning module and a population-based train-ing paradigm. Specifically, the strategic planningmodule incorporates user-specific characteristicsinto strategic planning using the Theory-of-Mind(ToM) (Premack and Woodruff, 1978; Wimmerand Perner, 1983). This involves analyzing usersmental states and future possible actions during in-teractions to understand their interests (Yang et al.,2021; Chawla et al., 2023a). Moreover, instead ofrelying on a solitary user simulator, our population-based training paradigm promotes the adaptationof the strategic planning module to various users,achieved by training it with more diverse user sim-ulators. Each simulator is equipped with extensivesets of non-collaborative strategies and role-playingpersonas (Chen et al., 2024). As such, TRIP essen-tially manipulates the experience of the dialogueagent, enabling it to recognize the importance oftailoring strategies for individual users. Our keycontributions are concluded below:",
  "Our research is closely tied to the strategic plan-ning and training paradigms to address the non-": "collaborative tasks in the era of LLMs. We providea literature review and highlight our differences.Strategic planning for non-collaborative dia-logues. Recent researches have introduced vari-ous methods based on LLMs to enhance their ef-fectiveness in strategic planning. These methodscan be categorized into two types: 1) Developingstimulus prompts to unleash the potential of LLMs.(Chen et al., 2023) validate the effectiveness of us-ing mixed-initiative prompts to tackle proactive di-alogue challenges. (Deng et al., 2023d) and (Zhanget al., 2023a) encourage LLMs to engage in self-reflection to plan their next actions. (Fu et al., 2023)employ self-play simulations to iteratively refinestrategic planning by soliciting feedback from otherLLMs. Nonetheless, as highlighted by (Deng et al.,2023e), the effectiveness of these approaches isimpeded by non-trainable parameters. 2) Equip-ping LLMs with an external strategy planner. Theplanner is capable of generating prompts at eachturn, providing nuanced, instance-specific guidanceand control over LLMs. This could be integratedusing methods like Monte Carlo Tree Search (Yuet al., 2023) or a plug-in model (Deng et al., 2023e),which can be fine-tuned for improving the strategicplanning capability without affecting the function-alities of LLM-powered dialogue agents. However,these methods still struggle to achieve promising re-sults due to their inability to integrate user-specificcharacteristics into their strategic planning. Com-plementary to (Deng et al., 2023e), our work inves-tigates the importance of tailored strategic planningby modeling user-related characteristics explicitly.Training paradigms for non-collaborative dia-logues. Current training paradigms involve thedialogue agent interacting with a single user sim-ulator to enhance its strategic planning capabil-ities.In specific, (Chawla et al., 2023b) builda user simulator that mimics human-human di-alogue data in a supervised manner, while (Yuet al., 2023; Deng et al., 2023e) resort to a role-playing LLM-based user simulator. However, asingle user simulator can only represent the behav-iors of one or a type of users, potentially leadingto the under-representation of other users behav-iors, as evidenced by (Liu et al., 2023; Shi et al.,2019). Therefore, existing training paradigms failto produce strategic planners that cater to diverseusers with varying behaviors. In this paper, ourwork investigates the importance of tailored strate-gic planning by diversifying the users behaviorsusing population-based training.",
  "Evaluation Setup": "Evaluation Overview. The environment encom-passes various synthetic user simulators showcas-ing diverse non-collaborative behaviors. In the eval-uation process, each dialogue agent must interactwith these simulators (Deng et al., 2023e). Dur-ing their interactions, the dialogue agent and usersimulator alternate in employing strategies in theirresponses with the ultimate aim of maximizingtheir own self-interest. The interactions continuesuntil the conversational goal is achieved or the max-imum number of turns is reached. We gather theseinteractions and assess the agents performances.Baselines. We consider two representative base-lines: Standard agent (i.e., vanilla LLM withoutany modification) and PPDPP agent (Deng et al.,2023e), which is current SOTA agent with a train-able external strategy planner1.Diverse User Simulators. Our simulators are syn-thesized with non-collaborative behaviors, guidedby their task-relevant personas. As evidenced byprevious study (Deng et al., 2023c; Bianchi et al.,2024; Huang et al., 2024), LLMs are limited todemonstrate non-collaborative behaviors. To this",
  "Notably, we also consider other existing dialogue agentsin our main experiments": "end, we prompt non-collaborative behaviors explic-itly into LLMs using the resisting strategies thatare designed to foil persuasion attempts (Fransenet al., 2015; Tian et al., 2020; Dutt et al., 2021).Initially, we equip LLMs with different personas(Jiang et al., 2023; Zhou et al., 2023b; Zhang et al.,2023b), which are used to select non-collaborativebehaviors from the set of resisting strategies. Fol-lowing (Wang et al., 2019; Jiang et al., 2024),we consider two types of personas, including Big-Five Personality2 (Goldberg, 1992) and Decision-Making Styles3 (Scott and Bruce, 1995), togetherwith LLM-generated cohesive description for eachfine-grained persona. Additionally, we employ re-sisting strategies outlined by (Dutt et al., 2021)to direct the behavior of simulators. Finally, ourmixed-initiative role-play prompt for each agent in-cludes the assigned persona, a set of resisting strate-gies, and conversation context. These elements aidin guiding user simulators to exhibit diverse non-collaborative behaviors. In total, we develop 300diverse user simulators for each evaluation task,representing 20 persona categories (i.e., Big-FivePersonality Decision-Making Styles).Evaluation Tasks. In line with (Deng et al., 2023d;Wang et al., 2019), we conduct experiments on twobenchmark non-collaborative tasks: the price nego-tiation task, utilizing the test4 dataset of Craigslist-Bargain (CB) (He et al., 2018) and the charity per-suasion task, employing the test dataset of Persua-sionForGood (P4G) (Wang et al., 2019). Notably,the dialogue agents play the role of buyer and per-suader, respectively, to accomplish their goals.Evaluation Metrics.Following (Deng et al., 2023e), we consider three commonly used met-rics: Success Rate (SR), Average Turn (AT) andSale-to-List Ratio (SL%). The SR measures effec-tiveness by the percentage of goal achievementwithin a maximum number of turns, while ATmeasures efficiency by the average number ofturns required to achieve the goal.As for theCB task, we additionally adopt the SL% (Zhouet al., 2019) to determine the effectiveness of goalcompletion. Formally, the SL% is expressed as(Pdeal P sellertarget)/(P buyertarget P sellertarget), where Pdealis the final deal price, P buyertarget and P sellertarget are thetarget prices of both parties. A higher SL% repre-",
  "Overall Performance0.580.146.721.010.310.090.320.239.200.76": ": The performance of the PPDPP dialogue agent testing across various personas of user simulators. Red(Blue) indicates the increased (decreased) performance compared to Standard dialogue agent. The symbol indicates that this performance exhibits minimal variation, specifically within a 5% range of the maximum value.The effectiveness of PPDPP varies significantly across different user personas.",
  "Experimental Findings": "We analyze the performances of existing dialogueagents across user simulators with various non-collaborative behaviors. Specifically, we assessthe advancements of PPDPP compared to the Stan-dard agent. As illustrated in , while PPDPPshows a notable improvement in overall perfor-mance, it does not adapt well to users employingdifferent non-collaborative strategies. Its effective-ness varies significantly among users with differ-ent personas, with its advantage over the Standardnot being significant in 17.77% of cases (e.g., itincreases SR by 0.02 for Analytical in price ne-gotiation.), and even performing worse than theStandard in 8.88% of cases (e.g., it decreases SRby 0.02 for Neuroticism in price negotiation). Thismotivates the need for a dialogue agent to performstrategic planning tailored to diverse users5.",
  "TRIP: Tailored Strategic Planning": "To enhance LLMs tailored strategic planning, wepropose an effective method TRIP, which developsan external planner by modeling user characteris-tics and training with diverse user simulators. Asillustrated in , our TRIP includes a user-aware strategic planning module and a population-based training paradigm. The former aims to explic-itly model user characteristics (e.g., mental statesand future actions), while the latter incorporatesdiverse user simulators for training simultaneously.",
  "User-Aware Strategic Planning": "TRIP aims to explicitly infer user characteristicsand then incorporate them into the strategic plan-ning module, parameterized by a trainable BERT.In particular, building upon the advanced Theory-of-Mind capability of LLMs (Sap et al., 2022;Moghaddam and Honey, 2023), TRIP capturesusers mental states and future possible actionsduring interactions to understand their interests andpredicts how TRIPs responses may influence them.In this case, mental states pertains to what they aimto accomplish, such as the target price or whetherthey will donate, while future actions relates towhat the user is likely to discuss next (Hu et al.,2023; Zhou et al., 2023a). Formally, given thedialogue history D = (usys1 , uusr1, ..., usyst, uusrt),where usysiand uusridenote the i-th utterancesof both parties and t is the number of utter-ances, we feed the dialogue history D into theLLM and prompt it to infer mental states Mand future actions F in an open-ended manner,i.e., PLLM(M, F|D). Subsequently, we feed the{M, F, D} into the strategy planner to predictthe next strategy. The output space of is a setof strategies6 pre-defined by (Deng et al., 2023e;Wang et al., 2019), each of them is attached with apre-defined natural language instructions.",
  "6e.g., the elicitation of specific emotions to influence other": ": TRIP Overview. This method includes a user-aware strategic planning module (UASP) and a population-based training paradigm (PBTP). The UASP incorporates user-specific characteristics into strategic planning usingthe Theory-of-Mind (ToM). The PBTP diversifies training user simulators to promote agents adaptation. We usenumbers to indicate the overall process of TRIP. propose a population-based reinforcement learning(RL) training paradigm, which aims to enhancethe adaptability of a dialogue agent to new usergroups by training with larger and more diversepopulations (Charakorn et al., 2020). We offer acomprehensive explanation of this approach below.Population Setup. Similar to .1, we build40 diverse user simulators, each embodying a spe-cific persona description. We ensure an balancedrepresentation of each persona category within ouruser simulators for population-based RL training.We donate these simulators as K = k1, k2, ...k40During each iteration, we sample among K usinga distribution p, allowing the dialogue agent S tointeract with it. The distribution p is initializedbased on the frequency of various personas.Reward Design. Following (Deng et al., 2023e),we prompt LLMs to judge the conversationprogress at each turn and transform it into scalarrewards. Specifically, in the negotiation task, weemploy a separate GPT3.5 (OpenAI, 2022) to as-sess whether both parties have reached a deal. Inthe persuasion task, we ask the GPT3.5-based usersimulator to express its willingness to donation.Our rewards are determined based on three situa-tions: 1) Successful goal achievement by the dia-logue agent results in a significant positive reward,defined as 1.0 in the charity persuasion task and thevalue of SL% in the price negotiation task. 2) Fail-ure to achieve goals leads to a substantial negativereward of -1.0 for the dialogue agent. 3) Further-more, we assign a small negative reward (-0.1) perturn to penalize the lengthy conversation, whichpromotes the efficient goal achievement. Optimization. During RL training, we maximizethe expected reward of the strategy planner byutilizing the REINFORCE algorithm (Williams,1992): log Rt, where denotesthe trainable parameter of the strategy planner, denotes the learning rate, and Rt is the total rewardaccumulating from turn t to the final turn T: Rt =Tt=t Ttrt, where is a discount factor.",
  "Experiments": "This sections aims to evaluate the effectiveness ofour TRIP, following the evaluation protocol pro-posed in .1. We initially report the overallperformances of dialogue agents in .1.Next, we conduct an in-depth analysis to revealthe tailored strategies of TRIP in .2. Fi-nally, we perform ablation studies in .3 tosort out the performance variation of different userawareness and training population, and find a dom-inant predictor for the tailored strategic planning.LLM-based baselines. We consider LLM-baseddialogue agents with two types of strategic plan-ning modules, as discussed in .1)Prompt-based planning, including Standard, Pro-CoT (Deng et al., 2023d) and ICL-AIF (Fu et al.,2023), which use mixed-initiative prompts, CoT,and AI feedback to select next strategies, respec-tively.2) External strategy planners, includingGDP-MCTS (Yu et al., 2023) and PPDPP (Denget al., 2023e), which utilize Monte Carlo TreeSearch and a trainable plug-in for determining next-step strategies, respectively. Note that all baselinesfail to model user-specific characteristics explicitlyand are trained using one user simulator. Imple- : The agents performance across various personas. We report their success rate on two tasks, namelyprice negotiation (Left) and charity persuasion (Right). TRIP achieves balanced improvements on all personas,significantly outperforming other agents by a considerable margin. Due to limited space, we report other resultsusing different metrics in Appendix D. mentation details are presented in Appendix B.Evaluation Metrics. We use the same automaticmetrics mentioned in section 3.1. Furthermore, weconduct human evaluation to assess the practicaleffectiveness of these dialogue agents. See moredetails of human evaluation in Appendix C.",
  "Overall Performance": "We evaluate the overall and fine-grained perfor-mance of all agents using automatic metrics in Ta-ble 2 and . Additionally, we report humanevaluation in to gauge their performanceduring interactions with real users.TRIP is a promising method for achieving ef-fective non-collaborative strategies tailored fordiverse users. As illustrated in , TRIP sig-nificantly outperforms all the baselines with a no-ticeable margin across two tasks. It not only effi-ciently achieves the conversational goal (less AT)but also effectively accomplishes tasks (higher SRand higher SL%). Moreover, as depicted in , TRIP shows balanced improvements across dif-ferent user personas, significantly outperformingother agents by a substantial margin, in contrastto the biased improvements of PPDPP in .2. This suggests that TRIP is capable of gen-erating strategies that generalize well to diverseusers. This also implies that the behavior patternpf a single LLM-based user simulator is limitedin scope. Moreover, our human evaluation resultsin show our TRIP largely outperform theStandard and PPDPP when interacting with realusers. Notably, we observed that PPDPP does notconsistently surpass the Standard approach acrossthe two tasks. For instance, while it achieves ahigher success rate in the negotiation task, it neces-",
  "Strategy Analysis": "In this section, we analyze the effectiveness of ourTRIP in tailored strategic planning. Specifically,in each user interaction, we gather the strategiesemployed by each agent at every turn and combinethem in a sequential order to form a strategy se-quence. Then, we compare the strategy sequences : Case study on the charity persuasion task (Top-3 conversation rounds). The user resisting strategiesand agent strategies are marked in bleu and red respectively. While PPDPP repeats its strategy usage pattern todifferent user types, TRIP effectively tailor its strategies for different users. When dealing with theOpenness persona(Left), TRIP introduces the charitable organization and evoke specific emotions to sway users decision. Conversely,in addressing the Neuroticism persona (Right), TRIP tends to discuss personal experiences related to charity andemploys reasoning persuade the user.",
  "TRIP (Ours)16.1420.26": ": The strategy distribution of different agents.The Intra-Persona metric donates the average distancefor a particular persona. The Inter-Persona metric do-nate the average distance for different personas. TRIPachieves the best performance, showcasing its effective-ness in devising tailored strategies for diverse users. employed by different agents. We utilize BERT(Devlin et al., 2018) and the t-SNE method (Van derMaaten and Hinton, 2008) to encode each strategysequence into an embedding vector. Subsequently,we use the Euclidean distance measure to calcu-late the average distance between any two strategysequences used by agents with the same persona,as well as the average distance between any twostrategy sequences used by agents with differentpersonas. This is akin to the metrics (i.e., the Intra-Class and Inter-Class analysis) used in the metriclearning community (Roth et al., 2019) and weterm them as the Intra-Persona and Inter-Persona.The results are shown in .TRIP demonstrates a greater awareness of pop-ulation dynamics, resulting in reduced varianceacross specific user simulators. As shown in Ta- ble 3, TRIP achieves the lowest Intra-Persona andthe highest Inter-Persona. This indicates that thestrategy sequences of TRIP exhibit similarity wheninteracting with users sharing the same personasand non-collaborative behaviors. Also, these se-quences are distinct when compared to users withdifferent personas. This further reveals that TRIPholds advantages in devising tailored strategies fordiverse users.For better understanding, we present a case studyin and examine the strategy sequence em-ployed by PPDPP and TRIP in an charity persua-sion task. Specifically, PPDPP repeats its strategyusage pattern to different user types, briefly usingof credentials and citing organizational impacts toestablish credibility and earn the persuadees trust.In contrast, TRIP demonstrates a deeper understand-ing of the users and provides more tailored strate-gies. When dealing with the Neuroticism persona,TRIP tends to discuss personal experiences relatedto charity and employs reasoning persuade the user.Conversely, in addressing the Openness persona,TRIP introduces the charitable organization andevoke specific emotions to sway users decision.The strategy sequence used by TRIP is believed tobe more persuasive, as demonstrated by (Barfordand Smillie, 2016; Wang et al., 2019), stating thatthe Openness users are inclined to embrace noveltyand be easily influenced by emotions, while theNeuroticism users are more likely to be influencedby others personal experiences. In this regard, we",
  "TRIPw/ 10 POP & w/o UA: In this variant, we re-move the user-aware strategic planning modulefrom TRIP w/ 10 POP": "We summarize the overall performance of eachmodel variation . Based on these results, wedraw the following observations:User-aware strategic planning and population-based training paradigm are both effective toproduce tailored strategic planning. Specifically,compared to TRIPw/o UA, we note TRIP improvesthe persuasion success rate (0.3233 0.4400) andthe deal benefit SL% (0.3144 0.3505). This sug-gest that incorporating user mental states and fu-ture actions can assist the agent in developing moreeffective strategies. Notably, this variant slightlydecreases the deal success rate (0.6988 0.6888).This can be attributed to the fact that deeply model-ing user characteristics may inadvertently decreasethe sellers willingness to engage in the deal, as the",
  ": The test performance of different number oftraining user simulators. PPDPP converges easily buthas a limited upper bound in terms of performance": "focus is on maximizing ones own benefits. More-over, compared to TRIPw/o POP, we observe thatTRIP yield positive improvements across all met-rics, such as significant increase in SL% (0.3505 0.4096). This demonstrates that diversifyingthe behaviors of training user simulators effectivelyimproves the agents performance.Diverse training populations is more benefi-cial to improve the adaptability of dialogueagents, but it may also present additional train-ing challenges.As shown in , com-pared to TRIPw/o UA and TRIPw/o POP, we find thatdiverse training populations is more importantfor TRIPs superiority. Moreover, we find thatTRIPw/o UA demonstrates higher performances thanTRIPw/ 10 POP & w/o UA and PPDPP (i.e., A singlefixed user simulator). To provide a detailed un-derstanding of the impact of the number of train-ing user simulators, we present their test perfor-mance of in 1000 training interactions, as de-picted in . Particularly, during the initial400 interactions, we observe that TRIPw/o UA andTRIPw/ 10 POP & w/o UA exhibit slower convergencecompared to PPDPP. This suggests that not keep-ing the training user simulator fixed can introduceinstability in the initial training phase, as also notedin (Lewis et al., 2017). However, beyond 500 in-teractions, the training process of TRIPw/o UA stabi-lizes, leading to a significant performance enhance-ment, surpassing the other two agents. Addition-ally, it is observed that PPDPPs performance de-clines after specific interactions (e.g., 600 in pricenegotiation), suggesting that extensive interactionswith a single user simulator cannot consistentlyenhance agents performance.",
  "In this study, we investigate the inadequacies ofcurrent LLM-based dialogue agents in catering indiverse non-cooperative users. To address this, we": "propose TRIP, a method designed to tailor strategicplanning for non-collaborative dialogues. The ideabehind our TRIP is simple, involving a user-awarestrategic planning module and a population-basedtraining paradigm. Experimental results across di-verse users demonstrate the superior effectivenessand efficiency of TRIP. We consider our work aslaying the groundwork for enhancing the adapt-ability and flexibility of non-cooperative dialogueagents in the era of LLMs. Moving forward, weplan to further explore the potential of population-aware agents in reducing the capital expenditure as-sociated with training and coaching novice agents.",
  "Limitations": "In this section, we discuss the limitations of thiswork from the following perspectives:Sensitivity of Prompts. Similar to other studieson prompting LLMs (Deng et al., 2023d), the eval-uation results are expected to be influenced by theprompts. Following (Deng et al., 2023e), we em-ploy the mixed-initiative format to formulate ourprompts, as it offers stability and control. Theimpact of prompts and their optimality present im-portant areas of investigation within LLMs, callingfor exploration in future studies.Limited Non-collaborative Tasks. We only con-duct our experiments on the two non-collaborativedialogue tasks (i.e., price negotiation and char-ity persuasion) due to their status as classic andwidely-recognized benchmarks (Deng et al., 2023d;Chawla et al., 2023a). In the future, we plan toapply our proposed TRIP in a broader range ofnon-collaborative dialogue scenarios (Zhang et al.,2024; Zhou et al., 2023b; Zhang et al., 2023b).",
  "Acknowledgements": "This work was supported in part by the Na-tional Natural Science Foundation of China (No.62272330 and No.62206191);in part bythe Natural Science Foundation of Sichuan (No.2023NSFSC0473); in part by the FundamentalResearch Funds for the Central Universities (No.2023SCU12089 and No. YJ202219); in part bythe Singapore Ministry of Education (MOE) Aca-demic Research Fund (AcRF) Tier 1 grant (No.MSS24C004).",
  "mixed emotions. Personality and individual differ-ences, 102:118122": "Federico Bianchi, Patrick John Chia, Mert Yuksek-gonul, Jacopo Tagliabue, Dan Jurafsky, and JamesZou. 2024. How well can llms negotiate? nego-tiationarena platform and analysis. arXiv preprintarXiv:2402.05863. Rujikorn Charakorn, Poramate Manoonpong, and NatDilokthanakul. 2020. Investigating partner diversifi-cation methods in cooperative multi-agent deep rein-forcement learning. In Neural Information Process-ing: 27th International Conference, ICONIP 2020,Bangkok, Thailand, November 1822, 2020, Proceed-ings, Part V 27, pages 395402. Springer. Kushal Chawla, Weiyan Shi, Jingwen Zhang, Gale Lu-cas, Zhou Yu, and Jonathan Gratch. 2023a. Socialinfluence dialogue systems: A survey of datasets andmodels for social influence tasks. In Proceedingsof the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pages750766. Kushal Chawla, Ian Wu, Yu Rong, Gale Lucas, andJonathan Gratch. 2023b. Be selfish, but wisely: In-vestigating the impact of agent personality in mixed-motive human-agent interactions. In Proceedingsof the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1307813092,Singapore. Association for Computational Linguis-tics. Maximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi,and Zhou Yu. 2023. Controllable mixed-initiativedialogue generation through prompting. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 951966, Toronto, Canada. Associationfor Computational Linguistics.",
  "The oscars of ai theater: A survey on role-playingwith language models": "Yang Deng, Wenqiang Lei, Minlie Huang, and Tat-SengChua. 2023a.Goal awareness for conversationalAI: Proactivity, non-collaborativity, and beyond. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 6:Tutorial Abstracts), pages 110, Toronto, Canada.Association for Computational Linguistics. Yang Deng, Wenqiang Lei, Minlie Huang, and Tat-SengChua. 2023b. Rethinking conversational agents inthe era of llms: Proactivity, non-collaborativity, andbeyond. In Proceedings of the Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval in the Asia Pacific Re-gion, SIGIR-AP 23, page 298301, New York, NY,USA. Association for Computing Machinery.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. Bert: Pre-training of deepbidirectional transformers for language understand-ing. arXiv preprint arXiv:1810.04805": "Esin Durmus, Karina Nyugen, Thomas I Liao, NicholasSchiefer, Amanda Askell, Anton Bakhtin, CarolChen,Zac Hatfield-Dodds,Danny Hernandez,Nicholas Joseph, et al. 2023. Towards measuringthe representation of subjective global opinions inlanguage models. arXiv preprint arXiv:2306.16388. Ritam Dutt, Sayan Sinha, Rishabh Joshi, Surya ShekharChakraborty, Meredith Riggs, Xinru Yan, Hao-gang Bao, and Carolyn Penstein Ros. 2021. Res-per:Computationally modelling resisting strate-gies in persuasive conversations.arXiv preprintarXiv:2101.10545.",
  "Zhiyuan Hu, Yue Feng, Yang Deng, Zekun Li, See-Kiong Ng, Anh Tuan Luu, and Bryan Hooi. 2023. En-hancing large language model induced task-orienteddialogue systems through look-forward motivatedgoals": "Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei,Jiancheng Lv, and Tat-Seng Chua. 2024. Concept an evaluation protocol on conversational recom-mender systems with system-centric and user-centricfactors. Chen Huang, Peixin Qin, Wenqiang Lei, and JianchengLv. 2023. Reduce human labor on evaluating con-versational information retrieval system: A human-machine collaboration approach. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 1087610891, Sin-gapore. Association for Computational Linguistics. Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim.2020.Bayes-adaptive monte-carlo planning andlearning for goal-oriented dialogues. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 34, pages 79948001. Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-juan Han, Chi Zhang, and Yixin Zhu. 2024. Evaluat-ing and inducing personality in pre-trained languagemodels. Advances in Neural Information ProcessingSystems, 36. Hang Jiang, Xiajie Zhang, Xubo Cao, Jad Kabbara, andDeb Roy. 2023. Personallm: Investigating the abilityof gpt-3.5 to express personality traits and genderdifferences. arXiv preprint arXiv:2305.02547. Simon Keizer, Markus Guhe, Heriberto Cuayhuitl,Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mi-hai Dobre, Alex Lascarides, and Oliver Lemon.2017. Evaluating persuasion strategies and deep rein-forcement learning methods for negotiation dialogueagents. In Proceedings of the 15th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics: Volume 2, Short Papers, pages480484, Valencia, Spain. Association for Computa-tional Linguistics. Deuksin Kwon, Emily Weiss, Tara Kulshrestha, KushalChawla, Gale M Lucas, and Jonathan Gratch. 2024.Are llms effective negotiators? systematic evaluationof the multifaceted capabilities of llms in negotiationdialogues. arXiv preprint arXiv:2402.13550.",
  "David Premack and Guy Woodruff. 1978. Does thechimpanzee have a theory of mind? Behavioral andbrain sciences, 1(4):515526": "Karsten Roth, Biagio Brattoli, and Bjorn Ommer. 2019.Mic: Mining interclass characteristics for improvedmetric learning. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV). Mustafa Safdari, Greg Serapio-Garca, Clment Crepy,Stephen Fitz, Peter Romero, Luning Sun, MarwaAbdulhai, Aleksandra Faust, and Maja Mataric. 2023.Personality traits in large language models. arXivpreprint arXiv:2307.00184.",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008.Visualizing data using t-sne. Journal of machinelearning research, 9(11)": "Xintao Wang, Yaying Fei, Ziang Leng, and Cheng Li.2023. Does role-playing chatbots capture the charac-ter personalities? assessing personality traits for role-playing chatbots. arXiv preprint arXiv:2310.17976. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-suasion for good: Towards a personalized persuasivedialogue system for social good. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 56355649, Florence,Italy. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2022. Self-consistency improves chainof thought reasoning in language models.arXivpreprint arXiv:2203.11171.",
  "Xiao Yu, Maximillian Chen, and Zhou Yu. 2023": "Prompt-based Monte-Carlo tree search for goal-oriented dialogue policy planning. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 71017125, Singa-pore. Association for Computational Linguistics. Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua,Suraj Sharma, Zhuang Li, Lizhen Qu, Zhaleh Sem-nani Azad, Ingrid Zukerman, and Gholamreza Haf-fari. 2024. Lets negotiate! a survey of negotiationdialogue systems. arXiv preprint arXiv:2402.01097.",
  "Qiang Zhang, Jason Naradowsky, and Yusuke Miyao.2023a. Ask an expert: Leveraging language mod-els to improve strategic reasoning in goal-orienteddialogue models. arXiv preprint arXiv:2305.17878": "Tong Zhang, Junhong Liu, Chen Huang, Jia Liu, Hon-gru Liang, Zujie Wen, and Wenqiang Lei. 2023b.Towards effective automatic debt collection with per-sona awareness. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing: Industry Track, pages 3245, Singapore.Association for Computational Linguistics. Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wen-qiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang,and Tat-Seng Chua. 2024. CLAMBER: A bench-mark of identifying and clarifying ambiguous infor-mation needs in large language models. In Proceed-ings of the 62nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1074610766, Bangkok, Thailand. As-sociation for Computational Linguistics.",
  "Xijia Zhang, Yue Guo, Simon Stepputtis, Katia Sycara,and Joseph Campbell. 2023c. Explaining agent be-havior with large language models. arXiv preprintarXiv:2309.10346": "Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju,Aditya Gupta, Kevin R McKee, Ari Holtzman, JayPujara, Xiang Ren, Swaroop Mishra, Aida Ne-matzadeh, et al. 2023a. How far are large languagemodels from agents with theory-of-mind?arXivpreprint arXiv:2310.03051. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,Yonatan Bisk, Daniel Fried, Graham Neubig, et al.2023b. Sotopia: Interactive evaluation for socialintelligence in language agents.arXiv preprintarXiv:2310.11667.",
  "Due to the significant human labor required forreal-user evaluations (Huang et al., 2023), our ex-periments utilize user simulators instead": "A.1.1Persona GenerationWe prompt GPT4 (OpenAI, 2023) to generate di-verse user personas by selecting attributes fromtwo persona types, namely Big-Five Personalityand Decision-Making Styles. Specifically, We al-low GPT-4 to choose an attribute for each personatype, resulting in attribute-based user personas com-prised of two fields, each containing a distinct at-tribute value. The prompt we use is provided in. In total, we create 20 attribute-baseduser personas and ensure that the number of eachattribute is balanced. We then prompt GPT4 torephrase these attribute-based personas into 300cohesive persona descriptions. The prompt we useis provided in . A.1.2Non-collaborative Behavior PromptingWe leverage the resisting strategies outlined in(Dutt et al., 2021) as users non-collaborative be-haviors. We provide the detailed explanations ofthese resisting strategies in . We designdetailed instructions and incorporate these resist-ing strategies with their explanations into our usersimulator prompting. A.1.3Comprehensive PromptingBy incorporating the persona description and resist-ing strategies, we construct comprehensive promptsfor our user simulators. Specifically, our prompt in-cludes two parts: task background and conversationhistory. In the task background, we guide LLMs",
  "A.2Evaluation Tasks": "Following (Bianchi et al., 2024; Deng et al., 2023e),we consider two classic tasks as our evaluation sce-narios, including price negotiation (He et al., 2018)and charity persuasion (Wang et al., 2019). Theprice negotiation task involves open-ended pricenegotiations where a buyer influences the seller to-wards a reasonable price, while the seller aims tomaximize their own profit. The charity persuasiontask involves asymmetric interactions guided bya persuader who endeavors to persuade the otherparty to make a charitable donation. Our evaluationis based on these two tasks, requiring the evaluateddialogue agents to take on the roles of buyer andpersuader, respectively, in order to achieve theirgoals. To support our evaluations, we adopt the testdataset of CraigslistBargain (He et al., 2018) andPersuasionForGood (Wang et al., 2019), makinguse of their pre-annotated background informationto streamline our assessment process. For the nego-tiation task, the background information includesitem details and the desired price of each party.For the persuasion task, it involves determining ifthe individual being persuaded initially intends tomake a donation. These background informationserve as specific scenarios for our evaluation.",
  "A.3Reliability Analysis": "Prior to conducting the interactive evaluation, wevalidate the reliability of using LLMs as user simu-lators that demonstrate non-collaborative behaviors.Following the approach described in (Deng et al.,2023e), we engage 5 human experts in conversa-tions with two groups, including our diverse usersimulators and 10 real users across two evaluationtasks. We collect 50 dialogues from each groupand evaluate the user responses in both single-turn and multi-turn open-ended conversations. Theevaluation focuses on the naturalness and utilityof the generated responses in these conversationsettings.Naturalness refers to the fluency andhuman-like nature of the responses, while utilityindicates their consistency with the role instruc-tions and non-collaborative behaviors. We employtwo annotators to conduct pairwise evaluations byrating \"Win/Tie/Lose\" between the two samples.As shown in , the user simulators exhibita notably superior performance compared to realusers, particularly when it comes to the naturalnessof responses in multi-turn conversations, whichshowcases the impressive language generation ca- pabilities inherent in LLMs. Furthermore, evencompared with human-annotated dialogues, theGPT3.5-based simulator shows competitive per-formance. These results validate the reliability ofadopting GPT3.5 as the user simulator.",
  "A.4Interactive Evaluation Protocol": "During the evaluation, each dialogue agent mustengage with these simulators (Deng et al., 2023e).During interactions, the dialogue agent and usersimulator alternate in employing strategies in theirresponses with the ultimate aim of maximizingtheir own self-interest. The interactions contin-ues until the conversational goal is achieved or themaximum number of turns T (i.e., T is set to 10 forboth tasks) is reached. To determine goal achieve-ment, we utilize AI feedback to assess whether thetask goal has been reached. Specifically, in pricenegotiation task, we employ a separate GPT3.5(i.e., LLMrwd) to assess whether both parties havereached a deal. We prompt LLMrwd to gener-ate feedback for the binary question Have theyreached a deal?. If the output of LLMrwd indi-cates that both parties have reached an agreement,we consider this as goal achievement. In charitypersuasion task, we additionally prompt the usersimulator to express his willingness to make a dona-tion at the end of each turn. In particular, we querythe user simulator \"Would you be interested in do-nating to Save the Children?\". If the feedback ispositive, we regard this as goal achievement. Con-versely, if the goal is not achieved, the interactioncontinues. Due to the subjectivity of the planning outcomeas well as the variance of the LLM-generated out-put, we follow a common practice (Wang et al.,2022; Deng et al., 2023e) to alleviate these issuesby sampling the decoded sequences l (i.e., l is setto 10 for both tasks) times.",
  "B.2Baselines Implementation Details": "We implement the existing LLM-based dialogueagents by following previous works.Standard: simply prompts LLMs to chat withusers using task instructions without consideringany dialogue strategy.ProCoT: we follow (Deng et al., 2023d) andprompt LLM to analyze the dialogue status andplan next strategy, and then generate a responsebased on the planned strategy.We provide itsprompt design in .ICL-AIF: we follow (Fu et al., 2023) and promptanother GPT3.5 for verbal feedback, offering sug-gestions to the dialogue agent upon completionof an interaction. Our implementation involvespresenting three suggestions at the conclusion ofeach interaction, while ensuring that only the mostrecent 20 suggestions are retained to prevent indef-inite expansion. The prompt we use is provided in.GDP-MCTS: we follow (Yu et al., 2023) and im-plement open-MCTS to help LLM for strategicplanning. This method is originally proposed forcharity persuasion dialogues. In order to furtheraccommodate the price negotiation applications,we just need to modify the task instruction and therole-playing description.PPDPP: we follow (Deng et al., 2023e) and adoptthe BERT7 model (Devlin et al., 2018) as our exter-nal planner. We implement PPDPP based on thetraining details provided in the original paper. Wehave made adjustments to the task instructions androle-playing descriptions, adapting them for use inthe context of charity persuasion.",
  "CHuman Evaluation": "Inspired by (Yu et al., 2023), we conduct interac-tive human evaluation using the LegoEval platform(Li et al., 2021) with crowdworkers on AmazonMechanical Turk. We primarily sought to evaluateTRIP against two competitive baselines (i.e., Stan-dard and PPDPP). In specific, we hire 20 crowd-workers with varying personas to converse withour three agents based on the price negotiation andcharity persuasion tasks. After conversations, wecollect 50 dialogues for each agent and calculatetheir performances using the same metrics men-tioned in .1.",
  "DMore Experimental Results": "In addition to the Success Rate, we report theagents performance across various personas usingthe metrics of Average Turn and Sale-to-List Ratio,as depicted in and . We discoverthat the overall performance and analysis conclu-sions remain largely consistent with .1. : The agents performance across various per-sonas. We report their SL % on the price negotiationtask. TRIP achieves balanced improvements on all per-sonas, significantly outperforming other agents by aconsiderable margin. : The agents performance across various personas. We report their average turn on two tasks, namelyprice negotiation (Left) and charity persuasion (Right). TRIP achieves balanced improvements on all personas,significantly outperforming other agents by a considerable margin.",
  "The prompt for user persona rephrase": "You need to incorporate the following persona attributes and generate a cohesive persona description.You need to ensure the description is easy to understand.********Big-Five Personality:Decision-Making Style:******** An Example:You are a 28-year-old female software developer. Your personality is characterized by opennessto experience, which means you are curious, imaginative, and willing to try new things. In youroccupation, you excel at analyzing problems and finding logical solutions. Your decision-making styleis analytical, meaning you carefully consider all available information before making a choice.",
  "Now enter the role-playing mode. In the following conversation, you will play as a seller in a pricebargaining game": "Your persona: <Persona Description>You must follow the instructions below during chat.1. Your utterances and bargain behavior need to strictly follow your persona. Varying your wordingand avoid repeating yourself verbatim!2. You can decide to change your target price flexibly based on your persona and the conversation. Your Response Strategy:1. \"Source Derogation\": Attacks the other party or questions the item.2. \"Counter Argument\": Provides a non-personal argument/factual response to refute a previous claimor to justify a new claim.3. \"Personal Choice\": Provides a personal reason for disagreeing with the current situation or choosesto agree with the situation provided some specific condition is met.4. \"Information Inquiry\": Requests for clarification or asks additional information about the item orsituation.5. \"Self Pity\": Provides a reason (meant to elicit sympathy) for disagreeing with the current terms.6. \"Hesitance\": Stalls for time and is hesitant to commit; specifically, they seek to further theconversation and provide a chance for the other party to make a better offer7. \"Self-assertion\": Asserts a new claim or refutes a previous claim with an air of finality/ confidence.8. \"Others\": Do not explicitly foil the negotiation attempts.",
  "Now enter the role-playing mode. In the following conversation, you will play as a Persuadee in apersuasion game": "Your persona: <Persona Description>You must follow the instructions below during chat.1. Your utterances need to strictly follow your persona. Varying your wording and avoid repeatingyourself verbatim!2. Pretend you have little knowledge about the Save the Children charity. You have little willingnessfor donation at the beginning of conversation.3. Your willingness for donation depends on your persona and how influenced you are by the Persuader.4. You need to determine whether to donate money or not. If you think your are convinced by thePersuader, you should donate money for the charity. Your Response Strategy:1. \"Donate\": show your willingness to donate.2. \"Source Derogation\": attacks or doubts the organisations credibility.3. \"Counter Argument\": argues that the responsibility is not on them or refutes a previous statement.4. \"Personal Choice\": Attempts to saves face by asserting their personal preference such as their choiceof charity and their choice of donation.5. \"Information Inquiry\": Ask for factual information about the organisation for clarification or as anattempt to stall.6. \"Self Pity\": Provides a self-centred reason for not being willing to donate at the moment.7. \"Hesitance\": Attempts to stall the conversation by either stating they would donate later or iscurrently unsure about donating.8. \"Self-assertion\": Explicitly refuses to donate without even providing a personal reason.9. \"Others\": Do not explicitly foil the persuasion attempts.",
  "The prompt of the ProCoT agent": "The Price Negotiation TaskAssume you are the buyer. Given the conversation history, in order to reach a better deal with the seller,please select the most appropriate dialogue strategy.You can only reply by selecting one of the following dialogue strategy to reach the goal: Greetings.Ask a question. Answer a question. Propose the first price. Propose a counter price. Use comparatives.Confirm information. Affirm confirmation. Deny confirmation. Agree with the proposal. Disagreewith a proposal.The following is the conversation history: [conversation] The Charity Persuasion TaskAssume you are the Persuader. Given the conversation history, in order to convince the persuadee todonate for charity, please select the most appropriate dialogue strategy.You can only reply by selecting one of the following dialogue strategy to reach the goal: Logicalappeal, Emotion appeal, Credibility appeal, Foot in the door, Self-modeling, Personal story, Donationinformation, Source-related inquiry, Task-related inquiry, Personal-related inquiry.The following is the conversation history: [conversation]",
  "The prompt of the ICL-AIF agent": "The Price Negotiation TaskNow enter the role-playing mode. In the following conversation, you will play as a coach in a bargaingame. There will be a buyer and a seller bargaining about a product price.Your task is to read the conversation between the buyer and the seller, then provide suggestions to thebuyer about how to buy the product with a lower price. Each suggestion should be only one short andsuccinct sentence.The following is the conversation: [conversation]Question: What are your suggestions? Answer: The Charity Persuasion TaskNow enter the role-playing mode. In the following conversation, you will play as a coach in apersuasion game. There will be a persuader who is trying to persuade a persuadee for charity donation.Your task is to read the conversation between the persuader and the persuadee, then provide suggestionsto the persuader about how to convince the persuadee to make a donation. Each suggestion should beonly one short and succinct sentence.The following is the conversation: [conversation]Question: What are your suggestions? Answer:",
  "The prompt of our TRIP agent in price negotiation": "Now enter the role-playing mode. In the following conversation, you will play as a buyer in a pricebargaining game.You are the buyer who is trying to buy the %s with the price of %s. Product description: %sPlease reply with only one short and succinct sentence. [action] Now start the game.",
  "The prompt of our TRIP agent in charity persuasion": "Now enter the role-playing mode. In the following conversation, you will play as a Persuader who istrying to persuade the Persuadee to donate to the charity called Save the Children.Save the Children is head-quartered in London, and they work to help fight poverty around the world.Children need help in developing countries and war zones. Small donations like $1 or $2 go a longway to help.You are the Persuader who is trying to convince the Persuadee to donate to a charity called Save theChildren. [action]Please reply with only one short and persuasive sentence."
}