{
  "Abstract": "Adversarial textual examples reveal the vulner-ability of natural language processing (NLP)models. Most existing text attack methods aredesigned for English text, while the robust im-plementation of the second popular language,i.e., Chinese with 1 billion users, is greatly un-derestimated. Although several Chinese attackmethods have been presented, they either di-rectly transfer from English attacks or adoptsimple greedy search to optimize the attack pri-ority, usually leading to unnatural sentences.To address these issues, we propose an adap-tive Immune-based Sound-Shape Code (ISSC)algorithm for adversarial Chinese text attacks.Firstly, we leverage the Sound-Shape code togenerate natural substitutions, which compre-hensively integrate multiple Chinese features.Secondly, we employ adaptive immune algo-rithm (IA) to determine the replacement order,which can reduce the duplication of popula-tion to improve the search ability. Extensiveexperimental results validate the superiority ofour ISSC in producing high-quality Chineseadversarial texts. Our code and data can befound in",
  "Introduction": "Deep Neural Networks (DNNs) have shown greatvulnerability towards text adversarial examples, pri-marily in English text (Papernot et al., 2016). Thisphenomenon poses a great challenge for the secu-rity application of DNNs models in text-relatedtasks, such as sentiment analysis (El Rahman et al.,2019) and toxic comment detection (Abbasi et al.,2022; Bespalov et al., 2023), etc. Therefore, it isessential to devise high quality text adversarial ex-amples to investigate the brittleness boundary andunderstand the behaviors of modern DNNs modelsbefore implementation.",
  ": Candidates comparison of English transferattack and Chinese Sound-Shape Code (SSC) attack.Intuitively, the Chinese SSC generates more naturalcandidates from the view of Chinese native speakers": "During the past decade, abundant attention hasbeen paid to craft English text adversarial exam-ples, ranging from character-level attack (Gao et al.,2018), word-level attack (Zang et al., 2020; Yuet al., 2022), sentence-level attack (Iyyer et al.,2018), and multi-level attack (Chen et al., 2021; Xuet al., 2024), usually with text insertion, deletion,substitution, and rewriting operations. Owing tothe concealment and flexibility, word substitutionbased text attack methods are gradually becomingthe most popular line (Qiu et al., 2022). In this line,two common steps are (1) generating high qual-ity substitution candidates, and (2) optimizing theattack priority. To generate semantic consistencycandidates, researchers have tried GloVe embed-ding, WordNet synonyms, HowNet sememe can-didates, Masked Language Models (MLM), andprompt engineering. In the second step, variousoptimization methods, e.g., word-saliency basedstatic optimization (Ren et al., 2019; Garg and Ra-makrishnan, 2020; Li et al., 2020), and objectiveguided dynamic heuristic optimization (Alzantotet al., 2018; Zang et al., 2020), have been widelyexplored.Nevertheless, the adversarial attack on Chinesetext, which ranks the second popular language with1 billion speakers (Comrie and Comrie, 2018), has received limited attention. Existing Chinese textattack methods can be roughly categorized intotwo groups, i.e., the direct transfer from the En-glish attack and Chinese characteristic based staticoptimization. The former group ignores the richChinese characteristics, e.g., the character structure,sound, and shape, which greatly limits the searchspace and usually results in unnatural adversarialsentences (as shown in ). The latter groupgenerates substitutions based on Chinese linguis-tic characteristics, for example, splitting charactersinto radicals (Nuo et al., 2020), converting sim-plified and traditional Chinese characters (Tonget al., 2020), and replacing characters with ones ofsimilar glyph or pinyin (Zhang et al., 2020b; Liuet al., 2023a). However, previous studies take onlyone of the above strategies at each attack period,which is largely insufficient to integrate multipleChinese linguistic information. Additionally, thelatter group usually employs word saliency basedstatic optimization (Zhang et al., 2020b; Liu et al.,2023a), while the pre-defined attack sequence cannot match the dynamic word saliency change withthe attack iterations going on.To address these problems, we propose a novelChinese text adversarial attack method, namedImmune-based Sound-Shape Code (ISSC). Specifi-cally, our ISSC simultaneously investigates multi-ple Chinese characteristics by analyzing the Sound-Shape Code (SSC), which contains both pronun-ciation information and visual information. TheseChinese characteristics enrich the search space andimprove the naturality of adversarial text from theview of native Chinese speakers. Then we devisean Immune Algorithm (IA) to optimize the attackpriority. Particularly, we enhance the diversity offeasible solutions by minimizing the reputation ratein the objective function and the half-populationvaccination operation. This is effective in improv-ing the search space and providing more opportu-nities to approach the global optimal. Extensiveexperiments demonstrate that our ISSC achievesthe highest attack success rate, and lower modifi-cation rate with more natural adversarial examplesin most cases. We emphasize our contributions asbelow. We generate natural Chinese substitutionsby combining the Sound-Shape Code (SSC),which carries both pronunciation and visualinformation. This is significant to ensure theadversarial output can be read smoothly. We design an adaptive immune algorithm (IA)method to determine the word replacement or-der. We carefully devise the objective functionand the half-population vaccination operationto reduce the population repetition rate. Thesetwo strategies extend the search space of clas-sical IA and provide a higher probability offinding the global optimal. We conduct extensive experiments on five pub-lic datasets to validate the effectiveness of ourmethod. The results manifest that our ISSCoutperforms the baselines in terms of ASRand text quality, and also shows superioritiesin transferability and adversarial training.",
  "Related Works": "In this section, we briefly review the text adversar-ial attacks in both English and Chinese.English Text Attack. According to the modi-fication granularity, English attack methods canbe categorized into character-level, word-level,sentence-level, and multi-level attacks.Gener-ally, the character modifications easily lead tomisspelled words, which is also the case in Chi-nese (He et al., 2023). Besides, sentence insertionor paraphrasing usually produces unreadable sen-tences with a relatively low attack success rate.Owing to the flexibility and invisibility, the word-level and multi-level attacks highly rely on the wordsubstitutions, so we focus more on the word sub-stitution based attacks. In this track, two commonsteps are (1) collecting appropriate substitution can-didates and (2) optimizing the replacement order.In the first step, the substitution candidates aretypically collected from GloVe embedding space(Alzantot et al., 2018), WordNet synonyms (Renet al., 2019), HowNet sememe candidates (Zanget al., 2020), Masked Language Model (MLM)(Garg and Ramakrishnan, 2020; Li et al., 2020),and prompt engineering (Xu et al., 2024). Thesemethods can also be utilized at the same time, asthey may produce complementary candidates. Inthe second step, researchers tend to determine theword replacement order via either word saliencybased static method (Ren et al., 2019; Li et al.,2020) or objective guided heuristic search (Alzan-tot et al., 2018; Zang et al., 2020). The static meth-ods compute the word importance score at once,with relatively higher efficiency than dynamic opti-mization, e.g., Genetic Algorithm (GA) and Parti-cle Swarm Optimization (PSO). While the adaptive",
  "search can better fit the dynamic change of wordimportance, and usually attains higher attack suc-cess rate with lower perturbation cost": "Chinese Text Attack. The researches on Chi-nese text attacks have received much less atten-tion than English text attacks. We would like todivide the existing Chinese text attack methodsinto English-transferred attacks and Chinese char-acteristic based attacks. Intuitively, some Englishtext attack methods are naturally suitable for Chi-nese text attacks, such as PSO(Zang et al., 2020),GA(Alzantot et al., 2018), and BEAT(Li et al.,2020). The reason is that their candidates selec-tion methods, i.e., HowNet, GloVe, and MLM, alsoprovide the Chinese option. However, this grouptends to ignore the rich linguistic characteristics ofChinese, so the candidates size and quality are bothgreatly limited. The Chinese characteristic based attacks gener-ate substitutions according to some unique Chi-nese features. Some researches focus on replacingcharacters with homophones (Wang et al., 2019;Zhang et al., 2020b; Wang et al., 2022; Liu et al.,2023a) and ones of similar glyph (Zhang et al.,2020b; Wang et al., 2022; Liu et al., 2023a). Someresearches generate substitutions based on split-ting characters of specific structures into radicals(Zhang et al., 2020b; Nuo et al., 2020) and the con-version of simplified and traditional fonts (Tonget al., 2020). However, existing researches fre-quently adopt only one of the above methods ineach attack period, which is insufficient to explorethe rich Chinese text features. Besides, existingChinese characteristic based attacks follow thestatic attack sequence, which usually leads to wordover-substitution and low attack success rate.",
  "Sound-Shape Code based Substitutions": "Chinese characters exhibit a diverse array of distinc-tive characteristics, which are conducive to the gen-eration of natural substitutions, in contrast to En-glish. Existing works frequently generate Chinesesubstitutions based on pinyin and glyph similarity.Chinese Pinyin is the widely adopted romanizationsystem for Standard Chinese. As shown in Fig-ure 2, the romanized spelling above each Chinese",
  ": Illustration of Chinese characters with dif-ferent structures. Some structures comprise subareas,depicted by rectangles": "character represents its pronunciation, includingthe initial, final, and tone. Besides, as a kind ofhieroglyphics, Chinese character comprises severalradicals. shows that some characters canbe transformed into ones with similar glyphs. Nev-ertheless, existing works fail to encompass multipleChinese characteristics, leading to unnatural substi-tutions, e.g., \"\"w (swiftly) [\"\"dng (stool)or \"\"w (object)] generated via Argot either fo-cus on visual perception or pronunciation ratherthan ours [\"\"w (gallop)].Sound-Shape Code (SSC) is a Chinese charac-ter encoding method and has been proven to solvethe task of Chinese text error correction effectively(Wang et al., 2020). It consolidates multiple Chi-nese characteristics into a unified dimension, offer-ing a holistic measure of character features. Thus,replacing characters with ones of similar SSC willbe imperceptible for a native Chinese speaker. Fig-ure 4 shows that the SSC consists of 11 bits and isdivided into two parts: sound code and shape code.The sound code comprises initials, finals, and tones,encoding the pronunciation information of Chinesecharacters. The shape code reflects the visual per-ception information of Chinese characters, includ-ing stroke count, structure, and four-corner code.The layout of a character largely affects how peopleperceive visual similarity between characters (Liuet al., 2010). The character structure () andfour-corner code () reflect the local andglobal layout of Chinese characters. They are help-ful in excluding visually dissimilar characters. TheStroke is the smallest unit of Chinese charactersand similar stroke counts tend to produce visuallysimilar characters.Suppose an input sentence X = [c1, c2, , cm]with m characters, we first segment it into a seriesof words (phrases) X = [w1, w2, , wn] withJieba1. Then we encode each word wi into Sound-Shape Code2 and get the top-k words with high : Illustration of Chinese characters with four-corner code. The four-corner code utilizes four digits 0to 9 to represent different groups of strokes for the fourcorners of a Chinese character, with a complement codeadded at the end. similarity as its substitutions Vssc(wi). For exam-ple, we encode \" (swiftly)\" and \" (gallop)\" intocodes \"5J04218127C\" and \"5J04218127E\", then,the longest common sub-strings length of soundcode and shape code are calculated respectively tomeasure the similarity.",
  "Adaptive Immune Optimization": "Before detailing our approach, we first clarify theconcepts of original immune algorithms (IA). In-spired by the biological immune, the IA is a heuris-tic optimization algorithm that simulates the pro-cess of organism producing antibodies when anti-gen invades, where the antigen and the antibodydenote an objective function and a feasible solution,respectively. Specifically, the algorithm evaluatesthe quality of the solution by calculating the affin-ity of the antibody to the antigen. It then performsclone selection, where the antibodies with superiorquality are chosen, followed by mutation operationto explore potentially feasible solutions. Those an-tibodies with lower quality are refreshed to randominitialization. It is worth mentioning that the IAavoids the concentrations of antibodies in the cloneselection step to enrich the diversity of feasible so-lutions. Therefore, the IA is capable of reducingpopulation redundancy and enhancing the overallquality of the population.Formally, given a segmented input sentenceXori = [w1, w2, , wn] with n words and itstrue label Ytrue, our ISSC method targets to craftan adversarial example Xadv to mislead the DNNsclassifier F. The optimization procedure is givenin Algorithm 1. Firstly, we initialize the popula-tion by repeating the mutation step for N timesto get N antibodies X 1 = {X11, X12, , X1n}.Following the GA and PSO, we adopt a singlepoint mutation strategy that an original word wi isselected randomly and replaced at a time. Thequality of each antibody Xti in the population",
  "j=1Sim(Xti, Xtj).(5)": "This reflects the similarity of antibodies in the samepopulation. If the antibodies are similar to eachother, it is easy to converge to a local optimal.Therefore, we reduce to improve the diversityof the feasible solutions, which provides higherprobability to find the global optimal. Particularly,the similarity of antibodies is defined via the edit-distance with a threshold s",
  ", len(Xti = Xtj) > s,(6)": "where len(Xti = Xtj) counts the number of differ-ent words (phrases) between Xti and Xtj. Basedon the objective function, we calculate the fitnessvalue of each antibodies and sort them in descend-ing order for the next step.In the clone selection step, we divide the popu-lation into two groups, i.e., the top Nc = N pcantibodies with high fitness value and the rest anti-bodies with low quality. For each antibody in theformer group, we randomly adopt the mutation op-eration according to the mutation probability pm,otherwise perform the clone operation, for m timesin line 5. Then, we store the one with the highest fit-ness from the m clones {Xtc,j}mj=1 of i-th antibodyXti. This allows for high-quality antibodies to con-duct localized exploration and preserve beneficialinformation at the individual level.The classical immune algorithm usually initial-izes the latter group of antibodies to eliminate low-quality feasible solutions, named population re-freshing. However, for adversarial text generation,the adversarial sample is initialized, which meansthat only a few words are modified. In the lateriterations, a small number of modifications is of-ten not enough to generate successful adversarial",
  "Input: Input sentence Xori = {w1, w2, , wn}and true label Ytrue, DNNs classifier F": "Output: Adversarial example Xadv1 Parameters: population size N = 40, maximumnumber of iteration times T = 30, proportion ofclone selection pc = 0.5, probability of mutationpm = 0.7, number of antibody clones m = 5,probability of vaccination pv = 0.5, weightparameter = 0.5, threshold s = 3, optimalantibody X = Xori;",
  "return Xadv = X;": "samples, so that the random refreshing will fail.Inspired by (Yuan et al., 2011), instead of randomrefreshing, we adopt a vaccine extraction and in-jection mechanism, simplified as vaccination, tosolve this problem. Specifically, we store the opti-mal individual during the iteration as the vaccineand inject the vaccine into the low-quality antibod-ies for better refreshing. In lines 7-8, each wordof antibody with low fitness determines whetherto move to the corresponding word of the vaccinewith a vaccination probability pv. As the iterationprogresses, the vaccine tends to have more modi-fications, increasing the likelihood of generatingsuccessful adversarial samples. Besides, this canretain the global advantage information at the pop-ulation level and enhance the cooperation amongindividuals in the same population.",
  "Datasets and Victim Models": "We conduct our experiments on five publicly avail-able datasets, including Chinanews (Zhang and Le-Cun, 2017), Chinese Sentiment Corpus (ChnSen-tiCorp) (Tan and Zhang, 2008), Original ChineseNatural Language Inference (OCNLI) (Hu et al.,2020), Ctrip Hotel Reviews (Ctrip)3, and JD.comReviews (JD)4. The Chinanews is a multi-classnews classification dataset. The ChnSentiCorp,Ctrip, and JD are all binary sentiment classificationdatasets. The OCNLI is used for natural languageinference (NLI) task. Statistical details of thesedatasets are summarized in .We assess the effectiveness of our method by at-tacking six popular victim models, including CNN,LSTM, BERT (Devlin et al., 2019), RoBERTa(Liu et al., 2019), ALBERT (Lan et al., 2020),and DistilBERT (Sanh et al., 2019). For CNNand LSTM, we use a 300-dimensional embeddinglayer. We download the BERT (google-bert/bert-base-chinese), RoBERTa (uer/roberta-base-wwm-chinese-cluecorpussmall), ALBERT (uer/albert-base-chinese-cluecorpussmall) and DistilBERT(distilbert/distilbert-base-multilingual-cased) fromthe Huggingface5 and fine-tune them with a com-mon default configuration. You can download thesemodels easily here 6.",
  "We compare our ISSC with both typical Englishtransfer methods, such as BERT-Attack (BEAT)(Li et al., 2020), GA (Alzantot et al., 2018) and": "PSO (Zang et al., 2020), and Chinese attack algo-rithms, such as Argot (Zhang et al., 2020b) andExpanding Scope (ES) (Liu et al., 2023a). As men-tioned in 2, existing works in Chinese scene fre-quently adopt a group of methods to generate sub-stitutions, which is insufficient to obtain naturalcandidate words. Besides the existing candidatesselection methods, e.g., GLoVe and masked lan-guage model, our ISSC can further takes the Chi-nese characteristic into consideration, so the qualityof adversarial text can be improved. On the otherhand, compared to the heuristic optimizations, e.g.,GA and PSO, the proposed Immune Algorithm(IA) is good at reducing the population duplication,which is significant to avoid local optimal.",
  "Evluation Metrics": "We evaluate the performance of each attack algo-rithm with the following metrics.Attack Success Rate. The attack success rate(ASR) is defined as the percentage of the number ofsamples that successfully mislead the victim modelover the total number of attacked samples.Modification Rate. The modification rate (MR)is defined as the percentage of the number of modi-fied Chinese characters to the length of the sentence.Different from the English scene, too long or tooshort phrases lead to additional modification rateswhen synonym substitution is performed.Semantic similarity and fluency. The seman-tic similarity and the fluency reflect the quality ofadversarial examples. We adopt the BERTScore(Zhang et al., 2020a) and perplexity to evaluate thesemantic similarity and fluency, respectively.",
  "Experimental Setup": "The parameter settings for our method are givenin line 1 of Algorithm 1. For the victim model,we finetune models on five Chinese datasets witha default training configuration. For the baselinesin English scene, we transfer them to the Chinesescene with their author recommended parametervalues. For the baselines in Chinese scene, wereplicate the Argot following the original paper,and experiment with ES implemented on the Tex-tAttack framework (Morris et al., 2020). For alltasks, we apply the stopwords modification andrepeat modification constraints. Additionally, forthe NLI task, only the premise text is allowed tobe modified. To achieve efficiency, we randomlyselect 500 instances from the test sets to craft adver-sarial examples. All experiments are implemented",
  "Average94.6874.6489.8569.0795.0197.2714.8821.7927.0119.2117.9312.29": ": The attack success rate and modification rate of all attack methods on five datasets. We highlight the bestresults in bold. The \"ACC\" represents the original accuracy of models. We achieve the best results in most cases,with attack success rate and modification rate outperforming the best methods by 2.26% and 2.59% on average. ArgotGAPSOBEATESISSC 0.80 0.85 0.90 0.95 1.00 BERTScore",
  "Experimental Results": "The experimental results of attack success rate andmodification rate are shown in . For theASR, our algorithm achieves the best results inmost cases and outperforms the best method by2.26% on average. Despite extensive modifica-tions, some methods are still unable to outperformour method. For the MR, our method can producefewer modifications in most cases, with an averagereduction of 2.59 % compared to the best method.Our ISSC achieves the second-best results on OC-NLI. We attribute this to the smaller input length. ArgotGAPSOBEATESISSC -7.5 -5.0 -2.5 0.0 PPL",
  ": Results of perplexity on all datasets": "Semantic similarity and fluency. To avoid hu-man perception, the adversarial examples should besemantically consistent with the original text. Weadopt BERTScore to evaluate the semantic consis-tency before and after attacks. As shown in , our ISSC achieves the highest semantic consis-tency on the whole and average. The perplexityreflects the fluency of sentences by calculating theprediction probability via a language model, i.e.,GPT-2. We calculate the change of PPL before andafter attacks, the smaller the decrease (the largerthe PPL), the more fluent the sentence. shows that ISSC generates more fluent adversarialexamples than other methods except for BEAT. The",
  "Transferability": "The transferability of adversarial examples refersto whether the adversarial examples designed ona model F1 can mislead another model F2 with-out any access to it.We select four baselineswith higher ASR and conduct the experiments onChnSentiCorp dataset. Specifically, we obtain theadversarial examples crafted on DistilBERT by var-ious methods and then perform the transfer attackon five unknown models (CNN, LSTM, BERT,RoBERTa, and ALBERT). The results of classi-fication accuracy on adversarial data are shown in. The experimental results illustrate thatISSC generates adversarial examples with highertransferability than others.",
  ": Decomposition results of various searcheson BERT with two datasets. The \"SSC+delete\" and\"SSC+PSO\" represent attacks with different searchmethods. The \"SSC+IA\" denotes our proposed ISSC": "model on clean JD dataset. Then we randomlygenerate 500 adversarial examples (10.4 % of theoriginal training set size) and add them to the train-ing set. Finally, we retrain the model and evaluateits robustness by calculating the ASR of differ-ent attack algorithms. As shown in , ourmethod can reduce the ASR of unknown methodsmore in most cases, and is competitive with modelsretrained by consistent attacks.",
  "Decomposition Analyses": "To demonstrate the advantages of our proposedadaptive immune algorithm (IA) over other searchmethods, we conduct decomposition experimentson BERT with Chinanews and ChnSentiCorpdatasets. The straightforward candidate searchescan be roughly divided into word saliency-basedstatic methods, e.g., removing the input word oneby one and calculating importance scores, andobjective-guided heuristic optimization methods,e.g., PSO. Specifically, we replace IA with othersearch methods, e.g., deletion-based static searchand the dynamic PSO, and keep the candidatemethod unchanged, i.e., SSC. Our IA achieves bet-ter performance in both ASR (5.63%) and MR(3.94%) as shown in . At the same time, italso proves that dynamic methods are often betterthan static searches.",
  ": Experimental results of target attacks on amulti-class classification dataset with different methods.The 0 and 1 represent the target labels that need to flipto by attack methods": "a reduction in almost all metrics after removingthe vaccination module, which indicates that thevaccination is conducive to improving the globalsearch ability. In particular, results show that theproposed vaccination module can largely reducethe number of queries of our method, which meansthat it has more advantages in the face of someaccess restrictions. This is because the module canhelp the population better initialize and help thealgorithm converge in advance rather than classicalrandom initialization.",
  "Target Attacks": "Our approach is primarily designed for untargetedattacks, which means that we only need to flip thetrue label to any wrong label. Meanwhile, our ap-proach can be easily modified for targeted attacksby changing the affinity in Eq (4). We conducta targeted attack experiment on BERT with theChinanews dataset. shows that both theattack success rate and adversarial sample qual-ity decrease significantly, indicating that targetedattacks are more difficult than untargeted attacksfor all methods. Besides, our ISSC can generatemore targeted adversarial examples with a lowermodification than other attacks.",
  "Conclusion": "In this paper, we proposed a novel adversarial Chi-nese text attack algorithm named Immune-basedSound-Shape Code (ISSC). The ISSC adopts theSound-Shape Code methods to generate naturalChinese substitutions and optimizes the attack pri-ority via adaptive Immune Algorithm (IA). We con-ducted extensive experiments to demonstrate theeffectiveness of our algorithm in terms of attacksuccess rate, text quality, transferability, adversar-ial training, and targeted attacks. In addition, weconducted ablation experiments to validate the en-hancement of global search capability to IA by thevaccination module. In the future, we hope ourapproach will draw attention to multilingual adver-sarial samples.",
  "Limitations": "Dynamic Parameter. The hyper-parameter set-ting of our search algorithm is simple but effective.More parameters can be designed to change withthe number of iterations to further improve the at-tack performance.Language Transfer Restriction. We propose asubstitution method to integrate multiple Chinesefeatures, some of which are limited to some EastAsian scripts and not applicable to Latin scripts. Ahmed Abbasi, Abdul Rehman Javed, Farkhund Iqbal,Natalia Kryvinska, and Zunera Jalil. 2022. Deeplearning for religious and continent-based toxic con-tent detection and classification. Scientific Reports,12. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.2018. Generating natural language adversarial ex-amples. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 28902896, Brussels, Belgium. Associationfor Computational Linguistics.",
  "Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, YingZhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, andYang Liu. 2024. Masterkey: Automated jailbreakingof large language model chatbots. In Proc. ISOCNDSS": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 41714186, Minneapolis, Minnesota. Asso-ciation for Computational Linguistics. Sahar A. El Rahman, Feddah Alhumaidi AlOtaibi, andWejdan Abdullah AlShehri. 2019. Sentiment analy-sis of twitter data. In 2019 International Conferenceon Computer and Information Sciences (ICCIS),pages 14. Ji Gao, Jack Lanchantin, Mary Lou Soffa, and YanjunQi. 2018. Black-box generation of adversarial textsequences to evade deep learning classifiers. In 2018IEEE Security and Privacy Workshops (SPW), pages5056.",
  "Siddhant Garg and Goutham Ramakrishnan. 2020": "BAE: BERT-based adversarial examples for text clas-sification. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 61746181, Online. Associationfor Computational Linguistics. Zheyu He, Yujin Zhu, Linlin Wang, and Liang Xu.2023. UMRSpell: Unifying the detection and cor-rection parts of pre-trained models towards Chi-nese missing, redundant, and spelling correction.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1:Long Papers), pages 1023810250, Toronto,Canada. Association for Computational Linguistics. Hai Hu, Kyle Richardson, Liang Xu, Lu Li, SandraKbler, and Lawrence Moss. 2020. OCNLI: OriginalChinese Natural Language Inference. In Findingsof the Association for Computational Linguistics:EMNLP 2020, pages 35123526, Online. Associ-ation for Computational Linguistics.",
  "3rd Workshop on Trustworthy Natural LanguageProcessing(TrustNLP2023),pages276286,Toronto, Canada. Association for Computational Lin-guistics": "Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang,Tianwei Zhang, Yepang Liu, Haoyu Wang, YanZheng, and Yang Liu. 2023b. Prompt injection at-tack against llm-integrated applications. Preprint,arXiv:2306.05499. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "ATraining Details": "The CNN consists of an embedding layer and a1-D convolutional layer containing 150 filters witha filter size of 3 4 5. The LSTM consists of anembedding layer and a bidirectional LSTM layerwith 150 hidden states. For CNN and LSTM, weuse a 300-dimension embedding layer to encodethe input word (phrase)7. Both of them have adropout rate of 0.3 and are training for 10 epochs.For the transformer-based models with the basesize, we train them for 3 epochs with a 3e-5 learn-ing rate.",
  "BEfficency Analysis": "We conduct all experiments on Enterprise LinuxWorkstation with 12 vCPU Intel(R) Xeon(R) Plat-inum 8255C CPU @ 2.50GHz, NVIDIA RTX2080Ti 11G GPU and 40GB RAM. liststhe time consumption and queries of attacking asample on average over all datasets. Regardingtime consumption, our ISSC achieves superior per-formance with an acceptable time cost comparedwith other methods. For the number of model as-sesses, the heuristic-based methods frequently per-form better with more queries than static methods,e.g., Argot, BEAT, and ES. Compared to heuristicsearch methods, e.g., PSO, our number of queriesincreases slightly, yet the attack results surpass itsignificantly.",
  "CLLM transfer attacks": "Large language models (LLMs) are revolutionizingmany fields of human endeavor and continue todevelop at a breathtaking pace, in terms of scaleand capabilities, but also architectures and applica-tions. LLMs generate text autoregressively, whichcan solve various downstream tasks in a zero-shotscenario. To valid the transferability of our meth-ods for LLMs, we conduct a simple transfer-basedattack against LLMs on GPT-4. Specifically, we randomly select 100 adversarial examples gener-ated on BERT with Chinanews, then manually teston the openai website8.As shown in ,LLMs are more robust compared to the smallermodels, which is consistent with the results of theAdvGLUE (Wang et al., 2021). Besides, we mainlyfocus on the classification robustness of LLMs, andresearchers are exploring the two prevalent typesof adversarial attacks on aligned unimodal LargeLanguage Models (LLMs): jailbreak attacks (Denget al., 2024) and prompt injection attacks (Liu et al.,2023b). In the future, we will give more emphasisto the adversarial security frontier of LLMs andcommit to addressing more complex security eval-uation issues of LLMs.",
  "Adversarial examples on Chinanews dataset": "Adversarial examples via Argot: 20088818Translation: Strong convection weather forecast continued to be issued by \"xu\" at 1800 on 8 August 2008.Prediction: Mainland china politics (98%) Hongkong macau politics (77%) Adversarial examples via ES: 20088818Translation: The Central Observatory continued to issue a severe convectiveweather forecast at 18:00 on 8 August 2008.Prediction: Mainland china politics (98%) Hongkong macau politics (76%) Adversarial examples via ISSC: 20088818Translation: The Central Observatory continued to issue a severe convectionweather forecast at 18:00 on August 8, 2008.Prediction: Mainland china politics (98%) Hongkong macau politics (77%)"
}