{
  "Abstract": "Capturing inter-modal incongruities within thetext-image pair is a critical challenge in multi-modal sarcasm detection (MSD). Fortunately,graph neural networks (GNNs) have madepromising advancements in MSD, which showadvantages in explicitly capturing data relation-ships. Nevertheless, current GNN-based MSDmethods do not effectively address some of theinherent limitations of GNNs, which include:1) neglecting high-order relationships, and 2)underestimating high-frequency messages. Inthis paper, we propose a Dual Graph-basedLearning Framework (DGLF) to address theabove two issues. Specifically, we construct ahypergraph to perform high-order aware prop-agation and a vanilla graph to perform high-frequency enhanced propagation, respectively.We empower GNNs to 1) better capture the in-herent and complicated relationships based onthe hypergraph and 2) deliver sufficient mod-eling through high-frequency enhanced mes-sages on the vanilla graph. Besides, we in-troduce multi-modal fusion information bottle-neck to effectively fuse the two learned graphfeatures. Experimental results on two bench-mark datasets show that the proposed modeloutperforms previous state-of-the-art methods.",
  "Introduction": "Due to the rise of social media platforms such asX and Facebook, multi-modal sarcasm detection(MSD) has garnered increasing research attention.MSD aims to recognize the sarcastic sentiment inmulti-modal social posts (Cai et al., 2019), whichtypically refer to textual sentences accompanyingimages. Unlike traditional text-only sarcasm detec-tion (Riloff et al., 2013; Poria et al., 2016; Zhanget al., 2016) focusing on inconsistencies in expres-sion within the text, the key objective of MSD is",
  "(a) what a gorgeous day": ": Two examples of multi-modal sarcasm. Inexample (a), the text refers to a gorgeous day, butthe accompanying image shows heavy rain, indicatingsarcastic. In example (b), the text mentions the bestquality beef, but the image displays fast food beefburgers, suggesting sarcastic as well. to effectively identify subtle inter-modal inconsis-tencies in the expression of sentiment within animage-text pair, as shown in .Towards this goal, a group of MSD works at-tempt to concatenate the textual and visual fea-tures to encapsulate sarcastic information (Schi-fanella et al., 2016), or leverage attention mecha-nism (Vaswani et al., 2017) to implicitly fuse fea-tures across modalities based on external knowl-edge (Cai et al., 2019; Pan et al., 2020). Morerecently, Graph Neural Networks (GNNs) haveachieved remarkable advancements in MSD, show-casing their exceptional ability by explicitly extract-ing structural information (Liang et al., 2021, 2022;Liu et al., 2022). As shown in (a), the con-ventional approach in this paradigm constructs aheterogeneous graph, where each token from bothmodalities is treated as a node, with similarity-based edge construction or carefully adjusted edgeweighting strategies. On this basis, it enables simul-taneous modeling of inter- and intra-modal tokendependencies through message passing, facilitatingtighter entanglement and richer interactions.Despite the promising progress these GNN-based MSD models have achieved, we discover",
  ": Conceptual comparison of the state-of-the-artmethods (a) and the proposed model DGLF (b). MIBis short for multi-modal information bottleneck": "that they still suffer from two main issues:(1) Neglect of high-order relationship explo-ration. They predominantly conceptualize rela-tionships among tokens in pairwise formulations,providing merely an approximation of higher-orderrelationships through the aggregation of multiplepairs (Feng et al., 2019; Bai et al., 2021). As de-picted in (b), crucial visual informationburger, which aligns with sarcastic textual cuesbest quality beef, may be scattered across the im-age (two burgers within the image). While it isfeasible to construct edges simultaneously amongtwo visual tokens involving burger and textualtokens of best quality beef, simplifying thesehigh-order relationships into pairwise formulationmight compromise their expressiveness (Sun et al.,2021). Therefore, the sophisticated and nuancedhigh-order relationships may not be fully capturedby existing GNN-based MSD methods.(2) Overlooking high-frequency messages ex-ploitation. The propagation rule of GNNs, charac-terized by the aggregation and smoothing of mes-sages from neighboring nodes, is widely regardedas an analogy to a fixed low-pass filter (Wu et al.,2019).It predominantly facilitates the flow oflow-frequency messages in the graph while signif-icantly attenuating high-frequency messages (Boet al., 2021). Conversely, in GNNs for MSD, thehigh-frequency messages may be more vital whichreflects discrepancies and inconsistencies in the ex-pression of sentiment. As such, the potential ofhigh-frequency information remains largely unex-ploited in existing GNN-based MSD frameworks.To tackle the above issues, as shown in Fig-ure 2(b), we propose a Dual Graph-based LearningFramework termed DGLF for MSD. For the firstissue, we construct a hypergraph (Feng et al., 2019)with edge-dependent node weights (Chitra andRaphael, 2019) to facilitate high-order aware prop-agation, where each token from both modalities is represented as a node. We construct intra- and inter-modal hyperedges, which can connect an arbitrarynumber of nodes. In this fashion, DGLF enablesthe natural encoding of high-order relationshipsbeyond pairwise formulation. For the second is-sue, we construct another vanilla graph to performhigh-frequency enhanced propagation, by adopt-ing a set of frequency filters (Dong et al., 2021;Bo et al., 2021), which distill different frequencyconstituents from node features. By adaptively inte-grating high-frequency enhanced messages, DGLFeffectively captures sarcastic inconsistencies in lo-cal neighborhoods, which is vital for MSD. More-over, we introduce multi-modal information bottle-neck (Wu et al., 2023; Zhu et al., 2024a) to effec-tively fuse the learned graph features, which nar-rows down the solution space, driving the modelsgaze toward shared modality information.Overall, our contributions are three-fold: (1) Wepropose DGLF, a novel dual graph-based learn-ing framework for MSD. To our best knowledge,we are the first to introduce hypergraph into MSD.(2) We construct a hypergraph and a vanilla graphto perform high-order aware and high-frequencyenhanced propagation, respectively. Besides, weintroduce multi-modal information bottleneck toeffectively fuse the learned graph features. (3) Ex-tensive experiments show that our model achievesnew state-of-the-art results, further analyses con-firm the effect of each component of our model.",
  "Methodology": "Task Definition.Given a sample Si from thetraining set, the objective of MSD is to determinewhether the sample implies any sarcasm by learn-ing a model f() using the text Ti and correspond-ing image Vi of Si. This conventional trainingprocedure is represented as yi = f(Ti, Vi | ) {0, 1}, where yi = 1 indicates the sample is pre-dicted to be sarcastic and vice versa; representsthe learnable model parameters. For simplicity, wetemporally omit the superscript i that indexes thetraining samples in the following.Next, before diving into the details of the pro-posed DGLFs architecture, we first introduce thefeature encoding (2.1) of modalities and construc-tion of the dual graphs (2.2) for the MSD task.",
  "For a fair comparison with previous works, givena textual sentence T = {t1, t2, . . . , tN} consist-": "ing of N words, we adopt the pre-trained BERTmodel (Devlin et al., 2019), to map each wordt into d-dimensional embedding,1 denoted asHt RNd. For a given image V RLhLw,following Liang et al. (2021); Liu et al. (2022),we first resize it to 224 224 pixels, i.e., L =Lh = Lw = 224. Then the image is divided intoM = p p patches2, w.r.t. V RM(L/pL/p).Subsequently, we feed the sequence of M imagepatches into a Vision Transformer (ViT) (Dosovit-skiy et al., 2021) with an MLP layer to acquire thevisual representation Hv RMd.",
  "Hypergraph Construction": "After we obtain textual and visual representationsHt and Hv through 2.1, we construct a hyper-graph G with edge-dependent node weights (Fenget al., 2019; Chitra and Raphael, 2019) from therepresentations as shown in (a, b).Mathematically, denote a hypergraph G=(V, E, , ) in which each node vi V( {t, v}, |V| = N + M) corresponds to a textualtoken vti or visual token vvi , where we initializethe node embeddings {vti, vvi } with the encodedrepresentations {hti, hvi }, respectively.For every hyperedge e E(|E| = 2+N +M), itencodes intra- or inter-modal dependencies. Specif-ically, each node vi ( {t, v}), where i spans therange 1 to N with N denoting the total numberof tokens in modality , is first connected to allother tokens in the same modality {vj|j = i, 1 j N} through a single intra-modal hyperedge.Here we obtain 2 intra-modal hyperedges. Further-more, each node vi is connected to all tokens inthe opposite modality {vk|1 k N}, with indicating the modality opposite to via an inter-modal hyperedge, where N represents the totalnumber of tokens in modality . Here we obtainN and M inter-modal hyperedges from N textualtokens and visual tokens, respectively. The sum ofN + M inter-modal hyperedges and 2 intra-modalhyperedges results in N + M + 2 hyperedges.Unlike prior methods (Liang et al., 2021, 2022; Liu et al., 2022) that resort to adjustments of edgeweighting strategies through complex relationshiplearning or similarity metrics, our DGLF embracessimplicity by adopting randomly initialized weight",
  "three inter-modal edges": ": The illustration of hypergraph (a, b) andvanilla graph (c, d), w.l.o.g, only the edges directedinto vv2, vt2, vv2 and vt2 are shown. As demonstrated,the hypergraph is adept at modeling high-order relation-ships, where four vertices in (a) (e.g., vv1, vv2, vv3 and vt2)are connected by a single hyperedge sharing the samecolor green; and in (c), the edge construction adheres tothe pairwise relationship assumption. values. Concretely, we introduce two distinct cate-gories of weights in G: (1) an edge weight (e)assigned to each hyperedge e, and (2) a nodeweight e(v) for every node v upon which hyper-edge e is incident, denoted as edge-dependent nodeweight (Chitra and Raphael, 2019). e(v) quanti-fies the significance of node v within hyperedgee, thereby reinforcing fine-grained intra- and inter-modal relationships. Denote A R|V||E| repre-sent the incidence matrix, in which a nonzero entryAve = 1 indicates that the hyperedge is incidentwith the node v; otherwise Ave = 0. Formally,edge-dependent node weights can be representedby a weighted incidence matrix A R|V||E|:",
  "e(v),if e is incident with node v,0,otherwise.(1)": "2.2.2Vanilla Graph ConstructionAs previously discussed, high-frequency infor-mation that reflects emotional discrepancies maybe more pivotal for MSD (Bo et al., 2021; Wuet al., 2019), and combining the power of mes-sages with different frequencies is worth explor-ing. This insight compels us to introduce a high-frequency enhanced propagation aimed at distillingvarying frequency importance. To this end, wefurther construct a vanilla graph G = {V, E}from the multi-modal encoded representations asshown in (c, d), in parallel with the hy-pergraph. Mathematically, denote a vanilla graphG = {V, E} whose nodes V are identical to theones in G, denoted with {vti , vvj }. The node em-beddings are similarly initialized with the encoded",
  "V(+1) = (D1G AWeB1 AV()),(2)": "in which V()={vi,()|{t, v}, i[1, N] when =t; i[1, M] when =v}, V()R|V|d is the input at layer ; is a non-linear activation function; We=diag((e1), . . . , (e|E|) denotes the hyperedgeweight matrix; DG R|V||V| and B R|E||E| denote the node degree and hyperedge degree ma-trix. In this manner, the high-order inter- and intra-modal relationships are gradually refined. After Literations, we get the outputs of the last iterationvi,(L) as the high-order aware representations.",
  "Fh = I D1/2GAD1/2G= L.(3)": "Note that Fh is equivalent to the normal-ized graph Laplacian matrix, which is consistentwith the theory that the Laplacian kernel can beemployed to highlight high-frequency informa-tion (Jain and Farrokhnia, 1991).Now, we employ these two filters to adaptivelyaggregate messages with varying frequencies. Tobe specific, we use a weighted sum to combinelow-frequency and high-frequency messages:",
  "|Ni|vj,(k),(5)": "where Ni denotes the neighboring nodes of nodei; wlij and whij denote the weight contributions ofnode js low-frequency and high-frequency mes-sages to node i, respectively with wlij + whij = 1.And they are calculated using a self-gating mecha-nism similar to Bo et al. (2021).By stacking K layers, each node receives thehigh-frequency enhanced messages, which are ig-nored by previous works, and we utilize outputsof the final layer vi,(K) as the high-frequency en-hanced representations.",
  "Information Bottleneck basedMulti-modal Fusion": "Ideally, concatenated representations should encap-sulate information shared across modalities. Thus,we introduce Multi-modal Fusion Information Bot-tleneck (MFB) (Wu et al., 2023; Mai et al., 2022),which effectively constrains the solution space tofocus more on the shared multi-modal informa-tion. Concretely, we first fuse different modalitiesto obtain concatenated high-order aware and high-frequency enhanced representations as follows:",
  "Training Objective": "After the above procedures, we follow Lianget al. (2021, 2022) to employ attention mecha-nism (Vaswani et al., 2017) based on H and V(resp. V) to obtain the high-order aware presen-tation f1 (resp. high-frequency enhanced presenta-tion f2). Now, we concatenate f1 and f2 to obtainthe final representation f, which is then fed into afully-connected layer with softmax normalizationto capture a probability distribution y Rdp ofsarcasm detection space as follows:",
  "Main Results": "Due to space limitation, we put experiment settingsin Appendix A.1. The performance comparison ofour model and baselines are shown in , fromwhich we have the following observations:(1) Our model achieves new state-of-the-art(SOTA) performance on all metrics and datasets.Specifically, on MMSD, DGLF overpasses HKE by1.62% and 2.91% on Acc. and F1; on MMSD2.0,it overpasses Att-BERT by 1.49% and 1.56% onAcc. and F1 respectively. This is because ourmodel captures complicated high-order relation-ships based on the hypergraph, and our designedhigh-frequency enhanced propagation based on thevanilla graph further improves the models abilityto detect inconsistencies in sarcasm.(2) The improvements on MMSD dataset aremuch sharper.This can be attributed to the factthat MMSD2.0 eliminated obvious sarcastic cues inMMSD, which has led to an obvious performancedecline in existing GNN-based methods that relyon edge weighting strategies with complex relation-ship learning or similarity metrics, sometimes evenunderperforming compared to Att-BERT. Thanksto the high-order and high-frequency informationcaptured, our model achieves consistent improve-ments over all baselines on both datasets.(3) Based on more advanced feature encoders(e.g., CLIP), our DGLF can still achieve signifi-cant improvements.We suspect the reason is thatthe advantages of our approach are orthogonal tothe ability of feature encoders. Our method canteach CLIP to model high-order relationships andhigh-frequency messages on dual graphs, whichcan hardly be learned in the pre-training process.",
  "DGLFCLIP (Ours)89.4385.81 89.27 87.5186.8281.90 89.85 85.69": ": Results comparison. denotes our re-implementation using the official code. - denotes missing resultsfrom the published work. Since CLIP-based methods use different pre-trained feature encoders, we gray out themfor a fair comparison. denotes the significance tests of DGLF and DGLFCLIP over baselines at p-value < 0.05. verify its effect, we design variant 1 as shown in. We can observe that Acc. drops by 1.95%on MMSD and 1.65% on MMSD2.0. Moreover,F1 drops more significantly: 2.16% on MMSDand 1.81% on MMSD2.0. This proves that model-ing the high-order relationships on the hypergraphG can naturally encode higher arity relationshipswithin and between visual and textual elements,thus significantly improving sarcasm detection.In 2.2.1, we define two types of weights inhypergraph G to capture the high-order relation-ships at a fine-grained level. To study its effect, weconduct the ablation experiments by removing dif-ferent sets of them. It can be seen that from variant2 to 4 in that removing either or both (i.e.,setting weight value (e) or/and e(v) as 1) leadsto performance decreases in all metrics on bothdatasets. This indicates that the formulated weightsin hypergraph G benefit the final performance. Effect of High-frequency Enhanced Propaga-tion.The aim of constructing another vanillagraph G is to perform high-frequency enhancedpropagation. To verify its effectiveness, we designvariant 5 by performing predictions using the high-order aware propagation on the hypergraph only.From , we find that variant 5 obtains dra-matic drops in all metrics on both datasets. The ap- parent performance gap verifies the high-frequencyenhanced messages in MSD, which can capture thevarying importance of sentiment discrepancy andcommonality within local neighborhoods.To further analyze the propagation of only high-frequency and low-frequency information in thevanilla graph G, we set Rl and Rh in Eqn. (4) tozero, respectively. From variant 6 and variant 7 in, we observe that erasing high-frequency in-formation results in a more noticeable performancedecline. This fully demonstrates the necessity ofhigh-frequency information for the MSD task andintuitively supports our motivation. Effect of Multi-modal Information Bottleneck.From variant 8, we can find that removingLMFB in final training loss leads to large perfor-mance decreases. Specifically, Acc. drops 1.28%and 0.56% on MMSD and MMSD2.0; F1 drops1.37% and 0.67% on MMSD and MMSD2.0. Thiscan verify the effectiveness of MFB, which con-strains the solution space of the two learned graphrepresentations to focus on cross-modal shared in-formation, thus boosting performance.",
  "-DGLF89.0186.9881.5278.60": "Effect of High-order Aware Propagation1w/o high-order aware propagation87.06 (1.95)84.82 (2.16)79.87 (1.65)76.79 (1.81)2w/o edge weight (e)87.25 (1.76)85.17 (1.81)80.81 (0.71)77.80 (0.80)3w/o node weight e(v)87.81 (1.20)85.69 (1.29)80.98 (0.54)77.97 (0.63)4w/o both weights87.08 (1.93)84.86 (2.12)80.35 (1.17)77.31 (1.29) Effect of High-frequency Enhanced Propagation5w/o high-frequency enhanced propagation86.73 (2.28)84.17 (2.81)79.51 (2.01)76.28 (2.32)6w/o low-frequency messages included88.07 (0.94)85.86 (1.12)80.67 (0.85)77.59 (1.01)7w/o high-frequency messages included87.84 (1.17)85.60 (1.38)80.46 (1.06)77.37 (1.23)",
  "(b)": ": (a) Performance variations when altering thevalue of in MFB loss (Eqn. (9)) on MMSD dataset.(b) Performance of using different pre-trained methodson MMSD. Base denotes without the proposed dualgraphs, i.e., only using BERT and ViT to conduct MSD. We varied the value of from 0.2 to 1.0 as per Eqn.(9) and the results are displayed in (a). Asdepicted, the performance remains relatively stableacross different values of , albeit with a slightdecreasing trend as increases. This suggests thatour model is largely insensitive to . However, as increases and the constraint tightens, there is agradual effect on the model performance.",
  ": Results of the proposed DGLF at differentgraph layers (a) L and (b) K on MMSD dataset": "search on the number of layers L and K, respec-tively. Specifically, we search them from 1 to 5on the validation set and the results on MMSDdataset are shown in . We observe thatthe effects of L and K are similar. At first, theresults steadily improve as stacking more layers,and peak at L = 2 and K = 3 respectively. Furtherstacking more layers has little positive impact, asit may incorporate noise from neighborhoods. ForMMSD2.0, we empirically found that the perfor-mance is not particularly sensitive to the numberof layers, displaying no specific pattern.",
  ": Case study": "with various pre-trained methods beyond CLIP, weconduct experiments using four variants that em-ploy different textual and visual encoders. Fromresults in (b), we find that the dual graphsare compatible with various pre-trained modelsand perform consistently outperform the baselinemodel without graphs. This verifies the effective-ness and generalizability of the proposed DGLF.Further, it is evident that using more sophisticatedpre-trained methods, such as ViT, BERT and CLIP,leads to superior performance. Comparison with Large Vision-Language Mod-els (LVLMs).To ascertain the competitive edgeof our model, we conducted comparative analy-ses against prevalent LVLMs including Qwen-VL-Chat (Bai et al., 2023), LLaVA-1.5 (Liu et al.,2024), and Gemini Pro (Team et al., 2023) follow-ing Wang et al. (2024). The obvious performancegap between ours and LVLMs in under-scores the persistent challenges that LVLMs en-counter in MSD, despite their advanced zero-shotlearning and chain-of-thought capabilities. It em-phasizes the need for dedicated efforts in designingeffective MSD frameworks. Combining the powerof LVLMs (Chen et al., 2024) to detect sarcasm isan interesting direction in future work.",
  ": Results on incorporating Optical CharacterRecognition (OCR) information on MMSD2.0 dataset": "Case (a).The sarcasm arises from the contrast be-tween the text, which suggests pleasant and warmconditions, and the image, which displays a muchcolder temperature typically not considered warm.Despite involving a more complex fusion architec-ture, HKE still employs a naive GNN propagationframework, transmitting low-frequency messagesbetween nodes. In contrast, our model obtains high-order and high-frequency messages from both thehypergraph and the vanilla graph, and accuratelycaptures the inter-modal inconsistencies, therebycorrectly identifying it as sarcastic. Case (b).The sarcasm likely stems from the ex-aggeration between the text and the image. The textdescribes waking up extremely early as reward-ing, which contrasts sharply with the image ac-companying it, depicting the reality of driving earlyin the morning. Unfortunately, previous SOTAGNN-based methods still predict it as non-sarcastic.Thanks to the proposed dual graph framework, ourmodel can propagate high-frequency messages thatreflect discrepancies in the graph, thereby effec-tively capturing inter-modality inconsistencies andachieving precise prediction.",
  "Error Analysis": "We further conduct error analysis to understandDGLFs performance. We observe that the major-ity of errors occur in samples where images containimportant textual information, such as the purelytextual image in (a) and in (b)which includes both textual and visual expressions.Based on these observations, we conducted a pre-liminary experiment to leverage textual informationwithin images by integrating OCR into DGLF forMSD. From the results in , we find a no-",
  "Related Work": "Multi-modal Sarcasm Detection.With the rapidpopularization of social media platforms, multi-modal sarcasm detection (MSD) has garnered in-creasing research attention in recent years (Zhuet al., 2024c; Wang et al., 2024).Some earlyworks (Xu et al., 2020; Pan et al., 2020; Xin et al.,2024) focused more on contextual dependenciesand utilized feature concatenation for multi-modalmodeling. More recently, researchers formulatedthe MSD task upon GNNs, which have shownpromising results. Therein, InCrossMGs (Lianget al., 2021) proposed in-modal and cross-modalgraphs to determine the sentiment inconsistencies.Based on this, CMGCN (Liang et al., 2022) ex-plored a cross-modal graph to model the contra-dictory sentiments between key textual and visualinformation. HKE (Liu et al., 2022) further in-troduced a GNN-based hierarchical framework byexploring both the atomic-level congruity and thecomposition-level congruity.Nevertheless, these GNN-based MSD modelsstill deliver insufficient high-order relationshipsand high-frequency messages, as we discussed. Graph Neural Networks.GNNs can explicitlymodel data relationships, which have been widelyemployed in various applications such as sentimentanalysis and argument pair extraction (Li et al.,2021; Chen et al., 2023; Sun et al., 2023; Zhu et al.,2024b). GNNs have also inspired MSD researchersand offer a new solution for the MSD task, fromunimodal setting (Lou et al., 2021) to multi-modalscenario (Qin et al., 2023).However, previous works fail to address the gen-eral limits of GNNs, which motivates our work. Multi-modal Information Bottleneck.The In-foMax principle proposed by Linsker (1988) seeksto maximize the mutual information between fea-ture and model output. Along this way, Han et al.(2021) built up a hierarchical mutual informationmaximization guided model for multi-modal senti-ment analysis. Wu et al. (2023) focused on video-based sentiment analysis and used contrastive learn-ing to achieve mutual information maximization.In this work, we utilize the lower bound of multi-modal information bottleneck (Mai et al., 2022)",
  "Conclusion": "In this paper, we propose a new GNN-based frame-work for MSD. We construct a hypergraph and avanilla graph to perform high-order aware propa-gation and high-frequency enhanced propagation,respectively. Based on this, we introduce multi-modal information bottleneck to effectively fusethe two learned graph representations. Extensiveexperiments and analyses on two MSD benchmarksshow the superiority of our proposed framework.",
  "Limitations": "Our DGLF has the following limitations: (1) Theproposed dual graph approach (combining hyper-graph and vanilla graph) might lead to increasedcomputational complexity. (2) The effectivenessof the DGLF might be contingent upon the qual-ity of the underlying pre-trained models (BERT,ViT), and can benefit from more advanced fea-ture encoders, which is not the focus of this work.(3) DGLF lacks validation on more diverse multi-modal MSD datasets that include additional modal-ities such as audio and video, as well as testingacross various other tasks, which may limit itsbroader generalizability and effectiveness.",
  "fusion model. In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 25062515. Association for Compu-tational Linguistics": "Feiyu Chen, Jie Shao, Shuyuan Zhu, and Heng TaoShen. 2023. Multivariate, multi-frequency and multi-modal: Rethinking graph neural networks for emo-tion recognition in conversation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1076110770. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou,Chenhang Cui, Zhenzhen Weng, Haoqin Tu, ChaoqiWang, Zhengwei Tong, Qinglan Huang, et al. 2024.Mj-bench: Is your multimodal reward model reallya good judge for text-to-image generation? arXivpreprint arXiv:2407.04842.",
  "Uthsav Chitra and Benjamin Raphael. 2019. Randomwalks on hypergraphs with edge-dependent vertexweights. In International conference on machinelearning, pages 11721181. PMLR": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, NAACL-HLT 2019, Minneapolis, MN, USA,June 2-7, 2019, Volume 1 (Long and Short Papers),pages 41714186. Association for ComputationalLinguistics. Yushun Dong, Kaize Ding, Brian Jalaian, Shuiwang Ji,and Jundong Li. 2021. Adagnn: Graph neural net-works with adaptive frequency response filter. In Pro-ceedings of the 30th ACM international conferenceon information & knowledge management, pages392401. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. In 9th International Conferenceon Learning Representations, ICLR 2021, VirtualEvent, Austria, May 3-7, 2021. OpenReview.net.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: Amethod for stochastic optimization. arXiv preprintarXiv:1412.6980": "Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi-aojie Wang, and Eduard Hovy. 2021. Dual graphconvolutional networks for aspect-based sentimentanalysis. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 63196329. Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang,and Ruifeng Xu. 2021. Multi-modal sarcasm de-tection with interactive in-modal and cross-modalgraphs. In MM 21: ACM Multimedia Conference,Virtual Event, China, October 20 - 24, 2021, pages47074715. ACM. Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui,Yulan He, Wenjie Pei, and Ruifeng Xu. 2022. Multi-modal sarcasm detection via cross-modal graph con-volutional network. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), ACL 2022,Dublin, Ireland, May 22-27, 2022, pages 17671777.Association for Computational Linguistics.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024. Visual instruction tuning. Advances inneural information processing systems, 36": "Hui Liu, Wenya Wang, and Haoliang Li. 2022. To-wards multi-modal sarcasm detection via hierarchicalcongruity modeling with knowledge enhancement.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages49955006. Association for Computational Linguis-tics. Chenwei Lou, Bin Liang, Lin Gui, Yulan He, YixueDang, and Ruifeng Xu. 2021. Affective dependencygraph for sarcasm detection.In Proceedings ofthe 44th international ACM SIGIR conference onresearch and development in information retrieval,pages 18441849.",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.Representation learning with contrastive predictivecoding. arXiv preprint arXiv:1807.03748": "Hongliang Pan, Zheng Lin, Peng Fu, Yatao Qi, andWeiping Wang. 2020.Modeling intra and inter-modality incongruity for multi-modal sarcasm de-tection. In Findings of the Association for Compu-tational Linguistics: EMNLP 2020, Online Event,16-20 November 2020, volume EMNLP 2020 of Find-ings of ACL, pages 13831392. Association for Com-putational Linguistics. Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. GloVe: Global vectors for wordrepresentation. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 15321543, Doha, Qatar.Association for Computational Linguistics. Soujanya Poria, Erik Cambria, Devamanyu Hazarika,and Prateek Vij. 2016. A deeper look into sarcastictweets using deep convolutional neural networks. InProceedings of COLING 2016, the 26th InternationalConference on Computational Linguistics: TechnicalPapers, pages 16011612. Libo Qin, Shijue Huang, Qiguang Chen, Chenran Cai,Yudi Zhang, Bin Liang, Wanxiang Che, and RuifengXu. 2023. MMSD2.0: towards a reliable multi-modalsarcasm detection system. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 1083410845. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Ellen Riloff, Ashequl Qadir, Prafulla Surve, LalindraDe Silva, Nathan Gilbert, and Ruihong Huang. 2013.Sarcasm as contrast between a positive sentimentand negative situation. In Proceedings of the 2013conference on empirical methods in natural languageprocessing, pages 704714. Rossano Schifanella, Paloma De Juan, Joel Tetreault,and Liangliang Cao. 2016. Detecting sarcasm inmultimodal social platforms. In Proceedings of the24th ACM international conference on Multimedia,pages 11361145. Xiangguo Sun, Hongzhi Yin, Bo Liu, Hongxu Chen,Jiuxin Cao, Yingxia Shao, and Nguyen QuocViet Hung. 2021. Heterogeneous hypergraph embed-ding for graph classification. In Proceedings of the14th ACM international conference on web searchand data mining, pages 725733. Yang Sun, Bin Liang, Jianzhu Bao, Yice Zhang, GengTu, Min Yang, and Ruifeng Xu. 2023. Probing graphdecomposition for argument pair extraction. In Find-ings of the Association for Computational Linguistics:ACL 2023, pages 1307513088, Toronto, Canada. As-sociation for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Peng Wang, Yongheng Zhang, Hao Fei, Qiguang Chen,Yukai Wang, Jiasheng Si, Wenpeng Lu, Min Li, andLibo Qin. 2024. S3 agent: Unlocking the powerof vllm for zero-shot multi-modal sarcasm detec-tion. ACM Trans. Multimedia Comput. Commun.Appl. Just Accepted. Felix Wu, Amauri Souza, Tianyi Zhang, ChristopherFifty, Tao Yu, and Kilian Weinberger. 2019. Simpli-fying graph convolutional networks. In Internationalconference on machine learning, pages 68616871.PMLR. Shaoxiang Wu, Damai Dai, Ziwei Qin, Tianyu Liu,Binghuai Lin, Yunbo Cao, and Zhifang Sui. 2023.Denoising bottleneck with mutual information max-imization for video multimodal fusion. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 22312243, Toronto, Canada. Association forComputational Linguistics.",
  "the Association for Computational Linguistics, pages37773786, Online. Association for ComputationalLinguistics": "Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.Tweet sarcasm detection using deep neural network.In Proceedings of COLING 2016, the 26th Inter-national Conference on Computational Linguistics:technical papers, pages 24492460. Zhihong Zhu, Xuxin Cheng, Zhaorun Chen, YuyanChen, Yunyan Zhang, Xian Wu, Yefeng Zheng, andBowen Xing. 2024a. Inmu-net: Advancing multi-modal intent detection via information bottleneckand multi-sensory processing. In ACM Multimedia2024. Zhihong Zhu, Xuxin Cheng, Hongxiang Li, YaoweiLi, and Yuexian Zou. 2024b.Dance with labels:Dual-heterogeneous label graph interaction for multi-intent spoken language understanding. In Proceed-ings of the 17th ACM International Conference onWeb Search and Data Mining, WSDM 24, page10221031, New York, NY, USA. Association forComputing Machinery. Zhihong Zhu, Xianwei Zhuang, Yunyan Zhang, DerongXu, Guimin Hu, Xian Wu, and Yefeng Zheng. 2024c.Tfcd: Towards multi-modal sarcasm detection viatraining-free counterfactual debiasing. In Proceed-ings of the Thirty-Third International Joint Confer-ence on Artificial Intelligence, IJCAI-24, pages 66876695. International Joint Conferences on ArtificialIntelligence Organization. Main Track.",
  "A.1Experimental Settings": "Datasets.We conduct experiments on twowidely-used benchmark datasets: MMSD (Caiet al., 2019) and MMSD2.0 (Qin et al., 2023).Specifically, MMSD is derived from English Twit-ter.Thereinto, tweets with some special hash-tags (e.g., sarcasm) are positive examples andthose without such hashtags are negative examples.MMSD2.0 is updated from MMSD, which is themost advanced benchmark in MSD. The statisticsof these two datasets are shown in .",
  ": Statistics of two experimental datasets": "embed each visual patch as a 768-dimensional em-bedding, i.e., d = 768. Adam (Kingma and Ba,2014) is utilized as the optimizer with a learningrate of 2e-5, and the mini-batch size is 16. Thecoefficient is set to 1e-5. We test L and K inthe range from 1 to 5 on the validation set andchoose the best-performing one to the test set, re-spectively. The hyper-parameter is set as 0.2.Paired t-test is performed to test the significanceof performance improvement with a default signifi-cance level of 0.05. All experiments are conductedon one NVIDIA GeForce RTX 3090. The resultsreported in all experiments are averages of 5 runswith different random seeds to ensure the final re-ported results are statistically stable.",
  "A.2Model Zoo": "We compared our proposed DGLF with a series ofstrong baselines, which can be broadly classifiedinto three main categories:(1) Text-only methods. These methods purelyrely on textual information for sarcasm detection,including TextCNN (Kim, 2014), a deep learn-ing model based on CNN; Bi-LSTM (Graves andSchmidhuber, 2005), a bidirectional LSTM net-work for text classification; SMSD (Xiong et al.,2019) explored a self-matching network to capturetextual incongruity information; and BERT (De-vlin et al., 2019), the vanilla pre-trained uncasedBERT-base taking [CLS] text [SEP] as in-put. (2) Image-only methods. The sarcasm detec-tion in these methods relies solely on image input,including ResNet (He et al., 2016) which trainsa sarcasm classifier; and ViT (Dosovitskiy et al.,2021), which utilizes the [class] token repre-sentations to detect the sarcasm. (3) Multi-modalmethods. These methods utilize both visual andtextual information for sarcasm detection, includ-ing HFM (Cai et al., 2019) introduced a hierarchi-cal fusion model for MSD; D&R Net (Xu et al.,2020) proposed a decomposition and relation net-work modeling both cross-modality contrast andsemantic association; Att-BERT (Pan et al., 2020)explored inter-modality attention and co-attention to model the incongruity of multimodal informa-tion; InCrossMGs (Liang et al., 2021) leveragedthe sarcasm relations from both intra- and inter-modality perspectives using local multi-modal fea-tures; CMGCN (Liang et al., 2022) explored thesarcastic relations across objects of the image andtokens of the text; HKE (Liu et al., 2022) utilizedboth the atomic-level congruity based on cross at-tention and the composition-level congruity basedon GNNs; and Multi-view CLIP (Qin et al., 2023)employed the pre-trained CLIP (Radford et al.,2021) model to detect different sarcastic cues cap-tured from multiple perspectives.Further, to investigate the effectiveness of ourDGLF when used with different pre-trained models,we also set the following variants: -GloVe+ViT:We replace pre-trained BERT in our proposedframework with GloVe (Pennington et al., 2014) toinitialize each word into a 300-dimensional embed-ding and utilize ViT for learning image-modalityrepresentations. -BERT+ResNet: Following Panet al. (2020), we replace the ViT in our frame-work with ResNet-152 (He et al., 2016) to em-bed each image patch as a 2048-dimensional vec-tor. -GloVe+ResNet: We use GloVe to acquireword embeddings and employ ResNet for learningimage-modality representations."
}