{
  "Abstract": "Eliciting chain of thought (CoT) rationalessequences of token that convey a reasoningprocesshas been shown to consistently im-prove LLM performance on tasks like questionanswering. More recent efforts have shownthat such rationales can also be used for modeldistillation: Including CoT sequences (elicitedfrom a large teacher model) in addition totarget labels when fine-tuning a small studentmodel yields (often substantial) improvements.In this work we ask: Why and how does thisadditional training signal help in model dis-tillation? We perform ablations to interrogatethis, and report some potentially surprising re-sults. Specifically: (1) Placing CoT sequencesafter labels (rather than before) realizes con-sistently better downstream performancethismeans that no student reasoning is necessaryat test time to realize gains. (2) When ratio-nales are appended in this way, they need notbe coherent reasoning sequences to yield im-provements; performance increases are robustto permutations of CoT tokens, for example.In fact, (3) a small number of key tokens aresufficient to achieve improvements equivalentto those observed when full rationales are usedin model distillation.",
  "Introduction": "Chain of thought (CoT) reasoningi.e., gen-erating tokens which communicate step-by-stepthinkingcan (sometimes dramatically) improvemodel performance on reasoning tasks (Wei et al.,2023). In the context of model distillation (Hintonet al., 2015), recent work has elicited such ratio-nale chains from massive LLMs (e.g., GPT-4) toaugment data with which to fine-tune much smaller(<2B parameters) task-specific models. illustrates this distillation approach: The studentmodel is trained to generate the rationales in addi-tion to the target token(s).This simple CoT-augmented distillation strategyconsistently and sometimes dramatically improves the performance of student models (Ho et al., 2023).For example, Li et al. (2023a) used rationales fromGPT-3 (175B) to teach a comparatively tiny studentLM (OPT-1.5B) to produce similar reasoning to-ken sequences at inference time. They show anaverage increase in task accuracy of 12.4% acrossthree commonsense reasoning datasets. Shridharet al. (2023) adopted a similar approach to fine-tune GPT-2 (large; 774M) on grade-school mathdatasets with improvements of 8.23% on GSM8Kand 16.20% on SVAMP. Beyond commonsensereasoning, Wadhwa et al. (2023) achieved SOTAresults (+6.23 absolute gain in micro-F1, on aver-age) with a distilled model for relation extractionby exploiting CoT rationales.In this work we ask: Why does distillation withCoT augmented targets consistently improve theperformance of distilled LMs? One might naivelysuspect that the student model benefits from learn-ing to mimic the relevant reasoning process. Butwe find that it is not the case that student modelsbenefit from reasoning at inference time.Rather, consistent with contemporaneous work(Chen et al., 2024), we observe that placing CoTsequences after target tokens for distillation actu-ally improves student performance (compared towhen CoT is pre-prended to labels). This means thestudent model need not bother generating its rea-soning at test time, as the label will be generatedahead of this anyway. Further, we find that ratio-nale grammatically is not necessary; one can shuf-fle rationale tokens and/or include only importanttokens from chains of thought during distillationand still realize performance benefits equivalent tothose observed when using the full rationales.Through ablations with three small student LMs(GPT-2, Phi-1.5, and Gemma-2B), we report thefollowing, sometimes counter-intuitive, findingsregarding CoT-augmented distillation and how ra-tionales benefit student models. We summarize ourkey findings as",
  "Pre CoTSupervised Fine-Tuning": ": For RQ1, we investigate augmenting CoTrationales obtained by very large (teacher) languagemodels like Mistral, after the target labels. In doing so,we inject the same CoT reasoning ability during super-vised fine-tuning (SFT) but do not condition generationof target label on the CoT itself at inference time. 1. CoT-augmented distillation works betterwhen rationales are provided after labels.Standard CoT reasoning elicited zero-shotfrom massive LMs yields rationales as pre-fixes that logically lead to the label token(s).But we find that smaller models perform con-sistently better when rationales follow labelsin distillation targets. 2. When appended to target labels, token-levelorder, length, and coherence of rationalesdoes not matter. However, these things domatter when rationales are preprended. Whenthe rationales are placed before the final la-bel during fine-tuning, masking, shuffling, oraltering coherent rationales significantly de-grades model performance. 3. Motivated by the preceding observations, werun controlled experiments to establish thatthere are certain key, contextual tokens thatconnect the input to the final label, and ap-pending these tokens to labels is sufficientto achieve performance on-par with coher-ent CoT-like rationales. It is solely the pres-ence of these tokens at training time that leadsto downstream performance improvements.",
  "Experimental Design": "CoT-augmented distillation entails eliciting ratio-nales from a large teacher model and using these asadditional training signal for a small student model.Rationales here comprise the logical steps taken to reach a response from a given input.1 Theseare inserted into distillation training targets, andthe student model is in this way taught to generatereasoning in addition to labels.This has been shown empirically to provide(sometimes dramatic) performance benefits (Liet al., 2023a). But why? What accounts for the suc-cess of CoT-augmented distillation? In this workwe investigate the following questions about therole of CoT-rationales in distillation. (RQ1) Doesthe placement of the reasoning chain relative tothe target label (pre- or post-) matter? Relatedly,might observed performance gains owe to simplyallowing the student model additional compute dur-ing inference? (RQ2) Must rationales feature logi-cal and coherent chain-of-thought reasoning, orcould we, e.g., scramble the ordering of tokens andstill observe improvements? Finally, (RQ3) couldwe realize the same benefits in distillation usingonly a handful of key tokens from rationales, rathercomplete reasoning sequences?To answer these questions empirically, we es-tablish baseline student LM performance, and thencompare this to ablated variants of CoT-augmentedmodels. We use a fixed ICL prompt (AppendixB) with the input and target label to elicit apossible rationale for each instance in a dataset.We use Mistral-7B-Instruct (Jiang et al., 2023)as the teacher model and GPT-2 (Radford et al.,2019), Gemma-2B (Team et al., 2024) and Phi-1.5 (Li et al., 2023b) as student models. Note thatone could instead replace Mistral-7B-Instruct withGPT-4 (or any other LLM capable of generatingCoT-style rationales in ICL settings) as the teachermodel. See Appendix B for the prompt used toelicit rationales for training instances of all datasetsused in our work.Following prior related work (Wei et al., 2023; Li et al., 2023a), we select three commonsense rea-soning datasets: CommonsenseQA (Talmor et al.,2019), OpenbookQA (Mihaylov et al., 2018), andQuaRel (Tafjord et al., 2018). Each dataset pro-vides an input consisting of a question, and a pre-defined set of answer choices. The target labels arethe correct answer choices (Appendix A).",
  ": Comparison of decoder-only models perfor-mance under baseline supervised fine-tuning (no CoT),standard (pre) CoT, and postfix CoT": "with a learning rate of 3e-5, batch size of 4 for Com-monsenseQA and OpenBookQa, and 8 for QuaRel,with a maximum input length of 512, maximumoutput length of 256. We evaluated checkpointsevery 500 steps with early stopping (patience = 10,threshold = 0.02). Because we are only interestedin measuring relative performance of fine-tunedmodels across ablations (as opposed to necessarilyrealizing SOTA performance), we left the remain-ing hyperparamters to their default values.",
  "RQ1: Positioning of Rationales": "Does it matter if we place CoT rationales before orafter target labels prior to distillation? Prior work(Wei et al., 2023) which elicited CoT reasoningfrom LLMs at inference time found that generatingthe chain after the final label performs comparablyto the baseline (i.e., no CoT). This would seemto suggest that reasoning at inference time iswhat yields improvements, but it is unclear whetherthis holds in the context of model distillation. Wecompare the performance of student LMs distilledfrom examples with rationales placed both beforeand after labels ().",
  "CoT before Label Friction is higher onrougher....[FIN_LABEL] B [FIN_LABEL]CoT after Label [FIN_LABEL] B[FIN_LABEL] Friction is higher onrougher": "We find that generating the CoT rationale beforethe label under-performs generating the rationaleafter the label. These findings are consistent acrossmodels and datasets (). Note that modelstrained to generate a CoT after the target label, donot need to do so at inference time. While in gen-eral CoT elicited from massive models is thoughtto improve performance by enabling explicit rea- soning, gains offered in the context of distillationmust be realized via some other mechanism (e.g.,enriched training signal).Next, we examine how and with what confidencedo models fine-tuned under different conditions en-code label information. We use ideas from Log-itLens (nostalgebraist, 2020), TunedLens (Belroseet al., 2023), and FutureLens (Pal et al., 2023),which suggest that decoder-only models think it-eratively and can be probed by inducing a dis-tribution over the output vocabulary conditionedon hidden states to measure model confidence atdifferent layers and time-steps within the model.For each dataset, we look at test instances thatare correctly predicted by all three model types,i.e., models distilled using: (i) No CoT; (ii) CoTbefore label; and (iii) CoT after label. illustrates model confidences (i.e., prob-abilities computed with a softmax over the LM-head predictions for the final label) at differentlayers and time points, up to and including the finallabel prediction.2 In 80% of correctly predicted outputs, modelstrained with rationales appended to the final la-bel (right-most subplot, ) correctly predictthe label with probability > 0.6 at layer 32 andabove. By contrast, models trained without anyrationales (left-most subplot) lack such confidence,especially at lower layers: Final label probabilitydoes not exceed 0.6 until layer 44. Finally, formodels trained with rationales prepended to targetlabels (middle sub-plot), the probability of the truelabel is 0.6 until layer 39 in 80% of correctly pre-dicted instances. In sum, this analysis (illustratedin ) reveals a clear difference: Added CoT-information during distillation yields models whichare more confident earlier on (positionally and lay-erwise) in the final output.",
  "Is it just the extra compute?Prior work by": "Goyal et al. (2024) observed performance improve-ments in LLMs when inputs were augmented withdummy tokens (at pretraining and inference time),suggesting that LLMs benefit from additional com-pute cycles. Here we investigate whether it is justthe added compute (i.e., steps/gradient updates overtarget label during fine-tuning) that provides gainscomparable to those achieved with CoT, or if itis the CoT rationales contain useful information.Instead of CoT rationales, we prepend a fix-sized",
  ": Performance of GPT-2 with constant numberof <unk> tokens prepended to the target label": "sequence of <unk> tokens to the target label andablate over the sequence length. summarizes our results with GPT-2 asthe student model; similar to Goyal et al. (2024)we observe that adding compute steps during train-ing leads to (sometimes substantial) improvementsin downstream performance. However, beyond acertain point ( 11 <unk> tokens) performanceplateaus, and then eventually declines. More im-portantly, at no point does the model outperform aCoT baseline (), suggesting that CoT ratio-nales do indeed incorporate information necessaryto achieve downstream improvements.",
  "In light of our findings from RQ1, we next inves-tigate what specific information in CoT rationalesimproves downstream performance. To this end,": "we assess how robust student models are to per-turbations of provided rationales. Specifically, weconsider: (i) Shuffling tokens within a rationale;and (ii) Incrementally masking tokens while retain-ing their relative order. ShufflingWe start by testing the robustness ofstudent LMs with respect to the coherence of ratio-nales. In particular, we shuffle tokens comprisingrationales at the instance level. To illustrate this,consider the following example. Question: If you hired a pitcher, (A) anerd (B) a bodybuilder, who likely canpitch a baseball faster?Original CoT Rationale: The answer is Bbecause bodybuilders typically have morestrength than nerds, which couldtranslate into a greater ability tothrow a baseball faster. [FIN_LABEL] B[FIN_LABEL]Shuffled Rationale: Baseball a fasterthrow to ability greater a intotranslate could which nerds thanstrength more have typicallybodybuilders because B is answer The.[FIN_LABEL] B [FIN_LABEL] We then train the student LM with these shuf-fled rationales in place of the original (coherent)versions, under both pre- and post-label settings. summarizes our findings from these exper-iments. We see that prepending the shuffled CoTrationales to target labels leads to sharp decline inperformance, whereas appending these to targetlabels has nearly no effect on subsequent model",
  ": Comparison of model performance while successively reducing the amount of available information in aCoT rationale through masking": "performance.Taken together with the results from RQ1, wehypothesize that this may be because prepending ra-tionales to target labels during distillation requiresthe student model to learn to generate coherentrationales in addition to producing correct labels.By contrast, when rationales are appended theycan serve as additional supervision during trainingwithout requiring coherent rationale generation atinference time. MaskingNext we run an ablation intended totest whether the full rationales are needed or if asubset of words is sufficient to realize the observedbenefits. We start by randomly masking varyingfractions of tokens within a rationale. For example: Question: If you hired a pitcher, (A) anerd (B) a bodybuilder, who likely canpitch a baseball faster?Original CoT Rationale: The answer is Bbecause bodybuilders typically have morestrength than nerds, which couldtranslate into a greater ability tothrow a baseball faster. [FIN_LABEL] B[FIN_LABEL]10% Masked: The answer is [MASK] becausebodybuilders typically have more [MASK] than nerds, which could translate into agreater ability to throw a baseball[MASK]. [FIN_LABEL] B [FIN_LABEL]50% Masked: The [MASK] is B [MASK]bodybuilders typically [MASK] more[MASK] than nerds, [MASK] couldtranslate [MASK] a greater [MASK] tothrow [MASK] baseball [MASK].[FIN_LABEL] B [FIN_LABEL]90% Masked: [MASK] [MASK] [MASK] B[MASK] [MASK] [MASK] [MASK] [MASK][MASK] [MASK] [MASK], [MASK] [MASK][MASK] [MASK] [MASK] [MASK] [MASK][MASK] [MASK] [MASK] [MASK] [MASK][MASK] [MASK] [MASK] [MASK] [MASK][MASK] faster. [FIN_LABEL] B [FIN_LABEL] We vary the proportion of masked tokens from 10%to 90% (in increments of 10-15%) and again testunder both pre- and post-label settings (see RQ1). reports performances as a function ofthe proportion of masked tokens as compared to anon-CoT baseline. When only a small fraction (upto 20%) of rationale tokens are masked, we ob-serve only marginal performance declines in bothsettings. However, as the proportion of maskedCoT tokens increases (40%+), we see rapid perfor-mance decline in the CoT before label case at",
  ": Comparison of model performance under differ-ent attribution methods relative to retaining full lengthpost-label CoT rationales": "60% masking, we find that the resultant distilledmodel performs worse than the baseline (i.e., with-out CoT).We observe that masking a high percentage oftokens prior to the label yields models that generatea variable but often large number of [MASK] tokensprior to target label, often reaching maximum out-put length (set as a decoding hyperparameter). Incontrast, in the CoT after label setting we observegains over the non-CoT distillation baseline up un-til a high fraction (> 60%) of tokens are masked;and beyond this point, the performance matchesthe vanilla (non-CoT) baseline.",
  "RQ3: Attribution from Rationales": "Having established that placing rationales after la-bels yields the best performance when performingCoT-augmented distillationeven without the fullreasoning chainwe now ask whether we can finda small subset of important tokens that are suffi-cient to realize performance benefits. To determineimportance, we consider both gradient-based attri-bution and human annotations. Attribution via integrated gradientsis amethod to estimate the importance of individualtokens with respect to model output (Sundarara-jan et al., 2017). To measure the relative impor-tance of rationale tokens on the final target label,we start with a baseline model (GPT-2) which is fine-tuned to generate a CoT-rationale beforethe final label.Considering a token sequencex = [x1, x2, . . . , xm, . . . , xn] where x1,...,m, cor-respond to the input tokens; xm+1,...,n1 are therationale tokens; and xn is the final target label,we compute an approximation of the integratedgradients for the i-th rationale token as:",
  "p": "where, x is a baseline (zero vector or a neutralinput), f(x) represents the models output and pis the granularity for the approximation. See Sun-dararajan et al. 2017 for details.The average length of a CoT rationale in our datais 36.3 tokens and we retain the top 15 tokens withthe highest attribution scores. illustratesthe process of computing a set of important tokensfor the target label. As an example from QuaRelstraining set3 Question: If you hired a pitcher, (A) anerd (B) a bodybuilder, who likely canpitch a baseball faster?Original CoT Rationale: [FIN_LABEL] B[FIN_LABEL] The answer is B becausebodybuilders typically have morestrength than nerds, which couldtranslate into a greater ability tothrow a baseball faster.Attributed Tokens: [FIN_LABEL] B[FIN_LABEL] B because body builders morestrength translate throw baseball fasterShuffled Attributed Tokens:[FIN_LABEL] B[FIN_LABEL] translate because more bodyB faster builders baseball strengththrow We fine-tune the models again, appending theattributed tokens to the final label, similar to thecase where CoT rationales are generated after thelabel at inference. summarizes our findings.We broadly observe that reducing the total numberof tokens in CoT rationale via gradient attributionleads to no significant difference in downstreammodel performance. This is consistent with ourfindings in RQ2, where masking a majority of CoTtokens appended to the target label did not signifi-cantly effect model performance.",
  "Question: Jason tried providing (A) less friction (B) more friction?": ": Comparison of Attribution Methods: Left side we have automated extraction via Integrated Gradientswhile the right side displays manually annotated words perceived by human annotators to be relevant. Human annotationsAs an alternative to scoringrationale tokens via gradient attribution, we eval-uate using tokens that humans perceive to be themost relevant to target labels. We hire annotatorson Prolific4 to identify minimal sets of (up to 15)words in rationales necessary to answer the ques-tion ().5 We first ran a small internal pilotto estimate time required and set fair pay rates.6",
  "Next we collected annotations for 2k instances ofthe QuaRel dataset in batches of 200 from crowd-workers fluent in English. We manually verified10% (20 instances) of each batch to ensure quality.7": "Replacing CoT rationales with words deemedimportant by annotators offers some gains, butsmaller than those from integrated gradients (Ta-ble 3). To measure overlap between tokens se-lected manually and via gradient attribution, weassume the latter to be the reference tokens andmeasure Precision (0.73) and Recall (0.59) of an-notated words8 In general, we find the set of tokensidentified through gradient attribution to be muchmore comprehensive than those selected by humanannotators.",
  "collect such tokens is to select from a large set ofwords a subset that have high similarity to the tar-get label token. To this end we use static Word2Vec(Mikolov et al., 2013) embeddings.9": "We select the 15 closest words to the target labelfor all training instances. For target labels withmultiple tokens, we take similarity with respect toonly the longest token. We then use these retrievedtokens in lieu of CoT rationales when fine-tuningstudent models. The question is whether the addi-tional information encoded in words similar to thetarget label yield performance gains comparable toCoT augmented distillation. reports results,which are largely negative for this experiment: Theobserved performance with relevant word aug-mentation is comparable to the no-CoT setting(baseline). This suggests that while rationales neednot be coherent to realize benefits, the tokens theycomprise must offer additional signal beyond be-ing simply similar to (in terms of co-occurence)target label tokens.",
  "Related Work": "Distillation via elicitationWest et al. (2022) con-sidered symbolic distillation where instead ofdistilling from soft representations like logits, theyproposed the use of LLMs as data generators tobe used to augment training data. Other recentwork has shown that explanations can serve as bothinputs (Hase and Bansal, 2022) and targets (Wiegr-effe et al., 2022), and can be used downstream toimprove task- (Wadhwa et al., 2023) and domain-specific (Ho et al., 2023) model performance.",
  "9word2vec-google-news-30010; trained on the GoogleNews dataset of 100 billion words": "Li et al. (2023a) first explored distillation perfor-mance to tasks like commonsense reasoning andprovided analyses intended to reveal factors thatmay be important in creating the teacher corpus,upon which our work builds on. Beyond directlyusing explanations-style rationales for fine-tuning,Deng et al. (2023) explored an alternative approachby using model hidden states to perform implicitreasoning, instead of producing rationale tokensone-by-one (i.e. Next Token Prediction), demon-strating that the chains of thought themselves maynot be fully necessary to achieve downstream fine-tuning performance gains.Our work deepens these efforts by focusing onanalyzing specific fine-tuning for distillation dy-namics in smaller models, and characterizing whenrationales generated by teacher LLMs are helpful. CoT with Small ModelsIn-context CoT prompt-ing (Wei et al., 2023) induces thinking step-by-step,such that the model generates intermediate reason-ing ultimately leading to a target label. Prior work(Ho et al., 2023; Magister et al., 2023) has shownthat small models may not be not inherently capa-ble of generating these reasoning chains, but canbe taught to do so using augmented training sets.Creating CoT-augmented training sets can be ex-pensive, and a number of prior works in the areahave investigated synthetic data generation. Forinstance, Hsieh et al. (2023) generate new targetlabels from few instances of labeled data. Li et al.(2023a) notably found that sampling multiple ratio-nales can improve small-model performance. Hanet al. (2023) decomposed the reasoning steps intomulti-round dialog and optimize for the correctpath using PPO algorithm while training smallermodels. Fu et al. (2023) emphasize the trade-offsbetween task-specific CoT-generation capability insmall models and their generalizability. Wang et al.(2023) establish the effect of faithfulness of elicitedrationales on the student models trained using them.A shared theme in these past papers have been thatthey explicitly look at improving the quality of ra-tionales themselves and its downstream effects onoverall model performance.Our work differs from these efforts looking onlyat the final label, manipulating the CoT rationalesat distillation (fine-tuning) time to probe how ratio-nales effect model performance.",
  "Contemporaneous WorkWhile engaged in thiswork, a few contemporaneous efforts have surfacedwhich make some observations that overlap with": "our findings. Chen et al. (2024) introduce post-semantic thinking (PST) to reduce the influenceof rationales on final output labels. Xu et al. (2024)reveal that preemptive answer generation (a targetlabel) within a CoT rationale is highly sensitive tomalicious attacks, which comports with our hypoth-esis (i.e. vice versa) that a faulty reasoning leadingto incorrect rationales can effect the overall modelperformance (which is solely evaluated on labelsgenerated after those rationales).",
  ". The coherence of rationales and their gram-matically. Finding: When rationales fol-low labels, the words they comprise can bescrambled and one still observes compara-ble gains": "3. Whether we need only a small set of keytokens from rationales (and how to identifythem). Finding: Gains comparable to CoT-augmented distillation can be realized us-ing a small set of tokens identified via gra-dient attribution; using manually selectedimportant words does not do as well, nordoes using tokens that are similar to labelwords. Some of these findings corroborate and deepenobservations made in contemporaneous work, e.g.,models can benefit from additional compute atinference time (Goyal et al., 2024), and CoT-augmentation fares best when rationales are placedafter the target labels (Chen et al., 2024). We havenot fully characterized the mechanism by whichCoT augmentation aids distillation, but we haveruled out some explanations and provided empiri-cal insights into when and how CoT augmentationprovides useful signal to student models.",
  "Limitations": "There are important limitations to this work andthe conclusions we can draw from it.First, we have only considered publicly availableopen-domain question-answering datasets in ouranalyses, to the exclusion of complex informationextraction tasks such as relation extraction whereCoT-augmented distillation has also proven useful(Wadhwa et al., 2023). We made this choice largelyin the interest of consistency with prior work, andto avoid complex evaluation challenges that occurduring generative relation extraction.Second, we did not attempt to improve the qual-ity of CoT rationales generated by teacher modelsthrough iterative prompt refinement or other tech-niques (Wang et al., 2023). We also elicited therationales for distillation from modestly sized opensource models, rather than (for example) GPT-4. Itmay be possible to elicit better rationales frommassive proprietary models, but it seems unlikely(though possible) that our conclusions vis-a-vis dis-tillation would change as a result.Third, our evaluation of using rationale tokensannotated manually is limited by the way weframed the task. It could be that an alternativedesign and/or annotation interface would yield dif-ferent annotations, and this may in turn lead todifferent conclusions regarding the utility of tokensselected in this way.Finally, we only experimented with English-language datasets and we therefore cannot saywhether these results would hold in other lan-guages.",
  "Ethics Statement": "This work required some human annotations whichwere collected using an online platform called Pro-lific. Prolific required us to pay workers per hour,and so we had to estimate the time required tocomplete one batch of annotations. To do so, we(the authors) carried out a small number of theseannotations to determine the approximate hourlycompensation. We then set the compensation rateto average $15 USD/hour. If annotators took longerthan expected to complete a batch of annotations,we paid bonuses to ensure that their cumulative payaveraged out to US$15/hour.",
  "Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023": "Large language models are reasoning teachers. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1485214882, Toronto, Canada.Association for Computational Linguistics. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alexander J. Ratner,Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.2023. Distilling step-by-step! outperforming largerlanguage models with less training data and smallermodel sizes. ArXiv, abs/2305.02301. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Liunian Harold Li, Jack Hessel, Youngjae Yu, XiangRen, Kai-Wei Chang, and Yejin Choi. 2023a. Sym-bolic chain-of-thought distillation: Small models canalso think step-by-step. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 26652679, Toronto, Canada. Association for Computa-tional Linguistics.",
  "Lucie Charlotte Magister, Jonathan Mallinson, JakubAdamek, Eric Malmi, and Aliaksei Severyn. 2023.Teaching small language models to reason": "Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question an-swering. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 23812391, Brussels, Belgium. Associationfor Computational Linguistics. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeffrey Dean. 2013. Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing sys-tems, pages 31113119.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Kumar Shridhar, Alessandro Stolfo, and MrinmayaSachan. 2023. Distilling reasoning capabilities intosmaller language models. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 70597073, Toronto, Canada. Association forComputational Linguistics. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks. In Proceed-ings of the 34th International Conference on MachineLearning - Volume 70, ICML17, pages 33193328.JMLR.org. Oyvind Tafjord, Peter Clark, Matt Gardner, Wen tauYih, and Ashish Sabharwal. 2018. Quarel: A datasetand models for answering questions about qualita-tive relationships. In AAAI Conference on ArtificialIntelligence. Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsenseknowledge. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41494158, Minneapolis, Minnesota. Association forComputational Linguistics. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models": "Peter West, Chandra Bhagavatula, Jack Hessel, JenaHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,Sean Welleck, and Yejin Choi. 2022.Symbolicknowledge distillation: from general language mod-els to commonsense models. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 46024625, Seat-tle, United States. Association for ComputationalLinguistics. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,Mark Riedl, and Yejin Choi. 2022.Reframinghuman-AI collaboration for generating free-text ex-planations. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 632658, Seattle, United States.Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "We conducted our experiments using three datasets;for completeness we provide details about thesehere": "CommonsenseQA(Talmor et al., 2019) is amultiple-choice question answering dataset thatrequires commonsense knowledge.Each ques-tion is accompanied by five answer choices;only one is correct.The dataset consists of12,102 questions split into a training, devel-opment, and test sets of set of 9,741, 1,221,and 1,140 questions, respectively.The follow-ing is an example from the training data (ID:7e93dacd4d1b7c7aa4c15f5da220bd59)",
  "Question: The two conglomerates decidedto reach tentative agreement to what?Choices:A: do businessB: accomplishC: stop arguingD: make progressE: digging holesAnswer: A (do business)": "OpenBookQA(Mihaylov et al., 2018) is de-signed to test an understanding of elementary sci-ence, combining factual knowledge with common-sense reasoning. The dataset contains 5,957 ques-tions, each with four answer choices (and one cor-rect response). This is split into training, develop-ment, and test sets of 4,957, 500, and 500 questionsrespectively. A unique aspect of OpenbookQA isits focus on scientific facts which students are ex-pected to know. The following is an example fromthe training data (ID: 12-271) Question: Skills are learnedcharacteristics. To get better at doingsomething, you must stretch yourself inways thatChoices:A: may be very uncomfortable at firstB: take very little timeC: are without learning from others andpast experiencesD: are without goals and commitmentAnswer: A (may be very uncomfortable atfirst) QuaRel(Tafjord et al., 2018) is a datasetfor reasoning over physical processes involvingcomparative relationships.It consists of 2,740multiple-choice questions, each with two answerchoices (one being correct).The questions re-quire reasoning about how physical processes af-fect different entities in qualitative ways.Thedataset provides train/development/test splits com-prising 1,948/278/514 questions.The follow-ing is an example from the training data (ID:QuaRel_V1_Fr_0344) Question: Ryan races his car and needsto drive in different types ofsituations. Ryan drives around in asandy desert, and then in an emptyparking lot. After each drive, Ryan seeshow warm his car got. Ryan notices thathis car was much warmer after driving inthe sand than it was after driving inthe parking lot. That is because thesand had _____ than the parking lot.Choices:A: more resistanceB: less resistanceAnswer: A (more resistance)",
  "BPrompts": "Our experiments required eliciting chain-of-thought (CoT) rationales from a teacher LLM tobe used in distillation. For this we used Mistral-7B-Instruct (Jiang et al., 2023). We used the followingrationale-augmented few-shot prompt to this end.The question, answer choices, and the target labelare taken from the original training instance, andthe CoT rationale provided was written by us (theauthors).",
  "CommonsenseQA": "<s>[INST] Given the following two examples ofquestion-answer-rationale triplets, provide arationale for the third example for why theselected choice answers the question. [\\INST]Question: The president had to make a decisionregarding the hate attack on his country, whatdid he do? Choices:A: wage war; B: fight enemy; C:kill; D: destroy enemy; E: attacked his countryAnswer: A (wage war)Rationale: The answer is A because thepresidents decision to address a hate attack onhis country typically involves taking militaryaction, such as waging war, to protect and defendthe nation. </s>Question: Letters are sometimes delivered by handthrough one of these? Choices:A: mail box; B:suitcase; C: front door; D: bowl; E: post officeAnswer: C (front door)",
  "OpenBookQA": "<s>[INST] Given the following two examples ofquestion-answer-rationale triplets, provide arationale for the third example for why theselected choice answers the question. [\\INST]Question: Oak tree seeds are planted and asidewalk is paved right next to that spot, untileventually, the tree is tall and the roots mustextend past the sidewalk, which means Choices:A:roots may fall apart; B: roots may begin to die;C: parts may break the concrete; D: roots may besplit;Answer: C (parts may break the concrete)Rationale: The answer is C because as the oaktree grows, its roots may exert pressure on thesidewalk, causing the concrete to crack or break.</s>Question: A cow eats some hay, an apple and apiece of bread. In its tummy Choices:A: mail box;B: suitcase; C: front door; D: bowl; E: postofficeAnswer: B (suitcase)Rationale:The answer is B because the cowsstomach contains digestive enzymes that breakdown the consumed food into smaller, solublemolecules through the process ofdissolution.</s>",
  "QuaRel": "<s>[INST] Given the following two examples ofquestion-answer-rationale triplets, provide arationale for the third example for why theselected choice answers the question. [\\INST]Question: Dan drives a car into the garage fromthe gravel parking lot. The car moves moresmoothly into the garage than the parking lot.This is because there is a bumpier surface in the(A) garage floor (B) gravel parking lot.Answer: B (gravel parking lot)Rationale: The answer is B because the questionstates that the car moves more smoothly into thegarage than the parking lot, indicating that thegravel parking lot has a bumpier surface comparedto the garage floor. </s>Question: he baseball team coach was consideringboth Ted and Roy for the right field position. Heneeded someone who could propel the ball all theway to the basemen and he knew Ted was morephysically fit and possessed greater physicalstrength than Roy. Who could likely throw theball a further distance? (A) Roy (B) TedAnswer: B (Ted)Rationale:The answer is B because the questionindicates that Ted is more physically fit andpossesses greater physical strength than Roy,suggesting that Ted is more likely to throw theball a further distance.</s>",
  "We used the Huggingface library (v4.26.1; Wolfet al. 2020) and publicly available checkpoints for": "both student11 and teacher12 models. GPT-2 andPhi-1.5 were fine-tuned on a single A100 instancewhile Gemma-2B was fine-tuned on 2 A100 in-stances. To monitor the training process, we eval-uated model checkpoints every 500 steps. Earlystopping was employed with a patience parameterof 10, meaning that training was halted if therewas no improvement in the evaluation-set accuracyfor 10 consecutive evaluations. The improvementthreshold was set to 0.02, ensuring that only signif-icant improvements were considered to continuetraining. This strategy helped to prevent overfittingand reduced unnecessary computational overhead.Upon publication, we release all code (includedelicited rationales for all datasets considered) nec-essary for reproducing our experiments.",
  "DRQ1 example heatmaps": "Now we visualize the predictions of individual lay-ers of GPT-2 fine-tuned with no, pre, and post-CoTrationales while processing the input Question: anelectric car contains a motor that runs on...Choices:A: gas; B: hydrogen; C: ions; D: plutonium, weare specifically interested in what the layers in thepenultimate time-step (w.r.t final target label) thinkthe next token should be."
}