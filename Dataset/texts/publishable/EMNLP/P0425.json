{
  "Abstract": "The patent citation count is a good indicatorof patent quality. This often generates mone-tary value for the inventors and organizations.However, the factors that influence a patent re-ceiving high citations over the year are still notwell understood. With the patents over the pasttwo decades, we study the problem of patent ci-tation prediction and formulate this as a binaryclassification problem. We create a semanticgraph of patents based on their semantic simi-larities, enabling the use of Graph Neural Net-work (GNN)-based approaches for predictingcitations. Our experimental results demonstratethe effectiveness of our GNN-based methodswhen applied to the semantic graph, showingthat they can accurately predict patent citationsusing only patent text. More specifically, thesemethods produce up to 94% recall for patentswith high citations and outperform existingbaselines. Furthermore, we leverage this con-structed graph to gain insights and explanationsfor the predictions made by the GNNs.",
  "Introduction & Related Work": "Patents play a pivotal role in driving innovation andfostering economic growth. They provide a legalframework that allows inventors (e.g., companies,researchers) exclusive rights to their creations for aspecified period, typically 20 years (Levin, 2004;Kitch, 1977; Encaoua et al., 2006). This exclusivitymotivates the inventors and the businesses to investin research and development, as they can benefitfrom their innovations.Patent citations are important in the context ofintellectual property (IP) and patent valuations andserve multiple important roles for patent examinersand applicants. Firstly, they aid patent examin-ers in assessing an inventions novelty and non-obviousness for granting patents to genuinely in-novative creations. Secondly, they assist inventorsby revealing the technological landscape and helpthem to refine claims and avoid any patent conflicts. Thirdly, patent citations play a significant role inassessing the value of patent portfolios, with morecitations often signifying greater influence in spe-cific industries. Further, researchers employ themto track tech trends and policy impact.Several studies have analyzed patent valuethrough the forward citations (Hall et al., 2001;Harhoff et al., 1999) and assessed economic valueof patents (Sampat and Ziedonis, 2005; Hall et al.,2005). Previous research endeavors have exploredbroader patterns of knowledge transfer (Singh,2003) through patent citations such as interactionsbetween academia and industry via citations be-tween academic papers and patents (Chen andHicks, 2004). One of the relevant work involvesprediction of patent value dependent on citationcount from the text (Hsu et al., 2020) with regres-sion. However, we differ in multiple ways: ourstudy formulates a classification task, construct asemantic-based network, uses graph neural network(GNN)-based methods, and generates explanations.In this paper, we perform an extensive empiricalstudy on the power of patent text to predict cita-tions. Our major contributions as follows.Problem and data. We study the problem of patentcitation prediction as a binary classification prob-lem. Our study includes granted patents over lasttwo decades and provides descriptive analyses onthe meta-data of the patents in three major classes.Method. We construct a patent semantic graphfrom the patent similarities and use graph neuralnetwork (GNN)-based methods for citation pre-diction. Our empirical evaluations show that theGNN-based methods can predict patent citationsonly using the patent text with high quality.Explanation. The constructed graph combinedwith an explanation technique are used to get in-sights of the predictions of the GNNs.Note that we have added more details for nextsections in the Appendix along with background,related work, and additional experiments.",
  "Problem Definition and Data": "We formulate the problem of patent citation pre-diction as a binary classification task where weclassify the patents as highly cited or low cited.Let P = {P1, P2, , Pm} be the set of m patents.As the patent citations vary over years, we use thecount of citations obtained by a patent after d yearsfrom the year of being granted. We denote the ci-tation of the patent Pi after d years as Cid. In theexperiments we use d = 3, 5, and 10 years anduse these to generate different labels and thus theygenerate different datasets.Our aim is to measure the impact of a patent byusing the citations of the patent. We focus on pre-dicting whether a particular patent will be highlycited (positive, denoted by 1) or low cited (negative,denoted by 0) at the time of its granting year byusing the text-based information from the patentitself. The decision on whether a patent belongs toa particular class (positive or negative) is based onthe distributions of the citations. We set a thresholdbased on the distribution. Let us assume the thresh-old is xth percentile. Thus, we define patent cita-tion class as positive based on whether the citationcount is higher than the value at the top xth per-centile. Similarly, a patent belongs to a low citedclass if the patent citation count is lower than thevalue at the bottom xth percentile.",
  "where Cx,h and Cx,l denote the values at the topxth percentile and the bottom xth percentilerespectively": "Other Class Labels. Though the above definitionproduces citation labels, one could design the la-bels other ways. Note that the above one producesan easy to classify dataset in the sense that thepatent with high and low citations are well sepa-rated in the distribution. In the experiments, weexplore other labeling settings. First, we define topxth percentile as high, bottom xth percentile aslow and the rest as middle (we set x = 10 in theexperiments). As the main goal is to identify high-quality or low-quality patents, we have divided thedatasets and taken pair-wise classification in three",
  ": #Patents in the individual CPC classes": "different settings: High vs rest, high vs middle,middle vs low. Please see Sec. 4.2 for details.Our classification problem. We investigate thepredictive power of the text in prediction of thequality of the patent, i.e., the patent citation count.To do so, we learn a prediction function f, wherethe features constructed from the patent text aregiven as input and the defined label y(Cid) acts asthe outcome variable.Data. Our study includes the granted patents fromthe United States Patent and Trademark Office(USPTO)1. The number of patents grow exponen-tially over the years. We have included recentpatents over the last two decades from 2000 to2022 for our analysis. Our study focuses on cita-tions which often depend on the area or topics ofthe invention, and thus, we consider on subcate-gories of patents. We consider patents under major(based on numbers) categories rather than all thepatents. We follow the standard classification sys-tem for patents called the CPC categorization. Wechoose top three CPC classes in terms of the num-ber of patents categorized in them. showsthe classes and the number of patents in each cate-gory. Descriptive analysis of the data is providedin the Appendix.",
  "Methods": "In patent citation prediction, there are two majorchallenges: (1) The texts in patents are not similarto the texts in research papers or news articles, (2)Our aim to build models that are explainable, i.e,we can find the reasoning behind their predictions.Text-based AI Methods. Modern AI tools have re-cently gained popularity in patent analysis (Shomeeet al., 2024). We use two methods to generate rep-resentations for the patent documents: Doc2Vec(Lau and Baldwin, 2016) and Patent Bert (Leeand Hsiang, 2020). These representations are usedin combination with a multi-layered perceptron(MLP) for the classification tasks in the experi-ments. PatentBert fine-tunes a pre-trained BERTmodel with patent data and applies the model tothe patent classification task.",
  "Graph-based AI Methods": "Graph construction. We construct a graph fromthe semantic similarity between the patents whereeach node is a patent. Two nodes are connectedif they have a high semantic similarity (0.6-0.8 more details in A.4.2). We represent the patent doc-uments with a 100-dimensional embeddings. Theseembeddings are generated from training a Doc2Vecmodel with approximately 200,000 patent textswhich include their titles, abstracts, and claims.Edges in the graph are computed based on the se-mantic similarity between the nodes (patent em-beddings computed above), specifically using theDoc2Vec features. An edge is created betweennodes when their similarity surpasses a selectedthreshold.Node Feature Representation. We use graph neu-ral network (GNN)-based method to perform thepatent citation prediction task. However, GNNsrequire initial features for the nodes. We againcompute these features based on the patent textfrom two different embedding model: Doc2Vec(Lau and Baldwin, 2016) and PatentBert (Lee andHsiang, 2020).Graph Neural Networks.Graph Neural Net-works (GNNs) (Kipf and Welling, 2016; Hamiltonet al., 2017) have proven to be effective in mak-ing predictions on such graphs by learning rele-vant low-dimensional node representations througha message-passing mechanism. During messagepassing, each node (u V ) updates its represen-tation by aggregating information from itself andits set of neighbors N(u). GNNs iteratively applythis aggregation scheme to refine the node repre-sentations, capturing the structural dependencieswithin the graph. The GNNs are effective for awide range of prediction tasks over graphs suchas node classification, link prediction, and graphclassification. We use three types on GNNs for ourstudy: GCN (Kipf and Welling, 2016), GraphSage(Hamilton et al., 2017), and Graph TransformerNetwork (GTN) (Yun et al., 2019).",
  "Experiments": "We use three types of patent data from three majorCPC (Cooperative Patent Classification) classes:A61, H04, and G06. This results in nine separatedatasets with three different periods of citation his-tory from the year of 2000: (1) Citation history for3 years (3-years-history): Patents published until2019 as we count citation up to 2022; (2) Citation history for 3 years (5-years-history): published un-til 2017. (3) Citation history for 3 years (10-years-history): Patents published until 2011. Recent twoyears of data from each patent dataset (among thenine datasets above) are kept for testing.",
  "Citation Prediction: Top vs Bottom": "Labels.We have created the labels of positiveand negative classes based on the citation countand the overall distribution. For the patents withhigh citations we choose top 10% patents based oncitations (positive class), and correspondingly wechoose bottom 10% patents for the negative class. Results. Our objective is to demonstrate the ef-ficacy of graph-based AI methods in the patentcitation prediction. We present the results for dif-ferent setting in (recall of the positive class,i.e., patents with high citations). Please see theresults for accuracy () (accuracy) and F1() in the Appendix. respectively. From Ta-ble 2, we observe that all the methods can retrievethe patents with high citations accurately. This is acritical task, as high-quality patents can have a sub-stantial impact on innovation, ultimately benefitingsociety. The results in shows how thecombination of textual semantics and the structurewithin the graph aids the models in understandingquality and thus leads to accurate predictions.",
  "Citation Prediction: Different Labels": "Labels.First, we define top xth percentile ashigh, bottom xth percentile as low and the rest asmiddle (x = 10). We have divided the datasets andtaken pair-wise classification in three different set-ting: high vs rest (), high vs middle (), middle vs low (). Results. As the labels are harder than the previ-ous labels (Sec. 4.1), the graph-based models per-form much better than just using MLP. The MLPbaselines produce almost similar results as random(note that a random model would generate accuracyof .5). Our graph-based models produce good per-formance in terms of four measures in all the threesettings, generating more than .7 in all the measures.Further, GSAGE and GTN are more sophisticatedmethod than GCN (e.g., GSAGE have generalizedaggregation function whereas GCN uses the meanas an aggregator (Hamilton et al., 2017)), and thusthey produce better results than GCN.",
  "Explanations with GNNs": "One primary motivation for designing graph-basedmethods is the capability to provide explanationsfor the predictions (Kakkad et al., 2023; Kosanet al., 2023). Note that it is difficult to explainpatent quality from the text itself with traditionalmethods such as LIME (Ribeiro et al., 2016) asthe patent text is domain-specific and often writtenby an expert lawyer with a lot of jargon. Thus,our graph construction method becomes useful forgenerating explanations. We choose a set of 50nodes from both the classes. GNNExplainer (Yinget al., 2019) is designed to explain the predictionbehavior of GNNs while producing a subgraph asan explanation for node classification tasks. In thiscontext, we can gain insights into the relationshipsbetween different nodes (patents) that impact cita-tions. We compare these two sets of explanationsubgraphs obtained for the nodes in both classes.We compute three graph-specific properties: den-sity, degree, and clustering coefficient (CC). Wereport the average of the values from the subgraphs",
  ": Comparison of graph-based properties in theexplanation subgraphs for nodes in both classes. CCdenotes average clustering co-efficient of the nodes": ": Explanation subgraph of node 7 with thepatent titled Interchangeable shaft assemblies for usewith a surgical instrument produced by the GNN-explainer method (Ying et al., 2019). Please refer to for the specific patent-related information.4.3.1Example Explanation SubgraphWe show an example of the explanation subgraphthat is obtained from our framework with the GN-",
  "SelfInterchangeable shaft assemblies for use with a surgical instrument508": "21DirectModular powered surgical instrument with detachable shaft assemblies59259DirectDrive system lockout arrangements for modular surgical instruments53863DirectRotary powered articulation joints for surgical instruments531193DirectLocking arrangements for detachable shaft assemblies4091389DirectRobotically powered surgical device with manually-actuatable reversing system117 1122IndirectShaft assembly arrangements for surgical instruments15320315IndirectArticulation mechanism for surgical instrument12061287IndirectSurgical device having multiple drivers1951201IndirectHand held rotary powered surgical instruments with end effectors1421118IndirectArticulatable surgical instrument configured for detachable use with a robotic system153 : Information on patents/nodes of example explanation subgraph in . We observe that explanationsubgraph attached to a highly cited node/patent consists of nodes/patents that are highly cited. Interestingly, all thenodes that are both indirectly or directly connected to the node/patent being explained have high citations.",
  "PatentBert-GCN0.660.730.670.700.690.730.750.740.720.730.850.78PatentBert-GTN0.670.760.650.700.700.740.760.750.730.750.820.78PatentBert-GSAGE0.670.760.640.690.690.740.730.730.730.750.830.78": ": Results on the test dataset with patents only from the year of 2016 in A61 where Acc denotes accuracy andPr, Re, F1 denote Precision, Recall and F1-score for the positive class. We construct three different training setsfrom a span of 5-years from 2000-2014. The results show that training with recent patents have a more accurateprediction of citation classes for the future patents. NExplainer method (Ying et al., 2019). In ,we present the subgraph resulting from the explana-tion of the patent titled Interchangeable shaft as-semblies for use with a surgical instrument (nodewith the index 7). Note that there are several nodesthat are directly connected (with the dark edges).The graph edges are color-coded to convey theirstrength: black edges represent strong connections,while the shadow lines indicate weaker connec-tions. We extract the critical subgraph nodes basedon the presence of black edge lines, signifying theirimportance in the explanation subgraph.Furthermore, to understand the example of thepatents in the explained subgraph, we present thepatent title, the number of citations, and the con-nection type in . The focal patent (node 7)is highly cited patent with 508 citations. Notably,both directly and indirectly connected nodes alsohave titles related to surgical devices and instru-ments same as the focal node, with high citationcounts. This explainer subgraph example suggeststhat the number of citations in the similar patentsmight indirectly impact the number of citation ofthe focal patent, even though our proposed GNNsdo not use this information for the prediction.",
  "We demonstrate that the recency of the patentsare useful for patent citation prediction. Here weevaluate the influence of patents from recent years": "within the A61 CPC class. We utilize three distincttraining sets with five years of patents: 2000-2004,2005-2009, and 2010-2014, respectively. The testset remain consistent across all experiments withpatents from 2016. The results, presented in , indicate that training with more recent patentsenhances the models predictive capabilities of ci-tation classes for the future patents. For instance,when using the PatentBert-GSAGE approach, weachieve higher levels of accuracy, precision, re-call, and F1-score when training with patents from2010-2014 to predict citations for patents in 2016.",
  "Discussions": "We draw several key takes from the study. (1)Text and network structure matter: Graph-basedAI models (GNNs) can predict patent citation ac-curately only from the text of title, abstract, andclaims. Understanding the network structure ofthe patent landscape is also important. (2) Expla-nation is the key: Though several deep learningmodels have good predictive power, they mightlack domain-specific explanations and the GNN-based explainers might be helpful. (3) Recent datais important: The text from recent patents are moreuseful for citation prediction, thus, models shouldbe mindful about the training data and possiblyneed re-training regularly.Code and data are accessible at",
  "Limitations": "This paper addresses a timely subject related toAI-based methods to predict patent citations. Thedataset and the model used for this study are pub-licly available. While the paper shows the capabil-ity graph-based approaches towards patent citationprediction, one could further investigate the reason-ing on patents getting high citations and build a fewprototypes.",
  "Fernando Benites, Shervin Malmasi, and MarcosZampieri. 2018.Classifying patent applica-tions with ensemble methods.arXiv preprintarXiv:1811.04695": "Gaetano Cascini and Manuel Zini. 2008. Measuringpatent similarity by comparing inventions functionaltrees. In Computer-Aided Innovation (CAI) IFIP 20thWorld Computer Congress, Proceedings of the Sec-ond Topical Session on Computer-Aided Innovation,WG 5.4/TC 5 Computer-Aided Innovation, September7-10, 2008, Milano, Italy, pages 3142. Springer. Alok K Chakrabarti, Israel Dror, and Nopphdot Eak-abuse. 1993. Interorganizational transfer of knowl-edge: an analysis of patent citations of a defensefirm. IEEE Transactions on Engineering Manage-ment, 40(1):9194.",
  "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013.Efficient estimation of wordrepresentations in vector space.arXiv preprintarXiv:1301.3781": "Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-jes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.2021. Deep learningbased text classification: a com-prehensive review. ACM computing surveys (CSUR),54(3):140. Diego Moll and Dilesha Seneviratne. 2018. Overviewof the 2018 alta shared task: Classifying patent ap-plications. In Proceedings of the Australasian Lan-guage Technology Association Workshop 2018, pages8488.",
  "Charles Oppenheim. 2000. Do patent citations count.The web of knowledge: A festschrift in honor of Eu-gene Garfield, pages 405432": "Subhash Chandra Pujari, Annemarie Friedrich, and Jan-nik Strtgen. 2021. A multi-task approach to neuralmulti-label hierarchical patent classification usingtransformers. In Advances in Information Retrieval:43rd European Conference on IR Research, ECIR2021, Virtual Event, March 28April 1, 2021, Pro-ceedings, Part I 43, pages 513528. Springer. Marco Tulio Ribeiro, Sameer Singh, and CarlosGuestrin. 2016. \" why should i trust you?\" explainingthe predictions of any classifier. In Proceedings ofthe 22nd ACM SIGKDD international conference onknowledge discovery and data mining, pages 11351144.",
  "A.1Background": "Types of Patents.In the United States, there arethree major types of patents granted by the UnitedStates Patent and Trademark Office (USPTO).These patents are designed to protect differentkinds of inventions and intellectual property. (1)Utility Patents: Utility patents are the most com-mon type of patent and cover new and useful pro-cesses, machines, manufactured articles, and com-positions of matter.(2) Design Patents: Designpatents protect the ornamental or aesthetic designof a functional item. They are often sought forproducts with unique visual characteristics, suchas consumer electronics, jewelry, and automotiveparts. (3) Plant Patents: Plant patents protect newand distinct varieties of plants that have been asexu-ally reproduced (e.g., through cuttings or grafting). Components of Utility Patents.In this work wemainly focus on utility patents. Utility patents,also known as patents for inventions, protect newand useful processes, manufactured articles, andcompositions of matter. We use these key compo-nents of a utility patent in our study: (1) Title: Thetitle provides a concise and descriptive name forthe invention. (2) Abstract: An abstract is a con-cise summary of the invention, typically limitedto 150-250 words. It provides a brief overview ofthe inventions technical aspects and applications.(3) Claims: The claims define the legal bound-aries of the patent. They precisely describe theelements or steps that make the invention uniqueand patentable. We use the text of title, abstractand claims to create features for our patent citationprediction task. The claims have been to useful forother task such as CPC (topic-based) classification(Lee and Hsiang, 2020). Title and abstracts areoften used in similar natural language processingtasks such as keyphrase generation (Meng et al.,2017). Importance of Patent Citations.Patent cita-tions, which refer to the references to prior patentswithin a newly granted patent, serve several pur-poses and are important for various stakeholdersin the intellectual property ecosystem. (1) Assess-ment of Novelty and Non-obviousness: Patent ex-aminers use patent citations to assess the noveltyand non-obviousness of a new invention. By exam-ining the references cited in a patent application,examiners can determine whether the claimed in- vention is truly novel and represents a non-obviousadvancement over prior art. This is a fundamentalstep in the patent examination process and helpsensure that only truly innovative inventions receivepatent protection. (2) Prior Art Search: For inven-tors and patent applicants, reviewing patent cita-tions can aid in understanding the existing land-scape of related technologies and inventions, oftenreferred to as \"prior art.\" This can help inventorsrefine their claims, identify gaps in existing knowl-edge, and potentially avoid pursuing inventions thatare unlikely to be granted patents due to the exis-tence of prior art. (3) Patent Valuation: A patentwith numerous citations from other patents may beconsidered more valuable because it indicates thatthe patented technology is widely recognized asinfluential or relevant within a specific industry orfield.In summary, patent citations are essential for theevaluation and utilization of intellectual property.They provide valuable information about the stateof innovation, the relationship between patents,and the technological advancements within spe-cific fields. Thus, we focus on building AI-basedmodels to predict the citations.",
  "A.2Related Work": "Patent classification.Recent advancements inMachine Learning have led to the application ofvarious ML techniques aimed at enhancing the effi-ciency of patent classification. Benites et al. (Ben-ites et al., 2018) presented a top-performing so-lution in the ALTA 2018 Shared Task on patentclassification (Moll and Seneviratne, 2018), uti-lizing the full text of patent documents. Graweet al. (Grawe et al., 2017) employed an LSTM inconjunction with word embeddings for classifica-tion. Risch and Krestel (Risch and Krestel, 2019)pre-trained fastText word embeddings using a sub-stantial corpus of patent documents, integratingthem with Gated Recurrent Units (GRUs) for clas-sification. Li et al. (Li et al., 2018) proposed Deep-Patent, which is a deep learning algorithm based onconvolutional neural networks. PatentBERT (Leeand Hsiang, 2019) focuses on fine-tuning a pre-trained BERT (Devlin et al., 2018) model whichuses only the first claim of a patent and achiev-ing noteworthy results. Patent2vec (Fang et al.,2021) adopted a multi-view graph-based approachwith tags to patent classification. Bai et al. (Baiet al., 2020) proposed a Multi-stage Feature Extrac-tion Network (MEXN), comprising a paragraph encoder and summarizer for all patent paragraphsto enhance classification. Pujari et al. (Pujari et al.,2021) developed a hierarchical transformer-basedmulti-task model that trained an intermediate SciB-ERT (Beltagy et al., 2019) layer using title andabstract as input text. In a comparative analysis ofBERT and SciBERT for patent classification, Al-thammer et al. (Althammer et al., 2021) discoveredthat the SciBERT model outperformed BERT. Za-heer et al. propose Big Bird (Zaheer et al., 2020), along-text transformer, and apply it to patent classi-fication by incorporating title, abstract, and claimsinto the classification process. Patent Similarity.Measuring similarity betweenpatents has become another prominent field of re-search involving patents. Consequently, a substan-tial body of research has concentrated on method-ological aspects, employing machine learning anddeep learning, particularly natural language pro-cessing (NLP) techniques, to gauge patent simi-larity. Cascini and Zini (Cascini and Zini, 2008)introduced a clustering algorithm that evaluatespatent similarity by taking into account hierarchi-cal and functional interactions among patents. Vec-tor space models have also been utilized in patentanalysis. Younge et al. (Younge and Kuhn, 2016)developed a single vector space-based model forautomatically measuring the continuous similar-ity distance between pairs of patents. Feng (Feng,2020) devised a similarity measurement techniqueusing vector space representations of patent ab-stracts with Document Vectors (Doc2Vec) (Le andMikolov, 2014). Noh and Lee applied text miningto patent analysis by employing keyword selectionand processing strategies (Noh et al., 2015). Sim-ilarly, Joung and Kim adopted a keyword-basedapproach for technology planning (Joung and Kim,2017). Recently, Yoo et al. (Yoo et al., 2023) pro-posed a hybrid method that automatically assessespatent similarity, taking into account both semanticand technological similarities. Patent Citations.Patent citations serve as asignificant metric to gauge intellectual heritageand influence. They have been employed to as-sess the dissemination and exchange of knowl-edge in research and development, as well as tomeasure research productivity and impact (Narin,1994). The information derived from patent cita-tions can effectively portray the transmission ofknowledge (Karki, 1997; Oppenheim, 2000). Pre-vious investigations have delved into the broader patterns of knowledge transfer through patent cita-tions. For instance, Chakrabarti et al. (Chakrabartiet al., 1993) scrutinized inter-organization patent ci-tation trends in defense-related research and devel-opment transitioning into the civilian sector. Chenand Hicks (Chen and Hicks, 2004) examined the in-teractions between academia and industry by scru-tinizing citations between academic papers andpatents in the field of tissue engineering. Verbeek etal. (Verbeek et al., 2003) explored the geographicdistribution of scientific researchs influence onpatents in the domains of biotechnology and in-formation technology. Singh (Singh, 2003) investi-gated how the social distance between inventors im-pacts the flow of knowledge within USPTO patents.These studies on knowledge diffusion were primar-ily based on the citation patterns between pairs ofentities.",
  "A.3Data": "Our study includes the granted (accepted) patentsfrom the United States Patent and Trademark Of-fice (USPTO)2. The number of patents grow expo-nentially over the years. We have included recentpatents over the last two decades from 2000 to2022 for our analysis. Our study focuses on cita-tions which often depend on the area or topics ofthe invention. This fact naturally leads us to focuson subcategories of patents. For a better under-standing on how patents are cited as well to buildbetter models to predict the citations, we considerpatents under major (based on numbers) categoriesrather than all the patents. We follow the standardclassification system for patents called the CPCcategorization3. We choose top three CPC classesin terms of the number of patents categorized inthem. shows the classes and the number ofpatents in each category.We show descriptive analysis of the data on thedistribution of several important components ofpatents for the three CPC classes. showsstatistics for all the three major CPC classes (A61,H04, and G06) on average number of inventors(team size), figures, sheets. One interesting ob-servation is that A61 (i.e., patents in the medicaldomain) has higher average then the other two forall the years. Over the years, all the values have anupward trend. Upward trend in team size impliesthe collaboration is increasing over the years. On the other hand, shows statistics on the aver-age number of claims (all and dependent) for allthe three major CPC classes. Note that the claimsdescribe the elements or steps that make the in-vention unique and patentable. Interestingly, in allareas, the number of claims look similar. Over theyears, all the values have mostly a downward trendindicating that number of claims might not be adriving factor to get a patent accepted.",
  "A.4.1Text-based AI Methods": "We use two methods to generate representations forthe patent documents from traditional text-basedAI or NLP models: Doc2Vec and Patent Bert. Notethat these representations are used in combinationwith a multi-layered perceptron (MLP) for the clas-sification tasks in the experiments. We describethese two methods one being generic and anotherfocusing on patent data: Doc2Vec (Le and Mikolov, 2014): Doc2Vec,also known as Paragraph Vector, is an exten-sion of Word2Vec (Mikolov et al., 2013), apopular method (Lau and Baldwin, 2016) forrepresenting paragraphs in stead of words as avector representation in natural language pro-cessing (NLP). While Word2Vec learns vectorrepresentations for words, Doc2Vec goes astep further by learning representations forentire documents or paragraphs while captur-ing the semantic meaning and context of adocument. Each document is represented asa fixed-length vector. We use the represen-tations produced by Doc2Vec and feed themthrough an MLP to predict the citation classof a patent.",
  "PatentBert (Lee and Hsiang, 2020): Thismethod fine-tunes a pre-trained BERT modeland applies it to the task of patent classifi-cation. It uses the BERT based pre-trainedmodel for fine-tuning": "A.4.2Graph-based AI MethodsGraph construction.We construct a graph fromthe semantic similarity between the patents whereeach node is a patent. Two nodes are connected ifthey have a high semantic similarity.Proximity creation via training the Doc2Vecmodel: We represent the patent documents witha 100-dimensional vector representations (embed-dings). These embeddings are generated from train-ing a Doc2Vec model with approximately 200,000patent texts which include their titles, abstracts,and claims. These embeddings are designed tocapture the semantic similarity between patent textdata, thus will help us to create the edges betweenpatents.Edge Construction: Edges in the graph are com-puted based on the semantic similarity betweenthe nodes (patent embeddings computed above),specifically using the Doc2Vec features. An edgeis created between nodes when their similarity sur-passes a selected threshold, typically falling withinthe range of 0.62 to 0.8. The choice of the similar-ity threshold is based on the desired density of thegraph, which we vary from 5 to 25.Node Feature Representation: We use graph neu-ral network (GNN)-based method to perform thepatent citation prediction task. However, GNNsrequire initial features for the nodes. We againcompute these features based on the patent text.Specifically, we create two distinct types of nodefeatures from two different embedding model (weuse these features separately in the experiments): Features from Doc2Vec: The first type ofnode features is generated using the Doc2Vecmodel trained in the previous step. These fea-tures are calculated based on the semantic con-tent of the patent text data.",
  "(c) Average #Sheets": ": Descriptive statistics for all the three major CPC classes (A61, H04, and G06) on (a) Average number ofinventors (team size), (b) Average number of figures, and (c) Average number of Sheets. Interestingly, A61 hashigher average then the other two for all the years. Over the years, all the values have an upward trend.",
  "(c) H04": ": Descriptive statistics on average number of claims (all and dependent) for all the three major CPC classes(a) A61, (b) H04, and (c) G06. Note that the claims describe the elements or steps that make the invention uniqueand patentable. Interestingly, in all areas, the number of claims look similar. Over the years, all the values havemostly a downward trend.",
  "Graph Neural Networks.Consider a graph, de-noted as G = (V, X, A), consisting of a set ofnodes (V ) and a set of edges (E). Let X Rnd": "represent the d-dimensional features of n nodesin V , while A {0, 1}nn is the adjacency ma-trix specifying edges in the edge set E. GraphNeural Networks (GNNs) (Kipf and Welling, 2016;Hamilton et al., 2017; Velickovic et al., 2018) haveproven to be effective in making predictions onsuch graphs by learning relevant low-dimensionalnode representations through a message-passingmechanism.During message passing, each node (u V ) up-dates its representation by aggregating informationfrom itself and its set of neighbors N(u). Mathe-matically, the update in l-th step can be representedas follows:",
  "h(l)u = AGGR(h(l1)u, {h(l1)i|i N(u)})(1)": "where h(l)u is the updated representation of nodeu at iteration l, obtained by applying the aggrega-tion operation (AGGR) to combine its previousrepresentation (h(l1)u) with those of its neighbor-ing nodes. The representation at the 0-th step is theinitial feature set of the nodes. GNNs iterativelyapply this aggregation scheme to refine the node representations, capturing the structural dependen-cies within the graph. The GNNs are effective fora wide range of prediction tasks over graphs suchas node classification, link prediction, and graphclassification. We use three types on GNNs for ourstudy.(1) GCN (Kipf and Welling, 2016): In the mes-sage passing framework, GCN uses sum as its Ag-gregation function. The propagation rule is as fol-lows:",
  "H(l) = (D1/2 AD1/2H(l1)W (l1))(2)": "where A = A + I is the adjacency matrix withself connections. W (l1) is layer specific weightmatrix. is the activation function. Hl is matrix ofactivation in l th layer. In theory, GCN considersspectral convolution on graph as a multiplicationof signal and filter. (2) GraphSage (Hamilton et al., 2017): Graph-Sage extends the ideas of message aggregation intwo important ways. First, considers multiple ag-gregator functions like mean, element wise maxpooling and LSTM. Second, it concatenates nodescurrent representation with the aggregated neigh-borhood vector.",
  "h(l)u = (WConcat(h(l1)u, AGGl1u)": "(3) Graph Transformer Network (GTN) (Yunet al., 2019): Graph Transformer Networks (GTN)uses self-attention mechanisms to capture relation-ships between the nodes in the graph. This self-attention mechanism makes it more effective in thetraditional prediction tasks over graphs. When X isthe set of node features, we can represent the nodeembeddings as H = Enc(X), where Enc is the en-coding function, typically based on self-attentionmechanisms. The self-attention mechanism com-putes attention scores between nodes and combinestheir features accordingly:",
  "(H Wv)": "Here, Att, , Wq, Wk, and Wv are Attention,softmax function, learnable weight matrices, anddk is the dimension of the key vectors. GTNsoften employ multi-head attention, which allowsthe model to focus on different aspects of thegraph simultaneously. The final output from theself-attention mechanism is typically used to per-form a graph convolution operation.This op-eration aggregates information from neighboringnodes to update node features. The graph convo-lution can be represented as: GraphConv(H) = (MultiHead(H) Wo). Here, is the activationfunction, and Wo is another learnable weight ma-trix.",
  "A.5Experimental Settings": "A.5.1DatasetThe focus of the study is on building methods topredict a quality of a patent from its citations. Es-sentially, we aim to classify patents with high andlow citations. Thus, given a new patent we wouldbe predict whether the patent will have high orlow citations. We use three types of patent datathat are prepared for three major CPC (CooperativePatent Classification)4 classes: A61, H04, and G06.This results in nine separate datasets with threedifferent periods of citation history from the yearof 2000: (1) Citation history for 3 years (3-years-history): Patents published until 2019 as we can",
  "count citation up to 2022; (2) Citation history for3 years (5-years-history): Patents published until2017. (3) Citation history for 3 years (10-years-history): Patents published until 2011": "Dataset Preparation for Classification TaskRecent two years of data from each patent dataset(among the nine datasets above) are kept for testing.The remaining data is used for model training. Thistest dataset is designed to understand the model be-havior to predict citation behavior on new unseenpatents. A.5.2Training and Test Data SplitWe consider several variations in splitting the Train-ing and Test dataset corresponding various experi-ments. The detailed description on class sizes andtrain/test splits are provided in Tables 9,10,11,12.We have considered different cut-off thresholds todetermine a patent to be highly cited or lowly cited.The cut-offs corresponding to experiments in Ta-ble 2 for highly cited patents in the A61, H04, andG06 CPC classes are 18, 15, and 16 citations re-spectively, with patents below these cut-offs areconsidered lowly cited. : Train/Test Data Distribution corresponding toexperiments in : 3-years-history (Train: 2000-2017, Test: 2018-2019), 5-years-history (Train: 2000-2015, Test: 2016-2017), 10-years-history (Train: 2000-2010, Test: 2011-2012)",
  "YearA61H04G06": "ity patents. Thus, we use Recall for the patentswith high citations. It measures the modelsability to identify patents with high citationout of all patents with high citations. A highrecall would suggest that the model has highcapability to identify high-quality patents. F1-Score on positive class: This assessesthe models ability to accurately predictpatents with high citations while balancingbetween precision and recall: F1positive =2PrecisionpositiveRecallpositivePrecisionpositive+Recallpositive . A.5.4Other SettingsAll experimental work has been conducted with aGoogle Cloud Ubuntu virtual machine with 64 GBof RAM and 8 vCPUs (equivalent to 4 physicalCPU cores). We have also set the maximum num-ber of epochs to 500, the optimizer as Adam opti-mizer, weight decay of 5e4, loss function as thecross-entropy function. We systematically vary the",
  "PatentBERT-GSAGE0.730.840.890.720.600.670.600.670.68": ": Accuracy for Citation classification (top vs bottom). We use Top 10% as the highly cited category (positiveclass) and Bottom 10% as the low cited category (negative class). Our graph-based methods often produce the bestresults (blue) and accuracy up to .89 indicating that they are effective in patent citation prediction."
}