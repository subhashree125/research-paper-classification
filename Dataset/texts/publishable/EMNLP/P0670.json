{
  "Abstract": "Large Language Models (LLMs) possess exten-sive knowledge and commonsense reasoningcapabilities, making them valuable for creatingpowerful agents. However, existing LLM agentframeworks have not fully utilized past experi-ences for improvement. This work introduces anew LLM-based agent framework called Retro-spex , which addresses this challenge by analyz-ing past experiences in depth. Unlike previousapproaches, Retrospex does not directly inte-grate experiences into the LLMs context. In-stead, it combines the LLMs action likelihoodwith action values estimated by a Reinforce-ment Learning (RL) Critic, which is trained onpast experiences through an offline retrospec-tion process. Additionally, Retrospex employsa dynamic action rescoring mechanism that in-creases the importance of experience-based val-ues for tasks that require more interaction withthe environment. We evaluate Retrospex inScienceWorld, ALFWorld and Webshop envi-ronments, demonstrating its advantages overstrong, contemporary baselines1.",
  "Introduction": "The emergence of LLMs has paved the way for thedevelopment of LLM-based agents. These agentsleverage the vast knowledge and commonsense rea-soning capabilities within LLMs to tackle a widerange of tasks (Wang et al., 2022a; Yao et al.,2022a; Shridhar et al., 2020; Yang et al., 2018;Thorne et al., 2018). Despite their potential, a sig-nificant challenge arises from the dependence ongeneral-purpose LLMs. Specifically, these agentsmight not be sufficiently adapted to the specificenvironments, potentially hindering their task com-pletion effectiveness.Training LLM-based agents for new environ-ments poses significant challenges. A commonapproach is to fine-tune the LLM using correct demonstrationssample trajectories that success-fully complete the task (Qin et al., 2024; Lin et al.,2023; Zeng et al., 2024). However, this approachfocuses on correct behaviors, limiting the agentsability to learn and recover from mistakes. Re-cently, efforts have been made to leverage imper-fect experiences for training LLM agents. Thesemethods fall into two categories: those that relyon working memory, like Reflexion (Shinn et al.,2023), and those that utilize cross-task experi-ences from long-term memory, such as Remem-berer (Zhang et al., 2024). Despite the progress,experiences are still not sufficiently used, as theyare only integrated into the LLMs context. Due tothe limited context length of LLMs, this constrainsthe inclusion of more comprehensive experiences.In this work, we propose a novel LLM-basedagent framework called Retrospex, which collectscross-task experiences for training a ReinforcementLearning (RL) Critic in a retrospection stage.The RL Critic is then used to support the LLMin decision making. Unlike previous studies (see), Retrospex does not directly integrate ex-periences into the context. Instead, it exploits anaction rescoring strategy to combine the likelihoodof the actions provided by the LLM and the actionvalues estimated by the RL Critic. In addition, Ret-rospex dynamically increases the weight of actionvalues from the RL Critic for tasks that requiremore interaction steps with the environment, allow-ing experiences to gradually play a more importantrole in difficult tasks.Retrospex has several advantages over previousapproaches. First, compared to RL-based agents,Retrospex can leverage the strength of LLMs formore effective decision making. Second, comparedto previous LLM-based agents, Retrospex can bet-ter utilize experiences without increasing the con-text length. Third, Retrospex is more flexible incontrolling how much experience is needed at eachstep thanks to the dynamic scoring method. Finally, LLM",
  ": Comparing different architectures for LLM-based Agents": "Retrospex is general and can be adapted to variousLLMs or RL methods. In this paper, we imple-ment RL Critic with a lightweight neural network,thus providing little inference overhead comparedto using only LLMs for action selection.We evaluate Retrospex in three text-based sim-ulation environments: ScienceWorld (Wang et al.,2022a), ALFWorld (Shridhar et al., 2020), andWebshop (Yao et al., 2022a). The experimentalresults demonstrate that integrating the RL Criticand dynamic action scoring in Retrospex enhancesthe performance of LLM-based agents, leading tosuccess rate improvements of 9% in ScienceWorld,3.5% in ALFWorld, and up to 5% in Webshop.Our contributions are summarized as follows:",
  "LLMs have been exploited to tackle a wide rangeof tasks such as reasoning (Wei et al., 2022; Ko-jima et al., 2022; Yao et al., 2024), self-verification": "(Wang et al., 2022b; Miao et al., 2023), problem de-composition or formalization (Madaan et al., 2024;Zhou et al., 2022), and planning (Yao et al., 2022b;Wu et al., 2023; Wang et al., 2023).Most of these aforementioned studies, however,do not utilize the agents past experiences forperformance improvement. To overcome this is-sue, recent studies leverage relevant experiences toprompt LLM for reasoning, allowing LLM-basedagents to learn from previous mistakes. Notableexamples include Relexion (Shinn et al., 2023), Re-memberer (Zhang et al., 2024), Salam (Wang andLi, 2023) and ExpeL (Zhao et al., 2024). How-ever, this approach is still limited by the contextlength of LLMs, hindering the ability to fully uti-lize past experiences. Our proposed approach com-bines LLMs action likelihood with RL Criticsaction values for action reranking. By doing so,there is no need to incorporate experiences into theLLM context, thereby mitigating the problem oflong context.",
  "LLM combined with RL": "RL has traditionally been used to train agents ca-pable of making sequential decisions. With theadvent of large language models (LLMs), many ef-forts have emerged to integrate LLMs with RL foragent training. These approaches can be broadlycategorized into two groups, as outlined below.The first group uses RL techniques to train LLMsas policy models for acting in new environments.This includes GPT-Critic (Jang et al., 2022), LID(Li et al., 2022), AgentTuning (Zeng et al., 2024),PAC (Springenberg et al., 2024), and A3T (Yanget al., 2024). LID uses LLMs for policy initializa-tion, while AgentTuning applies imitation learning (IL) to train adaptable agents. GPT-Critic and PAC,on the other hand, train LLMs as both critics and ac-tors using offline RL. Like AgentTuning, A3T andLID, we use IL to train a base LLM for decision-making. However, unlike these methods, Retro-spex also focuses on enhancing the base LLMsperformance during inference without LLM update.This approach avoids the computational cost andpotential risk of weakening the LLMs general ca-pabilities that could arise from frequent updates.The second group uses RL methods to train as-sistants that support LLMs in decision-making viaprompting, including Salam (Wang and Li, 2023),SayCan (Brohan et al., 2022), and Rememberer(Zhang et al., 2024). Salam uses IL to train anassistant who corrects mistakes and provides guide-lines. Rememberer uses Q-learning to estimate ac-tion values for past experiences stored in memory.During inference, Rememberer retrieves the mostrelevant experiences (along with their correspond-ing action values) and incorporates them into theLLMs context. Both Salam and Rememberer ex-tend the LLMs context with additional information.SayCan, on the other hand, estimates an affordancefunction to help ground the LLMs actions. Thefinal action probability is calculated by combiningthe LLMs likelihood with the affordance value.Our approach is most closely related to Say-Can but differs in two key aspects: 1) SayCansaffordance function is used for action grounding,whereas RL Critic in Retrospex is for action re-evaluation. SayCan allows for combining any LLMwith any affordance function, even independentlytrained ones. In contrast, we train an RL Criticon the action distribution supported by the LLM,enabling better value estimates for LLMs actions;2) Retrospex exploits dynamic scoring, whereasSayCan employs a static score combination.",
  "Methodology": "demonstrates the training process of Ret-rospex which involves a warm-up phase and a retro-spection phase. During the warm-up phase, we fine-tune the LLM based on expert (e.g. human) demon-strations and collect the working experiences of theLLM agent. In the retrospection stage, we train anRL Critic from the LLM agents experiences usingImplicit Q-learning (IQL), an offline RL method.By doing so, the RL Critic is expected to learn fromthe LLM agents mistakes and assist in making bet-ter decisions in the future.",
  "Imitation LearningInspired by (Lin et al., 2023;": "Zeng et al., 2024), we cast the action predictiontask as a text generation task. We fine-tune theLLM with expert demonstrations (i.e., golden tra-jectories). The objective is to equip the LLM withfundamental knowledge about the agents environ-ment. This process, known as Imitation Learning(IL), is essential for LLMs of moderate size but canbe skipped for powerful LLMs such as GPT-4.Formally, we train the LLM policy y = (x) sothat the generated action y is the most likely action(x) taken by a human expert. Here, x is the givencontext that contains a task description, and a se-quence of states, and actions. A state encapsulatesthe environment information at a specific time. Forsimplicity, we assume that states can be inferredfrom the initial agent state and subsequent obser-vations from the environment. A golden trajec-tory = {task1, s1, a1, s2, a2, s3, a3} can be de-composed into multiple (training) instances (x1 ={task1, s1}, y1 = a1), (x2 = {task1, s1, a1, s2},y2 = a2), and (x3 = {task1, s1, a1, s2, a2, s3},y3 = a3). The training objective then involvessolving the following optimization problem:",
  "xLNLL ((x), (x))": "where T denotes the set of golden trajectories and is one particular trajectory. LNLL represents thenegative log-likelihood loss, (x) denotes the ex-perts action for state x, and LLM is the estimatedpolicy model (the fine-tuned LLM). Collecting ExperiencesDue to the complexityof the environment and the limited size of demon-stration data, IL is often insufficient for obtainingan optimal policy. As such, we collect the expe-riences of the trained LLM interacting with theenvironment. Here, the format of each experiencetrajectory is similar to that of a golden trajectory,but an experience may contain suboptimal actionsand/or be a failed attempt to finish users tasks.",
  "Retrospection Stage": "The task of sequential decision can be formal-ized as a Markov Decision Process (MDP), whichis denoted as (S, A, p0(s), p (s | s, a) , r(s, a), ).Here, S, A are the state and action spaces, p0 is theinitial state distribution, p (s | s, a) is the environ-ment dynamics, r(s, a) is a reward function, and is a discount factor. The objective is to find a policy IL & Collecting ExperiencesWarm-up StageRetrospection Stage",
  "Mem": "Training tuples:(, , ,) (, , ,) ..... Q-Network (RL Critic) Implicit Q-Learning (IQL) : The training process of Retrospex includes two stages: 1) In the Warm-up stage, an imitation learning(IL) base agent is trained and used for experience collection; 2) In the Retrospection stage: Offline RL is used totrain RL Critic from offline experiences. Here, s and a denote states and actions, respectively. In the retrospectionstage, s and a indicate the following state and action.",
  "| s0 p0, at ( | st) , st+1 p ( | st, at)]": "RL can be used to solve the MDP problem andfind using interaction data. In general, RL canbe conducted online, where we update the LLM-based agent whenever we have a new experience.However, doing so can be expensive and unstable(Nottingham et al., 2023). As a result, we followthe offline RL approach, where we collect experi-ences to memory, and update the LLM-based agentonce we have enough experience.Offline RL uses a fixed experience memory totrain the action-value function Q(s, a). Here, theQ value corresponds to the expected cumulative re-ward (return) obtained by starting from the state s,performing action a, and then following the policy. This work exploits Implicit Q-Learning (IQL)(Kostrikov et al., 2022), which aims to handle theissue of overestimating Q-function due to unseenactions in offline RL. IQL builds on approximatedynamic programming methods that minimize tem-poral difference errors as follows:",
  "Here, D is the experience memory, s, s are thecurrent and next states respectively. A target Q-": "network Q is used for action selection, and anonline Q-network Q is used for value estimationupdate at each training step. After each trainingbatch, the target network is updated based on theonline network. To prevent the target network fromselecting actions that are not supported in the expe-rience memory (due to maxa Q) , IQL applies aseparate state value function V (s) to estimate:",
  "LV () = E(s,a)D[L2(Q(s, a) V(s))]": "Let u = Q(s, a)V(s), L2(u) = | 1(u <0)|u2 is the upper expectile function. It has beenproven that, by optimizing the above objective, wefit V(s) to approximate the maximum of Q overactions supported by the data distribution when 1 (Theorem 3 by Kostrikov et al. (2022)).After this estimation, IQL can apply V (s) to updatethe Q(s, a) with simple MSE loss.",
  "LQ() = E[(r(s, a) + V(s) Q(s, a))2]": "where the expectation is calculated by sampling(s, a, s) D. The value functions (Q-networkand V-network) can be realized in many forms,here we use GRU neural networks as shown in Fig-ure 2. We encode task description, state, and actionseparately with different GRU blocks, then concate-nate the embeddings together and send them to thenext linear layers. We use 2 linear layers after theencoding layer to get the final q and v values. Thestructure of the V-network is similar to Q-networkexcept that we do not have action a as the input andthe output is the state value V (s). Inference StageDynamic Action Rescoring LLMTop-k actions RL Critic",
  "S(a) = (t)p + (1 (t))q": "where b, d and (t) is the dynamic com-bination weight between p and q. When t = 0(corresponding to (t) of 1), we have few obser-vations and rely more on the commonsense of theLLM to guide action selection. As t increases, (t)decreases with a discount factor d, allowing theRL Critic to have a greater influence on decision-making. However, we set a lower bound of b for(t) to ensure the LLMs role is not reduced toodrastically for long trajectories.",
  "Experiments": "The experiments are conducted in three environ-ments: ScienceWorld (Wang et al., 2022a), ALF-World (Shridhar et al., 2020) and Webshop (Yaoet al., 2022a). We use Average Score (AS) andSuccess Rate (SR) to measure the performance2.Both metrics (AS and SR) are scaled to the rangeof in all three environments. The train-ing and testing sets for the three environments aresummarized in and .",
  "ScienceWorld": "Experimental SetupScienceWorld is a complexinteractive text environment that tests an agents sci-entific commonsense. In this environment, agentsare required to navigate through 10 interconnectedlocations (e.g., workshop, kitchen) and utilize thetools to complete tasks such as determine if themetal fork is electrically conductive. The environ-ment contains 200 objects, 25 high-level actions,resulting in approximately 200k possible action-object combinations. We collect 2157 golden (i.e.successful) trajectories to train Flan-T5-large3 in",
  "SciWorldFlan-T5-large (770M)2157 goldenGRU (2.7M)256636ALFWorldLLaMA3-8B-InstructAgentInstruct+GRU (2.7M)200067WebshopGRU (2.7M)150044": ": Training data used in the warmup and retrospection stages of Retrospex. Here, AgentInstruct+ is adataset used by (Zeng et al., 2024) for agent training, which consists of 1866 golden trajectories from a mix of 6environments including Webshop (351 golden trajectories) and ALFWorld (336 golden trajectories). SR denotes thepercentage of the successful trajectories in the memory used for retrospection stage training. We train one LLM forboth ALFWorld and Webshop environments.",
  "SciWorld30270ALFWorld6134Webshop-100 / 200 / 251": ": The number of subtasks and testing samplesin the three tested environments. For Webshop, weconduct evaluation on three different test sets used byZhang et al. (2024) (100 samples), Zeng et al. (2024)(200 samples), and Yang et al. (2024) (251 samples). the warm-up stage, and 2566 trajectories, whichcontain both fail and successful ones with SR of36%, for training GRU-based RL Critic in the ret-rospection stage. In the warm-up stage, we followthe same training strategy for imitation learning(IL) specified in SwiftSage (Lin et al., 2023). Wedenote this IL agent as IL-T5, which corresponds tothe Swift model in SwiftSage as well as Retrospex(w/o retrospection). For dynamic re-scoring, wechoose d = 0.97, b = 0.6 as our hyper-parameters. BaselinesWe compare Retrospex to baselines ofdifferent types: (1) LLM-based agents ReAcT (Yaoet al., 2022b) and Reflexion (Shinn et al., 2023); (2)online RL agent DRRN (He et al., 2016); and (3)SayCan (Brohan et al., 2022) which combines LLMwith an affordance function for action grounding.The details for SayCan, ReAct and Reflexion areprovided in SwiftSage paper (Lin et al., 2023). Itis noteworthy that we do not compare to SwiftSagehere as it exploits two large language models (GPTand IL-T5) for inference, resulting in a somewhatunfair comparison. The results of GPTP3.5-basedReAct and DRRN are produced by ourselves.",
  ": The AS and SR on ScienceWorld. Here, denotes the results from SwiftSage (Lin et al., 2023)": "the performance of DRRN is significantly worsethan other baselines, confirming the challenge oflearning an independent RL agent in an environ-ment with a large action space. Secondly, GPT4-based ReAct models can achieve relatively goodresults without any training, suggesting that power-ful LLMs can exploit its commonsense to recognizemeaningful action-object combinations for betterresults. Thirdly, the result of SayCan is not satisfac-tory with GPT4+SBERT, suggesting that exploit-ing a value function that is trained independentlyfrom the LLM-based actor might not be optimal.Last but not least, it is observable that IL-T5, arelatively small-size LLM-based IL agent, can out-perform Reflexion, which is based on the powerfulGPT4 model. This shows that the knowledge ofsmall size LLM (T5) might be sufficient for Sci-enceWorld, and IL is important to ground an agentin the targeted environment. In addition, the factthat Retrospex significantly outperforms IL-T5 by7 points in AS and 9 points in SR suggests the im-portance of the retrospection stage for agents toexplore and learn from mistakes. A more detailedlist of all 30 sub-tasks of ScienceWorld can be seenin in the Appendix.",
  "ALFWorld": "Experimental SetupALFWorld is also a suiteof text-based environments that challenge an agentto solve multi-step tasks based on TextWorld (Ctet al., 2018). The action and task formats of ALF-World are similar to ScienceWorld but simpler. Weexploit the AgentInstruct+ dataset for training aLLaMA3-8B-Instruct-based IL agent in the warm-up stage and collect 2000 trajectories with SR of67% for restrospection stage training. Here, Agent-Tuining+ includes both AgentInstruct4 for agentlearning capabilities and ShareGPT5 for generalcapability. The AgentInstruct+ dataset contains thegolden trajectories of both ALFWorld and Web-shop environments, thus we train only one IL agentfor being used in both environments. The combi-nation parameters of dynamic action rescoring inRetrospex for ALFWorld are d = 0.95, b = 0.6. BaselinesWe compare to Reflexion (GPT 3.5)and ExpeL (Zhao et al., 2024) and A3T (Yanget al., 2024). Concurrent to our work, Yang et al.(2024) proposes A3T, a self-improvement frame-work to train LLM agents by updating LLM multi-ple rounds. For the first round (round=0), imitationlearning is used to train LLM from golden trajec-tories, which is similar to our warm-up stage. Forother rounds (round > 0), A3T updates LLM usingcontrastive learning methods like DPO (Rafailovet al., 2023). A3T also proposes an interestingmethod for generating composed trajectories forexploration. The details of Reflexion, ExpeL, A3Tcan be found in A3T paper (Yang et al., 2024).",
  "Results presents the performance of com-pared methods on the ALFWorld environment. The": "results indicate that Retrospex outperforms thebase IL model (IL-LLaMA3) and A3T (round=0)but falls short when compared to A3T (round=1).Upon examining the training details of A3T, weobserve that A3T leverages 981 golden trajectoriesfor round=0 and 3431 trajectories (with a 90.2%success rate) for round=1. In contrast, Retrospexuses only 351 Webshop golden trajectories duringthe warm-up phase and 2000 trajectories with alower success rate of 67% for the retrospectionphase (see ). The discrepancy in data qual-ity and quantity accounts for the underperformanceof IL-LLaMA3 compared to A3T (round=0) andof Retrospex compared to A3T (round=1).Although Retrospex does not achieve state-of-the-art (SOTA) performance on ALFWorld, ourretrospection strategy remains valuable for threekey reasons. First, the retrospection phase signifi-cantly improves the base IL agent, as evidenced bythe 3.5% success rate (SR) increase of Retrospexcompared to IL-LLaMA3. Second, the retrospec-tion phase is much more cost-effective than a fulltraining round in A3T. Specifically, A3T requiresdirect updates to a large (7B) LLM, while Retro-spex only updates a smaller RL-Critic modelaGRU with 2.7M parameters. Frequently updatingLLM is not only computationally expensive butalso risks weakening its general capability due tocatastrophic forgetting. Finally, the strategies ofRetrospex and A3T can be combined as a smallRL-Critic model can be trained to assist with infer-ence between LLM update cycles in A3T, offeringa more practical approach.",
  "Experimental SetupWebshop (Yao et al.,": "2022a) is an online shopping website environmentwith 1.18M products and 12k human instructions.An agent is required to purchase a product basedon a user instruction such as I am looking for anightstand with drawers. To complete the task, theagent needs to perform actions such as searchingnightstand drawers, or choosing clickable buttons.We use the same IL-LLaMA3 as the IL base agentas described in the previous section. The RL-Critic,however, is trained specifically for Webshop. Theparameters of dynamic action rescoring for Web-shop are d = 0.9, b = 0.5.",
  ": Overall results on Webshop, where and results are from (Zhang et al., 2024) and (Zeng et al.,2024). The result of A3T is from (Yang et al., 2024)": "duct evaluations on different subsets of the originalWebshop test tasks, we conduct multiple tests forfair and comprehensive comparisons. Specifically,Rememberer uses the first 100 samples for testingwhereas AgentLM uses 200 samples. A3T bothcompares with AgentLM 6 and reports the resulton AgentBoard test set. Here, we compare to A3Tresults on the AgentBoard test set. Results shows the performance of Ret-rospex and other baselines on Webshop environ-ment. The experiment verifies the effectiveness ofRetrospex over Rememberer, AgentLM, and A3T(round=0), (round=1) on their respective reportedtest sets. It is noteworthy that Retrospex performswell compared to A3T even though we use onebase LLM for ALFWorld and Webshop, whereasA3T finetunes another LLM specifically for Web-shop. The retrospection stage in Retrospex helpsimprove the success rate (SR) significantly overthree test sets and improves AS in two over threetest sets. One explanation for why Retrospex ob-tains higher SR yet lower AS in the Rememberertest set is that IL-LLaMA3 may not return correctproducts in many cases (fail cases) yet it returnsclose enough products (high scores). We leavefurther investigation to future work.",
  "Analysis on Task Complexities": "To evaluate the performance of Retrospex acrossvarying task complexities, we categorize the tasksin ScienceWorld into three levels: short (fewer than20 steps), medium (20 to 50 steps), and long (morethan 50 steps). We then calculate the average score(AS) for each complexity level, with the results pre-sented in . Across all three levels, Retrospexdemonstrates significant improvements. Notably,tasks with medium-length trajectories show an av-erage score increase of more than 10 points, whiletasks with short and long trajectories see improve-ments of over 5 points.We also report the results for T5-then-IQL,where the top actions are reranked based solelyon RL-Critic scores. The inferior performance ofT5-then-IQL compared to IL-T5 suggests that theLLMs likelihood should not be disregarded whenselecting actions. This performance drop is espe-cially pronounced in tasks with short trajectories,highlighting the importance of LLM when we havefewer observations. This supports our intuition be-hind dynamic scoring, where we place greater trustin the LLM when the step count t is small.",
  ": Results on ScienceWorld with different dynamic scoring parameters": "select and fix a combination of the action likelihoodand value based on the study of different b values.The last column of shows the score witha static combination. It is observable that staticcombination underperforms dynamic combination,verifying the role of the discount factor d in incor-porating more experiences for long-horizon tasks. Parameter ChoiceWe analyze the two param-eters d and b in dynamic action rescoring. First,when d is small, the weight of the LLM score de-creases rapidly within a smaller number of steps.Consequently, the agent quickly shifts the focusto action values from IQL, leading to a drop inperformance. Secondly, for our method to be suf-ficiently effective, the LLM still needs a relativelylarge weight even at the end of a long trajectory. Assuch, we need to keep the value of b high to ensurethat the validity of the LLM scores is maintained.",
  "Analysis on Inference Time Cost": "Given the context of length N containing past in-teractions and thoughts, LLM-based agents needto generate the next action. As the action lengthis often much shorter compared to the contextlength, we simplify the analysis by estimating theinference time for ReAct, Reflexion, Remembererand Retrospex to generate one-token action withTransformer-based LLM. ReActThe time for ReAct to generate one-tokenaction is dominated by attention operations in LLM,which is N2 T1 where T1 is the computationaltime depending on the LLM model and the hard-ware infrastructure.",
  "RemembererThe time complexity is (N +Kl)2T1+(MT2+M log K). Here, K indicates thenumber of experiences incorporated into the LLM": "context, and l is the average experience length. As-suming bruteforce search with the support of amax-heap, MT2 + M log K is the time for retriev-ing K relevant experiences in the memory of sizeM, and T2 is the time for calculating the similaritybetween the current trajectory and each trajectoryin the memory. The size of memory M will be ac-cumulated, leading to longer inference over time. RetrospexThe time for one-token action gener-ation is N2 T1 + KT3 where T3 is the time forcalculating the Q-value with GRU. As T1 T3,Retrospex adds little inference overhead comparedto ReAct. However, as we can obtain better perfor-mance with Retrospex with smaller LLM (smallerT1), Retrospex can still win in inference time com-pared to ReAct based on GPT4. Compared to Re-flexion and Rememberer, due to shorter context,Retrospex is more efficient. In Retrospex, we stillneed to sample top-K actions, however, this is doneonly on the last layer, which is much less demand-ing compared to N2 T1 for LLM.",
  "Conclusion": "This work introduces a novel LLM-based agentframework, named Retrospex, that addresses thelimitations of prior approaches in leveraging expe-riences for decision-making. Retrospex overcomesthe context length restriction by separating experi-ence analysis (through a Reinforcement LearningCritic) from LLM-based action selection. Thisenables the agent to effectively utilize past experi-ences while retaining the strengths of LLMs. Ad-ditionally, the dynamic action rescoring methodallows for flexible control over the influence of ex-periences based on task complexity. Evaluationsdemonstrate that Retrospex achieves significant per-formance improvements compared to strong base-lines, highlighting its potential for real-world appli-cations of LLM agents.",
  "Limitations": "There are several limitations to our work. First,LLM-dependent action sampling and short-termevaluation can include limitations and biases fromthe LLM itself. In addition, the closely relatedmemory can also suffer from distributional biasesin trajectories and lack of exploration of the actionspace. Future work can be investigated to expandthe action exploration in the collection stage, thusgathering trajectories with more diversity. Second,current work does not explore the potential of in-corporating verbal feedback such as those fromReflexion for better retrospection of past experi-ences. Third, it will be interesting to improve thedynamic action scoring with an automatic modulethat decides the weights of experiences instead ofrelying on predefined hyperparameters.",
  "Ethic Statement": "In our study, we utilize open simulation environ-ments, ensuring that there is no direct interactionwith humans that could potentially cause harm. Thetraining data used in our experiments is sourcedfrom publicly available datasets, all of which arethoroughly cited and referenced in the main textto maintain transparency. By limiting our work tosimulated settings and publicly accessible data, weminimize ethical concerns related to privacy, con-sent, and safety. However, it is important to empha-size that while the current study avoids real-worldrisks, the broader application of LLM-based agentsrequires careful consideration. As these technolo-gies are increasingly deployed in real-world set-tings, it is essential to ensure that they are alignedwith human values, respect ethical guidelines, andmitigate potential biases.",
  "Scott Fujimoto, Herke Hoof, and David Meger. 2018.Addressing function approximation error in actor-critic methods. In International conference on ma-chine learning": "Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen,Jianfeng Gao, Lihong Li, and Li Deng. 2016. Deepreinforcement learning with a combinatorial actionspace for predicting popular reddit threads. In Pro-ceedings of the 2016 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP. Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim. 2022.Gpt-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems. In Ninth Inter-national Conference on Learning Representations,ICLR. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In Thirty-sixthConference on Neural Information Processing Sys-tems, NeurIPS.",
  "Ilya Kostrikov, Ashvin Nair, and Sergey Levine.2022. Offline reinforcement learning with implicitq-learning. In Tenth International Conference onLearning Representations, ICLR": "Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clin-ton Wang, Linxi Fan, Tao Chen, De-An Huang, EkinAkyrek, Anima Anandkumar, et al. 2022.Pre-trained language models for interactive decision-making. In Thirty-sixth Conference on Neural In-formation Processing Systems, NeurIPS. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brah-man, Shiyu Huang, Chandra Bhagavatula, PrithvirajAmmanabrolu, Yejin Choi, and Xiang Ren. 2023.Swiftsage: A generative agent with fast and slowthinking for complex interactive tasks. In Thirty-seventh Conference on Neural Information Process-ing Systems, NeurIPS. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang,Yujiu Yang, Yaohui Jin, Zhenzhong Lan, LingpengKong, and Junxian He. 2024. Agentboard: An ana-lytical evaluation board of multi-turn llm agents. InTwelfth International Conference on Learning Repre-sentations, ICLR.",
  "Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023.Selfcheck: Using llms to zero-shot check their ownstep-by-step reasoning. In Twelfth International Con-ference on Learning Representations, ICLR": "Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim,JB Lanier, Pierre Baldi, Roy Fox, and Sameer Singh.2023. Selective perception: Optimizing state descrip-tions with reinforcement learning for language modelactors. arXiv preprint arXiv:2307.11922. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, et al. 2024. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.In Eleventh International Conference on LearningRepresentations, ICLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Thirty-seventhConference on Neural Information Processing Sys-tems, NeurIPS.",
  "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,Shaden Smith, and Yuxiong He. 2021. Zero-infinity:Breaking the gpu memory wall for extreme scaledeep learning": "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing, EMNLP-IJCNLP. Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik R Narasimhan, and Shunyu Yao. 2023. Re-flexion: Language agents with verbal reinforcementlearning. In Thirty-seventh Conference on NeuralInformation Processing Systems, NeurIPS. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2020.Alfworld: Aligning text andembodied environments for interactive learning. InEighth International Conference on Learning Repre-sentations, ICLR. Jost Tobias Springenberg, Abbas Abdolmaleki, Jing-wei Zhang, Oliver Groth, Michael Bloesch, ThomasLampe, Philemon Brakel, Sarah Bechtle, Steven Kap-turowski, Roland Hafner, et al. 2024. Offline actor-critic reinforcement learning scales to large models.In Proceedings of the Fourty-first International Con-ference on Machine Learning, ICML.",
  "verification. In 2018 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACLHLT": "Danqing Wang and Lei Li. 2023. Learning from mis-takes via cooperative study assistant for large lan-guage models. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP. Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, andPrithviraj Ammanabrolu. 2022a. Scienceworld: Isyour agent smarter than a 5th grader? In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, EMNLP. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022b. Self-consistency improveschain of thought reasoning in language models. InEleventh International Conference on Learning Rep-resentations, ICLR. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu,Xiaojian Ma, and Yitao Liang. 2023. Describe, ex-plain, plan and select: interactive planning with llmsenables open-world multi-task agents.In Thirty-seventh Conference on Neural Information Process-ing Systems, NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. In Thirty-sixth Con-ference on Neural Information Processing Systems,NeurIPS. Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhut-dinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, andShrimai Prabhumoye. 2023. Plan, eliminate, andtracklanguage models are good teachers for embod-ied agents. arXiv preprint arXiv:2305.02412. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William Cohen, Ruslan Salakhutdinov, and Christo-pher D Manning. 2018. Hotpotqa: A dataset fordiverse, explainable multi-hop question answering.In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing, EMNLP.",
  "Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang,and Yang Liu. 2024. React meets actre: When lan-guage agents enjoy training data autonomy. CoRR": "Shunyu Yao, Howard Chen, John Yang, and KarthikNarasimhan. 2022a. Webshop: Towards scalablereal-world web interaction with grounded languageagents. In Thirty-fifth Conference on Neural Infor-mation Processing Systems, NeurIPS. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, TomGriffiths, Yuan Cao, and Karthik Narasimhan. 2024.Tree of thoughts: Deliberate problem solving withlarge language models. In Thirty-seventh Conferenceon Neural Information Processing Systems, NeurIPS. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao.2022b. React: Synergizing reasoning and actingin language models. In Eleventh International Con-ference on Learning Representations, ICLR. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, XiaoLiu, Yuxiao Dong, and Jie Tang. 2024. Agenttuning:Enabling generalized agent abilities for llms. In Find-ings of the Association for Computational LinguisticsACL. Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu,Zihan Zhao, and Kai Yu. 2024.Large languagemodels are semi-parametric reinforcement learningagents. Thirty-seventh Conference on Neural Infor-mation Processing Systems, ICLR. Andrew Zhao, Daniel Huang, Quentin Xu, MatthieuLin, Yong-Jin Liu, and Gao Huang. 2024. Expel:Llm agents are experiential learners. In Proceedingsof the AAAI Conference on Artificial Intelligence,AAAI. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei,Nathan Scales, Xuezhi Wang, Dale Schuurmans,Claire Cui, Olivier Bousquet, Quoc V Le, et al. 2022.Least-to-most prompting enables complex reasoningin large language models. In Eleventh InternationalConference on Learning Representations, ICLR.",
  "ASupplementary Details for DynamicRescoring Method": "Our method merges the probability of LLM andthe Q value from the IQL together and selects thefinal action. For detail, the LLM first generates sev-eral responses by nuclear sampling. After mappingthe responses into action space as aforementioned,LLM provides the probabilities p of these actioncandidates. We then normalize these values to ob-tain LLM scores. Here p means the probabilitiesof all actions given the current state.",
  "max(p) min(p)(1)": "The top-k actions are fed into the RL Critic. Thevalue function will give the action value q for eachaction, which is then normalized as follows. Hereq means the q values of all actions at the currentstate. When the q values are equal to each other,we give a score of 0.5 to all actions.",
  "S(a) = (t)p + (1 (t))q(4)": "where (t) is the dynamic combination weight be-tween p and q which changes with different valuesof the step t. In the first step t = 0, (t) is 1 andthe agent trusts the trained LLM completely. Thisis because when the trajectory is short with fewobservations from the environment, the LLM agentrequires fewer experiences for decision. As the stept increases, (t) will decline with a discount factord, giving more chance for RL Critic to influencethe decision making. However, we set the lowerbound limit for (t) to be b so the weight of p willnot be too low. The effects of different settings forb and d are shown in .",
  "Action history:|look around (+0) > N/A |": "Current environment:This room is called theworkshop. In it, you see: | the agent | a substancecalled air | a table. On the table are: a battery,a black wire, a orange light bulb, which is off, aorange wire, a red wire, a switch, which is off, aviolet light bulb, which is off, a yellow light bulb,which is off. | a ultra low temperature freezer. Theultra low temperature freezer door is closed. | Youalso see: | A door to the hallway |",
  "Webshop Sample Task": "Environment DescriptionYou are web shop-ping. I will give you instructions about what to do.You have to follow the instructions. Every roundI will give you an observation and a list of avail-able actions, you have to respond an action basedon the state and instruction. You can use searchaction if search is available. You can click one ofthe buttons in [clickables]. An action should be ofthe following structure:",
  "ALFWorld Sample Task": "Environment DescriptionInteract with a house-hold to solve a task. Imagine you are an intelligentagent in a household environment and your target isto perform actions to complete the task goal. At thebeginning of your interactions, you will be giventhe detailed description of the current environmentand your goal to accomplish. For each of your turn,you will be given a list of actions which you canchoose one to perform in this turn. ActionsYou should choose from two actions:THOUGHT or ACTION. If you chooseTHOUGHT, you should first think about the cur-rent condition and plan for your future actions, andthen output your action in this turn. Your outputmust strictly follow this format:",
  ". Think when necessary, try to act directly morein the process": "Initial ObservationYou are in the middle ofa room. Looking quickly around you, you see acabinet 16, a cabinet 15, a cabinet 14, a cabinet 13,a cabinet 12, a cabinet 11, a cabinet 10, a cabinet9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5,a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, acoffeemachine 1, a countertop 2, a countertop 1, adiningtable 1, a drawer 5, a drawer 4, a drawer 3, adrawer 2, a drawer 1, a fridge 1, a garbagecan 1, amicrowave 1, a safe 1, a sinkbasin 1, a stoveburner4, a stoveburner 3, a stoveburner 2, a stoveburner 1,and a toaster 1.",
  "B.2ScienceWorld Experimental Details": "Warm-up StageFollowing the training approachoutlined in (Lin et al., 2023), we enhance the tra-ditional one-hop imitation learning data to multi-hop data by incorporating a sliding window thatcaptures states and rewards from the previous 10 actions (K = 10). Additionally, we introduce a dedi-cated field to track visited rooms, ensuring no dupli-cation occurs. This approach provides agents withan extended context, thereby preventing redundantroom navigation. The main idea is to exploit nega-tive log-likelihood (NLL) loss to train the model toimitate the golden action.Our backbone model is Flan-T5-large, which istrained with a learning rate of 1e-4 and a batch sizeof 8. We terminate our model at step 8000. Forefficient training, we also employ DeepSpeed Zero-3 (Rajbhandari et al., 2021) for parallel trainingacross four V100 GPUs. Retrospection StageWe collect trajectories byletting the IL-based LLM interact with the Sci-enceWorld environment. We then break down eachtrajectory into steps in the form of (task description,current state, action, next state). More details re-garding the collected trajectories and the proportionof the positive trajectories are provided in .For ScienceWorld, the Q-network IQL consistsof 1 embedding layer, 5 GRU blocks, and 2 linearlayers. The input for the Q-function includes taskdescription, current state, and action. The state isdivided into freelook and inventory, the two spe-cific states in ScienceWorld. All these 5 parts (task,current state, action, freelook, and inventory) arepassed through separate GPUs. We then concate-nate the output of 5 GRU blocks before being fedinto the last 2 linear layers. The V -network of IQLis similar to Q-network but doesnt need to inputaction. As a result, the input for V network doesnot include the action part.We set the size of the embedding layer to be 64,the output layer of GRU and the first linear layer tobe 128. We train the IQL in 20 epochs with a batchsize of 128. Due to the light parameter of GRU, thetraining process is around 2 hours, which is muchless than 20h for the warm-up stage. The details oftraining parameters and training costs are listed in and . DRRNIn ScienceWorld, we separately train oneonline RL agent (DRRN) for each task of 30 tasks.For each task, the training step for DRRN is 10000,with a learning rate of 1e 4. The Q-network inDRRN is GRU+MLP with the embedding size of128 and the hidden size of 128, which is consistentwith ScienceWorld paper (Wang et al., 2022a).",
  "B.3Webshop Experimental Details": "Warm-up StageFollowing the approach in(Zeng et al., 2024), we construct our training databy combining the AgentInstruct and ShareGPTdatasets. The inclusion of ShareGPT helps preventcatastrophic forgetting, which could cause LLMsto lose their general capabilities. We refactor theAgentInstruct dataset so that each turn becomes anindividual sample. For ShareGPT, we extract sam-ples in a 20:80 ratio relative to AgentInstruct. Thisresults in a training dataset of 13,000 samples fromAgentInstruct and 52,000 samples from ShareGPT.We select the LLaMA3-8B-Instruct model as ourbackbone model. The training objective followsthe approach outlined in (Zeng et al., 2024). We,however, employ LoRA for fine-tuning, with rankand alpha set to 32 and 64. During fine-tuning,we compute the loss based on the models outputsusing SFTTrainer7. We utilize a learning rate of1e-4 and train for 2 epochs with a batch size of 2.To ensure an efficient training, we leverage Deep-Speed Zero-3 (Rajbhandari et al., 2021) for paralleltraining across four V100 GPUs. Retrospection StageWe collect trajectories onWebshop and perform preprocessing similar to thatin ScienceWorld. More details are provided in Ta-ble 1. For Webshop, we treat the state as a wholepart and use one GRU block for it, which is dif-ferent from ScienceWorld. The Q-network we useis Twin Q (Clipped Double Q-learning) (Fujimoto",
  ": Results of Retrospex in Webshop (AgentLMtest set) with IQL trained in different number of col-lected samples": "et al., 2018), which uses 2 networks with the samestructure and the last Q value is the minimum ofthese two networks. The structure of Twin Q isshown in . The number of steps is compar-atively small on Webshop, thus using Twin-Q canmake the Q-network more stable. The other part ofIQL is the same as that in ScienceWorld.We set the embedding size of the embeddinglayer to be 64, the output layer of GRU and thefirst linear layer to be 128. We train the IQL in20 epochs with batch size 128. Due to the lightparameter of GRU, the training process is around 2hours. The details of training parameters and costof warm-up and retrospection stage are listed in and . Analysis on Collected Samples showsthe impact of the number of samples collected onRL training and the final performance on the Web-shop environment. In order to discover the impactof the number of samples collected, we trained dif-ferent IQLs with a number of samples collectedfrom the Webshop environment ranging from 500to 2000. We find that using 1500 samples can ob-tain better performance which means using 2000samples may have resulted in the problem of over-fitting. Due to this consideration, we believe thatthe training of lightweight IQL needs to be furtherinvestigated to achieve a balance between overfit-ting and underfitting.",
  "B.5More Comparison with A3T": "We conduct a detailed comparison with A3T onWebshop and ALFWorld. The results are shownin and 13. In both environments, Retro-spex outperforms the result of A3T at round = 0.Because A3T continues to increase the trainingtrajectories and trains the LLM at each round, itworks better in round = 1, 2, 3 in the ALFWorldenvironment. However, compared to the expensivetraining investment of A3T, Retrospex still has ad-vantages. On Webshop, Retrospex outperformedall A3T rounds in 251 test cases of AgentBoard.",
  ": Results of our ablation study": "enceWorld and IL-LLaMA3 in ALFWorld, whichwe put here for cross-reference. As expected, re-moving action mapping leads to a performancedecline in Retrospex, consistent with previous find-ings (Brohan et al., 2022). However, the drop isrelatively modest, suggesting that action mappingis not the only key factor for Retrospex in theseenvironments. One possible explanation is that im-itation learning during the warm-up phase helpsthe LLM partially adapt to the target environment,reducing the occurrence of invalid actions."
}