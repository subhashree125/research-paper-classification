{
  "Abstract": "With the increasing capabilities of large lan-guage models (LLMs), in-context learning(ICL) has emerged as a new paradigm for nat-ural language processing (NLP), where LLMsmake predictions based on contexts augmentedwith a few examples. It has been a significanttrend to explore ICL to evaluate and extrap-olate the ability of LLMs. In this paper, weaim to survey and summarize the progress andchallenges of ICL. We first present a formaldefinition of ICL and clarify its correlation torelated studies. Then, we organize and discussadvanced techniques, including training strate-gies, prompt designing strategies, and relatedanalysis. Additionally, we explore various ICLapplication scenarios, such as data engineeringand knowledge updating. Finally, we addressthe challenges of ICL and suggest potential di-rections for further research. We hope that ourwork can encourage more research on uncover-ing how ICL works and improving ICL.",
  "Introduction": "With the scaling of model size and data size (Brownet al., 2020; Chowdhery et al., 2023; OpenAI, 2023;Touvron et al., 2023a,b), large language models(LLMs) demonstrate the in-context learning (ICL)ability, that is, learning from a few examples inthe context. Many studies have shown that LLMscan perform a series of complex tasks throughICL, such as solving mathematical reasoning prob-lems (Wei et al., 2022c). These strong abilitieshave been widely verified as emerging abilities forlarge language models (Wei et al., 2022b).The key idea of in-context learning is to learnfrom analogy. gives an example that de-scribes how language models make decisions viaICL. First, ICL requires a few demonstration ex-amples to form a prompt context. These examplesare usually written in natural language templates.Then, ICL concatenates a query question and the",
  "TextLabel": ": Illustration of in-context learning. ICL re-quires a prompt context containing a few demonstrationexamples written in natural language templates. Takingthis prompt and a query as the input, large languagemodels are responsible for making predictions. piece of prompt context together to form the input,which is then fed into the language model for pre-diction. Different from supervised learning, whichrequires a training stage that uses backward gra-dients to update model parameters, ICL does notperform parameter updates. The model is expectedto learn the pattern hidden in the demonstration andaccordingly make the right prediction.As a new paradigm, ICL has multiple attractiveadvantages. First, since the demonstration is writ-ten in natural language, it provides an interpretableinterface to communicate with LLMs (Brown et al.,2020). This paradigm makes it much easier to in-corporate human knowledge into LLMs by chang-ing the demonstration and templates (Liu et al.,2022; Lu et al., 2022; Wei et al., 2022c; Wu et al.,2023b). Second, in-context learning is similar tothe decision process of human beings by learningfrom analogy (Winston, 1980). Third, comparedto supervised training, ICL is a training-free learn-ing framework. This could not only greatly reducethe computational costs for adapting the modelto new tasks, but also make language-model-as-a-service (Sun et al., 2022) possible and can be easilyapplied to large-scale real-world tasks.Despite being promising, there are also interest-ing questions and intriguing properties that require",
  ": Taxonomy of in-context learning": "further investigation in ICL. Although a range ofvanilla GPT models show excellent ICL capability,several studies have found that this capability canbe significantly improved through adaptation dur-ing pretraining (Min et al., 2022b; Li et al., 2024c).Moreover, the performance of ICL is sensitive tospecific settings, including the prompt template, theselection and order of demonstration examples, andother factors (Wang et al., 2023e; Liu et al., 2024b).Additionally, optimizing the conciseness of demon-stration examples and improving the computationalefficiency of ICL are critical areas of ongoing re-search (Liu et al., 2024a). Furthermore, despitepreliminary explanations (Dai et al., 2023a; Jiang,2023), the underlying working mechanism of ICLremains unclear and requires further investigation.",
  "swer with the maximum score as the prediction,1": "conditioned a demonstration set C. C containsan optional task instruction I and k demonstrationexamples, thus C = {I, s(x1, y1), . . . , s(xk, yk)}or C = {s(x1, y1, I), . . . , s(xk, yk, I)}, wheres(xi, yi, I) is an example written in natural lan-guage according to the task. Depending on whetherk and the demonstration examples belong to thesame task, it can be categorized as task-specific ICLand cross-task ICL. In the latter, different exampleshave their own instructions. The likelihood of acandidate answer yj comes from a scoring functionf on the whole input sequence:",
  "y = arg maxyjY P(yj | x).(2)": "According to the definition, we can see that ICLdiffers from related concepts as follows: (1) PromptLearning: prompts can be discrete templates or softparameters that encourage the model to predict thedesired output. ICL can be regarded as a subclassof prompt tuning where the demonstration exam-ples are part of the prompt. Liu et al. (2023c) madea thorough survey on prompt learning, but ICL wasnot included in their study. (2) Few-shot Learning:few-shot learning is a general machine learning ap-proach that involves adapting model parameters toperform a task with a limited number of supervisedexamples (Wang and Yao, 2019). In contrast, ICLdoes not require parameter updates and is directlyperformed on pretrained LLMs.",
  "Warmup": "Another way to enhance ICL ability is adding acontinual training stage between pretraining andICL inference, which we call model warmup forshort. Warmup is an optional procedure for ICL,which adjusts LLMs before inference by modifyingor adding parameters. As most pretraining data are not tailored forICL (Chen et al., 2022), researchers have intro-duced various warmup strategies to bridge thegap between pretraining and ICL inference. BothMin et al. (2022b) and Wang et al. (2022b) pro-posed to continually finetune LLMs on a broadrange of tasks with multiple demonstration exam-ples, which boosts ICL abilities. To encouragethe model to learn input-label mappings from thecontext, Wei et al. (2023a) proposed symbol tun-ing, which substitutes natural language labels (e.g.,positive/negative sentiment) with arbitrary sym-bols (e.g., foo/bar). Chen et al. (2022) proposeda self-supervised method to align raw text withICL formats in downstream tasks. Besides, mul-tiple studies have indicated the potential value ofinstructions (Mishra et al., 2021; Wei et al., 2022a).Tuning the 137B LaMDA-PT (Thoppilan et al.,2022) on over 60 datasets verbalized via naturallanguage instruction templates, FLAN (Wei et al.,2022a) improves the ability of LLMs to follow in-structions, boosting both the zero-shot and few-shotICL performance. Chung et al. (2022) and Wanget al. (2022b) proposed to further scale up instruc-tion tuning with more than 1000+ task instructions.",
  "Demonstration Organization": "Many studies have shown that the performance ofICL strongly relies on the demonstration surface,including the selection, formatting, and orderingof demonstration examples (Zhao et al., 2021; Luet al., 2022). In this subsection, we survey demon-stration organization strategies and classify theminto three categories, as shown in . 4.1.1Demonstration SelectionDemonstrations selection aims to answer a funda-mental question: Which samples are good exam-ples for ICL? We categorize the related studies intotwo approaches: unsupervised methods based onpredefined metrics and supervised methods. Unsupervised MethodA straightforward ap-proach to selecting ICL examples is to choosethe nearest neighbors of input instances based ontheir similarities (Liu et al., 2022; Tanwar et al.,2023; Qin et al., 2023). Distance metrics, suchas L2 distance or cosine similarity based on sen-tence embeddings, are commonly used for this pur-pose. For example, Liu et al. (2022) proposedKATE, the first kNN-based unsupervised retrieverfor selecting in-context examples. Similarly, k-NNcross-lingual demonstrations can be retrieved formulti-lingual ICL to strengthen source-target lan-guage alignment (Tanwar et al., 2023). Su et al.(2023) proposed to combine graphs and confidencescores to select diverse and representative examples.In addition to distance metrics, mutual informa-tion (Sorensen et al., 2022) and perplexity (Gonenet al., 2023) have proven valuable for prompt se-lection without labeled examples or specific LLMs.Furthermore, using output scores of LLMs as unsu-pervised metrics has shown effectiveness in demon-stration selection (Wu et al., 2023b; Nguyen andWong, 2023; Li and Qiu, 2023). Particularly, Wuet al. (2023b) selected the best subset permutationof kNN examples based on the code length for datatransmission to compress label y given x and C.Li and Qiu (2023) used infoscore, i.e., the aver-age of P(y|xi, yi, x)P(y|x) for all (x, y) pairs ina validation set with a diversity regularization.",
  "Supervised MethodThough off-the-shelf re-trievers offer convenient services for extensive NLP": "tasks, they are heuristic and sub-optimal due to thelack of task-specific supervision. To address thisissue, numerous supervised methods have been de-veloped (Rubin et al., 2022; Ye et al., 2023; Wanget al., 2023e; Zhang et al., 2022a). EPR (Rubinet al., 2022) introduced a two-stage method to traina dense retriever for demonstration selection. For aspecific input, it first utilized unsupervised methods(e.g., BM25) to recall similar examples as candi-dates and then used this data to build a superviseddense retriever. Following EPR, Li et al. (2023d)adopted a unified demonstration retriever to selectdemonstrations across different tasks. Unlike priorwork that retrieves individual demonstrations, Yeet al. (2023) proposed retrieving entire demonstra-tion sets to model inter-relationships between ex-amples. Additionally, Mavromatis et al. (2023)introduced AdaICL, a model-adaptive method thatemploys LLM to predict the unlabeled data set,generating an uncertainty score for each instance.Based on prompt tuning, Wang et al. (2023e)viewed LLMs as topic models that can infer con-cepts from a few demonstrations and generate to-kens based on these concepts. They represent latentconcepts with task-related concept tokens, whichare learned to maximize P(y|x, ). Demonstra-tions are selected based on their likelihood to inferthe concept variable using P(|x, y). Additionally,reinforcement learning was introduced by Zhanget al. (2022a) for example selection. They formu-lated demonstration selection as a Markov decisionprocess (Bellman, 1957) and selected demonstra-tions via Q-learning. The action is choosing anexample, and the reward is defined as the accuracyof a labeled validation set.In order to have a more intuitive comparison ofthe performance of several unsupervised methods,we select topk (Liu et al., 2022), votek (Su et al.,2023), mdl (Wu et al., 2023b) to conduct experi-ments. The result is shown in . The detailsof the experiment can be found in Appendix B.",
  "Demonstration Reformatting": "In addition to directly selecting examples fromtraining data, another research trend involves utiliz-ing LLMs to reformat the representation of exist-ing demonstrations (Kim et al., 2022; Yang et al.,2023a; Hao et al., 2022b; Yang et al., 2023b; Liuet al., 2024a; Li et al., 2024a). For instance, Kimet al. (2022) proposed generating demonstrationsdirectly from LLMs to reduce the reliance on exter-nal demonstration data. Structured Prompting (Hao",
  "Llama3topk53.090.376.164.074.0 71.5votek54.988.972.657.778.3 70.5mdl54.489.176.559.974.6 70.9": ": Fair comparison of demonstration selectionmethods. CQA and News are abbreviations of Common-sense QA and AG News, respectively. The best resultsare bolded. Our experiments on topk (Liu et al., 2022),votek (Su et al., 2023), mdl (Wu et al., 2023b) show thatthe effectiveness of ICL example selection methods aremodel-dependent. On GPT-2, the mdl method performsthe best, while on the other three models, topk performsthe best. et al., 2022b) proposed to encode demonstrationexamples separately with special positional embed-dings, which are then provided to the test examplesusing a rescaled attention mechanism. Divergingfrom these methods, other approaches focus onmodifying the latent representation of demonstra-tions (Liu et al., 2024a; Li et al., 2024a). Specifi-cally, Liu et al. (2024a) developed In-Context Vec-tors (ICVs) derived from the latent embeddings ofdemonstration examples in LLMs. These ICVs areused during inference to adjust the latent states ofthe LLM, thereby enhancing the models ability tofollow the demonstrations more effectively.",
  "Demonstration Ordering": "Ordering the selected demonstration examples isalso an important aspect of demonstration organi-zation. Lu et al. (2022) have proven that order sen-sitivity is a common problem and always exists forvarious models. To handle this problem, previousstudies have proposed several training-free meth-ods for sorting demonstration examples. Particu-larly, Liu et al. (2022) arranged examples based ontheir proximity to the input, positioning the closestexample as the rightmost demonstration. Lu et al.(2022) introduced global and local entropy metrics,finding a positive correlation between these metricsand the ICL performance. Consequently, they uti-lized the entropy metric to determine the optimaldemonstration ordering. Additionally, ICCL (Liuet al., 2024b) suggested ranking demonstrationsfrom simple to complex, thereby gradually increas-ing the complexity of demonstration examples dur-ing the inference process.",
  "Instruction Formatting": "A common way to format demonstrations is con-catenating examples (x1, y1), . . . , (xk, yk) with atemplate T directly. However, in some tasks thatneed complex reasoning (e.g., math word prob-lems and commonsense reasoning), it is not easyto learn the mapping from xi to yi with only kdemonstrations. Although template engineeringhas been studied in prompting (Liu et al., 2023c),some researchers aim to design a better format ofdemonstrations for ICL by describing tasks withthe instruction I. Honovich et al. (2023) found thatgiven several demonstration examples, LLMs cangenerate task instructions themselves. Consider-ing the generation abilities of LLMs, Zhou et al.(2023c) proposed an Automatic Prompt Engineerfor automatic instruction generation and selection.",
  ": Summary of different scoring functions. Cov-erage refers to task coverage. The qualitative resultsfor Efficiency and Stability metrics are elaborated in and , respectively": "To further improve the quality of the automaticallygenerated instructions, several strategies have pro-posed using LLMs to bootstrap off its own genera-tions (Wang et al., 2023f; Chen et al., 2024). Addi-tionally, chain-of-thought (CoT) (Wei et al., 2022c)introduces intermediate reasoning steps betweeninputs and outputs to enhance problem-solving andcomprehension. Recent advancements have alsoemphasized the process of enhancing step-by-stepreasoning in models (Zhang et al., 2023c; Wanget al., 2022a; Zhou et al., 2023a).",
  "Scoring Function": "The scoring function determines how to transformthe predictions of a language model into an estima-tion of the likelihood of a specific answer. The Di-rect method uses the conditional probability of can-didate answers represented by tokens in the modelsvocabulary (Brown et al., 2020). The answer withthe highest probability is selected as the final an-swer, but this method restricts template design byrequiring answer tokens to be at the end of inputsequences. Perplexity (PPL) is another commonlyused metric that computes the sentence perplexityof the entire input sequence Sj = {C, s(x, yj, I)},which includes tokens from demonstration exam-ples C, the input query x, and the candidate la-bel yj. PPL evaluates the probability of the sen-tence, eliminating token position limitations butrequiring additional computation time. Min et al.(2022a) proposed using channel models (Channel)to compute the conditional probability in reverse,estimating the likelihood of the input query giventhe label. This approach requires language modelsto generate every token in the input, potentiallyboosting performance under imbalanced trainingdata. We summarize all three scoring functions in. Note that in , Efficiency refersto the language model inference latency; Cover-age reflects whether the method utilizes the outputprobability of the local or all token positions in theinput sequence; and Stability indicates whether",
  "Analysis": "To understand ICL, recent studies attempt to inves-tigate what influence ICL performance (Shin et al.,2022; Yoo et al., 2022; Kossen et al., 2023) andwhy ICL works (Dai et al., 2023a; Irie et al., 2022).In this section, we present a detailed elaborationof influencing factors (5.1) and learning mecha-nisms (5.2) of ICL, as illustrated in .",
  "Pretraining Stage": "We first introduce factors that influence the pre-training stage. The diversity of pretraining cor-pora significantly impacts ICL performance (Shinet al., 2022; Yadlowsky et al., 2023; Ravents et al.,2023). In particular, Shin et al. (2022) found thatthe source domain is more important than the cor-pus size, suggesting that combining multiple cor-pora may lead to the emergence of ICL ability.Similarly, Ravents et al. (2023) empirically identi-fied a task diversity threshold beyond which LLMsexhibit strong ICL capabilities in unseen tasks. An-other line of research investigates the impact of datadistribution on ICL (Chan et al., 2022; Wies et al.,2023). For instance, Chan et al. (2022) demon-strated that ICL capability emerges when the train-ing data exhibits specific distributional properties,such as burstiness, wherein items appear in clustersrather than being uniformly distributed over time.Beyond these works, several studies have investi-gated the impact of model architecture and trainingprocess on ICL performance (Wei et al., 2022b;Brown et al., 2020; Ding et al., 2024). Wei et al.(2022b) investigated the emergent abilities of manylarge-scale models on multiple tasks. They sug-gested that a pretrained model acquires some emer-gent ICL abilities when it reaches a large scaleof pretraining steps or model parameters. Dinget al. (2024) pointed out that the in-context sam-ples should attend to each other during inference,indicating that current causal LLMs may lead tosuboptimal ICL performance.",
  "Inference Stage": "During inference, there are also multiple proper-ties of demonstration examples that influence ICLperformance. Min et al. (2022c) proved that input-label settings such as the pairing format, the expo-sure of label space, and the input distribution con-tribute substantially to ICL performance. However,contrary to the conclusion in Min et al. (2022c)that input-label mapping matters little to ICL, latterstudies showed that the accurate mapping influenceICL performance significantly (Yoo et al., 2022;Pan et al., 2023a; Tang et al., 2023a). Wei et al.(2023b) further pointed that flipped or semantically-unrelated input-label mapping also can be learned.From the perspective of demonstration construc-tion, recent literature focuses on the diversity andsimplicity of demonstrations (An et al., 2023), theorder of samples (Lu et al., 2022; Zhang et al.,2022b; Liu et al., 2023b), and the similarity be-tween demonstrations and queries (Liu et al., 2022).For example, Liu et al. (2022) found that demon-stration samples with embeddings closer to thoseof the query samples typically yield better perfor-mance than those with more distant embeddings.Notably, despite efforts to refine demonstrations tooptimize the performance, there still remain clearfeature biases during ICL inference (Si et al., 2023).Overcoming strong prior biases and ensuring themodel gives equal weight to all contextual informa-tion remain challenges (Kossen et al., 2023).",
  "The ICL capability is intimately connected to spe-cific functional modules within Transformers. Asone of the core components, the attention module": "is a focal point in the study of ICL mechanism (Ols-son et al., 2022; Bietti et al., 2023; Dai et al., 2023a;Irie et al., 2022; Li et al., 2023c; Gao et al., 2023;Zhang et al., 2023b). Particularly, Olsson et al.(2022) identified specific attention heads, referredto as induction heads, that can replicate previouspatterns for next-token prediction, thus progres-sively developing ICL capabilities. Additionally,Wang et al. (2023b) focused on the informationflow in Transformers and found that during theICL process, demonstration label words serve asanchors, which aggregate and distribute key infor-mation for the final prediction.",
  "In this subsection, we introduce the theoretical in-terpretations of ICL from different views": "Bayesian ViewIn the Bayesian framework, ICLis explained as implicit Bayesian inference, wheremodels perform ICL by identifying a shared latentconcept among examples (Xie et al., 2022; Wieset al., 2023; Ahuja et al., 2023; Jiang, 2023; Wanget al., 2023e). Additional perspectives suggest thatLLMs encode the Bayesian Model Averaging al-gorithm via the attention mechanism (Zhang et al.,2023b). As the number of in-context examples in-creases, implicit Bayesian inference becomes anal-ogous to kernel regression (Han et al., 2023a). Gradient Descent ViewGradient descent offersanother valuable lens for understanding ICL. Daiet al. (2023a) identified a dual form between Trans-former attention and gradient descent, finding thatGPT-based ICL behaves similarly to explicit fine-tuning from multiple perspectives. Other studieshave attempted to establish connections betweenICL and gradient descent in simplified regressionsettings (von Oswald et al., 2023; Ahn et al., 2023;Mahankali et al., 2023; Li et al., 2023c). For in- stance, von Oswald et al. (2023) showed that linearattention-only Transformers with manually con-structed parameters are closely related to modelslearned by gradient descent. Li et al. (2023c) foundthat self-attention-only Transformers exhibit sim-ilarities with models trained via gradient descent.However, the simplified settings used in these stud-ies have led to debates about the direct applicabilityof these connections in real-world contexts (Shenet al., 2024). Fu et al. (2023) argued that Trans-formers perform ICL on linear regression usinghigher-order optimization techniques rather thangradient descent. Other ViewsBeyond connecting ICL with a sin-gle algorithm, researchers have analyzed it fromvarious perspectives, including ability decoupling,algorithmic learning, and information theory. Panet al. (2023b) decoupled ICL capabilities into taskrecognition ability and task learning ability, eachmanifesting under different conditions. Anothertypical theory abstracts ICL as an algorithmic learn-ing problem (Akyrek et al., 2023; Garg et al.,2022; Li et al., 2023e; Bai et al., 2023b), whereTransformers dynamically select algorithms, suchas gradient descent and ridge regression, tailored todifferent ICL instances. Moreover, Hahn and Goyal(2023) utilized information theory to show an er-ror bound for ICL under linguistically motivatedassumptions, explaining how next-token predictioncan bring about the ICL ability.These analytical studies have taken an essen-tial step to explain ICL. However, most of themfocused on simple tasks and small models. Extend-ing analysis on extensive tasks and large modelsmay be the next step to be considered.",
  "Application": "Given its user-friendly interface and lightweightprompting method, ICL has broad applications ontraditional NLP tasks (Kim et al., 2022; Min et al.,2022b; Zhu et al., 2023b). Particularly, by usingdemonstrations that explicitly guide the reasoningprocess, ICL manifests remarkable effects on tasksrequiring complex reasoning (Wei et al., 2022c; Liet al., 2023b; Zhou et al., 2022) and compositionalgeneralization (Zhou et al., 2023a).We explore several emerging and prevalentapplications of ICL, including data engineering,model augmentation, and knowledge updating. 1)Data Engineering: Unlike traditional methodssuch as human annotation and noisy automatic annotation, ICL generates relatively high-qualitydata at a lower cost, leading to improved perfor-mance. (Wang et al., 2021; Khorashadizadeh et al.,2023; Ding et al., 2023). 2) Model Augmentation:The context-flexible nature of ICL shows promisein model augmentation. It can enhance retrieval-augmented methods by prepending grounding doc-uments to the input (Ram et al., 2023). Addition-ally, ICL for retrieval demonstrates potential insteering models toward safer outputs (Panda et al.,2023; Meade et al., 2023). 3) Knowledge Up-dating: LLMs often contain outdated or incorrectknowledge (Dong et al., 2023). ICL has demon-strated efficacy in revising such knowledge throughcarefully crafted demonstrations, yielding highersuccess rates compared to gradient-based meth-ods (De Cao et al., 2021).As mentioned above, ICL has yielded significantbenefits on both traditional and emergent NLP ap-plications. The tremendous success of ICL in NLPhas inspired researchers to explore its potential invarious modalities beyond text (elaborated in Ap-pendix D), including vision (Bar et al., 2022; Wanget al., 2023c), vision-language (Tsimpoukelli et al.,2021; Alayrac et al., 2022), as well as speech appli-cations (Wang et al., 2023a; Zhang et al., 2023d).",
  "In this section, we review existing challenges anddiscuss future directions for ICL": "Efficiency and ScalabilityThe use of demonstra-tions in ICL introduces two challenges: (1) highercomputational costs with an increasing number ofdemonstrations (efficiency), and (2) fewer learn-able samples due to the maximum input length ofLLMs (scalability). Prior research has attempted tomitigate these issues by distilling lengthy demon-strations into compact vectors (Li et al., 2024d,c) orexpediting LLM inference times (Liu et al., 2023d).However, these methods often involve a trade-off inperformance or necessitate access to model param-eters, which is impractical for closed-source mod-els like ChatGPT and Claude (Zhou et al., 2023b).Thus, enhancing the scalability and efficiency ofICL with more demonstrations remains a signifi-cant challenge.",
  "GeneralizationICL heavily relies on high-quality demonstrations selected from annotated ex-amples, which are often scarce in low-resourcelanguages and tasks. This scarcity poses a chal-": "lenge to the generalization ability of ICL (He et al.,2024). Given that there is a substantial discrepancyin the availability of annotated high-resource dataand low-resource data, the potential to leveragehigh-resource data to address low-resource tasks ishighly appealing (Chatterjee et al., 2024; Tanwaret al., 2023). Long-context ICLRecent advances in context-extended LLMs have spurred research into theimpact of ICL when using an increasing numberof demonstration examples (Agarwal et al., 2024;Bertsch et al., 2024). However, researchers havefound that increasing the number of demonstrationsdoes not necessarily enhance performance and mayeven be detrimental. These performance declinesindicate a need for further investigation. Addition-ally, Li et al. (2024b) developed LongICLBench,which includes diverse extreme-label classificationtasks, revealing further weaknesses of LLMs incomprehending extended demonstrations.",
  "Conclusion": "In this paper, we comprehensively review the ex-isting literature on ICL, examining advanced tech-niques, conducting analytical studies, discussingrelevant applications, and identifying critical chal-lenges and potential directions for future research.To our knowledge, this is the first comprehensivesurvey dedicated to ICL. We aim to highlight thecurrent state of research in ICL and provide insightsto guide future work in this promising area.",
  "Limitations": "This paper offers a comprehensive examination andsummary of current methodologies and analyses inthe area of In-Context Learning (ICL). However,given the extensive body of related work, partic-ularly in demonstration design and the principleanalysis of ICL, we may have overlooked someequally valuable contributions. Additionally, weoutline several future directions for research in ICL,including long-context ICL, efficiency and scalabil-ity in ICL, etc. We plan to leave these aspects forfuture work. Furthermore, many papers covered bythis survey did not utilize the most up-to-date mod-els while running experiments. We advocate formore thorough and up-to-date research to provideactionable insights for practitioners. Rishabh Agarwal, Avi Singh, Lei M. Zhang, BerndBohnet, Luis Rosias, Stephanie Chan, Biao Zhang,Ankesh Anand, Zaheer Abbas, Azade Nova, John D.Co-Reyes, Eric Chu, Feryal Behbahani, AleksandraFaust, and Hugo Larochelle. 2024. Many-shot in-context learning. Preprint, arXiv:2404.11018. Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, andSuvrit Sra. 2023. Transformers learn to implementpreconditioned gradient descent for in-context learn-ing. In Advances in Neural Information ProcessingSystems 36: Annual Conference on Neural Informa-tion Processing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023.",
  "AI@Meta. 2024. Llama 3 model card. Technical report,Meta": "Ekin Akyrek, Dale Schuurmans, Jacob Andreas,Tengyu Ma, and Denny Zhou. 2023. What learn-ing algorithm is in-context learning? investigationswith linear models. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, NanningZheng, Jian-Guang Lou, and Dongmei Zhang. 2023.How do in-context examples affect compositionalgeneralization? In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2023, Toronto,Canada, July 9-14, 2023, pages 1102711052. Asso-ciation for Computational Linguistics. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023a. Qwen technical report. arXiv preprintarXiv:2309.16609.",
  "Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,Matthew R. Gormley, and Graham Neubig. 2024.In-context learning with long-context models: Anin-depth exploration. CoRR, abs/2405.00200": "Alberto Bietti, Vivien Cabannes, Diane Bouchacourt,Herv Jgou, and Lon Bottou. 2023. Birth of atransformer: A memory viewpoint. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, RussAltman, Simran Arora, Sydney von Arx, Michael S.Bernstein, Jeannette Bohg, Antoine Bosselut, EmmaBrunskill, Erik Brynjolfsson, S. Buch, Dallas Card,Rodrigo Castellon, Niladri S. Chatterji, Annie S.Chen, Kathleen A. Creel, Jared Davis, Dora Dem-szky, Chris Donahue, Moussa Doumbouya, Esin Dur-mus, Stefano Ermon, John Etchemendy, Kawin Etha-yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lau-ren E. Gillespie, Karan Goel, Noah D. Goodman,Shelby Grossman, Neel Guha, Tatsunori Hashimoto,Peter Henderson, John Hewitt, Daniel E. Ho, JennyHong, Kyle Hsu, Jing Huang, Thomas F. Icard, SaahilJain, Dan Jurafsky, Pratyusha Kalluri, SiddharthKaramcheti, Geoff Keeling, Fereshte Khani, O. Khat-tab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak,Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Ma-lik, Christopher D. Manning, Suvir P. Mirchandani,Eric Mitchell, Zanele Munyikwa, Suraj Nair, AvanikaNarayan, Deepak Narayanan, Benjamin Newman,Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim-itriou, Joon Sung Park, Chris Piech, Eva Portelance,Christopher Potts, Aditi Raghunathan, Robert Re-ich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani,Camilo Ruiz, Jack Ryan, Christopher Re, DorsaSadigh, Shiori Sagawa, Keshav Santhanam, AndyShih, Krishna Parasuram Srinivasan, Alex Tamkin,Rohan Taori, Armin W. Thomas, Florian Tramr,Rose E. Wang, William Wang, Bohan Wu, JiajunWu, Yuhuai Wu, Sang Michael Xie, Michihiro Ya-sunaga, Jiaxuan You, Matei A. Zaharia, MichaelZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021.On the opportunities and risks of foundation models.ArXiv. Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages632642, Lisbon, Portugal. Association for Compu-tational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual.",
  "Marc-Etienne Brunet, Ashton Anderson, and Richard S.Zemel. 2023.ICL markup:Structuring in-context learning using soft-token tags.CoRR,abs/2312.07405": "Stephanie C. Y. Chan, Adam Santoro, Andrew K.Lampinen, Jane X. Wang, Aaditya K. Singh, Pierre H.Richemond, James L. McClelland, and Felix Hill.2022. Data distributional properties drive emergentin-context learning in transformers. In Advances inNeural Information Processing Systems 35: AnnualConference on Neural Information Processing Sys-tems 2022, NeurIPS 2022, New Orleans, LA, USA,November 28 - December 9, 2022.",
  "Ding Chen, Shichao Song, Qingchen Yu, Zhiyu Li, Wen-jin Wang, Feiyu Xiong, and Bo Tang. 2024. Grimoireis all you need for enhancing large language models.CoRR, abs/2401.03385": "Mingda Chen, Jingfei Du, Ramakanth Pasunuru, TodorMihaylov, Srini Iyer, Veselin Stoyanov, and Zor-nitsa Kozareva. 2022. Improving in-context few-shotlearning via self-supervised training. In Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 35583573,Seattle, United States. Association for ComputationalLinguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2023. Palm: Scaling language mod-eling with pathways. J. Mach. Learn. Res., 24:240:1240:113.",
  "Fine-tune language models to approximate unbiasedin-context learning. CoRR, abs/2310.03331": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, ShumingMa, Zhifang Sui, and Furu Wei. 2023a. Why canGPT learn in-context? language models secretly per-form gradient descent as meta-optimizers. In Find-ings of the Association for Computational Linguistics:ACL 2023, Toronto, Canada, July 9-14, 2023, pages40054019. Association for Computational Linguis-tics. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023b. Instructblip: Towards general-purpose vision-language models with instruction tuning.In Ad-vances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023. Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-ing factual knowledge in language models. In Proc.of EMNLP, pages 64916506, Online and PuntaCana, Dominican Republic. Association for Com-putational Linguistics. Bosheng Ding, Chengwei Qin, Linlin Liu, Yew KenChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.Is GPT-3 a good data annotator?In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),",
  "Yeqi Gao, Zhao Song, and Shenghao Xie. 2023. In-context learning for attention scheme: from singlesoftmax regression to multiple softmax regressionvia a tensor trick. CoRR, abs/2307.02419": "Shivam Garg, Dimitris Tsipras, Percy Liang, and Gre-gory Valiant. 2022. What can transformers learn in-context? A case study of simple function classes. InAdvances in Neural Information Processing Systems35: Annual Conference on Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New Orleans,LA, USA, November 28 - December 9, 2022. Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,and Luke Zettlemoyer. 2023. Demystifying promptsin language models via perplexity estimation. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 1013610148. Association for Computa-tional Linguistics.",
  "Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, YuxianGu, and Furu Wei. 2022b. Structured prompting:Scaling in-context learning to 1,000 examples. ArXivpreprint, abs/2212.06713": "Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, XingXu, and Heng Tao Shen. 2023. ICL-D3IE: in-contextlearning with diverse demonstrations updating fordocument information extraction. In IEEE/CVF In-ternational Conference on Computer Vision, ICCV2023, Paris, France, October 1-6, 2023, pages 1942819437. IEEE. Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu,Zhiheng Xi, Tao Gui, Qi Zhang, and Xuanjing Huang.2024. Self-demos: Eliciting out-of-demonstrationgeneralizability in large language models. CoRR,abs/2404.00884.",
  "Clyde Highmore. 2024. In-context learning in largelanguage models: A comprehensive survey": "Or Honovich, Uri Shaham, Samuel R. Bowman, andOmer Levy. 2023. Instruction induction: From fewexamples to natural language task descriptions. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pages 19351952. Association for Computa-tional Linguistics. Qian Huang, Hongyu Ren, Peng Chen, Gregor Krzmanc,Daniel Zeng, Percy Liang, and Jure Leskovec. 2023a.PRODIGY: enabling in-context learning over graphs.In Advances in Neural Information Processing Sys-tems 36: Annual Conference on Neural InformationProcessing Systems 2023, NeurIPS 2023, New Or-leans, LA, USA, December 10 - 16, 2023. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,Saksham Singhal, Shuming Ma, Tengchao Lv, LeiCui, Owais Khan Mohammed, Barun Patra, QiangLiu, Kriti Aggarwal, Zewen Chi, Nils Johan BertilBjorck, Vishrav Chaudhary, Subhojit Som, Xia Song,and Furu Wei. 2023b. Language is not all you need:Aligning perception with language models. In Ad-vances in Neural Information Processing Systems 36:Annual Conference on Neural Information Process-ing Systems 2023, NeurIPS 2023, New Orleans, LA,USA, December 10 - 16, 2023. Kazuki Irie, Rbert Csords, and Jrgen Schmidhuber.2022. The dual form of neural networks revisited:Connecting test time predictions to training patternsvia spotlights of attention. In International Confer-ence on Machine Learning, ICML 2022, 17-23 July2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages96399659. PMLR. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,Brian OHoro, Gabriel Pereyra, Jeff Wang, Christo-pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,and Ves Stoyanov. 2022. Opt-iml: Scaling languagemodel instruction meta learning through the lens ofgeneralization.",
  "Hui Jiang. 2023.A latent space theory for emer-gent abilities in large language models.CoRR,abs/2304.09960": "Hanieh Khorashadizadeh,Nandana Mihindukula-sooriya, Sanju Tiwari, Jinghua Groppe, and SvenGroppe. 2023. Exploring in-context learning capabil-ities of foundation models for generating knowledgegraphs from text. arXiv preprint arXiv:2305.08804. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, TaeukKim, Kang Min Yoo, and Sang-goo Lee. 2022.Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration gen-erator. ArXiv preprint, abs/2206.08082.",
  "Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue,and Wenhu Chen. 2024b.Long-context llmsstruggle with long in-context learning.ArXiv,abs/2404.02060": "Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,Yuan Ni, Guotong Xie, Xiaoling Wang, and XipengQiu. 2023d. Unified demonstration retriever for in-context learning. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2023, Toronto,Canada, July 9-14, 2023, pages 46444668. Associa-tion for Computational Linguistics.",
  "Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, andWei Lu. 2024b. Lets learn step by step: Enhancingin-context learning ability with curriculum learning.Preprint, arXiv:2402.10738": "Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, BinhangYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,Yuandong Tian, Christopher R, and Beidi Chen.2023d. Deja vu: Contextual sparsity for efficientllms at inference time. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 2213722176.PMLR. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 80868098. Association for Computational Linguistics.",
  "Arvind Mahankali, Tatsunori B. Hashimoto, and TengyuMa. 2023. One step of gradient descent is provablythe optimal in-context learner with one layer of linearself-attention. CoRR, abs/2307.03576": "Costas Mavromatis,Balasubramaniam Srinivasan,Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala,Christos Faloutsos, and George Karypis. 2023.Which examples to annotate for in-context learn-ing? towards effective and efficient selection. CoRR,abs/2310.20046. Nicholas Meade, Spandana Gella, Devamanyu Hazarika,Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, andDilek Hakkani-Tur. 2023. Using in-context learn-ing to improve dialogue safety. In Findings of theAssociation for Computational Linguistics: EMNLP2023, Singapore, December 6-10, 2023, pages 1188211910. Association for Computational Linguistics. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, andLuke Zettlemoyer. 2022a. Noisy channel languagemodel prompting for few-shot text classification. InProc. of ACL, pages 53165330, Dublin, Ireland. As-sociation for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-naneh Hajishirzi. 2022b. MetaICL: Learning to learnin context. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 27912809, Seattle, United States.Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer. 2022c. Rethinking the role of demonstrations:What makes in-context learning work? In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2022, AbuDhabi, United Arab Emirates, December 7-11, 2022,pages 1104811064. Association for ComputationalLinguistics.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.2023a. What in-context learning \"learns\" in-context:Disentangling task recognition and task learning. InAnnual Meeting of the Association for ComputationalLinguistics. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.2023b. What in-context learning \"learns\" in-context:Disentangling task recognition and task learning. InFindings of the Association for Computational Lin-guistics: ACL 2023, Toronto, Canada, July 9-14,2023, pages 82988319. Association for Computa-tional Linguistics.",
  "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context retrieval-augmented lan-guage models. CoRR, abs/2302.00083": "Allan Ravents, Mansheej Paul, Feng Chen, and SuryaGanguli. 2023. Pretraining task diversity and theemergence of non-bayesian in-context learning forregression. In Advances in Neural Information Pro-cessing Systems 36: Annual Conference on NeuralInformation Processing Systems 2023, NeurIPS 2023,New Orleans, LA, USA, December 10 - 16, 2023. Ohad Rubin, Jonathan Herzig, and Jonathan Berant.2022. Learning to retrieve prompts for in-contextlearning. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 26552671, Seattle, United States.Association for Computational Linguistics. Abulhair Saparov and He He. 2023. Language modelsare greedy reasoners: A systematic formal analysisof chain-of-thought. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
  "Lingfeng Shen, Aayush Mishra, and Daniel Khashabi.2024. Do pretrained transformers learn in-context bygradient descent? Preprint, arXiv:2310.08540": "Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.Language models are multilingual chain-of-thoughtreasoners. ArXiv preprint, abs/2210.03057. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou,Margaret Li, Xi Victoria Lin, Noah A. Smith, LukeZettlemoyer, Wen tau Yih, and Mike Lewis. 2024.In-context pretraining: Language modeling beyonddocument boundaries. In The Twelfth InternationalConference on Learning Representations. Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, SungdongKim, HyoungSeok Kim, Boseop Kim, KyunghyunCho, Gichang Lee, Woomyoung Park, Jung-Woo Ha,and Nako Sung. 2022. On the effect of pretrainingcorpora on in-context learning by a large-scale lan-guage model. In Proceedings of the 2022 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 51685186, Seattle, United States.Association for Computational Linguistics. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng,Danqi Chen, and He He. 2023. Measuring induc-tive biases of in-context learning with underspecifieddemonstrations. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2023, Toronto,Canada, July 9-14, 2023, pages 1128911310. Asso-ciation for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013a. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts. 2013b. Recursive deep mod-els for semantic compositionality over a sentimenttreebank. In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2013, 18-21 October 2013, Grand HyattSeattle, Seattle, Washington, USA, A meeting of SIG-DAT, a Special Interest Group of the ACL, pages16311642. ACL.",
  "Taylor Sorensen, Joshua Robinson, Christopher Ryt-ting, Alexander Shaw, Kyle Rogers, Alexia Delorey,": "Mahmoud Khalil, Nancy Fulda, and David Wingate.2022. An information-theoretic approach to promptengineering without ground truth labels. In Proc. ofACL, pages 819862, Dublin, Ireland. Associationfor Computational Linguistics. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta, AdriGarriga-Alonso, et al. 2022. Beyond the imitationgame: Quantifying and extrapolating the capabilitiesof language models. ArXiv preprint, abs/2206.04615. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.Selective annotation makes language models betterfew-shot learners. In The Eleventh International Con-ference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
  "Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong Wang,and Zechao Li. 2023. Exploring effective factors forimproving visual in-context learning. arXiv preprintarXiv:2304.04748": "Mirac Suzgun, Nathan Scales, Nathanael Schrli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi,Denny Zhou, and Jason Wei. 2023.Challengingbig-bench tasks and whether chain-of-thought cansolve them. In Findings of the Association for Com-putational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pages 1300313051. Association forComputational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsenseknowledge. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41494158, Minneapolis, Minnesota. Association forComputational Linguistics. Ruixiang Tang, Dehan Kong, Longtao Huang, and HuiXue. 2023a. Large language models can be lazylearners: Analyze shortcuts in in-context learning.In Findings of the Association for ComputationalLinguistics: ACL 2023, Toronto, Canada, July 9-14,2023, pages 46454657. Association for Computa-tional Linguistics. Yuting Tang, Ratish Puduppully, Zhengyuan Liu, andNancy Chen. 2023b. In-context learning of large lan-guage models for controlled dialogue summarization:A holistic benchmark and empirical analysis. In Pro-ceedings of the 4th New Frontiers in SummarizationWorkshop, pages 5667, Singapore. Association forComputational Linguistics. Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur,and Tanmoy Chakraborty. 2023. Multilingual llmsare better cross-lingual in-context learners with align-ment. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pages 62926307. Association forComputational Linguistics. Romal Thoppilan, Daniel De Freitas, Jamie Hall,Noam Shazeer, Apoorv Kulshreshtha, Heng-TzeCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,Amin Ghafouri, Marcelo Menegali, Yanping Huang,Maxim Krikun, Dmitry Lepikhin, James Qin, DehaoChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,Maarten Bosma, Yanqi Zhou, Chung-Ching Chang,Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S.Meier-Hellstern, Meredith Ringel Morris, TulseeDoshi, Renelito Delos Santos, Toju Duke, Johnny So-raker, Ben Zevenbergen, Vinodkumar Prabhakaran,Mark Diaz, Ben Hutchinson, Kristen Olson, Ale-jandra Molina, Erin Hoffman-John, Josh Lee, LoraAroyo, Ravi Rajakumar, Alena Butryna, MatthewLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-hen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, andQuoc Le. 2022. Lamda: Language models for dialogapplications. ArXiv preprint, abs/2201.08239. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288.",
  "models. In Advances in Neural Information Pro-cessing Systems 34: Annual Conference on NeuralInformation Processing Systems 2021, NeurIPS 2021,December 6-14, 2021, virtual, pages 200212": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,and Subbarao Kambhampati. 2022. Large languagemodels still cant plan (a benchmark for llms on plan-ning and reasoning about change). ArXiv preprint,abs/2206.10498. Johannes von Oswald, Eyvind Niklasson, Ettore Ran-dazzo, Joo Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. 2023. Trans-formers learn in-context by gradient descent. In In-ternational Conference on Machine Learning, ICML2023, 23-29 July 2023, Honolulu, Hawaii, USA, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 3515135174. PMLR. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R. Bowman. 2019. Superglue: A stickierbenchmark for general-purpose language understand-ing systems.In Advances in Neural InformationProcessing Systems 32: Annual Conference on Neu-ral Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada,pages 32613275.",
  "Ben Wang and Aran Komatsuzaki. 2021.GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-guage Model": "Boshi Wang, Xiang Deng, and Huan Sun. 2022a. Itera-tively prompt pre-trained language models for chainof thought. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2022, Abu Dhabi, United Arab Emirates,December 7-11, 2022, pages 27142730. Associationfor Computational Linguistics. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,Huaming Wang, Jinyu Li, et al. 2023a. Neural codeclanguage models are zero-shot text to speech synthe-sizers. arXiv preprint arXiv:2301.02111. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023b. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023, Sin-gapore, December 6-10, 2023, pages 98409855. As-sociation for Computational Linguistics. Shuohang Wang, Yang Liu, Yichong Xu, ChenguangZhu, and Michael Zeng. 2021. Want to reduce la-beling cost? GPT-3 can help. In Findings of theAssociation for Computational Linguistics: EMNLP2021, Virtual Event / Punta Cana, Dominican Re-public, 16-20 November, 2021, pages 41954205.Association for Computational Linguistics. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen,and Tiejun Huang. 2023c. Images speak in images:A generalist painter for in-context visual learning. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 68306839. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,Chunhua Shen, and Tiejun Huang. 2023d.Seg-gpt: Towards segmenting everything in context. InIEEE/CVF International Conference on ComputerVision, ICCV 2023, Paris, France, October 1-6, 2023,pages 11301140. IEEE. Xinyi Wang, Wanrong Zhu, and William Yang Wang.2023e.Large language models are implicitlytopic models: Explaining and finding good demon-strations for in-context learning.arXiv preprintarXiv:2301.11916.",
  "Yaqing Wang and Quanming Yao. 2019. Few-shot learn-ing: A survey. CoRR, abs/1904.05046": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. 2023f. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), ACL 2023, Toronto, Canada, July 9-14, 2023,pages 1348413508. Association for ComputationalLinguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-jana Arunkumar, David Stap, Eshaan Pathak, Gi-annis Karamanolakis, Haizhi Gary Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuz-nia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-tel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-hit, Neeraj Varshney, Phani Rohitha Kaza, PulkitVerma, Ravsehaj Singh Puri, Rushang Karia, SavanDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,Sujan Reddy A, Sumanta Patro, Tanay Dixit, andXudong Shen. 2022b.Super-naturalinstructions:Generalization via declarative instructions on 1600+NLP tasks. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2022, Abu Dhabi, United Arab Emirates,December 7-11, 2022, pages 50855109. Associationfor Computational Linguistics. Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen,Pengcheng He, Weizhu Chen, Zhangyang (Atlas)Wang, and Mingyuan Zhou. 2023g. In-context learn-ing unlocked for diffusion models. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023.",
  "language models are zero-shot learners. In The TenthInternational Conference on Learning Representa-tions, ICLR 2022, Virtual Event, April 25-29, 2022.OpenReview.net": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. 2022b. Emer-gent abilities of large language models. Trans. Mach.Learn. Res., 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022c. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems35: Annual Conference on Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New Orleans,LA, USA, November 28 - December 9, 2022. Jerry W. Wei, Le Hou, Andrew K. Lampinen, XiangningChen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu,Denny Zhou, Tengyu Ma, and Quoc V. Le. 2023a.Symbol tuning improves in-context learning in lan-guage models. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2023, Singapore, December 6-10,2023, pages 968979. Association for ComputationalLinguistics. Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, AlbertWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,Da Huang, Denny Zhou, and Tengyu Ma. 2023b.Larger language models do in-context learning dif-ferently. CoRR, abs/2303.03846.",
  "Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, JiangtaoFeng, Jingjing Xu, Yu Qiao, and Zhiyong Wu. 2023a.Openicl: An open-source framework for in-contextlearning. CoRR, abs/2303.02913": "Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-peng Kong. 2023b. Self-adaptive in-context learn-ing: An information compression perspective for in-context example selection and ordering. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2023, Toronto, Canada, July 9-14, 2023, pages14231436. Association for Computational Linguis-tics.",
  "Auto-icl: In-context learning without human supervi-sion. CoRR, abs/2311.09263": "Zhe Yang, Damai Dai, Peiyi Wang, and Zhifang Sui.2023b. Not all demonstration examples are equallybeneficial: Reweighting demonstration examples forin-context learning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, Sin-gapore, December 6-10, 2023, pages 1320913221.Association for Computational Linguistics. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, andLingpeng Kong. 2023. Compositional exemplarsfor in-context learning. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 3981839833.PMLR. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-soo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,and Taeuk Kim. 2022. Ground-truth labels matter:A deeper look into input-label demonstrations. InProceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2022, Abu Dhabi, United Arab Emirates, December7-11, 2022, pages 24222437. Association for Com-putational Linguistics.",
  "Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, andZhaoran Wang. 2023b.What and how does in-context learning learn?bayesian model averag-ing, parameterization, and generalization.CoRR,abs/2305.19420": "Zhuosheng Zhang, Aston Zhang, Mu Li, and AlexSmola. 2023c. Automatic chain of thought prompt-ing in large language models. In The Eleventh In-ternational Conference on Learning Representations,ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Open-Review.net. Ziqiang Zhang, Long Zhou, Chengyi Wang, SanyuanChen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu,Huaming Wang, Jinyu Li, et al. 2023d. Speak for-eign languages with your own voice: Cross-lingualneural codec language modeling.arXiv preprintarXiv:2303.03926. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021.Calibrate before use: Im-proving few-shot performance of language models.In Proc. of ICML, volume 139 of Proceedings ofMachine Learning Research, pages 1269712706.PMLR. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei,Nathan Scales, Xuezhi Wang, Dale Schuurmans,Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.Chi. 2023a. Least-to-most prompting enables com-plex reasoning in large language models. In TheEleventh International Conference on Learning Rep-resentations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net.",
  "Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cot-terell, and Mrinmaya Sachan. 2023b.Efficientprompting via dynamic in-context learning. CoRR,abs/2305.11170": "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2023c. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, HanqiYan, Lin Gui, and Yulan He. 2023d. The mysteryand fascination of llms: A comprehensive survey onthe interpretation and analysis of emergent abilities.arXiv preprint arXiv:2311.00237.",
  "A.1Training": "To further enhanced ICL capabilities, methods pro-pose to train the LLMs in the stage of pre-trainingand warmup before ICL inference.3 Takeaway: (1) The key idea of training beforeinference is to bridge the gap between pretrainingand downstream ICL formats by introducing ob-jectives close to in-context learning. Warmup isoptional for ICL as many pretrained LLMs havemanifested the ICL ability. (2) Compared to in-context finetuning involving demonstration, instruc-tion finetuning without a few examples as demon-stration is simpler and more popular. All thesewarmup methods improve the ICL capability byupdating the model parameters, which implies thatthe ICL capability of the original LLMs has greatpotential for improvement. Therefore, althoughICL does not strictly require model warmup, werecommend adding a warmup stage before ICL in-ference. (3) The performance advancement madeby warmup encounters a plateau when increasinglyscaling up the training data, indicating that LLMsonly need a small amount of data to adapt to learnfrom the context during warmup.",
  "The performance of ICL strongly relies on thedemonstration surface, including the selection, for-matting, and ordering of demonstration examples": "3 Takeaway:(1) Demonstration selectionstrategies improve the ICL performance, but mostof them are instance level. Since ICL is mainlyevaluated under few-shot settings, the corpus-levelselection strategy is more important yet underex-plored. (2) The output score or probability distri-bution of LLMs plays an important role in instanceselecting. (3) For k demonstrations, the size ofsearch space of permutations is k!. How to find thebest orders efficiently or how to approximate theoptimal ranking better is also a challenging ques-tion. (4) Adding chain-of-thoughts can effectivelydecompose complex reasoning tasks into intermedi-ate reasoning steps. During inference, multi-stagedemonstration designing strategies are applied togenerate CoTs better. How to improve the CoTprompting ability of LLMs is also worth explor-ing. (5) In addition to human-written demonstra-tions, the generative nature of LLMs can be utilizedin demonstration designing. LLMs can generateinstructions, demonstrations, probing sets, chain-of-thoughts, and so on. By using LLM-generateddemonstrations, ICL can largely get rid of humanefforts on writing templates.",
  "A.3Scoring Function": "The scoring function determines how to transformthe predictions of a language model into an esti-mation of the likelihood of a specific answer. Theanswer with the highest probability is selected asthe final answer. 3 Takeaway: (1) Although directly adoptingthe conditional probability of candidate answers isefficient, this method still poses some restrictionson the template design. Perplexity is also a sim-ple and widely scoring function. This method hasuniversal applications, including both classificationtasks and generation tasks. However, both methodsare still sensitive to demonstration surface, whileChannel is a remedy that especially works underimbalanced data regimes. (2) Existing scoring func-tions all compute a score straightforwardly fromthe conditional probability of LLMs. There is lim-ited research on calibrating the bias or mitigatingthe sensitivity via scoring strategies.",
  "A.4Analysis": "Numerous analytical studies investigate influencingfactors of ICL during both the pretraining and infer-ence stages, and attempt to figure out the learningmechanisms of ICL from the perspective of func-tional modules and theoretical interpretation.3 Takeaway: (1) Knowing and considering whyICL works and what factors may influence can helpus improve the ICL performance. (2) Althoughsome analytical studies have taken a preliminarystep to explain ICL, most of them are limited tosimple tasks and small models. Extending analysison extensive tasks and large models may be thenext step to be considered. (3) Among existingwork, explaining ICL with gradient descent seemsto be a reasonable, general, and promising directionfor future research. If we build clear connectionsbetween ICL and gradient-descent-based learning,we can borrow ideas from the history of traditionaldeep learning to improve ICL.",
  "A.5In-context Learning Beyond Text": "The tremendous success of ICL in NLP has in-spired researchers to explore in-context learning indifferent modalities beyond natural language withpromising results.3 Takeaway: (1) Properly formatted data (e.g.,interleaved image-text datasets for vision-languagetasks) and architecture designs are key factorsfor activating the potential of in-context learning.Exploring it in a more complex structured spacesuch as for graph data is challenging and promis-ing (Huang et al., 2023a). (2) Findings in textualin-context learning demonstration design and selec-tion cannot be trivially transferred to other modal-ities. Domain-specific investigation is required tofully leverage the potential of in-context learningin various modalities.",
  "BExperimental Detail": "In the experiment,we utilize 8 demonstra-tions and test on gpt2 (Radford et al., 2019),gptj (Wang and Komatsuzaki, 2021), LLaMA3-8B-Instruct(AI@Meta, 2024) and Qwen2-7B-Instruct (Bai et al., 2023a). All experiments areexecuted on a single NVIDIA A100 (80G). Fordatasets we choose sst2 (Socher et al., 2013a),sst5 (Socher et al., 2013b), commonsense_qa (Tal-mor et al., 2019), ag_news (Zhang et al., 2015)and snli (Bowman et al., 2015). For the last twodatasets, we only select 1000 data from the train-",
  "AVG1.002.612.90": ": The qualitative results of the Efficiency met-ric in which record the language model infer-ence latency (including the time for scoring with dif-ferent scoring functions, with input data containing 8in-context examples). The unit is milliseconds (ms).Each cells parentheses contain the ratio of the latencyfor the current column model using the current row scor-ing function to the latency using direct inference. Thefinal calculated average is the average of these ratios.",
  "AVG0.930.783.03": ": The qualitative results of the Stability metricin which reflect whether the in-context learningability is easily affected by changes in demonstrationexamples. We conducted experiments using a test set ofsize 10k and set up 5 different random seeds. Each time,8 examples were randomly selected from 5k trainingexamples for the experiments. The table records thevariance of performance. ing set for retrieval and the first 1000 data fromthe test set for testing. During the inference phase,a PPL-based approach is employed. The entirecode framework is built upon OpenICL (Wu et al.,2023a), for which we extend our gratitude to theauthors. and show the quantitative resultson the efficiency and stability metrics for differentscoring functions in .",
  ": New challenging evaluation benchmarks forICL. For short, we use LLMAS to represent LLM As-sessment Suite (Valmeekam et al., 2022)": "3 can achieve results comparable to state-of-the-art (SOTA) finetuning performance on COPA andReCoRD, but still falls behind finetuning on mostNLU tasks.Hao et al. (2022b) showed the po-tential of scaling up the number of demonstrationexamples. However, the improvement brought byscaling is very limited. At present, compared tofinetuning, there still remains some room for ICLto reach on traditional NLP tasks.",
  "C.2New Challenging Tasks": "In the era of large language models with in-contextlearning capabilities, researchers are more inter-ested in evaluating the intrinsic capabilities of largelanguage models without downstream task finetun-ing (Bommasani et al., 2021).To explore the capability limitations of LLM onvarious tasks, Srivastava et al. (2022) proposedthe BIG-Bench (Srivastava et al., 2022), a largebenchmark covering a large range of tasks, includ-ing linguistics, chemistry, biology, social behav-ior, and beyond. The best models have alreadyoutperformed the average reported human-raterresults on 65% of the BIG-Bench tasks throughICL (Suzgun et al., 2023). To further explore tasksactually unsolvable by current language models,Suzgun et al. (2023) proposed a more challengingICL benchmark, BIG-Bench Hard (BBH). BBH in-cludes 23 unsolved tasks, constructed by selectingchallenging tasks where the state-of-art model per-formances are far below the human performances.Besides, researchers are searching for inverse scal-ing tasks,2 that is, tasks where model performancereduces when scaling up the model size. Suchtasks also highlight potential issues with the cur- rent paradigm of ICL. To further probe the modelgeneralization ability, Iyer et al. (2022) proposedOPT-IML Bench, consisting of 2000 NLP tasksfrom 8 existing benchmarks, especially benchmarkfor ICL on held-out categories.Specifically, a series of studies focus on ex-ploring the reasoning ability of ICL. Saparov andHe (2023) generated an example from a syntheticworld model represented in first-order logic andparsed the ICL generations into symbolic proofsfor formal analysis. They found that LLMs canmake correct individual deduction steps via ICL.Shi et al. (2022) constructed the MGSM bench-mark to evaluate the chain-of-thought reasoningabilities of LLMs in multilingual settings, findingthat LLMs manifest complex reasoning across mul-tiple languages. To further probe more sophisti-cated planning and reasoning abilities of LLMs,Valmeekam et al. (2022) provided multiple testcases for evaluating various reasoning abilities onactions and change, where existing ICL methodson LLMs show poor performance.In addition,Tang et al. (2023b) proposed abenchmark called SAMSum, which is a human-annotated dataset specifically designed for multi-turn dialogue summarization, to evaluate the qual-ity of dialogue summaries generated by LLMs viaICL.",
  ": Image-only and textual augmented promptingfor visual in-context learning": "tasks like image segmentation. This method isexpanded in Painter (Wang et al., 2023c), whichincorporates multiple tasks to develop a general-ist model with competitive performance. SegGPT(Wang et al., 2023d) further builds on this by inte-grating diverse segmentation tasks and exploringensemble techniques to enhance example quality.Additionally, Wang et al. (2023g) introduce thePrompt Diffusion model, the first diffusion-basedmodel with ICL abilities, guided by an extra textprompt for more precise image generation, as illus-trated in .Similar to ICL in NLP, the effectiveness of visualin-context learning greatly depends on the choiceof demonstration images, as shown in research by(Zhang et al., 2023a) and (Sun et al., 2023). Tooptimize this, Zhang et al. (2023a) examine twostrategies: using an unsupervised retriever to selectthe nearest samples with an existing model, and asupervised approach to train a specialized retrieverto boost ICL performance. These approaches im-prove results by ensuring semantic similarity andbetter alignment in viewpoint, background, and ap-pearance. Beyond retrieval, Sun et al. (2023) alsoinvestigate a prompt fusion technique to furtherenhance outcomes.",
  "D.2Multi-Modal In-Context Learning": "In the vision-language domain, a vision encoderpaired with a frozen language model demonstratesmulti-modal few-shot learning capabilities aftertraining on image-caption datasets, as shown by theFrozen model (Tsimpoukelli et al., 2021). Extend-ing this, Flamingo integrates a vision encoder withlarge language models (LLMs) for enhanced in-context learning across multi-modal tasks, leverag-ing large-scale web corpora (Alayrac et al., 2022).Similarly, Kosmos-1 exhibits zero-shot, few-shot, and multi-modal chain-of-thought prompting abil-ities (Huang et al., 2023b).METALM intro-duces a semi-causal language modeling objectiveto achieve strong ICL performance across vision-language tasks (Hao et al., 2022a).The ICL-D3IE approach employs a novel in-context learningframework that iteratively updates diverse demon-strationsincluding hard, layout-aware, and for-matting demonstrations to train large languagemodels (LLMs) for enhanced document informa-tion extraction (DIE)(He et al., 2023).Recentadvancements include creating instruction tun-ing datasets from existing vision-language tasksor with advanced LLMs like GPT-4, connectingLLMs with powerful vision foundational modelslike BLIP-2 for multi-modal learning (Xu et al.,2023b; Li et al., 2023a; Liu et al., 2023a; Zhu et al.,2023a; Dai et al., 2023b).",
  "D.3Speech In-Context Learning": "In the speech area, Wang et al. (2023a) treated text-to-speech synthesis as a language modeling task.They use audio codec codes as an intermediate rep-resentation and propose the first TTS frameworkwith strong in-context learning capability. Subse-quently, VALLE-X (Zhang et al., 2023d) extend theidea to multi-lingual scenarios, demonstrating su-perior performance in zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speechtranslation tasks.",
  "D.4Comparison with other survey papers": "Our survey was drafted and posted on the Arxiv atthe end of 2022, which is, to the best of our knowl-edge, the very first to review in-context learning inthe field. We also regularly update this survey in atimely manner, with four major revisions.Starting from 2023, we notice the emerge of sev-eral related survey in the field of in-context learn-ing. Xu et al. (2024) made a comprehensive reviewon the choices for models, training procedures andinference algorithms to retrieve demonstrative ex-amples of in-context learning. Li (2023) providedpractical suggestions on prompt engineering for in-context learning. Zhou et al. (2023d) and Highmore(2024) focused on the theoretical interpretation andanalysis of ICL, which corresponds to in this survey. All the above-mentioned survey pa-pers differ with ours in terms of scope and topics.This survey focused on the general development ofICL, including the formal definition of ICL, train-ing strategies, prompt designing strategies, analysis"
}