{
  "Abstract": "Pre-trained language models like BERT and T5serve as crucial backbone encoders for denseretrieval.However, these models often ex-hibit limited generalization capabilities andface challenges in improving in-domain accu-racy. Recent research has explored using largelanguage models (LLMs) as retrievers, achiev-ing state-of-the-art performance across varioustasks. Despite these advancements, the spe-cific benefits of LLMs over traditional retriev-ers and the impact of different LLM configura-tionssuch as parameter sizes, pre-training du-ration, and alignment processeson retrievaltasks remain unclear. In this work, we conduct a comprehensive em-pirical study on six key dimensions of denseretrieval capabilities, including in-domain accu-racy, data efficiency, zero-shot generalization,lengthy retrieval, instruction-based retrieval,and multi-task learning. We evaluate over 15different backbone LLMs and non-LLMs. Ourfindings reveal that larger models and extensivepre-training consistently enhance in-domainaccuracy and data efficiency.Additionally,larger models demonstrate significant potentialin zero-shot generalization, lengthy retrieval,instruction-based retrieval, and multi-task learn-ing. These results underscore the advantagesof LLMs as versatile and effective backboneencoders in dense retrieval, providing valuableinsights for future research and development inthis field.",
  ".Equal contribution.Corresponding author": "models have become the predominant choice inrecent neural retrieval approaches and are widelyapplied in various downstream tasks such as websearch, question answering, and sentence similarity(Karpukhin et al., 2020; Xiong et al., 2020; Muen-nighoff et al., 2022).In the past few years, dense retrieval modelsintensively adopted pre-trained language models,such as BERT (Devlin et al., 2018) and T5 (Raffelet al., 2020), as their backbone encoders. Thesemodels excel in identifying semantic similaritiesbetween queries and documents. However, theystill face significant challenges in becoming ver-satile enough to handle a wide range of retrievaltasks (Muennighoff et al., 2022). Their in-domainretrieval accuracy is often constrained by the capac-ity of their backbone encoders, such as the numberof parameters (Ni et al., 2021). Additionally, denseretrieval models typically struggle to generalize tounseen data, necessitating fine-tuning with a largeamount of labeled data to perform well in the tar-get domain. Finally, achieving versatility in denseretrieval models requires training on multiple re-trieval tasks simultaneously, which demands suffi-cient capacity from the backbone encoder (Zhanget al., 2023; Xiao et al., 2023).Recently Large Language Models (LLMs) havebeen prompted or fine-tuned as dense retrieval mod-els and achieved improved performance across awide range of retrieval tasks, thanks to their supe-rior capability for semantic understanding and richworld knowledge (Li et al., 2023; Wang et al., 2023;Zhuang et al., 2024; Muennighoff et al., 2024).These models vary in parameters from 2 billionto 56 billion, with pre-training sufficiency rang-ing from hundreds of billions to tens of trillionsof tokens, and include both base models and hu-man preference aligned chat models. Despite thecommon understanding that larger models gener-ally yield better performance (Kaplan et al., 2020;Biderman et al., 2023), the specific benefits of vary-",
  "ing parameter numbers, pre-training sufficiency,and alignment processes of backbone LLMs fordifferent retrieval tasks still remain unclear": "In this study, we focus on the following two re-search questions: 1) For different retrieval tasks,what specific benefits can LLMs offer compared tonon-LLMs as the backbone encoders? 2) For LLMswith varying configurations (i.e., different param-eter numbers, pre-training sufficiency and align-ment processes), what contributes more to differentretrieval tasks as the backbone encoder. We con-duct comprehensive empirical investigation acrossa wide range of retrieval tasks, assessing variouscritical retrieval capabilities: in-domain accuracy,data efficiency, zero-shot generalization, lengthyretrieval generalization, instruction-based retrieval,and multi-task learning. Our study explore over15 different backbone LLMs and non-LLMs, withparameter numbers ranging from 0.1 billion to 32billion and varying pre-training sufficiency, includ-ing both base LLMs and chat LLMs. Previous dense retrieval models have demon-strated inferior in-domain accuracy due to thelimited capacity of their backbone encoders (Niet al., 2021). We employ MS MARCO (Nguyenet al., 2016), one of the largest web search datasets,to train and evaluate the in-domain accuracy ofdense retrieval models with different backbone en-coders. Our results indicate that both increasingthe model size and enhancing pre-training suffi-ciency can consistently improve the upper limitof in-domain accuracy. Notably, we discover thatboth base LLMs and human-preference-alignedchat LLMs show comparable potential as back-bone encoders for dense retrieval tasks. By train-ing with different proportions of MS MARCO, weexplore data efficiency and find that scaling upmodel size facilitates convergence, allowing LLMsto converge swiftly even with limited annotateddata, without the need for intricate multi-stage train-ing processes. We examine generalization ability from threeperspectives: zero-shot generalization, lengthy re-trieval generalization, and instruction-based re-trieval generalization. First, we evaluate zero-shotgeneralization using BEIR benchmark (Thakuret al., 2021). Our findings indicate that modelsize is the most crucial factor for zero-shot re-trieval generalization. Moreover, traditional denseretrieval models are limited by the maximum inputlength used during pre-training and retrieval train- ing. We investigate whether LLM-based retrievers,pre-trained with longer context windows, can ef-fectively generalize to lengthy retrieval tasks evenwhen trained with shorter passage lengths. Finally,dense retrieval models often lack flexibility in han-dling varying retrieval intents (Su et al., 2022). Weexplore the capability of different models to incor-porate instructions during retrieval, discoveringthat training with instruction benefits LLMs butnot non-LLMs, and that human-preference align-ment does not significantly improve performancecompared to base LLMs.We further explore the multi-task learning ca-pabilities of models with different backbone en-coders, essential for developing versatile retrievers(Zhang et al., 2023; Xiao et al., 2023). We adoptfive distinct retrieval tasks, where interference ex-ists due to varying retrieval intents. Our findingsreveal that although all models experience perfor-mance decreases with multi-task training comparedto training on each single-task, increasing modelsize consistently mitigates this gap.To summarize, we make the following contri-butions: 1) We conduct a thorough experimentalstudy using more than 15 backbone encoders withdifferent configurations for dense retrieval acrosssix distinct retrieval tasks. 2) We demonstrate thatLLM-based retrievers consistently enhance perfor-mance across all retrieval tasks compared to non-LLM-based retrievers. 3) We investigate how dif-ferent configurations of backbone LLMs impacteach retrieval task, focusing on distinct retrievalcapabilities.",
  "Related Work": "The related works are reviewed from two aspects:dense retrieval, LLM-based retriever.First of all, in the realm of neural retrievers,dense retrieval models have consistently demon-strated superior performance over traditional sparsemodels like BM25 across a wide array of retrievaltasks (Karpukhin et al., 2020; Ni et al., 2021; Muen-nighoff et al., 2022). A critical factor contributingto the success of dense retrieval models is the uti-lization of powerful pre-trained language modelsas their initialization.Over the past few years, pre-trained languagemodels such as BERT (Devlin et al., 2018) andT5 (Raffel et al., 2020) have been intensively usedas backbone encoders for dense retrieval. For in-stance, GTR (Ni et al., 2021) highlights the in- domain accuracy and generalization capabilitiesof T5-based dense retrieval models, with modelparameters reaching up to 4.8 billion. Fang et al.(2024) explores scaling laws for dense retrievalmodels but restricts their study to BERT backboneswith up to 110 million parameters and only ex-plores the in-domain situation. Currently, state-of-the-art dense retrievers employ models with morethan 7 billion parameters or more as backbones.Neelakantan et al. (2022) discuss large-scale un-supervised text embedding pre-training, observingconsistent performance improvements when scal-ing up GPT-based dense retrieval model sizes from300 million to 175 billion parameters. Addition-ally, recent studies such as Wang et al. (2023) haveshown that fine-tuning directly with labeled datacan achieve strong performance. Our study focuseson fine-tuning directly using labeled data whilecomparing various backbone encoders. Large Language Models (LLMs) have recentlydemonstrated significant potential as backbone en-coders for dense retrieval, attributed to their vastnumber of parameters and extensive pre-training.Repllama (Ma et al., 2023) fine-tuned Llama-2-7Band Llama-2-13B to function both as dense retriev-ers and pointwise rerankers. LLaRA (Li et al.,2023) introduced two pretraining tasks specificallydesigned to better adapt the backbone Llama-2-7B model for dense retrieval, resulting in notableimprovements in both supervised and zero-shot sce-narios. E5-mistral and Gecko (Wang et al., 2023;Lee et al., 2024) enhanced the training of LLM-based dense retrievers using synthetic data, employ-ing models with 1.5 billion and 7 billion parametersto achieve notable results across various retrievaltasks. GRIT (Muennighoff et al., 2024) success-fully unified text embedding and generation withina single LLM, maintaining performance levels com-parable to those of specialized embedding-only andgenerative-only models, using a model with 56 bil-lion parameters (14 billion activation parameters).LLM2Vec (BehnamGhader et al., 2024) presentedan unsupervised method for transforming decoder-only LLMs into dense retrievers, demonstratingsignificant promise for adapting LLM backbone en-coders for dense retrieval in an unsupervised man-ner. PromptReps (Zhuang et al., 2024) employedhuman preference-aligned chat LLMs to producehigh-quality dense representations unsupervised.",
  "These models vary in parameters from 1.5 billionto 56 billion, with pre-training covering hundreds": "of billions to tens of trillions of tokens, and includeboth base LLMs and human preference-alignedchat LLMs. Despite the exciting advancementsin retrieval tasks achieved by leveraging variousLLMs with distinct configurations and diverse train-ing strategies, the specific benefits of variations inparameter count, pre-training extent, and alignmentprocesses of backbone LLMs for retrieval tasks re-main still uncertain.",
  "s(q, p) = hq, hp(1)": "This allows for the retrieval of relevant docu-ments by performing approximate nearest neighbor(ANN) search within the embedding space.In our study, we compare more than 15 backboneencoders, varying in model architecture (encoder-only and decoder-only), model size (0.1B to 32B),and pre-training sufficiency (up to 15T tokens).Consistent with prior research, we utilize the [CLS]token to obtain text representations for the BERTmodel and employ mean-pooling for the T5 model.For instance, BERT tokenizes the input text intoa sequence T: [CLS], t1, ..., tN, [EOS]. This tok-enized sequence is subsequently encoded by BERT,generating output embeddings that are combinedto form the text embedding, with the [CLS] tokenperforming this integration:",
  "ht = BERT(T)[CLS](2)": "When using large language model (LLM) as thebackbone encoder, text embeddings need to be cre-ated differently. Most LLMs use a decoder-onlyarchitecture and causal attention mechanism, mean-ing that only the last token in the input sequencecan access the global context. As a result, the textembedding is taken from the output embedding ofthe special token [EOS]:",
  "Empirical Study": "In this section, we aim to address two key researchquestions: 1) For different retrieval tasks, whatspecific benefits can LLMs offer compared to non-LLMs as the backbone encoders? 2) For LLMswith varying configurations (i.e., different param-eter numbers, pre-training sufficiency, and align-ment processes), what contributes more to differentretrieval tasks as the backbone encoder. To answerthese questions, we conduct a comprehensive em-pirical study across six critical dimensions of denseretrieval, each encompassing several specific re-trieval tasks. These dimensions are investigatedusing various pre-trained language models as back-bone encoders, focusing on: in-domain accuracy(.1), data efficiency (.2), zero-shot generalization (.3), lengthy retrievalgeneralization (.4), instruction-based re-trieval (.5), and multi-task learning (Sec-tion 4.6).",
  "In-domain Accuracy": "Setting We utilize MS MARCO (Nguyen et al.,2016) to train and evaluate the in-domain accu-racy of dense retrieval models with varying back-bones encoders. Specifically, we employ BERT(Devlin et al., 2018) with 110M and 330M parame-ters (BERT-base and BERT-large), T5 (Raffel et al.,2020) encoders with parameter numbers rangingfrom 110M to 4.8B, and a diverse set of LLMsincluding the Llama, Phi, Gemma, and Qwen1.5series (Touvron et al., 2023; Gunasekar et al., 2023;Bai et al., 2023; Team et al., 2024). It is impor-tant to note that different LLMs have varying con-figurations.For instance, the phi-1.5 model is a lightweight LLM with 1.3B parameters and ispre-trained on a relatively small amount of tokens(150B), indicating less pre-training sufficiency. Incontrast, the Llama-3-8B model is extensively pre-trained on over 15T tokens, significantly more thanthe 2T tokens used for Llama-2-7B. The Qwen1.5series offers a variety of models in different sizes,all pre-trained on the same corpus, enabling directcomparisons of the effects of scaling up model size.All models are trained with a batch size of 128and incorporate 7 hard negative samples to en-sure fair comparisons of in-domain retrieval accu-racy. All training operations take place on 8xA800(80GB) GPUs. We use the Adam optimizer withan initial learning rate of 3e-4 and linear decay.For training LLM retrievers, we employ LoRA (Huet al., 2021), which has demonstrated similar ef-ficacy to full-parameter fine-tuning for retrievaltasks (Ma et al., 2023). The in-domain accuracyof each model is evaluated using the MS MARCOdevelopment set, comprising 6,980 queries. Weuse NDCG@10, MRR@10, Recall@10, and Re-call@1000 as evaluation metrics, providing a com-prehensive analysis of in-domain performance. Results and Analysis As presented in , theresults indicate that model performance generallyimproves with an increase in parameter numbers.This trend is particularly noticeable within modelsfrom the same series. For instance, the Qwen1.5 se-ries demonstrates this progression: Qwen1.5-0.5Bmodel scores 36.7, while the Qwen1.5-32B modelachieves 42.6, representing an improvement of 5.9points. This trend suggests that increasing modelsize is a feasible way to yield better in-domainaccuracy. Detailed results are presented in . Additionally, the results demonstrate that LLM-based retrievers significantly outperform non-LLMretrievers. The performance of Gemma-2B has al-ready surpassed all BERT and T5-based modelsdespite having fewer parameters than the T5-xxlmodel. This suggests that LLMs extensive pre-training and advanced language understanding ca-pabilities offer significant advantages as backboneencoders for dense retrieval.An interesting observation is that smaller mod-els can sometimes marginally outperform largerones. The Qwen1.5-0.5B model, with fewer pa-rameters, surpasses the Phi-1.5-1.3B model andcompetes closely with the Phi-2-2.7B model. Thisperformance discrepancy may be attributed to dif-ferences in pre-training sufficiency. The Qwen1.5",
  ": Data efficiency": "models benefit from more extensive and diversepre-training data, totaling over 3 trillion tokens,whereas the Phi models are pre-trained on a smalleramount of high-quality data, with 150 billion to-kens for the Phi-1.5 and 1.4 trillion tokens forthe Phi-2. This extensive pre-training enables theQwen1.5-0.5B model to perform better when fine-tuned for retrieval tasks. A similar conclusion canbe drawn from the comparison between the Llama-3-8B and Llama-2-7B models, as well as betweenLLMs and non-LLMs. Extensive and varied pre-training of backbone encoders can significantly en-hance in-domain retrieval accuracy, even compen-sating for a smaller parameter count.",
  "Data Efficiency": "Setting We use checkpoints from models trainedon MS MARCO for different numbers of stepsto evaluate their performance on the developmentset, in order to better understand the impact ofparameter number and pre-training sufficiency ondata efficiency and convergence speed.We compare BERT-large, Qwen1.5-0.5B, andLlama-2-7B to explore the impact of data efficiencywith model parameter number and pre-trainingsufficiency. Notably, BERT-large and Qwen1.5-",
  "B have similar non-embedding parameter num-ber, while Qwen1.5-0.5B is based on decoder ar-chitecture and has undergone more extensive pre-training": "Results and Analysis As presented in , our findings indicate that larger model sizeslead to higher data efficiency and faster conver-gence. Specifically, after 100 training steps on MSMARCO, Llama-2-7B outperforms Qwen1.5-0.5Bby 5.4 points and BERT-large by 14.4 points. Thissuggests that with an increase in parameter num-ber, better performance can be achieved with lesslabeled data. Furthermore, as shown in ,when comparing the relative score difference be-tween 100 steps and the full training of 3700 steps,Llama-2-7B shows a score difference of 8.8 points,which is smaller than the 9.7 points for Qwen1.5-0.5B and 15.3 points for BERT-large. This indi-cates that larger models are able to converge faster. The experiment results also demonstrate thatLLMs have better data efficiency compared tonon-LLMs, even with similar parameter sizes.For example, after 100 training steps on MSMARCO, Qwen1.5-0.5B outperforms BERT-largeby 9 points. Despite having a similar number ofparameters, Qwen1.5-0.5B has undergone more",
  "Zero-Shot Generalization": "Setting Dense retrieval models typically strugglewith zero-shot retrieval on unseen data (Ni et al.,2021). We investigate the specific benefits thatLLM-based retrievers can bring to zero-shot gen-eralization, focusing on varying model sizes andpre-training sufficiency.We evaluate all models on 13 zero-shot retrievaltasks in the BEIR (Thakur et al., 2021) evalua-tion suite, which encompasses a diverse range ofretrieval tasks and domains, including medical re-trieval, financial retrieval, and duplication detec-tion. All models are directly transferred for zero-shot evaluation on BEIR after being trained on MSMARCO. During the evaluations, we set the max-imum length of the query to 64 tokens and themaximum length of the passage to 256 tokens.Results and Analysis The results are shown in, measured by average performance ofNDCG@10 across 13 retrieval tasks. LLM retriev-ers significantly outperform non-LLM retrievers in",
  ": Unseen instruction comparison. ID meansinstructions are seen during training, OOD means theinstructions are unseen during training": "zero-shot retrieval tasks, indicating that the exten-sive knowledge and robust generalization capabili-ties of LLMs are highly advantageous for zero-shotretrieval. Notably, this improvement is not merelya result of increased model size: even the Qwen1.5-0.5B model, which has a similar non-embeddingparameter count, demonstrates much better gener-alization (+1.6%) than the BERT-large model. Thishighlights the potential of LLMs to serve as robustencoders for various retrieval domains.For different configurations of LLMs, model sizeis the primary factor influencing their generaliza-tion capability. Unlike in-domain accuracy, whereboth model size and pre-training sufficiency areimportant, generalization performance is almostdirectly correlated with the number of parameters.For example, the Qwen-0.5B model, despite bene-fiting from more extensive pre-training, performsworse than the Phi-1.5-1.3B and Phi-2-2.7B mod-els with larger parameter sizes but less pre-trainingsufficiency. This suggests that larger models, withbetter capacity, can prevent overfitting to domain-specific retrieval data, resulting in better general-ization to unseen data.",
  ": Multi-task learning performance measured by NDCG@10. The performance discrepancy is compared totraining on each single task": "pre-training and retrieval training, while extendingthis length significantly increases computationalcosts (Chen et al., 2024). Given that LLMs arepre-trained with longer context windows, we inves-tigate if they can be trained with shorter passagelengths while effectively generalizing to longerlengths during retrieval. We use MS MARCO fortraining and set the maximum query length to 64tokens and the maximum passage length to 256tokens. All other hyperparameters are aligned withthose used in .1.For evaluation, we utilize NarrativeQA (Kocisk`yet al., 2018), which requires long context informa-tion to accurately retrieve target queries. The eval-uation was conducted with maximum lengths rang-ing from 256 to 8192 tokens for passages, with thegoal of thoroughly assessing each models lengthgeneralization capabilities in the retrieval task.Results and Analysis The results are illustratedin . The long context window of LLMsimproves length generalization compared to BERT.When evaluated with a context length of 256 tokenson the NarrativeQA Retrieval task, BERT-large out-performs Qwen1.5-0.5B by 0.4 points. However,with a length of 512 tokens, Qwen1.5-0.5B exceedsthe performance of BERT-large by 0.9 points. Thisinteresting finding demonstrates that LLM retriev-ers consistently generalize better with increasinginput lengths, while non-LLM retrievers like BERTstruggle with longer inputs and are constrained bya 512-token limit unless explicitly extended. De-tailed results are presentend in",
  "Instruction-Based Retrieval": "Setting Dense retrieval models often lack flexibil-ity in adapting to varying retrieval intents of users,which is both common and critical in real-worldretrieval scenarios (Su et al., 2022). We incorporateinstructions into the training of dense retrieval mod-els, aiming to evaluate the instruction comprehen-sion capabilities of models with different backboneencoders. Specifically, we prepare five retrievalinstructions and prepend them to queries duringtraining on MS MARCO. We conduct evaluationon six retrieval tasks, including both in-domainand out-of-domain scenarios, to determine whetherincorporating instructions can enhance the under-standing of retrieval intent thus improving generalperformance of different models. The instructionsare presented in .Results and Analysis As shown in , train-ing with instructions significantly improves the per-formance of LLM retrievers, whereas for BERTretrievers results in decreased performance. Thissuggests that LLMs have superior semantic under-standing, enabling them to adjust retrieval objec-tives based on instructions.We evaluate models on MS MARCO (Nguyenet al., 2016) development set using instructions notseen during training. The result is presented in. These instructions are complex modifi-cations of the training instructions (), de-signed to test the models robustness. The resultsshow that LLM retrievers exhibit strong robustness to these new instructions, while BERT experienceperformance degradation due to interference fromthe unseen instructions. This implies that LLMscan better utilize their capabilities in real-worldretrieval scenarios as backbone encoder for denseretrieval, offering better customizability and adapt-ability to meet diverse user retrieval needs.Furthermore, we adopt chat LLMs as backboneencoders to investigate if these aligned modelscould better utilize retrieval instructions, the resultis shown in . Contrary to expectations, chatLLMs do not show further improvements whentrained and tested under the same setting as basemodels. Thus, given the superior scalability of baseLLMs across various downstream tasks, the baseLLMs remain more suitable as backbone encodersfor dense retrieval models.",
  "Multi-Task Learning": "Setting Training a versatile dense retrieval modelis challenging due to the specific semantic infor-mation required by various retrieval tasks, oftencausing mutual interference (Zhang et al., 2023;Xiao et al., 2023; Neelakantan et al., 2022). Weexplore the multi-task learning capacity of differentbackbone encoders, which is essential for develop-ing robust retrievers.Our study encompasses four distinct retrievaltasks alongside a text similarity task: 1) ToolLLM(Qin et al., 2023): This task evaluates the abilityof retrievers to identify necessary tools based onprovided instructions and tool descriptions. Per-formance is measured using NDCG@5 on the testset. 2) QReCC (Anantha et al., 2020): This taskinvolves retrieving relevant knowledge based onthe concatenation of conversation context and themost recent query. Performance is assessed usingNDCG@3, in line with previous studies (Mao et al.,2023). 3) NLI (Bowman et al., 2015): We utilizethe NLI training set to establish text similarity capa-bilities and evaluate models on STS tasks from theMTEB (Muennighoff et al., 2022). 4) HotpotQA(Yang et al., 2018): This task tests retrieval perfor-mance in a multi-hop question-answering scenario.5) MS MARCO (Nguyen et al., 2016): This taskassesses the web search capabilities of differentmodels.Results and Analysis As shown in , theresults demonstrate a clear trend: as model sizeincreases, the average performance across the fivedistinct retrieval tasks improves. This indicates that larger models exhibit enhanced universalityand capacity, suggesting their greater potential toserve as versatile embedding models in multi-taskscenarios.In addition to comparing the absolute perfor-mance of each model across multiple tasks, we con-ducted experiments contrasting the performanceof models trained on each individual task versusjoint multi-task training. presents the rel-ative performance discrepancy. We observed thatmulti-task training results in a relative performancedecrease compared to single-task training across alltasks. This aligns with the hypothesis proposed by(Neelakantan et al., 2022), suggesting that certainretrieval tasks might have inherently conflictingdefinitions, such as search and sentence similaritytasks. Notably, the performance decrease dimin-ishes as model size increases, indicating that largermodels might be capable of learning the intrinsicrelationships and distinctions between tasks duringmulti-task training. This capability potentially al-lows these models to narrow the performance gapbetween multi-task and single-task training, and insome cases even achieve improvements over single-task training. This suggests that LLMs with moreparameter numbers have the potential to serve asversatile general-purpose retrievers across multipleretrieval tasks.",
  "Conclusions": "In this paper, we conduct a comprehensive empir-ical investigation into the benefits and configura-tions of LLMs as backbone encoders for denseretrieval tasks. Our focus is on comparing LLMswith non-LLMs and analyzing the impact of vari-ous LLM configurations, such as parameter count,pre-training sufficiency, and alignment processes.Our study highlights the significant advantages ofutilizing LLMs as backbone encoders for dense re-trieval tasks. We find that increasing the parametercount and ensuring sufficient pre-training of back-bone encoders enhance in-domain accuracy. Addi-tionally, adopting larger models consistently yieldsperformance gains in zero-shot retrieval general-ization, lengthy retrieval generalization, and multi-task learning. These insights provide a foundationfor future research aimed at optimizing dense re-trieval models by balancing model size and pre-training sufficiency of backbone LLMs to achievesuperior performance across diverse retrieval sce-narios.",
  "Limitations": "While our study provides valuable insights into thebenefits and configurations of LLMs as backboneencoders for dense retrieval tasks, several limita-tions should be considered: Firstly, some experi-ments lack comparisons with all other backbonemodels in the same series, such as in data effi-ciency and multitask performance. Secondly, thereare still some capability dimensions of retrievalmodels that havent been examined, such as multi-lingual retrieval and robustness against noisy data.Additionally, certain characteristics of LLMs, suchas whether they use unidirectional or bidirectionalattention mechanisms, and the overlap between pre-training data and downstream retrieval task data,have not been explored. Addressing these aspectsin future studies could provide a more complete,general conclusion.",
  "Ackonwledgements": "We would like to thank all the reviewers for theirhelpful feedback, and EMNLP 2024 and ACLRolling Review organizers for their efforts. Thiswork was supported by Beijing Natural ScienceFoundation (L243006) and CCF-BaiChuan-EbtechFoundation Model Fund. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,Shayne Longpre, Stephen Pulman, and SrinivasChappidi. 2020. Open-domain question answeringgoes conversational via question rewriting. arXivpreprint arXiv:2010.04898.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021. Unsupervised dense in-formation retrieval with contrastive learning. arXivpreprint arXiv:2112.09118. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv:2001.08361. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020.Dense passage retrieval foropen-domain question answering. arXiv preprintarXiv:2004.04906. Tomas Kocisk`y, Jonathan Schwarz, Phil Blunsom, ChrisDyer, Karl Moritz Hermann, Gabor Melis, and Ed-ward Grefenstette. 2018. The narrativeqa readingcomprehension challenge. Transactions of the Asso-ciation for Computational Linguistics, 6:317328. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen,Daniel Cer, Jeremy R Cole, Kai Hui, Michael Bo-ratko, Rajvi Kapadia, Wen Ding, et al. 2024. Gecko:Versatile text embeddings distilled from large lan-guage models. arXiv preprint arXiv:2403.20327.",
  "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016. Ms marco: A human-generated machine read-ing comprehension dataset": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernandez Abrego, Ji Ma, Vincent Y Zhao,Yi Luan, Keith B Hall, Ming-Wei Chang, et al.2021. Large dual encoders are generalizable retriev-ers. arXiv preprint arXiv:2112.07899. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, et al. 2023. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.arXiv preprint arXiv:2307.16789. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah ASmith, Luke Zettlemoyer, and Tao Yu. 2022. Oneembedder, any task: Instruction-finetuned text em-beddings. arXiv preprint arXiv:2212.09741. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology.arXivpreprint arXiv:2403.08295. Nandan Thakur, Nils Reimers, Andreas Ruckle, Ab-hishek Srivastava, and Iryna Gurevych. 2021. Beir:A heterogenous benchmark for zero-shot evalua-tion of information retrieval models. arXiv preprintarXiv:2104.08663. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighoff. 2023. C-pack: Packaged resourcesto advance general chinese embedding": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul Bennett, Junaid Ahmed, and ArnoldOverwijk. 2020. Approximate nearest neighbor neg-ative contrastive learning for dense text retrieval.arXiv preprint arXiv:2007.00808. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D Manning. 2018. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. arXiv preprint arXiv:1809.09600."
}