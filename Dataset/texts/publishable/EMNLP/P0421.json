{
  "Abstract": "Tool learning has generated widespread inter-est as a vital means of interaction betweenLarge Language Models (LLMs) and thephysical world.Current research predomi-nantly emphasizes LLMs capacity to utilizetools in well-structured environments whileoverlooking their stability when confrontedwith the inevitable noise of the real world.To bridge this gap, we introduce RoTBench,a multi-level benchmark for evaluating therobustness of LLMs in tool learning. Specifi-cally, we establish five external environments,each featuring varying levels of noise (i.e.,Clean, Slight, Medium, Heavy, and Union),providing an in-depth analysis of the modelsresilience across three critical phases: toolselection, parameter identification, and contentfilling. Experiments involving six widely-usedmodels underscore the urgent necessity forenhancing the robustness of LLMs in toollearning.For instance, the performance ofGPT-4 even drops significantly from 80.00to 58.10 when there is no substantial changein manual accuracy. More surprisingly, thenoise correction capability inherent in the GPTfamily paradoxically impedes its adaptability inthe face of mild noise. In light of these findings,we propose RoTTuning, a strategy that enrichesthe diversity of training environments to bolsterthe robustness of LLMs in tool learning.The code and data are available at",
  ": Example of noise affecting tool selection forLLMs. Although the functionality of the tool remainsunaffected by its name, renaming Get_Weather asABC impedes LLMs from utilizing the tool properly": "an ensemble of integrated tools.Each tool isuniquely identified by its name and is described bya succinct paragraph that explains its functionality.Similarly, every parameter within these tools ischaracterized by its name, along with a descriptionthat clarifies its purpose, its optionality, and otherpertinent details. Recent research has centered on examining howwell LLMs can effectively employ tools within acarefully designed and stable environment. Fromone perspective, specific studies have scrutinizedthe outcomes of LLMs tool usage, verifying boththe accuracy of tool selection and the efficacy ofthe generated responses (Qin et al., 2023b; Huanget al., 2023). This analysis involved evaluatingthe relevance of the selected tools and the finalresponses in fulfilling users requirements. On theother hand, other investigations have delved intothe intricate process of tool utilization by LLMs,striving for a more comprehensive assessment oftheir performance in tool learning (Chen et al.,2023d; Ye et al., 2024a). This includes an analysis of the diverse capabilities necessary for LLMs toexcel in tool learning while also identifying anylimitations they may have in this regard.However, these studies fail to account for therobustness of LLMs in the face of inevitable noisein real-world scenarios (Chen et al., 2023b; Liuet al., 2023).Using as a reference,LLMs recognize the tool for querying weatherinformation when named Get_Weather, but notwhen named ABC, despite the tools functionalityremaining unaffected by its name. Consequently, itbecomes imperative to investigate whether LLMscan proficiently identify these tools and configureparameters to meet user needs in noisy real-world environments. This research is essential toguarantee their reliability in practical applications.To fill this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustnessof LLMs in tool learning.Specifically, weestablish five external environments, which canbe categorized as Clean, Slight, Medium, Heavy,and Union in ascending order of noise levels.By evaluating the performance of LLMs acrossthree critical stages: tool selection, parameteridentification, and content filling, we aim to offer athorough and intricate analysis of the stability andreliability of LLMs in tool utilization.Through experiments conducted on six widely-used LLMs, we observe that the performanceof these models is remarkably sensitive to noise.For instance, the performance of GPT-4 evendrops significantly from 80.00 to 58.10 whenthere is no substantial change in manual accuracy.This underscores the pressing requirement toenhance the robustness of LLMs in tool learning.Interestingly, the GPT family of models inherentnoise correction capability appears to hinder itsperformance in mildly noisy environments.In light of these findings, we introduce RoTTun-ing, a technique aimed at augmenting the adapt-ability of LLMs to a wide range of environmentsby introducing greater environmental diversityduring the training phase. Our experimental resultsdemonstrate that our approach yields an averageperformance improvement of 16.10 points acrossdiverse environments.The main contributions of our work are summa-rized as follows:",
  "Related Work": "Analysis of Tool LearningGiven their extensiveworld knowledge and superior natural languageunderstanding, researchers have made attemptsto leverage LLMs for a wide range of everydayapplications (Ye et al., 2023). In order to pushthe boundaries of their capabilities, some scholarshave proposed enhancing LLMs with external tools,which has gained widespread acceptance (Schicket al., 2023; Tang et al., 2023).As researchin this area has deepened, certain scholars havesummarized the progress made in tool learningfor LLMs (Mialon et al., 2023; Qin et al., 2023a),sought to uncover developmental insights, andtrained more specialized LLMs for tool learningbased on these findings (Qin et al., 2023b; Zhuanget al., 2023; Hao et al., 2023).Furthermore,recognizing the complexity of tool learning, someresearchers have specialized in evaluating not onlythe outcomes of tool learning (Huang et al., 2023)but also the entire process (Chen et al., 2023d; Yeet al., 2024a). However, its worth noting that allof these current efforts primarily consider LLMstool usage in controlled environments, neglectingthe inherent complexities of real-life scenarios.Therefore, we have undertaken an in-depth analysisof the robustness of LLMs in tool learning toadvance research in a real-world context. Robustness Testing of LLMsRobustness isa critical factor in determining the stability ofLLMs and plays a pivotal role in their practicaldeployment in real-life applications, which hasgarnered significant attention from scholars. In",
  "# Sce# Query# Cat# Subcat# Tool": ": Statistics information of the data. # Sce, #Query, # Cat, # Subcat, and # Tool correspondto the count of scenarios, user queries, tool categories,tool subcategories, and individual tools, respectively. the early stages of research, some scholars con-ducted tests to assess the robustness of ChatGPTacross various natural language processing tasks,highlighting the substantial room for improvementin the current robustness of LLMs (Wang et al.,2023a; Chen et al., 2023c). Subsequently, otherresearchers specialized in creating benchmarks,such as PromptBench (Zhu et al., 2023), to examinethe consistency of LLM responses by introducingnoise into the prompts. Given that tool learningis poised to extend the capabilities of LLMs andits outcomes can directly impact the state of thephysical world (Ye et al., 2024a), it becomesimperative to thoroughly evaluate its robustness.",
  "Data Collection": "In order to thoroughly cater to real-world require-ments and encompass commonly utilized tools, weutilize ToolEyes (Ye et al., 2024a), an evaluationsystem designed for tool learning. This systemdefines seven real-world application scenarios.Within each of these scenarios, we randomlyselect 15 user queries for analysis. Since the rawdata offers tool information without standardizedinvocation paths, we have manually labeled thesepaths to facilitate the evaluation process. Detailedstatistics of the data can be found in .",
  "Environments Construction": "To comprehensively assess the resilience of LLMsin tool learning, we reference the hierarchicalclassification of noise in previous studies (Wanget al., 2021; Zhu et al., 2023; Dong et al., 2023)and design five distinct external environments.These environments feature varying noise levelsthat affect both the tool and its parameters.Clean-level environment employs a runtimeframework developed by ToolEyes. This frame-work furnishes essential information to LLMs forcomprehending tools, where the name of eachtool epitomizes its functionality and the names ofparameters signify their respective meanings. Thisenvironment comprises a total of 105 test cases.The remaining four environments are derivatives of this primary environment, each modified byincorporating distinct levels of noise.Slight-level environment encompasses threetypes of noise: insertion, omission, and substitution.These correspond to real-world occurrences suchas an excess of characters, missing characters, andcharacter errors when naming tools or parameters.Specifically, we introduce noise in the followingways: 1) We randomly select half of the availabletools within the environment. For these selectedtools, a random form of noise is applied, altering upto 1/3 of the characters, resulting in the creation of105 new data points. 2) For each tool, we randomlyselect half of the parameters and introduce noiseinto their names using the method described above,generating an additional 105 new data entries.By combining these two approaches, we createa Slight-level environmental test set consisting of210 test cases.Medium-level environment introduces twotypes of noise: reversal and nonsense.Thesemirror real-world scenarios where names arereversed or replaced with random strings, renderingthe information meaningless. To apply noise, wefollow these procedures: 1) We randomly selecthalf of the available tools. For these tools, thereis a 50% probability that their names will besubstituted with random strings, each containingup to 10 characters.Additionally, there is a50% chance that the names of these tools will bereversed. This process yields 105 test cases. 2)For each tool, half of the parameters are randomlychosen. These parameters may undergo a 50%chance of having their names substituted withrandom strings, each containing up to 5 characters,or a 50% chance of being reversed. This leadsto 105 test cases. It is worth noting that if thereversal process does not alter the name, it will bereplaced with a random string. Consequently, wehave successfully generated 210 test cases for theMedium-level environment.Heavy-level environment encompasses two dis-ruptive types of noise: exchange and addendum,reflecting real-world occurrences of name swap-ping and information supplementation. Noise isintroduced as follows: 1) All tool names within theenvironment are randomly shuffled. This shufflingdisrupts the association between a tools name andits functional description, challenging LLMs toaccurately comprehend the tools function despitethe disorganized name. This process yields 105 testcases. 2) Half of the tools are randomly chosen, and a new mandatory parameter is introducedwith a 50% probability. This parameter is givena name consisting of a random string of up to5 characters.LLMs are tasked with providinga specific string of up to 3 characters for theparameter based on its descriptive meaning. Thenames of these parameters are randomly shuffledwith a 50% probability. For tools with fewer thantwo parameters, noise is introduced by directlyadding new parameters. This process also resultsin 105 test cases.In total, 210 Heavy-levelenvironmental test cases have been generated.Union-level environment encompasses all previ-ously mentioned noise categories. Given that theprior noise environments already include noise forboth tools and parameters, we randomly chooseone noise generation method that impacts toolnames and another method that affects parametersfrom the three previous environment levels. Theseselected methods are simultaneously applied togenerate 105 test cases where both tool names andparameters are subjected to noise injection.",
  "Staged Evaluation": "We evaluate the robustness performance of LLMsat each of stages in tool learning and analyze theirrespective variations.Tool selection marks the initial phase of tool us-age by LLMs. During this process, LLMs identifysuitable tools for addressing the users query byinterpreting the functional descriptions offered bythe external environment and subsequently outputthe names of these tools. It should be emphasizedthat the name of the tool is essentially a label; thepractical deployment of the tool is governed by itsfunctional description. In evaluating a test case, thescore for its tool selection is defined as follows:",
  "sTS = I(t = t)(1)": "Here, I(x) equals 1 if the condition x is true, and0 otherwise. In this context, t represents the toolchosen by the LLMs, while t denotes the tool thatneeds to be selected.Parameter identification involves recognizingthe required parameters and outputting their re-spective names based on their specified needs,following the selection of the appropriate tool.This process necessitates choosing the mandatoryparameters, while the optional ones are selectedbased on actual requirements.Similar to toolselection, the name of the parameter serves as",
  "sPI = sTS I(P = P)(2)": "In this equation, P denotes the set of parametersidentified by LLMs, and P represents the set ofparameters that should be identified.Content filling constitutes the concluding phasein the tool usage process.Once the tool andits corresponding parameters have been selected,LLMs are tasked with breaking down the user-provided information for populating the contentof these parameters.Upon accomplishing thisstep, LLMs formally conclude the entire tool usagecycle, paving the way to receive the tools outputphase and initiate a new interaction. For each testcase, we define a content filling score as follows:",
  "Main Results": "As tool learning involves multiple turns of interac-tion between LLMs and the environment (Qin et al.,2023a; Ye et al., 2024a), with intricate intermediatetrajectories that cannot be easily compared, ouremphasis lies on evaluating the robustness ofvarious LLMs during their initial use of the tooland present the results in .3 The resultingdata reveals intriguing observations.The robustness of current LLMs in toollearning presents considerable scope for en-hancement. While human performance remainsrelatively stable across different environments,the performance of LLMs exhibits significantfluctuations. For instance, when transitioning fromClean-level environment to Union-level, humanperformance in tool selection only decreases by2.86 points, whereas the average performanceof all LLMs decreases by approximately 20.32points.To gain a clearer understanding, weemploy Welchs ANOVA (Bl, 1947) to analyzethe significance of LLMs performance during thecontent-filling stage across various environments.As illustrated in , our findings underscorethe consistency of human performance and thenoteworthy disparities in LLMs performanceacross different environments.Consequently,enhancing the robustness of LLMs in tool learningis an area that requires significant attention.Noise affecting tool names has a more pro-nounced impact on LLM performance thannoise introduced to parameters. We compute the",
  ": The performance of GPT-4 during the contentfilling phase in the first and third rounds of interaction": "absolute difference in average LLMs performancefor each type of noise added to tool namesor parameters, relative to their performance inthe Clean-level environment, respectively. Theresults depicted in show that tool namenoise significantly affects LLMs tool learningperformance throughout the entire process.Incontrast, noise in the parameters has minimalimpact on the robustness of LLMs during thetool selection stage and exerts less influence onsubsequent stages compared to tool name noise.Notably, LLMs exhibit greater robustness in theUnion-level environment than in the Heavy (Tool)environment, underscoring the substantial impactof tool naming on model robustness. OfferingLLMsinteractiveexamplesenhances their tool learning performance, yetit does not bolster their robustness.As toollearning entails multiple turns of interactionbetween LLMs and external environments, weinitially provide the first two turns of interactionsfor the test cases in each environment to evaluateLLMs performance during the third turn ofinteractions. Upon comparing GPT-4s results inthe first and third turns of interactions (), itbecomes evident that the provision of two turnsof interaction examples leads to a consistentperformance boost for GPT-4, resulting in anaverage performance improvement of 22.91 pointsacross various environments.However, whenexamining the performance variation values,it is noteworthy that the standard deviation ofits performance across environments increasedfrom 8.14 in the first turn to 12.56 in the thirdturn.This observation suggests that while itsperformance improves, its robustness does not seea corresponding enhancement.",
  "Why do GPT family of models NOTperform well in Slight-level environment?": "A particularly intriguing finding is that, in contrastto other LLMs, the GPT family of models exhibitsa lower performance in Slight-level environmentcompared to Medium-level, despite the limitedvalidity of the information provided by the latter.Our thorough investigation into the model outputshas revealed that this phenomenon can be attributedto the inherent noise correction capability of theGPT family of models. For instance, when theGPT family of models selects the tool labeled aspredOict_aTge, it automatically corrects the noisewithin it and generates predict_age as the output,consequently leading to an error. 4",
  ":The score in different stages (%) ofRoTLLaMA in various Environments": "Specifically, we instruct GPT-4 to create sevenfresh user queries within the context of a subset oftools, accompanied by three existing user queriesand two model-generated queries.To ensurediversity in our dataset, we scrutinize the newdata for redundancy in relation to each providedexample and eliminate queries with Rouge-Lvalues surpassing 0.55. This process yields a totalof 4,077 new user queries. Trajectory GenerationUpon obtaining high-quality user queries, we employ GPT-4 to producetool learning trajectories. To ensure the accuracyof the generated trajectories, we leverage thespecifically designed function call feature of GPT-4. Simultaneously, we guide GPT-4 in generatingthe associated thought process by incorporatinga system prompt.6 Furthermore, we specify thatGPT-4s tool usage is limited to a maximum of nineturns. By considering each turn of interaction as adistinct data point, this process results in a total of12,247 pieces of training data. Environment AugmentationTo enhance thevariety of environments, we modify the trajectoriesgenerated in the Clean-level environment to alignwith the characteristics of noisy environments.This strategy ensures data quality while addressingthe challenges of working in noisy settings. Tomitigate the potential drawbacks of data coupling,we introduce randomness by augmenting 3000trajectories for each of the Slight-, Medium-,and Heavy-level environments, along with 1500trajectories for Union-level environments. Whencombined with the data from the Clean-levelenvironment, this approach yields a total of22,747 trajectories, representing a diverse rangeof environmental conditions.",
  ": The means and standard deviations of ourmodels performance in the five environments": "interpolation (Chen et al., 2023a) techniqueto extend its context length to 8096. Based onprevious research indicating that fine-tuningwith LoRA (Hu et al., 2022) achieves superiorgeneralizationcomparedtofullparametricfine-tuning (Zeng et al., 2023), we opt for theLoRA fine-tuning approach.We conduct 5epochs of training to derive the ultimate model,RoTLLaMA, which exhibits robust generalizationacross multiple environments.",
  "We carry out a series of experimental analyses withRoTLLaMA on RoTBench to verify its advantageswhen facing various noise environments.7": "PerformanceWe analyze the performance ofRoTLLaMA in various environments, and theresults are presented in . The results revealthat RoTLLaMAs performance stability acrossdifferent environments significantly surpasses thatof GPT-4.Specifically, in the tool selectionphase, the extreme performance difference is only12.38, whereas GPT-4 demonstrates a much higherextreme difference of 21.90. Furthermore, in theparameter recognition and content filling phases,the extreme performance differences are 16.19 and14.76, respectively, both of which are smaller thanGPT-4s corresponding values of 20.95 and 20.95. Ablation StudyTo evaluate the effectiveness ofvarious components within our approach, we con-ducted ablation studies on RoTLLaMA. As shownin , when substituting full-parameter fine-tuning for LoRA fine-tuning (i.e., w/o LoRA),there is a slight decrease in model performance,and standard deviations across environments re-",
  "More experiments can be found in Appendix E": "main largely unchanged.This suggests thatemploying LoRA enhances model performancewithout significantly impacting its robustness.On the other hand, if we omit environmentaugmentation (i.e., w/o Augmentation), there isa notable decrease in both mean performance anda significant increase in standard deviation withineach environment. This underscores the crucial roleof environment augmentation in enhancing bothmodel performance and robustness. Furthermore,exclusively utilizing full-parameter fine-tuning onthe model (i.e., w/o Both) leads to a degradation of16.10 points in model performance.",
  "Conclusion": "In this paper, we introduce RoTBench, a multi-level benchmark for evaluating the robustness ofLLMs in tool learning. RoTBench contains fiveenvironments, each characterized by varying noiselevels, shedding light on the pressing need tobolster the robustness of LLMs. Furthermore, wepresent RoTTuning, an innovative approach thatsignificantly improves the robustness of LLMsin tool learning by increasing the diversity ofenvironments during the training phase.",
  "Limitations": "While we introduce a multi-level benchmarkfor evaluating the robustness of LLMs in toollearning and a training method aimed at increasingenvironmental diversity, our work does have somelimitations. On one hand, our primary focus ison assessing the robustness of LLMs in a singletool-use round, and we do not delve into whetherLLMs are able to self-correct their behavior inresponse to environmental feedback. However, weanalyze the performance of GPT-4 based on theinteraction trajectories in the first two rounds andfind that this does not enhance model robustness.On the other hand, While tool descriptions areundoubtedly crucial for understanding tools, ouranalysis centers on the noise present in tool namesand parameters.This choice is driven by ourdiscovery that LLMs comprehension of toolsprimarily relies on tool and parameter names ratherthan a nuanced understanding of the meaningsconveyed in tool documentation.Within thisframework, evaluating LLMs through RoTBenchcan effectively measure their tolerance to noise inthese additional details, thus propelling researchendeavors aimed at improving LLMs tool learning",
  "Acknowledgements": "The authors wish to thank the anonymous reviewersfor their helpful comments. This work was partiallyfunded by National Natural Science Foundationof China (No.62476061,62206057,62076069),Shanghai Rising-Star Program (23QA1400200),NaturalScienceFoundationofShanghai(23ZR1403500), Program of Shanghai AcademicResearch Leader under grant 22XD1401100. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones,Anna Chen,Anna Goldie,Azalia Mirhoseini,Cameron McKinnon, Carol Chen, Catherine Olsson,Christopher Olah, Danny Hernandez, Dawn Drain,Deep Ganguli, Dustin Li, Eli Tran-Johnson, EthanPerez, Jamie Kerr, Jared Mueller, Jeffrey Ladish,Joshua Landau, Kamal Ndousse, Kamile Lukosiute,Liane Lovitt, Michael Sellitto, Nelson Elhage,Nicholas Schiefer, Noem Mercado, Nova DasSarma,Robert Lasenby, Robin Larson, Sam Ringer, ScottJohnston, Shauna Kravec, Sheer El Showk, StanislavFort, Tamera Lanham, Timothy Telleen-Lawton, TomConerly, Tom Henighan, Tristan Hume, Samuel R.Bowman, Zac Hatfield-Dodds, Ben Mann, DarioAmodei, Nicholas Joseph, Sam McCandlish, TomBrown, and Jared Kaplan. 2022.ConstitutionalAI: harmlessness from AI feedback.CoRR,abs/2212.08073.",
  "Welch Bl. 1947.The generalisation of studentsproblems when several different population variancesare involved. Biometrika, 34(1-2):2835": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. InAdvances in Neural Information Processing Systems33:Annual Conference on Neural InformationProcessing Systems 2020, NeurIPS 2020, December6-12, 2020, virtual.",
  "Xiuying Chen, Guodong Long, Chongyang Tao,Mingzhe Li,Xin Gao,Chengqi Zhang,and": "Xiangliang Zhang. 2023b. Improving the robustnessof summarization systems with dual augmentation.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1:Long Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pages 68466857. Association forComputational Linguistics. Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng,Minlong Peng, Jie Zhou, Tao Gui, Qi Zhang, andXuanjing Huang. 2023c. How robust is GPT-3.5 topredecessors? A comprehensive study on languageunderstanding tasks. CoRR, abs/2303.00293. Zehui Chen, Weihua Du, Wenwei Zhang, KuikunLiu, Jiangning Liu, Miao Zheng, Jingming Zhuo,Songyang Zhang, Dahua Lin, Kai Chen, and FengZhao. 2023d. T-eval: Evaluating the tool utilizationcapability step by step. Guanting Dong, Tingfeng Hui, Zhuoma Gongque,Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He,and Weiran Xu. 2023.Demonsf: A multi-taskdemonstration-based generative framework for noisyslot filling task.In Findings of the Associationfor Computational Linguistics:EMNLP 2023,Singapore, December 6-10, 2023, pages 1050610518. Association for Computational Linguistics.",
  "Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.2023.Toolkengpt: Augmenting frozen languagemodels with massive tools via tool embeddings.CoRR, abs/2305.11554": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, SiyuanWu, Qihui Zhang, Yixin Liu, Pan Zhou, YaoWan, Neil Zhenqiang Gong, and Lichao Sun. 2023.Metatool benchmark for large language models:Deciding whether to use tools and which to use.CoRR, abs/2310.03128. Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang,Yihang Yao, Hanjiang Hu, and Ding Zhao. 2023.Towards robust and safe reinforcement learning withbenign off-policy data. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 2158621610.PMLR. Grgoire Mialon, Roberto Dess, Maria Lomeli, Christo-foros Nalmpantis, Ramakanth Pasunuru, RobertaRaileanu, Baptiste Rozire, Timo Schick, JaneDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, YannLeCun, and Thomas Scialom. 2023. Augmentedlanguage models: a survey. CoRR, abs/2302.07842.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,Huadong Wang, Cheng Qian, Runchu Tian, KunlunZhu, Shihao Liang, Xingyu Shen, Bokai Xu, ZhenZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,Xu Han, Xian Sun, Dahai Li, Jason Phang, ChengYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, andMaosong Sun. 2023a. Tool learning with foundationmodels. CoRR, abs/2304.08354. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, andMaosong Sun. 2023b. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.CoRR, abs/2307.16789. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle,Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, YossiAdi, Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, ManishBhatt, Cristian Canton-Ferrer, Aaron Grattafiori,Wenhan Xiong, Alexandre Dfossez, Jade Copet,Faisal Azhar, Hugo Touvron, Louis Martin, NicolasUsunier, Thomas Scialom, and Gabriel Synnaeve.2023. Code llama: Open foundation models for code.CoRR, abs/2308.12950. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Luke Zettlemoyer, NicolaCancedda, and Thomas Scialom. 2023. Toolformer:Language models can teach themselves to use tools.CoRR, abs/2302.04761.",
  "Nexusflow.ai team. 2023b. Nexusraven-v2: Surpassinggpt-4 for zero-shot function calling": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, BrianFuller, Cynthia Gao, Vedanuj Goswami, NamanGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,Madian Khabsa, Isabel Kloumann, Artem Korenev,Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu,Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, KalyanSaladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, Ranjan Subramanian, Xiaoqing Ellen Tan,Binh Tang, Ross Taylor, Adina Williams, Jian XiangKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, YuchenZhang, Angela Fan, Melanie Kambadur, SharanNarang, Aurlien Rodriguez, Robert Stojnic, SergeyEdunov, and Thomas Scialom. 2023b.Llama 2:Open foundation and fine-tuned chat models. CoRR,abs/2307.09288. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,Runkai Zheng, Yidong Wang, Linyi Yang, HaojunHuang, Wei Ye, Xiubo Geng, Binxing Jiao, YueZhang, and Xing Xie. 2023a.On the robustnessof chatgpt: An adversarial and out-of-distributionperspective. CoRR, abs/2302.12095. Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, YichengZou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, RuiZheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li,Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai,Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan,Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, BolinZhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, MinlongPeng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,Xipeng Qiu, and Xuanjing Huang. 2021. Textflint:Unified multilingual robustness evaluation toolkitfor natural language processing. In Proceedings ofthe Joint Conference of the 59th Annual Meetingof the Association for Computational Linguisticsand the 11th International Joint Conference onNatural Language Processing, ACL 2021 - SystemDemonstrations, Online, August 1-6, 2021, pages347355. Association for Computational Linguistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,Alisa Liu, Noah A. Smith, Daniel Khashabi, andHannaneh Hajishirzi. 2023b. Self-instruct: Aligninglanguage models with self-generated instructions.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), ACL 2023, Toronto, Canada, July9-14, 2023, pages 1348413508. Association forComputational Linguistics.",
  "Gong, Yang Shen, Jie Zhou, Siming Chen, TaoGui, Qi Zhang, and Xuanjing Huang. 2023.Acomprehensive capability analysis of GPT-3 and GPT-3.5 series models. CoRR, abs/2303.10420": "Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang,Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou,Qi Zhang, Tao Gui, and Xuanjing Huang. 2024a.Tooleyes: Fine-grained evaluation for tool learningcapabilities of large language models in real-worldscenarios. CoRR, abs/2401.00741. Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang,Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui,and Xuanjing Huang. 2024b. Toolsword: Unveilingsafety issues of large language models in toollearning across three stages.In Proceedings ofthe 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2024, Bangkok, Thailand, August 11-16, 2024,pages 21812211. Association for ComputationalLinguistics.",
  "Among open-source LLMs, we have chosen fourmodels that have undergone dedicated training fortool learning": "ToolLLaMA-2-7B-v1ToolLLaMA-2-7B-v1, de-veloped by Tsinghua University, is a tool-orientedLLM that harnesses the power of 126,000 datasamples, including more than 16,000 APIs, throughsupervised fine-tuning on LLaMA-2-7B-base. Thisenables ToolLLaMA-2-7B-v1 to effectively utilizevarious tools to meet diverse user requirements. ToolLLaMA-2-7B-v2ToolLLaMA-2-7B-v2 hasundergone fine-tuning from LLaMA-2-7B-base, byassimilating an expansive dataset comprising over120,000 solution paths and annotated chains ofthought. To the best of our knowledge, this modelstands as the most extensively trained tool-orientedLLM, utilizing the largest dataset and the broadestspectrum of tools among all available options. NexusRaven-13B-v1NexusRaven-13B-v1 is atool-oriented model that underwent fine-tuningbased on CodeLLaMA-13B. Distinguishing itselffrom prior models, NexusRaven-13B-v1 employscode nesting to invoke tools, generating the entireinference path simultaneously instead of followinga step-by-step approach. NexusRaven-13B-v2NexusRaven-13B-v2 en-hances the performance of NexusRaven-13B-v1by generating single, nested, and parallel functioncalls in various complex scenarios. Additionally,NexusRaven-13B-v2 can generate inference pathsfor the function calls it creates, thereby improvingoverall generalization.",
  "Among closed-source LLMs, we have opted fortwo of the most representative models from theGPT family": "GPT-3.5-turboGPT-3.5-turbo stands out as themost potent and cost-efficient model within theGPT-3.5 series. Tailored for conversations, it excelsin comprehending and generating natural language.Furthermore, it exhibits strong tool invocationcapabilities. GPT-4GPT-4 represents OpenAIs most robustLLM, surpassing its predecessor in delivering saferand more beneficial responses. Additionally, GPT-4 offers formal support for multimodal inputs andhas an expanded capability to address a broaderspectrum of social requirements.",
  "BExperimental Setup": "InferenceIn accordance with Ye et al. (2024a),we adopt the ReAct (Yao et al., 2023) formatfor inference, employing a consistent prompttemplate for both the ToolLLaMA-2-7B family ofmodels and the GPT family of models. However,as NexusRaven-13B fmaily of models utilizenested functions for output, we adhere to theguidelines outlined on their official website, whichnecessitate the use of a distinct set of template.8 Meanwhile, to evaluate human performance acrossenvironments with different noise levels, we enlistthree university students. Each student receivesidentical tool documentation and task descriptions.Independently, they completes the questions andthe average score derived from their responsesserved as the human performance benchmark. EvaluationWe score the performance of LLMsand Human using the evaluation methods definedin .3. In this system, each data point isscored as 0 or 1 at each stage. This is because,in the context of tool learning, tool calls eithersucceed or fail, and even small errors can causethe entire process to fail.In particular, In thetool selection phase, an error in tool selection canlead to overall failure, independent of parameteraccuracy. In the parameter identification phase,missing necessary parameters or wrong parameterselection can lead to failure.In the contentfilling phase, incorrect content input can lead toundesirable tool execution results.",
  "LLMs may err due to the strict sequential orderin which tools are called (e.g., obtaining parame-ter values for list_properties necessitates priorexecution of search_locations)": "Its notable that the models perception ofenvironmental complexity may diverge fromhuman intentions.For instance, in informationretrieval scenarios, LLMs exhibit inferior aver-age performance in the slight-level environmentcompared to the medium-level and heavy-levelenvironments, primarily due to limitations in noisecorrection capabilities (.3). Regarding the model itself, variations in train-ing methods and data can lead to unexpectedperformances in certain scenarios.For in-stance, ToolLLaMA-7B-v1 demonstrates a per-formance discrepancy between the clean-leveland union-level environments in the applicationmanipulation scenario, scoring 20 and 40, respec-tively.This disparity arises from its ability toperform better when only two tools are availablealongside ask_to_user and finish, whereasGPT4 consistently prompts for API keys even whenunnecessary.",
  "The Number of Tool HallucinationsWe com-pare the number of tool hallucinations for eachLLM in all environments and find that our modelhas significantly fewer hallucinations compared to": "the GPT family of models (). This demon-strates the effectiveness of our method in mitigatinginterference from various sources of noise whileaccurately acquiring environmental information.Its worth noting that the NexusRaven family ofmodels, which relies on CodeLLaMA (Rozireet al., 2023) as a base, also exhibits low toolhallucinations, suggesting that utilizing code-basedapproaches for tool learning is a viable direction. Performance of RoTToolLLaMATo confirmthe robustness of our method for enhancingestablished tool-oriented LLMs, we proceed tofine-tune ToolLLaMA-2-7B using our generatedtrajectories and obtain RoTToolLLaMA. Thecorresponding results presented in illus-trate that our methods fine-tuning significantlyenhances the models tool learning capabilityacross all stages, while also bolstering its overallrobustness. For instance, across the three stages,our method demonstrates performance extremesof 12.33/13.33/9.53 in various environments, com-pared to ToolLLaMA-2-7B-v2s 26.67/16.67/10.95.This further underscores the efficacy of our pro-posed approach.",
  ": The score in different stages (%) of RoTToolLLaMA in various Environments": "SystemYou are an expert in using tools to handle real-time queries from users.First I will give you the task description, and your task start.At each step, your task is to give your thought to analyze the current state, decide the next step, with afunction call to actually execute your step.After the call, you will get the call result, and you are now in a new state.Then you will analyze your status now, then decide what to do next...After many (Thought-call) pairs, you finally perform the task, then you can give your final answer.",
  "Desired format:Thought: The thoughtAction: The tool you decide to useAction Input: The parameters for the tool": "Remember:1. You should ALWAYS think about what to do, but all the thought is short, at most in 3 sentences.2. The action to take should be one of the given tools below.3. The Action Input needs to provide a dict similar to {parameter_1: value_1, parameter_2: value_2} tocall action.4. Always use the finish tool upon task completion. The final answer should be comprehensive enoughfor the user. If the task is unmanageable, use the finish tool and respond with I cannot handle the task.",
  ": The prompt used for NexusRaven-13B-v2, where {Tool Document} represents the tool documentationgiven to LLMs and {Query} represents the query given by the user": "SystemAs an expert, your assignment is to utilize the comprehensive documentation of various tools to developa series of problem scenarios that these tools can resolve. Ideally, each scenario should necessitate thesequential use of multiple tools for its resolution. Remember:1. The tools employed to address a problem should be a subset of the tools detailed in the provideddocumentation; ideally, each problem should require the use of more than one tool.2. The parameter values needed by each tool can either be directly extracted from the query or obtainedby invoking the specified other tool.3. The problem scenario should be expressed in a way that is understandable to humans, while alsoshowcasing the diverse functions of the provided tools and their interrelationships.",
  ": The prompt for query expansion, where {Tool Document} represents the tool documentation given toLLMs and {Examples} represents the examples for LLMs": "SystemYou are an expert in using tools to handle real-time queries from users.At each step, your task is to give your thought to analyze the current state, decide the next step, with afunction call to actually execute your step.After the call, you will get the call result, and you are now in a new state.Then you will analyze your status now, then decide what to do next...After a series of these thought-action pairs, you will complete the task and provide the final answer. Remember:1. You must ALWAYS select a specific function to execute your idea at each step.2. Before calling any function, you should ALWAYS give your thought, but limit it to a maximum of threesentences.3. ALWAYS use the finish tool upon task completion. The final answer should be comprehensiveenough for the user. If the task is unmanageable, use the finish tool and respond with I cannot handlethe task."
}