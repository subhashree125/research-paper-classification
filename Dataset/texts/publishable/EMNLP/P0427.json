{
  "Abstract": "The powerful generative abilities of large lan-guage models (LLMs) show potential in gener-ating relevance labels for search applications.Previous work has found that directly askingabout relevancy, such as How relevant is doc-ument A to query Q?\", results in sub-optimalranking. Instead, the pairwise-ranking prompt-ing (PRP) approach produces promising rank-ing performance through asking about pair-wise comparisons, e.g., Is document A morerelevant than document B to query Q?\". Thus,while LLMs are effective at their ranking abil-ity, this is not reected in their relevance labelgeneration. In this work, we propose a post-processingmethod to consolidate the relevance labels gen-erated by an LLM with its powerful rankingabilities.Our method takes both LLM gen-erated relevance labels and pairwise prefer-ences. The labels are then altered to satisfythe pairwise preferences of the LLM, whilestaying as close to the original values as pos-sible. Our experimental results indicate thatour approach effectively balances label accu-racy and ranking performance. Thereby, ourwork shows it is possible to combine both theranking and labeling abilities of LLMs throughpost-processing.",
  "Introduction": "Generative large language models (LLMs) haveshown signicant potential on question answer-ing and other conversation-based tasks (OpenAI,2023; Google et al., 2023) owing to their extraordi-nary generative abilities and natural language un-derstanding capabilities. Naturally, previous workhas further investigated the application of LLMs toother areas, including search and recommendationtasks (Zhu et al., 2023; Wu et al., 2023). The goalhere is to rank items according to their relevanceto a certain query. Generally, existing approacheshave applied LLMs to this task in two different ways: First, as pseudo-raters, LLMs are asked tosimulate human raters by generating a relevancelabel for each query-document pair (Liang et al.,2022), for example, through prompts such as Howrelevant is document A to query Q?\" Secondly, anLLM can also be asked directly about the order-ing of documents for a query. For example, thepairwise-ranking-prompting (PRP) method (Qinet al., 2023) uses a prompt like Is document Amore relevant than document B to query Q?\" Al-ternatively, LLMs can be asked to generate theentire ranking through a prompt like Rank the fol-lowing documents by their relevance to query Q:document A, document B, document C, etc. (Sunet al., 2023a) Thus, there are several distinct modesby which LLMs can be used for ranking purposes,which provide different kinds of output.Each mode of applying LLMs to ranking tasksoffers distinct advantages in terms of performanceand efciency.The pseudo-rater mode is cur-rently favored in LLM applications within rankingsystems due to its simplicity and high efciency(Liang et al., 2022; Sachan et al., 2022; Thomaset al., 2023; Oosterhuis et al., 2024). Given thehigh costs associated with deploying or trainingLLMs for high-throughput applications like searchand recommendations, it is, so far, only efcientlyfeasible to use LLMs as pseudo-raters to label afraction of raw data in zero-shot or few-shot fash-ion as a replacement of more expensive humanraters. However, the general LLMs are not tunedto generate meaningful ranking scores, as a result,there is still an apparent gap between state of the art(SOTA) ranking performance and the performancereached when leveraging LLM pseudo-labels formodel training (Thomas et al., 2023).In parallel to exploring the costly ne-tuning ofLLMs as ranking specialists (Nogueira et al., 2020;Zhuang et al., 2023b), previous work has also inves-tigated the direct ranking modes of LLMs, whereno netuning is involved. Some of these direct ranking modes, such as PRP (Qin et al., 2023), canreach SOTA ranking performance that is on-parwith LLMs netuned for ranking. Moreover, PRPenables open-source (OSS) LLMs to outperformthe largest commercial models like GPT-4 (Ope-nAI, 2023). However, document scoring by PRPsolely considers the resulting order of the candi-date list, and thus, the absolute values of scores aremeaningless. This makes PRP results unsuitable tobe directly used as pseudo-labels. For example, thePRP ranking score of a fair candidate in the list ofonly poor candidates would be comparable to thatof a good candidate in the list of strong competingcandidates, (see example in ). How to ef-fectively combine these direct ranking modes withthe pseudo-rater mode to consolidate ranking andrelevance predictions of LLMs remains an essen-tial challenge in applying LLMs to real world mainstream applications.In this work, we study post-processing meth-ods to do the consolidation, especially for the casewhen we have no human labelled data. We rst de-ne the problem in LLM ranking in , andpropose our post-processing methods to consoli-date LLM predictions for unlabelled data in Sec-tion 4. We discuss our experiments on public rank-ing datasets in and show our methodscould approach the state of the art ranking perfor-mance with minimal tradeoff in relevance predic-tion performance in . Our contributionsinclude:",
  "Related Work": "The strong capability of LLMs in textual under-standing has motivated numerous studies leverag-ing LLM-based approaches for textual informa-tion retrieval (Bonifacio et al., 2022; Tay et al.,2022b; Jagerman et al., 2023). Before the gen-erative LLM era, the focus was more on netun-ing pre-trained language models (PLMs) such as T5 (Nogueira et al., 2020; Zhuang et al., 2023b) orBERT (Nogueira and Cho, 2019) for the supervisedlearning to rank problem (Liu, 2009; Qin et al.,2021), which becomes less feasible with larger gen-erative LLMs. Two popular methods-relevancegeneration (Liang et al., 2022; Zhuang et al., 2023a)and query generation (Sachan et al., 2022)-aim togenerate per-document relevance scores or retrievalqueries using generative LLMs. These methodsare also termed pointwise approaches for ranking.More recent works (Sun et al., 2023a; Ma et al.,2023; Pradeep et al., 2023; Tang et al., 2023) ex-plore listwise ranking generation approaches bydirectly inserting the query and a list of documentsinto a prompt. Pairwise order generation throughpairwise prompts (Qin et al., 2023) turns out to bevery effective for ranking purposes, especially formoderated-sized LLMs. However, none of theseranking approaches using generative LLMs attemptto consolidate the results with relevance generation.Previous works on non-LLM neural rankers (Yanet al., 2022; Bai et al., 2023) focus on balanc-ing or aligning regression with ranking objec-tives during the model training, which is unfor-tunately not feasible for LLMs using zero-shotor few-shot prompting. Post-processing methodsthat calibrate model predictions using some vali-dation data could be potentially applicable. Orig-inally developed for classication model calibra-tion (Menon et al., 2012), these methods includeparametric approaches like Platt scaling (Platt,2000) for binary classication; piecewise lineartransformation (Ravina et al., 2021) for regres-sion; and non-parametric approaches like isotonicregression (Menon et al., 2012; Zadrozny andElkan, 2002), histogram binning, and Bayesianbinning (Zadrozny and Elkan, 2001; Naeini et al.,2015). But how effectively these post-processingapproaches could be extended to LLM-based rank-ing and relevance predictions has not been wellstudied in existing literature.",
  "Problem Formulation": "We formulate the problem of consolidating rankingand relevance predictions within this framework.Given a set of queries, for each query q, we havea set of corresponding candidate documents {d}q,and their ground truth labels, {y}q, as their rele-vance evaluations, such as graded relevance. Ourrst goal is to predict the relevance labels basedon the content of each corresponding candidate. Our second goal is to predict a ranked list of candi-dates, and we use {r}q to denote the rank of eachcandidate in this predicted ranking. The predictedranking is optimal when the ranks align with theorder of the relevance labels: ri rj if yi yjfor any pair of candidates (di, dj) belonging to thesame query q. Taken together, our overall task isto optimize LLM predictions for both relevanceestimation and ranking performance.",
  "Relevance Prediction": "For this purpose, in this work, we consider real-number predictions, i.e., yi R, as the relevancepseudo-labels for query-document pairs.Suchpointwise real-number ratings can be averages overthe annotations of multiple human raters. For LLM-based raters, pseudo-labels can be obtained fromthe average rating of raters with discrete outputspace (Thomas et al., 2023) or from ner-grainedrating generation (Zhuang et al., 2023a), or directlyleveraging the token probabilities to formulate therelevance predictions if available in the generativeLLMs (Liang et al., 2022).In specic, we use LLM as a rater to generateYes or No to answer the question does the pas-sage answer the query? for each query-documentpair. See Appendix A.1 for the prompt. We obtainthe generation probabilities Pi(Yes), Pi(No) andtake",
  "Ranking Prediction": "In the pairwise ranking prompting (PRP) mode,LLMs generate pairwise preferences: for any twodocuments d1 and d2, LLMs are prompted to gener-ate d1 or d2 to answer the question on whichof the passages is more relevant to the query? SeeAppendix A.2 for the prompt. Based on the resultsand the consistency of results when switching theorder of d1 and d2 in the prompt, we could have d1consistently better (d1 > d2), d2 consistently better(d1 < d2), and inconsistent judgement (d1 = d2),as the LLM generated preferences.To get a consistent ranking from these pairwisepreferences, we follow Qin et al. (2023) to computea ranking score si for each document di by perform-ing a global aggregation on all other candidates ofthe same query,",
  "j=iIdi=dj,(4)": "where Icond is an indicator function of the condi-tion cond: 1 when cond is true and 0 otherwise. siessentially counts number of wins for each docu-ment. We then sort the candidates by their rankingscores {s}q to get predicted ranking {r}q.The ranking performance is evaluated by thenormalized discounted cumulative gain (NDCG)metric:",
  "The Consolidation Problem": "Although the two formulations, relevance and rank-ing predictions, are conceptually aligned to thesame ground-truth labels, different modes aboveare leveraged in practice for different purposes: thepseudo-rater mode of LLMs, directly predictingthe candidate relevance to a query, gives relativelygood relevance estimation y (Liang et al., 2022),while the ranker mode of LLMs, using pairwiseprompting, achieves signicantly better NDCG butwith totally uncalibrated ranking scores s that have",
  "Ranking-aware Pseudo-Rater": ": Left: Example of PRP scores not calibrated over different queries. Right: Illustration of the ranking-aware pseudo-rater pipeline that generates ranking-aware ratings with LLMs from the input query and list ofcandidate documents. poor relevance prediction performance (Qin et al.,2023), or see for an example. How toaddress this dichotomy then is the problem that westudy in this paper.In the optimization problem with multiple ob-jectives like this, optimizing for both relevanceprediction and ranking performance, the success isdifcult to be measured with a single metric. Ad-ditionally, a tradeoff typically exists between thesemetrics (ECE and NDCG in our case) improvingone leading to demoting the other, represented by aPareto front in the gure of both metrics. Please seeexamples in . An improvement against thebaselines is qualied by whether the new methodcould push the Pareto front by positing metrics onthe better side of the current Pareto front.",
  "The Methods": "This section presents our post-processing methodsto consolidate the ranking scores s as well as thepairwise preferences from the LLM ranker modeand the relevance estimation y from the pseudo-rater mode, aiming to optimally balance rankingand relevance prediction performance. To make afair comparison with previous LLM rankers, westick to zero-shot prompting results with no trainingor netuning.Specically, we introduce a constrained regres-sion method to nd minimal perturbations of therelevance predictions y such that the resulting rank-ing matches the the pairwise preference predictionsof PRP. Additionally, we also introduce an ef-cient version of our constrained regression methodthat avoids querying an LLM to construct the com-plete quadratic number of pairwise constraints byselecting a linear-complexity subset of pairwisecomparisons. Finally, with the constrained regres-",
  "Constrained Regression": "The goal of the constrained regression methodsis to adjust the LLM relevance predictions y sothat their order aligns with the ranking order of thePRP results s. By minimizing the perturbations toadjust the predictions, the resulting scores shouldclosely match the original relevance predictionswhile adhering to the PRPs ranking performance.Formally, given a query q, we aim to nd a setof minimal linear modications {}q of the LLMrelevance predictions, so that for a PRP pairwisepreference di > dj or si > sj, the modied pre-dictions match that order: yi + i > yj + j. Ingeneral terms:",
  "fori, j {d}q,": "where ij = si sj, if preference is constructedfrom ranking scores, or ij = Idi>dj Idi<djif direct preference is considered. Thus, the signof ij indicates the pairwise order between i andj, and a lack of preference in ordering results inij = 0. We use {y + } as our nal predictionsfor both ranking and relevance.The mathematical problem posed in Eq. 7 isa well-known constrained regression problem thatcan easily be solved with publicly available existingmath libraries (Virtanen et al., 2020).",
  "stride = 1": ": Illustration of how to select LLM pairwiseconstraints in SlideWin and TopAll methods.Top:SlideWin method with window size 2 and stride 1takes o(kn) successive pair comparisons, illustrated bypaired arrows, to sort for top k results from some ini-tial ranking. Bottom: TopAll method considers top-kresults from an initial ranking and their pairwise con-straints with all other results, shown by o(kn) double-headed arrows. tions, as detailed in Section B. A limitation of theabove method is the need to identify all o(n2) pair-wise constraints through pairwise ranking prompt-ing to calculate ranking scores s in Eq. 4 for a listof size n. As the method only depends on pair-wise constraints given by ij, a simple way toimprove efciency is to reduce the number of pairconstraints to be processed by LLM. Here we introduce two efcient constraintchoices: SlideWin and TopAll, as illustrated in. (1) As the ranking performance focusesmostly on the top results (top 10 or top 20), PRPwork (Qin et al., 2023) proposes to just run a slid-ing window sorting from some initial ranking tond the top-k results with o(kn) pair comparisons.We just reuse these o(kn) pair comparisons as con-straints ij in Eq. 7. We call this variant SlideWin.(2) As our nal predictions rely upon the relevancescores y, we dont need to sort from random. As-suming the initial ranking from initial relevancescores y is close to the nal PRP ranking, we canjust consider pairwise constraints between the can-didates of top relevance predictions and the rest. Inspecic, we consider top-k in the relevance scores",
  "PRaterYesNoo(n)PRPNoYes, allo(n2)AllpairYesYes, allo(n2)SlideWinYesYes, partialo(n)TopAllYesYes, partialo(n)": "y and all other results in the candidate list, or top-kvs. all, where o(kn) pair constraints to be enforced.We call this variant TopAll.In , we summarize the use of LLM-generated relevance predictions y and pairwisepreferences {di > dj} and the method complex-ities in terms of LLM calls of all proposed meth-ods together with the Pseudo-rater and PRP base-lines. More efciency analysis can be found inAppendix B.",
  "Ranking-Aware Pseudo-Rater": "To conclude, we propose a ranking-aware pseudo-rater pipeline that leverages both the rating andranking capabilities of LLMs, as illustrated in Fig-ure 1. For a given query q and a list of candi-date documents {d}q, we formulate pointwise rat-ing and pairwise ranking prompts, then feed theseprompts to the central LLM to obtain initial rat-ings and pairwise preferences, respectively. Wethen combine the initial ratings and pairwise pref-erences using our constrained regression methodsfor consolidation. The output of this pipeline is theranking-aware pseudo labels.",
  "TREC-DL201943{0, 1, 2, 3}{0, 1/3, 2/3, 1}TREC-DL202054{0, 1, 2, 3}{0, 1/3, 2/3, 1}TREC-Covid50{0, 1, 2}{0, 1/2, 1}DBPedia400{0, 1, 2}{0, 1/2, 1}Robust04249{0, 1, 2}{0, 1/2, 1}": "and TREC-DL2020 competitions, as well as thosefrom TREC-Covid, DBPedia, and Robust04 in theBEIR dataset (Thakur et al., 2021). summa-rizes the statistics of queries and the range of labels.The candidate documents are selected from the MSMARCO v1 passage corpus, which contains 8.8million passages. LLM rankers are applied on thetop 100 passages retrieved by BM25 (Lin et al.,2021) for each query, same setting as existing LLMranking works (Sun et al., 2023a; Ma et al., 2023;Qin et al., 2023).",
  "Evaluation Metrics": "For ranking performance, we adopt NDCG (as de-ned in Eq. 5) as the evaluation metric, with highervalues indicating better performance. We primar-ily focus on NDCG@10, but also present NDCGwith other cutoff points in certain ablation studies.For the relevance prediction performance, we usethe mean squared error (MSE) in Eq. 2 and theempirical calibration error (ECE) in Eq. 3 as theevaluation metrics. The lower ECE values indi-cate better relevance predictions. In this work, wechoose M = 10 bins (Naeini et al., 2015) with eachbin containing approximately the same number ofdocuments ( 10 documents per bin).",
  "TopAll (Ours):The constrained regressionmethod with pairwise LLM constraints on thepairs between top k = 10 results by sorting onpseudo-rater predictions y versus all candidatesin the list": "Unless specied, all LLM results in above methodsare based on the FLAN-UL2 model (Tay et al.,2022a), an OSS LLM 1.In addition, motivated by the multi-objective ap-proach to consolidate ranking and relevance pre-dictions in non-LLM rankers (Yan et al., 2022),we also consider a simple weighted ensemble ofPRater predictions y and PRP scores s:",
  "We also compare a post-processing method requir-ing labelled data, specically the piecewise lineartransformation (PWL) introduced in Ravina et al": ": Evaluation of LLM-based ranking methods on both ranking (NDCG@10) and relevance prediction (ECEand MSE) metrics on TREC-DL 2019 and 2020, TREC-Covid, DBPedia, and Robust04. Bold numbers are the bestof all and numbers underlined are the best among proposed methods in each row. Upscript indicate statisticalsignicance with p-value=0.01 of better performance than the baselines, PRater for NDCG@10 and PRP for ECEand MSE.",
  "yMs > sM,": "where {sm, ym}Mm=1 are 2M tting parameters.ym+1 > ym and sm+1 > sm are enforced forany m to reinforce the monotonicity of the trans-formation to effectively scale predictions withoutaffecting the ranking order.We apply PWL to baseline methods PRaterand PRP as a special set of baselines with la-belled data available, named as PRater+PWL andPRP+PWL in the results. Comparing these withsupervised methods allow for a better understand-ing of our proposed unsupervised approaches. Tocompute the post-tting in PWL, we apply four-fold cross-validation to the test set data: we ran-domly divide the test set into four folds by queries,and then t the PWL transformation function onone set and predict on one of the others, repeatedly,to get PWL transformation results for the wholetest set.",
  "Despite its poor ECE, PRP has the best or nearlybest ranking performance in terms of NDCG": "The constrained regression approach can bestleverage the relevance estimations of PRater andthe ranking capability of PRP and reaches com-parable ranking performance in terms of NDCGto PRP, and on par or even better relevance pre-diction in terms of ECE to PRater. Our methods consolidate the ranking from PRPand relevance predictions from PRater effec-tively, evident by that the combined performanceon NDCG and ECE sits well beyond the Paretofronts of simple weighted Ensemble of the two. Our consolidation methods even outperformPRP+PWL, the one with extra data, in ECE on 4out of 5 datasets and while keeping ranking per-formance in NDCG@10 as good on all datasets.This is because supervised methods may notlearn effectively with limited annotations, which 0.640.660.680.700.72 NDCG@10 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 ECE TREC-DL2019 0.650.660.670.680.690.700.71 NDCG@10 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 TREC-DL2020 0.7000.7250.7500.7750.8000.825 NDCG@10 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 ECE TREC-Covid 0.300.350.400.45 NDCG@10 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 DBPedia 0.530.540.550.56 NDCG@10 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 ECE Robust04 PRaterPRPPRater + PRPAllpairSlideWinTopAll : Tradeoff plots on ECE versus NDCG@10 onve ranking datasets. NDCG@10 is higher the betterand ECE is lower the better. Overall better methods areon the top right corner of the plots. Lines correspondto the Pareto fronts of Ensemble of PRater and PRPby tuning the weight w in Eq. 8. Our consolidationmethods in are scattered in the Figure.",
  "is the case for public search datasets given thehigh cost of collecting human annotations": "Finally, efcient constrained regression methodsmay trade off some performance in ranking andregression for the efciency, but they can stilloutperform the baselines of PRater and PRP andweighted ensemble of the two in most of thedatasets. With these main results, we can answer the mainresearch questions. RQ1. Using the constrainedregression methods, we can boost the LLM raterswith the superior ranking capability of PRP rankerswhile keep their relevance predictions nearly un-touched. RQ2. Naive ensemble of LLM pseudo-rater predictions and PRP scores may lead to atradeoff between ranking and relevance predictionperformance. However, we can get over this trade-off with the constrained regression methods.",
  "PRater0.62580.65390.09490.0991PRP0.69850.70690.36980.3690Allpair0.69600.70540.08710.0865SlideWin0.67350.70460.09000.0911TopAll0.67940.70250.10380.0966": "ranking and regression metrics. We studied thesize effect by comparing results of the FLAN-UL2model (20B parameters) with those of the FLAN-T5-XXL 2 model (11B parameters). showsthat our constrained regression methods achievesignicantly better NDCG, and comparable or bet-ter ECE with the FLAN-UL2 model compared tothe FLAN-T5-XXL model. The same size effect isobserved in PRater and PRP as well. This showsour consolidation method scales together with theunderlying LLMs performance.We have also run experiments on the choicesof initial ranking models and choices of parame-ter k for efcient constrained regression methods(SlideWin and TopAll). The results are included inAppendix C.",
  "Conclusion": "In this work, we have studied the problem of consol-idating ranking and relevance predictions of LLMs.We have found that the direct scores from the zero-shot pairwise ranking prompting (PRP) poorly cor-relate with ground truth labels. To leverage the su-perior ranking ability of PRP while aligning closelywith the ground truth labels, we have investigatedpost-processing methods and proposed a class ofconstrained regression methods that combine point-wise ratings from the LLM raters and pairwise con-straints from the PRP rankers to take advantage ofthe two. We have demonstrated with experimentson public ranking datasets that our methods are ef-cient and effective, offering competitive or superiorranking performance compared to the PRP baselineand relevance prediction performance akin to thepointwise LLM rater. Last but not least, we haveproposed a novel framework on how to effectivelyuse generative LLMs to generate ranking-aware rat-ings, foundation for LLM-powered search ranking.",
  "Limitations": "First, our work mainly focused on consolidatingrelevance raters with pairwise LLM rankers dueto their effectiveness, particularly with moderate-sized open-sourced LLMs. Our methods can be ap-plied to listwise ranking results from listwise LLMrankers (Sun et al., 2023b) by decomposing theirranking results into pairwise comparisons. Our re-sults can be found in Appendix D. However, moreeffective methods to consolidate listwise rankers,may exist, which we consider for future work. Sec-ond, our framework assumes reasonable rating andranking performance by LLMs. Although gener-ally supported by advances in LLM research andvalidated across diverse datasets, more advancedadjustments may be required for scenarios whereLLMs perform suboptimally, such as in domainsopaque to the underlying LLMs. Aijun Bai, Rolf Jagerman, Zhen Qin, Le Yan, PratyushKar, Bing-Rong Lin, Xuanhui Wang, Michael Ben-dersky, and Marc Najork. 2023. Regression compat-ible listwise objectives for calibrated ranking with bi-nary relevance. In Proceedings of the 32nd ACM In-ternational Conference on Information and Knowl-edge Management, pages 45024508. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, andRodrigo Nogueira. 2022.InPars:Unsuperviseddataset generation for information retrieval. In Pro-ceedings of the 45th International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 23872392. Google, Rohan Anil, Andrew M. Dai, Orhan Fi-rat, Melvin Johnson, Dmitry Lepikhin, AlexandrePassos, Siamak Shakeri, Emanuel Taropa, PaigeBailey,Zhifeng Chen,Eric Chu,Jonathan H.Clark, Laurent El Shafey, Yanping Huang, KathyMeier-Hellstern, Gaurav Mishra, Erica Moreira,Mark Omernick, Kevin Robinson, Sebastian Ruder,Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,Gustavo Hernandez Abrego, Junwhan Ahn, Ja-cob Austin, Paul Barham, Jan Botha, James Brad-bury, Siddhartha Brahma, Kevin Brooks, MicheleCatasta, Yong Cheng, Colin Cherry, Christopher A.Choquette-Choo, Aakanksha Chowdhery, ClmentCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,Jacob Devlin, Mark Daz, Nan Du, Ethan Dyer, VladFeinberg, Fangxiaoyu Feng, Vlad Fienber, MarkusFreitag, Xavier Garcia, Sebastian Gehrmann, Lu-cas Gonzalez, Guy Gur-Ari, Steven Hand, HadiHashemi, Le Hou, Joshua Howland, Andrea Hu, Jef-frey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-cheriah, Matthew Jagielski, Wenhao Jia, KathleenKenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, MusicLi, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim,Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-cello Maggioni, Aroma Mahendru, Joshua Maynez,Vedant Misra, Maysam Moussalem, Zachary Nado,John Nham, Eric Ni, Andrew Nystrom, AliciaParrish, Marie Pellat, Martin Polacek, Alex Polo-zov, Reiner Pope, Siyuan Qiao, Emily Reif, BryanRichter, Parker Riley, Alex Castro Ros, Aurko Roy,Brennan Saeta, Rajkumar Samuel, Renee Shelby,Ambrose Slone, Daniel Smilkov, David R. So,Daniel Sohn, Simon Tokumine, Dasha Valter, Vi-jay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pi-dong Wang, Zirui Wang, Tao Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, LintingXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, StevenZheng, Ce Zheng, Weikang Zhou, Denny Zhou, SlavPetrov, and Yonghui Wu. 2023. PaLM 2 technicalreport.",
  "Kalervo Jrvelin and Jaana Keklinen. 2002. Cumu-lated gain-based evaluation of IR techniques. ACMTransactions on Information Systems, 20(4):422446": "Percy Liang, Rishi Bommasani, Tony Lee, DimitrisTsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-mar, et al. 2022.Holistic evaluation of languagemodels. arXiv preprint arXiv:2211.09110. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.2021. Pyserini: A Python toolkit for reproducibleinformation retrieval research with sparse and denserepresentations. In Proceedings of the 44th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR2021), pages 23562362.",
  "Rodrigo Nogueira and Kyunghyun Cho. 2019.Pas-sage re-ranking with BERT.arXiv preprintarXiv:1901.04085": "Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, andJimmy Lin. 2020.Document ranking with a pre-trained sequence-to-sequence model.In Findingsof the Association for Computational Linguistics:EMNLP 2020, pages 708718. Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, XuanhuiWang, and Michael Bendersky. 2024. Reliable con-dence intervals for information retrieval evaluationusing generative ai. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery andData Mining, pages 23072317.",
  "Ronak Pradeep, Sahel Sharifymoghaddam, and JimmyLin. 2023. Rankzephyr: Effective and robust zero-shot listwise reranking is a breeze!arXiv preprintarXiv:2312.02724": "Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu,Donald Metzler,Xuanhui Wang,et al. 2023.Large language models are effective text rankerswith pairwise ranking prompting.arXiv preprintarXiv:2306.17563. Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Ku-mar Pasumarthi, Xuanhui Wang, Michael Bender-sky, and Marc Najork. 2021. Are neural rankers stilloutperformed by gradient boosted decision trees? InProceedings of the 9th International Conference onLearning Representations. Walker Ravina, Ethan Sterling, Olexiy Oryeshko,Nathan Bell, Honglei Zhuang, Xuanhui Wang,Yonghui Wu, and Alexander Grushetsky. 2021. Dis-tilling interpretable models into human-readablecode. arXiv preprint arXiv:2101.08393. Devendra Singh Sachan, Mike Lewis, Mandar Joshi,Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, andLuke Zettlemoyer. 2022.Improving passage re-trieval with zero-shot question generation.arXivpreprint arXiv:2204.07496.",
  "Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,Dawei Yin, and Zhaochun Ren. 2023b.Is Chat-GPT good at search?investigating large lan-guage models as re-ranking agent. arXiv preprintarXiv:2304.09542": "Raphael Tang, Xinyu Zhang, Xueguang Ma, JimmyLin, and Ferhan Ture. 2023.Found in the mid-dle: Permutation self-consistency improves listwiseranking in large language models.arXiv preprintarXiv:2310.07712. Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-cia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,Neil Houlsby, and Donald Metzler. 2022a. Unify-ing language learning paradigms.arXiv preprintarXiv:2205.05131. Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni,Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, ZheZhao, Jai Gupta, et al. 2022b. Transformer memoryas a differentiable search index. In Advances in Neu-ral Information Processing Systems. Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot evalua-tion of information retrieval models. In Thirty-fthConference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (Round 2).",
  "Paul Thomas, Seth Spielman, Nick Craswell, andBhaskar Mitra. 2023. Large language models can ac-curately predict searcher preferences. arXiv preprintarXiv:2309.10621": "Pauli Virtanen, Ralf Gommers, Travis E Oliphant, MattHaberland, Tyler Reddy, David Cournapeau, Ev-geni Burovski, Pearu Peterson, Warren Weckesser,Jonathan Bright, et al. 2020. Scipy 1.0: fundamentalalgorithms for scientic computing in python. Na-ture methods, 17(3):261272. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang,Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu,Hengshu Zhu, Qi Liu, et al. 2023.A survey onlarge language models for recommendation. arXivpreprint arXiv:2305.19860. Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky,and Marc Najork. 2022. Scale calibration of deepranking models. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery andData Mining, pages 43004309. Bianca Zadrozny and Charles Elkan. 2001. Learningand making decisions when costs and probabilitiesare both unknown. In Proceedings of the 7th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, page 204213. Bianca Zadrozny and Charles Elkan. 2002. Transform-ing classier scores into accurate multiclass proba-bility estimates.In Proceedings of the 8th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, page 694699. Yutao Zhu, Huaying Yuan, Shuting Wang, JiongnanLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,and Ji-Rong Wen. 2023.Large language modelsfor information retrieval: A survey. arXiv preprintarXiv:2308.07107. Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan,Xuanhui Wang, and Michael Berdersky. 2023a. Be-yond yes and no: Improving zero-shot llm rankersvia scoring ne-grained relevance labels.arXivpreprint arXiv:2310.14122. Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, andMichael Bendersky. 2023b.RankT5: Fine-tuningT5 for text ranking with ranking losses. In Proceed-ings of the 46th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 23082313.",
  "A.3Code and Data Release": "Our experimental results are easily reproducible, using open-sourced LLMs and standard aggregationmethods (win counting, sorting, and sliding window) used in the work. We intend to release pairwisepreference results on all ve datasets from the two open-source LLMs to aid future research. Specically,we will release the data in JSON format, which will include query-document pair information (ids, text,label, retrieval rank and scores), along with the prompts used, the generated texts, and relevance estimationscores.",
  "BComputational Costs": "Our constrained regression methods are based on a traditional algorithm, the extra computation cost isnegligible compared with the LLM calls. Specically, depending on the model and the token lengths ofthe documents, the GPU time for LLM calls to obtain one relevance estimation or one pairwise preferencecould vary, but it is typically on the order of 10 ms to 1 s per LLM call. For PRP, a list of 100 documentswould require at least 100 s of GPU time to obtain all pairwise preferences. The constrained regression,independent of the model or the document length, can be solved (with scipy.optimize.minimize)in about 100 ms on common CPUs for a query of 100 documents.",
  "C.1LLM vs non-LLM raters": "A good relevance rater is important for the constrained regression methods to work. LLM pseudo-rater(PRater) scores are cheaper than the PRP scores, and are directly leveraged in our methods. On the otherhand, BM25 scores are fast ad hoc results for result retrieval and are thus available at ranking stage. Here,we study the effects of replacing the LLM rater (PRater) with non-LLM rater (BM25) as the base rater fory in all constrained regression methods and as the initial ranker to select pairwise constraints in efcientsliding window (SlideWin) and top vs all pairs (TopAll) methods.The results are summarized in . We have the following observations: First, the choice of thebase rater (Base) mainly affects the relevance prediction performance: ECE of results with PRater is",
  "BM25BM250.65240.5712BM25PRater0.69380.0918PRaterBM250.59490.3149PRaterPRater0.70250.0966": "signicantly better than of those with BM25, as the relevance prediction performance of the constrainedregression methods is mainly limited by the base scores y. In contrast, the choice of Base is nearlyinsignicant to the ranking performance in AllPair and SlideWin methods, but affects ranking more inthe TopAll method: TopAll with PRater Base always show better NDCG than TopAll with BM25 Base.Furthermore, the choice of the initial ranker (Init) is almost neutral on regression in terms of ECE, buthas a complex effect on ranking in NDCG in SlideWin and TopAll methods. We note that using PRateras initial ranker in SlideWin leads to slightly worse NDCG than using BM25. This is attributable to thebetter alignment of LLM relevance rater and PRP ranker, so that the pairwise constraints become lessinformative than starting from initial ranking of BM25. On the other hand, using PRater as initial rankerin TopAll leads to better NDCG when PRater is the base rater and worse NDCG when BM25 becomesthe Base. This is attributable to the alignment of initial ranker and base rater to select useful pairwiseconstraints. Based on these results, we recommend to use LLM PRater as the base rater for all constrainedregression methods and use BM25 as the initial ranker for SlideWin while PRater as the initial ranker forTopAll method. : Effects of top k parameters in sliding window (SlideWin) and top vs all pair (TopAll) constrained regres-sion methods on TREC-DL 2020. Bold numbers indicate the best metrics in each column per method.",
  "C.2Choice of parameter k": "We investigate the effect of hyper-parameter k in both SlideWin and TopAll methods. Note that thoughwe have chosen the same character k to represent the parameters, the actual meanings of the parametersare different in the corresponding methods: top k is the number of top results to be sorted in the SlideWin,and k is the number of the top results in the initial ranker to fetch pairwise constraints.In , primarily, we nd the choice of top k affects the ranking performance (NDCGs) only. Inspecic, ignoring numerical uctuations, increasing parameter k of SlideWin monotonically improves NDCG@m till k m. On the other hand, increasing parameter k of TopAll leads to non-monotonicNDCG@m that is optimized approximately around k m. The intuition of the difference betweenSlideWin and TopAll is that (1) the parameter k of SlideWin is the top number after pairwise ordering, sothat top k result orders will always be consistent with PRP results so as NDCG@m, as long as k > m; (2)while the parameter k of TopAll is the number of top results in initial ranker, which is different from thePRP results, so that when k < m, increasing k is likely improving NDCG@m as more top results areincluded, however, when k > m, more intra-top pair constraints become more dominant than top vs restpairs, which may break the order between top k vs rest results and lead to worse NDCG. : Consolidation results of listwise ranking on TREC-DL 2019 and TREC-DL 2020. ListRank methodreranks the top 20 results retrieved from BM25 and the top 20 results from PRater. Allpair method is then appliedto consolidate ListRank and PRater predictions. Bold numbers indicate the best metrics in each row.",
  "DApplying Consolidation Methods to Listwise Ranking": "Our consolidation methods are applicable to the LLM-based listwise ranking. In , we summarizeour results of the consolidation method (Allpair in specic) applied to the ListRank, our reproduction ofthe RankZephyr approach (Pradeep et al., 2023) on the PaLM 2 model (Google et al., 2023).In ListRank, we train an LLM to directly predict the nal ranking order of top 20 retrieved candidates.In specic, we have compared top 20 candidates retrieved with BM25 score (BM25 Top20) and with anLLM PseudoRater (UL2, PRater Top20 in ). As a validation of our reproduction, the NDCG@10of ListRank on BM25 Top20 is comparable to the value in in the RankZephyr paper (Pradeepet al., 2023).The NDCG metrics are measured with the predicted order of the top 20 results. The ECE and MSEmetrics are computed on scaled ranking scores from the predicted ranks ri:",
  "max(0, 21 ri)": "The Allpair columns next to the ListRank columns show our consolidation results with all pairwiseorder constraints of top 20 results from the ListRank predictions. In all consolidation results, the scoresare computed with the PRater scores as initial scores.As shown in , Allpair methods outperform both PRater and ListRank baselines in both rankingand relevance prediction. These results verify the generalizability and efcacy of our proposed method."
}