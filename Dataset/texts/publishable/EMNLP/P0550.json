{
  "Abstract": "Unsupervised multitask pre-training has beenthe critical method behind the recent successof language models (LMs). However, super-vised multitask learning still holds significantpromise, as scaling it in the post-training stagetrends towards better generalization. In thispaper, we explore supervised multitask pre-training by proposing Instruction Pre-Training,a framework that scalably augments massiveraw corpora with instruction-response pairsto pre-train LMs.The instruction-responsepairs are generated by an efficient instruc-tion synthesizer built on open-source mod-els. In our experiments, we synthesize 200Minstruction-response pairs covering 40+ taskcategories to verify the effectiveness of In-struction Pre-Training. In pre-training fromscratch, Instruction Pre-Training not only con-sistently enhances pre-trained base modelsbut also benefits more from further instruc-tion tuning. In continual pre-training, Instruc-tion Pre-Training enables Llama3-8B to becomparable to or even outperform Llama3-70B. Our model, code, and data are availableat",
  "Introduction": "On the path towards general artificial intelligence,multitask learning (Caruana, 1997) emerges asa promising approach. However, scaling super-vised multitask learning to the necessary degreeis very challenging. This motivates GPT-2 (Rad-ford et al., 2019) to explore unsupervised multi-task learning: pre-training on raw corpora throughcausal language modeling, which facilitates scalingup training data. Over time, unsupervised multitasklearning has evolved into the standard approach forpre-training language models (LMs) (Brown et al.,2020; Chowdhery et al., 2023), which is referredto as Vanilla Pre-Training in this paper.",
  "My classmates all go to different clubs. Helen wants to join the Reading Club": ":Comparison between Instruction Pre-Training and Vanilla Pre-Training. Instead of di-rectly pre-training on raw corpora, Instruction Pre-Training augments the corpora with instruction-responsepairs generated by an instruction synthesizer, then pre-trains LMs on the augmented corpora. Ins and Resrepresent instruction and response, respectively. Despite the success of unsupervised approaches,supervised multitask learning still holds signifi-cant promise. Instruction tuning (Wei et al., 2021),which fine-tunes pre-trained models using diversetasks framed through natural language instructions,significantly enhances task generalization (Sanhet al., 2021; Chung et al., 2024), re-emphasizingthe value of supervised multitask learning.In this paper, we introduce Instruction Pre-Training to explore supervised multitask learningfor pre-training. As shown in , instead ofdirectly pre-training on raw corpora, InstructionPre-Training augments each raw text with a set ofinstruction-response pairs1 generated by an instruc-tion synthesizer, and then pre-trains LMs using theaugmented corpora. These pairs are synthesizedbased on the content of massive raw corpora, en-suring high knowledge coverage and correctness.Therefore, we can scale up task synthesis with greatdiversity and quality (Li et al., 2023a).",
  "We use task and instruction-response pair inter-changeably, with the instruction as task input and the responseas task output": "To develop the instruction synthesizer, we con-vert a wide range of existing datasets into our re-quired format: each example consists of a set ofinstruction-response pairs and a piece of raw textthat these pairs condition on. Using this data col-lection, we fine-tune a language model to generateinstruction-response pairs based the correspond-ing raw text.The high diversity of the tuningdata enables the synthesizer to generalize to un-seen data, facilitating the synthesis of instruction-response pairs for raw pre-training corpora. Unlikeexisting works (Li et al., 2023b; Yehudai et al.,2024) using large or closed-source models (Ope-nAI, 2023; Yehudai et al., 2024) to generate syn-thetic data, we build our instruction synthesizerbased on open-source models (typically with 7Bparameters), which is much more cost-effective.This efficiency allows us to further scale up tasksynthesis: augmenting the raw corpora with 200Minstruction-response pairs across more than 40 taskcategories.We conduct experiments in both general pre-training from scratch and domain-adaptive contin-ual pre-training. In pre-training from scratch, our500M model pre-trained on 100B tokens reachesperformance of the 1B model pre-trained on 300Btokens. Moreover, models that have undergone In-struction Pre-Training gain significantly more fromfurther instruction tuning. In continual pre-training,Instruction Pre-Training consistently improves per-formance of Llama3-8B2 on two domains: financeand biomedicine, enabling it to be comparable toor even surpass Llama3-70B.In summary, our contributions include:",
  "Instruction Synthesizer": "Our goal in multitask fine-tuning is to de-velop a general synthesizer capable of generatinginstruction-response pairs for any raw text. There-fore, we evaluate its performance on both seendatasets (listed in Appendix A) and unseen datasets.The unseen datasets include SocialIQA (Sap et al.,2019), TextbookQA (Kembhavi et al., 2017), Wiki-Why (Ho et al., 2022), and FEVER (Thorne et al.,2018), each representing a specific instruction for-mat. Each example in these datasets comprises acontext (raw text) and a set of context-based tasks(instruction-response pairs). Response AccuracyGiven a raw text and a taskinstruction, the instruction synthesizer generates aresponse. We compute the F1 similarity betweenthe generated response and the gold response toevaluate response accuracy. Our instruction syn-thesizer is fine-tuned from the base Mistral-7Bmodel. For comparison, we also present the re-sults of the base model. As shown in , ourfine-tuned synthesizer significantly outperforms",
  "Math Questions": ": Tuning and inference framework of instruction synthesizer. During tuning, the instruction synthesizerlearns to generate instruction-response pairs for a given raw text. The tuning data are curated to be highly diverse,enabling the synthesizer to generalize to unseen data. During inference, we use this tuned instruction synthesizer togenerate instruction-response pairs for raw texts from pre-training corpora.",
  "LM Pre-Training": "Aftercollectingthesynthesizedinstruction-response pairs, we employ templates from Longpreet al. (2023) to diversify instruction formats, andtemplates from Cheng et al. (2023) to concatenateeach raw text with its instruction-response pairs.As shown in , by concatenating the textsand instruction-pairs from M rounds, we create anM-shot example for subsequent pre-training.Except for the pre-training data, Instruction Pre-Training keeps all other pre-training settings thesame as Vanilla Pre-Training: training with thenext-token prediction objective (Radford et al.,2018) and computing loss on all tokens. We con-duct both general pre-training from scratch anddomain-adaptive continued pre-training to verifythe effectiveness in different pre-training scenarios.",
  "pora, leaving the rest unchanged. Besides, we mixthe corpora with the data for fine-tuning the instruc-tion synthesizer to enhance task diversity": "Domain-Adaptive Continual Pre-TrainingFordomain-adaptive continual pre-training, the data re-quirement is much smaller. Therefore, we convertall raw corpora into instruction-augmented corpora.Following Cheng et al. (2023), we mix the cor-pora with the general instructions to benefit fromimproved prompting ability. Since the general in-structions collection contains the fine-tuning datafor the instruction synthesizer, we do not includethese fine-tuning data.",
  "General Pre-training From Scratch": "Pre-Training CorporaWe randomly sample asubset of RefinedWeb (Penedo et al., 2023) datasetfor raw pre-training corpora, consisting of 200Mpieces of text containing about 100B tokens.To create instruction-augmented corpora, weconduct two rounds of instruction synthesis, con-verting 1/5 of the raw corpora (40M raw texts) intoinstruction-augmented texts. The first round con-verts 20M raw texts, and the second round uses theraw texts and instruction-response pairs from thefirst round to convert another 20M raw texts. Theresulted corpora contain 200M synthesized pairsamounting to about 10B tokens. An example ofa 2-shot instruction-augmented text is shown in in Appendix.We then mix the fine-tuning data for instruc-tion synthesizer. Since the fine-tuning data amount(0.2B tokens) is too small compared to that of theraw corpora, we increase its sample ratio so that itrepeats 4 times throughout pre-training. Training and EvaluationWe adopt the architec-ture and tokenizer of Mistral (Jiang et al., 2023)to implement models of two different parameters:500M and 1.3B.Our pre-training settings largely follow Brownet al. (2020). To enhance training efficiency, weimplement the memory-efficient attention of xform-ers (Lefaudeux et al., 2022). Detailed hyperpa-rameters are listed in in Appendix. Thelm-evaluation-harness framework (Gao et al., 2023)is used for model evaluation, detailed evaluationsettings are in Appendix C.We also conduct instruction tuning on the pre-trained model with 500M parameters using the datafrom Longpre et al. (2023). The instruction-tunedmodels are evaluated on MMLU (Hendrycks et al.,2020) benchmark.",
  "Domain-Adaptive Continual Pre-Training": "Pre-Training CorporaWe use raw corpora fromtwo domains: PubMed Abstracts (Gao et al., 2020)for biomedicine and financial news (Yang et al.,2023) for finance.We conduct 3-round inference to covert all thedomain-specific corpora. Each round processes 1/3of the raw corpora, inheriting the raw texts andinstruction-response pairs from previous rounds.Examples of the instruction-augmented texts are in and 18 in Appendix.We then mix the instruction-augmented corporawith general instructions (Zhou et al., 2024; Xuet al., 2023; Lian et al., 2023), using the same mix-ing ratio as Cheng et al. (2023). Training and EvaluationWe continue to pre-train Llama3-8B on each domain respectively, de-tailed settings are in in Appendix. We fol-low the prompting settings in Cheng et al. (2023)to evaluate models on the domain-specific tasks.Detailed evaluation settings are in Appendix C.",
  "General Pre-Training From Scratch": "Pre-Trained Base Models presents thegeneral performance of the models after pre-training. To ensure a fair comparison with VanillaPre-Training, which uses only raw corpora, weinclude a baseline (Mix PT) that mixes the rawcorpora with the fine-tuning data for our in-struction synthesizer. Compared to Vanilla Pre-Training (Vanilla PT), incorporating the fine-tuningdata in Mix PT improves model performance on",
  ": Comparison between our pre-trained basemodels and others on general benchmarks. Detailedresults are in": "several benchmarks. By further transforming theraw corpora into instruction-augmented corpora, In-struction Pre-Training (Instruct PT) achieves evenbetter performance. Note that none of the evaluateddatasets are included in our fine-tuning data for theinstruction synthesizer. Nevertheless, the modelpre-trained on the data generated by the instructionsynthesizer shows improved performance on theseunseen datasets, demonstrating the effectiveness ofour method in enhancing model generalization.In , we compare our pre-trained modelswith other open-source models. Using 100B to-kens, our 500M model reaches the performanceof Pythia-1B (Biderman et al., 2023) trained with300B tokens and our 1.3 B model reaches the per-formance of BLOOM-3B (Workshop et al., 2022)trained with 341B tokens. This shows consistentdata efficiency of Instruction Pre-Training acrossdifferent model scales.",
  ":MMLU performance during instruc-tion tuning of models pre-trained via Vanilla Pre-Training (Vanilla PT) and Instruction Pre-Training (In-struct PT)": "Training quickly outperforms the model pre-trainedvia Vanilla Pre-Training, and we observe a stableincreasing trend of our model throughout the in-struction tuning process. We infer that the closeralignment of training tasks during the instructionpre-training and instruction tuning stages facili-tates a smoother transition between pre-trainingand fine-tuning. This alignment enables the modelto learn more rapidly on downstream tasks. There-fore, Instruction Pre-Training offers a promisingsolution to significantly reduce the number of fur-ther fine-tuning steps (Longpre et al., 2023; Jianget al., 2024c).",
  "Llama3-8B49.981.183.363.572.870.1Vanilla PT-8B62.984.782.265.464.972.0Instruct PT-8B74.687.182.465.763.674.7": ": Domain-specific task performance of Llama3-8B without continued pre-training, after continued pre-training via Vanilla Pre-Training (Vanilla PT), and after continued pre-training via Instruction Pre-Training (InstructPT). Both Vanilla PT and Instruct PT mix domain-specific corpora with general instructions to boost promptingability, and use the same number of tokens for model training. The performance of Llama3-70B is displayed forreference.",
  "Med.58.658.858.561.3Fin.73.373.173.174.7": ": Ablations on training data. w/o Corporaremoves domain-specific pre-training corpora. Rule-based replaces instruction-augmented corpora withthose created by the rule-based methods in Cheng et al.(2023). 1-shot replaces instruction-augmented corporawith those created through single-turn synthesis. Wereport the average task scores within each domain. tasks.Continual pre-training with InstructionPre-Training significantly enhances the domain-specific performance of Llama3-8B, achieving par-ity with or even surpassing Llama3-70B. On thefinance NER benchmark, where Instruction Pre-Training underperforms Vanilla Pre-Training, weobserve considerable variance, where even Llama3-70B underperforms Llama3-8B, suggesting thatthis benchmark may not be reliable. Ablations presents ablation results forour pre-training data, which consist of a mixtureof domain-specific instruction-augmented corporaand general instructions. w/o Corpora: Removing the domain-specificinstruction-augmented corpora eliminates thesource of domain-specific knowledge, leadingto reduced domain-specific performance.",
  "the base model on both seen and unseen datasets,demonstrating the effectiveness of our fine-tuning": "Instruction-Response Pair QualityGiven a rawtext, the instruction synthesizer generates a set ofinstruction-response pairs. We compute the F1 sim-ilarity between the generated pairs and the goldpairs to evaluate their quality. The evaluation isconducted in both zero-shot and few-shot settings:1) Zero-shot: the input to the instruction synthesizercontains only the raw text. 2) Few-shot: follow-ing Wang et al. (2023); Yehudai et al. (2024), afew examples from the same dataset as the goldinstruction-response pairs, each consisting of a rawtext and corresponding instruction-response pairs,are prepended to the testing raw text.As shown in , compared to the basemodel, our fine-tuned synthesizer significantly out-performs the baseline across all four dimensions:zero-shot, few-shot, seen, and unseen datasets. Inunseen datasets, the few-shot setting substantiallyoutperforms the zero-shot setting, indicating thatour synthesizer effectively leverages the patternof the few-shot examples to create instruction-response pairs for the testing text. Helpfulness on LM GeneralizationWe con-duct experiments using an LM (base Mistral-7B inour analysis) to assess the impact of synthesizedinstruction-response pairs on helping LMs general-ize to unseen tasks. Given a prompt concatenatinga testing raw text, synthesized pairs, and a test-ing instruction, the LM generates a response. Wethen compare the LMs performance on the testingtask with and without the synthesized pairs in theprompt to evaluate their effectiveness.We evaluate instruction-response pairs gener-ated using different methods: 1) Random: ran-domly sampled instruction-response pairs of a dif-ferent context. 2) Base: pairs synthesized based on",
  ": Helpfulness on LM generalization mea-sured by LM performance with or without synthesizedinstruction-response pairs in the prompt": "the testing raw text by the base Mistral-7B modelprompted with a few examples. 3) Ours: pairssynthesized based on the testing raw text by ourinstruction synthesizer using the same few-shot ex-amples as Base.As shown , w/o Pairs denotes the set-ting where synthesized pairs are excluded from theprompt. On both seen and unseen datasets, oursconsistently enhances the LMs performance on thetesting task, surpassing all baselines. This demon-strates the effectiveness of our synthesized tasksin improving the LMs ability to perform a widerange of tasks.",
  "Instruction-Augmented Corpora": "We analyze the instruction-augmented pre-trainingcorpora in terms of context relevance, responseaccuracy and task diversity.We sample 500instruction-augmented texts from the augmentedcorpora and use GPT-4 (OpenAI, 2023) to evaluatethe synthesized instruction-response pairs. Specif-ically, GPT-4 is prompted to assess whether thesynthesized instruction is relevant to the context ofthe raw text (context relevance) and whether theresponse is accurate based on the instruction andcontext (response accuracy). Additionally, to eval-uate task diversity, we prompt GPT-4 to categorizeeach instruction-response pair using a predefinedlist of task categories from Wang et al. (2022).As shown in , our instruction synthesizergenerates instruction-response pairs spanning 49different task categories, with over 85% relevanceto the context and 70% response accuracy. Wefurther group the task categories into 9 generaltask scenarios. shows the percentages of",
  ": Distribution of task scenarios of synthe-sized instruction-response pairs in the instruction-augmented corpora": "each task scenario in the instruction augmented cor-pora for general pre-training. Our synthesized taskscover all general task scenarios, demonstrating theeffectiveness of our instruction synthesizer in gen-erating a highly diverse tasks. We conduct furtheranalysis in Appendix E for human evaluation, Ap-pendix D for data contamination, and Appendix Ffor domain distribution and diversity.",
  "Related Work": "Synthetic Instruction GenerationThere havebeen many works studying synthetic instruc-tion generation, but they mainly focus on post-training (Xu et al., 2023; Li et al., 2023a), whilewe focus on pre-training. This makes these workscomplementary to ours. Moreover, our experimentsdemonstrate that instruction pre-trained modelsgain more from instruction post-training, highlight-ing the complementary nature.Regardless of the training stage, our method dif-fers from related works in several ways. Firstly, wefocus on learning from the raw corpora rather thandistilling knowledge from strong models (Xu et al.,2023; Mukherjee et al., 2023; Li et al., 2024). Sec-ondly, ours can be task-agnostic, in contrast to the more task-specific approaches (Wang et al., 2023;Honovich et al., 2023; Yehudai et al., 2024) relyingon a few gold examples. Additionally, we outper-forms rule-based methods (Cheng et al., 2023; Guet al., 2022b) by increasing instruction diversity.Moreover, the iterative techniques used in Li et al.(2023a); Lee et al. (2024); Yue et al. (2024) couldpotentially complement our method, areas we planto explore in future research. Data Curation for LM Pre-TrainingData cura-tion for LM pre-training typically involves collec-tion, cleaning, and organization. Most pre-trainingdata are collected from the Internet to ensure di-versity (Raffel et al., 2020; Penedo et al., 2023;Wenzek et al., 2020; Gao et al., 2020). Although di-verse, web-scraped data often contain low-qualityand duplicate content. Therefore, data cleaningtechniques are applied to these corpora, includ-ing language identification (Joulin et al., 2016),perplexity-based (Wenzek et al., 2020), classifier-based (Brown et al., 2020), and rule-based (Raffelet al., 2020; Rae et al., 2021) filtering. Data orga-nization aims at performing more fine-grained pro-gramming of the data, including data selection (Al-balak et al., 2024; Xie et al., 2024) and constructingtraining instances related to downstream usage (Guet al., 2022a, 2023; Shi et al., 2023; Jiang et al.,2024b; Maini et al., 2024). Our work explores anorthogonal direction: augmenting raw corpora withlarge-scale supervised signals.",
  "Conclusion": "This paper proposes Instruction Pre-Training to ex-plore supervised multitask learning for pre-training.Instead of directly pre-training on raw corpora, In-struction Pre-Training augments the corpora withinstruction-response pairs generated by an instruc-tion synthesizer. Our instruction synthesizer, fine-tuned from a highly diverse data collection, is ca-pable of generating diverse instruction-responsepairs from various corpora. In pre-training fromscratch, Instruction Pre-Training not only outper-forms Vanilla Pre-Training on the pre-trained basemodels but also benefits more from further in-struction tuning. In continual pre-training, Instruc-tion Pre-Training substantially enhances the per-formance of Llama3-8B in two different domains.Looking ahead, we hope our work can inspire fur-ther exploration into this promising area of super-vised multitask pre-training, effectively enhancingthe general abilities of LMs.",
  "Limitations": "While synthetic data offer numerous benefits, itis crucial to acknowledge the potential limitations.Our work, along with other works utilizing syn-thetic data (Liu et al., 2024), is inevitably lim-ited by the possibility of introducing hallucina-tions. As shown in our analysis in , theaccuracy of our instruction-augmented corpora isapproximately 70%, which may potentially mis-lead the pre-trained model. Future work couldexplore post-verification techniques such as thoseproposed by Li et al. (2023a); Lee et al. (2024);Yue et al. (2024); Yehudai et al. (2024) to filter outlow-quality data or develop methods to enhancethe reliability of the instruction synthesizer.Furthermore, works like Touvron et al. (2023); Jiang et al. (2023) have achieved impressive per-formance by pre-training on trillions of tokens,whereas our pre-training is currently limited to thescale of billions of tokens. Future research shouldinvestigate scaling laws for synthetic data and de-termine the optimal balance between quantity andquality of synthetic samples (Liu et al., 2024).",
  "This work was supported by the National ScienceFoundation for Distinguished Young Scholars (withNo. 62125604) and the NSFC key project (withNo. 61936010)": "Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-man, Harm de Vries, and Siva Reddy. 2022. Topi-ocqa: Open-domain conversational question answer-ing with topic switching. Transactions of the Associ-ation for Computational Linguistics, 10:468483. Shourya Aggarwal, Divyanshu Mandowara, VishwajeetAgrawal, Dinesh Khandelwal, Parag Singla, and Di-nesh Garg. 2021. Explanations for commonsenseqa:New dataset and models. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 30503065.",
  "Muennighoff, Bairu Hou, Liangming Pan, HaewonJeong, et al. 2024. A survey on data selection forlanguage models. arXiv preprint arXiv:2402.16827": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Mrouane Debbah, tienne Goffinet, Daniel Hess-low, Julien Launay, Quentin Malartic, et al. 2023.The falcon series of open language models. arXivpreprint arXiv:2311.16867. Julio Cesar Salinas Alvarado, Karin Verspoor, and Timo-thy Baldwin. 2015. Domain adaption of named entityrecognition to support credit risk assessment. In Pro-ceedings of the Australasian Language TechnologyAssociation Workshop 2015, pages 8490.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling.In InternationalConference on Machine Learning, pages 23972430.PMLR. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Mi-lan Deriu, Mark Cieliebak, and Eneko Agirre. 2020.Doqa-accessing domain-specific faqs via conversa-tional qa. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 73027314.",
  "Rich Caruana. 1997.Multitask learning.Machinelearning, 28:4175": "Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang,Ce Zhang, Frederic Sala, and Christopher R. 2024.Skill-it! a data-driven skills framework for under-standing and training language models. Advances inNeural Information Processing Systems, 36. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma,Sameena Shah, and William Yang Wang. 2022. Con-vfinqa: Exploring the chain of numerical reasoningin conversational finance question answering. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 62796292.",
  "Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023.Adapting large language models via reading compre-hension. In The Twelfth International Conference onLearning Representations": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-moyer. 2018. Quac: Question answering in context.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages21742184. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. In Proceedingsof the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long andShort Papers), pages 29242936. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,Noah A Smith, and Matt Gardner. 2021. A datasetof information-seeking questions and answers an-chored in research papers. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 45994610. Franck Dernoncourt and Ji Young Lee. 2017. Pubmed200k rct: a dataset for sequential sentence classi-fication in medical abstracts.In Proceedings ofthe Eighth International Joint Conference on Nat-ural Language Processing (Volume 2: Short Papers),pages 308313. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,",
  "Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation": "Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023.Pre-training to learn in context. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages48494870. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.2022a. Ppt: Pre-trained prompt tuning for few-shotlearning. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 84108423. Yuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang.2022b. Learning instructions with unlabeled data forzero-shot cross-task generalization. In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, pages 16171634. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations. Matthew Ho, Aditya Sharma, Justin Chang, MichaelSaxon, Sharon Levy, Yujie Lu, and William YangWang. 2022. Wikiwhy: Answering and explainingcause-and-effect questions. In The Eleventh Interna-tional Conference on Learning Representations. Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. 2023. Unnatural instructions: Tuning lan-guage models with (almost) no human labor. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1440914428. Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, andYejin Choi. 2019. Cosmos qa: Machine reading com-prehension with contextual commonsense reasoning.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 23912401. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024a.Mixtral of experts. arXiv preprint arXiv:2401.04088.",
  "extended-text reading comprehension. arXiv preprintarXiv:2401.07284": "Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Ro-driguez, Chunting Zhou, Graham Neubig, Xi Vic-toria Lin, Wen-tau Yih, and Srinivasan Iyer. 2024c.Instruction-tuned language models are better knowl-edge learners. arXiv preprint arXiv:2402.12847. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,Hanyi Fang, and Peter Szolovits. 2021. What diseasedoes this patient have? a large-scale open domainquestion answering dataset from medical exams. Ap-plied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, WilliamCohen, and Xinghua Lu. 2019. Pubmedqa: A datasetfor biomedical research question answering. In Pro-ceedings of the 2019 Conference on Empirical Meth-ods in Natural Language Processing and the 9th In-ternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 25672577. Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611.",
  "Armand Joulin, Edouard Grave, Piotr Bojanowski,Matthijs Douze, Hrve Jgou, and Tomas Mikolov.2016. Fasttext. zip: Compressing text classificationmodels. arXiv preprint arXiv:1612.03651": "Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,Jonghyun Choi, Ali Farhadi, and Hannaneh Ha-jishirzi. 2017. Are you smarter than a sixth grader?textbook question answering for multimodal machinecomprehension. In Proceedings of the IEEE Confer-ence on Computer Vision and Pattern recognition,pages 49995007. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,Shyam Upadhyay, and Dan Roth. 2018. Lookingbeyond the surface: A challenge set for reading com-prehension over multiple sentences. In Proceedingsof the 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Pa-pers), pages 252262. Tushar Khot, Peter Clark, Michal Guerquin, PeterJansen, and Ashish Sabharwal. 2020.Qasc: Adataset for question answering via sentence compo-sition. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 34, pages 80828090. Tom Kocisk`y, Jonathan Schwarz, Phil Blunsom, ChrisDyer, Karl Moritz Hermann, Gbor Melis, and Ed-ward Grefenstette. 2018. The narrativeqa readingcomprehension challenge. Transactions of the Asso-ciation for Computational Linguistics, 6:317328.",
  ". Chemprot-3.0: a global chemical biology dis-eases mapping. Database, 2016:bav123": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,and Eduard Hovy. 2017. Race: Large-scale read-ing comprehension dataset from examinations. InProceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing, pages 785794. Matthew Lamm, Jennimaria Palomaki, Chris Alberti,Daniel Andor, Eunsol Choi, Livio Baldini Soares,and Michael Collins. 2021.Qed: A frameworkand dataset for explanations in question answering.Transactions of the Association for computationalLinguistics, 9:790806. Nicholas Lee, Thanakul Wattanawong, Sehoon Kim,Karttikeya Mangalam, Sheng Shen, Gopala Anu-manchipali, Michael W Mahoney, Kurt Keutzer, andAmir Gholami. 2024. Llm2llm: Boosting llms withnovel iterative data enhancement. arXiv preprintarXiv:2403.15042. BenjaminLefaudeux,FranciscoMassa,DianaLiskovich,Wenhan Xiong,Vittorio Caggiano,Sean Naren, Min Xu, Jieru Hu, Marta Tintore,Susan Zhang, Patrick Labatut, Daniel Haziza,Luca Wehrstedt, Jeremy Reizenstein, and Grig-ory Sizov. 2022.xformers:A modular andhackable transformer modelling library. Haoran Li, Qingxiu Dong, Zhengyang Tang, ChaojunWang, Xingxing Zhang, Haoyang Huang, ShaohanHuang, Xiaolong Huang, Zeqiang Huang, DongdongZhang, et al. 2024. Synthetic data (almost) fromscratch: Generalized instruction tuning for languagemodels. arXiv preprint arXiv:2402.13064. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, OmerLevy, Luke Zettlemoyer, Jason E Weston, and MikeLewis. 2023a. Self-alignment with instruction back-translation. In The Twelfth International Conferenceon Learning Representations.",
  "Yichan Liang, Jianheng Li, and Jian Yin. 2019. A newmulti-choice reading comprehension dataset for cur-riculum learning. In Asian Conference on MachineLearning, pages 742757. PMLR": "Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-ner. 2019. Reasoning over paragraph effects in sit-uations. In Proceedings of the 2nd Workshop onMachine Reading for Question Answering, pages 5862. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion: Learning to solve and explain algebraic wordproblems. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 158167. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,Yile Wang, and Yue Zhang. 2021.Logiqa:achallenge dataset for machine reading comprehen-sion with logical reasoning. In Proceedings of theTwenty-Ninth International Conference on Interna-tional Joint Conferences on Artificial Intelligence,pages 36223628. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, YanzheZhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, DiyiYang, Denny Zhou, et al. 2024. Best practices andlessons learned on synthetic data for language models.arXiv preprint arXiv:2404.07503. Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The flancollection: Designing data and methods for effectiveinstruction tuning. In International Conference onMachine Learning, pages 2263122648. PMLR. Macedo Maia, Siegfried Handschuh, Andr Freitas,Brian Davis, Ross McDermott, Manel Zarrouk, andAlexandra Balahur. 2018. Www18 open challenge:financial opinion mining and question answering. InCompanion proceedings of the the web conference2018, pages 19411942. Pratyush Maini, Skyler Seto, He Bai, David Grangier,Yizhe Zhang, and Navdeep Jaitly. 2024. Rephrasingthe web: A recipe for compute and data-efficient lan-guage modeling. arXiv preprint arXiv:2401.16380. Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-lenius, and Pyry Takala. 2014. Good debt or baddebt: Detecting semantic orientations in economictexts. Journal of the Association for InformationScience and Technology, 65(4):782796. Clara H McCreery, Namit Katariya, Anitha Kannan,Manish Chablani, and Xavier Amatriain. 2020. Ef-fective transfer learning for identifying similar ques-tions: matching user questions to covid-19 faqs. InProceedings of the 26th ACM SIGKDD internationalconference on knowledge discovery & data mining,pages 34583465. Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question an-swering. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 23812391. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-naneh Hajishirzi. 2022. Metaicl: Learning to learnin context. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 27912809. Roshanak Mirzaee, Hossein Rajaby Faghihi, QiangNing, and Parisa Kordjamshidi. 2021. Spartqa: Atextual question answering benchmark for spatial rea-soning. In Proceedings of the 2021 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 45824598.",
  "OpenAI. 2023. Gpt-4 technical report. arXiv preprintarXiv:2303.08774": "Panupong Pasupat and Percy Liang. 2015. Composi-tional semantic parsing on semi-structured tables. InProceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 14701480. Guilherme Penedo, Quentin Malartic, Daniel Hesslow,Ruxandra Cojocaru, Alessandro Cappelli, HamzaAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,and Julien Launay. 2023. The refinedweb datasetfor falcon llm: outperforming curated corpora withweb data, and web data only.arXiv preprintarXiv:2306.01116.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, KatieMillican, Jordan Hoffmann, Francis Song, JohnAslanides, Sarah Henderson, Roman Ring, Susan-nah Young, et al. 2021. Scaling language models:Methods, analysis & insights from training gopher.arXiv preprint arXiv:2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. Squad: 100,000+ questions formachine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99106": "Victor Sanh, Albert Webson, Colin Raffel, StephenBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,et al. 2021. Multitask prompted training enableszero-shot task generalization. In International Con-ference on Learning Representations. Maarten Sap, Hannah Rashkin, Derek Chen, RonanLe Bras, and Yejin Choi. 2019. Social iqa: Com-monsense reasoning about social interactions. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 44634473. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou,Margaret Li, Xi Victoria Lin, Noah A Smith, LukeZettlemoyer, Wen-tau Yih, and Mike Lewis. 2023.In-context pretraining: Language modeling beyonddocument boundaries. In The Twelfth InternationalConference on Learning Representations.",
  "Proceedings of the 2021 Future of Information andCommunication Conference (FICC), Volume 2, pages589601. Springer": "Amir Soleimani, Christof Monz, and Marcel Worring.2021. Nlquad: A non-factoid long question answer-ing data set. In Proceedings of the 16th Conferenceof the European Chapter of the Association for Com-putational Linguistics: Main Volume, pages 12451255. Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,and Claire Cardie. 2019. Dream: A challenge dataset and models for dialogue-based reading compre-hension. Transactions of the Association for Compu-tational Linguistics, 7:217231. Oyvind Tafjord, Matt Gardner, Kevin Lin, and PeterClark. 2019. Quartz: An open-domain dataset ofqualitative relationship questions. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 59415946. JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal.2018.Fever: a large-scale dataset for fact extraction andverification. In Proceedings of the 2018 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long Papers), pages809819. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris,Alessandro Sordoni, Philip Bachman, and KaheerSuleman. 2017. Newsqa: A machine comprehensiondataset.In Proceedings of the 2nd Workshop onRepresentation Learning for NLP, pages 191200. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. 2023. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-jana Arunkumar, David Stap, et al. 2022. Super-naturalinstructions: Generalization via declarativeinstructions on 1600+ nlp tasks. In Proceedings ofthe 2022 Conference on Empirical Methods in Natu-ral Language Processing, pages 50855109.",
  "Dai, and Quoc V Le. 2021. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-neau, Vishrav Chaudhary, Francisco Guzmn, Ar-mand Joulin, and douard Grave. 2020. Ccnet: Ex-tracting high quality monolingual datasets from webcrawl data. In Proceedings of the Twelfth LanguageResources and Evaluation Conference, pages 40034012. Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-der M Rush, Bart Van Merrinboer, Armand Joulin,and Tomas Mikolov. 2015.Towards ai-completequestion answering: A set of prerequisite toy tasks.arXiv preprint arXiv:1502.05698. BigScience Workshop, Teven Le Scao, Angela Fan,Christopher Akiki, Ellie Pavlick, Suzana Ilic, DanielHesslow, Roman Castagn, Alexandra Sasha Luc-cioni, Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du,Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc VLe, Tengyu Ma, and Adams Wei Yu. 2024. Doremi:Optimizing data mixtures speeds up language modelpretraining. Advances in Neural Information Pro-cessing Systems, 36. Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, andWilliam Yang Wang. 2019. Tweetqa: A social mediafocused question answering dataset. arXiv preprintarXiv:1907.06292. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, QingweiLin, and Daxin Jiang. 2023. Wizardlm: Empoweringlarge pre-trained language models to follow complexinstructions. In The Twelfth International Conferenceon Learning Representations. Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bing-sheng Yao, Tongshuang Wu, Zheng Zhang, Toby Li,Nora Bradford, Branda Sun, et al. 2022. Fantasticquestions and where to find them: Fairytaleqaanauthentic dataset for narrative comprehension. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 447460.",
  "Hongyang Yang,Xiao-Yang Liu,and ChristinaDan Wang. 2023. Fingpt: Open-source financiallarge language models. FinLLM at IJCAI": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William Cohen, Ruslan Salakhutdinov, and Christo-pher D Manning. 2018. Hotpotqa: A dataset fordiverse, explainable multi-hop question answering.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages23692380. Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv,Nathaniel Mills, Assaf Toledo, Eyal Shnarch, andLeshem Choshen. 2024. Genie: Achieving humanparity in content-grounded datasets generation. arXivpreprint arXiv:2401.14367.",
  "Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen.2024. Mammoth2: Scaling instructions from the web.arXiv preprint arXiv:2405.03548": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics, pages 47914800. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, et al. 2024. Lima: Less is more for align-ment. Advances in Neural Information ProcessingSystems, 36.",
  "AData Collection for Fine-TuningInstruction Synthesizer": "displays our dataset collection for fine-tuning the instruction synthesizer. For each contextin the datasets, we gather all the downstream taskscorresponding to the context, and regard the con-text as the raw text and the downstream tasks asthe instruction-response pairs. For each dataset,we sample a maximum of 10K examples with thehighest number of instruction-response pairs, to en-hance task diversity while avoiding dataset predom-inance. Instruction-response pairs covers all theformats defined in (Longpre et al., 2023), includingfree-form completion, multiple-choice, free-formcompletion with chain-of-thought (CoT; Wei et al.,2022) and multiple-choice with CoT.",
  "BTuning and Inference Settings forInstruction Synthesizer": "Data FormatWe fill each data example into aspecifically designed template to explicitly separatedifferent parts. This facilitates the direct extractionof instruction-response pairs after inference. Weuse the template <CON> {text} </CON> to wrap theraw text. As shown in , we design differ-ent templates for different formats of instructions,and \\n\\n is used to connect instruction-responsepairs and link them with the raw text. Additionally,we use <s> before the beginning of each exam-ple and </s> after the end of each example. AnN-shot example is made by directly concatenatingN examples in a sequence. A case of a formatted2-shot data example for fine-tuning is displayed in. TuningTo constitute a few-shot example for fine-tuning, we concatenate as many formatted exam-ples as possible from the same dataset to match themaximum sequence length. The tuning hyperpa-rameters are in . InferenceDuring each round of inference, weconcatenate the formatted examples from previousrounds with the formatted raw text of the currentround as the input for the instruction synthesizer.Subsequently, the instruction synthesizer generatesa sequence of instruction-response pairs. The max-imum sequence length for inference correspondsto that of the target LM intended for pre-training.We use the vLLM (Kwon et al., 2023) frameworkfor acceleration. It takes about 1 day to synthe-size instruction-response pairs for 1B tokens of raw",
  "OPT: COT: Multiple Choice with CoT": ": Datasets for fine-tuning the instruction synthesizer, including Dasigi et al. (2021); Mller et al. (2020);Chen et al. (2022); Mirzaee et al. (2021); Weston et al. (2015); Khot et al. (2020); Aggarwal et al. (2021); Jin et al.(2019) in the expert materials domain, Rajpurkar et al. (2016); Adlakha et al. (2022); Choi et al. (2018); Pasupatand Liang (2015); Lamm et al. (2021); Yang et al. (2018) in the encyclopedia domain, Xu et al. (2022); Kocisk`yet al. (2018); Richardson et al. (2013) in the fiction domain, Soleimani et al. (2021); Trischler et al. (2017) in thenews domain, Joshi et al. (2017) in the trivia domain, Lai et al. (2017); Liang et al. (2019); Sun et al. (2019); Liuet al. (2021); Yu et al. (2019); Ling et al. (2017) in the academic tests domain, Xiong et al. (2019) in the socialmedia domain, and Reddy et al. (2019); Campos et al. (2020); Lin et al. (2019); Tafjord et al. (2019); Khashabi et al.(2018); Rogers et al. (2020); Huang et al. (2019) in the multi-domains sources domain. and HellaSwag (Zellers et al., 2019), and 5-shotperformance on tasks that are rather challengingand formatted as question-answering, includingARC (Clark et al., 2018), BoolQ (Clark et al.,2019), SIQA (Sap et al., 2019), OBQA (Mihaylovet al., 2018), and MMLU (Hendrycks et al., 2020).Using the lm-evaluation-harness framework, wereport the acc-norm score to follow Brown et al.(2020). Domain-Specific ModelsWe follow the prompt-ing settings of AdaptLLM (Cheng et al., 2023): forbiomedicine domain, we evaluate zero-shot per-formance on PubMedQA (Jin et al., 2019) andUSMLE (Jin et al., 2021), few-shot performanceon ChemProt (Kringelum et al., 2016), MQP (Mc-Creery et al., 2020) and RCT (Dernoncourt andLee, 2017); for finance domain, we evaluate zero-shot performance on ConvFinQA (Chen et al.,2022) and few-shot performance on FPB (Maloet al., 2014), FiQA SA (Maia et al., 2018), Head-line (Sinha and Khandait, 2021), and NER (Al-varado et al., 2015).",
  "We conduct human evaluation to analyze theinstruction-augmented corpora from the followingaspects:": "Response Accuracy: A binary score indicatingwhether the response is accurate based on theinstruction and context, where 1 means accurateand 0 means inaccurate. We report the averagescore of all responses. Context Relevance: A binary score indicatingwhether the instruction-response pair is relevantto the context of the raw text, where 1 meansrelevant and 0 means irrelevant. We report theaverage score of all instruction-response pairs. # Task Category: The evaluator categorizes eachinstruction-response pair using a predefined listof task categories from Wang et al. (2022). Wereport the number of different categories of allthe instruction-response pairs to show diversity.",
  "Domain Coverage (multi-domain only):Wespecifically compute domain coverage for thecases where a raw text contains multiple do-mains": "Domain Overlap: The overlap of raw text do-mains and instruction domains divided by theunion of raw text and instruction domains.As shown in ,the synthesizedinstruction-response pairs cover most of the do-mains in the raw text, with a high domain overlapwith the raw text. For the texts containing morethan one domain, our instruction synthesizer gen-erates, on average, 5 instruction-response pairs perraw text, with each pair potentially covering a dif-ferent domain. According to the domain cover-age (multi-domain only), when a single raw textincludes multiple domains, our instruction synthe-sizer can effectively generate instruction-responsepairs that cover most of the text domains. Domain DistributionWe analyze domain distri-butions of the following sources: Fine-tuning data for the instruction synthesizer. Raw pre-training corpora (Penedo et al., 2023). Synthesized instruction-response pairs based onthe raw pre-training corpora.As shown in , despite the domain distri-butions of fine-tuning data and raw corpora beingvery different, the synthesized pairs closely followthe domain distribution of the raw corpora.",
  "Fine-tune Data22.211.122.23.77.429.63.70.0Raw Corprora5.89.63.50.020.342.814.73.3Synthesized Pairs5.811.83.30.118.846.011.13.1": ": Domain distribution of fine-tuning data for the instruction synthesizer, raw corpora and synthesizedinstruction-response pairs. Encyclo\", Academic\", Expert\" and Social\" represent Encyclopedia, Academic Tests,Expert Materials and social media domains, respectively. <s> <CON> Our school life is very interesting! My friends and I study hard at school. And we are good atour lessons. We are very happy. We have lots of time for our hobbies. My classmates all want to go todifferent clubs. Helen wants to join the Reading Club. She loves reading books. The Reading Club meetsevery Wednesday at three thirty. Lily enjoys dancing. She wants to join the Dancing Club. It meets onMondays at four thirty. Theres also an Art Club. It meets on Fridays at four oclock. Nick doesnt wantto join the Art Club. He doesnt like drawing. He thinks it is too difficult for him . Nick likes playingcomputer games. He wants to join the Computer Club. It meets every Thursday at three forty-five. Mikeloves sports. He wants to join the football team. They play football every Monday at three thirty. I wantto join the Music Club. I like listening to music with my friends. The Music Club meets on Tuesday atthree fifteen. </CON>",
  "<QUE> How many friends does the story teller describe? <ANS> I have four friends. </END>": "<QUE> Are you and your friends smart? <ANS> unknown </END> </s><s> <CON> Billy and Sara arebrother and sister. They went to the beach with their family last July for a week, and had the best timeever! On Monday, Billy and Sara wanted to build a giant sandcastle. They invited their new friendsJack and Jane to help build the sandcastle. Jack and Jane had a house on the beach, so they were reallygood when it came to building sandcastles. They hoped that they could make the sandcastle taller thanthemselves, but they soon found they needed more help. They asked their cousin Joey to help them buildthe biggest sandcastle in the world! Joey wasnt the friendliest cousin in the world, but to Billy and Sarassurprise, Joey was happy to help build the sandcastle. Billy, Sara, Jake, Jane and Joey had spent the wholeday building the sandcastle, and finally, right before dinner time, they completed it. The sandcastle washuge! It had a river around the castle, and even a bridge to cross the river. It even had a flag at the top, anda wall that went around the castle too! They were so happy! The rest of the week at the beach was a lot of fun for Billy and Sara. On Tuesday, they went for ice cream.Saras ice cream fell and dripped all the way down to her tummy, but Billy gave her some of his. OnWednesday, they watched the fireworks at night. On Thursday, they went swimming all day long, movinglike worms in the water. On Friday, they had to go back home. They were sad, so they started countingdown the days until next year at the beach! </CON>",
  "Instruct PT1.3B100B60.5/30.962.249.255.973.633.454.327.3OPT1.3B300B60.1/31.162.448.458.271.034.053.825.1GPT-21.5B-60.2/29.663.547.356.270.533.250.826.3BLOOM3B341B63.1/35.362.248.857.470.533.054.625.9": ": Comparison between our pre-trained models and other open-source models (Radford et al., 2019;Biderman et al., 2023; Workshop et al., 2022; Zhang et al., 2022) on general benchmarks. WG and HS representWinoGrande and HellaSwag, respectively. Not a writer, a writer wannabe, editor, lit maj, or pretend literary critic. Just an avid reader/listener. Myratings are opinion only.I love all genres of books. However, when I listen to audio books as I clean, garden, drive they are betterwith a lot of heat!\"Laborious\"This might have been a bit more tolerable if narrator was better. I am happy to say that I did finish thebook but it just seemed to go and on. Like other listeners the book itself reminded me of a bad TV show.Not horrible but of all the books I have listened to this is just bearly average. Problem: Pick your answer from:a). They didnt like the genre.;b). They did nt have enough time to read it.;c). They did nt like the author.;d). They did nt like the narrator.;Q: What may be the reason for them not finishing the book?",
  "Answer: d)": "Customer Web Interaction: Fundamentals and Decision Tree From Virtual CommunitiesAuthorsEnrico Senger, Sandra Gronover, and Gerold Riempp, University of St. GallenAbstractIn order to utilise the new possibilities of Internet technology efficiently, many companies invest consider-able sums in the development of communication channels to customers. In this context, the often-quotedobjective of cost saving per interaction appears to be questionable, since new communication media havenot been able to fully substitute the existing systems. Costs are therefore more likely to rise than drop.The following article discusses potentials, criteria, conditions and consequences related to the use ofcomputer-mediated environments for customer interaction. The objective is to derive recommendationsfor action in respect of a context-dependent support, especially by means of web collaboration andself-service-options.Download Customer Web Interaction: Fundamentals and Decision Tree",
  ": A case of a 2-shot example in the general instruction-augmented corpora": "Read this article and answer questions# Correlation between increased airway responsiveness and severity of pulmonary edema.To determine whether the severity of the pulmonary edema in sheep models of cardiogenic and non-cardiogenic pulmonary edema correlate with concomitant alterations in airway responsiveness usingthree separate measures of pulmonary edema: post-mortem wet-to-dry lung weight ratio (W/D), chestradiograph (CXR) scores, and small airway wall area. Cardiogenic pulmonary edema was induced byincreasing left atrial pressure (increase PLA) and non-cardiogenic pulmonary edema was induced byintravenous administration of Perilla ketone (PK). (...) Does increased airway responsiveness correlate with pulmonary edema severity in sheep?Lets think first: Increased airway responsiveness correlates with severity of pulmonary edema in sheep...So the answer is [Yes] Read this article and answer questions# Immobilization and bioactivity evaluation of FGF-1 and FGF-2 on powdered silicon-doped hydroxyap-atite and their scaffolds for bone tissue engineering.Fibroblast growth factors (FGFs) are polypeptides that control the proliferation and differentiation ofvarious cell types including osteoblasts. FGFs are also strong inducers of angiogenesis, necessary toobtain oxygen and nutrients during tissue repair. (...) Do immobilization of fibroblast growth factors 1 and 2 on silicon-doped hydroxyapatite scaffolds for bonerepair?Lets think first: This study demonstrated the efficient immobilization of FGF-1 and FGF-2 on Si-HAand Si-HA scaffolds, retaining their biological activity on osteoblasts. Thus, these FGF/scaffolds may beuseful in bone tissue engineering applications... So the answer is [Yes] Read this article and answer questions# Phytotoxicity, cytotoxicity and genotoxicity evaluation of organic and inorganic pollutants rich tannerywastewater from a Common Effluent Treatment Plant (CETP) in Unnao district, India using Vigna radiataand Allium cepa.The leather industry is a major source of environmental pollution in India. The wastewater generated byleather industries contains very high pollution parameters due to the presence of a complex mixture oforganic and inorganic pollutants even after the treatment at a Common Effluent Treatment Plant (CETP)and disturbs the ecological flora and fauna. The nature, characteristics and toxicity of CETP treatedwastewater is yet to be fully elucidated. Thus, this study aims to characterize and evaluate the toxicity ofCETP treated tannery wastewater collected from the Unnao district of Uttar Pradesh, India. In addition tomeasuring the physico-chemical parameters, the residual organic pollutants was identified by GC-MSanalysis and phytotoxicity, cytotoxicity and genotoxicity of the treated wastewater was evaluated usingVigna radiata L. and Allium cepa L. (...) Is common effluent treatment plant wastewater safe for the environment?Lets think first: The present study revealed the presence of high levels of various pollutants in CETPtreated tannery wastewater. Moreover, the toxicity assessment showed the phytotoxic and genotoxic natureof the wastewater which suggests that this wastewater cannot be directly discharged into the environmentwithout any further treatment... So the answer is [No]",
  ": A case of a 3-shot example in the instruction-augmented corpora for biomedicine domain. Certainportions are omitted for brevity and are represented as (...)": "Answer questions based on this article:Once the MOASS is truly over would anyone like an AMA with DFV AND RC? I would love to learnwhat went on through their minds and the events all the way from 2019 to post-MOASS.They must be dying to talk about all the things that went on (but couldnt because of all the potentialcontroversy and lawsuits that can be had) and apes would love to get the official explanation on the cryptic,and some not so cryptic tweets from DFV and RC. Edit: it may be obvious but its just an opinion of mineon to see what they may have to say. If it does somehow gain enough traction, we would respectfully askthem if theyre interested. If not, no AMA. Simple as that. Ive been thinking what we should do is oncethe squeeze is over let it die down a bit and then we should start a gmecon or something similar. I wantedto right a post about it but my karma is too low so if someone else wants to put it out there and see whatpeople think that would be great. Personally Im in this stock for life and would love an annual eventwhere we could all meet up and have in person Q and As with RC, DFV and others, even someone likeJordan Belfort to hype up the apes after we take our tendies. also would be good to see all gamestopsideas for the future. Just a thought hope theres some way we could make this happen.",
  "question below:What might happen if they did an AMA with DFV and RC?answer below:They would ask questions about the cryptic tweets": "Answer questions based on this article:Pixars Lightyear snares $51 million in domestic openingPixars Lightyear rocketed to a $51 million domestic opening, the best performance of an animatedfeature since the pandemic began. Internationally, the Disney film tallied $34.6 million in ticket sales,bringing its global haul to $85.6 million. The animated films performance, while strong for a pandemicrelease, fell short of expectations. Box office analysts had foreseen Lightyear bringing in between $70million and $85 million domestically. Expectations were high because the last two films in the Toy Storyfranchise both opened to more than $100 million in ticket sales, according to data from Comscore. ToyStory 4 in 2019 topped $120 million in its domestic debut and Toy Story 3 generated more than $110million during its opening 2010. Lightyear had a great deal of potential on paper, but a number offactors resulted in this very rare box office misfire for a Pixar release, said Shawn Robbins, chief mediaanalyst at BoxOffice.com. Its unclear if tough box office competition with Universals Jurassic World:Dominion, which generated $58.6 million over the weekend, and Paramount and Skydances Top Gun:Maverick, which secured another $44 million, was the reason for Lightyears smaller-than-expectedopening or if consumers were confused about the film release. After all, there has not been a theatricalrelease of a Pixar film since 2020s Onward. (...)"
}