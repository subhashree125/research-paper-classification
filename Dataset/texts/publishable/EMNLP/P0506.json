{
  "Abstract": "Protein Language Models traditionally dependon Multiple Sequence Alignments (MSA) toincorporate evolutionary knowledge. However,MSA-based approaches suffer from substantialcomputational overhead and generally under-perform in generalizing to de novo proteins.This study reevaluates the role of MSA, propos-ing it as a retrieval augmentation method andquestioning the necessity of sequence align-ment. We show that a simple alternative, Re-trieved Sequence Augmentation (RSA), canenhance protein representation learning with-out the need for alignment and cumbersomepreprocessing.RSA surpasses MSA Trans-former by an average of 5% in both structuraland property prediction tasks while being 373times faster. Additionally, RSA demonstratesenhanced transferability for predicting de novoproteins. This methodology addresses a criticalneed for efficiency in protein prediction and canbe rapidly employed to identify homologous se-quences, improve representation learning, andenhance the capacity of Large Language Mod-els to interpret protein structures.1",
  "Introduction": "Proteins are fundamental yet complex componentsof life. They exhibit a diverse range of functionswithin organisms. The enigmatic characteristicof these macromolecules originates from the in-tricate interplay between their sequences, struc-tures, and functions, which is influenced jointly byphysics and evolution (Sadowski and Jones, 2009).Protein language models (Elnaggar et al., 2020;Jumper et al., 2021; Lin et al., 2022) capture theco-occurrence probability of amino acids observedin nature, thus encapsulating structural and evolu-tionary information within the resulting represen-tations. While this approach has demonstrated itseffectiveness (Elnaggar et al., 2021; Jumper et al.,",
  "Code and data are available at this repo": ": (Upper) Illustration of inference speed upby RSA compared to MSA on secondary structureprediction dataset with 8678 sequences. AcceleratedMSA (Acc-MSA) refers to MSA built from sequencesretrieved by our RSA retriever. (Lower) Illustrationof speed up by RSA retrieval compared to MSA ondatabase construction of 10000 protein sequences. 2021; Lin et al., 2022; Nijkamp et al., 2022; Riveset al., 2019), the evolutionary knowledge that canbe extracted from a single sequence remains insuf-ficient (Hu et al., 2022; Lin et al., 2022).In order to compensate for this limitation, mul-tiple sequence alignments (MSA)Yanofsky et al.1964; Altschuh et al. 1988; De Juan et al. 2013;Jumper et al. 2021) have been extensively usedas a foundational protein feature engineering tech-nique to extract protein evolutionary informationin protein models (Rao et al., 2021; Jumper et al.,2021; Abramson et al., 2024). MSA draws on theevolutionary principle that functional constraintsof species govern the mutation rate, which in turndrives the convergence of sequences. Therefore,key residues at functional sites tend to be con-served across protein families. MSA primarilyaligns these conserved regions across homologousproteins to identify critical functional residues,such as substrate binding sites (Kunji and Robin-son, 2006).Traditional approaches (Yanofskyet al., 1964; Altschuh et al., 1988) such as PottsModel (Balakrishnan et al., 2011), directly extractsstatistical features from MSA for structural predic-tion. In recent studies (Jumper et al., 2021; Raoet al., 2021; Yang et al., 2020; Ju et al., 2021), mod-",
  "The protein is most likely to be located on the membrane": ": Comparison between MSA Transformer and RSA. MSA Transformer aligns query to the proteindatabase and use axial attention to encode MSA feature. RSA could enhance protein language models by encodingboth retrieved and original sequences. RSA could enhance LLM as a tool. Both MSA Transformer and RSA fallwithin the retrieval framework; however, RSA doesnt require the alignment process. els like MSA Transformer (Rao et al., 2021) andAlphaFold (Jumper et al., 2021; Abramson et al.,2024) employ language models for predictions us-ing MSAs as input feature. Despite being a vitalcomponent for state-of-the-art models, MSA car-ries a NP-Complete computational cost that scaleswith O(LN) (Wang and Jiang, 1994), where L rep-resents the length of each sequence and N denotesthe number of sequences examined. Even withacceleration techniques, MSA remains computa-tionally intensive. For example, HHblits (Remmertet al., 2012) requires 10 seconds for a single itera-tion search on Pfam using 64 CPUs.This motivates us to investigate alternatives totraditional alignment by addressing two researchquestions: (1) Is alignment truly necessary forprotein language models, and (2) is there a cost-efficient substitute for MSA? To answer these questions, we revisit MSA froma data-centric point of view and prove through the-oretical analysis that it can be viewed as a retrieval-augmentation method (Goyal et al., 2022; Guuet al., 2020a; Khandelwal et al., 2019; Wang et al.,2022). We argue that MSA is retrieval throughalignment.Retrieval-augmentation employs alarge-scale memory of sequences as the knowledgebase and utilizes multiple related input sequencesinstead of the single input to connect to the requiredknowledge. This approach offers the potential formore interpretable and modular knowledge cap-ture (Guu et al., 2020b). It also enables rapid gener-alization to new domains (Khandelwal et al., 2019;Basu et al., 2022). Furthermore, we show that align-ment is not essential as long as we have a strongsequence encoder, i.e. a transformer-based proteinlanguage model. This finding echoes previous re-search (Bhattacharya et al., 2020) that co-evolutionpatterns found through alignment could be cap- tured with a single layer of attention without sharedparameters across sequences. Since MSA is notindispensable and works mainly by enhancing pro-tein language models as a retrieval-augmentationmethod, more efficient alignment-free retrieverscan naturally serve as a substitute for MSA. To this end, we explore Retrieved SequenceAugmentation (RSA) methods as a general frame-work to enhance protein representations. Specifi-cally, RSA employs a pre-trained dense sequenceretriever in search for protein sequences that aresimilar to the query sequence both in terms of ho-mology as well as structure. By encoding retrievedsequences alongside the original protein, the modelincorporates external knowledge and transfers itto new domains. Our assessment of this methodconsists of comprehensive experiments conductedacross seven distinct tasks, including protein struc-ture, function, evolution, and engineering, whichrequire diverse knowledge. Using a vast databaseof approximately 40 million protein sequences, weshow that a retrieval-based approach leveragingthis data consistently outperforms state-of-the-artmethods. Moreover, RSA employs retrieved se-quences from dense retrievers without requiringan alignment process, thus resulting in a 373-foldspeed-up and on-the-fly processing, as shown in. Additionally, RSA without additionalpretraining outperforms a pre-trained MSA Trans-former in the downstream tasks, particularly fordenovo proteins with few or no MSAs. It can beeasily incorporated to augment any pre-trained pro-tein language model, and be used as an efficienttool to boost the ability of large language model(LLM), e.g. GPT-4 (Achiam et al., 2023) to under-stand protein sequence. Consequently, we concludethat retrieval augmentation for proteins as a generalframework can be a sound replacement for MSA in",
  "Background and Problem Statement": "Given a protein x = [o1, o2, ...oL] comprising ofL amino acids, the objective of a protein languagemodel is to learn an embedding transferable to sub-sequent tasks, e.g. predicting properties of the se-quence p(y|x). The embedding is represented asEmbed(x) = [h1, h2, ...hL], where hi Rd.One approach to building an evolution-informedrepresentation is to encode the input from a Mul-tiple Sequence Alignment (MSA). An MSA in-cludes several protein sequences, each a homologof the query protein sequencetypically homolo-gous proteins from species that are evolutionarilyclose. These proteins are aligned together such thateach column in the alignment represents the evolu-tionary changes of an amino acid. In functionallyimportant regions, amino acids tend to remain morestable, whereas in other regions, amino acids mayundergo deletions, mutations, or insertions as evo-lution progresses. provides an illustration",
  ": Illustrated difference of aligned and unalignedprotein sequences. The white colour stands for theempty space in alignment \"-\"": "for MSA. In our analyses, we consider MSA as Naligned protein homologs r1, . . . , rN. Prior stud-ies (Yang et al., 2020; Ju et al., 2021) encode MSAas co-evolution statistics features R1...N and ag-gregate these features to derive the representation,while MSA Transformer (Rao et al., 2021; Jumperet al., 2021) perceives MSA as a matrix, employ-ing axial attention to extract salient evolutionarytraits. Here we also denote retrieved sequences asr1, . . . , rN and their features as R1...N, though noalignment is performed on these sequences.",
  "MSA is Retrieval through Alignment": "Inspired by Guu et al. (2020b), we rethink state-of-the-art evolution augmentation methods undera new framework: protein retrieval augmentation.Specifically, we consider these methods as learn-ing a downstream predictor p(y|x) based on anaggregation of homologous protein representationsR1...N. From the view of retrieval, p(y|x) is de-composed into two steps: retrieve and predict. Fora given input x, the retrieve step first finds possiblyhelpful protein sequence r from a sequence corpusR and then predict the output y conditioning onthis retrieved sequence:",
  "n=1p(y|x, rn)p(rn|x).(1)": "The probability p(r|x) denotes the possibility that ris sampled from the retriever given x. Intuitively itmeasures the similarity between the two sequencesr and x. This framework also applies to the MSA-based augmentation methods. We explain in detailusing a state-of-the-art MSA-augmentation modelMSA Transformer (Rao et al., 2021) as an example.In MSA Transformer, the axial attention layerscalculate self-attention both row-wise and column-wise. Column-wise attention is defined as follows,",
  "Retrieval Sequence AugmentationDense RetrievalNot Aligned(||X Rn||2)Ni=1 FNN(Embed(x; rn))n": ": Protein Retrieval Augmentation methods decomposed along a different axis. We formulate the aggregationfunction in the sequence classification setting and use a feed-forward neural network FFN() to map representationsto logits. The proposed variants vary in design axis from the existing methods. Note that MSA Transformerperforms the aggregation in each layer of axial attention, which differs from other variants.",
  "d)Rn(i)WV WO,(2)": "where Rn(i) denotes the i-th token representationof the n-th MSA sequence after performing the row-wise attention. Note that in MSA input, the firstsequence r1 is defined as the original sequence x.Then for a token prediction task, we define the i-thposition output as y and the predicted distributionp(y|x) can be expressed as:",
  "N": "d) is theweighting norm that represents the similarityof retrieved sequence rn and original sequencex; p(y|x, rn) is a predictor that maps the row-attention representation of rn and x to label. Eq.3gives a retrieval-augmentation view of MSA Trans-former that essentially retrieves homologous se-quences with multiple sequence alignment and ag-gregates representations of homologous sequenceswith regard to their sequence similarity. Takingone step further, we define a set of design dimen-sions to characterize the retrieving and aggregationprocesses. We introduce how popular models (Ap-pendix E) and our proposed methods (3) fall alongthem in . A detailed introduction of designdetails is available in Appendix D.Our discussion and formulation so far reach theconclusion that retrieval augmentation serves as acomprehensive framework capable of extracting HomologyStabilitySSPLoc 0.5 1.0",
  ": Comparison of variants on the retrieval formand the alignment form on property prediction tasks": "evolutionary knowledge, akin to multiple sequencealignment (MSA) augmentation methods. This un-derlines the prospects of retrieval sequence align-ment (RSA) superseding MSA augmentations asan efficient and generalizable approach.However, MSA-based methods claim a few ad-vantages: the alignment process can help the modelcapture column-wise residue evolution; and theMSA Retriever uses a discrete, token-wise searchcriterion that ensures all retrieved sequences arehomology. We introduce two variants to help chal-lenge these claims: 1) Unaligned MSA Augmen-tation (- Alignment), illustrated in , usesthe homologous sequences from MSA to augmentrepresentations without alignment and 2) Accel-erated MSA Transformer (+ Dense Retriever)explores substituting the discrete retrieval processin MSA with a dense retriever. An empirical studyon these models can be found in 2.3.",
  "Do we still need alignment for proteins?An Empirical Analysis": "It is commonly believed eliminating alignmentcould reduce expressiveness of proteins, as it high-lights residue-wise mutations and compares acrossa protein family (Altschuh et al., 1988; De Juanet al., 2013). Bhattacharya et al. (2020) insteadshow that a single layer of attention suffices to predict MSA-based statistics like pairwise residueco-evolution without shared parameters across thefamily. Truong Jr and Bepler (2023) also proposedusing Transformers to represent co-evolution with-out alignment. Here we compare variants againstMSA Transformer to further discuss the necessityof aligned feature when we have a strong proteinlanguage model as encoder. As shown in , alignment does not consistently improve perfor-mance and unaligned variants achieve comparableperformances on both homology and stability pre-diction. Additionally, a dense retriever competi-tively substitute aligner to find related sequencesfor retrieval augmentation. As alignment does notadditionally improves performance when we havea strong protein language model, we could safelysubstitute MSA for dense retrieval augmentationmethods.",
  "RSA": "Motivated by the potential of pre-trained retrieversto identify proteins that are homologous or geomet-ric similar, we propose a pipeline, RSA (RetrievalSequence Augmentation), to directly augment pro-tein models on-the-fly. RSA follows the retrieve-then-predict framework in Eq. 1. It comprisesof a neural sequence retriever p(r|x), and a pro-tein model that combines both original input andretrieved sequence to obtain prediction p(y|x, r). RSA Retrieveris used for finding the sequencesthat are semantically close to the query. The sim-ilarity score f(x, r) is defined as the negative L2distance between the embedding G of the two se-quences.",
  "(4)": "For protein retrieval, we aim to retrieve proteinsequences that have similar structures or are homol-ogous to the query sequence. Motivated by the highaccuracy of k-nearest neighbor retrieval with ESM-1b (Rives et al., 2019) pre-trained embeddings (asshown in ), we implement the embeddingfunctions using a 34-layer ESM-1b encoder. We ob-tain sequence embeddings by performing averagepooling over token embeddings. Note that find-ing the most similar proteins from a large-scalesequence database is computationally heavy. To ac-celerate retrieval, we use Faiss indexing (Johnson",
  "et al., 2019a), which uses clustering and quantiza-tion to allow efficient similarity search": "Retrieval Augmented Protein EncoderGiven asequence x and a retrieved sequence r with lengthL and M respectively, the protein encoder com-bines x and r for prediction p(y|x, r). To make ourmodel applicable to any protein learning task, weneed to augment both sequence-level representa-tion and token-level representation (essential forstructure prediction tasks). We concatenate thetwo sequences before input into the transformer en-coder, which uses self-attention to aggregate globalinformation from the retrieved sequence r into eachtoken representation.",
  "Attn(H[x;r]) = (AxHxW V + ArHrW V )W O(5)": "where H[x;r] = [hx1, hx2, ..., hxL, hr1...hrM] denotesthe input embedding of original and retrieved se-quences. The output token representation hi end-to-end learns to select and combine the representationof retrieved tokens. This can also be considereda soft version of MSA alignment. After comput-ing for each pair of (x, r), we aggregate them byweight p(r|x) defined in Eq. 4. TrainingFor downstream finetuning, we maxi-mize p(y|x) by training on the retrieval augmentedprotein encoder. We freeze the retriever parametersduring training. For a query sequence of length Lwith N retrieved proteins, suppose the length ofretrieved proteins L L the computation cost isN times the original model, O(NL2) for a trans-former encoder layer, which is as efficient as MSATransformer with O(NL2) + O(N2L) complexity.",
  "Downstream Task We evaluate RSA on sevendownstream tasks: secondary structure predic-": "tion (Klausen et al., 2019),contact predic-tion (AlQuraishi, 2019), remote homology predic-tion (Hou et al., 2018), subcellular localization pre-diction (Almagro Armenteros et al., 2017), stabilityprediction (Rocklin et al., 2017), protein-protein in-teraction (Pan et al., 2010) and structure predictionon CASP14 (Kryshtafovych et al., 2021). Pleaserefer to Appendix for more statistics ofthe datasets. The train-eval-test splits follow TAPEbenchmark (Rao et al., 2019) for the first four tasksand PEER benchmark (Xu et al., 2022) for subcel-lular localization and protein-protein interaction. Retriever and MSA SetupLimited by availablecomputation resources, we build a database onPfam (El-Gebali et al., 2018) sequences, whichcovers 77.2% of the UniProtKB (Apweiler et al.,2004) database and reaches the evolutionary scale.We generate ESM-1b pre-trained representationsof 44 million sequences from Pfam-A and useFaiss (Johnson et al., 2019b) to build the retrievalindex. For a fair comparison, the MSA datasetsare also built on the Pfam database. We use HH-blits (Remmert et al., 2012) to extract MSA, search-ing for 3 rounds with e-value threshold 1e-3. BaselinesWe apply our retrieval method to bothpre-trained and from-scratch language models. Fol-lowing Rao et al. (2019) and Rao et al. (2021),we compare our model with vanilla protein rep-resentation models, including LSTM (Liu, 2017),Transformers (Vaswani et al., 2017) and pre-trainedmodels ESM-1b (Rives et al., 2019), ProtBERT (El-naggar et al., 2020).We also compare withstate-of-the-art knowledge-augmentation models:Potts Model (Balakrishnan et al., 2011); MSATransformer (Rao et al., 2021) injects evolution-ary knowledge through MSA; OntoProtein (Zhanget al., 2022)uses gene ontology knowledge graphto augment protein representations and PMLM (Heet al., 2021b) uses pair-wise pretraining to enhanceco-evolution awareness. Training and EvaluationTo demonstrate RSAas a general method, we perform experiments bothwith a shallow transformer encoder, and a large pre-trained ProtBERT encoder. The Transformer modelhas 512 dimensions and 6 layers. Also, we com-bined our method with popular pre-trained proteinfolding architectures ESMFold and AlphaFold2.All self-reported models use the same truncationstrategy and perform parameter searches on thelearning rate, warm-up rate, and batch size.",
  "Main Results": "We show the result for downstream tasks in Ta-ble 3, including models with/without pretraining,and with/without knowledge augmentations. Weform the following conclusion: Retrieval SequenceAugmentations perform on par with or even betterthan other knowledge-augmented methods withoutadditional pre-training. Our method outperformsMSA Transformer on average by 5% and performson par with PMLM on structure and evolution pre-diction tasks. Notably, both MSA Transformer andPMLM perform additional pre-training with aug-mentations, while our method uses no additionalpre-training. From the results, we can see thatRSA combined transformer model also improvesby 10% than other shallow models. We also studyretrieval sequence augmentations on pre-trainedprotein folding models in . Despite RSAwas implemented without additional fine-tuningon folding models, we achieve a 2% improvementboth on ESMFold and AlphaFold2.",
  "Retrieval Augmentation for De NovoProteins with Few Homologs": "We test our model on a challenging problem inprotein prediction, the prediction for proteins withfew homologs, i.e. de novo (synthesized) proteinsand orphan proteins (Fang et al., 2022; Wu et al.,2022). This task is especially difficult for MSA-based methods as alignment-based method oftenfails to generate MSA for these proteins, resultingin degraded performance. We test our model on108 De Novo proteins from PDB (Berman et al.,2000) for the contact prediction task. It can beseen in that, RSA exceeds MSA trans-former on 63.8% of data, demonstrating that RSAis more capable of locating augmentations for out-of-distribution proteins. We also test our modelon the structure prediction task with 16 targetsfrom CASP14-FM. CASP14-FM are consideredmore difficult because the absence of related tem-plates requires the prediction methods to rely onde novo modeling techniques. We compare RSAaugmented ESMFold and AlphaFold2 model withbaselines in , showing improved or compet-itive prediction on the majority of the targets. Thisresults also show that our model surpasses MSA-based methods in transferring to unseen domains.",
  "RSA (ProtBERT backbone)0.6910.7170.9870.7780.795 0.827 0.723": ": Main Results for vanilla protein language models, knowledge-augmented baselines and our proposed RSAmethod. Note that italized result is reported by corresponding related work. The last column reports average resulton all six tasks. For MSA Transformer and RSA, we all use 16 sequences (N=16) for augmentation. For GremlinPotts model, we use the full MSA.",
  "RSA as a Tool for Large Language Model": "RSA can not only be used on small-scale represen-tation learning model, it can also augment largelanguage models, e.g. ChatGPT. Currently, evenGPT4 model shows limited understanding of bio-logical sequences. We follow ToolFormer (Schicket al., 2024) to equip RSA as a tool for GPT models,enabling LLM to query RSA and retrieve similarsequences as well as Pfam labels to improve under-standing of the protein sequence. We benchmarkRSA as a Tool on Gene Ontology tasks (Jensenet al., 2003). Results show that RSA as tool couldimprove protein understanding ability of LLMs forall tasks. We also show that RSA can be integrated",
  "Retrieval Speed": "A severe speed bottleneck limits the use of previousMSA-based methods. We compare the computa-tion time of RSA with MSA and an acceleratedversion of MSA as introduced in 2.2. As shownin , alignment time cost is much more in-tense than retrieval time. Even after reducing thenumber of alignment sequences to 500, acceler-ated MSA still need 270 min to build MSA. At thesame time RSA only uses dense retrieval, and isaccelerated 373 times. Also, MSA is limited by itscubersome construction of retrieval HHM profileto perform HHM-HHM search. By contrast, RSAonly needs to build the pre-trained features for thedatabase, which can be accelerated with GPUs andbatch forwarding. Results on a small database of10000 proteins demonstrate a speedup of 320 times.",
  ": The performance of RSA w.r.t. the number ofretrieved sequences on contact prediction": "Ablation on Aggregation:We compare RSAwith Accelerated MSA Transformer to evaluatewhether our aggregation method is beneficial forlearning protein representations. Note that onlypart of the retrieved sequences that are homologousare utilized after alignment. As shown in ,the performance of the Accelerated MSA Trans-former drops a lot compared to RSA. In contrast toMSA type aggregation, which is restricted by tokenalignment, our aggregation is more flexible and canaccommodate proteins with variant knowledge.",
  "Retrieved Protein Interpretability": "Dense Retrievers Find Homologous Sequences.As illustrated in (a), across all six datasets,our dense retriever retrieved a high percentage ofhomologous proteins that can be aligned to theoriginal protein sequence, comparable to traditionalMSA retrievers. We additionally plot each datasetsnegative log E-values distribution in (b).Accordingly, dense retrieval show high potential forfinding homologous sequences, which explains theability of RSA to capture evolutionary knowledge. RSA Retriever Find Structurally Similar Pro-teinIn (c), we plot the TM scores be-tween the RSA retrieved protein (structure obtainedwith ESMFold) and the origin protein on Protein-Net (AlQuraishi, 2019) test set. Most of the re-trieved proteins TM-score exceed 0.2 (acceptablestructural similarity) and about half are above 0.5(high similarity), indicating dense retrieval is capa-ble of finding proteins with structural knowledge. ContactFoldLocPPISSStability 0.00 0.25 0.50 0.75 1.00",
  "(c) TM-score distribution": ": (a) Plot of the percentage of sequences thathave found homologs on datasets for six tasks. (b) Plotof the -log(E-values) of MSA and Dense Retriever ob-tained sequences. E-values of both methods are obtainedwith HHblits(Remmert et al., 2012). Sequences with-log E-value >10 are high-quality homologs. (c) Cumu-lative distribution of TM-scores for retrieved proteins.",
  "Creating MSA with RSA": "Despite the cumbersome computation, MSA is stillwidely used at present in SOTA models. In thissection, we discuss the quality of MSA built byRSA, i.e. Accelerated MSA, a process 10 timesfaster. illustrates that Accelerated MSATransformer performs near to MSA Transformer(MSA N=16) for most datasets, except for Stabilityand PPI on which our retriever failed to find enoughhomologous sequences. Also, Accelerated MSAcould be used as input for AlphaFold2 model, asshown in . However, the performance variesamong samples, depending on retrieved sequencediversity, as further discussed in Appendix G.4.",
  "complex reasoning (Trivedi et al., 2022) and gener-alization (Khandelwal et al., 2019)": "Protein Language ModelsTo model and fur-ther understand the protein sequence data, lan-guage models are introduced to train on massdata (Heinzinger et al., 2019; Alley et al., 2019).Large scale pre-training enables language modelsto learn structural and evolutionary knowledge (El-naggar et al., 2021; Jumper et al., 2021; Lin et al.,2022). Despite these successes, many importantapplications still require MSAs and other externalknowledge (Rao et al., 2021; Jumper et al., 2021;He et al., 2021b; Zhang et al., 2021; Ju et al., 2021;Rao et al., 2020). MSAs have been shown effec-tive in improving representation learning, despitebeing extremely slow and costly in computation.Hu et al. (2022) and Hong et al. (2021) use denseretrieval to accelerate multiple sequence augmenta-tion, while still dependent on alignment procedures.Recent work (Fang et al., 2022; Lin et al., 2022;Wu et al., 2022; Chowdhury et al., 2022) exploresMSA-free language models though additional pre-training is involved. We take this step further toinvestigate retrieval-augmented protein languagemodels. Another line of work improves MSA qual-ity and generation speed by using generative mod-els to augment and produce MSAs (Zhang et al.,2020; Zheng et al., 2024; Chen et al., 2024). We dif-fer from this line of work as we discuss substitutesrather than augmentation of MSAs.",
  "Limitation": "One notable limitation of our method RSA is that itis highly dependent on high-quality pre-trained em-beddings and the abundance of protein sequences.We found that our retriever tends to perform bet-ter in a database that has more protein sequences that have not been screened by a clustering algo-rithm, like Uniclust30. This could be explainedby our nearest neighbor retrieval technique which often requires more similar sequences for augmen-tation. We also found different patterns in retrievalsequences from MSAs. Our retriever tends to showpolarized retrieval quality, either finding many evo-lutionary close sequences or failing to find anyhomologous sequences. We believe this is due tothe imbalanced training of pre-trained embeddingson different protein families and hope to mitigatethis issue with further training on retrieval datasets.We report other failed cases here for a morethorough view of our proposed method: Directly applying Accelerated MSAs to MSA-based pre-trained models often shows about2-3% decrease on downstream performancethan using original MSAs.This may bethe natural gap between Acc-MSA and pre-training data.However, Accelerated MSAs are10 times faster. The performance of RSA improves marginallywith more sequences when N > 16. Thisis because we use the softmax distributionover L2 metrics to perform weighting, therebyassigning low weights to sequences furtherfrom the query.",
  "Acknowledgement": "We would like to thank the anonymous reviewersfor their valuable suggestions that greatly helpedimprove this work. This work is partially supportedby the joint research scheme of the National Natu-ral Science Foundation of China (NSFC) and theResearch Grants Council (RGC) under grant num-ber N_HKU714/21. Josh Abramson, Jonas Adler, Jack Dunger, RichardEvans, Tim Green, Alexander Pritzel, Olaf Ron-neberger, Lindsay Willmore, Andrew J Ballard,Joshua Bambrick, et al. 2024. Accurate structure pre-diction of biomolecular interactions with alphafold 3.Nature, pages 13. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mo-hammed AlQuraishi, and George M Church. 2019.Unified rational protein engineering with sequence-based deep representation learning. Nature methods,16(12):13151322. Jos Juan Almagro Armenteros, Casper Kaae Sn-derby, Sren Kaae Snderby, Henrik Nielsen, andOle Winther. 2017. Deeploc: prediction of proteinsubcellular localization using deep learning. Bioin-formatics, 33(21):33873395.",
  "D Altschuh, T Vernet, P Berti, D Moras, and K Nagai.1988. Coordinated amino acid changes in homolo-gous protein families. Protein Engineering, Designand Selection, 2(3):193199": "Rolf Apweiler, Amos Bairoch, Cathy H Wu, Winona CBarker, Brigitte Boeckmann, Serenella Ferro, Elis-abeth Gasteiger, Hongzhan Huang, Rodrigo Lopez,Michele Magrane, et al. 2004. Uniprot: the univer-sal protein knowledgebase. Nucleic acids research,32(suppl_1):D115D119. SivaramanBalakrishnan,HetunandanKamisetty,Jaime G Carbonell, Su-In Lee, and Christopher JamesLangmead. 2011. Learning generative models forprotein fold families. Proteins: Structure, Function,and Bioinformatics, 79(4):10611078.",
  "Helen M Berman, John Westbrook, Zukang Feng, GaryGilliland, Talapady N Bhat, Helge Weissig, Ilya NShindyalov, and Philip E Bourne. 2000. The proteindata bank. Nucleic acids research, 28(1):235242": "Nicholas Bhattacharya, Neil Thomas, Roshan Rao, Jus-tas Dauparas, Peter K Koo, David Baker, Yun S Song,and Sergey Ovchinnikov. 2020. Single layers of at-tention suffice to predict protein contacts. Biorxiv,pages 202012. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,Trevor Cai, Eliza Rutherford, Katie Millican, Georgevan den Driessche, Jean-Baptiste Lespiau, BogdanDamoc, Aidan Clark, et al. 2021. Improving lan-guage models by retrieving from trillions of tokens.arXiv preprint arXiv:2112.04426.",
  "David De Juan, Florencio Pazos, and Alfonso Valencia.2013. Emerging methods in protein co-evolution.Nature Reviews Genetics, 14(4):249261": "Sara El-Gebali, Jaina Mistry, Alex Bateman, Sean REddy, Aurlien Luciani, Simon C Potter, MatloobQureshi, Lorna J Richardson, Gustavo A Salazar, Al-fredo Smart, Erik L L Sonnhammer, Layla Hirsh,Lisanna Paladin, Damiano Piovesan, Silvio C ETosatto, and Robert D Finn. 2018. The Pfam proteinfamilies database in 2019. Nucleic Acids Research,47(D1):D427D432. Ahmed Elnaggar, Michael Heinzinger, Christian Dal-lago, Ghalia Rehawi, Yu Wang, Llion Jones, TomGibbs, Tamas Feher, Christoph Angerer, MartinSteinegger, Debsindhu Bhowmik, and Burkhard Rost.2021. Prottrans: Towards cracking the language oflifes code through self-supervised learning. bioRxiv. Ahmed Elnaggar, Michael Heinzinger, Christian Dal-lago, Ghalia Rihawi, Yu Wang, Llion Jones, TomGibbs, Tamas Feher, Christoph Angerer, MartinSteinegger, et al. 2020. Prottrans: towards crackingthe language of lifes code through self-superviseddeep learning and high performance computing.arXiv preprint arXiv:2007.06225. Xiaomin Fang, Fan Wang, Lihang Liu, Jingzhou He,Dayong Lin, Yingfei Xiang, Xiaonan Zhang, HuaWu, Hui Li, and Le Song. 2022. Helixfold-single:Msa-free protein structure prediction by using proteinlanguage model as an alternative. arXiv preprintarXiv:2207.13921. Anirudh Goyal, Abram L. Friesen, Andrea Banino,Theophane Weber, Nan Rosemary Ke, Adria Puig-domenech Badia, Arthur Guez, Mehdi Mirza, KseniaKonyushkova, Michal Valko, Simon Osindero, Tim-othy Lillicrap, Nicolas Heess, and Charles Blundell.2022. Retrieval-augmented reinforcement learning.",
  "Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019b.Billion-scale similarity search with GPUs.IEEETransactions on Big Data, 7(3):535547": "Fusong Ju, Jianwei Zhu, Bin Shao, Lupeng Kong, Tie-Yan Liu, Wei-Mou Zheng, and Dongbo Bu. 2021.Copulanet: Learning residue co-evolution directlyfrom multiple sequence alignment for protein struc-ture prediction. Nature communications, 12(1):19. John Jumper, Richard Evans, Alexander Pritzel, TimGreen, Michael Figurnov, Olaf Ronneberger, KathrynTunyasuvunakool, Russ Bates, Augustin dek, AnnaPotapenko, et al. 2021.Highly accurate pro-tein structure prediction with alphafold.Nature,596(7873):583589. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv:2001.08361.",
  "Erik Nijkamp, Jeffrey Ruffolo, Eli N Weinstein, NikhilNaik, and Ali Madani. 2022. Progen2: exploringthe boundaries of protein language models. arXivpreprint arXiv:2206.13517": "Xiao-Yong Pan, Ya-Nan Zhang, and Hong-Bin Shen.2010. Large-scale prediction of human protein- pro-tein interactions from amino acid sequence based onlatent topic features. Journal of proteome research,9(10):49925001. Sudeep Pushpakom, Francesco Iorio, Patrick A Eyers,K Jane Escott, Shirley Hopper, Andrew Wells, An-drew Doig, Tim Guilliams, Joanna Latimer, ChristineMcNamee, et al. 2019. Drug repurposing: progress,challenges and recommendations. Nature reviewsDrug discovery, 18(1):4158. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, YanDuan, Peter Chen, John Canny, Pieter Abbeel, andYun Song. 2019. Evaluating protein transfer learningwith tape. Advances in neural information processingsystems, 32.",
  "Michael Remmert, Andreas Biegert, Andreas Hauser,and Johannes Sding. 2012. Hhblits: lightning-fastiterative protein sequence searching by hmm-hmmalignment. Nature methods, 9(2):173175": "Alexander Rives, Siddharth Goyal, Joshua Meier, DemiGuo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, andRob Fergus. 2019. Biological structure and functionemerge from scaling unsupervised learning to 250million protein sequences. Proceedings of the Na-tional Academy of Sciences of the United States ofAmerica. Gabriel J Rocklin, Tamuka M Chidyausiku, Inna Gore-shnik, Alex Ford, Scott Houliston, Alexander Lemak,Lauren Carter, Rashmi Ravichandran, Vikram K Mul-ligan, Aaron Chevalier, et al. 2017. Global analysisof protein folding using massively parallel design,synthesis, and testing. Science, 357(6347):168175.",
  "MI Sadowski and DT Jones. 2009.The sequencestructure relationship and protein function prediction.Current opinion in structural biology, 19(3):357362": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2024.Toolformer: Language models can teach themselvesto use tools. Advances in Neural Information Pro-cessing Systems, 36. Martin Steinegger, Markus Meier, Milot Mirdita, Har-ald Vhringer, Stephan J Haunsberger, and JohannesSding. 2019. Hh-suite3 for fast remote homologydetection and deep protein annotation. BMC bioin-formatics, 20(1):115. Ross Taylor, Marcin Kardas, Guillem Cucurull, ThomasScialom, Anthony Hartshorn, Elvis Saravia, AndrewPoulton, Viktor Kerkez, and Robert Stojnic. 2022.Galactica: A large language model for science. arXivpreprint arXiv:2211.09085. Harsh Trivedi, Niranjan Balasubramanian, TusharKhot, and Ashish Sabharwal. 2022.Interleav-ing retrieval with chain-of-thought reasoning forknowledge-intensive multi-step questions.arXivpreprint arXiv:2212.10509.",
  "Lusheng Wang and Tao Jiang. 1994. On the complexityof multiple sequence alignment. Journal of computa-tional biology, 1(4):337348": "Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xi-wen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu,Qi Xie, Bonnie Berger, et al. 2022. High-resolutionde novo structure prediction from primary sequence.BioRxiv, pages 202207. Minghao Xu, Zuobai Zhang, Jiarui Lu, ZhaochengZhu, Yangtian Zhang, Chang Ma, Runcheng Liu, andJian Tang. 2022. Peer: A comprehensive and multi-task benchmark for protein sequence understanding.arXiv preprint arXiv:2206.02096. Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhen-ling Peng, Sergey Ovchinnikov, and David Baker.2020.Improved protein structure prediction us-ing predicted interresidue orientations. Proceedingsof the National Academy of Sciences, 117(3):14961503.",
  "Dani Yogatama, Cyprien de Masson dAutume, andLingpeng Kong. 2021. Adaptive semiparametric lan-guage models. Transactions of the Association forComputational Linguistics, 9:362373": "Chengxin Zhang, Wei Zheng, SM Mortuza, Yang Li,and Yang Zhang. 2020. Deepmsa: constructing deepmultiple sequence alignment to improve contact pre-diction and fold-recognition for distant-homologyproteins. Bioinformatics, 36(7):21052112. He Zhang, Fusong Ju, Jianwei Zhu, Liang He, BinShao, Nanning Zheng, and Tie-Yan Liu. 2021. Co-evolution transformer for protein contact prediction.Advances in Neural Information Processing Systems,34:1425214263. Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, SiyuanCheng, Haosen Hong, Shumin Deng, Jiazhang Lian,Qiang Zhang, and Huajun Chen. 2022. Ontoprotein:Protein pretraining with gene ontology embedding.arXiv preprint arXiv:2201.11147. Wei Zheng, Qiqige Wuyun, Yang Li, Chengxin Zhang,P Lydia Freddolino, and Yang Zhang. 2024. Im-proving deep learning protein monomer and com-plex structure prediction using deepmsa2 with hugemetagenomics data.Nature Methods, 21(2):279289. Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, BianCheng, and Yizhou Yu. 2023. Protein representationlearning via knowledge enhanced primary structurereasoning. In The Eleventh International Conferenceon Learning Representations.",
  "ALimitations and Failed Case Analysis": "One notable limitation of our method RSA is that it is highly dependent on high-quality pre-trainedembeddings and the abundance of protein sequences. We found that our retriever tends to perform betterin a database that has more protein sequences that have not been screened by a clustering algorithm, likeUniclust30. This could be explained by our nearest neighbor retrieval technique which often requires moresimilar sequences for augmentation. We also found different patterns in retrieval sequences from MSAs.Our retriever tends to show polarized retrieval quality, either finding many evolutionary close sequences orfailing to find any homologous sequences. We believe this is due to the imbalanced training of pre-trainedembeddings on different protein families and hope to mitigate this issue with further training on retrievaldatasets.We report other failed cases here for a more thorough view of our proposed method:",
  "Directly applying Accelerated MSAs to MSA-based pre-trained models often shows about 2-3%decrease on downstream performance than using original MSAs. However, Accelerated MSAs are10 times faster": "The performance of RSA improves marginally with more sequences when N > 16. This is becausewe use the softmax distribution over L2 metrics to perform weighting, thereby assigning low weightsto sequences further from the query. We found that in protein folding tasks, performing Average Pooling on ESMFold/AlphaFold showsworse zero-shot performance than Max Pooling with a scoring model. This is due to the misalignmentof protein structures and simple weighting could result in averaging the structures of proteins withdifferent angles of view.",
  "BBroader Impact and Potential Risks": "In this section, we discuss the broader impact of RSA in terms of protein representation learning, de novoprotein understanding, as well as the potential application to large language models.RSA for Protein Representation Learning Developing efficient protein representation learningmethods will significantly improve the ability to analyze complex protein structures, functions, andinteractions. This would lead to a more comprehensive understanding of biological processes at themolecular level, consequently boosting advancements in the fields of bioinformatics and computationalbiology. In this paper, we propose RSA as an efficient and effective protein representation learningmethods, which will spur the development of protein representation learning methods. Notably, ourmethod requires no alignment methods. The traditional alignment process in MSA often requires massCPU engines mostly available to academics. Our method on the other hand only requires a small memoryGPU like 3090Ti and we will publicize our retrieval index, promoting democratic research in this field.RSA for De Novo Protein Understanding We have shown in our work that RSA could perform DeNovo Protein Understanding. This is particularly important for drug repurposing and virtual screeningtasks (Pushpakom et al., 2019) for drug discovery. This can contribute to the development of personalizedmedicine by facilitating the identification of disease-specific protein biomarkers and selecting molecularcures for various diseases. However, de novo protein understanding often relies on newly-designed proteindatabases, which may include sensitive information about individuals, such as their genetic makeup, orviolates intellectual property rights. Ensuring the privacy and security of this data is critical to preventmisuse and protect individual rights RSA as Tool for Large Language Models In addition to the potential impacts in the field of biology,our method could also improve the ability of Large Language Models in biological sequence understanding.Currently, large language models like ChatGPT show difficulty in understanding protein sequences. Weshowcase how RSA could improve this ability with the combination of retrieval and chain of thought. Thisapplication is valuable in education and training, as uses could rapidly learn about proteins through chatmodels, which help educate the next generation of researchers in bioinformatics, computational biology,and related fields. This will lead to a more skilled workforce in the life sciences.",
  "CA Brief Recap on Proteins": "Proteins are the end products of the decoding process that starts with the information in cellular DNA.As workhorses of the cell, proteins compose structural and motor elements in the cell, and they serve asthe catalysts for virtually every biochemical reaction that occurs in living things. This incredible arrayof functions derives from a startlingly simple code that specifies a hugely diverse set of structures. Infact, each gene in cellular DNA contains the code for a unique protein structure. Not only are theseproteins assembled with different amino acid sequences, but they also are held together by differentbonds and folded into a variety of three-dimensional structures. The folded shape, or conformation,depends directly on the linear amino acid sequence of the protein. In fact, this phenomenon is denotedas the sequence-structure-function paradigm. Here we will emphasize four key concepts in proteinunderstanding.1. What are proteins made of ?Amino acids. Within a protein, multiple amino acids are linked together by peptide bonds, therebyforming a long chain. There are 22 alpha-amino acids, from which proteins are composed. We modelthese amino acids in a similar way in NLP, as tokens. A tokenizer breaks the protein sequences into aminoacid tokens that could be modeled by protein language models.2. Protein structuresThere are four levels of structures in protein, as illustrated in :",
  "macromolecules with multiple polypeptide chains or subunits": "Predicting protein structure is an important and difficult task. In this work, we also perform experimentson three tasks secondary structure prediction, protein contact prediction (tertiary structure), and proteinfolding (tertiary structure), with increasing task difficulty.3. Protein Homology Protein homology is defined as shared ancestry in the evolutionary history oflife. There exists different kinds of homology, including orthologous homology that may be similar function proteins across species (human and mice -goblin), and paralogous homology that is the result ofmutations (human -goblin and -goblin). Homologies result in conservative parts in protein sequences,or leads to similar structures and functions.4. Multiple Sequence Alignments A method used to determine conservative regions and find homol-ogous sequences. An illustration () is given here to show how sequences are aligned. Alignedtokens may include the original amino acid, substitution, and deletions. The traditional way to generateMSA is using dynamic programming, with O(LN) complexity. Temporary methods use HMM-HMMalignment, as well as other acceleration methods. HH-Suite3 (Steinegger et al., 2019) reports a timecomplexity of O(NL2), which is still costly when performing alignment on a large database.",
  "Alignment Form indicates whether retrieved sequences are aligned": "Weight Form is the aggregation weight of homologous sequences, as the p(rn|x) in Eq. 3. Herewe denote this weight as n. Traditionally, aggregation methods consider different homologoussequences to be similarly important and use average weighting. MSA Transformer uses a weightedpooling method though the weights of n use global attention and are dependent on all homologoussequences. Aggregation Function is how the representations of homologous sequences are aggregated to theoriginal sequence to form downstream prediction, as in p(y|x, r). For example, considering thesequence classification problem, a fully connected layer maps representations to logits. The retrievalaugmentation probabilistic form first maps each representation to logits p(y|x, rn) and then linearlyweight the logits with n in Eq. 3. Here retriever and alignment are the main bottlnecks of retrieval augmentation methods. The aggregationfunction and weight form are mainly dependent on model architecture and we focus on the first twodimensions in this paper.",
  "EOverview of Previous Protein Representation Augmentation Methods": "Below we introduce several state-of-the-art evolution augmentation methods for protein representationlearning. These methods rely on MSA as input to extract representations. We use x to denote atarget protein and its MSA containing N homologous proteins. We consider MSAs as N alignedprotein homologs r1, . . . , rN. These studies (Yang et al., 2020; Ju et al., 2021) encode MSA as co-evolution statistics features R1...N and aggregate these features to derive the representation, while MSATransformer (Rao et al., 2021; Jumper et al., 2021) perceives MSA as a matrix, employing axial attentionto extract salient evolutionary traits. A unified view of these variants is available in and 3.2 in themain paper.Potts Model (Balakrishnan et al., 2011). This line of research fits a Markov Random Field to the un-derlying MSA with likelihood maximization. This approach is different from other protein representationlearning methods as it only learns a pairwise score for residues contact prediction. We will focus on othermethods that augment protein representations that can be used for diverse downstream predictions.Co-evolution Aggregator (Yang et al., 2020; Ju et al., 2021). One way to build an evolution informedrepresentation is to use a MSA encoder to obtain the co-evolution related statistics. By applying MSAencoder on the n-th homologous protein in the MSA, we can get a total of L d embeddings Rn, eachposition is a d channel one-hot embedding indicating the amino acid type. We use wn to denote the weight",
  "d)Rn(i)WV .(9)": "Knowledge Graph Augmentation (Zhang et al., 2022; Zhou et al., 2023). This line of researchaims at incorporating factual knowledge in protein representations. Different from MSA-based methodsthat draw evolution knowledge from raw protein sequences, these methods are dependent on proteinknowledge graphs that have been annotated by experts, therefore we only provide comparisons with thesemodels in experimental studies and dont incorporate them into our unified framework.",
  "Task NameDataset source#train sequences#test sequences": "Secondary Structure PredictionNetSurfP-2.0 (Klausen et al., 2019)8,678513Contact PredictionProteinNet (AlQuraishi, 2019)25,29940Remote Homology PredictionDeepsf (Hou et al., 2018)12,312718Stability PredictionRocklins Dataset (Rocklin et al., 2017)53,57112,851Subcellular LocalizationDeepLoc (Almagro Armenteros et al., 2017)8,9452,768Protein Protein InteractionPans Dataset (Pan et al., 2010)6,844227Protein FoldingCASP14 (Kryshtafovych et al., 2021)65 Secondary structure prediction (SSP)Task Formulation: 8-class classification oi {0, 1, . . . , 7}Task Description: Secondary structure prediction aims to predict the secondary structure of proteins,which indicates the local structures. This task predicts an 8-class label for each token, indicating whichlocal structure this amino acid belongs to.Task Impact: This task helps to determine whether a model captures protein local structure.Contact prediction (Contact):Task Formulation: 2-class classification (oi, oj) {0, 1}Task Description: Contact prediction predicts the medium-range and long-range (distance >6) residue-residue contact, which measures the ability of models to capture global tertiary structures. Task Impact: This task helps to determine whether a model captures protein tertiary structure. Theassessment of this task focuses specifically on medium- and long-range interactions due to their crucialimportance in the protein folding process.Homology prediction (Homology):Task Formulation: 1195-class classification x {0, 1 . . . 1194}Task Description: Homology prediction aims to predict the fold label of any given protein, which indicatesthe evolutionary relationship of proteins.Task Impact: Protein fold classification is important for both functional analysis and evaluating evolution-ary knowledge.Stability prediction (Stability):Task Formulation: regression x RTask Description: Stability prediction is a protein engineering task, which measures the change in stabilityw.r.t. residue mutations.Task Impact: Evaluate the ability of models to predict protein function as well as evaluate the ability ofmodels to understand mutations, which is crucial for drug discovery and protein engineering.Subcellular Localization (Loc):Task Formulation: regression x {0, 1, . . . , 7}Task Description: Subcellular localization refers to the process of determining the specific location orcompartment within a cell where a particular molecule or protein resides. This information is essentialfor understanding the function and behavior of molecules or proteins, as their subcellular locations oftendictate their roles in cellular processes, interactions with other molecules, and influence on cellularfunctions. For example, proteins on the cell membrane generally have signaling and regulatory functions.Task Impact: This task is closely related to protein functions and roles in biological processes.Protein-Protein Interaction (PPI):Task Formulation: two-class classification (x1, x2) {0, 1}Task Description: Protein-protein interaction predicts whether two proteins interact with each other.Task Impact: This task is crucial for protein function understanding and drug discovery.Protein Folding (Fold):Task Formulation: x S, where S is the 3d-structure of protein, including all coordinates of atoms.Task Description: Protein Folding predicts the structure of protein sequences.Task Impact: This task is known to be challenging, and requires elaborated knowledge of protein localand global structure to make atomic predictions.Dataset Details: We report test results on CASP14 public available targets. We also remove all sequencesover 800 tokens due to the computation memory limit. The reported targets are: T1024, T1025, T1026,T1027, T1028, T1029, T1030, T1031, T1032, T1033, T1034, T1035, T1036s1, T1037, T1038, T1039,T1040, T1041, T1042, T1043, T1045s1, T1045s2, T1046s1, T1046s2,T1047s1, T1047s2, T1048, T1049,T1050, T1051, T1053, T1054, T1055, T1056, T1057, T1058, T1059, T1060s2, T1060s3, T1062, T1063,T1064, T1065s1, T1065s2, T1066s1, T1066s2, T1067, T1068, T1069s1, T1069s2, T1070, T1071,T1072s1, T1072s2, T1073, T1074, T1075, T1076, T1077, T1078, T1079, T1082, T1083, T1084, T1085,T1086, T1087, T1088, T1089, T1090, T1092, T1093, T1094, T1095, T1096, T1098, T1099, T1100,T1101. The blue targets are from CASP14-FM set. gives the details of the datasets for these tasks.De Novo Contact Prediction: We follow Chowdhury et al. (2022) to curate a de novo dataset of108 proteins from Protein Data Bank (Bank). These proteins are originally designed de novo usingcomputationally parametrized energy functions and are well-suited for out-of-domain tests. Note thatdifferent from orphan dataset, MSA can be built for this dataset, though showing a decline in quality.",
  "F.3Model Hyperparameters": "All self-reported models use the same truncation strategy and perform parameter searches on the learn-ing rate among [3e 8, 3e 6, 3e 5, 3e 4, 1e 3], warm-up rate among [0, 0.08], seed among, and batch size among . For evaluation, we choose the best-performing model on the validation set and perform prediction on the test set. The best performinghyperparameters could be found in the file:",
  "F.4RSA and Variants Implementation Details": "F.4.1Retriever Implementation DetailsFirst, we calculate the ESM-1b embeddings of the 44 million sequences in Pfam-A 32.0. We use 16V100 GPUs to calculate the embeddings in a day. A GPU as small as 3090 Ti would be enough, thoughit would take longer. Then, we adopt Faiss (Johnson et al., 2019b) indexing to accelerate the retrievalprocess by clustering the pre-trained dense vectors. In our implementation, we use the Inverted file withProduct Quantizer encoding Indexing and set the size of quantized vectors to 64, the number of centroidsto 4096, and the number of probes to 8. The construction of the Faiss index takes roughly 30 minutesusing 0.5% randomly selected protein embeddings for index training. All embeddings as well as their idare subsequently added to the index.During retrieval, for each query sequence, we first use ESM-1b to calculate its embedding, and thenusing this embedding, we query faiss to find the top N nearest neighbor of this embedding, getting thedistance and sequence id of retrieved sequences. L2 distances are used to measure sequence similarity. F.4.2ProtBERT-RSA Architecture and ImplementationHere we provide the details for ProtBERT-RSA Architecture. An illustration of this process is alsoavailable in . Note that in Step 2 retrieval of Faiss index could be further accelerated with GPU. InStep 4, the predictions of pairwise augmentation could be accelerated with batching on GPU, concurrentlypredicting k augmented sequences at the same time.However, for large pre-trained models and when k is very large, the batch computation may exceedmemory limit. In this case, we provide implementation for gradient accumulation, which calculates lossand gradients for individual prediction (predictionsi) and sum up the gradients with gradient accumulation.This implementation is a convex upperbound for the original loss function and we have validated itsstability. This could also be implemented in batch size n, where each backward iteration calculates k/nretrieval augmentations, achieving trade-off between inference speed and memory limit.",
  ": Detailed illustration of ProtBERT-RSA architecture": "F.4.3RSA for Protein FoldingThe major difference of RSA prediction for protein folding from other tasks is that we use a ranker tochoose the final prediction rather than using weighted pooling. This is due to the misalignment of proteinstructures and simple weighting could result in averaging the structures of proteins with different angles ofview. We train the ranker together with pTM-score loss (Lin et al., 2022) and contrastive loss on a subsetof 1000 randomly chosen proteins from Protein Data Bank. These proteins are distinct from CASP14test set. The ranker takes in the original structure prediction of the protein sequence and the k augmentedpredictions, and generate the highest ranking prediction as the final result. As current protein foldingmodels are very large, we only provide zero-shot testing results on these pre-trained models, withoutfurther finetuning on our pipeline.",
  "Step 3. predictions_i = Model([query, retrieved_seq]), i=1,2,..kStep 4. prediction = Ranker(predictions_i), i=1,2,..k": "Due to the different model architectures of ESMFold and AlphaFold, we explain in details the inferencepipeline of Model([query, retrieved]).ESMFold-RSA ESMFold is a single sequence protein folding model that consists of a protein repre-sentation model and a folding trunk based on the extracted representation. As illustrated in (a),we concatenate query sequence with retrieved sequence and input them into the representation encoder.The encoder combines information from both query and retrieved sequence into query embedding viaself-attention. Then we could use the pre-trained folding trunk to predict the structure of the querysequence. This pipeline could also be accelerated with batch prediction.AlphaFold-RSA Different from ESMFold, AlphaFold encoder takes both single sequence representa-tion and pairwise representation as input. Therefore, as shown in FIgure 10(b), we generate the retrievedstructure encoding with AlphaFold based on retrieved sequences, then we generate the structure of thequery sequence based on the combination of single and pair representation. Note that we removed thetemplate and MSA input in AlphaFold to ablation the effect of RSA. F.4.4Accelerated MSAAccelerated MSA variant explores 165 substituting the discrete retrieval process in MSA with a denseretriever. We implement this method by first retrieving 500 sequences and then aligning these sequenceswith JackHMMer tool. Note that for most tasks we retrieve 500 sequences before alignment, as MSATransformer cant take in many sequences. The command for aligning is:",
  "G.1Comparison of the Running time between RSA vs MSA": "A severe speed bottleneck limits the use of previous MSA-based methods. In this part, we add analysison database construction time as well as give details for inference time calculation. We calculate thetotal time used in each retrieval inference by summing: alignment time and retrieval time, as shown in. Alignment time is the time used when finding MSA sequences through alignment and aligningfound sequences with HHblits. Retrieval time is the time used during dense retrieval, including calculatingthe embedding of the query sequence with GPU. It is notable from the figure that alignment itself is acomputationally costly procedure.Also, MSA is limited by its cubersome construction of retrieval HHM profile to perform HHM-HHMsearch. We follow the MSA custom database construction process in HHblits and compare with theconstruction time for RSA on a single V100 GPU (batch size=1) on a database of 10000 protein sequences.As shown in , our method use only 10 minutes to finish the construction, though building aprofile requires more than 3200 minutes.",
  "G.2Case Study": "We cherry-picked one example of ProtBERT and ProtBERT-RSA on homology prediction (1195 classclassification task) to showcase the interpretability as well as give intuition on our method. As shown in, our method takes the original sequence as well as 16 retrieved sequences for prediction. Afterweighted summing of all predicted results, the prediction of probability on ground truth label increase andgives the correct prediction. We checked the most highly weighted (top 5) retrieved sequences, all fiveproteins are Colicins, which is a family under Toxins membrane translocation domains. We can see fromthe case that weighting by distance helps the model focus on more similar retrieved instances.We also provide two case studies on how RSA improves ESMFold. For target T1055, a DNApolymerase processivity factor,RSA retrieves A0A1A8WBQ9_9APIC, A0A1Y4NGW6_9FIRM,A0A4V4NFM9_9ASCO,A0A1D3TXL7_9FIRM,A0A0V0QX86_PSEPJ,A9KN76_LACP7,A0A162CB07_9CRUS, A0A369KX60_9PROT,SKI2_SCHPO, and the highest ranking augmenta-tion prediction is from (T1055, A0A1A8WBQ9_9APIC). A0A1A8WBQ9_9APIC is a Merozoite surfaceprotein. Merozoite surface protein 7 (MSP7) is a protein of the malaria parasite that has been found tobe associated with processed fragments from the MSP1 protein in a complex involved in red blood cell : Illustration of speed up by RSA retrievalcompared to MSA on secondary structure predic-tion dataset with 8678 sequences. Accelerated MSArefers to the MSA Transformer with MSA sequencesretrieved by our RSA retriever.",
  ": Case study on homology prediction": "invasion. A0A1A8WBQ9_9APIC is a Merozoite surface protein C-terminal domain-containing proteinthat is related to DNA polymerase processivity factor through its requirement of a host factor, E. colithioredoxin, in order to carry out its function. They also show similar structures with a TM-score of 0.42.For target T1039, a virion RNA polymerase of crAss-like phage, RSA retrieves A0A078ATM6_STYLE,A0A1D8P931_9FLAO,A0A363CW97_9PROT,D7JGI7_9BACT,A0A0B3VPN2_9FIRM,A0A1E4TQ27_PACTA,A0A1M6KY55_9FLAO,A0A1X7R9D3_9SACH,A0A0R1SCS6_9LACO,A0A367GMI1_9SPHI,A0A2N1F639_9FLAO,A0A0D6TLE8_9FLAO,A0A3N4NFZ1_9FLAO,A0A1D2VEI9_9ASCO, A0A1L7I7H7_9FLAO, A0A1R0FA92_9RHIZ. The highest ranking augmentationprediction is from (T1039, A0A078ATM6_STYLE). A0A078ATM6_STYLE is a COMM domain-containing protein 1. It has no distinct functional relationship with T1039, though the second chain of thisprotein has a similar structure to T1039, with a TM-score of 0.34. : Structure Prediction for T1055, Cyan isthe color for Ground truth. Pink is the color for ESM-Fold. Pink is the color for ESMFold. Light purpleis the color for ESMFold-RSA. The TM-score forESMFold is 0.70, and the TM-score for ESMFold-RSA is 0.91.",
  "G.3Domain Adaptation Analysis": "In this section, we perform additional analysis on the domain adaptation ability on secondary structureprediction tasks. We perform training on NetSurfP-2.0(Klausen et al., 2019) training set and test on twodatasets with domain gaps. On CASP12, RSA marginally outperforms other baselines, as shown in . We also test on 10 de novo proteins (6YWC, 2LUF, 7BPM, 7BPL, 7CBC, 1FSD, 1IC9, 5JI4, 5KWO,6W6X). Since we didnt find secondary structure labels for these proteins, we provide visualization in, which shows that our model has an obvious overhead over MSA Transformer on predictinggeometric components.",
  "G.4Comparison of Accelerated MSA vs MSA quality": "Accelerated MSA performs worse than original MSA when directly applied to MSA Transformer, as wellas AlphaFold. In this section, we showcase successful and failed cases in AlphaFold and compare thecoverage of two kinds of MSA.As shown in , AlphaFold prediction is closely correlated to the coverage of MSA sequence.On cases where dense retriever fails to find a wide coverage of homologous sequences, AlphaFoldperformances drop starkly. Note that the MSA is implemented as ColabFold (Mirdita et al., 2022), using",
  "G.5Interpretability of RSA": "In addition to analysis on interpretability in 5.7 in the main paper, we provide further analysis of theinterpretability of RSA in terms of homology and structures.Retrieval rank does not necessarily corresponds to the sequence closest to the query sequencetoken-wise. As shown in , we calculate and rank the E-value of Top-32 retrieved proteinsequences in CB513 dataset. We then calculate the average rank for the 1st, 2nd,... 32nd proteins in thedataset. It shows that the top-1 protein only has an average rank of 11, indicating that the retrieval rankdoes not necessarily corresponds to the sequence closest to the query sequence token-wise. Therefore,using dense retrieval, our retrieved results are diverse in sequences, though close to the query sequencesin properties.Visualization on Retrieval Structural Similarity As shown in , we random picked a fewmore examples to illustrate the structural similarity between query protein and retrieval proteins. From thevisualization, we can see that retrieved proteins exhibit similar structure or structure components, whichcould be used to boost structural knowledge.",
  "G.6ProteinChat: RSA Empowers ChatGPT on Protein Understanding": "In the section, we point out a direction for our future work. RSA can not only be used on small-scalerepresentation learning model, it can also augment large language models, e.g. ChatGPT. Currently,even GPT4 model shows limited understanding of biological sequences, especially long sequences likeproteins. Galactica (Taylor et al., 2022) pretrains LLM on scientific text and biological sequences, however,calibration on the nature of these sequences are ubiquitous. Here, we propose a new method to enableChatGPT to understand protein sequences, ProteinChat, by directly applying the protein dense retrieverto ChatGPT. We implement ProteinChat by binding ChatGPT with four tools,",
  "PubMed Search, that facilitates elaborated enquiry into related scientific articles": "ProteinChat is implemented similar to ReAct agent (Yao et al., 2022), which views ChatGPT as anagent that uses chain-of-thoughts to break down a difficult task into parts that could be accomplished byavailable tools. Here we show several demos of ProteinChat.Demo 1 (using Interpro + Wiki Search + RSA): Question: Which family does this protein MSLNAEQKAKVVLEHGSSAHDTGSTEVQVALLTLRINDLQKHFLEHKKDHHSRRGLLRMVSQRRKLLDYLKKRNISKYTDLIQSLGLRK be-longs to? (Ground truth: Belongs to the universal ribosomal protein uS15 family.) Thought: I need to find out the functional and evolutionary annotations of this protein.Action: Protein RetrievalAction Input: MSLNAEQKAKVVLEHGSSAHDTGSTEVQVALLTLRINDLQKHFLEHKKD-HHSRRGLLRMVSQRRKLLDYLKKRNISKYTDLIQSLGLRK",
  "Observation:Here are 4 homologs for this protein separated by \",\":B4D3S0_9BACT,RS15_HAMD5, K6ZQA3_9ALTE, A0A1H6FC85_9GAMM": "Thought: I need to use the entry labels to search the InterPro database.Action: InterPro Annotation, Action Input: B4D3S0_9BACTObservation: This is a 30S ribosomal protein S15. It is found in organism Chthoniobacter flavusEllin428.It has molecular function that structural constituent of ribosome. It has a biological processtranslation. It has cellular component ribosome. Thought: I need to search Wikipedia to find out which family this protein belongs to.Action: Wikipedia Search, Action Input: 30S ribosomal protein S15Observation: Page: Ribosomal protein Summary: A ribosomal protein (r-protein or rProtein)is any of the proteins that, in conjunction with rRNA, make up the ribosomal subunits involved inthe cellular process of translation. E. coli, other bacteria and Archaea have a 30S small subunit anda 50S large subunit, whereas humans and yeasts have a 40S small subunit and a 60S large subunit.Page: 40S ribosomal protein S30 Summary: 40S ribosomal protein S30 is a protein that in humansis encoded by the FAU gene.",
  "Observation: Here are 4 homologs for this protein separated by \",\":A0A315V8B8_GAMAF,A0A673GQT0_9TELE ,A0A2K6SCP9_SAIBB ,A0A674GDP2_TAEGU": "Thought: I need to find out what this protein doesAction: InterPro Search, Action Input: A0A315V8B8_GAMAFObservation: This is a 40S ribosomal protein S19. It is found in organism Gambusia affinis(Western mosquitofish). Thought: I need to find out if this protein binds with RNAAction: Wikipedia Search, Action Input: 40S ribosomal protein S19 binds RNAPage: 40S ribosomal protein S19 Summary: 40S ribosomal protein S19 is a protein that inhumans is encoded by the RPS19 gene. Page: 5S ribosomal RNA Summary: The 5S ribosomalRNA (5S rRNA) is an approximately 120 nucleotide-long ribosomal RNA molecule with a mass of40 kDa. It is a structural and functional component of the large subunit of the ribosome in all domainsof life (bacteria, archaea, and eukaryotes), with the exception of mitochondrial ribosomes of fungiand animals. The designation 5S refers to the molecules sedimentation velocity in an ultracentrifuge,which is measured in Svedberg units (S). Page: Ribosomal protein Summary: A ribosomal protein(r-protein or rProtein) is any of the proteins that, in conjunction with rRNA, make up the ribosomal subunits involved in the cellular process of translation. E. coli, other bacteria and Archaea have a 30Ssmall subunit and a 50S large subunit, whereas humans and yeasts have a 40S small subunit and a60S large subunit. )",
  "Observation: Homologs of this protein has attributes: Iron_Traffic with importance 0.8876,E1_UFD with importance 0.1123": "Thought: I need to find out more information about iron trafficAction: PubMed Search, Action Input: iron trafficObservation: Title: Moving Fe2+ from ferritin ion channels to catalytic OH centers depends onconserved protein cage carboxylates Abstract: ... (too long, abbrevited)Title: L-ferritin binding to scara5: a new iron traffic pathway potentially implicated in retinopathyAbstract: ... (too long, abbrevited)Title: Ceruloplasmin-ferroportin system of iron traffic in vertebratesAbstract: ... (too long, abbrevited)Thought: I now know enough to write a detailed protocol.Final Answer: Based on the information gathered from the protein sequence and the relevantliterature, it appears that the protein is potentially involved in iron trafficking. Iron trafficking iscrucial for various cellular processes, and the ceruloplasmin-ferroportin system represents one ofthe major iron traffic pathways in vertebrates. Ferritin is an iron storage protein that has also beensuggested as an iron carrier. The Scara5 receptor, which is specific for L-ferritin, has been found invarious tissues, including the retina. To measure the function of this protein, we can design a protocol that investigates the proteinsrole in iron transport and its interaction with other proteins involved in iron trafficking, such asceruloplasmin, ferroportin, and Scara5. Heres a detailed protocol: 1. Protein expression and purification: Clone the gene encoding the protein of interest into anappropriate expression vector. Transform the vector into a suitable host, such as E. coli, and induceprotein expression. Purify the expressed protein using affinity chromatography and other relevantpurification techniques. 2. Iron-binding assay: Assess the ability of the purified protein to bind iron using a colorimetriciron-binding assay, such as the ferrozine assay. Measure the absorbance of the protein-iron complexat the appropriate wavelength to determine the iron-binding capacity of the protein. 3. Protein-protein interaction studies: Investigate the interaction between the protein of interestand other proteins involved in iron trafficking, such as ceruloplasmin, ferroportin, and Scara5. Thiscan be done using techniques like co-immunoprecipitation, pull-down assays, or surface plasmonresonance (SPR). Cellular iron transport assay: Transfect cells with a plasmid encoding the proteinof interest and perform an iron transport assay using radiolabeled iron (e.g., 59Fe). Measure the rateof iron uptake and release by the cells expressing the protein of interest compared to control cells. 4. Functional studies in animal models: Assess the in vivo function of the protein by generatinga knockout or transgenic animal model. Evaluate the iron status of the animals (e.g., serum iron,transferrin saturation, and ferritin levels) and examine the expression of other proteins involvedin iron trafficking (e.g., ceruloplasmin, ferroportin, Scara5) in the knockout or transgenic animalscompared to control animals. 5.Histological analysis: Examine the localization and distribution of the protein in various tissues,especially the retina, using immunohistochemistry. Determine the impact of the protein on retinaliron levels and assess the potential involvement of the protein in retinopathy."
}