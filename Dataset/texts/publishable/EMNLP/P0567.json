{
  "Abstract": "Aspect Sentiment Triplet Extraction (ASTE)aims to co-extract the sentiment triplets in agiven corpus. Existing approaches within thepretraining-finetuning paradigm tend to eithermeticulously craft complex tagging schemesand classification heads, or incorporate exter-nal semantic augmentation to enhance perfor-mance. In this study, we, for the first time,re-evaluate the redundancy in tagging schemesand the internal enhancement in pretrained rep-resentations. We propose a method to improveand utilize pretrained representations by in-tegrating a minimalist tagging scheme and anovel token-level contrastive learning strategy.The proposed approach demonstrates compara-ble or superior performance compared to state-of-the-art techniques while featuring a morecompact design and reduced computationaloverhead. Additionally, we are the first to for-mally evaluate GPT-4s performance in few-shot learning and Chain-of-Thought scenariosfor this task.The results demonstrate thatthe pretraining-finetuning paradigm remainshighly effective even in the era of large lan-guage models. The codebase is available at",
  "Introduction": "Aspect-Based Sentiment Analysis (ABSA) aimsto jointly extract opinion terms, aspect terms (tar-gets of the corresponding opinions), and their spe-cific sentiment polarities in a given corpus. Inthe milestone research by Peng et al. (2020), thecompound ABSA subtasks were consolidated intothe Aspect Sentiment Triplet Extraction (ASTE)task framework. For each input corpus, ASTEoutputs triplets in the form (Aspect, Opinion,Polarity), where the Aspect term is the target orentity being discussed, the Opinion term is the sen-timent or opinion expressed about the aspect, andPolarity indicates whether the opinion is positive,",
  "Aspect TermPositive Opinion TermNegative Opinion Term": ": An illustration for ASTE, given the sentence\"Bob Dylan is a great rocker, despite the broken CDs.\",there are three triplets to be extracted: (Bob Dylan,great, positive), (rocker, great, positive),(CDs, broken, negative). As an emerging fine-grained sentiment analysisinitiative, ASTE offers a more detailed and nuancedunderstanding of sentiments in text compared totraditional methods that provide only an overall sen-timent score (Peng et al., 2020). This aspect-levelstructured approach is inherently more challenging.Previous approaches to ASTE have generally fol-lowed two paradigms: Pipeline methods and JointTagging methods (Zhang et al., 2022a). Pipelinemethods decompose the ASTE task into multi-ple sequential subtasks, often suffering from er-ror propagation (Xu et al., 2020). Recent progressin Machine Reading Comprehension (MRC) alsocontributes to this paradigm (Zhai et al., 2022;Mao et al., 2021; Zou et al., 2024; Chen et al.,2021b). Joint tagging methods adopt a unified tag-ging scheme to extract all triplet elements in onestage (Xu et al., 2020). The key idea is to design atagging scheme (Zheng et al., 2017) that simulta-neously predicts aspect terms, opinion terms, andsentiment polarities. Further developments haveintroduced a Grid Tagging Scheme (GTS) to rep-resent the triplets on a unique 2D table(Wu et al.,2020a; Zhang et al., 2022b; Chen et al., 2021b,2022; Fei et al., 2022).Recent advances in these approaches have beenfocusing on the classification head design (Chen et al., 2022; Zhang et al., 2022b) and external se-mantic information enhancing (Chen et al., 2021b,2022; Fei et al., 2022; Jiang et al., 2023; Iswariet al., 2024). However, existing research has ne-glected the synergistic optimization within the jointtagging scheme and the integration of contextualword representations. In this research, we proposea method to effectively improve and utilize the rep-resentation capabilities of pretrained encoders inABSA by integrating a minimalist tagging schemeand a novel token-level contrastive learning ap-proach.The proposed approach demonstrates compara-ble or superior performance in comparison to state-of-the-art techniques, while featuring a more com-pact design and reduced computational overhead.Notably, even in the era of Large Language Models(LLMs), our method exhibits superior effectivenesscompared to GPT 3.5 and GPT 4 in both few-shotand Chain-of-Thought (Wei et al., 2022) learningscenarios. This study provides valuable insights forthe advancement of ASTE techniques within theparadigm of LLMs. Overall, our contributions aresummarized as follows:",
  ". Minimalist Grid Tagging Scheme: We pro-pose a novel minimalist joint tagging schemethat uses the fewest label classes to date": "2. Token-level Contrastive Learning Strategy:We introduce a token-level contrastive learn-ing framework that enhances the contextualembeddings produced by the pretrained model.This framework is seamlessly geared towardsour minimalist Grid Tagging Scheme (GTS)to effectively address the ASTE task. 3. Comprehensive Evaluation: We conduct ex-tensive experiments and evaluations on mul-tiple benchmark datasets, demonstrating theeffectiveness and superiority of our proposedmethods over existing approaches. Notably,we are the first to reveal GPT-4s performanceon this task, showcasing our methods supe-rior efficiency and effectiveness in the era oflarge language models.",
  "Peng et al. (2020) proposed a pipeline methodthat divides ASTE tasks into two stages: initiallyextracting (Aspect, Opinion) pairs and subse-quently predicting sentiment polarity. However,": "pipeline methods typically suffer from error prop-agation issues (Xu et al., 2020). Recent pipelinemethods treat ASTE as a Machine Reading Com-prehension problem, and develops seq2seq meth-ods such as machine reading comprehension (Zhaiet al., 2022; Mao et al., 2021; Zou et al., 2024;Chen et al., 2021b). Joint Tagging strategies areremarked by certain Unified Tagging Scheme de-signs, where elements of a triplet can be extractedsimultaneously. ET (Xu et al., 2020) introduced aposition-aware tagging scheme with a conditionalrandom field module, effectively addressing spanoverlapping issues. Recent joint paradigm methodshave refined the ASTE task with the developmentof proficient Grid Tagging Schemes (GTS).",
  "Grid Tagging Scheme": "Wu et al. (2020a) pioneered the adoption of agrid tagging scheme (GTS) for ASTE, yieldingsubstantial performance gains.Subsequent re-search refined and enhanced GTS. BDTF (Zhanget al., 2022b) designed a boundary-driven taggingscheme, effectively reducing boundary predictionerrors. Alternative research augmented GTS byintegrating external semantic information as struc-tured knowledge into their models. S3E2 (Chenet al., 2021b) retained the GTS tagging schemewhile introducing novel semantic and syntacticenhancement modules between word embeddingoutputs and the tagging scheme. EMGCN (Chenet al., 2022) incorporated external knowledge fromfour areasPart-of-Speech Combination, Syntac-tic Dependency Type, Tree-based Distance, andRelative Position Distancethrough an exogenoushard-encoding strategy. SyMux (Fei et al., 2022)contributed a unified tagging scheme capable ofhandling all ABSA subtasks by integrating insightsfrom GCN, syntax encoders, and representationmultiplexing.",
  "Contrastive Learning": "While contrastive learning has gained popularityin diverse NLP domains (Wu et al., 2020b; Giorgiet al., 2021; Gao et al., 2021; Zhang et al., 2021),its application to ASTE remains relatively unex-plored. Ye et al. (2021) adopts contrastive learninginto triplet extraction in a generative fashion. Wanget al. (2022) takes contrastive learning as a dataaugmentation approach. Yang et al. (2023) pro-posed an enhancement approach in pairing withtwo separate encoders.",
  "Overall Framework": "An overall description of the training process canbe found in . Basically our design canbe break down into the Minimalist Grid TaggingScheme (GTS) and the Token-level ContrastiveLearning Strategy.Tokenize the input sequence S using the Tok-enizer Tk and pass the tokenized sequence throughthe Pretrained Language Model PLM (such asBERT) to obtain contextualized representations h:h = PLM (Tk (S)) .(1)Then, the inference phase involves with formingthe Minimalist GTS and predicting the correspond-ing class for each cell. Once the GTS is predicted, itcan be decoded by the GTS decoder into the tripletsin natural language form. The training phase ad-ditionally introduces a novel contrastive learningstrategy, where similar and dissimilar pairs of con-textual representations are distinguished. The con-trastive loss is then weighted and summed withthe tagging loss, which is the classification lossbetween the predicted and ground truth taggingschemes.Our research benefits from the following twoclosely intertwined aspects: 1) The use of the Min-imalist GTS simplifies the learning process by re-ducing the number of labels, facilitating faster con-vergence and seamlessly gearing the contrastivelearning. 2) The token-level contrastive learningenhances the models ability to distinguish betweenrelated and unrelated elements within the input se-quence, thereby improving the overall accuracy ofthe tagging system. For a more detailed descriptionfor our algorithm pipeline, see the pseudo code inAppendix A.1.",
  "Tagging Scheme Design": "As defined by .1, once an input sentenceis encoded into a sequence of contextual represen-tations h = {h1, h2, ..., h|h|}, we form a |h| |h|matrix, that is, our tagging scheme tag|h||h|. Asshown in , on the rows we mark Aspecttokens by yellow and the columns we mark Opin-ion tokens by green (positive) and blue (negative).Then the each intersection of these marked rowsand columns can uniquely represent an identicalsentiment triplet. Thus, each such triplet can benoted by a 2-D area (submatirx) in the matrix,",
  ": An overview of the proposed method, wherethe Encoder denotes for the sequential combination ofa Tokenizer and a Pretrained Language Model (PLM)": "where Sentiment Polarity is indicated with POS.(positive), NEU. (neutral), or NEG. (negative) in thetop-left corner cell of the area, while CTD. indicatesthe continuation of the pairing relationship withinthe same region. MSK. (mask) on the diagonal rep-resents masked cells that are not involved in thecomputation. In , an example sentence istokenized and tagged. By defining our grid tagging scheme, we framethe triplet extraction problem as a 5-class classi-fication task, using the fewest number of labelsknown to date. In Appendices A.3 - A.5, we pro-vide rigorous proof and heuristic insights to justifyour design and ensure its rationality.",
  "We adopt a tagging loss to guide neural networklearning": "We concatenate the representation with its trans-posed form to construct a matrix. Then, we ap-ply the classification head Cls to the embeddings,followed by the softmax function, to obtain thepredicted classification probabilities for each cell:tagi,j = softmax(Cls(hi; hTj )), i, j = 1, ..., |h|.(2) The focal loss (Lin et al., 2017) is employed tomitigate class imbalance by placing greater em-phasis on examples that are difficult to classifycorrectly. This is achieved by down-weighting theloss for well-classified instances and focusing moreon misclassified instances. The formula for focal",
  "#sCTDCTDMSK": ": The grid tagging scheme employs the fewestclasses of labels while completely handle all the tripletcases without conflict, overlap or omission. Each areacircled in red dashed lines corresponds to a triplets. Forexample, intersection area between columns of \"broken\"and rows of \"CDs\" is marked as negative, with NEG. onits top-left cell and CTD. for others. It is worth mention-ing that the blank cells in the matrix are labeled as anadditional class but are omitted for visual simplicity.",
  "Contrastive Learning Label Matrix": "Contrastive learning is an unsupervised learningmethod that aims to learn effective feature embed-dings by pulling together similar pairs of samplesand pushing apart dissimilar pairs. In our design,we construct a label matrix where each cell is an-notated by either PULL or PUSH, which meansmaking the representations closer among tokenswithin the same class and farther between those ofdifferent classes. See an illustration of this strategyin .",
  "#sMSK. MSK. MSK. MSK. MSK. MSK. MSK. MSK. MSK. MSK. MSK": ": An illustration for the Contrastive Mask.Each token is paired with every other token, where PULLdenotes positive sample pairs, indicating that the tokensbelong to the same category and should be pulled closertogether, while PUSH denotes negative sample pairs, in-dicating that the tokens belong to different categoriesand should be pushed apart. The lower triangular partof the matrix, marked by MSK. are masked cells that arenot involved in the computation. For example, \"Bob\"and \"Dylan\" are marked as a positive sample pair withPULL, indicating similarity, while \"Bob\" and \"is\" aremarked as a negative sample pair with PUSH, indicatingdissimilarity.",
  "Implementation Details": "All experiments were performed on a single RTX2080 Ti. The best model weight on the devel-opment set is saved and then evaluated on thetest set.For the PLM encoder, the pretrainedweights bert_base_uncased and roberta_baseare downloaded from (Wolf et al., 2020). GPT 3.5-Turbo and GPT 4 are implemented using OpenAIAPI (OpenAI, 2024). The learning rate is 1 105",
  "Datasets": "We evaluate our method on two canonical ASTEdatasets derived from the SemEval Challenges(Pontiki et al., 2014, 2015, 2016). These datasetsserve as benchmarks in most aspects-based senti-ment analysis (ABSA) research. The first dataset,denoted as D1, is the Aspect-oriented Fine-grainedOpinion Extraction (AFOE) dataset introduced by(Wu et al., 2020a). The second dataset, denoted asD2, is a refined version by (Xu et al., 2020), build-ing upon the work of (Peng et al., 2020). Moredetails are provided in Appendix A.2.",
  "ASTE Performance": "4.4.1Comparison to Existing MethodsWe evaluate ASTE performance using the widelyaccepted (Precision, Recall, F1) metrics. Re-sults of the dataset D2 are shown in , whilethe results of D1 are presented in Appendix . The best results are highlighted in bold, and thesecond-best results are underscored. Our proposedmethod consistently achieves state-of-the-art per-formance or ranks second in most evaluated cases.Notably, on dataset D1, the proposed methodachieves a substantial 3.08% improvement in F1score on the 14Lap subset. This improvement is particularly significant given that the highest scoreon this subset is the lowest among all datasets,showcasing our methods effectiveness in handlingchallenging instances. Moreover, on the 14Res sub-set, our F1 score exceeds 76.00, which, to the bestof our knowledge, is the highest reported perfor-mance. For dataset D2, our method outperformsall state-of-the-art approaches by over 1 percent-age point on the 14Res, 14Lap, and 16Res subsets.Only on the 16Res subset does the BDTF method(Zhang et al., 2022b) achieve a slightly better per-formance.",
  "Comparison to GPT": "Our proposed method is based on the Pretrain-Finetuning paradigm, which is increasingly chal-lenged by large language models (LLMs) (Kojimaet al., 2022; Wei et al., 2021). It is concerned abouthow the advancing capabilities of LLMs might im-pact the ASTE task.When compared to advanced LLMs, the perfor-mance and computational efficiency of our methodstand out. As shown in Tables 8, 1, and 9, even thestate-of-the-art LLM, GPT-4, with its staggeringnumber of parameters, does not achieve satisfac-tory results for ASTE, even with few-shot learningand Chain-of-Thought (CoT) (Wei et al., 2022) en-hancement. Additionally, using LLMs introducessignificant computational overhead. For more in-formation on experiment setting see Appendix A.9.For detailed results see , 12, Appendix A.8and Appendix A.10. Note that, fine-tuning LLMsmay offer some improvements, but it also riskscatastrophic forgetting (Shi et al., 2024) and is leftfor future work.To our knowledge, this is the first formal studyto evaluate GPT-4s performance on these ASTEdatasets, providing valuable insights for future re-search.",
  "Performance on Other ABSA Tasks": "Our method can also effectively handle otherABSA subtasks, including Aspect Extraction (AE),Opinion Extraction (OE), and Aspect OpinionPair Extraction (AOPE). AE aims to extract allthe (Aspect) terms, OE aims to extract all theOpinion terms, and AOPE aims to extract all the(Aspect, Opinion) pairs from raw text. The re-sults for these tasks are presented in Appendix A.7,where our method consistently achieves best F1-scores across nearly all tasks.",
  "Sequence-taggingSpan-BART (Yan et al., 2021)65.5264.9965.2561.4156.1958.6959.1459.3859.2666.6068.6867.62JET (Xu et al., 2020)70.5655.9462.4055.3947.3351.0464.4551.9657.5370.4258.3763.83": "Seq2seqDual-MRC (Mao et al., 2021)71.5569.1470.3257.3953.8855.5863.7851.8757.2168.6066.2467.40BMRC (Chen et al., 2021a)72.1765.4368.6465.9152.1558.1862.4855.5558.7969.8765.6867.35COM-MRC (Zhai et al., 2022)75.4668.9172.0162.3558.1660.1768.3561.2464.5371.5571.5971.57Triple-MRC (Zou et al., 2024)--72.45--60.72--62.86--68.65 Table-fillingGTS (Wu et al., 2020a)67.7667.2967.5057.8251.3254.3662.5957.9460.1566.0866.9167.93Double-encoder (Jing et al., 2021)67.9571.2369.5562.1256.3859.1158.5560.0059.2770.6570.2370.44EMC-GCN (Chen et al., 2022)71.2172.3971.7861.7056.2658.8161.5462.4761.9365.6271.3068.33BDTF (Zhang et al., 2022b)75.5373.2474.3568.9455.9761.7468.7663.7166.1271.4473.1372.27STAGE-1D (Liang et al., 2023)79.5468.4773.5871.4853.9761.4972.0558.2364.3778.3869.1073.45 STAGE-2D (Liang et al., 2023)78.5169.373.6170.5655.1661.8872.3358.9364.9477.6768.4472.75STAGE-3D (Liang et al., 2023)78.5869.5873.7671.9853.8661.5873.6357.964.7976.6770.1273.24DGCNAP (Li et al., 2023)72.9068.6970.7262.0253.7957.5762.2360.2161.1969.7569.4469.58 LLM-basedGPT-3.5-turbo zero-shot44.8855.1349.4830.0441.0434.6936.0253.4043.0239.9257.7847.22GPT-3.5-turbo few-shot51.5165.1957.5539.7950.0944.3543.3463.0951.3951.1271.0159.45GPT-3.5-turbo CoT48.4759.0553.2430.4840.3034.7139.5156.7046.5744.0363.8152.10GPT-3.5-turbo CoT+few-shot49.4159.1553.8533.7842.3337.5739.0256.0846.0246.4966.9354.86GPT-4o zero-shot32.9938.1335.3717.8122.5519.9027.8537.7332.0532.1743.0036.80GPT-4o few-shot54.1166.2059.5538.2348.6142.8045.5760.4151.9552.9071.0160.63GPT-4o CoT41.2153.3246.4926.9837.7131.4633.0750.9340.1039.1458.1746.79GPT-4o CoT+few-shot46.8159.8652.5429.7140.8534.4035.0853.8142.4741.5361.0949.45",
  "OursMiniConGTS76.175.0875.5966.8260.6863.6166.5063.8665.1575.5274.1474.83": ": Experimental results on D2 (Xu et al., 2020). The best results are highlighted in bold, while the secondbest results are underscored. The results with are retrieved from (Yu Bai Jian et al., 2021). The results with areretrieved from (Xu et al., 2020). The results with are retrieved from (Peng et al., 2020). The results with areretrieved from (Mao et al., 2021).",
  "Ablation Study": "In this section, we conduct a series of ablationexperiments to demonstrate the superiority of ourmethod and eliminate potential confounding fac-tors. Experiments were conducted on the D1 andD2 datasets, using F1 scores as the comparisonmetric.Encoder. We replaced the RoBERTa encoder withBERT, resulting in a slight decrease in F1 scoreson both datasets, although our method still outper-formed most other approaches.Contrastive Learning. We deactivated the con-trastive mechanism in our method (denoted as w/o.contr) by setting the coefficient of the contrastiveloss to 0. The results in illustrate a signif-icant F1-score decrease of 2.12 7.29% in bothdatasets.Tagging Scheme. We substituted our proposedscheme with the conventional GTS tagging scheme (Wu et al., 2020a), resulting in a substantial perfor-mance decline () of 4.68 9.18%. Thisindicates that the contrastive learning methods,within our framework, is of strong reliance on anappropriate tagging scheme. This reinforces theeffectiveness of our compact yet impactful taggingscheme.",
  "Effect of Contrastive Learning": "In , an example is shown illustratinghow contrastive learning improves representation.The right upper-row subplots show the represen-tation outputs with contrastive learning, while thelower row subplots display that without contrastivelearning. Principal Component Analysis (PCA)(Mackiewicz and Ratajczak, 1993) is used to re-duce the vector dimensions to three for visualiza-tion purposes. The distributions indicate that con-trastive learning significantly enhances the repre-sentations, with similar classes of hidden word rep-resentations becoming more tightly clustered anddissimilar classes more distinct.",
  "Efficiency Comparison": "We compared the computational efficiency of Mini-ConGTS with other approaches, including baselineASTE methods and LLMs, on an ASTE task. Eval-uation metrics such as memory usage, number ofparameters, epoch/inference time, and F1 scoresare recorded in . Our approach not only re-quires less memory usage for higher performancecompared to traditional ASTE methods but alsooffers much faster runtime even using a relativelylower-cost GPU. provides another comparative analysisof tagging schemes. Our method has a compactdesign with the fewest classes of labels. Whatsmore, compared with other SOTA baselines, ourmethod does not rely on any additional linguisticinformation enhancement.",
  "Case Study": "A case analysis is presented in , where theproposed method demonstrates solid performance.Despite minor faults in missing the full terms, itexhibits a profound understanding of the case.It is quite interesting to investigate the GPTs er-ror cases. The findings reveal that the performanceof the GPT model is mixed - while it is able toidentify more aspect-opinion (A-O) pairs than theground truth annotations in some cases, this comesat the cost of a decreased precision. This suggests that the GPT model may be over-interpreting theinput, making inferences that go beyond what isstrictly supported by the text. Furthermore, theGPT model appears to be overly sensitive to thepresence of adverbs (such as very, a bit, etc.)in the input. This sensitivity manifests in the modelfrequently adding or removing adverbs when ex-tracting the Opinion components, which furthercontributes to a decrease in the overall accuracy ofthe ABSA task.These findings highlight the importance of de-veloping ABSA models that can strike the right bal-ance between extracting all relevant aspect-opinionpairs, while still maintaining a high degree of pre-cision. The effective use of encoding appears tobe a promising direction for achieving this balanceand advancing the state-of-the-art in Aspect-BasedSentiment Analysis.",
  "Limitations": "Our method is based on a 2D-matrix taggingscheme, where the time complexity for decod-ing, given the input corpus length N, is O(N2).This may be unacceptable when N is too large.Additionally, although we have demonstrated ourmethod on commonly used classic English datasets,it should be tested on more natural corpora and forits cross-language capability.",
  "Ethics & Potential Risks Statement": "In our experiments, we used widely accepteddatasets focused on e-commerce reviews, whichhave a lower risk of offensive content. We scruti-nized the data for biases against gender, race, andmarginalized groups. Despite these precautions,our model might still generate potentially offen-sive sentiment assessments if used inappropriately,such as evaluating ethical or moral statements. Wereserve the right to limit or modify the use of ourtechnology to prevent misuse.",
  "Acknowledgement": "This work was partially supported by the NationalKey R&D Program of China under Grant No.2022ZD0160101.The authors would also liketo thank Shanghai Artificial Intelligence Labora-tory for their support and resources, as well ascolleagues from Shanghai Jiao Tong University forvaluable discussions. This work was conducted dur-ing the internships of Qiao Sun, Liujia Yang, andMinghao Ma at the Shanghai Artificial IntelligenceLaboratory.",
  "Zhexue Chen, Hong Huang, Bang Liu, Xuanhua Shi,and Hai Jin. 2021b. Semantic and syntactic enhancedaspect sentiment triplet extraction. arXiv preprintarXiv:2106.03315": "Hongliang Dai and Yangqiu Song. 2019. Neural aspectand opinion term extraction with mined rules as weaksupervision. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 52685277. Hao Fei, Fei Li, Chenliang Li, Shengqiong Wu, JingyeLi, and Donghong Ji. 2022. Inheriting the wisdomof predecessors: A multiplex cascade framework forunified aspect-based sentiment analysis. In Proceed-ings of the Thirty-First International Joint Confer-ence on Artificial Intelligence, IJCAI, pages 40964103.",
  "LeoGao.2021.Onthesizesofopenaiapimodels. November 23,2023": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.Simcse: Simple contrastive learning of sentence em-beddings. In 2021 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP 2021,pages 68946910. Association for ComputationalLinguistics (ACL). John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.2021. Declutr: Deep contrastive learning for unsuper-vised textual representations. In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 879895.",
  "Baoxing Jiang, Shehui Liang, Peiyu Liu, Kaifang Dong,and Hongye Li. 2023. A semantically enhanced dualencoder for aspect sentiment triplet extraction. Neu-rocomputing, 562:126917": "Hongjiang Jing, Zuchao Li, Hai Zhao, and Shu Jiang.2021. Seeking common but distinguishing differ-ence, a joint aspect-based sentiment analysis model.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages39103922. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Yanbo Li, Qing He, and Damin Zhang. 2023. Dualgraph convolutional networks integrating affectiveknowledge and position information for aspect sen-timent triplet extraction. Frontiers in Neurorobotics,17.",
  "OpenAI. 2024.OpenAI API. Accessed: 2024-04-14": "Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu,and Luo Si. 2020. Knowing what, how and why: Anear complete solution for aspect-based sentimentanalysis. In Proceedings of the AAAI conference onartificial intelligence, volume 34, pages 86008607. Eleni Pontiki, Dimitra Hadjipavlou-Litina, Konstanti-nos Litinas, and George Geromichalos. 2014. Novelcinnamic acid derivatives as antioxidant and anti-cancer agents: Design, synthesis and modeling stud-ies. Molecules, 19(7):96559674. Maria Pontiki, Dimitrios Galanis, Harris Papageorgiou,Suresh Manandhar, and Ion Androutsopoulos. 2015.Semeval-2015 task 12: Aspect based sentiment analy-sis. In Proceedings of the 9th international workshopon semantic evaluation (SemEval 2015), pages 486495. Maria Pontiki, Dimitris Galanis, Haris Papageor-giou, Ion Androutsopoulos, Suresh Manandhar, Mo-hammed AL-Smadi, Mahmoud Al-Ayyoub, YanyanZhao, Bing Qin, Orphe De Clercq, et al. 2016.Semeval-2016 task 5: Aspect based sentiment anal-ysis.In ProWorkshop on Semantic Evaluation(SemEval-2016), pages 1930. Association for Com-putational Linguistics.",
  "data augmentation framework for aspect-based sen-timent analysis. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 66916704": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, andXiaokui Xiao. 2017. Coupled multi-layer attentionsfor co-extraction of aspect and opinion terms. InProceedings of the Thirty-First AAAI Conference onArtificial Intelligence, pages 33163322. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "Wikipedia. 2024.Gpt-3 - wikipedia. on04/14/2024)": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2020. Transformers: State-of-the-art naturallanguage processing. In Proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: system demonstrations, pages 3845. Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan,Xinyu Dai, and Rui Xia. 2020a. Grid tagging schemefor aspect-oriented fine-grained opinion extraction.In Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 25762585.",
  "Fan Yang, Mian Zhang, Gongzhen Hu, and XiabingZhou. 2023. A pairing enhancement approach foraspect sentiment triplet extraction. arXiv preprintarXiv:2306.10042": "Hongbin Ye, Ningyu Zhang, Shumin Deng, MoshaChen, Chuanqi Tan, Fei Huang, and Huajun Chen.2021. Contrastive triple extraction with generativetransformer. In Proceedings of the AAAI conferenceon artificial intelligence, volume 35, pages 1425714265. Samson Yu Bai Jian, Tapas Nayak, Navonil Majumder,and Soujanya Poria. 2021. Aspect sentiment tripletextraction using reinforcement learning. In Proceed-ings of the 30th ACM International Conference onInformation & Knowledge Management, pages 36033607. Zepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li,and Xiaojie Wang. 2022.Com-mrc: A context-masked machine reading comprehension frameworkfor aspect sentiment triplet extraction. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 32303241. Chen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang.2020. A multi-task learning framework for opiniontriplet extraction.In Findings of the Associationfor Computational Linguistics: EMNLP 2020, pages819828. Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-WenLi, Henghui Zhu, Kathleen Mckeown, Ramesh Nal-lapati, Andrew O Arnold, and Bing Xiang. 2021.Supporting clustering with contrastive learning. InProceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 54195430.",
  "A.3Rethinking the GTS": "Rethinking the 2D tagging scheme:Lemma 1. Specific to the ASTE task, when wetake it as a 2D-labeling problem, we are to 1) finda set of tagging strategies to establish a 1-1 mapbetween each triplet and its corresponding taggingmatrix. See the proof in Appendix Proof 1.Lemma 2. In a 2D-tagging for ASTE, at least threebasic goals must be met: 1) correctly identifyingthe (Aspect, Opinion) pairs, 2) correctly classi-fying the sentiment polarity of the pair based onthe context, and 3) avoiding boundary errors, suchas overlapping*, confusion, and conflict. See the *It occurs when one single word belongs to multipleclasses in different triplets.It occurs when there is a lack of location restrictionsso that multiple neighbored candidates can not be uniquelydistinguished.It occurs when one single word is composed of multipletokens, and the predict gives predictions that are not aligned",
  "Train8577596231015503291394Dev2102512212521176339Test3263382824072978514": ": Statistic information of our two experimentdatasets: #S, #T, #A, and #O denote the num-bers of Sentences, Triplets, Aspects, and Opin-ions; #S1, #S2, #S3 denote the numbers of sen-timents Positive, Neutral and Negative, respec-tively. proof in Appendix Proof 2.Theorem 1. From insight of the above lemmas, itcan be concluded that using enough (that is, follow-ing the 1-1 map properties in Lemma 1, as well asavoiding the issues in Lemma 2) labels will makeit a theoretically ensured tagging scheme.Assumption 1. Ceteris paribus, for a specific clas-sification neural network, the fewer the number oftarget categories, the easier it is for the network tolearn. This is a empirical and heuristic assumption,for the reasonable consideration of Simplificationof Decision Boundaries (Hinton and Salakhutdinov,2006) and Enhancement of Training Efficiency (lessparameters).Combining Theorem 1 and Assumption 1, feweryet enough labels can be heuristically better solu-tion with theoretical guarantee.With the above knowledge, our tagging schemeemploys a full matrix (illustrated as ) sothat rectangular occupations in its cells indicaterespective triplets, where each of the rectanglesrow indices correspond to the relative Aspect termand the column indices correspond to the Opinion.Hereafter, this kind of labels can be taken as a set",
  "with the word span": "of place holder, which is obviously a 1-1 mapmeeting Lemma 1.To further satisfy Lemma 2, we introduce an-other kind of labels, sentiment & beginning tag.This set of labels specializes in recognizing thetop-left corner of a shadowed area. Meanwhile,it takes a value from the sentiment polarity, i.e.Positive, Neutral, Negative. This tagging iscrucial to both identify the beginning of an tripletand label the sentiment polarity. shows a comprehensive case of our tag-ging scheme, in which the left matrix is an appear-ance of our tagging scheme, and it can be decom-posed into two separate components. The middlematrix is the first component, which takes onlyone tag to locate the up-left beginning of an area,and the second component simply predicts a binaryclassification to figure out the full area.Note that, this design benefits the taggingschemes decode process. By scanning across thematrix, we only start an examination function whentriggered by a beginning label like this, and thensearch by row and column until it meets any la-bel except a continued (CTD), which satisfiesLemma 2.",
  "Tk = (Ak, Ok, Sk) be a sentiment tripletconsisting of an aspect term Ak, an opinionterm Ok, and a sentiment Sk": "Tagging StrategyIf Ak starts at position i andOk starts at position j, then M[i][j] is taggedwith a unique label Lk that encodes Sk. This la-bel Lk uniquely identifies the triplet Tk, ensuringthat no other entry M[i][j] with (i, j) = (i, j)carries the same label unless it refers to the samesentiment context.",
  "Surjectivity:Each triplet Tk can beuniquely located and identified by its la-bel Lk in matrix M, where no two distincttriplets have the same label at the same ma-trix position": "ConclusionThe tagging scheme ensures thateach sentiment triplet Tk is uniquely mapped toa specific label in the matrix M, and each labelin M uniquely refers back to a specific triplet Tk.This guarantees a one-to-one correspondence be-tween the triplets and their tagging matrix repre-sentations, fulfilling the conditions required byLemma 1 for an effective and efficient ASTEprocess.",
  "Proof Using Contraposition": "1. Assuming Incorrect Identification: As-sume that some (Aspect, Opinion) pairs areincorrectly identified in M. This wouldmean that there exists at least one pair (i, j)where M[i][j] does not represent the actual(Aspect, Opinion) relationship in S. Thismisrepresentation leads to incorrect senti-ment analysis results, which contradicts therequirement of the task to provide accuratesentiment analysis, thereby proving that ouridentification must be correct.",
  "Legend": ": Decomposition of the tagging scheme into two components: 1) a beginning mark matrix with sentimentlabels; and 2) a placeholder matrix denoting regions of triplets with 1s and default regions with 0s. Rememberthat each row is taken as candidates for an Aspect and each column is taken as candidates for an Opinion. Naturally,each cell in the square matrix can be seen as an ordered pair for a unique candidate of <Aspect, Opinion>. Whenwe simply sum the two components up, we have the left-hand tagging scheme in , where the Sentiment &Beginning Tag is like a trigger (just like you click your mouse), and the Place Holder is like a continued shift(continue to hold and drag the mouse to the downright). 2. Assuming Incorrect Classification: As-sume the sentiment polarity Sk is incor-rectly classified in M. This would implythat the sentiment associated with an (As-pect, Opinion) pair is wrong, leading to asentiment analysis that does not reflect thetrue sentiment of the text. Given that theprimary goal of ASTE is to accurately iden-tify sentiments, this assumption leads to acontradiction, thereby establishing that ourclassification must be accurate. 3. Assuming Existence of Boundary Errors:Assume boundary errors such as overlaps orconfusion occur in M. Such errors wouldprevent the clear identification and classi-fication of sentiment triplets, leading to in-correct or ambiguous extraction outcomes.This would undermine the integrity and us-ability of the ASTE process, contradictingthe tasks need for precise extraction mech-anisms. Hence, we prove that boundaryerrors must be effectively managed. ConclusionThe contraposition approach so-lidifies that the tagging strategy for ASTE in a2D labeling framework successfully achieves thecorrect identification of pairs, accurate classifi-cation of sentiment, and effective managementof boundary errors, as any failure in these as-pects leads to contradictions with the task re-quirements.",
  "MethodsBrief Introduction": "PipelineOTE-MTL (Zhang et al., 2020)It proposes a multi-task learning framework including two parts: aspect and opinion tagging, alongwith word-level sentiment dependency parsing. This approach simultaneously extracts aspect andopinion terms while parsing sentiment dependencies using a biaffine scorer. Additionally, it employstriplet decoding based on the aforementioned outputs during inference to facilitate triplet extraction.Li-unified-R+PD (Peng et al., 2020)It proposes an unified tagging scheme, Li-unified-R, to assist target boundary detection. Two stackedLSTMs are employed to complete aspect-based sentiment prediction and the sequence labeling.CMLA+C-GCN (Wang et al., 2017)It facilitates triplet extraction by modelling the interaction between the aspects and opinions.Two-satge (Peng et al., 2020)It decomposes triplet extraction to two stages: 1) predicting unified aspect-sentiment and opinion tags;and 2) pairing the two results from stage one.RI-NANTE+ (Dai and Song, 2019)It adopts the same sentiment triplets extracting method as that of CMLA+, but it incorporates a novelLSTM-CRF mechanism and fusion rules to capture word dependencies within sentences. Sequence-taggingSpan-BART (Yan et al., 2021)It redefines triplet extraction within an end-to-end framework by utilizing a sequence composedof pointer and sentiment class indexes. This is achieved by leveraging the pretrained sequence-to-sequence model BART to address ASTE.JET (Xu et al., 2020)It extracts triplets jointly by designing a position-aware sequence-tagging scheme to extract the tripletsand capturing the rich interactions among the elements. Seq2seqDual-MRC (Mao et al., 2021)It proposes a solution for ASTE by jointly training two BERT-MRC models with parameters sharing.BMRC (Chen et al., 2021a)It introduces a bidirectional MRC (BMRC) framework for ASTE, employing three query types:non-restrictive extraction queries, restrictive extraction queries, and sentiment classification queries.The framework synergistically leverages two directions, one for sequential recognition of aspect-opinion-sentiment and the other for sequential recognition of opinion-aspects-sentiment expressions. Table-fillingGTS (Wu et al., 2020a)It proposes a novel 2D tagging scheme to address ASTE in an end-to-end fashion only with oneunified grid tagging task. It also devises an effective inference strategy on GTS that utilizes mutualindication between different opinion factors to achieve more accurate extraction.Double-encoder (Jing et al., 2021)It proposes a dual-encoder model that capitalizes on encoder sharing while emphasizing differences toenhance effectiveness. One of the encoders, referred to as the pair encoder, specifically concentrateson candidate aspect-opinion pair classification, while the original encoder retains its focus on sequencelabeling.S3E2 (Chen et al., 2021b)It represents the semantic and syntactic relationships between word pairs, employs GNNs for encoding,and applies a more efficient inference strategy.EMC-GCN (Chen et al., 2022)It employs a biaffine attention module to embed ten types of relations within sentences, transformingthe sentence into a multi-channel graph while incorporating various enhanced linguistic features toenhance performance. Additionally, the method introduces an effective strategy for refining word-pairrepresentations, aiding in the determination of whether word pairs are a match or not. LLM-basedzero-shotPerforming aspect-based sentiment analysis using an LLM. The specific method involves inputting aprompted sentence and directly outputting the corresponding [A, O, S] triplets. An example of the textgiven to the LLM, with the prompt added, is as follows: \"Perform aspect-based sentiment analysis onthe provided text and return triplets as [Aspect, Opinion, Sentiment]. You only need to provide thetriplets, no additional explanations are required. The provided text: {sentence}\" few-shotBuilding upon the zero-shot method, a small number of examples from the training set are added to theprompted sentence: \"Perform aspect-based sentiment analysis on the provided text and return tripletsas [Aspect, Opinion, Sentiment]. For example: input: {train sentence} output: {train triplets}, ...(some other examples). You only need to provide the triplets, no additional explanations are required.The provided text: {sentence}\". We utilized 5-shot, 10-shot, and 20-shot methods, all randomlysampled from the training set. The results indicate that the 5-shot method performed the best, whilethe performances of the 10-shot and 20-shot methods showed a decline. The tables presents the outputresults for the 5-shot method.",
  "OursMiniConGTS75.8776.1276.0067.4561.0164.0766.8464.0865.4369.3874.4071.80": ": Experimental results on D1 (Wu et al., 2020a). The best results are highlighted in bold, while the secondbest results are underscored. The results with are retrieved from (Yu Bai Jian et al., 2021). The results with areretrieved from (Xu et al., 2020). The results with are retrieved from (Wu et al., 2020a). The results with areretrieved from (Peng et al., 2020).",
  "[Saketini, wonderful, positive],[bar menu, taken off, negative]": "I went in one day asking for a table for a group and wasgreeted by a very rude hostess.[hostess, rude, negative][hostess, rude, negative][hostess, rude, negative][table for a group, asking, neutral],[hostess, very rude, negative][table for a group, asking, neutral],[hostess, very rude, negative] But make sure you have enough room on your credit card asthe bill will leave a big dent in your wallet.[bill, big, negative][credit card, enough room, positive],[bill, big dent, negative],[wallet, big dent, negative]",
  "[bill, big dent, negative]": ": In summary, there are several challenges observed in the performance of GPT models concerning triplets. Firstly, there is a prominent issue of \"hard\" matching, whereGPT models tend to introduce additional modifiers or adverbs in the opinion component, leading to a lack of exact correspondence. Secondly, during zero-shot inference,GPT models tend to generate multiple predicted triplets, resulting in decreased precision. This behavior particularly hampers the precision of the models predictions. Thirdly,inconsistencies arise in handling triplets involving structures such as [A, O1 and O2, S] and [A, O1, S], [A, O2, S]. This inconsistency is challenging to mitigate due to itsdependence on annotation practices and conventions. Upon closer examination, the issues observed do not appear to be as pronounced as indicated by the evaluation metrics.Rather, they often manifest as cases where the general idea is correctly captured, but the precise format or phrasing does not align perfectly. Notably, the performance of GPT-4deteriorates due to its occasional tendency to not merely \"extract\" fragments from sentences but to generate its own summarizations. Consequently, evaluating against triplets thatoriginate solely from annotated sentences poses a challenge in achieving alignment. Furthermore, GPT-4 exhibits a proclivity for extracting longer sequences of words as aspectsor opinions, while GPT-3.5 tends to produce shorter sequences that better conform to typical annotation scenarios.",
  "MethodsPrompts": "zero-shotSuppose you are an expert of aspect-based sentiment analysis. Perform aspect-based sentimentanalysis on the provided text and return triplets as [Aspect, Opinion, Sentiment]. You only need toprovide the triplets, no additional explanations are required. The provided text: {sentence} few-shotSuppose you are an expert of aspect-based sentiment analysis. Perform aspect-based sentimentanalysis on the provided text and return triplets as [Aspect, Opinion, Sentiment]. For example:input: The food is uniformly exceptional , with a very capable kitchen which will proudly whip upwhatever you feel like eating , whether it s on the menu or not .output: [food, exceptional, positive], [kitchen, capable, positive]...(generated from training set)Now I will provide a new sentence, and you only need to provide the triplets [Aspect, Opinion,Sentiment] without any additional explanations. The provided sentence: {sentence} CoTSuppose you are an expert of aspect-based sentiment analysis. Please analyze the given text foraspect-based sentiment analysis using the following steps:Definitions:- Aspect: An aspect is a specific part or feature of the entity being discussed. It is usually a noun or anoun phrase.- Opinion: An opinion is a descriptive term or phrase that expresses a sentiment towards the aspect. Itis usually an adjective or a descriptive phrase.- Sentiment: The sentiment is the overall feeling expressed towards the aspect, categorized as positive,negative, or neutral.Instructions:1. Read the text and identify all aspects mentioned.2. For each identified aspect, determine the opinion expressed and the sentiment (positive, negative,neutral).3. Summarize the findings in the format [Aspect, Opinion, Sentiment]. Each triplet must contain anaspect, an opinion, and a sentiment.4. If there is a one-to-many relationship between aspects and opinions, list multiple triplets.5. Use all words from the original text to answer without any changes.Example: (automatically generated by ChatGPT-4o)Text: \"The restaurant has a great ambiance, but the service is poor and the food is average.\"Steps:1. Identify aspects: ambiance, service, food.2. Evaluate opinions and sentiments:- ambiance: Opinion - great, Sentiment - positive- service: Opinion - poor, Sentiment - negative- food: Opinion - average, Sentiment - neutral3. Summarize:- [ambiance, great, positive]- [service, poor, negative]- [food, average, neutral]Please analyze the following text: {sentence} CoT+few-shotSuppose you are an expert of aspect-based sentiment analysis. Please analyze the given text foraspect-based sentiment analysis using the following steps:Definitions:- Aspect: An aspect is a specific part or feature of the entity being discussed. It is usually a noun or anoun phrase.- Opinion: An opinion is a descriptive term or phrase that expresses a sentiment towards the aspect. Itis usually an adjective or a descriptive phrase.- Sentiment: The sentiment is the overall feeling expressed towards the aspect, categorized as positive,negative, or neutral.Instructions:1. Read the text and identify all aspects mentioned.2. For each identified aspect, determine the opinion expressed and the sentiment (positive, negative,neutral).3. Summarize the findings in the format [Aspect, Opinion, Sentiment]. Each triplet must contain anaspect, an opinion, and a sentiment.4. If there is a one-to-many relationship between aspects and opinions, list multiple triplets.5. Use all words from the original text to answer without any changes.Example: (generated from training set)Text: ...Steps:1. Identify aspects: ...2. Evaluate opinions and sentiments:- ...3. Summarize:- [..., ..., ...]...Please analyze the following text: {sentence}"
}