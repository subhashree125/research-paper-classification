{
  "Abstract": "Audio deepfake detection (ADD) is essentialfor preventing the misuse of synthetic voicesthat may infringe on personal rights and privacy.Recent zero-shot text-to-speech (TTS) modelspose higher risks as they can clone voices witha single utterance. However, the existing ADDdatasets are outdated, leading to suboptimalgeneralization of detection models. In this pa-per, we construct a new cross-domain ADDdataset comprising over 300 hours of speechdata that is generated by five advanced zero-shot TTS models. To simulate real-world sce-narios, we employ diverse attack methods andaudio prompts from different datasets.Ex-periments show that, through novel attack-augmented training, the Wav2Vec2-large andWhisper-medium models achieve equal errorrates of 4.1% and 6.5% respectively. Addition-ally, we demonstrate our models outstandingfew-shot ADD ability by fine-tuning with justone minute of target-domain data. Nonetheless,neural codec compressors greatly affect the de-tection accuracy, necessitating further research.Our dataset is publicly available 1.",
  "Introduction": "Audio deepfakes, created by text-to-speech (TTS)and voice conversion (VC) models, pose severerisks to social stability by spreading misinforma-tion, violating privacy, and undermining trust. Foradvanced TTS models, the subjective score of thesynthetic speech can surpass that of the authen-tic speech (Ju et al., 2024) and humans are oftenunable to recognize deepfake audio (Mller et al.,2022; Cooke et al., 2024). Consequently, it is im-perative to develop robust audio deepfake detection(ADD) models capable of identifying impercepti-ble anomalies.Several datasets built upon various TTS and VCmodels have been released to benchmark the ADD task (Yi et al., 2022; Yamagishi et al., 2021; Frankand Schnherr, 2021; Wang et al., 2020; Yi et al.,2023). However, these datasets mainly includethe traditional TTS models rather than the emerg-ing zero-shot TTS models. Moreover, there is alack of transparency regarding the specific types ofmodels used within these datasets, hindering com-prehensive analysis of cross-model performance.Additionally, the range of attacks these datasetsconsider is confined to conventional methods, ex-cluding attacks associated with deep neural net-works (DNNs), such as noise reduction and neu-ral codec models. Based on the aforementioneddatasets, a multitude of detection models have beenproposed. These models incorporate diverse fea-tures, such as the traditional linear frequency cep-stral coefficient (Yan et al., 2022) and features de-rived from self-supervised learning (Zeng et al.,2023; Martn-Doas and lvarez, 2022), emotionrecognition (Conti et al., 2022), and speaker iden-tification models (Pan et al., 2022). These studiesmainly concentrate on a single benchmark dataset.To demonstrate generalization capabilities, sev-eral studies have implemented cross-dataset eval-uation (Mller et al., 2022; Ba et al., 2023). Fur-thermore, to enhance the models generalizability,researchers have explored the combination of datafrom various sources (Kawa et al., 2022) and theintegration of multiple features (Yang et al., 2024).In this paper, we present a novel cross-domainADD (CD-ADD) dataset, which encompasses morethan 300 hours of speech data generated by fivecutting-edge, zero-shot TTS models. We test ninedifferent attacks, including those involving DNN-based codecs and noise reduction models. Forcross-domain evaluation, rather than adopting thenaive cross-dataset scenario, we formulate a uniquetask for zero-shot TTS models by analyzing pair-wise cross-model performance and utilizing audioprompts from different domains. Experiments re-veal:",
  "Encoder-decoder (YourTTS (Casanova et al.,": "2022), WhisperSpeech (Kharitonov et al.,2023), Seamless Expressive (Barrault et al.,2023), and OpenVoice (Qin et al., 2023)):An encoder extracts semantic information,while a decoder incorporates speaker embed-dings from the speech prompt. Together withthe vocoder, the autoregressive (AR) or non-autoregressive (NAR) decoder generates per-sonalized speeches.When the encoder istrained to remove speaker-specific informa-tion from the input speech, it transforms intoa VC model. For zero-shot TTS, AR decoding may introduceinstability, leading to errors such as missing words.Additionally, poor-quality speech prompts, char-acterized by high noise levels, can result in un-intelligible output. To address these issues, weenforce quality control during dataset construction",
  "Cross-model ADD excludes data from oneTTS model during training and uses data fromthis TTS model only during testing": "ADD models should generalize to in-the-wildsynthetic data, which requires a well-designedcross-model evaluation that can represent the real-world scenario.To select the appropriate TTSmodel for testing, we conduct a pairwise cross-model evaluation, where the Wav2Vec2-base modelis trained exclusively on the data produced by a sin-gle TTS model and subsequently evaluated on thedatasets generated by alternative TTS models. Weidentify the TTS model that poses the greatest chal-lenge, as evidenced by the high equal error rate(EER), and use it as the test set.",
  ": Categories of tested attacks": "(Noise-white) and environmental noise (Noise-env) (Maciejewski et al., 2020) with a signal-to-noise ratio ranging from 15dB to 20dB, use artifi-cial reverberation (Reverb) with a duration of 0.2to 0.4 seconds, and apply a low-pass filter (LPF)within the 4kHz to 8kHz range. Furthermore, weemploy lossy compression methods such as MP3and a DNN-based Encodec model (Dfossez et al.,2022) operating at bit rates of 6kbps (Codec-6) and12kbps (Codec-12). In terms of noise reduction,we utilize the conventional noise gate approachto eliminate stationary noise and the time-domainSepFormer model (Subakan et al., 2021).",
  "ADD Methods": "We fine-tune pre-trained speech encoders for theADD task, namely, Wav2Vec2 (Baevski et al.,2020) and the Whisper encoder (Radford et al.,2022). We merge multi-layer features by usinglearnable weights, and employ a classifier headwith two projection layers and one global pool-ing layer to obtain the final logits. To adapt themodel to attacks, we consider all attacks with thesame probability on-the-fly during training. Wealso consider a few-shot scenario, where we ex-tend the cross-model evaluation by fine-tuning theADD model with just one minute of target-domainspeech data. This experiment simulates a situationwhere only the limited synthetic speech from a TTSmodel is available, such as the speech from a demowebsite or a single video.",
  "Experimental Setups": "The training set for the CD-ADD dataset wasgenerated using the train-clean-100 subset of Lib-riTTS (Zen et al., 2019), and the dev-clean and test-clean subsets of LibriTTS, along with the test setof TEDLium3 (Hernandez et al., 2018), were uti-lized for the evaluation datasets. The transcriptionswere used as the input text, and the real speech sig-nals were used as the real samples and the speechprompts. For dataset construction, we used the fivezero-shot TTS models mentioned in .1.",
  ":Cross-model EER matrix, where theWav2Vec2-base model was trained using data generatedfrom a single TTS model and subsequently evaluatedon data originating from other TTS models": "We adopted a CER threshold of 10% and a max-imum retry limit of five. For cross-model evalua-tion, the speech from Seamless Expressive servedas the test set. Appendix A provides comprehen-sive details on the TTS model checkpoints and themodels used for attacks, and Appendix B presentsthe specific statistics of the CD-ADD dataset thatis comprised of over 300 hours of training data and50 hours of test data.For the ADD task, we combined our CD-ADDdataset with the ASVSpoof2019 (Wang et al., 2020)training set and fine-tuned the base model, whichincludes Wav2Vec2 (Baevski et al., 2020) and theWhisper encoder (Radford et al., 2022), for fourepochs with a learning rate of 3e 5 and a batchsize of 128. For attack-augmented training, we in-creased the number of epochs to eight, as the modelconverges more slowly due to attacks. The proba-bility of each attack was 10% and only one attacktype was used for each utterance. For the evalua-tion metric, we adopted the widely used equal errorrate (EER).",
  "Pairwise Cross-Model Evaluation": "As illustrated in , the pairwise evaluationindicates that the ADD system exhibits optimal per-formance when both the training and testing setsare derived from the same TTS model. This trendholds true irrespective of the speech prompts do-main (whether they originate from the in-domainLibriTTS dataset or the cross-domain TEDLiumdataset), with the EERs consistently remaining be-low 1%. However, in the cross-model evaluation,the EERs vary significantly among different TTSmodel combinations. For example, the Wav2Vec2-",
  "+ Aug.+ Aug": "Baseline0.1 / 0.10.0 / 0.17.9 / 21.45.0 / 10.1Noise-white9.4 / 9.10.8 / 0.734.7 / 45.09.9 / 10.3Noise-env9.0 / 4.70.5 / 0.329.2 / 31.19.4 / 9.3Reverb13.0 / 17.11.1 / 1.229.6 / 33.118.1 / 23.7LPF1.3 / 1.20.1 / 0.314.3 / 23.46.6 / 8.9MP30.3 / 0.20.0 / 0.113.2 / 22.15.4 / 8.3Codec-122.9 / 1.40.3 / 0.321.4 / 31.011.4 / 18.3Codec-67.4 / 5.20.9 / 1.230.5 / 35.218.5 / 28.9",
  ": Performance of Wav2Vec2-base under variousattacks measured by EER (%) on Libri and TED testsets respectively. \"+Aug.\" indicates all attacks areincluded during training": "base model fine-tuned with YourTTS-synthesizeddata can generalize to VALL-E-synthesized data,achieving EERs of 0.14% and 0.61% for the Libriand TED subsets of the CD-ADD test sets, re-spectively. However, it struggles to generalize tothe Seamless Expressive model, resulting in muchhigher EERs of 29.71% and 44.00%. This indicatesthat randomly choosing a test set whose speechdata is generated by a TTS model could result inoverestimated generalizability of the ADD model,due to shared artifacts between TTS models andpotential overfitting. Therefore, we selected Seam-less Expressive as the test set as it has notably highEERs. It is worth noting that the model trained onthe prevalent ASVSpoof dataset fails to generalizeto the zero-shot TTS models. However, combiningASVspoof with the CD-ADD dataset can slightlyimprove the performance (), so these twodatasets are combined by default in subsequent ex-periments.",
  ": Few-shot performance of three base modelsmeasured by EER (%)": "attacked models are only slightly higher than thebaseline. In the cross-model setup, a significantdecrease in EERs is observed for the augmentedmodel compared to the non-augmented model. No-tably, certain attacks improve the ADD modelsgeneralizability, as indicated by the reduced EERsin the TED subset. For example, compared withthe EER of 10.1% for the baseline, the LPF reducesthe EER to 8.9%, the MP3 compression reducesthe EER to 8.3%, and the SepFormer reduces theEER to 5.5%. All these attacks remove spectralinformation and force the ADD model to rely moreon features from the low-frequency band, thus mit-igating overfitting. However, certain attacks, suchas reverberation and the Encodec, lead to relativelyhigh EERs. The encoder-decoder architecture andthe vector quantization of the Encodec, especiallyat lower bit rates, have the potential to obliterateessential features for detecting synthetic speeches.",
  "Results of Few-Shot Fine-Tuning": "compares the cross-model ADD perfor-mance of three base models: Wav2Vec2-base,Wav2Vec2-large, and Whisper-medium.TheWav2Vec2-large and the Whisper-medium mod-els have similar performance, notably superior tothe Wav2Vec2-base model ( (a, b)). Withthe most challenging Encodec attack, the Whis-per model performs significantly better than theWav2Vec2 models ( (c, d)). We can also ob- serve that with only one minute of in-domain datafrom Seamless Expressive, the EER can be reducedsignificantly. This suggests that our models are ca-pable of fast adaptation to in-the-wild TTS systemswith just a few samples from a demo website or avideo, which is crucial for real-world deployment.However, we find that in-domain fine-tuning is lesseffective when the audio is compressed with theEncodec, as the reduction in EER is less significant.",
  "Conclusion": "In conclusion, our study presents a CD-ADDdataset, addressing the urgent need for up-to-dateresources to combat the evolving risks of zero-shotTTS technologies. Our dataset, comprising over300 hours of data from advanced TTS models,enhances model generalization and reflects real-world conditions. This paper highlights the risksof attacks and the potential of few-shot learning inADD, facilitating future research.",
  "Limitation": "The current CD-ADD dataset is limited to five zero-shot TTS models. Future expansions are plannedto include a broader range of zero-shot TTS mod-els, as well as conventional TTS and VC models,to improve the dataset diversity. Additionally, theattack-augmented training is constrained to a sin-gle attack per sample, with separate analysis con-ducted for each attack. Subsequent research willfocus on investigating the effects of combined at-tacks. Furthermore, the performance in ADD taskswith audio compressed by neural codecs is subop-timal, requiring the development of optimizationstrategies and the exploration of more neural codecmodels.",
  "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli. 2020.Wav2Vec 2.0: A framework for self-supervised learn-ing of speech representations. Proc. NeurIPS": "Loc Barrault, Yu-An Chung, Mariano Coria Megli-oli, David Dale, Ning Dong, Mark Duppenthaler,Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar,Justin Haaheim, et al. 2023. Seamless: Multilingualexpressive and streaming speech translation. arXivpreprint arXiv:2312.05187. Edresson Casanova, Julian Weber, Christopher DShulby, Arnaldo Candido Junior, Eren Glge, andMoacir A Ponti. 2022. Yourtts: Towards zero-shotmulti-speaker tts and zero-shot voice conversion foreveryone. In Proc. ICML, pages 27092720. PMLR. Emanuele Conti, Davide Salvi, Clara Borrelli, BrianHosler, Paolo Bestagini, Fabio Antonacci, AugustoSarti, Matthew C Stamm, and Stefano Tubaro. 2022.Deepfake speech detection through emotion recogni-tion: a semantic approach. In Proc. ICASSP, pages89628966. IEEE.",
  "Joel Frank and Lea Schnherr. 2021. Wavefake: A dataset to facilitate audio deepfake detection. In Proc.NeurIPS": "Franois Hernandez, Vincent Nguyen, Sahar Ghannay,Natalia Tomashenko, and Yannick Esteve. 2018. Ted-lium 3: Twice as much data and corpus repartitionfor experiments on speaker adaptation.In Proc.SPECOM, pages 198208. Springer. Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, DetaiXin, Dongchao Yang, Yanqing Liu, Yichong Leng,Kaitao Song, Siliang Tang, et al. 2024.Natural-speech 3: Zero-shot speech synthesis with factor-ized codec and diffusion models.arXiv preprintarXiv:2403.03100.",
  "Cem Subakan, Mirco Ravanelli, Samuele Cornell,Mirko Bronzi, and Jianyuan Zhong. 2021. Atten-tion is all you need in speech separation. In Proc.ICASSP, pages 2125. IEEE": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,Huaming Wang, Jinyu Li, et al. 2023. Neural codeclanguage models are zero-shot text to speech synthe-sizers. arXiv preprint arXiv:2301.02111. Xin Wang, Junichi Yamagishi, Massimiliano Todisco,Hctor Delgado, Andreas Nautsch, Nicholas Evans,Md Sahidullah, Ville Vestman, Tomi Kinnunen,Kong Aik Lee, et al. 2020. Asvspoof 2019: A large-scale public database of synthesized, converted andreplayed speech. Computer Speech & Language,64:101114. Junichi Yamagishi, Xin Wang, Massimiliano Todisco,Md Sahidullah, Jose Patino, Andreas Nautsch,Xuechen Liu, Kong Aik Lee, Tomi Kinnunen,Nicholas Evans, et al. 2021. Asvspoof 2021: ac-celerating progress in spoofed and deepfake speechdetection. In ASVspoof 2021 Workshop-AutomaticSpeaker Verification and Spoofing CoutermeasuresChallenge. Rui Yan, Cheng Wen, Shuran Zhou, Tingwei Guo, WeiZou, and Xiangang Li. 2022. Audio deepfake de-tection system with neural stitching for add 2022.In ICASSP 2022-2022 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 92269230. IEEE.",
  "Yujie Yang, Haochen Qin, Hang Zhou, ChengchengWang, Tianyu Guo, Kai Han, and Yunhe Wang. 2024.A robust audio deepfake detection system via multi-view feature. In Proc. ICASSP, pages 1313113135.IEEE": "Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, HaoxinMa, Chenglong Wang, Tao Wang, Zhengkun Tian,Ye Bai, Cunhang Fan, et al. 2022. Add 2022: the firstaudio deep synthesis detection challenge. In Proc.ICASSP, pages 92169220. IEEE. Jiangyan Yi, Jianhua Tao, Ruibo Fu, Xinrui Yan, Chen-glong Wang, Tao Wang, Chu Yuan Zhang, XiaohuiZhang, Yan Zhao, Yong Ren, et al. 2023. Add 2023:the second audio deepfake detection challenge. arXivpreprint arXiv:2305.13774.",
  "BAppendix: CD-ADD Dataset": "presents the statistics of the CD-ADDdataset.The average utterance length exceedseight seconds, which is longer than that of tradi-tional ASR datasets. The number of utterances forTTS models is less than that of real utterances be-cause some synthetic utterances fail to meet theCER requirements. Among them, VALL-E has thefewest utterances due to the decoder-only modelsrelative instability. compares five zero-shot TTS models in terms of the word-error-rate(WER) and speaker similarity. Speaker similarityis based on the LibriTTS test-clean subset, whereECAPA-TDNN is used to extract speaker embed-dings. VALL-E and WhisperSpeech have the high-est speaker similarity scores, while OpenVoiceranks lowest. Conversely, VALL-E achieves thehighest WER, and OpenVoice has the lowest."
}