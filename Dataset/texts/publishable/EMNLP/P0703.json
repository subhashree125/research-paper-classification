{
  "Abstract": "Logical reasoning remains a challenge for natu-ral language processing, but it can be improvedby training language models to mimic theoremprovers on procedurally generated problems.Previous work used domain-specific proof gen-eration algorithms, which biases reasoning to-ward specific proof traces and limits auditabil-ity and extensibility. We present a simpler andmore general declarative framework with flex-ible context-sensitive rules binding multiplelanguages (specifically, simplified English andthe TPTP theorem-proving language). We con-struct first-order logic problems by selecting upto 32 premises and one hypothesis. We demon-strate that using semantic constraints duringgeneration and careful English verbalization ofpredicates enhances logical reasoning withouthurting natural English tasks. We use relativelysmall DeBERTa-v3 models to achieve state-of-the-art accuracy on the FOLIO human-authoredlogic dataset, surpassing GPT-4 in accuracywith or without an external solver by 12%.",
  "Introduction": "Language models trained only on natural languageshow lackluster capabilities at logical reasoning(McCoy et al., 2023; Mahowald et al., 2024). Asa countermeasure, we can train neural models tomatch the output of symbolic reasoning systems(e.g., logic theorem provers, or other algorithms)on procedurally generated problems, to sharpentheir reasoning capabilities. This process improvesaccuracy on some human-authored problems (Wuet al., 2021; Clark et al., 2020; Wu et al., 2022; Liuet al., 2023).Previous work on synthetic first-order logic(FOL) reasoning datasets, RuleTaker (Clark et al.,2020), LogicNLI (Goodwin et al., 2020a) and FLD(Morishita et al., 2023), write dedicated code re-implementing the FOL axioms from scratch togenerate proofs, and translate the generated prob-lems to natural language. We propose Unigram, a framework for synthetic reasoning data gener-ation, specifically designed to generate problemsjointly into multiple languages. We represent gram-mars with concise and expressive rules bindingtwo languages, and constraints to prune unwantedgenerations.We write the most extensive grammar of FOLsemantic fragments to our knowledge. We struc-ture the generated expressions into PREMISE, HY- POTHESIS pairs, and annotate their logical relation-ship (entailment/contradiction/neutral) with a FOLsolver, following the natural language inference(NLI) framework (Goodwin et al., 2020b). A sim-plistic FOL NLI problem is: PREMISE: Everyonewho is happy is rich. Mary is rich. HYPOTHESIS:Mary is happy LABEL: Neutral.We fine-tune DeBERTa NLI models (He et al., 2021) on Unigram-FOL and compare it with previ-ous similar datasets. The 184M parameters (base-size) beats GPT-4 augmented or not with exter-nal theorem provers, on the FOLIO (Han et al.,2022) dataset. Our contributions are as follows: (i)A dataset of reasoning problems expressed in En-glish and TPTP (a language that can be interfacedwith numerous theorem provers) alongside Vam-pire proof annotations, covering FOL with equalityand both finite and open domains, improved com-positionality, and more extensive quantifiers. (ii)Ablations measuring the effect of constraining ma-terial conditionals usage, of using realistic Englishpredicates, and of reimplementing LogicNLI withdeclarative generation instead of proof tree genera-tion, highlighting that declarative can work betterbut that a richer logical modeling drives most of theimprovement. (iii) A general reasoning problemgrammar-based generation framework relying onsolvers. The generation library, grammars, models,and generated dataset are publicly available1.",
  "Related work": "Synthetic datasets for reasoningNumerousworks investigate the logical capabilities of NLPmodels using textual datasets and symbolic reason-ing (Helwe et al., 2022). We focus on the grammar-derived synthetic datasets. RuleTaker (Clark et al.,2020) explores this area with a subset of first-orderlogic. LogicNLI addresses a broader FOL subset(Tian et al., 2021). FLD explores full FOL (Mor-ishita et al., 2023) and increased compositionality.Richardson et al. (2020); Richardson and Sabhar-wal (2022) use a solver to study the satisfiability innatural language using the Z3 solver and dedicatedgeneration logic on constrained problems. PrOn-toQA (Saparov and He, 2023) generates proofsfrom ontologies and then derives questions fromthe proofs to analyze chains of thoughts in lan-guage models. Other work explore non-standardlogic with synthetic dataset, notably probabilistic(Sileo and Moens, 2023), paraconsistant (Kazemiet al., 2024), epistemic (Sileo and Lernould, 2023)logics. Generation frameworksMultiple frameworksalready implement generation from handwrittengrammars. NLTK (Bird and Loper, 2004) has acontext-free grammar tool, but cannot natively han-dle multiple languages or large-scale generation.Grammatical Framework (Ranta, 2004) is the clos-est tool to ours. It enables generation from abstractgrammars and linearization into concrete gram-mars (e.g. French and English) but it is translation-oriented and not context-sensitive. GLIF (Schaeferand Kohlhase, 2020) extends Grammatical Frame-work to parse English into logical formulas but isnot suited for generation either.",
  "Forward inference": "Previous NLI-style FOL reasoning datasets (Rule-Taker, LogicNLI, FLD) generate examples usingproof generators that are based on the axiomsof FOL. This requires domain-specific generationcode and introduces unwanted complexity. Elim-ination and Introduction rules can cancel eachother and create an illusion of reasoning depth.We found that some examples in the Proofwriterdataset (Tafjord et al., 2021) directly contain thepremise in the hypothesis despite having a reason-ing depth of 5. When constructing NLI pairs, gen- erating neutral examples requires special strategiesintroducing a sampling bias, and it can be the samefor contradiction generation. Proof generation tech-niques enable high reasoning depth but at the costof breadth (linguistic variety and reasoning vari-ety).",
  "Declarative generation": "We fully rely on an existing FOL solver andwe propose Unigram, a simpler, more genericmethod to generate problems with multilingualgrammars where rules bind multiple surface formrealization templates. A Unigram Rule declarationspecifies a type signature,and two surfaceform realizers, and optional validity constraints:R(output_type, input_types, realizers, constraints)The signature specifies the type of the rule outputand the type of the arguments. The realizers takethe arguments as input and map them to a string.We can have a realizer for a logical representationand a realizer for English. Using functions allowsmore expressivity than context-free grammars(Hunter, 2021), but for most cases with cantreat template strings as functions using Pythonstring.format. Constraints and realizers canaccess the state of the current generation as ananytree tree.Constraints are binary functionschecking construction validity.One usefulconstraint is distinctness, e.g. (arguments of thesame type should have a different realization), toavoid repetitions or statements like Mary likesMary. We enable this constraint by default. Generation algorithmWe use a depth-first al-gorithm that recursively fills in the leftmost non-terminal leaf with random type-matching rule sam-pling until constraints are satisfied. This enablesleft-to-right generation, allowing realizers and con-straints to access the current context. We recur-sively call realizers to construct surface forms (e.g.English text).",
  "Application to first-order logic (FOL)": "We use Unigram to enrich FOL problem generationwhile also avoiding ambiguity, starting as a super-set of LogicNLI (grammar in Appendix B). Tocreate a problem, we uniformly sample 1 to 32 sen-tences as premises and 1 sentence per hypothesisensuring that all symbols are present in the premise.We exclude non-satisfiable formulas (paradoxes)in premise groups and hypotheses. We label pairsas ENTAILMENT if (premise hypothesis) is unsatisfiable, as CONTRADICTION if (premise hypothesis) is unsatisfiable, and as NEUTRAL oth-erwise. Following Ruletaker and LogicNLI, wecreate problems with predicates over named indi-viduals (e.g. Mary is young). We generate gender-balanced English surnames with CensusName. Wenow present new logical modeling features absentfrom the previous comparable datasets: Explicit finite and open domainsWe explic-itly mention the domain when using the quanti-fiers. We introduce two locations, anywhere, anda room with occupants e.g. Mary, and Paul arethe only persons in the room.which logicallymeans x, room(x) (x = Mary x = Paul).We can then quantify over the room (everyonein the room) or anywhere (everyone anywhere).By doing this, we can generate induction prob-lems (checking that everyone in the room is happyif Mary and Paul are happy) and test reasoningwith both finite and open domains. This requireshandling FOL with equality which was not imple-mented in previous work. Quantifiers and logical relationshipsWe ex-tend previous work with more complete quan-tifiers not all, nobody, not everyone.Weleverage context-sensitivity to create a rule forpolysyllogisms (predicate chains of the formall A are B, all B are C, all C are D. We also intro-duce only if, unless, otherwise as conditionals andallow sentence-level negation. Constraining material conditionalsLike previ-ous work, we use material conditional to expressconditional statements: if p then q is formalized asp q i.e. p q. This means that the implicationis true if p is false, and that negating p q entailsq both p which can be counter-intuitive. We usea constraint to eliminate all conditionals within thescope of negations and of other conditionals. Improving predicate verbalizationRuleTakerand LogicNLI use adjectives as logical predicatesbut do not handle their semantic interference. Rule-Taker do not consider being both blue and beinggreen as contradictory. LogicNLI uses 379 adjec-tives treated as independent, including ugly andugliest. FLD uses pseudo language like the larddoes hurtle pushup. We prompted GPT-4 (Mayversion) to Generate 150 predicates where eachpredicate does not contradict nor entail any otherpredicate. Two examples: \"enjoys wildlife photog-raphy\" and \"owns a smart tv\". We remove errors",
  "and provide manual negations. We also use rela-tionships (like, is a sibling of, modeling symmetryaxioms when relevant, and adjectives": "Logical representation languagePrevious Log-icNLI, RuleTaker, FLD, and FOLIO all use theirown logical format, representing formulas as listsor strings. We use the TPTP (Sutcliffe, 2010) FOFlanguage which is a standard syntax for theoremprovers evaluation and is compatible with many the-orem provers, notably Vampire (Reger et al., 2022),Z3 (De Moura and Bjrner, 2008) or Prover9 (Mc-Cune, 2005). We select the Vampire (Reger et al.,2022) theorem prover which provides short andreadable proofs and details all the premises usedduring a derivation. Complexity controlMethods based on forwardinference can theoretically control the proof depthusing hyperparameters. Here, to avoid mostly sam-pling shallow problems, we limit the number ofnon-neutral examples where the proof to the num-ber of examples using 5 inputs, for each number ofinputs. Neutrals are still a majority by an order ofmagnitude. To sample hard neutral examples, weuse a Gradient Boosting classifier with 100 trees(and scikit-learn 1.5.0 (Pedregosa et al., 2011) de-fault parameters otherwise) to predict the labelsbased on unigram counts of the logical operatorsin the premise and hypothesis. We train on 1k ex-amples, discard these, and then discard the mostconfident neutral predictions to achieve balancedlabels.",
  "Methodology": "We fine-tune a pre-trained NLI model on multi-ple synthetic FOL datasets: LogicNLI, FLD, Rule-Taker, and on Unigram-FOL. We then evaluate thedirect effect on other three-way entailment down-stream tasks, and on further fine-tuning on the train-ing data of evaluation tasks (Phang et al., 2018).We use the DeBERTa-v3 (He et al., 2021) NLImodels trained on the tasksource collection (Sileo,2024)2.We use a learning rate of 1e5 forDeBERTa-large and 2e5 (Mosbach et al., 2021)for DeBERTa-base, 1 or 3 epochs (based on intrin-sic validation accuracy) and Huggingface Trans-formers (Wolf et al., 2019) version 4.41 defaultTrainer arguments otherwise.",
  "D-largeUnigram-FOL+FLD78.288.665.278.442.257.975.4": ": Comparison of auxiliary synthetic training datasets effect on the evaluation tasks. We report the averageaccuracy of two runs. D column refer to zero-shot D test accuracy after synthetic auxiliary training, and +ft refersto the test accuracy after auxiliary training then further fine-tuning D training set (in the previous column). We generate 100k examples with a 80/10/10train/dev/test split. but we only use 40k trainingtraining examples to match FLD. We use the FLDversion of FLD. We use the ProofWriter (Tafjordet al., 2021) open-world-assumption version ofRuleTaker. We exclude LogicNLI examples la-beled as paradoxes and we map all labels to NLIlabels.",
  "Evaluation datasets": "We evaluate on two pure reasoning datasets, FO-LIO and Fragments, and on two more generaldatasets:FOLIO (Han et al., 2022) containshuman-written FOL problems. We evaluate onthe validation set to compare to Olausson et al.(2023) results who report 72.5% accuracy usinga GPT-4 with a solver and 75.3% with chain-of-thoughts. We construct another validation set from10% of train and map labels to NLI labels. (Weiet al., 2022) WANLI (Liu et al., 2022) is a NLIdataset with diverse and challenging reasoning pat-terns. ConTRoL (Liu et al., 2021) is a NLI datasetrequiring multiple premises to derive the correctlabel, measuring contextual reasoning. Fragments(Richardson et al., 2020) is based on formal seman-tics templates and evaluate reasoning with quanti-fiers; this dataset is mostly suited to evaluation, astraining quickly leads to almost perfect test accu-racy.",
  "Comparison with previous synthetic datasets shows the accuracy of multiple auxil-iary training datasets on the evaluation dataset": "Unigram-FOL outperforms RuleTaker, LogicNLI,and FLD on all tasks with a comfortable margin,and leads to lesser degradation on the datasets thatare not only focused on logic (WANLI, ConTRoL).The last line of the table combines Unigram-FOL(with the full 100k examples) with FLD and showsthat combining generation methods can furtherpush the state of the art on FOLIO.We conduct ablations to better understand thesource of this improvement, presented in the mid-dle of . Unigram-LogicNLIWe use our declarative gen-eration method on the base LogicNLI grammarto disentangle the effect of the generation tech-nique from the grammar itself. This outperformsthe original LogicNLI but not Unigram-FOL whichhighlights the value of our additional constructions. Replacing Realistic PredicatesWe replace ourgenerated predicates with the original LogicNLIadjectives (containing semantic interferences); thisdegrades FOLIO accuracy but does not stronglyimpact other NLI tasks, notably Fragments whichmainly use adjectives as predicates. Removing Conditionals ConstraintsUnrestrict-ing usage of material conditionals harms the zero-shot transfer on FOLIO and the capabilities at moregeneral reasoning, which confirms that removingcounter-intuitive constructs can help transferability.",
  "Conclusion": "We showed that simple declarative grammarspaired with solvers can outperform complex prooftree generators for reasoning dataset generationsand released a new FOL reasoning dataset, mod-els, and ablations. Our framework can help futurereasoning research, notably on explanation sincefully aligned TPTP code can be leveraged to modelnecessity and sufficiency. We plan to extend Uni-gram to planning, constraint satisfaction and modallogic.",
  "Limitations": "Reasoning methods based on neural networks donot provide formal guarantees and can introducebiases in real applications. They can be used as acomplement to externalization methods (Olaussonet al., 2023). Automatically formalizing a problemis difficult and can lead to mistakes (Olausson et al.,2023) which could be detected by internalization-based methods. Our dataset could be used to auto-mate formalization but we did not try such experi-ments. In addition, our work is only conducted withEnglish language and encoder models, mainly usedfor verification and not generation. We only usedone model architecture, DeBERTa, while other ar-chitectures like Albert (Lan et al., 2020) or otherrecursive architectures could be more suited to rea-soning.",
  "Ethical considerations": "Our models are derived from language modelswhich inherit bias from their training corpus. Wedid not conduct any human annotations, relying onalready annotated datasets to validate our method-ology. We use encoder models which have lowerenergy consumption than decoders (Luccioni et al.,2024) and performed experiments with less than20 total days on a Nvidia A100 GPU. Steven Bird and Edward Loper. 2004. NLTK: The natu-ral language toolkit. In Proceedings of the ACL In-teractive Poster and Demonstration Sessions, pages214217, Barcelona, Spain. Association for Compu-tational Linguistics.",
  "Emily Goodwin, Koustuv Sinha, and Timothy JODonnell. 2020a. Probing linguistic systematicity.arXiv preprint arXiv:2005.04315": "Emily Goodwin, Koustuv Sinha, and Timothy J.ODonnell. 2020b. Probing linguistic systematic-ity. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages19581969, Online. Association for ComputationalLinguistics. Simeng Han, Hailey Schoelkopf, Yilun Zhao, ZhentingQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-terina Zubova, Yujie Qiao, Matthew Burtell, et al.2022. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840.",
  "Tim Hunter. 2021. The chomsky hierarchy. A compan-ion to Chomsky, pages 7495": "Mehran Kazemi, Quan Yuan, Deepti Bhatia, NajoungKim, Xin Xu, Vaiva Imbrasaite, and Deepak Ra-machandran. 2024.Boardgameqa: A dataset fornatural language reasoning with contradictory infor-mation. Advances in Neural Information ProcessingSystems, 36. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,Kevin Gimpel, Piyush Sharma, and Radu Soricut.2020. Albert: A lite bert for self-supervised learningof language representations. In International Confer-ence on Learning Representations. Alisa Liu, Swabha Swayamdipta, Noah A. Smith, andYejin Choi. 2022. WANLI: Worker and AI collabora-tion for natural language inference dataset creation.In Findings of the Association for ComputationalLinguistics: EMNLP 2022, pages 68266847, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Hanmeng Liu, Leyang Cui, Jian Liu, and Yue Zhang.2021. Natural language inference in context - investi-gating contextual reasoning over long texts. Proceed-ings of the AAAI Conference on Artificial Intelligence,35(15):1338813396.",
  "William McCune. 2005. Release of prover9. In Milehigh conference on quasigroups, loops and nonasso-ciative systems, Denver, Colorado": "Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi,and Yasuhiro Sogawa. 2023. Learning deductive rea-soning from synthetic corpus based on formal logic.In International Conference on Machine Learning,pages 2525425274. PMLR. Marius Mosbach, Maksym Andriushchenko, and Diet-rich Klakow. 2021. On the stability of fine-tuning{bert}: Misconceptions, explanations, and strongbaselines. In International Conference on LearningRepresentations. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,Armando Solar-Lezama, Joshua Tenenbaum, andRoger Levy. 2023. LINC: A neurosymbolic approachfor logical reasoning by combining language modelswith first-order logic provers. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 51535176, Singapore.Association for Computational Linguistics. Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, et al. 2011. Scikit-learn: Machinelearning in python. the Journal of machine Learningresearch, 12:28252830.",
  "Aarne Ranta. 2004. Grammatical framework. Journalof Functional Programming, 14(2):145189": "Giles Reger, Martin Suda, Andrei Voronkov, LauraKovcs, Ahmed Bhayat, Bernhard Gleiss, Marton Ha-jdu, Petra Hozzova, JR Evgeny Kotelnikov, MichaelRawson, et al. 2022. Vampire 4.7-smt system de-scription. Kyle Richardson, Hai Hu, Lawrence Moss, and AshishSabharwal. 2020. Probing natural language inferencemodels through semantic fragments. Proceedingsof the AAAI Conference on Artificial Intelligence,34:87138721. Kyle Richardson and Ashish Sabharwal. 2022. Pushingthe limits of rule reasoning in transformers throughnatural language satisfiability.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 36, pages 1120911219.",
  "DamienSileoandAntoineLernould.2023": "MindGames: Targeting theory of mind in large lan-guage models with dynamic epistemic modal logic.In Findings of the Association for Computational Lin-guistics: EMNLP 2023, pages 45704577, Singapore.Association for Computational Linguistics. Damien Sileo and Marie-francine Moens. 2023. Prob-ing neural language models for understanding ofwords of estimative probability. In Proceedings ofthe 12th Joint Conference on Lexical and Compu-tational Semantics (*SEM 2023), pages 469476,Toronto, Canada. Association for Computational Lin-guistics.",
  "Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021": "ProofWriter: Generating implications, proofs, andabductive statements over natural language. In Find-ings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021, pages 36213634, Online.Association for Computational Linguistics. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao,Hao He, and Yaohui Jin. 2021. Diagnosing the first-order logical reasoning ability through LogicNLI.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages37383747, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics.",
  "AFOL-nli example": "PREMISE :Christopher, Donald, Gene are the only persons in the room.Everyone in the room who collects antique jewelry plays the drums.Someone in the room designs and sews custom cosplay costumes for conventions.Christopher collects classic novels.Everyone in the room who enjoys deep-sea diving and exploring underwater caves enjoys kayaking or is anight owl or both.Christopher enjoys kayaking.Everyone in the room enjoys kayaking only if they collects antique jewelry.HYPOTHESIS :Christopher collects antique jewelry.LABELentailmentPREMISE (TPTP): room(c) & room(d) & room(g) &(![X]:(room(X) => (X='c' | X='d' | X='g'))) &(![X]:(room(X) => ((collects_jewelry(X)) => (plays_drums(X))))) &(?[X]:(room(X) & (designs_cosplay(X)))) &collects_novels(c) &(![X]:(room(X) => ((enjoys_diving(X)) =>(enjoys_kayaking(X) | is_night_owl(X))))) &enjoys_kayaking(c) &(![X]:(room(X) => (enjoys_kayaking(X) <= collects_jewelry(X))))"
}