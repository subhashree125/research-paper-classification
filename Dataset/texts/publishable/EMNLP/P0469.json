{
  "Abstract": "Multimodal large language models (MLLMs)have attracted increasing attention in the pastfew years, but they may still generate descrip-tions that include objects not present in thecorresponding images, a phenomenon knownas object hallucination. To eliminate halluci-nations, existing methods manually annotatepaired responses with and without hallucina-tions, and then employ various alignment al-gorithms to improve the alignment capabil-ity between images and text. However, theynot only demand considerable computation re-sources during the finetuning stage but alsorequire expensive human annotation to con-struct paired data needed by the alignment al-gorithms. To address these issues, we proposean efficient fine-grained unlearning framework(EFUF), which performs gradient ascent utiliz-ing three tailored losses to eliminate halluci-nations without paired data. Extensive exper-iments show that our method consistently re-duces hallucinations while preserving the gen-eration quality with modest computational over-head. Our code and datasets are available at",
  "Introduction": "In the burgeoning field of artificial intelligence,the advent of multimodal large language models(MLLMs) has opened new frontiers in human-computer interaction, data processing, and auto-mated content generation (Zhu et al., 2023; Liuet al., 2023b; Chen et al., 2023; Ye et al., 2023).These sophisticated models, capable of understand-ing both text and images, have significantly ad-vanced our ability to automate complex tasks.However, an intriguing and critical phenomenonknown as hallucination in these models posesunique challenges for current research. Halluci-nation in MLLMs refers to the generation of in-consistent responses that are not grounded by the",
  ": An example of hallucination in MLLM": "multimodal context (Sun et al., 2023). For exam-ple, as shown in , the caption includes theobject landing gear, but in fact it does not appearin the image. Such hallucinations will lead to mis-information, potentially undermining user trust innumerous downstream applications.Recent methods for mitigating multimodal hal-lucination can be divided into two categories:inference-based methods (Lee et al., 2023; Zhouet al., 2023; Yin et al., 2023; Wang et al., 2023;Sicong Leng, 2023; Wang et al., 2024; Chen et al.,2024) and finetuning-based methods (Sun et al.,2023; Yu et al., 2023; Liu et al., 2023a; Zhao et al.,2023; Jiang et al., 2023). Inference-based meth-ods correct or restrict generated content throughexternal expert review, self-reflection or decodingstrategies during inference stage. However, theyusually require additional inference steps with in-creased costs and delay (Yu et al., 2023). Fur-thermore, each task demands specific procedure orprompt (Xu et al., 2024), adding to the complexity of implementation. Overcoming these drawbacks,finetuning-based approaches are proposed to ad-just the model directly through specialized datasetsand preference alignment algorithms. These algo-rithms, including RLHF (Sun et al., 2023; Liu et al.,2023a), DPO (Yu et al., 2023; Zhao et al., 2023;Zhou et al., 2024) and contrastive learning (Jianget al., 2023), enhance the congruence between textand images, leading to improved alignment. Al-though they have achieved good performance, twocritical issues emerge:First, their data demands are substantial, asthey require a comprehensive set of paired posi-tive and negative samples for effective finetuning.The alignment algorithms they employed demandpaired hallucinated and non-hallucinated responsesfor each query. Acquiring such specific and variedresponse sets for each query presents a significantchallenge. Recent methodologies in this field pre-dominantly rely on human labor to annotate theoutput from the MLLM, requiring specialized ex-pertise and incurring considerable expenditure oftime and financial resources.Second, The finetuning of MLLM utilizing thesealignment algorithms usually demands consider-able computational resources. Most of these tech-niques are sophisticated and necessitate the simul-taneous operation of multiple models to executepreference alignment, thereby escalating the over-all cost significantly.To tackle the above issues, we propose theEfficient Fine-Grained Unlearning Framework(EFUF), which offers the advantage of not neces-sitating paired data and being more efficient dur-ing the finetuning phase. Our method, groundedin the principles of unlearning, mainly relies onperforming gradient ascent on negative samplesto mitigate hallucinations, eliminating the needfor costly manually-annotated paired data. Addi-tionally, it consumes considerably fewer compu-tational resources. Unlike traditional alignmentalgorithms that require simultaneous operation ofmultiple models to execute preference alignment,EFUF operates without this requirement.The key to applying the unlearning algorithm ishow to curate positive and negative samples, i.e.,distinguish between real and hallucinated objects,in a manner that is both cost-effective and reliable.Intuitively, the similarity between objects and theircorresponding images can act as an indicator forhallucinations, since the image contains real ob-jects but not the hallucinated ones. Inspired by Zhao et al. (2024), we propose to utilize the CLIPmodel (Radford et al., 2021) to evaluate text-imagecongruence. Trained on a vast corpus of text-imagepairs, CLIP stands as a robust tool to help identifyhallucinations.After ascertaining the capability of CLIP througha preliminary experiment, we curate our datasetmanually-free by utilizing CLIP scores, before ap-plying our unlearning-based method to MLLMs.This process enables us to harness the power ofunlearning, offering a potent and efficient approachfor mitigating hallucinations in MLLMs.Our contribution can be summarized as follows:",
  "Hallucination Mitigation for MLLM": "To mitigate hallucinations for MLLM, variousmethods have been proposed. According to dif-ferent phase during which they tackle the hallucina-tions, their work can be divided into two categories:(1) Inference-based methods. They employ ex-ternal experts, self-reflection framework or decod-ing strategies to constrain or modify generated con-tent during the inference phase, thereby reducinghallucinations. For example, LURE (Zhou et al.,2023) utilizes manually-crafted features to detecthallucinations and therefore revises the generatedtext. Woodpecker (Yin et al., 2023) proposes topost-edit hallucinations by combining the output ofMLLMs and a more accurate expert VQA modelusing GPT-3.5. VIGC (Wang et al., 2023) iter-atively refines the instruction data using genera-tion and correction framework. VOLCANO (Leeet al., 2023) trains the MLLM to give self-feedback,and then performs self-reflection on the originalgenerated text according to the feedback. VCD (Sicong Leng, 2023) first introduces contrastive de-coding in MLLM by disturbing the visual inputsand calculate visual uncertainty to restrict the gen-eration of hallucinated tokens. ICD (Wang et al.,2024) utilizes disturbance on instructions insteadof images. HIO (Chen et al., 2024) employs a hal-lucinated model to further widen the gap betweenhallucinated and correct tokens, achieving bettercontrastive outcomes. Although these methods donot need to train the model, they require additionalinference steps with increased costs and delay (Yuet al., 2023), and specific procedure and promptmust be designed for each task (Xu et al., 2024).(2) Finetuning-based methods. Overcoming thepotential drawbacks of the first category, thesemethods involve crafting specific datasets and fine-tuning the model, aiming for better alignment be-tween images and text. For instance, LLaVA-RLHF(Sun et al., 2023) first adopts RLHF to mitigate hal-lucinations. Based on this work, RLHF-V (Yu et al.,2023) introduces fine-grained alignment by man-ually correcting the outputs of MLLMs. Beyondstandard RLHF, some works utilize other improvedalgorithms for better efficiency, e.g., DPO (Zhaoet al., 2023; Zhou et al., 2024), instruction tuning(Liu et al., 2023a), and contrastive learning (Jianget al., 2023). However, these methods require ex-pensive manually annotated paired data, and mostof them also demand substantial computational re-sources during the finetuning stage. Therefore, inthis work, we focus on reducing the data and com-putation requirements.",
  "Unlearning": "Unlearning refers to a technique designed to inducea model to \"forget\" specific behaviors or data, pri-marily through the application of gradient ascentmethods (Cao and Yang, 2015). Recently, unlearn-ing for LLM is receiving increasing attention. Janget al. (2023) demonstrate that straightforward gradi-ent ascent can effectively eliminate privacy vulner-abilities in LLMs. Later, Yao et al. (2023) proposethe use of random mismatch and restrictions onKL divergence for positive samples, reducing thenegative impact of unlearning on the general per-formance of LLMs.In our research, we extend the concept of un-learning to the realm of multimodal hallucinationmitigation in MLLMs, proposing a novel solutionfor enhancing model reliability and accuracy inmultimodal contexts.In contrast to earlier ap-proaches that apply unlearning across the entirety of a models responses, our methodology focusesexclusively on the unlearning of hallucinated ob-jects. This precise, fine-grained unlearning strategyallows for a more sophisticated refinement of themodels outputs, ensuring that only inaccuraciesare corrected without diminishing the models capa-bilities in other areas. To the best of our knowledge,this is the first attempt to adopt unlearning to mul-timodal large language models.",
  "Hallucinated v.s. Non-Hallucinated": "Our approach involves employing the CLIP modelto assess the similarity between text and corre-sponding images, with the objective of determin-ing whether there is a discernible difference inthe similarity scores of hallucinated versus non-hallucinated content. Following Zhou et al. (2023),we manually annotate 200 image captions gener-ated by MiniGPT (Zhu et al., 2023) and LLaVA(Liu et al., 2023b), labeling objects as either halluci-nated or non-hallucinated. Subsequently, we definean object-level image-relevance score by calculat-ing fine-grained CLIP similarities for these objectsin relation to their associated image segments, aim-ing to uncover any significant disparities in scoredistributions.Formally, let V = {v1, v2, ..., vm} denotes thecollection of images, and T = {t1, t2, ..., tm}is the corresponding captions generated by theMLLM. For each ti T, we manually anno-tated all the objects in the caption, represented byOi = {o1i , o2i , ..., oni }, and O = {O1, O2, ..., Om}.After that, we determine whether the object is hal-lucinated, i.e., whether it appears in the image, as-signing each object a binary value h(oji) as follows:",
  "LLaVANo28.642.652.5 1012Yes26.112.27": ": Statistics and significance test on samplesgenerated by MiniGPT4 and LLaVA. Hal. indicateswhether the objects are hallucinated, Mean and Std.represent their average and standard deviation of image-relevance scores, and p is the p-value of t-test. object oji in either group and its corresponding im-age vi. Given that most objects cover only a portionof the image, we segment the image into patchesand employ a sliding window technique to identifythe best match. Thus, the image-relevance scorefor each object is determined as follows:",
  "S(oji) = maxwiWi CLIP(oji, wi),(1)": "where Wi represents the set of sliding windowsover the patches of the image vi.This methodology enables us to obtain two setsof image-relevance scores S1 = {S(o)|o H1}and S0 = {S(o)|o H0}. In the next section, wewill examine the distributions of these scores andvalidate our hypothesis that text-image similaritycan indicate the likelihood of hallucination.",
  "Results and Analysis": "In our analysis, we applied a two-sample t-test toexamine the differences between the score distribu-tions of hallucinated and non-hallucinated objects.The results, as detailed in , reveal a notablediscrepancy between the mean values of these dis- tributions, as indicated by the p-value. This statisti-cal evidence allows us to confidently reject the nullhypothesis that the two distributions have identicalmeans, underscoring the utility of CLIP similarityscores in detecting hallucinations.To provide a clearer understanding of thesedifferences, we visualized the score distributionsthrough density plots. These plots, illustrated in, demonstrate that scores for hallucinatedobjects typically fall below 32, whereas scoresfor non-hallucinated objects generally exceed 23for both the two models. Our quantitative analy-sis further reveals that among the objects scoringabove 32, only 0.6% and 1.6% are hallucinated, andamong those below 23, only 2.3% and 1.7% are nothallucinated, for MiniGPT and LLaVA respectively.These findings not only substantiate our hypothe-sis but also suggest that definitive thresholds canbe established to effectively segregate positive andnegative samples for the purpose of unlearning.",
  "Overview": "After ascertaining the capability of CLIP through apreliminary experiment, we design EFUF, whoseoverview is shown in . Drawing from estab-lished methodologies in prior research (Sun et al.,2023; Yu et al., 2023; Liu et al., 2023a; Zhao et al.,2023; Jiang et al., 2023), our approach is bifur-cated into two key stages: dataset construction andthe unlearning process itself. Initially, we harnessCLIP scores to identify and segregate various sam-ples; after that, unlearning is applied on the modelwith the curated samples.Concretely, in constructing the dataset, we firstprompt the model to generate captions for given",
  "clip score 21": ": An overview of EFUF. EFUF is divided into two stages: dataset formation and unlearning process.Initially, we extract objects from generated captions and calculate their image relevance utilizing CLIP, followed bythe construction of three datasets. Subsequently, three corresponding losses are tailored to finetune the model. images. After that, we utilize the CLIP model tocalculate the fine-grained similarity score of the ob-ject phrases in text and the corresponding segmentsin image. By setting thresholds for the scores, weare able to discern and compile distinct samplesfrom the generated text, forming a dataset for fine-tuning that circumvents the need for labor-intensivemanual annotation. During the finetuning phase,we employ an efficient unlearning method, whichinvolves the development of three distinct types oflosses. These losses are designed to aid the modelin discarding incorrect multimodal alignments thatcould lead to hallucinations, while preserving thecorrect alignments essential for tasks. Unlearninggenerally requires less computation resources com-pared with conventional alignment algorithms inthe finetuning stage, so the computation amountcan also be effectively reduced.",
  "Dataset Formation": "Prior to implementing unlearning with MLLMs,its imperative to define the targets of unlearningand accordingly assemble the requisite positiveand negative samples. As evidenced in .2, specific thresholds can effectively delineatebetween these samples. Hence, we apply these pre-determined image-relevance thresholds to filter thehallucinated and non-hallucinated objects.Given that a single response may encompassboth hallucinated and non-hallucinated objects, afine-grained approach to unlearning is warranted.Rather than attempting to unlearn an entire re- sponse wholesale, we opt for a targeted strategyfocusing on the subsentences corresponding to theobject, delineated by punctuation. Moreover, topreserve the models overarching sentence compre-hension and capabilities, we also compile samplesof the complete sentences based on the mean image-relevance scores of all included objects, in additionto the positive and negative subsentences. Thesethree categories of samples collectively form thedataset tailored for the unlearning process, facili-tating a more nuanced and effective mitigation ofmultimodal hallucinations.Formally, let D = {v; x; y} denotes a finetuningdataset for MLLM, where v is the image, x is thetext query (prompt), and y is the text answer. Thepositive subsentence dataset is formulated as",
  "Unlearning for MLLM": "After constructing the dataset, the final phase ofour approach is the application of unlearning tech-niques to the model.Prior studies (Eldan andRussinovich, 2023) have shown that employingsolely the unlearning loss severely undermines themodels linguistic comprehension, rendering it in-capable of producing coherent sentences. Thus,we introduce a dual-faceted fine-grained unlearn-ing approach: applying a negative loss to the sub-sentences containing hallucinated objects, and apositive loss to those containing non-hallucinatedobjects. This strategy aims to curtail the productionof hallucinated content while encouraging preciseobject representation, thus diminishing the occur-rence of hallucinations. Meanwhile, we also pro-pose a sentence loss, aiming to preserve the modelsability to generate cohesive, long-form text. In thefollowing, we will introduce these losses in detail.As is indicated by previous works, the core ofunlearning is the gradient ascent strategy. Formally,unlearning updates the model parameters by:",
  "Experimental Settings": "Dataset.We adopt MSCOCO (Lin et al., 2014)as our dataset. Since our approach necessitates onlythe images themselves, their annotations are usedexclusively for evaluation. Details of our datasetcan be found in Appendix A.2. Evaluation Metrics.Following Yu et al. (2023),our assessment encompasses two dimensions: trust-worthiness measured by the degree of hallucination,and helpfulness determined by the quality of thegenerated text. To quantify hallucinations, we uti-lize CHAIR (Rohrbach et al., 2018), MHumanEval(Yu et al., 2023) and POPE (Fu et al., 2023). For",
  "ShareGPT4V46.822.331.09.987.843.329.215.489.60.157+ EFUF36.918.414.05.488.146.932.518.191.10.159": ": Performance comparison of various MLLMs with and without EFUF. Hallucination is assessed usingCHAIR (ChairS, ChairI), MHumanEval (HumanS, HumanI), and POPE metrics. Quality is evaluated based onconsistency with ground truth (Bleu1, Bleu2), informativeness (Info.), and fluency (ppl.). A downward arrow ()indicates that lower values are better, whereas an upward arrow () signifies that higher values are preferable. generation quality, we leverage the BLEU (Pap-ineni et al., 2002) score for assessing the consis-tency with ground truth, evaluate informativenessthrough GPT-4s judgment (OpenAI, 2023), anduse GPT-2s perplexity score (Radford et al., 2019)to determine text fluency. Details on the evaluationmetrics are provided in Appendix A.3.",
  "Baselines": "To affirm the robustness of EFUF across a spec-trum of MLLMs, we conducted evaluations againsta suite of state-of-the-art base models. These in-clude MiniGPT4 (Zhu et al., 2023), mPLUG-owl(Ye et al., 2023), LLaVA (Liu et al., 2023b), andShareGPT4V (Chen et al., 2023), which are pre-trained on extensive multimodal datasets and sub-sequently finetuned on high-quality instructions. Inour experiments, we integrate EFUF into them toobtain the enhanced model.",
  "As is shown in , we evaluate EFUF across avariety of MLLMs, assessing both the hallucinationrate and generation quality": "Hallucination Rate.Based on the results, ourapproach demonstrates a consistent reduction inhallucination rates across all four MLLMs, with anaverage improvement of approximately 15% and5% on the ChairS and ChairI metric, 18% and 8%on the HumanS and HumanI metric, and 1% on thePOPE metric. These findings validate the effective-ness and adaptability of our method, emphasizingits capacity to notably lower hallucination ratesacross cutting-edge models. Generation Quality. also highlights theimprovements of EFUF in generation quality. Re-sults show that our method not only reduces thehallucination rate but also enhances overall genera-tion quality. Specifically, it improves BLEU-1 by4%, BLEU-2 by 3%, BLEU-4 by 2%, informative-ness by 1%, and fluency by 1%, across the fourmodels. These enhancements stem from two mainfactors: the unlearning strategy which promotesaccurate object generation, and the sentence lossdesign which enhances fluency.",
  "Ablation Study": "Without loss of generality, we select the MiniGPT4model for the ablation study to investigate the ef-fects of different modules of our proposed method.As outlined in .3, our approach is funda-mentally comprised of two key elements: the sen-tence loss and the unlearning mechanism, whichitself includes the negative loss and the positive loss.In order to quantify the contribution of each com-ponent, we contrast EFUF against the followingconfigurations: (1) vanilla unlearning: a strategyemploying the coarse-grained unlearning, leverag-ing both positive and negative entire sentences iden-tified based on their sentence-level image relevancescores; (2) fine-grained unlearning: the unlearningstrategy applied in EFUF, but without the sentenceloss; (3) sentence-loss-only method: a method thatsolely applies the sentence loss of EFUF, omittingthe unlearning aspects. The subsequent content de-tails the outcomes and insights derived from theseexperimental comparisons.",
  ": Performance comparison of different hallucina-tion mitigation methods for LLaVA on metrics measur-ing VQA and reasoning capability": "rate reduction and BLEU score enhancement, whenthe method of vanilla unlearning and sentence lossare applied. However, these gains are trivial com-pared to those achieved by fine-grained unlearningand the complete EFUF, highlighting the essen-tial role fine-grained unlearning plays in mitigatinghallucinations and generating correct objects. Effects of the Sentence Loss.Compared toEFUF, the fine-grained unlearning approach re-sults in a slightly lower hallucination rate but atthe cost of informativeness and fluency. In thisscenario, BLEU scores fall short of capturing thisissue, as they only measure n-gram matches. Thedecline in fluency is highlighted by a significant in-crease in perplexity, rendering the responses largelyunreadable by humans. Manual examination fur-ther reveals that the generated content often con-sists fragmented and incoherent sentences. Con-versely, method employing only the sentence loss and EFUF do not exhibit these flaws, emphasizingthe vital function of sentence loss in maintaininghigh-quality text generation.In summary, our analysis confirms the neces-sity of integrating both fine-grained unlearning andsentence loss to effectively reduce hallucinationswithout compromising the models proficiency ingenerating comprehensive, fluent sentences. Thiscombined approach ensures model performancewhile notably reduces hallucinations.",
  "Comparison with Other Methods": "To further evaluate the performance of EFUF, wecompare it with other methods tailored to halluci-nation mitigation. These include LLaVA-RLHF(Sun et al., 2023), HA-DPO (Zhao et al., 2023),and POVID (Zhou et al., 2024), which are all eval-uated using their officially released checkpoints.We benchmark EFUF against these methods on theLLaVA model, since their checkpoints are all basedon LLaVA. Hallucination Rate & Generation Quality.Wemeasure EFUFs generation quality along with hal-lucination rate in . Compared to other hallu-cination mitigation methods, EFUF demonstratescomparable or superior performance, while requir-ing minimal data construction cost and training re-sources among all. Additionally, our improvementsin generation quality are on par with RLHF-basedmethods, which typically demand expensive human RLHFDPOCLEFUF0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 A100 GPU hours",
  "annotations and significant computations. Theseoutcomes highlight our methods effectiveness andefficiency": "VQA & Reasoning Capability.To provide amore holistic evaluation of EFUF, we also as-sessed its performance on VQA and reasoningtasks. We employed benchmarks such as MME (Fuet al., 2024), GQA (Hudson and Manning, 2019),ScienceQA (Lu et al., 2022), and QBench (Wuet al., 2024). reports the results for thebaseline model, EFUF, and competing methods.EFUF demonstrates modest performance fluctua-tion across these benchmarks compared to otherhallucination mitigation strategies, indicating thatour method does not negatively affect VQA andreasoning capabilities.",
  "Training Cost": "EFUF distinguishes itself from conventional fine-tuning approaches to hallucination mitigationthrough its markedly lower end-to-end trainingcosts. A key advantage of EFUF lies in its datasetconstruction process, which obviates the need forcostly human annotations. Traditional methods typ-ically rely on extensive human-labeled datasets, of-ten comprising around 10,000 samples at expensessurpassing $3,000 (Sun et al., 2023; Yu et al., 2023).Otherwise, they create the dataset with the assis-tance of GPT-4, involving up to 500,000 samplespre-screened before manual review, incurring costsfor around 200 million tokens equivalent to $2,000(Liu et al., 2023a; Jiang et al., 2023).In stark contrast, EFUFs resource efficiencyextends to its training demands. As depicted in, EFUFs training on an A100 GPU for a MiniGPT4 model requires merely 3 GPU hours, afraction of the resources needed by other methods.For comparison, RLHF-based finetuning typicallyconsumes 20 GPU hours (Sun et al., 2023), DPOranges from 8 (Yu et al., 2023) to 16 (Zhao et al.,2023) GPU hours, and contrastive learning methodrequires around 10 GPU hours (Jiang et al., 2023).This substantial reduction on resource require-ments in both dataset construction and trainingstage not only makes EFUF a cost-effective ap-proach but also enhances its scalability and acces-sibility for broader applications in hallucinationmitigation within the realm of multimodal largelanguage models.",
  "Additional Analyses": "To further substantiate the effectiveness of EFUF,we provide extensive supplementary analyses in theappendices. As presented in Appendix B, EFUFcomplements and enhances the performance of ex-isting hallucination mitigation strategies. We alsoexplore the impact of varying weights as hyper-parameters in Appendix C. Finally, a case studydetailed in Appendix D quantitatively evaluates thegenerated text under different methods, showcasingthe distinct advantages of our proposed solution.",
  "Conclusion": "In this paper, we find that text-image similarity ishelpful for identifying multimodal hallucinations,and propose a novel unlearning framework to mit-igate hallucinations in MLLM. Specifically, wefirst curate different samples utilizing the image-relevance score derived from CLIP similarity, andthen design three distinct losses to perform unlearn-ing on the curated samples. Extensive experimentson different baselines show that our method ef-fectively reduces multimodal hallucinations whileretaining the general performance of the model.",
  "Limitations": "The limitations of our work mainly contain twoaspects. Firstly, the exploration of alternative meth-ods for assessing text-image similarity presents anavenue for further research. Our findings affirmthe utility of text-image relevance in constructingdatasets for the unlearning process, with the rele-vance scores derived using the CLIP model. Ad-ditional methodologies for determining text-imagerelevance warrant exploration, which may furtheroptimize the construction of unlearning datasets. Secondly, in line with most preceding research, ourinvestigation primarily addresses object hallucina-tions, gauged by the presence or absence of thedepicted object in the corresponding image. Theexploration of other varieties of hallucinations, in-cluding but not limited to the attributes or posi-tioning of objects within the image, represents asignificant area for future work.",
  "Ronen Eldan and Mark Russinovich. 2023.Whosharry potter? approximate unlearning in llms. CoRR,abs/2310.02238": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023. MME: A comprehensive evaluationbenchmark for multimodal large language models.CoRR, abs/2306.13394. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.2024. Mme: A comprehensive evaluation benchmarkfor multimodal large language models.",
  "Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-joon Seo. 2023. Volcano: Mitigating multimodalhallucination through self-feedback guided revision.CoRR, abs/2311.07362": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023. Eval-uating object hallucination in large vision-languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 292305. Association for Computational Lin-guistics. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick. 2014. Microsoft COCO:common objects in context. In Computer Vision -ECCV 2014 - 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part V, volume 8693 of Lecture Notes in ComputerScience, pages 740755. Springer.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b.Visual instruction tuning.CoRR,abs/2304.08485": "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. In The 36th Conference on Neu-ral Information Processing Systems (NeurIPS).",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, July 6-12, 2002, Philadelphia,PA, USA, pages 311318. ACL. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Pytorch:An imperative style, high-performance deep learninglibrary. In Advances in Neural Information Process-ing Systems 32, pages 80248035. Curran Associates,Inc. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the 38th InternationalConference on Machine Learning, ICML 2021, 18-24July 2021, Virtual Event, volume 139 of Proceedingsof Machine Learning Research, pages 87488763.PMLR.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2018. Object hallu-cination in image captioning. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, Brussels, Belgium, October 31- November 4, 2018, pages 40354045. Associationfor Computational Linguistics. Guanzheng Chen Xin Li Shijian Lu Chunyan Miao Li-dong Bing Sicong Leng, Hang Zhang. 2023. Miti-gating object hallucinations in large vision-languagemodels through visual contrastive decoding. arXivpreprint arXiv:2311.16922. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,Chunyuan Li, Yikang Shen, Chuang Gan, Liang-YanGui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,and Trevor Darrell. 2023. Aligning large multimodalmodels with factually augmented RLHF.CoRR,abs/2309.14525. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, HuapingZhong, Pan Zhang, Xiaoyi Dong, Weijia Li, WeiLi, Jiaqi Wang, and Conghui He. 2023. VIGC: vi-sual instruction generation and correction. CoRR,abs/2308.12714.",
  "Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Largelanguage model unlearning": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, MingYan, Yiyang Zhou, Junyang Wang, Anwen Hu,Pengcheng Shi, Yaya Shi, Chenliang Li, YuanhongXu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang,and Fei Huang. 2023. mplug-owl: Modularizationempowers large language models with multimodality.CoRR, abs/2304.14178. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, HaoWang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,and Enhong Chen. 2023. Woodpecker: Hallucina-tion correction for multimodal large language models.CoRR, abs/2310.16045. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, YifengHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-TaoZheng, Maosong Sun, and Tat-Seng Chua. 2023.RLHF-V: towards trustworthy mllms via behavioralignment from fine-grained correctional human feed-back. CoRR, abs/2312.00849.",
  "A.1Implementation Details": "For dataset construction, in order to efficiently ob-tain the object set O, we prompt the LLaMA-2-70b(Touvron et al., 2023) model to extract all the ob-jects from the response text. During training, weonly tune each models multimodal mapping layers,i.e., ones that map image feature to text token em-bedding. We train each model for a fixed 1 epochwith AdamW (Loshchilov and Hutter, 2019) as theoptimizer, and report their performance on test set.We implement all the models with the PyTorchframework (Paszke et al., 2019), and run experi-ments on an NVIDIA A100 GPU (NVIDIA et al.,2020). For hyperparameters, we set the weight ofunlearning loss 1 to 0.3, the weight of sentenceloss 2 to 0.2, the learning rate to 1e-5, weightdecay to 0.05. Based on the analysis in ,the threshold for normal object T0 and hallucinatedobject T1 is set to 32 and 23, respectively. Besides,to ensure that the number of the entire sentencesamples is similar to that of the positive and neg-ative subsentences, we set the threshold for entiresentence T2 to 27.5.",
  "A.2Dataset": "MSCOCO (Lin et al., 2014) is a comprehensivedataset, encompassing over 300,000 images acrossmore than 80 categories, each meticulously anno-tated. Our approach, which leverages text imagecongruence for alignment, necessitates only theimages themselves and their associated prompts,omitting any need for annotations. Following Zhouet al. (2023); Liu et al. (2023a), we randomly select3,200 images with annotation for validation andtesting, ensuring no overlap with the training im-ages to maintain the integrity of our experimentalconditions.",
  "A.3Evaluation Metrics": "A.3.1Metrics on Hallucination RateTo quantify the rate of hallucinations, we utilizeCHAIR (Rohrbach et al., 2018) and MHumanEval(Yu et al., 2023), which allow us to measure hallu-cinations at both the sentence and instance levelsfor model-generated content. Additionally, POPE(Fu et al., 2023) is incorporated into our evaluationto directly assess the models via VQA. Details ofthese metrics are given below.(1) CHAIR. Caption Hallucination Assessmentwith Image Relevance (CHAIR, Rohrbach et al.,2018) is a widely-used metric for evaluating hallu-cination. It quantifies hallucination by calculatingthe ratio of non-existent objects referenced in themodels response to the total number of objectsmentioned. It features two variations: CHAIRSfor sentence-level and CHAIRI for instance-level.Both aim to measure object hallucination, albeitfrom different perspectives:",
  "|{all responses}|,(10)": "where hallucinated responses refer to the responsescontaining at least one hallucinated objects.(2) MHumanEval. Recognizing the limitationsof CHAIR in covering only a set of pre-definedobject categories, we also incorporate human judg-ment into our evaluation. Following (Yu et al.,2023), we select a random subset of 100 responsesfor expert review to identify hallucinated and non-hallucinated objects. Similar to CHAIR, we re-port hallucination rates at both the object level andthe response level, offering a holistic view of themodels accuracy in depicting real-world objects.(3) POPE. Consistent with prior studies (Zhaoet al., 2023; Jiang et al., 2023), our evaluation in-corporates the Polling-based Object Probing Evalu-ation (POPE) methodology (Li et al., 2023). POPEleverages an automated segmentation tool to delin-eate objects within images, subsequently queryingthe model regarding their presence, as well as in-troducing random non-existent objects. We presentthe F1 scores, offering insights into the modelsimage perception capabilities.",
  "fluency. These metrics collectively assess the out-puts relevance, alignment, and readability.(1) Informativeness. Inspired by (Yu et al.,": "2023), this metric assesses the extent to whichthe generated captions encapsulate the primary el-ements depicted in the image. Utilizing the richannotations provided by the COCO dataset, weengage GPT-4 (OpenAI, 2023) to compare the an-notated objects, the ground-truth caption, and themodel-generated caption, subsequently assigning acoverage score. This process ensures that the eval-uation focuses on the captions ability to highlightsignificant image details.(2) Consistency to human response. The fi-delity of model-generated content to human-craftedresponses is gauged using the BLEU (Papineniet al., 2002) score, which measures the linguisticsimilarity between the machines output and expert-written ground truth captions. This metric servesas an indicator of how well the models responsesalign with human expectations and standards.(3) Fluency. The smoothness and natural flowof the text produced by the model are evaluatedthrough its perplexity when processed by a pre-trained GPT-2 (Radford et al., 2019) model. Alower perplexity score signifies higher text fluency,indicating that the generated narrative is coherentand easily comprehensible, mirroring the linguisticquality of the text.",
  "BEFUF is beneficial to otherhallucination mitigation methods": "EFUF stands out not only for its effectiveness andefficiency in dataset construction and training butalso for its compatibility with existing hallucinationmitigation strategies, such as RLHF and instructiontuning. This compatibility suggests that MLLMsalready enhanced with such techniques can furtherbenefit from the integration of EFUF, potentiallyleading to additional performance improvements.To validate this proposition, we conduct incre-mental experiments, selecting models enhancedwith RLHF (LLaVA-RLHF, Sun et al., 2023) andinstruction tuning (LRV, Liu et al., 2023a) as ournew baseline for comparison. These models arethen incrementally trained with EFUF. Results, de-tailed in , indicate a notable reduction inhallucination rates post-EFUF application, with-out compromising the quality of the generated text.This outcome underscores EFUFs value as an ad-ditive method, capable of augmenting the perfor-",
  "CEffects of different weight": "In this segment, we delve into the effects of vary-ing the weight assigned to the negative loss 1 andsentence loss 2 on the performance outcomes ofShareGPT4V model when trained using our EFUFstrategy. The investigation is aimed at understand-ing how adjustments in these parameters influenceboth the reduction in hallucination rates and theoverall quality of generated content, with resultsreported on validation set. (1) Effects of negative loss weight 1 As sum-marized in , as 1 is incremented from 0.1to 0.4, we initially note enhancements in both hal-lucination reduction and generation quality metrics,up until a value of 0.2. Beyond this threshold andpast the value of 0.3, a new trend emerges: whilethe rate of hallucinations continues to decline, a no-ticeable degradation in generation quality becomeapparent. This is particularly evident in the met-rics assessing informativeness and fluency, with themost pronounced effects observed once 1 exceeds0.4. Our case study further reveals the modelsdiminishing capacity to construct lengthy, informa-tive sentences at the value of 0.4, suggesting anoverly aggressive unlearning weight might inadver-tently impair the models foundational knowledgeand capabilities.",
  "Given these findings, a value of 0.3 for 1 isidentified as the optimal balance point, effectivelyminimizing hallucinations without compromisingthe integrity of generation quality": "(2) Effects of sentence loss weight 2 Contrast-ingly, the impact of 2 generally mirrors the in-verse of 1s effects. A value of 0.1 yields re-duced fluency, suggesting that such a low sentenceloss weight fails to exert sufficient influence. Con-versely, elevating 2 to 0.3 incites an increase inthe hallucination rate. This phenomenon can be at-tributed to an overly dominant sentence loss weight,which biases the model towards learning entire sen-tence patterns at the expense of neglecting to un-learn hallucinated content. Consequently, a valueof 0.2 for 2 is identified as the optimal setting,striking a balance between minimizing hallucina-tions and maintaining high-quality sentence gener-ation.",
  "DCase Study": "In this part, we present a comparative analysisthrough a case study, aiming to elucidate the dis-tinct advantages of our method EFUF. This com-parison involves the baseline MiniGPT4 model, aversion subjected solely to sentence loss, and themodel enhanced with our EFUF strategy.The case study, as depicted in , high-lights a scenario where the base MiniGPT4 modelerroneously predicts non-existent elements, suchas large windows and bookshelves. This er-ror is a clear instance of multimodal hallucination,where the generated content includes objects notpresent in the input image.The sentence-loss-only approach, while attempting to better alignthe model with multimodal contexts, falls short ofcompletely correcting these hallucinations. Thisshortfall is attributed to finetunings inherent limi-tation: it lacks a mechanism to explicitly signal tothe model which objects are inaccurately generatedand thus should be excluded from the output.In contrast, our EFUF approach successfullyaddresses this challenge. By integrating a fine-grained unlearning strategy, EFUF effectively dis-courages the generation of objects with low rel-evance to the given image. This direct interven-tion ensures that the model refrains from includ-ing hallucinated objects in its outputs, showcasinga significant improvement over the baseline andsentence-loss-only method.",
  "several tables and chairs set up in the room, with people sitting at them": "working on their laptops. The room has large windows on two sides, allowing natural light to pour in. There are also several bookshelves along the walls, filled with books and other materials. The overall atmosphere of the room is one of productivity and focus, with people working diligently on their tasks."
}