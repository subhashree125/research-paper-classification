{
  "Abstract": "We present a novel approach to modeling fic-tional narratives. The proposed model createsembeddings that represent a story such that sim-ilar narratives, that is, reformulations of thesame story, will result in similar embeddings.We showcase the prowess of our narrative-focused embeddings on various datasets, ex-hibiting state-of-the-art performance on multi-ple retrieval tasks. The embeddings also showpromising results on a narrative understandingtask. Additionally, we perform an annotation-based evaluation to validate that our introducedcomputational notion of narrative similarityaligns with human perception. The approachcan help to explore vast datasets of stories, withpotential applications in recommender systemsand in the computational analysis of literature.",
  "Introduction": "Narrative understanding is a field that has receivedmuch attention in the last few years. Various ap-proaches have tested models either on narrative-based question answering tasks or performed intrin-sic evaluations, such as narrative cloze evaluations,where models need to predict missing events in asequence.In this work, we seek to address the topic of storyembeddings with a focus on narrative, meaning rep-resentations that prioritize the aspect of what ishappening rather than the surface-level informationof how it is being told. For example, a love storywith a specific twist can be set in different settings(outer space or countryside), with a different cast(e.g., different names and some different traits forall characters), or in a shortened version, withoutfundamentally changing the narrative. After alter-ing the storys final twist, the new narrative couldstill be considered similar without being identical.Researchers in the ACL community have, in thecontext of fictional works, often used the terms nar-rative and story without a clear distinction (e.g., Chaturvedi et al., 2018; Chambers and Jurafsky,2009). The field of narratology has a multitudeof competing terms to offer, specifically to distin-guish between the order of events as presented tothe reader (commonly used terms are Syuzhet, Plotand Discours) and that of the actual happenings inthe narrated world (commonly used terms are Fab-ula, Story and Histoire) (Kukkonen, 2019). In thiswork, we refer to the story as the entirety of the nar-ration abstracted from the individual formulation,whereas we use narrative specifically to refer to thestorys structure. Thus, a narrative could broadlybe seen as the order and relationship of events inthe story, but it does not include other information,such as the setting, tone, and style of the story. This work presents a contrastive-learning-basedapproach for training story embeddings using a pre-existing dataset. We assume that any fictional textcan be represented by its summary for our purposesof modeling the narrative. While there are variouscharacteristics of a story that can not be gleanedfrom a summary, such as the style and mood ofa text, the narrative is core to what is representedin a summary. Thus, summaries are the perfecttesting ground for narrative embeddings, althoughan expansion to full texts in the future is desirable. It has been observed that retellings of specifi-cally fairytales have recently increasingly beenpublished, with many retellings changing the set-ting to a modern-day one or introducing the repre-sentation of minorities (Goldman, 2023). As such,they represent a structurally similar story, with anew setting and limited alterations to the narrative.Other retellings, however, change the story signif-icantly, sometimes merely retaining themes fromthe original work. On a limited scale, previouswork has addressed the automatic identificationof stories following the same plot (Glass, 2022).In this work, we consider this task as a possibleapplication of story embeddings.",
  "Related Work": "A substantial line of work (e.g. Chambers and Ju-rafsky, 2008, 2009; Granroth-Wilding and Clark,2016) has dealt with graph-based representations ofnarratives, specifically with predicting missing nar-rative triples and inferring schemas of commonlyre-occurring narratives. Lee and Jung (2020) takewhat can be considered a hybrid approach, build-ing explicit networks but using contextual vectorrepresentations rather than lexical items to repre-sent triples. Similarly, using less contextual infor-mation, in prior work, we trained narrative tripleembeddings based on narrative chains (Hatzel andBiemann, 2023). Following ever-increasing ad-vancements in the field of language models andmotivated by the information loss inherent to ex-tracting narrative triples, this work seeks to apply amore distantly supervised approach to representingstories.Our work builds on two previously releaseddatasets (Hatzel and Biemann, 2024; Chaturvediet al., 2018). Both datasets contain story summariesextracted from Wikipedia. Specifically, both seekto find different formulations of summaries forvery similar stories. The movie remake datasetby Chaturvedi et al. (2018) contains a relativelysmall collection of summaries from multiple re-makes of the same movie. In contrast, our previ-ously released dataset, Tell-Me-Again (Hatzel andBiemann, 2024), collects summaries from multipleWikipedia language versions of the same fictionalwork. The movie remake dataset only contains266 summaries and is thus not suited for training,whereas Tell-Me-Again contains roughly 30,000stories. Each story comes with up to five differ-ent summaries, originally extracted from multi-ple Wikipedia language versions and automaticallytranslated into English. The dataset additionallycomes with a pseudonymized variant, explicitly cre-ated for training models that do not focus on entitynames. In this variant, entity names are replacedin each summary by alternatives in an internallyconsistent manner. These pseudonymized versionsare created using rule-based replacement strategieson top of a model-based coreference resolution sys-tem.ROCStories is a dataset for testing common-sensereasoning,firstreleasedin2016(Mostafazadeh et al., 2016) with the intro-duction of the Story Cloze Task.In the task,systems pick one of two sentences as the end of a five-sentence story. One choice is a logicalconclusion to the story, but the other choice onlymatches in terms of vocabulary and is not a fittingconclusion to the story. As a result, humans cansolve the Story Cloze Task perfectly, but at thetime of publication, the best-performing system inan accompanying shared task reached only around75% accuracy. The original task formulation didnot allow for supervised learning, providing onlycomplete five-sentence stories without two choicesas training data. The creation of semantic sentence representa-tions with large language models (LLMs) hasrecently gained much interest.While Wanget al. (2024) train embeddings from last-tokenhidden states, it has been suggested that thecausal attention mechanism in generative decoder-only models limits their effectiveness for embed-dings (BehnamGhader et al., 2024).Alterna-tives have been proposed in the form of addingbidirectional attention back into existing models(BehnamGhader et al., 2024) or by duplicating theinput sequence, thereby functionally allowing eachtoken to attend to every other token (Springer et al.,2024). Ultimately, the new approaches were shownto be more training-sample-efficient but did notshow real inference quality gains over the exten-sively finetuned E5 model by Wang et al. (2024).Embedding approaches are typically focused onvery short sequences of text, particularly individ-ual sentences (Reimers and Gurevych, 2019; Niet al., 2022). Doc2Vec (Le and Mikolov, 2014) is astatic-embedding-based approach to document em-beddings. While it was primarily evaluated on shortsegments, it does not have a limitation regardingthe input size, a common constraint in transformer-based approaches. The definition of what exactly constitutes narra-tive similarity has been addressed by Chen et al.(2022a) in their corresponding codebook (Chenet al., 2022b). In a pairwise similarity annotationtask, they explicitly ask annotators to consider thenarrative schemas and to ignore the specific namesof entities, only considering their roles. They donot define an exact measure of how distances be-tween schemas are determined, nor do they instructannotators to write down explicit schemas. Despitethese limitations, they achieve comparatively goodinter-annotator agreement (0.69 Krippendorfs )on narrative similarity of news articles.",
  "Our Approach": "Our model, called StoryEmb, is a causal languagemodel whose last token representation is fine-tunedon similarity tasks using augmented data. Ourmodel is trained to produce representations thatare similar for multiple summaries of the samestory. As a foundation model, we use Mistral-7B(Jiang et al., 2023a). Specifically, we use E5 (Wanget al., 2024), an adapter-finetuned variant, trainedusing synthetic data, for similarity modeling. Asstory similarity is a complex task, we assume thata more capable model would perform better; dueto hardware constraints, we chose a 7B parametermodel.We train our model using Gradient Cache (Gaoet al., 2021) to enable large batch sizes on limitedhardware while reaching identical results to tradi-tional similarity training. In training, we optimizefor reducing the cosine similarity between pairs ofsummaries labeled as the same while maximizingthe cosine similarity between those pairs that, bynature of belonging to different works, are implic-itly labeled as different. Our approach follows Gaoet al. (2021) in using contrastive MSE-loss for simi-larity training. We use a batch size of 1000 positivepairs and in-batch negatives. For the optimizer, weuse Adam with a learning rate of 5 105 and per-form early stopping on a subset of pseudonymizedsummaries from the Tell-Me-Again dataset. Thetraining is limited to the adapter parameters and, aswe are training based on their weights, we followWang et al. (2024) and use LoRA with rank r = 16and = 32. While our training setup differs invarious details (we use a different loss and do notemploy hard negatives), the training can be consid-ered a continued fine-tuning of E5 with a similarobjective, just focusing on narrative similarity.Our training data is sampled from the Tell-Me-Again dataset but limited to only summaries with aminimum of 10 and a maximum of 50 sentences inlength. This is motivated by the desire to exclude(a) very short synopses and loglines on the low endand (b) documents that are too memory-demandingon the high end. The length limit could be subjectto further experimentation in the future. We evalu-ate whether the data augmentation approach re-placing names with alternative ones in a consistentmanner proposed by Hatzel and Biemann (2024)can improve the performance of a similarity model.To this end, we compare an augmented versionof our model, trained on pseudonymized versions of the original summaries, and a non-augmentedversion, trained on the original summaries.Following the E5 paper, we add a query prefixto each document. Through manual exploration onthe development set, we selected the query, Re-trieve stories with a similar narrative to the givenstory: . While many of the original applications ofE5 follow an asymmetric setup where the query andthe document are encoded using separate prompts,our prompt aligns well with one of their evaluationprompts: Retrieve tweets that are semanticallysimilar to the given tweet. BehnamGhader et al. (2024) have recently in-troduced a more sample-efficient way, calledLLM2Vec, to train LLMs for sentence reprenta-tions. In preliminary experiments, we found, per-haps in part due to length limitations in training asa result of the full-attention setup, an LLM2Vec-based model to perform inferiorly to our model.",
  "Experiments": "After training, we perform several downstream taskexperiments to explore the capabilities and charac-teristics of our narrative embeddings. Three experi-ments test narrative retrieval capabilities (.1). We also perform an experiment focused onnarrative understanding (.2). All our ex-periments in this paper are limited to English data.Recall that our training data consists of pairs ofstory summaries automatically translated from var-ious languages to English.",
  "Narrative Retrieval": "Using four different tasks, we test if our embed-dings can be used for retrieving narratively similarstories. All retrieval tasks are performed using em-bedding cosine distances.For the initial three retrieval experiments, thosewith gold data available, we follow Chaturvedi et al.(2018) in using P@1 (precision at one), in otherwords accuracy for the most relevant result. Ad-ditionally, we introduce the P@N (precision at n)metric to allow for easy interpretation of the re-sults. It measures the precision in the N-top re-sults, where N is the number of gold items in therespective cluster. For reference, we also includethe more established information retrieval metricsof MAP (mean average precision), NDCG (normal-ized discounted cumulative gain), and R-Precision(Manning et al., 2008).",
  "In-Task Performance": "In prior work (Hatzel and Biemann, 2024), wetested various existing models on pseudonymizedand non-pseudonymized versions of the dataset,finding that all models, especially smaller ones,perform very poorly on the pseudonymized ver-sions. In the existing publication, we attribute thisto those models reliance on entity names, show-ing that a bag-of-word system based only on entitymentions already performs well.",
  "In-Domain Adaptation: Movie RemakeDataset": "We expect retrieval performance on the movie re-make task to be worse than on the Tell-Me-Againdataset, as one would expect summaries across re-makes to show more variations than summariessourced from various languages. This would alignwith our previous results (Hatzel and Biemann,2024), where the best-performing model reached aP@1 of 64.4% on the remake dataset but reached90.5% on the Tell-Me-Again dataset (in a setupwhere the Tell-Me-Again dataset was subsampledto replicate the movie remake datasets distribu-tion). As for the Tell-Me-Again dataset, we test onboth the original summary and the pseudonymizedversion, with two model variants trained on eithervariant of the dataset.",
  "Retellings": "We collect a small set of summaries of works offiction, each considered a retelling or a retellingsoriginal. The collection methodology amounted toprompting ChatGPT for close retellings to limit thevariations in the narrative.1 The model was essen-tially used to suggest retelling relationships, andthe list was subsequently checked for validity usingmanual web searches. While we considered otherapproaches, such as using existing lists of retellings,we decided, in part due to a lack of authoritativelists of this nature, to retrieve very commonly men-tioned pairs using a language model instead. Afterdiscarding various suggestions that did not haveEnglish Wikipedia articles with plot summaries,we ended up with 13 clusters of retellings totaling30 story summaries.Retellings often change the story in major ways,more so than we would expect in a movie remake.We expect retellings to deviate more from eachother than both multiple summaries of the same",
  "See Appendix B for the prompt and further details": "story and summaries of movie remakes.How-ever, they may retain similar or identical characternames, a characteristic that is not aligned with ourpseudonymized training data. Given these char-acteristics, we initially anticipated that our modelwould find the retelling retrieval task more chal-lenging than identifying movie remakes. We re-lease retelling the dataset, including the full sum-maries, alongside our code, in a format matchingthat by Chaturvedi et al. (2018) for easy compari-son.",
  "Segment Retrieval": "To generalize these findings to a broader story re-trieval problem, we perform an annotation-basedexperiment, asking LLM judges and human annota-tors to rate the narrative similarity of text pairs.While a human-curated dataset of similar storypairs may also be desirable, we do not see a clearpath to creating one. A human judgment of similar-ity relies on recalling a large set of stories, which isnot generally achievable with annotators. So, ourexperiment instead relies on testing pairs of textsthat the model considers to be very similar or dis-similar using human annotators. We follow Chenet al. (2022a) in broadly annotating for similarityin narrative schemas without making them explicitduring annotation. A more precise definition of nar-rative similarity on the basis of schemas could bethe subject of future work, but we do not considerit essential for this limited-scale experiment.For this experiment, we select a moderately sizedfiction dataset in which we expect to find frequentoccurrences of similar scenes. We select a set ofpublic-domain detective novels for this purpose.2 The novels are split into segments of no more than2000 whitespace-separated tokens using a rule-based splitting solution.3 Said segments are subse-quently summarized using LLaMA3s 70B4 (at full16-bit precision) variant with the prompt Pleasesummarize the following text in three sentencesor less.. The resulting summaries are embeddedusing our StoryEmb model.Initially, we remove all obviously similar pairsof summaries by discarding all pairs with a simi-larity higher than 0.3 according to MiniLM.5 Thisensures that duplicates that occur across documents in the dataset are not used as trivial examples ofnarrative similarity. We evaluate the similarity ofthe 50 most similar segment pairs and 50 least-similar pairs in two setups: (a) first with an LLMjudge and (b) with a human judge. For the lat-ter, we sample just 10% of segments, using thesame similarity ranks for both models (samplingfrom the same ranks in terms of similarity in thepseudonymized and the standard E5 model with atask prefix). Judges are asked to rate the similarityof segments on a scale of 1-10. The LLM judgeevaluates our embeddings in two scenarios basedon the segments original full text and its automat-ically generated summary. For time reasons, thehuman judge only operates on automatically gen-erated summaries. For the judge model, we useGPT4-o in a two-turn setup; for further details onthe LLM judge setup, see Appendix A.",
  "Narrative Understanding: ROCStories": "Finally, we perform an experiment aimed at validat-ing the common-sense understanding of our modelusing the Story Cloze Task. In story cloze, given acommon-sense story of four sentences, the systemhas to select the final fifth sentence of the storyfrom two choices: an incoherent but surface-level-consistent ending and the correct and semanticallycoherent one. To test our embeddings, we take anunconventional approach to inference on this task,enabling evaluation without a classification head orsimilar techniques. We embed three components:the first four sentences of the story that we referto as the anchor a and two variants of the entirefive-sentence story with either the second or thefirst option: s1 and s2. Our system predicts thestory that is closer to the anchor embedding. Theintuition behind this is that a good story embed-ding already encodes expected outcomes, leadingto a vastly different embedding for the incorrect,unfitting ending.",
  "Results": "As seen in , our StoryEmb model achievesstate-of-the-art results on the Tell-Me-Againdataset, outperforming all other tested models in allbut one metric. On the test set, our model, trainedonly using the augmented Wikipedia summaries,reaches a P@N of 65.89% in the pseudonymizedsetup on a dataset of almost 10,000 story sum-maries.That is to say, over all retrieved sum-maries, which correspond to the number of goldreformulations for each summary, 65.89% of themare correct. This is a pronounced drop comparedto the 85.90% on the original texts, but com-pared to other models, the drop is much smaller.This is also true for the most competitive model,Sentence-T5 in its XXL variant (Ni et al., 2022),which marginally outperforms StoryEmb in P@1on the non-pseudonymized dataset. Sentence-T5reaches a P@1 of 94.98%, while StoryEmb onlyreaches 94.64% on the original data.On thepseudonymized data, however, Sentence-T5 onlyreaches a P@1 of 67.28%, while our approachyields 82.6%.We also test a pre-trained doc2vec model (Lauand Baldwin, 2016) as a more traditional base-line with no inherent length limitation.Out-side of our own models performance, it is in-teresting to see doc2vec outperform E5 by faron the pseudonymized version of the dataset; thestatic-embedding model exhibits no noticeabledrop in performance from the standard to thepseudonymized setting (in fact, the results on thepseudonymized version are marginally better forall metrics). Upon consideration, this is not sur-prising as the static word embeddings in doc2vecmay have a hard time with generic entity names,especially personal names.Whiletheperformanceincreaseonthepseudonymized texts is expected, it is surprisingthat, even for the non-pseudonymized texts, themodel trained on augmented data performs bet-ter. As noted earlier, our models training wasstopped early based on the performance on thepseudonymized texts (for both model variants).The training finished after just three training steps(after seeing no improvements for two more steps),with each step taking roughly 1 h 40 min on twoA100 GPUs. In fact, the unaugmented model con-tinues to improve on the non-pseudonymized dataafterward, presumably due to an ever-increasingfocus on entity names as a shortcut to solving the",
  "Movie Remakes": "The results for the movie remake dataset listed in are state-of-the-art for said dataset witha top P@1 score of 83.26%, improving by morethan 20 points over the original story-kernel ap-proach by Chaturvedi et al. (2018). On this dataset,we also outperform Sentence-T5 by a considerablemargin, reaching 80.28%, an almost 7-point im-provement over their result of 73.35% in terms ofP@N. Again, we can clearly observe the positiveeffects of the pseudonymization data augmentation.We also provide results for the unaugmented Sto-ryEmb model trained for two more steps, therebyalmost doubling the fine-tuning data. Yet, despitethe additional training data, our non-augmentedmodel on the non-pseudonymized dataset does notmeaningfully improve. Additional training onlyimproves the P@1 score of 63.09% by just over .2points to 63.30%, clearly demonstrating the effec-tiveness of the data augmentation approach. Bothversions of our model substantially outperform thebase E5 model, an effect that we attribute to do-main adaptation, including an adaptation to longerdocuments.An interesting takeaway from the results on themovie remake dataset is a very pronounced drop inthe performance of sentence T5 as compared to theTell-Me-Again results. While the model showeda P@N of 94.98 on the non-pseudonymized Tell-Me-Again data, its performance dropped by morethan 17 points to 77.61% on the movie remakedataset, while our augmented StoryEmb model lostless than 6 points on the same metric (85.9% to80.28%) across the datasets. Initially, we suspectedthat this may be caused by a case of training dataleakage, with T5 having incorporated Wikipediacross-language version training. This could not beconfirmed as, when limiting evaluation to worksfrom 2022 or 2023, after the release of Sentence-T5, it actually performed comparatively even bet-ter, reaching a P@1 of 96%, where our augmentedmodel only reached 83%. This points to a seman-tic difference in remakes compared to languageversions that StoryEmb captures better as the ex-planation, demonstrating superior generalizationcapabilities for narrative retrieval in StoryEmb.",
  "The retrieval performance on the retelling datasettests our models capabilities in an alternative sce-": "nario with different requirements. On this dataset,Sentence-T5 outperforms our model by a consider-able margin, reaching a P@1 of 70.0%, while ourbest-performing model-variant, the unaugmentedmodel, reaches 60.0%. Our model still handilyoutperforms vanilla E5 at a P@1 of 16.67%. Addi-tionally, despite its great performance on previoustasks, the model trained on augmented data nowunderperforms as compared to the unaugmentedversion. Given the datasets limited size, we expectthat an entity-focused approach works better. Weexpect that names serve as easy disambiguatorsin a smaller dataset but lose discriminative perfor-mance as the number of samples grows. To testthis hypothesis, we add the summaries from themovie remake dataset as distractors to the task. Inthis setup, we see the margin by which T5 over-performs shrink considerably, especially for theP@N metric, where the best Sentence-T5 modelnow only outperforms our unaugmented model byroughly 2 points, with a score of 57.69% as com-pared to 59.62%. See Appendix C for the full tablewith all metrics.Interestingly, and despite the much smallerdataset size of only 30 rather than 266 summaries,our models and the baselines retrieval perfor-mance is much worse than on the movie remakedataset. This indicates that identifying retellingsis a challenging task. At the same time, it is un-clear if retellings are best identified using narra-tive features, given that they may only align withthe storys themes. Our augmented models un-derperformance may indicate that a name-focusedapproach is better suited to this task.",
  "Scene Retrieval": "shows the narrative similarity ratings of ourLLM judge and human annotator (the first author ofthis paper). The LLM judge favors the StoryEmbmodel when operating on the summaries of the re-trieved segments, with the score increasing from5.1 to 5.36 out of 10 when using our model insteadof E5. This difference is much more pronouncedwhen the LLM judge operates on the full segmentsinstead. While our model is still rated at 5.36, theE5 model now only gets a score of 4.94. Our modelalso does better at retrieving dissimilar passages.We consider those passages retrieved by StoryEmbto have an average similarity of 3.6/10, in our an-notations, whereas the segments retrieved by E5score 4.6/10.The LLM judgments on the full-text segments",
  "Top5.15.364.945.365.86.6Bottom4.063.844.223.724.63.6": ": Mean narrative similarity score on a scale of 1-10 in top vs. bottom ranked scenes in terms of similarityas judged by an LLM judge or an annotator, after re-moving obvious duplicates. The first author performedthe annotations. GPT-3 with only 7B parameters. At the same time,Sentence-T5, an otherwise strong model, does notexhibit great performance. Note that our results areobtained on the development set. Recall that wedo not perform specific training for this task; Weneither train on next-sentence prediction nor usetraining data similar to the story cloze dataset.The results show that an expected event in thestory changes the embedding less than an unex-pected one. Thus, this experiment indicates a highlevel of narrative understanding exhibited by ourstory embeddings.",
  "RoBERTasupervised97.9": ": We list the accuracy at picking the correct storyending from two options on the ROCStories dataset.The superscript denotes that the embedding distanceapproach outlined in .2 is used and evaluatedon the development set. The GPT-3 and FLAN resultsare taken from Wei et al. (2022), and the supervisedRoBERTa result is taken from Jiang et al. (2023b). dararajan et al., 2017), extending the concept toSiamese networks, specifically sentence encoders.In essence, the approach samples gradients alongan interpolation from a semantically neutral se-quence to the analyzed sequence, identifying fea-tures that the output is sensitive to. The result isa token-token matrix across two input sequences,signifying the contribution of any two terms to theoverall similarity of the two sequences. Moeller et al. (2024) operate only on modelsthat use average pooling across tokens. We adapttheir implementation to work with decoder-onlymodels and use 50 interpolation steps. E5 employslast-token pooling, using the last tokens hiddenstate as a sentence representation. As a result, theattribution scores are muddled: information needsto flow to the last token, leading to the majorityof the similarity being explained by changes inthe last tokens representation. In an effort to getinterpretable attribution scores despite the poolingapproach, we display the delta in attribution scoresfrom the E5 model to our StoryEmb model instead. illustrates that our augmented-data ap-proach does, in fact, place less emphasis on namedentities than the vanilla E5 model. We comparesimilarity across the two sentences Alice wakesup. and Alice falls down.. Generally, negativevalues in an attribution indicate that two specifictokens interact to reduce the similarity of the twosentences. The attribution scores are com-puted by deducting the E5 attribution values fromthe StoryEmb attribution values. Thus, a negativevalue means our StoryEmb model places compara-",
  "Alice": "w akes up . 0.006 0.004 0.002 0.000 0.002 0.004 0.006 : Attribution scores on individual tokens inthe final layer of our StoryEmb model are shown as adelta from the E5 model. Negative scores indicate lesscontribution to the similarity in the StoryEmb model. Inthe example, it seems clear that less emphasis is placedon named entities. tively less emphasis on said value. In this case, ourfine-tuned model places much less importance onthe name Alice than the original E5 model does.To generalize from the single example, we col-lect attribution scores for 50 random sentencesfrom the STS benchmark datasets test set (Ceret al., 2017). We collect average attribution scoresfor part-of-speech and named-entity tags, showingthe results in . One can see the expectedeffect from our training on the part of speech tags;the model places much less emphasis on propernouns (PROPN), while verbs (VERB) contributeslightly more to similarity scores. This effect isalso visible for the named entity tags, with persons(PERS) and organizations (ORGS) being consid-ered less relevant. Using the named-entity tags,however, we can also observe an unwanted sideeffect of the data augmentation; as dates are notremoved by the data augmentation, they can stillserve as a shortcut for solving the task, and ourStoryEmb model prioritizes them.We have restricted the analysis to single sen-tences as it is computationally expensive and couldnot feasibly be performed on entire stories.",
  ": The average contribution to sentence similarityof selected named-entity and parts-of-speech tags wasanalyzed on layer 31 of the E5 and StoryEmb models.The statistics exclude our task prefix": "segments, using the approach from .1.4,excluding trivially similar texts. We found manyinstances of meaningfully narratively similar textsbeing retrieved by StoryEmb. In , we showa pair of segment summaries from the detectivenovel dataset, with the segment from The Tri-umphs of Eugene Valmont being considered the17th closest, by StoryEmb, to the In the Fog seg-ment. E5, meanwhile, only considers it to be the221st closest segment. The two segments share avery similar narrative of a necklace theft with a sub-sequent arrest but lack any shared named entities.",
  "Conclusion": "In this work, we presented an approach to creat-ing embeddings that represent stories, specificallytheir narratives. Our StoryEmb model, trained onthe Tell-Me-Again dataset, shows state-of-the-artperformance on both the corresponding test splitand on the movie remake dataset (Chaturvedi et al.,2018), far outperforming both recent LLM-basedmodels and the static-embedding baseline. Wedemonstrate the models retrieval capability in prac-tice on summaries of passages of literary texts. Fur-ther, we explore the retrieval of retellings on a novelsmall-scale dataset, opening up another potentialapplication avenue.On the movie remake dataset, we clearly demon-strate the effectiveness of the Tell-Me-Again dataaugmentation approach, producing better resultson both pseudonymized and non-pseudonymizedversions of the texts. Similarity score attributionindicates that the data augmentation techniques em-ployed have the desired effect: making the modelplace less emphasis on names.Our StoryEmb model also performs strongly on The narrator, a Queens Messenger, wastraveling from Paris to Marseilles with avaluable diamond necklace belonging to theQueen of England. During the journey, hewas distracted by a charming woman whostole the necklace from his bag while pre-tending to be friendly and conversational.When the narrator discovered the theft, herushed to the police station and used his cre-dentials to demand assistance in capturingthe thief, explaining that the successful re-covery of the necklace would bring greatrewards and gratitude from three powerfulnations.",
  "Summarized segment from In the Fog by RichardHarding Davis": "A private detective from London explainshow he became involved in a case involvinga stolen necklace and a man who jumpedoverboard from a yacht. The detective fol-lowed the thief and ended up on the yacht,where he claims the man may have escapedwith the jewels. Despite his story, the En-glishman is arrested and held for three weeksuntil a letter arrives from New York, re-vealing that the missing Frenchman, MartinDubois, is alive and blaming the police forany harm that may have come to him.",
  "Future Work": "In the future, we expect to explore application fieldsfor story embeddings further. Outside of employinglarger foundation models, the contrastive learningapproach can potentially be improved. We assumethat one may also, at the cost of compute resources,be able to create a better model by not using a pre-trained similarity model but instead starting from aplain language model.Similarly, there is potential to generate bettertraining data by (a) improving the pseudonymizedversions or (b) creating new summaries, potentiallyby combining multiple existing ones using LLMprompting.",
  "Regarding training data leakage, we can assumethat the foundation model has seen all Wikipedia": "summaries. We do not expect this to be a substan-tial issue as we expect the different language ver-sions to individually be trained on, as evidenced byrelatively poor performance without further train-ing. However, it is possible that the data augmenta-tion strategy has limited success with entity namesbeing inferred from the unredacted text seen intraining.It is possible that, given further hyperparametertuning, the results could be noticeably improved.Contrastive learning, especially in the image space,has seen many optimizations. Due to resource con-straints, this work was out of scope for this study.In initial experiments, we did not succeed withbatch-sampling techniques, but it can be assumedthat further exploration could yield improvements.The representations we present lack interpretabil-ity compared to schema-based approaches to narra-tive modeling. At present, we cannot confidentlyidentify which aspects of a story and its narrativeare captured in our embeddings, and more work isrequired to understand exactly which informationis captured by StoryEmb.Our ROCStories results were obtained on thedevelopment set as the test set is privately held.We contacted the original authors and researcherswho recently reported results on the dataset butwere unable to get our predictions for the privatetest set scored. We have no reason to believe ourperformance on the test set would be worse.",
  "Ethical Considerations": "We do not see any major ethical problems. Thedata augmentation strategy in the original Tell-Me-Again dataset picks names based on US censusstatistics, potentially contributing to a system thatmay be regionally and culturally biased. This limi-tation is inherent to many NLP systems and shouldbe addressed before approaches like this are usedproductively.",
  "Siva Reddy. 2024. LLM2Vec: Large Language Mod-els Are Secretly Powerful Text Encoders. Preprint,arxiv:2404.05961": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017.SemEval-2017Task 1: Semantic Textual Similarity Multilingualand Crosslingual Focused Evaluation. In Proceed-ings of the 11th International Workshop on SemanticEvaluation (SemEval-2017), pages 114, Vancouver,Canada. Association for Computational Linguistics. Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-vised Learning of Narrative Event Chains. In Pro-ceedings of ACL-08: Human Language Technologies,pages 789797, Columbus, Ohio, USA. Associationfor Computational Linguistics. Nathanael Chambers and Dan Jurafsky. 2009. Unsu-pervised learning of narrative schemas and their par-ticipants. In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2 - ACL-IJCNLP09, volume 2, pages 602610, Suntec, Singapore.Association for Computational Linguistics. Snigdha Chaturvedi, Shashank Srivastava, and DanRoth. 2018. Where Have I Heard This Story Be-fore? Identifying Narrative Similarity in Movie Re-makes. In Proceedings of the 2018 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 2 (Short Papers), pages 673678,New Orleans, Louisiana, USA. Association for Com-putational Linguistics. Xi Chen, Ali Zeynali, Chico Camargo, Fabian Flock,Devin Gaffney, Przemyslaw Grabowicz, Scott Hale,David Jurgens, and Mattia Samory. 2022a. SemEval-2022 task 8: Multilingual news article similarity. InProceedings of the 16th International Workshop onSemantic Evaluation (SemEval-2022), pages 10941106, Seattle, Washington, USA. Association forComputational Linguistics. Xi Chen, Ali Zeynali, Chico Q. Camargo, FabianFlock, Devin Gaffney, Przemyslaw A. Grabow-icz, Scott A. Hale, David Jurgens, and MattiaSamory. 2022b. SemEval-2022 Task 8: Multilin-gual news article similarity. Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.2021. Scaling Deep Contrastive Learning Batch Sizeunder Memory Limited Setup. In Proceedings of the6th Workshop on Representation Learning for NLP(RepL4NLP-2021), pages 316321, Online. Associa-tion for Computational Linguistics.",
  "Melanie Goldman. 2023.The Rise of FairytaleRetellings in Publishing. Publishing Research Quar-terly, 39(3):219233": "Mark Granroth-Wilding and Stephen Clark. 2016. WhatHappens Next? Event Prediction Using a Composi-tional Neural Network Model. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 30, pages 27272733, Phoenix, Arizona, USA. Hans Ole Hatzel and Chris Biemann. 2023. Narrativecloze as a training objective: Towards modeling sto-ries using narrative chain embeddings. In Proceed-ings of the 5th Workshop on Narrative Understand-ing, pages 118127, Toronto, Canada. Associationfor Computational Linguistics. Hans Ole Hatzel and Chris Biemann. 2024. Tell MeAgain! a Large-Scale Dataset of Multiple Summariesfor the Same Story. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1573215741, Turin, Italy.ELRA and ICCL. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Lelio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothee Lacroix,and William El Sayed. 2023a. Mistral 7B. Preprint,arxiv:2310.06825. Yifan Jiang, Filip Ilievski, and Kaixin Ma. 2023b. Trans-ferring Procedural Knowledge Across CommonsenseTasks. In 26th European Conference on Artificial In-telligence, September 30October 4, 2023, Krakow,Poland, pages 11561163, Krakow, Poland. IOSPress.",
  "Karin Kukkonen. 2019.Plot.In The LivingHandbook of Narratology. Hamburg:HamburgUniversity": "Jey Han Lau and Timothy Baldwin. 2016. An EmpiricalEvaluation of doc2vec with Practical Insights intoDocument Embedding Generation. In Proceedingsof the 1st Workshop on Representation Learning forNLP, pages 7886, Berlin, Germany. Association forComputational Linguistics. Quoc Le and Tomas Mikolov. 2014. Distributed Repre-sentations of Sentences and Documents. In Proceed-ings of the 31st International Conference on MachineLearning, pages 11881196, Beijing, China. PMLR.",
  "Christopher D. Manning, Prabhakar Raghavan, andHinrich Schutze. 2008. Evaluation in informationretrieval. In Introduction to Information Retrieval,pages 151175. Cambridge University Press": "Lucas Moeller, Dmitry Nikolaev, and Sebastian Pado.2024. Approximate Attributions for Off-the-ShelfSiamese Transformers. In Proceedings of the 18thConference of the European Chapter of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 20592071, St. Julians, Malta. Asso-ciation for Computational Linguistics. Nasrin Mostafazadeh, Nathanael Chambers, XiaodongHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,Pushmeet Kohli, and James Allen. 2016. A Corpusand Cloze Evaluation for Deeper Understanding ofCommonsense Stories. In Proceedings of the 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 839849, San Diego,California, USA. Association for Computational Lin-guistics. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant,Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. In Findings of the As-sociation for Computational Linguistics: ACL 2022,pages 18641874, Dublin, Ireland. Association forComputational Linguistics. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics.",
  "BRetelling Dataset": "We prompt ChatGPT with the following prompt:Little Fuzzy is a modern retelling of FuzzyNation that is relatively close in terms of storyit can thus be considered a close retelling. Whatare some other pairs of close retellings?Note that we did not see a large variation ofretellings produced on variations of the prompt,with many pairs frequently reoccurring even whenexplicitly asking for distant or far remakes. For thisreason, we decided not to build two splits of thedataset with far and close remakes."
}