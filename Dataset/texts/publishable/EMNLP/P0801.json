{
  "Abstract": "Medical decision rules play a key role inmany clinical decision support systems (CDSS).However, these rules are conventionally con-structed by medical experts, which is expensiveand hard to scale up. In this study, we explorethe automatic extraction of medical decisionrules from text, leading to a solution to con-struct large-scale medical decision rules. Weadopt a formulation of medical decision rulesas binary trees consisting of condition/decisionnodes. Such trees are referred to as medical de-cision trees and we introduce several generativemodels to extract them from text. The proposedmodels inherit the merit of two categories ofsuccessful natural language generation frame-works, i.e., sequence-to-sequence generationand autoregressive generation. To unleash thepotential of pretrained language models, wedesign three styles of linearization (natural lan-guage, augmented natural language and JSONcode), acting as the target sequence for ourmodels. Our final system achieves 67% treeaccuracy on a comprehensive Chinese bench-mark, outperforming state-of-the-art baselineby 12%. The result demonstrates the effective-ness of generative models on explicitly model-ing structural decision-making roadmaps, andshows great potential to boost the developmentof CDSS and explainable AI. Our code will beopen-source upon acceptance.",
  "Introduction": "Currently, the development of clinical decision sup-port systems (CDSS) relies heavily on manual enu-meration of medical decision rules (Matsumuraet al., 1986; Grosan et al., 2011; Shortliffe andSeplveda, 2018). Although this paradigm bringsCDSS interpretability and reliability, its request ofextensive labor poses a challenge on scaling, giventhe huge amount of potential medical decision rules(Tsumoto, 1998). And the fact that some medical",
  ": An example (translated from Chinese) ofextracting tree-form medical decision rules from clinicalguidelines and textbooks": "decision rules get occasionally updated make thechallenge even worse. This motivates researchersto explore the automation of medical decision rulesconstruction. Inspired by the fact that human doc-tors acquire medical decision rules from textbooksand clinical guidelines, a recent study proposes toimitate this process via deep learning methods (Zhuet al., 2022).There exist two typical formulations of medicaldecision rules: first-order predicate logic formu-las (Matsumura et al., 1986; Tsumoto, 1998) andmedical decision trees (Zhu et al., 2022), wherethe latter is an extension of the former. Formally, amedical decision tree is a binary tree consisting ofcondition nodes and decision nodes. Each node is arelation triple or multiple relation triples combinedby logical operators (OR, AND). The decisionnodes are leaf nodes of the tree, whereas the con-dition nodes are internal nodes. And the transitionfrom one node to another represents judgment ordecision-making. A first-order predicate logic for-mula in conjunctive normal form can be viewedas a special case of a medical decision tree where there is only one condition node and one decisionnode. Hence, we adopt the tree-form formulationin this paper.Different from traditional information extrac-tion tasks, e.g., name entity recognition (Tan et al.,2021; He and Tang, 2022), relation triple extraction(Yan et al., 2021; He and Tang, 2023) and event ex-traction (Yang et al., 2021; He et al., 2023), wherethe target output is a set of unitary/dual/multivari-ate tuples, the target output of medical decisiontree extraction is a logically combined complex ofrelation triples. The logical coherence exhibited bysuch complexes mimics that of human language.This motivates us to adopt generative approachesfor medical decision tree extraction, so as to bettermodel the intrinsic logical connection among therelation triples inside a medical decision tree.Reflecting on the exciting success within thefield of natural language generation, we can ob-serve that two paradigms (sequence-to-sequence,autoregressive generation) along with the idea ofpretraining play the crucial roles. In this work,we try to replicate the success of sequence-to-sequence/autoregressive generation on the task ofmedical decision tree extraction.In order to maximally elicit the potential of pre-trained generative language models, three designsof medical decision tree linearization are trialed: 1)natural language (NL) style of linearization, wherethe relation triples are verbalized and naturally as-sembled with conjunctions; 2) augmented naturallanguage (AugNL) style of linearization, whereeach relation triple is represented as an augmentedtoken, sharing equal status with natural languagetokens; 3) JSON style of linearization, the mostwidely used data interchange format that representsdata objects as keyvalue pairs. The linearizedmedical decision trees act as the target sequencesduring training, and are generated then parsed intotree structure during inference.The proposed sequence-to-sequence models em-ploy an encoder-decoder architecture with a pair ofpretrained language encoder and decoder, as wellas a query-based entity-relation extractor. Underthis paradigm, relation triple extraction is treatedas a sub-task and the models fulfill it via the entity-relation extractor. Whereas the proposed autore-gressive models are instantiated from decoder-onlylarge language models (LLMs). In this discipline,relation triple extraction is treated as an auxiliarytask for multi-task learning without introducingextra parameters. Benchmarking on Text2DT (Zhu et al., 2022),a comprehensive Chinese dataset, we find thatgenerative models are much more capable of ex-tracting medical decision tree than state-of-the-art(SOTA) discriminative models. Our experimentsalso show that a carefully designed sequence-to-sequence model (.2) is competitive to aLLM-based autoregressive model (.3) thatis 10+ times larger.Our contributions are summarized as follows: We propose several generative models un-der the sequence-to-sequence/autoregressiveparadigms to better capture the intrinsic log-ical connection among the relation tripleswithin a medical decision tree and extract thetree from text accurately.",
  "Medical Decision Tree Linearization": "To linearize medical decision trees into NL orAugNL style sequences as target output for training,we traverse each tree in pre-order, insert transitionconjunctions (if, else, then, otherwise) be-tween nodes according to the node position, andjoin the relation triples within each node with log-ical conjunctions (or, and). This procedureis depicted in Algorithm 1. The specific differ-ences between NL and AugNL styles are explainedin .2.4. The JSON-style linearization ismore straightforward, see Appendix D for the de-tails. Since CPT (Shao et al., 2021), so far the bestChinese language encoder-decoder is pretrained ontext corpora and unable to generate code, we onlytry the JSON-style linearization on autoregressiveLLMs (ChatGPT and ChatGLM).",
  "(b)": ": An overview of our generative medical decision tree extraction models. (a) A sequence-to-sequencemodel that extracts relation triples within input text and translates the text along with the extracted relation triplesinto a linearized medical decision tree. (b) An autoregressive model that follows task instructions to generate alinearized medical decision tree conditioned on input text. See for the original Chinese-language prompts. steps: 1) encodes the input text and entity/relationqueries with a pretrained language encoder; 2) gen-erates the entity/relation set with a query-basedentity-relation extractor; 3) generates the linearizeddecision tree with a pretrained language decoder,conditioned on the text encoding, relation repre-sentation and extracted relation set; 4) parse thelinearized decision tree. Detailed designs are intro-",
  "Query-based Entity-relation Extraction": "The query-based entity-relation joint extractor isthe one proposed by He and Tang (2023), whichconsists of a shared decoder, an entity decoder, arelation decoder, an entity predictor, a relation typepredictor and a subject-object predictor. It also owns a series of learnable entity queries Qe andrelation queries Qr (each query is a vector), whichare concatenated with input text X. Pretrained lan-guage encoder and shared decoder transform theconcatenation into text encoding Hx along withcontextual entity/relation representation He/Hr.Entity decoder and relation decoder further updateHe into He, update Hr into Hr, Hh, Ht via lineartransform and attention mechanism.The predicted sets of entities E and relationsR are finally computed based on He, Hr, Hh, Ht.Only R is utilized by downstream modules whileE is not. See Appendix A or the work by He andTang (2023) to learn more about this module. 2.2.2Relational ContextSince a medical decision tree is essentially a com-bination of relation triples, leveraging the predictedrelation set as an additional decoding context mayhelp the pretrained language decoder keep aware ofwhich triples are already included in the generatedsequence and which ones are not. It can address theproblem of low triple coverage in the predicted de-cision tree. Motivated by this idea, three designs ofrelational context are attempted: 1) Relation querycontext (RQC), the representation vectors Hr ofrelation queries corresponding to all extracted re-lation triples; 2) Relation-centric textual context(RTC), a cross-attention-based context, where textencoding Hx acts as key and value, relation queryvectors Hr corresponding to all extracted relationtriples act as query; 3) Harmonized relation context(HRC), the fusion of RQC and RTC through gatingmechanism.To inject the relational context into the model,we concatenate text encoding Hx with the rela-tional context in the sequence dimension and to-gether they serve as the decoding context for thepretrained language decoder:",
  "yt = DecodeSearch(P(yt), y<t, R)(4)": "where ; means concatenation, y<t is the generatedtokens by time step t, hdt1 is the undated hiddenstate of current time step, LMHead is a classifierthat first convert current hidden state into vectorof size |V | that apply SoftMax to obtain predictedprobability distribution P(yt) over the vocabulary,DecodeSearch is the decode search strategy (we",
  "Constrained Decoding": "In order to utilize apriori decision tree linearizationgrammar (as shown in Algorithm 1) to constrainthe candidate space of generated target sequencewith the set of extracted relations, we employ a spe-cially designed constrained decoding (CD) strategyduring generative inference.Specifically, the strategy restricts the candidatetoken vocabulary at each generation step based onthe generated sequence prefix using a trie. The con-struction of the trie takes into account the followingscenarios: 1) if the sequence prefix is if, the can-didates include the first token of all head entities;2) if the sequence prefix is else, the candidatetoken is only then; 3) if the sequence prefix isthen, the candidates include , and the first tokenof each head entity; 4) if the sequence prefix is ,,the candidate token is only if; 5) if the sequenceprefix is the first half of an entity/relation name, thecandidates are the first token of the second half ofthe entity/relation name; 6) if the sequence prefixis a complete head entity, the candidates are thefirst token of all relation names with that entity asthe head; 7) if the sequence prefix is a completerelation name, the candidates include the first to-ken of all tail entities; 8) if the sequence prefix is acomplete tail entity, the candidates include then,otherwise, and </s>.",
  "AugNL-style Linearization": "Augmenting natural language (Mialon et al., 2023)with tokens of other modalities (e.g., vision(Zhuet al., 2023; Liu et al., 2023) and knowledgegraph(Pan et al., 2023)) can not only provide com-plementary context but also greatly enhance theexpression ability. Distinguish from NL style oflinearization (Paolini et al., 2021; Lu et al., 2022),where relation triples have to get verbalized beforebeing placed in the target sequence, in AugNL styleof linearization relation triples are considered as ba-sic tokens of high-level abstract semantics and getnaturally embedded in the target sequence, whichdecreases the average length of linearized relationtriples by 10+ times.The technical difference between sequence-to-sequence models with NL-style linearization andAugNL-style linearization lies in the decoding mechanism. Models with AugNL-style lineariza-tion employ a pointer-based copy mechanism,where the relational part of generated sequence ismade up of pointers to extracted relation triples andthe conjunction part of generated sequence is madeup of pointers to predefined structure tokens (i.e.,or, and, if, then, otherwise, ,, </s>):",
  "Autoregressive Models": "In contrast to sequence-to-sequence models, ourautoregressive models inherit from decoder-onlyLLMs, as shown in (b). When properlyprompted with examples, a LLM can handle simpletasks without supervision, which is known as theability of in-context learning (ICL). After super-vised fine-tuned (SFT), a LLM will get better atmodeling the desired output of complex tasks.We explore the ICL as well as SFT settings.For the first setting, two LLMs, ChatGPT (gpt-3.5-turbo) and ChatGLM are employed, and the NL,JSON styles of linearization are tried (note thatAugNL style is inapplicable here). For the SFTsetting, we only consider ChatGLM (for reproduc-tivity concern) and the NL style linearization (sincethe ICL results suggest this style of linearization ismore suitable for ChatGLM, see .2). 2.3.1Few-shot In-context LearningIn the in-context learning (ICL) setting, autore-gressive models are prompted with task instructionfor medical decision tree extraction and few-shotdemonstration. Specifically, the prompt for autore-gressive models with NL-style linearization underthe ICL setting is similar to the one in (b),except that it contains 5 examples of expected input-output (randomly sampled from the training set).The prompt template is shown in Appendix D. 2.3.2Multi-task Joint Fine-tuningDifferent from unsupervised in-context learning,supervised fine-tuning helps a LLM master com-plex tasks through end-to-end training on a diverseset of instruction-response pairs. In this work, we propose a multi-task joint fine-tuning method forour autoregressive models, where medical decisiontree extraction is the main task, relation triple ex-traction and tree shape extraction serve as the aux-iliary tasks. And a novel progressively-dynamicsampling strategy helps the model gradually ac-quire easy-to-hard structural extraction abilities.Prompts for these tasks are illustrated in Figure 2(b). The target output of medical decision treeextraction is just the NL-style linearized tree. Thetarget output of relation triple extraction is all men-tioned relation triples in list format (ordered bytextual position). The target output of tree shapeextraction is the skeleton of a tree, made up of con-junctions and ellipses. Our progressively-dynamicsampling strategy is inspired by curriculum learn-ing (Wang et al., 2021). With the increase of train-ing step, the sampling rate of each task changesaccording to the assumed task difficulty: for rela-tion triple extraction, the sampling rate goes from0.8 to 0 linearly; for tree shape extraction, the sam-pling rate goes from 0.7 to 1 linearly; for the maintask, the sampling rate stays as 1.",
  "Data augmentation and model ensemble": "The SOTA baseline, PromptRE (Jiang et al., 2022),leverages R-Drop (Wu et al., 2021) for data aug-mentation, and assembles predictions of relationtriples after each round of relation extraction. How-ever, their practices are inapplicable to generativemodels. For a fair comparison, we devise a generaldata augmentation method and model ensemblemethod for the task. To obtain augmented samples,we randomly replace entities within the train datawith their synonyms. For model ensemble, our sys-tem first vote on the tree structures predicted bymultiple models and then vote on the content (logi-cal operator and relation triples) of each node. Notethat, our top models outperform SOTA baselineseven without these tricks (see .2).",
  "Data and Evaluation Metrics": "We conduct experiments on the only available medi-cal decision tree extraction dataset, Text2DT, whichis from a shared task of the 8th China Health Infor-mation Processing Conference (Zhu et al., 2022)and get included in the CBLUE 3.0 benchmark(Zhang et al., 2022). Built on a rich corpus of Chi-nese medical textbooks and guidelines, it coversdiagnosis and treatment knowledge of around 200",
  "Ablation Study": "We conduct extensive ablation experiments on theproposed generative models to verify the contribu-tions of different components and determine theoptimal design choice among alternative compo-nent designs. The results are shown in Tables 2-4. presents the results for sequence-to-sequence models with NL-style linearization. Byapplying constrained decoding, the tree accuracyimproves from 45.75% to 48.25%, validatingthe necessity of constrained decoding. Besides,relation-centric textual context works better thanrelation query context or harmonized relation con-text, boosting tree accuracy by 2.75%. This resultindicates a higher acceptance of relation-centric tex-tual context by the pretrained decoder, comparedto the relation query representations output by therelation set generator. The reason may lie in the se-mantic space consistency between relation-centrictextual context and natural language, making itmore conducive to natural language generation. For sequence-to-sequence models with AugNL-style linearization, the combination of relation-centric textual context and harmonized relationembeddings works better than other alternatives,as shown in . This is expected, since harmo-nized relation embeddings are designed to bridgethe relational context and textual context. For autoregressive models under the SFT set-ting, the auxiliary relation triple extraction andtree shape extraction tasks contribute equally tomodel performance, leading to 4% absolute treeaccuracy increment respectively. When the twoauxiliary tasks are applied together, tree accuracyincreases from 53% to 59.5%. By incorporatingprogressively-dynamic sampling, tree accuracy fur-ther increases by 0.5% and reaches 60%.",
  "Error Analysis": "To discover the performance bottleneck of this taskand facilitate future research, we analyze the errorsby our top-performing models. Error distributionson the test split of Text2DT are shown in ,from which we can observe that: 1) The amount ofLogical operator errors is the least, while relationtriple errors occur most frequently, especially forgenerative models with NL-style linearization. 2)Sequence-to-sequence models with NL-style lin-earization have difficulty in correctly predictingthe tree structures. 3) Assembling CPT (AugNL)and ChatGLM (NL) reduces relation triple errorsbut not the logical operator errors or tree structureerrors. 4) Compared to sequence-to-sequence mod-els, autoregressive models produces much moresubject/object entity errors, which means they areweak at identifying entity boundaries.",
  "Sequence-to-sequence Generation": "The idea of sequence-to-sequence was originatedfrom Sutskever et al. (2014), and then dominatedneural machine translation (Wu et al., 2016; Zhanget al., 2019) with Transformer (Vaswani et al.,2017). There are many encoder-decoder languagemodels, e.g. T5 (Raffel et al., 2019) and CPT (Shaoet al., 2021), that are pretrained with sequence-to-sequence learning tasks. Some works accom-plish generating a linearized structure from textin a sequence-to-sequence manner, e.g.neuralAMR (Konstas et al., 2017) for AMR parsing, andText2Event (Lu et al., 2021) for event extraction.Compared to events and AMR graphs, medicaldecision trees additionally carry logical semanticknowledge, which is non-trivial to capture.",
  "The autoregressive generation paradigm employsa single decoder network to generate an output": "sequence by iteratively predicting the next tokenconditioned on the current prefix, without the useof an encoder network. Despite its simplicity, thisparadigm is shown to generalize better under thezero-shot and few-shot settings (Wang et al., 2022).Besides, it is easier to scale up, leading to LLMs,e.g. GPT-4 (Achiam et al., 2023) and ChatGLM(Du et al., 2022). Many works tackle informationextraction tasks by prompting LLMs to autoregres-sively generate structural content in JSON or otherformats (Xu et al., 2023).",
  "Medical Decision Tree Extraction": "Existing medical decision tree extraction methods(Wu, 2022; Jiang et al., 2022) rely on discrimi-native models. Wu (2022) proposes to combinea BERT-style language model (Cui et al., 2021)with a Biaffine model (Dozat and Manning, 2016)to extract the relation triples, classify the logicalconnection among triples, and compose the tree.SOTA method, PromptRE (Jiang et al., 2022), for-mulates medical tree extraction as a multi-roundconditional relation extraction task, where each par-ent node is a condition for extracting relation triplesof its left/right child from text. Concurrent withour work, Text2MDT (Zhu et al., 2024) comes upwith an expanded version of the Text2DT dataset(unavailable yet) and reports some new results.",
  "Conclusion": "In this study, we present several generative mod-els to extract medical decision trees, which arevaluable for CDSS but costly to acquire manually.The proposed models inherit two mainstream textgeneration paradigms, i.e. sequence-to-sequencegeneration and autoregressive generation, whichbring advantage in modeling both source text andthe intrinsic logical connection among tree compo-nents. Experiments show that our method wins theSOTA discriminative method by a large margin, es- tablishing new SOTA with 67% tree accuracy and78% path F1 score. Besides, an in-depth analysisof error distribution reveals the pros and cons of dif-ferent models, paving the way for future researchon this area. Another direction for future efforts isthe evaluation of predicted decision trees clinicalusefulness in real-world scenario, which requiresconsideration of potential ethical risks and carefulexperimental designs.",
  "Although the proposed method is applicableto languages like English, we only experimenton a public Chinese dataset, since there are noother available datasets": "Entity normalization is not covered in thiswork, which means the extracted rules arenot readily compatible with existing biomedi-cal knowledge bases like UMLS. Future workshould include entity normalization a step ofpost processing, or enhance the formulationand models to support entity normalization. We only look into the extraction of medical de-cision rules in this study, but not decision ruleson other knowledge-intensive domains, suchas mineral exploration (Duda et al., 1981) andmathematics (Beeson, 1989). However, theproposed method is in fact domain-agnosticand we believe there is no barrier to extendour method to other domains. We thank the reviewers for their insightfulcomments and valuable suggestions.Thisstudy is partially supported by National KeyR&D Program of China (2023YFC3502900),National Natural Science Foundation of China(62276082), Research Grants Council of theHong Kong Special Administrative Region, China(UGC/FDS16/E09/22), Major Key Project of PCL(PCL2021A06), Shenzhen Soft Science ResearchProgramProject(RKX20220705152815035),Shenzhen Science and Technology Researchand Development Fund for Sustainable Develop-mentProject(GXWD20231128103819001,No.KCXFZ20201221173613036,20230706140548006)andtheFundamentalResearch Fund for the Central Universities(HIT.DZJJ.2023117). Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Crina Grosan, Ajith Abraham, Crina Grosan, and AjithAbraham. 2011. Rule-based expert systems. Intelli-gent systems: A modern approach, pages 149185": "Yuxin He, Jingyue Hu, and Buzhou Tang. 2023. Re-visiting event argument extraction: Can EAE mod-els learn better when being aware of event co-occurrences?In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 1254212556, Toronto, Canada. Association for Computa-tional Linguistics. Yuxin He and Buzhou Tang. 2022. Setgner: Generalnamed entity recognition as entity set generation.In Proceedings of the 2022 Conference on Empir-ical Methods in Natural Language Processing, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Yiwen Jiang, Hao Yu, and Xingyue Fu. 2022. Med-ical decision tree extraction: A prompt based dualcontrastive learning method. In China Health In-formation Processing Conference, pages 103116.Springer. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, YejinChoi, and Luke Zettlemoyer. 2017. Neural AMR:Sequence-to-sequence models for parsing and gener-ation. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 146157, Vancouver,Canada. Association for Computational Linguistics.",
  "Ilya Loshchilov and Frank Hutter. 2017. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, JialongTang, Annan Li, Le Sun, Meng Liao, and ShaoyiChen. 2021. Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages27952806, Online. Association for ComputationalLinguistics. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, HongyuLin, Xianpei Han, Le Sun, and Hua Wu. 2022. Uni-fied structure generation for universal informationextraction. In Annual Meeting of the Association forComputational Linguistics.",
  "Xin Wang, Yudong Chen, and Wenwu Zhu. 2021.A survey on curriculum learning. IEEE Transac-tions on Pattern Analysis and Machine Intelligence,44(9):45554576": "Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, WeiChen, Min Zhang, Tie-Yan Liu, et al. 2021. R-drop:Regularized dropout for neural networks. Advancesin Neural Information Processing Systems, 34:1089010905. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,Mohammad Norouzi, Wolfgang Macherey, MaximKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.2016. Googles neural machine translation system:Bridging the gap between human and machine trans-lation. arXiv preprint arXiv:1609.08144.",
  "Zihong Wu. 2022. Research on decision tree methodof medical text based on information extraction. InChina Health Information Processing Conference,pages 127133. Springer": "Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, TongXu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, andEnhong Chen. 2023. Large language models for gen-erative information extraction: A survey. Preprint,arXiv:2312.17617. Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, andZhongyu Wei. 2021. A partition filter network forjoint entity and relation extraction. In Proceedings ofthe 2021 Conference on Empirical Methods in Nat-ural Language Processing, pages 185197, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Hang Yang, Dianbo Sui, Yubo Chen, Kang Liu, JunZhao, and Taifeng Wang. 2021.Document-levelevent extraction via parallel prediction networks. InAnnual Meeting of the Association for ComputationalLinguistics. Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang,Lei Li, Xin Shang, Kangping Yin, Chuanqi Tan, JianXu, Fei Huang, Luo Si, Yuan Ni, Guotong Xie, Zhi-fang Sui, Baobao Chang, Hui Zong, Zheng Yuan,Linfeng Li, Jun Yan, Hongying Zan, Kunli Zhang,Buzhou Tang, and Qingcai Chen. 2022. CBLUE: AChinese biomedical language understanding evalua-tion benchmark. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 78887915,Dublin, Ireland. Association for Computational Lin-guistics. Wen Zhang, Yang Feng, Fandong Meng, Di You, andQun Liu. 2019. Bridging the gap between trainingand inference for neural machine translation. In Pro-ceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 43344343, Florence, Italy. Association for ComputationalLinguistics.",
  "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023. Minigpt-4: Enhancingvision-language understanding with advanced largelanguage models. arXiv preprint arXiv:2304.10592": "Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xi-aoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, andGuotong Xie. 2024. Text2mdt: Extracting medicaldecision trees from medical texts. arXiv preprintarXiv:2401.02034. Wei Zhu, Wenfeng Li, Xiaoling Wang, Wendi Ji, Yuan-bin Wu, Jin Chen, Liang Chen, and Buzhou Tang.2022. Extracting decision trees from medical texts:An overview of the text2dt track in chip2022. InChina Health Information Processing Conference,pages 89102. Springer.",
  "X = [X; Qe; Qr] R(L+Me+Mr)d(6)": "where d is model dimension, Me and Mr are thenumbers of entity/relation queries (set as the max-imum amount of entity/relation in a single text ofthe corpus).The bidirectional self-attention of the pretrainedencoder is modified into one-way self-attention.Concretely, the upper right L (Me + Mr) sub-matrix of the attention mask gets filled with nega-tive infinity value so that the entity/relation queriesbecome invisible to the token encodings, while theentity/relation queries can still attend to each otherand the token encodings. After multiple one-wayself-attention layers and feed-forward layers, theencoder outputs the contextual token encodings aswell as the contextual entity/relation queries.The entity-relation shared decoder consists ofone-way self-attention layers that mimics theones in the encoder, as well as bidirectional self-attention layers that updates the entity/relationqueries via modeling the interaction among them.It outputs the updated token representations Hx,entity queries He and relation queries Hr.The entity generator consists of an entity decoderand an entity predictor. It first linearly transformsthe token encodings Hx into entity-view:",
  "tek = arg max P tek(14)": "Note that, entities whose predicted type label is will be excluded from the generated entity set.The relation generator consists of a relation de-coder, a subject-object predictor and a relation typepredictor. The relation decoder work in the samemanner as the entity decoder, except that the rela-tion decoder splits relation queries into head/tailqueries before decoding:",
  "Hxr = FC(Hx)(16)Hh, Ht, Hr = RelDecoder(Hh, Ht, Hr|Hxr )(17)": "The subject-object predictor then predicts theboundary and entity type of the head/tail entityassociated with each relation queries. This processis similar to the entity prediction process. The onlydifference is that the entities queries becomes thehead/tail queries Hh/t and the token encodings isnow in relation-view Hxr .The relation type predictor classifies the categoryof i-th relation query according to Hri :",
  "EImplementation details": "Our sequence-to-sequence models is initializedwith CPT-large(Shao et al., 2021), which has 20-layer encoder and 4-layer decoder. The numbersof entity queries and relation queries are set as 30,25 respectively. We train the models in 2 stages: inthe first stage (70 epochs), the pretrained languagedecoder are frozen and the encoder, entity-relationextractor are optimized with the entity-relation ex-traction loss; in the second stage (100 epochs), allmodules are jointly optimized. The learning rate ofthe encoder and decoder are set as 3e-5 and 4e-5respectively. An AdamW(Loshchilov and Hutter,2017) optimizer with linear warm-up is employed.For ICL, the autoregressive models are two plug-and-play commercial natural language assistant:1) ChatGPT (gpt-3.5-turbo version); 2) ChatGLM(chatglm_pro version). We invoke them via API.The default temperature is applied and the numberof examples within each prompt is set as 5. The 5examples are randomly sampled from the trainingset in a stratified manner (each instance correspond-ing to one tree structure). For SFT, the autoregres-sive models are initialized with ChatGLM-6B and",
  ": Statistics of the Text2DT Dataset (C/Drepresents a condition/decision node)": "tuned with LoRA(Hu et al., 2021). The LoRA rank,learning rate, batch size and number of training setsis set as 8, 2e-4, 8 and 2000 respectively.The number of parameters of our sequence-to-sequence models is less than 1B. Whereas the num-ber of parameters of our autoregressive modelsbased on ChatGLM is 6B. All experiments are con-",
  "GPerformance on Generating Trees ofDifferent Depths": "There are 7 types of tree structures in the datasetand the depth of annotated decision trees rangesfrom 2 to 5, as illustrated in . To analyze thedifference between generative models on extractingtrees of different complexity, we split the test setaccording to tree depth and evaluate the modelperformance on each split respectively. The resultsare illustrated in , from which we can drawthe following conclusions:",
  "For sequence-to-sequence models, the perfor-mance gap between NL and AngNL styles oflinearization lies on extracting deeper trees": "In the ICL setting, ChatGPT with JSON-stylelinearization gains most of its points fromtrees of depth 2. Under other circumstances,both ChatGPT and ChatGLM perform quitepoorly, regardless of the linearization style. : The prompt for generating the JSON-style linearized medical decision tree (utilized by autoregreesivelarge language models under the ICL setting). It includes 5 instances for demonstration, which are randomlysampled from the training set in a stratified manner (each instance corresponding to one tree structure).",
  "The performance gains after ensemble vary withdifferent paradigms of models, as observed in Ta-ble 1. We suspect this is due to the difference in the": "diversity\" of trees generated by different models.To verify that, we measures the similarity betweenmedical decision trees using edit distance. The editdistance for medical decision trees is the minimumnumber of tree edit operations (i.e., inserting ordeleting a node, changing a node role, inserting ordeleting a triplet and modifying a logical operator)required to transform one tree into another. For agroup of trees, the average edit distance betweeneach pair of trees is denoted as the diversity\". Fig-ure 7 shows the diversity of trees generated byvarious models. It is observed that the diversityof trees by sequence-to-sequence models is muchstronger than that of autoregressive models, andthat the diversity is the strongest when integrating : Comparison of different generative models on extracting trees of different depths. Results here arerecorded for 5-model ensembles and it is inapplicable to include error bars. Trees of depth=5 only exist in thetraining data but not the evaluation data, so there is not result for depth=5.",
  ": Diversity of trees generated by different models and its correlation with the performance gain afterensemble": "these two paradigms of models.In , a scatter plot with a (least square)fitted line depicts the correlation between tree di-versity and performance increment after ensemble.It certifies that the tree diversity has a weak positivecorrelation with the increment of Triple/Node F1after ensemble, and a strong positive correlationwith the increment of Path F1 and Tree Acc afterensemble."
}