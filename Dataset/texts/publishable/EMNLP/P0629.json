{
  "Abstract": "In recent years, the rapid increase in onlinevideo content has underscored the limitationsof static Video Question Answering (VideoQA)models trained on fixed datasets, as they strug-gle to adapt to new questions or tasks posed bynewly available content. In this paper, we ex-plore the novel challenge of VideoQA within acontinual learning framework, and empiricallyidentify a critical issue: fine-tuning a large lan-guage model (LLM) for a sequence of tasksoften results in catastrophic forgetting. To ad-dress this, we propose Collaborative Prompt-ing (ColPro), which integrates specific ques-tion constraint prompting, knowledge acquisi-tion prompting, and visual temporal awarenessprompting. These prompts aim to capture tex-tual question context, visual content, and videotemporal dynamics in VideoQA, a perspectiveunderexplored in prior research. Experimen-tal results on the NExT-QA and DramaQAdatasets show that ColPro achieves superiorperformance compared to existing approaches,achieving 55.14% accuracy on NExT-QA and71.24% accuracy on DramaQA, highlightingits practical relevance and effectiveness.",
  "Introduction": "Video Question Answering (VideoQA) is criticalfor video understanding, involving training ma-chine learning models to accurately respond toquestions across various tasks (e.g., finding spe-cific information Choi et al., 2021, counting ob-jects Xiao et al., 2021, recalling actions Zhanget al., 2022) based on given video content. How-ever, existing VideoQA models are typicallytrained on fixed datasets in static environments.With a continual increase in the number of videoson the internet every day, these static models mayface challenges in answering new questions posed",
  "(b) The proposed Collaborative Prompting with continual learning": ": (a) Existing fine-tuning techniques train fordifferent VideoQA tasks, which could lead to catas-trophic forgetting, and generate inferior results. (b) Weintroduce the Collaborative Prompting (ColPro) withina continual learning framework, which retains task-specific knowledge to generate accurate answers, wherePN denotes a projection layer. by the newly available content. One straightfor-ward solution to overcome this challenge is to fine-tune the models when new data is introduced. How-ever, this approach can lead to higher computa-tional costs when retraining on all the data. Alter-natively, fine-tuning only on newly available videoquestion-and-answer pairs may lead to the catas-trophic forgetting issue (McCloskey and Cohen,1989), as shown in (a). This motivates us to explore continual learningtechniques (Rebuffi et al., 2017; Rolnick et al.,2019; Wang et al., 2022b) for VideoQA, facilitatingongoing fine-tuning of models across a sequenceof data while mitigating catastrophic forgetting ofprevious tasks (e.g., finding information, or count-ing objects mentioned earlier), thereby addressingthe needs of real-world dynamic environments. Re-cent continual learning techniques (Wang et al., 2022b,a) have achieved good performance by em-ploying rehearsal-free methods, such as learnableprompting (Jia et al., 2022) and prefix-tuning (Liand Liang, 2021). These approaches eliminate theneed for memory-intensive stored experiences fromprevious tasks (Rolnick et al., 2019; Cha et al.,2021), reduce computation costs, and minimizeforgetting. Specifically, L2P (Wang et al., 2022b),DualPrompt (Wang et al., 2022a), DBI (Qian et al.,2023), and ProgPrompt (Razdaibiedina et al., 2023)employ task-aware prompting techniques to fine-tune pre-trained models for downstream tasks usingfewer learnable parameters. While these methodshave improved performance in vision and languagecontinual learning tasks, they often transfer eithersingle-modal (text only) or multimodal (text andimages) information from task to task. In terms ofVideoQA, it is crucial to incorporate textual ques-tion context (A1), visual content (A2), and videotemporal dynamics (A3) in the continual trainingsetting. In this paper, we introduce CollaborativePrompting (ColPro), which explores these aspectsfor the VideoQA problem, and represents an areathat has not been fully explored in prior research,as shown in (b). The core idea of ColPro is to empower a basemodel to achieve enhanced performance whentransferring across a sequence of tasks. Inspiredby the robust reasoning abilities of recent LargeLanguage Models (LLMs), we instantiate the basemodel as a LLM (e.g., LLaMA Touvron et al.,2023) to generate accurate answers from textualquestions and video inputs. Specifically, ColPro in-tegrates three types of prompting techniques: task-specific question constraint prompting (TQCP),knowledge acquisition prompting (KAP), and vi-sual temporal awareness prompting (VTAP), aimedat enhancing accuracy in answer prediction whileminimizing forgetting. TQCP enables the model togain awareness of the task type and select the cor-rect prompt representation using a negative guidingapproach (Jiang et al., 2024; Li et al., 2024). Thismethod allows the prompt representation to posi-tively correlate with the current task-specific ques-tion and negatively correlate with negative ques-tion samples. Additionally, KAP acquires task-specific question and video information to enhanceaccurate answer prediction (for A1). Furthermore,VTAP integrates visual information with the LLMby continuously incorporating video dynamics intoprompts through autoregressive temporal dynamics and video distillation loss (for A2 and A3). Withthese prompting strategies, ColPro encapsulatesmultimodal information to enhance task-specificquestion answering and mitigate catastrophic for-getting during inference.Our main contributions to this paper are as fol-lows: (1) We explore the novel problem of videoquestion answering (VideoQA) in a continual learn-ing context, and demonstrate a critical issue: effi-ciently fine-tuning a LLM for a sequence of tasksleads to catastrophic forgetting. This motivatesus to conduct empirical studies to mitigate this is-sue. (2) We propose Collaborative Prompting (Col-Pro), which utilizes three distinct aspects: textualquestion context, visual content, and video tem-poral dynamics, in VideoQA to facilitate knowl-edge transfer to future tasks. (3) We conduct ex-tensive experiments on the split VideoQA dataset(NExT-QA (Xiao et al., 2021) and DramaQA (Choiet al., 2021)) for continual task-specific answer pre-diction. Our findings show that ColPro achievesstate-of-the-art results, with 55.14% accuracy onNExT-QA and 71.24% accuracy on DramaQA.",
  "Video Question Answering": "VideoQA is a fundamental task in video under-standing, aiming to answer questions based onvideo content (Xiao et al., 2023; Gao et al., 2023a;Choi et al., 2021). Many recent works have ex-plored LLM-based VideoQA (Yu et al., 2024; Luoet al., 2023; Ko et al., 2023), which requires a LLMto predict the correct answer given a video andquestion pair. Flipped-VQA (Ko et al., 2023) usesthe prompting technique to fine-tune a LLM tolearn the specific VideoQA task. SeViLA (Yu et al.,2024) is built based on a pre-trained large image-language model (Li et al., 2023), extending its capa-bilities to perform reasoning on video inputs. How-ever, most existing methods are trained on fixeddatasets to handle reasoning in static environments,which struggle to answer new questions or tasksposed by newly available content. In contrast, westudy a continual video question answering prob-lem, and address its inherent challenges caused bycatastrophic forgetting.",
  "ProjectionProjectionProjection": ": Illustration of the Collaborative Prompting (ColPro) framework. Left: The training process incorporatesColPro into the first j ColPro Guided Pre-trained Layers to enhance answer prediction accuracy while minimizingforgetting. Right: Three detailed prompting techniques within ColPro are demonstrated: task-specific questionconstraint prompting (TQCP), knowledge acquisition prompting (KAP), and visual temporal awareness prompting(VTAP). Together, these techniques encapsulate the textual question context, visual content, and video temporaldynamics for each VideoQA task. sue of catastrophic forgetting (Li and Hoiem, 2017;Rebuffi et al., 2017; Rolnick et al., 2019). Existingmethods can be summarized into three categories:rehearsal-based (Buzzega et al., 2020; Rolnicket al., 2019), architecture-based (Li et al., 2019;Ke et al., 2020), and regularization-based (Aljundiet al., 2018; Paik et al., 2020). Rehearsal-based ap-proaches involve constructing a subset of learnedsamples in a memory buffer and replaying themwhen learning a new task. Architecture-based ap-proaches allocate separate sets of dedicated param-eters for each different task. Regularization-basedapproaches preserve changes to weights associatedwith older tasks and selectively stabilize parameterchanges. Recent studies (Wang et al., 2022b,a;Gao et al., 2023b) draw inspiration from learn-able prompting (Lester et al., 2021; Zhang et al.,2023a) in natural language processing to addresscatastrophic forgetting by learning a small numberparameter that is attached to a pre-trained model.Specifically, L2P (Wang et al., 2022b) utilizes aset of task-specific learnable prompts to dynam-ically instruct a pre-trained model for continuallearning. ProgPrompt (Razdaibiedina et al., 2023)adopts progressive networks with a pre-trained lan-guage model to learn prompts for different tasksand sequentially concatenates the task-specificlearned prompts for text classification. Learning-Accumulation-Ensemble (LAE) (Gao et al., 2023b)utilizes different Parameter-Efficient Fine-Tuning(PEFT) methods such as adaptor (Houlsby et al., 2019), Lora (Hu et al., 2021), and prompt-ing (Lester et al., 2021) for image classification.Recent visual question answering models (Leiet al., 2023; Qian et al., 2023) have been ex-ploring the continual learning techniques to an-swer new questions with given images withoutexperiencing catastrophic forgetting. Zhang et al.,2023b and Lei et al., 2023 introduce replay-basedmethod to address image-based question answer-ing tasks. Qian et al., 2023 use multimodal decou-pled prompts to interact with a pre-trained vision-language model, capturing the intricate relation-ships between modalities. Similar to adapter-basedLAE (Gao et al., 2023b), Dynamic Adapter Merg-ing (DAM) (Cheng et al., 2024) utilizes an adaptor-based framework (Houlsby et al., 2019) for videoquestion answering. Unlike DAM, which addressesdomain shift in datasets using an adaptor, our workaims to guide a LLM to comprehend multimodalinformation, including question context, visualcontent, and temporal dynamics, through a novelprompting technique called ColPro. To the best ofour knowledge, this approach is the first of its kind.",
  "we present the overall architecture of the proposedmethod and its training objective": "Continual Learning Scenarios.In continuallearning scenarios, a model is trained sequen-tially through various stages using a dataset D =<d1, d2, ..., dT >, where dt (1 t T) denotesthe t-th training task, and data from previous tasksis not accessible during the training of stage t. Inthis paper, we study the problem of rehearsal-freecontinual learning on video question answeringtasks, where the data dt = <V t, Qt, At> consistsof video V t, question Qt, and answer At pairs. Forour experiments, we segment the types of ques-tions into T tasks followed by (Zhang et al., 2023b;Lei et al., 2023) to benchmark our proposed ap-proach on the NExT-QA (Xiao et al., 2021) andDramaQA (Choi et al., 2021) datasets. Followingthe settings in existing rehearsal-free works (Wanget al., 2022b; Razdaibiedina et al., 2023), we as-sume a pre-trained LLM model (e.g., LLaMA (Tou-vron et al., 2023)) is available for our experiments. Prompting for LLM-based Video Question An-swering. Prompting, a learnable prompt-basedlearning technique (Zhang et al., 2023a) hasbeen introduced as a streamlined fine-tuning ap-proach, transforming large language models (e.g.,LLaMA (Touvron et al., 2023)) into highly efficientinstruction-following models. The core concept ofprompting is to incorporate additional instructionsinto pre-trained LLMs, enabling them to performdownstream tasks in both NLP and multimodal rea-soning contexts (Liu et al., 2024; Zhu et al., 2023).In this work, we leverage the efficient instruction-following capabilities and outstanding reasoningabilities of LLMs to achieve accurate multimodalquestion answering in a continual learning scenario.We illustrate the prompting for LLM-based contin-ual VideoQA as follows.Our primary focus is on leveraging LLMs forcontinual learning in VideoQA tasks, with LLaMA-Adapter (Zhang et al., 2023a) serving as our base-line. We adopt their prompt tuning adaptation ap-proach to incorporate task-specific information bylearning through LLaMA layers, given the inputof task-specific questions, videos, and answers. Inthe inference stage, the frozen model employs thelearned task-specific prompt knowledge to predicttask-specific answers. In our framework, given theN-layers LLaMA, we inject prompts for the firstj-layers LLaMA transformer layers, named Col-Pro Guided Pre-trained Layers (.). We maintain the pre-trained model frozen while tuning a selectfew additional learnable prompts. Rather than ap-pending prompts directly to the input tokens, ourapproach involves adding prompts to the keys andvalues within the Multihead Self-Attention (MSA)layer, following the structure described in Trans-former architectures (Vaswani et al., 2017). Withthe split sets of learnable prompts denoted as Pkand Pv Rld, integrated into the key Hk and val-ues Hv within the LLaMA model, where Hq rep-resents the query, the attention module is adaptedas follows:",
  "MSA = Concat(H1, ..., Hm)Wo(2)": "where [; ] denotes concatenation, Wo is projectionmatrices, Hi denotes i-th head and m is the numberof total heads. In this paper, we adopt the comple-mentary learning principle (Wang et al., 2022a),incorporating learnable General prompts Pg (G-Prompt) and Expert prompts Pe (E-Prompt) intothe first j layers of the LLaMA model, where theG-Prompt is applied to the first i layers to capturetask-invariant knowledge, whereas the E-Prompt isapplied to the subsequent layers from i + 1 to j fortask-specific knowledge adaptation. Through thisprompting approach, we effectively train a smallnumber of parameters while retaining the knowl-edge of existing tasks, all without the need for ex-ternal memory.Overall Architecture.Our proposed method,termed collaborative prompting (ColPro) for con-tinual VideoQA, is illustrated in . Leverag-ing LLM-based VideoQA models as a foundation,our goal is to establish a cohesive set of collab-orative and interactive prompts. This approachaims to mitigate the issue of catastrophic forgettingoften associated with straightforward sequentialfine-tuning methods.Each training task set consists of video V t, ques-tion Qt, and answers At in pairs. We extract asequence of visual tokens V = {v1, . . . , vNv} RNvD from the raw video using a frozen visualencoder (Radford et al., 2021), and utilize a to-kenizer to process the raw question and answerinto tokens, i.e., Q = {q1, . . . , qNq} RNqD and A = {a1, . . . , aNa} RNaD, where Nv,Nq and Na denote the number of video frames,lengths of question and answer tokens, respec-tively. During the training stage, the task-specifictoken sequences qt, vt, and at are concatenated",
  "Xt, P = (< Qt, Vt, At >, P),(3)": "where Xt = <Xtq, Xtv, Xta> denotes the sequenceof output features for question, video and an-swer for task t. In our framework, Pg is trainedalone using the global cross-entropy loss simi-lar to existing methods (Wang et al., 2022a,b),while we focus on optimizing the E-Prompt Peto effectively capture and preserve task-specificknowledge, thereby reducing catastrophic forget-ting.During the inference stage, LLaMA takesVt, Qt, and the learned prompts P to predict task-specific answers.",
  "Collaborative Prompting": "We systematically explore continual learning, fo-cusing on integrating multimodal distributions intoa unified set of prompts. This approach provides acomprehensive framework for continuous improve-ment in LLM-based VideoQA. Our methodologyincludes collaboratively incorporating prompts de-signed for task-specific question constraints, visualtemporal awareness, and knowledge acquisition. Task-specific Question Constraint Prompting(TQCP). TQCP extracts question-specific knowl-edge from learned prompt representations, enhanc-ing task awareness during the inference stage. Dif-ferent from existing methods (Wang et al., 2022a,b)that rely on a known task identity to select andtrain specific sets of prompts alongside the classi-fier, we directly utilize the question type to guidethe learning of a single set of prompts for questionawareness. Drawing inspiration from a recent neg-ative label guided algorithm (Jiang et al., 2024; Liet al., 2024), we enable Pe to be positively corre-lated with the current task-specific question type(e.g., how many) and negatively correlated withnegative question samples (e.g., negative questiontypes: what, where, etc). This facilitates task-typeawareness and links input features to E-promptsduring the inference stage. To achieve this, we opti-mize Xtq and Pe with question generation loss andnegative questions (Q) guided loss, which allowsthe given prompt to learn question-specific repre-sentation for the current task. It can be formulated",
  ",": "(6)where P(qn+1) = Softmax(Linear([Xq; Pe])) fortask t, and [; ] denotes concatenation.P =<Pe, Pg> and is a temperature parameter. Weemploy cross-entropy loss Lgenqlocally to generatetask-specific questions based on learned Xq andPe. Lnegqto correlate the given question Q+ withPe, where sim(, ) computes the cosine similari-ties between the Pe and the i-th positive questionQ+i (resp. j-th negative question Qj ) samples inthe batch B.Visual Temporal Awareness Prompting (VTAP).VTAP aims to bridge the gap between video fea-tures and the LLM, allowing E-prompts to incor-porate visual information with temporal dynamics.This improves the video understanding abilities ofthe LLM and enhances its answer prediction capa-bilities with given questions and videos. However,modeling both the visual content of videos andtheir temporal dynamics simultaneously presentsa challenge. To overcome this, we guide the E-prompt in learning video with temporal dynamicsby using the question and answer choices as priorknowledge and leverage the autoregressive sequen-tial abilities of the LLM to model and predict theorder of video frames based on preceding frames.Furthermore, we distill video information extractedfrom an image encoder (Radford et al., 2021) intoan E-prompt (Zhong et al., 2024; Li et al., 2023),enabling the LLM to understand visual features. Inthis work, we use contrastive loss (Lconq) to facili-tate this process, which is formulated as follows:",
  "where P(vn+1) = Softmax(Linear([Xv; Pe])) fortask t, Ldynvis the optimization function for video": "temporal dynamic modelling, and sim(, ) com-putes the cosine similarities between the Pe andthe i-th video Vi (resp. j-th video Vj) in the batchB for current task.Knowledge Acquisition Prompting (KAP). KAPinjects task-specific multimodal information fromthe question and video into the E-prompts to ac-curately predict answers for the current task. Toachieve this, at the training stage, Pe leverages theautoregressive abilities of the LLM to encapsulatethe task-specific context information of V, Q, andall answer choices A as prior knowledge to pre-dict the specific answer. The objective function isformulated as:",
  "Datasets": "We use the multi-choice NExT-QA dataset (Xiaoet al., 2021), which includes various types of ques-tions. These include causal questions, such as why(CW) and how (CH), that ask for the intentionsor reasons behind earlier actions; temporal ques-tions, which determine the relationships betweenactions like what are (TC), what did (TN), andwhat was (TP); and descriptive questions, like howmany (DC), where (DL) and other types of ques-tion (DO), which focus on visible contents suchas places and attributes. We split (Lei et al., 2023;Zhang et al., 2023b) the NExT-QA dataset intoeight distinct tasks based on question types in theNExT-QA dataset. In CL, the order of task learn-ing impacts the learning outcome. Therefore, weconducted experiments and set our tests in the se-quence that resulted in the highest forgetting rate(suffers more in catastrophic forgetting) using thebaseline method. The sequence of the training or-der follows this sequence: <TP, CW, DC, TC, DL,DO, TN, CH>. DramaQA dataset (Choi et al.,2021) features a video story understanding withhierarchical difficulty levels. We split it into 5 dis-tinct tasks according to the question types, and : The results on the NExT-QA dataset which aredivided into 8 tasks, where the Avg. Acc denotes aver-age accuracy across tasks and Avg. Fog is the averageforgetting rate. The symbols and indicate whethera higher or lower value is preferable for a given metric,respectively.",
  "Evaluation Metrics": "We evaluate methods using two metrics: the aver-age final accuracy (Avg. Acc), where higher valuesare better and represent the final accuracy averagedover N tasks for all previous classes, and the av-erage forgetting (Avg. Fog) that is widely used inexisting works (Wang et al., 2022b,a; Qian et al.,2023), where the lower the better which indicatesthe tasks experienced less forgetting averaged overN tasks.",
  "Implementation Details": "We train CL VideoQA for five epochs on bothdatasets with a batch size of 8 for the NExT-QAdataset (4 for DramaQA) with the gradient accu-mulation technique. We fine-tuned the LLaMA-7Bmodel in this paper. AdamW optimizer is used with = (0.9, 0.95). We search learning rate and weightdecay in [0.05, 0.1] and [0.15, 0.25], respectively.The number of video frames V is set to 10. Eachframe is resized by 224 224 and fed into CLIPVIT-L/14 to extract frame features. The total se-quence length of the concatenated visual, question,and answer tokens is 128 for NExT-QA and 280for DramaQA. Temperature parameter is set to 1.The prompt tokens are empirically set to 10 for pk,",
  "Comparison with Continual LearningMethods": "compares the performance of the Collabo-rative Prompting (ColPro) on the split NExT-QAbenchmark with existing CL approaches, includ-ing the fine-tuned LLaMA (Touvron et al., 2023)with addtional projection layer, L2P (Wang et al.,2022b), DualPrompt (Wang et al., 2022a), Pro-Prompt (Razdaibiedina et al., 2023), DAM (Chenget al., 2024), and LAE (Gao et al., 2023b). Addi-tionally, we report deep CL implementations of theL2P+, DualPrompt+, and LAE+ methods, whichactivate more layers of LLaMA for CL tasks byapplying prompts to 18 layers. In the comparisons,our proposed ColPro achieved better average pre-diction accuracy and significantly lower averageforgetting compared to existing methods. This im-provement in average forgetting can be attributedto the fact that the ColPro method experiences lessforgetting and allows better forward transfer of dif-ferent tasks, which is beneficial in CL VideoQA.Similarly, we compare the performance of the Col-Pro on the split DramaQA benchmark with existingCL approaches in , further validating theeffectiveness of our proposed method in addressingcatastrophic forgetting issues. These tables indicatethat the models experience catastrophic forgetting,with the Avg. Fog score up to 24%. This under-scores the need to address catastrophic forgettingin video QA, and we have minimized the forgettingwith ColPro.",
  "Comparison with Parameter-EfficientFine Tuning Methods": "demonstrates the performance of ColProwith other Parameter-Efficient Fine-Tuning (PEFT)methods that can be used to address catastrophicforgetting in CL settings, such as LLaMA-Adapter(L-Adapter) (Zhang et al., 2023a), Lora (Hu et al.,2021), and Prefix (Li and Liang, 2021). Our pro-posed method shows a significant improvement inminimizing forgetting compared to our baselineLLaMA-Adapter method, as evidenced by a lowerAvg. Fog and a higher Avg. ACC when evaluatedwith the NExT-QA and DramaQA datasets. ColProalso outperforms Lora and Prefix, demonstratingthe effectiveness of our specially designed strategyfor LLM-based CL VideoQA settings.",
  "Ablation Study": "The Effectiveness of Multimodal Prompting.Our proposed method includes three primary mul-timodal interaction prompting strategies: ques-tion constraint (TQCP), visual temporal alignment(VTAP), and knowledge acquisition (KAP). Eachstrategy is optimized with its respective optimiza-tion function, Lq, Lv, and La. We performed ab-lation studies on these strategies and reported theresults in . A tick indicates that the respec-tive prompting strategies was used during modeltraining, while a cross means not included. No-tably, La is always included to optimize the modelwith the ground truth answer. We can observe thatthe inclusion of each optimization function signif-icantly impacts the models performance. Specif-ically, models trained with all three optimizationfunctions consistently achieve higher accuracy andlower forgetting. The ablation results demonstratedshow that incorporating VTAP enhances the ac-curacy of the LLM, while utilizing TQCP helpsthe model suffer less from forgetting. This under-scores the importance of question constraint andvisual temporal alignment prompting, which helpthe LLM gain awareness of the task type to re-duce catastrophic forgetting and understand the",
  "Conclusion": "In this work, we explore the novel problem ofVideoQA, which efficiently fine-tunes the LLMto answer new questions with video in a contin-ual learning context. We propose CollaborativePrompting (ColPro) to integrate textual questioncontext, visual content, and video temporal dynam-ics in each learning phase, facilitating knowledgetransfer to future tasks while minimizing catas-trophic forgetting. We achieves state-of-the-artresults on split NExT-QA and DramaQA datasets.",
  "Limitations": "We propose the efficient Collaborative Prompt-ing (ColPro), which integrates task-specific ques-tion constraint prompting, knowledge acquisitionprompting, and visual temporal awareness prompt-ing with a large language model (LLM) to enhancethe performance of continual VideoQA. However,catastrophic forgetting remains high for the Dra-maQA dataset using our method, indicating a sub-stantial decline in performance for VideoQA pre-diction when using LLaMA-7B. Furthermore, wedid not experiment with larger models (e.g., 33BLLM) due to memory constraints, which limitsour ability to explore catastrophic forgetting that may arise when fine-tuning other LLMs for CLVideoQA.Acknowledgments: This research/project is sup-ported by the National Research Foundation, Sin-gapore under its AI Singapore Programme (AISGAward No: AISG-PhD/2021-08-024[T]). Any opin-ions, findings and conclusions or recommendationsexpressed in this material are those of the author(s)and do not reflect the views of National ResearchFoundation, Singapore. Rahaf Aljundi, Francesca Babiloni, Mohamed Elho-seiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018.Memory aware synapses: Learning what (not) to for-get. In Proceedings of the European conference oncomputer vision (ECCV), pages 139154. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Da-vide Abati, and Simone Calderara. 2020. Dark expe-rience for general continual learning: a strong, simplebaseline. Advances in neural information processingsystems, 33:1592015930.",
  "Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin,Mohit Bansal, and Gedas Bertasius. 2024. Dam: Dy-namic adapter merging for continual video qa learn-ing. arXiv, 2403.08755": "Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ah-jeong Seo, Youwon Jang, Minsu Lee, and Byoung-Tak Zhang. 2021.Dramaqa: Character-centeredvideo story understanding with hierarchical qa. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pages 11661174. Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang,and Mike Zheng Shou. 2023a. Mist: Multi-modaliterative spatial-temporal transformer for long-formvideo question answering. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 1477314783. Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, GangZhang, Bernard Ghanem, and Jian Zhang. 2023b. Aunified continual learning framework with generalparameter-efficient tuning. In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 1148311493. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational conference on machine learning, pages27902799. PMLR.",
  "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.The power of scale for parameter-efficient prompttuning. arXiv preprint arXiv:2104.08691": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR. Tianqi Li, Guansong Pang, Xiao Bai, Wenjun Miao,and Jin Zheng. 2024. Learning transferable nega-tive prompts for out-of-distribution detection.InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages1758417594.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024. Visual instruction tuning. Advances inneural information processing systems, 36": "Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,Minghui Qiu, Pengcheng Lu, Tao Wang, andZhongyu Wei. 2023. Valley: Video assistant withlarge language model enhanced ability.arXivpreprint arXiv:2306.07207. Michael McCloskey and Neal J Cohen. 1989. Catas-trophic interference in connectionist networks: Thesequential learning problem. In Psychology of learn-ing and motivation, volume 24, pages 109165. Else-vier. Inyoung Paik, Sangjun Oh, Taeyeong Kwak, and InjungKim. 2020. Overcoming catastrophic forgetting byneuron-level plasticity control. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 34, pages 53395346. Zi Qian, Xin Wang, Xuguang Duan, Pengda Qin,Yuhong Li, and Wenwu Zhu. 2023. Decouple beforeinteract: Multi-modal prompt learning for continualvisual question answering. In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 29532962. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR.",
  "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-thy Lillicrap, and Gregory Wayne. 2019. Experiencereplay for continual learning. Advances in neuralinformation processing systems, 32": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, RuoxiSun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, GuolongSu, Vincent Perot, Jennifer Dy, et al. 2022a. Dual-prompt: Complementary prompting for rehearsal-free continual learning. In European Conference onComputer Vision, pages 631648. Springer. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,Jennifer Dy, and Tomas Pfister. 2022b. Learning toprompt for continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 139149. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-SengChua. 2021.Next-qa: Next phase of question-answering to explaining temporal actions. In Pro-ceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 97779786. Junbin Xiao, Pan Zhou, Angela Yao, Yicong Li,Richang Hong, Shuicheng Yan, and Tat-SengChua. 2023.Contrastive video question answer-ing via video graph transformer.IEEE Transac-tions on Pattern Analysis and Machine Intelligence,45(11):1326513280.",
  "Shoubin Yu, Jaemin Cho, Prateek Yadav, and MohitBansal. 2024. Self-chained image-language modelfor video localization and question answering. InNeurIPS": "Jipeng Zhang, Jie Shao, Rui Cao, Lianli Gao, Xing Xu,and Heng Tao Shen. 2022. Action-centric relationtransformer network for video question answering.IEEE Transactions on Circuits and Systems for VideoTechnology, 32(1):6374. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hong-sheng Li, and Yu Qiao. 2023a. Llama-adapter: Effi-cient fine-tuning of language models with zero-initattention. arXiv preprint arXiv:2303.16199. Xi Zhang, Feifei Zhang, and Changsheng Xu. 2023b.Vqacl: A novel visual question answering continuallearning setting. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition (CVPR), pages 1910219112. Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, andDacheng Tao. 2024. Panda: Prompt transfer meetsknowledge distillation for efficient model adaptation.IEEE Transactions on Knowledge and Data Engi-neering, pages 114.",
  "A.1Critical Continual Learning Order": "In , we outline the sequence of continuallearning tasks in VideoQA, enabling us to identifyand select the most critical tasks that are particu-larly susceptible to catastrophic forgetting. By un-derstanding which learning order is most affectedby this phenomenon, we can prioritize and im-plement targeted strategies to mitigate forgetting,thereby enhancing the overall robustness and effec-tiveness of the continual learning system. We usebaseline model (Touvron et al., 2023) with addi-tional linear layer for this experiment. We can seethat the learning sequences <TP, CW, DC, TC, DL,DO, TN, CH> have higher Avg. Fog for NExT-QAdataset.",
  "A.2Learning Parameter Analysis with FullModel Fine-tuning": "Since most parameters are fixed at the inferencestage, the performance of a fine-tuned prompt-based model may be worse than that of a fullyfine-tuned model for each specific task. However,during the training stage, fine-tuning the entireLLM incurs high computational costs. Here, weprovided an analysis of this aspect to better under-stand the trade-offs between the effectiveness andcomputation cost of these two approaches usingthe score we get from the DramaQA dataset as anexample. According to the training parameters in-dicated in LLaMa (Touvron et al., 2023) and Lora(Hu et al., 2021), we can assume that to fully fine-tune an LLM requires more than 500M parameters,whereas our prompt-based method only requiresaround 33.5M parameters. Although the Avg. Acc(assumed to be >71.24) of full LLaMA fine-turningmay be higher than our score, but it requires amuch higher computation cost. Our method canefficiently and effectively fine-tune LLaMA-7Bmodel for CL in VideoQA using a single 24GB",
  "A.3Continual Learning Setting andExamples": "In this paper, we split the dataset towards thefunction-incremental setting of continuous learn-ing, similar to existing CL ImageQA works (Leiet al., 2023; Qian et al., 2023), to better evaluatethe CL VideoQA task. We split the dataset accord-ing to different functions. For instance, we splitNExT-QA into causal reasoning function, whichincludes logic understanding of asking why (CW)and how (CH), temporal reasoning function thatinvolves the relationship understanding of objectsor attributes recognition in what are (TC), what did(TN), and what was (TP), and descriptive reasoningfunction encompasses knowledge understanding ofhow many (DC), where (DL), and other types of questions (DO), as illustrated in .1. Sim-ilar for DramaQA, we split the dataset accordingto the function of each question type. The rawvideo examples for CL VideoQA with their corre-sponding question type and answer are illustratedin . The figure showing the differencesbetween them for NExT-QA (Xiao et al., 2021)dataset."
}