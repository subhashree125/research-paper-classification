{
  "Abstract": "Large language models (LLMs) have demon-strated considerable proficiency in general nat-ural language processing (NLP) tasks. Instruc-tion tuning, a successful paradigm, enhancesthe ability of LLMs to follow natural languageinstructions and exhibit robust generalizationacross general tasks. However, these modelsoften encounter performance limitations acrossmultiple tasks due to constrained model ca-pacity. Expanding this capacity during the in-struction tuning phase poses significant chal-lenges. To address this issue, we introduceparameter-efficient sparsity crafting (PESC),which crafts dense models into sparse modelsusing the mixture-of-experts (MoE) architec-ture. PESC integrates adapters into the MoElayers of sparse models, differentiating expertswithout altering the individual weights withinthese layers. This method significantly reducescomputational costs and GPU memory require-ments, facilitating model capacity expansionthrough a minimal parameter increase whenguaranteeing the quality of approximation infunction space compared to original sparse up-cycling. Our empirical evaluation demonstratesthe effectiveness of the PESC method. Us-ing PESC during instruction tuning, our bestsparse model outperforms other sparse anddense models and exhibits superior generalcapabilities compared to GPT-3.5. Our codeis available at",
  "Introduction": "Recent advancements in NLP have been signifi-cantly propelled by the advent of LLMs such asGPT (Brown et al., 2020; OpenAI, 2023), Llama(Touvron et al., 2023a,b), Mistral (Mistral AI, 2023;Jiang et al., 2024), etc. The increasing scale ofLLMs has established them as the experts for NLPtasks due to their exceptional ability to identifycomplex linguistic patterns (Wei et al., 2022). MBPP NaturalQuestions",
  ": Camelidae-834B-pro achieves excellent per-formance across general tasks": "A prominent method for training LLMs is in-struction tuning (Wei et al., 2021). This approachutilizes large-scale, well-formatted instruction data,enabling LLMs to refine their pre-trained represen-tations to comply with human instructions (Taoriet al., 2023; Xu et al., 2024; Dettmers et al., 2024;Mukherjee et al., 2023). Such instruction-tunedLLMs exhibit remarkable generalization capabil-ities in NLP tasks (Longpre et al., 2023). Thisgeneralization requires training on a broad rangeof instruction-following tasks from multiple do-mains such as math, code, biology, etc (Chunget al., 2022; Sanh et al., 2021). However, the in-herent complexity of these tasks can hinder modelfine-tuning (Zhang and Yang, 2021). Specifically,models of certain sizes may struggle to optimizelosses from conflicting tasks, resulting in subparperformance for general tasks.The scaling law (Chung et al., 2022) suggeststhat increasing the models scale is crucial for bet-ter performance. Expanding the models capacitycan also improve instruction tuning effectivenessfor general tasks (Kaplan et al., 2020). Nonetheless, most LLMs are pre-trained dense models designedbased on transformer architecture, which limitsscalability during instruction tuning. Komatsuzakiet al. (2023) presented a method for upcyclingdense models into sparse activated MoE models,which boast greater capacity (Shazeer et al., 2017;Lepikhin et al., 2020; Fedus et al., 2022; Puigcerveret al., 2023). Notably, Shen et al. (2023) suggestedthat MoE models respond more effectively to in-struction tuning compared to dense models. Conse-quently, converting dense models into MoE mod-els during instruction tuning has the potential toachieve great performance on general tasks. Thisconversion involves initializing each expert in theMoE models as a copy of the feedforward neu-ral network (FFN) layers (Chen et al., 2015; Raeet al., 2021). Given the parameter scale of currentLLMs, training such giant models requires updat-ing the weights of experts in the MoE layer, whichis constrained by GPU memory resources and com-putational costs.To mitigate these challenges, we introduceparameter-efficient sparsity crafting (PESC), anapproach that effectively expands model capac-ity while synergizing with parameter-efficient fine-tuning (PEFT) techniques (Houlsby et al., 2019;Dettmers et al., 2024). PESC involves insertingadapters (Houlsby et al., 2019) into the MoE layersof sparse models, allowing differentiation betweenexperts without altering each experts weights inthe MoE layers when guaranteeing the qualityof the approximation in function space comparedto original sparse upcycling (Komatsuzaki et al.,2023). Considering that the more sophisticatedconstruction can improve the approximation (Dinget al., 2022), we also apply the QLoRA (Dettmerset al., 2024) technique to update other weights inthe sparse models. As shown in , ourCamelidae-834B-pro, instruction fine-tuned uti-lizing PESC, achieved the best performance amongvarious open-source sparse models and dense mod-els. Our contributions are described as follows:",
  "Preliminaries": "Adapters. Houlsby et al. (2019) proposed the inte-gration of adapters into pre-trained transformer-based models to enhance parameter efficiency.This approach involves tuning only the parametersadded by the adapters. An adapter consists of twomatrices, W down Rd1d2 and W up Rd2d1,coupled with a non-linear function (). Here, d1and d2 denote the feature dimensions in the pre-trained models and the adapters hidden dimension,respectively, with d2 < d1 typically. Given a fea-ture U RNd1 in the pre-trained model, theoutput of the Adapter module is expressed as:",
  "i=1R(x)iEi(x),(2)": "where R(x)i represents the output of the gatingnetwork for the i-th expert, and Ei(x) is the outputof the i-th expert.Sparsity Crafting. Building on the concept ofsparsity upcycling (Komatsuzaki et al., 2023), spar-sity crafting leverages the weights of dense mod-els. As depicted in , sparsity crafting in-volves a transformative process: substituting the",
  ": Detailed design of the MoE layer for PESCutilizing parameter-efficient experts. All the FFN layersshare the same weights": "FFN layer F within each block of the dense trans-former model with an MoE layer. This replacementgives rise to an innovatively sparse transformerblock. During the initialization phase of sparsitycrafting, each expert Ei within the MoE layer is ini-tialized with the FFN layer F. To ensure structuralcoherence, other components, such as the normal-ization and attention layers, are replicated directlyfrom the dense transformer block.For clarity, let us define Fi(i) as the objectivefunction for the i-th expert in the MoE layer, wherei represents the parameters for Ei. i is initializedfrom o, which are the parameters of the FFN layerF from the original dense model. The essence ofthe sparsity crafting training regimen lies in theoptimization of Fi(i). The goal is to derive +i ,the optimized parameters for each expert. This isformally expressed as:",
  "Parameter-Efficient Sparsity Crafting": "As shown in Equation (3), traditional sparsity craft-ing necessitates optimizing the parameters {i}ni=1for each expert Ei in the MoE layer, leading tosignificant resource consumption, including train-ing time and memory costs due to the extensiveparameters of FFN layers in LLMs. Consequently,as illustrated in , we introduce PESC,an approach that addresses the high training timeand memory costs associated with sparsity craft-ing in LLMs. Specifically, PESC, leveraging theparameter-efficient fine-tuning (PEFT) paradigm,focuses on tuning a smaller subset of parameters toachieve efficiency. The core of PESC lies in its objective function,Fi(i, i), where i represents the select parame-ters for tuning. Notably, the parameters of i is sig-nificantly less than i, as indicated by |i| |i|,where | | indicates the number of parameters in-volved. Each expert Ei begins the process withthe initial state (o, o), where o is initializedto zero to facilitate identity mapping, resulting inFi(o, o) = Fi(o). The training procedure forPESC is thus the optimization of Fi(o, i), lead-ing to a solution +i defined as:",
  "| Fi(+i , o) Fi(o, +i )| < ,(6)": "where is the approximation error.This canbe achieved by designing an approximate func-tion Fi(o, +i ) that closely matches Fi(+i , o)(Houlsby et al., 2019; Ding et al., 2022). Consid-ering that the trajectory of i optimization approxi-mately follows a manifold, which can be projectedinto a lower-dimensional space such as adapterin Equation (1). The approximation error is con-tingent on the representational capacity of the in-serted adapters. Given the universal approximationproperty of MLP layers with general activationfunctions, the Adapter module is a universal ap-proximator (Funahashi, 1989; Leshno et al., 1993;Kidger and Lyons, 2020). As a result, utilizing theadapters as i can effectively ensure the quality ofthe approximation of Fi(+i , o).",
  "Parameter-Efficient Experts. According to theanalysis in .2, adapters can guarantee agood lower bound in Equation (6). Consequently,we can introduce parameter-efficient MoE layers": "by integrating adapters, thereby achieving sparsityin a more parameter-efficient manner.In the training of sparse transformer blocks, gra-dients are back-propagated to each expert, necessi-tating parameter updates. For a collection of n ex-perts, original sparsity crafting demands a compu-tational cost n times that of a single FFN layer. Asdepicted in , our PESC utilizes adapters tocircumvent redundant updates of the expert weightsi. Specifically, we update the i of n insertedadapters to differentiate between experts withoutaltering each experts original weights o replicatedfrom the original FFN layer. Thus, for a given inputx, Equation (2) can be reformulated as:",
  "Ai(x) = (xW idown)W iup + x.(8)": "Considering that the more sophisticated construc-tion can improve the approximation, we can alsoupdate the shared weights o of {Ei}ni=1. As il-lustrated in Equation (7), this approach allows forefficient scaling of the model capacity by intro-ducing a minimal number of parameters across ninserted adapters.Top-K Gate Router. Within the sparse transformerblock, the MoE layer encompasses a specified num-ber of experts. A router, employing a softmax acti-vation function, models a probability distributionover these experts, reflecting each experts capa-bility to process incoming tokens. The routersweights, denoted as W r, which are integrated intothe sparse transformer block, are initially randomlyinitialized. As depicted in , we utilizethe top-k gate router within the sparse transformerblock (Lepikhin et al., 2020; Du et al., 2022). Thisrouter activates the most suitable two experts outof n experts {Ei}ni=1 for each token x in an inputsequence. After receiving the input token x, therouter produces router logits R(x) = W r x. Be-fore being normalized via a softmax distributionover the available n experts, we perform the Keep-TopK function. The KeepTopK function is appliedto retain only the top-k values of the router logits,assigning to the rest, effectively zeroing thempost-softmax normalization. Thus, given a tokenx, the routers output logit is represented as:",
  "R(x) = Softmax(KeepTopK(W r x)).(9)": "The gate value of each expert Ei for the input to-ken x is R(x)i. Despite an increase in parameters,the experts of the MoE layer are activated sparsely,implying that only a limited subset of experts isused per input token. This approach enhances thecapacity of the model while maintaining compu-tational efficiency. The top-k gate router selectsthe best two experts for each token during infer-ence. In an MoE layer with n experts, this enablesup tonkdifferent combinations of experts, as op-posed to a single combination in the traditionaltransformer architecture, providing enhanced com-putational adaptability.Experts Loading Balance. The top-k gate router,through its gating mechanism, tends to dispropor-tionately favor a few experts, leading to an im-balance where these experts are more frequentlytrained and consequently chosen by the router. Tocounter this imbalance and promote uniform expertutilization, an auxiliary loss as suggested by Feduset al. (2022) is integrated during training for eachsparse transformer block. With n experts and abatch B containing T tokens, this auxiliary lossL for experts loading balance is calculated as thescaled dot-product of vectors f and p,",
  "i=1f i pi,(10)": "where fi denotes the fraction of tokens dispatchedto expert i and pi represents the fraction of routerprobability allocated to expert i. is a multiplica-tive coefficient for the auxiliary losses. We utilizean = 102 which was sufficiently large to en-sure load balancing while small enough to not over-whelm the primary cross-entropy objective. As theideal scenario entails uniform routing across the nexperts, both vectors should ideally have values of1n. The auxiliary loss of Equation (10) fosters thisuniform distribution, achieving its minimum undersuch conditions.",
  "Settings": "Training Data. To demonstrate the learning abilityof the sparse model with MoE layers, we simulta-neously trained the model on a diverse set of skills,encompassing coding, mathematical, and other gen-eral abilities from various subjects. This traininginvolved integrating three distinct datasets fromvaried domains during the instruction tuning phase:SlimOrca (Lian et al., 2023; Mukherjee et al., 2023; Longpre et al., 2023), Magicoder (Wei et al., 2023),and MetaMathQA (Yu et al., 2023) datasets. Afterfiltration and sampling, we can get two instructiondatasets including IDAE-500K and IDAE-720K fi-nally. We provide more details of IDAE datasets inAppendix A.Evaluation Benchmarks. Our evaluation com-pares the performance of dense and sparse mod-els on academic benchmarks. The dense modelsinclude Llama2 (Touvron et al., 2023b), Vicuna(Zheng et al., 2023), Yi (01 AI, 2023), SUSChat(SUSTech IDEA, 2023), Qwen (Bai et al., 2023),GPT3.5 (Brown et al., 2020), and our Camel mod-els, while the sparse models encompass Mixtral(Jiang et al., 2024), DeepSeekMoE (Dai et al.,2024), and our Camelidae models. Evaluationsare conducted using OpenCompass (OpenCompass,2023), LM-Eval-Harness (Gao et al., 2023), andour internal evaluation libraries, summarizing per-formances across well-known benchmarks. Thesebenchmarks are illustrated as follows:",
  "Word Knowledge (WK): Assessment of0-shot performance on NaturalQuestions(Kwiatkowski et al., 2019) and TriviaQA(Joshi et al., 2017) utilizing the exact match(EM) metric": "Aggregated Benchmarks: Overall results forMMLU (Hendrycks et al., 2020) (5-shot) uti-lizing accuracy scores metrics.Notably, for more detailed experiment results,please refer to Appendix C.Camel and Camelidae Models. We fine-tunedCamel and Camelidae models using identicaldatasets, IDAE-500K, to ensure fair comparisonsbetween dense and sparse models. Specifically,Camel models are dense models while Camelidaemodels are sparse models with MoE architecture.Notably, to further enhance the capabilities of thesparse models, we also utilize IDAE-720K for theinstruction-tuning of the Camelidae-pro model. AllCamelidae models utilize the top-2 gate router. Implementation Details. We employed QLoRA(Dettmers et al., 2024) techniques for effective fine-tuning of both the Camel and Camelidae modelsderived from Llama2-7B (Touvron et al., 2023b),Llama2-13B (Touvron et al., 2023b), and Yi-34B(01 AI, 2023). As for the QLoRA configuration,we used a 4-bit quantization scheme for our experi-ments, which significantly reduces memory usagewhile preserving model performance. This pro-cess entailed using a constant learning rate sched-ule with a warm-up ratio of 0.03, and the pagedAdamW (Dettmers et al., 2024; Loshchilov andHutter, 2017) optimizer with a learning rate of2 104, no weight decay, a batch size of 128,and a sequence length of 2048 tokens. The mod-els underwent instruction tuning for one epoch on16 A100 GPUs, each equipped with 80G memory.Please refer to Appendix B for more details.",
  "Comparison with Chat LLMs": "We present the performance of various chat LLMson a set of standardized benchmarks. The chat mod-els evaluated are Camelidae-834B-pro, Mixtral-87B-Instruct (Jiang et al., 2024), DeepSeekMoE-16B-Chat (Dai et al., 2024), Yi-34B-Chat (01 AI,2023), Llama2-70B-Chat (Touvron et al., 2023b),Qwen-72B-Chat (Bai et al., 2023), and GPT-3.5(Brown et al., 2020). The benchmarks cover arange of domains, including multiple-choice ques-tions across 57 subjects (MMLU), grade-schoolmath (GSM8K), math problems across variousdifficulty levels (MATH), Python coding tasks(HumanEval), Python code generation (MBPP),commonsense reasoning (HellaSwag), and worldknowledge question answering (NaturalQuestions).As shown in .1, Camelidae-834B-pro demonstrates its strengths in its wide range ofknowledge, mathematical, coding, and common-sense reasoning capabilities across various sparseand dense models.Knowledge and Reasoning Abilities. Camelidae-834B-pro demonstrates impressive performanceon MMLU with a high success rate of 75.7%, indi-cating its wide-ranging professional and academicknowledge. Meanwhile, Camelidae-834B-proscores 31.2% on NaturalQuestions, demonstratinga comprehensive world knowledge base. AlthoughCamelidae-834B-pro is weaker than some mod-els in the HellaSwag benchmark, its 85.2% accu-racy is still decent for commonsense reasoning.Mathematical Proficiency. Camelidae-834B-pro excels on the GSM8K benchmark with 79.4%",
  "NaturalQuestions (EM)17.617.824.726.831.632.231.2TriviaQA (EM)51.051.057.559.463.363.462.5": ": Overall performance on all the evaluation benchmarks of dense models (Camel) and sparse (Camelidae)models across different model sizes. We bold the highest scores separately for different model sizes. accuracy, the highest among models. However, its24.0% score on the MATH benchmark lags behindGPT-3.5, indicating a relative weakness in solvingmore complex mathematical problems.Coding Skills.Camelidae-834B-pro demon-strates strong coding abilities with 48.8% accu-racy on the HumanEval benchmark, comparableto GPT-3.5, and a 43.2% pass rate on the MBPPPython code generation benchmark, showcasing itsprowess in understanding and generating code.",
  "Dense models vs. Sparse Models. We evaluate theefficacy of our novel training methodology througha comparative analysis of Camelidae models, en-compassing both dense and sparse configurations": "across various parameter sizes, as delineated in Ta-ble 2 and . Camelidae models demonstratea significant advantage over counterparts acrossdifferent model sizes. This superiority is particu-larly evident in tasks requiring a deeper understand-ing, including code and mathematical benchmarks,highlighting the efficacy of our training approach inaugmenting model capabilities. To ensure equitablecomparisons, Camel and Camelidae models werefine-tuned using the same dataset, IDAE-500K. Asindicated in , the Camelidae models, assparse models, consistently display superior perfor-mance over the dense Camel models of comparablesizes. Moreover, Camelidae-8x34B-pro, which istrained utilizing the IDAE-720K dataset, outper-forms Camelidae-8x34B which indicates that the",
  "Camelidae-834B38B59.342.750.579.747.875.6Camelidae-834B-pro38B59.946.051.779.246.975.7": ": Overall performance on grouped benchmarksof various dense models (Llama2-Chat (Touvron et al.,2023b), Vicuna (Zheng et al., 2023), Yi-Chat (01 AI,2023), SUSChat (SUSTech IDEA, 2023)) across differ-ent model sizes. We bold the highest scores separatelyfor different model sizes. effectiveness of our method is sustained even withthe increment of the training data volume.Numbers of Experts. The results from the study,as shown in , clearly demonstrate that in-creasing the number of experts in the MoE layerssignificantly enhances the models performance.This trend is evident in the progressive improve-ment in scores across various academic bench-marks as the number of experts increases from4 to 16 in the Camelidae models. Notably, theCamelidae-167B model exhibits exceptional per-formance on all the benchmarks. This positivecorrelation between the number of experts and themodels performance indicates the untapped poten-tial of our approach. Specifically, a further increasein the number of experts might yield even moresubstantial advancements in model performance.",
  ": Evaluation on different numbers of experts inthe MoE layers. We bold the highest scores for eachgrouped benchmark": "distribution patterns of selected experts across var-ious dataset subsets. These included SlimOrca(Lian et al., 2023; Mukherjee et al., 2023; Longpreet al., 2023), Magicoder (Wei et al., 2023), andMetaMathQA (Yu et al., 2023). The outcomes ofthis analysis are depicted in , with particu-lar emphasis on the 15th layers of the Camelidae-87B model.Our findings highlight discernible variations inthe distribution of experts among the three datasets.For instance, Expert 1 exhibits a notably higheractivation within the Magicoder dataset, while Ex-pert 6 demonstrates a significant activation rate inthe MetaMathQA dataset relative to other experts.These observations suggest that the router operateswith a structured syntactic approach. Importantly,despite the variation in expert selection across dif-ferent datasets, certain experts (specifically Experts1, 2, 5, and 6) consistently exhibit elevated activa-tion rates.",
  "Dense and Sparse Models": "Traditional dense models activate all parametersduring training and inference, leading to high com-putational and memory requirements as modelsizes increase. In contrast, sparse models, employ-ing the MoE architecture (Shazeer et al., 2017),activate only a subset of the total available parame-ters for each input token. In sparse models, the FFNlayer is replaced by an MoE layer, directing eachinput token to a select group of expert networksfor processing. The final token representation is an amalgamation of outputs from these chosen ex-perts. Despite an increase in parameters, the sparseactivation of experts ensures computational effi-ciency while enhancing model capabilities. Thesparse models with MoE architecture have beenextensively explored in the field of NLP (Lepikhinet al., 2020; Du et al., 2022; Fedus et al., 2022),particularly with its integration into the transformerblock. Our approach adopts the routing strategyfrom (Lepikhin et al., 2020; Du et al., 2022), withselective parameter activation to achieve computa-tional efficiency.",
  "Reuse of Trained Weights": "Recent studies have focused on improving train-ing efficiency by leveraging pre-existing modelweights for a warm start, thus minimizing train-ing expenses (Chen et al., 2015; Rae et al., 2021;Yang et al., 2021; Lin et al., 2021; Lan et al., 2019).Sparse Upcycling (Komatsuzaki et al., 2023) intro-duces a methodology to initialize sparse MoE mod-els using weights from a pre-trained dense model.This approach significantly reduces the computa-tional resources needed compared to the trainingof the original dense model. Sparse Upcycling in-volves the direct transfer of layer normalization, at-tention, and embedding parameters from the densemodel to the new sparse model. Moreover, it re-places some Multilayer Perceptron (MLP) layerswith MoE layers, initializing the experts in theselayers with weights from the dense models MLP.This process effectively transfers valuable learnedrepresentations from the dense models pre-trainingphase into the sparse model. In our research, weadopt this method, reusing weights from a pre-trained dense model for our PESC method.",
  "Parameter-Efficient Fine-Tuning": "Traditionally, full fine-tuning has been the normfor adapting pre-trained models, including LLMs.However, due to the immense size of LLMs, thisapproach demands substantial computational re-sources. To mitigate this, numerous PEFT meth-ods have emerged (Houlsby et al., 2019; Hu et al.,2021; Li and Liang, 2021; Liu et al., 2022; Wuet al., 2024a). PEFT focuses on training a lim-ited subset of parameters, either from the exist-ing model or newly added ones. Adapter-basedmethods (Houlsby et al., 2019; Hu et al., 2021;Liu et al., 2022; Wu et al., 2024a) integrate small,learnable modules called adapters into pre-trainedmodels, fine-tuning only these newly inserted pa- rameters. Among these, QLoRA (Dettmers et al.,2024) has gained popularity for its efficiency infine-tuning LLMs, yielding results comparable tofull fine-tuning. Another emerging trend in PEFTis prefix-/prompt-tuning (Lester et al., 2021; Li andLiang, 2021), involving the addition of learnabletoken vectors to either the keys and values in atten-tion modules or directly to the input sequence. Inthis study, we insert adapters after the copied FFNlayers to construct MoE layers and employ QLoRAto update the other weight metrics of LLMs.",
  "Mixture of LoRA Experts": "Other works also explore the combination of MoEwith PEFT techniques (Diao et al., 2023; Gouet al., 2023; Wu et al., 2024b; Liu et al., 2023; Luoet al., 2024; Dou et al., 2024). For instance, Lo-RAMoE (Dou et al., 2024) focuses on the retentionof world knowledge, and MoELoRA (Luo et al.,2024) focuses on the Math and CommonSense Rea-soning ability utilizing PEFT frameworks whichunify MOE and LoRA. However, the mixture ofLoRA framework incurs additional computationalcosts including higher memory usage and slowerspeed without parallelism during the training andinference process. Our PESC method, in contrast,does not face these challenges. PESC builds onthe adapter-based model framework, fine-tuningmultiple adapters inserted after the copied FFNlayers instead of all the copied FFN layers in cor-responding experts. In our MoE design of PESC,each expert utilizes a single adapter module, sig-nificantly reducing the overall memory footprintcompared to LoRA module, which would requiremultiple modules per expert due to its placementin FFN and attention layers. This distinction is par-ticularly crucial when dealing with a large numberof experts, as memory constraints become increas-ingly challenging. Moreover, our adapter-basedexperts enable parallel computation across expertsdue to their independence from each others out-puts, unlike LoRA, where dependencies betweenlayers could limit parallelism. This design acceler-ates training time, especially in scenarios where thenumber of experts grows large, ensuring scalabilityand efficiency. Its also worth noting that LoRAmight require merging weights into the main modelfor inference, leading to increased memory usageand potential latency issues, especially since mul-tiple tokens activate different experts. On the con-trary, the adapter-based parameter-efficient MoEdoes not impose such overhead during inference,",
  "Conclusion": "In this paper, we introduce Parameter-EfficientSparsity Crafting (PESC) which upcycles densemodels into sparse models utilizing the MoE ar-chitecture. PESC incorporates adapters (Houlsbyet al., 2019) within the MoE layers of sparse mod-els, enabling the differentiation of experts withoutmodifying the individual weights of each expert,and guarantees the quality of the approximationcompared to traditional sparsity upcycling (Komat-suzaki et al., 2023) in function space (.2).This technique significantly reduces computationalcosts and GPU memory requirements comparedto sparse upcycling. It facilitates the expansionof model capacity with a minimal parameter in-crease due to the integration of adapters. We applythe PESC method to instruction tuning across vari-ous general tasks, resulting in notable performanceenhancements on various benchmarks ().Additionally, we develop sparse models, Cameli-dae, using the PESC approach and achieve supe-rior performance across various open-source sparsemodels and demonstrate superior general capabili-ties compared to GPT-3.5.",
  "Limitation": "The PESC method introduces slightly more param-eters compared to some PEFT techniques (LoRA,etc.). The instruction tuning process of the sparsemodels utilizing the PESC method would requiremore GPU memory and computation time com-pared to dense models. Although PESC enhancesthe performance of instruction tuning for generaltasks, it may still not match the performance ofsparse upcycling with full fine-tuning, as PESC isa mathematical approximation of sparse upcyclingas illustrated in Equation (6).",
  "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. PiQA: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. In Advances in neural information process-ing systems. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374.",
  "Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.2015. Net2Net: Accelerating learning via knowl-edge transfer. arXiv preprint arXiv:1511.05641": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu,Huazuo Gao, Deli Chen, Jiashi Li, WangdingZeng, Xingkai Yu, Y Wu, et al. 2024. DeepSeek-Moe:Towards ultimate expert specialization inmixture-of-experts language models. arXiv preprintarXiv:2401.06066.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, andLuke Zettlemoyer. 2024. QLoRA: Efficient finetun-ing of quantized LLMs. In Advances in Neural Infor-mation Processing Systems": "Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, andTong Zhang. 2023. Mixture-of-Domain-Adapters:Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories. In Proceed-ings of the Annual Meeting of the Association forComputational Linguistics, pages 51135129. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-han Yang, Yusheng Su, Shengding Hu, Yulin Chen,Chi-Min Chan, Weize Chen, et al. 2022. Delta Tun-ing: A comprehensive study of parameter efficientmethods for pre-trained language models.arXivpreprint arXiv:2203.06904. Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, WeiShen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhi-heng Xi, Xiaoran Fan, et al. 2024. LoRAMoE: Alle-viating World Knowledge Forgetting in Large Lan-guage Models via MoE-Style Plugin. In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics, pages 19321945. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.GLaM: Efficient scaling of language models withmixture-of-experts. In International Conference onMachine Learning.",
  "Ken-Ichi Funahashi. 1989. On the approximate real-ization of continuous mappings by neural networks.Neural networks, 2(3):183192": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation. Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, HangXu, Aoxue Li, Dit-Yan Yeung, James T Kwok, andYu Zhang. 2023.Mixture of Cluster-conditionalLoRA Experts for Vision-language Instruction Tun-ing. arXiv preprint arXiv:2312.12379.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, and Ja-cob Steinhardt. 2021. Measuring mathematical prob-lem solving with the math dataset. arXiv preprintarXiv:2103.03874. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for NLP.InInternational Conference on Machine Learning. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. LoRA: Low-Rank Adaptation of LargeLanguage Models. In International Conference onLearning Representations. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, et al.2024.Mixtral of Experts.arXiv preprintarXiv:2401.04088.",
  "Patrick Kidger and Terry Lyons. 2020. Universal ap-proximation with deep narrow networks. In Confer-ence on learning theory, pages 23062327. PMLR": "Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023.Sparse Upcycling: Training mixture-of-experts fromdense checkpoints. In International Conference onLearning Representations. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural Questions: a benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,Kevin Gimpel, Piyush Sharma, and Radu Soricut.2019. AlBert: A lite bert for self-supervised learn-ing of language representations.arXiv preprintarXiv:1909.11942. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,Dehao Chen, Orhan Firat, Yanping Huang, MaximKrikun, Noam Shazeer, and Zhifeng Chen. 2020.GShard: Scaling giant models with conditional com-putation and automatic sharding.arXiv preprintarXiv:2006.16668.",
  "Wing Lian, Guan Wang, Bleys Goodson, Eugene Pent-land, Austin Cook, Chanvichet Vong, and \"Teknium\".2023. Slimorca: An open dataset of gpt-4 augmentedflan reasoning traces, with verification": "Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang,Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, WeiLin, et al. 2021.M6-10T: A sharing-delinkingparadigm for efficient multi-trillion parameter pre-training. arXiv preprint arXiv:2110.03888. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-fel. 2022. Few-shot parameter-efficient fine-tuningis better and cheaper than in-context learning. InAdvances in Neural Information Processing Systems. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu,Derong Xu, Feng Tian, and Yefeng Zheng. 2023.MoELoRA: An MoE-based parameter efficient fine-tuning method for multi-task medical applications.arXiv preprint arXiv:2310.18339. Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The flancollection: Designing data and methods for effectiveinstruction tuning. arXiv preprint arXiv:2301.13688.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM": "Victor Sanh, Albert Webson, Colin Raffel, Stephen HBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, ArunRaja, et al. 2021. Multitask prompted training en-ables zero-shot task generalization. arXiv preprintarXiv:2110.08207. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,Andy Davis, Quoc Le, Geoffrey Hinton, and JeffDean. 2017. Outrageously large neural networks:The sparsely-gated mixture-of-experts layer. arXivpreprint arXiv:1701.06538. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, ShayneLongpre, Jason Wei, Hyung Won Chung, BarretZoph, William Fedus, Xinyun Chen, et al. 2023.Mixture-of-experts meets instruction tuning: A win-ning combination for large language models. arXivpreprint arXiv:2305.14705.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford Alpaca:An Instruction-following LLaMA model": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652."
}