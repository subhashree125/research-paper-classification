{
  "Abstract": "Utilizing large language models (LLMs) forzero-shot document ranking is done in one oftwo ways: (1) prompt-based re-ranking meth-ods, which require no further training but areonly feasible for re-ranking a handful of can-didate documents due to computational costs;and (2) unsupervised contrastive trained denseretrieval methods, which can retrieve relevantdocuments from the entire corpus but requirea large amount of paired text data for con-trastive training. In this paper, we proposePromptReps, which combines the advantagesof both categories: no need for training andthe ability to retrieve from the whole corpus.Our method only requires prompts to guidean LLM to generate query and document rep-resentations for effective document retrieval.Specifically, we prompt the LLMs to repre-sent a given text using a single word, and thenuse the last tokens hidden states and the cor-responding logits associated with the predic-tion of the next token to construct a hybriddocument retrieval system. The retrieval sys-tem harnesses both dense text embedding andsparse bag-of-words representations given bythe LLM. Our experimental evaluation on theMSMARCO, TREC deep learning and BEIRzero-shot document retrieval datasets illustratesthat this simple prompt-based LLM retrievalmethod can achieve a similar or higher re-trieval effectiveness than state-of-the-art LLMembedding methods that are trained with largeamounts of unsupervised data, especially whenusing a larger LLM.",
  ":Overview of PromptReps.LLMs areprompted to simultaneously generate dense and sparserepresentations, then used to build search indexes": "assistance, and conversational agent (Hendryckset al., 2021; Liu et al., 2023). Inspired by the suc-cess of LLMs on natural language understandingtasks, research has explored the potential of usingLLMs to perform unsupervised document ranking.One line of work focuses on directly prompt-ing LLMs to infer document relevance to a givenquery (Sachan et al., 2022; Zhuang et al., 2023;Ma et al., 2023; Sun et al., 2023; Pradeep et al.,2023; Zhuang et al., 2024; Qin et al., 2024). Forinstance, RankGPT (Sun et al., 2023) casts docu-ment re-ranking as a permutation generation task,prompting LLMs to generate re-ordered documentidentifiers according to the documents relevanceto the query. These methods leverage LLMs fordocument ranking in a complete zero-shot settingwhere no further training is required. However,these methods can only serve as a second-stage re-ranker on a handful of candidate documents. Thisis because each prompt requires one full LLM infer-ence: for example, in the case of a corpus with 1Mdocuments, a pointwise approach would requirethe construction of 1M prompts and thus the exe-cution of 1M (costly) LLM inferences making itunfeasible for an online search engine. Another line of research leverages LLMs asa text embedding model for dense document re-trieval (Lee et al., 2024; Wang et al., 2024a,b;BehnamGhader et al., 2024). For example, E5-mistral (Wang et al., 2024b) employs LLMs tocreate synthetic datasets of query-document pairs.These paired text data are then used to perform un-supervised contrastive training for a Mistral LLM-based dense retriever. Since the queries and docu-ments are encoded with LLMs separately; i.e., us-ing a bi-encoder architecture, these methods couldserve as a first-stage document retriever. However,all existing LLM-based retrievers require an un-supervised contrastive training step to transforma generative LLM into a text-embedding model.Even with parameter-efficient training techniquessuch as LoRA (Hu et al., 2022), this extra train-ing is still very expensive. For example, the con-trastive training of E5-mistral using a large batchsize (2048) and LoRA took 18 hours on 32 V100GPUs (Wang et al., 2024b).In this work, we propose a new zero-shotLLM-based document retrieval method calledPromptReps. We demonstrate that LLMs can bedirectly prompted to produce query and documentembeddings, which can serve as effective text repre-sentations for neural retrieval systems. Specifically,we prompt an LLM by asking it to use a single wordto represent a query or a document. Then, we ex-tract the last layers hidden state of the last token inthe prompt as the dense representation of the inputtext. Simultaneously, we utilize the logits associ-ated with predicting the subsequent token to forma sparse representation. As illustrated in ,through a single forward pass, we generate text rep-resentations for a document that can be indexed fordense, sparse, or hybrid search architectures. Wealso explore alternative representations in additionto the core idea in this paper, where we generatemultiple words, and use multiple embeddings torepresent an item (Figures 3 and 4).Our empirical evaluation on multiple datasetsshow that PromptReps can achieve a similar orhigher zero-shot retrieval effectiveness than previ-ous trained LLM-based embedding methods, es-pecially when a large LLM is utilized. Of keyimportance is that our method is the first LLM-based method that can effectively perform full cor-pus retrieval while at the same time not requiringcontrastive training, demonstrating that prompt en-gineering for generative LLMs is capable of gener-ating robust representations for retrieval.",
  "Supervised neural retrievers": "Neural retrievers based on the bi-encoder archi-tecture bring significant improvements over tradi-tional best-match retrievers such as BM25. Denseretrievers such as DPR (Karpukhin et al., 2020)and ANCE (Xiong et al., 2021) are based ontransformer language models and encode textinto low-dimensional vectors, conducting searchwith nearest neighbor search.On the otherhand, sparse neural retrievers such as DeepIm-pact (Mallia et al., 2021), uniCOIL (Lin and Ma,2021), TILDE (Zhuang and Zuccon, 2021c,b), andSPLADE (Formal et al., 2021), also based on trans-former language models, encode text into high-dimensional sparse vectors as bag-of-words repre-sentations, conducting search with inverted index.Recent works also explored fine-tuning generativeLLMs as dense retrievers such as RepLLaMA (Maet al., 2024) and LLaRA (Liao et al., 2024). A hy-brid neural retrieval system refers to a system thatcombines the rankings provided by both dense andsparse retrievers, often resulting in an enhanced fi-nal ranking (Lin and Ma, 2021; Wang et al., 2021).All these retrievers are trained with supervisedrelevance judgment data (e.g., MSMARCO (Ba-jaj et al., 2018)) using contrastive learning. Ourwork instead focuses on building a hybrid neuralretrieval system with zero-shot dense and sparsedocument representations without supervised con-trastive learning and based on generative LLMs.This capability has two implications: (1) no con-trastive training is required, which is expensivewhen applied to LLMs with several billions param-eters, and (2) no human-labelled training data isrequired, which may be laborious and expensive toobtain. With regards to the first point, Wang et al.(2024b) reported that the training of E5-mistrail(7B parameters) took about 18 hours on 32 V100GPUs, for an approximate cost of USD $2,300,1",
  "Based on 4 On-Demand p3dn.24xlarge instances, June 2024.2Emissions and water consumption estimates obtained usingthe frameworks of Scells et al. (2022); Zuccon et al. (2023)": "crease costs. Our proposed method does not incurthese additional contrastive pre-training costs. Withregards to the second point, dense retrievers haveshown to have poor generalisability when appliedto data out-of-domain or out-of-task compared tothe data used for contrastive training (Thakur et al.,2021; Zhuang and Zuccon, 2021a, 2022; Ren et al.,2023; Lin et al., 2023; Lupart et al., 2022). In pres-ence of shift in data between training and deploy-ment, retrieval losses can be significant: dense re-trieval effectiveness can plummet far below that ofbest-match models like BM25 (Khramtsova et al.,2023, 2024). The acquisition of in-domain/in-tasktraining data can be costly, laborious and impracti-cal/impossible especially in domain-specific appli-cations when dealing with sensitive, private data.",
  "Unsupervised neural retrievers": "There have also been attempts at training ef-fective neural retrievers without relying on hu-man relevance judgments. Methods such as Con-triever (Izacard et al., 2022) and E5 (Wang et al.,2024a), train a dense retriever with large-scalepseudo query-document pairs to build unsupervisedtraining data. LLMs have also been adapted as un-supervised text embedding models for first-stagedocument retrieval. For instance, HyDE (Gao et al.,2023a) enhances query representations for an unsu-pervised retriever by replacing the original querywith LLM-generated hypothetical documents.More recent work has focused on directly con-verting generative LLMs into a text-embeddingmodel with unsupervised contrastive pre-training.Methods like E5-Mistral-Inst (Wang et al., 2024b)and Gecko (Lee et al., 2024) use large-scaleweakly supervised paired text data or LLM-generated query-document pair data to per-form contrastive training on top of LLMs.LLM2Vec (BehnamGhader et al., 2024), on theother hand, conducts further masked next tokenprediction pre-training with bidirectional attention,and SimCSE (Gao et al., 2021) trains on raw textdata to transform LLMs into text encoders. Al-though no labeled data is used, these methods re-quire synthetic or unsupervised paired text data toperform contrastive pre-training (thus still expe-riencing training costs in terms of computations;and further computational costs may be associatedwith the generation of synthetic training data). Ourmethod instead relies solely on prompt engineeringto transform LLM into a robust text encoder fordocument retrieval without any extra training.",
  "Prompting LLMs for document ranking": "Inspired by the prompt-following capacity ofLLMs, recent studies have explored promptingLLMs for document re-ranking.For instance,UPR (Sachan et al., 2022) ranks documents point-wise by prompting the LLM to generate a relevantquery for a given document and rank documentsbased on the likelihood of generating the query.RankGPT (Sun et al., 2023) and LRL (Ma et al.,2023) propose to re-rank a list of documents at onceand generate permutations for the reordered list.Pairwise (Qin et al., 2024) and Setwise (Zhuanget al., 2024) prompting methods have also beenexplored to improve effectiveness and efficiencyin the LLM re-ranking pipeline. These methodsare only feasible for re-ranking a handful of candi-date documents, thus limited to second-stage docu-ment re-ranking. In contrast, our approach utilizesprompts to construct the first-stage retrievers.",
  "Prompting LLMs for sentence embedding": "The methods most similar to ours prompt LLMs togenerate sentence embeddings for semantic textualsimilarity (STS) tasks (Jiang et al., 2023b; Lei et al.,2024; Zhang et al., 2024). These previous methodsalso used an Explicit One-word Limitation (EOL)prompt, which also instructs LLMs to represent asentence with one word. However, these methodsonly evaluate such prompts on STS datasets, andtheir effectiveness on information retrieval datasetswith large document corpora is unknown. Addition-ally, these methods only represent text with denseembeddings from the hidden states; our methodinstead generates dense and sparse representationssimultaneously to build a hybrid retrieval system.Our empirical results show that dense embeddingsalone perform poorly for document retrieval taskswith some LLMs, but sparse representations aremuch more robust, and the best retrieval effective-ness is achieved with the hybrid retrieval systemwith scaled model size.",
  "PromptReps": "Previous work that leverages LLMs for documentranking are limited to document re-ranking taskswith prompts or rely on contrastive learning totransform a generative LLM into an embeddingmodel for document retrieval. Unlike these previ-ous works, here we aim to directly prompt LLMsto generate both dense embedding representationsand sparse bag-of-words representations for docu- ment retrieval without any form of extra training ef-fort. To achieve this, we devise the prompt as illus-trated in as the input text for LLMs, where<System> <User> and <Assistant> are LLM pre-defined conversational prefix tokens and [text] isthe placeholder for passage text.When using this prompt for text generation, thelanguage model needs to find a single word in itstoken vocabulary that can best represent the givenpassage to generate. However, since there couldbe multiple words to represent the passage, theremight be multiple tokens in the vocabulary thathave a high probability of being sampled by thelanguage model. Such a distribution over the vo-cabulary, which is often refers to as logits, couldprovide a good representation of the given passage.In addition, since the logits are computed by thelast layer hidden state3 of the last input token ( ), which is a dense vector embedding, it could alsoserve as a dense representation of the passage.Based on the above intuition, we develop asparse + dense hybrid document retrieval systemby utilizing both the next token logits and the lastlayer hidden states outputted by the LLM with ourdesigned prompt.Specifically, during the document indexingphase, we pass all the documents (one at the time)with our prompt into the LLM to get output hid-den states and logits. To build a sparse retrievalpipeline with logits, we first need to sparsify thelogits representation to be able to perform efficientsparse retrieval. This is because logits originallyhad values for all tokens in the vocabulary, essen-tially forming dense vectors with dimensions equalto the vocabulary size. To sparsify the logit rep-resentations for sparse retrieval, we perform thefollowing steps: 1. Lowercase the input document text to align withthe phrase Make sure your word is in lower-case. in the prompt since this phrase skewedthe sampling distribution towards lowercase to-kens (a sparser distribution). We then utilizethe NLTK toolkit (Bird and Loper, 2004) to ex-tract all words in the document, filtering outstandard English stopwords and punctuation.",
  ". Next, we use the LLMs tokenizer to tokenizeeach extracted word and obtain their token IDs.4": "3Often through dot product between the last hidden state withall token embeddings.4Note that many words may be split into sub-tokens, resultingin multiple token IDs, all of which are considered in the logits We retain only the values corresponding to theobtained token IDs in the logits and set the restof the dimensions to zero, thereby consideringonly tokens present in the documents, thus en-abling exact term matching in retrieval. 3. Next, we follow the SPLADE recipe (Formalet al., 2021), using the ReLU function to removedimensions with negative values and applyinglog-saturation to the logits to prevent certaintokens from dominating. To further enhance thesparsity of logits, we only keep tokens withinthe top 128 values if the logits had more than128 non-zero values after the previous steps.",
  ". Finally, the logits are quantized by multiplyingthe original values by 100 and taking the inte-ger operation on that, and the obtained valuesrepresent the weights of corresponding tokens": "With these adjustments, the logits representationsof documents are heavily sparsified, allowing forefficient sparse retrieval with an inverted index.For dense retrieval, we directly use the hiddenstates as the embeddings of the documents. Forindexing these embeddings, we simply normal-ize all the embeddings and add them into an Ap-proximate Nearest search (ANN) vector index. InAppendix A, we provide example Python code ofgenerating dense and sparse representations withPromptReps.At query time, we process the queries exactlythe same as the documents, with the only excep-tion being that the term passage in the prompt isreplaced with query.5 The dense representationof the query is utilized for semantic search via theANN index, while the sparse representation of thequery is employed for exact term matching via theinverted index. Following previous work (Wanget al., 2021), we compute the final document scoresby applying min-max normalization to both denseand sparse document scores. These normalizedscores are then linearly interpolated with equalweights to produce the final document scores. Wedo not explicitly tune the weight because our set-ting is zero-shot retrieval, and we wanted to main-tain the zero-shot nature of our approach. Never-theless, in Appendix B, we explore the impact ofthe different weight settings.",
  "Experimental Setup": "Dataset and evaluation: We evaluate the docu-ment ranking effectiveness of both baseline meth-ods and our proposed PromptReps using MS-MARCO (Bajaj et al., 2018) passage retrieval,TREC deep learning (Craswell et al., 2020) andBEIR (Thakur et al., 2021). These datasets encom-pass various IR tasks, providing a heterogeneousevaluation environment. For MSMARCO we re-port MRR@10 and for TREC deep learning andBIER we report nDCG@10 scores, the commonlyemployed evaluation measure for these datasets.Baselines: We compare PromptReps with strongunsupervised first-stage retrievers including BM25,a classic term frequency-based sparse retrievalmethod, and E5-PTlarge (Wang et al., 2024a), astate-of-the-art BERT large-based dense embed-ding method trained on 1.3B carefully crafted un-supervised text pairs. LLM2Vec (BehnamGhaderetal.,2024),aLlama3-8B-InstructLLM-based dense embedding method trained with bi-directional attention, masked next token prediction,and SimCSE (Gao et al., 2021) on the Wikipediacorpus. In addition, We also report state-of-the-art supervised contrastive, fine-tuned BERT-basedsparse retriever SPLADE++ (Formal et al., 2022)and dense retriever DRAGON+ (Lin et al., 2023).We note that these methods are trained with lots ofsupervised training data and knowledge distillationfrom teacher models, thus it is unfair to comparewith our method and other unsupervised baselines.However, we think it is useful to compare with su-pervised methods to understand the gap betweensupervised and unsupervised methods.Implementation of PromptReps: PromptRepsis implemented using four base LLMs: Mistral- 7b-Instruct-v0.26 (Jiang et al., 2023a), Phi-3-mini-4k-instruct7 (Abdin et al., 2024), Llama3-8B-Instruct,8 and Llama3-70B-Instruct9 (AI@Meta,2024). Dense and sparse document and query en-codings are implemented using the HuggingfaceTransformers library (Wolf et al., 2020) and theTevatron toolkit (Gao et al., 2023b). The Faisslibrary (Douze et al., 2024) is used to build theANN index with cosine similarity as the embed-ding distance metric. We simply use brute forcesearch for ANN (IndexFlatIP in Faiss) for a faircomparison with the baselines. For sparse retrieval,Pyserini (Lin et al., 2021) is utilized to constructthe inverted index. For the dense and sparse rank-ing hybrid, the Ranx library (Bassani and Romelli,2022) is employed. In our experiments, we reportdense only, sparse only, and the full hybrid results.",
  "Zero-shot Results": "We start by showing our overall results on theBEIR dataset, which we treated as test set; we thenanalyse choices in instantiation of PromptReps, in-cluding different variations in the prompt usingthe MSMARCO and TREC deep learning datasets,which we used as development datasets to informthe choices we made to run PromptReps on BEIR.",
  "avg52.4649.1050.66": "across numerous datasets, achieving a higher aver-age nDCG@10 score. This outcome implies thateven with a large-size LLM, bi-directional atten-tion enabled, additional pre-training, and SimCSE-based unsupervised contrastive training, there re-mains a gap in transforming a decoder-only LLMinto an effective retrieval method.On the other hand, E5-PTlarge, based on theBERT-large model, is the first method that canoutperform BM25 without any supervised train-ing data. However, it has been trained on a massive,carefully mined text pair dataset with a large batchsize, which may require more data-collecting ef-forts and computational resources than LLM2Vec.PromptReps with Llama3-8B-Instruct LLM haslower nDCG@10 scores when only using denseor sparse retrieval. However, the hybrid system(combining dense and sparse) contributes notableretrieval effectiveness improvements, surpassingBM25 and approaching the state-of-the-art E5-PTlarge. Notably, this is achieved without any formof extra training but solely relying on prompts.The scaling law observed for LLMs (Kaplanet al., 2020) and dense retrievers (Fang et al., 2024)also applies here. When changing Llama3-8B-Instruction to Llama3-70B-Instruction, the denseand sparse retrieval effectiveness of PromptRepsfurther improves, with the hybrid approach compa-rable to E5-PTlarge.",
  "Further hybrid with BM25": "In we report results of unsupervised LLM-based retrievers further hybrid with BM25 on BEIRdatasets. For PromptReps, we generate hybrid rank-ing by combining dense, sparse, and BM25 rank-ings using min-max normalization, assigning equal weight to each. For the baseline, we report E5-PTlarge hybrid with BM25, also using min-max nor-malization and equal weights. Compared to theirstandalone retrieval effectiveness reported in Ta-ble 1, the effectiveness of all LLM-based retrieverssignificantly improved. E5-PTlarge and PromptRepswith Llama 70B model surpassed supervised train-ing methods. These results demonstrate that it ispossible to build a strong retrieval system withLLMs and BM25 without the need for any super-vised training.",
  "Sensitivity to different prompts": "In the previous experiments, we always use theprompt illustrated in . In this section, westudy how different prompts impact the retrievaleffectiveness. Particularly, we design six differ-ent prompts,10 listed in , and conduct ex-periments on TREC deep learning 2019 and 2020datasets, and MSMARCO passage retrieval devsub-dataset. We use Llama3-8B-Instruction as thebase LLM for PromptReps. The results are listedin . We also report results of Recall@1000and other base LLMs in Appendix C. The results demonstrate that PromptReps canachieve a similar level of retrieval effectivenessas BM25 and surpass LLM2Vec with most of theprompts. The only prompt that does not work wellis prompt #4, which does not include the phraseThe word is: to force the LLM to generatethe representative word as the next token. Thisis expected because, without this phrase, the firstgenerated token would be a general token such asThe which is not representative of the input text. Interestingly, our results also show that LLMshave instruction-following ability in this represen-tation generation task. For instance, comparingprompts #1 and #2, the only difference is the phrasein a retrieval task, and the prompt with thisphrase yields higher retrieval effectiveness acrossall datasets. Additionally, comparing prompts #1and #6, the difference is the phrase Make sure yourword is in lowercase, which matches our sparseexact matching mechanism where we first lower-case the input text. This phrase can further improvethe retrieval effectiveness. Finally, using the adjec-tive phrase most important in the prompt doesnot significantly impact the results.",
  "IDPrompts": "1Use one word to represent the passage in a retrieval task.<A>The word is: \"2Use one word to represent the passage.<A>The word is: \"3Use one most important word to represent the passage in a retrieval task. Make sure your word is in lowercase.<A>The word is: \"4Use one word to represent the passage in a retrieval task.<A>5Use one most important word to represent the passage in a retrieval task.<A>The word is: \"6Use one word to represent the passage in a retrieval task. Make sure your word is in lowercase.<A>The word is: \"",
  "Impact of different LLMs": "In this section, we explore how different baseLLMs impact PromptReps. For this study, we in-vestigate five state-of-the-art open-sourced decoder-only LLMs, covering different model sizes andmodels with or without instruction tuning. We useprompt #6 for all LLMs11 and report MRR@10scores on the MSMARCO datasets. The results areillustrated in ; more detailed results includ-ing on TREC deep learning datasets are reported inAppendix C.The results show that the hybrid retrieval effec-tiveness of PromptReps consistently outperformsBM25, regardless of which LLM is used, with theonly exception of Mistral-7B-Instruct. When us-ing the Mistral-7B-Instruct LLM, the dense-onlyretriever performs poorly.Surprisingly, imple-menting PromptReps with Phi-3-mini-4k-instructachieved much higher retrieval effectiveness thanthat of Mistral-7B-Instruct, despite having far fewerparameters (3.8B).",
  ":MRR@10 scores on MSMARCO ofPromptReps with different LLMs": "Meta-Llama-3 models are generally very effec-tive for our method. For 8B models, the instruction-tuned model performs significantly better than thepretrained-only model, indicating that the instruc-tion fine-tuning is helpful to further improve ourmethod. The 70B instruction-tuned model achievedthe best hybrid retrieval results, but the dense-onlyand sparse-only retrieval effectiveness is similarto the 8B instruction-tuned model. These resultsagree with the BEIR results presented in .",
  "Supervised Results": "We have demonstrated the strong zero-shot effec-tiveness of PromptReps. Now we explore the ques-tion: Can PromptReps serve as a better initializa-tion for LLM-based embedding models in down-stream contrastive supervised fine-tuning?To address this question, we conduct supervisedfine-tuning experiments using MSMARCO. Specif-ically, we follow the RepLlama training recipe (Maet al., 2024) to fine-tune the LLama-3-8B-Instructbase model with InfoNCE loss and hard nega-tive passages mined by a BM25 and dense re-trieval hybrid system. The detailed training hyper-parameters are listed in Appendix D. For the Re-pLlama baseline, we adhere to the original imple-mentation, which appends the prefixes Query: and Passage: to the query and document text,respectively, and adds the end-of-sentence tokenat the end of the text. The output embedding of this token is then used to represent the text. ForPromptReps, we use our proposed prompt (#6 in) and the last token hidden states and log-its as the dense and sparse representation of thetext. For fine-tuning PromptReps, we explore twosettings, PromptReps-dense only, which only usesthe dense representation of PromptReps to calcu-late document scores during training and inference.This setting ensures a fair comparison with Re-pLlama, as the only difference is the prompt used.The other setting involves using both dense andsparse document scores to calculate the loss, sim-ply adding the two losses as the final loss. Duringinference, we report the dense, sparse, and hybridretrieval effectiveness separately for this setting.We train both RepLlama and PromptReps for1 epoch using the full MSMARCO training data,which contains 490k training examples. In additionto the full training, we also explore a low-resourcetraining setting, where we sample 1k examplesfrom the entire MSMARCO dataset. We then splitthe 1k examples into a training and a validationset with a 9:1 ratio. We monitor the validationloss after each training epoch and stop the training,selecting the best checkpoint if no lower validationloss is observed for three consecutive epochs.The results are shown in . Surprisingly,with only 1k training examples, ranking effective-ness of RepLlama improved from 0 to a compet-itive score. This finding suggests that it is possi-ble to convert an LLM into an effective embed-ding model with little training data. On the otherhand, PromptReps-dense only achieved the bestMRR@10 score on MSMARCO dev. However,the hybrid training loss coupled with hybrid re-trieval achieved the highest effectiveness across dif-ferent training settings on TREC DL; the only ex-ception being the full-data setting on MSMARCOdev. These results demonstrate that PromptRepscould be seen as a simple approach to obtaininga better initialization of LLM-based embeddingmodels, which is more cost-effective than meth-ods requiring further pre-training (BehnamGhaderet al., 2024; Li et al., 2023).",
  "RepLlama30.063.1073.35PromptReps-dense only43.8161.4673.00PromptReps-dense43.8161.0473.65PromptReps-sparse45.6050.1562.81PromptReps-hybrid56.6664.0173.87": "the first generated token. We define this settingas First-token single-representation (FTSR). Wehave demonstrated that this simple way of gen-erating representations is effective for documentretrieval; however, these representations might besub-optimal. For example, LLMs use sub-word tok-enization algorithms such as SentencePiece (Kudoand Richardson, 2018). This tokenization mightsplit a word into sub-words, meaning that the firstgenerated token might just be a sub-word. Us-ing the representation of the whole word might bea better representation than the first token repre-sentation. Additionally, previous works in multi-vector dense retrieval such as ColBERT (Khattaband Zaharia, 2020) demonstrated that using mul-tiple representations could be beneficial for doc-ument retrieval. How can we use PromptReps toalso generate single-word representations or multi-ple representations that can potentially enhance theretrieval effectiveness? In this section, we explorethese alternative representations.First-word single-representation (FWSR) andMulti-token single-representation (MTSR). Insteadof using the representations of the first generatedtoken, these two methods let the LLM finish thegeneration12 of the whole word or multiple words,controlled by the given prompt (Use one word orUse three words), as illustrated in . Theend of generation is detected by the token . Wethen pool all the representations of the generatedtokens to form a single dense and sparse representa-tion for the input text. For the dense representationwe use mean pooling and for the sparse representa-",
  "tion we use max pooling. Once representations areobtained, the scoring is the same as FTSR": "Multi-token multi-representation (MTMR) andMulti-word multi-representation (MWMR). Insteadof using a single representation for retrieval, thesetwo methods prompt the LLM to generate multiplewords and then index each generated representa-tion separately. The difference between the two isthat MTMR keeps all the token representations inthe index, while MWMR first groups tokens intowords by using space, and then creates a singlerepresentation for each word by using max poolingfor sparse representations and mean pooling fordense representations. During retrieval, we followthe ColBERT scoring method where the relevancescore of a document is computed by the sum of themaximum similarity of each query representationagainst each document representation (). Hybrid retrieval results are shown in ,and full dense and sparse retrieval results in Ap-pendix E. Results show that all the explored meth-ods are able to perform document retrieval. TheFTSR and MTSR generally perform the best. How-ever, we note that MTSR requires more token gen-eration steps and thus has higher query latency. TheFWSR performs the worst, suggesting that sub-word representations hurt the retrieval performancefor single-word generation prompts. On the otherhand, multi-representation methods with ColBERTscoring methods do not seem beneficial. Thus, weconclude that the simplest FTSR is sufficient torepresent the input text for document retrieval.",
  "Conclusion": "We introduced PromptReps, a simple yet effectivemethod that prompts LLMs to generate dense andsparse representations for zero-shot document re-trieval without any further training. We show thatmodern LLMs are effective text encoders by them-selves, and prompt engineering is sufficient to stim-ulate their text encoding ability.For future works, techniques like few-shot in-context learning (Brown et al., 2020), chain-of-thought prompting (Wei et al., 2022), and auto-prompt optimization methods (Yang et al., 2024;Fernando et al., 2023), which have proven to be ef-fective in text-generation tasks, could potentially beleveraged here to enhance embedding generation.Moreover, it has been shown that the instruction-following ability of LLMs could be transferredto embedding models with synthetic instructionfine-tuning data (Wang et al., 2024b).In ourwork, we always keep the instruction prompt con-sistent across different IR tasks, which could besub-optimal. It is interesting to investigate how tocustomize instructions for PromptReps to generateembeddings specific to different domains, tasks, oreven to multi-lingual and cross-lingual IR settings.Finally, our prompting method could be seen asa simple approach to obtaining a better initializa-tion of LLM-based embedding models and all theprevious contrastive pre-training with paired textdata and synthetically generated data could be ap-plied on top of our method and could potentiallyyield improved LLM-based embedding models.",
  "Limitations": "PromptReps has higher query latency than otherLLM-based dense retrievers if no further optimiza-tion is implemented. This limitation comes fromtwo aspects.First, although the computation of document rep-resentations happens offline thus will not affectquery latency, the query representations are createdonline. PromptReps adds extra prompt texts on topof the query text thus has a longer input length and LLM inference time is proportional to promptlength. However, we believe this limitation can bemitigated by leveraging recent works on promptcompression to compress the fixed prompt tokensinto few or even a single latent token (Ge et al.,2024; Cheng et al., 2024).Second,thehighesteffectivenessforPromptRepsisachievedinthehybridre-trieval setting. Compared to previous works whichuse dense representations only, the hybrid settingrequires both dense and sparse retrieval, thus theextra sparse retrieval introduces extra query latency(and requires additional disk/memory space for theinverted index). However, PromptReps actuallyonly requires a limited query latency overheadif dense and sparse retrieval are implemented inparallel. In our method, obtaining both dense andsparse representations only requires a single LLMforward inference; the only extra computationis the dot product of the dense vector with thetoken embeddings, which is very fast on GPU.For document search, since we heavily sparsifiedthe sparse representation, in our experiments, oursparse retriever is much faster than BM25, and thebottleneck is the dense retriever. Since the denseand sparse search could be run in parallel and thehybrid operation is a simple linear interpolationof both rankings (very fast on CPU), the querylatency of the hybrid process only depends on thedense retrieval latency, and it is thus very close toprevious methods.",
  "Ethical Considerations": "In our experiments, we use PromptReps coupledwith LLMs with a large number of parameters (upto 70B in our experiments) to encode the BEIR andMSMARCO datasets, which contain millions ofdocuments. Although no LLM training was con-ducted, we are aware that our experiments mightstill have consumed significant energy, thus con-tributing to CO2 emissions (Scells et al., 2022) and water consumption (Zuccon et al., 2023). Inaddition, since we leverage LLMs in a black-boxmanner and LLMs generation might contain bi-ases (Gallegos et al., 2024), the representationsgenerated by LLMs may be biased towards cer-tain contents or topics. Future work could considerhow to mitigate biases in PromptReps via promptengineering.",
  "AI@Meta. 2024. Llama 3 model card": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-drew McNamara, Bhaskar Mitra, Tri Nguyen, MirRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,and Tong Wang. 2018.MS MARCO: A humangenerated machine reading comprehension dataset.Preprint, arXiv:1611.09268. Elias Bassani and Luca Romelli. 2022. ranx.fuse: APython library for metasearch. In Proceedings ofthe 31st ACM International Conference on Informa-tion & Knowledge Management, CIKM 22, pages48084812, New York, NY, USA. Association forComputing Machinery. Parishad BehnamGhader, Vaibhav Adlakha, MariusMosbach, Dzmitry Bahdanau, Nicolas Chapados, andSiva Reddy. 2024. Llm2vec: Large language mod-els are secretly powerful text encoders. Preprint,arXiv:2404.05961. Steven Bird and Edward Loper. 2004. NLTK: The natu-ral language toolkit. In Proceedings of the ACL In-teractive Poster and Demonstration Sessions, pages214217, Barcelona, Spain. Association for Compu-tational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.",
  "ChrisanthaFernando,DylanBanarse,HenrykMichalewski, Simon Osindero, and Tim Rock-tschel. 2023.Promptbreeder:Self-referentialself-improvement via prompt evolution. Preprint,arXiv:2309.16797": "Thibault Formal, Carlos Lassance, Benjamin Pi-wowarski, and Stphane Clinchant. 2022. From dis-tillation to hard negative sampling: Making sparseneural IR models more effective. In Proceedings ofthe 45th International ACM SIGIR Conference onResearch and Development in Information Retrieval,SIGIR 22, page 23532359, New York, NY, USA.Association for Computing Machinery. Thibault Formal, Benjamin Piwowarski, and StphaneClinchant. 2021. SPLADE: Sparse lexical and ex-pansion model for first stage ranking. In Proceedingsof the 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval,SIGIR 21, pages 22882292, New York, NY, USA.Association for Computing Machinery. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow,Md Mehrab Tanjim, Sungchul Kim, Franck Dernon-court, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed.2024. Bias and fairness in large language models:A survey. Computational Linguistics, 50(3):10971179. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.2023a. Precise zero-shot dense retrieval without rel-evance labels. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 17621777,Toronto, Canada. Association for Computational Lin-guistics. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.2023b. Tevatron: An efficient and flexible toolkitfor neural retrieval. In Proceedings of the 46th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 23,pages 31203124, New York, NY, USA. Associationfor Computing Machinery.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "SimCSE: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 68946910, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen,and Furu Wei. 2024. In-context autoencoder for con-text compression in a large language model. In TheTwelfth International Conference on Learning Repre-sentations. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2022. Unsupervised dense infor-mation retrieval with contrastive learning. Preprint,arXiv:2112.09118. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023a. Mistral 7b. Preprint,arXiv:2310.06825.",
  "Ting Jiang, Shaohan Huang, Zhongzhi Luan, DeqingWang, and Fuzhen Zhuang. 2023b. Scaling sentenceembeddings with large language models. Preprint,arXiv:2307.16645": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781,Online. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020. ColBERT: Effi-cient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR20, pages 3948, New York, NY, USA. Associationfor Computing Machinery. Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Bak-tashmotlagh, Xi Wang, and Guido Zuccon. 2023. Se-lecting which dense retriever to use for zero-shotsearch. In Proceedings of the Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval in the Asia Pacific Re-gion, SIGIR-AP 23, page 223233, New York, NY,USA. Association for Computing Machinery. Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Bak-tashmotlagh, and Guido Zuccon. 2024. LeveragingLLMs for unsupervised dense retriever ranking. InProceedings of the 47th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR 24, page 13071317, NewYork, NY, USA. Association for Computing Machin-ery. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen,Daniel Cer, Jeremy R. Cole, Kai Hui, MichaelBoratko, Rajvi Kapadia, Wen Ding, Yi Luan, SaiMeher Karthik Duddu, Gustavo Hernandez Abrego,Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Pra-teek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. 2024. Gecko: Ver-satile text embeddings distilled from large languagemodels. Preprint, arXiv:2403.20327.",
  "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.2021. Pyserini: A Python toolkit for reproducible": "information retrieval research with sparse and denserepresentations. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21,pages 23562362, New York, NY, USA. Associationfor Computing Machinery. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and XilunChen. 2023. How to train your Dragon: Diverse aug-mentation towards generalizable dense retrieval. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 63856400, Singapore.Association for Computational Linguistics. Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,Mengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao,Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen,Tianming Liu, and Bao Ge. 2023.Summary ofChatGPT-related research and perspective towardsthe future of large language models. Meta-Radiology,1(2):100017.",
  "Ronak Pradeep, Sahel Sharifymoghaddam, and JimmyLin. 2023. RankVicuna: Zero-shot listwise docu-ment reranking with open-source large language mod-els. Preprint, arXiv:2309.15088": "Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, JialuLiu, Donald Metzler, Xuanhui Wang, and MichaelBendersky. 2024. Large language models are effec-tive text rankers with pairwise ranking prompting.Preprint, arXiv:2306.17563. Ruiyang Ren, Yingqi Qu, Jing Liu, Xin Zhao, QifeiWu, Yuchen Ding, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2023. A thorough examination on zero-shot dense retrieval. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1578315796, Singapore. Association for Computa-tional Linguistics. Devendra Sachan, Mike Lewis, Mandar Joshi, ArmenAghajanyan, Wen-tau Yih, Joelle Pineau, and LukeZettlemoyer. 2022. Improving passage retrieval withzero-shot question generation. In Proceedings ofthe 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 37813797, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Harrisen Scells, Shengyao Zhuang, and Guido Zuccon.2022. Reduce, reuse, recycle: Green informationretrieval research. In Proceedings of the 45th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 22,page 28252837, New York, NY, USA. Associationfor Computing Machinery. Weiwei Sun, Lingyong Yan, Xinyu Ma, ShuaiqiangWang, Pengjie Ren, Zhumin Chen, Dawei Yin, andZhaochun Ren. 2023. Is ChatGPT good at search?investigating large language models as re-rankingagents. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, pages 1491814937, Singapore. Association forComputational Linguistics. Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In Thirty-fifth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track (Round 2). Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Liang Wang, Nan Yang, Xiaolong Huang, Binx-ing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-jumder, and Furu Wei. 2024a. Text embeddings byweakly-supervised contrastive pre-training. Preprint,arXiv:2212.03533.",
  "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,Rangan Majumder, and Furu Wei. 2024b. Improv-ing text embeddings with large language models.Preprint, arXiv:2401.00368": "Shuai Wang, Shengyao Zhuang, and Guido Zuccon.2021. BERT-based dense retrievers require interpo-lation with bm25 for effective passage retrieval. InProceedings of the 2021 ACM SIGIR InternationalConference on Theory of Information Retrieval, IC-TIR 21, pages 317324, New York, NY, USA. Asso-ciation for Computing Machinery. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In International Conference on LearningRepresentations.",
  "Simple techniques for enhancing sentence embed-dings in generative language models.Preprint,arXiv:2404.03921": "Shengyao Zhuang, Bing Liu, Bevan Koopman, andGuido Zuccon. 2023. Open-source large languagemodels are strong zero-shot query likelihood modelsfor document ranking. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 88078817, Singapore. Association for Com-putational Linguistics. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman,and Guido Zuccon. 2024. A setwise approach foreffective and highly efficient zero-shot ranking withlarge language models. In Proceedings of the 47thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR24, page 3847, New York, NY, USA. Associationfor Computing Machinery. Shengyao Zhuang and Guido Zuccon. 2021a. Deal-ing with typos for BERT-based passage retrieval andranking. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 28362842, Online and Punta Cana, Domini-can Republic. Association for Computational Lin-guistics.",
  "Shengyao Zhuang and Guido Zuccon. 2021b.Fastpassage re-ranking with contextualized exact termmatching and efficient passage expansion. Preprint,arXiv:2108.08513": "Shengyao Zhuang and Guido Zuccon. 2021c. TILDE:Term independent likelihood model for passage re-ranking. In Proceedings of the 44th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, SIGIR 21, pages14831492, New York, NY, USA. Association forComputing Machinery. Shengyao Zhuang and Guido Zuccon. 2022. Character-bert and self-teaching for improving the robustnessof dense retrievers on queries with typos. In Proceed-ings of the 45th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR 22, page 14441454, New York,NY, USA. Association for Computing Machinery. Guido Zuccon, Harrisen Scells, and Shengyao Zhuang.2023. Beyond CO2 emissions: The overlooked im-pact of water consumption of information retrievalmodels. In Proceedings of the 2023 ACM SIGIRInternational Conference on Theory of InformationRetrieval, ICTIR 23, page 283289, New York, NY,USA. Association for Computing Machinery."
}