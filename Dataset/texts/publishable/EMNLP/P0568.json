{
  "Abstract": "Large Language Models (LLMs) undergo ex-tensive evaluation against various benchmarkscollected in established leaderboards to assesstheir performance across multiple tasks. How-ever, to the best of our knowledge, there is alack of comprehensive studies evaluating thesemodels linguistic abilities independent of spe-cific tasks. In this paper, we introduce a novelevaluation methodology designed to test LLMssentence generation abilities under specific lin-guistic constraints. Drawing on the linguis-tic profiling approach, we rigorously investi-gate the extent to which five LLMs of varyingsizes, tested in both zero- and few-shot sce-narios, effectively adhere to (morpho)syntacticconstraints. Our findings shed light on the lin-guistic proficiency of LLMs, revealing boththeir capabilities and limitations in generatinglinguistically-constrained sentences1.",
  "Introduction": "Recent advancements in Natural Language Pro-cessing (NLP) have been significantly shaped bythe emergence and refinement of Large-scale Lan-guage Models (LLMs) (Achiam et al., 2023; Tou-vron et al., 2023; Jiang et al., 2023). These modelsdemonstrated remarkable capabilities in solvingmultiple tasks and in generating coherent and con-textually relevant texts, underscoring their poten-tial for capturing complex linguistic structures withhigh precision and accuracy (Contreras Kallenset al., 2023). Such capabilities have been exten-sively evaluated against several benchmarks, asevidenced by the success of platforms such as theOpenLLM Leaderboard (Beeching et al., 2023), ina task-oriented scenario covering a wide range ofNLP tasks such as commonsense reasoning (Zellerset al., 2019; Hendrycks et al., 2021a), mathemat-ical problem-solving (Hendrycks et al., 2021b),",
  ": Illustrated examples of the evaluation method-ology. An LLM is prompted to generate a sentencewhile adhering to a targeted linguistic constraint (e.g.use of verbs and subordinate propositions)": "etc. Nevertheless, to the best of our knowledge,a comprehensive evaluation of LLMs linguisticabilities, independent of specific tasks and possiblycross-cutting across them, is still missing. From adifferent perspective, studies on Controllable TextGeneration (CTG) indirectly tested these abilitiesby evaluating LLMs in the resolution of specificgenerative tasks, such as text simplification (Li andShardlow, 2024) or paraphrase generation (Sunet al., 2021), when conditioned for targeted linguis-tic constraints. Our hypothesis is that, while earlierinvestigations have demonstrated the implicit en-coding of many linguistic phenomena within therepresentations of smaller models (Jawahar et al.,2019; Tenney et al., 2019; Rogers et al., 2020; Et-tinger, 2020; Ramnath et al., 2020), there is noguarantee that generative LLMs can comply withsuch properties in generating texts.Building upon these premises, in this work wepresent the results of an extensive evaluation de-signed to test LLMs linguistic abilities to generatesentences while adhering to targeted linguistic con-straints representative of various morpho-syntacticand syntactic phenomena. For this purpose, we de-vised a methodology inspired by the linguistic pro-filing approach proposed by van Halteren (2004), in which large numbers of counts of linguistic fea-tures are used as a text profile. Thus, it allowscomputing the distribution of constrained linguisticproperties within generated sentences (see ). The approach was tested on the English lan-guage and against five LLMs of different sizes,both in zero- and few-shot scenarios. The approachwe devised aims to provide several insights into thelinguistic proficiency of LLMs, shedding light ontheir capabilities and limitations in producing textthat adheres to targeted linguistic constraints.",
  "Related Work": "The remarkable and unprecedented performanceof LLMs across diverse tasks has significantly in-creased awareness regarding the importance of eval-uating these models. This awareness has prompteda comprehensive reflection on the multifaceted na-ture of evaluation protocols (Chang et al., 2024).Additionally, the rapid emergence of open leader-boards has become pivotal for comparatively as-sessing the capabilities and limitations of variousmodels. A well-known example is the OpenLLMLeaderboard platform2, which provides officialrankings for evaluating the performance of mod-els, or the Italian LLM-Leaderboard (Bacciu et al.,2024), specifically developed to evaluate Italianmodels. Existing leaderboards report LLMs per-formance across a spectrum of text-understandingand generation tasks. However, to the best of ourknowledge, evaluation methodologies specificallydesigned to quantitatively assess the multilevel lin-guistic abilities of LLMs in text generation, inde-pendent of specific tasks, are lacking. Insights in this direction have emerged from thedefinition of prompting methodologies that havebeen used to assess the linguistic competence ofthe models in the resolution of diverse NLP tasks.Li et al. (2022) proposed for the first time a studydevoted to evaluating GPT-2 (Radford et al., 2019)in the resolution of 5 tasks, e.g. Part-Of-Speechtagging, showing that the tested properties are in-deed encoded in the pre-trained model. Follow-ing a similar approach, Blevins et al. (2023) pro-posed a structured prompting to evaluate the abil-ities of GPT-neo (Black et al., 2021) and GPT-3(Brown et al., 2020) models in the resolution ofsequence tagging tasks, e.g. Named Entity Recog-nition. Di Marco et al. (2023) tested multilingualBERT (Devlin et al., 2019) and XLM-RoBERTa(Conneau et al., 2020) in encoding morphologi-cal (e.g. the number of nouns) and syntactic (e.g.distinction between subject and object) properties.On a different side, studies focused on CTG as-sessed the linguistic capabilities of LLMs throughdirect examinations of their adherence to linguisticconstraints following an instructional tuning phase(Zhou et al., 2023) or indirectly through the anal-ysis of their performance in diverse tasks. An ex-ample of the latter is the study by Sun et al. (2023),which examined the controllability of LLMs across5 generation tasks. Specifically, they showed thatin the syntactically-controlled paraphrase genera-tion task, ChatGPT is good at mimicking syntacticstructures from sentences, but it struggles whenthe syntactic parses are directly incorporated intothe prompt. Alhafni et al. (2024), instead, investi-gated the abilities of LLMs in adhering to lexicaland morpho-syntactic constraints, for the task ofpersonalized text generation.",
  "Approach": "We systematically evaluate the ability of severalLLMs to generate sentences with targeted linguis-tic constraints corresponding to a set of morpho-syntactic and syntactic properties of a sentence,denoted as P = {p1, p2, ..., pn}. Specifically, weprompted the models to generate sentences contain-ing these properties within a fixed prompt structure.In particular, for each property pi, we asked themodels to generate a fixed number of sentenceshaving a precise value vpi, as drawn from a set ofpossible values V p = {vp1, vp2, ..., vpn}. For in-stance, a prompt asking the model to generate asentence with two verbs will have the following structure: Generate a sentence with 2 verbs3.Given the well-known difficulty of LLMs inproducing texts with precise numerical constraints(Sun et al., 2023), we decided to constrain the mod-els on increasing values of linguistic propertiesV pi, in order to evaluate their ability also to gen-erate sentences following incremental constraints.Our premise lies in the fact that while an LLM maystruggle to precisely generate a sentence with anexact value of a particular linguistic property, it islikely to be sensitive to incremental values, i.e. itcan generate a sentence characterized by either theabsence or the frequent occurrence of a linguisticproperty.We tested our approach across both zero- andfew-shot scenarios. Specifically, for the few-shotevaluation, we augmented each prompt with fiveexemplar sentences for each linguistic constraint.",
  "Linguistic Properties": "The set of linguistic properties P we used as con-straints during the generation process encompassesdiverse morpho-syntactic and syntactic phenom-ena of a sentence. They have been shown to playa highly predictive role when leveraged by tradi-tional learning models on various classificationproblems and can also be effectively used to profilethe knowledge encoded in the internal represen-tations of a pre-trained Transformer-based model(Miaschi et al., 2020; Sarti et al., 2021).Morpho-syntactic properties. They include theset of Part-Of-Speech (POS) defined by the Univer-sal Dependency project (de Marneffe et al., 2021)and can be grouped into two main subsets: con-tent (PROPN, NOUN, VERB, ADJ, ADV, PRON)and functional (NUM, CCONJ, AUX, ADP, DET,SCONJ, PUNCT).Syntactic properties. They include three sub-setsencompassing a range of linguistic phenomena,specifically chosen to test the diverse syntactic abil-ities of linguistically constrained LLMs. Namely,they are meant to profile the generated sentencesaccording to: i) the global and local syntactic treestructure, including the depth of the syntactic tree(hereafter referred to as max_depth) and length ofthe longest dependency link (max_link); ii) wordorder phenomena, circumscribed to the distribu-tion of the English canonical order of the two mainelements of the sentence, i.e. pre-verbal subjects(subj_pre) and post-verbal objects (obj_post); iii)",
  "Values Selection": "To ensure the selection of authentic values of lin-guistic properties, we relied on the largest En-glish Universal Dependency (UD) treebank, i.e.English Universal Dependency (EWT) (Silveiraet al., 2014), version 2.5 (Zeman et al., 2019). Toavoid dealing with excessively short or long sen-tences, possibly containing non-standard values,we filtered the treebank to retain only sentencescontaining a minimum of 5 and a maximum of40 tokens. The resulting dataset contains 19,282sentences.The values were extracted from EWT sentenceswith ProfilingUD tool (Brunato et al., 2020), whichallows the extraction of more than 130 propertiesrepresentative of the linguistic structure underlyinga sentence and derived from raw, morpho-syntacticand syntactic levels of annotation based on the UDformalism.In the few-shot configuration, we used 5 exem-plar sentences extracted from EWT. This was doneto guarantee the comparability across the experi-ments, since, for certain values of properties vpi,EWT contains no more than 5 sentences. Sets of values.As outlined in Sec. 3, we askedeach model to generate a fixed number of sen-tences following a set of increasing values V p ={vp1, vp2, ..., vpn} for each linguistic property. Inparticular, we selected five increasing values foreach linguistic property4. Specifically, we askedeach model to generate 50 sentences for every valuevpi within the set of five values Vp, thus obtaininga total of 250 sentences per property.",
  "Models": "We tested the abilities of LLMs of different sizes,ranging from 2 to 13 billion parameters. We optedto leverage the instruction-tuned variants of thesemodels since our aim was to assess the abilities ofLLMs specifically tailored to adhere more closelyto prompts with detailed instructions. In particular,we utilized Gemma in both 2B and 7B parametersvariants (Team et al., 2024), LLaMA-2 (7B and",
  "Evaluation": "The constrained generation abilities of the mod-els were evaluated according to two metrics. First,we computed the Success Rate (SR) for each ofthe five values vpi in the set of values Vp. Thiswas measured by the fraction of times the modelgenerated a sentence whose property value exactlycorresponds to the one provided. Moreover, formonitoring the ability of the model to adhere to theconstraints in terms of increasing property values inthe set Vp, we computed the Spearman correlationcoefficients () between the increasing propertyvalues extracted from EWT and those extractedfrom the sentences generated by the models. Thismetric offers an overview of how well the modelsare capable of following the constraints at a macro-level, whether increasing, decreasing, or removinga specific property when asked. Given the acknowl-edged complexity of evaluating LLMs, we optedfor two different evaluation metrics, since they of-fer two distinct perspectives on models behavior.To extract the set of linguistic properties P fromthe models outputs we utilized ProfilingUD.",
  "Results": "In the following sections, first we present the resultsobtained by the models in following the linguisticconstraints (Sec. 4.1 and 4.2). Subsequently, we an-alyze how increasing the linguistic property valuesshapes all the other sentence properties accordingly(Sec. 4.3). reports the results in terms ofSR and Spearman correlation () between the lin-guistic properties extracted from the EWT treebankand the generated sentences.",
  "How Precisely do LLMs FollowConstraints?": "As expected, and in line with our initial hypothesis,we generally obtained lower SR values comparedwith scores (see column Success Rate in ).By examining the average scores (Avg row), it is ev-ident that Mistral is the most accurate model, bothin a zero- and few-shot scenario. This suggeststhat it is the most proficient model in masteringthe (morpho)syntactic knowledge we considered.Conversely, Gemma2 exhibits on average the poor-",
  "For details about compute parameters and computationalbudget see Appendix B": "est accuracy. As expected, the larger model vari-ants (Gemma-7 and LLaMA-13) outperform thesmaller variants (Gemma-2 and LLaMA-7), with amore notable difference between the two Gemmamodels. However, it appears that the type of mod-els architecture also plays a main role. In fact,Mistral, despite having fewer parameters, outper-forms a model with almost double of its parameters(LLaMA-13). As expected, all models tend to be-come more accurate after the few-shot learning,particularly the two versions of Gemma, which be-come even more accurate than Mistral, which tendsto lose accuracy. This result is quite surprising,given that the examples provided in this experimen-tal scenario are the same for all models. However,Sun et al. (2023) found a similar trend when testingthe numerical planning abilities of LLMs, showingdeteriorated performances after a few-shot phase.Morpho-syntactic constraints. First, we ob-served that, on average, all models tend to adhereslightly more accurately to these constraintsrather than syntactic ones. Focusing on the differ-ences between content and functional POS, we didnot observe notable differences among the modelsin the zero-shot scenario. However, diverse trendsemerge in the few-shot scenario: the majority ofmodels (except the two versions of LLaMA) aremore accurate in generating sentences with anexact number of functional words. This is es-pecially the case of Gemma7, which becomes themost accurate model for the majority of POS. Onthe contrary, Mistrals ability to generate sentenceswith a precise number of functional words, particu-larly adpositions (ADP) and subordinate conjunc-tions (SCONJ), deteriorates.Syntactic constraints. For all models, the con-straints most challenging to adhere to in the zero-shot scenario are the depth of the syntactic treeof the sentence (max_depth) and the length of thelongest dependency link (max_link), both assumingthe knowledge of either global or local structure ofthe sentence. Quite interestingly, the SR for thesetwo properties remains quite low also after the few-shot learning, even with some differences amongthe models. Specifically, Gemma-2 becomes themost accurate model in generating sentences witha precise max_depth and Gemma-7 the most ac-curate one in generating sentences with a precisemax_link. On the contrary, in the few-shot scenario,Mistrals SR remains on average stable, but it dete-riorates significantly in generating sentences with acontrolled number of pre-verbal subjects (subj_pre)",
  "Avg38.8642.829.8929.3737.540.470.630.520.510.71": ": Success rate (%) and Spearman correlation coefficients between morpho-syntactic and syntactic propertiesextracted from the gold and the generated sentences. The best and worst scores for each property and each metricare highlighted inandrespectively. Non-statistically significant correlation scores are reported with ##. and subordinate clauses (subord_prop).Sets of constraint values. illustrates,for each model and each property, the SR scoresobtained in the generation of sentences with a valuevpi, reported on the x-axis. This analysis enables usto identify linguistic control elements that modelscan adhere to more accurately, thereby indicatingtheir proficiency in mastering specific property val-ues within the spectrum of English language pos-sibilities. Focusing on the 0-shot scenario, we cannotice that lower scores are generally associatedwith the last set, which corresponds to the highestvalue of each property. This suggests that modelsgenerally encounter more difficulty in generat-ing sentences with higher values (potentially lessfrequent in the English language) of a property.However, this trend does not hold for all features: e.g. subj_pre exhibits increasing SR scores as thevalue of the property increases from 2 to 4, andall models struggle to generate sentences withoutnouns. Conversely, scores tend to be higher whenthe value is 0, which corresponds to generating asentence lacking a given property. These two op-posite trends serve as evidence that, on average,models are capable of distinguishing when theyare asked to generate a sentence with or withouta given feature. Focusing instead on the few-shotscenario, we can observe a reverse trend: the SRof the last group of values, slightly increases, al-though remaining comparatively lower on averagethan the others. This suggests that the models arespecializing their linguistic abilities according tothe characteristics of the provided EWT samples.",
  "How Well Do LLMs Adhere to IncreasingConstraints?": "As discussed previously, although a model mayhave difficulty producing sentences with an exactvpi, it may be able to understand the differencebetween ranges of values. The Spearman columnin shows the models abilities (in termsof scores) to generate sentences characterizedby a distribution of linguistic properties that varyaccording to the relative increasing pvi. Althoughthere are general trends similar to those observedearlier, we note some significant differences. First,it is evident that the performance surpasses that ofthe SR evaluation. This confirms our initial hypoth-esis, indicating that while a model may strugglein adhering to a specific value, it shows greatersensitivity to the variations of values within theprovided prompts. Another notable difference isthat i) although Mistrals scores tend to be on av-erage lower in the few-shot scenarios compared tothe zero-shot ones, the model always remains thebest-performing one (Avg row of ) and ii)the difference between variants of the same model(i.e. Gemma and LLaMA) is more pronounced thanin the previous evaluation. Additionally, the resultsof this evaluation emphasize the models strongeradherence to morpho-syntactic rather than syntac-tic constraints. This suggests that the models are",
  "better able to master categorical knowledge, whichis simpler compared to the relational competencerequired to adhere to syntactic constraints": "Morpho-syntactic constraints. On average allmodels demonstrate higher correlation scores whenconstrained for content than for functional POS,both in zero- and few-shot scenarios. This suggeststhat for all models, it is easier to generate newsentences containing an increasing number ofopen word classes than closed ones. This resultindicates a further distinction compared to the pre-vious evaluation method. In addition, similar towhat was previously observed, Mistrals ability toadhere to POS constraints appears to diminish inthe few-shot setting. An in-depth analysis revealsthat, according to the present evaluation scenario,this is mostly due to a significant drop in its abilityto create sentences with an increasing number offunctional words. Syntactic constraints. Differently to what wasobserved in .1, in zero-shot not all modelsshow the lowest correlation scores for max_depthand max_link, among all syntactic constraints.Thus, even if all models consistently struggle toadhere to specific values of these two syntacticproperties correctly, each model has a differentsensitivity to adhere to their incremental values.However, similarly, after the few-shot learning, the",
  ": Counts of how many times the correlationwith the requested linguistic constraint or with sentencelength (n_tokens) is the highest w.r.t the correlationswith the other properties": "majority of the models (except Mistral) seem toacquire this capacity and in particular, Gemma-2,which became the most proficient model in generat-ing sentences controlled for the depth of the syntac-tic tree, even if we have obtained a non-significantcorrelation in the zero-shot scenario. As observedfor the morpho-syntactic constraints, Mistral in thefew-shot diminishes its syntactic abilities. This ismostly the case of the models capacity to generatesentences with an increasing max_link value andamount of subordinate clauses, specifically preced-ing the main clause (subord_pre).",
  "How Do Linguistic Constraints ShapeSentence Properties?": "In the previous sections, we analysed how wellLLMs can generate sentences following a linguisticconstraint. However, generating new sentences forincreasing values pvi of a given linguistic constraintmay lead to correlated changes in the distributionof other linguistic characteristics of the sentence.To explore this, for each controlled property wecomputed the correlation between the increasingproperty values in Vp (hereafter referred to as con-trolled values) and the values of all the linguisticproperties as generated by the LLMs (referred toas predicted values), including also the correlationwith the length of the sentence (n_tokens).As can be seen in , the correlation valuesare mostly positive, confirming that constrainingthe models to increase values of a specific linguis-tic property implies a corresponding increase inthe others. However, some sparse cases of neg-ative correlations can be observed, which signifi-cantly grow overall across models in the few-shotmatrices. This is particularly evident in the caseof Mistral when constrained to generate sentenceswith an increasing amount of numbers (NUM) andproper nouns (PROPN), and Gemma-7 constrainedfor determiners (DET). This suggests that the mod-els tend to specialize according to the provided samples, adjusting the (morpho)syntactic structureof the generated sentences accordingly.The diagonal scores of each matrix indicate thecorrelation between the increase of the controlledand predicted values of the same linguistic property.Consequently, the scores tend to be higher for mod-els with higher Spearman values, as shown in . However, upon examining how frequently the score between the same controlled/predicted con-straint is the highest among all other correlations,we note some notable results outlined in .As indicated in the Constraint column, among the20 constraints considered, such cases are relativelyinfrequent, and their frequency increases from thezero- to the few-shot scenario. This trend suggeststhat constraining generation for a specific lin-guistic element does not always primarily en-hance that element; rather, numerous other ele-ments are implicated. This is especially the caseof sentence length, a characteristic closely asso-ciated with many other (morpho)syntactic proper-ties of the sentence. This relationship is demon-strated by the n_tokens column, which indicatesthat in many cases, increasing the value of a lin-guistic property results in longer sentences. Thisholds particularly true for the zero-shot, while af-ter the few-shot phase, the models tend to masterthe ability to follow the linguistic constraints, sug-gesting that they are not simply creating longersentences, but rather sentences with a varied(morpho)syntactic structure. The only exceptionis represented by Mistral. For instance, when themodels are constrained to increase the frequency ofsubordinating conjunctions, there is a high correla-tion with the distribution of subordinative clauses,indicating the models proficiency in utilizing sub-ordination. Interestingly, this holds already in thezero-shot configuration. Differently, after the few-shot learning phase, as the controlled values of pro-nouns increase, the linguistic property of the gen-erated sentence that exhibits the most significantincrease is the distribution of pre-verbal subjects.This potentially indicates that the models (exclud-ing Gemma2) are generating personal pronouns toserve as subjects.Our investigation into the (morpho)syntactic pro-file of the generated sentences naturally leads usto compare the combinatorial properties specificto the English language with those specific to sen-tences produced by the LLMs. Therefore, we con-ducted a comparative analysis between the gener-ated sentences and those from EWT, which are re- ADJ ADV NOUN PRON",
  "SCONJ": "max_depth max_link obj_post subj_pre subord_post subord_pre subord_prop n_tokens Mistral 5-shot : Correlations matrices between controlled and predicted values. The correlation with the length ofthe generated sentences (n_tokens) is also reported. Gray cells () correspond to non-statistically significantcorrelations. garded as representative of English language norms.Our underlying assumption is that despite their in-herently different content, the closer the similaritiesin their (morpho)syntactic properties, the more re-liably we can consider the generated sentences asnaturalistic, i.e. as representative of English lan-guage constructs. Thus, we computed the distancebetween each matrix of and the matrix ofthe Spearman correlation scores among the valuesof the 20 considered linguistic properties in EWTsentences6. We can consider this second matrix asrepresentative of the authentic combinatorial prop-erties of English since it reports, for each propertyP, the correlation among their increasing valuesvpi with the same property and all the other sen-tence properties. The average distances among allproperties are reported in . Notably, theranking of the models by cosine distance mostlyresemble the trends previously observed. Specifi-cally, the distance is higher in the 0-shot, especiallyfor Gemma2, which exhibited the lowest correla-tion score as shown in . This suggests thatall models instructed with only 5 exemplars EWTsentences during the few-shot learning phase tendto generate more naturalistic sentences.",
  "Conclusion": "In this paper, we presented the results of an eval-uation methodology aimed at testing the linguis-tic abilities of LLMs of different sizes to generateEnglish sentences according to multiple linguisticconstraints. Inspired by the linguistic profiling ap-proach, it is not intended to be a task-oriented evalu-ation. Given the acknowledged multifaceted natureof evaluating LLMs (Chang et al., 2024), it includestwo evaluation metrics showing that while modelsmay struggle in adhering to a specific constraintvalue, they show greater sensitivity to the valuevariations. The differences between the scores ofthe two metrics seem to confirm that they offer twodistinct perspectives on models behaviour, sug-gesting their suitability for different tasks. Namely,for tasks requiring precise adherence to constraints,e.g. Text Simplification or Question Answering, the Success Rate evaluation is preferred. Conversely,for tasks that rely on the models ability to fol-low incremental constraints, e.g. Style Transfer, anevaluation based on Spearman correlation is moreappropriate. In addition, our results demonstratedthat linguistically-constrained models are capablenot only of generating longer sentences but alsosentences with varied (morpho)syntactic structuresthat align with the EWT sentences, which are con-sidered representative of the English language.",
  "Limitations": "In this section, we discuss the limitations of ourwork. 1) Linguistic properties: Despite coveringmultiple aspects of the (morpho)syntactic structureof a sentence, the set of linguistic properties chosenfor our experiments are only a possible subset ofcharacteristics that can be used as a testbed in LLMevaluation. Future work should aim to include abroader range of linguistic properties to provide amore comprehensive understanding of the multi-level linguistic competence encoded in LLMs. 2)Tested LLMs: Another limitation is the range ofLLMs we tested. Ideally, it would be beneficial toinclude fully open LLMs, to avoid potential biasesassociated with proprietary models. This wouldalso allow us to anchor the findings not only to thesize of the models (in terms of parameters) but alsoto their pre-training datasets (e.g. textual genres,linguistics characteristics), thereby offering a morerobust analysis. 3) Generated sentences: In ourexperiments, we focused on the linguistic structureof the generated sentences without assessing theiroverall quality. Nevertheless, the quality checkof generated sentences is beyond the scope of ourstudy, as it is worth noting that different workshave specifically addressed the grammaticality andfluency of LLMs generations (Zhang et al., 2023).However, upon closer inspection, we observed thatthe vast majority of the generated sentences werefluent, highlighting the remarkable capabilities ofthese models in producing coherent texts8. A fur-ther research direction could concern a more com-prehensive evaluation where the linguistic abilitiesof LLMs will be compared with their fluency andgrammaticality. 4) Multilinguality: Our resultsare limited to the English language, which con-strains the generalizability of our findings to otherlanguages. Since our approach is based on the",
  "A sample of the generated sentences by the tested LLMsis reported in Appendix D": "extraction of linguistic properties (and values) re-lying on the UD formalism, which is inherentlymultilingual, it would be relatively straightforwardto port this approach to other languages. There-fore, in future work, it would be beneficial to testthe approach on a diverse set of languages to eval-uate its generalizability and to explore potentialcross-linguistic differences in LLM performance.This would provide valuable insights into how wellLLMs handle linguistic diversity and the extent towhich models trained on languages other than En-glish exhibit different behaviours and capabilities.",
  "TEAMING-UP-TeamingupwithSocialArtificialAgentsproject under the PRIN grantno. 20177FX2A7 funded by theItalian Ministry of University andResearch": "Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Bashar Alhafni, Vivek Kulkarni, Dhruv Kumar, andVipul Raheja. 2024. Personalized text generationwith fine-grained linguistic control. In Proceedingsof the 1st Workshop on Personalization of GenerativeAI Systems (PERSONALIZE 2024), pages 88101,St. Julians, Malta. Association for ComputationalLinguistics. Andrea Bacciu, Cesare Campagnano, Giovanni Trap-polini, and Fabrizio Silvestri. 2024.DanteLLM:Lets push Italian LLM research forward! In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 43434355, Torino, Italia. ELRA and ICCL.",
  "Terra Blevins, Hila Gonen, and Luke Zettlemoyer. 2023": "Prompting language models for linguistic structure.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 66496663, Toronto, Canada.Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. In Advancesin Neural Information Processing Systems, pages18771901. DominiqueBrunato,AndreaCimino,FeliceDellOrletta,GiuliaVenturi,andSimonettaMontemagni. 2020.Profiling-UD: a tool forlinguistic profiling of texts.In Proceedings ofthe Twelfth Language Resources and EvaluationConference, pages 71457151, Marseille, France.European Language Resources Association. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. 2024. A sur-vey on evaluation of large language models. ACMTransactions on Intelligent Systems and Technology,15(3):145. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Pablo Contreras Kallens, Ross Deans Kristensen-McLachlan, and Morten H Christiansen. 2023. Largelanguage models demonstrate the potential of sta-tistical learning in language.Cognitive Science,47(3):e13256.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training of": "deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Marion Di Marco, Katharina Hmmerl, and Alexan-der Fraser. 2023. A study on accessing linguisticinformation in pre-trained language models by us-ing prompts. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 73287336, Singapore. Association forComputational Linguistics.",
  "Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, andJacob Steinhardt. 2021b. Measuring mathematicalproblem solving with the math dataset. NeurIPS": "Ganesh Jawahar, Benot Sagot, and Djam Seddah.2019. What does BERT learn about the structure oflanguage? In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 36513657, Florence, Italy. Association forComputational Linguistics. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Anna Rogers, Olga Kovaleva, and Anna Rumshisky.2020. A primer in BERTology: What we know abouthow BERT works. Transactions of the Associationfor Computational Linguistics, 8:842866": "Gabriele Sarti,Dominique Brunato,and FeliceDellOrletta. 2021. That looks hard: Characteriz-ing linguistic complexity in humans and languagemodels. In Proceedings of the Workshop on Cogni-tive Modeling and Computational Linguistics, pages4860, Online. Association for Computational Lin-guistics. Natalia Silveira, Timothy Dozat, Marie-Catherinede Marneffe, Samuel Bowman, Miriam Connor, JohnBauer, and Chris Manning. 2014. A gold standarddependency corpus for English. In Proceedings ofthe Ninth International Conference on LanguageResources and Evaluation (LREC14), pages 28972904, Reykjavik, Iceland. European Language Re-sources Association (ELRA). Jiao Sun, Xuezhe Ma, and Nanyun Peng. 2021. AESOP:Paraphrase generation with adaptive syntactic control.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages51765189, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, QianHu, Rahul Gupta, John Wieting, Nanyun Peng, andXuezhe Ma. 2023. Evaluating large language modelson controlled generation tasks. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 31553168, Singapore.Association for Computational Linguistics. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology.arXivpreprint arXiv:2403.08295.",
  "FeaturesPrompt": "POSGenerate a sentence with vpi<POS>max_depthGenerate a sentence with a treeheight of vpi in the syntactic treemax_linkGenerate a sentence with thelongest dependency link of vpiwordsobj_postGenerate a sentence with vpipost-verbal objectssubj_preGenerate a sentence with vpi pre-verbal subjectssubord_postGenerate a sentence with vpi sub-ordinate propositions followingthe main clausesubord_preGenerate a sentence with vpi sub-ordinate propositions precedingthe main clausesubord_propGenerate a sentence with vpi sub-ordinate propositions",
  "PUNCTSCONJ": "max_depth max_linkobj_postsubj_pre subord_post subord_pre subord_prop 0.2 0.0 0.2 0.4 0.6 0.8 1.0 : Correlation matrix of the EWT Treebank.Each row in the matrix shows the Spearman correlationscores between the increasing values of a linguistic prop-erty vpi across all sentences in the treebank (y-axis) withthe same property and all the other sentence properties(x-axis). Gray cells () correspond to non-statisticallysignificant correlations.",
  "CRelationship with EWT Treebank": "This appendix contains additional materials usefulfor deepening the understanding of the relation-ship between the (morpho)syntactic profiles of sen-tences generated by LLMs and those specific to theEnglish Treebank.Specifically, presents the correlationmatrix among the values of each linguistic propertyextracted from the selected sentences in the EWT.This matrix is intended to provide an overview ofthe combinatorial properties of the English lan-guage as represented by the EWT. As expected, thediagonal scores of the matrix are the highest, equalto 1. It follows from the fact that they reflect thecorrelation between the increasing value of a givenlinguistic property and itself. Note that these scoresare higher than the diagonal scores of the matricesthat show the correlations between the controlledand predicted values of the same linguistic property,as illustrated in . As discussed in .3, this indicates that constraining generation fora specific linguistic property does not primarily en-hance that property in the generated sentences, butrather affects multiple sentence properties. As a",
  ": Cosine distances between the correlation matrixof EWT and the predicted correlation matrices for eachmodel. The lowest and highest cosine distances arehighlighted inandrespectively": "consequence, the correlation between each prop-ertys increasing trend and the sentences length ishigher in the generated sentences () than inthe EWT sentences, as the length of the sentence ishighly related to the increase of any other linguisticstructures.In we report instead the cosine distancesbetween the correlation matrix of EWT and thepredicted correlation matrices () for eachmodel and each linguistic property. Unlike , which presents the average distances across allproperties, this table provides detailed distancesfor each individual property. As the distance be-tween the matrices varies significantly dependingon the linguistic characteristic considered. Thisindicates that the ability of LLMs to generate sen-tences with linguistic structures specific to the En-glish language depends on the particular linguisticphenomenon. Additionally, Mistral is not consis-",
  "PropertyC-valueG-valueLLMSentence": "ADJ11Gemma2The beautiful moon shone brightly that night.ADJ43Gemma2The vibrant colors painted upon the ancient wallswhispered secrets to the curious visitor.ADJ65Gemma2The vast and colorful array of wildflowers presenteda delightful landscape of exquisite beauty and rarewonder.ADJ00MistralJohn kicks the ball.ADJ22MistralBlue and elegant peacocks strut proudly on the lawn.ADJ66MistralThe magnificent, radiant, enchanting, vibrant,exquisite, breathtaking sunset left everyone in awe.NOUN12Gemma2There is only one thing in the world that can makeyou forget how to breathe.NOUN44Gemma2The weary travellers traversed the desolate plains,seeking respite from the scorching heat.NOUN77Gemma2A company that distributes solar panels across differ-ent cities earned the admiration of the communityafter their initiative to help the environment.NOUN00MistralIts raining.NOUN11MistralThe clock chimes.NOUN77MistralThe monkey grabbed the red umbrella, the yellowballoon, the wooden spoon, the green apple, theblue bottle, and the white tablecloth.subord_prop00Gemma2She made a wise and insightful decision.subord_prop22Gemma2Despite the storm, the lighthouse remained steadfast,projecting unwavering beams of hope that guidedsailors to safety.subord_prop43Gemma2The tiny spider, despite the large cage, managed toweave a beautiful web that was admired by all whosaw it.subord_prop11MistralAfter finishing dinner, well watch a movie.subord_prop12MistralAlthough it was raining, we decided to go for awalk. : Samples of sentences generated in the zero-shot configuration by two different LLMs, each constrainedfor a subset of linguistic properties: adjectives (ADJ), nouns (NOUN) and subordinate clauses (subord_prop). Thecontrolled value (C-value) of each property in the prompt and the actual value (G-value) of the property in thegenerated sentences are provided. Note that we reported samples where the models either correctly or incorrectlyfollow the constraint. Instances of the controlled property are highlighted in bold within the generated sentences."
}