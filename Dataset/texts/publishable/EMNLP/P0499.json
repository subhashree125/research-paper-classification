{
  "Abstract": "Scene Graph Generation (SGG) provides basiclanguage representation of visual scenes, re-quiring models to grasp complex and diversesemantics between objects. This complexityand diversity in SGG leads to underrepresen-tation, where parts of triplet labels are rare oreven unseen during training, resulting in impre-cise predictions. To tackle this, we propose inte-grating the pretrained Vision-language Modelsto enhance representation. However, due tothe gap between pretraining and SGG, directinference of pretrained VLMs on SGG leads tosevere bias, which stems from the imbalancedpredicates distribution in the pretraining lan-guage set. To alleviate the bias, we introducea novel LM Estimation to approximate theunattainable predicates distribution. Finally,we ensemble the debiased VLMs with SGGmodels to enhance the representation, wherewe design a certainty-aware indicator to scoreeach sample and dynamically adjust the en-semble weights. Our training-free method ef-fectively addresses the predicates bias in pre-trained VLMs, enhances SGGs representation,and significantly improve the performance.",
  "Introduction": "Scene Graph Generation (SGG) is a fundamen-tal vision-language task that has attracted mucheffort. It bridges natural languages with scene rep-resentations and serves various applications, fromrobotic contextual awareness to helping visuallyimpaired people. The key challenge in SGG is tograsp complex semantics to understand inter-objectrelationships in a scene.Existing researches in SGG focus primarily onrefining model architectures that are trained fromscratch with datasets like Visual Genome (Krishnaet al., 2017) or Open Images (Kuznetsova et al.,2020). However, SGG tasks inherently face an-other challenge of underrepresentation. Due to",
  "on": ": Illustration of the underrepresentation issuein Visual Genome. We highlight the relation class car-rying\" from the top-right imbalanced class distribution.We present various samples with their training repre-sentation levels and confidence scores for the groundtruth class, where lower scores indicate poorer predic-tion quality. We find that samples less represented bythe training set tend to have lower-quality predictions. the inherent complexities of SGG, there exists ex-ponential variability of triplets combined by thesubject, object, and relation (predicate). It is ex-tremely challenging for a training set to cover suchdiversity. As a result, a part of the test distributionis underrepresented in training, leading to poor pre-diction quality. In a severe case, some triplet labelsthat appear in the test set are unseen in training.In , we highlight the relation class car-rying from Visual Genome, showing samples andtheir confidence scores of the ground truth classfrom a baseline models predictions. While well-represented samples score higher, the samples la-beled with unseen triplets like woman carryingtowel\" score fairly low. Furthermore, one womancarrying umbrella\" scores only 0.15 due to the um-brella being closed, while its counterpart with anopen umbrella scores markedly higher (0.65). Al-though the triplet is seen in training set, the closedumbrella is still short of representation.A straightforward solution to this issue is to expand the models knowledge by integrating ad-vanced vision-language models (VLMs) pretrainedon extensive datasets (Kim et al., 2021; Li et al.,2020, 2019; Qi et al., 2020; Yu et al., 2022; Radfordet al., 2021), using their comprehensive knowledgeto compensate for underrepresented samples. Em-ploying the Masked Language Modeling (MLM)prompt format, such as woman is [MASK] towel,allows for direct extraction of relation predictionsfrom the fill-in answers provided by zero-shotVLMs, which fully preserve the pretraining knowl-edge. Nonetheless, this direct inference of zero-shot models on SGG introduces significant predi-cate bias due to disparities in data distribution andobjectives between pretraining and SGG tasks.This predicate bias originates from the imbal-anced frequency of predicates in the pretraining lan-guage set, causing the VLMs to favor the predicatesthat are prevalent in the pretraining data. Unfortu-nately, existing debiasing methods rely on explicittraining distribution, which is often unattainablefor pretrained VLMs: (1) The pretraining data areoften confidential. (2) Since the pretraining objec-tives are different with SGG, there is no direct labelcorrespondence from pretraining to SGG.To alleviate the predicate bias, we introduce anovel approach named Lagrange-Multiplier Es-timation (LM Estimation) based on constrainedoptimization. Since there is no explicit distributionof relation labels in the pretraining data, LM Esti-mation seeks to estimate a surrogate distribution ofSGG predicates within VLMs. Upon obtaining theestimated distribution, we proceed with predicatesdebiasing via post-hoc logits adjustment. Our LMEstimation, as demonstrated by comprehensive ex-periments, is proved to be exceedingly effective inmitigating the bias for zero-shot VLMs.Finally, we ensemble the debiased VLMs withthe SGG models to address their underrepresenta-tion issue. We observe that some samples are betterrepresented by the zero-shot VLM, while othersalign better with the SGG model. Therefore, wepropose to dynamically ensemble the two models.For each sample, we employ a certainty-awareindicator to score its representation level in thepretrained VLM and the SGG model, which sub-sequently determines the ensemble weights. Ourcontributions can be summarized as follows: While existing methods primarily focuses onrefining model architecture, we are among thepioneers in addressing the inherent underrepre-sentation issue in SGG using pretrained VLMs.",
  "Related Work": "Scene Graph Generation (SGG) is a fundamentaltask for understanding the relationships betweenobjects in images. Various of innovations (Tanget al., 2019; Gu et al., 2019; Li et al., 2021; Linet al., 2022a, 2020, 2022b; Zheng et al., 2023; Xuet al., 2017) have been made in supervised SGGfrom the Visual Genome benchmark (Krishna et al.,2017). A typical approach involves using a FasterR-CNN (Sun et al., 2018) to identify image regionsas objects, followed by predicting their interrela-tions with a specialized network that considers theirattributes and spatial context. Existing efforts (Liet al., 2021; Lin et al., 2022a,b; Zheng et al., 2023)mainly focus on enhancing this prediction network.For instance, (Lin et al., 2022b) introduced a regu-larized unrolling approach, and (Zheng et al., 2023)used a prototypical network for improved represen-tation. These models specially tailored for SGGhas achieved a superior performance.Unbiased Learning in SGG has been a long-standing challenge. Started by (Tang et al., 2020),the debiasing methods (Dong et al., 2022; Li et al.,2021; Yan et al., 2020; Li et al., 2022b,a) seek toremoving the relation label bias stemming from theimbalanced relation class distribution. These workshave achieved more balanced performance acrossall relation classes. However, these methods relyon the interfere during training and are not feasibleto the predicate bias in pre-trained VLMs.Pre-trained Vision-Language models (VLMs)have been widely applied in diverse vision-language tasks (Su et al., 2019; Radford et al., 2021;Kim et al., 2021; Li et al., 2020) and have achievedsubstantial performance improvements with thevast knowledge base obtained during pre-training.Recently works start to adapt the comprehensivepre-trained knowledge in VLMs to relation recog-nition and scene graph generation (He et al., 2022;Gao et al., 2023; Li et al., 2023; Yu et al., 2023;",
  "+ ()* 9!\"(|,, -, ,,-)": ": Illustration of our proposed architecture. left: the visual-language inputs processed from image regionsxi,j and object labels (zi, zj), either provided or predicted by Faster R-CNN detector. middle: the fixed zero-shotVLM fzs and the trainable task-specific models fsg, which we use a fine-tuned VLM as example. right: the relationlabel debias process and the certainty-aware ensemble. Zhang et al., 2023; Zhao et al., 2023). Throughprompt-tuning, (He et al., 2022) is the first employ-ing VLMs to open-vocabulary scene graph genera-tion. Then more approaches (Zhang et al., 2023; Yuet al., 2023; Gao et al., 2023) are designed towardsthis task. These works demonstrate the capabilityof VLMs on recognizing relation, inspiring us toutilize VLMs to improve the SGG representation.",
  "Setup": "Given an image data (x, G) from a SGG datasetDsg, the image x is parsed into a scene graphG = {V, E}, where V is the object set and E is therelation set. Specifically, each object v V con-sists of a corresponding bounding box b and a cat-egorical label z either from annotation or predictedby a trained Faster R-CNN detector; each ei,j Edenotes the relation for the subject-object pair viand vj, represented by a predicate label y Ce.The predicate relation space Ce = {0}Cr includesone background class 0, indicating no relation, andK non-background relations Cr = [K]. The objec-tive is to learn a model f that, given the predictedobjects zi and zj for each pair with their croppedimage region xi,j = x(bi bj), produces logits ofor all relations y Ce, i.e., o = f(zi, zj, xi,j).",
  "As depicted in , our framework f compris-ing two branches: a fixed zero-shot VLM fzs and atask-specific SGG model fsg trained on Dsg. Here,we employ a SGG fine-tuned VLM as fsg, where": "we forward the image region xi,j to the visual en-coder and use the prompt template what is therelationship between the {zi} and the {zj}? as thetext input. Then, a classifier head is added to the[CLS] token to generate logits osg of all relationsy Ce. Our experiments also adopt SGG modelsfrom recent works as fsg.Another zero-shot model, represented as fzs,leverages pretrained knowledge to the SGG taskwithout fine-tuning. By providing prompts to zero-shot VLMs in the form {zi} is [MASK] {zj}, onecan derive the predicted logits okzs of K relationcategories from the fill-in answers. In SGG, thebackground class is defined when a relation is out-side Cr = [K]. Predicting the background relationis challenging for fzs: In pretraining phase, themodel has not been exposed to the specific defini-tion of background. Therefore, we rely solely onfsg to produce the logits of background class:",
  "[o0sg, oksg] = fsg(zi, zj, xi,j) RK+1,(1)": "The two branches prediction reflect the label dis-tribution of their training sets, leading to potentialpredicates bias in output logits if the target distribu-tion differs. To address this, we conduct predicatedebiasing using our Lagrange-Multiplier Estima-tion (LM Estimation) method along with logitsadjustment, generating the debiased logits okzs andoksg. The details are demonstrated in .3.To mitigate the underrepresentation issue, weensemble the debiased two branch to yield the finalimproved prediction, where we employ a certainty-",
  "Ptr(r|zi, zj, xi,j) = softmax(ok)(r), r Cr (2)": "In our task, the training set Dtr can be either theSGG dataset Dsg or the pretraining dataset Dpt, onwhich the SGG model fsg and the zero-shot modelfzs are respectively trained.In the evaluation phase, our goal is to estimatethe target test probability Pta rather than Ptr. ByBayes Rule, we have the following:",
  "Pta(r)(4)": "In a case where training distribution Ptr(r) notequals to the target distribution Pta(r), known aslabel shift, the misalignment results in the modelspredicted probability Ptr(r|zi, zj, xi,j) not equalsto the actual test probability, Pta(r|zi, zj, xi,j).In our framework in , fzs is trained onDpt and fsg on Dsg, whose training label distribu-tions Ptr(r) are pt RK and sg RK, respec-tively. The prevalent evaluation metric, Recall, isdesigned to assess performance when the test labeldistribution Pta(r) is the same as the training dis-tribution sg. In contrast, the mean recall metricseeks to evaluate performance in a uniform testdistribution where Pta(r) = 1/K. The Ptr(r) andPta(r) in each case can be summarized as follow:",
  "a uniform distribution evaluated by mean Recall.In this scenario, the target distribution Pta(r) =1/K diverges from the imbalanced distributionsg in Dsg shown in top right of": "For the zero-shot VLM fzs with Ptr(r) = pt,the Pta(r) = Ptr(r) holds in both training anduniform targets. Firstly, the label distributionpt in the pretraining set Dpt differs from sg,resulting in Ptr(r) = sg under the training-aligned target. Secondly, the imbalanced pred-icates distribution in Dpt also leads to Ptr(r) =1/K under the uniform target distribution. Post-hoc Logits Adjustments.The first case,where Ptr(r) = sg but Pta(r) = 1/K, is a long-existing issue with many effective approaches pro-posed in SGG. However, existing methods are notfeasible in the second case for their debiasing inthe training stage, while the pretraining stage offzs are not accessible. A feasible debiasing methodfor already-trained models is the post-hoc logit ad-justment (Menon et al., 2020). Denoting the initialprediction logits as ok and the debiased logits asok, one can recast Equation 4 into a logits form:",
  "ok(r) = ok(r) log Ptr(r) + log Pta(r)(6)": "It suggests that given the target label distribution,the unbiased logits ok(r) can be obtained through apost-hoc adjustment on the initial prediction logitsok(r), following the terms value in Equation 5.While sg can be obtained simply by counting thelabel frequencies in Dsg, pt is the predicates distri-bution hidden in the pretraining stage.Lagrange Multiplier Estimation.To estimatept, we proposed a novel method based on con-strained optimization. Our initial step involvescollecting all samples that have non-backgroundrelation labels r Cr from the training or vali-dation set of Dsg. Leveraging the collected data,our optimization objective is to solve the optimalpt that minimizes the cross-entropy loss betweenthe adjusted logits okzs (following Equation 5 and 6 using pt) and the ground truth relation labels r.Since the data are collected from Dsg, we des-ignate the term Pta(r) to sg to offset the interfer-ence of its label distribution and ensure the solvedPtr(r) = pt. This approach allows us to estimatept by solving a constrained optimization problem,where we set the constraints to ensure the solved",
  "rpt(r))(8)": "After obtaining pt and sg, we can then applythe post-hoc logits adjustments for predicates debi-asing following Equation 5 and 6, which producestwo sets of unbiased logits from the initial predic-tion of fzs and fsg, denoted as okzs and oksg.Upon mitigating the predicates bias inside fzs,we can leverage the model to address the underrep-resentation issue in fsg. From the debiased logitsokzs and oksg, we compute the probabilities towardsr Cr, where we adopt a -calibration outlined in(Kumar et al., 2022) to avoid over-confidence:",
  "Oscar": ": The relation label distributions on VisualGenome. The upper figure illustrates the distributionacross all classes, while the lower one shows the prob-ability distribution on some typical categories. TrainSet: The class distribution sg in training set. ViLT andOscar: The estimated distribution pt using LM Estima-tion in the two pre-training stages. logits predicted by fsg without debiasing (Equa-tion 1), the background and non-background prob-ability can be calculated by softmax function:Psg(y = 0|zi, zj, xi,j) = 1 softmax(osg)0Psg(y = 0|zi, zj, xi,j) = softmax(osg)0(12)Finally, the ensembled prediction on Ce is:",
  "Summary": "We integrate VLMs to mitigate the underrepre-sentation challenge inherent to SGG, where wepropose the novel LM Estimation to approximatethe unattainable pretraining distribution of predi-cates, pt, and conduct predicate debiasing for eachmodel. Unlike previous SGG methods that are op-timized for one target distribution per training, ourmethod enables seamlessly adaptation between dif-ferent targets without cost, outperforming existingSGG approaches under each target distribution.",
  "PENET-Rwt + Ours31.8(+0.4)39.9(+1.1)42.3(+1.6)19.2(+0.3)23.0(+0.8)24.5(+1.0)": ": The mean Recall results on Visual Genome comparing with state-of-the-art models and debiasing methods.The results and performance gain applying our method is below the row of corresponding baseline. ft: The model isfine-tuned on Visual Genome. la: The prediction logits is debiased by logits adjustment with sg. : Due to theabsence of part of the results, we re-implement by ourselves. our significant performance improvement througha comparative analysis with previous methods. Sec-tion 4.3 provides an illustrative analysis of the pred-icates distribution estimated by our LM Estimation.Subsequently, .4 offers an ablation study,analysing the contribution of individual compo-nents in our design to the overall performance.",
  "Experiment Settings": "Datasets. The Visual Genome (VG) dataset con-sists of 108,077 images with average annotationsof 38 objects and 22 relationships per image. ForVisual Genome, we adopted a split with 108,077images focusing on the most common 150 objectand 50 predicate categories, allocating 70% fortraining and 30% for testing, alongside a validationset of 5,000 images extracted from the training set.Evaluation Protocol.For the Visual Genomedataset, we focus on two key sub-tasks: PredicateClassification (PredCls) and Scene Graph Classifi-cation (SGCls). We skip the Scene Graph Detection(SGDet) here and provide a discussion in supple-mentary, considering its substantial computationaldemands when employing VLMs and limited rele-vance to our methods core objectives. Our primaryevaluation metrics are Recall@K and mean Re-call@K (mRecall@K). Additionally, we proposeanother task of relation classification that calculatesthe top-1 predicate accuracy (Acc) for samples la-beled with non-background relations, where wefocus on the ability of model on predicting the re-lation given a pair of objects in the scene.Baselines and Implementation. Here we utilize two prominent zero-shot vision-language models,ViLT (Kim et al., 2021) and Oscar (Li et al., 2020),as fzs. For the task-specific branch fsg, we em-ploy three baseline models trained in SGG: (1) Toexplore the fine-tuning performance of VLMs onSGG, we fine-tune ViLT and Oscar using the Pred-Cls training data and establish them as our first twobaselines. (2) To show our methods compatibilitywith existing SGG models, we undertake PENET(Zheng et al., 2023), a cutting-edge method withsuperior performance, as our third baseline. Inour ensemble strategy, we explore three combina-tions: \"fine-tuned ViLT + zero-shot ViLT\", \"fine-tuned Oscar + zero-shot Oscar\", and \"PENET +zero-shot ViLT\", where each model is debiased byour methods. Following previous settings, an in-dependently trained Faster R-CNN is attached tothe front of each VLM model for object recogni-tion. During pre-training, both ViLT and Oscaremploy two main paradigms: Masked LanguageModeling (MLM) and Visual Question Answering(VQA). In MLM, tokens in a sentence can be re-placed by [MASK], with the model predicting theoriginal token using visual and language prompts.In VQA, the model, given a question and visual in-put, predicts an answer via an MLP classifier usingthe [CLS] token. For our task, we use MLM forthe fixed branch fzs with the prompt zi is [MASK]zj. and VQA for fine-tuning fsg, where we intro-duce a MLP with the query \"[CLS] what is therelationship between the zi and the zj?\", wherethe embedding of [CLS] token is forwarded to the",
  "PENET + Ours62.0(+0.3)69.0(+0.8)71.1(+1.0)38.1(+0.2)41.8(+0.5)42.9(+0.6)": ": The Recall results on Visual Genome dataset comparing with state-of-the-art models and debiasing methods.The results and performance gain applying our method is below the row of corresponding baseline. ft: The model isfine-tuned on Visual Genome. : Due to the absence of part of the results, we re-implemented by ourselves.",
  "Efficacy Analysis": "To assess the efficacy of our method, in this section,we compare our method with recent studies througha detailed result analysis on Visual Genome. TheRecall and mean Recall results are presented in Ta-ble 2, which showcases a performance comparisonwith a variety of cutting-edge models and debiasingmethods. We ensure to compare against previousmethods under their best-performance metric. Forbaseline models without debiasing strategies, wecompare with their superior Recall metrics and ex-clude their lower mean Recall performances. Simi-larly, for the debiased SGG models, we only focuson their mean Recall outcomes.Baseline Performance.Our analysis beginswith the three fsg baselines: fine-tuned ViLT, fine-tuned Oscar, and PENET. Specifically, for scenar-ios where the desired target is a uniform distribu-tion assessed by mean Recall, we apply the post-hoc logits adjustment to the two fine-tuned base-lines following Equations 5 and 6. For PENET,we implement a reweighting loss strategy (PENET-Rwt) following (Zheng et al., 2023) to train a debi-ased version tailored for the uniform target distri-bution, which achieved optimal performance.Our main experiment results are presented in and . As shown in , withouttask-specific designs, the two fine-tuned VLMs fallbehind the SGG models on Recall and scored 67.6and 68.4 on R@100, while PENET takes the lead. However, as shown in , when evaluatedunder the uniform target distribution and adjustedusing simple post-hoc logits adjustment, the fine-tuned VLMs surpass all the cutting-edge debiasedSGG models in mean Recall, achieving 41.3 and44.5 of mR@100. Our Improvements.Subsequently, we employour certainty-aware ensemble to integrate debiasedzero-shot VLMs fzs into the fsg baselines, whereeach fzs is debiased by our LM Estimation. In, for each fsg baseline, we observed a no-table performance boost after applying our meth-ods (+1.4 / + 2.0 / + 1.6 in mR@100 and +1.7/ +1.4 / + 1.0 in R@100). In both mRecall andRecall, our methods achieve the best performance(46.5 on mR@100 and 71.1 on R@100), whilethe improvement on mean Recall is particularlystriking and surpasses the gains observed on Re-call (+1.4/+2.0/+1.6 vs. +1.7/+1.4/+1.0). The re-sults show that our methods achieve a significantimprovement in each baseline, achieving the bestperformance compared to all existing methods. Our results indicate the effectiveness of our meth-ods, leading to a marked boost in performance.Moreover, the improvement in PENET baselinesshows the adaptability of our method to existingSGG-specialized models. In addition, we observethat our representation improvements leads to amore significant gain in mean recall than in re-call, suggesting the underrepresentation problem ismore common in tail relation classes.",
  "Ens. Gain+0.03+3.29+0.61+1.87+0.98+5.71+2.96+4.07": ": Top-1 accuracy and class-wise mean accuracy of relation classification on Visual Genome. All: The testresults for all triplets with non-background relation labels. Unseen: The test results for triplets that are absentfrom the training set. Initial: The initial zero-shot VLMs without debiasing. Debiased: The zero-shot VLMs afterdebiasing using our LM Estimation. ens: Ensemble of the fine-tuned VLMs and Initial or Debiased zero-shotmodel. Ens. Gain: the performance gain of ensemble compared to the fine-tuned model.",
  "Estimated Distribution Analysis": "In , we depict the predicate distributionsof zero-shot ViLT and Oscar solved by LM Estima-tion, comparing them with the distribution in VGtraining set. The upper chart in depictsthe distributions across all relations, where we findthat all three distributions exhibit a significant im-balance. Furthermore, we extract the distributionof typical relations in the lower chart, where wesee a substantial discrepancy among the three dis-tributions. This variation affirms the two scenariosof Pta(r) = Ptr(r) discussed in .3, pre-cluding the direct application of zero-shot VLMswithout debiasing, indicating the necessity of ourLM Estimation and subsequent debiasing method.",
  "Ablation Study": "In this section, we conduct an ablation study onVisual Genome dataset. Initially, we assess theeffectiveness of our LM Estimation in addressingthe predicates bias of zero-shot VLMs. Further-more, we evaluate the capability of our method toenhance representation by focusing on the unseentriplets, which are entirely absent during training.To precisely evaluate the performance in rela-tion recognition and eliminate any influence fromthe background class, we require the model to per-form relation classification exclusively on sampleslabeled with non-background relations.Subse-quently, we calculate the top-1 accuracy (Acc) andclass-wise mean accuracy (mAcc) as new metricsto accurately gauge the models effectiveness in thiscontext. Our findings are comprehensively detailedin , which details on two sample splits: oneencompassing all triplets and the other exclusivelyfocusing on unseen triplets. For each splits, we ex-amine the performance of the two fine-tuned VLMs, fsg, their initial and debiased zero-shot models, fzs,and the ensemble of corresponding models.Predicate Debiasing. In .3, we introduceour LM Estimation method for predicate debias-ing. Here, we further evaluate the efficacy of ourdebiasing. We initially analysis on the relation clas-sification accuracy of the zero-shot VLMs beforeand after debiasing. As presented in (theViLT-zs and Oscar-zs rows), without debiasing, theaccuracies of initial predictions are lower eitherin all triplets or unseen triplets. However, afterdebiasing through LM Estimation, there is a no-table enhancement in the zero-shot performance.For unseen triplets, the debiased zero-shot VLMseven surpass the performance of their fine-tunedcounterparts, suggesting our method effectively ad-dresses the predicate bias and smoothly adapts thepretraining knowledge to the SGG task.Furthermore, from the ensemble performancein (the ViLT-ens and Oscar-ens rows), wenotice that ensembling the initial fzs hardly im-proves the performance, only achieving a slightgain of +0.33/+0.03 on all triplets and +0.68/+2.29on unseen triplets. In contrast, ensembling the debi-ased fzs achieves a significantly more pronouncedimprovement, achieving +2.17/+1.83 gain on alltriplets and +5.09/+4.01 on unseen triplets.To keep consistent with previous settings, wepresent the Recall and mean Recall ablation resultsin . We observe a substantial improvementin both mean Recall and Recall when ensemblingwith our debiased zero-shot VLMs (the highlightedrow in each group), while directly ensembling theinitial zero-shot VLMs even harm to the perfor-mance (the middle row in each group). These re-sults starkly underlines the necessity and efficacyof our LM Estimation in predicate debiasing.",
  "Oscar-ens (Debiased)60.5(+1.4)67.4(+1.7)69.3(+1.7)": ": The mean Recall and Recall ablation resultson Visual Genome. Initial: The initial zero-shot VLMswithout debiasing. Debiased: The zero-shot VLMs afterpredicates debiasing. ens: Ensemble of the fine-tunedVLMs and Initial or Debiased zero-shot model. Representation Enhancement. To validate theenhancement of representation, we specifically ex-amine the samples labeled with unseen triplets.These triplets are present in the test set but ab-sent from the training set, which is the worst taildistribution in the underrepresentation issue. reveals that, across all triplets, the accura-cies of both zero-shot VLMs (fzs) fall short of theirfine-tuned counterparts (fsg). For example, the de-biased zero-shot Oscar model achieves 33.96/57.31of mAcc/Acc, which are lower than the fine-tunedOscar (41.99/67.16). However, within the subset ofunseen triplets, the debiased zero-shot fzs outper-forms the fine-tuned fsg: The debiased zero-shotOscar achieves 16.01/20.05 of mAcc/Acc, outper-forming the fine-tuned model (13.85/18.01).These findings substantiate our hypothesis thatzero-shot models, with their pretraining knowledgefully preserved, are better at handling underrepre-sented samples compared to SGG-specific models.This advantage is particularly evident in the con-text of unseen triplets, where comprehensive pre-training knowledge of zero-shot models confers asignificant performance benefit.Moreover, we find that the gain of ensemble issignificantly higher for unseen triplets (DebiasedViLT: +5.09/+4.01, Debiased Oscar: +5.71/4.07)than for all triplets (Debiased ViLT: +2.17/+1.83,Debiased Oscar: +3.29/1.87). This indicates thatthe underrepresented samples are improved muchmore than the well-represented samples, receivinghigher gains than average. Considering the pro-portion of unseen triplets in all triplets, we inferthe overall performance gain mainly comes from",
  "Conclusion": "In conclusion, our study has made significantstrides in efficiently and effectively integrate pre-trained VLMs to SGG. By introducing the novelLM Estimation, we effectively mitigate the predi-cate bias inside pre-trained VLMs, allowing theircomprehensive knowledge to be employed in SGG.Besides, our certainty-aware ensemble strategy,which ensembles the zero-shot VLMs with SGGmodel, effectively addresses the underrepresenta-tion issue and demonstrates a significant improve-ment in SGG performance. Our work contributes tothe field of SGG, suggesting potential pathways forreducing language bias of pretraining and leveragethem in more complex language tasks.",
  "Limitation": "Though our methods does not require any train-ing, comparing with original fsg, our ensembleframework still adds computational cost from fzssinference. This inference can be costly in an ex-treme case that one scene has too many objects topredict their relations. Besides, even after we solvethe word bias inside VLMs, the final ensemble per-formance relies highly on the pre-training quality,which requires the fzs to be pre-trained on compre-hensive data to improve SGGs representation. An-other limitation arises from the forwarding patternin VLM, where we adopt a pair-wise forwardingthat taking a pair of objects along with their imageregion and text prompt. In this way, each possibleobject pair requires an entire forwarding of VLM.This process is rapid when the object is certainlydetected. However, in the scenario of Scene GraphDetection, the large amounts of proposals can bringunavoidable time cost to our pipeline. We providea more detailed discussion in appendix. Tianshui Chen, Weihao Yu, Riquan Chen, and LiangLin. 2019. Knowledge-embedded routing networkfor scene graph generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 61636171.",
  "Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao,and Qianru Sun. 2023. Compositional prompt tuningwith motion cues for open-vocabulary video relationdetection. arXiv preprint arXiv:2302.00268": "Jiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, JianfeiCai, and Mingyang Ling. 2019. Scene graph genera-tion with external knowledge and image reconstruc-tion. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages19691978. Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li.2022. Towards open-vocabulary scene graph genera-tion with prompt-based finetuning. In European Con-ference on Computer Vision, pages 5673. Springer.",
  "Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:Vision-and-language transformer without convolu-tion or region supervision. In International Con-ference on Machine Learning, pages 55835594.PMLR": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual genome: Connecting language and vi-sion using crowdsourced dense image annotations.International journal of computer vision, 123:3273. Ananya Kumar, Tengyu Ma, Percy Liang, and AditiRaghunathan. 2022. Calibrated ensembles can mit-igate accuracy tradeoffs under distribution shift. InUncertainty in Artificial Intelligence, pages 10411051. PMLR. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,Stefan Popov, Matteo Malloci, Alexander Kolesnikov,et al. 2020. The open images dataset v4: Unifiedimage classification, object detection, and visual re-lationship detection at scale. International Journalof Computer Vision, 128(7):19561981. Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang,Songyang Zhang, and Jun Xiao. 2022a. The devil isin the labels: Noisy label correction for robust scenegraph generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1886918878. Lin Li, Jun Xiao, Guikun Chen, Jian Shao, YuetingZhuang, and Long Chen. 2023. Zero-shot visual re-lation detection via composite visual cues from largelanguage models. arXiv preprint arXiv:2305.12476.",
  "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-JuiHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-ple and performant baseline for vision and language.arXiv preprint arXiv:1908.03557": "Rongjie Li, Songyang Zhang, Bo Wan, and Xuming He.2021. Bipartite graph network with adaptive messagepassing for unbiased scene graph generation. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1110911119. Wei Li, Haiwei Zhang, Qijie Bai, Guoqing Zhao, NingJiang, and Xiaojie Yuan. 2022b. Ppdl: Predicateprobability distribution based loss for unbiased scenegraph generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1944719456. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,Xiaowei Hu, Lei Zhang, Lijuan Wang, HoudongHu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-languagetasks. In Computer VisionECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XXX 16, pages 121137. Springer. Xin Lin, Changxing Ding, Jinquan Zeng, and DachengTao. 2020. Gps-net: Graph property sensing net-work for scene graph generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 37463753. Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, andDacheng Tao. 2022a. Hl-net: Heterophily learningnetwork for scene graph generation. In proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1947619485. Xin Lin, Changxing Ding, Jing Zhang, Yibing Zhan, andDacheng Tao. 2022b. Ru-net: Regularized unrollingnetwork for scene graph generation. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1945719466.",
  "Xudong Sun, Pengcheng Wu, and Steven CH Hoi. 2018.Face detection using deep learning: An improvedfaster rcnn approach. Neurocomputing, 299:4250": "Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi,and Hanwang Zhang. 2020. Unbiased scene graphgeneration from biased training. In Proceedings ofthe IEEE/CVF conference on computer vision andpattern recognition, pages 37163725. Kaihua Tang, Hanwang Zhang, Baoyuan Wu, WenhanLuo, and Wei Liu. 2019. Learning to compose dy-namic tree structures for visual contexts. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 66196628. Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017. Scene graph generation by iterative mes-sage passing. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages54105419. Shaotian Yan, Chen Shen, Zhongming Jin, JianqiangHuang, Rongxin Jiang, Yaowu Chen, and Xian-Sheng Hua. 2020. Pcpl: Predicate-correlation per-ception learning for unbiased scene graph generation.In Proceedings of the 28th ACM International Con-ference on Multimedia, pages 265273. Gengcong Yang, Jingyi Zhang, Yong Zhang, BaoyuanWu, and Yujiu Yang. 2021. Probabilistic modelingof semantic ambiguity for scene graph generation. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1252712536.",
  "Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, andYueting Zhuang. 2023. Visually-prompted languagemodel for fine-grained scene graph generation in anopen world. arXiv preprint arXiv:2303.13233": "Rowan Zellers, Mark Yatskar, Sam Thomson, and YejinChoi. 2018. Neural motifs: Scene graph parsingwith global context. In Proceedings of the IEEE con-ference on computer vision and pattern recognition,pages 58315840. Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, andTat-Seng Chua. 2017. Visual translation embeddingnetwork for visual relation detection. In Proceed-ings of the IEEE conference on computer vision andpattern recognition, pages 55325540.",
  "graph using pre-trained visual-semantic space. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 29152924": "Long Zhao, Liangzhe Yuan, Boqing Gong, Yin Cui, Flo-rian Schroff, Ming-Hsuan Yang, Hartwig Adam, andTing Liu. 2023. Unified visual relationship detec-tion with vision and language models. arXiv preprintarXiv:2303.08998. Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, andJingkuan Song. 2023. Prototype-based embeddingnetwork for scene graph generation. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2278322792. Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, andYin Li. 2021. Learning to generate scene graph fromnatural language supervision. In Proceedings of theIEEE/CVF International Conference on ComputerVision, pages 18231834.",
  "AMore Theoretical Justifications": "In the main paper, we introduce the post-hoc logitsadjustment methods (Menon et al., 2020) for labeldebiasing, which is first proposed in long-tail clas-sification. In the main paper, we skipped part ofthe derivation due to the limit of length. Here, weprovide a detailed derivation for easier understand-ing.Taking (zi, zj, xi,j) as input for a subject-objectpair, the conditional probability for the relations isP(r|zi, zj, xi,j). From the Bayes Rule, the condi-tional probability can be expressed as:",
  "Pta(zi, zj, xi,j)(16)": "Then let us look into each term. Firstly, theP(zi, zj, xi,j) is irrelavant with r and thus has noeffect on the relation label bias. Therefore, the nu-merator term can be replaced by a constant C andomitted in further computation. Secondly, when fo-cusing on the label bias, according to the prevalentlabel-shift hypothesis proposed in long-tail classi-fication, one can assume P(zi, zj, xi,j|r) to be the",
  "B.1Scene Graph Detection": "In our main paper, we skipped the SgDet sub-task,considering its substantial computational demandswhen employing VLMs and limited relevance toour methods core objectives. In this section, weprovides a discussion and a brief correspondingexperiments results.Existing SGG models usually employs a FasterR-CNN (Sun et al., 2018) detector and fix the num-ber of generated proposals to be 80 per image for afair comparison. However, unlike the existing rela-tion recognition networks that processes all pairs of proposals in an image simutaniously, the atten-tion module in VLMs requires a one-by-one pair asinput. In this case, inferencing one image requires8080 times of forwarding.This huge inference cost make it less practical tocompare with existing methods under the currentprevalent settings. However, it does not suggestusing VLMs in SGG is meaningless. We stronglybelieve that the main concern of SGG task is tocorrectly recognize the relation given a pairs ofobjects, instead of the object detection, given thefact that the detector could be trained separatelywhile achieving the same good performance. Andby equipping with more efficient and effective de-tectors, the performance in Scene Graph Detectionand Scene Graph Classification should be closed toPredicate Classification.",
  "B.2Analysis on Tail Categories": "In this section, we conducted an additional experi-ment to demonstrate the performance enhancementfor tail relation classes. We divided the relationcategories into three splits, frequent, medium, andrare, based on the frequency in the training set.Subsequently, we evaluated and reported the en-semble gain on mean Recall@100 for each splitbrought by our methods. We opted for mean Re-call@100 as the metric due to its superior represen-tation of rare relations and reduced susceptibilityto background class interference. Across all threebaselines, we observed a substantial improvementin performance for rare relation categories, whichconfirms our hypothesis that the underrepresenta-tion issue is more severe in rare relation classes.",
  "CMore Details of Implementation": "This section shows more details of our implemen-tation. In existing models designed for SGG, theobject detector is attached in front of the relationrecognition network and jointly trained with the ob-jectives of SGG tasks. However, when fine-tuning VLMs on SGG tasks, this paradigm could be time-consuming and less flexible, given the higher train-ing cost of VLM comparing with existing models.Therefore, we decide to take the Faster R-CNNdetector out and train it separately without themain network.This implementation is provedto be effective when we take the detector out ofPENET (Zheng et al., 2023) and train it separatelywith the PENET relation network. We observethat the independently trained detector achievedthe same performance with that jointly trained withthe PENET. Hence, all fine-tuned VLMs in thispaper used a separately-trained Faster R-CNN de-tector. In the fine-tuning stage on Visual Genome,we employ two different paradigms for ViLT (Kimet al., 2021) and Oscar (Li et al., 2020) for a moregeneral comparison. We freeze the ViLT backbonewhile training the MLP head for 50 epochs. Inanother way, we use an end-to-end fine-tuning for70k steps on Oscar. We keep the fine-tuning costcomparable to the existing SGG models, whichensures its practical feasibility.Why dont we debias on the triplets distribu-tion instead of the relation words distribution?In the paper, we declare the relation words biascaused by different frequency of relation labels.And the underrepresentation issue caused by dif-ferent representation level of samples. One caninfer that the representation level is largely effectby the frequency of triplets. In other words, thesamples of frequent triplets are usually better rep-resented in training compared with those samplesof rare triplets. Therefore, one intuitive thinkingis to debias directly on the triplets distribution bysubstracting log P(zi, zj, r) instead of the relationwords distribution log P(r). This thought is indeedthe most throughly debiasing strategy. However,one need to consider that the conditional prior oflog P(r|zi, zj) could largely help the prediction ofrelationship (Tang et al., 2020). For example, innatural world, the relation between a man\" and ahorse\" is more likely to be man riding horse\" thanman carrying horse\". Directly debiasing on thetriplets distribution would erase all these helpfulconditional priors, resulting in a drastically drop inperformance.",
  "Question 1: Is our improvement from repre-sentation improvement or simply parameter in-crease from ensembled VLMs?Because of": "the predicates biases in pretraining data, integrat-ing large pretrained models does not guaranteeimprovement. In of the main paper, weshowed that ensembling the original VLMs withoutdebiasing cannot bring any improvements. Only byintegrating the VLM debiased by our LM Estima-tion can enhancements be brought.By integrating our debiased VLM, the under-representation issue is alleviated since underrepre-sented samples are improved much more than well-represented samples. In in the main paper,we show that unseen triplets are improved higherthan all triplets average. Integrating our debiasedVLMs indeed brings a slight overall improvement,but most are from addressing the representationimprovement.Question 2: Is it fair for us to use distinct Ptato measure Recall and mRecall and comparewith existing methods? Unlike previous methodsin SGG, our framework accepts a user-specifiedtarget distributions Pta as input. In SGG settings,measuring both Recall and mRecall is to evaluateunder two distinct test distributions, as discussedin .3 of our main paper. For our method,using the same Pta under these two distinct distri-butions will input a wrong distribution Pta that isfar from the actual target. This goes against ouroriginal intention.Previous methods are measured by both metricswithout any change because once trained, unlessby time-costing re-training, they cannot be trans-ferred from one target distribution Pta to anotherP ta. However, our method achieves this transferinstantaneously by simply + log (P ta/Pta) to thelogits. So it is fair to compare with previous meth-ods since our transfer adds no extra time cost.Question 3: Is underrepresentation issue a spe-cific characteristic problem for SGG? The prob-lem of this inadequate sample representation is atypical and specific characteristics of SGG and isfar more severe than that in other related fields, likelong-tailed classification in Computer Vision. InSGG, a samples representation includes two ob-jects attributes and their high-level relationship.Due to this unique complexity, it is extremely hardfor SGG datasets to adequately represent all tripletscombinations. For instance, there are 375k tripletscombinations in Visual Genome (Krishna et al.,2017), much more than the label sets of any classi-fication dataset in Computer Vision. This inevitablyleads to the majority of triplets having only a fewsamples in training."
}