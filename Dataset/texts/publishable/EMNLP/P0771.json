{
  "Abstract": "Smaller-scaleVision-LanguageModels(VLMs) often claim to perform on parwith larger models in general-domain visualgrounding and question-answering benchmarkswhile offering advantages in computationalefficiency and storage. However, their abilityto handle rare objects, which fall into the longtail of data distributions, is less understood. Torigorously evaluate this aspect, we introducethe \"Uncontextualized Uncommon Objects\"(UOUO) benchmark. This benchmark focuseson systematically testing VLMs with bothlarge and small parameter counts on rareand specialized objects. Our comprehensiveanalysis reveals that while smaller VLMsmaintain competitive performance on commondatasets, they significantly underperform ontasks involving uncommon objects. We alsopropose an advanced, scalable pipeline fordata collection and cleaning, ensuring theUOUObenchmarkprovideshigh-quality,challenging instances. These findings highlightthe need to consider long-tail distributionswhen assessing the true capabilities of VLMs.Code and project details for UOUO can befound at",
  "Introduction": "The advent of Vision-Language Models (VLMs)has marked a revolutionary leap in the integrationof natural language processing and computer vi-sion, largely due to the capabilities of the self-attention mechanism and the Transformer archi-tecture (Vaswani et al., 2023). These technolo-gies allow VLMs to effectively process and fuseinformation from both text and images, leadingto significant advancements in tasks that requiremultimodal understanding, such as visual questionanswering and image captioning (Radford et al.,2021; Li et al., 2023; Alayrac et al., 2022; Xu et al.,2023; Young et al., 2014). VLMs, trained on large-scale datasets, typicallyboast high performance on general tasks involvingeveryday objects and common scenarios (Li et al.,2024; Du et al., 2022; Wang et al., 2023). However,models of smaller scale, defined here as havingfewer than 70 billion parameters, often claim tomatch the capabilities of their larger counterpartson general domain tasks (Lin et al., 2015; Agrawalet al., 2016; Yu et al., 2016; Liu et al., 2024; Goyalet al., 2017; Yu et al., 2023b) while offering ad-vantages in computational efficiency and storage.Despite these claims, the No-Free-Lunch Theorem(Wolpert and Macready, 1997) suggests that thesesmaller models may compromise on their abilityto handle less common or more complex scenariosthat lie in the long tail of data distributions.One natural and intuitive hypothesis is that theyare sacrificing their fitness to the elements on thelong tail of the distribution. Empirical observationsof real-world data frequently align with Zipfs andPower Law (Piantadosi, 2014; Clauset et al., 2009),which indicates that while some objects and con-cepts are exceedingly common, a vast number ofthem are rare and fall into the long tail of the dis-tribution . Understanding how well VLMs handlethese rare and uncommon instances is crucial forassessing their true robustness and applicabilityacross diverse and nuanced contexts.Despite the importance of this evaluation, thereis currently a lack of dedicated benchmarks thatsystematically test VLMs on objects and conceptsthat are significantly outside the everyday norm.To address this gap, we introduce the \"Uncontextu-alized Uncommon Objects\" (UOUO) benchmark.The object class distribution of UOUO is system-atically out of common image sources such as Im-ageNet (Russakovsky et al., 2015), COCO (Linet al., 2015), and Open Image Dataset (Kuznetsovaet al., 2020). Our goal is to rigorously test andquantify the performance of both large-scale andsmall-scale VLMs on elements from the long tail of",
  ": UOUO Data Curation Pipeline. Snowflake means frozen weights, and fire means tune-able weights": "the distribution to showcase their knowledge gap.The contribution of our work is three-fold. (1)We compile a million-scale dataset specifically de-signed to include uncommon and uncontextualizedobjects, which are rarely encountered in everydaycontexts but are significant in specialized domains.(2) We evaluate the performance gap between large-scale and small-scale VLMs when dealing withthese rare elements, showcasing the significantknowledge and performance gap between large-and small-scale model on the long-tail distributions.(3) We propose a systematic pipeline for automaticand scalable data collection and cleaning, ensuringhigh-quality and representative testing instances.",
  "Related Work": "Real-world VQA BenchmarksBased on oursurvey, the typical real-world visual question an-swering datasets (excluding mathematics, celebrity,landmark, place, OCR and chart-reading) used inpopular open-source VLMs such as LLaVa (Liet al., 2024), CogVLM (Wang et al., 2023) BLIP2(Li et al., 2023), Qwen VL (Bai et al., 2023) andMiniCPM-V (Yu et al., 2023a) includes the follow-ing: COCO (Lin et al., 2015), RefCOCO (Yu et al.,2016), NoCAPs (Agrawal et al., 2019), MMBench(Liu et al., 2024), VQA-v2 (Goyal et al., 2017),OK-VQA (Marino et al., 2019), MME (Fu et al.,2024), GQA (Hudson and Manning, 2019).Much to our surprise, it turns out that the im-age sources of GQA, RefCoCo, OK-VQA, MMECoarse-Grained Recognition, VQA-v2, and a sig-nificant proportion of MMBench are all directrandom samples from COCO. Only NoCAPs fea-tures novel object classes (sourced from the 600-categories Open Image Dataset (Kuznetsova et al.,2020) outside COCOs less-than-100 common classes. This showcases the significant limitationof categorical diversity of extant VQA datasets.The knowledge and performance gap between thesmall- and large- scale VLMs might be concealedin such low coverage and diversity. Existing Datasets with Uncommon Object La-belsIn extant datasets, Stanford Cars (Krauseet al., 2013), CUB-bird (Wah et al., 2011), Deepfish(Saleh et al., 2020), ROCOv2 (Rckert et al., 2024),FGVC-Aircraft (Maji et al., 2013) also features rareobject labels. Some non-academic mine & stonedatasets, and chemical objects datasets can also befound on internet. However, the typical emphasisof these datasets is either fine-grained subtype orsubspecies of common objects, or domain-specificexpert knowledge. In realistic use cases such asautonomous car or embodied robotics, such knowl-edge might have limited generalizability.",
  "Domain Selection and Scraping": "To construct the UOUO (Uncontextualized Uncom-mon Objects) benchmark, we began by selectingspecific domains that are rich in specialized knowl-edge yet contain objects and tools that are rarelyencountered by the general public. Our focus wason the industry sector, given its diversity and thepresence of numerous specialized tools and equip-ment. These artificial tools are significantly outof the distribution of ImageNet, COCO, and OpenImage Dataset.We used Wikipedia as a starting point, tar-geting the page dedicated to manufacturing( Foreach sub-sector identified within this domain, weemployed GPT-4-Turbo (OpenAI, 2024) to gener- ate a list of the top 50 objects or tools pertinent toexperts in the field but obscure to the general popu-lace. This list was generated through prompt-basedquerying, asking the model to identify objects thatare crucial within the industry but not commonlyknown.Once we had our list of uncommon objects, weperformed a Google Image Search for each objectname. For each query, we collected the top 50 im-age results. This approach allowed us to gathera diverse set of images representing each objectunder different conditions and contexts. For de-tailed dataset statistics of UOUO, we refer readersto Appendix C. Mannual AnnotationThe image instances col-lected from Google Image Search can be noisy,with perhaps one fifth irrelevant instances for eachqueried uncommon category. To ensure the qualityand relevance of the dataset, we implemented arigorous annotation and cleaning process, combin-ing manual and automated techniques. Our teammanually reviewed and annotated on a subset ofthe collected categories of images to identify andremove outliers and noisy data. Categories withconsistent visual representation across exampleswere retained, while those filled with ambiguous orirrelevant images were discarded. This initial cura-tion aimed to maintain high fidelity to the objectsintended representation. The instruction for man-ual annotation of UOUO can be found in AppendixB. Automatic Data CleaningWe utilized the CLIPmodel to further enhance the dataset. CLIP (Con-trastive LanguageImage Pre-training) providesembeddings for both images and text, enablingus to compute similarities within and across cat-egories. For each image, we extracted its CLIPimage embedding Eci and the text embedding Tcof its corresponding category name (Radford et al.,2021; Sun et al., 2023). We calculated the cosinesimilarity between all pairs of image embeddingswithin each category to construct a GRAM matrixG, where Gi,j = Cosine(Eci , Ecj). Additionally,we computed the image-text similarity for each im-age as Cosine(Eci , Tc), alongside statistical metricssuch as the percentile, mean, and variance of theaverage similarity within each category.The complete feature set includes image embed-dings, GRAM percentiles (25th, 50th, and 75th),GRAM mean and variance, the instances meansimilarity with other images, and the percentiles of",
  ": With MMD, we can retrieve harder negativeexamples and construct higher-quality test instances": "its pairwise similarities. Additionally, it incorpo-rates image-text similarity metrics, correspondingpercentiles, z-scores, and the instance label.Using these computed features, we applied anXGBoost classifier to label each image instance.This classifier was trained on manually cleaneddata from 500 categories to distinguish betweenhigh-quality and low-quality instances based ontheir similarity scores.We optimized our XGBoost classifier (Chen andGuestrin, 2016) through 5-fold cross-validation andgrid search to identify the best hyperparameters.The optimal configuration consisted of a maximumtree depth of 6, 200 estimators, a learning rate of0.15, a subsample ratio of 1.0, gamma value of 0.1,and a colsample-bytree of 1.0. Additionally, theregularization parameters included reg-lambda of1.5 and reg-alpha of 0.0, with a minimum childweight of 1.0.The classifier achieved an accuracy of 0.8754 oncross-validation, closely aligning with human judg-ment, and exhibited Macro-Average Precision, Re-call, and F1-Score of 0.8631, 0.8353, and 0.8460,respectively.",
  "Test Instances Generation": "Background Removal and DecontextualizationConnectionist neural networks (including VLMs)are notoriously known for their tendency of overfit-ting to spurious correlations present in the trainingdata. For instance, in our collected data, bulldozersare often seen in construction scenes laden with ma-terials such as sand, concrete, and bricks. This highco-occurrence can lead models to rely on these con-textual cues rather than truly understanding and rec-ognizing the bulldozer itself. To mitigate this issueand ensure that models focus on the objects rather than their typical environments, we implement arobust background removal process to decontextu-alize all candidate objects in our dataset. To achieveeffective background removal, we utilize a state-of-the-art, off-the-shelf background removal model(BRIA-AI, 2024). Testing Instances GenerationTo assess theperformance of Vision-Language Models on ourUOUO benchmark, we generated challenging testinstances designed to probe the models capabili-ties beyond common knowledge. Specifically, weemploy the CLIP embeddings combined with theMaximum Mean Discrepancy (MMD) with a Gaus-sian RBF kernel (Dziugaite et al., 2015) to identifyand retrieve hard negative examples.Let x and y be the sets of CLIP embeddingsfor two different object categories, each of shape(n, d), where n is the number of embeddings and dis the embedding dimension.The Maximum Mean Discrepancy (MMD) be-tween sets of embeddings x and y is calculated asfollows:",
  "ai bj2": "For our calculations, we set = 10.We use the Mosaic Image Augmentation Tech-nique (Ge et al., 2021) to generate testing datain a scalable way. Each testing data point is cre-ated from four images, each background-removed.The four images contain objects of different cat-egories but share some similar visual propertiessuch as structures, colors, or textures. The selec-tion of these images is determined by the MaximumMean Discrepancy (MMD) distance between thecategories they belong to. The closer the MMDdistance, the more similar in features they mightappear. We create an 800x800 canvas large enoughto accommodate all four images. Then, each of thefour images is augmented and positioned on thecanvass top-left, top-right, bottom-left, or bottom-right. The ground-truth bounding box for the ob-ject grounding is generated from the segmentationmask of background removal and normalized tobe dimension-insensitive, accounting for potentialdifferences in the VLMs rescaling process. showcases an exemplar test instance.",
  "Experiment": "ProceduresFollowing the aforementioned testinstance generation, we test both open sourceVLMs that are trained to perform grounding, in-cluding: llava-v1.5-7b, llava-v1.5-13b (Liu et al.,2023), llava-v1.6-vicuna-7b, llava-v1.6-vicuna-13b, llava-v1.6-34b (Li et al., 2024), cogvlm-v1.5-vicuna-7b (Wang et al., 2023), and propriety VLMsincluding: gemini-1.5-pro (Team, 2024), gpt-4-turbo, gpt-4o (OpenAI, 2024).We test VLMs performance on both randomlygenerated test instances and the MMD-augmentedhard instances. We employ two metrics to quantitfythe performance: mIoU - Mean IoU (Intersectionover Union), a standard metric for object segmen-tation; and Accuracy , which we prompt the VLMto output one positions from \"top-left, top-right,bottom-left, bottom-right\", and directly evaluatewhether the answer matches the ground truth. Theprompts used in this experiment can be found inAppendix A. Observations and AnalysisWe present all ex-perimental results in . (a) Comparing hor-izontally across columns, we observe significantperformance drops of smaller-scale models in bothmIoU and Accuracy with the application of MMD-based hard instance generation. Notably, the per-formance drops of many of them are around 30%.This provides solid support for our initial hypothe-sis that smaller-scale models have some, but insuf-ficient fitness to the long-tail distribution objects.Furthermore, the drastic performance change show-cases MMDs effectiveness in generating hard in-stances and non-robustness of existing groundingmodels. (b) Comparing vertically within columns,the central tendency is that larger scale models (ex-cept Genimi which might not be trained to performgrounding) perform much better than small-scalemodels in accuracy. This reveals the concealed gapof knowledge horizon of small- and large- scalemodels, which is usually unobservable in bench-marks consist of common objects. (c) The obser-vation that GPT-4 series can still handle the task",
  "Conclusion": "In our work, we introduced the UOUO benchmarkto assess VLMs on objects out of everyday dis-tributions. Our findings show that while smallerVLMs perform well on tasks of common objects,they struggle significantly with uncommon objects,unlike larger models which handle these challengesmuch better. This highlights the need to considerlong-tail distributions in evaluations. The system-atic data curation, filtering, and hard test instancegeneration pipeline for UOUO construction hashigh extensibility, paving the road of future re-search of long-tail distribution objects. UOUOitself could also be expanded in this way, extendingbeyond the domain of manufacturing and to otherbroad category of objects.",
  "Limitations": "One limitation of our work is the reliance on au-tomated data collection and cleaning processes,though efficient, may introduce biases or fail tocapture nuanced representations compared to fullymanual curation. We also note that the Mosaic Image Augmentation was applied with the assump-tion that the model takes single-image inputs. Ourpreliminary experiment showed most VLMs havelimited to none multi-image inference support,thus multi-image inputs results are not includedin UOUO benchmark. The UOUO benchmark cur-rently emphasizes static images, potentially over-looking the dynamic and context-dependent natureof object recognition in real-world scenarios. Fu-ture extensions should explore a wider range ofuncommon objects across various fields and con-sider the inclusion of video or sequential data tobetter reflect real-world applications. Addressingthese limitations will enhance the comprehensive-ness and applicability of the UOUO benchmark.",
  "Acknowledgement": "This work was supported by DARPA ECOLEHR00112390063 and by the National ScienceFoundation grants NSF CNS 19-00875, NSF CNS21-06592, NSF OAC 18-35834 KN, NSF CCF22-17144. This research used the Delta advancedcomputing and data resource which is supportedby the National Science Foundation (award OAC2005572) and the State of Illinois. Any results andopinions are our own and do not represent views ofNational Science Foundation.",
  "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Mar-garet Mitchell, C. Lawrence Zitnick, Dhruv Batra,and Devi Parikh. 2016. Vqa: Visual question answer-ing. Preprint, arXiv:1505.00468": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,Rishabh Jain, Mark Johnson, Dhruv Batra, DeviParikh, Stefan Lee, and Peter Anderson. 2019. no-caps: novel object captioning at scale. In Proceed-ings of the IEEE International Conference on Com-puter Vision, pages 89488957. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Brock, Aida Nematzadeh, Sahand Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karen Simonyan. 2022.Flamingo: a visual language model for few-shotlearning. Preprint, arXiv:2204.14198. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, textreading, and beyond. Preprint, arXiv:2308.12966.",
  "Gqa: A new dataset for real-world visual reason-ing and compositional question answering. Preprint,arXiv:1902.09506": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3d object representations for fine-grainedcategorization. In 2013 IEEE International Confer-ence on Computer Vision Workshops, pages 554561. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,Stefan Popov, Matteo Malloci, Alexander Kolesnikov,Tom Duerig, and Vittorio Ferrari. 2020. The openimages dataset v4: Unified image classification, ob-ject detection, and visual relationship detection atscale.International Journal of Computer Vision,128(7):19561981.",
  "Steven T. Piantadosi. 2014. Zipfs word frequency lawin natural language: a critical review and future direc-tions. Psychonomic Bulletin & Review, 21(5):11121130": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. Preprint, arXiv:2103.00020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael Bernstein,Alexander C. Berg, and Li Fei-Fei. 2015. Imagenetlarge scale visual recognition challenge. Preprint,arXiv:1409.0575. Johannes Rckert, Louise Bloch, Raphael Brngel,Ahmad Idrissi-Yaghir, Henning Schfer, Cynthia S.Schmidt, Sven Koitka, Obioma Pelka, Asma BenAbacha, Alba G. Seco de Herrera, Henning Mller,Peter A. Horn, Felix Nensa, and Christoph M.Friedrich. 2024. Rocov2: Radiology objects in con-text version 2, an updated multimodal image dataset.Preprint, arXiv:2405.10004. Alzayat Saleh, Issam H Laradji, Dmitry A Kono-valov, Michael Bradley, David Vazquez, and Mar-cus Sheaves. 2020. A realistic fish-habitat dataset toevaluate algorithms for underwater visual analysis.Scientific Reports, 10(1):14671.",
  "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.Berg, and Tamara L. Berg. 2016. Modeling context inreferring expressions. Preprint, arXiv:1608.00272": "Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, YifengHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-TaoZheng, Maosong Sun, et al. 2023a. Rlhf-v: Towardstrustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprintarXiv:2312.00849. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and Li-juan Wang. 2023b. Mm-vet: Evaluating large multi-modal models for integrated capabilities. Preprint,arXiv:2308.02490.",
  "mIoU PromptPlease provide the bounding boxcoordinate (x1,y1,x2,y2) of {object name} in theimage with the format \\n item1:(x1,y1,x2,y2)": "Accuracy PromptIdentify the location of thegiven object in this 2x2 mosaic image. The possibleanswers are: top left, top right, bottom left,bottom right, or none. Only give a deterministicresponse as one of the possible answers. If theobject is not present, the response should be none.Please do not give more than one response. \\nobject name: {object name}\\n Location: Prefix settingAll other settings follow themodels defaults.For instance, in the case ofllava, the prompt prefix is: A chat between a cu-rious human and an artificial intelligence assis-tant. The assistant gives helpful, detailed, andpolite answers to the humans questions. USER:<image>\\n{question} ASSISTANT:"
}