{
  "Abstract": "Massive-scale historical document collectionsare crucial for social science research. Despiteincreasing digitization, these documents typi-cally lack unique cross-document identifiers forindividuals mentioned within the texts, as wellas individual identifiers from external knowl-edge bases like Wikipedia/Wikidata. Existingentity disambiguation methods often fall shortin accuracy for historical documents, whichare replete with individuals not rememberedin contemporary knowledge bases. This studymakes three key contributions to improve cross-document coreference resolution and disam-biguation in historical texts: a massive-scaletraining dataset replete with hard negatives -that sources over 190 million entity pairs fromWikipedia contexts and disambiguation pages -high-quality evaluation data from hand-labeledhistorical newswire articles, and trained modelsevaluated on this historical benchmark. We con-trastively train bi-encoder models for corefer-encing and disambiguating individuals in histor-ical texts, achieving accurate, scalable perfor-mance that identifies out-of-knowledge base in-dividuals. Our approach significantly surpassesother entity disambiguation models on our his-torical newswire benchmark. Our models alsodemonstrate competitive performance on mod-ern entity disambiguation benchmarks, particu-larly on certain news disambiguation datasets.",
  "Introduction": "Massive scale historical document collections -such as historical newspapers or the 14 billion doc-uments in the U.S. National Archives - are cen-tral source materials for social science research.While historical documents are increasingly be-ing digitized, they are not typically tagged withunique cross-document identifiers for individualsmentioned in the texts, or with individual iden-tifiers from an external knowledge base such asEnglish Wikipedia/Wikidata, which provides struc-tured data for over a million individuals. While there is a large literature on entity dis-ambiguation to a knowledge base (but to a lesserextent, entity coreference across documents in acorpus), we found that existing methods did notmeet our accuracy requirements when applied tohistorical documents. Some widely-used meth-ods require the entity to be in the knowledge base,whereas historical documents are replete with indi-viduals not in Wikipedia. Indeed, one motivationfor disambiguating entities in historical texts is tounderstand why some people are remembered andothers are not. Historical texts have a different dis-tribution of entities than modern texts; for example,there may be fewer hyperlinks in crawl corpora - acommon source of training data - to these individ-uals Wikipedia pages. Moreover, historical textsoften have OCR noise, and language can evolveacross time. To feasibly run entity disambiguationover massive-scale historical texts, the method alsoneeds to be highly computationally efficient, asacademic and archival budgets are typically highlyconstrained, and the amount of potential historicalmaterial to disambiguate is vast.This study makes three central contributions de-signed to improve and encourage further researchon cross-document coreference resolution and dis-ambiguation of individuals in historical texts: amassive-scale training dataset replete with hardnegatives, high quality evaluation data drawn fromhistorical documents, and entity coreference anddisambiguation models trained and evaluated onthese data.Our first contribution is to train coreference anddisambiguation models, with a focus on historicaltexts, and news in particular. Our aims are: 1) ac-curate performance, 2) highly scalable, 3) allowingout-of-knowledge base mentions, and 4) trainablewith simple recipes and limited compute.A contrastively trained bi-encoder retrieval ar-chitecture, widely used for open-domain retrieval(e.g., Karpukhin et al. (2020)), is an excellent fit for these requirements. In a contrastively trainedbi-encoder, the neural network encodes mentions(in the document corpus or a knowledge base) thatrefer to the same entity nearby in embedding spaceand encodes mentions referring to different enti-ties further apart. At inference time, a given entitymention is disambiguated to the nearest entry ofthe knowledge base if their encodings are withinsome threshold similarity. If the mention encodingis sufficiently dissimilar to all entries in the knowl-edge base, the entity is marked as out-of-knowledgebase. Analogously, an arbitrary number of entitymentions across documents in a corpus (e.g.,, dif-ferent newspaper articles) can be coreferenced byclustering their embeddings.This approach is highly scalable because eachentity mention and entry in the knowledge base isembedded only once, no matter how many entitiesare to be disambiguated. Moreover, a Facebook AISimilarity Search (FAISS) (Johnson et al., 2019)backend can be used to retrieve the most similarknowledge base embedding for each mention in thedocument corpus. FAISS is extremely optimized,scaling up to billion-scale datasets on relativelymodest hardware. This architecture also easily han-dles out-of-knowledge-base entities. Assuming anappropriate training dataset, it is straightforwardto train, making it feasible, even on a highly con-strained academic compute budget, to update themodels as the deep learning literature advances orto tune them for specific document collections.A second central contribution of the paper isthe creation of a massive-scale dataset for con-trastive training of entity coreference and entitydisambiguation models. WikiConfusables con-structs over 190 million entity pairs for contrastivetraining. Positive pairs come from contexts (para-graphs) in Wikipedia that contain hyperlinks to thesame page (for coreference), or from a context andthe first paragraph of the relevant entity that it linksto (for disambiguation).We obtain hard negative entity pairs - e.g., pairsthat are highly confusable - at scale from Wikipediadisambiguation pages, which list entities that haveconfusable names or aliases. The hard negativepairs come from contexts that link to differentpages on a given disambiguation page. For ex-ample, the disambiguation page \"John Kennedy\"includes John F. Kennedy the president, JohnKennedy (Louisiana politician), John F. KennedyJr, and a variety of other John Kennedys. Hard neg-atives sample contexts mentioning John F. Kennedy (e.g., with hyperlinks to John F. Kennedys page)and pair them with contexts mentioning other enti-ties from the John Kennedy disambiguation page.We overrepresent hard negatives from within fam-ilies (e.g., paired mentions of Henry Ford Jr. andHenry Ford Sr.) by mining family members fromWikidata pages. These challenging cases are verycommon in historical texts (e.g., fathers and sonswith the same name and profession). Constructingour open-source WikiConfusables required sub-stantial wrangling of Wikipedia dumps, and wehave made it publicly available (CC-BY) to allowother researchers to more easily exploit this richsource of information.TheextensivehardnegativepairsinWikiConfusablesallowustocontrastivelytrain our entity coreference and disambiguationmodels on four Nvidia A6000 GPU cards, amodest setup by deep learning standards.Incontrast, contrastive training with random negativepairs requires massive batch sizes to achieve strongperformance (He et al., 2020).We train our LinkMentions coreference modelon paired contexts around Wikipedia mention hy-perlinks. We further tune this coreference modelfor disambiguation, creating LinkWikipedia, bytraining on paired contexts and first paragraphs. Ifdesired, this model can be further tuned on targetdata. We do so for historical newspapers, creat-ing the LinkNewsWikipedia model by tuning on ahand-labeled dataset linking individuals in newspa-pers to Wikipedia.A third contribution is the development of a highquality benchmark that coreferences and disam-biguates individuals in historical newswire arti-cles from the 1950s and 1960s, that appear in theNewswire dataset (Silcock et al., 2024). We handdisambiguate entities - or mark them as not in theknowledge base - creating the Entities of theUnion historical benchmark.We document performance on Entities ofthe Union that significantly exceeds that of otherwidely used entity disambiguation models. We arealso competitive in disambiguating individuals inmodern benchmarks, especially on the MSNBCand ACE2004 benchmarks (both of which disam-biguate modern news), where we outperform otherstate-of-the-art models. This suggests that our mod-els and training data have broader applications be-yond historical documents, especially to modernnews.Cross-document coreference is moreover highly accurate. Cross-document coreference resolutionis often central to cataloging historical documentcollections, and can be applied as a first step tocreating a knowledge base when the individualsin a corpus are not covered in existing knowledgebases.Finally, we briefly illustrate some of the factsthat can be gleaned from tagging unique individualsin historical document collections, by applying theLinkNewsWikipedia model to a large-scale histor-ical news dataset. We are enthusiastic about thepromise of coreferenced and disambiguated histori-cal documents to lead to many informative insights.Our datasets and models are open-source, with aCC-BY license, and we hope that they encouragefurther engagement with historical cross-documentcoreference resolution and disambiguation.The rest of this study is organized as fol-lows: discusses the related literature,and introduces the novel massive scaleWikiConfusables training dataset and historicalbenchmark. develops models for en-tity coreference and disambiguation, and evaluates model performance. appliesour models to a massive-scale historical news cor-pus. Finally, considers limitations, and discusses ethical considerations.",
  "Literature": "Entity disambiguation has inspired a variety of ar-chitectures, including a masked language model(LUKE) (Yamada et al., 2022) and a neural transla-tion model (GENRE) (De Cao et al., 2020). Whilethese architectures work well for some problems,they are not well-suited for disambiguation of large-scale historical corpora. The masked languagemodel approach limits to the top 50K Wikipediaentries, many of whom are not people, due to com-putational constraints in computing the softmax. Italso does not allow out-of-knowledge base entitiesand requires sparse entity priors. The neural transla-tion approachs sequence-to-sequence architectureis slow at inference time, requiring around 60 timeslonger to run than other models considered in ourcomparisons.This paper follows the most scalable entity dis-ambiguation approaches in employing a bi-encoderarchitecture. One of the inspirations for the currentstudy is BLINK (Wu et al., 2019), which modelsentity disambiguation as a text retrieval problem,using a contrastively trained BERT (Devlin et al., 2018) bi-encoder and a re-ranking cross-encoder.The model assumes all entities are in the knowledgebase. This study updates the bi-encoder architec-ture with advances made over the past five years(such as using mean pooling rather than a [CLS] to-ken for the representation (Reimers and Gurevych,2019) and advances in training efficiency). It alsodevelops an expansive training dataset, replete withhard negatives. We develop a coreference modeland incorporate it into the disambiguation pipelineand allow for out-of-knowledge base individuals.Another well-known model that uses a bi-encoder is ReFinED (Ayoola et al., 2022), an en-tity linking model that performs mention detectionand disambiguation for all mentions within a doc-ument in a single pass. Like BLINK, it uses a bi-encoder architecture but allows entities to be out-of-knowledge base. This study compares the perfor-mance of our models to GENRE, BLINK, and Re-FinED, widely-used models with well-maintainedcodebases.The cross-document coreference resolution liter-ature is less dense, but a closely related study is Hsuand Horwood (2022), which uses a contrastivelytrained RoBERTa (Liu et al., 2019) bi-encoder andclustering for cross-document resolution of entitiesand events. They do not consider disambiguationto an external knowledge base.A key distinction between this study and mostexisting entity disambiguation benchmarks is itsfocus on real-world contexts with lesser known en-tities - many lost to history except in the contexts ofthe documents being considered. Most benchmarksonly contain in-knowledge base entities. An excep-tion is Kassner et al. (2022). They detect out-of-knowledge base mentions by clustering representa-tions, with a cluster defined as out-of-knowledgebase if there are no Wikipedia embeddings in thecluster. They then add the mean embeddings ofthe out-of-knowledge base clusters to the knowl-edge base index and run entity linking with thisfully comprehensive embedding index. They useBLINK as the encoder. To create a dataset without-of-knowledge base entities, they link a largecrawl corpus (OSCAR) to two Wikipedia dumpstaken at time t0 and t1. Links to pages added be-tween t0 and t1 are then out of knowledge basewhen disambiguating to the knowledge base in t0.This type of out-of-knowledge base entity is dif-ferent to the type we encounter in historical texts,as they are entities that were prominent enough atthe time of mention to link to Wikipedia but are not in an earlier snapshot. In contrast, in historicalapplications, many individuals are simply not veryprominent. We do not compare on this benchmark,as all these entities were in the late-2022 Wikipediasnapshot we used for training. Hence, they wereseen in training by our models (but not by some ofthe older comparisons) and no longer approximatetruly out-of-knowledge base entities. We do notcompare on the dataset introduced by Zaporojetset al. (2022) for similar reasons.",
  "Training Data": "High-quality hard negatives, as well as paired posi-tive data, are needed to train contrastive modelsfor entity coreference and disambiguation. Wecreate a novel, massive-scale training dataset -WikiConfusables - by mining entity pairs from Wikipedia disambiguation pages and Wikidata fam-ily relationships.Entity contexts are drawn from a WikipediaXML dump1 from November 11, 2022, with men-tions of each entity appearing as a hyperlink totheir page. We split the entities into train, test, andvalidation sets, pairing mentions of the same entityalong with their context (defined by the paragraphcontaining the entity mention) to create positivepairs. We create easy negatives by pairing anentity mention with that of a different entity. Weobtain hard negatives using Wikipedias disam-biguation pages, e.g., John Fitzgerald Kennedy andJohn Kennedy (Louisiana senator). We further en-hance our training data with in-context negatives,other entities that appear in the context window ofthe entity under consideration. describesthe resulting dataset.",
  ": Statistics on dataset size": "We also create data for disambiguation by link-ing contexts with entity mentions to their associatedtemplate, forming positive pairs. To create the tem-plate, we use Wikidata names, aliases, and occupa-tions/positions held by individuals. For example,for President Kennedy: \"John F. Kennedy is of typehuman. Also known as Kennedy, Jack Kennedy, President Kennedy, John Fitzgerald Kennedy, J.F. Kennedy, JFK, John Kennedy, John FitzgeraldJack Kennedy, and JF Kennedy. Has worked asa politician, journalist, and statesperson.\" We thenappend this text with the first paragraph of the as-sociated Wikipedia page.Easy negatives are created by linking contextswith random entity templates.Similar to ourcoreference training, we use Wikipedia disam-biguation pages to associate entity contexts withhard negative templates. We also create negativepairs using family relationships in Wikidata - e.g.,John F. Kennedy and Jacqueline Kennedy Onas-sis (who could often be referred to as \"Mrs. JohnKennedy\"). We split the entities into an 80-10-10train-validation-test split.Finally, we further adapt the training domainto newspapers. An advantage of our bi-encoderarchitecture is that it is straightforward to tune tospecific domains, plausibly helpful given historicaldocument collections can be quite idiosyncratic.Weprepareahand-labeleddataset,NewsConfusables, to tune our LinkWikipediamodel to the historical news domain. First, weobtain names and aliases of individuals fromWikidata, then do a sparse search for them in anewspaper corpus spanning a century. We handlabel whether the article refers to the anchor (e.g.,John F. Kennedy) or someone with the same nameor alias (e.g., City Councilman Jack Kennedy).When they refer to different individuals, these formhard negatives. We create extra hard negatives bymatching an individual with another individualmentioned in the same context, and Wikipedia hardnegatives by matching an individual with anotherindividual mentioned in the same Wikipediadisambiguation dictionary.Easy negatives arecreated by matching with a random individual.Further information on this labeling process isgiven in the supplementary materials.",
  "Evaluation Data": "An important contribution of the study is to createhigh quality evaluation data for entity coreferenceresolution and disambiguation with historical docu-ments. Our Entities of the Union benchmarklabels historical, off-copyright U.S. newswire ar-ticles (Silcock et al., 2024). We double label 157newswire articles, from 4 different days from 4years in the 1950s and 60s, totaling 1,137 personmentions. The articles were labeled by highly moti- vated North American undergraduate students2 andall discrepancies were resolved by hand. We labeldays on which State of the Union addresses tookplace, as there are modestly more coreferences toresolve, providing more power for evaluating thistask.We split Entities of the Union into a 50-50evaluation-test split, so the coreference clusteringthreshold can be chosen on the val split. compares the size of the Entities of the Uniontest split to other widely-used modern benchmarksfor entity disambiguation.",
  ": Entity and people mentions across differentbenchmarks": "Note that our full dataset is twice the size of thetest set described here. The larger WNED datasetsare generated automatically from web texts - e.g.,using links from elsewhere on the web to Wikipedia- and hence are more akin to WikiConfusables.Note that these widely used-benchmarks haveall entities in the knowledge base. In Entitiesof the Union (validation and test splits), thereare 220 unique individuals that are in Wikipedia,totaling 898 mentions. There are 239 mentions thatare not in Wikipedia.",
  "Methods": "LinkMentions and LinkWikipedia coreferencementions across documents in a corpus and dis-ambiguate person mentions to a knowledge base,respectively. An overview of the model architec-ture is shown in .We separate named entity recognition - whichtags the tokens in a text that refer to named enti-ties - from entity disambiguation, rather than do-ing them end-to-end as in entity linking. Evenin noisy historical news articles, we are able toachieve 94% accuracy tagging people with namedentity recognition as a token classification task. Er-rors tend to occur when OCR noise is so severethat there is little hope of disambiguating the en-tity. Hence, there is not much scope for errors to",
  "They were paid at the rate set by our department": "propagate. Separating named entity recognitionand coreference/disambiguation simplifies the ar-chitecture, making it easier for the social sciencecommunity to implement or customize to individu-alized applications.We moreover focus on [PER] (person) tags fromnamed entity recognition, as these are of primaryinterest for many historical applications. We foundthat locations could be disambiguated very wellusing non-neural methods and Geonames, a largerstructured database of georeferenced locations.Coreference Resolution Model: Our corefer-ence resolution model links mentions of a givenperson across documents in a corpus. Dependingon the question at hand, coreference resolution canproduce the final output or can be used to create aprototype entity for disambiguation to an externalknowledge base.We choose a bi-encoder infrastructure, as bi-encoders are relatively straightforward for re-searchers with limited exposure to deep learningto customize to novel settings. This is essential foracademic applications, which tend to be highly di-verse and hence often require customization. Theytrain with relatively little supervised data and fine-tuning is not very sensitive to hyperparameter se-lection. Training a bi-encoder is feasible on a smallcompute budget. Pre-trained bi-encoder models arelightweight and offer efficient inference for largedatasets. Other popular architectures often havelimitations on the size of the knowledge base thatcan be used, cannot handle out-of-knowledge baseentities, or are very slow to run. Out-of-knowledgebase entities and a large set of potential entities todisambiguate to are particularly common in histori-cal applications. Finally, bi-encoders are easily im-plementable using the sentence transformers pack-age, which continues to have an active user com-munity. This will hopefully add to the longevity ofa bi-encoder approach to disambiguation.To contrastively train LinkMentions, using the179,069,981 coreference training pairs in our novelWikiConfusables coreference training set, we em-ploy Online Contrastive Loss as implemented inReimers and Gurevych (2019), with cosine simi-larity and margin of 0.4, and utilize AdamW asthe optimizer with a linear warm up scheduler setto 18.2%. Our training setup includes 4 NvidiaA6000 GPUs, a batch size of 512, and a learningrate of 1e-5. We train for a single epoch, processingeach pair in the training split only once. The bestmodel is chosen based on the pair-wise classifica- : Entity Disambiguation Pipeline. Newspaper articles with pre-identified entities are embedded usingLinkMentions and then clustered to group mentions of the same entity. Templates constructed from Wikidata andWikipedia are then embedded using the LinkNewsWikipedia to create a lookup FAISS index. The news articlescorresponding to the same cluster are embedded using LinkNewsWikipedia and mean-pooled to create a prototypeembedding to query the lookup index. The entity of the nearest Wikipedia template to the query is assigned to eacharticle in the entity cluster. tion F1 score on the validation set, with the highestvalidation F1 being 92.75%.We use a sequence length of 256, and initializewith an all-mpnet-base-v2 Sentence-BERT model(Reimers and Gurevych, 2019) from the HuggingFace hub. This is a lightweight model, making itmore feasible for those in the academic communitywith limited resources to train and deploy it at scale.The model is trained in Pytorch (Paszke et al., 2019)with hyperparameters tuned using hyperband im-plemented in Weights and Biases (Weights&Biases,2023). Given the large training dataset, we found itbeneficial to divide it into 10 chunks before training.After completing each chunk (1/10 of an epoch),we resumed training from an intermediate check-point and lowered the learning rate to 2e-6 afterthe first chunk to minimize the risk of the opti-mizer overshooting the minima. Since trainingeach chunk began with a warm up, our approach ef-fectively simulates a linear scheduler with restarts.We observed significant performance improve-ment by using special tokens ([M] Entity [\\M])around an entity mention (Wu et al., 2019). Forinstance, \"Eisenhower sharing a light moment withPresident-elect [M] John F. Kennedy [\\M] duringtheir meeting in the Oval Office.\"At inference time, the contrastively trained LinkMentions is used to embed the mentions, andthen they are grouped together via hierarchical ag-glomerative clustering (with average linkage) usingcosine similarity. A threshold of 0.175 was chosenon the validation set. (Other clustering methodsare straightforward to swap in, as desired.) In thenewspaper corpus, we coreference entities acrossarticles within dates.Disambiguation: For disambiguation, we fine-tune our coreference model on the disambiguationportion of WikiConfusables, with similar hyper-parameters to those used in coreference training,but without restarts or chunking. The learning rateis 2e-6, with a 20% warm up, and batch size is 256.The model is trained for three epochs, and the bestcheckpoint is selected based on the classificationF1, achieving a maximum validation F1 of 97%.This model can be further tuned for specificapplications.We create LinkNewsWikipediaby tuning on the paired disambiguation data inNewsConfusables. We use an identical trainingsetup, achieving a maximum validation F1 of 85%.Next, we prepare a lookup corpus to disam-biguate entity mentions to the correct entity us-ing semantic information from both the contextaround the mention and information from a tem-plate we create from Wikipedia and Wikidata pages of individuals, as described above. We prune ourknowledge base to remove extraneous entities. Weinclude only entities of instance type human, whohave a birth or death date, as we found most that didnot were instance type errors. We remove pages ofindividuals born after the conclusion of the corpusand remove entities with no overlap and a high editdistance between the Wikidata label and the asso-ciated Wikipedia page title. We found that thosepages overwhelmingly were not a person page forthat Wikidata entry.The resulting knowledge base has 1.12 millionperson pages. We embed these templates using ourdisambiguation model and store them in a FAISSIndexFlatIP index (Johnson et al., 2019).To run disambiguation, we embed mentions us-ing the disambiguation model. Using the clustersobtained from coreferencing, we mean pool withineach cluster to create the entity prototype embed-dings, and use these to query the nearest neigh-bor(s) in the knowledge base. If there is no em-bedding in the knowledge base within a thresholdcosine similarity of the query - where this thresholdis chosen on a validation set - we mark the entityas not in the knowledge base.To choose the no-match threshold, we annotatethe output of our disambiguation pipeline on a setof 6,425 pairs sampled from 13 years in a large-scale newswire dataset (Silcock et al., 2024). Wethen find the cut-off threshold that maximizes pair-wise classification precision and use it as the no-match threshold. We chose a threshold of 0.14986.If the returned matches are very close to eachother, we find there are modest gains from dis-ambiguating to the most popular near entity. Ifthe nearest neighbor is within the no-match thresh-old distance for a match, and the second-nearestneighbor is at least 0.01 cosine distance from thenearest neighbor, we disambiguate to the nearestneighbor. Otherwise, we use Qrank3, which ranksWikidata entities by aggregating page views onWikipedia, Wikispecies, Wikibooks, Wikiquote,and other Wikimedia projects. We keep all entitiesthat are within 0.01 cosine distance of the nearestneighbor to the query, and choose the one with thehighest Qrank.We do not add a re-ranking step with a cross-encoder, as in Wu et al. (2019), in order to maxi-mize scaleability to massive datasets. However, training a cross-encoder on WikiConfusableswould be straightforward.For reproducibility and ease of access, we havemade our models and training/evaluation data avail-able on the Hugging Face hub (links are redactedto maintain anonymity for review). All code is inour GitHub repository.",
  "Evaluation": "To measure performance on coreferencing and dis-ambiguating individuals in historical documents,we apply our models and others from the literatureto Entities of the Union. We made significantefforts to run existing models on our evaluationdata, but not all models in the literature have main-tained their codebases, or even made code available.Moreover, some models are simply not suitablefor the task. For example, LUKE (Yamada et al.,2019) limits to top 50K Wikipedia entities (manyof whom are not people), meaning relatively fewentities in our datasets are in the knowledge base.We run BLINK (Wu et al., 2019), GENRE (De Caoet al., 2020), and ReFinED (Ayoola et al., 2022), allprominent models in the entity disambiguation liter-ature with well-maintained codebases. We closelyfollow their implementations, providing details inthe supplementary materials. The latter two mod-els have a zero-shot version and an AIDA-CoNLLfine-tuned version. We report results for both.These are disambiguation, not coreference, mod-els. The coreference literature is thinner and dis-ambiguation is our main focus, and so we do nothave comparisons for this task. Coreferencing in-dividuals across newswire articles is very accurate,achieving an adjusted rand index (ARI) of 96.42. documents that on the Entities of theUnion dataset, both our zero-shot LinkWikipediamodelandfine-tunedLinkNewsWikipediamodel beat other models by a wide margin.LinkNewsWikipediacorrectlyretrievesorclassifies as out-of-knowledge base 78% ofindividual mentions, whereas LinkWikipedia hasan accuracy of 74%. The next best alternative isReFinED, which correctly disambiguates around65% of mentions. When only considering entitiesinWikipedia,LinkNewsWikipediacorrectlydisambiguates 89% of entities, LinkWikipediacorrectly disambiguates 85% of entities, and thenext best alternative is GENRE, with an accuracyof 81%. Hence, while much of the advantage iswith out-of-knowledge base entities, our models",
  ": Benchmark performance comparison across different methods. The first row evaluates on all entities inEntities of the Union, whereas the second row only considers in-knowledge base entities": "also do better on in-knowledge base entities, evenzero-shot.We also compare performance on disambiguat-ing people in existing, widely-used benchmarks.While modern data are not our main focus,our models do reasonably well.In particular,LinkNewsWikipedia achieves a near-perfect 98%accuracy on MSNBC. LinkWikipedia has 89%accuracy, as compared to the next best (GENRE)with 84% accuracy. This suggests that our mod-els - beyond being suited to historical applications- can also be well-suited to disambiguating mod-ern news. We also beat other models on the newsdataset ACE2004, which has very few people. Onother modern benchmarks, there are model(s) thatperform better, but our performance is in the rangeof the other models.",
  "LinkWikipedia26.562.673.873.874.0NewsLinkWikipedia26.569.177.977.978.3": ": Ablations. The first column uses a Sentence-BERT MPNet model, which we use to initialize training.The second column discards the coreferencing step, aswell as birthdate filtering and Qrank re-ranking. Thenext three columns add back coreference resolution,birthdate filtering, and Qrank re-ranking, respectively. We conduct ablations in , to quantify thecontributions of different elements in our disam-biguation pipeline. Base MPNet disambiguateswith a Sentence-BERT (Reimers and Gurevych,2019) MPNet (Song et al., 2020) model (all-mpnet-base-v2), the base model that we initialize with.This model is not intended for entity disambigua-tion, but we include it to quantify how much isgained through our training. Performance is verypoor. The next column reports results from ourtrained disambiguation models, without corefer-ence resolution or additional processing steps: fil- tering entities in the knowledge base to be bornprior to the end date of the corpus and re-rankingby Wikipedia QRank when the nearest entities arevery close to each other. Accuracy falls relativeto the baseline by around ten percentage pointsin both models. Adding coreference resolutionrestores almost all of this decline, with birthdatefiltering and Qrank re-ranking contributing little.Coreference resolution plausibly combines infor-mation across mentions and reduces noise, leadingto overall better quality disambiguation.",
  "Exploring Entities in Historical News": "To give a flavor of how our framework can be com-bined with historical documents, we apply our dis-ambiguation pipeline to a large-scale corpus of his-torical newswire articles - sent out over newswiressuch as the Associated Press between 1878 and1977 (Silcock et al., 2024). The dataset contains2.7 million unique articles, reproduced in a corpusof local news over 32 million times.We disambiguate 15,323,463 person mentions,encompassing 61,933 unique individuals. Only4.6% of disambiguated entity mentions refer towomen, with Golda Meir being the most mentionedwoman. The most mentioned entity is DwightD. Eisenhower, appearing in 9,530 unique articleswhich are reproduced an average of 33.7 times.Richard Nixon, Harry S. Truman, and Adolf Hitlerare the next most mentioned in unique newswirearticles. plots their mention counts. ForU.S. presidents, electoral cycles are clearly visible.Entity disambiguation also allows us to see in-dividuals occupations in Wikidata.The mostcommon occupations of disambiguated entities arepolitician, military officer and lawyer.This dataset, also publicly available, providesfascinating data that researchers can use to studywho appeared in historical news and which of these",
  ": Mentions against Wikipedia Qrank": "individuals are remembered via Wikipedia today.To examine whether we are able to detect lessprominent entities as well as prominent entities,we plot log mentions by decile in the large scalenewswire dataset (Silcock et al., 2024) - a measureof how prominent individuals were in the news his-torically - against log Wikidata Qrank by decile,showing the correlation between these two mea-sures.We find a virtually linear relationship, showingthat we do not underdetect less prominent entities,or overdetect prominent entities.",
  "Conclusion": "We provide new data for training and evaluat-ing entity coreference and disambiguation. Wepropose bi-encoder models trained on this data,which achieve high accuracy for disambiguatingand coreferencing entities in historical documents,as well as on existing entity disambiguation bench-marks.Our models are able to handle out-of-knowledge base individuals.Our data provide high-quality resources for otherresearchers developing methods for entity corefer-",
  "Limitations": "The present paper focuses on individuals and dis-ambiguation to Wikipedia/Wikidata. We found dis-ambiguating locations to Wikipedia to be unproduc-tive, as we could achieve very strong performanceusing sparse methods and Geonames, a more com-prehensive database of locations. In the future, wehope to extend the model to organizations (thoughin practice, many historical organizations end upbeing out-of-knowledge base).While Wikipedia is extensive, there are manypeople who never entered this knowledge base,and various biases may influence which histori-cal figures are remembered in Wikipedia. Part ofthe objective of applying LinkNewsWikipedia toa massive scale corpus of historical newswires is tounderstand more about which individuals were con-sidered broadly newsworthy at the time but havesince been forgotten. This information could beused to expand databases such as Wikipedia in thefuture. Nevertheless, we cannot disambiguate anindividual who is not in the knowledge base, nomatter how noteworthy they were historically.",
  "Ethical Considerations": "This study presents no major ethical concerns. Itsmethods are entirely open source, and its trainingdata are entirely in the public domain. We disam-biguate individuals in widely reproduced historicalnewspaper articles, which are in the public domainand hence do not pose privacy concerns.It is possible that some applications could raiseconcerns. Historical news, government publica-tions, and other historical documents reflect thebiases of their time and may contain factual in-accuracies or offensive content. Moreover, whileour models are reasonably accurate, they are notperfect, and depending on the usage of the output,human revision of the match - potentially bringingin additional information - may be required. It isimportant to interpret the disambiguated texts criti-cally, as is the norm in rigorous historical research.",
  "Sensitivity to inference hyperparameters": "The two main hyperparameters at inference timeare the clustering threshold used in the coreferencestep, and the no-match threshold used in the dis-ambiguation step. In figure 4 in the main text, wedemonstrate that our results are not particularlysensitive to other choices.Here we examine sensitivity of our resultsto choices of clustering threshold and no-matchthreshold, with our fine-tuned disambiguationmodel. graphs the disambiguation accuracy onthe test set of Entities of the Union for differ-ent choices of clustering threshold. The results arequite flat around the threshold that we chose, basedon the validation set. shows sensitivity of these same resultsto choice of no-match threshold. The results are notespecially sensitive. The threshold can be changed2 percentage points with little change to the re-sults. In fact, we can see here that the thresholdthat we chose was not in fact the optimal threshold,but this makes minimal difference to our overallresults.",
  "It also shows a code starting with Q. Thisis the unique reference for the person inWikidata. You can query search this there( and pull up more": "detail about the person, including informationon positions theyve held, what else theyvedone etc. Most of these people also appear onWikipedia, so this is also a good source forfinding out more info about the person. Then there are (up to) 32 passages of text.In each of these there is a highlighted term.Your job is to label each of these passages forwhether the highlighted term is the same asthe person in the box at the top (positive) ornot (negative). In some cases this will bepretty clear, and in some cases it might need abit of digging.",
  "Entities of the Union annotatorinstructions": "To create Entities of the Union, 1,137 entitymentions across 157 newswire articles were double-annotated by undergraduate research assistants. An-notator labeling instructions were as follows:Weve pulled out lots of articles from the dayof the State of the Union speech in 1958, 1959,1960 and 1961. A team of RAs over this semesterhas been labeling the spans in these texts that re-fer to an entity. What we want you to focus on isworking out which unique entity these spans referto. The database of unique identifiers that we willwork from is Wikidata ( you search for entities on here, you will seethey have a unique identifier beginning with Q (eg.Melissas is Q58009782 ). For each entity in thearticles wed like you to find the unique identifier inwikidata (if it exists). It might also be useful to usewikipedia in difficult cases - all pages on wikipediawill have an entry in wikidata, but wikidata is big-ger than wikipedia, so there might be some entitiesin wikidata that you dont find in wikipedia [...]theres 14 entities to label on average per article.The entity in question should be highlighted in thetext.Annotators were encouraged to reach out withquestions and clarifications.",
  "Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019.Billion-scale similarity search with gpus.IEEETransactions on Big Data, 7(3):535547": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020.Dense passage retrieval foropen-domain question answering. arXiv preprintarXiv:2004.04906. Nora Kassner, Fabio Petroni, Mikhail Plekhanov, Se-bastian Riedel, and Nicola Cancedda. 2022. Edin:An end-to-end benchmark and pipeline for unknownentity discovery and indexing.arXiv preprintarXiv:2205.12570. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. PyTorch:An Imperative Style, High-Performance Deep Learn-ing Library. In Advances in Neural Information Pro-cessing Systems 32, pages 80248035. Curran Asso-ciates, Inc.",
  "Ikuya Yamada, Koki Washio, Hiroyuki Shindo, and YujiMatsumoto. 2019. Global entity disambiguation withpretrained contextualized embeddings of words andentities. arXiv preprint arXiv:1909.00426": "Ikuya Yamada, Koki Washio, Hiroyuki Shindo, andYuji Matsumoto. 2022. Global entity disambiguationwith bert. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 32643271. Klim Zaporojets, Lucie-Aime Kaffee, Johannes Deleu,Thomas Demeester, Chris Develder, and Isabelle Au-genstein. 2022. Tempel: Linking dynamically evolv-ing and newly emerging entities. In Thirty-sixth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track."
}