{
  "Abstract": "The proliferation of Internet memes in the ageof social media necessitates effective identifi-cation of harmful ones. Due to the dynamicnature of memes, existing data-driven modelsmay struggle in low-resource scenarios whereonly a few labeled examples are available.In this paper, we propose an agency-drivenframework for low-resource harmful meme de-tection, employing both outward and inwardanalysis with few-shot annotated samples. In-spired by the powerful capacity of Large Mul-timodal Models (LMMs) on multimodal rea-soning, we first retrieve relative memes withannotations to leverage label information asauxiliary signals for the LMM agent. Then,we elicit knowledge-revising behavior withinthe LMM agent to derive well-generalized in-sights into meme harmfulness. By combiningthese strategies, our approach enables dialecti-cal reasoning over intricate and implicit harm-indicative patterns. Extensive experiments con-ducted on three meme datasets demonstrate thatour proposed approach achieves superior per-formance than state-of-the-art methods on thelow-resource harmful meme detection task.",
  "Introduction": "The rise of social media has catalyzed the emer-gence of a new multimodal entity: the meme. Typ-ically, a meme combines a visual element withconcise text, making it highly sharable and capableof quick proliferation across various online plat-forms. Although often viewed humorously, memescan become vehicles of harm when their mix ofimage and text is strategically used in the contextof political and socio-cultural divisions.A widely accepted definition of harmful memes1",
  ": Example of trending memes on social media.Meme text: (a) DONT WORRY, IM WORKING ONA MURDER HORNET VACCINE.; (b) ARREST BILLGATES": "embedded text that have the potential to cause harmto an individual, an organization, a community, orsociety in general (Sharma et al., 2022). For exam-ple, during the COVID-19 pandemic, a frequentlyshared meme shown in (a) was createdby anti-vaccination groups using a manipulatedimage of Bill Gates. The widespread dissemina-tion of this multimodal content2, which spread fearabout COVID-19 vaccines, significantly harmedBill Gates personal reputation and underminedefforts to bolster public immunity. Therefore, it be-comes imperative to develop automatic approachesfor harmful meme detection to effectively unveilthe dark side of memes on social media.Previous studies employing deep neural net-works (DNNs) have advanced harmful meme de-tection (Kiela et al., 2020; Pramanick et al., 2021a)by utilizing data-driven multimodal models (Pra-manick et al., 2021b; Cao et al., 2022; Lin et al.,2023a), which rely heavily on extensively anno-tated data for training. However, these DNN-basedapproaches (Cao et al., 2023; Lin et al., 2024b)encounter significant challenges when it comes todetecting newly emerging memes related to break-ing events, since the rapid development of suchharmful memes makes it impractical to swiftlyannotate enough training data (Cao et al., 2024).Moreover, the dynamic and evolving nature of In- ternet memes raises concerns about the sustainedeffectiveness of traditional data-driven methods forharmful meme detection in real-world scenarios.This highlights the need for more adaptive and re-sponsive approaches in the detection of harmfulmemes under the low-resource regime where onlylimited few-shot meme annotations are available.Intuitively, the key to low-resource harmfulmeme detection is to strengthen the adaptabilityto continually changing online memes, and the ca-pacity to respond promptly to new trends and con-texts: 1) We posit that the shared characteristicsamong similar memes could facilitate the adaptabil-ity of harmful meme detection, as memes evolveby retaining inherent patterns. For instance, as de-picted in , Bill Gates shown in the memeof (a) could be further incorporated with ajoker face, which is extended to create a new harm-ful variant of (b). 2) On the other hand,rather than using gradient descent to update modelparameters, we resort to imitating human learn-ing processes that accumulate experience withoutmodifying the models weights, thus mitigating therisk of overfitting to sparse annotations of harmfulmemes. As an example, to illustrate the harmful-ness of the memes in , a human checkerneeds the reasoning knowledge to gather the experi-ence that Bill Gates is frequently vilified in harmfulmemes by anti-vaccination campaigners due to hisadvocacy for vaccine development. Thus we devisea gradient-free approach to capture common fea-tures of harmful memes and derive insights fromlimited annotated training meme data for bettergeneralization in a low-resource learning context.Inspired by the powerful capacity of LMMsfor reasoning with contextual background knowl-edge (Brown et al., 2020; Liu et al., 2023a), wepropose a novel agentic approach: LOREHM, to-wards Low-Resource Harmful Meme detection byregarding LMMs as agents. To this end, we proposecapturing the harmfulness of memes by employingboth outward and inward analysis with limited few-shot annotated meme samples. Specifically, 1) forlooking outward at a meme, we first retrieve its sim-ilar memes with labels to leverage the annotationinformation as explicit auxiliary signals for harm-fulness preference, thereby facilitating the decision-making of the LMM agent. 2) In terms of lookinginward at a meme, we employ the fundamentalskill of human learning, positioning the LMM asa learner agent to derive well-generalized insightsfrom its failed attempts on the limited annotated meme samples, which aims to capture the implicitharmfulness meanings not conveyed through thesuperficial texts and images of memes. 3) Finally,we combine the strategies of looking outward andinward to enable the LMM agent to perform thefinal harmfulness inference. In this manner, weenhance the LMMs ability as a trustworthy agentto detect harmful content concealed in the intrinsicmultimodal information in memes. Our contribu-tions are summarized as follows in three folds:",
  "To our best knowledge, we are the first to alle-viate the low-resource issue of harmful memedetection from a fresh gradient-free perspec-tive on harnessing advanced LMMs.3": "We propose a novel agency-driven approachfor low-resource harmful meme detection, toaugment the LMM agent with harm-indicatedsignals from retrieval of outward relativelysimilar memes and insight of inward multi-modal knowledge-revising, which facilitatesharmfulness inference in a few-shot regime. Extensive experiments conducted on threememe datasets confirm that our agenticparadigm could yield superior few-shot perfor-mance than previous state-of-the-art baselinesfor low-resource harmful meme detection.",
  "Harmful Meme Detection": "Harmful meme detection is an expanding field, bol-stered by large meme benchmarks (Kiela et al.,2019; Pramanick et al., 2021a; Lin et al., 2024c),and initiatives like the Hateful Memes Chal-lenge (Kiela et al., 2020) by Facebook for detectingmemes in hate speech (Das et al., 2020). These de-velopments have spurred research into detectingharmful memes (Pramanick et al., 2021a), a taskcomplicated by their multimodal nature, which of-ten involves both texts and images. As unimodalmethods like BERT (Devlin et al., 2019) or FasterR-CNN (Ren et al., 2016) fall short in addressingthese complexities, recent studies have increasinglyturned to multimodal approaches to improve thedetection performance of harmful memes.Previous studies have employed classical two-stream models that integrate textual and visual fea-tures, which are learned from text and image en-coders, typically utilizing attention-based mecha-",
  "Our source code is available at": "nisms and multimodal fusion techniques for clas-sifying harmful memes (Kiela et al., 2019, 2020;Suryawanshi et al., 2020; Pramanick et al., 2021b).Another branch was fine-tuning pre-trained mul-timodal models specifically for the task (Lippeet al., 2020; Muennighoff, 2020; Velioglu and Rose,2020; Hee et al., 2022). Recent efforts have alsosought to explore the use of data augmentationtechniques (Zhou et al., 2021; Zhu et al., 2022),ensemble methods (Zhu, 2020; Velioglu and Rose,2020; Sandulescu, 2020), harmful target disentan-glement (Lee et al., 2021), and prompt-based tun-ing (Cao et al., 2022; Ji et al., 2023; Cao et al.,2023). Lately, Lin et al. (2023a) proposed to distillmultimodal reasoning knowledge from Large Lan-guage Models (LLMs) to detect harmful memes.However, such data-driven approaches fail todetect harmful evolving memes in low-resourceregimes because they often require sizeable train-ing data unavailable for emerging events. Althougha recent work (Cao et al., 2024) employed low-rankadaptation (LoRA) (Hu et al., 2021) for detectingharmful memes in the few-shot setting, we delveinto the low-resource harmful meme detection with-out updating any model weights, which utilizesmultimodal LLMs (i.e., LMMs) in an agency-driven manner by leveraging both LMMs internalknowledge and multimodal retrieval-augmentedgeneration to examine the harmfulness of a meme.",
  "LLM Agent": "Retrieval-augmented generation enriches the inputspace of LLMs with retrieved text passages (Guuet al., 2020; Lewis et al., 2020), resulting in signif-icant enhancements in knowledge-intensive tasksand decision-making agents, either through fine-tuning or utilization with off-the-shelf LLMs (Liuet al., 2022; Schick et al., 2023; Ram et al., 2023;Izacard et al., 2023; Jiang et al., 2023; Asai et al.,2023; Gao et al., 2023; Wang et al., 2023c). The in-tegration of LLMs as agents spans various domains,including code generation and game-playing, show-casing their robust planning and reasoning capabil-ities in diverse settings (Wang et al., 2023a; Yaoet al., 2022; Shen et al., 2023; Mu et al., 2023; Honget al., 2023; Liu et al., 2023b; Zhao et al., 2024; Sunet al., 2023; Qian et al., 2023). These advancementsunderscore the ability of LLMs to tackle complextasks with minimal supervision. Concurrently, self-improvement methodologies (Chen et al., 2022,2023; Shinn et al., 2024; Madaan et al., 2023) haveemerged, leveraging feedback-driven processes to iteratively refine generated outputs. In contrast tomany previous agent tasks that operate within envi-ronments providing real feedback, harmful memedetection lacks an environment that supplies theagent with authentic responses. In this work, wefocus on a novel agentic paradigm by devisingthe proprietary integration of the vision-languageretrieval-augmented and self-improvement mecha-nisms for LMMs (Liu et al., 2023a; OpenAI, 2023),to detect harmful memes with limited few-shot an-notations, a realistic yet urgent task that is inher-ently a binary multimodal classification challenge.",
  "Overview": "Problem Statement.We define a harmful memedetection dataset as a set of memes where eachmeme M={I, T , y} is a triplet represent-ing an image I that is associated with a textT , and a ground-truth harmfulness label y {harmful, harmless}. In this work, to investigatelow-resource harmful meme detection with LMMagents, we convert the harmful meme detectiontask into a natural language generation paradigm,where our model takes the image I and text T asthe input and generates a text sequence to clearlyexpress whether the meme is harmful or not.The scarcity of high-quality labeled data is grad-ually becoming the norm with the rapid mergingand evolution of memes (Sharma et al., 2022). Inthis work, we define a low-resource setting wherewe have a test set Stest, and a very small training setStrain comprising N samples. Since our method isgradient-free, we denote Strain as the reference setSref. Our objective is to optimize the model basedon the very limited reference set Sref, enabling it toeffectively generalize to the test dataset Stest.Our core idea is to regard the LMM as an agent,and then enhance the agents memory by usinginformation from relative memes and insights de-rived from the agents past unsuccessful evaluationsof memes, thereby improving its performance onharmful meme detection in low-resource scenes.We enable the LMM agent to fully capitalize on avery limited labeled dataset through two strategies:1) Relative Sample Augmentation (3.2) for learn-ing by looking outward, and 2) Meme Insight Aug-mentation (3.3) for learning by looking inward.Both strategies allow for the efficient extraction ofharmfulness cues from limited annotated memes.This empowers the LMM agent to perform more ExperienceGathering",
  "Retrieval": "TextEncoder(TE) 1. Evaluate the potential for the meme to offendor harm specific groups of people, and considerthe broader societal impact of normalizingharmful stereotypes or behaviors. 2. Evaluate if the humor in the meme relies onsexual innuendo or objectification, and considerthe potential harm of normalizing such content. 3. ... insight set MultimodalEmbedding",
  "Set": "Meme 26, Similarity Score: 0.708Reference Meme 11, Similarity Score: 0.702Reference Meme 20, Similarity Score: 0.684Reference Meme 21, Similarity Score: 0.646Reference Meme 7, Similarity Score: 0.642Reference Meme 10, Similarity Score: 0.638 ... Available actionsadd downvote edit upvote Agent: Thought: ...This imagery and text combination trivializes domestic violence, implying that a woman's place is inthe kitchen and suggesting a correlation between disobedience and physical punishment. Such content perpetuatesharmful stereotypes about gender roles and domestic violence. The humor here is based on sensitive and protectedcharacteristics, specifically gender, and it desensitizes viewers to the serious issue of domestic violence...Answer: harmful. User: Given the meme, with the Text: \"I GOT A BLACK EYE... I STAY IN THE KITCHEN NOW. Black eye\"embedded in the image, your task is to determine whether this meme is harmful or not... A classifier that can identify common features among multiple memes has labeled this meme as harmful. Pleasereview the classifier's judgment carefully and use your extensive knowledge to analyze and understand this memebefore providing your final verdict ... Note:1. Evaluate the potential for the meme to offend or harm specific groups of people, and consider the broader societalimpact of normalizing harmful stereotypes or behaviors.2. ...",
  "Experience": "pool User: Given the meme, with the Text: \"When acougar finally ensnares a young virgin First time?\"embedded in the image, your task is to determinewhether this meme is harmful or not...Agent: Thought: ...The meme leverages humorbased on the cultural trope of a \"cougar\" andmakes light of a sexual encounter, drawing aparallel to a life-threatening scenario in the moviefor comedic effect. ...While the meme plays onstereotypes and may be seen as distasteful orinappropriate by some, it does not seem topromote or endorse harmful behavior or attitudesdirectly. Answer: harmless. ... ... RelativeMemes Ground Truth Label:",
  "Relative Sample Augmentation": "With the aid of LMMs, it becomes plausible to offera powerful few-shot performance of low-resourceharmful meme detection by In-context Learning(ICL) (Brown et al., 2020). However, the ICL ap-proach involves integrating few-shot demonstra-tions with test instances repeatedly, which incurssubstantial computational overhead at each infer-ence step. This can be impractically burdensomefor real-world applications. Generally, Internetmemes evolve dynamically yet exhibit inherentpatterns (Baran, 2001). In this section, drawing in-spiration from retrieval-augmented generation, wepropose to capitalize on the highly shared charac-teristics of harmful memes to enhance the detectionperformance of LMMs in the low-resource setting.Specifically, our approach utilizes explicit labels oftop-retrieved meme samples, derived from multi-modal information retrieval, as auxiliary signals.",
  "where Emb is the multimodal embedding of M,VE() and TE() represent the frozen pre-trainedvision and text Transformer encoders, respectively,": "while and denote the ratio factors. Then, wecan encode all the meme samples from both thereference set Sref and the test set Stest as Equation 1,to obtain the fixed embeddings of meme samples.Given a target meme Mtest in the test set Stest, toretrieve the most relative candidate meme samplesin the reference set Sref to the target meme, wecalculate the similarity between the embedding ofthe target meme and the embedding of each memeMref in Sref as follows:",
  "d = sim(Mref, Mtest),(2)": "where d represents the normalized cosine similar-ity score between the meme Mref in the referenceset Sref and the target meme Mtest. The functionsim() computes the similarity by comparing themultimodal embeddings of a pair of memes. Thusall the similarity scores of the candidate memes inthe reference set to the target meme could form thesimilarity vector D = {d | Mref Sref} RN.Afterwards, we select the top K memes4 fromSref with the highest similarity scores as the relativememes to the target meme Mtest:",
  "H = {Mref | d TopK(D)},(3)": "where H is the set of the K memes from Srefmost similar to the target meme Mtest. The func-tion TopK() ranks and selects the top K highestscores from the set D of similarity scores, therebyidentifying the K most relevant memes in the ref-erence set Sref to the target meme Mtest.",
  "Note that K should be an odd number (K < N)": "3.2.2Voting MechanismThe memes in the retrieved set H exhibit a high de-gree of multimodal similarity with the target meme.This similarity is a crucial indicator of the com-mon salient multimodal features shared betweenthe memes, which are often products of meme evo-lution and integration. Therefore, we exploit theharmfulness of the retrieved set to explicitly inferthe predicted harmfulness label of the target meme.Based on the retrieved set H where the K memesare well-annotated, we employ a voting mechanismto obtain a preliminary prediction for the targetmeme according to the polarity of the ground-truthlabels of memes in the retrieved set:",
  "Meme Insight Augmentation": "Although Relative Sample Augmentation as 3.2can enhance the LMM agents ability by provid-ing explicit harmfulness references as prior for thetarget meme, previous literature has shown thatmemes generally contain implicit harmful infor-mation that is more subtle and challenging for theLMM to identify (Lin et al., 2023a). To addressthis, we propose enabling the LMM agent to bettercapture implicit harmfulness in memes by lookinginward through multimodal knowledge revising. 3.3.1Experience GatheringTo gather the experience that serves as the basis forinsight extraction, we first evaluate the harmfulnessof each Mref in the reference set Sref in a zero-shot manner. For performance and explainability,we choose Chain-of-Thought (CoT) as the baseplanning algorithm (Kojima et al., 2022) and feedthe image Iref and text Tref of Mref to the agent: traj = {LMM(XCoT, Iref, Tref) | Mref Sref},(5)where traj is the trajectories gathered through theLMM agents direct judgment on the meme Mrefin Sref, LMM() denotes the LMM agent, XCoT isthe zero-shot CoT instruction designed as: Given the meme, with the Text: {Tref} embed-ded in the image {Iref}, your task is to determinewhether this meme is harmful or not, in order tomaintain the harmlessness and integrity of informa-tion on the Internet. Please leverage your extensiveknowledge to deeply analyze and understand thismeme, and give your final judgment. Your outputshould strictly follow the format: {Thought: [Youranalysis] Answer: [harmful/harmless]}.Furthermore, in order to make the LMM agentsself-reflection more accurate and efficient, we ob-tain a self-reflect set that contains only erroneoustrajectories by comparing the answer indicated bythe trajectory traj and the ground truth label yrefcorresponding to Mref as follows:",
  "Rset = {traj |traj = yref, Mref Sref},(6)": "where Rset is a self-reflect set that contains n er-roneous trajectories (n < N),traj is the answerindicated by the trajectory traj. The self-reflectset Rset could be regarded as an experience pool toprioritize challenging meme examples in the refer-ence set Sref, which were misjudged by the zero-shot inference of the LMM agent, while avoidingexcessive attention to trivial examples that havealready been correctly detected by the LMM agent. 3.3.2Insight ExtractionTo derive general insights about the harmfulnessmeaning of memes, we first initialize an empty setof insights, which is denoted as E0, then iterativelyfeed the failed trajectories from the experience pool(i.e., the self-reflect set Rset) into the LMM, prompt-ing the LMM to reflect on these trajectories.Specifically, for gaining general insights intolow-resource harmful meme detection, rather thanobsessed with a specific failed meme, we promptthe LMM to perform a series of operations (ADD,DOWNVOTE, UPVOTE, EDIT) on the insight set.This approach allows for a broader understandinginstead of directly outputting simple insights basedon the erroneous trajectory of a particular memein Rset. In each iteration, the LMM takes a trajec-tory from the experience pool, then analyzes thereasons for its failure, and in conjunction with thecurrent insight set, determines the operations to beperformed on the current insight set as follows:",
  "Ei = Oi(Ei1).(8)": "The performed operations include: ADD, to in-troduce a new generic insight; DOWNVOTE, todownvote an existing insight; UPVOTE, to agreewith an existing insight; and EDIT, to modify thecontents of an existing insight. An added insightwill have an initial importance count, which willincrement if subsequent operations UPVOTE orEDIT are applied to it, and decrement if DOWN-VOTE. We denote En as the final insight set.",
  "Inference": "For the given target meme Mtest, we can attainboth the preliminary prediction P in 3.2 and theinsight set En in 3.3. Finally, the agent utilizesthe preliminary assessment P as prior, under theguidance of the insight set En, to evaluate whetherthe meme is harmful or not, culminating in a finaljudgment output as LMM(XCoT, Itest, Ttest, P, En).We set the number N of memes in the referenceset Sref as 50, and the number K of memes in theretrieved set H as 5. We select LLaVA-34B (Liuet al., 2023a) and GPT-4o (OpenAI, 2023) as thetwo representative backbones of the LMM agent.",
  "DatasetsWe use three publicly available memedatasets for evaluation: (1) HarM (Pramanick et al.,2021a), (2) FHM (Kiela et al., 2020), and (3)MAMI (Fersini et al., 2022). HarM consists of": "memes related to COVID-19. FHM was releasedby Facebook as part of a challenge to crowd-sourcemultimodal harmful meme detection in hate speechsolutions. MAMI encompasses a dataset of memesthat are predominantly derogatory towards women,exemplifying typical subjects of online vitriol. Dif-ferent from FHM and MAMI, where each memewas labeled as harmful or harmless, HarM wasoriginally labeled with three classes: very harmful,partially harmful, and harmless. For a fair com-parison, we merge the very harmful and partiallyharmful memes into the harmful class, followingthe setting of recent work (Pramanick et al., 2021b;Cao et al., 2022; Lin et al., 2023a). BaselinesWe compare LOREHM with severalstate-of-the-art (SoTA) systems for low-resourceharmful meme detection: 1) PromptHate (Caoet al., 2022); 2) MR.HARM (Lin et al., 2023a); 3)Pro-Cap (Cao et al., 2023); 4) OPT-30B (Zhanget al., 2022); 5) OpenFlamingo-9B (Awadallaet al., 2023); 6) Mod-HATE (Cao et al., 2024); 7)LLaVA-34B (Liu et al., 2023a); 8) GPT-4o (Ope-nAI, 2023); 9) LOREHM (*): Our proposed agen-tic approach based on LLaVA-34B and GPT-4o.We use the accuracy and macro-averaged F1 (dom-inant) scores as the evaluation metrics.The data statistics, baseline descriptions andmodel implementation are detailed in the AppendixA, B, and C, respectively.",
  ": Ablation studies on our proposed framework based on different LMM agents": "sure fair comparisons in the low-resource few-shotregime, all the baselines are trained using the sameamount of limited annotated meme data. 2) For thesecond group, the baselines are low-resource andLMM-based. Both OPT-30B and OpenFlamingo-9B are LMMs with pre-training, while Mod-HATEis based on the LoRA tuning specific to the task.We can find that neither general pre-training norspecific LoRA tuning could enhance the perfor-mance of low-resource harmful meme detectionin the few-shot setting. 3) In terms of the twoof the most powerful cutting-edge LMMs in thethird group, LLaVA-34B is the representative open-source LMM with instruction tuning while GPT-4o is closed-source and enhanced by reinforce-ment learning with human feedback (Ouyang et al.,2022). Compared with the baselines in the sec-ond group, the two SoTA LMMs by few-shot ICLprompts show performance improvement in gen-eral, since their advanced training strategies op-timize alignment with human values and betteradapt to in-context learning. Meanwhile, GPT-4odemonstrates better performance than LLaVA-34B.4) By standing upon the shoulders of giants, ourproposed LOREHM could achieve superior perfor-mance than the LMM backbones (i.e., LLaVA-34B and GPT-4o) with few-shot ICL prompts,which notably improves over GPT-4o by 2.75%,4.40%, and 2.46% in terms of macro-averagedF1 score on HarM, FHM, and MAMI. Overall,our LOREHM based on both representative open-source and closed-source LMMs showcases consis-tent and adaptable performance across all bench-mark datasets for harmful meme detection, thanksto its astute discernment of harmful memes in thelow-resource few-shot setting.",
  "Ablation Study": "We perform ablative studies by adding theparadigms on LMM agents to draw more insightfulcomparisons among variants of LMMs, as shownin . LLaVA-34B and GPT-4o are selectedas the representative LMMs from the open-sourceand closed-source perspectives. We devise fivevariants of paradigms based on LMM agents forlow-resource harmful meme detection: 1) w/ 0-shotPrompt: Directly prompt a representative LMM,to infer harmfulness for harmful meme detection;2) w/ 50-shot ICL: Prompt the LMM with 50-shotICL demonstrations, the similar setting to the third-group baselines in ; 3) w/ Relative SampleAugmentation (RSA): Augment the LMM agentby looking outward with the label information ofthe retrieved memes as prior; 4) w/ Meme InsightAugmentation (MIA): Augment the LMM agent bylooking inward through mimicking the knowledge-revising behavior of human problem-solving skillsto get general insights into the meme harmfulness;5) w/ LOREHM: Our proposed LOREHM basedon the full integration of the RSA&MIA strategies.We have the following observations: 1) The di-rect deployment w/ 0-shot Prompt on LLaVA-34B and GPT-4o struggles since the models are notspecifically designed for this task. 2) The 50-shotICL prompting strategy could effectively enhancethe detection performance of LMMs, though therepeated combination of 50-shot examples withtest instances incurs significant computational over-head during each inference step. 3) The overall per-formances of the w/ RSA and w/ MIA settingsare equally matched even slightly improved whencompared with the w/ 50-shot ICL setting. How-ever, LLaVA-34B w/ RSA on FHM and MAMIdatasets does not perform well, for which we spec-ulate the reason is that LLaVA-34B tends to be-lieve the prior judgment given by the RSA strategy",
  "Meme Text: THE LAST TIME IMARCHED IT WAS DOWN THE AISLE": "(a) Output Thought:...The humor derives from the juxtaposition of batsand rats, suggesting that rats are relieved or happybecause bats have taken their place as the mosthated animal. This meme is light-hearted and relieson anthropomorphism to create a humoroussituation....Answer: Harmless (c) Output Thought:...The meme does reinforce a stereotype of womenbeing confined to domestic roles and suggests thatthe woman's only significant action was gettingmarried. This can be harmful as it trivializeswomen's contributions outside of traditionaldomestic roles and may perpetuate the idea thatwomen should conform to these outdatednorms.The meme's reinforcement of traditionalgender roles, combined with its potential totrivialize modern feminist movements and thestruggle for gender equality, outweighs itshumorous intent. Therefore, while it might not beovertly aggressive, the subtle perpetuation ofharmful stereotypes makes it potentiallydamaging...Answer: Harmful",
  ": Examples of correctly predicted memes in (a)HarM, (b) FHM, and (c) MAMI datasets": "instead of more insight reflection as GPT-4o. 4)Moreover, the full paradigm w/ LOREHM couldyield an absolute performance advantage ratherthan using either w/ RSA or w/ MIA alone, whichimplies the proposed two strategies are not incre-mental but complementary each other. On the otherhand, the w/ LOREHM setting achieves excellentand robust performance based on both LLaVA-34Band GPT-4o, which demonstrates that the choiceof LMMs is orthogonal to our proposed agenticparadigm that can be easily augmented with newlyreleased stronger LMMs without any other change.",
  "Case Study": "To better understand how the LMM agent deciphersthe test meme samples, we conduct a case studyon the output thought of the LMM agent for thecorrectly predicted samples, where we show impor-tant content in the thought and truncate others. asexemplified in .From the LMM agents output thought in thenatural text, we can observe that: 1) The agenteffectively links multimodal information relatedto the meme text and image using commonsenseknowledge. For example, in (a), rats arerelieved or happy in the thought could be linkedto the dancing rats in the image, and the juxtaposi-tion of bats and rats in the thought could be linkedto Bats and Rats in the text; In (b), K Macro F1 Score FHM(LLaVA-34B)HarM(LLaVA-34B)MAMI(LLaVA-34B) FHM(GPT-4o)HarM(GPT-4o)MAMI(GPT-4o) N Macro F1 Score FHM(LLaVA-34B)HarM(LLaVA-34B)MAMI(LLaVA-34B) FHM(GPT-4o)HarM(GPT-4o)MAMI(GPT-4o)",
  ": Effect of TopK and N-shot": "Down syndrome in the thought could be linked toextra chromosome in the text; and in terms of Fig-ure 3(c), women being confined to domestic rolesin the thought could be linked to a woman doinghousework in the image. 2) Furthermore, the agentdemonstrates advanced reasoning by consideringthe interplay of multimodal information. In Fig-ure 3(a), the thought takes into account the culturalcontext and the potential emotional response of theviewer, ultimately appreciating the light-heartedintent behind the memes creation; The thought in(b) explores the implications of makinglight of a serious issue, recognizing the potentialharm in perpetuating stereotypes and insensitivitytowards individuals with Down syndrome; For Fig-ure 3(c), the thought scrutinizes the reinforcementof domestic role stereotypes, evaluating the poten-tial for such stereotypes to desensitize viewers tothe importance of gender equality. In this way, therich but implicit correlations between the memetext and image are explained in readable snippets,which can be potentially valuable for aiding humancheckers in verifying model predictions in the low-resource setting. We also provide more case studiesand error analysis in the Appendix H and I.",
  "Effect of Labeled Data Size": "To study the effects of the labeled data size in ouragency-driven paradigm, we conduct an analysis ofperformance with TopK (left) and N-shot (right)as shown in . We use the control variablemethod to adjust the values of K and N, respec-tively. It is observed that: 1) As K increases, thegap between different backbone variants decreases.2) Despite increasing the number N of labeled ex-amples, performance plateaus or declines in someconfigurations, indicating that harmful meme detec-tion remains challenging in a few-shot framework.This suggests that simply adding more examples isnot enough. Innovative approaches and advancedtechniques are needed to better leverage few-shotmeme examples and capture multimodal subtleties.",
  "Conclusion and Future Work": "In this paper, we delved into the low-resource issueof harmful meme detection with limited few-shotannotations. To this end, we proposed an agency-driven and gradient-free approach, which seam-lessly integrated the Relative Sample Augmenta-tion and the Meme Insight Augmentation strate-gies to strengthen LMMs as a trustworthy agent.Comprehensive experiments and analyses confirmthe advantages of our agentic framework. Futureefforts aim to enhance our research by further re-lieving the inherent bias and variation in LMMs.",
  "There are multiple ways to further improve thiswork:": "Firstly, although harmfulness is defined muchbroader than hatefulness or misogyny in pre-vious literature (Pramanick et al., 2021a),in the future, we would try to incorporatemore of the related meme datasets beyondour task to further broaden the boundariesof this low-resource research, such as of-fensiveness (Suryawanshi et al., 2020), sar-casm (Lin et al., 2024a), and even code-mixeddata (Maity et al., 2022), etc. Secondly, in this work, we primarily focuson the few-shot setting to address the low-resource issue. We plan to further explorethe zero-shot setting, which presents an evenmore challenging aspect of the low-resourceproblem. Additionally, it is also crucial forour future research to understand meme datain the context of low-resource domains andlanguages on social media (Lin et al., 2022). Thirdly, while this work focuses on improvingfew-shot performance in low-resource harm-ful meme detection, assessing the quality ofthe extracted insights remains challenging andinherently qualitative. Given that our agenticframework can generate readable snippets forcognitive-view reasoning, we plan to conducta systematic study to evaluate and claim ex-plainability. This would constitute anothertargeted area of research.",
  "Ethics Statement": "The purpose of this work is to prevent the spread ofharmful meme information and to ensure that peo-ple are not subjected to prejudice or racial and gen-der discrimination. Nevertheless, we are aware ofthe potential for malicious users to reverse-engineerand create memes that go undetected or misunder-stood by AI systems based on LOREHM. This isstrongly discouraged and condemned. Interven-tion with human moderation would be requiredin order to ensure that this does not occur. Re-search indicates that evaluating harmful or hatefulcontent can have negative effects. Our proposedagency-driven paradigm with LMMs could gener-ate insightful thought, which can provide humanusers or checkers with dialectical thinking that al-lows them to better decode the underlying meaningof memes. Another consideration is the usage ofFacebooks meme dataset; users will have to agreewith Facebooks usage agreement to gain accessto the memes. The usage of Facebooks memesin this study is in accordance with its usage agree-ment. All the datasets only include memes and donot contain any user information.",
  "This work is partially supported by National Natu-ral Science Foundation of China Young ScientistsFund (No. 62206233) and Hong Kong RGC ECS(No. 22200722)": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. In Advances in NeuralInformation Processing Systems. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.In The Twelfth International Conference on LearningRepresentations. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.2023. Openflamingo: An open-source framework fortraining large autoregressive vision-language models.arXiv preprint arXiv:2308.01390. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966.",
  "Stanley Baran. 2001. Prologueframing public life: Abridging model for media research. Framing PublicLife: Perspectives on Media and Our Understandingof the Social World, page 7": "Sidney Black, Stella Biderman, Eric Hallahan, QuentinAnthony, Leo Gao, Laurence Golding, Horace He,Connor Leahy, Kyle McDonell, Jason Phang, et al.2022. Gpt-neox-20b: An open-source autoregres-sive language model. In Proceedings of BigScienceEpisode# 5Workshop on Challenges & Perspectivesin Creating Large Language Models, pages 95136. Tom B Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. In Proceedings of the 34th InternationalConference on Neural Information Processing Sys-tems, pages 18771901. Rui Cao, Ming Shan Hee, Adriel Kuek, Wen-HawChong, Roy Ka-Wei Lee, and Jing Jiang. 2023. Pro-cap: Leveraging a frozen vision-language model forhateful meme detection. In Proceedings of the 31thACM international conference on multimedia. Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and JingJiang. 2022. Prompting for multimodal hateful memeclassification. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 321332.",
  "Xinyun Chen, Maxwell Lin, Nathanael Schaerli, andDenny Zhou. 2023. Teaching large language modelsto self-debug. In The 61st Annual Meeting Of TheAssociation For Computational Linguistics": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scalinglanguage modeling with pathways. arXiv preprintarXiv:2204.02311. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Albert Li, Pascale Fung, and Steven C. H.Hoi. 2023. Instructblip: Towards general-purposevision-language models with instruction tuning.ArXiv.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. Bert: Pre-training of deepbidirectional transformers for language understand-ing. In Proceedings of NAACL-HLT, pages 41714186": "Elisabetta Fersini, Francesca Gasparini, Giulia Rizzi,Aurora Saibene, Berta Chulvi, Paolo Rosso, AlyssaLees, and Jeffrey Sorensen. 2022. Semeval-2022 task5: Multimedia automatic misogyny identification.In Proceedings of the 16th International Workshopon Semantic Evaluation (SemEval-2022), pages 533549. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.2023. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64656488.",
  "Ming Shan Hee, Roy Ka-Wei Lee, and Wen-Haw Chong.2022. On explaining multimodal hateful meme de-tection models. In Proceedings of the ACM WebConference 2022, pages 36513655": "Sirui Hong, Mingchen Zhuge, Jonathan Chen, XiawuZheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al.2023. Metagpt: Meta programming for multi-agentcollaborative framework. In The Twelfth Interna-tional Conference on Learning Representations. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Gautier Izacard, Patrick Lewis, Maria Lomeli, LucasHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and EdouardGrave. 2023. Atlas: Few-shot learning with retrievalaugmented language models. Journal of MachineLearning Research, 24(251):143.",
  "Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, EthanPerez, and Davide Testuggine. 2019. Supervisedmultimodal bitransformers for classifying images andtext. arXiv preprint arXiv:1909.02950": "Douwe Kiela, Hamed Firooz, Aravind Mohan, VedanujGoswami, Amanpreet Singh, Pratik Ringshia, andDavide Testuggine. 2020. The hateful memes chal-lenge: detecting hate speech in multimodal memes.In Proceedings of the 34th International Conferenceon Neural Information Processing Systems, pages26112624. Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-YenLo, et al. 2023. Segment anything. arXiv preprintarXiv:2304.02643. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In ICML 2022Workshop on Knowledge Retrieval and LanguageModels. Zhanghui Kuang, Hongbin Sun, Zhizhong Li, XiaoyuYue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei,Yiqin Zhu, Tong Gao, Wenwei Zhang, et al. 2021.Mmocr: a comprehensive toolbox for text detection,recognition and understanding. In Proceedings of the29th ACM International Conference on Multimedia,pages 37913794.",
  "Roy Ka-Wei Lee, Rui Cao, Ziqing Fan, Jing Jiang, andWen-Haw Chong. 2021. Disentangling hate in onlinememes. In Proceedings of the 29th ACM Interna-tional Conference on Multimedia, pages 51385147": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. In Proceedings ofthe 34th International Conference on Neural Infor-mation Processing Systems, pages 94599474. Hongzhan Lin, Zixin Chen, Ziyang Luo, MingfeiCheng, Jing Ma, and Guang Chen. 2024a. Cofipara:A coarse-to-fine paradigm for multimodal sarcasmtarget identification with large multimodal models.arXiv preprint arXiv:2405.00390. Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma,Bo Wang, and Ruichao Yang. 2024b. Towards ex-plainable harmful meme detection through multi-modal debate between large language models. InThe ACM Web Conference 2024, Singapore. Hongzhan Lin, Ziyang Luo, Jing Ma, and Long Chen.2023a.Beneath the surface: Unveiling harmfulmemes with multimodal reasoning distilled fromlarge language models. In The 2023 Conference onEmpirical Methods in Natural Language Processing.",
  "Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang,and Jing Ma. 2024c. Goat-bench: Safety insights tolarge multimodal models through meme-based socialabuse. arXiv preprint arXiv:2401.01523": "Hongzhan Lin, Jing Ma, Liangliang Chen, Zhiwei Yang,Mingfei Cheng, and Chen Guang. 2022. Detect ru-mors in microblog posts for low-resource domainsvia adversarial contrastive learning.In Findingsof the Association for Computational Linguistics:NAACL 2022, pages 25432556. Hongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang,Liangliang Chen, and Guang Chen. 2021. Rumordetection on twitter with claim-guided hierarchicalgraph attention networks. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 1003510047. Hongzhan Lin, Haiqin Yang, Ziyang Luo, and Jing Ma.2024d. Unleashing trigger-free event detection: Re-vealing event correlations via a contrastive derange-ment framework. In ICASSP 2024-2024 IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 1017110175. IEEE. Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang,Ziyang Luo, Shuming Shi, and Ruifang Liu. 2023b.Zero-shot rumor detection with propagation structurevia prompt learning. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 37, pages52135221. Phillip Lippe, Nithin Holla, Shantanu Chandra, San-thosh Rajamanickam, Georgios Antoniou, EkaterinaShutova, and Helen Yannakoudakis. 2020. A multi-modal framework for the detection of hateful memes.arXiv preprint arXiv:2012.12871. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, YuanhanZhang, Sheng Shen, and Yong Jae Lee. 2024. Llava-next: Improved reasoning, ocr, and world knowl-edge (january 2024). URL github.io/blog/2024-01-30-llava-next, 1(8).",
  "Men, Kejuan Yang, et al. 2023b. Agentbench: Eval-uating llms as agents. In The Twelfth InternationalConference on Learning Representations": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:Empowering code large language models with evol-instruct. In The Twelfth International Conference onLearning Representations. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,et al. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on NeuralInformation Processing Systems. Krishanu Maity, Prince Jha, Sriparna Saha, and Push-pak Bhattacharyya. 2022. A multitask framework forsentiment, emotion and sarcasm aware cyberbullyingdetection from multi-modal code-mixed memes. InProceedings of the 45th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 17391749. Yao Mu, Qinglong Zhang, Mengkang Hu, WenhaiWang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,Yu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision-language pre-training via embodied chain of thought.In Thirty-seventh Conference on Neural InformationProcessing Systems.",
  "OpenAI. 2023.Gpt-4 technical report.ArXiv,abs/2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Shraman Pramanick, Dimitar Dimitrov, RituparnaMukherjee, Shivam Sharma, Md Shad Akhtar,Preslav Nakov, and Tanmoy Chakraborty. 2021a. De-tecting harmful memes and their targets. In Find-ings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021, pages 27832796. Shraman Pramanick, Shivam Sharma, Dimitar Dim-itrov, Md Shad Akhtar, Preslav Nakov, and TanmoyChakraborty. 2021b. Momenta: A multimodal frame-work for detecting harmful memes and their targets.In Findings of the Association for ComputationalLinguistics: EMNLP 2021, pages 44394455.",
  "Chen Qian, Xin Cong, Cheng Yang, Weize Chen,Yusheng Su, Juyuan Xu, Zhiyuan Liu, and MaosongSun. 2023. Communicative agents for software de-velopment. arXiv preprint arXiv:2307.07924": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International con-ference on machine learning, pages 87488763. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context retrieval-augmented lan-guage models. Transactions of the Association forComputational Linguistics, 11:13161331. Shaoqing Ren, Kaiming He, Ross Girshick, and JianSun. 2016. Faster r-cnn: Towards real-time object de-tection with region proposal networks. IEEE Trans-actions on Pattern Analysis and Machine Intelligence,39(6):11371149.",
  "Vlad Sandulescu. 2020. Detecting hateful memes us-ing a multimodal deep ensemble. arXiv preprintarXiv:2012.13235": "Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2023.Toolformer: Language models can teach themselvesto use tools. In Thirty-seventh Conference on NeuralInformation Processing Systems. Shivam Sharma, Firoj Alam, Md Shad Akhtar, DimitarDimitrov, Giovanni Da San Martino, Hamed Firooz,Alon Halevy, Fabrizio Silvestri, Preslav Nakov, andTanmoy Chakraborty. 2022. Detecting and under-standing harmful memes: A survey. In Proceedingsof the Thirty-First International Joint Conference onArtificial Intelligence, pages 55975606. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,Weiming Lu, and Yueting Zhuang. 2023. Hugging-gpt: Solving ai tasks with chatgpt and its friendsin hugging face. In Thirty-seventh Conference onNeural Information Processing Systems. Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik Narasimhan, and Shunyu Yao. 2024. Re-flexion: Language agents with verbal reinforcementlearning. Advances in Neural Information Process-ing Systems, 36. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,and Chao Zhang. 2023. Adaplanner: Adaptive plan-ning from feedback with language models. In Thirty-seventh Conference on Neural Information Process-ing Systems. Shardul Suryawanshi, Bharathi Raja Chakravarthi, Mi-hael Arcan, and Paul Buitelaar. 2020. Multimodalmeme dataset (multioff) for identifying offensive con-tent in image and text. In Proceedings of the secondworkshop on trolling, aggression and cyberbullying,pages 3241.",
  "Anja Hauth, et al. 2023.Gemini: A family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Riza Velioglu and Jewgeni Rose. 2020. Detecting hatespeech in memes using multimodal deep learningapproaches: Prize-winning solution to hateful memeschallenge. arXiv preprint arXiv:2012.12975": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, andAnima Anandkumar. 2023a.Voyager: An open-ended embodied agent with large language models.In Intrinsically-Motivated and Open-Ended LearningWorkshop@ NeurIPS2023. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,Lei Zhao, Xixuan Song, et al. 2023b. Cogvlm: Vi-sual expert for pretrained language models. arXivpreprint arXiv:2311.03079. Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023c.Self-knowledge guided retrieval augmentation forlarge language models. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 1030310315. Zhengyuan Yang, Linjie Li, Kevin Lin, JianfengWang, Chung-Ching Lin, Zicheng Liu, and LijuanWang. 2023.The dawn of lmms:Preliminaryexplorations with gpt-4v (ision).arXiv preprintarXiv:2309.17421, 9(1). Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao. 2022.React: Synergizing reasoning and acting in languagemodels. In The Eleventh International Conferenceon Learning Representations. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: Anopen bilingual pre-trained model. In The Eleventh In-ternational Conference on Learning Representations. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068. Andrew Zhao, Daniel Huang, Quentin Xu, MatthieuLin, Yong-Jin Liu, and Gao Huang. 2024. Expel:Llm agents are experiential learners. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 38, pages 1963219642.",
  "BBaselines": "We compare LOREHM with several state-of-the-art (SoTA) systems for low-resource harmful memedetection: 1) PromptHate (Cao et al., 2022): aprompt learning approach that concatenates thememe text and the image caption as the promptfor masked language modeling; 2) MR.HARM (Linet al., 2023a): a two-stage framework that distillsmultimodal reasoning knowledge from LLMs forharmfulness inference; 3) Pro-Cap (Cao et al.,2023): a caption-enhanced version of PromptHate,by leveraging pre-trained vision-language mod-els with probing queries, to improve the imagecaption in the text prompt; 4) OPT-30B (Zhanget al., 2022): an early and representative large lan-guage model with the in-context learning ability,widely recognized stand-in for GPT-3 (Brown et al.,2020); 5) OpenFlamingo-9B (Awadalla et al.,2023): an open-source replication of Flamingomodels (Alayrac et al., 2022) that enhances pre-trained, frozen language models by enabling themto cross-attend to the outputs of a frozen visionencoder during the next token prediction; 6) Mod-HATE (Cao et al., 2024): a modularized networksfor low-resource harmful meme detection, whichtrain a set of modules capable of relevant tasks andlearn a composition of modules with the few-shotexamples; 7) LLaVA-34B (Liu et al., 2023a): anenhanced version of LLaVA, with improved rea-soning, OCR, and world knowledge capabilities;8) GPT-4o (OpenAI, 2023): an optimized versionof the proprietary GPT-4 architecture developed byOpenAI, which includes capabilities for processingmultiple modalities; 9) LOREHM (*): Our pro-posed agentic approach for low-resource harmfulmeme detection, based on LLaVA-34B and GPT-4o. We use the accuracy and macro-averaged F1(dominant) scores as the evaluation metrics, where",
  "CImplementation Details": "To demonstrate the generalizability of our frame-work, we utilize the representative LMMs LLaVA-34B and GPT-4o as the LMM agent from boththe open-source and closed-source perspectives.Specifically, we implement the llava-v1.6-34band gpt-4o-2024-05-13 versions for LLaVA-34Band GPT-4o, respectively. Note that the choice ofLMMs is orthogonal to our proposed paradigm,which can be easily replaced by newly strongerLMMs without further modification. To make ourresults reproducible, we set the temperature as 0without any sampling mechanism. The frozen pre-trained vision and text Transformer encoders areimplemented as CLIP (Radford et al., 2021) withthe specific version ViT-L/14@336px. To choosethe trade-off parameters and , we conducted agrid search within the range and set and as0.2 and 0.8, respectively. For the experiments, allthe baselines are tested in the 50-shot setting withbalanced classes. In the context of few-shot learn-ing, evaluations can exhibit high variability due tothe selection of meme sample examples. To miti-gate this, we generate multiple few-shot referencesets using different random seeds for a more reli-able few-shot performance evaluation. We createfive sets of few-shot examples, each with a differ-ent random seed, for each 50-shot setting. Conse-quently, we report the average accuracy and macro-averaged F1 scores computed over the test set, fol-lowing model optimization based on these variousfew-shot samples. As our approach is gradient-free,there are no training parameters.We use the released source codes to repro-duce PromptHate, MR.HARM, Pro-Cap, and Mod-HATE. To support the in-context learning of theLLM and/or LMM baselines, we need to first con-vert the memes image into an acceptable textualinput, because the current LLMs and/or LMMsdo not support 50 images as input to have a goodperformance. It is also a limitation of existingLMMs. We first in-paint the memes by combiningMMOCR (Kuang et al., 2021) with SAM (Kirillovet al., 2023) to extract the text and pure image inmemes. Then we apply LLaVA to generate textualdescriptions about the dominant objects or events",
  "Discription:": "Given the meme, with the Text: {} embeded in the image{ }, your task is to determine whether thismeme is harmful or not, in order to maintain theharmlessness and integrity of information on theInternet. A classifier that can identify common features amongmultiple memes has labeled this meme as {}, Please review the classifier's judgment carefully and use yourextensive knowledge to analyze and understand thismeme before providing your final verdict. If youdisagree with the classifier's judgment, you mustprovide exceptionally thorough and persuasive reasons.",
  ":Prompt template for final judgment ofLOREHM": "in the memes image. To ensure the reproducibilityof the LLM and/or LMM baselines, we also set thetemperature as 0 without any sampling mechanism.In our approach, to better utilize the preliminaryprediction P provided by Relative Sample Aug-mentation, we design the prompt template as:A classifier that can identify common featuresamong multiple memes has labeled this meme as{P}, Please review the classifiers judgment care-fully and use your extensive knowledge to analyzeand understand this meme before providing yourfinal verdict. If you disagree with the classifiersjudgment, you must provide exceptionally thoroughand persuasive reasons.For the insight set En provided by Meme InsightAugmentation, we sequentially concatenate eachinsight. Subsequently, we concatenate these twoparts with the XCOT, resulting in the final promptinput for the LMM agent, as illustrated in .For gaining general insights into low-resourceharmful meme detection, XReflect is designed as. To restrict the number of insights, we setthe capacity of the insight set to 10 by default. Oncethe insight set is full, the LLM agent is prohibitedfrom producing the ADD action.All experiments were conducted using OpenAIAPI and four NVIDIA A40 48GiB GPUs. Eval-uation of HarMs test set using LOREHM basedon GPT-4o takes approximately 40 minutes, whileFHM requires 1 hour and MAMI requires 2 hours.When employing LOREHM based on LLaVA-34B,evaluation times extend to 2 hours for HarM, 3hours for FHM, and 6 hours for MAMI, approxi-mately.Due to privacy and ongoing research consider-ations, the code used in this study is not included You are an advanced reasoning agent that can add, edit or remove rules from your existing rule set, based onforming new critiques of past task trajectories. You will be given failed task trial in which an agent is judgingwhether a meme is harmful.",
  "Here are the EXISTING RULES:{}": "By examining the failed trial and the list of existing rules, you can perform the following operations: add, edit, remove, oragree so that the new rules are GENERAL and HIGH LEVEL insights of the failed trial or proposed way of Thought sothey can be used as helpful tips to different memes in the future. Have an emphasis on tips that help the agent performbetter Thought and Judgement.",
  "Follow the below format:<OPERATION> < RULE NUMBER>: <RULE>": "The available operations are: AGREE, REMOVE (if one existing rule is contradictory or similar/duplicated to otherexisting rules), EDIT (if any existing rule can be enhanced, rewrite and improve it), ADD (add new rules that are verydifferent from existing rules and relevant for other memes). Each needs to CLOSELY follow their correspondingformatting below:AGREE < EXISTING RULE NUMBER>: < EXISTING RULE>REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE>EDIT < EXISTING RULE NUMBER>: <NEW MODIFIED RULE>ADD <NEW RULE NUMBER>: <NEW RULE> Do not mention the trial in the rules because all the rules should be GENERALLY APPLICABLE.Each rule should be concise and easy to follow. Any operation can be used MULTIPLE times. Do at most 4 operations andeach existing rule can only get a maximum of 1 operation.",
  "Algorithm 2 LOREHM - Relative Sample Aug-mentation": "Initialize:Feature Representation from Algorithm 1:referencememesembeddinginformationEmbref,target memes embedding information Embtest;Set of reference memes Sref;Preliminary prediction P;Set of Top K similar memes H ;Harmful counter harmful_count 0;Similarity scores of two memes d;Set of Similarity scores D ;Function capable of selecting the top K samplesin the target set TopK().for each Mref Sref do",
  "Algorithm 4 LOREHM - Inference": "Initialize:Preliminary prediction P from Algorithm 2;Insight set En from Algorithm 3;The visual information Itest of a meme Mtest;The text information Ttest of a meme Mtest;Large Multimodal Model LMM;Chain-of-thought prompt XCoT;Final prediction pfinal.pfinal LMM(XCoT, Itest, Ttest, P, En)return pfinal same contents for the same instance, we utilize thespecific version lava-v1.6-34b of LLaVA-34Band gpt-4o-2024-05-1 of GPT-4o, and further setthe parameter temperature as 0 without any sam-pling mechanism, that is, the greedy decoding wasadopted to ensure the deterministic results for thecontent generation with the same prompt. 2) TestSet Leakage: The test set leakage issue does not ex-ist in the open-source LLM (Liu et al., 2023a) as thepaper has clearly described the instruction-tuningdata used for training, which does not include anydata used in our experiments. However, we can-not fully guarantee the exclusion of potential dataleakage with GPT-4o, as its internal workings re-main opaque. Nevertheless, as evidenced by the re-sults in , where the LLaVA-34B or GPT-4owas directly deployed to test on the three standarddatasets, the absence of significant test set leakageis implied. This is inferred from the fact that directzero-shot application of the LMMs did not yielddisproportionately high performance, which wouldbe expected if the models were benefiting fromtest set leakage. Moreover, we can consistentlyobserve enhanced performance in the variants ofour proposed framework based on both LLaVA-34B and GPT-4o. This suggests that such improve-ment is basically attributed to our designed agenticparadigm rather than test set leakage. 3) Gener-alizability: We believe our LOREHM paradigmis a general technique that works with emergingstronger LMMs, because our approach works notonly on GPT-4o, but also well on the open-sourceLLaVA-34B, which is not an OpenAI system.",
  "FDiscussion about RSA": "For the proposal of the Relative Sample Augmenta-tion mechanism, we chose not to use the retrievedmemes as direct input to the LMM agent. Thisdecision was made to explicitly augment the LMMagent with the label information of the retrievedmemes. Simply inputting the retrieved memes backinto the LMM agent would merely re-utilize its in-ternal knowledge without incorporating externallabel information. Additionally, using retrievedmemes along with labels as explicit signals is notmore effective and somewhat overlaps with theconcept of Meme Insight Augmentation. Instead,using a voting mechanism to integrate the labelinformation of retrieved memes as auxiliary sig-nals for outward analysis complements the agentsknowledge-revising strategy through inward anal- ysis. The macro-averaged F1 scores of the votingmechanism are approximately 79%, 59%, and 70%on the HarM, FHM, and MAMI test sets, respec-tively. Although the voting mechanism itself is notentirely robust, it can still provide effective label in-formation for the LMM agent as a prior preference.From both theoretical and practical perspectives,our current design is the most reasonable and effec-tive approach.",
  "GDiscussion about MIA": "In our design of Meme Insight Augmentation, theknowledge-revising behavior of human beings isnon-trivial when applied to harmful meme detec-tion. Unlike many previous agent tasks that operatewithin environments providing real feedback, harm-ful meme detection lacks such an environment tosupply the agent with authentic responses. Addi-tionally, because harmful meme detection is fun-damentally a binary classification task, informingthe agent of an unsuccessful attempt essentially re-veals the correct answer. These factors make thedirect application of methods like Reflexion (Shinnet al., 2024), which relies on environmental feed-back to determine the success of an attempt anddecide whether to reflect and retry based on insightsgained from previous attempts, inapplicable. Simi-larly, ExpeL (Zhao et al., 2024), a method requiringmultiple passes on the same data to obtain both suc-cessful and failed trajectories, cannot be directlyutilized. Additionally, during our detailed examina-tion of data samples, we discovered a few instancesof contentious annotations. More specifically, inthe rare instances of incorrect or disputable annota-tions, even though we engage in retrieved memeswith the incorrect golden label as part of the votingmechanism in the Relative Sample Augmentationphase, the well-generalized insights extracted fromthe knowledge-revising process also contributes toa better understanding of our approachs robust-ness, particularly when dealing with inaccuratelyannotated data. Therefore, our proposed MIA com-ponent is specially devised to couple with the RSAcomponent, which is complementary to each other.",
  "(a)": "GPT-4o Output Thought: ... The meme uses exaggeration for comediceffect.\\n\\nWhile the meme could be seen asplaying on stereotypes, it is clear that it isintended as a joke rather than a factualstatement. It does not contain any explicitharmful language or imagery....GPT-4o+RSA Output Thought: ... The classifier's judgment of the meme asharmful is likely based on these factors, asit perpetuates harmful stereotypes and couldoffend or misinform viewers.....",
  "(b)": "GPT-4o Output Thought:... While the meme does contain criticalcommentary on a public figure's statement,it primarily serves to ridicule the idea ratherthan to spread misinformation. It useshumor to reinforce the consensus amongmedical professionals about the dangers ofsuch suggestions. ....GPT-4o+MIA Output Thought:... The meme could be seen as mocking thesuggestion and highlighting its absurdity,but it also risks trivializing a serious publichealth issue. Given the potential formisinformation and the importance ofmaintaining trust in health directives, thismeme could contribute to harmfulbehaviors or undermine public healthmeasures.....",
  ": Examples of correctly predicted harmfulmemes in (a) HarM and (b) MAMI datasets": "From the output thought in natural text, we ob-serve that: 1) Our proposed LOREHM could offera more profound analysis compared to both the0-shot and 50-shot GPT-4o methods. For exam-ple, in (a), the 0-shot GPT-4o might deemthe meme harmless, failing to grasp its satirical na-ture, while the 50-shot GPT-4o, though more crit-ical, might not fully consider the memes broaderimplications. Our proposed LOREHM identifiesthe memes potential to trivialize the pandemicsseverity with the phrase Going golfing & ignor-ing pandemic leading us to classify the memeas harmful, as it could undermine the importanceof public health measures during a critical time.Whats more, in (b), the 0-shot GPT-4omight overlook the memes reinforcement of tradi-tional gender roles, while the 50-shot GPT-4o, influ-enced by limited examples, might not fully appreci-ate the cultural implications. LOREHM, however,scrutinizes the memes depiction of a woman ina 1950s-style housewife outfit with the text THELAST TIME I MARCHED IT WAS DOWN THEAISLE concluding that it subtly perpetuates out-dated gender norms, which can be harmful in thecontext of modern feminist movements. 2) Ourmethod exhibits notable efficiency within the con-",
  ": Examples of correctly predicted harmfulmemes in (a) FHM and (b) HarM datasets": "straints of low-resource scenarios (Lin et al., 2022).Compared to the 50-shot GPT-4o, which relies ona limited set of examples to improve its judgments,our method shows superior performance with thesame resource constraints. By effectively lever-aging few-shot learning and relevant sample re-trieval, our method can discern harmful contentwith greater accuracy. This capability is crucialwhen the availability of training examples is scarce,as our method requires fewer examples to achievea higher level of performance, thus outperformingthe 50-shot GPT-4o in equal-resource conditions.",
  "H.2Case Study of RSA & MIA": "We provide a case study of the LMM agents out-put thoughts, to investigate the effect of the RSAand MIA strategies on the correctly predicted harm-ful meme samples by LOREHM, as illustrated in.From the output thought in natural text, we ob-serve that: 1) The Relative Sample Augmenta-tion (RSA) mechanism enhances the LMM agentby incorporating label information from retrievedmemes, providing the LMM agent with additionalcontext and insights. For example, in (a),the GPT-4o output considered it harmless, inter-preting it as a joke. However, with RSA, the LMM (a) Output Thought:...While the meme may beintended to highlight the historicalimpact and the gravity of theactions of this individual, it isimportant to consider thesensitivity of the subject matter.The meme could be seen astrivializing or making light of avery serious and tragic period inhistory...",
  ": Examples of wrongly predicted memes by ourproposed framework with the ground truth (a) harmlessand (b) harmful": "agent identified it as perpetuating harmful stereo-types, recognizing the potential to offend or mis-inform viewers. This shows how RSA integratesexternal labels as auxiliary signals, complementingthe agents internal knowledge-revising strategyand creating a balanced mechanism that leveragesboth internal and external information sources. Byfocusing on the labels of retrieved memes ratherthan directly inputting the memes themselves, RSAensures that the augmentation process adds uniquevalue to the LMM agents capabilities. 2) TheMeme Insight Augmentation (MIA) mechanismoffers significant benefits by enabling the LMMagent to revise its knowledge effectively. For in-stance, in (b), the GPT-4o output thoughtit was a humorous critique and not harmful. How-ever, MIA identified the risk of trivializing a seriouspublic health issue, noting the potential for misin-formation and its impact on public health measures.In emerging events (Lin et al., 2024d) on socialmedia (Lin et al., 2021), MIA could promote aknowledge-revising behavior akin to human rea-soning, allowing the LMM agent to better interpretmemes and distinguish between harmful and harm-less content.",
  "IError Analysis": "To better understand the behavior of our frameworkand facilitate future studies, we conduct an erroranalysis on the wrongly predicted memes by ourproposed framework. shows two exam-ples of memes wrongly classified by LOREHM.For the harmless meme in (a), featuring a historical figure with the text so many lives lostbecause of this man, LOREHM incorrectly cat-egorized it as harmful. The original thought pro-vided by LOREHM was that the meme could beseen as trivializing a serious and tragic period inhistory. The misjudgment in this case likely arosefrom LOREHMs failure to effectively associatethe image with the accompanying text, leading toan overemphasis on the historical figures nega-tive connotations without considering the full con-text provided by the memes message. In contrast,the harmful meme in (b), which depictedseveral clergymen with the text money is evilgive it to us was deemed harmless by LOREHM.This misjudgment indicates an inherent bias inLOREHM towards religious-themed memes. Con-trary to being overly sensitive, LOREHMs biasled to an underestimation of the potential negativeimpact of such content. Additionally, we also in-vestigated more errors resulting from the impactof visual artifacts like image quality, occlusion, ob-scurity, etc. We found that low-quality images leadto the wrong recognition of superficial patterns andignoration of the occlusion.",
  "JRelated Work about LLMs and LMMs": "Recently, LLMs have demonstrated exceptionalversatility across various tasks.Significant ad-vancements by leading tech companies have re-sulted in highly proficient, though often propri-etary, LLMs (Brown et al., 2020; OpenAI, 2023;Chowdhery et al., 2022; Team et al., 2023). Mean-while, the NLP community has seen the rise ofopen-source LLMs, with publicly shared modelweights (Black et al., 2022; Zeng et al., 2022; Tou-vron et al., 2023a,b; Luo et al., 2023). More re-cently, LLMs have also been developed to adaptto processing both textual and visual data, mark-ing a significant advancement. Recent researchhas focused on constructing versatile multimodaldatasets (Yang et al., 2023) from platforms likeGPT-4 and GPT-4V (OpenAI, 2023), fine-tuningopen-source LMMs, e.g., LLaVA (Liu et al., 2023a)and other innovative projects (Bai et al., 2023; Daiet al., 2023; Wang et al., 2023b). GPT-4o, the lat-est version of the GPT family, has been releasedlately. These LMMs have shown excellent emer-gent abilities in multimodal tasks. In this work, weutilize the representative LMMs, i.e., LLaVA-34Band GPT-4o, as the backbone variants of the LMMagent, to explore the low-resource issue (Lin et al.,",
  "HarM by LOREHM(GPT-4o)": "Evaluate if the humor in the meme is appro-priate and does not trivialize serious issues,create unnecessary panic, target individuals orgroups in a derogatory manner, or underminecritical health measures and policies, espe-cially during ongoing crises like pandemics. Consider the broader context and potentialnegative impact of the meme, especially insensitive situations such as health crises or po-litical discourse, to determine if it could causefear, anxiety, spread misinformation, trivializeserious issues, or undermine important publichealth measures. Identify if the meme mocks, trivializes, or un-dermines public health measures and policies,as this can contribute to harmful behaviors,spread misinformation, and undermine publictrust in health directives and governance. Ensure the meme does not encourage or nor-malize behavior that could be harmful in reallife, particularly in the context of public healthand safety, such as trivializing quarantine, pol-icy effectiveness, or other critical health mea-sures.",
  "Evaluate the context and possible interpreta-tions of both the image and text together todetermine if the meme could be harmful": "Use a balanced approach, considering the po-tential for harm while recognizing the context,intent, and humor of the meme. Clearly dif-ferentiate between harmful content and satireor light-hearted humor to avoid unnecessarycensorship. Assess the potential for a meme to be per-ceived as offensive by considering the broadercontext and audience, but avoid overemphasiz-ing potential offense if the context and humorsuggest a harmless or light-hearted intent.",
  "MAMI by LOREHM(GPT-4o)": "Recognize and account for humor or satire, en-suring they are not mistaken for serious harm-ful content. Consider the context, intent, andwhether the humor directly trivializes serioussubjects such as violence, sexual behavior, orperpetuates harmful stereotypes. When evaluating humor, assess whether it per-petuates or trivializes harmful stereotypes, bi-ases, serious life-threatening situations, or ex-plicit content inappropriately. Consider theoverall impact on the targeted group, whilebalancing this with the intent and context ofthe humor.",
  "HarM by LOREHM(LLaVA-34B)": "Assess the overall impact of the meme, in-cluding whether it is likely to be perceived aslight-hearted or offensive, especially in sensi-tive contexts such as health crises, disasters,social issues, or activism. Consider whetherit trivializes serious situations, promotes irre-sponsible or dangerous behavior, spreads mis-information, or is insensitive to those affected,even if the intent is humorous. Consider the context and intent of the meme,especially if it is meant to be humorous orrelatable, but also weigh the potential for it tobe perceived as insensitive, harmful, or mis-leading in serious situations. Evaluate whether the meme directly or indi-rectly promotes harmful stereotypes, ideas, orpersonal attacks, trivializes serious issues orindividuals efforts, spreads misinformation,or could be perceived as insensitive or harm-ful. Evaluate whether the meme trivializes serioussituations, issues, or the efforts of activists,which could be harmful or offensive to thoseaffected, and consider the potential for it tospread misinformation, encourage irresponsi-ble behavior, or be perceived as insensitive,especially in the context of public health andsafety. Evaluate whether the meme could potentiallyencourage harmful actions or behaviors, espe-cially in the context of public health and safety.Consider the potential for it to downplay theseverity of serious issues, spread misinforma-tion, or be perceived as insensitive, even if theintent is humorous. Consider the potential for the meme to spreadmisinformation, harmful stereotypes, or con-tribute to harmful narratives, even if it is in-tended as a joke, especially in the context ofpublic health and safety."
}