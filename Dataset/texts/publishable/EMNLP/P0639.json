{
  "Abstract": "In real-world scenarios, it is desirable for em-bodied agents to have the ability to leveragehuman language to gain explicit or implicitknowledge for learning tasks. Despite recentprogress, most previous approaches adopt sim-ple low-level instructions as language inputs,which may not reflect natural human commu-nication. Its not clear how to incorporate richlanguage use to facilitate task learning. Toaddress this question, this paper studies dif-ferent types of language inputs in facilitatingreinforcement learning (RL) embodied agents.More specifically, we examine how differentlevels of language informativeness (i.e., feed-back on past behaviors and future guidance)and diversity (i.e., variation of language ex-pressions) impact agent learning and inference.Our empirical results based on four RL bench-marks demonstrate that agents trained with di-verse and informative language feedback canachieve enhanced generalization and fast adap-tation to new tasks. These findings highlightthe pivotal role of language use in teaching em-bodied agents new tasks in an open world. 1",
  "Introduction": "Developing embodied agents that can understandand communicate with humans in natural languageto learn and accomplish tasks is a long-standinggoal in artificial intelligence. In recent years, theintegration of human language and reinforcementlearning (RL) has seen significant advancements.Unlike traditional RL methods that typically relyon numerical reward signals to guide agent learn-ing, recent works (Cheng et al., 2023; Lin et al.,2023) explore using language as a rich and intu-itive signal to shape an agents behaviors. For ex-ample, when the agent is making mistakes duringthe task completion, providing language feedback",
  "*Equal contribution.1Sourcecodeavailableat": "can largely improve the instantaneous performancethus enhancing the overall agent learning efficiencyand effectiveness (McCallum et al., 2023).However, existing methods generally employsimple instructions, such as \"turn left\" and \"putthe apple to the table\" to teach/control an agent(Hanjie et al., 2021; Zhang and Chai, 2021; Linet al., 2023; McCallum et al., 2023; Shridhar et al.,2021). While useful, these instructions may notfully reflect the flexibility of language use in tasklearning and collaboration (Chai et al., 2018, 2019;Zhang et al., 2022, 2023; Dai et al., 2024a). Inthe real world, humans often express complex lan-guage instructions that are more informative. Forinstance, when a student makes a mistake, a teachermay help them to retrospect on what went wrong(i.e., hindsight instructions) and then guide themon what should be done next to finish the goal (i.e.,foresight instructions). In addition, humans arelikely to engage in conversations with more diverselanguage patterns, describing the same goal withdifferent expressions and styles. Therefore, we askthe following question:",
  "How do the informativeness and diversity ofnatural language used during RL training affectan agents ability to learn tasks?": "We take a popular offline RL model - decisiontransformer (DT) (Chen et al., 2021) - as a back-bone architecture and conduct a comprehensivestudy to examine how informativeness and diver-sity of language use may impact agents learningability. To control informativeness, we leverageexpert agents actions as a reference to generatehindsight reflection and foresight guidance, usinghand-crafted language templates. To increase di-versity, we construct a GPT-augmented languagepool, where GPT-4 (OpenAI, 2024) is used to aug-ment hand-crafted templates into much more nat-ural and richer expressions. We further extendedDT into a multi-modal Language-Teachable DT (LTDT) and demonstrated that LTDT agents thatare trained with diverse and informative languagesignificantly outperform the counterpart agents thatare trained either with simple language alone orwith no language inputs. Notably, we found thateven with just one language template, combininghindsight and foresight feedback together improvesagents performance by an average of 9.86 points( from 37.95% to 47.81%) on four popular offlineRL benchmarks. When more language diversityis incorporated into training, an additional 10.14points (from 47.81% to 57.95%) are obtained.The contributions of this paper can be summa-rized as follows: We investigate in detail, for the first time,how language informativeness and diversityaffect offline RL agents in task learning, anddemonstrate their important roles in improv-ing agents performance, adaptability, and ro-bustness.",
  "Related Work": "Offline Reinforcement LearningOffline rein-forcement learning (RL) has become a focal pointof research due to its ability to utilize pre-existingdatasets for training agents without real-time in-teractions. Several algorithms address the uniquechallenges of offline RL, such as mitigating extrap-olation errors and ensuring robust policy evalua-tion. A survey by (Prudencio et al., 2023) outlinesthe fields taxonomy and open problems. Bench-marking efforts by (Fujimoto et al., 2019) assessvarious batch deep RL algorithms. Key approachesinclude Conservative Q-Learning (CQL) (Kumaret al., 2020), Implicit Q-Learning (IQL) (Kostrikovet al., 2021), and the Decision Transformer (DT)(Chen et al., 2021), which treats RL as a sequencemodeling problem (Janner et al., 2021). Recentwork also explores generalization across tasks (Leeet al., 2022; Reed et al., 2022; Schubert et al., 2023),the use of exploratory data (Yarats et al., 2022), andintegrating large language models (LLMs) (Mir-chandani et al., 2023). Efficient online RL lever-aging offline data is also a focus (Ball et al., 2023; Modhe et al., 2023). Our research builds on the De-cision Transformer (DT) by integrating languagefeedback, creating the Language-Teachable Deci-sion Transformer (LTDT). This novel approach in-corporates rich, human-like language instructions,improving agent learning and decision-makingthrough enhanced informativeness and diversityof language inputs. Language in Reinforcement LearningThe in-tersection of natural language and RL offers newways to develop intuitive and effective learningparadigms for embodied agents. Initial works uti-lized language for feedback and task instructions(She and Chai, 2017; Nguyen et al., 2017; Shrid-har et al., 2020). Recent studies have exploredvarious methods for incorporating language feed-back in RL, such as the LTC paradigm (Wanget al., 2023), lifelong robot learning with human-assisted language planners (Parakh et al., 2023),and frameworks for rich information requests (Daiet al., 2020; Tseng et al., 2021; Nguyen et al., 2022).Language for corrections (Sharma et al., 2022; Liuet al., 2023) and as reward signals (Xie et al., 2023;Goyal et al., 2019; Yu et al., 2023) has shownto enhance agent performance. Vision-languagejoint training approaches, like CLIP (Radford et al.,2021), BLIP-2 (Li et al., 2023), and InstructBLIP(Dai et al., 2023), demonstrate the potential of com-bining visual and language modalities for RL tasks(Ma et al., 2023; Nguyen et al., 2019; Khandel-wal et al., 2022). Further, multimodal prompts forrobotic manipulation (Jiang et al., 2023; Fan et al.,2022) and LLMs for planning in robotics (Ahnet al., 2022; Huang et al., 2022; Singh et al., 2023;Yao et al., 2022; Dai et al., 2024b) highlight theevolving role of language in RL. Other works, like(Mehta et al., 2023), focus on generating problem-specific language feedback templates. In contrast,our work focuses on the informativeness and diver-sity of language instructions, two problem-agnosticyet easy-to-implement properties. By using bothhindsight and foresight language templates and en-hancing diversity through GPT-4, we demonstratenotable improvements in agent performance andgeneralizability, showcasing the impact of complexlanguage inputs in offline RL training.",
  "find the message": ": An overview of four environments used for experiments. It shows tasks to be learned in each environment;examples of hindsight (marked H) and foresight (F) language feedback ( next to the gear icon are hand-craftedtemplates and next to the GPT icon are GPT-4 generated feedback); as well as low-level actions in each environment.",
  "Offline Reinforcement Learning": "To support a systematic study of language use, weformulate the problem in the offline reinforcementlearning setting. At each time step t, the agentreceives an observation ot, a reward rt for its pre-vious action, and a language feedback lt. Theagent then executes an action at according to apolicy , which is conditioned on the entire in-teraction history ht up to time t, i.e., (at | ht),where ht = {ot, rt, lt, a<t} represents the his-tory of observations, rewards, language feedback,and past actions up to time t. The agents goal isto complete the task by maximizing the expecteddiscounted sum of rewards E[Tt=1 trt] where Tis the episode length, and is the discount fac-tor. In offline RL, the training trajectories are pre-collected with an expert agent (a well-trained agentor a planner-based expert with privileged informa-tion). The trained agents are evaluated interactivelywith the environment.",
  "tasks": "3.2.1InformativenessInformativeness refers to the richness of infor-mation content in language feedback. FollowingCheng et al. (2023), we categorize feedback intotwo types: hindsight and foresight. Hindsightfeedback involves comments or critiques aboutthe agents past actions. For example, \"Excellent,you are moving towards the goal!\" encourages theagent to continue its current path, while \"You aregetting too close to the enemy.\" alerts the agentabout a mistake. Hindsight feedback reflects onincorrect actions taken in previous steps, which canguide agents toward success by narrowing down thesearch space for correct actions (See Appendix Dfor more analysis.) Conversely, foresight feedbackguides potential future actions. For instance, \"Youshould go right to get closer to the target.\" directsthe agent towards the goal, and \"You should go leftto avoid the enemy on the right.\" helps the agentmake strategic decisions to avoid threats. Languagefeedback is considered most informative when itincludes both hindsight and foresight elements, andleast informative when neither is present.3.2.2DiversityDiversity in language feedback refers to the vari-ety of ways the same information is conveyed. Iffeedback is provided using only one template, itis less diverse. It becomes more diverse when thesame information is expressed in many different",
  "Environments": "As shown in , we conduct experimentsacross four environmentsHomeGrid, ALFWorld,Messenger, and MetaWorldeach featuring dis-crete action spaces, with hand-crafted hindsightand foresight language instructions.More information and examples of languages foreach environment can be found in Appendix A.HomeGrid (Lin et al., 2023) is a multitask gridworld designed to evaluate how well agents canunderstand and use various types of language tocomplete tasks. It includes five task types (FIND, GET, CLEAN UP, REARRANGE, OPEN), involvinginteraction with objects and trash bins with a totalof 38 tasks. The agent receives a reward of 1 whenthe task is completed and receives a reward of 0.5if a subgoal is completed.ALFWorld (Shridhar et al., 2021) is a text-gameenvironment that aligns with the embodied AL-FRED benchmark (Shridhar et al., 2020) and pro-vides simulation for household tasks. It includes sixtypes of tasks which require the agent to navigateand interact with household objects by followinglanguage instructions. The agent gets a rewardof 1 when the task is completed. We adopt thehindsight and foresight language templates fromLLF-ALFWorld introduced in (Cheng et al., 2023),which adds an extra language wrapper to the origi-nal ALFWorld environment.Messenger (Hanjie et al., 2021) is a grid worldwith several entities. The agents task is to retrievea message from one entity and deliver it to anothergoal entity, while avoiding enemies. At the start ofeach episode, the agent is provided with a manualdescribing the randomized roles of the entities andtheir movement dynamics. The agent receives areward of 1 when the task is completed.MetaWorld (Yu et al., 2019) is a benchmark thatconsists of a variety of manipulation tasks per-formed by a simulated Sawyer robot arm. It in-cludes 50 types of common robot manipulationtasks. We select two of them in our experiments:",
  "Trajectory Generation": "To improve model generalization and avoid overfit-ting, it is essential to train on diverse, sub-optimaltrajectories rather than relying solely on optimalones generated by an expert agent (Kumar et al.,2020; Chen et al., 2021). We achieve this by intro-ducing perturbations into the expert planner (seeAppendix B), allowing the non-expert agent toproduce sub-optimal trajectories. This promotesbroader exploration of the state-action space, en-hancing the models ability to generalize to unseenscenarios (Kumar et al., 2020; Chen et al., 2021).During data collection, we begin by appendingthe task description Td to the trajectory sequenceand initializing the environment with a fixed seed.A non-expert agent, using a sub-optimal policy derived from the expert agents optimal policy ,interacts with the environment. At each time step,",
  "H: So far, so good, youre doing great!F: To access the recycling bin, youllneed to pedal": ": A demonstration of hindsight and foresight language feedback generation. In our framework, the agent executes the trajectory, while the expert agent , with access to privileged ground truth knowledge, is used solelyto provide information for generating language feedback to . At time step t, hindsight language is generated bycomparing the agents action at1 with the expert agents action at1, whereas foresight language is generated byreferring to the expert agents action at to guide the agent on the next step. To increase the diversity of languagefeedback, we construct a pool of language templates comprising GPT-augmented languages, and sample candidateinstructions as online language feedback.",
  "Language Feedback Generation": "For the second part of the dataset D, we collectthe language feedback along the non-expert agentstrajectory. As shown in , we follow a struc-tured process to generate diverse and informativelanguage feedback. For the state at time step t, theexpert agent proposes an expert action at (e.g.\"down\") at this state, which is further transformedinto a foresight template lforet(e.g. \"Turn back\")by the environment simulator, guiding the agent onwhat should be done at this state. After the non-expert agent steps the environment (into time stept + 1) with its generated action at (e.g. \"down\"),the environment simulator generates a hindsighttemplate lhindt+1 (e.g. \"You are doing well so far.\")based on the comparison between agent action atand expert agent action at at the last time step t,reflecting on whether the agent is on the right track.For each foresight/hindsight template, we useGPT-4 (OpenAI, 2024) to augment it into morenatural and varied expressions. (e.g. We can aug-ment \"You are doing well so far.\" into \"Up until now, youre doing wonderfully.\" or \"So far, so good,youre doing great!\".) We compile all the rewrittensentences into a set called the GPT-augmented lan-guage pool. At each step of the non-expert agent,we randomly select one candidate from the pool asthe language instruction. This process ensures thefeedback provided to the agent has high level ofdiversity and enriches the learning experience.The level of informativeness and diversity ofthe language feedback depends on the inclusion ofhindsight and foresight (e.g. concatenated whenboth are required) and the use of GPT-augmentedlanguage pool. The language feedback at each timestep will finally get concatenated with the trajectorysequence into (Td, R1, s1, a1, l1, . . . Rt, st, at, lt).The dataset collection algorithm is summarizedin Algorithm 1.",
  ": Language-Teachable Decision Transformer": "(Td, R1, s1, a1, l1, . . . , Rt, st, at, lt), with the lan-guage feedback input appended at each step anda task description (TD) input prefixed at the be-ginning of the sequence. Like the original DT,the embeddings of these inputs are passed throughthe Causal Transformer, which encodes positionalinformation to maintain sequence order. The trans-formers output is used to predict the next actionin the sequence, conditioned on the state, return-to-go, action, and language feedback in the last Ktime steps, with the task description as the prefix(4K + 1 tokens in total), as shown in .Training Similar to the original DT training, givenan offline dataset of trajectory sequences, we sam-ple a sub-sequence of length K (with 4K + 1 to-kens), and the prediction head is trained to predictdiscrete actions with the cross-entropy loss or con-tinuous actions with the MSE loss.Language Embeddings We use language em-beddings from a frozen Sentence-BERT model(Reimers and Gurevych, 2019) in all environments.We find Sentence-BERT more sensitive to languagefeedback changes, capturing nuanced semantic dif-ferences better.",
  "Experimental Setup": "Setup for RQ 1. For research question 1, we com-pare performance on seen tasks (see Appendix Cfor detailed task settings) between agents trainedwith varying levels of language informativenessand diversity: 1) No Language agent trained with-out any language instructions; 2) Template Fore-sight agent trained with hand-crafted foresight lan-guage templates; 3) Template Hindsight agenttrained with hand-crafted hindsight language tem-plates; 4) Template Hindsight + Foresight agent,trained with hand-crafted foresight and hindsightlanguage templates; and 5) GPT-augmented Hind-sight + Foresight agent trained with hindsight andforesight languages from the GPT-augmented lan-guage pool.Setup for RQ 2. For research question 2, we com-pare generalizability on unseen tasks (see AppendixC for detailed task settings) between agents withdifferent pre-training language settings: 1) No-Lang pre-trained agent pre-trained without anylanguage instructions; 2) GPT-augmented hind-sight pretrained agent pre-trained with hindsightlanguage from the GPT-augmented language pool;3) GPT-augmented foresight pretrained agentpre-trained with foresight language from the GPT-augmented language pool; 4) GPT-augmentedhindsight + foresight pretrained agent pre-trainedwith both hindsight and foresight language fromthe GPT-augmented language pool. For few-shotadaptation, we fine-tune the pre-trained agents withboth hindsight + foresight language from the GPT-augmented language pool for all settings.Evaluation. To simulate realistic human-robot in-teraction during evaluation, we employ GPT-3.5-turbo or GPT-4 to provide language feedback. Ateach time step, hindsight and foresight information,based on the expert agents actions, is processedas shown in . GPT then generates and de-livers the relevant language feedback to the agent.Additionally, GPT assesses whether feedback isnecessary; if not, it returns an empty string. SeeAppendix G for examples.",
  "Template Foresight": ": Comparison of agent performance in four environments (averaged across 100 seeds in each environment)under varying levels of language feedback informativeness and diversity. Agents trained with more informative lan-guage feedback exhibit progressively higher performance. Furthermore, given the same informativeness (Hindsight+ Foresight), increasing diversity with the GPT-augmented language pool leads to the highest performance. 5 shot10 shot20 shot 0.0 0.2 0.4 0.6",
  "Messenger": "No LanguageGPT augmented hindsight GPT augmented foresightGPT augmented Hindsight + Foresight : In the Messenger environment, when trainedwith more diverse foresight and hindsight languages,the agents can perform better than those trained withoutlanguages. Furthermore, agents trained with more infor-mative languages demonstrate stronger performance. right\" is correct; if \"pick the object\" is wrong, \"dropthe object\" is correct). However, for the bin manip-ulation mistake, hindsight feedback is less helpfulsince the action space grows larger (pedal/lift/grasp,compared to binary opposite actions in Navigationand Object Pick/Drop), and there are no clear im-plications for the correct action.",
  "Efficiency Gain": "Efficiency GainFitted Efficiency Gain Trend : Efficiency gain vs. task difficulty. We fit the scatter plots with a second-degree polynomial to visualize theoverall trend. As task difficulty increases, the general trend of the efficiency gain is to rise initially and then decline,suggesting: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) languagefeedback is most helpful in increasing efficiency for moderate tasks. 0%20%40%60%80%100%",
  ": Performance vs. language frequency. Agentsperform better with more frequent language feedbackacross four environments": "successful task completion with fewer steps.The agents are evaluated on a hundred seeds,where each seed represents a distinct task. We de-fine the task difficulty of each task (or seed) basedon the ranking of the average success rate of agentstrained without language feedback when tested onthat task. This reflects the difficulty for the agentsto learn the task from the training trajectories with-out language assistance.As shown in , the efficiency gain gener-ally rises with increasing learning difficulty, thendeclines. This suggests that: (1) for tasks that aretoo easy or too hard, language feedback does notimprove efficiency; (2) language feedback is mosthelpful in increasing efficiency for moderate tasks.Performance vs. language frequency. For Sec-tion 6.2, we leave it to the online GPT to decidewhether to give language feedback at each time stepto simulate a more realistic human-robot interac-tion. In this section, we explore how the frequencyof language feedback impacts agent performance.We control feedback frequency using a probabilis-tic approach and extract feedback from the GPT-augmented language pool with languages not usedduring training. The evaluation is performed onagents trained with both hindsight and foresightfeedback from GPT-augmented languages.As illustrated in , agents performance 0.1 0.2 0.3 0.4 0.5",
  "Metaworld": "Empty feedbackDisturbed feedback Normal feedbackBaseline trained without languages : We investigate two special evaluation settings:(1) no language feedback is provided during evaluationand (2) adversarial language feedback is given at everystep. Results show that agents trained with the GPT-augmented language still outperform the no-languageagent (the black dotted line) in the adversarial setting,and also achieve better performance in some environ-ments while no language is given. improves steadily across all environments withmore frequent language feedback during evaluation.This finding suggests that agents trained with infor-mative and diverse language feedback can continu-ally absorb and leverage new information when ad-ditional feedback is provided, leading to enhancedperformance. Performance under Empty / Disturbed Lan-guage. We investigate the performance of agentstrained with GPT-augmented languages under twoconditions:the absence of language feedback(empty feedback) during testing and the presenceof disturbed languages (disturbed feedback) at ev-ery step. As disturbed language includes redundant,useless or misleading information (e.g., incorrectactions or objects), and is generated using GPT-augmented templates filled with disturbed content.The purpose of this ablation is to simulate empty orimperfect instructions that humans could possiblycause. The results in reveal two noteworthy be-haviors: (1) Fundamental understanding of the task:When comparing the pink bar with the black dottedline, we observe that when given empty feedback,the agent trained with informative and diverse lan-",
  "No Lang0.3230.270GPT-augmented H0.4500.378GPT-augmented F0.5120.464GPT-augmented H + F0.6230.608": ": Comparison of agents performance adapted(for RQ 2) and evaluated with aligned language type inHomeGrid environment on RQ 1 and Messenger envi-ronment on RQ 2. Aligned (Adapt &) Eval refers to(adaptation &) evaluation with same type of languagein training and Online GPT Eval refers to online GPTevaluation (results in .2). The results show thatGPT-augmented Hindsight + Foresight evaluated withonline GPT still outperforms other training settings evenwith aligned language evaluation, indicating higher lan-guage informativeness and diversity enhance intrinsictask understanding. guage (pink bar) performs as well as or better thanthe agent trained without language (black dottedline). This suggests that our agent does not over-rely on language feedback, but instead developsa strong intrinsic understanding of the underlyingtask. (2) Robustness against perturbation: Whencomparing the green bar with the black dotted line,we see that the performance of the agent trainedwith informative and diverse language does notdrop below the no-language agents performance,indicating the agents robustness in the face of mis-leading informationa desirable property since,in real-world deployment, human feedback mayoccasionally be useless or incorrect.Performance under aligned language type withtraining. As stated in .1, we use onlineGPT for all evaluations in RQ 1 and 2 to mimic real-life human language environments. In this section,we align the evaluation language type (and adap-tation language type in RQ 2) with each agentscorresponding training language type for furtherinvestigation (e.g. No Language Agent is eval-uated with empty language; Template HindsightAgent is evaluated with Template Hindsight). Ex-periments on RQ 1 and 2 are conducted on Home-Grid and Messenger respectively, with the resultspresented in .The results show that:(1) aligning the in-formativeness and diversity levels between train-ing, adaptation and evaluation improves the fi- nal performance for all types; (2) more impor-tantly, even with aligned evaluation and adaptationlanguage, no other settings have outperformedGPT-augmented Hindsight + Foresight evalu-ated with online GPT. This further demonstratesthat high informativeness and diversity in traininglanguage help agents intrinsically understand tasksto achieve better performance.",
  "Conclusion": "In this paper, we investigate how the informative-ness and diversity of language feedback affectembodied agents.We introduce the Language-Teachable Decision Transformer (LTDT), whichmakes decisions based on human language feed-back. To facilitate the training of LTDT agents,we propose an easy-to-use pipeline for collectingoffline hindsight and foresight GPT templates. Wecompare the performance of agents by varying theinformativeness and diversity of the training lan-guages across four reinforcement learning environ-ments and evaluate the agents ability to understandreal-world human language using online GPT as aproxy. Our results demonstrate that training withmore informative and diverse language feedbacksignificantly enhances agent performance and en-ables fast adaptation to unseen tasks.",
  "Limitations": "Our study has several limitations. First, the investi-gated environments are primarily game-based anddo not test the agents ability to incorporate real-lifevisual inputs. Future work will focus on evaluatingagents in more realistic and complex environmentsthat involve real-world visual inputs and challenges.Second, while GPT language outputs can producediverse and contextually relevant language, theymay not fully cover all human language styles andnuances. Specifically, GPT models might misscertain idioms, dialects, or culturally specific ref-erences that are prevalent in human communica-tion. Future work will aim to incorporate a broaderspectrum of language variations and test agents inscenarios involving more diverse linguistic inputs.",
  "Ethical Impacts": "Our study, conducted entirely within simulated en-vironments, does not present immediate ethicalconcerns. The teachable nature of our Language-Teachable Decision Transformer (LTDT) methodis designed to make AI agents more controllable and better aligned with human values, promotingsafer and more ethical interactions. By enhancingagent performance through informative and diverselanguage instructions, we aim to foster AI systemsthat are more transparent and responsive to humanguidance, addressing ethical considerations in thedeployment of artificial intelligence. As AI be-comes more mainstream, these considerations areincreasingly pertinent, and our work strives to ad-vance AI technology responsibly.",
  "Acknowledgements": "This work was supported by NSF IIS-1949634 andhas benefited from the Microsoft Accelerate Foun-dation Models Research (AFMR) grant program.We would like to thank the anonymous reviewersfor their valuable comments and suggestions. Michael Ahn, Anthony Brohan, Noah Brown, Yev-gen Chebotar, Omar Cortes, Byron David, ChelseaFinn, Chuyuan Fu, Keerthana Gopalakrishnan, KarolHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-month, Nikhil Joshi, Ryan Julian, Dmitry Kalash-nikov, Yuheng Kuang, Kuang-Huei Lee, SergeyLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-ter Pastor, Jornell Quiambao, Kanishka Rao, JarekRettinghouse, Diego Reyes, Pierre Sermanet, Nico-las Sievers, Clayton Tan, Alexander Toshev, VincentVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,Mengyuan Yan, and Andy Zeng. 2022. Do as i canand not as i say: Grounding language in robotic af-fordances. In arXiv preprint arXiv:2204.01691.",
  "Philip J Ball, Laura Smith, Ilya Kostrikov, and SergeyLevine. 2023. Efficient online reinforcement learningwith offline data. arXiv preprint arXiv:2302.02948": "Joyce Chai, Maya Cakmak, and Candy Sidner. 2019.Teaching robots new tasks through natural interac-tion. In K. A. Cluck and J. E. Laird, editors, Inter-active Task Learning: Agents, Robots, and HumansAcquiring New Tasks through Natural Interactions.MIT Press. Joyce Chai, Qiaozi Gao, Lanbo She, Shaohua Yang,Sari Saba-Sadiya, and Guangyue Xu. 2018. Lan-guage to action: Towards interactive task learningwith physical agents. In Proceedings of the Twenty-Seventh International Joint Conference on ArtificialIntelligence, IJCAI 2018, July 13-19, 2018, Stock-holm, Sweden, pages 29. ijcai.org.",
  "Yinpei Dai, Jayjun Lee, Nima Fazeli, and Joyce Chai.2024a. Racer: Rich language-guided failure recov-ery policies for imitation learning. arXiv preprintarXiv:2409.14674": "Yinpei Dai, Hangyu Li, Chengguang Tang, YongbinLi, Jian Sun, and Xiaodan Zhu. 2020. Learning low-resource end-to-end goal-oriented dialog for fast andreliable system deployment. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, pages 609618. Yinpei Dai, Run Peng, Sikai Li, and Joyce Chai. 2024b.Think, act, and ask: Open-world interactive person-alized robot navigation. In 2024 IEEE InternationalConference on Robotics and Automation (ICRA),pages 32963303. IEEE. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-dlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,De-An Huang, Yuke Zhu, and Anima Anandkumar.2022. Minedojo: Building open-ended embodiedagents with internet-scale knowledge. Advances inNeural Information Processing Systems, 35:1834318362.",
  "Michael Janner, Qiyang Li, and Sergey Levine. 2021.Offline reinforcement learning as one big sequencemodeling problem. Advances in neural informationprocessing systems, 34:12731286": "Yunfan Jiang, Agrim Gupta, Zichen Zhang, GuanzhiWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, An-ima Anandkumar, Yuke Zhu, and Linxi Fan. 2023.Vima: General robot manipulation with multimodalprompts. In Fortieth International Conference onMachine Learning. Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi,and Aniruddha Kembhavi. 2022. Simple but effec-tive: Clip embeddings for embodied ai. In Proceed-ings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1482914838.",
  "Zeyi Liu, Arpit Bahety, and Shuran Song. 2023.Reflect: Summarizing robot experiences for fail-ure explanation and correction.arXiv preprintarXiv:2306.15724": "Yecheng Jason Ma, William Liang, Vaidehi Som,Vikash Kumar, Amy Zhang, Osbert Bastani, and Di-nesh Jayaraman. 2023. Liv: Language-image repre-sentations and rewards for robotic control. Preprint,arXiv:2306.00958. Sabrina McCallum, Max Taylor-Davies, Stefano Al-brecht, and Alessandro Suglia. 2023. Is feedback allyou need? leveraging natural language feedback ingoal-conditioned rl. In NeurIPS 2023 Workshop onGoal-Conditioned Reinforcement Learning. Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz,Xin Deng, Ahmed Hassan Awadallah, and Julia Kisel-eva. 2023. Improving grounded language understand-ing in a collaborative environment by interactingwith agents through help feedback. arXiv preprintarXiv:2304.10750. Suvir Mirchandani, Fei Xia, Pete Florence, DannyDriess, Montserrat Gonzalez Arenas, Kanishka Rao,Dorsa Sadigh, Andy Zeng, et al. 2023. Large lan-guage models as general pattern machines. In 7thAnnual Conference on Robot Learning. Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, DhruvBatra, Govind Thattai, and Gaurav Sukhatme. 2023.Exploiting generalization in offline reinforcementlearning via unseen state augmentations.arXivpreprint arXiv:2308.03882.",
  "Khanh Nguyen, Hal Daum III, and Jordan Boyd-Graber. 2017. Reinforcement learning for banditneural machine translation with simulated humanfeedback. arXiv preprint arXiv:1707.07402": "Khanh Nguyen, Debadeepta Dey, Chris Brockett, andBill Dolan. 2019.Vision-based navigation withlanguage-based assistance via imitation learningwith indirect intervention.In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1252712537. Khanh X Nguyen, Yonatan Bisk, and Hal Daum Iii.2022. A framework for learning to request rich andcontextually useful information from humans. In In-ternational Conference on Machine Learning, pages1655316568. PMLR.",
  "OpenAI. 2024.Gpt-4 technical report.Preprint,arXiv:2303.08774": "Meenal Parakh, Alisha Fong, Anthony Simeonov, TaoChen, Abhishek Gupta, and Pulkit Agrawal. 2023.Lifelong robot learning with human assisted languageplanners. In CoRL 2023 Workshop on Learning Ef-fective Abstractions for Planning (LEAP). Rafael Figueiredo Prudencio, Marcos ROA Maximo,and Esther Luna Colombini. 2023. A survey on of-fline reinforcement learning: Taxonomy, review, andopen problems. IEEE Transactions on Neural Net-works and Learning Systems. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIBlog": "Scott Reed, Konrad Zolna, Emilio Parisotto, Ser-gio Gmez Colmenarejo,Alexander Novikov,Gabriel Barth-maron, Mai Gimnez, Yury Sulsky,Jackie Kay, Jost Tobias Springenberg, et al. 2022. Ageneralist agent. Transactions on Machine LearningResearch. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics. Ingmar Schubert, Jingwei Zhang, Jake Bruce, SarahBechtle, Emilio Parisotto, Martin Riedmiller, Jost To-bias Springenberg, Arunkumar Byravan, LeonardHasenclever, and Nicolas Heess. 2023.A gener-alist dynamics model for control. arXiv preprintarXiv:2305.10912. Pratyusha Sharma, Balakumar Sundaralingam, ValtsBlukis, Chris Paxton, Tucker Hermans, Antonio Tor-ralba, Jacob Andreas, and Dieter Fox. 2022. Cor-recting robot plans with natural language feedback.Preprint, arXiv:2204.05186. Lanbo She and Joyce Chai. 2017. Interactive learningof grounded verb semantics towards human-robotcommunication. In Proceedings of the 55th AnnualMeeting of the Association for Computational Lin-guistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 16341644.Association for Computational Linguistics. Mohit Shridhar, Jesse Thomason, Daniel Gordon,Yonatan Bisk, Winson Han, Roozbeh Mottaghi, LukeZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-mark for interpreting grounded instructions for every-day tasks. Preprint, arXiv:1912.01734. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2021. ALFWorld: Aligning Text andEmbodied Environments for Interactive Learning.In Proceedings of the International Conference onLearning Representations (ICLR). Ishika Singh, Valts Blukis, Arsalan Mousavian, AnkitGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: Generating situated robot task plans usinglarge language models. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA),pages 1152311530. IEEE. Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, andBill Byrne. 2021.Transferable dialogue systemsand user simulators. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:",
  "Kuan Wang, Yadong Lu, Michael Santacroce, YeyunGong, Chao Zhang, and Yelong Shen. 2023. Adapt-ing llm agents through communication. Preprint,arXiv:2310.01444": "Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,Qian Luo, Victor Zhong, Yanchao Yang, and TaoYu. 2023. Text2reward: Automated dense rewardfunction generation for reinforcement learning. arXivpreprint arXiv:2309.11489. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao. 2022.React: Synergizing reasoning and acting in languagemodels. In The Eleventh International Conferenceon Learning Representations. Denis Yarats, David Brandfonbrener, Hao Liu, MichaelLaskin, Pieter Abbeel, Alessandro Lazaric, and Ler-rel Pinto. 2022. Dont change the algorithm, changethe data: Exploratory data for offline reinforcementlearning. In ICLR 2022 Workshop on GeneralizablePolicy Learning in Physical World. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,Karol Hausman, Chelsea Finn, and Sergey Levine.2019. Meta-world: A benchmark and evaluationfor multi-task and meta reinforcement learning. InConference on Robot Learning (CoRL). Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kir-mani, Kuang-Huei Lee, Montse Gonzalez Arenas,Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasen-clever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu,Andy Zeng, Tingnan Zhang, Nicolas Heess, DorsaSadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. Lan-guage to rewards for robotic skill synthesis. Arxivpreprint arXiv:2306.08647. Yichi Zhang and Joyce Chai. 2021. Hierarchical tasklearning from language instructions with unifiedtransformers and self-monitoring. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 42024213, Online. Associationfor Computational Linguistics. Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks,Nikhil Devraj, Ziqiao Ma, Keunwoo Yu, Yuwei Bao,and Joyce Chai. 2022. DANLI: Deliberative agent forfollowing natural language instructions. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 12801298,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Yichi Zhang, Jianing Yang, Keunwoo Yu, Yinpei Dai,Shane Storks, Yuwei Bao, Jiayi Pan, Nikhil Devraj,Ziqiao Ma, and Joyce Chai. 2023. Seagull: An em-bodied agent for instruction following through situ-ated dialog. In Alexa Prize SimBot Challenge Pro-ceedings.",
  "A.2.1HomeGrid": "HomeGrid is a multitask grid world designedto evaluate how well agents can understand anduse various types of language to complete tasks.Agents will receive both task specifications andlanguage hints, providing prior knowledge aboutworld dynamics, information about world states, orcorrections to assist the agents. We adopt the lan-guage hints in HomeGrid as foresight and furtherextend the environment to provide hindsight thatprovides comments on agents past performance.Agents are expected to ground both hindsight andforesight to the environment to achieve higher per-formance. It includes five task types involving in-teraction with objects and bins (find, get, clean up,rearrange, open), with a total of 38 tasks. Object locations, bin locations, and bin dynamics are ran-domized. The agent receives a reward of 1 whenthe task is completed, and receives a reward of0.5 if a subgoal exists (e.g., get the object in theclean-up task) and gets completed. Each templatelanguage is augmented to 70 sentences in the GPTtemplate pool. Examples of hindsight and foresightlanguages are as follows:",
  "A.2.2ALFWorld": "ALFWorld is a text-game environment that alignswith the embodied ALFRED benchmark (Shridharet al., 2020) and provides simulation for house-hold tasks. It includes six types of tasks whereagents need to navigate and interact with house-hold objects through text actions. The location ofthe task objects is randomly located among 50 loca-tions in each episode, making the task challengingfor the agent to plan and for the subgoals. Forthe experiment, we adopt LLF-ALFWorld (Chenget al., 2023), which provides an extra languagewrapper for hindsight and foresight language gen-eration over the original ALFWorld. The languagesare generated based on both agents past actionsand the optimal trajectory for the current episode.Agent gets a reward of 1 when the task is completed.Each template is augmented to 200 sentences in",
  "\"Moving on, consider the {action}action.\"": "Language instructions are generated based on ex-perts next action and whether agents past actionsare aligned with expert past actions, consideringwhether agents have moved to the target positionand conducted correct interaction with the objects. A.2.3MessengerMessenger is a grid world with several entities.The agents primary task is to retrieve a messagefrom one entity and deliver it to another goal entity,all while avoiding enemies. At the start of eachepisode, the agent is provided with a manual de-scribing the randomized roles of the entities andtheir movement dynamics. The challenge lies inthe fact that the agent does not have access to thetrue identity of each entity and must ground thetext manual to the dynamics, necessitating multi-hop reasoning. (For example, grounding the \"anapproaching queen is a deadly enemy\" to the obser-vations of dynamics.) (Lin et al., 2023) The agentreceives a sparse reward of 1 when the task is com-pleted. Each template language is augmented to 80sentences in the GPT template pool. Examples ofhindsight and foresight languages are as follows:",
  "\"Not detecting any danger, its safe.\"": "When generating the language instructions, wecompare the agents actions and the experts ac-tions, considering the locations of the target andnearest enemy, calculating the distance and gener-ate the hindsight reflections based on some engi-neered rules. A.2.4MetaWorldMetaWorld is a simulated benchmark that includesa variety of manipulation tasks performed using aSawyer robot arm. It includes 50 types of robotmanipulation tasks common in daily life. Sinceour main goal is not meta-learning, we select the\"assembly\" and \"hammer\" tasks for pretraining andadaptation in our experiments. This requires theagent to pick up the tool and aim at the specific tar-get with high precision. To increase the challengeof the tasks, we introduce random disturbances atrandom steps. This requires the robot to actively re-cover and return to its normal trajectory wheneverit deviates. The agent receives a sparse reward of 1when completing the task. Each template languageis augmented to 180 template languages in the GPTtemplate pool. Examples of hindsight and foresightlanguages are shown in the following:",
  "BAgent for Offline Data Collection andLanguage Feedback Generation": "We use an expert agent and a non-expert agent withsub-optimal policies during the data collection. Thesub-optimal policy is used for introducing some er-rors or perturbations in the training data, and lettingthe expert policy continue to recover. This helpsagents learn to recover from potential failures usinghindsight reflections and foresight instructions. Inour experiments, we introduced 10-20% randomnoise in each trajectory as a sub-optimal policy. Wefound that this level of perturbation aids learning,but excessive disturbance (e.g., >50% per trajec-tory) significantly degrades performance as agentsstart learning suboptimal behaviors.",
  "B.1HomeGrid": "For the HomeGrid environment, we design an ex-pert planer to work as the expert agent. We firstdivide the task into several sub-tasks (i.e. divide\"open the recycling bin\" into 1. \"navigate to thebin\", 2. \"open the bin\"). For navigation (moveto some place) sub-tasks, we implement breadth-first search to find the optimal path; for inter-action sub-task (interact with object), we outputthe corresponding action. We implement the non-expert agent by adding \"perturbation\" into the ex-pert planer. For example, we randomly reverse the",
  "B.3Messenger": "As for the Messenger environment, we implementan expert agent using the A* algorithm (Hart et al.,1968). We define the cost by the distance to thetarget and the distance to the nearest enemies, andthen heuristically search in the grid environment.The non-expert agent in the data collection is im-plemented by adding random disturbance to theexpert agent.",
  "RANGE and OPEN ; 2) in ALFWorld, we train andevaluate on multi-tasks including PICK&PLACE,": "CLEAN&PLACE and HEAT&PLACE tasks; 3) inMessenger, we train and evaluate on the task goalfirst retrieve the message and then deliver to targetentity; and 4) in MetaWorld, we train and evalu-ate on the ASSEMBLY task, in which the robot armneeds to pick up the wrench and put it on the peg.Task Setting for RQ 2. We evaluate agents perfor-mance on unseen tasks by first pre-training agentson certain tasks and then adapting agents to un-seen tasks with few-shot episodes. Specifically, 1)in HomeGrid, we take FIND, GET, REARRANGE, OPEN tasks for pre-training and the CLEAN-UP taskfor adaptation and evaluation; 2) in ALFWorld, wetake PICK&PLACE and CLEAN&PLACE for pre-training and HEAT&PLACE tasks for adaptationand evaluation; 3) in Messenger, we take first re-trieve the message and then deliver to target entity\" as the pretraining task and first get to the targetentity and then retrieve the message\" (where theorder of the goal is reversed compared to the pre-training tasks) for adaptation and evaluation; 4) inMetaWorld, we take the ASSEMBLY task for pre-training, and the HAMMER task for adaptation andevaluation.",
  "DImpact of hindsight on future steps": "Compared to foresight feedback, which providesinstructions for the correct action in the next step,hindsight feedback reflects on incorrect actionstaken in previous steps. This retrospective analysiscan still guide agents toward success by narrow-ing down the search space for corrective actions.To demonstrate the effectiveness of hindsight feed-back, we conduct a quick comparative study be-tween the No Language agent and the TemplateHindsight agent in HomeGrid. The study wasdesigned as follows:",
  "Navigation37.6 0.346.2 0.2Object Pick/Drop37.4 2.541.8 1.6Bin manipulation23.5 1.224.8 0.9": ": Comparison of performance between No Lan-guage Agent and Template Hindsight Agent on dif-ferent Mistake Types.results indicate that for the navigation and objectpick/drop mistakes, hindsight feedback is highlybeneficial. This is because identifying a wrong ac-tion usually directly implies the correct action forthose mistakes (e.g., if \"turn left\" is wrong, \"turn 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8",
  "EMore results on the Messengerenvironment": "In the Messenger environment, models trainedwith only template foresight or hindsight languagesstruggle to generalize to diverse languages duringtesting. Without exposure to diverse languages dur-ing training, these models fail to extract the learnedhindsight or foresight information from mixed anddiverse languages. However, demonstratesthat models trained with more diverse hindsight orforesight languages can overcome the generaliza-tion problem, and outperform those trained withoutlanguage feedback, showcasing the importance ofdiversity in the training languages. Furthermore,the agents trained with both hindsight and foresightinformation still perform the best, aligning withresults in other environments.",
  "F.1HomeGrid": "Estimated parameter size of the models: 12.191MB. For research question 1, we train the modelwith 100 trajectories. For research question 2, thepretraining stages use 6432 trajectories. The mod-els are trained on one Nvidia RTX A6000. Forresearch question 1, training takes 3 GPU hours.For research question 2, pretraining takes 4 GPUhours and adaptation takes 3 GPU hours. Hyperpa-rameters shown in Appendix .",
  "F.2ALFWorld": "Estimated parameter size of the models: 6.5 MB.For research question 1, we train the model with1000 trajectories. For research question 2, the pre-training stages use 10000 trajectories. The modelsare trained in one Nvidia RTX A6000. For re-search question 1, training takes 3 GPU hours. Forresearch question 2, pretraining takes 4 GPU hoursand adaptation takes 3 GPU hours. Hyperparame-ters shown in Appendix .",
  "F.3Messenger": "Estimated parameters size of the models: 289.681MB. We train the models with 10000 data trajecto-ries during the pretraining stage for seen tasks. Thepretraining stage for seen tasks takes 5 GPU hourson one Nvidia RTX A6000. The adaptation stagefor unseen tasks takes 1 GPU hour. Hyperparame-ters are shown in Appendix .",
  "F.4MetaWorld": "Estimated parameters size of the models: 289.681MB. We train the models with 20000 data trajec-tories during the pretraining stage for seen tasks.The pretraining stage for seen tasks takes 2.5 GPUhours on one Nvidia RTX A6000. The adaptationstage for unseen tasks takes 1 GPU hour. Hyperpa-rameters are shown in Appendix ."
}