{
  "Abstract": "Argument mining (AM) involves the identi-cation of argument relations (AR) betweenArgumentative Discourse Units (ADUs). Theessence of ARs among ADUs is context-dependent and lies in maintaining a coher-ent ow of ideas, often centered around therelations between discussed entities, topics,themes or concepts.However, these rela-tions are not always explicitly stated; rather,inferred from implicit chains of reasoningconnecting the concepts addressed in theADUs.While humans can infer such back-ground knowledge, machines face challengeswhen the contextual cues are not explicitlyprovided.This paper leverages external re-sources, including WordNet, ConceptNet, andWikipedia to identify semantic paths (knowl-edge paths) connecting the concepts discussedin the ADUs to obtain the implicit chains ofreasoning. To effectively leverage these pathsfor AR prediction, we propose attention-basedMulti-Network architectures.Various archi-tecture are evaluated on the external resources,and the Wikipedia based conguration attainsF-scores of 0.85, 0.84, 0.70, and 0.87, re-spectively, on four diverse datasets, showingstrong performance over the baselines.",
  "Introduction": "Argument mining involves identifying the argu-mentative structure within a text.It includessegmenting arguments into Argumentative Dis-course Units (ADUs) (Peldszus and Stede, 2015a),distinguishing argumentative units from non-argumentative ones, classifying ADUs, labelingargument relation (AR) between ADUs, and iden-tifying argument schemes (Persing and Ng, 2016;Stab and Gurevych, 2017; Lawrence and Reed,2020). This study focuses on classifying the ARbetween ADUs into three categories: Inference(RA) (when one ADU supports the other), Conict(CA) (when one ADU attacks the other), and None(when there is no AR). The nature of AR is inherently context-dependent (Potash et al., 2017; Habernal et al.,2017; Choi and Lee, 2018; Rinott et al., 2015),relying on maintaining a coherent ow of inter-connected ideas. This cohesion is often centeredaround the connections between the discussed en-tities, topics, themes or concepts, commonly re-ferred to as Local coherence (Foltz et al., 1998;Marcu, 2000). Local coherence facilitates smoothidea transitions between ADUs by recognising in-herent regularities in entity distribution. Similarly,other entity-based theories of discourse (Givn,1987; Prince, 1981) and Centering Theory (Groszet al., 1995) propose that these regularities con-tribute to the coherence of discourse by guidingthe organisation of ideas around salient entities.Following a similar framework, aspect-based argu-ment mining techniques use the relationships be-tween the concepts discussed in ADUs, to identifyargument structures (Misra et al., 2017; Dragoniet al., 2018; Gemechu and Reed, 2019; Trautmann,2020). Yet, the contexts required to link these con-cepts are not always explicit and are often inferredfrom background knowledge.Pre-trained large language models (LLMs) havetransformed NLP, moving from traditional featureengineering to data-driven approaches. Studies in-dicate that these models implicitly capture varioustypes of knowledge, including relational, common-sense, and structural linguistic knowledge, withintheir parameters (Petroni et al., 2019; Goldberg,2019; Safavi and Koutra, 2021; AlKhamissi et al.,2022). While excelling in various NLP tasks, theirability to encode the necessary background knowl-edge for identifying ARs remains uncertain (Kass-ner and Schtze, 2019). For example, Polu et al.(2022) revealed their limitations in chaining multi-ple steps of complex logical reasoning, while Mer-rill et al. (2021) demonstrated they fail to compre-hend the semantics behind commonsense reason-ing tasks. This limitation is critical in AR iden- tication, as linking ADUs relies on the implicitchain of reasoning, often inferred from the chainof relations between the concepts discussed in theADUs. This highlights the need for supplementarycontextual information from external sources toestablish these connections.Consider the ADUs from the 2016 presidentialelection debate corpus (Visser et al., 2019) in . Identifying the AR between (1) and (2) relieson recognising the relationship between NAFTAagreement and USA, whereas for (4) and (5), itrequires understanding building electric grid isan economic activity. While these connectionsare straightforward for human experts, computersface challenges as such interconnections are oftenimplicitly inferred. For example, the AR between(3) and (4) is direct as the relation between theconcepts mentioned in the respective ADUs canbe obtained from an ontology (Miller, 1995; Speeret al., 2017) (Electric grid; grid is directly relatedto power; electrical power in WordNet (Miller,1995)) or by comparing their embeddings (Pilehvaret al., 2013; Le and Mikolov, 2014; Reimers andGurevych, 2019). However, identifying the ARbetween (4) and (5) is challenging since the pathlinking electric grid to economic activity ismissing in existing knowledge resources includ-ing WordNet (Miller, 1995) or ConceptNet (Speeret al., 2017) or DBpedia.1 However, the conceptsare indirectly linked in Wikipedia through a chainof concepts interlinked using a set of semantic re-lation types: economic activity involves innova-tion which constitutes developing clean energytransmitted by electric grid. This study aims toidentify and leverage the chain of such semanticrelations between the concepts, to capture implicitreferential information between ADUs (Asher andLascarides, 2003) and use it for AR prediction.",
  "NoADUs": "1[USA]C [is in deep trouble]OC2[NAFTA agreement]C [is defective]OC3[We]C [can have]OC [clean energy]A4[We]C [can build]OC a new modern [electric grid]A5[This]C [is a lot of]OC new [economic activity]A : Examples from 2016 presidential election de-bate corpus (Visser et al., 2019) to illustrate the rela-tion between the functional components of ADUs. Crepresents the theme of the sentence, A represents theaspects specialising the theme, while the opinion on Cis represented by OC. Leveraging knowledge from external resourceshas been shown to improve performance in AM(Kobbe et al., 2019; Botschen et al., 2018; Frommet al., 2019; Plenz et al., 2023) and related tasks,such as semantic plausibility (Wang et al., 2018),identifying inferences (Chen et al., 2017), and de-termining entailment (Glockner et al., 2018). How-ever, existing studies on AR prediction exclusivelyutilise structured knowledge bases and overlooksemi-structured resources like Wikipedia, whichcontains over 6,805,837 articles (as of April 1,2024), offering richer connections through hyper-links embedded within articles. Moreover, thesemethods rely on entities, events, and factual infor-mation sourced from structured databases, limitingtheir applicability to specic domains. In contrast,using generic semantic relation types that encodeAR ensures adaptability across domains (refer to for examples of such relation types). Fur-thermore, they lack effective method for integrat-ing the external information into model architec-tures, relying instead on conventional feature en-gineering techniques. For instance, Kobbe et al.(2019) leverage features derived from graph rep-resentations of the resources, including the inter-concept distances within the graph.Similarly, Plenz et al. (2023) employ semantic similarity todetermine the relevance of external knowledge, inconjunction with traditional features derived fromthe graph representation of the resources.In this paper, we propose traversing Wikipedia,WordNet, and ConceptNet to nd semantic pathslinking concepts mentioned in ADU pairs. ARsbetween ADUs are identied by leveraging thesepaths using attention-based Multi-Network archi-tectures. To establish a benchmark, we evaluateLLMs across various congurations, comparingthe knowledge obtained from external resourceswith that inherent in LLMs. The evaluation demon-strated that integrating external resources consis-tently enhances performance, showing strong per-formance over the baselines and comparison ap-proaches. Additionally, we assess the effectivenessof the attention-based Multi-Network architecturein leveraging external knowledge, demonstratingits superiority over the standard linear classica-tion baseline. The contribution of this paper is four-fold: (a) the utilisation of both structured and semi-structured external resources for AR prediction,(b) architecture for effectively leveraging externalknowledge, (c) features adaptable across domains,and (d) the state-of-the-art performance.",
  "Related Works": "In the literature, AM has been approached usingvarious congurations, including dependency pars-ing (Peldszus and Stede, 2015b), discourse parsing(Muller et al., 2012), sequence tagging (Eger et al.,2017; Mayer et al., 2020), and sequence classi-cation congurations (Reimers et al., 2019; Ruiz-Dolz et al., 2021; Mayer et al., 2020). Variousworks tackle specic AM tasks. Some focus ex-clusively on argument segmentation (Chernodubet al., 2019; Ajjour et al., 2017), while others startwith segmented data and focus solely on AR iden-tication (Potash et al., 2016; Gemechu and Reed,2019; Ruiz-Dolz et al., 2021). Potash et al. (2016)train an encoder-decoder (Sutskever et al., 2014)with attention mechanism (Bahdanau et al., 2014)to identify AR. Gemechu and Reed (2019) decom-pose ADUs into ne-grained components and useclassiers to predict AR based on the relations be-tween these components. Chakrabarty et al. (2020)identify argument components and ARs withinboth inter-turn and intra-turn interactions in dia-logues. They classify ARs as a binary prediction,determining only the presence of a relation withoutspecifying its type. Their ndings indicate that us-ing distant-labeled data and integrating discourserelations from Rhetorical Structure Theory (Mannand Thompson, 1988) improve performance.End-to-end AM approaches address multipleAM tasks, simultaneously. Persing and Ng (2016)and Stab and Gurevych (2017) adopt a pipelinearchitecture and train separate models for eachsubtask to then utilise an Integer Linear Program-ming (ILP) model to encode global constraints.Eger et al. (2017) propose a neural end-to-end ap-proach, framing the task in various congurationsincluding dependency parsing and token-based se-quence tagging. They also employ a multi-tasksetup to leverage the dependencies between AMtasks, including component identication and ARprediction. Their best-performing congurationachieves an F1-score of 0.51 for AR identicationon the AAEC dataset. Peldszus and Stede (2016)aim to map RST trees to argumentation structures(Taboada and Mann, 2006) using sub-graph match-ing and an evidence graph model. They evaluatevarious features of their system on the AMT datasetand achieve an overall F-measure of 0.76 in iden-tifying ARs. Similarly, Morio et al. (2022) intro-duce an end-to-end cross-corpus training strategythat facilitate information transfer between datasets. Mayer et al. (2020) address argument componentand relation identication on a dataset comprisingvarious disease treatments. The approach involvescombining static and dynamic embeddings usingvarious congurations of RNN and CRFs. Theydemonstrate the efcacy of specialised LLMs likeSciBERT (Beltagy et al., 2019), highlighting theirrelevance in medical domain adaptations. How-ever, most of these works rely on the informationexplicitly provided in the argument alone.Recent AM works ne-tuned LLMs in sequenceclassication fashion (Reimers et al., 2019; Ruiz-Dolz et al., 2021). Studies show that such LLMsimplicitly capture relational, commonsense, andstructural linguistic knowledge (Petroni et al.,2019; Goldberg, 2019; Safavi and Koutra, 2021;AlKhamissi et al., 2022). Despite their signicantperformance, the ability of LLMs to encode therequisite background knowledge for identifyingARs remains uncertain, raising concerns about re-lying solely on LLMs for this task (Kassner andSchtze, 2019). For instance, Polu et al. (2022)exposed their limitations in complex logical rea-soning, while Merrill et al. (2021) showed theystruggle in comprehending the semantics of com-monsense reasoning tasks.The works most related to ours are those of Kobbe et al. (2019) and Plenz et al. (2023), asthey also leverage external knowledge bases toidentify AR. However, their methodologies dif-fer signicantly from ours. Firstly, they rely onstructured knowledge bases with predened rela-tion types, while we also use semi-structured re-sources like Wikipedia that cover diverse relations.Furthermore, they struggle to effectively integrateexternal information into model architectures, re-lying instead on conventional feature engineeringtechniques that exploit structural features obtainedfrom sub-graph extracted from external knowledgebases. For instance, Kobbe et al. (2019) use fea-tures like the frequency of relations existing be-tween ADUs. Similarly, Plenz et al. (2023) lever-age the similarity between external knowledge andADUs to identify relevant sub-knowledge graphsand exploit the sub-graph to extract categoricalfeatures, such as the number of shared conceptsbetween ADU pairs and the path lengths betweenthe concepts. Additionally, the formalisation ofthe concepts used for alignment with externalresources is vague, relying on arbitrary entity men-tioned in the ADUs. Moreover, their approach forAR identication has not been evaluated.",
  "Methodology": "Our approach comprises two main stages: rst, wealign a pair of ADUs (premise-conclusion) withexternal knowledge resources and extract the rele-vant knowledge paths connecting them; second, weincorporate these knowledge paths into our modelarchitecture using attention-based Multi-Networksto predict the AR between the ADUs. The follow-ing subsections provide details on the data usedand the processes involved in each step.",
  "Data": "We use four corpora. The rst is AAEC (Stab andGurevych, 2017) which has a total of 402 argu-ments. ADUs under each argument are labelledas premise, claim or major claim. It has 147,271tokens, 6,089 ADUs and 5335 ARs (4841 supportand 497 attack).The second corpus is the Argumentative MicroText (AMT) (Peldszus and Stede, 2013) which is acollection of 112 short texts collected from humansubjects in German translated into English. It isannotated following the argumentation structureoutlined by MicroTextAnnotation. The structureconsists of a central claim, and supporting ADUs.It has a total of 8007 tokens, 576 ADUs and 443ARs (272 support and 171 attack).The third corpus is part of the US 2016 presi-dential election debate corpus (US2016) (Visseret al., 2019) which is annotated based on InferenceAnchoring Theory (IAT) (Budzynska and Reed,2011). Argument components are referred to aspropositions, with the relations between them an-notated as default inference for support and defaultconict for attack. The corpus has a total of 15805tokens, 1473 ADUs and 584 ARs (505 support and79 attack).The fourth corpus is the AbstRCT corpus(Mayer et al., 2020) which consists of abstractsextracted from the MEDLINE database. Argu-ment components are categorised into major claim,claim, and evidence components, and the relationsbetween them are categorised into support, attack,and partial-attack. The corpus consists of 100,253tokens, 4,679 ADUs, and 2,634 ARs, including344 attack relations (combining attack and partial-attack relations) and 2,290 support relations.As described above, argument components areannotated non-uniformly across datasets, based onthe underlying theoretical framework. For exam-ple, in AAEC, argument components are annotated as premises, claims, and major claims. However,in US2016, the components are not explicitly cate-gorised, but the premise-conclusion notion can beinferred from the direction of the AR. As our cur-rent objective does not involve classifying the com-ponents or the direction of the relation, we focus onthe AR existing between the components withoutclassifying the categories of the components (intoclaim/conclusion/major-claim, premise/evidence).",
  "External Knowledge Alignment andPaths Extraction": "Each ADU is annotated into its four functionalcomponents, following the framework proposed byGemechu and Reed (2019) (see Appendix A.3 formore details). These components are used to alignthe respective ADUs with the external resources.The functional components consist of target con-cepts (C), aspects (A), opinions on C (OC), andopinions on A (OA). C refers to the set of con-cepts related to the ADUss topic, while A refersto the set of concepts further specifying that topic(examples provided in ). In this study, wefocus on C and A, which represent the topics andaspects addressed by the ADUs. The statistics ofthese components can be found in in theAppendix.To extract relevant external knowledge, wealign these components with two ontological re-sourcesWordNet (Miller, 1995) and ConceptNet(Speer et al., 2017)as well as a semi-structuredresource, Wikipedia. The detailed alignment pro-cess is described in Sections 3.2.1 to 3.2.2.",
  "Ontology as External Source": "We traverse WordNet (Miller, 1995) and Concept-Net (Speer et al., 2017) Synset hierarchies andalign the components of ADUs with the Synsets,to identify the chain (path) of Synsets that connectsthe components. The alignment relies on cosinesimilarity between the embeddings of the compo-nents and Synsets, determined by the cosine simi-larity threshold . Sentence-transformer (Reimersand Gurevych, 2019) is utilised to identify the em-beddings. For more details on the embeddings andsimilarity threshold, check Appendix A.3.2.By treating the ontology as a graph, with Synsetsas nodes and relation types as edges, we begin thesearch with one of the components and traverse theknowledge graphs until either the other componentis found or the search depth reaches the threshold = 5. For more details on setting the value of , refer to Appendix A.3.3. If the search is suc-cessful, we concatenate the Synsets and the typeof semantic relation between, otherwise return theconcatenation of both components with the con-stant string None in between. We use relationtypes with frequency higher than m=3 to form thepaths (see Appendix A.3.4 for more informationabout the relation ltering process).",
  "Wikipedia as External Source": "We also traverse Wikipedia to identify the chain(path) of Wikipedia pages linking the functionalcomponents of ADUs. For any pair of compo-nents (e.g., C1, A2 or C1, C2 or A1, A2) asso-ciated with a pair of ADUs (p1, p2), the initialstep involves aligning these components with cor-responding Wikipedia pages. This alignment isachieved by computing the similarity between theembeddings (Reimers and Gurevych, 2019) of theWikipedia page titles and the components.Viewing Wikipedia as a graph (with pages asnodes and hyperlinks as edges), we begin a breadth-rst search from the Wikipedia page of one concept(c1), continuing until we locate the second concept(c2) or reach a depth threshold, = 5. Duringthis search, we record sentences (S) containingWikipedia page titles of the current page (hl1) andthe hyperlinks leading to the next Wikipedia page(hl2) along the path. These sentences contributeto the formation of a tuple: hl1, hl2, keywords,where the keywords represent the semantic relationtype linking hl1 and hl2 within the sentences.We utilise semantic role labeling (SRL) to iden-tify the keywords that connect hl1 and hl2 withinthe sentences (S) containing the hyperlinks. TheSRL tool from AllenNLP 2 is used for this purpose.The process involves extracting subject-predicatestructures that link hl1 and hl2 in the sentencesinvolving the hyperlinks, followed by identifyingphrases that connect them across the semantic rolesassigned (see Appendix A.3.5). Top m most fre-quent relations are selected to construct the paths.",
  "Attention-Based Multi-Network": "We investigate two attention-based Multi-Networkcongurations,namely Siamese and Triplet(Schroff et al., 2015) networks, built using pre-trained LLM blocks.Initially, we utilise theSiamese network involving two sub-networks,where one sub-network encodes the concatenationof both ADUs together while the other encodesthe external information. Furthermore, we exam-ine Triplet network, which uses three sub-networkto encodes each of the ADUs and the external re-sources separately.Siamese Network Architecture with Atten-tion. In this setup, given the two sub-networks(E1 and E2) in Siamese network, E1 processes theconcatenation of the pair of ADUs (premise andconclusion), while E2 handles the concatenationof the information from external resources. Crossattention layer (Vaswani et al., 2017) (ED-att-1)is applied to the outputs of these sub-networksfor attending to the external resources relevant tothe premise and conclusion (see ). Ac-cordingly, the output of E1, which represents thepremise and conclusion, functions as the query,while the output of E2, representing externalknowledge, serves as the keys and values. Thissetup allows the premise and conclusion to queryrelevant external information. It employs multi-head attention (Vaswani et al., 2017) h, where eachhead j computes scaled dot-product attention usingquery Qj, key Kj, and value Vj matrices, whichare linear transformations of the input hidden statehi. The nal attention weight ei is obtained byconcatenating over all attention heads. The result- ing attention weights are then multiplied with theoutput of E1, and passed through a fully connectedclassication layer, for AR classication. This fu-sion allows the model to integrate the original rep-resentations of the premise and conclusion with theextracted external information (see detailed modelparameters in the Appendix A.5).Triplet Network Architecture with Atten-tion. In contrast to the Siamese Architecture, theTriplet Network Architecture consists of three sub-networks: E1, E2, and E3 (see in Ap-pendix 3.3.1). Sub-networks E1 and E2 encodethe premise and conclusion, respectively, while E3encodes the external knowledge connecting them.Two cross-attention layers are introduced (ED-att-1 and ED-att-2). ED-att-1 focuses on the relationbetween the premise and conclusion, where theoutput of E1 serves as queries and the output of E2is used as keys and values. On the other hand, ED-att-2 attends to the external knowledge relevant tothe premise and conclusion. Specically, the out-put of ED-att-1 acts as the query, while the outputof E3 is used as keys and values. Similar to theSiamese architecture, we combine the output of thetwo attention layers for classication. The ratio-nale behind this approach is that ED-att-1 encodesthe relation between the premise and conclusion,while ED-att-2 encodes the relevant external re-source, enabling the model to effectively leverageboth the relationship between the premise and con-clusion and the relevant external knowledge forAR classication. 3.3.2LLMs as Baseline ModelsWe establish LLMs without external resources asbaseline models under two congurations: few-shot and fully ne-tuning congurations. We evalu-ate these baselines against congurations that lever-age external knowledge sources to enhance the per-formance of LLMs.Few-shot setup: We prompt GPT-43, a gen-erative LLM, to perform two tasks: (a) predict-ing ARs for comparative analysis against mod-els utilising external resources, and (b) generat-ing paths between ADU components for compar-ison with models using paths derived from ontol-ogy and Wikipedia. Accordingly, GPT-4-generatedpaths are used as external knowledge to train theMulti-Network conguration for AR classication.This enables a direct comparison between GPT-4-generated paths and those obtained from other ex- ternal knowledge sources. The experimental setupfor prompting GPT-4 is provided in A.4.Fine-tuning setup: We also ne-tuned BERT(Devlin et al., 2018) using various congurationsfor comparison. Initially, we use the vanilla se-quence classication setup (SCVbert), wherethe concatenation of ADUs is presented as an input.Furthermore, we ne-tune BERT within Siamesearchitectures, both with (SMAbert) and with-out attention layers (SMVbert). See A.1 for thedetails of model conguration and experimentalsetups.",
  "Experimental Setup": "The dataset is randomly partitioned, with 70%,10%, and 20% allocation for training, valida-tion, and testing respectively, ensuring uniformitythroughout the dataset. Refer to in theAppendix for the breakdown of ARs accross thedatasets. Results represent the average of threeruns using different random seeds. Precision (P),recall (R), and F-measure (F) are computed, andmacro-averaged P, R, and F are reported for thetest dataset (more experimental setup provided inAppendix A.1). The datasets and code used in ourexperiments are publicly available.4",
  "Model Congurations": "The encoder blocks in both the Siamese and Tripletnetworks are built using BERT. The cross-attentionlayers use 8 heads, matching standard transformerarchitecture, with outputs concatenated and thenpassed to the feedforward and classication layers.Both the Siamese and Triplet networks use a sin-gle BERT model for all encoders, sharing identicalparameters across the networks. We apply cross-entropy loss based on the nal classication layeroutput, as the Triplet loss in the multi-network ar-chitecture is not suitable for AR prediction. Weevaluate various congurations leveraging the twoontological resources (WordNet, and ConceptNet)and Wikipedia across the four datasets. These con-gurations encompass three Triplet network archi-tectures: TLAwp for Wikipedia, TLAwnfor WordNet, and TLAcn for ConceptNet.Similarly, three Siamese network architecturesare evaluated across these ontological resources:SMAwp, SMAwn, and SMAcn.",
  "Att + ExtPRFPRFPRFPRF": "TLAgpt773.0842.0803.0714.0854.0773.0563.0723.0633.0732.0854.0793.0SMAwn840.1810.1820.1830.1790.1810.1620.1720.1670.1830.0820.0830.0SMAcn830.3810.2820.2810.2820.2810.2650.1720.1680.1850.1850.0850.1SMAwp850.2810.1830.1820.2830.2820.1650.2730.2690.2850.1860.0860.0TLAwn850.1820.1840.1830.2840.2840.1640.2720.3680.2830.1830.0830.0TLAcn840.1820.2830.1820.1840.1830.1650.2730.3690.2860.0850.0860.0TLAwp860.1830.2850.2830.1850840.0660.1750.1700.1870.1860.0870.0 : Performance of our models and the comparison systems including, (Potash et al., 2016) (P2016), (Egeret al., 2017) (E2017), (Peldszus and Stede, 2016) (PS16), (Kobbe et al., 2019) (K2019), (OpenAI, 2023) (GPT-4),(Gemechu and Reed, 2019) (GR2019), (Mayer et al., 2020) (M2020) across the four datasets. The reported resultshave been averaged from 3 randomly initialised sequential runs. The table is divided into subsections: Comparisonapproaches; LLM-alone; non-attention with external sources; attention-based with external resources. Furthermore, to evaluate the attention layersimpact on external resources, we compare Tripletand Siamese architectures without attention layersacross the three external resources, totaling six con-gurations: TLVwp, TLVwn, TLVcn,SMVwp, SM Vwn, and SMVcn. Fi-nally, we evaluate the Triplet architecture on GPT-4generated paths (TLVgpt, TLAgpt).",
  "Results and Discussions": "The evaluation results depicted in revealedclear trends in performance. Particularly, the inu-ence of model architecture and the incorporationof external knowledge on AR prediction. This isevident by the performance improvement observedin congurations with such integration comparedto those without.Models incorporating external resources outper-formed those lacking such integration, indicatingthe importance of leveraging additional knowledge sources for AR identication. This led to a notableenhancement, surpassing the baseline by over 5.4%in F-measure. For example, the Siamese architec-ture leveraging Wikipedia achieved an average F-measure of 80% across datasets, whereas its coun-terpart, lacking the external resource, achieved74%. This nding aligns with previous researchdemonstrating that while LLMs tend to encodeworld knowledge, LLMs alone may not fullypresent the depth and specicity of knowledge re-quired for certain tasks, such as AR identicationinvolving structured and chained reasoning (Kass-ner and Schtze, 2019; Polu et al., 2022; Merrillet al., 2021). Likewise, models equipped with at-tention mechanisms consistently surpassed thosewithout, demonstrating an average increase in F-measure of over 2% across diverse congurations.Notably, Triplet Network architecture with atten-tion mechanism leveraging Wikipedia as an ex-ternal knowledge source, attained an average F- measure of 81% across the datasets. This demon-strates a signicant performance improvement inAR identication, highlighting the effectiveness ofthe architecture in integrating external knowledge.We also compare our approach to other relatedworks including Potash et al.s (2016), Eger et al.s(2017), Peldszus and Stedes (2016), Kobbe et al.s(Kobbe et al., 2019), OpenAIs GPT-4 (OpenAI,2023), Gemechu and Reeds (2019; 2023) andMayer et al.s (2020) work. Please note that di-rect comparisons with some of these works needadditional contextual nuance in interpretation dueto variations in task setup and complexities. Forinstance, the works of Eger et al. (2017) and Mayeret al. (2020) involve argument segmentation in ad-dition to AR identication as an end-to-end task.In our case, the goal is to identify AR based on cor-rect segments in the gold datasets. Similarly, Plenzet al. (2023) evaluate their approach on several AMtasks, including ValNov Shared Task (Heinischet al., 2022), which involves assessing the validityand novelty of a conclusion given a premiseatask closely related to AR prediction. They reportan F1 score of 70.69% for this task. As can beseen from , our approach outperforms thecomparison systems, including OpenAIs GPT-4(OpenAI, 2023) across the datasets.Model Architecture Inuence.As shownin , incorporating attention layers intoMulti-Network architectures brought clear benets.Multi-network congurations with attention mech-anisms outperformed the vanilla sequence classi-cation setup, both with and without external knowl-edge, achieving an average F1 gains of 6.4% and1%, respectively. Attention-based congurationsleveraging external resources consistently outper-form their counterparts without attention, yield-ing an average F1-score improvement of 2%. Theattention-based Triplet architecture outperformedtheir counterpart Siamese architecture, with an av-erage performance increase of 1.2% in leverag-ing external knowledge. It is noteworthy that inthe absence of attention and external resources,multi-network congurations (SMVbert) un-derperform as compared to the vanilla sequenceclassication approach (SCVbert).This highlights the efcacy of attention-basedMulti-Network architectures in leveraging exter-nal resources for AR prediction, contrasting withstandard sequence classication setups. Addition-ally, the performance advantage of Triplet archi-tecture over Siamese architecture can be attributed to its design, enabling each sub-network to focuson learning two levels of alignment: between thepremise and conclusion, and between the externalresource and the premise-conclusion pair. To ex-plore whether the performance gap solely stemsfrom the additional parameters in the attentionlayer, we introduced extra linear layers to the Multi-Network architecture (without attention layers) andobserved no change in performance despite theadditional layers. However, attention analysis isrequired to substantiate this claim.External knowledge inuence.Wikipedia-based models outperform the baselines andontology-based models across all four datasets.The attention-based Triplet-network on Wikipedia(TLAwp) achieved F-measures of 0.85, 0.84,0.70, and 0.87 in identifying AR on AAEC,AMT, US2016, and AbstRCT, respectively. Uponanalysing the paths connecting the componentsof ADUs, we found that 37% of concepts notpresent in ontological resources are connectedin Wikipedia, while only 7% of concepts ab-sent in Wikipedia are covered by ontological re-sources. For further details, please refer to Ap-pendix A.3.6. This disparity highlights the rich-ness of Wikipedias network of hyperlinks, whichconnect pages using a variety of relationships, un-like ontological resources that rely on predened,narrow sets of semantic relations.Although exploring combinations of externaldatabases such as WordNet and ConceptNet couldoffer additional insights, their contributions weremarginal, with only a 7% improvement overWikipedias paths. This motivated our focus onWikipedia as it provides the most comprehensiveset of connections. For simplicity and clarity, wechose to highlight the most impactful results ratherthan explore more complex combinations. How-ever, a combined approach across all three sourcescould be considered in future iterations.Models trained on GPT-4 generated paths outper-formed those without external knowledge, align-ing with other works leveraging LLM-generatedcommonsense knowledge (Bansal et al., 2022).However, despite exhibiting higher accuracy, theystill demonstrated lower precision compared toapproaches utilising external knowledge sources.The observed high recall and low precision canbe attributed to the models tendency to identifyunintended paths between concepts. Twenty er-rors were randomly selected for analysis, with twohuman annotators collaboratively examining the paths. Of these, 14 errors were deemed contextu-ally irrelevant, despite the logical coherence evi-dent in the generated paths. These paths introducechains of thought that diverge from the originalargument, as noted in previous studies that relyon LLMs for generating commonsense knowledge(Levy et al., 2022). It is important to note that in our study,GPT serves primarily as a comparison systemrather than a core external resource. The Stan-dardized Mean Difference (SMD) shows thatwhile GPT generally outperforms baseline mod-els, the improvement varies across datasets.The TLAGPT model surpasses the baselineSMABERT, achieving an overall SMD of 0.59.For datasets like AAEC, AMT, and US2016, the av-erage SMD is 1.57, indicating signicant enhance-ments in those contexts. However, in the AbstRCTdataset, GPT generated paths based congurationsunderperform and negatively affected overall per-formance. This discrepancy underscores the needfor careful selection and integration of externalknowledge sources to enhance model efcacy. Anerror analysis can be found in Appendix A.3.6.",
  "Conclusion": "Our exploration of various model congurationsunderscored the importance of external resourcesand multi-network architecture with attentionmechanisms in AR prediction. Models augmentedwith external resources consistently outperformthose relying solely on LLMs. This emphasisesthe necessity of leveraging supplementary knowl-edge sources to enrich LLMs for AR prediction.Furthermore, multi-network architectures with at-tention mechanisms, notably the attention-basedTriplet Network architecture, demonstrates supe-riority across all congurations. Further work isrequired to delve deeper into attention analysis, toshed-light on its role in encouraging the model tofocus in aligning the premise with the conclusion,as well as in linking the premise-conclusion pairwith external knowledge. While congurationsleveraging Wikipedia outperformed those usingother resources, more work is required to evaluatethe quality of keywords representing semantic rela-tions between concepts identied from Wikipediaagainst the standard semantic relation types in on-tologies. Furthermore, alternative methods for ex-tracting these keywords should be explored.",
  "Limitations": "Although our work presents promising advance-ments, it also entails the following limitations.Cross-Domain Evaluation. Robust evaluationinvolving cross-domain evaluation, where modelsare trained on one domain and evaluated on a newdomain, is essential for uncovering the robustnessof the proposed approaches. While our evalua-tion has primarily focused on specic domains ordatasets, cross-domain evaluation can provide in-sights into the generalisability and adaptability ofthe models across diverse domains and real-worldapplications.External Knowledge Alignment and Rela-tion Identication.More work is required inaligning the concepts with external resources,particularly in disambiguating the senses of theSynsets and Wikipedia page titles. Our currentapproach relies on simple similarity measures be-tween the embeddings of glosses of the resourcesand the components, which may lead to missingalignments and incorrect alignment. Improvingthe alignment procedure to account for semanticambiguity and variability in external resources iscrucial for enhancing the effectiveness of the pro-posed approach. Additionally, sophisticated tech-niques are needed to identify the semantic relationtypes existing between Wikipedia hyperlinks. Un-like ontologies, Wikipedia does not encode explicitsemantic relation types between hyperlinks. There-fore, developing robust method to identify seman-tic relations from Wikipedia articles can improvethe quality and relevance of external knowledgeintegration in AR prediction.Interpretability and Explainability. The ex-planations provided regarding the performance ofthe architectures and external resources are basedon the analysis of empirical results. While empir-ical analysis is valuable for understanding modelbehavior, additional techniques beyond the resultsthemselves can provide deeper insights into modelperformance. Exploring techniques such as modelvisualisation, attention mechanisms analysis, andinterpretability methods like LIME (Local Inter-pretable Model-Agnostic Explanations) (Ribeiroet al., 2016) or SHAP (SHapley Additive explana-tions) (Lundberg and Lee, 2017) can help uncoverthe underlying reasons behind model decisions andcongurations. Complementing empirical analysiswith interpretability techniques can allow a morecomprehensive understanding of model behavior.",
  "Acknowledgements": "This research is supported in part by:theSwiss National Science Foundation under grant10001FM_200857; the Volkswagen Foundationunder grant 98 543; and the Ofce of the Directorof National Intelligence (ODNI), Intelligence Ad-vanced Research Projects Activity (IARPA), viaContract 2022-22072200004. The views and con-clusions contained herein are those of the authorsand should not be interpreted as necessarily rep-resenting the ofcial policies, either expressed orimplied, of ODNI, IARPA, or the U.S. Government.The U.S. Government is authorised to reproduceand distribute reprints for governmental purposesnotwithstanding any copyright annotation therein.",
  "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, DianaInkpen, and Si Wei. 2017. Neural natural languageinference models enhanced with external knowl-edge. arXiv preprint arXiv:1711.04289": "Artem Chernodub, Oleksiy Oliynyk, Philipp Heidenre-ich, Alexander Bondarenko, Matthias Hagen, ChrisBiemann, and Alexander Panchenko. 2019. Targer:Neural argument mining at your ngertips. In Pro-ceedings of the 57th Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 195200. Carlos Chesnevar, Jarred McGinnis, Sanjay Mod-gil, Iyad Rahwan, Chris Reed, Guillermo Simari,Matthew South, Gerard Vreeswijk, and Steven Will-mott. 2006. Towards an argument interchange for-mat. The knowledge engineering review, 21(4):293316. HongSeok Choi and Hyunju Lee. 2018.Gist atsemeval-2018 task 12: A network transferring infer-ence knowledge to argument reasoning comprehen-sion task. In Proceedings of The 12th InternationalWorkshop on Semantic Evaluation, pages 773777.",
  "Barbara J Grosz, Aravind K Joshi, and Scott Weinstein.1995. Centering: A framework for modelling thelocal coherence of discourse": "Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,and Benno Stein. 2017.The argument reason-ing comprehension task: Identication and recon-struction of implicit warrants.arXiv preprintarXiv:1708.01425. Philipp Heinisch, Anette Frank, Juri Opitz, MoritzPlenz, and Philipp Cimiano. 2022. Overview of the2022 validity and novelty prediction shared task. InProceedings of the 9th Workshop on Argument Min-ing, pages 8494.",
  "R OpenAI. 2023.Gpt-4 technical report. arxiv2303.08774. View in Article, 2:13": "Andreas Peldszus and Manfred Stede. 2013. Rankingthe annotators: An agreement study on argumenta-tion structure. In Proceedings of the 7th linguisticannotation workshop and interoperability with dis-course, pages 196204. Andreas Peldszus and Manfred Stede. 2015a.Jointprediction in mst-style discourse parsing for argu-mentation mining. In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 938948. Andreas Peldszus and Manfred Stede. 2015b.Jointprediction in mst-style discourse parsing for argu-mentation mining. In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 938948.",
  "Nils Reimers and Iryna Gurevych. 2019.Sentence-bert:Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084": "Nils Reimers, Benjamin Schiller, Tilman Beck, Jo-hannes Daxenberger, Christian Stab, and IrynaGurevych. 2019.Classication and clustering ofarguments with contextualized word embeddings.arXiv preprint arXiv:1906.09821. Marco Tulio Ribeiro, Sameer Singh, and CarlosGuestrin. 2016. \" why should i trust you?\" explain-ing the predictions of any classier.In Proceed-ings of the 22nd ACM SIGKDD international con-ference on knowledge discovery and data mining,pages 11351144. Ruty Rinott, Lena Dankin, Carlos Alzate Perez,Mitesh M Khapra, Ehud Aharoni, and NoamSlonim. 2015.Show me your evidence-an auto-matic method for context dependent evidence de-tection. In Proceedings of the 2015 conference onempirical methods in natural language processing,pages 440450. Ramon Ruiz-Dolz, Jose Alemany, Stella M Heras Bar-ber, and Ana Garca-Fornes. 2021. Transformer-based models for automatic identication of argu-ment relations: A cross-domain evaluation. IEEEIntelligent Systems, 36(6):6270.",
  "Dietrich Trautmann. 2020.Aspect-based argumentmining. arXiv preprint arXiv:2011.00633": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in neural information pro-cessing systems, pages 59986008. Jacky Visser, Barbara Konat, Rory Duthie, Marcin Kos-zowy, Katarzyna Budzynska, and Chris Reed. 2019.Argumentation in the 2016 us presidential elections:annotated corpora of television debates and socialmedia reaction. Language Resources and Evalua-tion, pages 132.",
  "A.1.1Training ProcedureHyper-parameters: We employ Adam optimisa-tion (Kingma and Ba, 2014) to minimise the cost": "function. The learning rate is set to 2e5 with abatch size of 16. Categorical cross-entropy losswas used as the loss function.Gradient Clipping:To prevent explodinggradients during training, we apply gradientclipping.We use a maximum gradient norm(max_grad_norm) parameter set to 1.0 to deter-mine the threshold for gradient clipping.Warm-up and Learning Rate Schedule: Weemployed a linear warm-up strategy for the learn-ing rate.The number of warm-up steps is setto 10% of the total training steps. Following thewarm-up phase, the learning rate schedule is de-termined by a lambda function. This function lin-early increases the learning rate during the warm-up phase and decreases it linearly thereafter.Early Stopping: We implement early stoppingto prevent overtting and to determine the opti-mal number of epochs. This technique involvescontinuously monitoring the loss and F-score onthe validation set throughout training. If there is asustained degradation in performance over consec-utive epochs, training is terminated to prevent themodel from being inuenced by noise present inthe training data.",
  "A.1.2Input Setup": "For the baseline sequence classication congura-tions, we concatenate the premise to the conclusionusing a special token [SEP]. In the Siamese archi-tecture, one of the sub-networks takes the concate-nation of the premise and conclusion based on thespecial token [SEP], while the other takes the con-catenation of the paths. The paths are concatenatedusing the special token [SEP].The number and length of the paths between thecomponents of the ADUs vary, with some ADUsnot involving any path at all. For ADU pairs in-volving a large number of paths exceeding the max-imum sequence length, we concatenate the pathsuntil the maximum sequence length is reached. Insuch cases, we sort the paths based on their fre-quency. The concatenation process starts from themost frequent paths until the maximum sequencelength is reached.",
  "AAEC4235411605591210117US20163535551810116MTC19012027175534AbstRCT16032412293445869": ": Distribution of support and attack relationsacross the training, validation, and test splits for thedatasets.RA and CA refer to terms from the AIF(Chesnevar et al., 2006), where RA stands for Rule (ofinference) Application, representing a relation of sup-port or inference, and CA stands for Conict (scheme)Application, indicating a relation of conict or attack.",
  "A.3ADU Decomposition": "To identify the functional components (C and A)from ADUs, we adopt a sequence labeling ap-proach following the methodology outlined byGemechu and Reed (2019). Unlike Gemechu andReed (2019) method, which employs a convolu-tional neural network (CNN), we ne-tune BERTfor token classication using their dataset anno-tated with the BIO sequence labeling scheme, out-performing their top-performing method by 3%and achieving a macro F-score of 0.784.Weutilise the HuggingFace implementation of BERT( bert-base-uncased 6). The inputs are paddedto 256 maximum size. We use the train-test splitin the original dataset. Training is conducted over6 epochs, and evaluation is reported as the averageperformance over 3 runs of the experiment on the",
  "test dataset. Using the ne-tuned model, we iden-tify the functional components of ADUs, and thedistribution of these components is presented in": "A.3.1Alignment of Ontologies andWikipediaFor aligning ontologies and Wikipedia with thecomponents of ADUs, the cosine similarity be-tween the embeddings of the components and theSynsets of the ontologies or the correspondingWikipedia page title is used. Additionally, weutilise the similarity between the concepts andthe gloss texts of the respective sources for disam-biguating senses, for concepts involving multiplesenses.",
  "variant, for determining similarity. We set a simi-larity threshold of = 0.80 based on experimentalcomparisons of similarity scores between relatedand unrelated text pairs in the STSB dataset.8": "The dataset is originally annotated on a scale of0-5 based on the degree of similarity. We trans-form the original 5-class labels into binary labels,where labels below 4 are considered unrelated, andlabels 4 and above are deemed related. In the orig-inal annotation rubric provided by SemEval-2017(Cer et al., 2017), label 3 indicates sentences thatare roughly equivalent, but some important infor-mation differs. However, we found that this de-nition allows for a certain degree of looseness insimilarity assessment. Consequently, to impose astricter criterion for similarity, we decided to raisethe threshold from label 3 to label 4. To this end,we calculate the similarity between the sentencepairs in the training dataset and select the thresholdyielding the highest F1-score. We compute F1-scores at 20 similarity threshold points (rangingfrom 0 to 1 with increments of 0.05), as outlinedin Algorithm 1. A.3.3Search Depth ThresholdTo estimate the optimal depth threshold for navigat-ing through the knowledge graphs, we employ thefollowing procedure: we randomly select 20 pairsof concepts and initiate a complete search from one",
  "end forreturn best_threshold": "concept to identify paths leading to the other. Thisprovides a total of 728 paths with various depthsfrom the three resources. Human annotators thenevaluate the relevance of the retrieved paths basedon a binary value indicating if the path is relevantto the given AR or not. The cumulative F1-scoreat each depth is computed based on the total num-ber of relevant paths retrieved up to that depth.The depth with the highest cumulative F-score ischosen as the optimal threshold. Accordingly, thethreshold of = 5 yielded the highest score.",
  "A.3.4Filtering semantic relations": "A total of 7959 unique relation types are extracted.Please note that similar relation types like leadsto, leads and can lead to are counted as differ-ent relation types, as we only consider surface-levelcounts. To exclude arbitrary paths between con-cepts only relation types with a frequency greaterthan m=3 are considered. This yields a total of1488 unique relation types. However, as can beseen in , manual analysis revealed similari-ties among certain tuples; for example, the relationtype inuences is similar to other relations likecontributes to, leads to, and results in.Some concepts are directly related through sin-gle relation type (one-hop path), while others areindirectly connected via paths involving multiplerelation types (multi-hop). See examples in .The length of these paths ranges from 1 (indicatingdirect links between concepts) to 5 (the maximumsearch depth), with an average path length of 1.9.",
  "A.3.5Extracting keywords encodingsemantic relation types fromWikipedia": "The AllenNLP semantic role labeling (SRL)9 isused to parse sentences and assign semantic rolesto each word.This enables to extract phraseslinking the concepts of interest along the subject-predicate structure of the sentences. To mention,if one concept is identied as the agent and an-other as the patient, the phrase denoting the actionperformed by the agent on the patient is used asthe relation type between them. Consider the con-cepts exercise and cardiovascular diseases in thesentence:",
  "vascular diseases, including heart attackand stroke": "Below is the output of SRL for this sen-tence (the concepts are highlighted in lightblue while the keywords representing the re-lation type are highlighted in red): {verbs:[{verb:According,description:[V:According]totheAmericanHeartAssociation , exercise reduces the riskof cardiovascular diseases , includingheart attack and stroke, tags: [B-V,O, O, O, O, O, O, O, O,O, O, O, O, O, O, O, O,O,O,O]},{verb:reduces,description:[ARGM-ADV:AccordingtotheAmericanHeartAssociation],[ARG0:exercise][V:reduces][ARG1:theriskofcardiovasculardiseases,includingheartattackandstroke],tags:[B-ARGM-ADV,I-ARGM-ADV,I-ARGM-ADV, I-ARGM-ADV, I-ARGM-ADV,I-ARGM-ADV,O,B-ARG0,B-V,B-ARG1, I-ARG1, I-ARG1, I-ARG1,I-ARG1, I-ARG1, I-ARG1, I-ARG1,I-ARG1, I-ARG1, I-ARG1]}, {verb:including,description:AccordingtotheAmericanHeartAssociation,exercise reduces the risk of [ARG2:cardiovascular diseases] , [V: including][ARG1:heartattackandstroke],tags:[O,O,O,O,O,O,O,O,O,O,O,O,B-ARG2, I-ARG2, O, B-V, B-ARG1,I-ARG1,I-ARG1,I-ARG1]}],words:[According,to,the,American, Heart, Association, ,,exercise,reduces,the,risk,of, cardiovascular, diseases, ,,including,heart,attack,and,stroke]}We navigate through the SRL output to identifythe predicate-argument structures connecting bothconcepts (exercise and cardiovascular diseases inthis case). We then use predened rules to extractkeywords encoding the semantic relations existingbetween the concepts. To mention, if one conceptis part of ARG0 and the other being part of ARG1,the predicate term is used as the relation type. Inthe example output above, the predicate term repre-senting the semantic relation type is reduces. Moreexamples are provided below. The pair of concepts",
  "Predicate structure: [ARG2: Sustain-able energy] [V: involves] [ARG1: in-creasing production of renewable energy, making safe energy universally avail-able, and energy conservation]": "A.3.6External Resource EvaluationsOntology and Wikipedia: We analyse the threeresources to showcase their contributions in termsof coverage and the quality of connections.Coverage. The aim is to show the proportionof pairs connected exclusively by one resource butnot by others. To this end, we randomly select 500unconnected pairs from each resource and generatea heatmap illustrating the ratio of pairs exclusivelyconnected by each resource compared to the oth-ers to identify which resource is most effective incovering concepts absent in others. On average,Wikipedia covers 37% of pairs unconnected in bothWordNet and ConceptNet, while only 7% of theconcepts missing in Wikipedia are covered by bothWordNet and ConceptNet. Please note that pairsof concepts connected by relation types occurringless than three times are considered unconnected.Connection Quality. We further analyse thequality of the paths by ranking component pairsbased on the number of paths linking them fromeach respective resource. From this ranking, weselect the top 25 most connected and 25 least con-nected pairs from each resource for detailed eval-uation. Two annotators independently assess therelevance of these paths by assigning binary la-bels, reecting their subjective evaluations of thepaths pertinence to the AR between the ADUs. The evaluation shows that Wikipedia emerges asthe top-rated source for both well-connected andleast connected paths, achieving an F1 score of0.73. ConceptNet follows closely with an F1 scoreof 0.71, while WordNet has an F1 score of 0.68, in-dicating its comparative effectiveness in providingrelevant connections.GPT-generated paths: As shown in ,conguration utilising GPT-generated paths showhigher accuracy but lower precision. Of the totalerrors observed, 79% are identied as false pos-itives for approaches using GPT-generated pathsin predicting AR, while the average false positiverate for the other three external resources is 53%.To further investigate, we randomly select 20 er-rors and engage two human annotators to jointlyanalyse the paths connecting the pair of ADUs.Out of the 20 errors, the paths for the 14 of theerrors are categorised as contextually irrelevantfor the ADU pairs. The primary reason cited bythe annotators for the irrelevant paths indicatesthat while the generated paths make logical senseand provide valid lines of reasoning between theADUs, there were no AR between these ADUs asoriginally annotated in the dataset.For example, consider the pair of ADUs \"Re-searches into humanities and art still need largeamount of money\" and \"a government should spareeffort on young children education as well as uni-versities\", taken from the argument graph depictedin (taken from AAEC dataset). GPT iden-tied the following semantic relation paths linkingthe concepts \"money\" and \"young children educa-tion\":",
  "money encourages resource alloca-tion drives research and development inspires pedagogical advancements": "Despite these two ADUs not being linked by ARin the gold dataset, the paths between the conceptsthey address mimic the paths typically associatedwith ADUs involving AR. However, the reasoningconveyed by these paths is categorised as unin-tended, as they involve reasoning diverging fromthe original argument, and the AR between theseADUs is absent in the gold dataset.The same applies to the paths identied for theconcepts money and future addressed by the pairof ADUs: \"Researches into humanities and artstill need large amounts of money\" and \"both arecrucial on the way to a brighter future\".",
  ". Congurations: We use GPT-4 based ongpt-3.5-turbo-instruct. We set a maxi-mum token limit of 2048, a temperature of0.7, a top-p probability of 0.9": "2. Prompts Strategy: We explored two strate-gies: zero-shot and few-shot prompts.Inthe zero-shot setting, only instruction basedprompts without examples are used.Wealso try few-shot setup, where specic exam-ples are provided as part of the instruction.Interestingly, our analysis revealed that theexample-based experiment achieved a 1.3%,2.1% higher score compared to the zero-shotprompt in the AR prediction and path genera-tion, respectively. As a result, our experimentis based on example-based prompting. Wecreate prompt templates that include instruc-tions and two examples randomly selectedfrom a list of examples.These examples consist of ADU pairs, concept pairs identi-ed from the ADUs, and paths obtained fromthree external resources. The placeholder vari-ables in the template are replaced with theADUs, concepts, and paths.",
  "Prompt Design for Path Generation. GPT-4 istasked with generating paths between componentsof ADUs using the following template:": "You are a model trained toidentify chains of semantic relationsbetween a pair of concepts derivedfrom two sentences (ADU1 and ADU2).Given concepts c1 and c2 extracted fromADU1 and ADU2 respectively,your goal is to identify chains ofsemantic relation types connectingthese concepts. These relations mayinclude meronymy, hypernymy,hyponymy, cause-effect, or any othervalid semantic relation. Concepts areoften indirectly linked viaintermediate concepts and theirrelations. Include both direct andindirect paths between the conceptswhenever possible,using onlythe context provided by the ADU pairs.Provide up to 10 paths if possible;otherwise, return an empty list.Each relation type should berepresented as a tuple in the format(concept1, relation type, concept2). For indirect paths involvingmultiple tuples, return themas a list of tuples.Example 1:between the concepts\"USA\" and \"NAFTA\" identified fromthe pairof ADUs\"USA is in deep trouble\"and \"NAFTA agreement is defective\",a valid list of paths could be,[[(\"USA,part-of,NAFTA)\"],[\"(USA, has, trade deal),(trade deal, instance of, NAFTA\")].Example 2: between the concepts{c1} and {c2} identified from thepair of ADUs {ADU1} and {ADU2},the list of paths should include,{list_path}.Provide your answer as a python list. Note: In Example 1, we show an actual exam-ple, but it should be a placeholder variable in theprompt template, as shown in Example 2.Prompt Design for Zero-Shot AR Prediction:We prompt GPT-4 to classify the relationship be-tween the ADUs as supporting, contradicting, orhaving no clear AR using the following prompttemplate. You are a 3-class classifier modeltasked with assigning a labelto the argument relation betweentwo argument units(argument 1 and argument 2).Classify the followingpair of arguments,argument 1: {ADU_1}argument 2: {ADU_2},into:\"support\" (if argument 1 supportsargument 2),\"contradict\" (if argument 1 attacksargument 2),and \"None\" (if no argument relationexists betweenargument 1 and argument 2).Please enter:1 - for support,2 - for contradict,0 - for None relation.Examples from each argumentrelation types are provided below:Example 1: the argument relation betweenthe argument \"people feel, when they have been voicing opinions ondifferent matters, that theyhave been not listened to\",and the argument \"peoplefeel that they have been treateddisrespectfully on all sides of thedifferent arguments and disputesgoing on\" is support, andhenceprediction label is 1.Example 2: The argument relationbetween \"there would be no non-tariffbarriers with the dealdone withthe EU\" and the argument\"there are lots ofnon-tariff barrierswith the deal done with the EU\"is contradiction, andhence prediction label is 2.",
  "A.5Multi-Network Architectures": "The encoder blocks within the multi-networks areconstructed using the HuggingFace implementa-tion of BERT (bert-base-uncased) 10. In all con-gurations, we utilise 8 attention heads, which isa common feature in standard transformer imple-mentations. This design choice allows the model toattend to different parts of the input sequence simul-taneously, enhancing its ability to understand andrepresent complex relationships within the data. A.5.1Attention Mechanisms inMulti-Network ArchitecturesThe Triplet Network architecture is aimed to en-code the individual components of ADUs as wellas the external knowledge paths connecting them.The architecture consists of three sub-networks,each focusing on a different aspect of the input:",
  ". Second Attention Layer: Building upon theoutput of the rst attention layer, this layeraligns the information from the rst attentionlayer with the external knowledge providedby Sub-Network 3": "Finally, the outputs of the attention layers arepassed through feedforward and a nal linear clas-sication layer to predict AR. We experiment withtwo congurations for representing the ADUs andthe external knowledge as input to the attentionlayer: (a) using the nal output of the [CLS] tokenand (b) using the mean of the last hidden layer ofall tokens from BERTs output. Consistently, themean of the last hidden layer of all tokens yieldssuperior performance compared to the [CLS] to-ken. Since our goal is a classication task thatpredicts the AR across three classes, we utilisecross-entropy loss based on the output of the linearclassication layer. Triplet loss is not suitable forour specic task in either the Siamese or Tripletcongurations. A.5.2Training Complexity and AdditionalParametersThe inclusion of cross-attention layers and feed-forward layers in both the Siamese and TripletNetwork architectures introduces additional param-eters and computational complexity over the base-line BERT model, which consists of 110 million parameters.In the Siamese Network, cross-attention is ap-plied between the premise-conclusion pair and ex-ternal knowledge. The cross-attention mechanismcontributes approximately 1.77 million parame-ters (specically, 1,769,424 parameters), while thesubsequent feedforward layer adds around 2.37million parameters (specically, 2,367,486 pa-rameters). Additionally, the classication layer,which accounts for 3 output classes, adds about2,304 parameters. This brings the total parame-ter count for the Siamese model to approximately116.5 million, introducing about 4.14 million pa-rameters beyond the baseline.The Triplet Network further increases complex-ity, employing two cross-attention layers betweenthe premise, conclusion, and external knowledge,which contribute around 3.54 million parameters(specically, 3,538,944 parameters). The feedfor-ward layers contribute approximately 4.74 millionparameters (specically, 4,734,972 parameters),and the classication layer again adds 2,304 pa-rameters. This results in a total of approximately123 million parameters, which is around 8.28million more than the baseline.The added cross-attention layers introduce addi-tional computational steps by computing attentionscores between inputs (premise, conclusion, andexternal knowledge), thereby increasing trainingcomplexity relative to the baseline model, whichsolely ne-tunes the BERT layers without externalknowledge integration."
}