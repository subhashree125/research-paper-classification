{
  "Abstract": "Detecting logical fallacies in texts can helpusers spot argument flaws, but automating thisdetection is not easy. Manually annotating fal-lacies in large-scale, real-world text data to cre-ate datasets for developing and validating de-tection models is costly. This paper introducesCOCOLOFA, the largest known English logicalfallacy dataset, containing 7,706 comments for648 news articles, with each comment labeledfor fallacy presence and type. We recruited143 crowd workers to write comments embody-ing specific fallacy types (e.g., slippery slope)in response to news articles. Recognizing thecomplexity of this writing task, we built anLLM-powered assistant into the workers inter-face to aid in drafting and refining their com-ments. Experts rated the writing quality andlabeling validity of COCOLOFA as high andreliable. BERT-based models fine-tuned usingCOCOLOFA achieved the highest fallacy detec-tion (F1=0.86) and classification (F1=0.87) per-formance on its test set, outperforming the state-of-the-art LLMs. Our work shows that com-bining crowdsourcing and LLMs enables usto more effectively construct datasets for com-plex linguistic phenomena that crowd work-ers find challenging to produce on their own.COCOLOFA is public at CoCoLoFa.org/.",
  "Introduction": "Logical fallacies are reasoning errors that under-mine an arguments validity (Walton, 1987). Com-mon fallacies like slippery slope or false dilemmadegrade online discussions (Sahai et al., 2021) andmake arguments seem more dubious, fostering mis-information (Jin et al., 2022). Automatically detect-ing logical fallacies in texts will help users identifyargument flaws. However, automatically identify-ing these fallacies in the wild is not easy. Fallaciesare often buried inside arguments that sound con-vincing (Powers, 1995); over 100 types of logicalfallacies exist (Arp et al., 2018). The nature of the : Examples from COCOLOFA. For each newsarticle, we hired crowd workers to form a thread of com-ment. Each worker was assigned to write a commentwith a specific type of logical fallacy (or a neutral argu-ment) in response to the article. problem makes it expensive to build large-scale la-beled datasets needed for developing fallacy detec-tion models. Prior works have created datasets forlogical fallacies (): LOGIC dataset collectedexamples from online educational materials (Jinet al., 2022); LOGICCLIMATE dataset collected in-stances from news articles, specifically targetinga particular topic range and identifying commonfallacious arguments related to those topics (Jinet al., 2022); Argotario dataset was collected us-ing a gamified crowdsourcing approach (Habernalet al., 2017); and the dataset proposed by Sahaiet al. (2021) leveraged existing community labelsfrom Reddit users. These previous efforts are in-",
  ": Comparison of datasets of logical fallacies. COCOLOFA is the largest and has the longest text units": "spiring, but they often did not focus on enabling fal-lacy detection in the wild, as each made significanttrade-offs to ease the challenges of labeling falla-cies: focusing on smaller scales (1,000+ instances;no negative samples), specific topics like climatechange rather than a broader range, or clear edu-cational examples instead of complex web discus-sions. One exception is the Reddit dataset (Sahaiet al., 2021), which is relatively large and includesmessy Reddit comments. However, it isolates com-ments from their original threads, limiting the useof context to boost detection and understanding ofhow fallacies unfold in online discussions.This paper presents COCOLOFA, a dataset con-taining 7,706 comments for 648 news articles, witheach comment labeled for fallacy presence and type(). The intuition of our data collection ap-proach is first to specify a fallacy type (e.g., slipperyslope) and present a news article (e.g., on abortionlaws) to crowd workers, and then ask them to writecomments that embody the fallacy in response tothe article (e.g., Abortion legalization leads to nor-malization of killing.) Recognizing the difficultyof this writing task, we built an LLM-powered as-sistant in the interface to help workers draft andrefine comments. Our data collection approachreplaces the data annotation process with data gen-eration, reducing the need of hiring workers to filterout a large amount of non-fallacious instances atfirst and making the data collection more scalable.In addition, it increases the ability to control tar-geted fallacy types for researchers. Compared toprevious work (), COCOLOFA is the largestNLP dataset of logical fallacies, featuring the high-est average sentence and word counts per instance.Two experts rated the writing quality and labelingvalidity of COCOLOFA as high and reliable. Theexperiments show that COCOLOFA can be used to effectively develop fallacy detection and clas-sification models. As a broader implication, ourwork shows how crowdsourcing can be integratedwith large language models (LLMs) to constructdatasets for complex linguistic phenomena that arechallenging for crowd workers to produce on theirown. This opens up new possibilities for futureNLP datasets.",
  "Related Work": "Logical Fallacy Datasets.We discussed the ma-jor logical fallacy datasets in the Introduction (Sec-tion 1); this section focuses on extra studies not pre-viously covered. A follow-up of Argotario (Haber-nal et al., 2017) collected data on 6 types of logi-cal fallacies and labeled 430 arguments (Habernalet al., 2018). Similarly, Bonial et al. (2022) usedthe same annotation schema to identify logical fal-lacies in 226 COVID-19 articles across variousmediums. Other research has specifically aimedat detecting logical fallacies in news articles. Forexample, Da San Martino et al. (2019) annotated451 news articles with 18 propaganda techniques,12 of which qualify as logical fallacies. Addition-ally, Helwe et al. (2024) annotated 200 samplesfrom merged existing datasets with a unified taxon-omy and justifications. These datasets are relativelysmall, highlighting the challenges of annotatinglarge-scale texts for logical fallacies. Emergingresearch is also exploring the synthesis of logicalfallacy datasets using LLMs (Li et al., 2024). LLM-Assisted Crowdsourced Data Creation.Veselovsky et al. (2023) found that many crowdworkers submitted summaries were created usingLLMs. We saw it as an interesting opportunityrather than a threat. Integrating LLM assistancedirectly into the workers interface offers benefits for both workers and requesters. For workers, built-in LLMs can aid in complex writing tasks thatmight otherwise be too challenging and eliminatethe need to switch between browser tabs to useexternal LLMs. For requesters, having a built-inLLM allows for storing all prompts used and textsproduced by the LLM, ensuring a more transparentunderstanding of how LLMs outputs are woveninto the final data. Previous work has integrated AImodels into worker interfaces to help produce ex-amples that trigger specific model behaviors, suchas model-fooling examples (Bartolo et al., 2022).In this paper, we advocate using LLMs to helpworkers generate complex examples.",
  "COCOLOFA Dataset Construction": "We constructed COCOLOFA, a dataset that con-tains 7,706 comments in the online comment sec-tions of 648 news articles. Each comment is taggedfor the presence of logical fallacies and, where ap-plicable, the specific type of fallacy. 143 crowdworkers, aided by GPT-4 integrated into their in-terface, wrote these comments. COCOLOFA alsoincludes the titles and contents of the news arti-cles, all of which are CC-BY 3.0 licensed. We splitthe dataset into train (70%), development (20%),and test (10%) sets by article, ensuring a balancedrepresentation of 21 topics across the splits. Thissection overviews the data construction steps.",
  "Selecting News Articles": "We crawled news articles from Global Voices, anonline news platform where all of their news ar-ticles are under the CC-BY 3.0 license.1 To sim-ulate heated online discussions, we took a data-driven approach to select news articles on topicsthat often provoke disagreements and numerousopinions. We first selected a set of article tags,provided by Global Voices, that are traditionallymore controversial, such as politics, women-gender, migration-immigration, and, freedom-of-speech. Second, we crawled all the 25,370 arti-cles published from Jan. 1st, 2005, to Jun. 28th,2023, that have these tags. Third, we trained anLDA model (Blei et al., 2003) to discover 70 topicswithin these news articles. Finally, according to thetop 40 words of each topic, we manually selected21 interested topics and filtered out irrelevant news",
  "Global Voices: Besidescommon news topics like economics and international rela-tions, Global Voices also focuses on topics related to humanrights, such as censorship, LGBTQ+, and refugees": "articles. Using top frequent words to select repre-sentative events was also used in constructing otherdatasets that sampled real-world events (Huanget al., 2016). As a result, a total of 15,334 newsarticles were selected, of which 650 published af-ter 2018 were randomly selected to construct theCOCOLOFA dataset.2 See Appendix A for details.",
  "Fallacy Types Included in COCOLOFA": "Over 100 informal logical fallacies exist (Arp et al.,2018), making it impractical to cover all in adataset. We reviewed how past studies, such asSahai et al. (2021), Jin et al. (2022), Habernal et al.(2017), and Da San Martino et al. (2019), selectedfallacy types. Following Sahai et al. (2021), wechose eight common logical fallacies in online dis-cussions: (1) Appeal to authority, (2) appeal tomajority, (3) appeal to nature, (4) appeal to tra-dition, (5) appeal to worse problems, (6) falsedilemma, (7) hasty generalization, and (8) slip-pery slope. Appendix B shows the definitions andexamples of these eight fallacies.3",
  "Collecting Comments with SpecifiedLogical Fallacies from Crowd WorkersAssisted by LLMs": "We designed a crowdsourcing task instructingcrowd workers to write comments containing spe-cific logical fallacies. The intuition is that showingan often controversial topic (e.g., abortion) along-side a logical fallacy definition (e.g., slippery slope)allows workers to easily come up with relevantcommentary ideas with the fallacy (e.g., Abortionlegalization leads to normalization of killing.). Af-ter drafting their idea quickly, LLMs like GPT-4can be employed to elaborate and refine the com-ment with the worker. shows the workerinterface, which has a simulated news commentsection (left) and instructions and questions (right).The workflow of crowd workers is as follows.",
  "C": ": Different components in the task interface: A) The news article and comments, B) Questions for sanitycheck, C) Instruction of writing fallacious comments, D) Text box and the drop-down list for choosing the respondedcomment, E) GPT-4 generated guideline and example. Step 2:Answer Attention-Check Questionsabout the News.As an attention check, theworker will then be asked to answer three multiple-choice questions related to the news (B).These questions are: (1) What topic does thisnews focus on?, (2) Which is the summary of thisnews?, and (3) What opinions are presented inthis news? (Choose three answers). We promptedGPT-4 to generate correct and incorrect options forthese questions. The prompt used (see Appendix C)was empirically tested and was effective in filteringout underperforming workers. The workers whoseanswering accuracy was lower than 0.6 were disal-lowed to enter our system for 24 hours. Step 3: Draft a Comment Containing the Spec-ified Logical Fallacy and Revise with LLMs.We divided the writing task into two smaller steps:drafting and revising. First, workers were presentedwith a logical fallacy definition, such as Appeal to Tradition (C), and then tasked withwriting a response to a news article, requiring atleast two sentences or a minimum of 10 words(D). They could see comments from otherworkers on the same article and had the optionto either comment directly on the article or replyto existing comments. Each worker was exposedto an article only once. We assigned the fallacyfor each task (see .4). The fallacy defini-tions we provided on the interface were a shortenversion so that the instruction can be concise andeasy to follow. The shorten version of fallacy def-initions is detailed in Appendix B. Second, afterdrafting, workers were instructed to click the Get(Another) Suggestion button for a detailed revi-sion suggestion and example embodying the fallacy(E). We prompted GPT-4 (see Appendix C)to generate the suggestion and example automati-cally based on (i) the news article, (ii) the commentdraft, and (iii) the target fallacy. Workers can re-",
  ": Statistics of the COCOLOFA dataset. We di-vided COCOLOFA into Train, Dev, and Test sets atratios of 0.7, 0.2, and 0.1 respectively": "vise their comments and click the button again fornew suggestions based on the revised comment.Within each task, they can click the button up tofive (5) times. Copy-and-paste was disabled in theinterface, so workers had to type their comments. Rationale for the Workflow Design.This work-flow used LLMs to assist workers, making a hardwriting task easier. Meanwhile, it forced workersto provide their insights as input for LLMs, ensur-ing data diversity and a human touch. The built-inLLM assistance decreased the likelihood of work-ers turning to external LLMs, allowing researchersto provide a prompt that fully considered the con-text, including news content, the specific fallacy,and workers opinions. Notably, our approachhaving workers write comments embodying a par-ticular logical fallacy is conceptually similar toArgotario (Habernal et al., 2017). Our method dif-fers in two ways: First, we provided real-worldnews as context, requiring workers to base theirfallacious arguments on these articles. Second,we conducted multiple rounds of comment collec-tion for each article, allowing workers to respondto others comments. These two factors allowedCOCOLOFA to more accurately simulate the com-ment sections of real-world news websites.",
  "Implementation Details": "Four Rounds of Data Collection.Our data col-lection process had four iterations. For each itera-tion, we added the comments collected from pre-vious iterations underneath the article section onthe interface. Workers in the 2nd to 4th iterationscan respond to previous comments by selecting thecomment ID from a drop-down list (D).Each worker only interacted with an article once.",
  "ing task management.4 For each news article, werecruited 12 workers (3 per iteration) across 12 Hu-man Intelligence Tasks (HITs) to write comments.5": "In the first three iterations, each task randomly re-ceived one of eight logical fallacy types with a 10%probability, or a 20% chance to comment with-out fallacious logic. In the fourth iteration, weincreased the probability to 60% for commentswithout fallacious logic and reduced it to 5% foreach fallacy type to gather more negative samples.Workers were paid by $2 USD for each HIT, whichtakes about 10 minutes on average, leading to anestimated hourly wage of $12. Resulting Dataset.We posted HITs in smallbatches, closely monitoring data quality daily andmanually removing low-quality responses, i.e.,those that are (1) obviously off-topic (e.g., sayingthis task is interesting), (2) writing exactly the samecomment for multiple articles, or (3) repeating thesame word for the whole comment. Completing50 news articles typically took about one week,likely due to our exclusive use of workers withMasters Qualifications. 143 workers contributedto the dataset. After removing articles with fewerthan 6 comments, the final dataset contained 648news articles and 7,706 comments. showsthe statistics of COCOLOFA. Worker-LLM Interactions.Within our study,each worker asked LLM an average of 1.39 times(SD=0.81) when writing a comment. Workers com-pletely followed the LLMs suggestions in only3% of comments. The average Levenshtein ra-tio between the workers comment and the LLMslast suggestion is 0.35 (1 means the sentences areidentical), indicating a significant difference. Weobserved that most workers either paraphrased thesuggestions or added details to their comments.",
  ": Cohens agreement between experts andlabels, as well as the agreement between two experts.COCOLOFA yielded slightly higher agreements": "years of experience in the fields of English compo-sition and rhetoric, and another has over 20 yearsof experience in translation. Both of them alsohave rich experience in editing academic articlesand volumes. They were compensated $50-$60per hour. We randomly selected 20 news articlesand asked the experts to annotate fallacies in allcomments (237 comments in total). For each fal-lacy type, we converted labels into binary Yes/No(indicating the presence of the fallacy) and calcu-lated the Cohens kappa () agreement betweenexperts and COCOLOFAs labels, as well as theagreement between two experts. We also sampled25 instances for each fallacy type plus none (i.e.,25 (8 + 1) = 255 instances in total) from theReddit dataset (Sahai et al., 2021) and asked thesame experts to annotate them as a comparison. shows the results. COCOLOFAyielded slightly higher inter-annotator agreements, while experts often dis-agreed with each other. shows that ex-perts generally agreed more on the COCOLOFAslabel than on the Reddit dataset. However, Expert2 consistently showed more disagreement with thelabels in both datasets for most fallacy types. Ta-ble 3 also shows low agreement between experts onboth datasets, particularly for hasty generalization.As shown in Sahai et al. (2021) and Alhindi et al.(2022), this level of value is normal in annotat-ing logical fallacy data. We computed confusionmatrices for experts annotations and labels in bothdatasets. The confusion matrix comparing the twoexperts on COCOLOFA is shown in , andthe others are in Appendix E. shows thatmost disagreements occur in determining the pres-",
  "ence of a fallacy rather than its type. We discuss thepossible reasons for high disagreement in labelinglogical fallacies further in Discussion ()": "COCOLOFA was rated more fluent and gram-matically correct.We also asked the experts torespond to the following questions for each com-ment using a 5-point Likert scale, from 1 (StronglyDisagree) to 5 (Strongly Agree): (Q1) Disregard-ing any logical fallacies, this comment is grammat-ically correct and fluently written. (Q2) Thiscomment appears to have been written by a per-son rather than by a language model such asChatGPT. (Q3) I feel confident about my an-notation. (Q4) I need some additional contextto annotate the comment. For Q1, COCOLOFAscored an average of 4.38 (SD=0.91), comparedto 4.21 (SD=1.04) for Reddit, suggesting thattexts in COCOLOFA were generally consideredmore fluent and grammatically correct. For Q2,COCOLOFA scored 4.39 (SD=0.79), and Redditscored higher at 4.58 (SD=0.59), indicating that ex-perts found Reddits texts more human-like. Thisechoes the findings in , which shows a lowerinter-annotator agreement for Reddit, likely dueto its messier, real-world internet text. Althoughhumans sometimes struggle to distinguish LLM-generated texts, the purpose of Q2 was to ensurethat COCOLOFAs text did not obviously appearmachine-generated, such as through identifiableerrors like repetition, which humans can recog-",
  "nize (Dugan et al., 2023). There was no clear dif-ference between COCOLOFA and Reddit for Q3(4.53, 4.57) and Q4 (1.59, 1.60)": "Concerns over argumentation scheme.Duringthe annotation process, experts identified that someworkers did not include fallacies in their comments.Instead, they used argumentation schemes to maketheir argument fallacy-like but valid. To addresssuch an issue, some previous work, such as Ruiz-Dolz and Lawrence (2023), suggested using a se-ries of critical questions of the corresponding ar-gumentation scheme to assess the validity of anargument. However, having annotators or com-ment writers go through these questions for eachcomment will significantly limit the scalability ofour approach. Given that experts only identified12 out of 237 comments to be fallacy-like, weconsidered our approach a reasonable trade-off.",
  "BERT.We fine-tuned BERT (Devlin et al., 2019)and used the encoded embedding of the [CLS] to-ken to predict the label": "NLI.Inspired by Jin et al. (2022), we fine-tunedan NLI model with a RoBERTa (Liu et al., 2019)as the backbone. We treated the input comment asthe premise and the label as the hypothesis. For thedetection task, the hypothesis template was Thetext [has/does not have] logical fallacy. Forthe classification task, the template was The texthas the logical fallacy of [label name]. LLMs.We prompted two commonly used LLMs,GPT-4o and Llama3(8B), for detecting and clas-sifying logical fallacy.6We designed differentprompts (see Appendix C), including both zero-shot and few-shot, as well as Chain-of-Thought(COT) prompting (Wei et al., 2022). Use of Context.For Reddit and COCOLOFA,which provide context such as news titles or par-ent comments, we incorporated this context intomodels inputs. For BERT and NLI models, weappended the context to the target comment. ForLLMs, we used placeholders in the prompt to in-clude this information. Further implementationdetails are available in Appendix F.",
  "Results of Fallacy Detection": "BERT-based models fine-tuned on COCOLOFAhad better generalizability than when fine-tunedon Reddit. shows the detection task re-sults. BERT fine-tuned on COCOLOFA achievedthe highest F1 score (0.86) on its test set andshowed better generalizability compared to whenfine-tuned on Reddit. It surpassed BERT fine-tunedon Reddit in LOGIC and LOGICCLIMATE. On theReddit dataset, it scored only 0.05 F1 points lowerthan BERT fine-tuned on Reddit (0.63 vs. 0.68),but on COCOLOFA, BERT fine-tuned on Redditscored 0.13 F1 points lower (0.73 vs. 0.86).State-of-the-art LLMs still showed strong perfor-mance, achieving the best F1 on Reddit and the bestrecall on LOGIC. Notably, LLMs performed poorlyon LOGICCLIMATE, where fallacious sentenceswere extracted from context. This might suggestthat contextual understanding is crucial for LLMpredictions, indicating a need for further research.",
  "BERT-based models fine-tuned on COCOLOFAhad better generalizability, with classificationseeming to be easier than detection": "shows the classification results, which are similarto those of the detection task. The NLI modelaBERT-based modelfine-tuned on COCOLOFA,achieved the highest F1 score (0.87) on its testset. Both BERT and NLI models fine-tuned onCOCOLOFA exhibited better generalizability thanthose fine-tuned on Reddit. When tested on theReddit dataset, BERT and NLI models fine-tunedon COCOLOFA scored only 0.19 and 0.09 F1points lower, respectively, than their Reddit-tuned",
  "Llama3zero-shot41853 27 3676 43 55few-shot796551 89 6562 95 75COT652861 53 5677 56 65": ": The result of fallacy detection task.ForLOGIC and LOGICCLIMATE (CLIMATE), we reportedthe Recall rate as they only have positive samples.While for others, we reported Precision, Recall, andF1 score. The highest (second-highest) scores are set inbold (underlined). counterparts. Conversely, on COCOLOFA, Reddit-tuned BERT and NLI models scored 0.24 and 0.21F1 points lower, respectively, than those tuned onCOCOLOFA.Additionally, LLMs, particularlyGPT-4o, performed best on the Reddit dataset. Wealso observed that classification tasks generally per-formed better than detection tasks, indicating thatdetermining the type of fallacy in a comment mightbe easier than deciding whether a fallacy exists.",
  "Results of Fallacy Detection in the Wild": "The primary motivation for this project is to iden-tify logical fallacies in the wild (Ruiz-Dolz andLawrence, 2023). Therefore, we additionally testedour models on the New York Times CommentsDataset (Kesarwani, 2018). We sampled 500 com-ments and hired an expert (one in ) to labelthe fallacies. shows the results of fallacydetection on this dataset. The expert annotating theNYT comments identified several fallacies beyondthe eight predefined types, so we report two setsof results for each model: one where commentswith additional fallacy types are treated as falla-cious (positive samples), and another where theyare considered non-fallacious (negative samples). Detecting fallacies in real-world settings is stillchallenging.Although LLMs outperformed allfine-tuned models, their low F1 score of 0.34 in thesecond setting (i.e., negative) indicates that LLMsare still unreliable in precisely identifying logicalfallacies, motivating the need for further research.",
  "Llama3zero-shot584140574241few-shot523332575048COT564847635858": ": The result of fallacy classification task. Thehigh performance for most models suggests that oncethe fallacies are detected, it is easy for model to discerntheir types. Noted that the F1 scores we reported weremacro F1 across all fallacy types. The highest (second-highest) scores are set in bold (underlined). The results also show that BERT-based models fine-tuned on COCOLOFA outperformed those fine-tuned on Reddit in most cases except for the Recallon NLI models, suggesting COCOLOFAs poten-tial in training more generalizable models. Addi-tional experimental results on the NYT dataset canbe found in Appendix G.",
  "Discussion": "Throughout the project, we learned that annota-tors often disagree when labeling logical fallacies,as consistently shown by the low inter-annotatoragreement reported in all related literature (Sahaiet al., 2021; Alhindi et al., 2022), including ourown. This section outlines the three main sourcesof disagreement we identified and offers designsuggestions for mitigating (or retaining) them.",
  "Sources of Disagreement": "ComplexityinDefiningLogicalFallacies.Many fallacies are similar or overlap, with a sin-gle text potentially presenting multiple fallacies.Furthermore, different datasets can provide incon-sistent definitions for the same fallacy name. Forexample, appeal to authority might be definedas either mention of false authority or referralto a valid authority without supporting evidence,adding to the confusion (Alhindi et al., 2022). Addi-tionally, when asking experts to annotate the NYTdataset, they identified many comments that em-bodied other types of fallacy, such as ad hominem,even though they were outside the eight types of",
  "few-shot43 / 1687 / 8758 / 28COT48 / 2080 / 6860 / 30": ":The result of fallacy detection on 500NYT samples. The left/right numbers are the scoreswhere other types of fallacy were considered as posi-tive/negative. Models trained on COCOLOFA outper-form those trained on Reddit. The highest (second-highest) scores are set in bold (underlined). fallacies we predefined in our annotation interface.These fallacies have inherently vague boundaries.For example, ad hominem fallacies are difficult toclassify as they require distinguishing between per-sonal attacks aimed at undermining an argumentand simple insults. These complexities suggest thatfallacy labeling efforts can benefit from standard-ized definitions and allowing multiple labels peritem to capture nuanced perspectives. Variability in Annotators Judgments of Falla-cies.In our study, one expert consistently iden-tified more fallacies than the other, highlightingthat annotators can differ significantly in their in-terpretations of rhetorical strategies. For instance,both experts identified an appeal to authority in acomment on abortion legality, which stated: Themajoritys voice should be the guiding light for law-makers. Thats what democracy is about. How-ever, one expert considered this a valid rhetoricalusage, not a fallacy, explaining that it was used todefine democracy within the text, while the otherexpert simply labeled it as a fallacy. Requiringannotators to provide rationales may clarify theirreasoning for classifying texts as fallacious. Divergence Between Writer Intent and ReaderPerception.Despite instructions for workers towrite comments with a specific fallacy, annotatorssometimes identified different fallacies. This high-lights the challenge of aligning readers interpre-tations with writers intentions. It also raises aquestion: who determines whether a text contains afallacy and what type of fallacy it representsthe writer, the reader, or an external party? These dis-crepancies may stem from the nature of fallacies,which can be based on words, sentences, or com-plex reasoning within the broader context (Bonialet al., 2022), as readers and writers may focus ondifferent elements within the same comments.",
  "Design Suggestions": "We propose three design suggestions for futureprojects involving human labeling of logical fal-lacies in text: (i) provide clear, operationalizedinstructions, (ii) implement a multi-class label-ing scheme that allows a text instance to containmultiple fallacies, and (iii) collect rationales foreach fallacy label, ensuring that if an instance is la-beled with multiple fallacies, each one is supportedby a distinct rationale. Prior works have adoptedsome of these approaches. For (i), Ruiz-Dolz andLawrence suggested using critical questions, suchas How well supported by evidence is the alle-gation made in the character attack premise?, tovalidate whether a text contains a fallacy. For (ii),the Climate dataset employed multi-label annota-tion (Jin et al., 2022). For (iii), Sahai et al. hadannotators answer specific questions for each fal-lacy label. While these approaches have been indi-vidually explored in prior studies, we recommendcombining all three to create a more comprehensiveand robust annotated dataset. The project that mostclosely aligns with this approach is by Helwe et al.,which annotated 200 text instances using a unifiedmulti-label scheme. They noted, however, that suchdetailed annotation is very resource-intensive, assome annotators took four hours to label 20 items.We suspect some of our suggestions may also becostly to scale. More research is needed to explorethe trade-offs between data quality and scalability.",
  "Conclusion and Future Work": "This paper presents COCOLOFA, the largestknown logical fallacy dataset, curated through acollaboration between LLMs and crowd workers.BERT-based models fine-tuned using COCOLOFAachieved good performances in fallacy detectionand classification tasks. In the future, we plan todevelop models that use context and reasoning toidentify fallacies, especially on out-of-distributiondata. Additionally, while COCOLOFA includeseight fallacy types, over a hundred exist. We aimto expand it to cover more.",
  "Limitations": "Like most crowdsourced datasets, COCOLOFA in-herits the biases of using online crowdsourcingplatforms to collect data. For example, the crowdworkers on Amazon Mechanical Turk are not nec-essarily representative of the user populations onsocial media and news platforms; they may prior-itize different topics and hold opinions that differfrom those of typical online users. In addition, thewriting style of commenting in the crowdsourcingtask may also differ from that of debating online.Although we developed a platform that simulatedthe interface of the online news comment section,the real-time feedback and the vibe of online dis-cussion are still difficult to simulate. Apart fromthe content, the masters qualification we requiredcrowd workers to have may lower the demographicdiversity (Loepp and Kelly, 2020), leading to afurther risk of bias.Besides, we integrated GPT-4 into our platformto assist crowd workers in writing high-qualitycomments. However, we acknowledge that GPT-4may have a preferred stance (e.g., North Ameri-can attitudes) when generating example arguments.Although we forced workers to provide input andincluded that input in the prompt to guide the gen-eration, the biases in GPT-4 may still exist andnegatively affect the human written comments.Another limitation is that COCOLOFA currentlyconsiders only eight types of fallacy, as we men-tioned in the future work. Given that there are manycommon fallacy types apart from the fallacies wecollected, models trained on our dataset may onlyhave a limited ability to detect fallacies in the wild.",
  "Ethics Statement": "Although COCOLOFA is collected for logical fal-lacy detection, we acknowledge the potential mis-use of the dataset for training models to generatefallacious comments. Furthermore, our data col-lection process has revealed that GPT-4 has thecapability to generate such comments, posing risksof propagating misinformation online. Therefore,we advocate for research aimed at LLMs to preventthe generation of harmful and misleading content.",
  "We thank Meta Research for their support of thiswork, and Jack Urbanek and Pratik Ringshia fortheir technical assistance with the Mephisto frame-work. We are also grateful to the two experts re-": "cruited via UpWork for data labeling and the crowdworkers from Amazon Mechanical Turk for datasetcreation. Special thanks to the anonymous review-ers for their valuable feedback and to PhakphumArtkaew and Reuben Woojin Lee for their helpduring the early stages of the project. Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, andSmaranda Muresan. 2022.Multitask instruction-based prompting for fallacy recognition. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing.",
  "David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003. Latent dirichlet allocation. J. Mach. Learn.Res., 3": "Claire Bonial,Austin Blodgett,Taylor Hudson,Stephanie M. Lukin, Jeffrey Micher, DouglasSummers-Stay, Peter Sutor, and Clare Voss. 2022.The search for agreement on logical fallacy annota-tion of an infodemic. In Proceedings of the Thir-teenth Language Resources and Evaluation Confer-ence. Giovanni Da San Martino, Seunghak Yu, AlbertoBarrn-Cedeo, Rostislav Petrov, and Preslav Nakov.2019. Fine-grained analysis of propaganda in newsarticle. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP). Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies. Liam Dugan, Daphne Ippolito, Arun Kirubarajan,Sherry Shi, and Chris Callison-Burch. 2023. Realor fake text?: Investigating human ability to de-tect boundaries between human-written and machine-generated text. Proceedings of the AAAI Conferenceon Artificial Intelligence. Ivan Habernal, Raffael Hannemann, Christian Pol-lak, Christopher Klamm, Patrick Pauli, and IrynaGurevych. 2017. Argotario: Computational argu-mentation meets serious games. In Proceedings ofthe 2017 Conference on Empirical Methods in Natu-ral Language Processing: System Demonstrations. Ivan Habernal, Patrick Pauli, and Iryna Gurevych. 2018.Adapting serious game for fallacious argumentationto German: Pitfalls, insights, and best practices. InProceedings of the Eleventh International Confer-ence on Language Resources and Evaluation (LREC2018). Chadi Helwe, Tom Calamai, Pierre-Henri Paris, ChloClavel, and Fabian Suchanek. 2024. MAFALDA: Abenchmark and comprehensive study of fallacy de-tection and classification. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers). Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,Ishan Misra, Aishwarya Agrawal, Jacob Devlin, RossGirshick, Xiaodong He, Pushmeet Kohli, Dhruv Ba-tra, et al. 2016. Visual storytelling. In Proceedingsof the 2016 conference of the North American chap-ter of the association for computational linguistics:Human language technologies. Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, XiaoyuShen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan,Rada Mihalcea, and Bernhard Schoelkopf. 2022.Logical fallacy detection. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2022,Abu Dhabi, United Arab Emirates.",
  "Aashita Kesarwani. 2018. New york times commentsdataset": "Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang,Qianyu He, Yanghua Xiao, and Deqing Yang. 2024.Reason from fallacy: Enhancing large language mod-els logical reasoning through logical fallacy under-standing. In Findings of the Association for Compu-tational Linguistics: NAACL 2024, pages 30533066,Mexico City, Mexico. Association for ComputationalLinguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. ArXiv.",
  ": Top 10 words of the selected topics": "Appeal to majority.Definition: When the claimthat most or many people in general or of a par-ticular group accept a belief as true is presentedas evidence for the claim. Accepting another per-sons belief, or many peoples beliefs, without de-manding evidence as to why that person acceptsthe belief, is lazy thinking and a dangerous wayto accept information. Example: Up until the late16th century, most people believed that the earthwas the center of the universe. This was seen asenough of a reason back then to accept this as true. Appeal to nature.Definition: When used as afallacy, the belief or suggestion that natural isbetter than unnatural based on its naturalness.Many people adopt this as a default belief. It is thebelief that is what is natural must be good (or any other positive, evaluative judgment) and that whichis unnatural must be bad (or any other negative,evaluative judgment). Example: I shop at Natu-ral Happy Sunshine Store (NHSS), which is muchbetter than your grocery store because at NHSS ev-erything is natural including the 38-year-old storemanagers long gray hair and saggy breasts. Appeal to tradition.Definition: Using historicalpreferences of the people (tradition), either in gen-eral or as specific as the historical preferences ofa single individual, as evidence that the historicalpreference is correct. Traditions are often passedfrom generation to generation with no other ex-planation besides, this is the way it has alwaysbeen donewhich is not a reason, it is an absenceof a reason. Example: Marriage has traditionally",
  "been between a man and a woman; therefore, gaymarriage should not be allowed": "Appeal to worse problems.Definition: Tryingto make a scenario appear better or worse by com-paring it to the best or worst case scenario. Exam-ple: Be happy with the 1972 Chevy Nova you drive.There are many people in this country who donteven have a car. False dilemma.Definition:When only twochoices are presented yet more exist, or a spectrumof possible choices exists between two extremes.False dilemmas are usually characterized by eitherthis or that language, but can also be characterizedby omissions of choices. Example: You are eitherwith God or against him. Hasty generalization.Definition: Drawing aconclusion based on a small sample size, ratherthan looking at statistics that are much more inline with the typical or average situation. Example:My father smoked four packs of cigarettes a daysince age fourteen and lived until age sixty-nine.Therefore, smoking really cant be that bad for you. Slippery slope.Definition: When a relatively in-significant first event is suggested to lead to a moresignificant event, which in turn leads to a moresignificant event, and so on, until some ultimate,significant event is reached, where the connectionof each event is not only unwarranted but with eachstep it becomes more and more improbable. Exam-ple: We cannot unlock our child from the closetbecause if we do, she will want to roam the house.If we let her roam the house, she will want to roamthe neighborhood. If she roams the neighborhood,she will get picked up by a stranger in a van, whowill sell her in a sex slavery ring in some othercountry. Therefore, we should keep her locked upin the closet.",
  "CGPT-4 Prompts": "For the few-shot prompt, we manually select 4samples from the Reddit and COCOLOFA datasetas the example data and write the explanation forthem as the demonstrative output. For the Chain-of-Thought prompt, we ask LLMs to first answerseveral questions w.r.t. logical fallacy, then use theanswers to determine the presence and the type ofa logical fallacy in the input.",
  "Prompt for Detection (Zero-shot)": "Determine the presence of a logical fal-lacy in the given [COMMENT] throughthe logic and reasoning of the con-tent.If the available information isinsufficient for detection, output un-known. Utilize the [TITLE] and [PAR-ENT COMMENT] as context to supportyour decision, and provide an explana-tion of the reasoning behind your de-termination. The output format shouldbe [YES/NO/UNKNOWN] [EXPLANA-TIONS]",
  "ipi log pi(2)": "H is the Shannon Diversity Index (Shannon, 1948),S is the total number of unique structures, andpi is the proportion of a unique structure withinits category. The value of J ranges from 0 to 1,with higher values indicating greater evenness instructure diversity. shows the statistics foreach thread structure type in COCOLOFA. In to-tal, COCOLOFA had 347 unique thread structures,most of which were of Single Conversation. Thediversity of thread structures was high.",
  "FExperimental Details": "We had two different versions of BERT and NLImodels. One was fine-tuned on the Reddit dataset,the other was fine-tuned on COCOLOFA. We fine-tuned them with default hyperparameters set in theoriginal paper, i.e., Sahai et al. (2021) and Jin et al.(2022), respectively. Both models were fine-tunedon a server with an A100 GPU. The training tookless than 2 hours for each settings. We ran Llama3on the same server with Ollama,8 a package thatallows us to run open-weight LLMs with 4-bitsquantization on a local server. The inference took 5to 20 seconds for each instance, depending on theprompt and the input.",
  "GAdditional Results on NYT": "To increase the reliability of the NYT annotation,we hired another expert to annotate 250 NYT com-ments sampled from the annotation set. The overallCohens kappa score between two experts is 0.22,echoing our finding in Sec 4 that it is hard to obtainhigh IAA in logical fallacy annotation, and thatlogical fallacy detection in the wild is hard. shows the performance of differentmodels on the 250 samples. We considered bothunion and intersection labels, where the former oneconsidered a borderline case as fallacy while thelatter one considered it as non-fallacy. The resultsuggests that models fine-tuned on COCOLOFAgenerally outperform those trained on Reddit, align-ing with the result we showed in Sec 5.4."
}