{
  "Abstract": "LLMs acquire a wide range of abilities dur-ing pre-training, but aligning LLMs under Re-inforcement Learning with Human Feedback(RLHF) can lead to forgetting pretrained abili-ties, which is also known as the alignment tax.To investigate alignment tax, we conducted ex-periments with existing RLHF algorithms us-ing OpenLLaMA-3B, which revealed a pro-nounced alignment tax in NLP tasks. Whereas,despite various techniques to mitigate forget-ting, they are often at odds with the RLHF per-formance, leading to a trade-off between align-ment performance and forgetting mitigation,leading to an alignment-forgetting trade-off. In this paper we show that model averaging,which simply interpolates between pre and postRLHF model weights, surprisingly achievesthe most strongest alignment-forgetting Paretofront among a wide range of competing meth-ods. To understand its effectiveness, we offertheoretical insights into model averaging, re-vealing that it enhances performance Paretofront by increasing feature diversity on the lay-ers where tasks share overlapped feature spaces.Empirical evidence corroborates our analysisby showing the benefits of averaging low-leveltransformer layers. Building on the analysisand the observation that averaging different lay-ers of the transformer leads to significantly dif-ferent alignment-forgetting trade-offs, we pro-pose Heterogeneous Model Averaging (HMA)to Heterogeneously find various combinationratios of model layers. HMA seeks to maxi-mize the alignment performance while incur-ring minimal alignment tax. Moreover, we val-idate HMAs performance across a range ofRLHF algorithms over OpenLLaMA-3B andfurther extend our findings to Mistral-7B whichis evaluated by open-sourced preference modeland GPT4. Code available here1.",
  "Introduction": "Large Language Models (LLMs), such as GPT4(OpenAI, 2023), Bard (Google, 2023), and Claude(Anthropic, 2023), have attracted widespread atten-tion due to their remarkable achievements. LLMsare pre-trained on vast datasets, which equip themwith the ability to effectively handle diverse tasks,e.g., GPT-3 showcases its prowess in varioustasks such as reasoning, common sense question-answering (QA), translation, and so on.While LLMs exhibit strong abilities among vari-ous benchmarks, they still require alignment withhuman preferences, including the principles of be-ing helpful, honest, and harmless as outlined by(Askell et al., 2021). The goal is to ensure thatLLMs are designed to assist users in completingtasks, provide truthful information without decep-tion, and avoid causing harm, whether physical,psychological, or social, to individuals or the en-vironment. The process of aligning LLMs withhuman preferences often involves the applicationof Reinforcement Learning with Human Feedback(RLHF) (Ouyang et al., 2022), as shown in .Although RLHF allows LLMs to align with humanexpectations, prior studies (Askell et al., 2021; Ope-nAI, 2023; Song et al., 2023) have found that thisapproach can lead to forgetting in the diverse abili-ties that the LLMs have already acquired, as illus-trated in . This phenomenon, also knownas the alignment tax\" in the literature, has accu-mulated substantial attention from both academiaand industry (Ouyang et al., 2022; Anthropic, 2023;Askell et al., 2021; Tu et al., 2023; Noukhovitchet al., 2023).Investigating alignment tax.In this paper,we first conduct a comprehensive investigation onalignment tax and develop methods to reduce align-ment tax while maintaining the alignment perfor-mance. In particular, we followed the approach pre-sented by (Ouyang et al., 2022) and evaluated align-",
  ": Illustration of RLHF procedure and the align-ment tax": "ment tax using multiple NLP benchmarks fromcommon sense QA, such as ARC Easy and Chal-lenge (Clark et al., 2018), Race (Lai et al., 2017),and PIQA (Bisk et al., 2020), reading compre-hension benchmarks including SQuAD (Rajpurkaret al., 2018) and DROP (Dua et al., 2019), and trans-lation tasks, including WMT 2014 French to En-glish translation (Bojar et al., 2014) (c.f. ).Our primary focus is on aligning the OpenLLaMA-3B on the helpfulness and harmlessness dataset(Bai et al., 2022) using Rejection Sampling Fine-tuning methods (Dong et al., 2023) (also knownas the best-of-n algorithm). In the later part, weextend our experiments to Mistral-7B and DirectPreference Optimization (DPO, (Rafailov et al.,2023)). We mainly focus on RSF and DPO sincethey are popular and nearly all of the latest open-sourced LLMs on the leaderboards are aligned bythese two methods2. Indeed, we observed a sub-stantial alignment tax on these benchmarks consis-tently, confirming the findings of (Ouyang et al.,2022; Gao et al., 2023). Specifically, as we gaineda higher reward during RLHF, indicating betteralignment with human preference, the alignmenttax also increased simultaneously, clearly inducinga alignment-forgetting trade-off.Surprising effectiveness of model averagingover. We then compare various methods developedin different communities as potential rescues to al-leviate the alignment tax. This includes the modelaveraging method (Wortsman et al., 2022b,a; Linet al., 2023) from out-of-distribution (OOD) gener-alization literature, regularization-based techniquesfrom the continual learning literature (Panigrahiet al., 2023; Xuhong et al., 2018; Buzzega et al.,2020; Huang et al., 2021), low-rank adaptation(LoRA) (Huang et al., 2021) from the parameter-efficient fine-tuning literature, as well as the uti-lization of reward penalty from the reinforcementlearning literature (Ziegler et al., 2019; Wu et al.,2021a; Ouyang et al., 2022; Yuan et al., 2023). In-terestingly, we found that model averaging, which simply interpolates between the weights of modelsbefore and after RLHF, achieves the most efficientalignment-forgetting Pareto front. In Appendix C.1,we further show and discuss the in-effectivenessof Experience Reply (Rebuffi et al.) method com-pared with MA.Understanding the effectiveness of model av-eraging. To understand the effectiveness of modelaveraging, we provide theoretical insights based onthe framework of (Lin et al., 2023). In particular,we show that the method can enhance Pareto frontby increasing feature diversity on layers where twotasks share similar feature spaces. Empirical evi-dence also indicates that averaging the low-levellayers of Transformers consistently improves bothalignment reward and NLP task performance. Thisaligns with our theoretical insights, as tasks couldshare similar lower-level features, e.g., better wordrepresentation on low-level layers benefits bothNLP and alignment tasks.Heterogeneous model averaging. We noticedthat averaging different layers of the Transform-ers unveiled notably distinct patterns of alignment-forgetting trade-off, aligning with our earlier anal-ysis that tasks may exhibit varying overlappingfeature spaces in different layers. Motivated bythis observation, we propose Heterogeneous ModelAveraging (HMA), which adaptively averages dif-ferent parts of the models during model averag-ing. We start by dividing the transformer into Kparts and assigning unique averaging ratios for eachpart, represented as i for the ith part.HMA aims to maximize alignment reward by op-timizing the averaging ratios (1, . . . , K) whilemaintaining the overall alignment tax, thus con-sistently improve the alignment-forgetting Paretofront. To demonstrate the efficiency of HMA, wealso contrasted our method with other RLHF tech-niques, including Direct Preference Optimization(DPO). (Rafailov et al., 2023) We further substanti-ate our findings on Mistral-7B where evaluationsconducted by open sourced perference model andGPT4, which further corroborates our empiricalfindings on OpenLLaMA-3B.We summarize our contributions as follows: We provide a comprehensive investigation ofthe alignment tax challenge in RLHF on NLPtasks.We systematically compare a widerange of methods to alleviate alignment taxand highlight model averaging as a particu-larly effective approach. We provide theoretical insights into the effi-ciency of model averaging in enhancing thealignment-forgetting trade-off, demonstratingthat both NLP and alignment tasks can bene-fit from the increased feature diversity frommodel averaging in the shared feature space. Motivated by our analysis, we introduce Het-erogeneous Model Averaging (HMA), whichoptimizes the averaging ratios of differentmodel layers to maximize alignment per-formance. HMA consistently improves thePareto front across different benchmarks, andit also generalizes well across various RLHFalgorithms and different model types, such asOpenLLaMA-3B and Mistral-7B, evaluatedby open-sourced preference model and GPT4. The paper is structured as follows: we conducta systematic investigation of existing methods in-4. In , we provide insights intothe effectiveness of model averaging. Subsequently,we propose Heterogeneous Model Averaging in. We conclude the paper in .",
  "Discussion with existing works": "In this section, we provide comparison of this workwith existing works to highlight the novelty of ourfindings. We defer more comprehensive relatedworks to Appendix A.Existing works of model averaging for LLMs.Previous research has covered certain aspects ofmodel averaging. (Ram et al., 2024) demonstratethe utilization of model averaging to constructa more resilient reward model for reinforcementlearning with human feedback (RLHF). In a similarvein, (Rame et al., 2024) employ model averagingto merge policy models trained for distinct objec-tives, facilitating multi-objective RLHF. (Sanyalet al., 2023) introduce the integration of movingaveraging to enhance pre-training. However, noneof these studies investigate the alignment tax, andtheir findings are independent of our research.Existing works on finding adaptive combina-tions for model merging. Previous studies (Yanget al., 2023; Akiba et al., 2024) have also discussedthe idea of dynamically assigning different weightsto different layers when merging models, aimingto maximize performance on a specific task (e.g.,Ti). These approaches assume access to the task-specific data Ti. However, considering the natureof alleviating alignment tax, which aims to miti-gate forgetting across a extremely wide range of tasks (Tj1...TjK), these methods fail to effectivelyoptimize performance for multiple tasks simulta-neously. In the Appendix E.4, we demonstratethat using the method proposed by (Yang et al.,2023), which optimizes for a single task, does noteffectively address forgetting on the other tasks.Furthermore, our work is the first to provide an ex-planation for the surprising effectiveness of modelaveraging in alleviating forgetting, as well why weshould assign heterogeneous combination ratios.Existing works on the forgetting of languagemodels. Most research on forgetting in languagemodels focuses on sequentially pre-training (Chenet al., 2023; Gong et al., 2022; Jin et al., 2021; Qinet al., 2022; Liu et al., 2021) or fine-tuning tasks(Sun et al., 2019; Razdaibiedina et al., 2023; Wuet al., 2021b; Zhang et al., 2022; Madotto et al.,2020), e.g., sequentially training on task Ti andthen task Tj. They evaluate forgetting by measur-ing the models performance on a task (e.g., taskTi) after training it on another task (e.g., task Tj).However, these methods have not explored the ef-fectiveness of model averaging. In our case, wedemonstrate the significant power of model aver-aging which outperform a wide range of existingmethods. Furthermore, existing works assume thatthe data size of each task is comparable (i.e., thedataset size of Ti and Tj is similar), allowing for asubset (e.g., 10%) of old task data replay, which isshown to effective alleviate the forgetting withoutexcessive computation overhead in their settings.However, in our alignment tax situation, we aimto preserve a wide range of abilities gained dur-ing pre-training, which is challenging since pre-training datasets are often not publicly available.In Appendix C.1, we show that even when we haveaccess to the pre-training data and replay a subsetup to four times larger than the RLHF data (whichcosts significant computation overhead), experi-ence replay still under-performs model averagingin two out of three benchmarks. This is likely dueto the vast size of the pre-training data, where thesubset only covers a small fraction of it (e.g., onlycovers ~0.01% of the pre-training data). So replaymethods are less practical for alleviating alignmenttax.",
  "Basic Setting. We chose the OpenLLaMA-3Bmodel (Geng and Liu, 2023) because (1) it iscomputational friendly compared with 7B models(2) it has openly available pre-training dataset,": "which is convenient to investigate ExperienceReplay in Appendix. C.1. Furthermore, we extendthe experiments to Mistral-7B in Sec. 6. Followingthe standard procedure outlined in (Ouyang et al.,2022), we initially conducted instruction tuning,followed by RLHF. Here, represents an LLMwith parameters , with the pre-trained modeldenoted as pre. We commenced with instructionfine-tuning for pre on ShareGPT 3, which yielded0. Subsequently, RLHF was performed on 0 toobtain . Similar to the methodology proposed in(Ouyang et al., 2022), the alignment tax was eval-uated by comparing the performance regressionof with 0 across various NLP tasks. The wholeprocedure and notations are illustrated in .Datasets for Evaluating Alignment Tax. Fol-lowing the approach in (Ouyang et al., 2022), ourevaluation of alignment tax encompasses variousNLP benchmarks: (a) Common Sense QA: Thisincludes ARC Easy and Challenge (Clark et al.,2018), Race (Lai et al., 2017), and PIQA (Bisket al., 2020), with the performance being assessedusing accuracy. (b) Reading Comprehension: weemploy SQuAD (Rajpurkar et al., 2018) and DROP(Dua et al., 2019) to gauge reading comprehensionability, with evaluation based on the F1 score forboth datasets. (c) Translation: Our evaluation uti-lizes WMT 2014 French to English translation (Bo-jar et al., 2014), with performance measured usingBLEU (Papineni et al., 2002) scoring.RLHF Basics. In our notation, denotes thepolicy induced by the LLM . Additionally, x rep-resents the input prompt and a denotes the output(which is also referred to as an action in RL lit-erature (Schulman et al., 2017)). Drawing from(Ouyang et al., 2022; Bai et al., 2022; Dong et al.,2023; Touvron et al., 2023; Rafailov et al., 2023),we assume the existence of a ground-truth rewardfunction r(x, a) : X A , where X andA denote the spaces of x and a respectively. Theprimary objective of RLHF is to maximize:",
  ": Illustration of Heterogeneous Model Averag-ing (HMA) when K = 3": "in Sec. 6. Essentially, the RSF learns from thebest-of-n policy (Nakano et al., 2021), which sam-ples n responses for each prompt query and returnsthe one with the highest reward. As suggestedby (Dong et al., 2023; Touvron et al., 2023; Gul-cehre et al., 2023), we adopt an iterative trainingset-up for the implementation instead of alwayssampling the samples from the starting checkpointbecause we find that the iterative training is farmore sample-efficient. Specifically, for each itera-tion, we first sample a batch of prompts and gener-ate n responses for each prompt from the currentmodel. Then, we use the reward model to computethe rewards for each prompt-response pair, and foreach prompt, we select the one with the highestreward into a small subset. By this process, wecollect a batch of samples from the best-of-n policythat are with high reward. We simply fine-tune thecurrent model on this subset to get the next modeland the next iteration begins.",
  "Evaluating Existing Methods": "In of Appendix E.1, we visualize thetraining procedure in terms of the alignment-forgetting trade-off during RLHF. Specifically, wecan clearly see that as the RLHF proceeds, the re-ward begins to increase while the translation andreading comprehension ability continues to drop.Interestingly, we observe that the performance ofcommon sense increases first and then drops. Giventhat alignment tax is inherently a catastrophic for-getting issue, we then proceed to explore methodsto reduce alignment tax. Research focused on re-ducing forgetting is mainly classified into two maincategories, depending on the availability of the pre-training dataset. We also investigate the rewardpenalty method developed in RL community inAppendix C.2.",
  "maxExEa(|x)[r(x, a)] + 0,(2)": "where we use = 1, 2 which corresponds to theL1 and L2 (Xuhong et al., 2018) penalties, respec-tively. (c) Low-Rank Adaptation (LoRA) (Hu et al.,2021). It introduces trainable rank decompositionmatrices into linear layers to update 0 duringRLHF. (d) Knowledge distillation (Buzzega et al.,2020; Huang et al., 2021). We use 0 serves asthe teacher and as the student, with a penaltyimposed as:",
  "maxExEa(|x)[r(x, a)] + (x) 0(x)22": "(e) Model Averaging (MA) (Wortsman et al.,2022a,b). This involves simply interpolating be-tween 0 and to yield the policy (1)0+,where is a hyper-parameter ranging from 0to 1.(f) Stochastic Moving Averaging (SMA)(Noukhovitch et al., 2024). More implementationdetails are provided in the appendix.Results. depicts the performance ofeach aforementioned method. The results demon-strate that these approaches effectively alleviate thealignment tax; however, they also result in a reduc-tion in the RLHF reward, indicating a clear trade-off between reward and alignment tax. Notably,despite its simplicity, the Pareto-front of model av-eraging supersedes nearly all other methods acrossvarious hyper-parameters. In Appendix C.1 andC.2, we compared model averaging with Experi-ence Replay (ER) and KL reward penalty methodsfor Proximal policy optimization (Schulman et al.,2017) algorithms, the conclusions are similar.",
  "Unravelling the Mysteries of ModelAveraging for Alleviating AlignmentTax": "Given the promising performance of model aver-aging, we try to understand the efficacy of modelaveraging in this Section and motivate our methodto improve it. We utilize the theoretical frameworkproposed by (Lin et al., 2023) to gain insights intoits effectiveness in alignment tax. While the frame-work addresses classification problems, the insightsderived can aid our understanding of model aver-aging. We also conduct empirical analysis using agenerative model (Openllama-3B) to verify thesetheoretical insights. Analyzing the performance of model averaging in alignment tax is more in-tricate compared to the work of the study by (Linet al., 2023) focuses on out-of-distribution (OOD)scenarios, where the same task is performed underdifferent distributions. In contrast, our focus inalignment tax is to comprehend the performancetrade-offs among different tasks. To illustrate, con-sider the entire feature space Y and two tasks withlabel spaces Ya Y and Yb Y, with the sim-plifying assumption that |Ya| = |Yb| = K. While(Lin et al., 2023) only considers the case whereYa = Yb, we extend these results to encompass thecase where Ya = Yb.Theoretical Settings. Suppose we have manyfeatures Sx = {xi}Di=1 where each feature xi Rd and the observed feature x RdD is a con-catenation of x1, ..., xD. Following (Lin et al.,2023), we adopt a simplified model f(x) = w(x)where w RdK, (x) = Di=1 ixi andi {0, 1}, i. Suppose we have two modelsfa() = waa() and fb = wbb() for tasksTa and Tb, respectively, relying on feature setsSx,a Sx and Sx,b Sx, with |Sx,a| = |Sx,b| =n, and |Sx,a Sx,a| = no overlapped features.The averaged model of fa and fb is favg() =wavgavg(), where wavg = (wa + wb)/2 andavg,i = (a,i + b,i)/2, i (Lin et al., 2023). Togain an intuitive understanding, we compare modelaveraging in two cases: Case (1) when the tasksare quite similar (|YA YB| = K) and Case (2)when the tasks are independent (|YA YB| = 0).4 Furthermore, even if the tasks are very similar,fitting two models on them can rely on differentfeatures due to randomness in data or training pro-cedures (Lin et al., 2023; Allen-Zhu and Li, 2020).We will investigate the performance of model aver-aging in Case (1) and (2) to gain insights on whenit works. Following (Lin et al., 2023), we assumeeach feature is weak, failing with probability p.The effectiveness of model averaging is given by",
  "where Aa(f) and Ab(f) denote the accuracy of fon task a and b, respectively. We use (1) to denotethe effective averaging robustness for Case (1) andsimilarly define (2) for Case (2)": "4Notably, the overlap in features is independent of theoverlap in label space. For instance, when classifying a dog,we can use either the animal shape or the texture (overlappedlabel space, non-overlapped feature); when classifying a dogor a cat, we can both use the animal shape (non-overlappedlabel space, overlapped feature). 5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)Regularization-KDRegularization-L1Regularization-L2MoAGraftLoRAEarly Stopping 5.05.56.06.57.0 HH RLHF Reward 0.538 0.540 0.542 0.544 0.546 0.548 0.550 0.552 Commonsense QA (ACC) MA (RSF)Regularization-KDRegularization-L1Regularization-L2MoAGraftLoRAEarly Stopping 5.05.56.06.57.0 HH RLHF Reward Translation Fr-En (BLEU) MA (RSF)Regularization-KDRegularization-L1Regularization-L2MoAGraftLoRAEarly Stopping",
  "where the equality holds when no = n and Fp(x)is a cumulative density function in Appendix F.4": "Implications.Proposition 5.1 demonstrates thatwhen Ta and Tb are more similar, the averagingof models (fa and fb) yields greater improvement.However, this improvement is reduced if fa andfb use more overlapping features. Recall that eachweak feature can fail with probability p. If Ta andTb are similar, the feature utilized by the two mod-els would be projected into a shared space, allowingmodel averaging to take advantage of a more di-verse set of features. This diversity reduces theprobability of model failure because a diverse setof features is less likely to fail together simultane-ously (Lin et al., 2023). However, if Ta and Tb aredissimilar, for example, if |Ya Yb| = 0 and thefeature spaces corresponding to Ya and Yb are dis-joint, then the features in the space of Ya would notprovide any information for predicting Yb. There-fore, averaging fa and fb would not improve theprediction of either task in this case. Refer to Ap-pendix F.3 for a detailed discussion.Notably, the model 0 excels in NLP abilitiesbefore RLHF, while the model excels in align-ment reward after RLHF. Using an analogy, wecan equate NLP tasks with Ta, alignment with Tb,0 to fa, and to fb. Recall that we adopt a sim-plified model for theoretical analysis by consid-ering only one layer feature learner, although, inpractice, we average a deep Transformer with 26layers. Research has shown that different layersin deep neural networks capture varying levels offeatures (Yosinski et al., 2015; Zeiler and Fergus,2014; Simonyan and Zisserman, 2014). For in-stance, low-level layers capture low-level features. Furthermore, tasks share similar feature space ata low level (alternatively, from the perspective oflow-level layers, tasks look more similar). Forexample, improving the low-level features suchas better word representation could enhance bothRLHF reward and NLP tasks. Therefore, accordingto Proposition 5.1, averaging the low-level layerscould potentially elicit more improvements in bothTa (NLP tasks) and Tb (alignment reward) thanhigher layers.Empirical Validation. We categorize the 26transformer layers of Openllama into three parts:the input part (layers 1-8), the middle part (lay-ers 9-17), and the output part (layers 18-26). Thisdivision is depicted in . We use the super-scripts , , and to denote the input, middle,and output parts, respectively. For instance, represents the middle layers (9-18) of . Here, 0and respectively refer to the models before andafter RLHF. We investigate the impact of averagingone part instead of the whole Transformer: givena combination ratio , we average the i-thpart of (i.e., [i]) with the corresponding part of 0(i.e., [i]0 ), while keeping the remaining two parts of unchanged. So when we average the input part,the j-th part of the averaged model is:",
  "[j] + (1 )[j]0 , if j = 1,[j], if j = 2, 3": "The results of the above scheme are denoted asInput Part MA\". Middle Part MA\" and OutputPart MA\" represent that we average the middle andoutput parts, respectively. illustrates thatthe alignment-forgetting trade-off varies distinctlywhen different parts of the transformers are aver-aged. Specifically, when we average the low-levellayers, we observe a magical improvement inboth the NLP tasks and alignment rewards, whichis consistent with our previous analysis. Further-more, we show results in Appendix E.2 that themagical improvement in averaging the low-levelparts is consistent among DPO and PPO models. 4.55.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)Input Part MA (RSF)Middle Part MA (RSF)Output Part MA (RSF)",
  "Heterogeneous Model Averaging": "We have already shown that averaging differentlayers results in diverse patterns of alignment-forgetting trade-off (Wu et al., 2022; Lee et al.,2022b). Therefore, different layers should not beequally treated during averaging. This leads to anatural question: can we enhance the alignment-forgetting trade-off by using adaptive weights fordifferent layers? Consequently, we conduct proof-of-concept experiments to provide affirmative an-swers to this question and subsequently propose apractical algorithm.Proof of Concept. The following proof of con-cept experiments provide insights into average dif-ferent layers with various ratios. We use differentaveraging ratio, i.e., 1, 2, 3, for the three parts.Specifically, the ith part of the averaged model issimply i[i] + (1 i)[i]0 . We try three patternsexperiment given a base {0.2, 0.3, 0.4} : (a)1 = 2 = 3 = ; (b) 1 = 2 = , 3 = 0.1, and (c) 1 = , 2 = 3 = 0.1. Weuse (||), (||0.1) and (|0.1|0.1)to denote these three patterns, respectively. Theseresults confirm that certain ratio combinations ex-ceed the trade-off curve of vanilla model averaging,as displayed in in Appendix C.3. Notably,some combination ratios consistently outperformthe equal ratio across various benchmarks. Thisaffirms the potential to identify consistent combina-tion ratios that demonstrate superior performanceacross a broad spectrum of benchmarks in terms ofalignment-forgetting trade-off.Heterogeneous Model Averaging. Upon divid-ing the Transformer into K parts, our objective isto adaptively determine a combination ratio for dif-ferent layers that consistently perform well acrossan extensive range of tasks. The conventional aver-aging method uses a shared for all layers, playinga crucial role in defining the trade-off between re- ward and tax. We aim to identify an optimized com-bination of (1, ..., K) to replace a uniform . Let(K) represent the model merged by (1, ..., K).In particular, the kth component of the mergedmodel (K) is given by",
  "The intuition behind HMA is outlined as follows:(1) When maintaining the mean, i.e., 1": "Kk k, as, we can compare HMA performance with theperformance of vanilla model averaging with thesame . (b) We only optimize K parameters, whereK is typically small. For example, we adopt K = 3by default and also include results with varying Kto the ablation study. This helps to ensure that theforgetting level of (1, ..., K) remains close to. Intuitively, if we optimize a large number ofparameters, it could easily lead to over-fitting inthe in-domain (RLHF reward) and may also resultin more significant forgetting. The whole algorithmis summarized Algorithm 1 in appendix.Results. The results of HMA are shown in Fig-ure 5. We can see that HMA can consistently pushforward the Perato-front of the vanilla model aver-aging. Furthermore, such improvement is consis-tent over various RLHF algorithms. More detailedresults (e.g., on Commonsense QA and Translationwith different RLHF algorithms) of HMA can befound in Appendix E.5.Ablation results on different K. We tested dif-ferent values of K with = 0.2, 0.4, 0.6 as il-lustrated in (Right). The trade-off curveshows a slight decrease as we increase K from 3to 6 and 9, but still consistently improves over thevanilla model averaging. This decrease is likelydue to overfitting. Specifically, comparing the per-formance of HMA with different K for the samemean ratio, we observe that as the alignment re-ward increases with an increase in K from 3 to 9,the reading comprehension performance drops.How to choose the averaging ratio. In prac-tice, we determine the averaging ratio for adopt- 6.06.26.46.66.87.0 HH RLHF Reward 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 Reading Comprehension (F1) MA (RSF)HMA (RSF) 5.65.86.06.26.4 HH RLHF Reward 15.5 16.0 16.5 17.0 17.5 Reading Comprehension (F1) MA (DPO)HMA (DPO) 6.06.26.46.66.87.0 HH RLHF Reward 15.0 15.5 16.0 16.5 17.0 17.5 Reading Comprehension (F1) MA (RSF)HMA Block3 (RSF)HMA Block6 (RSF)HMA Block9 (RSF)",
  ": Results of Zephyr-7B- evaluated by opensourced preference model. (Top) Similar trends eval-uated by PairRM when we average different blocks.(Bottom) Our HMA consistently improve over MA": "ing vanilla MA or our HMA. Changing the av-eraging ratio for MA and HMA is convenient asthese methods are applied after training the vanillaRLHF checkpoint. The comprehensive results inFigures 3, 5, and 16 (details in Appendix C.4) showthat = 0.2 can consistently alleviate the align-ment tax without hurting alignment performance.Further results of Zephyr-7B are shown in .Additionally, the performance of the averaging ra-tio on different benchmarks () exhibits sim-ilar trends. Hence, we believe = 0.2 is a suitablechoice that can generalize to more tasks.",
  "Zephyr-7B-Gemma11.3%41.1566.338.09HMA (Ours)11.5%42.4566.438.71": ": GPT4 evaluation of experiments of Zephyr-7B- and Zephyr-7B-Gemma on Alpaca benchmark.Reading is short for Reading Comprehension, which isevaluated by F1. CommonSence is evaluated by Accu-racy (%). Trans is short for Translation Fr-En, evaluatedby BLEU. Other models results. To further validate ourmethod on larger LLMs, e.g., Mistral-7B (Jianget al., 2023a) based models, we apply model av-eraging (MA) and Heterogeneous Model Aver-aging (HMA) on Zephyr-7B-5 (Tunstall et al.,2023) which is trained with DPO on the SFT ver-sion, Mistral-7B-SFT-6. We also apply HMA onZephyr-7B-Gemma 7 which is aligned based onGemma-7B8 model. Here we use the the publiclyavailable preference model PairRM (Jiang et al.,2023b) to judge the helpfulness and evaluate mod-els on AlpacaEval 2.0 (Li et al., 2023). We re-ports the win rates of each model. (Top)shows that the trends of averaging different layersevaluated by PairRM are similar with the resultsevaluated by our own reward model. The resultsrange across = 0, 0.2, . . . , 1.0 depicted in Fig-ure 6 (Bottom) demonstrate that MA effectivelyachieves a strong Pareto front to mitigate forgettingin the Mistral-7B models. Additionally, our HMAalgorithm shows further improvement compared tothe MA method.GPT4 Evaluation. We also use GPT4 to evalu-ate HMA on AlpacaEval 2.0 (Li et al., 2023). Dueto the limited quota, we only compare HMA with = 0.2 with vanilla Zephyr-7B- ( = 0.2 is rec-ommended by the previous discussion). In ,we summarize their Win-Rate against GPT4 as wellas their performance on NLP tasks. We show thatHMA consistently outperforms Zephyr-7B- onall the metrics.",
  "Anthropic. 2023. Introducing claude": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, DeepGanguli, Tom Henighan, Andy Jones, Nicholas Joseph,Ben Mann, Nova DasSarma, et al. 2021.A generallanguage assistant as a laboratory for alignment. arXivpreprint arXiv:2112.00861. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot,Daniel Guo, Daniele Calandriello, Michal Valko, and RmiMunos. 2023. A general theoretical paradigm to under-stand learning from human preferences. arXiv preprintarXiv:2310.12036. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,Deep Ganguli, Tom Henighan, et al. 2022. Training ahelpful and harmless assistant with reinforcement learningfrom human feedback. arXiv preprint arXiv:2204.05862. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.2020. Piqa: Reasoning about physical commonsense innatural language. In Proceedings of the AAAI conferenceon artificial intelligence, volume 34, pages 74327439. Ondrej Bojar, Christian Buck, Christian Federmann, BarryHaddow, Philipp Koehn, Johannes Leveling, ChristofMonz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al.2014. Findings of the 2014 workshop on statistical ma-chine translation. In Proceedings of the ninth workshop onstatistical machine translation, pages 1258.",
  "Ralph Allan Bradley and Milton E Terry. 1952. Rank anal-ysis of incomplete block designs: I. the method of pairedcomparisons. Biometrika, 39(3/4):324345": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.Language models are few-shot learners. Advances in neu-ral information processing systems, 33:18771901. Pietro Buzzega, Matteo Boschini, Angelo Porrello, DavideAbati, and Simone Calderara. 2020. Dark experience forgeneral continual learning: a strong, simple baseline. Ad-vances in neural information processing systems, 33:1592015930. Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars,Joelle Pineau, and Eugene Belilovsky. 2021. New insightson reducing abrupt representation change in online contin-ual learning. arXiv preprint arXiv:2104.05025. Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and JoellePineau. 2020. Online learned continual compression withadaptive quantization modules. In International Confer-ence on Machine Learning, pages 12401250. PMLR. Stephen Casper, Xander Davies, Claudia Shi, Thomas KrendlGilbert, Jrmy Scheurer, Javier Rando, Rachel Freed-man, Tomasz Korbak, David Lindner, Pedro Freire, et al.2023. Open problems and fundamental limitations of rein-forcement learning from human feedback. arXiv preprintarXiv:2307.15217.",
  "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,Shane Legg, and Dario Amodei. 2017. Deep reinforcementlearning from human preferences. Advances in neuralinformation processing systems, 30": "Xu Chu, Yujie Jin, Wenwu Zhu, Yasha Wang, Xin Wang,Shanghang Zhang, and Hong Mei. 2022. Dna: Domaingeneralization with diversified neural averaging. In Interna-tional Conference on Machine Learning, pages 40104034.PMLR. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, AshishSabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.Think you have solved question answering? try arc, the ai2reasoning challenge. arXiv preprint arXiv:1803.05457.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. 2018. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprintarXiv:1810.04805": "Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, JipengZhang, Wei Xiong, and Tong Zhang. 2023. Lmflow: Anextensible toolkit for finetuning and inference of large foun-dation models. arXiv preprint arXiv:2306.12420. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, ShizheDiao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023.Raft: Reward ranked finetuning for generative foundationmodel alignment. arXiv preprint arXiv:2304.06767. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, GabrielStanovsky, Sameer Singh, and Matt Gardner. 2019. Drop:A reading comprehension benchmark requiring discrete rea-soning over paragraphs. arXiv preprint arXiv:1903.00161. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, DimitrisTsipras, Firdaus Janoos, Larry Rudolph, and AleksanderMadry. 2020. Implementation matters in deep policy gra-dients: A case study on ppo and trpo.arXiv preprintarXiv:2005.12729.",
  "J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora:Low-rank adaptation of large language models. ArXiv,abs/2106.09685": "Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, andDiyi Yang. 2021. Continual learning for text classifica-tion with information disentanglement based regularization.arXiv preprint arXiv:2104.05489. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, ChrisBamford, Devendra Singh Chaplot, Diego de las Casas,Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-cile Saulnier, et al. 2023a. Mistral 7b. arXiv preprintarXiv:2310.06825.",
  "Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023b.Llm-blender: Ensembling large language models withpairwise ranking and generative fusion. arXiv preprintarXiv:2306.02561": "Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-WenLi, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2021.Lifelong pretraining: Continually adapting language mod-els to emerging corpora. arXiv preprint arXiv:2110.08534. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, JoelVeness, Guillaume Desjardins, Andrei A Rusu, KieranMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgettingin neural networks. Proceedings of the national academyof sciences, 114(13):35213526.",
  "Zihan Liu, Genta Indra Winata, and Pascale Fung. 2021.Continual mixed-language pre-training for extremely low-resource neural machine translation.arXiv preprintarXiv:2105.03953": "Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, SeungwhanMoon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, andZhiguang Wang. 2020. Continual learning in task-orienteddialogue systems. arXiv preprint arXiv:2012.15504. James L McClelland, Bruce L McNaughton, and Randall COReilly. 1995. Why there are complementary learningsystems in the hippocampus and neocortex: insights fromthe successes and failures of connectionist models of learn-ing and memory. Psychological review, 102(3):419. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, LongOuyang, Christina Kim, Christopher Hesse, Shantanu Jain,Vineet Kosaraju, William Saunders, et al. 2021. Webgpt:Browser-assisted question-answering with human feedback.arXiv preprint arXiv:2112.09332.",
  "Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, PengLi, Maosong Sun, and Jie Zhou. 2022. Elle: Efficientlifelong pre-training for emerging data. arXiv preprintarXiv:2203.06311": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,Gabriel Goh, Sandhini Agarwal, Girish Sastry, AmandaAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learningtransferable visual models from natural language supervi-sion. In International Conference on Machine Learning,pages 87488763. PMLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,Christopher D Manning, and Chelsea Finn. 2023. Directpreference optimization: Your language model is secretlya reward model. arXiv preprint arXiv:2305.18290.",
  "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Knowwhat you dont know: Unanswerable questions for squad.arXiv preprint arXiv:1806.03822": "Rajkumar Ramamurthy, Prithviraj Ammanabrolu, KiantBrantley, Jack Hessel, Rafet Sifa, Christian Bauckhage,Hannaneh Hajishirzi, and Yejin Choi. 2022.Is rein-forcement learning (not) for natural language process-ing?: Benchmarks, baselines, and building blocks fornatural language policy optimization.arXiv preprintarXiv:2210.01241. Alexandre Rame, Guillaume Couairon, Corentin Dancette,Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, andMatthieu Cord. 2024. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned ondiverse rewards. Advances in Neural Information Process-ing Systems, 36. Alexandre Ram, Nino Vieillard, Lonard Hussenot, RobertDadashi, Geoffrey Cideron, Olivier Bachem, and JohanFerret. 2024. Warm: On the benefits of weight averagedreward models. arXiv preprint arXiv:2401.12187.",
  "Anastasia Razdaibiedina, Yuning Mao, Rui Hou, MadianKhabsa, Mike Lewis, and Amjad Almahairi. 2023. Pro-gressive prompts: Continual learning for language models.arXiv preprint arXiv:2301.12314": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl,and Christoph H Lampert. 2017. icarl: Incremental clas-sifier and representation learning. In Proceedings of theIEEE conference on Computer Vision and Pattern Recogni-tion, pages 20012010. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl,Christoph H Lampert, and icarl. Incremental classifier andrepresentation learning. In Conference on Computer Visionand Pattern Recognition (CVPR), pages 55335542. Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,Irina Rish, Yuhai Tu, and Gerald Tesauro. 2018. Learningto learn without forgetting by maximizing transfer and min-imizing interference. arXiv preprint arXiv:1810.11910.",
  "Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018.Online structured laplace approximations for overcomingcatastrophic forgetting. Advances in Neural InformationProcessing Systems, 31": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, ArnaudStiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitaskprompted training enables zero-shot task generalization.arXiv preprint arXiv:2110.08207. Sunny Sanyal, Atula Tejaswi Neerkaje, Jean Kaddour, Ab-hishek Kumar, et al. 2023. Early weight averaging meetshigh learning rates for llm pre-training. In Workshop onAdvancing Neural Network Training: Computational Effi-ciency, Scalability, and Resource Optimization (WANT@NeurIPS 2023). Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,Suzana Ilic, Daniel Hesslow, Roman Castagn, Alexan-dra Sasha Luccioni, Franois Yvon, Matthias Gall, et al.2022. Bloom: A 176b-parameter open-access multilinguallanguage model. arXiv preprint arXiv:2211.05100.",
  "Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu,Xu Yinghui, Peng Cui, and Yuan Qi. 2023. Provably in-variant learning without domain information": "Ross Taylor, Marcin Kardas, Guillem Cucurull, ThomasScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-ton, Viktor Kerkez, and Robert Stojnic. 2022.Galac-tica: A large language model for science. arXiv preprintarXiv:2211.09085. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-tinet, Marie-Anne Lachaux, Timothe Lacroix, BaptisteRozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al.2023. Llama: Open and efficient foundation languagemodels. arXiv preprint arXiv:2302.13971.",
  "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023b.A comprehensive survey of continual learning: Theory,method and application. arXiv preprint arXiv:2302.00487": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.2022. Self-instruct: Aligning language model with selfgenerated instructions. arXiv preprint arXiv:2212.10560. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, RebeccaRoelofs, Raphael Gontijo-Lopes, Ari S Morcos, HongseokNamkoong, Ali Farhadi, Yair Carmon, Simon Kornblith,et al. 2022a. Model soups: averaging weights of multiplefine-tuned models improves accuracy without increasinginference time. In International Conference on MachineLearning, pages 2396523998. PMLR. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li,Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes,Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong,et al. 2022b. Robust fine-tuning of zero-shot models. InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 79597971. Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim,Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong,and Ludwig Schmidt. 2021. Robust fine-tuning of zero-shot models. 2022 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 79497961.",
  "Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon,Ryan Lowe, Jan Leike, and Paul Christiano. 2021a. Re-cursively summarizing books with human feedback. arXivpreprint arXiv:2109.10862": "Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li,Guilin Qi, and Gholamreza Haffari. 2021b. Pretrainedlanguage model in continual learning: A comparative study.In International Conference on Learning Representations. Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li,Guilin Qi, and Gholamreza Haffari. 2022. Pretrained lan-guage model in continual learning: A comparative study.In International Conference on Learning Representations.",
  "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mo-hammad Saleh, and Peter J Liu. 2023. Slic-hf: Sequencelikelihood calibration with human feedback. arXiv preprintarXiv:2305.10425": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen,Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou,et al. 2023. Secrets of rlhf in large language models part i:Ppo. arXiv preprint arXiv:2307.04964. Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, RenzheXu, Peng Cui, and Tong Zhang. 2022a. Model agnosticsample reweighting for out-of-distribution learning. In In-ternational Conference on Machine Learning, pages 2720327221. PMLR.",
  "ARelated Work": "Large Language Models.Large Language Models (LLMs) are pre-trained using vast amounts of dataand has the ability to handle a diverse set of tasks. An excellent line of LLMs includes GPT (Brownet al., 2020; OpenAI, 2023), Bard (Google, 2023), Claude (Anthropic, 2023), LLaMA (Touvron et al.,2023), Galactica (Taylor et al., 2022), Bloom (Scao et al., 2022). It is a common practice to fine-tunethe LLMs to obtain better performance on a specific task (Diao et al., 2023), follow the instruction ofhumans (Ouyang et al., 2022; Sanh et al., 2021; Wang et al., 2022) and align with humans preferences(Christiano et al., 2017; Askell et al., 2021; Bai et al., 2022; Ouyang et al., 2022; Dong et al., 2023). Reinforcement Learning with Human Preference (RLHF).RLHF (Christiano et al., 2017) hasattracted considerable attention in the past few years, particularly after the tremendous success of theChatGPT (Ouyang et al., 2022; OpenAI, 2023). There is a rich literature on RLHF and the relateddiscussions which cannot be comprehensively reviewed here due to the space constraint. We thus refer theinterested readers to the survey paper like (Casper et al., 2023) but focus on the algorithmic designs here.Proximal Policy Optimization (PPO) (Schulman et al., 2017) is the predominant approach in RLHF whoseeffectiveness has been showcased by ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Bard(Google, 2023). However, it is known that the PPO is unstable and sample-inefficient in aligning LLMs(Choshen et al., 2019) and imposes a heavy burden on the GPU resources as it requires loading multiple(typically four) models at the same time (Yuan et al., 2023; Dong et al., 2023). In view of this, attemptshave been made to propose alternative approaches to the PPO algorithm. There is a line of work using therejection sampling (also referred to as the best-of-n sampling in the literature) (Nakano et al., 2021), toreinforce the dataset used to finetune the LLM, including (Dong et al., 2023; Yuan et al., 2023; Touvronet al., 2023; Gulcehre et al., 2023). Among them, (Dong et al., 2023; Touvron et al., 2023; Gulcehreet al., 2023) adopt an iterative framework, which is more sample-efficient and effective, while (Yuan et al.,2023) highlights the importance of sampling strategy. In comparison to the original rejection samplingalgorithm, which generates n responses but only output the one with the highest reward, the LLMs alignedby iterative rejection sampling balance the goal of alignment and the inference cost. Meanwhile, there isalso another line of work aiming to derive algorithms from the reverse KL-constrained contextual bandit(Rafailov et al., 2023; Zhao et al., 2023; Wang et al., 2023a; Azar et al., 2023; Xiong et al., 2023), whosetheoretical property is studied in (Xiong et al., 2023). Among them, Direct Preference Optimization (DPO)(Rafailov et al., 2023) has appeared to be one of the most attractive algorithms, which optimizes the LLMswithout the reward modeling and directly by preference learning from an offline dataset. In view of thesuccess of DPO, there has also been a debate on whether reward modeling is necessary, where (Rafailovet al., 2023; Zhao et al., 2023; Azar et al., 2023) support bypassing reward modeling. Although there aremany works on reward optimization, the forgetting issue (also referred to as the alignment tax (Casperet al., 2023) in the literature) of RLHF algorithms has not been comprehensively studied. Therefore, wechoose three representative algorithms, including the PPO (Schulman et al., 2017), RSF (Dong et al.,2023), and DPO (Rafailov et al., 2023) in this work, to study the catastrophic forgetting issue of LLMsafter RLHF. Pretraining, fine-tuning, and distributional shift.Before the emergence of foundation models, thepre-training and fine-tuning paradigm had already achieved remarkable accomplishments across numerousapplications (He et al., 2016; Radford et al., 2021; Devlin et al., 2018). However, when deploying pre-trained models into real-world applications and fine-tuning them, a common challenge arises: encounteringnovel samples from a target distribution that differs from the fine-tuning distribution (Andreassen et al.,2021; Goyal et al., 2022; Zhang and R, 2022; Lin et al., 2022a; Zhou et al., 2022a,b; Lin et al., 2022b;Tan et al., 2023). To address this issue, several approaches have been proposed. For instance, (Wortsmanet al., 2021; Cha et al., 2021b; Chu et al., 2022) suggest leveraging the weight ensemble of the pre-trainedmodel and the fine-tuned model to enhance out-of-distribution (OOD) performance. Another strategy,as proposed in (Kumar et al., 2022), is the LP-FT technique, which involves initializing the pre-trainedfeature extractor with a reasonably good classifier. This initialization is particularly important when theclassifier is randomly initialized, as the pre-trained features can easily be distorted to accommodate the",
  "random classifier during fine-tuning, exacerbating the issue of catastrophic forgetting": "Catastrophic forgetting and continual learning.DNN tends to lose the knowledge of previouslylearned task (e.g., pretraining task) when it begins to learn a new task (e.g., the fine-tuning task) (McClel-land et al., 1995). Various attempts have been made to alleviate catastrophic forgetting. (Xuhong et al.,2018; Ritter et al., 2018; Aljundi et al., 2018; Schwarz et al., 2018) impose a penalty on the change ofthe parameter on the new task. (Yu et al., 2021) transfers knowledge from related new knowledge typesback to the old types by continually training the representations of old knowledge with the data for newknowledge using a self-training loss. (Yu and Ji, 2023) observes that LLMs tend to rely on pre-existingknowledge, neglecting recent facts and leading to incorrect reasoning chains that ultimately diminish theefficacy of information updates, and proposes to mitigate exposure bias by incorporating the selection ofrelevant facts into training losses. (Kirkpatrick et al., 2017) gain intuition from Taylor expansion of thelosses of the old task at the point of fine-tuned parameter, and further proposes EWC by incorporating theHassien matrix into parameter regularization. The reply-based method tries to approximate and recoverthe old data distribution. Popular methods in this direction include sampling methods which store a fewold training samples with a small memory buffer (Vitter, 1985; Riemer et al., 2018; Chaudhry et al., 2018;Cha et al., 2021a; Caccia et al., 2021), and generative methods which generate samples from the olddistributions with a generative model (Caccia et al., 2020). Knowledge distillation (KD) methods try tokeep the prediction of the fine-tuned model close to that of the old model. KD can be naturally combinedwith experience reply. For example, (Rebuffi et al., 2017) proposes to perform KD on the samples of newtasks as well as the old samples stored in the buffer.Notably, previous continual learning focuses on sequentially learning tasks which learns a sequence oftasks in order and measures the forgetting of older tasks when learning new tasks (Wang et al., 2023b).Whereas, we focus on the generality forgetting of the pre-trained foundation model during fine-tuning aspecific task. Alignment tax.(Ouyang et al., 2022) reports that they observe significant alignment tax when develop-ing InstructGPT. They have also tried to adopt Experience Replay to alleviate this issue, which is followedby (Zheng et al., 2023). However, we show in Appendix C.1 that Experience Relay is less favorable whencompared with model averaging. (Noukhovitch et al., 2024) tried to use stochastic weight averaging,which still under-performs our method as shown in . (Li et al., 2024) finds that DPO inducesless alignment tax compared with other RLHF algorithms, which is consistent with our findings (e.g.,). (Askell et al., 2021) reports that they didnt observe significant alignment tax when promptingLLM to align with humans. However, we focus on a more standard setting that the LLM is fully fine-tunedfor RLHF.",
  "BRLHF Basics": "Following (Ouyang et al., 2022; Bai et al., 2022; Dong et al., 2023; Touvron et al., 2023; Rafailov et al.,2023), we assume that there exists a ground-truth reward function r(x, a) : X A where Xand A are the spaces of prompt and action. The preference ranking satisfies the Bradley-Terry model(Bradley and Terry, 1952): the probability of a1 A being preferred is",
  "exp(r(x, a1)) + exp(r(x, a2)).(4)": "We denote an LLM by a policy that maps x to a distribution over the response space A. The maingoal of RLHF is to align the staring checkpoint 0 with the human preference so that it achieveshigh reward measured by r, but we may also impose additional constraints to avoid overfitting likerequiring the models to stay close to the 0. In practice, we learn from a preference dataset of the formD = {(x, aw, al)}, where aw is the preferred response. Typically, we will first train a reward model r asthe Maximum Likelihood Estimation (Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023) on thepreference dataset D and then perform reward optimization by different algorithms.Rejection Sampling Finetuning (RSF) is proposed in (Dong et al., 2023; Touvron et al., 2023; Yuanet al., 2023; Gulcehre et al., 2023) with several variants. Essentially, the RSF learns from the best-of-n policy (Nakano et al., 2021), which samples n responses for each prompt query and returns the one withthe highest reward. As suggested by (Dong et al., 2023; Touvron et al., 2023; Gulcehre et al., 2023), weadopt an iterative training set-up for the implementation instead of always sampling the samples from thestarting checkpoint because we find that the iterative training is far more sample-efficient. Specifically, foreach iteration, we first sample a batch of prompts and generate n responses for each prompt from currentmodel. Then, we use the reward model to compute the rewards for each prompt-response pair and foreach prompt, we select the one with the highest reward into a small subset. By this process, we collect abatch of samples from the best-of-n policy that are with high reward. We simply fine-tune the currentmodel on this subset to get the next model and the next iteration begins.PPO is the the classical method for RLHF and has gained its success in aligning Chat-GPT (OpenAI, 2023). In contrast to the implementation in traditional DRL scenario, for alignment of LLMs, following(Ziegler et al., 2019; Wu et al., 2021a; Ouyang et al., 2022; Rafailov et al., 2023; Liu et al., 2023), wemodify the reward optimization as the following KL-regularized form:",
  "B.1Algorithm of Heterogeneous Model Averaging": "Reward Preserving Updating. It is noteworthy that Eqn. (3) represents a RL problem. To implementEqn. (3), RL algorithms such as RSF, PPO, or DPO need to be implemented, involving extra implementa-tion details that depend on the algorithm. To address this issue, we propose a proxy distillation method.Specifically, given a policy after RLHF, we generate a proxy dataset by",
  "(x,a)Dlog[(K)(a|x)].(9)": "5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)Replay (Penalty=0.25)Replay (Penalty=0.5)Replay (Penalty=1.0)Replay (Penalty=2.0)Replay (Penalty=4.0) 5.05.56.06.57.0 HH RLHF Reward 0.538 0.540 0.542 0.544 0.546 0.548 0.550 0.552 Commonsense QA (ACC) MA (RSF)Replay (Penalty=0.25)Replay (Penalty=0.5)Replay (Penalty=1.0)Replay (Penalty=2.0)Replay (Penalty=4.0) 5.05.56.06.57.0 HH RLHF Reward Translation Fr-En (BLEU) MA (RSF)Replay (Penalty=0.25)Replay (Penalty=0.5)Replay (Penalty=1.0)Replay (Penalty=2.0)Replay (Penalty=4.0)",
  "C.1Experience Replay": "In our alignment tax situation, we aim to preserve a wide range of abilities gained during pre-training.It is possible to replay a small subset of pretraining data, which also known as Experience Replay (ER)(Rebuffi et al.; Shin et al., 2017). However, this method is less practical since pre-training datasets ofmost models are often not publicly available. Further more, even if we can access the pre-training data,retaining a subset of the pre-training data entails extra computational costs and implementation intricacies,making it less preferable (Noukhovitch et al., 2023). In this part, we compare ER with MA. Specifically,we include a small proportion of randomly subsampled pre-training data during the RLHF stage. Here,we denote Dpre as the pre-training data distribution, and our objective is to solve the following:",
  "maxExEa(|x)[r(x, a)] + E(x,a)Dpre log (a|x)": "We experiment with different penalty weights such as 0.25, 0.5, 1, 2, and 4. Importantly, we utilizethe data proportion as a proxy for setting the penalty weight. For instance, we do not explicitly apply apenalty of 4 when = 4; instead, we include 4 times the replay data over the RLHF data in a batch. Referto the Appendix D for more details.Results. The results of ER are displayed in . Additionally, we include the performance of modelaveraging for comparison. It is evident that while ER has access to pre-training data, it only demonstratessuperior performance over model averaging in the Reading Comprehension dataset ( - Left), andfalls short of model averaging in the Commonsense QA ( - Middle) and Translation ( -Right) benchmarks.Discussion of ER results. The differing performance of ER compared to model averaging is somewhatsurprising. Despite maintaining extra pre-training data, which is four times larger than the RLHF data(400M token), ER under-performs model averaging in two out of three benchmarks. This may be attributedto the vast size of the pre-training data (1.2T token), such that even when replaying a subset four timeslarger than the RLHF data, it only covers about 0.03% of the pre-training data. Consequently, the datacorresponding to certain abilities may be underrepresented in the replay dataset. With a substantialpre-training dataset and a wide range of abilities to preserve, it becomes challenging to maintain allabilities through replay. 5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (PPO)PPO-KL-0.2PPO-KL-0.1PPO-Lora-KL-0.2PPO-Lora-KL-0.1PPO-Lora-KL-0.05PPO-EarlyStopping",
  "C.2Reward Penalty": "It is a common practice to impose KullbackLeibler (KL) penalty on the RL reward in the PPO. Such apenalty can also regularize the policy to stay closer to the initial policy, which in return can reduce thealignment tax. Following (Ziegler et al., 2019; Wu et al., 2021a; Ouyang et al., 2022; Yuan et al., 2023),we modify the raw reward function with an additional KL penalty (Ziegler et al., 2019).",
  "maxExEa(|x)[r(x, a)] KL(||0),(10)": "where we use KL(||0) to denote Ex[KL((|x)||0(|x))] for short. We compare vanilla modelaveraging methods with the reward penalty by considering different KL penalties in {0.05, 0.1, 0.2}.The results are shown in . We can see that while a larger KL penalty can partially mitigate theforgetting issue, the model averaging is much more effective than the reward penalty in terms of thealignment-forgetting trade-off.",
  "C.3Consistency of different combination ratios among various tasks": "We try three patterns experiment given a base {0.2, 0.3, 0.4} : (a) 1 = 2 = 3 = ; (b)1 = 2 = , 3 = 0.1, and (c) 1 = , 2 = 3 = 0.1. We use (||), (|| 0.1) and(| 0.1| 0.1) to denote these three patterns, respectively. These results confirm that certain ratiocombinations exceed the trade-off curve of vanilla model averaging, as displayed in . Notably,some combination ratios consistently outperform the equal ratio across various benchmarks. This affirmsthe potential to identify consistent combination ratios that demonstrate superior performance across abroad spectrum of benchmarks in terms of alignment-forgetting trade-off. 6.66.76.86.97.0 HH RLHF Reward 14.0 14.5 15.0 15.5 16.0 16.5 Reading Comprehension (F1) MA ( | | ) MA ( | |0.1) MA ( |0.1|0.1) 6.66.76.86.97.0 HH RLHF Reward 0.544 0.546 0.548 0.550 0.552 Commonsense QA (ACC) MA ( | | ) MA ( | |0.1) MA ( |0.1|0.1) 6.66.76.86.97.0 HH RLHF Reward 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 Translation Fr-En (BLEU) MA ( | | ) MA ( | |0.1) MA ( |0.1|0.1)",
  "D.1Rejection Sampling Fine-tuning Implementation": "The rejection sampling fine-tuning (RSF) is proposed in (Dong et al., 2023; Touvron et al., 2023; Yuanet al., 2023; Gulcehre et al., 2023) with several variants. Essentially, RSF earns from the best-of-n policy(Nakano et al., 2021), which samples n responses for each prompt query and returns the one with thehighest reward. In this work, we implement the algorithm with the official code provided in LMFlow9.We adopt most of the hyper-parameters as suggested by (Dong et al., 2023) and focusing on tuning thelearning rate by searching over {1 106, 2 106, 1 105} and 1 105 is taken for our mainexperiments.As suggested by (Dong et al., 2023; Touvron et al., 2023; Gulcehre et al., 2023), we adopt an iterativetraining set-up for the implementation instead of always sampling the samples from the starting checkpointbecause we find that the iterative training is far more sample-efficient. Specifically, for each iteration,we first sample a batch (2048) of prompts and generate n = 32 responses for each prompt from currentmodel. Then, we use the reward model to compute the rewards for each prompt-response pair, and foreach prompt, we select the one with the highest reward into a small subset. Through this process, wecollect 2048 samples from the best-of-32 policy that are with high reward. We simply fine-tune the currentmodel on this subset to get the next model and the next iteration begins.When RSF is combined with other methods for preventing the model from forgetting, we follow(Touvron et al., 2023; Dong et al., 2023) to align the models in a distillation style. Specifically, we runRSF algorithm as described above until the model converges to a rather stable level of reward. Then,we collect the best-of-32 samples along the way of training and fine-tune the model from the startingcheckpoint with the additional methods for mitigating the forgetting issue. In comparison, we note that(Touvron et al., 2023) only uses the largest 70B Llama 2-Chat models to collect best-of-n samples andother smaller models are then fine-tuned on these collected data and (Dong et al., 2023) uses LLaMA-7Bto run RSF and uses the collected data to fine-tune other LLMs.",
  "D.2Implementation of PPO": "The experiments with PPO in this work are conducted using the open-source package TransformerReinforcement Learning (TRL)10. It is known that the PPO is significantly less stable as compared tosupervised learning (Choshen et al., 2019) and sensitive to the hyper-parameter and code-level optimization(Engstrom et al., 2020). To tune PPO to its best performance, we include several empirical enhancementsand we record our tuning process, as well as the successful/unsuccessful attempts in this subsection forinterested readers.First, we follow (Ramamurthy et al., 2022) to warm up by finetuning the model on the preferred samplesof the preference dataset for 1 epoch for a more stable training process. Moreover, in contrast to theimplementation in traditional DRL scenario, for alignment of LLMs, following (Ziegler et al., 2019; Wuet al., 2021a; Ouyang et al., 2022; Rafailov et al., 2023; Liu et al., 2023), we will also modify the rewardoptimization as the following KL-regularized form:",
  "(a|x),": "where > 0 is a hyper-parameter to control the level of KL penalty.However, even though we first finetune the models with the preferred samples and train with anadditional KL penalty, the PPO training can still lead to an unstable reward level and failure. For thefirst issue, with the ultimate hyper-parameter, we will run PPO with three independent seeds and take thebest models. We now focus on the second issue. One notable failure signal of PPO training is that themodels suddenly refuse to answer the question (prompt), or reply with incomplete sentences, which maybe detected by (1) a shorter average response length; (2) the incomplete sentences in randomly displayedsample responses within one iteration; (3) sudden drop in reward value. Once such a drop happens, themodels just collapse and the training fails.Hyper-parameter tuning. To mitigate this issue, we carefully tune the learning rate, KL coefficient,update epoch, batchsize by grid search. We observe that for full training (without LoRA), a learning ratewith 1 106 is most suitable in terms of the trade-off between reward learning and training stability.Update epoch = 2 performs best in our preliminary experiments for parameter tuning. A batchsize that istoo large (2048) or too small (128) leads to unstable training. Therefore, we fix the batchsize as 512 andthe update epoch as 2 to further tune the KL coefficient and learning rate. Ideally, in the mathematicalformulation of KL-constrained RLHF, a smaller KL coefficient should lead to a higher reward value. Inpractice, we observe that for KL coefficient [0.05, 0.3], a smaller KL coefficient leads to a higherultimate reward value of the obtained policy. However, for < 0.05, the model collapses before itachieves the highest reward possible, leading to a even worse model compared to = 0.05. The resultsare observed across more than 20 independent runs. Therefore, in the ablation study of the impact of KLcoefficient for PPO, we choose = 0.05 as the smallest KL coefficient. We mention in passing that dueto the same instability issue, the LoRA training may also achieve better reward because we can optimizethe model well with LoRA, while the full-trained models collapse before it achieve its best performance.Restart trick in critic training. To further understand the reason why the PPO fails, we examineseveral training records provided by wandb. We found that before (or simultaneously) the models collapse,the critic loss increases significantly. After looking at the source code of TRL, we notice that there is ascaling factor of the critic loss of 0.1, which may also suggest that the training processes of the critic andactor are different. Motivated by these observations, we try out different learning rates for the critic: (1) alarger learning rate for the critic; (2) a smaller learning rate for the critic; (3) decay/increase the learningrate of the critic every 10 batch of the training. Unfortunately, we do not see significant improvementin either the training stability or the ultimate reward value. We noticed that the instability from valueestimation (critic training) seems to be a well-studied problem in the DRL literature. For instance, (Leeet al., 2022a) proposes to use a pessimistic (conservative) reward signal, which is obtained by rewardmodel ensemble, which is also recommended in theoretical RLHF studies (Zhu et al., 2023; Xiong et al.,2023). However, this requires to load multiple reward models at the same time, which is infeasible for us due to computational constraint. Motivated by the trick of PaLM (in the pre-trained stage) (Chowdheryet al., 2023), which call back whenever the spikes happen in the loss curve, we simply train the modeltwice. Specifically, we run PPO training first and save the intermediate models for every iteration. Oncethe model collapses, we simply restart from a model 3 iterations before the training fails and re-initiatethe critic model. Then, we skip the actor training for 1 iteration as a warm-up stage of the restarted critic.We observe that though the training still collapses easily after 10-20 iterations of training, we do achieve amuch higher reward value.It is also interesting to design new algorithms to mitigate the value estimation error for a more stablePPO-based training, and we leave it for future study since it is beyond the scope of this work.",
  "D.3Implementation of DPO": "We implement DPO by the open-source package Transformer Reinforcement Learning (TRL). We mainlyuse 0.1 in our experiments but also try out 0.3 and 0.5 since the authors of original paper recommend toset it from 0.1 to 0.5. Then, we mainly tune the learning rate. We use the evaluation loss (which generallyaligns with the evaluation accuracy) on the validation set of reward modeling for the model selection. Weobserve that for learning rate in {1 106, 2 106, 1 105}, 1 106 achieves the lowest evaluationloss so it is adopted in our experiments. We train DPO for up to 3 epochs and evaluate the model every0.5 epoch by the evaluation loss on the validation set. The lowest evaluation loss and highest evaluationaccuracy are achieved at the end of the first epoch so we use the model as the representative model of DPOthough we do observe the validation reward of the model at 0.5 epoch of the training is slightly higher. Wesuspect that this is because the equivalence of reward modeling and policy training are equivalent for DPOonly when the optimization error is zero (see (Rafailov et al., 2023; Azar et al., 2023) for a detailed proof).In practice, since the samples are finite and we may not solve the non-convex optimization by finding itsexact minimizer, the reward of the generator may not align with the accuracy of the discriminator (rewardmodel).",
  "(a) Early Stopping: The whole RSF is conducted for 10 iterations and we choose the model of RSF atnumbers of iterations of 2,4,6,8 as the early stopping checkpoints": "(b) Regularization towards 0 in the weight space: For these kinds of methods. We alternative thetraining loss at the SFT stage in RSF by adding the regularization terms with different penalties.Specifically, we test {0.04, 0.1, 0.4, 0.6, 1} for the L1 penalty and {0.01, 0.04, 0.06, 0.08, 0.1} forL2 penalty. (c) Low-Rank Adaptation (LoRA): We implement two levels of LoRA. The typical version only considersthe low-rank adaptation of MLP blocks and we have tested several ranks for 16-512, while onlyrank 512 gives a reasonable performance on the final alignment result. The other is the low-rankadaptation of both MLP and attention blocks, in this case, rank 16 makes a good performance onalignment. (d) Knowledge distillation: The implementation of this approach is similar to the Regularization method.We add the knowledge distillation term as a regularization term in the SFT stage. The penalty usedhere are {105, 103, 101}. (e) Model Averaging: We simply interpolate the modules of linear layers in the whole model, e.g., Q, K,V projection layers in attention and MLP layers. We will vary the from 0 to 1. The start point ofthe model averaging is the model after instruction following and the end point of that is the modelafter RLHF. For the experience replay (ER) method, we uniformly sample the pre-trained data of Open-LLaMA-3Baccording to the penalty. Specifically, given the alignment data of 400M tokens and a penalty of 2, wewill sample 800M token data from the pre-trained data. And then add data to conduct the SFT loss as apenalty.",
  "where (x) =1": "1+exp(x) is the sigmoid function si can take any real number. For each s1, . . . , sK, wecan easily find the corresponding 1, . . . , K of Eqn. (11) belongs to the . In this way we can optimizeon s1, . . . , sK rather than 1, . . . , K. Moreover, the in Eqn. (11) can serve as a boundary controlparameter, that is, if we set K = 3, = 1, then each i can just take values over [0.2, 0.5]. In practice,we will search the {0, 0.1, . . . , 0.9} to get the best model. To get D, we will use the prompts from the training RLHF dataset to generate the full response withdifferent policy . Then we sample about 2000 pieces generated responses from the set consisting ofthe 5000 samples with the highest rewards. Then we can just take the s1, . . . , sK as the optimizationparameters and just finetuning them on the D. Besides directly optimizing the Eqn. (9), we also test adding regularization terms of 1, . . . , K.Generally we just add weighted L1 loss i wi|i | as the regularization terms. wi is chosen to makethe middle part of the module change not too much. Typically, we only average the weights in the linear layers and the 1, . . . , K works on transformerlayers which contain self-attention and MLP. For the head layer, we just set the average weight as .",
  ": Results of AdaMerging. We optimize AdaMerging on Reading Comprehension and found it can hardlydo well on Common Sense": "4.55.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)Input Part MA (RSF)Middle Part MA (RSF)Output Part MA (RSF) 5.05.56.06.5 HH RLHF Reward 15.0 15.5 16.0 16.5 17.0 17.5 Reading Comprehension (F1) MA (DPO)Input Part MA (DPO)Middle Part MA (DPO)Output Part MA (DPO) 5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (PPO)Input Part MA (PPO)Middle Part MA (PPO)Output Part MA (PPO)",
  "E.3Comparison of RLHF Algorithms": "We compare the alignment-forgetting trade-off of RSF, DPO and PPO in . We observe that RSFis consistently better than DPO. However, we also note that this is not a fair comparison since DPO doesnot directly optimize for the reward. 5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)MA (DPO)MA (PPO) 5.05.56.06.57.0 HH RLHF Reward 0.5375 0.5400 0.5425 0.5450 0.5475 0.5500 0.5525 0.5550 0.5575 Commonsense QA (ACC) MA (RSF)MA (DPO)MA (PPO) 5.05.56.06.57.0 HH RLHF Reward Translation Fr-En (BLEU) MA (RSF)MA (DPO)MA (PPO)",
  "E.4Results of AdaMerging (Yang et al., 2023)": "Previous studies (Yang et al., 2023) have also discussed the idea of dynamically assigning differentweights to different layers when merging models, aiming to maximize performance on a specific task(e.g., Ti). These approaches assume access to the task-specific data Ti. However, considering the natureof alleviating alignment tax, which aims to mitigate forgetting across a extremely wide range of tasks(Tj1...TjK), these methods fail to effectively optimize performance for multiple tasks simultaneously.Specifically, we want to preserve the abilities on a wide range of tasks and it is hard to get the data for allthese tasks. Further more, some ability such as in-context learning does not have a clear correspondingtraining set. So it is less practical to find training set for AdaMerging.Here we demonstrate when we use AdaMerging to optimizes for task A and the training set does notcover task B, AdaMerging can not preserve the ability on task B. Specifically, we provide AdaMergingwith labeled data for Reading Comprehension (i.e., task A) and optimize the 26 layer-wise merging ratiosas (Yang et al., 2023). To have a clear comparison with vanilla model averaging, we try different meanaveraging ratio for AdaMerging among 0.2, 0.4 and 0.6. We also show both the results on task A and B. In contrast, our HMA only require the RLHF data and does not need any data from the tasks which wewant to preserve ability. shows that HMA can alleviate the alignment tax evaluated on a widerange of tasks.",
  "We provide the detailed results of Heterogeneous model averaging on various benchmarks, e.g., ReadingComprehension, Commonsense QA and translation, and different RLHF methods, e.g., RSF, PPO, andDPO": "5.05.56.06.57.0 HH RLHF Reward Reading Comprehension (F1) MA (RSF)HMA (RSF) 5.05.56.06.57.0 HH RLHF Reward 0.538 0.540 0.542 0.544 0.546 0.548 0.550 0.552 0.554 Commonsense QA (ACC) MA (RSF)HMA (RSF) 5.05.56.06.57.0 HH RLHF Reward Translation Fr-En (BLEU) MA (RSF)HMA (RSF) 5.005.255.505.756.006.256.50 HH RLHF Reward 15.0 15.5 16.0 16.5 17.0 17.5 18.0 Reading Comprehension (F1) MA (DPO)HMA (DPO) 5.005.255.505.756.006.256.50 HH RLHF Reward 0.5400 0.5425 0.5450 0.5475 0.5500 0.5525 0.5550 0.5575 Commonsense QA (ACC) MA (DPO)HMA (DPO) 5.005.255.505.756.006.256.50 HH RLHF Reward Translation Fr-En (BLEU) MA (DPO)HMA (DPO)",
  "F.1Re-statement of Formal Settings": "Notation. Consider that the full class space M contains M classless, i.e. y {e1, e2, ..., eM}, whereei denotes the M-dimensional unit vector with ith element equaling 1, e.g., e2 = [0, 1, 0, ..., 0]. a(k)means the kth element of vector a, A(k) means the kth column of matrix A. We use IM to represent aM M identity matrix, e.g., IM = [e1, e2, ..., eM]. We omit the subscript of I when no confusion arises.Following (Lin et al., 2023), suppose we have N weak features {xi}Ni=1 where xi Rd and the whole",
  "feature x RdN is the concatenation of them, i.e., x = Concat{xi}Ni=1= [x1, . . . , xN]. Consider": "that each model f is composed of a featurizer {0, 1}N and a classifier w RdK. first selectsfeature by x. For example, suppose x = [x1, x2, x3] and = , then x = x1 + x2. Then theclassifier w RdK is fit based on the features selected by as w = arg minv E[(v(x), y)]+v22,where is the cross-entropy loss function.We simplified (Lin et al., 2023)s Definition 1 and only consider weak features as following:",
  "2nn + no)(13)": "Estimating (2) corresponding to Case (2).Without loss of generality, we assume the Ya is {1, ..., K}and Yb is {K + 1, ..., 2K}. Denote the feature learnt by (wa, a) and (wb, b) as x1, ..., xn andxnno+1, ..., xn, ...x2nno. Since Aa(favg), Ab(favg) 0, we trivially have (1) Fp((1 p))nby combing Proposition 7 of (Lin et al., 2023).According to the Lemma 5 of (Lin et al., 2023), we have",
  "F.3Discussion on the Effect of Task Similarity on Model Averaging": "We illustrate why model averaging would not lead to much improvement if two tasks are dissimilar, i.e.,|Ya Yb| = 0. Without loss of generality, we assume the Ya is {1, ..., K} and Yb is {K + 1, ..., 2K}.Since w is the minimum norm solution based on , we know that wb(k) = 0 for k = 1, ..., K. From theprevious proof, we know that",
  "(wa(k) + wb(k))x(a + b)|y=ek = wa(k)xa + wb(k)xb|y=ek": "Since wb(k) = 0, the above equation equals wa(k)xa, which is simply the performance of fa.Intuitively, wb(k)xb maps the feature xb into the space spanned by wb. However, since wb is allzero in the dimension 1, ..., K, so wb(k)xb has no impact on the prediction of task a (i.e., among class1, ..., K)."
}