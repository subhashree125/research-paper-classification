{
  "Abstract": "Conversational search requires accurate inter-pretation of user intent from complex multi-turn contexts.This paper presents ChatRe-triever, which inherits the strong generaliza-tion capability of large language models to ro-bustly represent complex conversational ses-sions for dense retrieval. To achieve this, wepropose a simple and effective dual-learningapproach that adapts LLM for retrieval via con-trastive learning while enhancing the complexsession understanding through masked instruc-tion tuning on high-quality conversational in-struction tuning data. Extensive experiments onfive conversational search benchmarks demon-strate that ChatRetriever substantially outper-forms existing conversational dense retrievers,achieving state-of-the-art performance on parwith LLM-based rewriting approaches. Further-more, ChatRetriever exhibits superior robust-ness in handling diverse conversational con-texts.Our work highlights the potential ofadapting LLMs for retrieval with complex in-puts like conversational search sessions andproposes an effective approach to advance thisresearch direction.",
  "Introduction": "Conversational search is rapidly gaining promi-nence and reshaping how users interact with searchengines to foster a more natural information-seeking experience. At the heart of a conversationalsearch system lie two key components: retrievaland generation (Gao et al., 2022; Zhu et al., 2023).The retrieval process is tasked with sourcing rel-evant passages, which the generation componentthen uses to craft the final response. Conversa-tional retrieval plays a crucial role in ensuring theaccuracy and reliability of the system responses byproviding relevant passages (Liu et al., 2023).Compared to traditional ad-hoc web search, con-versational retrieval requires an accurate under-",
  ": Illustration of adapting LLM for query rewrit-ing and conversational dense retrieval": "standing of the users real search intent withinlonger, noisier, and more complex conversationalcontexts. A shortcut approach is to transformthe conversational session into a standalone queryrewrite, enabling the usage of ad-hoc retrievers forconversational retrieval. However, the addition-ally introduced rewriting process is hard to directlyoptimize towards better retrieval, and it also in-troduces extra search latency from the rewritingstep (Yu et al., 2021). In contrast, the end-to-endconversational dense retrieval appears to be morepromising, as it directly encodes the original con-versational search session and passages into denserepresentations without additional input processingand can enjoy the efficiency benefit from advancedapproximate nearest neighbor search algorithms(e.g. Faiss (Johnson et al., 2021)).Nonetheless, the effectiveness of existing con-versational dense retrievers largely trails behindstate-of-the-art conversational query rewriting ap-proaches, which leverage large language models(LLMs). Owing to their strong text understand-ing and generation capabilities, LLM-based rewrit-ers (Mao et al., 2023b; Ye et al., 2023) have demon-strated exceptional effectiveness, even outperform-ing human rewrites. Given that LLMs are inher-ently generative models, they can naturally serve asa high-quality conversational rewriter just throughprompting (). The question that remains is:whether the potent capabilities of LLMs can be har-nessed to substantially enhance the performanceof conversational dense retrievers.Several studies have explored tuning LLMs for dense retrieval but with a primary focus on ad-hocsearch (Asai et al., 2023; Su et al., 2023; Ma et al.,2023; Wang et al., 2024; Muennighoff et al., 2024).While in conversational search, the multi-turn ses-sions exhibit greater diversity, complex expres-sions, and longer-tail intents compared to single-turn ad-hoc queries, posing severe challenges to thesession representation learning. Additionally, theseapproaches often rely on manually designed andfixed instruction templates, which can considerablylimit their ability to generalize and handle intricateconversational scenarios.In this work, we propose adapting LLM itselfto serve as a powerful conversational dense re-triever. To achieve this, we select high-qualityconversational instruction tuning data (Ding et al.,2023) as our training data and propose a simpledual-learning approach called Contrastive Session-Masked Instruction Tuning (CSIT) for the modeltraining. Specifically, we adopt the classical con-trastive ranking loss function (Izacard et al., 2022)to fine-tune LLM from a generative model to aretrieval (or representational) model on the multi-turn instruction (i.e., session)-response pairs, usingthe special tokens at the end of the input text torepresent the entire text. Meanwhile, we mix thebasic contrastive learning with a session-maskedinstruction tuning objective, where we mask all to-kens except the special tokens of the session whencomputing the language modeling loss of the re-sponse tokens. The incorporation of this generativeinstruction tuning loss forces a strong enhancementin the learning of the complex session representa-tion since the response tokens have to be generatedsolely based on the special tokens representing thesession. Furthermore, it also helps retain the stronggeneralization capability of LLM for retrieval.Our resulting model, which we call ChatRe-triever, can inherit the strong generalization capa-bility of LLM to robustly represent complex conver-sational sessions for dense retrieval. We conductedextensive experiments across five conversationalsearch benchmarks, where ChatRetriever substan-tially outperforms existing conversational denseretrievers. Notably, it achieves absolute NDCG@3improvements of 6.8% and 12.2% on CAsT-20and CAsT-21, respectively, matching the perfor-mance of the leading LLM-based conversationalquery rewriting methods. Beyond standard evalu-ations using fixed conversational trajectories, wealso developed two robustness evaluation methodsto assess the resilience of conversational retrieval approaches by altering the historical context. Cha-tRetriever demonstrates markedly more stable per-formance in our robustness test, showcasing its su-perior robustness in comparison to baselines whenfaced with varied contexts.Our contributions can be summarized as:(1) We introduce ChatRetriever, the first LLM-adapted conversational dense retriever, whichsubstantially outperforms existing conversationaldense retrievers and achieves performance compa-rable to LLM-based rewriting approaches.(2) We propose Contrastive Session-Masked In-struction Tuning for such a retrieval-oriented adap-tion for LLM, which can help achieve better com-plex session representation and generalization.(3) We design two robustness evaluation meth-ods for conversational retrieval by systematicallyvarying the conversation contexts. Results high-light ChatRetrievers superior generalization ca-pability in handling diverse conversational searchscenarios.",
  "Related Work": "Conversational search has seen the developmentof two primary approaches: conversational queryrewriting (CQR) and conversational dense retrieval(CDR). The former approach transforms theconversational search problem into a traditionalad-hoc search problem by reformulating theconversational context into a standalone query.Techniques in this area range from selectinguseful tokens from the context (Voskarides et al.,2020; Lin et al., 2021b) to training generativerewriters based on session-rewrite pairs (Yu et al.,2020; Wu et al., 2022; Mao et al., 2023a; Moet al., 2023a). Inspired by the strong languagegeneration capability of LLMs, some studies (Maoet al., 2023b; Ye et al., 2023; Yoon et al., 2024)propose to leverage LLMs as query rewriters andachieve amazing performance.Conversationaldense retrieval (CDR), on the other hand, directlyencodes the entire conversational session forend-to-end dense retrieval (Yu et al., 2021). Effortsin this direction have focused on improving sessionrepresentation through various perspectives suchas context denoising (Mao et al., 2022a; Mo et al.,2023b; Mao et al., 2023c), data augmentation usingother corpus and LLMs (Lin et al., 2021a; Maoet al., 2022b; Dai et al., 2022; Jin et al., 2023; Chenet al., 2024; Mo et al., 2024c,a), and hard nega-tive mining (Kim and Kim, 2022; Mo et al., 2024b). LLM-based and instruction-aware retrieval. Ex-isting research has demonstrated that similar tothe scaling laws (Kaplan et al., 2020) observed inLLMs, increasing the scale of models, data, andcomputing resources can also enhance the perfor-mance of retrieval models (Ni et al., 2022). Toincorporate the ability to follow instructions intoretrievers, some studies (Su et al., 2023; Asai et al.,2023) propose the creation of fixed instruction tem-plates for various retrieval tasks, and use theseinstruction-enhanced datasets to train the retriev-ers. Moreover, there have been efforts to adaptLLMs for retrieval purposes by training on im-proved search data (Ma et al., 2023; Wang et al.,2024) or developing new search-oriented trainingobjectives (Li et al., 2023). However, these ap-proaches often rely on manually designed and fixedinstruction templates, which can limit the general-ization capabilities of the retrievers across diverseinstructions. Additionally, they are typically de-signed for single-turn ad-hoc search, lacking thecapability to comprehend long and complex searchsessions. In contrast to LLMs, which can smoothlyunderstand a wide range of complex user inputs,existing LLM-based retrievers still exhibit a largegap in their generalization capabilities, particularlyin the context of conversational search.",
  "Methodology": "We describe our simple and effective dual-learningapproach, Contrastive Session-Masked InstructionTuning (CSIT), which is designed to adapt LLMto a generalized and robust conversational denseretriever. An overview is shown in . Contrastive instruction tuning. Recent workshave demonstrated the effectiveness of simply us-ing the contrastive ranking loss to adapt LLM toa retriever (Asai et al., 2023; Su et al., 2023; Maet al., 2023; Wang et al., 2024; Muennighoff et al.,2024). However, their generalization capability canbe limited as they overfit the narrow distributionof ad-hoc queries and fixed instruction templatesthey were trained on. We fine-tune LLM on diverseconversational instruction tuning data for more gen-eral conversational retrieval adaption. Specifically,given a training sample {(x, y+)} from conversa-tional instruction tuning dataset, where x comprisesall historical turns and the current instruction (wecall x a session) and y is the response, we fine-tune",
  "(x, y+) + yD (x, y),(1)": "where (x, y) = exp((E(x) E(y))/), E() isthe shared text encoder of the retriever. D is anegative response collection for x. is a hyperpa-rameter temperature.To encode text with LLM, we append t specialtokens ([EMB1], ..., [EMBt]) to the end ofthe input text and utilize the representation ofthe last token ([EMBt]) as the comprehensiverepresentation of the entire text. This approachis analogous to the text-level chain-of-thought(CoT) (Wei et al., 2020) for LLMs. We hypothesizethat these t consecutive special tokens act as arepresentational chain-of-thought, expanding andguiding the learning space to achieve a moreeffective representation. Session-masked instruction tuning. To enhancethe generalized encoding of complex search ses-sions, we integrate a session-masked instructiontuning objective with the fundamental contrastivelearning. Given a training sample (x, y+), we con-catenate the instruction and the response to formone input sequence s:",
  "..., y+M, [EMB1], ..., [EMBt]],(2)": "where xi and y+i represent the i-th token of thesession and the response, respectively. N and Mdenote the total number of tokens in the sessionand the response, respectively. We then input thissequence into the LLM to obtain the token rep-resentations. Specifically, the representations forthe (N + t) session tokens are obtained through astandard auto-regressive process. However, for thesubsequent (M+t) response token representations,we mask the N session token representations andallow only the attention of t special session tokensand their preceding response tokens. We achieveit by applying a customized attention mask matrixillustrated on the right side of . Corre-spondingly, the loss function of the session-maskedinstruction tuning is defined as:",
  "ChatRetriever": ": Overview of CSIT. We fine-tune LLM to be ChatRetriever using dual learning objectives. We use the lastspecial token (i.e., <EMB_3>) to represent the input text, which can be session or response. In the session-maskedattention matrix, the blue squares denote the session or the response tokens while the green squares denote theirspecial tokens. By masking the session text and forcing cor-rect generation for the response tokens, we builda closer connection between the session represen-tation and the response token representations. Themodel has to perform a more nuanced understand-ing of the complex session and accurately encodethem into the t session special tokens.We combine the contrastive instruction tuningand the session-masked instruction tuning to formthe final training objective of ChatRetriever:",
  "where is a hyperparameter to balance the twolosses": "Discussion.Our dual-learning approach CSITtakes inspiration from several notable works inLLM-based retrieval and input compression suchas RepLLaMA (Ma et al., 2023), E5mistral-7b (Wanget al., 2024), GRIT (Muennighoff et al., 2024), Gist-ing (Mu et al., 2023), and AutoCompressor (Cheva-lier et al., 2023). However, CSIT distinguishesfrom them in the following key aspects: (1) Re-pLLaMA and E5mistral-7b primarily focus on con-trastive learning using (synthetic) ad-hoc searchdata with pre-defined instruction templates, whichis hard to generalize to complex conversationalsearch scenarios. (2) GRIT aims to build a uni-fied model for both retrieval and generation, in-corporating vanilla instruction tuning and usingdifferent training data for its contrastive learningand instruction tuning. (3) The mechanism of oursession-masked instruction tuning shares similari-ties with Gisting and AutoCompressor, but they arefor a completely different target: improving long-context language modeling, not retrieval. In con-trast, CSIT stands out from these works by specifi-cally addressing the challenges of adapting LLMgeneralized to complex conversational retrieval.",
  "Setup": "Training data. We fine-tune LLM to be ChatRe-triever on high-quality conversational instructiontuning datasets. We select training samples thatare informative, diverse, and exhibit information-seeking intents. Our final training data comprisestwo sources: (1) The Question About the Worldsubset of UltraChat (Ding et al., 2023) and (2)MSMARCO (Nguyen et al., 2016) passage rankingdataset. Ultrachat is a multi-turn instruction tuningdataset while MSMARCO can be deemed asa single-turn search-oriented instruction tuningdataset by treating the query as the instruction andthe positive passage as the response. We find thatincorporating MSMARCO is important to improvethe basic (ad-hoc) retrieval performance. Evaluation data and metrics.We conductevaluations on five public conversational searchbenchmarks, including QReCC (Anantha et al.,2021),TopiOCQA(Adlakhaetal.,2022),CAsT-19 (Dalton et al., 2020), CAsT-20 (Daltonet al., 2021), and CAsT-21 (Dalton et al., 2022).The retrieval corpus sizes of these five datasetsare in the tens of millions.Among them, thelarge-scale QReCC and TopiOCQA have trainingsets, while the other three CAsT datasets are smalldatasets that only have test sets. We mainly reportNDCG@3 to evaluate the retrieval performance,as conversational search is more concerned withthe top results (Dalton et al., 2021). Baselines.We compare ChatRetriever againstthe following three types of retrieval baselines.The first is CQR baselines, including T5QR (Linet al., 2020), ConvGQR (Mo et al., 2023a), andLLM4CS (Mao et al., 2023b).The original",
  "ModelBase Model#Model ParameterQReCCTopiOCQACAsT-19CAsT-20CAsT-21": "Conversational Query RewritingT5QRT5-base (Raffel et al., 2020)250M31.822.241.729.933.0ConvGQRT5-base (Raffel et al., 2020)250M41.024.343.433.127.3LLM4CS (REW)ChatGPT-3.5 (OpenAI)Unknown--43.135.740.4LLM4CS (RAR)ChatGPT-3.5 (OpenAI)Unknown--45.339.544.9LLM4CSChatGPT-3.5 (OpenAI)Unknown--51.545.549.2 LLM-based RetrievalLLM EmbedderBGE (Xiao et al., 2023)110M50.522.436.615.331.2INSTRCUTORGTR-XL (Ni et al., 2022)1.5B42.312.326.817.332.4RepLLaMALLaMA-2 (Touvron et al., 2023)7B31.815.031.618.332.7E5mistral-7bMistral (Jiang et al., 2023)7B32.916.931.315.432.4GRITMistral (Jiang et al., 2023)7B33.517.330.919.333.6 Conversational Dense RetrievalConv-ANCEANCE (Xiong et al., 2021)110M45.620.534.127.534.2ConvDRANCE (Xiong et al., 2021)110M35.726.443.932.437.4DialogInpainterT5-Large (Raffel et al., 2020)770M--47.033.2-LeCoRESPLADE (Formal et al., 2022)110M48.531.442.229.032.3",
  "ChatRetrieverQwen (Bai et al., 2023)7B52.540.152.140.049.6": ": Results of the normal evaluation on five conversational search benchmarks. The base models of CQRmethods are their rewriters and the model parameters are also counted as the rewriters parameters. denotessignificant differences to baselines (p < 0.05). The best results are bold and the second-best results are underlined. LLM4CS has three prompting methods: REW,RAR, and RTR, and it requires multiple roundsof generation, which is time-consuming.Forefficiency consideration, we additionally comparewith its two single-generation variants based onRAR and REW; The second is CDR baselines,including ConvDR (Yu et al., 2021), Conv-ANCE (Mao et al., 2023c), DialogInpainter (Daiet al., 2022), and LeCoRE (Mao et al., 2023c);The third is the LLM-based retriever baselines,including INSTRUCTOR (Su et al., 2023), LLMEmbedder (Zhang et al., 2023), RepLLaMA (Maet al., 2023), E5mistral-7b (Wang et al., 2024), andGRIT (Muennighoff et al., 2024). More baselinedetails on in Appendix A. Implementations.We initialize ChatRetrieverwith Qwen-7B-Chat (Bai et al., 2023) and trainit on eight 40G A100 GPUs using LoRA (Hu et al.,2022) with a maximum input sequence length of1024. The training process involves 2500 steps witha learning rate of 1e-4, a gradient accumulation of4 steps, a batch size of 64, and 4 hard negativesper sample. For consistency, we adopt the chatmlinput format of Qwen-Chat to form the input ofChatRetriever. We add three special tokens (i.e.,<|extra_1|>, <|extra_2|>, and <|extra_3|>) at theend of the instructions and responses. We tested values ranging from 0.1 to 1 and ultimately set",
  "Normal Evaluation": "The retrieval performance comparisons on thefive datasets are reported in .Our pro-posed ChatRetriever outperforms all the baselinemethods across these datasets. Existing conversa-tional dense retrievers are constrained by limitedmodel capacity and data quality, resulting in sub-optimal performance for conversational retrievaltasks. Prior to ChatRetriever, there was a consid-erable performance gap between existing conver-sational dense retrieval methods and the state-of-the-art LLM-based conversational query rewriter(i.e., LLM4CS). Specifically, the absolute gaps be-tween the best existing CDR model and LLM4CSwere 1.6%, 12.2%, and 11.8% on the three CAsTdatasets, respectively. However, ChatRetriever canachieve comparable or even superior performanceto LLM4CS, highlighting the high potential of end-to-end conversational dense retrieval compared tothe two-stage approach of conversational queryrewriting methods. If we force LLM4CS to gener-ate a single output (RAR) or only consider queryrewriting (REW) for efficiency, the advantages of",
  ": Results of the robust evaluation. Diff. represents the absolute difference compared to the results in and SD represents the standard deviation, where a smaller value means more stable": "ChatRetriever become even more pronounced, withover 4% absolute gains. We also observe that ex-isting LLM-based retrievers do not perform wellon conversational retrieval tasks. This can be at-tributed to the fact that they are fine-tuned solely ontemplated instructions, which fails to fully leveragethe generalization capabilities of LLMs to handlecomplex and diverse conversational scenarios.",
  "Robustness Evaluation": "Existing evaluations for conversational retrieval aremainly conducted on fixed conversation trajecto-ries. In this section, we evaluate the robustness ofconversational retrievers in different contexts. Ourprinciple is modifying the context but fixing thecurrent query (i.e., search intents) for each turn sothat the original relevance labels can be re-used.Specifically, we propose the following two typesof context modification:(1) Partial response modification: We do not usethe provided responses in the evaluation dataset.Instead, for each turn, we input the current query,the context, and the top-3 passages retrieved by theconversational retriever, and prompt LLM to gen-erate the response. The simulated online nature ofgenerating responses turn-by-turn better matcheshow conversational retrieval systems are used inpractice. However, a problem with this online eval-uation manner is that the query of the next turn inthe original dataset may become unreasonable aftermodifying its last response (Li et al., 2022). Wepropose a simple heuristic method to tackle thisproblem with LLM. Specifically, we prompt LLMto judge whether the current query is reasonablegiven the context. If not, we replace the currentquery with its human rewrite to make it stand on itsown without needing external context. Otherwise,we can use the original query. The prompts can befound in Appendix B.(2) Full context modification: For each turn, wesupply the original query and its human-modified version to the LLM, prompting it to generate newcontexts (See Appendix C). We finally got fivedifferent contexts for each turn.We evaluate conversational retrievers based ondifferent contexts generated by these two modifi-cation methods using ChatGPT 3.5. For the par-tial response modification setting, we report the re-trieval performances and their absolute differences(Diff.) compared to the original counterpart resultsreported in . For the full context modifica-tion setting, we report the Mean performance ofdifferent runs and their standard deviation (SD).The robust evaluation results are shown in .For the partial response modification setting, itshows that the performance changes of ChatRe-triever are the smallest. By referring to , wealso observe a general degradation in retrieval per-formance compared to the original context. Thisdegradation may stem from the retrieved passagesbeing inaccurate, consequently leading to inaccu-rate responses, and then affecting the retrieval per-formance of the subsequent turns.For the full context modification setting, the ro-bustness of ChatRetriever is further highlighted byits small average standard deviation of 1.7, whichis lower compared to the 3.0 and 2.1 standard de-viations observed for ConvDR and LeCoRE, re-spectively. These results demonstrate the strongrobustness of ChatRetriever to different conversa-tional search contexts. In contrast, the LLM4CS,which utilizes ChatGPT for query rewriting, showsan even lower standard deviation of 1.3, demon-strating the superior robustness of ChatGPT forconversational query rewriting.",
  ": Results of ablation studies": "tion tuning with vanilla instruction tuning. shows the ablation results. We find thateither removing the representational CoT or remov-ing or replacing session-masked instruction tun-ing can lead to performance degradation. By con-trast, the session-masked instruction tuning, whichachieves 6.6% relative performance gains acrossthe three CAsT datasets on average, is shown tobe more effective than representational CoT, whichachieves 3.4% relative performance gains on aver-age. The results suggest that our two techniqueshave positive effects in helping adapt LLMs forconversational retrieval. We also studied the influ-ence of the number of special CoT tokens, whichcan be found in Appendix 5.",
  "Influence of LLMs": "shows the comparisons between differentsettings about the backbone LLM of ChatRetriever.(1) Base vs. Chat. Our results indicate that theChat model outperforms the Base model, whichaligns with our expectations. We hypothesize thatthe ability to follow instructions well is indicativeof strong generalization capabilities, which are cru-cial for complex conversational search tasks. There-fore, the Chat model, having been fine-tuned forconversational instructions, provides a more appro-priate foundation for this task.(2) Different LLMs.We find that differentLLMs have similar performance under our train-ing recipe. The relatively worst variation based onLLaMA-2 still largely outperforms existing con-versational dense retrieval baselines on the morecomplex CAsT-20 and CAsT-21 datasets, and also outperforms smaller ChatRetrievers.(3) LoRA vs. full parameter tuning. Due toconstraints in computing resources, our investiga-tion into training modes (i.e., LoRA vs. full param-eter tuning) was limited to the 1.8B scale model.Our findings indicate that employing LoRA train-ing yields inferior performance compared to fullparameter tuning. However, this may be attributedto the LoRA parameter capacity being insufficientfor the 1.8B model.",
  "Influence of Training Data": "Fine-tuning on different data sources. presents the performance of ChatRetriever whentrained solely on UltraChat, solely on MSMARCO,and on a combination of QReCC+MSMARCO(i.e., replacing UltraChat with the QReCCstraining set). The model performance is evaluatedusing both session inputs and human rewrite inputs(i.e., converted to ad-hoc search). We find thattraining exclusively on UltraChat leads to a declinein performance for both input types, with a morepronounced degradation observed for the rewriteinput. Conversely, training solely on MSMARCOyields comparable results for the rewrite input butconsiderably worse performance for the sessioninput.These results suggest that MSMARCOeffectively enhances the ad-hoc retrieval capabil-ities of LLMs, possibly due to its well-curatedhard negatives. However, ad-hoc search data fromMSMARCO alone is insufficient for transferringthe generalization capability of LLMs to themore complex context of conversational search.The traditional conversational QA data (i.e.,QReCC) is also not highly effective for LLMs inlearning a diverse range of complex conversationalpatterns.To optimize LLM to be a universalconversational retriever, we recommend combininggeneral conversational instruction tuning data (e.g.,UltraChat) with ad-hoc search-oriented instructiontuning data (e.g., MSMARCO).",
  ": Comparisons of using different data sourcescombinations for training. U, M, and Q represent Ultra-Chat, MSMARCO, and QReCC, respectively": "Continually fine-tuning baselines on the sametraining data of ChatRetriever.In ,we follow the original training settings of thebaselines. Here, we further fine-tune baselineson the training data of ChatRetriever. Results areshown in and we find: (1) GRIT, a unifiedretrieval and generation model based on LLM,showed substantial performance improvementafter fine-tuning on conversational instructiontuning data. Its performance approached that ofChatRetriever without session-masked instructiontuning, although it still lagged behind the final Cha-tRetriever. (2) The performance of Conv-ANCE,ConvDR, and LeCoRE did not show noticeableimprovements and even experienced declines inQReCC and TopiOCQA. This may be because thatthe newly introduced training data disrupted theiroriginal in-domain training-test settings, as theywere initially trained on the in-domain training setsof QReCC and TopiOCQA. This also highlights",
  "the robust generalization of ChatRetriever, which,when trained only on general conversationalinstruction tuning data, can effectively adapt tovarious conversational search test sets": "Data volume. shows the performance ofChatRetriever across various training steps. It is ob-served that the performance attains a relatively highlevel at 500 steps and subsequently experiencesmarginal improvements as the number of trainingsteps increases. The performance stabilizes uponreaching 2500 steps. Furthermore, the trends forinputs with sessions and human rewrites are similar.These findings suggest that, under our framework,adapting LLMs to function effectively as conversa-tional retrievers may require only a small amountof high-quality data.",
  "Influence of Number of Special Tokens": "In , we present the performance of ChatRe-triever when varying the number of special tokensused for text representation. Our findings suggestthat the inclusion of additional special tokens gener-ally enhances retrieval performance. This improve-ment may be attributed to the fact that a sequenceof consecutive special tokens can serve as a formof representational-level CoT, effectively expand-ing the learning space. However, we observe thatperformance plateaus when the number of specialtokens exceeds three. Consequently, we finally ap-pend three special tokens in our implementation. Number of Special CoT Tokens NDCG@3 49.9 51.5 52.151.9 52.352.0 CAsT-19 Number of Special CoT Tokens NDCG@3 38.5 39.4 40.040.139.839.9 CAsT-20 Number of Special CoT Tokens NDCG@3 47.5 49.1 49.649.449.449.5 CAsT-21",
  "Conclusion": "In this paper, we introduce ChatRetriever, a largeconversational retrieval model adapted from LLM.We propose a novel contrastive session-masked in-struction tuning approach for this adaptation andfine-tune LLM on high-quality conversational in-struction tuning data. Experimental results on fiveconversational retrieval datasets demonstrate thesuperior performance and robustness of ChatRe-triever. Looking ahead, we aim to further exploreand expand the generalization capabilities of Cha-tRetriever in a broader range of complex IR sce-narios beyond conversational search, such as legalcase retrieval, product search, and other instruction-followed search tasks. We envision ChatRetrieverto be as versatile as LLMs, capable of acceptingand understanding any conversational inputs andretrieving useful information for those inputs.",
  "Limitations": "Efficiency.As indicated in , ChatRe-triever is a 7B model which is much larger thanexisting CDR models. Our preliminary findings(.5) suggest that the large model size isa crucial factor for ChatRetrievers exceptionalperformance. However, this also raises efficiencyconcerns. With an embedding dimension of 4096,ChatRetriever incurs higher time and storage costsfor indexing and retrieval. Nevertheless, ChatRe-trievers enhanced retrieval accuracy potentiallyreduces the need for extensive passage re-ranking,which could, in real-world applications, offset theinitial higher costs by ultimately reducing the totaltime spent on ranking. Hard Negatives. Unlike typical search datasetsthat provide a large retrieval corpus, the conver-sational instruction tuning dataset we used (i.e.,UltraChat) consists of only multi-turn instructions(i.e., sessions) and responses. In this work, we simply chose the CAsT-21 corpus for the hardnegative mining of UltraChat (see Appendix A.3).However, as existing studies have shown, hardnegatives are crucial for improving retrievalperformance (Zhan et al., 2021; Zhou et al.,2022).Therefore, a better strategy for mininghard negatives tailored to instruction tuning datais desirable. We plan to explore using LLMs togenerate hard negatives for instructions. Generalizability.ChatRetriever has not yetachieved the same level of generalization as LLMs,particularly in following complex retrieval instruc-tions, addressing very detailed information needs,or performing in-context learning across variousspecific domains. It is worth noting that existinginstruction-aware retrievers (Su et al., 2023; Zhanget al., 2023; Muennighoff et al., 2024) also havelimitations in perceiving complex (multi-turn)instructions that largely fall short of the generalityof LLMs, as highlighted in this work ()and also in recent studies (Oh et al., 2024; Welleret al., 2024). As stated in our conclusion, we arecommitted to further advancing ChatRetrieversgeneralization capabilities to match those of LLMs.",
  "Acknowledgement": "This work was supported by the Beijing Mu-nicipal Science and Technology Project No.Z231100010323009, National Natural ScienceFoundation of China No.62272467, the fundfor building world-class universities (disciplines)of Renmin University of China, Public Comput-ing Cloud, Renmin University of Chin, and theOutstanding Innovative Talents Cultivation FundedPrograms 2024 of Renmin University of China.The work was partially done at the EngineeringResearch Center of Next-Generation IntelligentSearch and Recommendation, MOE. Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-man, Harm de Vries, and Siva Reddy. 2022. Topi-ocqa: Open-domain conversational question answer-ing with topic switching. Transactions of the Associ-ation for Computational Linguistics, 10:468483. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,Shayne Longpre, Stephen Pulman, and SrinivasChappidi. 2021.Open-domain question answer-ing goes conversational via question rewriting. InNAACL-HLT, pages 520534. Association for Com-putational Linguistics. Akari Asai, Timo Schick, Patrick S. H. Lewis, XilunChen, Gautier Izacard, Sebastian Riedel, HannanehHajishirzi, and Wen-tau Yih. 2023. Task-aware re-trieval with instructions. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 36503675.Association for Computational Linguistics. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609.",
  "Haonan Chen, Zhicheng Dou, Kelong Mao, JiongnanLiu, and Ziliang Zhao. 2024. Generalizing conversa-tional dense retrieval via llm-cognition data augmen-tation. arXiv preprint arXiv:2402.07092": "Alexis Chevalier, Alexander Wettig, Anirudh Ajith, andDanqi Chen. 2023. Adapting language models tocompress contexts. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 38293846. Association for Compu-tational Linguistics. Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao,Aida Amini, Qazi Mamunur Rashid, Mike Green,and Kelvin Guu. 2022. Dialog inpainting: Turningdocuments into dialogs. In International Conferenceon Machine Learning, pages 45584586. PMLR.",
  "Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2022.Trec cast 2021: The conversational assistance trackoverview. In In Proceedings of TREC": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,Shengding Hu, Zhiyuan Liu, Maosong Sun, andBowen Zhou. 2023. Enhancing chat language modelsby scaling high-quality instructional conversations.In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 30293051. Association for Computational Linguistics. Thibault Formal, Carlos Lassance, Benjamin Pi-wowarski, and Stphane Clinchant. 2022. From dis-tillation to hard negative sampling: Making sparseneural IR models more effective. In SIGIR, pages23532359. ACM.",
  "Jianfeng Gao, Chenyan Xiong, Paul Bennett, andNick Craswell. 2022.Neural approaches to con-versational information retrieval.arXiv preprintarXiv:2201.05176": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Hamish Ivison, Yizhong Wang, Valentina Pyatkin,Nathan Lambert, Matthew E. Peters, Pradeep Dasigi,Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy,and Hannaneh Hajishirzi. 2023. Camels in a chang-ing climate: Enhancing LM adaptation with tulu 2.CoRR, abs/2311.10702. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2022. Unsupervised dense in-formation retrieval with contrastive learning. Trans.Mach. Learn. Res., 2022. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, andJun Zhao. 2023. Instructor: Instructing unsupervisedconversational dense retrieval with large languagemodels. In Findings of the Association for Compu-tational Linguistics: EMNLP 2023, Singapore, De-cember 6-10, 2023, pages 66496675. Associationfor Computational Linguistics.",
  "Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao.2023. Making large language models A better foun-dation for dense retrieval. CoRR, abs/2312.15503": "Huihan Li, Tianyu Gao, Manan Goenka, and DanqiChen. 2022. Ditch the gold standard: Re-evaluatingconversational question answering. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2022, Dublin, Ireland, May 22-27, 2022, pages80748085. Association for Computational Linguis-tics. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021a. Contextualized query embeddings for con-versational search. In Proceedings of the 2021 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP). Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.2020.Conversational question reformulation viasequence-to-sequence architectures and pretrainedlanguage models. arXiv preprint arXiv:2004.01909. Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.2021b. Multi-stage conversational passage retrieval:An approach to fusing term importance estimationand neural query rewriting. ACM Transactions onInformation Systems (TOIS), 39(4):129.",
  "Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, andJimmy Lin. 2023. Fine-tuning llama for multi-stagetext retrieval. CoRR, abs/2310.08319": "Kelong Mao, Zhicheng Dou, Bang Liu, Hongjin Qian,Fengran Mo, Xiangli Wu, Xiaohua Cheng, and ZhaoCao. 2023a. Search-oriented conversational queryediting. In ACL (Findings), volume ACL 2023 ofFindings of ACL. Association for Computational Lin-guistics. Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou,Haonan Chen, and Hongjin Qian. 2023b. Large lan-guage models know your contextual search intent: Aprompting framework for conversational search. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 12111225. Association for Computa-tional Linguistics. Kelong Mao, Zhicheng Dou, and Hongjin Qian. 2022a.Curriculum contrastive context denoising for few-shot conversational dense retrieval. In Proceedingsof the 45th International ACM SIGIR conference onresearch and development in Information Retrieval(SIGIR). Kelong Mao, Zhicheng Dou, Hongjin Qian, FengranMo, Xiaohua Cheng, and Zhao Cao. 2022b. Con-vtrans: Transforming web search sessions for con-versational dense retrieval. In Proceedings of the2022 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP). Kelong Mao, Hongjin Qian, Fengran Mo, ZhichengDou, Bang Liu, Xiaohua Cheng, and Zhao Cao.2023c. Learning denoised and interpretable sessionrepresentation for conversational search. In Proceed-ings of the ACM Web Conference, pages 31933202. Fengran Mo, Abbas Ghaddar, Kelong Mao, Mehdi Reza-gholizadeh, Boxing Chen, Qun Liu, and Jian-Yun Nie.2024a. CHIQ: contextual history enhancement forimproving query rewriting in conversational search.CoRR, abs/2406.05013. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu,Kaiyu Huang, and Jian-Yun Nie. 2023a. ConvGQR:generative query reformulation for conversationalsearch. In ACL, volume ACL 2023. Association forComputational Linguistics. Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao,Yutao Zhu, Peng Li, and Yang Liu. 2023b. Learningto relate to previous turns in conversational search.In 29th ACM SIGKDD Conference On KnowledgeDiscover and Data Mining (SIGKDD).",
  "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016. Ms marco: A human generated machine read-ing comprehension dataset. In CoCo@ NIPS": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernndez brego, Ji Ma, Vincent Y. Zhao,Yi Luan, Keith B. Hall, Ming-Wei Chang, and YinfeiYang. 2022. Large dual encoders are generalizableretrievers. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2022, Abu Dhabi, United Arab Emirates, De-cember 7-11, 2022, pages 98449855. Associationfor Computational Linguistics. Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin,Hansol Jang, Changwook Jun, and Minjoon Seo.2024. Instructir: A benchmark for instruction follow-ing of information retrieval models. arXiv preprintarXiv:2402.14334.",
  "OpenAI": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. J. Mach. Learn. Res., 21:140:1140:67. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.Smith, Luke Zettlemoyer, and Tao Yu. 2023. Oneembedder, any task: Instruction-finetuned text em-beddings. In Findings of the Association for Com-putational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pages 11021121. Association forComputational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Nikos Voskarides, Dan Li, Pengjie Ren, EvangelosKanoulas, and Maarten de Rijke. 2020. Query reso-lution for conversational search with limited supervi-sion. In Proceedings of the 43rd International ACMSIGIR conference on research and development inInformation Retrieval (SIGIR), pages 921930.",
  "text embeddings with large language models. CoRR,abs/2401.00368": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2020.Chain of thought prompting elicits reasoning in largelanguage models. Advances in neural informationprocessing systems. Orion Weller, Benjamin Chang, Sean MacAvaney, KyleLo, Arman Cohan, Benjamin Van Durme, DawnLawrie, and Luca Soldaini. 2024. Followir: Evaluat-ing and teaching information retrieval models to fol-low instructions. arXiv preprint arXiv:2403.15246.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighof. 2023.C-pack: Packaged resourcesto advance general chinese embedding.CoRR,abs/2309.07597": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In 9th International Conference on LearningRepresentations, ICLR 2021, Virtual Event, Austria,May 3-7, 2021. Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yil-maz. 2023. Enhancing conversational search: Largelanguage model-aided informative query rewriting.In Findings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 59856006. Association for Computa-tional Linguistics. Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon,Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024.Ask optimal questions: Aligning large languagemodels with retrievers preference in conversationalsearch. CoRR, abs/2402.11827. Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, PaulBennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-shot generative conversational query rewriting. InProceedings of the 43rd International ACM SIGIRconference on research and development in Informa-tion Retrieval (SIGIR), pages 19331936. Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, andZhiyuan Liu. 2021. Few-shot conversational denseretrieval. In Proceedings of the 44th InternationalACM SIGIR conference on research and developmentin Information Retrieval (SIGIR). Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, MinZhang, and Shaoping Ma. 2021. Optimizing denseretrieval model training with hard negatives. In SI-GIR 21: The 44th International ACM SIGIR Confer-ence on Research and Development in Information",
  "Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,and Jian-Yun Nie. 2023. Retrieve anything to aug-ment large language models. CoRR, abs/2310.07554": "Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao,Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Ma-jumder, Ji-Rong Wen, and Nan Duan. 2022. Simans:Simple ambiguous negatives sampling for dense textretrieval. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing:EMNLP 2022 - Industry Track, Abu Dhabi, UAE, De-cember 7 - 11, 2022, pages 548559. Association forComputational Linguistics. Yutao Zhu, Huaying Yuan, Shuting Wang, JiongnanLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,and Ji-Rong Wen. 2023. Large language modelsfor information retrieval: A survey. arXiv preprintarXiv:2308.07107.",
  "A.2Baselines": "We provide a more detailed introduction to the base-lines:T5QR (Lin et al., 2020): a T5-based queryrewriting method trained with human rewrites asthe supervised signals.ConvGQR (Mo et al., 2023a): A unified frame-work for query reformulation that integrates rule-based query rewriting with a generative model toexpand queries.LLM4CS (Mao et al., 2023b): A state-of-the-artLLM-based prompting method for conversationalquery rewriting. LLM4CS has two three promptingmethods: REW, RAR, and RTR. REW only gen-erates a rewrite and RAR additionally generatesa hypothetical response. While RAR generates a rewrite and response in a two-step manner. ForLLM4CS (REW) and LLM4CS (RAR), we onlygenerate once for efficiency consideration and thusdo not need aggregation.Conv-ANCE (Mao et al., 2023c), which usesthe classical ranking loss to train the session em-beddings based on ANCE (Xiong et al., 2021).ConvDR (Yu et al., 2021), which uses knowl-edge distillation to learn the session embeddingsfrom rewrites.DialogInpainter (Dai et al., 2022), which is fine-tuned from the T5-large model using informationseeking dialogues generated from large web cor-pora.LeCoRE (Mao et al., 2023c), which extendsSPLADE (Formal et al., 2022) to be a conversa-tional lexical retriever using multi-level denoisingmethods.INSTRUCTOR (Su et al., 2023), a general re-triever tailored to various tasks and domains bytrained with various task-specific instructions.LLM Embedder (Zhang et al., 2023): a uni-fied retrieval model that can support diverse re-trieval augmentation needs of LLMs. It is fine-tuned on various tasks and datasets such as MS-MARCO, NQ, ToolLLM, QReCC, FLAN, Books3,and Multi-Session Chat.RepLLaMA (Ma et al., 2023), a large ad-hocretriever fine-tuned from LLaMA-7B on the MS-MARCO dataset.E5mistral-7b (Wang et al., 2024), a large ad-hoc re-triever fine-tuned from Mistral-7B on the syntheticdataset generated by ChatGPT and MSMARCO.GRIT (Muennighoff et al., 2024), a unifiedmodel for retrieval and generation. It is fine-tunedbased on Mistral-7B. The retrieval part is fine-tuned on the E5 (Wang et al., 2024) dataset withtask-specific instructions while the generation partis fine-tuned on the Tulu 2 (Ivison et al., 2023)dataset.",
  "A.3Hard Negatives": "For UltraChat, we first use in-context learning withQwen-7B-Chat, similar to the approach in (Maoet al., 2023b), to generate a query rewrite for eachturn. We then obtain hard negatives by randomlysampling from the top-15 to top-30 retrieval resultsusing the LLM Embedder on the CAsT-21 corpuswith rewrites. The hard negatives for MSMARCOare consistent with those used in (Ma et al., 2023). Generate a response to the current query given the contextand retrieved passages. If the passages are relevant and useful, referring to their information when forming your response. Otherwise, you may disregard them.",
  "Given a conversational query, its context-independentrewrite, and its response, generate two turns ofconversational context for it": "This turn:# Query: How much does it cost for someone to fix it?# Rewrite: How much does it cost for someone to repair a garage door opener?# Response: Garage door opener repair can cost between $100 and $300 depending on the extent of the problem. Return to Top. The type of garage door you select -- and any extra pieces or labor required -- will influence how much you pay to have it professionally # Synthetic Conversation Context:Query1: How much does a new garage door opener cost?Response1: The cost of a new garage door opener can range from $150 to $500, depending on the brand, features, and installation requirements. Query2: What are some common problems with garage door openers?Response2: Some common problems with garage door openers include issues with the remote control, the motor, the sensors, or the door itself. : An example prompt to generate synthetic con-versation text in the experiment of full context modifica-tion. Italicized contents are filled into the placeholdersof the prompt. The green content is the model output.",
  "DSettings of Continually Fine-tuningBaselines": "Since the training data of ChatRetriever only con-tains session-response pairs but does not containhuman rewrites, we use in-context learning withQwen-7B-Chat, similar to the approach in (Maoet al., 2023b), to generate query rewrite for eachturn and use them for the training of ConvDR andLeCoRE. GRIT and Conv-ANCE are fine-tunedwith their original contrastive ranking loss."
}