{
  "Abstract": "Long-horizon decision-making tasks presentsignificant challenges for LLM-based agentsdue to the need for extensive planning overmultiple steps. In this paper, we propose ahierarchical framework that decomposes com-plex tasks into manageable subgoals, utilizingseparate LLMs for subgoal prediction and low-level action generation. To address the chal-lenge of creating training signals for unanno-tated datasets, we develop a reward model thatleverages multimodal environment feedback toautomatically generate reward signals. We in-troduce Environment Preference Optimization(EPO), a novel method that generates prefer-ence signals from the environments feedbackand uses them to train LLM-based agents. Ex-tensive experiments on ALFRED demonstratethe state-of-the-art performance of our frame-work, achieving first place on the ALFREDpublic leaderboard and showcasing its poten-tial to improve long-horizon decision-makingin diverse environments.",
  "Introduction": "Long-horizon decision-making/planning remainsa formidable challenge for Large LanguageModel(LLM)-based agents (Valmeekam et al.,2023; Liu et al., 2023; Silver et al., 2024). Thesetasks require extensive planning over multiplesteps, maintaining coherence and goal orientation,which is difficult for LLMs that are typically de-signed for more immediate and localized predic-tions. Moreover, a key issue of finetuning LLMs forembodied agents is the need of large scale labeleddata (Reed et al., 2022). The same issue is reflectedin researchers effort in building reward modelsfrom vision foundation models as we might need toobtain internet-scale data of task demonstrations(Fan et al., 2022).",
  "*: Equal contribution. Code and dataset can be foundat": "To tackle the first challenge, a straightforwardway is to first let the LLM decompose the long-horizon task into shorter horizon subtasks, and thenuse different LLMs as the policies at different lev-els, i.e., use one LLM-based policy to generatesubgoals, and use another LLM generate low-levelactions given the subgoals, both of which requiresignificantly fewer planning steps. This decompo-sition facilitates more effective planning and exe-cution by leveraging the predictive power of LLMsat both the subgoal and action levels.However, the problem of how to efficiently trainthese LLM-based agents remains. In this paper, weconsider the setting where only part of the datasetare annotated with ground-truth actions and sub-goals, and we need to find a way to create train-ing signals for the unannotated dataset. The com-mon training signals for decision-making agentsare based on the rewards received during inter-actions with the environment (Sutton and Barto,1998). But the manual design of reward functionsis both time-consuming and prone to inaccuracies,which hinders the scalability and adaptability ofLLM-based agents in dynamic and diverse envi-ronments. Consequently, there is a growing needfor methods that can automatically generate rewardsignals from the environment, thus bypassing thecomplexities associated with human-engineered re-wards. This motivation drives us to explore rewardmodeling approaches that can leverage multimodalfeedback from the environment, such as visual andinteraction data, to guide the learning process ofLLM-based agents by leveraging the public pre-trained foundation models.On the other hand, recent advancements in pref-erence optimization techniques, such as DirectPreference Optimization (DPO) (Rafailov et al.,2023), have shown that LLMs can be effectivelytrained using preference-based signals rather thanexplicit reward functions. DPO leverages the in-herent capabilities of LLMs to model preferences between different outputs, facilitating a more intu-itive and flexible training paradigm. This insightinspires us to develop a novel method that com-bines the strengths of preference optimization withautomatic reward modeling to enhance the per-formance of LLM-based agents in long-horizondecision-making tasks.In this paper, we propose a hierarchical LLMs-based framework for long-horizon decision makingproblems. Our agent decomposes complex tasksinto manageable subtasks by training two LLMs topredict the subgoal decomposition and low-levelactions respectively. To retrieve enough trainingsignals from the unannotated dataset, we proposea LLM-based reward model that is able to inte-grate the multimodal environment feedback infor-mation and automatically generate reward signalsfor the unannotated dataset. Then, we introduceEnvironment Preference Optimization (EPO), amethod that generates preference signals automati-cally from the environments feedback. EPO ranksthe proposed actions and subgoals based on the es-timated rewards and constructs a preference datasetthat guides the training of LLM-based agents. Thisapproach leverages both annotated and unannotateddatasets, significantly expanding the training dataavailable for improving agent performance.To validate our framework design, we conductextensive experiments on ALFRED (Shridhar et al.,2020a), a popular household simulation environ-ment for embodied agents. Our method achievesthe state-of-the-art performance on ALFRED. Wealso find that unified environment feedback signifi-cantly help decision-making agents in both subgoaldecomposition level and environment interactionlevel. Moreover, in the setup where there exists alarge dataset of task specifications but only a smallannotated task and demonstrations, our frameworkallows agent to benefit from the unannotated newtasks while significantly outperforming supervisedtraining, indicating the potential of our framework.To sum up, we make the following contributions: 1. We propose a hierarchical LLMs-based frame-work for long-horizon decision-making prob-lems, where both levels of LLMs can bejointly trained with preference signals gen-erated from a LLM-based reward model.",
  "Related Work": "Foundational Models for Embodied Agents. Anumber of recent works have explored foundationalmodels for embodied agents (Driess et al., 2023;Stone et al., 2023; Brohan et al., 2023; Zitkovichet al., 2023). Our work is inspired by many previ-ous language grounding agents work (Singh et al.,2023; Ahn et al., 2022; Huang et al., 2022) onrobotics. These studies work on grounding natu-ral language prompt or robotic actions with sym-bolically represented visual or interaction infor-mation. Similarly effort in grounding languageto visual information for embodied agents havebeen done in (Song et al., 2023a). Among worksin simulation, Pashevich et al. (2021) present theend-to-end approach for decision-making agents,which directly predicts the agents next action fromtask specification and visual input without subgoalalignment and map-based navigation. Min et al.(2021) introduce a hierarchical approach, whichhas dominated due to their superior performance.Fu et al. (2024) leverages LLM to help learningskills from demonstrations. Our hierarchical LLMsframework is also inspired by many prior hierar-chical RL works (Nachum et al., 2018; Levy et al.,2019; Fu et al., 2023).Reward Modeling with Foundational Models.Foundation models with their capability in encod-ing generic representations of a modality have mo-tivated researchers to use them to generate rewardsignals in order to bypass human reward engineer-ing. Among these efforts, Sontakke et al. (2023);Escontrela et al. (2023); Chen et al. (2021); Fanet al. (2022); Mahmoudieh et al. (2022) use visionfoundation models to estimate the reward by align-ing visual features with desired actions or state tran-sitions. However, these approaches often requirelarge scale data. In contrast, we are interested in",
  "EPO has been top of the leaderboardas of the release date of this paper": "using pretrained LLMs to generate reward signals(Kwon et al., 2023) from all symbolically repre-sented environment feedback. Within this scope,Song et al. (2023b); Yu et al. (2023); Ma et al.(2023); Huang et al. (2023); Wang et al. (2023)use language models to generate rewards to helprobot learn skills based on the symbolic states. Forembodied agents, ELLM (Du et al., 2023) proposea framework to use LLMs to guide agents explo-ration and generate reward based on the task goalsin 2D games and robotic simulators. Compared toexisting works, we fill in the blank by proposinga generic framework that use LLMs to synthesizereward from multimodal environment feedback.Preference-Based Learning for LanguageModels. Aligning language models to human pref-erence (Ouyang et al., 2022) has greatly improvedlanguage models to follow human instructions. Re-cent development such as Direct Preference Op-timization (Rafailov et al., 2023), self-rewardinglanguage models (Yuan et al., 2024) in preferencealignment allows the language model to directlylearn the preference relation and also learn from itsown synthesized data. Inspired by these work, weextend the definition of preference into the align-ment between environment feedback and agent ac-tions with respect to the task specification. Weleverage the algorithmic advantage demonstratedin DPO and the idea of self data synthesis (Leeet al., 2023) to train LLM-based embodied agentsto ground language to environment feedback.",
  "Problem Setup": "In this paper, we consider the decision-makingagents that take in human language instructions Gas well as visual observations o from environmentE, and generate a sequence of actions a to interactwith the environment, aiming to achieve the goal de-scribed by G. Low-level action a is parameterizedby an action type l and optionally a target object kl,in the form of natural language, e.g. Pickup (apple),Moveforward (None). We consider the setting of learning from demonstrations and we have accessto the environment E that each task is associatedwith, where the reward function is not providedif we let the agent interact. We assume the agentis given a partially-annotated dataset a certainportion of the dataset are unannotated. Each tra-jectory from the fully annotated part of the datasetconsists of {G, E, g1, a1, g2, a2, }, where gt de-notes the assigned subgoal for current timestep t.g from the dataset is also described by language.The unannotated part of the dataset consists of taskgoals and environments (no reward function) with-out the ground-truth low-level actions and subgoals{G1, E1, G2, E2, }. The performance of ouragent is measured with task success rate, which isthe percentage of test tasks completed given a setof human task instructions.",
  "Hierarchical LLMs-based Agent": "LLMs are known for struggling with long-horizonplanning tasks. A natural way to alleviate this issueis by decomposing the tasks into shorter-horizonsubtasks. We show our hierarchical LLMs-basedagent framework in . We finetune pre-trained LLMs to output predictions for subgoalsgiven the general task goal, and finetune anotherLLM to output predictions for low-level actionsgiven the subgoals. Specifically, we parameter-ize each subgoal with a high-level action type hand a target object/position kh, both in the formof language, similar to what we set for the low-level actions. Note that the subgoal may look sameto some low level actions, e.g. pickup potato.However, the pickup low level action can beexecuted only when the agent is at a place nearthe potato and facing towards it, while the sub-goal pickup potato needs to be executed fromanywhere and may require many low-level actionsfor navigation. Given the task instruction G (e.g.Wash the apple on the counter) and the originalsubgoal described by natural language (e.g., Findthe apple), the high-level decomposition module(parameterized by an LLM) h outputs the decom-posed subgoals {h, kh} = h(G, g), e.g. HeatCup.We find this subgoal decomposition design es-pecially beneficial for training embodied agentsthat directly use LLMs as their policies since: 1.Subgoals with a fixed form instead of the free-formlanguage from the dataset enable us to better in-fer the preference signals between two possibleresponses (see .4). 2. It functions as a",
  "Environment": "We conduct experiments on ALFRED (Shridharet al., 2020a), a popular household simulation envi-ronment based on AI2-THOR (Kolve et al., 2017)for embodied agents. It consists of 120 indoor sim-ulations of different room types. The official expertdemonstration dataset consist of 8055 task demon-stration annotated with 25,743 natural languageinstructions in English. The entire dataset is splitinto 21023 instructions in training set, 820 in seenvalidation set whose environment scenes are sharedwith those in the training set, 821 in unseen valida-tion whose environment scenes are not available intraining set. Only the task instructions in trainingand validation set are paired with the subgoal andlow-level action annotations. Subgoals and actionsannotations are in the form of structured naturallanguage. In this environment, our agent receives",
  "Reward modeling from EnvironmentFeedback": "One of the key motivations of this paper is to by-pass the complex human-based reward engineeringand learn to automatically generate feedback sig-nals for a diverse set of unannotated tasks that canhelp train the LLM-based agent. To this end, wepropose an approach to learn a reward model thatis able to generate feedback signals from the multi-modal observations of the environment. We showthe proposed Reward Modeling and EPO trainingframework in . Environment Feedback. We consider two typesof environment feedback that an embodied agentcan typically receive. The first one is visual posi-tional feedback, i.e., each timestep the agent willreceive a visual observation (image) describing thecurrent environment, and we apply pretrained vi-sion models to retrieve visual positional feedbackV in the form of labels or natural language. Forexample, given an observed frame in a house, anobject detection model of our agent will output alist of objects detected from its label space or atextual description such as \"a computer on top ofa desk\". The second type of feedback is interac-tion feedback. If the agent attempt to interact withthe detected objects using low-level actions likepick up or close, it will receive a interactionfeedback I in the form of boolean values or naturallanguage. For example, our agent could attempt toPick up Cup, then it will receive a boolean valueindicating if its action succeeded.",
  "EPO Dataset": ": An illustration of our pipeline to train reward model for grounding environment feedback with humaninstructions. We supervisedly train the reward model given the annotated data. Then we use the reward model tolabel unannotated data to obtain the preference relations. Then we form the EPO datasets and optimize our agentpolicies using the proposed EPO algorithm. formation as F. Our reward model R takes in thefeedback information F, task specific input T andpredicted subgoal/action P from the LLM, and out-puts a reward value which describes the alignmentscore of the proposed output with respect to thetask input, given newly observed environment feed-back. Here, feedback information F can be visualfeedback V , interaction feedback I, or both. Taskspecific input T can be the input of the high-leveldecomposition module {G, g} or that of the low-level interaction module {h, kh, apast}. Predictedoutput P can be the output of the subgoal decom-position module {h, kh} or that of the interactionmodule a.",
  "r = R(F, T, P)(1)": "To train this reward model, we construct positivepairs based on whether the proposed output is cor-rect with respect to the task input and assign themwith high rewards. Similarly we construct nega-tive pairs with incorrect proposed output and lowrewards. For instance, if the visual positional feed-back we get from the environment after symbolicrepresentation F is there exists a cup and an appleon the counter and our task instruction T is pickup the red apple on the left side of the cup, wewill construct the positive pair using the correctlabel, so our proposed answer P is Pick up objectapple. When constructing the negative pair, wehave the same F and T, but the proposed answeris randomly chosen from possible outputs, it canbe Pick up object cup. In this way, we constructa synthetic dataset that maps the environment feed-back, task specifications, and proposed answers toreward values. Then we train the reward modelusing the cross-entropy loss.",
  "Environment Preference Optimization": "With the trained reward model, we can leveragethe unannotated dataset by evaluating our agentsproposed subgoals or low-actions according to thegiven environment feedback and task specification.We first pretrain the hierarchical LLM modules onthe annotated dataset. Then on the unannotateddataset, we use our reward model to evaluate theLLM modules outputs and rank them accordingto the estimated reward. After that, we will havea ranking of the outputs (p1, p2, . . . , pn), where p1denotes the output that is given the highest reward:rp1 = max rpi. It holds that rpi > rpj if i < j.Fromtheresponseranking,wecanconstructapreferencedatasetD={(F1, T1, pw1, pl1), (F2, T2, pw2, pl2), . . .}, wherepw1 is the proposed output that is more likelycorrect, pl1 is the less likely one. Given that theenvironment feedback and our reward modellabeling might not be perfect, especially underthe circumstance of insufficient labeled data, wepropose Environment Preference Optimization(EPO) which combines DPO (Rafailov et al., 2023)training with an token-level alignment loss. Weprovide additional token-level constraint whilepreserving the learning of preference relations.The training objective is as below:",
  "denotes the LLM we are trying to optimize andit can be either the subgoal decomposition mod-ule h or the low-level interaction module l": "denotes the logistic function. is the hyperparam-eter for scaling. We use sup to denote the LLMlearned from the annotated dataset and denote thelogits of our model output tokens as p. The trainingobjective of LD is to maximize the log probabilitydifference between the chosen response and therejected response, which is calculated based on alltokens. Note that DPO does not force the model toalign with the chosen output, instead it encouragesthe model to maximize the reward difference be-tween chosen outputs and rejected outputsit doessoft-alignment. However, in our case we stillwant our model to \"hard-align\" to the labels withthe highest reward since they are mostly likely to bethe correct label. Furthermore, we want to reducethe algorithmic instability rises in soft-alignment,which could hamper LLMs to follow certain de-sired output format. For example in practice, wewant our high-level subgoal decomposition policyto output both of the subgoal parameters h andkh. With the alignment loss (first term in Eqn 2),we guide the optimization process to reduce thealgorithmic instability rises especially when wetrain with a large amount of unlabeled data. In thisway, we let the model learn the preference relationbetween answers but also align towards the mostcorrect outputs with parameters in given formatsince it does the reward modeling and the tokenlevel optimization at the same time. Note that inpractice, we apply EPO to both high- and low-levelpolicies training process.",
  "Implementation details": "We use pretrained RCNN as the object detec-tion model and Mask-RCNN as the segmentationmodel (He et al., 2017). For representing visual in-formation, we also want to study how visual detailinformation (e.g. image captions) could contributeas a form of environment feedback. Therefore, weuse BLIP-2 (Li et al., 2023) as our image caption-ing model and we apply it at the view-points wherewe can interact with the objects.For both levels of our agent modules, and re-ward models, we use Llama2-7B (Touvron et al.,2023) as the large language model backbone anduse LoRA (Hu et al., 2022) to efficiently finetunethe language models.Agent Learning. In order to validate the effec-tiveness of our framework in learning from unanno-tated dataset, we split the annotated trained datasetinto a labeled dataset for which we have accessto the annotated labels and a unlabeled dataset forwhich we have only access to the task specifica-tions without labels, to mimic the real world sce-nario where we have only limited annotated expertdemonstrations but can access to many new taskspecifications. On the unlabeled dataset, we use ourreward model trained on the labeled dataset to in-ference reward for each possible outputs. Then weform the environment preference dataset based onthe rewards of the outputs. More details about ourexperimental setting can be found in the appendix.",
  "Results": "In this section, we first compare the overall perfor-mance of our framework with the state-of-the-artmethods on ALFRED public leaderboard and thenmodularly study the components of our framework.We obtain all the results following the standard set-ting in ALFRED where we first let the agent learnfrom the given dataset offline, and then test the theonline rollout performance of the learned policies(modules) on the given set of new test tasks.",
  ": Comparison with SOTA methods on ALFRED test set. GC stands for goal-conditioned. PLW stands forpath length weighted. We get the data of the baselines from ALFREDs public leaderboard": "best setup for all our module. That means weuse the subgoal decomposition module and interac-tion module both trained on environment feedbackwith reward modeling and EPO. In , ourmethod significantly outperforms previous workover 12.0% on unseen test tasks while achievingSOTA performance on both unseen and seen scenar-ios in all metrics, indicating the effectiveness of ourapproach. Moreover, our method achieves signifi-cant superior performance on path length weighted(PLW) metrics, which indicates the efficiency ofour method in completing the tasks in fewer steps.It is worth mentioning that our approach does notuse semantic voxel map (Shridhar et al., 2020b),which requires the access of environment meta data.Our approach uses agent exploration (Appendix B)to obtain object location information which gen-eralizes better to real world scenarios without themeta information defined in simulators.",
  "How well does EPO learn fromunannotated data?": "Environment preference optimization enhancesthe agents performance via training on theunannotated data. We compare to SupervisedFine-Tuning (SFT), where we directly prepend theenvironment feedback to task information and trainonly use the annotated dataset. To study whetherour proposed framework can further improve itselfthrough learning from unannotated dataset, we con-sider three data split. First, full/No-Split means weuse the entire annotated ALFRED dataset. Second,90/10 means we use 90% of the demonstration withtheir annotations and 10% of the demonstrationwithout annotation. Lastly, 10/90 refers to the splitwhere only 10% of the data we use is annotated and90% is unannotated. We can see that in all three se-tups, our method based on environment preferenceoptimization outperforms supervised fine-tuning.As we increase the amount of unannotated data, one can observe that our framework start to show moresignificant superior performance than supervisedfine-tuning. This trend of our proposed EPO per-forming better when there exists more unannotateddata indicates that the data efficiency and potentialof EPO in real application scenarios, as data effi-ciency is one of the most important problems forlearning from demonstrations in practice.",
  "How well do different environmentfeedbacks help decision making?": "Reward modeling can help improve low-levelinteraction module. Previous work on ALFRED(Min et al., 2021) makes the hypothesis that, AL-FREDs low-level action dynamics to accomplishthe interaction subgoals are quite deterministic andcan potentially be handled with a deterministic pro-gram. We consider the comparison between ourlearning-based interaction module (LLM) againstthe hard-coded deterministic program. Here we usethe same subgoal decomposition policy which issupervised fine-tuned with the environment feed-back and only change the interaction module fora fair comparison. As shown in , with re-ward modeling and EPO training, our LLM-basedinteraction module is able to achieve better per-formance than the hard-coded program. We alsoobserve that without reward modeling, our inter-action module fails to achieve comparable resultwith respect to the deterministic program due to theinaccuracy in choosing low-level actions. We find",
  "(a)(b)": ": An visual illustration of how EPO improved both high-level subgoal decomposition policy and thelow-level interaction policy. In the left figure, we present the difference between a baseline high-level policy and aEPO trained counterpart. We observe that the latter one can correctly figure out the subgoal. In the right figure, wepresent the difference between a baseline low-level policy and a EPO trained counterpart. We observe that the latterone can conduct post adjustment to successfully execute the actions. that since the interaction module in this setup isonly trained to imitate previous action trajectories,it fails on the test tasks when the setup is differentfrom the training settings. For example, we wouldexpect the agent to first open the drawer\" whenthe drawer is closed before attempting to pickupthe pen. However, in training data, the majority ofpickup object actions do not require to open thereceptacle object first.",
  "ModelNoNo0.29070.2707ModelYesNo0.51160.4744ModelYesYes0.55420.5341": ":Comparison between static program andlearning-based interaction module. Feedback indicateswhether we include feedback information. Reward in-dicates whether we use SFT or EPO with data gatheredduring interaction. Environment feedback can help subgoal de-composition. We use supervised finetuning to fine-tune the subgoal decomposition policy with envi-ronment feedback and use the static program asthe interaction module as a fair comparison. Boththe learning algorithm and the interaction moduleare the same as the baseline module. As shownin , with either interaction feedback or vi-sual feedback or a combination of both, we obtainperformance gain on both seen and unseen tasks.We also find that a combination of both types offeedback reaches the best performance and that the interaction feedback exhibits more benefit for train-ing than only using visual feedback. One possiblereason is that our image captioning model onlygives a scene description while the interaction feed-back that is more concrete indicator on whether theobject is a potential candidate for subgoals.",
  "Qualitative Analysis": "In addition to quantitative experiments, we visual-ize the performance of our policies and investigatetheir effectiveness. (a) shows a compari-son between the baseline policy and the EPO-tunedpolicy. We see that the baseline policy outputs sub-goal predictions closely following the language butoutputs the wrong object cup that the low-levelinteraction module cannot process. However, fromenvironment feedback we detected mug exists.Our EPO-tuned policy is able to output the correctparameterization for the subgoal and complete thetask. (b) shows a comparison between thehard-coded deterministic program and our learning-based low-level interaction module. We find thatthe deterministic program fails because although it outputs the action that is nearly correct but theagent is not close enough to the object so the action(Putobject) cannot be executed. On the other hand,after EPO-tuning our module learn to first outputactions to adjust its pose, which leads to successinteraction with the environment.",
  "Conclusion": "In this paper, we presented a hierarchical LLM-based framework for long-horizon decision-makingtasks, addressing the inherent challenges of exten-sive planning and the need for scalable training sig-nals. By leveraging a reward model that integratesmultimodal environment feedback, and introducingEnvironment Preference Optimization (EPO), wesuccessfully generated training signals for unanno-tated datasets. Our framework demonstrated state-of-the-art performance on the ALFRED bench-mark. Future work will focus on exploring the inte-gration of additional types of multimodal feedbackto further enhance the agents decision-making ca-pabilities, as well as extending our framework toreal world robotics tasks.",
  "Limitations": "We evaluate the proposed method on ALFRED,where the low-level action space is discrete andannotated with language. For some continuouscontrol tasks, the action space can be much largerand hard to interpret. Future work will focus onexploring the integration of additional types of mul-timodal feedback to further enhance the agentsdecision-making capabilities, as well as extendingour framework to real world robotics tasks.",
  "Acknowledgement": "This work was conducted using computational re-sources and services at the Center for Computationand Visualization, Brown University. The projectwas in part supported by ONR grant #N00014-22-1-2592, and the Samsung Global Research Outreachprogram. The authors would like to thank CalvinLuo, Tian Yun, as well as the anonymous reviewersfor valuable feedbacks. Michael Ahn, Anthony Brohan, Noah Brown, YevgenChebotar, Omar Cortes, Byron David, Chelsea Finn,Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-man, et al. 2022. Do as i can, not as i say: Ground-ing language in robotic affordances. arXiv preprintarXiv:2204.01691.",
  "Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,and Yoav Artzi. 2022. A persistent spatial semanticrepresentation for high-level natural language instruc-tion execution. In CoRL": "Anthony Brohan, Noah Brown, Justice Carbajal, Yev-gen Chebotar, Joseph Dabis, Chelsea Finn, KeerthanaGopalakrishnan, Karol Hausman, Alexander Herzog,Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, RyanJulian, Dmitry Kalashnikov, Yuheng Kuang, IsabelLeal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Ut-sav Malla, Deeksha Manjunath, Igor Mordatch, OfirNachum, Carolina Parada, Jodilyn Peralta, EmilyPerez, Karl Pertsch, Jornell Quiambao, KanishkaRao, Michael S. Ryoo, Grecia Salazar, Pannag R.Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Son-takke, Austin Stone, Clayton Tan, Huong T. Tran,Vincent Vanhoucke, Steve Vega, Quan Vuong, FeiXia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, andBrianna Zitkovich. 2023. RT-1: robotics transformerfor real-world control at scale. In Robotics: Scienceand Systems XIX, Daegu, Republic of Korea, July10-14, 2023.",
  "Annie S Chen, Suraj Nair, and Chelsea Finn. 2021.Learning generalizable robotic reward functionsfrom\" in-the-wild\" human videos": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, SalemLahlou, Lucas Willems, Chitwan Saharia, Thien HuuNguyen, and Yoshua Bengio. 2019. Babyai: A plat-form to study the sample efficiency of grounded lan-guage learning. In 7th International Conference onLearning Representations, ICLR 2019, New Orleans,LA, USA, May 6-9, 2019. OpenReview.net. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, CoreyLynch, Aakanksha Chowdhery, Brian Ichter, AyzaanWahid, Jonathan Tompson, Quan Vuong, TianheYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-manet, Daniel Duckworth, Sergey Levine, VincentVanhoucke, Karol Hausman, Marc Toussaint, KlausGreff, Andy Zeng, Igor Mordatch, and Pete Florence.2023. Palm-e: An embodied multimodal languagemodel.In International Conference on MachineLearning, ICML 2023, 23-29 July 2023, Honolulu,Hawaii, USA, volume 202 of Proceedings of MachineLearning Research, pages 84698488. PMLR.",
  "Yuqing Du, Olivia Watkins, Zihan Wang, Cdric Co-las, Trevor Darrell, Pieter Abbeel, Abhishek Gupta,and Jacob Andreas. 2023. Guiding pretraining inreinforcement learning with large language models": "Alejandro Escontrela, Ademi Adeniji, Wilson Yan, AjayJain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee,Danijar Hafner, and Pieter Abbeel. 2023. Video pre-diction models as rewards for reinforcement learning.arXiv preprint arXiv:2305.14343. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-dlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,De-An Huang, Yuke Zhu, and Anima Anandkumar.2022. Minedojo: Building open-ended embodiedagents with internet-scale knowledge. In Thirty-sixthConference on Neural Information Processing Sys-tems Datasets and Benchmarks Track. Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin,George Konidaris, Nicolas Le Roux, Marc-AlexandreCt, and Xingdi Yuan. 2024. Language-guided skilllearning with temporal variational inference. CoRR,abs/2402.16354. Haotian Fu, Shangqun Yu, Saket Tiwari, MichaelLittman, and George Konidaris. 2023. Meta-learningparameterized skills. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 1046110481.PMLR.",
  "Minae Kwon, Sang Michael Xie, Kalesha Bullard, andDorsa Sadigh. 2023. Reward design with languagemodels": "Harrison Lee, Samrat Phatale, Hassan Mansoor, KellieLu, Thomas Mesnard, Colton Bishop, Victor Car-bune, and Abhinav Rastogi. 2023. Rlaif: Scalingreinforcement learning from human feedback with aifeedback. arXiv preprint arXiv:2309.00267. Andrew Levy, George Dimitri Konidaris, Robert PlattJr., and Kate Saenko. 2019. Learning multi-level hi-erarchies with hindsight. In 7th International Confer-ence on Learning Representations, ICLR 2019, NewOrleans, LA, USA, May 6-9, 2019. OpenReview.net.",
  "So Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-mar, Yonatan Bisk, and Ruslan Salakhutdinov. 2021.Film: Following instructions in language with modu-lar methods. arXiv preprint arXiv:2110.07342": "Ofir Nachum, Shixiang Gu, Honglak Lee, and SergeyLevine. 2018. Data-efficient hierarchical reinforce-ment learning. In Advances in Neural InformationProcessing Systems 31: Annual Conference on Neu-ral Information Processing Systems 2018, NeurIPS2018, December 3-8, 2018, Montral, Canada, pages33073317. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744.",
  "Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D Manning, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. NeurIPS": "Scott Reed, Konrad Zolna, Emilio Parisotto, Ser-gio Gomez Colmenarejo,Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,Jackie Kay, Jost Tobias Springenberg, et al. 2022. Ageneralist agent. arXiv preprint arXiv:2205.06175. Mohit Shridhar, Jesse Thomason, Daniel Gordon,Yonatan Bisk, Winson Han, Roozbeh Mottaghi, LukeZettlemoyer, and Dieter Fox. 2020a.Alfred: Abenchmark for interpreting grounded instructions foreveryday tasks. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 1074010749.",
  "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2020b. Alfworld: Aligning text andembodied environments for interactive learning. InICLR": "Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B.Tenenbaum, Leslie Pack Kaelbling, and MichaelKatz. 2024.Generalized planning in PDDL do-mains with pretrained large language models. InThirty-Eighth AAAI Conference on Artificial Intelli-gence, AAAI 2024, Thirty-Sixth Conference on Inno-vative Applications of Artificial Intelligence, IAAI2024, Fourteenth Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2014, Febru-ary 20-27, 2024, Vancouver, Canada, pages 2025620264. AAAI Press. Ishika Singh, Valts Blukis, Arsalan Mousavian, AnkitGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: Generating situated robot task plans usinglarge language models. In ICRA.",
  "Sumedh Anand Sontakke, Jesse Zhang, Sb Arnold,Karl Pertsch, Erdem Biyik, Dorsa Sadigh, ChelseaFinn, and Laurent Itti. 2023. Roboclip: One demon-stration is enough to learn robot policies. In NeurIPS": "Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrish-nan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart,Sean Kirmani, Brianna Zitkovich, Fei Xia, ChelseaFinn, and Karol Hausman. 2023. Open-world objectmanipulation using pre-trained vision-language mod-els. In Conference on Robot Learning, CoRL 2023,6-9 November 2023, Atlanta, GA, USA, volume 229of Proceedings of Machine Learning Research, pages33973417. PMLR.",
  "Richard S. Sutton and Andrew G. Barto. 1998. Re-inforcement learning - an introduction. Adaptivecomputation and machine learning. MIT Press": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Karthik Valmeekam, Matthew Marquez, Sarath Sreed-haran, and Subbarao Kambhampati. 2023. On theplanning abilities of large language models - A criti-cal investigation. In Advances in Neural InformationProcessing Systems 36: Annual Conference on Neu-ral Information Processing Systems 2023, NeurIPS2023, New Orleans, LA, USA, December 10 - 16,2023. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang,Yian Wang, Zackory Erickson, David Held, andChuang Gan. 2023. Robogen: Towards unleashinginfinite data for automated robot learning via genera-tive simulation. CoRR, abs/2311.01455. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kir-mani, Kuang-Huei Lee, Montse Gonzalez Arenas,Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasen-clever, Jan Humplik, et al. 2023. Language to re-wards for robotic skill synthesis.arXiv preprintarXiv:2306.08647.",
  "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.2024.Self-rewarding language models.arXivpreprint arXiv:2401.10020": "Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu,Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Ste-fan Welker, Ayzaan Wahid, Quan Vuong, VincentVanhoucke, Huong T. Tran, Radu Soricut, AnikaitSingh, Jaspiar Singh, Pierre Sermanet, Pannag R.Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Rey-mann, Kanishka Rao, Karl Pertsch, Igor Mordatch,Henryk Michalewski, Yao Lu, Sergey Levine, LisaLee, Tsang-Wei Edward Lee, Isabel Leal, YuhengKuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J.Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexan-der Herzog, Karol Hausman, Keerthana Gopalakr-ishnan, Chuyuan Fu, Pete Florence, Chelsea Finn,Kumar Avinava Dubey, Danny Driess, Tianli Ding,Krzysztof Marcin Choromanski, Xi Chen, YevgenChebotar, Justice Carbajal, Noah Brown, AnthonyBrohan, Montserrat Gonzalez Arenas, and KehangHan. 2023. RT-2: vision-language-action modelstransfer web knowledge to robotic control. In Confer-ence on Robot Learning, CoRL 2023, 6-9 November2023, Atlanta, GA, USA, volume 229 of Proceedingsof Machine Learning Research, pages 21652183.PMLR.",
  "ASymbolic Representation and PromptExamples": "In dealing with multimodal feedback information,it is crucial for us to design structure prompt tointeract the LLMs. Luckily, the task specifications, subgoal and low-level action annotations arealready in the form of text so we do not need to fur-ther tune them. The visual and interaction feedbackhowever, needs to proper symbolically represented.For example, when our object detector finds visi-ble objects, our agent will interact with it. If theattempted interaction is successful, our agent willreceive a boolean value from the system. We woulddescribe this event as action successful for ourlow-level policies. In gathering the environmentfeedback, we would just simply append the nameof the object to the existing object list. Visual feed-back, which is the image captioning data, is alreadyin the form of text. illustrates prompt ex-amples of our pipeline.",
  "BAdditional Algorithm Details": "In Algorithm 1, we provide the detailed steps of ourenvironment preference data generation process.We first infer reward values from possible outputsfrom the policy using the reward model. Then werank all the possible outputs based on reward. Thenwe pick the output with the highest reward as thechosen prompt and the rest as the rejected output,the prompt is environment feedback f prepend totask specification .Language Model Training For all our policies,we use pretrained Llama-7B as the backbone LLM.It has around 7 billion parameters. All our exper-iments are conducted on NVIDIA A6000 GPU. We use LoRA to efficiently fine-tune the languagemodels with the datasets we design. Specifically,we use r = 8, = 32, and lora dropout equalsto 32. We use a learning rate of 1e 5 and adamoptimizer. The target fine-tuning modules are theq-projection layers and the v-projection layers. Ap-proximately, 5% of the parameters are trained. Wefind our training usually coverges within 1 epoch.For EPO training, the learning rate is set to 1e 6.In all our training, we use a batch size of 32. Totrain the reward model, we use Llama2 with a clas-sification head instead of casual generation. ForBLIP-2, we only use the image as input to generatethe captions. We did try providing additional textin the prompt but did not observe any clear benefitsto the results.ALFRED There are two categories of subgoals,navigation and interaction. We use the determinis-tic navigator provided by (Shridhar et al., 2020b),which needs the view-point location to navigate to.However, we did not use environment meta infor-mation to obtain the view-points for the objects.Our agent exploration process is able to successfulrecord possible view-points for successful interac-tion. The only meta information we use is actionsuccess and agent inventory. To determine the ob-ject to navigate to, we use the target object of thenext subgoal as the navigation target. To interactwith objects in ALFRED, one needs to output ainteraction mask. We do so using the MaskRCNNmodel provided by (Pashevich et al., 2021). Weuse the checkpoints from Episodic Transformer(Pashevich et al., 2021).Environment exploration In order to receivefeedback from the environment, we need an struc-tured process of exploration. First, we define theconcept of view-points\", which indicates the loca-tion, direction and camera angle. A view-point isparameterized with four variables x, y, r, h. x andy indicates the grid coordinates of the agent in the3D-environment. r indicates the direction whichour agent is facing. h indicates the eye level angleof our agents. We consider the height of our agentfixed at all time. We explore the environment to letthe agent visit as much view-points as possible. Weallow agents to explore all possible locations andview-points to interact with the visible objects.Through our exploration, we apply object detectorto obtain the visible objects. We record the objectthat our agent successfully interacted with. Afterexploration, we will have a view-point point mapof all objects the agent has interacted with. Decomposition module The input of our de-composition module is the task instructions andthe output is generated text that indicates the sub-goal prediction. The generated text will be post-processed into high-level actions and target objectsin the form of texts. One could form this problemas a classification task without the intermediate text.But we argue that generating free-form languagegeneralizes better to environments and tasks whenthe possible subgoals of our agent are hard to bedefined in a closed set.Interaction module. After our agent predictsthe subgoals, it uses an interaction module to out-put the low-level actions to complete each subgoalsequentially. There exists two types of subgoals:navigation and interaction. For a navigation sub-goal, we use a view-point-based navigation plannerwith the object location information we gained dur-ing agent exploration. For interaction subgoals, asnoticed by previous work (Min et al., 2021), theaction sequences required to complete them can bequite deterministic and is possible to solved themwith a static program. Nevertheless, we proposea learning-based method in which our model usesa large language model as its backbone. It takesin the subgoal information, the interaction feed-back from its previous action, and its historical ac-tions in completing this subgoal, all symbolically-represented in text and outputs the next low-levelaction. Our model generalize better to the scenar-ios which action dynamics are less deterministic. Itpredicts the next action based on interaction feed-back and previous actions in an auto-regressivemanner. Later in experiments, we show that thislearning-based module can be further improvedwith environment feedback and EPO.Reward Modeling Recall that our reward modelestimates the likelihood of the output is correct andform the environment preference dataset throughranking. In training the reward model for the sub-goal decomposition module, we use the annotateddataset to form input consist of environment feed-back F, task specification T, and proposed answerP. Then we label the correct annotation with 1 toform a positive pair and randomly select an incor-rect output from each of the parameters to form 2negative pairs. After we train the reward model, wemake inference on the unannotated dataset whereF and T are available but the proposed answeris from our SFT pretrained module or other pos-sible outputs. Then we can form the preferencedataset by comparing the reward between proposed answers given the same F and T. In training thereward model for interaction module, we gather on-line data by allowing our agent to attempt variouspose changes and interactions until it could succeedits intended action. Then we record the actions ledto successful interaction and other unsuccessful ac-tions to form the positive and negative pairs. Thenthe process to form the preference dataset is simi-lar with that of the subgoal decomposition module.We did not any AI assistant in writing this paper.Baselines We compare the overall performanceof our framework with the state-of-the-art methodson ALFRED public leaderboard. We obtain all theresults following the standard setting in ALFREDwhere we first let the agent learn from the givendataset offline, and then test the online rollout per-formance of the learned policies (modules) on thegiven set of new test tasks. All baselines have ac-cess to the same amount of information, as this isthe standard setting required by ALFRED to get ascore on the public leaderboard. Thus we believethe comparison with all the baselines is fair. Wewill add more descriptions for each baseline listedin the updated version of our paper as suggested.Specifically, HLSM proposes to build a persistentspatial semantic representation from natural lan-guage instructions. FILM involves the creation ofa semantic map of the environment and a semanticsearch policy to navigate and interact based on theinstructions provided. EPA uses a discrete graphrepresentation enriched with new perceptions dur-ing exploration, allowing the agent to generate newplanning problems and recover from action fail-ures. Prompter introduces a method that replacesthe traditional semantic search module in embodiedinstruction following systems with language modelprompting. CAPEAM enhances an agents abilityto perform household tasks by integrating semanticcontext and maintaining the state of objects withinthe environment.",
  ": Results on BabyAI": "Results on BabyAI We also conduct a set of ex-periments on BabyAI (Chevalier-Boisvert et al.,2019) minibosslevel, which is an environmentwhere an agent navigates and interacts in a gridworld to achieve a goal described in language. As shown in , we observe that EPO with en-vironment feedback (object type observed by theagent) can boost task success rate from 0.7409 to0.9905 and with 10% of labeled data and EPO, ourpolicy can reach 0.9781 task success rate, which isjust 0.0124 less than using all labeled training data. : A illustration of prompt to our LLM policies.From top to bottom: example of baseline subgoal policy,example of baseline interaction policy, example of inter-action feedback , example of visual feedback , exampleof reward model training Data, example of EnvironmentPreference Data"
}