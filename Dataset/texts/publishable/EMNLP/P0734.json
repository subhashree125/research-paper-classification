{
  "Abstract": "Despite their widespread adoption, large lan-guage models (LLMs) remain prohibitive touse under resource constraints, with their evergrowing sizes only increasing the barrier foruse. One noted issue is the high latency associ-ated with auto-regressive generation, renderinglarge LLMs use dependent on advanced com-puting infrastructure. Assisted decoding, wherea smaller draft model guides a larger targetmodels generation, has helped alleviate this,but remains dependent on alignment betweenthe two models. Thus if the draft model is in-sufficiently capable on some domain relativeto the target model, performance can degrade.Alternatively, one can leverage multiple draftmodels to better cover the expertise of the tar-get, but when multiple black-box draft modelsare available, selecting an assistant without de-tails about its construction can be difficult. Tobetter understand this decision making prob-lem, we observe it as a contextual bandit, wherea policy must choose a draft model based ona context. We show that even without priorknowledge of the draft models, creating an of-fline dataset from only outputs of independentdraft/target models and training a policy overthe alignment of these outputs can accelerateperformance on multiple domains provided thecandidates are effective. Further results showthis to hold on various settings with multipleassisted decoding candidates, highlighting itsflexibility and the advantageous role that suchdecision making can play.",
  "*Correspondance to": "et al., 2023; Ouyang et al., 2022). However, theuse of resource intensive models and techniquesremains a pre-requisite and accordingly, methodshave been developed and applied to alleviate con-cerns relating to the practical usability of thesemodels (Dettmers et al., 2022; Dao, 2024). One ma-jor area that has observed consistent improvementover time is the auto-regressive decoding aspectof text generation, where each generation of a newtoken requires a complete inference pass throughthe model, which under-utilizes the property of at-tention and the ability of modern accelerators (e.g.GPUs, TPUs) to parallelize computations (de Jonget al., 2022; Kim et al., 2023a).A growing approach towards addressing this isspeculative decoding (Xia et al., 2023; Leviathanet al., 2023). In speculative decoding, latency isreduced by minimizing the amount of high-latencysequential computations and replacing them withcheaper ones. Rather than sampling directly fromthe larger model, the sampling is approximatedwith samples from a smaller and cheaper modelthrough accept-reject sampling. Specifically, asmall draft model auto-regressively generates textwhich is then verified by a larger target model inparallel (Stern et al., 2018; Sun et al., 2021). Thusthe large model does not need to generate text re-peatedly but rather guides the small model by cor-recting outputs when it is truly incapable. This canreduce the number of calls to the large LLM, sav-ing both time and memory. However two modelsare required, along with some similarity in theirgenerative abilities in order for this method to seesignficant speedups. While approaches exist to cir-cumvent some of these needs (Yang et al., 2023;Zhang et al., 2023; Li et al., 2024; Cai et al., 2024;Hooper et al., 2024), these are often limited by theneed for additional tuning (Liu et al., 2024b; Caiet al., 2024; Li et al., 2024), which is difficult inresource constrained settings, or quality degrada-tion in generations (Kim et al., 2023b). Because of State-Action-Reward PairsCandidate Draft ModelsOineDataset",
  "Cost Penalties": ": Overview of our methodology. We first train a policy using offline data collected from greedily decodedoutput from each model, which are scored to produce reward samples. At test time, the policy takes in a query q toselect a draft candidate model, which is then used for assisted generation with the target model. the evident size-cost tradeoff, this is very efficientif the draft model is well aligned to the target.However, while one can ensure that the finaloutput follows the target distribution (Chen et al.,2023), selecting an inadequate draft model can leadto a lack of acceleration due to the signficant num-ber of rejections that will occur. While other meth-ods allow for changes in the output distributionshift (Kim et al., 2023a; Zhou et al., 2024; Fu et al.,2024) to further speed-up inference, such types ofshifts can be problematic in many high-risk scenar-ios. From this perspective, the presence of multipledraft models, each suited for different settings, canbe helpful for inference acceleration without de-gredations in generation quality. By dynamicallychoosing a draft model, speedups can be achievedon multiple domains with marginal additional costs.However this requires learning how to choose thebest draft option given a context, introducing a de-cision making problem which needs to be solved.So how can this decision making process belearned? We start by observing this as a contextualbandits (Woodroofe, 1979; Auer, 2003), where thegoal is to have a policy select a draft model basedon a given query. This also requires rewards fromwhich the policy can learn to estimate and comparethe ideality of different actions that can be taken.To this end, we use an offline process to estimatethe contextual alignment between different draftmodels and the target model on a set of trainingexamples (), enabling the construction ofa dataset that defines the preference a target canhave towards specific draft candidates. This en-ables us to train a policy that can take into account such preferences without knowing further detailsabout the draft models. By deloying this policy atinference time it becomes possible to weigh thesepreferences, leading to speedups in generation withthe target. We further show that this policy is usefulwhen using self-speculative decoding, whereby thedraft model is a subset of the target model parame-ters. To summarize our contributions: We frame a speculative decoding scenario asa contextual bandits problem, where multipledraft models serve as arms that each produce areward, an abstraction of the inference speed-up relative to using the target model on its ownwhich is not known a priori. We demonstrate that offline training of a deci-sion making agent through only the similaritybetween the draft and target model generations,the agent can correctly select which draft modelto use for a given input query.",
  "Motivation": "Assume a large target model, Me, incurs largeend-to-end latencies that one wants to avoid. Spec-ulative decoding aims to solve the latency issue byusing a draft model to approximate the target model.However, as previously discussed, the draft modelmust be similar to the target model otherwise the sampling distribution is too different and produceno speedups. Therefore, while draft models canhelp, they are only reliable when their knowledgedistribution resembles that of the target. Accord-ingly, using only one draft model may not servewell in general if the target has multiple expertises.But by dynamically choosing between differentdraft models in any given scenario, then benefitsfrom each draft model can be observed as long asthe decision maker is competent and efficient.",
  "Problem Formulation": "When presented with a query q, selecting a draftmodel among multiple unique candidates can leadto varying performance based on the chosen option.From a contextual bandits lens, q is a context forwhich there are k arms that each returns an inde-pendent reward r. Each of arm corresponds to adifferent drafter whose reward is the time it takesto generate the output sequence through specula-tive decoding. Accordingly, each arm can producea different reward for each q. The objective thenconsists of learning a policy (|q) which, for anygiven context q, can select among the arm whichcan produce the greatest reward. From a specula-tive decoding scenario, the goal is to select the draftmodel whose abilities best align with the target forthat given query, as this will minimize the numberof times the target model must be invoked.Randomly choosing a draft model risks signif-icant increases in latency, therefore learning tomake the correct decision in a sample efficientmanner is important. While the ideal reward isthe real/observed speed-up, this can be expensiveif the aligment with draft models is unknown. Assuch, a cheaper proxy may be necessary. However,two factors have a direct effect on the true reward:1) the alignment between target and drafter and 2)the size of the drafter. This provides an alternativeway to collect policy training data: use the draftmodels auto-regressively and compute alignmentscores with the target outputs, then adjust thesebased on the size of the drafter. Next, we describehow we collect our data to train a policy offline.",
  "sji = f(oei, oji)(1)": "as a way to measure the alignment between tar-get and candidates for qi. It is further possible toincorporate a score for the inference speed. Forexample, if we consider some relative measure ofthe inference speed for the specific drafter to be cji,then one can adjust the score as a weighted sum",
  "Decision Making": "With the offline dataset, it becomes possible totrain a policy which can independently act on acontext by choosing a drafter to use with the target.We consider each (qi, j, sji) as state-action-rewardtuples used to train .Within the contextual bandits reformulation,each query-action pair (qt, at) Q A(qt) isthe drafter which produced an observed rewardr(qt, at). Here, we use the score sji directly asthe reward, as it acts as an estimate for the effec-tiveness of drafter j on the context. The policy isrepresented by a mapping (a|q) from Q A toR and we want to find parameters that maximize",
  "Greedy 9.761.09 (28.560.16 ms/token)37.7229.831.17 (31.630.19 ms/token)37.761.13Dynamic 9.621.07 (29.200.17 ms/token)36.6429.451.16 (31.880.17ms/token)37.341.11": ": Quality, decoding speeds and acceptance rates when using a policy for selecting between different draftmodels of the same size but specialized on different domains. Across the two domains, both a greedy and dynamicpolicy can accelerate decoding on both domains, with marginal differences between the exact nature of the policy.Acceptance rate computation is described in Appendix A. varying the draft options along different axes suchas alignment with the target model, sizes of thedraft models, architecture of the drafter/target andthe level of independence between the draft andtarget models. Each of these forms a dedicatedexperiment detailed in the sections that follow. Data Collection.For each experiment, we col-lect offline data using task-specific training datasetsplits. Each model is used to generate a greedy de-coded sample from each, which is used to constructa reward dataset. To score samples against the tar-get model output, we use the ROUGE-L score. Policy Training.To train our policy, the inputis a sentence embedding of the query from thetarget model and the output is a distribution overthe drafting candidates. We train on the offlinedataset for 3 epochs using a fixed batch size of 64and AdamW (Loshchilov and Hutter, 2019) witha learning rate of 1e-3 and weight decay 1e-2. Allother hyperparameters are set to their default valuesin PyTorch. In all experiments, our policy consistsof a 3 layer multi-layer perceptron. Hidden layershave a fixed dimensions of 512 with a tanh activa-tion function. The input dimension is the hiddendimension size of the target model and the outputsize is the number of drafting options. Inference.We sample using a temperature T of 1and draft tokens set at 7. We use both a policy thattakes the greedy action and another that samplesfrom the output distribution.1 The policy takes ina sentence embedding of the query and returns a",
  "Results": "Learning to choose the draft model.For ourfirst experiment, we use a T5 (Raffel et al.,2020) encoder-decoder models.As the target,we use an instruction-finetuned (Wei et al., 2022)Flan-T5-XXL (Chung et al., 2022) while our draftcandidates are publicly available T5-Small mod-els, one the base version and another fine-tuned ontext summarization2. We evaluate on translation(IWSLT2017 EN-DE (Cettolo et al., 2017)) andtext summarization (XSUM (Narayan et al., 2018)). compares when draft models of the samesize vary in their domain of expertise. While eachmodel accelerates generation non-trivially withintheir knowledge domain (EN-DE for T5-Small andXSUM for T5-Small-XSum), they are largely un-helpful or detrimental when used outside their do-main of expertise, as seen with the 1% slowdownfrom T5-Small on XSUM and a 17% decrease us-ing T5-Small-XSum on EN-DE. In comparison, thepolicy ensures acceleration within both domainswith neglibile latency from decision making.This highlights some immediate benefits of pol-icy use, namely that it can identify the correctdraft model for a context without any explicit infor-mation regarding the draft candidates themselves.Rather, generating sampling outputs from eachdraft model and the target individually is sufficientto develop a general ability to differentiate betweendomains through the use of the computed rewards.",
  "Policy with tradeo Flan-T5-Small (80M)Flan-T5-Base (250M)": ": Effect of varying the tradeoff between output alignment and draft model size (controlled through ). Eachcompares the use of Flan-T5-Small as a draft model (red horizontal line). As increases, the model increasinglyuses the smallest draft model for decoding, demonstrating that the offline dataset is sufficient to learn how to balancethe quality of the draft models outputs and the cost of using it. All cases use speculative sampling/decoding.",
  ": Speeds of different draft models on XSUM witha Flan-T5-XXL model expert (averaged over 5 seeds).Observed decoding speed varies as an effect of draftersize and alignment with the expert": "Balancing quality and speed.It is also impor-tant that the draft model is sufficiently inexpensiveto use relative to the target model. This motivatesour second experiment, which is evaluated only onXSUM, but compares draft candidates that vary interms of size and target model alignment. Multi-ple draft models are compared: a Flan-T5-Small(80M parameters), the same T5-Small (60M) mod-els mentioned above, and Flan-T5-Base (220M). shows the speed-ups earned through spec-ulative decoding using these different draft candi-dates and a Flan-T5-XXL target. Although larger draft models may be better aligned with the targetcompared to smaller options, using them incurs alatency that can end up being less efficient.Balancing alignment and efficiency is thereforean issue to consider when deciding between can-didates. shows how offline training canhelp accomplish this. Setting to vary between theobjectives defined in 2.3, where we use fixed in-ference costs based on the size of the draft models,we observe how a dynamic policy can eventuallyadapt to the preferences set by the choice of .For example, as approaches 1, the policy placesincreasing preference on the smallest draft modelregardless of quality. Meanwhile 0 showsincreasing preference towards the model that hasgreatest alignment with the target generations.This demonstrates the general flexibility that cancome with using such a weighting scheme of differ-ent rewards, while demonstrating that even simplerproxies for the inference penalty are sufficient toproperly balance the two.",
  "How many examples need to be learned to dif-ferentiate?It is further necessary to consider": "the number of examples that are needed for thedecision maker to properly learn to differentiatebetween different examples. To this end, we in-vestigate how quickly the policy can learn to usethe annotated scores within the offline dataset todemonstrate a visible speed-up improvement. Were-use our models from the first experiment, butkeep track of the decoding speed as the number ofexamples used to train our policy increases.As we can observe in , learning to selectthe correct model occurs rather quickly, as train-ing for fewer than a total of 10000 examples issufficient to attain a level of performance that isequivalent to training on the entire offline dataset,which consists of nearly 400 thousand examples.This result demonstrates the general efficiency ofthis method, as collecting and training the policyon outputs from a minimal amount of examplesshows the ability to generalize quite strongly. Auto-regressive generation as an option.Sce-narios exist where the draft models will not be use-ful, in which case using the target auto-regressivelyremains the most reasonable option.To this end, we attempt to observe how provid-ing this option to the decision maker can affect ourprevious experiments. We repeat the same exper-iment from but allow our policy to learnto choose to generate auto-regressively. To avoidtrivially perfect matching of outputs, we sampleoutputs from the target model and score againstthe greedy output. Due to the large size of the tar-get compared to the drafters, we use = 0.5 tobalance the size and quality scores.",
  ": Decoding speeds on GSM8K (test set) with aFlan-T5-XXL expert. Inference on the latter two tasksis negligibly different from": "We further verify whether the policy can ignoredraft models when they are not useful. We exper-iment by including GSM8K to our tasks, whichonly the target is aligned. Since neither draft modelcan accelerate inference on this task, the policyshould ideally avoid drafting for examples fromthis setting. Since GSM8K is significantly smallerthan EN-DE and XSUM, we reduce the number ofexamples to match all datasets in terms of size. shows auto-regressive generation to (un-surprisingly) outperform assisted generation. How-ever, using a policy shows comparable speed toauto-regressive generation, indicating that it learnsto ignore the draft models due to the stark contrastin the greedy outputs from each model. Generalization to Multi-Task Drafters.Todemonstrate the applicability of this method tomore general settings, in particular cases wherethe draft models may be competent at multipletasks, we further apply our policy-based selec-tion method to SpecBench (Xia et al., 2024) us-ing a Vicuna-33B (Chiang et al., 2023) target withsmaller draft models (). Given the size ofSpecBench (480 examples, divided equally into6 tasks), we use this exclusively as a test-set. Totrain our policy, we use the original task datasetsfrom which SpecBench examples were extractedand sample even amounts of examples from each(2000). For MT-BENCH, there are only 80 totalexamples which are all included in the test set butwhich we sample with replacement to use for atraining set. Accordingly, results on this task maybe over-confident. Because Vicuna models aredecoder-only Transformers, we adjust the sentencerepresentation to be the final hidden representationof the input sequence. Our results show that ourinitial findings from a T5 architecture hold, sug-gesting that such a policy-based training method isboth robust and generalizable to different settings.",
  "(1) Share a vocabulary with the target.(2) Align with the target on the tasks of interest": "Such models can be difficult to obtain, leading toself-drafting (Yang et al., 2023; Li et al., 2024;Hooper et al., 2024), where the draft model existswithin the target. To explore the differences withthis setting, we conduct an additional ablation.We use a LLaMA-2-13B-Chat model (Touvronet al., 2023) using early exits, following (Kave-hzadeh et al., 2024) with the use of a single lan-guage modeling head for all exits. While othermethods exist, these can possess a combinatorialnumber of potential draft options and necessitatepre-determined path flows during inference (Zhanget al., 2023). Meanwhile, methods that use addi-tional language modeling heads for parallel decod-ing require additional parameters which can bothbecome irreconcilable with resource constraintsor degrade generation quality (Cai et al., 2024).Results on ALPACA and TRIVIAQA, conductedon each dataset independently, under this setup() show that although intermediate layer drafting results in a decrease in decoding speed,using a policy can minimize performance loss inparticular with the presence of an auto-regressiveoption, highlighting that the proposed offline policylearning approach has potential for self-drafting aswell. This demonstrates the use of a policy remainsa useful manner to fall back to the most effectivedecoding options. Furthermore, when consideringthe case where the auto-regressive option is notavailable, we note that the policy methods are ca-pable of recovering to a performance similar to thebest case intermediate drafter on ALPACA. Whilethis is not the case with TRIVIAQA, this is perhapsattributed to the short answers within this dataset.",
  "Discussion": "LLM Routing.LLMs have demonstrated re-markable capabilities across a range of tasks, butthere exists wide variation in their costs and capa-bilities. Very broadly, more capable models tendto be more expensive than less capable models.This leads to a dilemma when deploying LLMs inthe real-world - routing all queries to the largest,most capable model leads to the highest-quality re-sponses but can be expensive, while routing queriesto smaller models can save costs but may result inlower-quality responses. Similarly, not all modelsmay be well suited for the same set of tasks, mean-ing that routing to the most suitable model can beof great importance as well.Our work shares a great deal of similarity withthis notion of model routing, or selecting the bestmodel based on the query. In particular, the setof draft models can be considered to be a groupof sub-networks, similar to a Mixture-of-Experts(MoE) (Shazeer et al., 2017) style paradigm. Thepolicy meanwhile acts as a router to the correct sub-network. More advanced routing techniques (Feduset al., 2021; Ong et al., 2024) have been explored asa way to leverage the multitude of LLMs that exist",
  "in the wild, but have yet to be widely used withindownstream settings such as speculative decoding": "Adaptive Speculative Decoding.Speculativedecoding methods require the use of many pre-defined hyper-parameters which can signficantlyinfluence acceleration, with even minor changeshaving noticable effects. Recent work has begunto explore how to decouple this process, such asby dynamically selecting the number of draftingtokens to generate at each decoding step (Wanget al., 2024; Liu et al., 2024a). Kavehzadeh et al.(2024) further discussed dynamically selecting amodel per instance, however their method is limitedto their specific setup due to needing to computeconfidence scores after generation at early exits.While we do not introduce a new decoding algo-rithm, we make a first attempt to make the specula-tive decoding adaptive through the ability to switchbetween multiple draft models based on the input.However, more complex levels of adaptivity maybe necessary as each decoding step may not be thesame, necessitating perhaps a need to carefully ad-just different hyperparameters through the processin order to maximize acceleration. Decision Making for Assisted Decoding.As-sisted decoding can require making multiple deci-sions. One of these is determining an ideal numberof draft tokens to decode at each step. Another re-lates to how to reject tokens, which commonly useseither greedy (Xia et al., 2023) or sampling-basedtoken-matching heuristics (Leviathan et al., 2023).However, there are trade-offs when enforcing spe-cific choices, which requires further investigationto better understand how to tune such techniques.This work proposes adding an additional de-cision at the beginning of the decoding process,namely at the beginning of the process under theassumption that multiple drafting options exist.While we limit ourselves to make a more completeanalysis within a more self-contained setting, vari-ous ways to have these methods co-exist within onelarger pipeline are possible. However such workis left for future exploration due to the non-trivialnature of understanding how different choices andeffect overall reported results in conjunction. Measuring Alignment Between Outputs.Weobserve that token-level similarity scores are effec-tive for training the decision maker, which can beattributed to the fact that assisted decoding itselfrelies on matching the token-level distribution of outputs. As such, if the greedy-decoded outputfrom a draft model highly resembles the target out-put, it follows that this will be represented by ahigher degree of similarity between the probabiltydistributions in the logit space, which can then leadto fewer rejections when sampling.However, such metrics have limitations (Deutschet al., 2022) due to capturing primarily superficialelements of text, where marginal differences indistribution have large effects on the output text.Furthermore, different metrics may overfit specifictasks, necessitating the need for better measures ofdraft/target alignment, which can hopefully lead tobetter estimation of rewards for training improvedpolicies, either by desigining better metrics them-selves or by learning to compare features at dif-ferent levels of granularity (ex. target and draftlogits against text outputs). Additionally, semanticmeaning also can play an important role, as outputswith signficant structure may still possess the samemeaning, something that token-level similarity met-rics will not adequately capture. Speculative Decoding as Approximate Inference.Speculative decoding can be analogized as a formof approximate inference where due to the in-tractability of performing inference with a model ofinterest, approximation methods are used to learnan estimate of the model. While training the draftmodel is equivalent to performing variational infer-ence (i.e. approximating an intractable distributionwith a surrogate), this can be expensive. Accord-ingly, training only a policy can be seen as weigh-ing a set of fixed distributions to act as a bettersurrogate for the target model.Some works have further attempted to studyspeculative decoding from this angle. In particular,Zhou et al. (2024) explore such a process by build-ing a draft model through the use of KL-divergencelosses, effectively building a posterior distributionof the target model based on likelihood informationfrom the draft output. Liu et al. (2024b) meanwhileexplore the same technique as the distribution ofexamples changes, building a draft model that canadapt to changing user inputs. Such settings alsocould perhaps benefit from multiple draft models,where conditioning on the query can enable moreeffective adaptation of draft models to better gener-alize to unseen settings.",
  "Hosting Multiple Draft Models.An importantaspect of this method relates to the need to host mul-tiple draft models in conjunction with the expert": "This can incurr additional costs, in particular if theexpert and selected drafter do not reside in the samedevice. While methods such as self-drafting avoidthis issue and the possibility to create minimally-sized drafters generally alleviates the concern ofexcessive memory usage, one particular aspect ofconsideration remains hardware level optimizationswhich can best enable for the selected drafters tobe loaded at maximal speed, avoiding additional la-tency that can result from the bandwidth constraintsthat relate to data transfer between devices.",
  "Conclusion": "This work presents the first work at attempting tointegrate assisted generation within a setting wheremultiple black-box draft candidates exist. Whenno a-priori knowledge of which draft candidate isbest suited for assisting the decoding of a givenexample, the problem can be modeled as a contex-tual bandits problem, where the goal is to estimatethe unknown reward from each drafting option.Our work demonstrates that offline RL presentsan efficient method for learning to distinguish theavailable options and provide accelerated decodingacross various examples within this setting, with alogical way to collect offline data from models forlearning. Our results and ablations show that learn-ing a policy with this approach can adapt to generalpreferences while accounting for more complexaspects of the decision making, highlighting its ro-bustness. Furthermore, such a method is scalableand robust to the introduction of more draft modelsor the removal of draft models, presenting a viablealternative to settings where a uniquely superiordraft model may be unavailable.Nevertheless, areas of further development exist.For example, learning in an online fashion mayrender this method more broadly applicable. Al-ternatively, exploring how to dynamically choosedrafters at every decoding step rather than per ex-ample, as well as combining this direction of workwith that which attempts to adaptively choose thespeculation length at every step, are feasible waysof combining our findings with concurrent work inthe hopes of reaping the benefits of all methods.",
  "Additional storage and memory": "The usage of multiple models that draft indepen-dently requires additional memory, which can bebe more difficult to manage when there are explicitconstraints on this front (self-drafting avoids thisdue to the use of a single model). Furthermore,collecting an offline dataset can be difficult in somespecific scenarios where inference is burdensome,for example when input/output sequences are verylong, or when many offline examples are required.",
  "Self-Drafting": "We work on a setting where we do not conduct anyadditional training of parameters that are explicitlylinked to the language model itself, whether theyare existing parameters or new paramters added asa result of the method. While there are ways inwhich our explored method can be applied to theseas well, computational limitations make it difficultto rigorously conduct such studies at the momentand we leave it to future work for this reason.",
  "Ethics Statement": "This paper discusses the concept of dynamicallychoosing between of multiple black-box draft mod-els for speculative decoding, proposing an offlinereinforcement learning approach for adaptively se-lecting a good draft model for assistance. Our re-sults are relate to the decoding speed of models,which is unlikely to lead to ethical concerns orproblematic interpretations of such results.",
  "Acknowledgements": "Jerry Huang recieved financial support under theform of a National Science and Engineering Re-search Council (NSERC) Canada Graduate Schol-arship, a Fonds de Recherche du Qubec Nature ettechnologies (FRQNT) Training Scholarship and aBourse dExcellence Hydro-Qubec. Sarath Chan-dar is supported by a Canada CIFAR AI Chair,the Canada Research Chair in Lifelong MachineLearning and a NSERC Discovery Grant.",
  "Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,Jason D. Lee, Deming Chen, and Tri Dao. 2024.Medusa: Simple llm inference acceleration frame-work with multiple decoding heads": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli,Jan Niehues, Sebastian Stker, Katsuhito Sudoh,Koichiro Yoshino, and Christian Federmann. 2017.Overview of the IWSLT 2017 evaluation campaign.In Proceedings of the 14th International Conferenceon Spoken Language Translation, pages 214, Tokyo,Japan. International Workshop on Spoken LanguageTranslation. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. 2023. Accelerating large language modeldecoding with speculative sampling.Preprint,arXiv:2302.01318. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. Preprint, arXiv:2204.02311. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,",
  "Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. 2022. GPT3.int8(): 8-bit matrix mul-tiplication for transformers at scale. In Advances inNeural Information Processing Systems": "Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. Onthe limitations of reference-free evaluations of gen-erated text. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1096010977, Abu Dhabi, United ArabEmirates. Association for Computational Linguistics. William Fedus, Barret Zoph, and Noam M. Shazeer.2021. Switch transformers: Scaling to trillion pa-rameter models with simple and efficient sparsity.Journal of Machine Learning Research, 23:120:1120:39.",
  "ColemanHooper,SehoonKim,HivaMoham-madzadeh, Hasan Genc, Kurt Keutzer, Amir Gho-lami, and Sophia Shao. 2024. Speed: Speculativepipelined execution for efficient decoding. Preprint,arXiv:2310.12072": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361. Parsa Kavehzadeh, Mojtaba Valipour, Marzieh Tahaei,Ali Ghodsi,Boxing Chen,and Mehdi Reza-gholizadeh. 2024. Sorted LLaMA: Unlocking thepotential of intermediate layers of large languagemodels for dynamic inference. In Findings of the As-sociation for Computational Linguistics: EACL 2024,pages 21292145, St. Julians, Malta. Associationfor Computational Linguistics. Sehoon Kim, Coleman Hooper, Thanakul Wattanawong,Minwoo Kang, Ruohan Yan, Hasan Genc, GraceDinh, Qijing Huang, Kurt Keutzer, Michael W. Ma-honey, Sophia Shao, and Amir Gholami. 2023a. Fullstack optimization of transformer inference. In Archi-tecture and System Support for Transformer Models(ASSYST @ISCA 2023). Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Ji-tendra Malik, Michael W. Mahoney, Amir Gholami,and Kurt Keutzer. 2023b. Speculative decoding withbig little decoder. In Thirty-seventh Conference onNeural Information Processing Systems.",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata.2018. Dont give me the details, just the summary!topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 17971807, Brussels, Bel-gium. Association for Computational Linguistics. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chi-ang, Tianhao Wu, Joseph E. Gonzalez, M WaleedKadous, and Ion Stoica. 2024. Routellm: Learn-ing to route llms with preference data.Preprint,arXiv:2406.18665.",
  "OpenAI. 2024.Gpt-4 technical report.Preprint,arXiv:2303.08774": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, AlexRay, John Schulman, Jacob Hilton, Fraser Kelton,Luke E. Miller, Maddie Simens, Amanda Askell, Pe-ter Welinder, Paul Francis Christiano, Jan Leike, andRyan J. Lowe. 2022. Training language models tofollow instructions with human feedback.ArXiv,abs/2203.02155. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167.",
  "Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang. 2021": "Instantaneous grammatical error correction with shal-low aggressive decoding. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 59375947, Online. Associationfor Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Preprint,arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc. Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu,Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng,Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, andDepei Qian. 2024. Minions: Accelerating large lan-guage model inference with adaptive and collectivespeculative decoding. Preprint, arXiv:2402.15678. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V Le. 2022. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Preprint,arXiv:2201.11903.",
  "Michael Woodroofe. 1979. A one-armed bandit prob-lem with a concomitant variable.Journal of theAmerican Statistical Association, 74(368):799806": "Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, FuruWei, and Zhifang Sui. 2023.Speculative decod-ing: Exploiting speculative execution for accelerat-ing seq2seq generation. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 39093925, Singapore. Association for Com-putational Linguistics. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhi-fang Sui. 2024. Unlocking efficiency in large lan-guage model inference: A comprehensive survey ofspeculative decoding. Preprint, arXiv:2401.07851.",
  "Seongjun Yang, Gibbeum Lee, Jaewoong Cho, DimitrisPapailiopoulos, and Kangwook Lee. 2023. Predictivepipelined decoding: A compute-latency trade-off forexact llm decoding. Preprint, arXiv:2307.05908": "Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,Gang Chen, and Sharad Mehrotra. 2023.Draft& verify:Lossless large language model accel-eration via self-speculative decoding.Preprint,arXiv:2309.08168. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,Aditya Krishna Menon, Afshin Rostamizadeh, SanjivKumar, Jean-Franois Kagy, and Rishabh Agarwal.2024. Distillspec: Improving speculative decodingvia knowledge distillation. In The Twelfth Interna-tional Conference on Learning Representations.",
  "A.1Baselines": "To baseline and compare our architectural con-straints against (Leviathan et al., 2023), we par-tially benchmark our experiments against theirs.These are presented in and 8. We con-duct this as we suspect a difference in both systemarchitechture used for experiments as well as forimplementation of models.We observe that their results are generally showspeedups that are consistently 2.5 to 3.0 as largeas ours, with minor deviations. We attribute thisto the usage of different computational resourcesand potential implementation differences. Addi-tionally, given the small amount of variation in therelative differences between the observed and re-ported speedups, we contend that these differencesare not due to errors in implementation.",
  "A.6Accept Rate Computation": "To compute the accept rate of tokens, we define thenumber of generated tokens in a given draft asthe total number of tokens generated by the draftmodel (this is equivalent to ). The number ofaccepted tokens in a given draft is the number ofgenerated tokens that are validated as correct bythe target model. When a token is rejected within adraft, all subsequent tokens are considered rejectedas well. The accept rate is then the quotient of thetotal number of accepted tokens divided by the totalnumber of generated tokens.",
  "A.7Computing Wall-Clock Performance": "To compute the wall-clock time when using a pol-icy, we include the amount of time used to infer onthe policy. However, we do not include the timeneeded to generate the sentence representation.This is because upon generating the original sen-tence representation, the large models KV cachecan be updated to store these for the future verifi-cation passes, meaning that they do not need to berecomputed again in the future. As such, we treatthis initial pass through as being part of the firstverification pass.Additionally, one could theoretically save on thepolicy inference by performing batched inferenceon many examples at once. However, this is notparticularly applicable in practice, where differentinputs arrive at different times. As such, we treateach example individually and include these timeswithin the per-example speeds."
}