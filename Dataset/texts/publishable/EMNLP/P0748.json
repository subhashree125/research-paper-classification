{
  "Abstract": "Analogical reasoning plays a critical role inhuman cognition, enabling us to understandnew concepts by associating them with familiarones. Previous research in the AI communityhas mainly focused on identifying and gener-ating analogies and then examining their qual-ity under human evaluation, which overlooksthe practical application of these analogies inreal-world settings. Inspired by the human ed-ucation process, in this paper, we propose toinvestigate how analogies created by teacherlanguage models (LMs) can assist student LMsin understanding scientific concepts, therebyaligning more closely with practical scenarios.Our results suggest that free-form analogies canindeed aid LMs in understanding concepts. Ad-ditionally, analogies generated by student LMscan improve their own performance on scien-tific question answering, demonstrating theircapability to use analogies for self-learningnew knowledge. Resources are available at",
  "Introduction": "Analogy plays a crucial role in human cognition,facilitating the understanding of complex and un-familiar concepts by relating them to familiarones (Bunge, 1981; Glynn et al., 1989; Hofstadter,2001; Bartha, 2013). For example, illus-trates how using the solar system as an analogy canenhance understanding of the complex structureof atoms. Given its significant value across vari-ous fields, including creativity (Kang et al., 2022)and education (Richland and Simms, 2015; Tha-gard, 1992), the topic of analogy has been drawingsignificant research attention in the AI community.Traditional research on analogy primarily fo-cuses on evaluating (Allen and Hospedales, 2019;Schluter, 2018; Czinczoll et al., 2022; Chen et al.,2022) and enhancing (Ushio et al., 2021; Yuan",
  "LMs": ": An example of the SCUA task. Given a sci-entific concept (i.e., Atom), we ask teacher LMs togenerate an analogy to explain the concept and thenlet student LMs answer the related scientific questionsaround this concept, both with and without the aid ofthe generated analogy. et al., 2024) the analogical reasoning capabili-ties of language models (LMs) in word analogies(e.g., king is to man as queen is to woman).Recent advancements in large language models(LLMs) (OpenAI, 2022, 2023) have shifted thisfocus from simple word analogies to exploringanalogies between more complex situations suchas systems (Yuan et al., 2023), processes (?Sultanand Shahaf, 2022; Ding et al., 2023; Sultan et al.,2024), paragraphs (Webb et al., 2022; Wijesiriwar-dene et al., 2023), and stories (Jiayang et al., 2023).However, these studies mainly examine whetherLLMs can generate appropriate analogies underhuman evaluation without thoroughly assessing thepractical functionality of the generated analogiesin real-world scenarios.In this paper, drawing on principles of humaneducation, we propose the SCUA, i.e., ScientificConcept Understanding with Analogy task, whichaims to investigate whether analogies generatedby teacher LMs can assist student LMs in under-standing scientific concepts. Specifically, as shownin , given a scientific concept, we ini-tially prompt teacher LMs, (e.g., GPT-4 (OpenAI, 2023) and Claude (Anthropic, 2024)), to generatean analogy that explains the concept. Then, wecollect related scientific questions around this con-cept from the database and let student LMs (e.g.,GPT-3.5 (OpenAI, 2022) and Vicuna (Chiang et al.,2023)) attempt to answer these questions, with andwithout the use of the generated analogy.Under this setting, we conduct extensive exper-iments to evaluate strong and weak LMs with dif-ferent analogy types. The main findings are asfollows:",
  "Related Work": "Analogical ReasoningAnalogical reasoning haslong interested the AI community (Davies, 1985;Gentner and Forbus, 2011; Mitchell, 2021). Tradi-tional research has focused on word analogies, ex-amining linear relationships between words (Glad-kova et al., 2016; Schluter, 2018; Fournier et al.,2020; Ushio et al., 2021). With the development ofLLMs (OpenAI, 2022, 2023; Team and Google,2023), there has been a shift toward exploringanalogies between situations, establishing map-pings between concepts across two domains basedon shared relational structures (Sultan and Sha-haf, 2022; Ding et al., 2023; Jiayang et al., 2023;Sultan et al., 2024). Compared to these studies,our research is the first to explore how analogiesgenerated by teacher LMs can aid student LMs inunderstanding scientific concepts, which is morealigned with real-world scenarios. Explanation GenerationWith the rising capabil-ities of LLMs, prior research has adopted methods,e.g., Chain of Thought (CoT) (Wei et al., 2022;Zhang et al., 2023), to generate a reasoning processbefore answering. Due to the relatively limited capabilities of smaller LMs, some studies employknowledge distillation, which involves generatingreasoning samples using larger LMs to instructsmaller models (Wang et al., 2023a; Hsieh et al.,2023; Wang et al., 2023b; Lin et al., 2023; He et al.,2024). Compared to these studies, our work is thefirst to explore explanations with analogical reason-ing in understanding scientific concepts.",
  "Task Formulation": "As illustrated in , given a scientific conceptC, we initially ask teacher LMs to generate analo-gies CA to explain this concept. Then we will givea scientific question Q and m candidate answerA = {Ai}mi=1, which is related to the scientificconcept C. The ultimate goal of student LMs is tomake the correct choice Y for X = (Q, A, CA).",
  "Word Analogy: We adopt the format from Chenet al. (2022) in generating word analogies (A isto B as C is to D)": "Structured Analogy: Structured analogies orig-inate from the Structure Mapping Theory (Gen-tner and Markman, 1997), which posits thatanalogies are formed by identifying common re-lational structures between two concepts. Thus,in addition to using one concept to explain an-other, we also ask the LMs to incorporate relatedconcepts to demonstrate the analogy further. Free-form Analogy: These analogies utilize un-structured natural language to explain one con-cept through another. The popularity of this typeis increasing with advancements in LLMs (Wije-siriwardene et al., 2023; Ye et al., 2024).",
  "Free-Form Analogy": "Imagine a group of children, each holding a different number of balloons and standing in a room. Overtime, they start trading balloons to balance out their amounts until each child is holding roughly the samenumber. Thermal equilibrium works similarly with temperature. If you place a hot object and a coldobject close together, heat (like the balloons) will transfer from the hot object to the cold one until both...",
  "Scientific QA for Student LMs": "In the field of human education, a teacher typicallyintroduces a concept to the class and often usesan analogy to clarify the concept (Thagard, 1992;Heywood, 2002; Gray and Holyoak, 2021). Forexample, when explaining the concept of a cell,drawing an analogy to an automobile factory en-hances the understanding, e.g., mitochondria arepowerhouses. Such analogies help students graspthe concept of a cell, enabling them to correctlyanswer related questions on homework quizzes. Toalign with this, in SCUA task, given a concept withits analogy generated by teacher LMs, we ask thestudent LMs to answer questions related to theconcept. The details of the prompt templates areavailable in Appendix C.2.",
  ": The performance of different student LMsunder different types of analogies generated by GPT-4": "RQ1: Can Analogy from Teacher Models Em-power Student Models?We adopt Zero-shotPrompting (Direct) and Chain-of-Thought Prompt-ing (CoT) (Wei et al., 2022) as baselines.3 Theresults in indicate that: 1) Free-form analo-gies can indeed help student LMs understand sci-entific concepts better than Zero-shot and CoTPrompting, improving their ability to answer sci-entific questions. 2) The analogies generated byGPT-4 improve the ability of student LMs mostsignificantly, indicating the potential of GPT-4 toassist weaker LMs in learning new knowledge. 3)For the GPQA dataset, characterized by specializedconcepts and difficult scientific questions, Vicuna-7B and Vicuna-13B perform poorly with Zero-shotand CoT Prompting. However, with analogies, theirperformance is effectively enhanced. This findinginspires future work to explore using analogies tohelp the model learn new concepts. RQ2: Which Type of Analogy Can Better Em-power Student Models?Apart from free-formanalogy, we also expect to examine two other anal-ogy types, i.e., structured analogy and word anal-ogy, focusing on their effectiveness in aiding stu-dent LMs to grasp scientific concepts. As shown in",
  ":Comparisonofself-generatedanalo-gies (AnalogySelf) and GPT-4 generated analogies(AnalogyGPT-4) for the performance of student LMs onARC dataset": ", advanced language models such as GPT-4 and Claude-v3-Sonnet and open-source mod-els like Mixtral-8x7B are proficient in generatinghigh-quality word analogies for scientific concepts.However, the generation quality significantly di-minishes for free-form and structured analogies,especially in professional fields (e.g., wettabil-ity and contact angle hysteresis in the GPQAdataset).In comparison, reveals that compared toword analogy, free-form and structured analogiesare more effective in helping models understandscientific concepts due to their more informativecontent. Future studies can consider strategies thatinitially have models generate high-quality wordanalogies, and then expand them into structuredand free-form analogies to enhance their quality. RQ3: How About Self-generated Analogy?Inaddition to using analogies generated by teacherLMs, we also ask student LMs to generate analo-gies to help themselves understand scientific con-cepts and answer related questions. As shown in Ta-ble 4, compared to CoT prompting, self-generatedanalogies can improve the models understandingof scientific concepts and enhance its ability to an-swer related questions. Moreover, for some models,self-generated analogies outperform those gener-ated by GPT-4, indicating their ability to use analo-gies to self-learn new knowledge.",
  "Conclusion": "In this paper, we propose the SCUA task, whichsimulates the human education process to explorehow analogies created by teacher LMs can helpstudent LMs understand scientific concepts. Ourresults suggest that free-form analogies indeed aidLMs in comprehending concepts and enhance theirability to answer related scientific questions accu-rately. Additionally, analogies generated by student",
  "Limitations": "First, this paper only considers scientific concepts.We do not cover concepts in other fields, such ashistorical events and social concepts. Second, someprevious work (Saha et al., 2023) uses explanationsgenerated by stronger LMs to help weaker LMs.However, we argue that models may have differ-ent strengths in different tasks. Therefore, we dis-tinguish between teacher LMs and student LMswithout fully evaluating their capabilities. Futurework can explore this perspective. Additionally,our evaluation is limited to multiple-choice tasks.Investigating the performance on more complextasks, such as RAG, would be beneficial.",
  "We hereby acknowledge that all authors of thiswork are aware of the provided EMNLP Code ofEthics and honor the code of conduct": "Use of Human AnnotationsEvaluation on thegenerated analogies from stronger LMs in SCUA isimplemented by three annotators recruited by ourinstitution. The construction team remains anony-mous to the authors. We ensure that the privacyrights of all annotators are respected throughoutthe annotation process. All annotators are compen-sated above the local minimum wage and consentto the use of SCUA for research purposes, as de-scribed in our paper. The annotation details areshown in Appendix A. RisksThe datasets we conduct in the experimentare sourced from publicly available sources, i.e.,ARC Challenge Set and GPQA. However, we can-not guarantee they are free of socially harmful ortoxic language. Additionally, analogy evaluationrelies on commonsense, and different individualswith diverse backgrounds may have varying per-spectives. We use ChatGPT to correct grammaticalerrors in this paper.",
  "Mario Bunge. 1981. Analogy between systems. Inter-national Journal Of General System, 7(4):221223": "Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, ZhongqiaoLi, Xinbo Zhang, Changzhi Sun, Lei Li, YanghuaXiao, and Hao Zhou. 2022. E-KAR: A benchmarkfor rationalizing natural language analogical reason-ing. In Findings of the Association for Computa-tional Linguistics: ACL 2022, pages 39413955,Dublin, Ireland. Association for Computational Lin-guistics. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023). Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Tamara Czinczoll, Helen Yannakoudakis, PushkarMishra, and Ekaterina Shutova. 2022.Scientificand creative analogies in pretrained language mod-els. In Findings of the Association for ComputationalLinguistics: EMNLP 2022, pages 20942100, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics.",
  "Douglas R Hofstadter. 2001. Analogy as the core ofcognition. The analogical mind: Perspectives fromcognitive science, pages 499538": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, RanjayKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-tilling step-by-step! outperforming larger languagemodels with less training data and smaller modelsizes. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 80038017,Toronto, Canada. Association for Computational Lin-guistics. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang,Weiqi Wang, Chunkit Chan, Dongyu Ru, QipengGuo, Hongming Zhang, Yangqiu Song, Yue Zhang,and Zheng Zhang. 2023. StoryAnalogy: Derivingstory-level analogies from large language models tounlock analogical understanding. In Proceedingsof the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1151811537,Singapore. Association for Computational Linguis-tics.",
  "Paul Thagard. 1992.Analogy, explanation, and ed-ucation. Journal of research in science teaching,29(6):537544": "Asahi Ushio, Luis Espinosa Anke, Steven Schockaert,and Jose Camacho-Collados. 2021. BERT is to NLPwhat AlexNet is to CV: Can pre-trained languagemodels identify analogies?In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 36093624, Online. As-sociation for Computational Linguistics. PeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,and Xiang Ren. 2023a. PINTO: Faithful languagereasoning using prompt-generated rationales. In TheEleventh International Conference on Learning Rep-resentations. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao,Bing Yin, and Xiang Ren. 2023b. SCOTT: Self-consistent chain-of-thought distillation. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 55465558, Toronto, Canada. Association forComputational Linguistics.",
  "Taylor Webb, Keith J Holyoak, and Hongjing Lu. 2022.Emergent analogical reasoning in large languagemodels. arXiv preprint arXiv:2212.09196": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,and Denny Zhou. 2022. Chain of thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems. Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bi-mal Gajera, Shreeyash Gowaikar, Chandan Gupta,Aman Chadha, Aishwarya Naresh Reganti, AmitSheth, and Amitava Das. 2023. ANALOGICAL -a novel benchmark for long text analogy evaluationin large language models. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 35343549, Toronto, Canada. Association forComputational Linguistics. Xiao Ye, Andrew Wang, Jacob Choi, Yining Lu, ShreyaSharma, Lingfeng Shen, Vijay Tiyyala, Nicholas An-drews, and Daniel Khashabi. 2024.Analobench:Benchmarking the identification of abstract and long-context analogies. arXiv preprint arXiv:2402.12370. Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao,and Deqing Yang. 2023. Beneath surface similarity:Large language models make reasonable scientificanalogies after structure abduction. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 24462460, Singapore. Association forComputational Linguistics.",
  "ACrowd-sourcing Details": "We have recruited a team of three undergraduates.To process conflicting annotations, we adopt a vot-ing majority principle to determine the results. Wepay each annotator $8/h, exceeding the local mini-mum wage. The screenshots of the instructions andinterface for quality check of the extracted conceptsand generated analogies annotation are shown in and .",
  "temperature difference between the two": "- the milk is hot , and the mug isrelatively cold. But if you wait a fewminutes before taking a sip , you 'llnotice that the mug has warmed up, andthe milk has cooled down a bit. This isbecause heat has transferred from thehot milk to the cooler mug until they 've",
  "reached a point where they 're the same": "temperature. This is thermal equilibrium.Just like the hot milk and the mug ,when two objects at differenttemperatures come into contact , heatwill always flow from the hotter objectto the cooler one. This continues untilthey reach thermal equilibrium , or thesame temperature. Once they 're at thesame temperature , there 's no more heatflow because there 's no temperaturedifference to drive it.Concept: {scientific_concept}Analogy:",
  "and a cold object interact to reach": "thermal equilibrium , weights on a scaleinteract to reach a balanced state. Thehot object , like a heavier weight , hasan excess (of heat or weight) that ittransfers to the cold object or lighterweight.2. Heat transfer corresponds to weightredistribution: In thermal equilibrium ,heat transfers from the hot object tothe cold object until they reach thesame temperature. Similarly , on abalancing scale , weight redistributesfrom the heavier side to the lighterside until they reach the same level.3. The point of equilibrium correspondsto the balance point on a scale: Inthermal equilibrium , the point ofequilibrium is when both objects reachthe same temperature. On a balancingscale , the balance point is reached when"
}