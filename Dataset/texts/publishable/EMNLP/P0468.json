{
  "Abstract": "The impressive performance of proprietaryLLMs like GPT4 in code generation has ledto a trend to replicate these capabilities inopen-source models through knowledge dis-tillation (e.g.Code Evol-Instruct).How-ever, these efforts often neglect the crucial as-pect of response quality, relying heavily onteacher models for direct response distilla-tion. This paradigm, especially for complexinstructions, can degrade the quality of syn-thesized data, compromising the knowledgedistillation process.To this end, our studyintroduces the Adaptive Modular ResponseEvolution (AMR-Evol) framework, which em-ploys a two-stage process to refine responsedistillation. The first stage, modular decom-position, breaks down the direct responseinto more manageable sub-modules. The sec-ond stage, adaptive response evolution, au-tomatically evolves the response with the re-lated function modules. Our experiments withthree popular code benchmarksHumanEval,MBPP, and EvalPlusattests to the superior-ity of the AMR-Evol framework over base-line response distillation methods. By compar-ing with the open-source Code LLMs trainedon a similar scale of data, we observed per-formance enhancements:more than +3.0points on HumanEval-Plus and +1.0 points onMBPP-Plus, which underscores the effective-ness of our framework. Our codes are avail-able at",
  "Teacher Model": "Response def calculate_determinant(matrix): if len(matrix) != 3 or len(matrix) != 3: raise ValueError('Matrix must be a 3x3 matrix') determinant = 0 for i in range(3): submatrix = [[matrix[j][k] for k in range(3) if k != i] for j inrange(1, 3)] sub_determinant = calculate_determinant(submatrix) sign = (-1) ** i determinant += sign * matrix[i] * sub_determinant return determinant if len(matrix) == 2 else round(determinant)",
  ": Direct distillation from the teacher modelpossibly yields low quality responses for complex tasks,thereby causing confusion within the student model": "pass rates exceeding 85% on the well-known Hu-manEval benchmark (Chen et al., 2021). Despitetheir strengths, the closed-source nature sparks ac-cessibility and privacy concerns (Wu et al., 2023).In response, there is a trend of adopting knowl-edge distillation (Xu et al., 2024) to transfer theadvanced code generation ability from the propri-etary LLMs to open-source counterparts, therebyenhancing their capabilities while ensuring broaderavailability and owner autonomy. Given that accessing the model weights of pro-prietary LLMs is infeasible, the knowledge distilla-tion pipeline is considered as a process where theteacher models synthesize supervised data, primar-ily consisting of instruction-response pairs (Liuet al., 2024). Student models are subsequentlytrained on this data, enabling the transfer of ca-pabilities from the teacher models.For exam-ple, Chaudhary (2023) employs the self-instructmethod (Wang et al., 2022) to prompt the teachermodel to generate new coding instructions based onpredefined seed tasks. Similarly, OSS-Instruct (Wei et al., 2023) utilizes a variety of code snippetssourced from GitHub to inspire GPT-3.5 to pro-duce novel coding instructions. Likewise, CodeEvol-Instruct (Luo et al., 2024) employs iterativeprompting to progressively elevate the complexityof code instructions provided by teacher models.Each of these methods has proven effective in dis-tilling coding knowledge from teacher models.Despite these advancements, there remains anunresolved challenge in enhancing the quality ofcode response distillation within the data synthe-sis process. In this setting, code responses serveas labels that teach the student models. Previousworks have shown that higher-quality responsescan lead to more effective distillation (Zhou et al.,2023; Mukherjee et al., 2023). However, currentmethods (Chaudhary, 2023; Wei et al., 2023; Luoet al., 2024) tend to rely solely on teacher modelsfor direct response distillation. As shown in Fig-ure 1, this approach is limited by the capabilities ofthe teacher models, making it difficult to produceaccurate responses for complex tasks. The issuebecomes even more challenging with methods likeCode Evol-Instruct, which deliberately amplify thecomplexity of instructions. Consequently, relyingon direct distillation can result in lower-quality re-sponses, ultimately affecting the performance ofthe student models (Wang et al., 2024).A straightforward yet costly solution to guaran-tee response quality is to hire human annotators tocraft the unit tests for each response. These testscould then be used in an execution-based strategyto validate answers. However, this method is fi-nancially prohibitive because it requires the recruit-ment of annotators with extensive programmingexpertise. Alternatively, depending on the teachermodel to automatically generate unit tests for self-repair (Chen et al., 2023a; Olausson et al., 2023;Chen et al., 2023c) introduces the same concern ofresponse quality, providing no certainty regardingthe correctness of the code repair.To address the challenge of distilling high-quality code responses from teacher models, weintroduce a novel framework named AdaptiveModular Response Evolution (AMR-Evol). In, the example reveals that the direct re-sponse distillation can somewhat capture the es-sential concepts required for solving coding tasks;however, it often deviates from the specific require-ments and incorporates logical errors. Motivatedby this observation, AMR-Evol leverages the out-puts of direct distillation as seed data and employs a two-stage processnamely, modular decomposi-tion and adaptive response evolutionto graduallyrefine the distilled code responses. By intricatelyrefining the process of response distillation, ourframework elicits better knowledge distillation ofthe student models.In the first stage of our AMR-Evol, we adopt theidea from modular programming (Dijkstra, 1967)to manage the complexity of distilling code re-sponses. By utilizing direct responses as the seeds,this method breaks down the coding task intosmaller, more manageable sub-modules. This strat-egy shifts the focus of the teacher models towardssolving these sub-modules step-by-step rather thangenerates a complete solution in a single attempt,whose effectiveness has been verified in recentChain-of-X works (Wei et al., 2022; Le et al., 2023;Xia et al., 2024).Additionally, while coding tasks may vary signif-icantly in objectives, the modular components needto construct their solutions frequently exhibit com-monalities, or can even be identical (Parnas, 1972).Hence, our adaptive response evolution stage lever-ages an auxiliary functional module database tostore all validated modules for reuse. During re-sponse generation, this process utilizes the modulesformulated in the decomposition stage to retrievesuitable, pre-validated modules from the database.These related modules serve as in-context exam-ples, aiding the adaptive refinement of responses,thus reducing our sole reliance on teacher models.As evolution progresses, any newly created mod-ules that differ from those in the database are addedafter a verification process by the teacher model.We apply our AMR-Evol framework to differentstudent models and select the most representativecoding benchmarks, including HumanEval (Chenet al., 2021), MBPP (Austin et al., 2021), andEvalPlus (Liu et al., 2023), for evaluation. Theresults reveal that our AMR-Evol framework con-sistently surpasses other response distillation meth-ods, namely direct response distillation, chain-of-thought distillation, and response repairing. Theseresults affirm the superiority of our approach in im-proving knowledge distillation for LLMs in codegeneration. Moreover, by integrating our AMR-Evol with Code Evol-Instruct, one of the SOTA in-struction construction methods, our models achievebetter performance than the open-source alterna-tives trained on a comparable data scale. Specifi-cally, we observed an improvement of more than+3.0 on HumanEval-Plus and +1.0 on MBPP-Plus.",
  "Related Work": "LLMs and Code Generation.Recently, LLMshave showcased significant achievements acrossa vast array of tasks. Leading tech firms havemade substantial progress in developing highly ad-vanced close-source LLMs, including OpenAIsGPT4 (OpenAI, 2023), Googles PaLM (Chowd-hery et al., 2022; Anil et al., 2023b) and Gem-ini (Anil et al., 2023a), as well as AnthropicsClaude (Anthropic, 2023). On the other side, theAI community has also seen the launch of sev-eral open-source LLMs, with model weights be-coming publicly available.MistralAI has con-tributed the Mistral-Series (Jiang et al., 2023).Google has released UL2-20B (Tay et al., 2022)and Gemma (Mesnard et al., 2024).TsinghuaUniversity introduced GLM-130B (Zeng et al.,2022) and MiniCPM (Hu et al., 2024), while Metahas made available OPT (Zhang et al., 2022) andLLaMA1&2&3 (Touvron et al., 2023a,b; Meta,2024).Furthermore, Allen AI has introducedthe wholly open-sourced LLM, OLMo (Groen-eveld et al., 2024), and Microsoft has released Phi-series (Gunasekar et al., 2023; Li et al., 2023b).Although a gap remains between the open-sourcemodels and their closed-source counterparts, thisgap is gradually narrowing.In parallel, recent research efforts have beendirected towards leveraging LLMs for code-related tasks to address the understanding andgeneration of code.OpenAI has unveiledCodex (Chen et al., 2021), Google has pro-posed CodeGemma (Google, 2024), and Salesforcehas introduced CodeGen-Series (Nijkamp et al.,2023b,a), and CodeT5&Plus (Wang et al., 2021,2023). Contributions from Tsinghua Universityinclude CodeGeeX (Zheng et al., 2023), and theBigCode Project has developed StarCoder1&2 (Liet al., 2023a; Lozhkov et al., 2024). Meta hasalso made its mark with the CodeLlama (Rozireet al., 2023), while DeepSeek has open-sourcedthe DeepSeekCoder (Guo et al., 2024).Theseinitiatives underscore the growing interest in em-ploying powerful base LLMs for code generation.Our work introduces a novel method for more ef-fectively distilling code knowledge from closed-source models to these open-source base models,thereby enhancing the coding performance.",
  "Knowledge Distillation for Code Generation.To enhance the capabilities of open-source LLMsfor code generation, recent works have adopted the": "knowledge distillation paradigm, utilizing closed-source LLMs as teachers for supervised data syn-thesis (Chen et al., 2023b; Zheng et al., 2024;Li et al., 2024; Yuan et al., 2024).For exam-ple, Chaudhary (2023) employs the self-instructmethod (Wang et al., 2022) to generate trainingdata, while Magicoder (Wei et al., 2023) generatestraining content using code snippets from GitHub.WizardCoder (Luo et al., 2024), on another hand,introduces the Code Evol-Instruct approach to pro-gressively increase the complexity of coding tasks.Despite these advancements, a common limitationamong these efforts is their primary focus on thecreation of code instructions, often overlookingthe criticality of enhancing code response distil-lation. Our research takes an orthogonal path byconcentrating on the refinement of code responsedistillation, offering a novel perspective comparedto previous works.",
  "Direct Response Distillation": "In the knowledge distillation framework, the fore-most goal is enabling the student model Ms toassimilate the strategies deployed by the teachermodel Mt in tackling code generation tasks. Utiliz-ing approaches like Code Evol-Instruct facilitatesthe generation of an extensive dataset of code in-structions {I} by the teacher model. Subsequently,the direct response distillation method employs theteacher model to process these task instructions toproduce the corresponding code responses Rd, re-sulting in a paired dataset, Ddirect = {(I, Rd)}.Then, the student model Ms learns from thisdataset through supervised fine-tuning.",
  "Adaptive Modular Response Evolution": "As discussed in , direct responses Ddirectto complex instructions can result in suboptimalquality, which in turn impacts the performance ofthe student model Ms. While these responses ofteninclude logical errors or may not fully align withthe precise requirements of the tasks, they generallyremain close to correct and capture the essentialconcepts needed for task solution. To address this, Module 3 def validate_matrix(matrix: list) -> None: \"\"\" Description: Validates if the input matrix is a 3x3 matrix. Parameters: - matrix (list): The input matrix to be validated. Raises: - ValueError: If the matrix is not a 3x3 matrix. \"\"\" Coding Tasks Design a function that implements the Laplace expansion theoremto calculate the determinant of a 3x3 matrix, but now with the added condition ofutilizing nested for loops and ensuring that the matrix elements are limited tointegers only.",
  "Direct Distillation": "Module 2 def calculate_minor_matrix(matrix: list, row: int, col: int) -> list: \"\"\" Description: Calculates the minor matrix by removing the specified row Parameters: - matrix (list): The input matrix. - row (int): The row index to be removed. - col (int): The column index to be removed. Returns: - list: The minor matrix after removing the specified row \"\"\" Module 1 def calculate_determinant(matrix: list) -> int: \"\"\" Description: Calculates the determinant of a 3x3 matrix using Laplace expansion theorem. Parameters: - matrix (list): The 3x3 matrix for which determinant needs to be calculated. Returns: - int: The determinant of the input matrix. \"\"\"",
  "Decomposition": "Retrieved Module 3 def search_element(matrix, x): \"\"\" Search for a given element in a sorted matrix. Args: Retrieved Module 2 def Determinant(A: list) -> float: \"\"\" Calculate the determinant of the provided matrix A. Args: A (list): The input matrix to calculate the determinant. Returns: float: The determinant of the matrix. \"\"\" def Submatrix(A, i, j): Retrieved Module 1 def Submatrix(A: list, i: int, j: int) -> list: \"\"\" Get the submatrix of A by removing the ith row and jth column. Args: A (list): The input matrix to extract the submatrix from. i (int): The index of the row to remove. j (int): The index of the column to remove. Returns: list: The submatrix of A. \"\"\" return [row[:j] + row[j+1:] for row in (A[:i] + A[i+1:])] Refined Response def determinant_3x3(matrix: list) -> int: det = 0 for i in range(3): det += (-1) ** i * matrix[i] * determinant_2x2( submatrix_2x2(matrix, 0, i)) return det def determinant_2x2(matrix: list) -> int: return matrix * matrix - matrix * matrix def submatrix_2x2(matrix: list, i: int, j: int) -> list: submatrix = [row[:j] + row[j + 1:] for row in matrix[:i] + matrix[i + 1:]] return submatrix",
  ": Our Adaptive Modular Response Evolution (AMR-Evol) framework with modular decomposition andadaptive response evolution elicits better response distillation for LLMs in code generation": "our AMR-Evol framework capitalizes on these di-rect response distillations as a starting point. Itincorporates a two-stage methodmodular decom-position and adaptive response evolutionfor anautomated refinement process that improves thequality of responses, thereby enhancing the effi-cacy of distillation. Modular Decomposition (MD).In the first stageof our framework, we employ the principle of mod-ular programming (Dijkstra, 1967) to tackle thecomplexity inherent in distilling code responses.Our method utilizes direct responses Rd as a start-ing point, guiding the teacher model Mt in break-ing down the given code instructions into a series ofsmaller, well-defined sub-modular functions. Werepresent this process mathematically as follows:",
  "{F m1 , F m2 , . . . , F mn } Mt (I, Rd) ,(1)": "where each function module F miis conceptualizedto fulfill a distinct subset of requirements stipu-lated by the code instruction I. This decompositionbreaks down complex instructions into a series ofeasier and more manageable sub-modules, enablingthe teacher model to tackle each one with less dif-ficulty. This results in a more effective responsedistillation process. Adaptive Response Evolution (ARE).In thesecond stage, we observe that while coding instruc-tions may greatly differ, the sub-modules neededfor assembling the final solution often share similar-ities or can even be identical (Parnas, 1972). Lever-aging this insight, we establish an auxiliary func-tional module database {F vi }, which archives allvalidated modules for future reuse. This databaseacts as a repository, enabling the retrieval of previ-ously validated sub-modules to foster the creationof new code responses.Building upon the modular decompositionachieved in the first stage, {F m1 , F m2 , . . . , F mn }, weinitially convert both the newly decomposed andpreviously archived functional modules into densevector representations through a sentence embed-dings model Mr:",
  "Ramr Mt (I, {F mi }, {F vi }) ,(4)": "where Ramr represents the refined code responses.These responses, alongside the original instructionI, compile an evolved dataset aimed at optimizingthe knowledge distillation process.As the process evolves, our framework iden-tifies new modules within Ramr that exhibit no-table differences from those currently in thedatabasejudged by the cosine similarity betweenthe new modules and existing ones. Modules thatare distinct undergo a rigorous verification stageprior to their integration into the database. This crit-ical stage harnesses the capabilities of the teachermodel for generating unit tests tailored to the func-tionalities of the specific modules. This procedurenot only assesses the functional correctness of thenew modules but also ensures that they meet thepredefined quality standards, thereby streamliningthe process of enriching the module database withreliable and effective components. Functional Module Database.The functionalmodule database is pivotal within our AMR-Evolframework. We begin by compiling a collectionof seed functions that have been validated. Lever-aging the self-instruct method (Wang et al., 2022),we prompt our teacher models to generate a di-verse range of function modules. Following this,we adopt a strategy similar to CodeT (Chen et al.,2023a), instructing the teacher models to produceunit tests that verify the functionality of these mod-ules. Only the functions that pass these unit testsare included in our dataset. Through this stringentprocess, we construct a seed functional moduledatabase that becomes a fundamental componentof our framework.",
  "Setup": "Baselines.Within our evaluation framework, wecompare the performance of our framework againstseveral baselines in code response distillation. Thefirst of these, referred to as direct, utilizes teachermodels to distill code responses in a straightfor-ward manner, as detailed in .1. The sec-ond baseline employs the Chain-of-Thought (CoT)prompting method for distilling responses (Hsiehet al., 2023). This approach is analogous to thefew-shot CoT method (Wei et al., 2022), in whichthe teacher model first provides a step-by-step ex-planation leading up to the formulated response.Our third baseline, AnsRepair, draws inspirationfrom previous works (Chen et al., 2023a; Olaussonet al., 2023; Chen et al., 2023d), where the teachermodels are utilized to generate unit tests. Thesetests serve to evaluate the correctness of the gen-erated responses. If the responses fail these tests,the teacher models are subsequently invoked tomake the necessary corrections. More details aboutbaseline methods are included in the Appendix A. Datasets and Benchmarks.Our framework fo-cuses on distilling responses and necessitates adataset of instructions. To this end, we utilize asubset of the training set from the MBPP as ourseed data. This is then expanded using the self-instruct method with the teacher model to generatearound 10k instructions. With these newly derivedinstructions, we employ a process akin to the CodeEvol-Instruct to iteratively synthesize a spectrumof complex coding instructions across three distinctlevels of complexity. This variety allows us to as-sess our frameworks efficacy in handling complexinstructions. More data construction and decontam-ination details can be found in the Appendix B.",
  ":Comparison of various response dis-tillationmethodsforcodegeneration,utilizingCodeLlama-7b-hf as the student model": "For performance evaluation, we utilize thewell-known coding benchmark,namely Hu-manEval (Chen et al., 2021), MBPP (Austin et al.,2021), and EvalPlus (Liu et al., 2023). HumanEvalcontains 164 coding problems with an average of9.6 test cases per problem. MBPP includes 399coding problems, each with three automated testcases. EvalPlus extends the number of test cases for both HumanEval and MBPP, resulting in enhancedversions named HumanEval-Plus and MBPP-Plus.Following EvalPlus, we report our methods effec-tiveness in terms of pass rates using greedy decod-ing, which helps minimize the impact of any ran-domness in the results. More details are includedin the Appendix C. Implementation Details.For all experiments,weemployOpenAIsclose-sourcedLLM,gpt-3.5-turbo-1106 as our teacher model andchoose two popular open-sourced code LLMs,deepseek-ai/deepseek-coder-6.7b-base(Guoetal.,2024)andmeta-llama/CodeLlama-7b-hf (Rozire et al.,2023) as our student models. For the dense em-beddings, we adopt one of the SOTA embeddingsmodels, Alibaba-NLP/gte-large-en-v1.5 (Liet al., 2023c) as our representation model. Thesupervised knowledge distillation phases of allexperiments are conducted with 200 training steps,3 epochs, a sequence length of 2048 and theAdamW optimizer (Loshchilov and Hutter, 2019).For further training details and prompting designes,please refer to the Appendix D.",
  "Main Results": "In , our AMR-Evol consistently out-performs various response distillation meth-odsforcodegeneration,whenadoptthedeepseek-coder-6.7b-baseasthestudentmodel.Specifically, at Complexity Level 1,AMR-Evolexhibitedsuperiorresults,withimprovements ranging between +2.8 to +4.0 acrossall tasks.Our method maintained this lead inComplexity Level 2, with the most substantialgains in MBPP and MBPP-Plus, at +3.0 and+2.7, respectively. Notably, even at the highestcomplexity (Level 3), the method continued toshow incremental enhancements, most prominentlya +2.5 increase in MBPP-Plus. The performanceexhibits AMR-Evols consistent proficiency ineliciting better code knowledge distillation acrossvarying degrees of complexity.When utilizing CodeLlama-7b-hf as the studentmodel, reveals that the performance pat-terns of AMR-Evol closely paralleled its efficacywith the previous model. Albeit with modest im-provements at Complexity Level 1, AMR-Evolshowed more enhancement in higher complexityscenarios. At Complexity Level 2, our methodachieves increases of +2.4 on HE and +2.8 on",
  "Analysis": "Quality Comparison.Our experimental findingsillustrate the effectiveness of our AMR-Evol inenhancing the knowledge distillation. To furthervalidate the efficacy of AMR-Evol in producingbetter instruction fine-tuning data, we conducteda manual evaluation. We randomly selected thesample sets of 120 coding problems for each lev-els of complexity. Given that all samples are cod-ing challenges, their responses can be definitivelyclassified as either correct or incorrect. Two ex-perienced programmers were engaged to reviewand label the code responses generated by variousmethods as suitable or not. The manual assessmentresults, depicted in , reveal that althoughno method attained complete perfect, AMR-Evoldemonstrated consistently superior performancecompared to all other baseline methods across allcomplexity levels. In Appendix E, we also includesome examples of responses generated by differentmethods to qualitatively compare their quality. Ablation.In , we present an ablationstudy meticulously designed to identify the individ-ual contributions of modular decomposition (MD)and adaptive response evolution (ARE) to the effi-cacy of our framework. First, we remove the MDstage in our framework by adopting direct responseto retrieve the related function modules for ARE.This led to a performance drop, underscoring itscrucial role in our framework. Specifically, the",
  ": Ablation studies by removing modular decom-position (MD) or adaptive response evolution (ARE) inour framework": "omission of MD typically results in the recall ofonly one function module based on the direct re-sponse. However, while direct responses addressmore complex or larger coding tasks, function mod-ules target tasks with finer granularity. This differ-ence creates a gap, making it challenging for theretrieved function modules to effectively contributeto refining the direct responses. Subsequently, we exclude the ARE stage, whichalso resulted in a performance decline, highlightingits vital role in the framework. Without ARE, thegeneration of responses is solely reliant on the mod-ular decomposition output, lacking the improve-ments that come from in-context learning withrelated function modules. This places the entireresponsibility for refining responses on the inher-ent capabilities of the teacher model. This anal-ysis strongly reinforces the indispensable natureof both MD and ARE within our framework. InAppendix F, we also present examples to showcasethe output of the MD stage and the top-1 functionmodules retrieved from the database.",
  "Comparing with Open Code LLMs": "To delve deeper into the efficacy of our frame-work, we have incorporated AMR-Evol with one ofthe SOTA instruction construction methods, CodeEvol-Instruct, to expand our SFT data set. We havegenerated around 50k instructions using this ap-proach and employed AMR-Evol to distill coderesponses from the teacher models (GPT3.5). Sub-sequently, we used deepseek-coder-6.7b-baseand CodeLlama-7b-Python-hf as our two studentmodels for training. For a relative fair comparison,we compare our fine-tuned student models againstpublicly available academic Code LLMs, whichare trained with a similar scale of SFT data and em-ploy the same base models as ours. This includesMagiCoder-DS/CL (Wei et al., 2023), WaveCoder-DS (Yu et al., 2023), and WizardCoder-CL (Luoet al., 2024). We also compare against official in-struction models, namely DeepSeek-Coder-Instructand CodeLlama-Instruct, to showcase performancegaps. For more discussions about baseline selectionand SFT details, please refer to the Appendix G. showcases the exceptional performanceof DeepSeekCoder-AMR-Evol across all tasks.When compared to MagiCoder-DS, trained with75k SFT data, and WaveCoder-DS, distilled fromGPT4, the AMR-Evol version notably stands out",
  ": Comparing different models on the harder codegeneration tasks, CodeContest (CC) (Li et al., 2022)and APPS (Hendrycks et al., 2021). DS-Instruct =DeepSeekCoder-Instruct. DS-AMR-Evol is our model": "by demonstrating substantial performance gains:+2.4 on HE, +3.2 on HE-Plus, and +1.0 on MBPP-Plus. Even when compared to the official instruc-tion model, which is trained with more than 20times as much data, our model achieves comparableperformance on MBPP and MBPP-Plus. Similarly,the CodeLlama-AMR-Evol variant exhibits supe-rior performance in most tasks, with performanceimprovements of +3.6 on HE, +3.0 on HE-Plus,and +1.5 on MBPP-Plus, respectively. Moreover,our model significantly outperforms CodeLlama-Instruct, which is an official model from Meta. Inaddition, the Pass@k sampling results, presentedin Appendix G, , also evident the betterperformance of our models.Since HumanEval and MBPP cover basic cod-ing tasks, weve gone further to evaluate dif- ferent models on advanced coding challenges,specifically CodeContest (Li et al., 2022) andAPPS (Hendrycks et al., 2021). All models gener-ate the answers with greedy decoding. As seen in, our model not only performs better overallbut also beats the official instruction model, despiteit being trained on much more data than ours.",
  "Our framework has room for enhancement in sev-eral aspects:": "First,despite showcasing ourmethods capacity to improve the accuracyof code response distillation, achieving 100%accuracy remains unattainable. While our ap-proach does alleviate this concern to someextent, the risk of delivering low-quality re-sponses that could potentially mislead the stu-dent models cannot be entirely eliminated. Fu-ture endeavors could explore the integrationof tools, such as compilers, to further refinethe quality of the responses. Second, our frameworks enhanced capabilityfor code knowledge distillation is accompa-nied by a requirement for multi-stage genera-tion, leading to increased costs in leveragingthe teacher models. This cost-performancetrade-off has been discussed in Appendix H,where we conclude that the benefits in per-formance outweigh the incremental costs in-curred. Third, the design of our method is narrowlyfocused on code knowledge distillation, lim-iting its broader application across generaldomains. The foundation of our frameworkin modular programming principles presentsconsiderable obstacles in adapting its methodfor use in non-coding areas. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023a. Gemini: A fam-ily of highly capable multimodal models. CoRR,abs/2312.11805. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-rav Mishra, Erica Moreira, Mark Omernick, KevinRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,Yuanzhong Xu, Yujing Zhang, Gustavo Hernndezbrego, Junwhan Ahn, Jacob Austin, Paul Barham,Jan A. Botha, James Bradbury, Siddhartha Brahma,Kevin Brooks, Michele Catasta, Yong Cheng, ColinCherry, Christopher A. Choquette-Choo, AakankshaChowdhery, Clment Crepy, Shachi Dave, MostafaDehghani, Sunipa Dev, Jacob Devlin, Mark Daz,Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-aoyu Feng, Vlad Fienber, Markus Freitag, XavierGarcia, Sebastian Gehrmann, Lucas Gonzalez, andet al. 2023b.Palm 2 technical report.CoRR,abs/2305.10403.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei": "Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual.",
  "Hailin Chen, Amrita Saha, Steven C. H. Hoi, and ShafiqJoty. 2023b. Personalised distillation: Empoweringopen-sourced llms with adaptive learning for codegeneration. CoRR, abs/2310.18628": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,Henrique Pond de Oliveira Pinto, Jared Kaplan,Harrison Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Joshua Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluat-",
  "Xinyun Chen, Maxwell Lin, Nathanael Schrli, andDenny Zhou. 2023d. Teaching large language mod-els to self-debug. CoRR, abs/2304.05128": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. CoRR, abs/2204.02311.",
  "Google. 2024. Codegemma: Open code models basedon gemma": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-gia, Rodney Kinney, Oyvind Tafjord, Ananya HarshJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,Shane Arora, David Atkinson, Russell Authur, Khy-athi Raghavi Chandu, Arman Cohan, Jennifer Du-mas, Yanai Elazar, Yuling Gu, Jack Hessel, TusharKhot, William Merrill, Jacob Morrison, Niklas Muen-nighoff, Aakanksha Naik, Crystal Nam, Matthew E.Peters, Valentina Pyatkin, Abhilasha Ravichander,Dustin Schwenk, Saurabh Shah, Will Smith, EmmaStrubell, Nishant Subramani, Mitchell Wortsman,Pradeep Dasigi, Nathan Lambert, Kyle Richardson,Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Sol-daini, Noah A. Smith, and Hannaneh Hajishirzi. 2024.Olmo: Accelerating the science of language models.CoRR, abs/2402.00838.",
  "Harkirat Singh Behl, Xin Wang, Sbastien Bubeck,Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, andYuanzhi Li. 2023. Textbooks are all you need. CoRR,abs/2306.11644": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, KaiDong, Wentao Zhang, Guanting Chen, Xiao Bi,Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-feng Liang. 2024. Deepseek-coder: When the largelanguage model meets programming - the rise of codeintelligence. CoRR, abs/2401.14196. Dan Hendrycks, Steven Basart, Saurav Kadavath, Man-tas Mazeika, Akul Arora, Ethan Guo, Collin Burns,Samir Puranik, Horace He, Dawn Song, and JacobSteinhardt. 2021. Measuring coding challenge com-petence with APPS. In Proceedings of the NeuralInformation Processing Systems Track on Datasetsand Benchmarks 1, NeurIPS Datasets and Bench-marks 2021, December 2021, virtual. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, RanjayKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-tilling step-by-step! outperforming larger languagemodels with less training data and smaller modelsizes. In Findings of the Association for Compu-tational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pages 80038017. Association forComputational Linguistics. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He,Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,Yuxiang Huang, Weilin Zhao, Xinrong Zhang,Zhen Leng Thai, Kai Zhang, Chongyi Wang, YuanYao, Chenyang Zhao, Jie Zhou, Jie Cai, ZhongwuZhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li,Zhiyuan Liu, and Maosong Sun. 2024. Minicpm: Un-veiling the potential of small language models withscalable training strategies. CoRR, abs/2404.06395. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Hung Le, Hailin Chen, Amrita Saha, Akash Gokul,Doyen Sahoo, and Shafiq Joty. 2023. Codechain: To-wards modular code generation through chain of self-revisions with representative sub-modules. CoRR,abs/2310.08992.",
  "Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie DelGiorno, Suriya Gunasekar, and Yin Tat Lee. 2023b.Textbooks are all you need II: phi-1.5 technical report.CoRR, abs/2309.05463": "Yujia Li, David H. Choi, Junyoung Chung, Nate Kush-man, Julian Schrittwieser, Rmi Leblond, Tom Ec-cles, James Keeling, Felix Gimeno, Agustin DalLago, Thomas Hubert, Peter Choy, Cyprien de Mas-son dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, AlexeyCherepanov, James Molloy, Daniel J. Mankowitz,Esme Sutherland Robson, Pushmeet Kohli, Nandode Freitas, Koray Kavukcuoglu, and Oriol Vinyals.2022. Competition-level code generation with alpha-code. CoRR, abs/2203.07814.",
  "Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,Pengjun Xie, and Meishan Zhang. 2023c. Towardsgeneral text embeddings with multi-stage contrastivelearning. arXiv preprint arXiv:2308.03281": "Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-ming Zhang. 2023. Is your code generated by chatgptreally correct? rigorous evaluation of large languagemodels for code generation. CoRR, abs/2305.01210. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, YanzheZhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, DiyiYang, Denny Zhou, and Andrew M. Dai. 2024. Bestpractices and lessons learned on synthetic data forlanguage models. CoRR, abs/2404.07503. Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-erico Cassano, Joel Lamy-Poirier, Nouamane Tazi,Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,Tianyang Liu, Max Tian, Denis Kocetkov, ArthurZucker, Younes Belkada, Zijian Wang, Qian Liu,Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry YueZhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade,Wenhao Yu, Lucas Krau, Naman Jain, Yixuan Su,Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai,Niklas Muennighoff, Xiangru Tang, MuhtashamOblokulov, Christopher Akiki, Marc Marone, Cheng-hao Mou, Mayank Mishra, Alex Gu, Binyuan Hui,Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Pa-try, Canwen Xu, Julian J. McAuley, Han Hu, TorstenScholak, Sbastien Paquet, Jennifer Robinson, Car-olyn Jane Anderson, Nicolas Chapados, and et al.2024. Starcoder 2 and the stack v2: The next genera-tion. CoRR, abs/2402.19173. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder:Empowering code large language models with evol-instruct. In The Twelfth International Conference onLearning Representations. Thomas Mesnard, Cassidy Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,Morgane Rivire, Mihir Sanjay Kale, Juliette Love,Pouya Tafti, Lonard Hussenot, Aakanksha Chowdh-ery, Adam Roberts, Aditya Barua, Alex Botev, AlexCastro-Ros, Ambrose Slone, Amlie Hliou, AndreaTacchetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Cristian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan,Jeremy Chen, Johan Ferret, Justin Chiu, and et al.2024. Gemma: Open models based on gemini re-search and technology. CoRR, abs/2403.08295.",
  "David Lorge Parnas. 1972. On the criteria to be used indecomposing systems into modules. Commun. ACM,15(12):10531058": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,and Yuxiong He. 2020. Deepspeed: System opti-mizations enable training deep learning models withover 100 billion parameters. In KDD 20: The 26thACM SIGKDD Conference on Knowledge Discoveryand Data Mining, Virtual Event, CA, USA, August23-27, 2020, pages 35053506. ACM. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Dfossez, Jade Copet,Faisal Azhar, Hugo Touvron, Louis Martin, Nico-las Usunier, Thomas Scialom, and Gabriel Synnaeve.2023. Code llama: Open foundation models for code.CoRR, abs/2308.12950. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, NeilHoulsby, and Donald Metzler. 2022. Unifying lan-guage learning paradigms. CoRR, abs/2205.05131. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288.",
  "Punta Cana, Dominican Republic, 7-11 November,2021, pages 86968708. Association for Computa-tional Linguistics": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought promptingelicits reasoning in large language models. In Ad-vances in Neural Information Processing Systems 35:Annual Conference on Neural Information Process-ing Systems 2022, NeurIPS 2022, New Orleans, LA,USA, November 28 - December 9, 2022.",
  "Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,Reynold Cheng, Jinyang Li, Can Xu, DachengTao, and Tianyi Zhou. 2024. A survey on knowl-edge distillation of large language models. CoRR,abs/2402.13116": "Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang,Can Xu, Yishujie Zhao, Wenxiang Hu, and QiufengYin. 2023. Wavecoder: Widespread and versatileenhanced instruction tuning with refined data genera-tion. CoRR, abs/2312.14187. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,Ruobing Xie, Yankai Lin, Zhenghao Liu, BowenZhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.2024. Advancing llm reasoning generalists with pref-erence trees. Preprint, arXiv:2404.02078. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, Weng Lam Tam, ZixuanMa, Yufei Xue, Jidong Zhai, Wenguang Chen, PengZhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: an open bilingual pre-trained model. CoRR,abs/2210.02414. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, ChristopherDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-ter, Daniel Simig, Punit Singh Koura, Anjali Srid-har, Tianlu Wang, and Luke Zettlemoyer. 2022.OPT: open pre-trained transformer language mod-els. CoRR, abs/2205.01068.",
  "with multilingual evaluations on humaneval-x. CoRR,abs/2303.17568": "Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu,Bill Yuchen Lin, Jie Fu, Wenhu Chen, and XiangYue. 2024. Opencodeinterpreter: Integrating codegeneration with execution and refinement. CoRR,abs/2402.14658. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,Luke Zettlemoyer, and Omer Levy. 2023. LIMA:less is more for alignment. In Advances in NeuralInformation Processing Systems 36: Annual Confer-ence on Neural Information Processing Systems 2023,NeurIPS 2023, New Orleans, LA, USA, December 10- 16, 2023.",
  "BDatasets": "Our framework concentrates on distilling re-sponses and requires a dataset of instructions forthis purpose. As indicated in , we enumeratethe quantity of instructions used in our experiments.We initiate our process with the MBPP trainingset (task-ids 601-974) as a seed dataset, which en-hances our ability to generate Python code effec-tively. To prevent any overlap with the EvalPlustest data, we are diligent in omitting any samplesthat coincide with the test set, thereby narrowingour training set to 332 unique MBPP tasks. Wethen utilize this filtered seed data and apply theself-instruction method to construct instructions.Subsequently, we employ the Code Evol-Instructmethod to iteratively generate instructions of vary-ing complexity across three distinct levels.To ensure decontamination of our datasets, weinvoke a method akin to the work of Code Evol-Instruct (Luo et al., 2024) for data filtering. This in-volves employing the gte-large-en-v1.5 model to treat each test set sample as a query, which re-trieves the top five most similar samples from thetraining data. Subsequently, these pairs are eval-uated by GPT4 in a binary classification task todecide whether a match exists. Detected matcheslead to the exclusion of those specific training sam-ples to eliminate potential data leakage.",
  "DImplementation Details": "Our AMR-Evol framework encompasses a two-stage process. In the first stage, Modular Decom-position is applied to break down the code instruc-tions into multiple sub-modules, using the directresponses as the initial seed data. The prompt uti-lized for this stage is demonstrated above. Duringthe second stage, Adaptive Response Evolution re-fines these decomposed sub-modules, utilizing theretrieved modules to develop the final answer. Thecorresponding prompt for this stage is as follows:",
  "### Correct Solution:": "For all instruction construction processes, we setthe temperature to 0.7 and the sequence length to2048. For all response distillation processes, thetemperature is fixed at 0.0, and the sequence lengthis set to 3000. We train the models for 200 stepsacross 3 epochs with a sequence length of 2048,employing the AdamW optimizer, BF16 precision,and DeepSpeed Zero-2 (Rasley et al., 2020). Thetraining is conducted on 4 A800 GPUs.",
  "GComparing with Open Code LLMs": "To compare with other Open Code LLMs, we in-tegrate our AMR-Evol framework with Code Evol-Instruct to continually expand our SFT dataset. Wealso employ the same data decontamination methodto prevent data leakage. We have generated ap-proximately 50k training samples. Subsequently,we fine-tuned our models using settings similar tothose detailed in Appendix D. Given the larger vol-ume of data, we opted to increase the number oftraining steps to 400.To obtain a relative fair comparison, we only in-clude the open code LLMs which are trained witha similar scale of SFT data and employ the samebase models as ours, including MagiCoder-DS/CL,WaveCoder-DS, and WizardCoder-CL. We alsocompare against official instruction-based models,namely DeepSeekCoder-Instruct and CodeLlama-Instrut. However, these official models are trainedwith more than 20 times data than ours, which leadto unfair comparison. We only want to showcasethe performance gaps.Models with a higher parameter count havebeen excluded from our comparison, such asDeepSeekCoder-Instruct-33B, WizardCoder-33B-v1.1, Codestral-22B-v0.1,4, CodeLlama-Instruct-34B, and Starcoder2-15b-Instruct.5 These modelsconsiderably exceed the size of our own, renderinga direct comparison unfair. Additionally, modelsthat primarily derive their learning from GPT4 areexcluded, including MagiCoder-S-DS, WaveCoder-DS-Ultra, and OpenCodeInterpreter (Zheng et al.,",
  ": Adopting open-source model, Llama-3-70B-Instruct, as our teacher model": "2024). As our teacher model is based on GPT-3.5,a direct comparison with these GPT4-based mod-els would not be equitable. Non-academic models,such as CodeQwen (Bai et al., 2023), are also ex-cluded since the methods behind their constructionare not disclosed.In , all models employ greedy decodingto generate answers for each question. To presentadditional results and align with some previousstudies (Chen et al., 2021; Luo et al., 2024), wealso display results obtained through sampling in. The temperature is set to 0.2, and thenumber of samples is fixed at 200. Following themethod of prior work (Chen et al., 2021), we cal-culate the pass@1 and pass@10 scores. It is alsoevident that our models outperform the baselinemodels.",
  "HData Synthesis Cost Trade-off": "Differing from direct distillation, our frame-work necessitates multi-stage response distillation,which increases the cost of using the API of theteacher model (around 4 times).However, Ta-ble 1 and 2 showcase that our method can out-performance the direct distillation over all tasksand different student models. In addition, we adoptthe gpt-3.5-turbo-1106 as our teacher model,whose API price is low. Therefore, we concludethat the benefits in performance outweigh the incre-mental costs incurred.",
  "JBroader Impact": "Our research presents a novel framework for trans-ferring code knowledge from closed-source LLMsto open-source LLMs. This framework is designedto generate code responses for various coding in-structions during the data synthesis process. Whileour approach has been shown to improve responsequality, as illustrated in , it does not guar-antee absolute correctness. Consequently, data gen-erated through our method may still contain errors.It is essential to filter out these erroneous samplesbefore deploying our approach in real-world appli-cations to mitigate the risk of misuse."
}