{
  "Abstract": "Polysemy and synonymy are two crucial inter-related facets of lexical ambiguity. While bothphenomena are widely documented in lexicalresources and have been studied extensivelyin NLP, leading to dedicated systems, they areoften being considered independently in prac-tictal problems. While many tasks dealing withpolysemy (e.g. Word Sense Disambiguiation orInduction) highlight the role of words senses,the study of synonymy is rooted in the studyof concepts, i.e. meanings shared across thelexicon. In this paper, we introduce ConceptInduction, the unsupervised task of learninga soft clustering among words that defines aset of concepts directly from data. This taskgeneralizes Word Sense Induction. We pro-pose a bi-level approach to Concept Inductionthat leverages both a local lemma-centric viewand a global cross-lexicon view to induce con-cepts. We evaluate the obtained clustering onSemCors annotated data and obtain good per-formance (BCubed F1 above 0.60). We findthat the local and the global levels are mutuallybeneficial to induce concepts and also sensesin our setting. Finally, we create static embed-dings representing our induced concepts anduse them on the Word-in-Context task, obtain-ing competitive performance with the State-of-the-Art.",
  "Introduction": "A crucial challenge in understanding natural lan-guage comes from the fact that the mapping be-tween word forms and lexical meanings is many-to-many, due to polysemy (i.e., the multiplicity ofmeanings for a given form)1 and synonymy (i.e.,the multiplicity of forms for expressing a givenmeaning). Both polysemy and synonymy havebeen thoroughly studied in NLP, but mostly as in-dependent problems, giving rise to dedicated sys-tems. Thus, Word Sense Disambiguiation (WSD)",
  "In this paper, we take polysemy in its most comprehensivedefinition, also including homonymy": "aims at correctly mapping word occurrences to oneof its senses (Raganato et al., 2017), while WordSense Induction (WSI), its unsupervised counter-part, aims at clustering word occurrences into la-tent senses directly from data (Manandhar et al.,2010; Jurgens and Klapaftis, 2013). More recently,researchers have proposed the task of Word-in-Context (WiC), which consists in classifying pairsof word occurrences depending on whether they re-alize the same sense or not (Pilehvar and Camacho-Collados, 2019). All these works take a word cen-tric view, which aims at identifying or character-izing the different senses of a given word, wherethese senses are bound to a word. Another lineof work, which takes a broader lexicon-wide per-spective, is concerned with identifying synonyms,which are equivalence classes over different wordsthat point to the same concept (Zhang et al., 2021;Ghanem et al., 2023), where concepts are semanticentities that are not bound to a word. In WordNet(Miller, 1995; Fellbaum, 1998), concepts are calledsynsets, defined as sets of synonyms. However, out-side of lexical resources, synonymy and polysemyare usually considered as independent problemsin the NLP literature. Yet, these two views arecomplementary. In lexicology, they correspond totwo perspectives on the word-meaning mapping:semasiology and onomasiology. The former is theword-to-meanings view, where one can observepolysemy by looking at the different meanings agiven word has. The latter is the meaning-to-wordsview, in which one can study synonymy by look-ing at the inventory of words that speakers use toexpress the same meaning.In this paper, we propose a new task, called Con-cept Induction, that directly aims at learning con-cepts in an unsupervised manner from raw text.More precisely, this task aims at learning a softclustering over a target lexicon (i.e., a set of words),in such a way that each cluster corresponds to a(latent) concept. Thus, this task both addresses polysemy (since polysemous words should appearin multiple clusters) and synonymy (since synony-mous words should appear in the same cluster(s)).Inducing concepts can be interesting for many ex-ternal applications, like building lexical resourcesfor low-resources languages (Velasco et al., 2023),and can bring a different perspective in computa-tional studies of meaning, moving the usual word-centric focus to a more meaning-centric state.Our approach to Concept Induction relies onword occurrences for a target lexicon, representedas word embeddings derived from a ContextualizedLanguage Model (in this case, BERT Large (De-vlin et al., 2019)), which are then grouped, usinghard clustering algorithms, into concept denotingclusters. While these concept clusters could in prin-ciple be obtained directly from word occurrences,we propose a bi-level methodology that leveragesboth a local, lemma-centric clustering (i.e., oper-ating on only specific word occurrences), and aglobal, cross-lexicon clustering (i.e., operating onall words occurrences). From this perspective, ourapproach generalizes, and in fact builds upon clas-sical Word Sense Induction, in that word sensesare learned jointly alongside with concepts. We hy-pothesize that an approach taking both complemen-tary resolutions in account will lead to improvedConcept Induction and Word Sense Induction, i.e.that the two objectives can be mutually beneficial.To validate our approach, we carried out exper-iments on the SemCor dataset, which providesa set of concepts (taking the form of WordNetsynsets) related to word occurrences. We foundthat our bi-level clustering approach accuratelylearn concepts, achieving F1 scores above 0.60on the task of Concept Induction compared toWordNets synsets, outperforming competing ap-proaches that use only local and global views. Thisdemonstrates the benefits of our bi-level approach,and its ability to leverage both local and globalviews when inducing concepts. Interestingly, weshow that the benefits go both ways: our proposedapproach outperforms lemma-centric approacheswhen evaluated for WSI. Finally, we show thatconcept-aware static embeddings derived from ourapproach are also competitive with state-of-the-artapproaches efficient on the Word-in-Context task,while using less training data. Through the newtask of concept induction, we also contribute in anew way to the ongoing debate regarding the abil-ity to align vector representations extracted fromContextualized Language Models to the seman- tic representations posited by (psycho-)linguists.In this vein, we conduct a qualitative evaluationof obtained clusters to ensure they indeed reflectconcepts and gather synonyms. The source codewe used for experiments is available at",
  "Lexical resources for concepts": "Princetons WordNet (PWN) (Miller, 1995; Fell-baum, 1998) is a lexical database that has beenbeen the most widely used as a reference for mostwordsense-related tasks for many years. In Word-Net, the entry corresponding to a lemma has dif-ferent wordsenses, each of them mapping to asynset. Synsets are WordNets equivalents of ourconcepts. Lemmas whose wordsenses belong tothe same synset are synonymous. WordNet 3.0contains 117,659 synsets and is built from thework of psycholinguists and lexicographers, thatnot only describes synonymy but also other lexicalrelations such as hypernymy/hyponymy, antonymy,meronymy/holonymy, etc. But the amount of re-sources needed to create such lexical databaseswith human experts is considerable, making thema very rare and precious resource. They are notavailable for a large number of active languages,and even more rare for dead languages (Bizzoniet al., 2014; Khan et al., 2022).",
  "Word senses with Language Models": "With the recent development of neural Contextual-ized Language Models (CLM), several work usetheir hidden-layers to extract vector representationsof word usages and retrieve word senses. Theserepresentations are fed to a classification (for WSD)or a clustering (in the case of WSI) algorithm todistinguish the words senses (Scarlini et al., 2020;Nair et al., 2020; Saidi and Jarray, 2023). Theseembeddings-based approaches have applications inother fields: Kutuzov and Giulianelli (2020) andMartinc et al. (2020) use sense clusters found usingCLM embeddings to study the change in meaningof words, and Chronis and Erk (2020) propose amany-Kmeans method to investigate semantic sim-ilarity and relatedness. Another line of work useslist of substitute tokens sampled from the CLMhead to infer senses (Amrami and Goldberg, 2019;Eyal et al., 2022) and are sucessful on WSI bench-marks like Manandhar et al. (2010) and Jurgensand Klapaftis (2013).",
  "Structures of Meaning in CLM": "Recent research probes neural CLMs for aligne-ments between representations from their latentspaces and semantic patterns and relations. Sec-tion 7.2 of Haber and Poesio (2024) summarizesfindings about polysemy in contextualized CLMs,showing that these models were able to detect pol-ysemy and in some cases distinguish actual poly-semy from homonymy. They report that represen-tations from different senses may however overlap.Hanna and Marecek (2021) shows that pretrainedBERT embed knowledge of hypernymy but is lim-ited to the more common hyponyms. Velasco et al. (2023) build on top of WSI tech-niques in an attempt to automatically construct aWordNet for Filipino, thus proposing a modelingof synonymy in this language. However, the evalu-ation of the synsets they obtained is limited by thelack of sense-annotated data for Filipino, and theycould not evaluate the impact of their methodologyon the two levels (senses and concepts).Works like Ethayarajh (2019) and Chronis andErk (2020) study the kind of information that wasdistributed across layers. The former concludesthat syntactic and word-order information are dis-tributed in the first layers while in deeper layers,representations are heavily influenced by contexts.The latter demonstrates, with a multi-prototypesembedding approach, that semantic similarity isbest found in moderately late layers, while related-ness is best found in last layers.",
  "Concept Induction": "Our main motivation behind Concept Induction isto present a view of the mapping between wordsand their meaning(s).2 This view is systemic, mean-ing that it should not be defined for individualwords neither for individual concepts, but ratheracknowledging these as a whole with interactionsand relations. This extends beyond the primaryobjective of WSI, which defines word senses aspertaining to individual words only and does notexplore relations between lemmas or concepts.",
  "(2022); see also coexpression and synexpression in the termi-nology proposed by Haspelmath (2023)": "set of target lemmas is referred to as the lexicon,while the corpus is the set of all occurrences. Ourgoal is to study the meaning of target words as theyare used in the corpus.In this study we call sense of a word its usage torefer to a concept. A polysemous word has multiplesenses, each of them referring to a distinct concept.Two words are said to be synonyms for a givenconcept when each of them has one of their sensesreferring to this shared concept. Senses are definedlocally, i.e. bound to an individual word of thelexicon, as opposed to concepts which are definedglobally, i.e. across the whole lexicon. An oc-currence of a word w realizes one of ws senses.Consider the words test and trial and the fol-lowing corpus: (A) the jury found them guilty ina fair trial. (B) candidates competed in a trial ofskill. (C) the hero underwent a test of strength. Thecorpus is composed of two occurrences of trialand one occurrence of test. In the corpus, trialis polysemous. Its first sense, illustrated in A, refersto a process of law. Its second sense, in B, refers tothe concept of the act of undergoing testing. Thesense of test in sentence C also corresponds tothis concept: its a case where test and trialsare synonymous. Shifting the focus from senses toconcepts, we will say that B and C instantiate thesame concept, while A is an instance of a differentconcept.",
  "Task definition": "The goal of Concept Induction (CI) is to automati-cally learn a set of concepts directly from the data,i.e. learning a soft clustering CW in the set oftarget words W that should correspond to the mul-tiple concepts instantiated by occurrences of thecorpus. CW is a soft clustering because a wordcan be assigned to several clusters (when it is poly-semous). Using a different perspective than WSI,the framework of Concept Induction provides amore complete view on meaning across the lexicon.Both WSI and CI capture polysemy, but CI alsoreveals synonymy across the lexicon. Like WSI,Concept Induction does not require a pre-definedset of concepts.",
  ": Illustration of our framework. The words trial is polysemous and has two senses corresponding to twodifferent concepts, and is synonym with test for this second meaning": "For a given word w W, the set Ow can bepartitionned according to its different senses. Wedenote swj the part of occurrences of w in the corpuscorresponding to the j-th sense of w. We refer tothese groups of occurrences as the sense clusters ofw. The set Sw = {swj }1jnw forms a partition ofOw, and we call S the set of all sense clusters of allwords, i.e. S = wW Sw. S is a local (lemma-centric) partition of the whole O. The task of WordSense Induction aims at learning the partition Sgiven a corpus O.In this work, we aim at dividing the corpus intoconcepts instead of senses. We denote ck the groupof occurrences of words corresponding to the con-cept indexed by k, and C = {ck}1kp the par-tition of O in p concept clusters. Unlike senseclusters of S, a concept cluster ck C can gatheroccurrences of different words: C is a global par-tition. Each occurrence owi of a word w W is as-sociated to a sense cluster swj and a concept clusterck C. We can say that a concept correspondingto ck is instantiated by occurrence owi through thesense corresponding to swj , or conversely that owiuses the sense reflected in swj to mean the conceptdescribed by concept cluster ck. All occurrences ofsense cluster swj S appear in the same conceptcluster ck C.In summary, S and C are partitions of O and arenaturally constrained as follows:",
  ". All swj Sw (i.e. same word) refer to distinctconcepts": "From the partition C on occurrences, one canderive CW , a clustering of the set of words Winto concepts. To each concept cluster ck C weassociate a cluster in CW that contains all lemmasof W whose occurrences were assigned to ck. InCW , a polysemous word with n senses appears inn distinct clusters (one per sense), and synonymsappear in at least one common cluster (one pershared concept).",
  "We denote CW the word-level soft-clusteringand C the partition of occurrences that are learnedon the data": "In we illustrate this framework, usinga corpus of occurrences of the words test andtrial. In this scenario, W = {test, trial} andtwo concepts are instantiated: a process of law todetermine someones guilt and a challenge to eval-uate a skill. The lemma trial exhibits two sensesas it has occurrences corresponding to both con-cepts: trial is polysemous. The second concept isalso instantiated by occurrences of test, thereforetrial and test show synonymy in this case. Thistoy example also follows all constraints formulatedabove.",
  "Methodology": "In this section we describe the methods we pro-pose and evaluate for Concept Induction. We learna clustering CW drawing inspiration from the re-lations between O, S, C and CW . In particular,the overall objective of our methodology consist infinding C (i.e. partition occurrences into conceptclusters) to derive CW . .3 highlightedthat there are two levels of partitions: a local level(senses) and a global one (concepts). The proposedapproaches rely on both levels and the use of aContextualized Language Model (CLM) to gatherrepresentations of occurrences influenced by thecontext.",
  "Proposed Bi-level Method": "Local (lemma-centric) clusteringFirstly, wepropose to learn a word-sense partition for eachtarget words individually. Using the CLM hid-den layers, we extract a vector representation (theoccurrence embedding) of every occurrence owi .We then learn a partition Sw of each Ow using aclustering algorithm on the embeddings. Each Sw describes the locally estimated sense clusters ofword w. Jointly considering these partitions forall w W, we obtain a partition S of the wholeset of occurrences O. This partition is local in thesense that each word has its occurrences clusteredindependently from other words. Global (cross-lexicon) clusteringOnce we havea local clustering S, we turn from consideringwords independently to consider all words together.In this step, we learn a global clustering by merginglocal clusters of occurrences. To do so, we averageembeddings of all occurrences in the same localcluster to get a single embedding representing eachlocal cluster. Then we run a second clustering al-gorithm, this time using the averaged embeddingsof local clusters. This global clustering defines anew partition C of the the corpus O: when two lo-cal clusters sw1jand sw2j are merged into the sameglobal cluster ck (because their embeddings wereclustered together), all their occurrences are as-signed to global cluster ck. From this global oc-currence partition C we can easily extract CW , aword-level soft-clustering of lemmas whose occur-rences appear in the same ck.This Bi-level method directly implements thesystem of contraints described in .3. Onlyconstraint 5 is not enforced by design. Indeed, ourlocal clusters being learned and not informed by an expert, the local clustering step may make er-rors, especially if the data for a given word aresparse. Allowing the global clustering to mergelocal clusters enables the correction of local clus-terings recall errors using information from theglobal level.We also want to highlight that the proposedmethodology is generic, in the sense that it is nottied to a specific choice of clustering algorithm.",
  "Local-only and Global-only": "Sense-inducing systems (WSI approaches) that cre-ate only local clusters of occurrences for each wordare said to be Local-only systems. We use themas baseline models that only produce word-levelclusters of size 1 and do not reflect synonymy, butstill learn polysemy.On the other hand, consider a system in whicheach occurrence is mapped to its own local cluster(i.e. no actual local clustering step), and the globalstep divides occurrences directly into global clus-ters. We refer to this kind of system as Global-onlyapproaches. They allow to evaluate how useful thelocal clustering step is in the process: we hypothe-size that the local step in Bi-level will reduce poten-tial variance in occurrences by aggregating them,increasing Precision compared to Global-only.",
  "Settings": "Data.We choose to use the annotated part of theSemCor 3.03 corpus. This dataset contains occur-rences for a wide number of words, and morpho-syntactic annotations provide their lemma and theirPart-of-Speech tag. Among all lemmas having atleast 10 annotated occurrences, we keep only nouns(excluding proper nouns)4 composed only of al-phabetical characters with a minimum length or 3",
  "~mihalcea/downloads.html#semcor": "4For the sake of simplicity and clarity, this study is focusedonly on nouns. Indeed, other Parts-of-Speech induce extradifficulties. Verbs for instance required extra preprocessingsteps and decisions (e.g. include or exclude gerundive uses,past participle employed like adjectives, etc.). Extension ofexperiments to other PoS is left to future work. letters. The resulting lexicon W contains 1,560 dif-ferent lemmas, for which we gather a corpus O con-taining a total of 52,997 occurrences5. SemCor isalso semantically annotated, with each occurrenceof a target lemma assigned to a synset in WordNet,that we consider to be the concept it refers to. Wederive a reference partition of the occurrences Cand a reference soft-clustering of the words CW",
  "from annotations, for a total of 3,855 different con-cepts (WordNets synsets) covered in O. This set ofconcepts is the subset of WordNet correspondingto the textual data": "Evaluation of Concept InductionWe comparethe learned word clustering CW to the referenceCW . We choose to use the BCubed metrics (Baggaand Baldwin, 1998), obtaining Precision and Re-call for the evaluated clustering compared to thereference, as well as an F1 score. To account foroverlapping clusters, we use the Extended BCubedmetrics proposed by Amig et al. (2009), which hasalready been used as evaluation in SemEval2013WSI task (Jurgens and Klapaftis, 2013).Using BCubed metrics, for a given evaluatedclustering, low precision would mean that groupedlemmas should not have been clustered togetherbecause none of their occurrence annotations mapto a shared concept according to annotations . Lowrecall means that the evaluated system fails to cap-ture clusters of lemmas whose occurrences share aconcept according to annotations. The number ofcommon clusters between two words also impactsBCubed metrics: if two lemmas appear togetherin too many clusters compared to the referenceclustering, precision is decreased; if the number ofcommon clusters is too low, recall is decreased. Development.To learn the clustering, candidatesystems have access to the full set of occurrences-in-context but not their annotations. To choose theappropriate set of hyperparameters, we create a Devsplit of the annotations by randomly sampling 10%of concepts and revealing semantic annotations ofthe corresponding occurrences. We use them toevaluate Concept Induction for this small set ofconcepts, and choose the set of hyperparametersthat scores best in BCubed F1.",
  "Evaluation splitsIn the final evaluation phase,we compute scores on all concepts / all occurrences,": "5Sentences in which the lemma appears, paired with itsposition within them. If the lemma appears multiple timesin the same sentence, we create several distinct occurrences,where only the position varies. including the Dev split, as concepts in it are partof the whole subset of WordNet described by Sem-Cors annotations. In the full data, we found that88% of the concepts were instantiated using onlya single lemma. To better evaluate cases of syn-onymy, we also evaluate systems on a subset ofthe corpus, denoted Synon, that contains onlyoccurrences of concepts showing synonymy (theremaining 12% of concepts, instantiated through atleast 2 distinct lemmas). Statistics are provided in in Appendix B. Note that it only changesthe set of concepts/lemmas for which the system isbeing evaluated, not the clusterings training data.",
  "Systems and baselines": "Clustering Algorithms.We try two differ-ent clustering algorithms relying on differentparadigms: Kmeans (used in Chronis and Erk(2020)), a centroid-based algorithm with a fixednumber of clusters, and Agglomerative clustering(used in Saidi and Jarray (2023); Velasco et al.(2023); dubbed Agglo for short), a deterministichierarchical approach using a distance thresholdto create a dynamic number of clusters instead ofusing a fixed one. Another difference betweenKmeans and Agglo is that the former assumes thatexpected clusters are of nearly-spherical shape andbalanced in number of points, while the latter doesnot make assumptions on the shape of data. Detailsof tested hyperaprameter values are provided inAppendix C.",
  "Representations.Following Chronis and Erk": "(2020) and Eyal et al. (2022), we use BERT Large(Devlin et al., 2019), a masked language modelwith 24 layers and 345M parameters. This allowsfor direct comparisons with these approaches. Also,BERT Large was found by Haber and Poesio (2021)to allow for better grouping of sense interpretationsthan other LLMs.6 We average subwords embed-dings if needed. It is a common practice in previouswork on semantic-related tasks to use the average ofthe last 4 layers to get embeddings; we decided toadopt the same \"4 layers average pooling\" strategy,but trying with different possible sets of layers (seeAppendix C). Therefore, for a set of four layers,we average hidden states across the selected layersto get a single 1024-dimensional vector. We foundthat layers 14 to 17 obtained the best results on Devfor all methods (global/local-only and bilevel).",
  ": Concept Induction BCubed Precision (P), Re-call (R) and F1on the SemCor data averaged over 5runs": "Sense-inducing systems.Comparison to Local-only systems will give a (strong) baseline just byinducing senses without aiming at concepts. Weused the same clustering algorithms. We also im-plement the WSI method proposed by Eyal et al.(2022). It relies on a different paradigm, using theLanguage Model for substitution instead of wordembeddings. From lists of substitutes, they build agraph of substitutes in which they find communitiesand then assign each occurrence to a community ofsubstitutes to find the wordsenses. Because Local-only methods only induce senses, their hyperpa-rameters are chosen to maximize a WSI objectiveon polysemous words of the dev split. BaselinesWe construct a candidate clusteringCW where each lemma has its own cluster. Thisbaseline model is referred to as the Lemmas base-line. This is to evaluate the extent to which theinformation contained by the lemma alone can beused to induce concepts without any knowledge onword senses neither on context. As a second base-line, we create for each lemma as many singletonsas the number of different concepts its occurrencesare annotated with. All created clusters are of size1: we account perfectly for polysemy but not atall for synonymy. This second baseline is dubbedOracle WSI.",
  "Concept Induction in SemCor": "In we display the Concept Induction scores(F1) of proposed baselines and systems on the fullSemCor data and on the Synon. split. On the fulldata, both the Lemmas and Oracle WSI baselinesachieve very good performance because they have,by design, a perfect precision (they do not clusterlemmas at all and do not overestimate the num- ber of clusters) and because 88% of concepts areinstantiated with only a single lemma (thus theirrecall is still good). However, they are very limitedon the Synon. split of the data, where concepts areinstantiated with multiple lemmas.The proposed Concept Induction systems reachscores ranging from .56 to .66 on the full data, halfof them outperforming the Lemmas baseline, andfrom .59 to .62 on the Synon. split, outperformingall other systems. While still challenging, it ex-hibits that it is indeed possible to induce WordNet-based concepts in a corpus using LMs hidden layersvectors.We also see that Kmeans-based approaches areconsistently outperformed by Agglomerative meth-ods. This indicates that the representational spacesin LM hidden layers are not organized in a nearly-spherical fashion as Kmeans algorithm assumes,but rather are populated less uniformly. This isreflected in precision and recall: Agglomerativesystems reach a higher precision than Kmeans withsimilar recall.Overall, results are in favor of Bi-level ap-proaches over Global-only systems, with substan-tial improvements in F1 on the full data while ob-taining (nearly) identical performance on conceptsof multiple lemmas, and large increases in pre-cision while the loss in recall is minimal. Thisdemonstrates that considering the local (lemma-centric) perspective is beneficial to a global (cross-lexicon) view when inducing concepts. The localclustering, with the subsequent representation aver-aging, helps reducing variance in occurrences andtherefore allow to reach higher levels of precisionin the global clustering compared to Global-only.We would also like to emphasize that, while Global-only systems are more simple in design, their com-putational cost is usually higher than Bi-level ones,especially when the clustering algorithms timecomplexity is quadratic with respect to the numberof occurrences.",
  "Qualitative Analysis of Concepts Clusters": "We manually annotate word clusters (obtained fromour best-performing approach, the Agglo Bi-levelsystem) containing at least 2 lemmas accordingto the semantic similarity between lemmas. Dis-tribution of cluster sizes (in number of lemmas)can be found in Appendix D. We distinguish fourcategories: synonyms when lemmas are cognitivesynonyms (e.g. necessity and need), near-synonyms for lemmas close to be synonyms but",
  ": Qualitative manual evaluation of obtained wordclusters of size 2": "showing slight difference in meaning (e.g. dutyand task, the former being stronger than the lat-ter),7 related when lemmas show a topical (e.g.dirt, sand and mud) or lexical relations (e.g.antonyms like man and woman) and invalidclusters when lemmas show no semantic relation(e.g. child and idea).Proportions of these annotations are displayed in with respect to the cluster size, the numberof lemmas in the cluster. For a given cluster size,if the number of clusters exceeds 50, we randomlysample 50 clusters to be annotated. Overall, theproportion of synonyms and near-synonyms is gen-erally above 50% and less than 10% of clustersare invalid, indicating that most learned conceptsare reliable and meaningful. We argue that the re-maining related term clusters, while not synonyms,may still be interesting in less fine-grained stud-ies. The portion of related clusters is in line withfindings from previous work showing that BERTwas also reflective of other lexical relations, suchas hypernymy (Hanna and Marecek, 2021).",
  "Benefits at the Local Level": "We now turn back to the local level and assesswhether the information brought at the global levelhelps distinguishing senses of individual words.Here we do not evaluate the word-level soft clus-tering, but the occurrence-level division of Sem-Cors data, considering each word independently.In other words, we evaluate WSI in SemCor usingannotations as the reference sense clustering. Evaluation of induced sensesFor each wordw W, we compare how its set of occurrencesOw is divided in C to how it is divided in the ref-erence C provided by annotations using BCubedmetrics, and we average scores obtained across W.We display the WSI BCubed F1, as in previous WSI",
  ": WSI BCubed F1and sense number correlationcoefficient on SemCor full data. Not computed forKmeans because the number of cluster is constant": "tasks like Jurgens and Klapaftis (2013). Follow-ing Amrami and Goldberg (2019), we report theSpearman correlation coefficient between the num-ber of clusters a lemma is assigned to and its num-ber of senses according to annotations, to ensurethat the number of created senses actually scaleswith the actual degree of polysemy.Note that, for CI systems, we evaluate the divi-sion of occurrences provided by the final cluster-ing C (i.e. how occurrences are clustered after theglobal step and its potential merge operations). Thequality of sense clusters induced by the local-steponly is actually evaluated with Local-only systems. Local results.Results of this local evaluationare displayed in . Let us recall that Local-only systems hyperparameters are chosen to max-imize the WSI F1on the dev split, while those ofCI systems maximize the Concept Induction F1.Nonetheless, one can observe that all CI systemsoutperform their Local-only counterparts, achiev-ing higher WSI F1and even though their hyper-parameters are not chosen to match the WSI itself.This indicates that the information brought at theglobal level by considering cross-lexicon relationsmay indeed help improving WSI, and benefits be-tween local and global levels go both ways.We explain the relatively poor performance ofState-of-the-Art WSI system by the fact that weare in a particular setting, where the number of oc-currences per lemma is relatively low in SemCor(30 per lemma on average) and so is the averagenumber of occurrences per concept. Data sparsityis a favorable ground for word senses to be misrep-resented. As such, methods meant to be appliedon larger datasets like the one of Eyal et al. (2022)may not work as well as expected. Our results showthe limitations of these systems when the amountof training data is low and the interest of aiming at",
  ": Accuracy scores on the nouns of the WiC testdataset (Pilehvar and Camacho-Collados, 2019)": "concepts to get senses. This scenario is motivatedin areas where data are not available in large quan-tities and still require to induce senses. In the caseof the study of Lexical Semantic Change (the evo-lution of word meanings over time), recent worksperform WSI in diachronic corpora that are oftenunbalanced and small (Tahmasebi et al., 2021).",
  "Extrinsic Evaluation withConcept-aware Embeddings": "In their work, Eyal et al. (2022) derive sense-awarestatic embeddings from their WSI method, train-ing them on the Wikipedia dataset and used themfor the Word-in-Context (WiC) task. They achievenearly-SotA results on the dataset proposed by Pile-hvar and Camacho-Collados (2019), and report tobe outperformed only by methods using externallexical knowledge and resources. We proceed tothe same extrinsic evaluation of our work, con-structing concept-aware embeddings using conceptclusters of Concept Induction systems (Global-onlyand Bi-level Agglo). To obtain such embeddings,we average all vectors representating occurrencesin SemCor contained each global cluster to get onevector per concept cluster.The WiC task consists of determining whethertwo occurrences of a target lemma w correspond tothe same sense. The WiC datasets target words arenouns and verbs, but like in the rest of this paper,we restrict our scope to nouns.To solve the task, we use BERT Large to createrepresentations of the two target occurrences. Eachof them is assigned to a concept by finding theclosest concept-aware using cosine distance. Thedecision depends on whether the two occurrencesare mapped to the same concept (true) or to dis-tinct ones (false). Results are displayed in .Our concept-aware embeddings obtain very similarresults to those of their sense-aware embeddings,with ours derived from our bi-level approach evenoutperforming their CBOW method. Interestingly,our embeddings were trained with far fewer re-sources than theirs, as we used 52 997 occurrences from the SemCor dataset while they used a dumpof Wikipedia, gathering millions of occurrences.This emphasizes the value of concept-aware embed-dings: the use of cross-lexicon information allowscompetitive results with fewer resources.",
  "Conclusion": "In this paper, we argued that, while word sensesallow to investigate polysemy, concepts are a largerperspective that allows the study of polysemy aswell as synonymy. We defined Concept Induction,the unsupervised task to learn a soft-clustering ofwords in a large lexicon, directly from their in-context occurrences in a corpus. Then, we pro-posed a formulation of this problem in terms oflocal (lemma-centric) and global (cross-lexicon)complementary views, and tested an approach thatuses information from both levels using contextu-alized Language Models. On concept-annotatedSemCor corpus, we found that this bi-level viewwas beneficial for Concept Induction, and even forWord Sense Induction with a low amount of train-ing data. We validated the quality of obtained clus-ters with manual annotations, ensuring that clustersmostly correspond to actual synonyms and con-cepts. Finally, we showcased an external applica-tion of our methodology to create concept-awareembeddings that can be competitive to other meth-ods on semantic tasks, such as Word-in-Context.Concept Induction opens the way for a differentperspective on lexical semantics in NLP, and canbe a basis for many studies of lexical meanings asit is expressive enough to reflect relations on bothsides of the word-meaning mapping.",
  "Limitations": "The formal framework we defined uses terminol-ogy and notions from rather structuralist/relationalassumptions of the languages lexical system (e.g.senses, discrete concepts, etc.).We made thischoice based on how lexical databases like Word-Net (and its derivatives), or other like the HistoricalThesaurus of English for instance, are designedusing the \"word/sense/concept\" structure. From apurely practical point of view, this choice makessense as these resources would be the primarysource for task datas annotations. Conceptually,senses are also a notion widely used in computa-tional linguistics and we wanted to propose Con-cept Induction as a step \"beyond\" this conventionalaspect and its related tasks. Future research may ex- plore definitions/extensions of Concept Inductionoutside of this structuralist/relational framework,towards cognitive semantics for instance (Geer-aerts, 2010).Evaluating Concept Induction is mainly limitedby the low amount of suitable annotated corpora.Not only the data need to be annotated in concepts,but these annotations must cover a wide varietyof lemmas for synonymy to be sufficiently repre-sented in the corpus. Future work may find or cre-ate datasets meeting these requirements to evaluateConcept Induction outside of SemCor.For now, the study is limited to nouns. Perfor-mances of benchmarked algorithms and systemsmay change with other Part-of-Speech tags.Our Bi-level method allows the global clusteringto merge local clusters, leveraging lexicon-levelinformation to be used to correct Word Sense In-duction errors at the lemma-level. By its sequentialnature, our method does not allow to split local clus-ters using global-level information, which couldlead to better results. Further research directionsinclude creating an iterative version of our method-ology (alternating local and global clustering), orattempting to tackle both clustering objectives si-multaneously with bi-level constrained clustering.Our results about sense-induction at the locallevel showed that usual WSI methods may not berobust in our setting where there are few occur-rences for some lemmas. We demonstrated that,in this setting, concept-inducing methods provideda better division in word senses. In many fieldsof linguistics, corpora are not very large and donot contain hundreds of occurrences for each word.Nonetheless, it is still uncertain if this observedadvantage of CI systems would still hold on big-ger datasets with many occurrences per lemma, asetting better-suited for usual WSI methods.In this paper, we limited our study to Nouns, themorpho-syntactic class exhibiting the most promi-nent semantic features. We leave to further researchthe study of Concept Induction for Verbs, Adjec-tives, or the heterogeneous family of Adverbs.",
  "Asaf Amrami and Yoav Goldberg. 2019. Towards bettersubstitution-based word sense induction": "Amit Bagga and Breck Baldwin. 1998. Entity-basedcross-document coreferencing using the vector spacemodel. In 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics, Volume 1,pages 7985, Montreal, Quebec, Canada. Associationfor Computational Linguistics. Yuri Bizzoni, Federico Boschetti, Harry Diakoff, Ric-cardo Del Gratta, Monica Monachini, and GregoryCrane. 2014. The making of Ancient Greek WordNet.In Proceedings of the Ninth International Conferenceon Language Resources and Evaluation (LREC14),pages 11401147, Reykjavik, Iceland. European Lan-guage Resources Association (ELRA). Gabriella Chronis and Katrin Erk. 2020. When is abishop not like a rook? when its like a rabbi! multi-prototype BERT embeddings for estimating semanticrelationships. In Proceedings of the 24th Confer-ence on Computational Natural Language Learning,pages 227244, Online. Association for Computa-tional Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Kawin Ethayarajh. 2019. How contextual are contextu-alized word representations? Comparing the geom-etry of BERT, ELMo, and GPT-2 embeddings. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 5565,Hong Kong, China. Association for ComputationalLinguistics. Matan Eyal, Shoval Sadde, Hillel Taub-Tabib, and YoavGoldberg. 2022. Large scale substitution-based wordsense induction. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 47384752,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Dirk Geeraerts. 2010. Theories of Lexical Semantics.Oxford University Press": "Sana Ghanem, Mustafa Jarrar, Radi Jarrar, and IbrahimBounhas. 2023. A benchmark and scoring algorithmfor enriching Arabic synonyms. In Proceedings ofthe 12th Global Wordnet Conference, pages 274283,University of the Basque Country, Donostia - SanSebastian, Basque Country. Global Wordnet Associa-tion. Janosch Haber and Massimo Poesio. 2021. Patterns ofpolysemy and homonymy in contextualised languagemodels. In Findings of the Association for Computa-tional Linguistics: EMNLP 2021, pages 26632676,Punta Cana, Dominican Republic. Association forComputational Linguistics.",
  "Martin Haspelmath. 2023. Coexpression and synexpres-sion patterns across languages: comparative conceptsand possible explanations. Frontiers in Psychology,14": "David Jurgens and Ioannis Klapaftis. 2013. SemEval-2013 task 13: Word sense induction for graded andnon-graded senses. In Second Joint Conference onLexical and Computational Semantics (*SEM), Vol-ume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 290299, Atlanta, Georgia, USA. Associationfor Computational Linguistics. Fahad Khan, Francisco J. Minaya Gmez, RafaelCruz Gonzlez, Harry Diakoff, Javier E. Diaz Vera,John P. McCrae, Ciara OLoughlin, William MichaelShort, and Sander Stolk. 2022. Towards the construc-tion of a WordNet for Old English. In Proceedings ofthe Thirteenth Language Resources and EvaluationConference, pages 39343941, Marseille, France. Eu-ropean Language Resources Association. Andrey Kutuzov and Mario Giulianelli. 2020. UiO-UvA at SemEval-2020 task 1: Contextualised em-beddings for lexical semantic change detection. InProceedings of the Fourteenth Workshop on SemanticEvaluation, pages 126134, Barcelona (online). Inter-national Committee for Computational Linguistics. Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,and Sameer Pradhan. 2010. SemEval-2010 task 14:Word sense induction &disambiguation. In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 6368, Uppsala, Sweden. Associa-tion for Computational Linguistics. Matej Martinc, Syrielle Montariol, Elaine Zosa, andLidia Pivovarova. 2020. Capturing evolution in wordusage: Just add more clusters? In Companion Pro-ceedings of the Web Conference 2020, WWW 20,page 343349, New York, NY, USA. Association forComputing Machinery.",
  "Sathvik Nair, Mahesh Srinivasan, and Stephan Mey-lan. 2020. Contextualized word embeddings encodeaspects of human-like word sense knowledge": "Mohammad Taher Pilehvar and Jose Camacho-Collados.2019. WiC: the word-in-context dataset for evalu-ating context-sensitive meaning representations. InProceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pages 12671273,Minneapolis, Minnesota. Association for Computa-tional Linguistics. Alessandro Raganato, Jose Camacho-Collados, andRoberto Navigli. 2017. Word sense disambiguation:A unified evaluation framework and empirical com-parison. In Proceedings of the 15th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Volume 1, Long Papers, pages99110, Valencia, Spain. Association for Computa-tional Linguistics.",
  "Nina Tahmasebi, Lars Borin, and Adam Jatowt. 2021.Survey of computational approaches to lexical se-mantic change detection. Computational approachesto semantic change, 6(1)": "Dan John Velasco, Axel Alba, Trisha Gail Pelagio,Bryce Anthony Ramirez, Jan Christian Blaise Cruz,Unisse Chua, Briane Paul Samson, and CharibethCheng. 2023. Towards automatic construction of Fil-ipino WordNet: Word sense induction and synset in-duction using sentence embeddings. In Proceedingsof the First Workshop in South East Asian LanguageProcessing, pages 112, Nusa Dua, Bali, Indonesia.Association for Computational Linguistics. Jingqing Zhang, Luis Bolanos Trujillo, Tong Li, Ash-wani Tanwar, Guilherme Freire, Xian Yang, Julia Ive,Vibhor Gupta, and Yike Guo. 2021. Self-superviseddetection of contextual synonyms in a multi-class set-ting: Phenotype annotation use case. In Proceedingsof the 2021 Conference on Empirical Methods in Nat-ural Language Processing, pages 87548769, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics.",
  "|g(w1) g(w2)|": "with w1 and w2 two lemmas, and g a referenceclustering function and f the clustering functionwe want to evaluate. MP (resp. MR) can be com-puted for every lemma w1 with every other lemmaw2 sharing at least one cluster with w1 in f (resp.in g). We denote MP(w1, ) and MP(w1, ) theobtained averages. In the case of non-overlappingclusters, this formulation gives the same result asthe original (non-extended) BCubed. To evaluateWSI, the formulation is the same but we do notevaluate at the word-level but at the occurrence-level.",
  "By default we fix = 1, as we compare the learnedclustering and the reference clustering as equalsand therefore do not find that Precision and Recallshould be weighted differently": "Amig et al. (2009) showed that the benefitsof BCubed over other clustering scores. For in-stance, Rand Index does not handle well the caseof many small clusters, which is likely to be thecase for Concept Induction. We also prefer Ex-tended BCubed over Overlapping Normalized Mu-tual Information (McDaid et al., 2011) as the latteris matching-based. That is, the repetition (or non-repetition) of identical clusters will have no impacton the measure. However, we can easily imagineidentical clusters of words to be repeated as theymay correspond to distinct concepts. In ExtendedBCubed, repeated clusters are taken in account aswe measure the number of times two lemmas areclustered together. The denominator of MP ensuresthat over-estimating the number of common clus-ters is also penalized, and those of MR ensures thatunder-estimating is penalized. Min operators arethere to prevent both quantities to grow over 1.",
  "C.1CLM layers": "Prior work like Ethayarajh (2019) showed thatlater layers usually correlates with deeper levels ofcontextualization and more semantic information,Chronis and Erk (2020) showed that moderately-late were preferred for lexical similarity while verylast layers were preferred for semantic relatedness.To get embeddings, we try 4 sets of layers corre-sponding to different depths: first layers (1 to 4),",
  "SystemsBest hyperparameters": "Local-only Kmeansk = 3Local-only Agglolinkage = average, = 1.0Global-only Kmeans = 120%Global-only Agglolinkage = average, = 3.5Bi-level Kmeansk = 8, = 120%Bi-level Agglolinkage = average (both), local = 0.0, global = 4.5Bi-level Kmeans (local Agglo)linkage = average local = 0.0, = 120%Bi-level Agglo (local Kmeans)k = 10, linkage = average, global = 4.5",
  ": Best hyperparameters on the Dev split": "moderately early layers (8 to 11), moderately late(14 to 17), and last layers (21 to 24). To get therepresentation of a words occurrence, we simplyaverage its embeddings from the four chosen layersinto one single 1024-dimensional embedding. ForConcept Induction, we find that best results wereobtained using layers 14 to 17, that are the reportedresults.",
  "C.2Hyperparameters": "For Eyal et al. (2022), we tried different resolu-tion, varying it from 1e-3 to 10, for the Louvainclustering but found very little to no effect.For Kmeans at the local level, we varied the num-ber of clusters k between 2 and 10. For Agglom-erative clustering at both levels, we tried single,average and complete linkage.The distance threshold in Agglo was indexedon the distribution of distances. We fixed an hyper-parameter and derived = avg(d) . std(d)with d the distribution of distances between clus-tered instances. We made vary between -4 and+8. For global Kmeans, the number of clusterswas indexed using a proportion on the number oflemmas (e.g. 120% W), varying from 40% to400%. This may help transfering hyperaparametersto other dataset in future research.Best hyperaparameters choices are in"
}