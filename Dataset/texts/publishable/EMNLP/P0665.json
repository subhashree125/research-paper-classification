{
  "Abstract": "Reinforcement learning shows promise in op-timizing dialogue policies, but addressing thechallenge of reward sparsity remains crucial.While curriculum learning offers a practical so-lution by strategically training policies fromsimple to complex, it hinges on the assump-tion of a gradual increase in goal difficulty toensure a smooth knowledge transition acrossvaried complexities. In complex dialogue envi-ronments without intermediate goals, achievingseamless knowledge transitions becomes tricky.This paper proposes a novel Bootstrapped Pol-icy Learning (BPL) framework, which adap-tively tailors progressively challenging subgoalcurriculum for each complex goal through goalshaping, ensuring a smooth knowledge transi-tion. Goal shaping involves goal decompositionand evolution, decomposing complex goals intosubgoals with solvable maximum difficulty andprogressively increasing difficulty as the policyimproves. Moreover, to enhance BPLs adapt-ability across various environments, we explorevarious combinations of goal decompositionand evolution within BPL, and identify two uni-versal curriculum patterns that remain effectiveacross different dialogue environments, inde-pendent of specific environmental constraints.By integrating the summarized curriculum pat-terns, our BPL has exhibited efficacy and ver-satility across four publicly available datasetswith different difficulty levels.",
  ": Example user goals with increasing complexity": "User goal g1: The user wants to book a flight ticket fromNew York to Los Angeles today.User goal g2: The user wants to book a direct flight fromNew York to Los Angeles today and reserve ahotel room for one night at the departure city.User goal g3: The user wants to book a business class flightticket for an evening flight from New York toLos Angeles today. Additionally, they need toreserve two nights of hotel rooms at both thedeparture and arrival cities, and book ticketsto nearby attractions for two people. state tracking, dialogue policy (DP), and naturallanguage generation(NLG) (Chen et al., 2017).Among these, DP plays a pivotal role in determin-ing system responses based on dialogue state input,directly influencing system success (Zhang et al.,2022b). While large language models (LLMs) in-deed exhibit vast potential, the end-to-end frame-work introduces challenges in controllability and in-terpretability (Rohmatillah et al., 2023). Therefore,it is more common to leverage LLMs for replacingspecific components within pipeline frameworks ofTOD systems, such as NLG (Zeng et al., 2024),NLU (Mirza et al., 2024), or word-level compo-nents (Yi et al., 2024), rather than all componentsin pipelines. Reinforcement learning (RL) emergesas a preferred DP approach due to its adeptness insequential decision-making. However, optimizingdialogue policies using RL faces hurdles due to thesparse dialogue goal rewards, requiring extensiveexploration to achieve the goal and trigger learningsignals (Kwan et al., 2023; Takanobu et al., 2020). Curriculum Learning (CL) strategically ordersDP learning from easy to difficult to alleviate re-ward sparsity challenges. This ordered learningstrategy allows DP to use simpler goals knowledgeor skills as a foundation for tackling more complexones (known as knowledge transition) (Narvekaret al., 2020; Geishauser et al., 2022). These CL-based methods typically require goal difficulty toincrease gradually over time. However, in complex dialogue scenarios where intermediate goals areabsent, achieving smooth knowledge transitions be-comes tricky. Taking Tab.1 as an example, existingCL methods typically rank user goals from easy(g1) to difficult (g3), facilitating smooth transitions.Nonetheless, in complex dialogue environments,simpler goals like g1 and g2 are often absent asintermediate steps. Training difficult goals g3 di-rectly requires numerous rounds of interactions toyield meaningful rewards, ultimately diminishinglearning efficiency (Lu et al., 2019).To this end, this paper introduces BootstrappedPolicy Learning (BPL), a novel framework utiliz-ing goal shaping to dynamically tailor a subgoalcurriculum for each complex user goal. This cur-riculum progressively increases in difficulty to en-sure smooth knowledge transitions. Goal shapinginvolves two operations: goal decomposition andevolution. Goal decomposition decomposes com-plex goals into subgoals with solvable maximumdifficulty, reducing their complexity. Meanwhile,goal evolution gradually increases the difficulty ofsubgoals in line with the policys growing capa-bilities, ultimately enabling mastery of the entiregoal. On the one hand, BPL efficiently guides thepolicys progression from easier to more difficultgoals, ensuring a smooth knowledge transition. Onthe other hand, the customized subgoal curricu-lum aligns with the policys developing abilities,enhancing training efficiency.To enhance BPLs generality across diverse en-vironments, we explore various combinations forgoal decomposition and evolution within the BPLframework and identify optimal combination pat-terns for dialogue datasets with different difficultycharacteristics. It allows BPL practitioners to ef-ficiently learn dialogue policies in future datasetsby selectively choosing suitable BPL combinationsbased on these identified patterns. Additionally, weidentify two universal, dataset-independent combi-nation patterns that maintain effectiveness acrossvarious dialogue environments, independent of spe-cific environmental constraints. In summary, ourcontribution is three-fold: We propose a novel Bootstrapped Policy Learn-ing (BPL) framework that dynamically tailors sub-goal curriculum through goal shaping, facilitatingsmooth knowledge transition. We extract optimal combination patternswithin the BPL framework, facilitating the selec-tion of suitable BPL combinations for diverse di-alogue datasets, broadening its applicability and potential impact across various environments. We identify two universal combination pat-terns, which transcend dataset-specific constraintsand outperform existing CL approaches across aspectrum of dialogue datasets.",
  "Related Work": "Our work is closely related to two areas of research:curriculum learning and goal decomposition inpipeline-based task-oriented policies.Curriculum learning (CL) has proven its efficacyin accelerating learning in both supervised learning(Bengio et al., 2009; Zhang et al., 2022a) and rein-forcement learning (RL) (Florensa et al., 2017; Renet al., 2018; Whlke et al., 2020; Wu and Vorob-eychik, 2022; Klink et al., 2022). As a naturalextension, the integration of CL with deep RL fordialogue policies has been progressively garneringmore attention (Saito, 2018; Zhao et al., 2021; Liuet al., 2021; Zhao et al., 2022), aiming to enhancelearning efficiency through well-structured curricu-lum sequences. Earlier approaches relied on man-ually pre-defined goal sequences (Wu et al., 2018;Budzianowski et al., 2018). For instance, (Saito,2018) employed a coarse-grained criterion, artifi-cially defining the number of slots in user goalsfor curriculum sequencing. However, such crite-ria lack precision in achieving optimal curriculumsequencing. (Zhao et al., 2021) addressed this by in-troducing an RL-based teacher model consideringboth efficiency and diversity in curriculum sequenc-ing. However, this approach incurs additional costsfor teacher model design and training. Meanwhile,(Liu et al., 2021) and (Zhao et al., 2022) proposeddistinct difficulty evaluation criteria based on usergoals, involving the differential space of dialoguestates and cumulative rewards obtained. Yet, thesemethods assume each user goal is trained at leastonce to calculate its difficulty scores. In summary,prior research mainly focused on meticulous goalsequencing in curriculum policy learning. In con-trast, our approach allows for the dynamic creationof an intrinsic subgoal curriculum tailored to eachcomplex goal. Moreover, as highlighted in theintroduction, existing methods struggle with com-plex environments lacking intermediate goals forsmooth knowledge transitions, a gap our proposedapproach seeks to address.Our framework integrates goal decompositionalgorithms. Current algorithms are limited to eas-ily decomposable multi-domain goals (Peng et al.,",
  ": Illustration for dialogue policy learning usingproposed BPL framework": "2017) or rely on extensive successful experiencefor training the subgoal discovery network (SDN)(Tang et al., 2018). However, accumulating suchextensive data might be undesirable or unneces-sary, given the promising performance of other CL-based dialogue policies. In contrast, our approacheliminates the need for manual decomposition orsignificant data costs. Instead, the BPL frameworkautomatically generates an intrinsic subgoal cur-riculum for each complex goal based solely on acoarse-grained difficulty criterion.",
  "Bootstrapped Policy Learning": "This section introduces our Bootstrapped PolicyLearning (BPL) framework, as depicted in ,composed of two integral components: Decom-poser and Evolver. The user simulator randomlyselects a user goal to start the conversation. If thedialogue fails, Decomposer decomposes the usergoal into a subgoal with solvable maximal diffi-culty for goal decomposition. Conversely, upondialogue success, Evolver increases the complexityof the subgoal for goal evolution, until the dialoguepolicy masters the entire goal.",
  "Difficulty Criteria for User Goals": "This section introduces fundamental concepts suchas user goal, entire goal, subgoal, and current usergoal. Additionally, it outlines a coarse-graineddifficulty criterion for user goals, crucial for con-structing both Decomposer and Evolver, offeringtheoretical foundation. 3.1.1User GoalUser goals describe user needs and dialogue objec-tives. Typically, a user goal g comprises a set ofconstraints C and requests R, where C denotes theinformation constrained provided by the user andR denotes the information required by the user (Lu et al., 2019). To represent user goals, we assume aset of slot names S and the domain of values V (s)for each slot name s. An information constraintprovided by the user is of the form s = v for s Sand v V (s), indicating that slot s has value v.A user request is then considered as a set of slotnames for which the user seeks values.Taking a train-ticket booking as an example, theuser goal is to inquire about the departure time oftodays trains from A to B, where the user goal g isin the following form:",
  "|gent|if conversation success,Rminif conversation fails,1otherwise(2)": "where gent is the entire goal corresponding to thecurrent user goal gcurr, |g|, indicates the total num-ber of slots in g, Rmax is the maximum reward forsuccessful completion of gent, Rmin is the penaltyfor a failed user goal, and 1 serves as a fixedpenalty to encourage shorter dialogues, with thesevalues set within Sec.4.2. 3.1.2Difficulty EvaluationDialogue success hinges on accurately identifyingall provided information C from the user, respond-ing correctly to all user requests R, and success-fully booking a ticket meeting the specified infor-mation. Thus, the difficulty of user goal g variesbased on the number of information and requestsin C and R. Fewer constraints and requests re-sult in fewer agent actions required to complete g,reducing error risks. The user goals difficulty is",
  "Definition 4 Goal Difficulty D(gi) = |Ci| + |Ri|,where |Ci| is the number of Ci in user goal gi, and|Ri| is the number of Ri in user goal gi": "Consider the user goal g in Equ.1, its difficultyis 4, calculated as D(g) = |C| + |R| = 3 + 1 = 4.While other factors influence user goal difficulty(e.g., the differential space of dialogue states (Liuet al., 2021) and the cumulative rewards obtained(Zhao et al., 2022), the sum of slot entropies (Pa-pangelis et al., 2017)), precisely defining them ischallenging. Our approach avoids this by using thecoarse-grained criterion outlined above.Based on this difficulty measure, we introducethe core concepts of goal shaping:",
  "Decomposer": "The left side of depicts changes in the usergoal gi during goal decomposition, involving: i)Boundary state detection, identifying state s4 near-est to the goal state within a failed dialogue trajec-tory of gi; ii) Goal Decomposition, dividing thecurrent user goal gi into a corresponding boundarysubgoal based on the detected boundary state s4;iii) Goal Substitution, substituting the current usergoal gi with the boundary subgoal (the orange one).",
  "Boundary State Detection": "A state qualifies as a boundary state under the fol-lowing conditions:i) All slot-value pairs in the state are presentin the goal state; A dialogue state st captures thedialogue session until time t, including the currentuser action, previous agent action, dialogue history,and mentioned slot-value pairs. The goal state sgcontains all slot-value pairs representing g1.ii) the distance d between the boundary stateand the goal state is the shortest. d is determinedby the number of mismatched slot-value pairs: d =N(sg) N(s), where where N(sg) represents thenumber of slots contained in the goal state sg. Infact, N(sg) = D(g). N(s) represents the numberof slots in the current state s that are the same asthose in the goal state sg. The difference betweenthe two indicates the distance from the current states to the goal state sg.In cases of multiple boundary states, the mostrecent state is selected as the boundary state, asit took more dialogue rounds to reach this state.If no state in the dialogue trajectory matches anyslot-value pair of the goal state, a slot-value pairis randomly selected from the inform_slot set inthe goal as the boundary state.",
  "Decomposition Condition": "The decomposers role is to decompose tricky usergoals during training, and avoid decompositionfor simple goals. Three decomposition conditionsguide BPL in identifying optimal moments for de-composition across dialogue scenarios:Failure at any time (A): the decomposer activateswhenever a user goal fails.Failure after training for N2 epochs (T): a usergoal persists failing after N epochs of policy learn-ing, it undergoes decomposition.Failure M times consecutively2 (C): User goalsfailing consecutively M times indicate surpassingthe policys capability, prompting decomposition.",
  "Evolver": "The right side in depicts changes in the usergoal gi during goal evolution, comprising threestages: i) Evolutionary segmentation, dividing thefailed subgoal into an evolved part for subgoal evo-lution and a retained part for the next iteration,based on the dialogue policys performance. ii)Subgoal evolution, merging the evolved part andthe current goal gi into a new goal. iii) Goal Substi-tution, replacing the original user goal gi with theevolved new goal.",
  "Evolutionary Segmentation": "This stage randomly allocates slot-value pairs froma failed subgoal to the evolved and retained parts,depending on the dialogue policys capability. Poli-cies with better performance allocate more pairs tothe evolved part. To strike a balance, we exploremethods to assess segmentation strategies.Fixed number of slots (F): Only one slot as theevolved part, regardless of policy ability.Obtained rewards control (R): Inspired by (Zhaoet al., 2022), the number of evolved slots NoEis determined by comparing cumulative rewardsR to Rgmax for subgoal g, calculated as NoE =Rg Rgmax Ngf , where Ngf is the failed subgoalsslot count, Rg is cumulative rewards obtainedby the agent after executing user goal g, Rgmaxis the maximum reward that can be obtained bycompleting this user goal, calculated as Rgmax =Rmax gcurr/gent, and is the floor function.",
  "Experiment": "Our experiments utilize four datasets:Movie-Ticket Booking, Restaurant Reservation, Taxi Or-dering, and Multiwoz 2.1 (Li et al., 2016, 2018;Budzianowski et al., 2018). The first three aresingle-domain datasets with varying difficulty lev-els, while Multiwoz 2.1 spans seven domains. Forthe single-domain experiments, we employed theMicrosoft Dialogue Challenge platform, which pro-vides a unified experimental environment, standard-ized datasets, and publicly available rule-based usersimulators, facilitating collaboration and bench-marking within the dialogue research community.Multi-domain experiments using the Multiwozdataset were conducted on the ConvLab-2 platform,which also offers standardized datasets and a pub-licly available agenda-based simulator4. To betterevaluate our method, we also conducted experi-ments with human users, as shown in .5.User goal difficulty positively correlates withslot count (Zhao et al., 2021; Liu et al., 2021;",
  ": The distribution of the number of slots foruser goals in each dataset": "Budzianowski et al., 2018). Slot distributions wereplotted for each dataset, as shown in , reveal-ing 5 to 7 slots for movie goals (indicating simplic-ity), 5 to 12 slots for restaurant goals (reflectingvaried difficulty), and 8 to 13 slots for taxi goals(suggesting higher complexity). In summary, dif-ficulty levels were: Movie = Easy, Restaurant =Moderate, Taxi = Difficult. Multiwoz 2.1 domainsizes aided in controlling slot counts for datasetsof different difficulty levels.We first analyze and summarize the optimal BPLcombination patterns in dialogue environmentsacross varying difficulty characteristics. Then, wevalidate the effectiveness and adaptability of thesesummarized patterns within three Multiwoz 2.1-based environments with varying difficulty levels.",
  "Baselines": "This paper aims to deal with the constraints im-posed by CL in the application of RL-based task-oriented dialogue policies by goal decompositiontechniques. Therefore, we selected all the state-of-the-art techniques (including RL-based task-oriented dialogue policies with CL or goal decom-position), as well as the standard baseline modelDQN, for comparison:Standard baseline model",
  "Settings": "We standardized common parameters across allmodels for fairness and selected unique optimal pa-rameters for each model. All models use a single-layer perceptron with 80 neurons and RMSpropoptimizer, with fixed hyperparameters: learningrate at 0.001, batch size at 16, and discount fac-tor at 0.95. The experience replay buffer size is10, 000. During training, an -greedy strategy with = 0.1 is used for exploration. To mitigate cur-riculum sequencing cost, only 120 dialogues areutilized for warm start, curriculum initialization(if necessary), and training the subgoal discoverynetwork in SDN5. A total of 500 epochs are allo-cated for joint training of the dialogue policy andcurriculum fine-tuning. Reward parameters are set",
  "Fixed number of slots (F)BPL-CRObtained rewards control (R)BPL-CEExploration degree control (E)": "with Rmax at 2L and Rmin at L, Eq.2. L denotesthe maximum allowed number of dialogue turns,which defaults to 40 across all domains. For a faircomparison, we use the ground-truth goal informa-tion to decompose and evolve user goals duringthe training phase. However, we do not use anygoal information during the test phase. We utilizeground-truth goal information to decompose andevolve user goals during the training phase but donot use any goal information during testing. Thespecifics are as follows:Training Phase: At the start of a dialogue,the user simulator initiates the dialogue with auser goal randomly sampled from the trainingset. Throughout the dialogue, the dialogue pol-icy has no ground-truth goal information. If thedialogue fails, BPL decomposes the sampled goalbased on the dialogue trajectory and ground-truthgoal. If the dialogue succeeds and the sampled goalis a sub-goal, BPL evolves the user goal.Test Phase: The user simulator begins the dia-logue by randomly sampling a user goal from thetest goals set. Similar to training, the policy doesnot access ground-truth information. We then eval-uate performance using the metrics provided by the datasets, including success rate, average turns,and average rewards. All results are computed overten runs of 1,000 dialogues, with each run testedon 100 dialogues with different random seeds aftertraining on a single dialogue. We conducted statis-tical tests using the t-test. The differences betweenthe results of all agent pairs evaluated at the sameepoch are statistically significant (p < 0.05).",
  "Analysis": "To explore optimal combinations patterns in diversedialogue environments and establish a general ap-proach, we conducted analytical experiments, withmain results presented in Tab.2 (abbreviated inTab.3)6. Findings based on experimental resultsare summarized below:For low-difficulty datasets Early decomposition (Condition A) outper-forms delayed decompositions (T and C), suggest-ing dialogue policies struggle with reward scarcityeven in simple datasets. Combining Condition C with evolutionary seg-mentation (R and E) yields poor results, as simpleuser goals are less prone to continuous failures,limiting the benefits of BPL. Combining Condition A with evolutionary seg-mentation (R or E) efficiently meets learning de-mands in low-difficulty datasets, resulting in signif-icant performance improvements.In low-difficulty datasets, the optimal combi-nation is early decomposition (A) combined withevolutionary segmentation (R or E).For medium difficulty datasets: Condition A performs worse than T and C due",
  "BPL-CF0.6664-5.5312.661st0.662819.5310.040.3001-17.4929.00BPL-CR0.6950-2.3017.520.51173.5812.810.3329-12.8827.66BPL-CE0.71260.4910.060.4983-1.2014.540.3423-11.7327.62": "to a mix of simple and difficult user goals, where Tand C accurately identify difficult goals. Evolutionary segmentation (F) outperformsR or E, as gradual difficulty increases align withimproving policy capability.For medium-difficulty datasets, the optimal com-bination pattern includes selecting difficult goalidentification (T or C) and evolutionary segmenta-tion with gradual difficulty increase (F).For high-difficulty datasets: Early decomposition (Condition A) is crucialfor improving learning efficiency and final perfor-mance, outperforming other conditions. Evolution of difficult user goals should involvea gradual difficulty increase, rendering evolutionarysegmentation (R or E) with a significant difficultyboost unsuitable.For high-difficulty datasets, the optimal combi-nation pattern involves early decomposition (Condi-tion A) and gradual increases in user goal difficultythrough evolutionary segmentation (F).The universal good combination:BPL-TF/TE, involving difficult goal identifica-tion (Condition T) and slow or adaptive evolution(F or E), shows effectiveness across various dia-logue environments, promising broad applicability.In summary, utilizing the outlined optimal cur-riculum patterns allows strategic selection of suit-able BPL combinations for efficient dialogue policylearning, adapted to future datasets with differentdifficulties. When encountering unknown difficul-ties of future datasets, adopting the universal cur-riculum pattern adeptly handles diverse dialogueenvironments. Crucially, the BPL framework of-",
  "Validation": "As per prior research findings (Zhao et al., 2021;Liu et al., 2021; Budzianowski et al., 2018), thedatasets difficulty correlates with the distributionof slots in user goals in the datasets. In a multi-domain environment, the more domains involvedin user goals, the more slots are included, thusincreasing the datasets difficulty. Therefore, wecontrol the difficulty of the MultiWOZ dataset bymanipulating the number of domains involved inthe user goals.To validate the effectiveness and generality ofthe BPL framework, we selected two optimal com-binations and two universal combinations based onsummarized curriculum patterns. These methodswere compared with baselines across datasets fromMultiwoz 2.1, featuring diverse difficulties. Resultsin Tab.4 align with Analysis experiments. DQNexcels in simple domains but struggles in mediumand difficult ones due to random user goal selec-tion. SNA-DQN and SND-DQN establish learningsequences based on slot difficulty, impacting ef-ficiency, yet inflexible curricula hamper learning,especially in challenging domains. VACL and SDNuse precise criteria but require pre-assessment data.ACL-DQN improves in simple and medium do-mains but faces challenges in difficult ones. HRLexcels in multi-domains but struggles in single-domains with complex goals. Conversely, BPLsoutperform baselines, especially those selected us- ing optimal curriculum patterns, underscoring tai-lored combinations efficacy in known difficultydatasets. Universal BPL combinations remain ben-eficial in uncertain difficulty scenarios. Overall,BPL proves versatile and effective across variouschallenging dialogue tasks, showcasing consistentperformance across diverse datasets. Besides the experiments presented in the mainbody of the paper, we have conducted supplemen-tary experiments, to further analyze the reasonsbehind the outstanding performance of BPL. Theseresults are available in Appendices B and C.",
  "Human Evaluations": "We evaluated the validation of BPL through hu-man evaluation involving 98 student participants,employing established metrics consistent with ourstudys datasets and commonly used ones (e.g.,(Liu et al., 2021; Peng et al., 2017; Zhao et al.,2022; Tang et al., 2018)): success rate (SR) and av-erage scores (AS) ranging from 1 to 5. These met-rics gauge naturalness, coherence, and task com-pletion capability. Participants interacted with dia-logue systems by engaging with randomly selecteduser goals matched in difficulty, unaware of the spe-cific algorithm employed. They could discontinueinteraction if deemed unproductive. The results,derived from at least 35 meaningful dialogues, to-taled 1460 collected dialogues. Human evaluationresults, illustrated in Tab. 5, showcased superiorperformance compared to the baseline, consistentwith simulated experiment outcomes.",
  "Conclusion and future work": "This study introduces a novel BPL frameworkadept at handling diverse environments with dif-fering difficulty levels, thereby facilitating efficienttask-oriented dialogue policy learning. It dynam-ically generates progressive subgoal curricula forcomplex goals through goal shaping, involving twokey processes: 1) goal decomposition, extractingsolvable boundary subgoals from user goals basedon dialogue trajectories, and 2) goal evolution, pro-gressively increasing the difficulty of subgoals untilmastery of the entire goal. To enhance the versa-tility of BPL, we systematically explore variouscombinations of goal decomposition and evolutionwithin the framework, summarizing optimal cur-riculum patterns for dialogue datasets with varyingdifficulty levels. By incorporating curriculum pat-terns, the BPL framework can selectively choosesuitable combinations to handle dialogue datasetswith known difficulty characteristics. Moreover,we identify two universal combination patterns thatmaintain effectiveness and generality across diversedialogue environments, irrespective of specific en-vironmental constraints. In the future, our focuswill delve into mechanisms for transferring knowl-edge acquired from subgoals to new agents.",
  "Limitation": "A limitation of most curriculum learning meth-ods, including our approach, is that the knowledgelearned from subgoals/previous tasks is only trans-ferred and accumulated to the current agent forlearning on a specific dataset. Either replacingthe dataset or replacing the agent requires retrain-ing again. Thus, an interesting question for futurework is: how can we transfer and accumulate suchknowledge to the new agents and datasets? We sincerely thank the reviewers for their valu-able feedback and constructive comments, whichgreatly improved the paper. We also acknowledgethe volunteers from the Changsha University of Sci-ence and Technology who assisted with our humanexperiments. This work was supported by HunanProvincial Natural Science Foundation of China(Grant No.2024JJ6062).",
  "Hongshen Chen, Xiaorui Liu, Dawei Yin, and JiliangTang. 2017.A survey on dialogue systems: Re-cent advances and new frontiers. SIGKDD Explor.,19(2):2535": "Carlos Florensa, David Held, Markus Wulfmeier,Michael Zhang, and Pieter Abbeel. 2017. Reversecurriculum generation for reinforcement learning. In1st Annual Conference on Robot Learning, volume 78of Proceedings of Machine Learning Research, pages482495. PMLR. Christian Geishauser, Carel van Niekerk, Hsien-ChinLin, Nurul Lubis, Michael Heck, Shutong Feng, andMilica Gasic. 2022. Dynamic dialogue policy forcontinual reinforcement learning. In Proceedings ofthe 29th International Conference on ComputationalLinguistics, COLING 2022, Gyeongju, Republic ofKorea, October 12-17, 2022, pages 266284. Inter-national Committee on Computational Linguistics. Pascal Klink, Haoyi Yang, Carlo DEramo, Jan Peters,and Joni Pajarinen. 2022. Curriculum reinforcementlearning via constrained optimal transport. In ICML,volume 162 of Proceedings of Machine LearningResearch, pages 1134111358. PMLR. Wai-Chung Kwan, Hongru Wang, Huimin Wang, andKam-Fai Wong. 2023. A survey on recent advancesand challenges in reinforcement learning methods fortask-oriented dialogue policy learning. Int. J. Autom.Comput., 20(3):318334. Xiujun Li, Yun-Nung Chen, Lihong Li, JianfengGao, and Asli Celikyilmaz. 2017. End-to-end task-completion neural dialogue systems. In IJCNLP,pages 733743. Asian Federation of Natural Lan-guage Processing.",
  "Goal-oriented dialogue policy learning from failures.In AAAI, pages 25962603. AAAI Press": "Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo,and Sinchana Ramakanth Bhat. 2024. ILLUMINER:instruction-tuned large language models as few-shotintent classifier and slot filler. In Proceedings of the2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion, LREC/COLING 2024, 20-25 May, 2024, Torino,Italy, pages 86398651. ELRA and ICCL. Sanmit Narvekar, Bei Peng, Matteo Leonetti, JivkoSinapov, Matthew E. Taylor, and Peter Stone. 2020.Curriculum learning for reinforcement learning do-mains: A framework and survey. J. Mach. Learn.Res., 21:181:1181:50. Alexandros Papangelis, Stefan Ultes, and YannisStylianou. 2017.Domain complexity and policylearning in task-oriented dialogue systems. In Ad-vanced Social Interaction with Agents - 8th Interna-tional Workshop on Spoken Dialog Systems, IWSDS2017, Farmington, PA, USA, 6-9 June 2017, revisedselected papers, volume 510 of Lecture Notes in Elec-trical Engineering, pages 6369. Springer. Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, AsliCelikyilmaz, Sungjin Lee, and Kam-Fai Wong. 2017.Composite task-completion dialogue policy learn-ing via hierarchical deep reinforcement learning. InEMNLP, pages 22312240. Association for Compu-tational Linguistics. Zhipeng Ren, Daoyi Dong, Huaxiong Li, and Chun-lin Chen. 2018. Self-paced prioritized curriculumlearning with coverage penalty in deep reinforcementlearning. IEEE Trans. Neural Networks Learn. Syst.,29(6):22162226.",
  "Da Tang, Xiujun Li, Jianfeng Gao, Chong Wang, Li-hong Li, and Tony Jebara. 2018. Subgoal discovery": "for hierarchical dialogue policy learning. In Proceed-ings of the 2018 Conference on Empirical Methodsin Natural Language Processing, Brussels, Belgium,October 31 - November 4, 2018, pages 22982309.Association for Computational Linguistics. Jan Whlke, Felix Schmitt, and Herke van Hoof.2020. A performance-based start state curriculumframework for reinforcement learning. In AAMAS,pages 15031511. International Foundation for Au-tonomous Agents and Multiagent Systems. Junlin Wu and Yevgeniy Vorobeychik. 2022. Robustdeep reinforcement learning through bootstrappedopportunistic curriculum. In ICML, volume 162 ofProceedings of Machine Learning Research, pages2417724211. PMLR.",
  "Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang,Yukai Xu, and Weiran Xu. 2024. Divtod: Unleash-ing the power of llms for diversifying task-orienteddialogue representations. CoRR, abs/2404.00557": "Bowen Zhang, Songjun Cao, Xiaoming Xhang, YikeZhang, Long Ma, and Takahiro Shinozaki. 2022a.Censer: Curriculum semi-supervised learning forspeech recognition based on self-supervised pre-training. In Interspeech, pages 26532657. ISCA. Haodi Zhang, Zhichao Zeng, Keting Lu, Kaishun Wu,and Shiqi Zhang. 2022b.Efficient dialog policylearning by reasoning with contextual knowledge.In Thirty-Sixth AAAI Conference on Artificial Intelli-gence, AAAI 2022, Thirty-Fourth Conference on In-novative Applications of Artificial Intelligence, IAAI2022, The Twelveth Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2022 VirtualEvent, February 22 - March 1, 2022, pages 1166711675. AAAI Press. Yang Zhao, Hua Qin, Zhenyu Wang, Changxi Zhu, andShihan Wang. 2022. A versatile adaptive curricu-lum learning framework for task-oriented dialoguepolicy learning. In Findings of the Association forComputational Linguistics: NAACL, pages 711723.Association for Computational Linguistics.",
  "In this section of the appendix, we provide com-prehensive results of the performance of different": "agents across different epochs and datasets withvarying difficulty levels. The complete results foreach agent on the three distinct datasets, includingthe highlighted optimal outcomes, are detailed inTab.6. Additionally, we enhance our analysis byincorporating box plots with variance to visuallyrepresent the performance of different agents on thethree datasets with varying difficulty levels. Thesegraphical representations are displayed in -9.Notably, we have included the average success rateof the best-performing baseline model as a refer-ence line for enhanced comparison. The resultsof these supplementary experiments further rein-force the efficacy and versatility of our proposedframework, as presented in the main paper. Byvalidating our approach in a broader experimentalsetup, we draw more comprehensive conclusions,highlighting the potential of our method in enhanc-ing dialogue policy learning.",
  "BCase Study": "To further validate the efficacy of the BPL frame-work in facilitating knowledge transfer betweensubgoals and original goals or similar goals, wepresent an example and visual dialogue trajectoriesbetween two similar user goals (g1 and g2) withcommon subgoal (g1), as depicted in Tab.7. Basedon the example of a failed dialogue for user goalg1, we identify a boundary state s4 and its corre-sponding subgoal g1 through goal decomposition.The acquired knowledge of completing subgoalg1 can seamlessly transfer to accomplishing usergoal g2, as g1 is also a subgoal of g2. The visualdialogue trajectories reaffirm this outcome, reveal-",
  "BPL-CE0.0148-42.2929.240.2414-18.2221.890.763331.1417.11": "ing a significant overlap in the dialogue paths forachieving user goals g1 and g2. Once the agenthas mastered the skill of reaching state s4, it canmore effortlessly reach the goal states of g1 and g2,in contrast to starting exploration from the initialstate s0. This tangible demonstration reinforcesthe potential of the BPL framework to facilitate",
  "BPL-*: Includes only the decomposer compo-nent, with ablation conducted for all three decom-position conditions (e.g., BPL-AR corresponds tousing decomposition condition A)": "BPL-*F/R: Integrates both decomposer andevolver components. Optimal evolutionary waysbased on summarized optimal curriculum patternsare selected (e.g., evolutionary way R for Moviedataset, F for Restaurant and Taxi domains). As the evolver relies on subgoals decomposed bythe decomposer, individual ablations on the evolverare not performed. Comparing BPL-* with BPL-*F/R reveals the evolvers utility within the BPLframework. The experimental results in reveal important findings. In the Movie dataset, de-composition conditions (A, T, and C) all positivelycontribute to the BPL framework, with condition Aexhibiting the most substantial improvement. Thecombination of the evolver and decomposer yieldsthe best outcomes. In the Restaurant dataset, de-composition condition A adversely affects BPL-AFwithout the evolver due to the datasets blend ofsimple and complex user goals. Nonetheless, theevolver and later-executed conditions (T and F)can mitigate this effect. In the challenging Taxidataset, the decomposers impact outweighs theevolvers, as it addresses the sparse reward issueby simplifying user goals. In conclusion, theseablation experiments highlight the distinct contri-butions of the decomposer and evolver componentswithin the BPL framework, confirming the validityof our identified optimal curriculum patterns.",
  "DEffect of varying N&M values on BPL": "Intuitively, N and M control the decompositioncondition, and their number significantly impactsdialogue policy learning. Therefore, we conductedexperiments with different numbers of M and Nvalues on three datasets across different difficul-ties. and 5 shows the moving average successrate during the learning. The results show that forthe medium difficult dataset, both N and M pro-vide accurate discrimination of difficult user goalsfor decomposition. In contrast, for the easy anddifficult dialogue datasets, the user goals are gen-erally easy or difficult, thus, N and M play littlerole. It validates our conclusion in the analyticalexperiments again. When N = 0, two models areactually the BPL-AF model, so it does not incorpo-rate the value choice. We default N to 100 and Mto 2 for all domains, while the default for the Taxidataset M is 3.",
  "User: The nearest Culvers": "Agent: Alright. How many people will be in the taxi?User: 5 people.Agent: Okay. Whats the pick-up location?User: Actually, where is the pick-up location?Agent: Whats the pick-up location?User: Im asking you about the pick-up location.Agent: Sure, but I need to know the pick-up location.User: Can you tell me the pick-up location?Agent: Whats the pick-up location?......",
  "User: Im heading to the nearest Culvers": "Agent: Got it. How many people will be traveling?User: There will be 4 of us.Agent: Great. Is there anything else I can assist youwith regarding your booking?User: Yes, I also need to know the pick-up locationand the estimated fare for this taxi.Agent: Certainly. The pick-up location will be at 123Main Street, and the estimated fare for the ride willbe calculated based on the distance and time,approximately $35.User: Perfect, thats all I needed to know. Thank you!Agent: Youre welcome!"
}