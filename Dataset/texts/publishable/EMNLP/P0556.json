{
  "Abstract": "RecentLargeVision-LanguageModels(LVLMs) demonstrate impressive abilitiesonnumerousimageunderstandingandreasoning tasks.The task of fine-grainedobject classification (e.g., distinction betweenanimal species), however, has been probed in-sufficiently, despite its downstream importance.We fill this evaluation gap by creating FOCI(Fine-grained Object ClassIfication), a difficultmultiple-choice benchmark for fine-grainedobject classification, from existing objectclassification datasets:(1) multiple-choiceavoids ambiguous answers associated withcastingclassificationas open-endedQAtask; (2) we retain classification difficulty bymining negative labels with a CLIP model.FOCI complements five popular classificationdatasets with four domain-specific subsetsfrom ImageNet-21k. We benchmark 12 publicLVLMs on FOCI and show that it tests fora complementary skill to established imageunderstanding and reasoning benchmarks.Crucially, CLIP models exhibit dramaticallybetter performance than LVLMs.Sincethe image encoders of LVLMs come fromthese CLIP models, this points to inadequatealignment for fine-grained object distinctionbetween the encoder and the LLM and warrants(pre)training data with more fine-grainedannotation. We release our code at",
  "Introduction": "Large Vision Language Models (LVLMs)LargeLanguage Model (LLM) that have been adaptedto process images as input alongside texthaveshown impressive performance on a wide rangevision-language tasks (Li et al., 2023b; Liu et al.,2023b; OpenAI, 2023a; Anil et al., 2023). LVLMsare mutually compared using a range of bench-marks that test for various image understanding andreasoning skills, such as existence and counting ofobjects, localization, comparison between objects",
  ": The importance of object recognition: LLaVA1.5 fails to identify the dog breed. Idefics-2 correctlyrecognizes it and gives a correct fact as a result": "or identifying object attributes (Goyal et al., 2017;Hudson and Manning, 2019; Liu et al., 2023c).LVLMs are, however, barely ever tested for fine-grained object classificationthe ability to cor-rectly recognize different animals, plants, or man-made objectswhich is, we argue, an importantskill that complements general image understand-ing.1 Besides it being an end-task in itself, e.g.,to answer questions such as What is this flower/ animal / car / building / ... called?, it is oftenimplicitly needed in information-seeking situations,where the success depends on the models abilityto correctly and precisely identify an object (Isthis mushroom edible?, How to cook this dish?,or What is this dog breed used for?) (Hu et al.,2023; Chen et al., 2023; Mensink et al., 2023). Asillustrated in , only one of the LVLMs cor-rectly identifies the dog breed in the image andcan follow up with relevant information. Note thatthis is different from general L(V)LM hallucination(Zhang et al., 2023b), where models invent incor-rect information. Instead, the generated content iscorrect for the object, but the object is misclassi-fied: the information about Samoyed by LLaVa 1.5",
  "is correct, but the dog in the image is a Keeshond": "To fill this gap in LVLM evaluation, we createa comprehensive benchmark dubbed FOCI (Fine-grained Object ClassIfication) that tests modelsfine-grained object recognition over a wide rangeof object categories. Our key contribution is a well-defined task formulation that avoids pitfalls of priorwork: We argue that an open question answering(QA) formulation (i.e., answer the question Whatis this?), as done, e.g., by Xu et al. (2023a), is anill-defined task for two reasons. 1) the completeset of admissible answers is not provided (e.g., ad-missible answers for the dog in includeKeeshond, Dutch Barge Dog, and Wolfspitz). Forobjects with only a few synonym labels, one canprovide all answer options but this does not scaleto hundreds or thousands of objects. Constraineddecoding to only the admissible labels is computa-tionally expensive for large label sets (Chen et al.,2022). 2) The expected taxonomy level of the an-swer is not specified. For the given example, dog,Spitz, and Keeshond are all ontologically correctanswers; but recognizing a Keeshond is much moredifficult than recognizing a dog. To address theabove shortcomings, we formulate object classifi-cation as a multiple-choice problem To avoid thatthe reduction to only a handful candidate answersrenders the task trivial, we use a CLIP model (Rad-ford et al., 2021a) in a zero-shot configuration tomine difficult choices from the pool of class labels.We assemble FOCI from 5 popular classificationdatasets for different domains (flowers, cars, food,aircraft, pets) and additionally create 4 domain sub-sets from ImageNet-21k (Deng et al., 2009) foranimals, plants, food, and man-made objects. We extensively evaluate 12 publicly availableLVLMs on FOCI and find that many of them likethe popular LLaVA 1.5 struggle with fine-grainedobject classification. We observe that models withsimilar performance on established benchmarkscan yield quite different and uncorrelated resultson FOCI, highlighting that fine-grained object clas-sification is indeed a distinct skill for LVLMs, andthat FOCI should thus complement existing imageunderstanding and reasoning benchmarks. Com-paring the models further, we observe that the scaleof their (pre-)training data seems to impact theirperformance on FOCI significantly more than forimage understanding tasks. A comparison with theunderlying CLIP models used as the LVLMs im-age encoders shows that the encoders zero-shot accuracy provides an upper bound for the LVLM,with the LVLM performance lagging drasticallybehind. This suggests that the alignment betweenthe image encoder and LLM in LVLMs seems tobe insufficiently semantically fine-grained. We fi-nally perform controlled experiments to isolate themodeling and training decisions that impact themodels performance in FOCI. As is the case withother benchmarks, both larger LLMs and strongerimage encoders improve results. Most importantly,incorporating captions into the training data thatexplicitly name the downstream objects helps withclassification. Similarly, including fine-grainedclassification objectives to the training mix canimprove models FOCI performance.",
  "Related Work": "Large Vision-Language Models. LVLMs alignpre-trained image encoders (generally a VisionTransformer (ViT) (Dosovitskiy et al., 2021) fromCLIP (Radford et al., 2021a)) to a Large LanguageModel (LLM), yielding an LLM that can workwith images as input besides text (Chen et al.,2022; Alayrac et al., 2022; Li et al., 2023b; Daiet al., 2023; Liu et al., 2023b,a; Bai et al., 2023;Laurenon et al., 2023; Chu et al., 2023; Zhanget al., 2023a). LVLMs are commonly trained intwo stages: first, an alignment module betweenthe image encoder and the LLMa shallow feed-forward network (Liu et al., 2023b,a) or more com-plex modules like a resampler (Alayrac et al., 2022;Li et al., 2023b)that projects image tokens intothe LLM input embedding space is trained usingimage-caption pairs. In the second stage, the modelis trained for general-purpose inference on a mix oftasks, e.g., visual Q&A (Goyal et al., 2017; Hudsonand Manning, 2019) and (visual) chat instructiondata (Chiang et al., 2023; Liu et al., 2023b). Whilethe second stage is fairly similar across the recentmodels, the first stage is where training greatlyvaries: on the low end, models are trained withless than a million examples (Liu et al., 2023a,2024); on the high end, over a billion image-textpairs are used (Bai et al., 2023; Dong et al., 2024;Laurenon et al., 2024). Despite differences indata size, models on both ends of the spectrum canachieve competitive results on popular benchmarks.In this work, we show that better visio-linguisticalignment in the first training stage substantiallyboosts fine-grained object classification abilities.",
  "Test LVLM": ": Testing LVLMs on object classification through multiple-choice: (1) We compute the CLIP cosinesimilarity between a test image and class labels; we select the correct label and the three most similar (wrong) labelsto (2) formulate a multiple-choice problem, which (3) is given to the LVLM who has to predict the correct choice. marks, e.g., VQAv2 (Goyal et al., 2017), GQA(Hudson and Manning, 2019), MME (Fu et al.,2023), MMBench (Liu et al., 2023c), Seed-Bench(Li et al., 2023a), or (Tong et al., 2024), test LVLMsfor image understanding and reasoning capabilitiessuch as recognition of color and other attributes,object counting, recognizing object position andorientation and similar. Other benchmarks likeMMMU (Yue et al., 2023) test world knowledgeand reasoning capabilities in different domains.Although (fine-grained) object classification isa prominent end-task in itself and relevant in con-versational applications, it is barely considered inLVLM evaluation protocols. The work that ad-dresses the task is limited. (i) Models with in-context learning capabilities are evaluated on few-shot object classification but the models do notclassify images in isolation and instead comparethe target image with labeled in-context examples(Tsimpoukelli et al., 2021; Alayrac et al., 2022). (ii)Pali (Chen et al., 2022) was evaluated on ImageNet(Deng et al., 2009) by scoring every class labels,which is computationally expensive. (iii) LVLM-e-Hub (Xu et al., 2023a) includes some image classi-fication datasets (like ImageNet) but they formulateit as open-ended QA task with ambiguity over ex-pected answers, which leads to low accuracy scoresfor all models. (iv) In knowledge-intensive VQA,models have to recognize the correct object (e.g., aspecific building) to answer correctly; objects arerecognized either implicitly (the QA model needsto know which object it is to answer correctly) orexplicitly when a knowledge base is used to re-trieve relevant information (Hu et al., 2023; Chenet al., 2023; Mensink et al., 2023).Contemporary work by Kim and Ji (2024);",
  "Multiple-Choice Image Classification": "Image classification is a fundamental problem incomputer vision with a plethora of datasets avail-able. In this work, we focus on fine-grained objectclassification where models have to differentiatebetween several objects belonging to a specific do-main, e.g., animal species or car models. We lever-age existing datasets as resources for annotated dataand frame object classification as a multiple-choicetask with well-defined answer candidates. Why Multiple-Choice? The standard formulationof object classification tasks for LVLMs is via ques-tion answering, with open-ended answer generationXu et al. (2023b). This formulation, we argue, rep-resents an ill-posed problem for two main reasons:(1) the expected level of granularity in the objecttaxonomy that is expected as the answer is not de-fined, and is difficult to define in general (e.g., forthe image from , dog, Spitz, or Keeshondare all correct labels); (2) the set of admissible an-swers in existing datasets is not complete: mostobjects have multiple synonymous labels, all ofwhich constitute a correct answer (e.g., Keeshond,Dutch Barge Dog, and Wolfspitz), but only subsetsof those are provided as admissible labeles in exist-ing datasets. Providing complete synonym sets andspecifying the expected level of granularity of the answer is, in the general case, infeasible, and whileincorporating LLMs into the evaluation might al-leviate some issues even with incomplete answers(Maas et al., 2024), this would greatly increasethe cost of evaluating models. Instead, we proposeto formulate fine-grained object classification as amulti-choice task, where the models are providedwith a set of candidate answers from which the cor-rect answer is to be selected; this way the expected(i.e., correct) output is well-defined. Mining Hard Choices. To maintain difficulty de-spite the reduction to only a small set of candidatelabels, we mine for each example image difficultincorrect labels from all class labels used in theconcrete image classification dataset. We arguethat a reduction to the most likely incorrect classesretains the task difficulty as even in classificationover large class sets (e.g., thousands of classes),models easily discern between unrelated classesand most errors stem from close classes anyways(e.g., in the Oxford-Pets dataset, which covers 37cat and dog breeds, cat breeds are irrelevant fordog images). We use a CLIP model for miningdifficult candidates: for every example image, weselect the three most similar (incorrect) class labelsas negative choices. We rank the dataset classes foran image using the standard CLIP zero-shot setup:the text encoder embeds all class labels, the imageencoder embeds the image, and the class labels areranked in decreasing order of cosine similarity oftheir respective text embeddings with the imageembedding. We avoid biasing the choice selec-tion towards any concrete LVLM in our evaluationby selecting OpenCLIP ViT-L/14 (Ilharco et al.,2021): its image encoder has not been used by anyof the LVLMs. illustrates both the processof mining negatives for an image and testing anLVLM on the resulting set of candidate choices.Our CLIP-based mining of hard negatives is criti-cal for the difficulty of our benchmark: dependingon the initial classification dataset, LVLMs may ex-hibit 20-50 points lower performance compared toa variant where negatives are randomly selected.2",
  "Tested with LLaVA 1.5": "three most similar negatives). We complement (1)established datasets commonly used for evaluatingCLIP models (Radford et al., 2021a; Ilharco et al.,2021) with (2) additional challenging larger-scaledatasets that we derive from ImageNet-21k (Denget al., 2009). For the former, we select the fol-lowing five datasets: FGVC-Aircraft (Maji et al.,2013) contains images of 100 different aircrafttypes; Flowers102 (Nilsback and Zisserman, 2008)contains images of 102 different flower species;Food101 (Bossard et al., 2014) covers 101 dishes;Oxford-Pet (Parkhi et al., 2012) contains imagesof 37 cat and dog breeds. Stanford-Cars (Krauseet al., 2013) covers 196 car models.As some of the above datasets are not particu-larly challenging for existing CLIP models in zero-shot evaluations, we additionally construct fournew challenging datasets from ImageNet-21k (IN-21k). We first merge ImageNet-COG (Sariyildizet al., 2021) (5k classes) and ImageNet-1k (IN-1k),for a total of 6k classes that are all leaf nodes in theWordNet (Miller, 1994) taxonomy: this means thatno two labels stand in the is-a relation and therecannot be multiple correct answers stemming fromdifferent taxonomy levels (e.g., dog and Pomera-nian). Next, we group the classes according totheir WordNet lexicographer file names, and cre-ate a dataset for each of the four most representedones: Animal (1322 classes), Plant (957 classes),Food (563 classes), and Artifact (man-made ob-jects, 2631 classes). We prepend IN- (ImageNet-)in our experiment to mark these datasets.One could, in principle, add more object typesand domains to the evaluation: our goal was to in-clude a reasonably diverse set of domains, fromwhich, when put together in a benchmark, onecould reliably extrapolate general fine-grained ob-ject recognition abilities of LVLMs. For furtheranalysis, in Appendix D we additionally evaluateLVLMs on more general (i.e., not domain-specific)object classification under different image distri-bution shifts (using ImageNet-1k) and for geo-graphic distribution shifts with common objectsphotographed in different regions of the world, us-ing GeoDE (Ramaswamy et al., 2023).",
  "Model#PPretrainTask Mix": "Idefics-1 (Laurenon et al., 2023)9B350M1MIdefics-2 (Laurenon et al., 2024)8B1.5B?BLIP2 Flan-T5-XL (Li et al., 2023b)4B130MInstructBLIP Flan-T5-XL (Dai et al., 2023)4B130M1MInstructBLIP Vicuna (Dai et al., 2023)8B130M1MInternLM XComposer 2 (Dong et al., 2024)7B>1B600MLLaVA 1.5 (Liu et al., 2023a)7B560k660kLLaVA-Next (Mistral) (Liu et al., 2024)7B560k760kMobileVLM V2 (Chu et al., 2024)7B1.2M2.4MPali-Gemma (Beyer et al., 2024)3B1B?Phi-3-Vision (Abdin et al., 2024)4B>10M>1MQwen-VL-Chat (Bai et al., 2023)10B1.4B50M : The 12 tested public LVLMs. We provide pa-rameters count (#P; LLM + image encoder parameters)and the dataset size (in images) used during the pretrain-ing and task mix training phase. For some fields, we puta conservative estimate or ? if no estimate is possible. Model and Inference Details.. Our selected mod-els span a variety of architectures and trainingparadigms. summarizes key information(the number of parameters and the size of the train-ing data) for each model. Due to our hardwareconstraints, we benchmark models with LLMs hav-ing 7B parameters. At inference time, we providethe LVLMs with the image and the four candidatechoices. The choices are in random order to avoidmodel-specific preferences for answer positions(Liu et al., 2023c)); the model provides as outputone of the choices, which is compared with theground truth label: we then report the performancein terms of accuracy. See Appendix A for furtherdetails on models, the inference setup, and datasets.",
  "Results": "FOCI vs. Other Benchmarks. displays theresults for the 12 benchmarked LVLMs on FOCI.We first compare the models performance and rel-ative ranking on FOCI with their results on pop-ular image understanding benchmarks (we showthe models performance on GQA (Hudson andManning, 2019), MMBench (Liu et al., 2023c),and MMMU (Yue et al., 2023) in in theAppendix C). Models results on FOCI are muchless correlated with their respective results on otherbenchmarks: better results on GQA, MMBench,or MMMU do not necessarily imply better re-sults for fine-grained object classification and viceversa. Qwen-VL, for example, is amongst the best-performing models in object classification in FOCI,but is fares much worse on the standard bench-marks, where several yield better results. On theother hand, Phi-3-Vision has among the best resultson the standard benchmarks but exhibits only av- erage performance on FOCI. These results indicatethat fine-grained object classification is a skill thatis complementary to what other image understand-ing and reasoning benchmarks test and as suchshould be added to LVLM evaluation protocols. Training Data. One important factor for strongobject recognition on FOCI seems to be the amountof image-text data used for (pre-)training the align-ment component of the LVLM in the first trainingphase (see 2). On the common understandingbenchmarks, models like LLaVA 1.5, and LLaVA-Next show strong results despite being pretrainedwith <1M image-text pairs. However, the two bestmodels on FOCI, Idefics-2 and Qwen-VL, are bothpretrained on 1.5B images and drastically outper-form the LLaVA models. Pali-Gemma with 1B pre-training examples also shows a strong performancedespite its small LLM size. This suggests thatobject classification requires larger-scale trainingfor a much more fine-grained alignment betweenthe image encoder and LLM, compared to what isneeded in general for image understanding, whichtypically requires reasoning over coarser objectcategories (e.g., dog or tree) or attributes of theseobjects (e.g., color and shape); this knowledge isreadily included in the alignment captions (andalso in the image understanding tasks included inthe fine-tuning training mix). We isolate the effectof the alignment training data (in a smaller-scalesetup) in 5. The results for InstructBLIP are some-what inconclusive: with Flan-T5-XL as LLM, itexhibits good FOCI performance, but with Vicuna(and otherwise identical training) the results aresubstantially worse. This would suggest that, otherthan the scale of the alignment training, the LLMitself plays an important role. Other Factors. Very high image resolution, whichis highly beneficial for OCR-heavy tasks like chartunderstanding (Liu et al., 2024), does not seem tobe relevant for fine-grained object classification.This stems from the comparison between LLaVA1.5 and LLaVA-Next, where the latters main dif-ference w.r.t. the former is training with (and infer-ence on) images of higher resolution. This is unsur-prising as images in object classification datasetstypically contain large centered objects, makinglarger resolution unnecessary for solving the task.The LLM and image encoder are likely also majorfactors for the ultimate performance but we cannotisolate them in this observational analysis; instead,we consider them in controlled experiments in 5.",
  "LVLM vs. Its Corresponding CLIP": "Several of the tested LVLMs keep their underly-ing CLIP image encoder frozen throughout training.This means that the cross-modal alignment betweenthe CLIPs image encoder and its text encoder is un-touched, allowing us to compare the performanceof these LVLMs directly against the CLIP modelsfrom which they take the image encoder.Specifically, we consider three LVLMs withtheir corresponding CLIP models: Idefics-1, whichuses OpenCLIP ViT-H/14 (Ilharco et al., 2021),LLaVA 1.5, which uses OpenAI ViT-L/14 (Rad-ford et al., 2021b), and InstructBLIP Flan-T5 withEVA-1 ViT-g/14 (Fang et al., 2022). CLIP Zero-Shot Classification as Upper Bound.The image and text encoder of a CLIP model weretrained jointly on huge datasets; in contrast, thealignment of the CLIPs image encoder to the LLMis learned with comparatively less image-text data(e.g., InstructBLIP is pre-trained with 100M sam-ples while EVA-1 was trained with 11B samples).We compare in the LVLM performanceagainst the zero-shot classification accuracy of the Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy ImageNet-1k CLIP correct yesno",
  ": Accuracy of three LVLMs on ImageNet-1k,for example subsets on which the zero-shot classifica-tion with the corresponding CLIP model is (in)correct": "corresponding CLIP model (for a fair comparison,CLIP only considers the same 4 labels as LVLMdoes in multiple-choice formulation). We observethat the LVLM performance is indeed consistentlylower than that of the corresponding CLIP model.However, while the CLIP zero-shot classificationaccuracy seems to be an upper bound for the LVLM,the gaps vary substantially across the FOCI datasets:from <10% on IN-Artifact to 40-50% on Oxford-Pets. These results indicate that, while the align-ment between the image encoder and LLM is under-trained in general, there are also drastic differencesin the quality of alignment for different types ofobjects (i.e., domains). For certain domains (e.g.,Oxford-Pets) the LLM seems to struggle to processthe image features, despite the CLIP image encoderencoding sufficient information (as evidenced bythe much better corresponding CLIP performance). CLIP wrong = LVLM wrong? We analyzethe predictions of LVLMs on instances that the cor-responding CLIP model misclassifies to measurewhether those classification errors propagate to theLVLM: in other words, if the CLIP model is wrong,is the LVLM using its image encoder also boundto misclassify the image? summarizes 1.7B3B7B LLM Size",
  ": Results with MobileVLM v2 over its threeLLM sizes with otherwise identical training": "the results of this analysis on ImageNet-1k (in ourmulti-choice formulation) for three LVLMs; forthe FOCI datasets we provide the same analysis in in the Appendix. We observe that LVLMaccuracy plummets on examples on which the cor-responding CLIP fails: in fact, for instances thatCLIP cannot correctly classify, the performanceof the corresponding LVLM gets close to random(25%) for all three LVLMs in the analysis. Theseobservationsthat CLIP performance is an upper-bound for LVLM accuracy and that its errors prop-agate to the LVLMhighlight that the selectionof an image encoder is a key design decision forLVLMs performance and suggest that future im-provements in image encoding are likely to alsopropagate to LVLM object recognition capabilities.",
  "Controlled Experiments": "We next perform a set of controlled experiments todisentangle the effects of individual LVLM designchoices on (fine-grained) object classification. Ouranalysis encompasses three main factors: (1) theLLM size, (2) the image encoder, and (3) targetedchanges to the training data. For (2) and (3), wetrain LVLMs following the LLaVA 1.5 recipe withStableLM 2 Zephyr 1.6B (Bellagente et al., 2024)as LLM and OpenAI CLIP-L/14-224 as the imageencoder (see the Appendix B for training details). LLM Size. Larger LLMs generally make for betterLVLMs, yielding better benchmark performancedue to (inter alia) improved reasoning capabilities(Liu et al., 2023a; Karamcheti et al., 2024; Chuet al., 2024). Our multiple-choice object classifica-tion is not difficult from a reasoning or language-understanding perspective, but it requires familiar-ity with thousands of objects, which may be be- IN-Food IN-Artifact IN-Animal IN-Plant FGVC-Aircrafts Flowers102 Food101 Oxford-Pets",
  "Baseline53.1253.7152.5241.19No Pretrain51.9451.5652.3238.71Synthetic54.4655.1253.8041.48Template54.8158.8250.8040.69QA Task57.4059.8954.9143.64": ": Results for experiments with changes to thetraining data on: ImageNet-1k overall (IN-1k) and bro-ken down for the training half and the held-out test half,and the average results over the 9 FOCI datasets. yond the knowledge stored in smaller LLMs. Forthis analysis, we turn to the MobileVLM v2 modelseries (Chu et al., 2024): with models trained ontop of 1.7B, 3B, and 7B LLM backbones and oth-erwise identical architecture (image encoder andalignment module) and training procedure (dataand training protocol for both the LLMs and subse-quent LVLMs), we can isolate the effect of LLMsize. summarizes the results. Expectedly,the performance on all FOCI datasets consistentlyimproves with increased LLM size: we believethat this is because smaller LLMs simply encodeless world knowledge and have semantically poorerrepresentations for (fine-grained) objects. Image Encoder. Following the observation thatthe quality of the CLIP image encoder may capthe LVLMs performance (), we investi-gate the effect that LVLMs image encoder hason fine-grained object recognition. Our baselineLVLM aligns the OpenAI CLIP-L/14-224 (CLIP-224 for short) image encoder with the LLM. Wethen create two other LVLMs by changing the im-age encoder with: (1) OpenAI CLIP-L/14-336(CLIP-336 for short), which takes images of largerresolution and (2) SigLIP SO400M-224 (SigLIPfor short) (Zhai et al., 2023) as a better image encoder, boasting substantially higher benchmarkresults on image processing benchmarks. summarizes the results. On one hand, encodingimages in higher resolution (with CLIP-336, i.e.,increasing from 224px to 336px) leads to only amarginal 1 accuracy point gain, averaged overall FOCI datasets. The effect seems to depend onthe object type: we see gains of over 5 points onFlowers102 & Food102 but also a 5-point drop onOxford-Pets. The SigLIP encoder, on the otherhand, greatly improves the baseline performanceacross the board. The absolute gains of the SigLIP-based LVLM over the baseline LVLM (CLIP-224encoder) are, however, not proportionate to gainsthat the corresponding SigLIP CLIP model yieldsover CLIP-224 in zero-shot object classification.For example, while SigLIP beats CLIP-224 by 27%on FGVC-Aircraft,3 the SigLIP-based LVLM beatsthe CLIP-224-based LVLM on the same dataset byonly 3%; inversely, on Food101, SigLIP has onlya 2% edge in CLIP comparison, but yields 12%better performance in LVLM comparison. Training Data. The two LVLMs trained with mostdata, Idefics-2 and Qwen-VL (>1.5B images intotal over both training stages) demonstrated thebest performance on FOCI (). As this scaleof training is beyond the (computational) budgetof most practitioners, we set to quantify the FOCIgains from adding training data at smaller datascales, concretely at the data budget of LLaVA 1.5(ca. 1.2M images in total, see ).Changes to Pretraining. We hypothesize that alarger pretraining corpus benefits the LVLM dueto having more of the objects named explicitly inthe corresponding captions. We test this explicitlyby replacing a portion of the LLaVA 560k pre-traing images (with captions) with images fromthe ImageNet-1k train split to test if recognitionperformance for those classes improves. To have aheld-out control set, we only use 500 of the 1000classes (choosing every other class) for training;we select 280 images per class, which yields 140ktraining examples in total or 25% of the LLaVA pre-training data. We consider three training strategiesfor the added ImageNet images: i) with syntheticcaptions, generated using BLIP (Li et al., 2022)(Synthetic); this setup tests the effect of imageswith objects but with captions that do not necessar-ily name them (e.g., for an image of a Keeshond,BLIP-generated caption will likely contain dog",
  "Taken from: openclip_classification_results.csv": "but not Keeshond). ii) with template captions(Template) such as a picture of a $label.; suchcaptions are not visually descriptive but explicitlyname the object in the image. iii) we skip the pre-training phase entirely (No Pretrain) and performthe task mix training on the randomly initializedalignment module; on standard benchmarks, skip-ping pretraining has been reported not to notablyaffect performance (Karamcheti et al., 2024). Changes to Task Mix Phase. We incorporate Ima-geNet as an open-ended QA Task where the modelis prompted to name the image object without can-didate answers. We use the open-ended QA for-mulation in training to avoid model adaptation tothe multiple-choice formulation of the task we useat test time on FOCI. We again use 500 (out of the1000) ImageNet classes and sample 150 examplesper class (75k training examples in total). We donot otherwise change the LLaVA task mix data. Results. We report the results of this ablation in. Skipping the pretraining step entirely (NoPretrain) reduces the average FOCI performanceby over 2 accuracy points: this suggest that pretrain-ing of the alignment module on image-text pairsis important for fine-grained object classification,unlike what was recently reported for other tasks(Karamcheti et al., 2024). Training on images withboth Synthetic and Template captions has a verylimited effect on FOCI performance and the unseenTest Half of ImageNet. Training on Syntheticbrings a 1.5-point gain for the 500 ImageNetobject classes seen in training (Train Half in Ta-ble 3); in comparison, the Template captions bringa much more significant gain of 5% for seen ob-ject classes: this strongly suggests that explicitlymentioning the objects in the captions is key forlearning the alignment module that allows LVLMsbetter fine-grained object classification; just hav-ing images containing the object does not suffice(or is, at least, less effective). Note that only thefeed-forward alignment module is trained in thefirst phase, so the improvements with Templatecaptions can only be the result of having learned abetter alignment and not due to the image encoderor LLM (both frozen) obtaining better representa-tions of objects and their mentions, respectively.Including ImageNet as open-ended QA Task to thesecond task mix training phase has a larger effecton performance. For 500 of ImageNet-1k seen intraining (Train Half), we observe a 6% improve-ment, but also a 2-point improvement on the images",
  "Conclusion": "In this work, we evaluate the capabilities of LVLMsfor fine-grained object classification over differ-ent domains. We address the ambiguity of open-ended QA-based object classification evaluationand propose to replace it with a multiple-choiceformulation, in which we retain the task difficultyby mining difficult (semantically closest classes)choices with a CLIP model. This way, we cre-ate FOCI, a novel benchmark consisting of 9 fine-grained multi-choice object classification datasets.We benchmark 12 public LVLMs, demonstratingthat their performance on FOCI is largely uncorre-lated with that on other image understanding andreasoning benchmarks: this renders fine-grainedobject classification a skill that is complementary towhat the existing benchmarks test the LVLMs for.Our ablations identify the quality of the image en-coder and the amount of explicit caption mentionsof image objects in LVLM training data as factorsthat drive the performance. We hope our work stim-ulates wider research efforts on improving LVLMsfor fine-grained object classification, in particularconceptual innovation (e.g., more effective train-ing data and protocols for object classification withLVLMs) that goes well beyond mere scaling ofLVLM pretraining to billions of image-text pairs.",
  "Limitations": "We identify three main limitations for our work:First, while the goal of this work is not to eval-uate every possible domain, we still likely exhibita bias towards Anglospheric concepts as multipledatasets were created at British and US universitiesand use images sourced from the English internet.ImageNet in particular shows such biases (Liu et al.,2021) in image source and for its classes. Whilewe briefly consider performance over geographicdistribution shifts in the Appendix, we still likelyoverestimate performance for diverse cultural ob-jects and concepts from around the globe.Another limitation stems from the multiple-choice formulation: while it allows for well-definedanswers, users in the wild are more likely touse an open-ended formulation.While we ex-pect results between the two formulations to cor-relate, some objects may be harder to classify in amultiple-choice setup due to the presence of chal-lenging confounder options, and vice versa, some objects may be easier to classify in multiple-choicewith the correct name as an option.Finally, we only evaluate public LVLMs usingLLMs of 7B parameters or less. We do not con-sider larger models (e.g., LLaVA 1.5 with Vicuna-13B) or proprietary LVLMs (e.g., GPT4 (OpenAI,2023b) or Gemini (Anil et al., 2023)) because theinference time is too high on our compute (or notpossible at all VRAM-wise) for the former and tooexpensive with >100,000 of API calls for the latter.",
  "This work was in part supported by the Alexandervon Humboldt Foundation": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari,Harkirat Behl, Alon Benhaim, Misha Bilenko, Jo-han Bjorck, Sbastien Bubeck, Martin Cai, CaioCsar Teodoro Mendes, Weizhu Chen, VishravChaudhary, Parul Chopra, Allie Del Giorno, Gus-tavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter,Amit Garg, Abhishek Goswami, Suriya Gunasekar,Emman Haider, Junheng Hao, Russell J. Hewett,Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Ma-houd Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Chen Liang, Weishung Liu, EricLin, Zeqi Lin, Piyush Madan, Arindam Mitra, HardikModi, Anh Nguyen, Brandon Norick, Barun Patra,Daniel Perez-Becker, Thomas Portet, Reid Pryzant,Heyang Qin, Marko Radmilac, Corby Rosset, Sam-budha Roy, Olatunji Ruwase, Olli Saarikivi, AminSaied, Adil Salim, Michael Santacroce, Shital Shah,Ning Shang, Hiteshi Sharma, Xia Song, MasahiroTanaka, Xin Wang, Rachel Ward, Guanhua Wang,Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu,Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu,Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang,and Xiren Zhou. 2024. Phi-3 Technical Report: AHighly Capable Language Model Locally on YourPhone. _eprint: 2404.14219. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Brock, Aida Nematzadeh, Sahand Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karen Simonyan. 2022.Flamingo:a Visual Language Model for Few-Shot Learning.CoRR, abs/2204.14198.ArXiv:2204.14198. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Gemini: A Fam-ily of Highly Capable Multimodal Models. CoRR,abs/2312.11805. ArXiv: 2312.11805. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023.Qwen-VL: A FrontierLarge Vision-Language Model with Versatile Abili-ties. CoRR, abs/2308.12966. ArXiv: 2308.12966. Marco Bellagente, Jonathan Tow, Dakota Mahan, DuyPhung, Maksym Zhuravinskyi, Reshinth Adithyan,James Baicoianu, Ben Brooks, Nathan Cooper,Ashish Datta, Meng Lee, Emad Mostaque, MichaelPieler, Nikhil Pinnaparaju, Paulo Rocha, HarrySaini, Hannah Teufel, Niccol Zanichelli, and CarlosRiquelme. 2024. Stable LM 2 1.6B Technical Report.CoRR, abs/2402.17834. ArXiv: 2402.17834. Lucas Beyer, Andreas Steiner, Andr Susano Pinto,Alexander Kolesnikov, Xiao Wang, Daniel Salz,Maxim Neumann, Ibrahim Alabdulmohsin, MichaelTschannen, Emanuele Bugliarello, Thomas Un-terthiner, Daniel Keysers, Skanda Koppula, FangyuLiu, Adam Grycner, Alexey A. Gritsenko, NeilHoulsby, Manoj Kumar, Keran Rong, Julian Eisen-schlos, Rishabh Kabra, Matthias Bauer, Matko Bosn-jak, Xi Chen, Matthias Minderer, Paul Voigtlaen-der, Ioana Bica, Ivana Balazevic, Joan Puigcerver,Pinelopi Papalampidi, Olivier J. Hnaff, Xi Xiong,Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.2024. PaliGemma: A versatile 3B VLM for transfer.CoRR, abs/2407.07726. ArXiv: 2407.07726. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.2014. Food-101 - Mining Discriminative Compo-nents with Random Forests. In Computer Vision -ECCV 2014 - 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part VI, volume 8694 of Lecture Notes in ComputerScience, pages 446461. Springer. Xi Chen, Xiao Wang, Soravit Changpinyo, A. J.Piergiovanni, Piotr Padlewski, Daniel Salz, Sebas-tian Goodman, Adam Grycner, Basil Mustafa, Lu-cas Beyer, Alexander Kolesnikov, Joan Puigcerver,Nan Ding, Keran Rong, Hassan Akbari, GauravMishra, Linting Xue, Ashish Thapliyal, James Brad-bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, An-dreas Steiner, Anelia Angelova, Xiaohua Zhai, NeilHoulsby, and Radu Soricut. 2022. PaLI: A Jointly-Scaled Multilingual Language-Image Model. CoRR,abs/2209.06794. ArXiv: 2209.06794. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-ravit Changpinyo, Alan Ritter, and Ming-Wei Chang.2023. Can Pre-trained Vision and Language ModelsAnswer Visual Information-Seeking Questions? InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 1494814968. Association for Computational Linguistics. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, ShuangXu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,Bo Zhang, Xiaolin Wei, and Chunhua Shen. 2023.MobileVLM : A Fast, Strong and Open VisionLanguage Assistant for Mobile Devices.CoRR,abs/2312.16886. ArXiv: 2312.16886. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, ShuangXu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu,Xinyang Lin, Bo Zhang, and Chunhua Shen. 2024.MobileVLM V2: Faster and Stronger Baseline forVision Language Model.CoRR, abs/2402.03766.ArXiv: 2402.03766. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023.InstructBLIP: Towards General-purposeVision-Language Models with Instruction Tuning.CoRR, abs/2305.06500. ArXiv: 2305.06500. J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical im-age database. In 2009 IEEE Conference on ComputerVision and Pattern Recognition, pages 248255. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, SongyangZhang, Haodong Duan, Maosong Cao, WenweiZhang, Yining Li, Hang Yan, Yang Gao, XinyueZhang, Wei Li, Jingwen Li, Kai Chen, ConghuiHe, Xingcheng Zhang, Yu Qiao, Dahua Lin, andJiaqi Wang. 2024. InternLM-XComposer2: Master-ing Free-form Text-Image Composition and Compre-hension in Vision-Language Large Model. CoRR,abs/2401.16420. ArXiv: 2401.16420. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021. An Image isWorth 16x16 Words: Transformers for Image Recog-nition at Scale. In 9th International Conference on",
  "Learning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. OpenReview.net": "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, LedellWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,and Yue Cao. 2022.EVA: Exploring the Limitsof Masked Visual Representation Learning at Scale.CoRR, abs/2211.07636. ArXiv: 2211.07636. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023. MME: A Comprehensive EvaluationBenchmark for Multimodal Large Language Models.CoRR, abs/2306.13394. ArXiv: 2306.13394. Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, andD. Parikh. 2017.Making the V in VQA Matter:Elevating the Role of Image Understanding in VisualQuestion Answering. In 2017 IEEE Conference onComputer Vision and Pattern Recognition (CVPR),pages 63256334. Dan Hendrycks, Steven Basart, Norman Mu, SauravKadavath, Frank Wang, Evan Dorundo, Rahul De-sai, Tyler Zhu, Samyak Parajuli, Mike Guo, DawnSong, Jacob Steinhardt, and Justin Gilmer. 2021a.The Many Faces of Robustness: A Critical Analy-sis of Out-of-Distribution Generalization. In 2021IEEE/CVF International Conference on ComputerVision, ICCV 2021, Montreal, QC, Canada, October10-17, 2021, pages 83208329. IEEE. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-hardt, and Dawn Song. 2021b. Natural AdversarialExamples. In IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2021, virtual, June19-25, 2021, pages 1526215271. Computer VisionFoundation / IEEE. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-Rank Adaptationof Large Language Models.In The Tenth Inter-national Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022. Open-Review.net. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-wal, Mandar Joshi, Kenton Lee, Kristina Toutanova,and Ming-Wei Chang. 2023. Open-domain VisualEntity Recognition: Towards Recognizing Millionsof Wikipedia Entities. In IEEE/CVF InternationalConference on Computer Vision, ICCV 2023, Paris,France, October 1-6, 2023, pages 1203112041.IEEE.",
  "Drew A. Hudson and Christopher D. Manning. 2019": "GQA: A New Dataset for Real-World Visual Rea-soning and Compositional Question Answering. InIEEE Conference on Computer Vision and PatternRecognition, CVPR 2019, Long Beach, CA, USA,June 16-20, 2019, pages 67006709. Computer Vi-sion Foundation / IEEE. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman,Cade Gordon, Nicholas Carlini, Rohan Taori, AchalDave, Vaishaal Shankar, Hongseok Namkoong, JohnMiller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-wig Schmidt. 2021. OpenCLIP. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna,Percy Liang, Thomas Kollar, and Dorsa Sadigh. 2024.Prismatic VLMs: Investigating the Design Spaceof Visually-Conditioned Language Models. CoRR,abs/2402.07865. ArXiv: 2402.07865.",
  "Jeonghwan Kim and Heng Ji. 2024. Finer: Investigatingand Enhancing Fine-Grained Visual Concept Recog-nition in Large Vision Language Models. CoRR,abs/2402.16315. ArXiv: 2402.16315": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013.3D Object Representations for Fine-Grained Categorization. In 2013 IEEE InternationalConference on Computer Vision Workshops, ICCVWorkshops 2013, Sydney, Australia, December 1-8,2013, pages 554561. IEEE Computer Society. Hugo Laurenon, Lucile Saulnier, Lo Tronchon, StasBekman, Amanpreet Singh, Anton Lozhkov, ThomasWang, Siddharth Karamcheti, Alexander M. Rush,Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023.OBELISC: An Open Web-Scale Filtered Datasetof Interleaved Image-Text Documents.CoRR,abs/2306.16527. ArXiv: 2306.16527.",
  "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge,Yixiao Ge, and Ying Shan. 2023a. SEED-Bench:Benchmarking Multimodal LLMs with GenerativeComprehension. CoRR, abs/2307.16125. ArXiv:2307.16125": "Junnan Li, Dongxu Li, Silvio Savarese, and StevenC. H. Hoi. 2023b. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders andLarge Language Models. CoRR, abs/2301.12597.ArXiv: 2301.12597. Junnan Li, Dongxu Li, Caiming Xiong, and StevenC. H. Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Un-derstanding and Generation. In International Con-ference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages1288812900. PMLR. Fangyu Liu, Emanuele Bugliarello, Edoardo MariaPonti, Siva Reddy, Nigel Collier, and Desmond El-liott. 2021. Visually Grounded Reasoning acrossLanguages and Cultures. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2021, Virtual Event /Punta Cana, Dominican Republic, 7-11 November,2021, pages 1046710485. Association for Computa-tional Linguistics.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b.Visual Instruction Tuning.CoRR,abs/2304.08485. ArXiv: 2304.08485": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, Kai Chen, and DahuaLin. 2023c. MMBench: Is Your Multi-modal Modelan All-around Player?CoRR, abs/2307.06281.ArXiv: 2307.06281. Ilya Loshchilov and Frank Hutter. 2019. DecoupledWeight Decay Regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net.",
  "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B.Blaschko, and Andrea Vedaldi. 2013.Fine-Grained Visual Classification of Aircraft.CoRR,abs/1306.5151. ArXiv: 1306.5151": "Oscar Maas, Benno Krojer, and Aishwarya Agrawal.2024. Improving Automatic VQA Evaluation UsingLarge Language Models. In Thirty-Eighth AAAI Con-ference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Ar-tificial Intelligence, IAAI 2024, Fourteenth Sympo-sium on Educational Advances in Artificial Intelli-gence, EAAI 2014, February 20-27, 2024, Vancouver,Canada, pages 41714179. AAAI Press. Thomas Mensink, Jasper R. R. Uijlings, Llus Castrejn,Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha,Andr Arajo, and Vittorio Ferrari. 2023. Encyclope-dic VQA: Visual questions about detailed propertiesof fine-grained categories. In IEEE/CVF Interna-tional Conference on Computer Vision, ICCV 2023,Paris, France, October 1-6, 2023, pages 30903101.IEEE.",
  "OpenAI. 2023b.GPT-4 Technical Report.CoRR,abs/2303.08774. ArXiv: 2303.08774": "Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman,and C. V. Jawahar. 2012. Cats and dogs. In 2012IEEE Conference on Computer Vision and PatternRecognition, Providence, RI, USA, June 16-21, 2012,pages 34983505. IEEE Computer Society. Angline Pouget, Lucas Beyer, Emanuele Bugliarello,Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, andIbrahim Alabdulmohsin. 2024. No filter: Culturaland socioeconomic diversity in contrastive vision-language models. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021a. Learn-ing Transferable Visual Models From Natural Lan-guage Supervision. arXiv preprint, abs/2103.00020._eprint: 2103.00020. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021b. Learn-ing Transferable Visual Models From Natural Lan-guage Supervision. In Proceedings of the 38th In-ternational Conference on Machine Learning, ICML2021, 18-24 July 2021, Virtual Event, volume 139 ofProceedings of Machine Learning Research, pages87488763. PMLR. Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, AaronAdcock, Laurens van der Maaten, Deepti Ghadi-yaram, and Olga Russakovsky. 2023. GeoDE: a Ge-ographically Diverse Evaluation Dataset for ObjectRecognition. In Advances in Neural Information Pro-cessing Systems 36: Annual Conference on NeuralInformation Processing Systems 2023, NeurIPS 2023,New Orleans, LA, USA, December 10 - 16, 2023. Mert Blent Sariyildiz, Yannis Kalantidis, Diane Lar-lus, and Karteek Alahari. 2021. Concept General-ization in Visual Representation Learning. In 2021IEEE/CVF International Conference on ComputerVision, ICCV 2021, Montreal, QC, Canada, October10-17, 2021, pages 96099619. IEEE.",
  "In Advances in Neural Information Processing Sys-tems 32: Annual Conference on Neural InformationProcessing Systems 2019, NeurIPS 2019, December8-14, 2019, Vancouver, BC, Canada, pages 1050610518": "Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang,Yu Qiao, and Ping Luo. 2023a. LVLM-eHub: AComprehensive Evaluation Benchmark for LargeVision-Language Models. CoRR, abs/2306.09265.ArXiv: 2306.09265. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang,Yu Qiao, and Ping Luo. 2023b. LVLM-eHub: AComprehensive Evaluation Benchmark for LargeVision-Language Models. CoRR, abs/2306.09265.ArXiv: 2306.09265. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,Ruoqi Liu, Ge Zhang, Samuel Stevens, DongfuJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Bo-tao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,Boyuan Zheng, Zhenzhu Yang, Yibo Liu, WenhaoHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023.MMMU: A Massive Multi-discipline MultimodalUnderstanding and Reasoning Benchmark for ExpertAGI. CoRR, abs/2311.16502. ArXiv: 2311.16502.",
  "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,and Lucas Beyer. 2023. Sigmoid Loss for LanguageImage Pre-Training. CoRR, abs/2303.15343. ArXiv:2303.15343": "Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, ChaoXu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,Songyang Zhang, Haodong Duan, Wenwei Zhang,Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, KaiChen, Conghui He, Xingcheng Zhang, Yu Qiao,Dahua Lin, and Jiaqi Wang. 2023a.InternLM-XComposer: A Vision-Language Large Model forAdvanced Text-image Comprehension and Composi-tion. CoRR, abs/2309.15112. ArXiv: 2309.15112. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, Longyue Wang, Anh Tuan Luu, WeiBi, Freda Shi, and Shuming Shi. 2023b. Sirens Songin the AI Ocean: A Survey on Hallucination in LargeLanguage Models. CoRR, abs/2309.01219. ArXiv:2309.01219. Yuhui Zhang, Alyssa Unell, Xiaohan Wang, DhrubaGhosh, Yuchang Su, Ludwig Schmidt, and SerenaYeung-Levy. 2024. Why are Visually-Grounded Lan-guage Models Bad at Image Classification? CoRR,abs/2405.18415. ArXiv: 2405.18415.",
  "ModelCheckpoint": "Idefics-1 (Laurenon et al., 2023)HuggingFaceM4/idefics-9b-instructIdefics-2 (Laurenon et al., 2024)HuggingFaceM4/idefics2-8bBLIP2 Flan-T5-XL (Li et al., 2023b)Salesforce/blip2-flan-t5-xlInstructBLIP Flan-T5-XL (Dai et al., 2023)Salesforce/instructblip-flan-t5-xlInstructBLIP Vicuna (Dai et al., 2023)Salesforce/instructblip-vicuna-7bInternLM XComposer 2 (Dong et al., 2024)internlm/internlm-xcomposer2-vl-7bLLaVA 1.5 (Liu et al., 2023a)llava-hf/llava-1.5-7b-hfLLaVA-Next (Mistral) (Liu et al., 2024)llava-hf/llava-v1.6-mistral-7b-hfMobileVLM V2 (Chu et al., 2024)mtgv/MobileVLM_V2-7BPali-Gemma 1google/paligemma-3b-mix-224Phi-3-Vision (Abdin et al., 2024)microsoft/Phi-3-vision-128k-instructQwen-VL-Chat (Bai et al., 2023)Qwen/Qwen-VL-Chat",
  "We expect the model to answer with a letter andcount the example as correct if the generated an-swer begins with the letter corresponding to thecorrect answer": "Dataset Details:In general, we evaluate on thefull test split (or, if no public test split existslike with ImageNet, the validation split) of everydataset.The datasets that we constructed from ImageNet-21k (Animal, Plant, Food, Artifact) are the excep-tion: due to the large amount of classes, we onlyuse 10 images per class instead of the full 50 tokeep computation time manageable. In addition,we use the processed version of ImageNet-21k and not the original (>1TB large) version for disk spacereasons; the processed version has all images re-sized to 224224px. During creating of the fourdatasets, we remove all classes that have no uniquelabel (keeping only the first occurrence of a label)to achieve a 1-to-1 mapping between classes andlabels.",
  "BTraining Details": "We closely follow the architecture and trainingprotocol of LLaVA 1.5 (Liu et al., 2023a).As LLM, we use the instruction-trained Sta-bleLM 2 1.6B Zephir (Bellagente et al., 2024)(stabilityai/stablelm-2-zephyr-1_6b),which is a small but performant LLM. The defaultimage encoder is OpenAI CLIP ViT-L/14-224.Training is done on a single NVIDIA RTX 3090with training one model taking less than 2 days.We train the models using AdamW optimizer(Loshchilov and Hutter, 2019) with a cosine learn-ing rate decay schedule. For the pre-training phase,we use learning rate 1e-3, weight decay 0, andbatch size 256. For the task-mix training phase, weuse learning rate 2e-4, weight decay 0, and batchsize 128; we do not fine-tune the full LLM butapply LoRA (Hu et al., 2022) to all weights withr = 64, = 128.",
  "CLVLM Performance on PopularBenchmarks": "We collate public results on select popular bench-marks for evaluating LVLMs (GQA (Hudson andManning, 2019), MMBench (Liu et al., 2023c),and MMMU (Yue et al., 2023)) for the models of. Comparing these results against the per-formance in object classification shows that thelatter is an independent skill that does not directlycorrelate with these benchmarks.",
  "ImageNet Image Distribution Shifts.There areseveral datasets that collect new images for theclasses of ImageNet-1k (Deng et al., 2009), orat least for a subset of them. Here, we consider": "ImageNet-Adversarial (Hendrycks et al., 2021b),which contains images for 200 classes that are diffi-cult to correctly classify for a model trained on theImageNet-1k training split; ImageNet-Rendition(Hendrycks et al., 2021a), which contains for 200classes images of the objects where the image ispainted, a plushy, origami, or other renditions; andImageNet-Sketch (Wang et al., 2019), which con-tains black-and-white drawings for all 1000 classes. CLIP models generally excel at transferring be-tween the different image distributions due to theirlarge-scale training (Radford et al., 2021b). Weevaluate in if LVLMs see similar resultsdespite training the alignment with the image en-coder on magnitudes less data and generally onlywith natural images. We observe that the rankingbetween the models is similar to our evaluation onFOCI in . The changes in accuracy fromImageNet-1k to the variants are qualitatively simi-lar to the underlying CLIP models for the LVLMs.This suggests that other representations of objects(like sketches) are encoded similarly enough bythe image encoder that the LVLM can recognizewithout extra training on different image types. Geographic Shifts with GeoDE.We now con-sider geographic distribution shift using GeoDE(Ramaswamy et al., 2023), a dataset with 40classes for which there are images evenly dis-tributed around the globe for six regions: Europe,Africa, Southeast Asia, West Asia, East Asia, andthe Americas (which does not include here the USor Canada). Results of the tested LVLMs are re-ported in . While GeoDE is a generallyeasy dataset with high accuracy throughout, westill observe substantial differences between theregions: European images consistently enjoy thehighest accuracy, all non-African regions followclose by with 0-3 points worse than Europe, and fi-nally, the African images noticeably trail behind by2-4 points lower accuracy compared to the overallaverage accuracy. This shows that geographic bi-ases in the training data, both for the image encoderand for the LVLM (Pouget et al., 2024), result indisadvantages for large parts of the population.",
  ": Results for the three sizes of MobileVLM v2": "Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy FGVC-Aircrafts CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy Flowers102 CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy Food101 CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy Oxford-Pets CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy",
  "Stanford Cars": "CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy IN-Animal CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy IN-Plant CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy IN-Artifact CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 0.8 1.0 accuracy IN-Food CLIP correct yesno"
}