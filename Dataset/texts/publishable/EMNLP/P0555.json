{
  "Abstract": "Reasoning is key to many decision making pro-cesses. It requires consolidating a set of rule-like premises that are often associated with de-grees of uncertainty and observations to drawconclusions. In this work, we address both thecase where premises are specified as numericprobabilistic rules and situations in which hu-mans state their estimates using words express-ing degrees of certainty. Existing probabilis-tic reasoning datasets simplify the task, e.g.,by requiring the model to only rank textual al-ternatives, by including only binary randomvariables, or by making use of a limited set oftemplates that result in less varied text. In this work, we present QUITE, a question an-swering dataset of real-world Bayesian reason-ing scenarios with categorical random variablesand complex relationships. QUITE provideshigh-quality natural language verbalizations ofpremises together with evidence statements,and expects the answer to a question in theform of an estimated probability. We conductan extensive set of experiments, finding thatlogic-based models outperform out-of-the-boxlarge language models on all reasoning types(causal, evidential, and explaining-away). Ourresults provide evidence that neuro-symbolicmodels are a promising direction for improv-ing complex reasoning. We release QUITE andcode for training and experiments on Github.1",
  "Introduction": "Reasoning about causality is an integral part of in-telligence, as it helps to understand and predict theworld. In the real world, causes and associationscan rarely be determined with complete certainty,and reasoning becomes inherently difficult if uncer-tainties are involved (Pearl, 1989). An automatedsystem for interpreting text describing causal re-lationships and their associated numeric probabil-ities or verbalized degrees of uncertainty would",
  ": Percentage of instances solved correctly foreach Bayesian reasoning type. The neuro-symbolicMistral-FT+ProbLog approach is robust against the in-herent difficulties of different reasoning types": "be highly useful in domains such as requirementsengineering (Yang et al., 2012) or text-mining inclinical documentation (Turner et al., 2021). Mod-eling linguistically expressed uncertainty has beenan active research area for decades in natural lan-guage processing (NLP) (Szarvas et al., 2008; Jeanet al., 2016; Sileo and Moens, 2023). Recently, large language models (LLMs) haveshown superior performance on many NLP tasks.However, they fall short of incorporating princi-pled reasoning mechanisms, with frequent failurecases (Kiciman et al., 2023), and their mathemat-ical skills decline if presented with unseen cases(Frieder et al., 2023; Yousefzadeh and Cao, 2023).In zero-shot or chain-of-thought (CoT) promptingsettings, open-source and open-weights LLMs arealso unable to outperform a random baseline inBayesian inference in higher-order causal networks(Jin et al., 2023). GPT-3 and GPT-4 are somewhatbetter, yet do not excel at the task.",
  "Jin et al. (2023) compile the CLADDER datasetbased on toy causal inference scenarios takenfrom textbooks and literature on causal reasoning.CLADDER evaluates performance by rungs of the": "Causal InferenceThe driver has attended a seniordriving training and the car'smileage is 20,000.What is the likelihood of beinginvolved in a moderateaccident? p = 0.685 Evidential InferenceThe car has an antilock breakingsystem.What is the likelihood that theowner's risk aversion is high? p = 0.724 Explaining awayThere was a mild accident.The mileage of the car is1,000,000.What is the likelihood for the driverhaving poor driving skills? p = 0.1",
  "Numeric Background Premises": "In 2% of the cases, the riskaversion behaviour of a car owner can bedescribed as psychopathic, adventurousin 58% of the cases, normal in 30% andcautious in 10% of the cases. [...] For anolder luxury model, there's a 30%probability it has anti-lock brakes, and a70% likelihood it doesn't. [...] If a personhas a normal risk aversion, there is a 70%probability that they have not takena senior driving training or safety course,and a 30% chance that they have taken it.[...] If a car has anti-lock brakes, has beendriven for twenty thousand miles, and isdriven poorly, there is a 40% chance ofno accident, a 30% chance of amild accident, a 20% chance of amoderate accident, and a 10% chance ofa severe accident.",
  "?": "WEP-based Background Premises If the insurance holder is a senior with acautious behaviour in terms of avoidingrisk, then it is highly likely that they havereceived senior training, and it isimprobable that they have not receivedsuch training. [...]Given that the insurance holder hasundergone senior training, the probabilityof the insurance holder havingsubstandard, normal, or expert drivingskills is as about half, unlikely, and highlyunlikely, respectively. [...] If the make andmodel of the car is a luxury car and it is acurrent year model, then anti-lock brakesare almost certainly present in the car. Ifthe car has anti-lock brakes, the mileageis 50,000, and the driving quality isexcellent, then it is almost certain thatthere will be no accident. [...]",
  "ladder of causation (Pearl and Mackenzie, 2018).2": "While this work inspired ours, it suffers from sev-eral limitations: First, its networks only includebinary random variables. Second, questions are ofthe form Does X increase the likelihood of Y?and expect yes/no answers (with a 50:50 distribu-tion). Hence, it is not designed to estimate thecorrelation of model output with the probabilitiesestimated according to the Bayesian network.In this paper, we present QUITE, a new bench-mark for Quantifying Uncertainty in natural lan-guage Text. As illustrated in , QUITEgoes one step further, leveraging toy and real-worldcausal networks and asking the model to output amuch finer-grained numeric probability estimate.In addition, our work is the first to make use ofcategorical random variables (and not just binaryvariables as in existing related datasets). Despite us-ing real-world networks, our dataset is not solvablefrom a question-evidence baseline alone, whichdemonstrates that the model cannot solve the tasksolely from background knowledge acquired dur-ing pre-training. To the best of our knowledge,our work is the first to explicitly distinguish the 2The rungs of the ladder are: (1) statistical dependenciesbased on observations: If I am vaccinated, how likely amI to survive? (2) interventions: If I get vaccinated, whatis the likelihood of surviving? (3) counterfactual reasoning:Would a person have survived if they had been vaccinated?",
  "three Bayesian inference types causal inference,evidential reasoning, and explaining-away, whichdirectly reflect the reasoning paths in the network.3": "Following the recent trend of probing LLMs fortheir mathematical capabilities, the BLInD dataset(Nafar et al., 2024) focuses on the numericBayesian reasoning capabilities of GPT models,hence only uses dummy event variables (e.g., or-ange event). Nafar et al. find that using program-aided language models (Gao et al., 2023) andneuro-symbolic approaches can drastically increasemodel performance. In their work, however, it re-mains an open research question whether this ap-proach scales to less template-like and more variednatural language text. QUITE goes one importantstep into this direction: with the support of LLMs,we verbalize complex real-world Bayesian reason-ing scenarios in a linguistically more varied style.Our validation shows that instances in QUITE are ofhigh accuracy with regard to probabilistic informa-tion, and use more complex yet mostly error-freelanguage compared to existing datasets.Existing datasets focus on probing for Bayesianreasoning capabilities when presented with verbal-ized numeric conditional probability tables. QUITEalso offers a setting that mimicks human conver-",
  "We address them in situations corresponding to rung 1 ofthe causal ladder": "sation, replacing probabilities with words of esti-mative probability (WEPs) such as unlikely orimprobable. This scenario has previously onlybeen investigated in the context of natural languageinference (Sileo and Moens, 2023). While in thissetting, parsing is more difficult for all models,performance is encouraging. Our results illustratethat when targeting natural text, structured causalmodels in combination with LLMs are a promisingapproach to estimating likelihood.A highly interesting finding of our experimentalstudy is that all included LLMs (including GPTmodels) fail on questions requiring evidential andexplaining-away reasoning both in zero-shot andCoT settings (see ). We hypothesize thatthe LLMs in our study have learned a good conceptof causality during pre-training, and that causalreasoning scenarios can often be solved based onstatistical patterns. Potentially, LLMs incorporatebiases for assuming causal analyses, as these typesof relationships are more frequently expressed inpretraining data. By contrast, our experimental re-sults demonstrate that a fine-tuned neuro-symbolicsystem has no difficulties solving the latter twocategories as well. We hence conclude that for in-tegrating complex reasoning capabilities into NLPsystems, neuro-symbolic models are a promising(if not necessary) direction.Our contributions are as follows: (1) We presenta novel dataset of verbalizations of Bayesian net-works including categorical variables in two ver-sions (with explicit probabilities vs. words of es-timative probability), symbolic target representa-tions, and question-evidence pairs that provide sim-ulated observations and queries asking for probabil-ities. (2) To the best of our knowledge, all closelyrelated recent prior work uses either vanilla or CoTLLMs, or generates neuro-symbolic representa-tions simply via manually designed prompts. Ourwork is the first to explicitly fine-tune state-of-the-art LLMs on semantic parsing to probabilistic first-order programming language ProbLog (De Raedtet al., 2007; Fierens et al., 2013). It consistentlyand strongly outperforms purely LLM-based ap-proaches in probabilistic reasoning on QUITE.",
  "review existing literature on modeling uncertaintyin language, benchmark datasets for Bayesian rea-soning, and semantic parsing to logical forms": "Bayesian Networks and Reasoning Patterns.Bayesian networks (BNs) represent joint probabil-ity distributions over a set of random variables andprobabilistic dependencies between them. Thesenetworks are modelled as directed acyclic graphswith nodes and directed edges between the nodes.Nodes represent random variables that can taketwo or more states. Edges correspond to probabilis-tic dependencies that are represented in so-calledconditional probability tables (CPTs). A randomvariable Xi is an observable attribute that can ran-domly take two or more disjoint states. For exam-ple, the outcome of throwing a coin could be eitherhead or tail. A Bayesian network therefore repre-sents a joint probability distribution over a set ofrandom variables {X1, . . . , Xn}: P(X1, . . . , Xn).A conditional probability distribution, denoted byedges in the network, is a modification of the jointprobability in which a random variable is condi-tioned on one or more other random variables:P(Xi|Xj, . . . ). Hence, there is now a dependencybetween Xi and all its parents in the graph, i.e., thevalue of the parents directly influences the outcomeof Xi.The combined nature of directed edges allowsfor different reasoning patterns. Causal reasoningrequires drawing conclusions about an effect if itscause is observed. Vice versa, reasoning aboutthe cause of an observed effect is called evidentialreasoning. Finally, drawing conclusions about acause if an effect and further causes of this effectare observed is called explaining-away. For a morein-depth introduction to Bayesian networks andreasoning patterns, please refer to Appendix A.",
  "Modeling uncertainty.BioScope (Szarvas et al.,": "2008; Farkas et al., 2010; Vincze, 2010) is an earlywork addressing the modeling of uncertainty inbiomedical text by marking triggers and their scope.A cluster of works has focused on modal verbswhich are a frequent trigger (Ruppenhofer and Re-hbein, 2012; Zhou et al., 2015; Henning et al., 2022;Wagner and Zarrie, 2023; Owan et al., 2023).Recently, much research concentrates on prob-ing how LLMs react to prompts containing expres-sions of (un)certainty. Zhou et al. (2023) find thatLLMs are highly sensitive to epistemic markersof certainty in the prompt, decreasing question an-swering (QA) performance drastically. Conversely,",
  ": Comparison of QUITE, CLADDER, and BLInD": "models have been tested with regard to whetherthey can express their own confidence in an answer(Tian et al., 2023).Modeling uncertainty has been investigated us-ing Natural Language Inference (NLI) tasks. Sileoand Moens (2023) frame uncertainty-based reason-ing as NLI to study how LLMs deal with words ofestimative probability (WEP) such as likely orimprobable. The task of Uncertain NLI (UNLI)(Chen et al., 2020) targets predicting a numericscore for the uncertainty in entailment between two(non-quantified) statements. Talman et al. (2023)explicitly model the variation in judgments of NLIinstances exhibited by groups of annotators. Bayesian reasoning:benchmark datasets.CLADDER (Jin et al., 2023) consists of 10k in-stances of verbalized Bayesian networks and asso-ciated questions. Their stories provide an overallsummary of the direct effects and spell out theCPTs. The BLInD dataset Nafar et al. (2024) teststo what extent GPT-3.5 and GPT-4 can performBayesian reasoning using template-based descrip-tions of dummy events. In contrast, QUITE focuseson complex real-world scenarios. CLADDER andBLInD instances are verbalized exclusively basedon templates, while QUITE exhibits a much largerlinguistic variety and higher grammaticality. Keydifferences between the three datasets are summa-rized in . All three studies (including ours)find that basic QA prompting does not work verywell, but that COT prompting brings improvements.One example instance from of each dataset is pro-vided in Appendix I. Semantic parsing to logical form.Constructingstructured representations from natural languagetext has been a long-standing research area in NLP(Zettlemoyer and Collins, 2007; Reddy et al., 2016;Kim et al., 2021). Recent work involves the special-ization of LLMs on this task. Olausson et al. (2023)present a framework called LINC that translateslogical statements into domain-specific languages,where the LLM acts as semantic parser and bridges the gap between natural language and structured,neuro-symbolic representations. Ye et al. (2023)employ an LLM to generate declarative sets of rulesthat are handed over to a SAT solver executable.Nafar et al. (2024) prompt LLMs to generate sym-bolic ProbLog code for solving the BLInD dataset.",
  "Numeric background premises areverbal-izationsofCPTsexplicitlymentioningpercentages.:In 2% of the cases, the riskaversion behaviour of a car owner can bedescribed as psychopathic [...]": "WEP-based background premises areverbal-izations of CPTs replacing every numericprobability value by an uncertainty quantifier(cf. .3.1): There is almost no chancethat the risk aversion behaviour of a car ownercan be described as psychopathic [...] Question-evidence (QE) pairs: Evidencesareobservations that set the value of one ormultiple random variables in the Bayesianmodel to a particular value. Queries are thenasking for the probability of a single randomvariable that is inferable given the evidence, i.e.,P(XQ = xmqq |XE1 = xm1E1 , . . . , XEj = xmjEj ),where xmiEi refers to the value that is assigned torandom variable XEi.",
  "Data Collection": "Our dataset is composed from a collection of pub-licly available BNs compiled from the literature.They reflect realistic probabilistic relationships inseveral domains (medicine, severe weather fore-casting, car insurance, mildew growth, phytoph-thora species, protein signalling, water treatment,and software troubleshooting). Our first data sourceis the bnlearn library (Scutari, 2010), which is com-monly used in benchmarking scenarios for algo-rithms for BNs (Liu et al., 2022; Daly and Shen, 2009; Lu et al., 2012). Our second source is theBNMA BN repository.4 Some of the BNs containnode counts in the order of magnitude 100 or 1000,hence, to keep the networks manageable in terms ofsize, we split them into subnetworks. We marginal-ize root nodes in subnetworks if they have ances-tors in the larger original network to obtain self-contained BNs. An example for this process isprovided in Appendix E. We end up with a totalof 14 different BNs, split into a total of 30 subnet-works. Our BNs contain nodes of degree 0 to 3,i.e., there are zero to three conditions (parent nodes)on which a random variable can depend. To thebest of our knowledge, we are the first to verbalizethese widely known networks to natural language,thereby making them available as a resource forNLP research.",
  "Dataset Creation Steps": "We semi-automatically create the natural languagepart of the dataset with the help of LLMs as illus-trated in Appendix H. As LLM backbone in ourpipeline, we use Mixtral-8x7B-Instruct-v0.1 (Jianget al., 2024).5 Each background premise in thedataset describes the probabilities (either expressednumerically or using WEPs) for all possible as-signments to one random variable Xi, given onespecific assignment to all the conditions. We gener-ate template-based premises by iterating over everyentry of each CPT and fill templates of the formIf [Conditions], then [Probabilities], where condi-tions refer to all incoming edges in the Bayesiannetwork and probabilities to the currently selectedvariable. Next, to create natural language premises,we prompt Mixtral with a prompt containing tech-nical explanations of the variables in the network,few-shot examples as well as the template-basedpremises. In contrast to related datasets that fullyrely on rule-based templates, QUITE hence con-tains more varied descriptions.For each network,we create a representation in ProbLog in a semi-automatic way: We manually define predicates forall nodes and categories in the CPTs, and then usea rule-based conversion. For the entire ProbLogdata (1192 statements), the first author of the paperhas manually checked if the statements match theirProbLog counterparts and if the wording and use of",
  "WEP-based Background Premises": "To guide the LLM to express natural language un-certainty in a principled way, we rely on a humanstudy conducted by Fagen-Ulmschneider (2015)that includes the subjective judgements by morethan 100 people who were asked to judge whichnumeric probability they associate with each ad-verb in a list. These adverbs are often referred to aswords of estimative probability (WEP), a term thatmainly originates from the work of Kent (1964),which investigates the mapping between specificuncertainty quantifiers and probabilities (see Ap-pendix B). We map each probability value to theclosest adverb. In case there is more than one pos-sible adverb (e.g., 10% maps to improbable, littlechance, and chances are slight), one of them israndomly selected. Additionally, to simulate sub-jectivity, we select the second-closest adverb in10% of the cases. If all states of a random variablehave the same probability, we manually correct theverbalization to equally likely.The heuristic of choosing the WEPs based on thepremises probabilities works well in most cases,yet we observe that this heuristic does not fully fitcases where all states of a categorical random vari-able have a low probability. For example, assumewe have P(Xi = x1i ) = 0.2, P(Xi = x2i ) = 0.2,P(Xi = x3i ) = 0.3, and P(Xi = x4i ) = 0.3. Thiswould lead to the following verbalization: It isprobably not the case that Xi takes the value x3i orx4i , and it is unlikely that it takes x1i or x2i . We man-ually add additional information to these instancedescribing the state that is still the most likely one.We leave the adaption of WEPs to this edge case asa direction for future research.",
  "Question-Evidence (QE) Pairs": "We construct QE pairs as follows. As evidences,we randomly sample 1 to n-1 observations per in-stance (i.e., XE1 = xm1E1 , . . . , XEj = xmjEj ) andlet Mixtral transform them to natural languagestatements such as The accident was mild. Forthe question, we sample one node Xq for whichMixtral formulates a question of the form: Whatis the likelihood of Xq having the value xmQ?Each QE pair requires calculating the probability",
  ": Dataset statistics for QUITE. Subscripts denotestandard deviation": "P(XQ = xmQ|XE1 = xm1E1 , . . . , XEj = xmjEj ). Theground truth answer (numeric probability value)is calculated based on the underlying probabilisticmodel of each subnetwork. Most QE instances canbe clearly categorized into their respective reason-ing pattern. We determine causal and evidentialreasoning by inspecting the list of parent and childnodes, respectively, and check if one of them isobserved, i.e., part of the evidence. To identifyexplaining-away QEs, we only check if one of thedirect child nodes of XQ and one of their direct par-ents is observed. The test set contains 92 causal, 62evidential and 26 explaining-away QE instances.",
  "Dataset Statistics": "We provide detailed statistics for QUITE in .We ensure that subnetworks that are derived fromthe same original network are assigned to onlytraining or test data, respectively. All QE pairs havebeen manually checked and if necessary correctedby the first author. On average, there are three tofour states per random variable. The average num-ber of background premises reflects the amounts ofprobabilistic statements that need to be processedbefore reasoning. The average number of premisesper network in QUITE is much higher than thosein related works, reflecting its challenging nature.The statistics in the lower part of differbetween training and test split due to taking theoriginal networks into account.",
  "Validation.The first author of the paper has per-formed extensive checking and correcting for the2384 premise statements. As a second validationstep, two of the (non-first) authors of this paper": "that were not exposed to the generation processbefore are presented with 400 randomly sampledpremises of QUITE (200 numeric and 200 WEP-based premises). They are asked to assess whetherthe LLM-generated output contains all input vari-ables and states and whether probabilities havebeen translated correctly. Of the numeric premises,193 instances (96.5%) correctly describe the under-lying probability distribution without ambiguities.Most errors relate to rounding close-to-zero proba-bilities to zero. Of the WEP-based instances, 188instances (94%) are correct, with the LLM misin-terpreting the input and wrong representations ofthe probability values being the main error causes.Our validation study shows that QUITE containsmostly well-formed instances that correctly reflectthe random variable states and probabilities. Linguistic Quality Assessment.To assess thelinguistic quality of QUITE, we make use of Gram-marly,6 a state-of-the-art commercial writing assis-tant. We compare QUITE to CLADDER (excludingits non-sensical subset) and BLInD. We randomlysample premises, evidences, and queries until acharacter count of approximately 95,000 has beenreached.Results are provided in . On average,QUITE has much fewer grammar and spelling mis-takes per instance than CLADDER.7 This high-lights the advantage of LLM-based generation overtemplate-based instance generation. QUITE hasthe most specific vocabulary, demonstrated by thehighest amount of rare words, which Grammarlydefines as words that do not belong to the 5k mostfrequent English words. According to the Flesh-Kincaid readability score (Kincaid, 1975), BLInDrequires skills of the level of 8th/9th graders, CLAD- DER and the WEP-based part of QUITE need 10thto 12th grade skills, and the numeric part of QUITErequires college-level reading skills.Finally, QUITE edges out on the two otherdatasets in terms of the overall Grammarly score.This as a strong indicator that our dataset is muchcloser to human-like natural language. We con-clude from this analysis that our dataset makes useof rich language with a complex vocabulary, and isclose to human-like language. Overall, QUITE iswell-suited for assessing the reasoning capabilitiesof state-of-the-art models in realistic scenarios.",
  "LLM Prompting Methods": "We experiment with several prompting techniquesfor state-of-the-art LLMs of different sizes.8 Inthe zero-shot setting, we provide all backgroundpremises of the network, a set of evidences and aquestion asking for the probability of a specificrandom variable taking a selected state.9TheCAUSALCOT technique was introduced by Jinet al. (2023) and asks the model to build up theprobabilistic graph, to extract the question type andto perform the mathematical calculation step bystep.",
  "Neuro-symbolic Approach": "Our ProbLog-based approach separates problemunderstanding and probabilistic reasoning, firstparsing each premise (both numeric and WEP-based), evidence statements, and queries into aProbLog program.In logic programming lan-guages, declarative programs are defined as a seriesof rules and facts that, in combination, evaluate totrue or false. In Prolog, rules are defined in first-order logic, where a rule body defines which condi-tions need to be met (i.e., need to be true) in orderfor the rule head to be evaluated as true. ProbLog(De Raedt et al., 2007; Fierens et al., 2013), whichwe use in this work, is a probabilistic program-ming language that extends the functionality ofProlog. It allows the specification of probabilisticmodels by declaring the probability distributions inFOL-style formulas. Since our dataset comprises 8GPT4-Turbo (turbo-2024-04-09) (OpenAI, 2024), Llama-3-8B-Instruct (AI@Meta, 2024), Mistral-7B-Instruct-v0.3(Jiang et al., 2023), and Mixtral-8x7B-Instruct-v0.1 (Jianget al., 2024). The temperature is set to 0.0.9We also performed preliminary experiments with an one-shot example which did not result in consistent improvements.",
  "::risk_aversion(car_owner, psychopathic);0.58::risk_aversion(car_owner, adventurous);0.30::risk_aversion(car_owner, normal);0.10::risk_aversion(car_owner, cautious)": "We first parse background premises intoProbLog using either a zero-shot LLM (ProbLog-Prompt) or an LLM fine-tuned for text-to-ProbLogparsing (ProbLog-FT). The QE pairs are parsedas a second step (i.e., after the vocabulary of predi-cates has been determined by the premise parsingstep). This is in particular important when usinga prompt-based LLM for semantic parsing. Afterparsing into a ProbLog program, the solver exe-cutes the code to determine the answer. A fullProbLog example for QUITE is provided in Ap-pendix D. For ProbLog-FT, we use Mistral-7B dueto its large context window of 32k tokens.",
  "Baselines": "As a trivial baseline, we report a system always pre-dicting 50%. The Regression-FT is a Llama2-7Bmodel (Touvron et al., 2023) trained for regres-sion with sigmoid output given all premises, evi-dences and the question. The input to the regressionlayer is the embedding of the last token. Addition-ally, we fine-tune a Mistral-7B model on predictingthe probability as text, e.g., The probability is p(LLM-FT).",
  "Evaluation Metrics": "As QUITE comprises two versions of the back-ground premises, we can investigate the followingresearch questions: (1) Given numeric premises, ev-idences, and a question, can the model(s) correctlycalculate the likelihood of events/states? (2) In thecase of linguistically specified uncertainty (WEP-based premises), can the model provide close esti-mates of the likelihood of events/states? For eachmodel, we hence report the percentages of correctpredictions,10 wrong predictions, and error caseswithout a valid numeric answer (e.g., in case ofinvalid ProgLog programs or if an LLM refuses",
  "to answer). RSME, computed asni=1(pi pi)2": "nreports how far numeric estimates deviate from theground truth. We report two variations for handlingerror cases: RMSE50% has a fallback to 50% asdefault answer for any invalid model output. Therationale behind choosing 50% as fallback valuefor RMSE50% is that whenever a model refuses toanswer or produces invalid output (e.g., erroneousProblog code), we can only make a random guess.Since 50% reflects an equal likelihood of some-thing being the case or not, it is a natural choice forthe fallback case.RMSEnonError is computed only over valid, butnot necessarily correct predictions.Note thatRMSEnonError scores are not comparable acrossrows. RMSE50% does not directly report the qual-ity of valid predictions, i.e., where the model orProblog solver return actual numeric values. Toalso judge the quality of the valid numeric predic-tions, RMSEnonError only takes instances into ac-count for which there are valid predictions. Sincethe amount of valid predictions heavily alters be-tween the different models and approaches, thisscore does not necessarily refer to the same in-stances between the different models, but can beused to interpret how close the numeric output of amodel is to the correct answer.",
  "Results for Numeric Premises": "The results for numeric background premises arereported in the upper part of . The QE-only baseline represents GPT-4 performance whenomitting the premises. Only 3.1% of the cases aresolvable by GPT-4 without referring to informa-tion in the premises, which means that backgroundknowledge and reasoning is required for solvingQUITE instances. This experiment validates thesuitability of QUITE to test for probabilistic reason-ing performance.The Regression-FT model does not predict anyinstances correctly and has a similar performanceas the baseline that always predicts the averageprobability value.LLM-FT correctly predictsthe output in 1/5 of the cases without invalid re-sponses, which implies that this method leveragedpre-training information in a better way.Next, we compare results for two promptingtechniques. In terms of accuracy and RMSE, GPT-4 outperforms the open-weights models by a largemargin. Jin et al. (2023) report considerable perfor-mance gains (10-20%) over zero-shot settings when using CAUSALCOT on CLADDER. On QUITE, weobserve that both GPT-4 and Llama-3 slightly profitfrom the CAUSALCOT technique, but the gain isnot as large as observed on CLADDER. Results aresomewhat inconclusive as performance for Mixtraleven drops when integrating CAUSALCOT.ProbLog-FT outperforms all other models andapproaches by a large margin. It is the only ap-proach that finds the correct answer to about everysecond question. Outsourcing all mathematicalsteps that are required to obtain the final answer toan external solver is much more effective overall,also achieving the best RMSE50 score, demonstrat-ing a clear benefit over the trivial baselines andover all prompting-based approaches. To substan-tiate the need for fine-tuning, we prompt GPT-4on the task of generating ProbLog code (ProbLog-Prompt). This model suffers from producing a veryhigh number of parsing errors (approximately 76%of the cases).",
  "Results for WEP-based Premises": "In the case of WEP-based background premises(lower part of ), we focus on the RMSEscores. Interestingly, when provided with the WEP-based premises, GPT-4 still arrives at the correctsolution in 8.7% of all cases, indicating that it lever-ages the textual descriptions in the premises. Com-pared to using numeric premises, Problog-FT pro-duces more parsing errors, indicating that moretraining data is necessary in this setting. Most no-tably, however, ProbLog-FT (7B parameters) per-forms on par with GPT-4 (estimated 8x222B pa-rameters), which indicates that fine-tuning neuro-symbolic models is a promising direction to im-prove automatic reasoning.",
  "Results by Reasoning Type": "(on page 1) breaks down the results by rea-soning type, showing how many instances in eachcategory have been solved correctly by the best-performing models. Causal reasoning seems to bethe easiest type of reasoning. All models exceptfor ProbLog-FT fail on evidential and explaining-away reasoning. We conclude that LLMs show rea-sonable skills in forward-style reasoning, whereasbackward-style reasoning seems to be a major issue.Once a valid representation of the causal structurehas been assembled, however, our neuro-symbolicmodels can perform any type of reasoning, illustrat-ing an important advantage of our neuro-symbolicapproach.",
  "Error Analysis": "In this section, we provide qualitative and quanti-tative analyses for different failure cases of our ap-proaches. Further analyses on the effect of networksize on performance are provided in Appendix G. Neuro-symbolic Approach.The ProbLog-FTmodel has two main sources of errors: syntax er-rors and unknown clauses, i.e., using undefinedpredicates. To get a sense of whether the mainsource of errors is step 1 (premise parsing) or step2 (QE parsing), we conduct an oracle experiment(cf. bottom row in ) in which the alreadyparsed premises are provided to the network andonly QE parsing is performed by the model. Inthis setup, the model is able to get four out of fivecases right, which strongly indicates that parsingthe lengthy premises is the main source of errors. Prompting.Our qualitative analysis reveals thatfor prompt-based LLM approaches, mathemati-cal errors are a frequent error case, with wronganswers being produced due to rounding errors orerroneous calculations. In other cases, the LLMsrefuse to answer because of the mathematical com-plexity or asks whether it should continue. Occa-sionally, the models insist on not having enough",
  "Conclusion and Outlook": "In this paper, we have presented QUITE, a newquestion answering dataset that provides Bayesianreasoning scenarios for a variety of domains andthat can be used to assess uncertainty-based reason-ing with LLMs. From a large set of experimentsusing numeric probabilistic premises and premisesexpressed using words of estimative probability,we conclude that a neuro-symbolic approach com-bining probabilistic logic programming and fine-tuned LLMs as semantic parsers is most promis-ing. Moreover, we find that non-specialized LLMsmostly fail on this task. Outlook.For increasing the robustness of logic-based semantic parsing models, different ap-proaches should be further investigated. For ex-ample, constrained decoding techniques could beused to ensure that only valid predicates can be gen-erated at any point in time. Next steps include alsostudying reasoning in modal and counterfactualscenarios.",
  "Limitations": "In this work, we investigate probabilistic reason-ing with uncertainty using verbalized Bayesian net-works. This approach assumes that the entire prob-ability distribution is known and given at any time.However, in real-life scenarios (e.g., in data that isobtained from production plants), probability dis-tributions are often underspecified or influencingfactors are not even known, i.e., there are hiddenvariables that influence the relationships. Further-more, our models act upon a limited number ofsentences at once, whereas uncertainty descriptionscould also be provided in longer texts that also con-tain information that is not relevant to the reasoningprocess.In its current version, QUITE operates on rung1 of the ladder of causation (Pearl and Mackenzie,2018). Our work could of course be extended inthe future to also cover rung 2 of the ladder of cau-sation, meaning that based on the networks, onecould perform interventional queries (do-operator)that dynamically modify the probabilistic relation-ships. To do that, we need to generate additionalqueries of form If we force [...], does that leadto [...] and map that onto the do/1 predicate inProbLog. As a first step, however, we decidedto carefully study rung-1 questions with regard tothree Bayesian reasoning types.As with most benchmarks these days, there isa potential issue of data contamination, i.e., theLLMs could have seen relevant parts of QUITEin their pre-training corpus. Our natural languagecorpus is based on plain probability tables. Thesetables could have been part of the pretraining cor-pus. Some of the networks in our dataset weredescribed in published work before. Therefore, itcould be that some of the relationships betweenrandom variables are vaguely known to the LLMs.However, we argue that no paper describes largeBNs in every detail, preventing the LLMs fromlearning every network detail by hard. This as-sumption is supported by the poor performance ofthe question-evidence only baseline.",
  "Ethical Considerations": "QUITE builds upon data from many different sci-entific and non-scientific domains. These includedifferent Bayesian networks from domains relatedto medical treatment and health issues. However,we emphasize that QUITE and our proposed modelsin their current version should not be used for any kind of reliable decision making in medicine andhealth-related issues. All probabilistic networksin QUITE only reflect a subset of the entire causalrelationships that might exist and are used for as-sessing self-contained Bayesian reasoning withoutconsidering the much broader scientific knowledgeavailable to the world. Furthermore, we did not ver-ify the correctness of the observed data by checkingthe biomedical literature.",
  "Acknowledgements": "We would like to express our deepest gratitude toMarco Scutari, who is the author of bnlearn andgave us the permission to use the Bayesian net-works for our research. We also thank the anony-mous reviewers for their valuable feedback. Finally,we would like to thank our colleagues at Bosch Re-search for all the valuable discussions, feedbackand ideas on and for our work.",
  "Wade Fagen-Ulmschneider. 2015. Perception of proba-bility words": "Richrd Farkas, Veronika Vincze, Gyrgy Mra, JnosCsirik, and Gyrgy Szarvas. 2010.The CoNLL-2010 shared task: Learning to detect hedges andtheir scope in natural language text.In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning Shared Task, pages 112, Uppsala, Sweden. Association for ComputationalLinguistics. Daan Fierens, Guy Van den Broeck, Joris Renkens,Dimitar Sht. Shterionov, Bernd Gutmann, Ingo Thon,Gerda Janssens, and Luc De Raedt. 2013. Inferenceand learning in probabilistic logic programs usingweighted boolean formulas. CoRR, abs/1304.6810.",
  "Simon Frieder, Luca Pinchetti, Alexis Chevalier,Ryan-Rhys Griffiths, Tommaso Salvatori, ThomasLukasiewicz, Philipp Christian Petersen, and JuliusBerner. 2023. Mathematical capabilities of chatgpt": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-ham Neubig. 2023. PAL: program-aided languagemodels. In International Conference on MachineLearning, ICML 2023, 23-29 July 2023, Honolulu,Hawaii, USA, volume 202 of Proceedings of MachineLearning Research, pages 1076410799. PMLR. Sophie Henning, Nicole Macher, Stefan Grnewald,and Annemarie Friedrich. 2022. MiST: a large-scaleannotated resource and neural models for functionsof modal verbs in English scientific text. In Find-ings of the Association for Computational Linguistics:EMNLP 2022, pages 13051324, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics.",
  "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2021. Lora: Low-rank adaptation oflarge language models": "Pierre-Antoine Jean, Sbastien Harispe, Sylvie Ranwez,Patrice Bellot, and Jacky Montmain. 2016. Uncer-tainty detection in natural language: a probabilisticmodel. In Proceedings of the 6th International Con-ference on Web Intelligence, Mining and Semantics,WIMS 2016, Nmes, France, June 13-15, 2016, pages10:110:10. ACM. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele,Ojasv Kamal, LYU Zhiheng, Kevin Blin, Fer-nando Gonzalez Adauto, Max Kleiman-Weiner,Mrinmaya Sachan, et al. 2023. Cladder: Assess-ing causal reasoning in language models. In Thirty-seventh Conference on Neural Information Process-ing Systems.",
  "Emre Kiciman, Robert Ness, Amit Sharma, and Chen-hao Tan. 2023. Causal reasoning and large languagemodels: Opening a new frontier for causality. CoRR,abs/2305.00050": "Gene Kim, Viet Duong, Xin Lu, and Lenhart Schu-bert. 2021. A transition-based parser for unscopedepisodic logical forms. In Proceedings of the 14th In-ternational Conference on Computational Semantics(IWCS), pages 184201, Groningen, The Netherlands(online). Association for Computational Linguistics. J.P. Kincaid. 1975. Derivation of New Readability For-mulas: (automated Readability Index, Fog Countand Flesch Reading Ease Formula) for Navy EnlistedPersonnel. Research Branch report. Chief of NavalTechnical Training, Naval Air Station Memphis.",
  "Judea Pearl and Dana Mackenzie. 2018.The Bookof Why: The New Science of Cause and Effect, 1stedition. Basic Books, Inc., USA": "Siva Reddy, Oscar Tckstrm, Michael Collins, TomKwiatkowski, Dipanjan Das, Mark Steedman, andMirella Lapata. 2016.Transforming dependencystructures to logical forms for semantic parsing.Transactions of the Association for ComputationalLinguistics, 4:127140. Josef Ruppenhofer and Ines Rehbein. 2012. Yes wecan!? annotating English modal verbs. In Proceed-ings of the Eighth International Conference on Lan-guage Resources and Evaluation (LREC12), pages15381545, Istanbul, Turkey. European LanguageResources Association (ELRA).",
  "Damien Sileo and Marie-Francine Moens. 2023. Prob-ing neural language models for understanding ofwords of estimative probability": "Gyrgy Szarvas, Veronika Vincze, Richrd Farkas, andJnos Csirik. 2008. The BioScope corpus: anno-tation for negation, uncertainty and their scope inbiomedical texts. In Proceedings of the Workshop onCurrent Trends in Biomedical Natural Language Pro-cessing, pages 3845, Columbus, Ohio. Associationfor Computational Linguistics. Aarne Talman, Hande Celikkanat, Sami Virpioja,Markus Heinonen, and Jrg Tiedemann. 2023.Uncertainty-aware natural language inference withstochastic weight averaging. In Proceedings of the24th Nordic Conference on Computational Linguis-tics (NoDaLiDa), pages 358365, Trshavn, FaroeIslands. University of Tartu Library. Katherine Tian, Eric Mitchell, Allan Zhou, ArchitSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,and Christopher Manning. 2023. Just ask for cali-bration: Strategies for eliciting calibrated confidencescores from language models fine-tuned with humanfeedback. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 54335442, Singapore. Association forComputational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Mark Turner, Julia Ive, and Sumithra Velupillai. 2021": "Linguistic uncertainty in clinical NLP: A taxonomy,dataset and approach. In Experimental IR Meets Mul-tilinguality, Multimodality, and Interaction - 12thInternational Conference of the CLEF Association,CLEF 2021, Virtual Event, September 21-24, 2021,Proceedings, volume 12880 of Lecture Notes in Com-puter Science, pages 129141. Springer. Veronika Vincze. 2010. Speculation and negation an-notation in natural language texts: what the case ofBioScope might (not) reveal. In Proceedings of theWorkshop on Negation and Speculation in NaturalLanguage Processing, pages 2831, Uppsala, Swe-den. University of Antwerp. Jonas Wagner and Sina Zarrie. 2023. Probing BERTsability to encode sentence modality and modal verbsense across varieties of English. In Proceedings ofthe 15th International Conference on ComputationalSemantics, pages 2838, Nancy, France. Associationfor Computational Linguistics. Hui Yang, Anne N. De Roeck, Vincenzo Gervasi, Al-istair Willis, and Bashar Nuseibeh. 2012. Specu-lative requirements: Automatic detection of uncer-tainty in natural language requirements. In 2012 20thIEEE International Requirements Engineering Con-ference (RE), Chicago, IL, USA, September 24-28,2012, pages 1120. IEEE Computer Society.",
  "Yaowei Zheng, Richong Zhang, Junhao Zhang, YanhanYe, Zheyan Luo, and Yongqiang Ma. 2024. Llamafac-tory: Unified efficient fine-tuning of 100+ languagemodels. arXiv preprint arXiv:2403.13372": "Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto.2023. Navigating the grey area: How expressionsof uncertainty and overconfidence affect languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 55065524, Singapore. Association for Com-putational Linguistics. Mengfei Zhou, Anette Frank, Annemarie Friedrich, andAlexis Palmer. 2015. Semantically enriched modelsfor modal sense classification. In Proceedings of theFirst Workshop on Linking Computational Modelsof Lexical, Sentential and Discourse-level Seman-tics, pages 4453, Lisbon, Portugal. Association forComputational Linguistics.",
  "A.1Bayesian Networks": "A Bayesian network is a directed acyclic graph(DAG) G = (V, E) that represents a joint proba-bility distribution P over a set of random variables{X1, . . . , Xn}. Each random variable Xi can takevalues from their respective domain i, which isthe set of possible realizations {x1i , . . . , xki }, wherek = |i|. If k = 2, then we say that Xi adheresto a Bernoulli distribution, whereas k > 2 makesthem a categorical random variable. Furthermore,to ensure a valid probability distribution, it mustalways hold that kj=1 P(Xi = xji) = 1.Each node vi V in G represents one variableXi. An edge ei,j E between two nodes vi, vjrepresents a correlation between the two associ-ated random variables and thereby models the con-ditional probability distribution (CPD) P(Xj|Xi).These CPDs are of special importance when deal-ing with so-called observations. Mathematicallyspeaking, observations modify the joint probabilitydistribution over G as follows:",
  "A.2Reasoning Patterns": "Directed edges in a Bayesian network not only indi-cate a dependence, but they also allow for differenttypes of reasoning when observing one or morerandom variables. In the following, we assume asimple three-node network with nodes X1, X2, X3and edges X1 X3 and X2 X3. Causal Reasoning: We know the cause and drawconclusions about the effect. Suppose we lookat the connection X1 X3 and observe thevalue of X1. This gives us a strong hint on whatthe status of X3 could likely be. The underly-ing probability distribution is P(X3|X1). Forinstance, assume we observe that the weather is",
  "rainy. This leads us to the conclusion that thestreets are very likely to be wet": "Evidential Reasoning: The other way round isto observe X3 and reason about X1 in X1 X3. We now know about the value of the ef-fect, which we can use to make assumptionsabout the most likely cause. The mathematicalterm is P(X1|X3). This probability is not di-rectly represented in this Bayesian network, butit can be calculated by using Bayes theorem:P(X1|X3) = P(X1,X3)",
  "P(X3) . Let us now assume weobserve wet streets. This gives us strong hintson whether it has been raining before": "Explaining-Away: This requires multiple causeswith a common effect, shaping a so-called v-structure (X1 X3 X2). Now observingone of the potential causes (X1 or X2) and theeffect explains the influence of the other causesaway. Assume that we again observe wet streets.This could be due to rain, but also due road clean-ing machines. If we now obtain the knowledgethat it is raining or was raining, we can assumethat road cleaning is unlikely. The state of theweather explains away the need for road clean-ing machines. We enrich our dataset by categorizing the queriesinto their respective reasoning type(s) if applicable,making it possible to also investigate the robust-ness of LLMs with respect to different reasoningpatterns.",
  "BWords of Estimative Probability (WEP)": "lists the WEPs which are used to modeluncertainty in QUITE.The table provides amapping between adverbs and numeric probabil-ities, estimated via a survey conducted by Fagen-Ulmschneider (2015).Every numeric value ismapped to the closest adverb, not considering theconfidence intervals in the table. However, we in-troduce one exception in the mapping: if a numericprobability is below 45%, it is not mapped to theclosest adverb (i.e., about even), but instead to prob-ably not. This is to make sure that values of 38%for instance are not mapped to about even.",
  ": Parameters for LoRA-based fine-tuning onQUITE": "up of billions of parameters, we use LoRA (Huet al., 2021) to fine-tune low-rank adapters onQUITE. We use the code from Tunstall et al. (2023)to fine-tune all models on Nvidia H100 and A100GPUs. All models are trained using full precision,i.e., FP32 to make sure that we do not lose perfor-mance. The hyperparameters for each model arelisted in . To keep fine-tuning sustainable,we refrained from performing an extensive hyper-parameter search. Instead, we selected commonlyused values (e.g., cf. Zheng et al. (2024)). Mod-els are selected based on their performance on thedevelopment set, which is a subset of train, andevaluated only once on test.",
  ": Exemplary network from QUITE about the relationship between gallstones, flatulence and amylase levels": "here which we are going to refer to as G (gall-stones), F (flatulence), and A (amylase levels). Itis important to mention that the models are onlygiven the background premises, i.e., statements 0,1, 2, 3, and 4. In a first step, they have to buildup the graph shown, either as internal representa-tions in their embeddings, as textual output in achain-of-thought style of reasoning, or explicitly asProbLog code. Furthermore, it is up to the model todetermine not only the involved random variables,but also their possible states, i.e., i. For gall-stones (G) and flatulence (F), it is a yes or nodecision, i.e., |G| = |F | = |{yes, no}| = 2.Amylase levels show that QUITE also containsmany categorical variables since A can take thevalues 0-299, 300-499, or 500-1400, i.e.,|A| = |{0 299, 300 499, 500 1400}| = 3.Next, we look at a specific question-evidence(QE) pair with one observiation (evidence) and onequestion:",
  "% Premise 20 . 4 3 0 7 : : f l a t u l e n c e ( p a t i e n t ): notg a l l s t o n e s ( p a t i e n t )": "% Premise 30 . 9 3 4 6 : : amylase ( p a t i e n t , 0299 ) ;0 . 0 4 6 7 : : amylase ( p a t i e n t , 300499 ) ;0 . 0 1 8 7 : : amylase ( p a t i e n t , 5001400 ): g a l l s t o n e s ( p a t i e n t ) . % Premise 40 . 9 7 3 0 : : amylase ( p a t i e n t , 0299 ) ;0 . 0 1 6 9 : : amylase ( p a t i e n t , 300499 ) ;0 . 0 1 0 1 : : amylase ( p a t i e n t , 5001400 ): notg a l l s t o n e s ( p a t i e n t ) .",
  ": Full ProbLog code for the gallstone-flatulence-amylase instance": "depicts how this mathematical prob-lem is represented in ProbLog. The first five state-ments define the probabilistic model. We expectthe model to perform semantic parsing from thenatural language input to this structured represen-tation. Each right-hand side represents the con-ditions for the left-hand side. Furthermore, sinceamylase levels is a categorical variable, it is nec-essary to connect all possible states via a semi-colon in order to match them to the same proba-bility distribution. This is an additional difficultyof QUITE since models can not rely on just writ-ing down binary predicates. Next, the evidence/2predicate is used to set the observation. Here itis of key importance that the model only reusespredicates that were already defined in the parsedpremises above. Finally, the question is definedusing query/1.When calling ProbLog on thisprogram, it outputs amylase(patient,500-1400):0.011316399, which perfectly matches our calcula-tion by hand.",
  "EBN Subsetting": "Assume we want to extract the network D C E from the five-node network depicted in .It is not possible to just cut the connections A Cand B C since node C only holds tables forCPDs that depend on A and B respectively. There-fore, we modify the subnetwork by marginalizingout A and B from the probability distribution ofC:",
  "FFurther Dataset Statistics": "In Appendix D, we used a three node networkfrom QUITE. This was for demonstration purposes.However, QUITE contains networks of much largersizes, i.e., with up to 13 nodes. shows thedistribution of node counts across the 30 networksin the dataset. The largest network holds a jointprobability distribution over 13 random variables.Also, more than half of the networks do have atleast 5 nodes. The median size is 6, showing thatthe majority of our networks have of large size. depicts the distribution of premisecounts in QUITE. The median of the distributionis 34 and the maximum is 119. This shows thatbuilding up the probabilistic model from the set ofpremises is already a computationally demandingtask and requires very long context understanding.",
  "GFurther Analysis": "and sort the results of ProbLog-FT and CAUSALCOT on the ten networks in the testin ascending order by number of premises. Thereis a clear trend that shows that a growing number ofbackground premises lead to an increasing amountof failure cases. This can be explained by the factthat having many background premises requires themodel to work with an increasingly large messagecontext of all already-parsed ProbLog premises.When analyzing the performance for differentnumbers of states per (categorical) random variable(cf. and ), one cannot identifya clear trend. An increasing number of states seemmore challenging, but we suspect that other factorssuch as complexity of the domain might play abigger role. From this, we conclude that the amountof background premises seems to have a largerinfluence on the failure probability than the averageamount of states per random variable.",
  "HData Generation Pipeline": "displays an overview of our data genera-tion pipeline. Every CPT entry is verbalized usingthe Mixtral LLM. Randomly sampled evidencesand question nodes, which build the QE pairs inQUITE, are also transferred into natural languageusing Mixtral. For that, we prompt the LLM withthe instruction to formulate grounded statements inthe case of evidences and questions asking for prob-abilities in the case of queries. For each BN, wecreate an equivalent ProbLog representation thatis used to generate the ground truth answer and tofine-tune the LLMs.",
  "In this section, we provide one sample from CLAD-": "DER and BLInD each.CLADDER uses pre-defined BN structures withthree or four nodes. The following backgroundpremises are taken from a three-node network andrepresent the distribution over two binary randomvariables that can take the values true or false: The overall probability of alarm set byhusband is 3%. For husbands that dontset the alarm, the probability of ringingalarm is 74%. For husbands that set thealarm, the probability of ringing alarmis 22%.",
  "One instance in BLInD is the following set ofbackground premises:": "If purple event is False, then grey eventis True with probability of 39%. If purpleevent is False, then grey event is Falsewith probability of 61%. If purple eventis True, then grey event is True with prob-ability of 3%. If purple event is True,then grey event is False with probabilityof 97%. purple event is true with proba-bility of 55%. purple event is false withprobability of 45%. The corresponding question What is the probabil-ity that grey event is True given that purple eventis False? requires the following computation toobtain the answer: P(grey = True|purple =False). Again, all random variables, in this casethe color events, can only take the two states trueor false."
}