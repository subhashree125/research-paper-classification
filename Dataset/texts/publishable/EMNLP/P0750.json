{
  "Abstract": "Large Language Models (LLMs) have emergedas highly capable systems and are increasinglybeing integrated into various uses. Neverthe-less, the rapid advancement in their deploymenttrails a comprehensive understanding of theirinternal mechanisms, as well as a delineationof their capabilities and limitations. A desiredcharacteristic of an intelligent system is its abil-ity to recognize the scope of its own knowl-edge. To investigate whether LLMs embodythis attribute, we develop a benchmark thatchallenges these models to enumerate all in-formation they possess on specific topics. Thisbenchmark assesses whether the models recallexcessive, insufficient, or the precise amount ofrequired information, thereby indicating theirawareness of how much they know about thegiven topic. Our findings reveal that the emer-gence of this property varies across different ar-chitectures and manifests at diverse rates. How-ever, with sufficient scaling, all tested modelsare ultimately capable of performing this task.The insights gained from this research advanceour understanding of LLMs, shedding light ontheir operational capabilities and contributingto the ongoing exploration of their intricate dy-namics.",
  "Introduction": "Large Language Models are renowned for theirability to memorize vast amounts of information en-countered during training (OpenAI, 2023; Touvronet al., 2023; Gemini Team, 2023). This information,stored in their parameters, can be recalled duringinference, serving both for information retrievaland problem-solving (Vinyals and Le, 2015; Rad-ford et al., 2019; Chung et al., 2022a; Geva et al.,2023). While it is well-established that LLMs canact as knowledge bases (Petroni et al., 2019; Heinz-erling and Inui, 2021; AlKhamissi et al., 2022), theextent to which they understand their own knowl-edge is less clear (Liang et al., 2024). For instance, do these models know if or when they know theanswer to a question (Kadavath et al., 2022; Yinet al., 2023)? Can they quantify their own expertiseon a topic? Are they aware if some of their knowl-edge contradicts other information they possess?Can they differentiate between explicitly learnedinformation and implicit knowledge?These questions are crucial, as awareness ofones own knowledge and limitations is a vital as-pect of any intelligent system. Without it, an AIcould be prone to hallucinate (Ye et al., 2023; Xuet al., 2024), lie about its expertise (Azaria andMitchell, 2023; Pacchiardi et al., 2023), overesti-mate its responses (Desai and Durrett, 2020; Ope-nAI, 2023), or contradict itself (Chen et al., 2023),all of which are undesirable traits for AI systemsintended to be useful.This study focuses on understanding whetherLLMs know the extent of their knowledge on spe-cific topics, such as individuals, locations, eventsor concepts. To explore this, we task LLMs withenumerating everything they know about a giventopicno more, no less. Should a model consis-tently recall just the right amount of information,it suggests an understanding of its own knowledge.Conversely, if a model does not know how much itknows, it may recall too little or hallucinate addi-tional information.Our approach involves fine-tuning LLMs on thediary entries of various fictitious individuals. Eachentry is treated as an individual document in ourfine-tuning dataset, with each diarist authoring arandom number of entries. During inference, weask the models to recall all diary entries of a spec-ified individual in chronological order. We thenevaluate whether the recalled entries match the orig-inal entries both in terms of content and quantity. provides an illustrative example.WebenchmarktheperformanceoftheOPT (Zhang et al., 2022), Pythia (Biderman et al.,2023), and Flan-T5 (Chung et al., 2022b) suites of",
  "models. Our key findings are as follows:": "All tested LLMs, if scaled sufficiently, demon-strate an understanding of how much theyknow. This capability appears to emerge un-der different conditions depending on the ar-chitecture. For example, the smallest OPTmodel can perform this task effectively if thefine-tuning dataset is sufficiently large. In con-trast, Pythia and Flan-T5 require joint scalingof both the dataset and model size to performwell.",
  "However, when these conditions are not met(i.e., insufficient scaling), models often re-call a random number of diary entries, eitherrecalling too few or hallucinating additionalones": "Interestingly, the number of diary entries torecall and their length do not impact modelperformance, meaning that when models arecapable of performing this task, they are justas good at recalling eight entries as they are atrecalling one. Finally, we discuss potential factors responsiblefor the observed differences in the emergence ofthis capability. Overall, our work contributes toa deeper understanding of the inner workings ofLLMs, shedding light on a not-so-well-understoodaspect of these models.",
  "Methodology": "The foundation of our analysis hinges on the abilityof models to memorize and recall information. Toavoid the influence of existing data, which mightbe part of the pre-training corpus of the languagemodels we are benchmarking, we generate our own.This ensures that the models have never encoun-tered the data during pre-training, thereby prevent-ing any contamination of our results.In essence, our approach involves: (i) gener-ating the training documents, (ii) fine-tuning alanguage model using its pre-training objective tomemorize these documents, and (iii) testing thelanguage models ability to recall all related docu-ments. We delineate each stage of our frameworkin the following sections.",
  "Recallallof{name}sdiaryentries, in order": "The answer to the question is the concatenation ofthe individuals diary entries in ascending order byentry numbers. illustrates examples ofboth generated documents and questions.To effectively train the model, we incorporate90% of the question-answer (Q/A) pairs, alongwith all diary entries, into the training set. Theremaining 10% of the questions are evenly dividedinto a validation set and a test set. By adding Q/Aexamples to the training set, the model can learn theevaluation task, similar to the process of instruction-tuning.Initially, we trained the model first on the doc-uments and then on the evaluation task. However,this approach led to catastrophic forgetting of thedocuments and overfitting on the Q/A examples.Therefore, we decided to fine-tune the model onboth simultaneously to prevent these issues.",
  "Fine-Tuning & Evaluation": "To benchmark an LLM, we begin by fine-tuningit using its pre-training objective, such as causallanguage modeling, on our training set. This fine-tuning process mirrors the standard training of anLLM on a text corpus. Depending on the architec-ture of the LLM, we format the input as follows: Decoder-Only Models (e.g., OPT): For bothdiary entries and Q/A pairs, the training ob-jective is causal language modeling. In thecase of Q/A pairs, we concatenate the questionwith the answer into a single text sequence,separated by an end-of-line token (\\n). Encoder-Decoder Models (e.g., Flan-T5):When processing a diary entry, the first line(e.g., Toms Diary Entry 1) is input to theencoder, and the decoder generates the entiredocument. For Q/A pairs, the question is fedto the encoder, and the decoder predicts theanswer.",
  "Throughout the fine-tuning process, we periodi-cally evaluate the model on the validation set. Fordecoder-only architectures, the model is promptedwith a question with the goal of generating the": "corresponding answer. For encoder-decoder archi-tectures, the question is given to the encoder andthe decoder must produce the answer.We fine-tune up until the validation performanceplateaus. We then select the best checkpoint basedon peak validation performance, and evaluate themodel on our test set using the same procedure aswith the validation set. Performance is measuredin terms of accuracy, defined as the number of cor-rectly answered questions. An answer is deemedcorrect if it matches the ground truth exactly, withno errors in the number of documents recalled andthe content of each recalled document.",
  "Design Motivation": "Requiring the model to consolidate informationfrom multiple training documents allows us to as-sess whether it understands the extent of its knowl-edge related to the individual in question. Specifi-cally, during training, the model memorizes the di-ary entries. Then, in the evaluation phase, it needsto know how many documents to recall, meaningthe model must know how many diary entries itknows about the individual. If a model consistentlyrecalls the exact number of documents, it shows anunderstanding of its own knowledge. Conversely, amodel which does not know how many documentsit knows, would recall a random number.As for our choice of using synthetic data, it al-lows us to precisely control its distribution andproperties. This extends to the length and con-tent of the documents, as well as the number ofdiary entries authored by an individual. By usingattributes as the body of the documents, we canmanage the entropy, ensuring that each sentencecontains a fixed amount of information. Conse-quently, adding an additional sentence consistentlyincreases the documents information by that fixedamount.This approach enables us to examine how docu-ment length affects the model in a more controlledmanner compared to using real data. While wehave arbitrarily chosen individuals as the topic link-ing multiple documents, this could have been anyother concept. We believe this choice does notimpact the observed trends in the results.Overall, our benchmark is designed to facilitatethe study of this problem and its key variables ina controlled environment, emulating the challengefaced by language models of memorizing informa-tion during training and understanding the extentof their knowledge concerning specific topics.",
  "Setup": "Dataset. To evaluate the impact of the numberof training examples on the model performance,we generate six datasets containing 1K to 32K di-arists, with each successive dataset doubling in sizecompared to its predecessor. By incrementally en-larging the dataset size as described, models seea broader array of examples from which they canlearn to derive their generative capabilities, whilesimultaneously being challenged to memorize alarger volume of documents.For each individual, we generate 1 to 8 diary en-tries, with each entry consisting of 1 to 8 attributes.The training, validation and test sets each containan equal distribution of individuals who have writ-ten one, two, three, etc. diary entries. Similarly,we maintain a uniform distribution for documentlengths. Dataset details, such as the number of au-thors, diary entries, and Q/A pairs, are provided inAppendix B. Models. We benchmark the following suit of pub-licly available models: decoder-only OPT (125Mto 2.7B) (Zhang et al., 2022) and Pythia (70M to2.8B) (Biderman et al., 2023), and encoder-decoderFlan-T5 (80M to 3B) (Chung et al., 2022b). Acomparison of these architectures is provided inAppendix C. Training hyper-parameters are pro-vided in Appendix D. Unless specified otherwise,reported metrics are based on the test set.",
  "Results": "Effect of Architecture & Scale. We first evaluatethe impact of architecture, model size, and datasetsize on performance. We fine-tune each model onour datasets and report their performance as solidlines, labeled as standard setup in . Thehorizontal axis represents model size, the verticalaxis indicates the percentage of correctly answeredquestions, and the line color signifies the datasetsize. Each line on the plot corresponds to a specificarchitecture (e.g., OPT), ranging from the smallestto the largest model, trained on a particular datasetsize.For the OPT suite, we observe that performanceimproves with an increase in either model size ordataset size. Notably, as the dataset grows larger,the performance gap between different model sizesdiminishes. Specifically, the smallest OPT model(125M parameters) shows significant performance improvement with larger datasets, with no evidentsigns of saturation.Conversely, for Pythia models, merely scalingthe dataset size does not enhance the performanceof the two smallest models as effectively as withOPT. Rather, the architecture benefits most fromsimultaneously scaling both dataset and model size.Finally, the performance of Flan-T5 modelsshows minimal improvement as both dataset andmodel size increase. However, a notable excep-tion occurs with the largest model, which exhibitsa sudden spike in performance when trained onthe largest dataset. This behavior contrasts sharplywith the results observed for OPT models, indicat-ing that the capability being studied can emergeat different rates and under varying conditions de-pending on the architecture used.Notably, all models achieve near 100% accuracyon the Q/A pairs in the training set, as well as inmemorizing and recalling individual diary entries(not shown in any figure). Therefore, the observedperformance gap is not due to difficulty in memo-rizing the training data. Effect of Distributed Information. We comparethe model performance against a second set of mod-els trained in a simpler setup. Particularly, this sec-ond group of models is trained on identical datasets,but with all diary entries authored by the same in-dividual merged into a single training documentrather than each entry being its own document.This approach is equivalent to training the modelson the answers directly, requiring them to simplymemorize and recall single documents. The perfor-mance gap between these two setups highlights theadded difficulty of dealing with information spreadacross multiple training documents. This distribu-tion could affect how information is stored in themodels parameters, potentially making it harderfor the model to consolidate it when it is dispersed.In , the results of training within thismore straightforward setup are shown as dashedlines, labeled simplified setup. In all cases, thesemodels exhibit significantly improved performancecompared to the same base model trained withinthe distributed setup. Interestingly, all Flan-T5models achieve near-perfect accuracy in this sim-plified setup whereas OPT and Pythia suites do not,despite performing well and improving with scale.To better illustrate the performance gap betweenboth setups, we provide a clear visualization in. The vertical axis shows the accuracy",
  "Dataset Size": ": Gap in accuracy between the standard and simplified setup in , for a same sized model trained ona same sized dataset.The effect of scaling the dataset and model size varies greatly depending on the architecture. gap between the simplified and standard setup,for models of the same size, trained on datasetscontaining the same number of individuals. Resultsare grouped by model size, with colors denotingdataset size.For the OPT models, the gap narrows as thedataset size increases across all model sizes. In thecase of Pythia, the gap only seems to narrow forlarger models trained on sufficiently large datasets.Lastly, for Flan-T5, the performance gap barelyshrinks as both dataset and model size scale, withthe exception of the largest model trained on thelargest dataset. It remains unclear why Flan-T5 models performso well in the simpler setup but so poorly in thestandard setup. Given that the model has near per-fect accuracy in the prior, its poor performance inthe latter cannot be attributed to an issue in themethodology, as the process is the same in bothcases. The only difference is that, in the latter case,the model must recall information from multipledocuments rather than a single one. Therefore, themodel specifically has an issue with this aspect.For all models, it is uncertain whether their per-formance in both setups will continue to improvewith scale and if the gap will eventually disappear. Effect of Number of Documents. Next, we ex-plore how the number of documents to be consoli-dated and recalled impacts model performance. In, we report accuracy grouped by the num-ber of documents in the target answer (horizontalaxis). Line color indicates model size. For clarity,we only display the performance of models trainedon our largest dataset. Notably, there are no resultson the simpler setup in this and further analyses. Surprisingly, models do not demonstrate a de-cline in performance when more diary entries needto be recalled. Given the increased content to begenerated, one might expect a higher propensity forerrors in the model answers. This counterintuitiveobservation could be attributed to the controllednature of our experimental setup, where the modelknows it must recall between one and eight docu-ments. Real-world scenarios might yield differentresults and a significant increase in the number ofdocuments could potentially lead to performancedegradation, warranting further research.",
  "Documents to Recall": "OPTPythiaFlan-T5 : Number of documents recalled by each modelwhen the target is four. Results are averaged over modelsizes for simplicity, from models trained on our largestdataset. Full results are provided in Appendix E. To gain deeper insights into model behavior, weanalyze the number of documents recalled by themodels in comparison with the target number ofdocuments. shows this distribution whenthe target number of documents is four. Perfor-mance is averaged over model size for simplicity.Full results are available in Appendix E. We findthat, regardless of the target number of documentsto recall, Flan-T5 models tend to recall a randomnumber of documents. This contrasts with OPTmodels, which recall the expected number of en-tries across all scales. As for Pythia, the smallermodels struggle to recall the correct number of doc-uments; however, this capability seems to improveas the model size increases. Effect of Document Length.Previously, ourmethod for measuring accuracy involved count-ing the number of model answers that matched thetarget answer exactly. We now shift our focus toevaluating the accuracy of individual documentswithin a models answer, which we refer to as doc- ument accuracy.In this analysis, we only consider the documentsrecalled by the model that are also present in thetarget answer, regardless of whether these docu-ments are correct. Our objective is to examine howthe length of the target documents influences themodels ability to recall them accurately. Hence,we restrict our analysis to this specific subset ofdocuments, as we need a target for their length.For these selected documents, we count thosethat are free of errors and represent this rate onthe vertical axis of . The performance iscategorized by the target length on the vertical axis,and the line color indicates the size of the model.To ensure clarity, we present results exclusively forthe models trained on our largest dataset.Across all models, performance seems unaf-fected by document length, for which one might an-ticipate an increased likelihood of errors as the doc-ument grows longer. This consistent performancecould be attributed to the models expectation thatdocuments typically contain between one and eightsentences. Additionally, LLMs are known to bequite effective at memorizing documents. To as-certain whether increased document length wouldeventually degrade model performance, furtherscaling and testing with longer documents wouldbe necessary.Regarding model scale, only Pythia benefitsfrom increased size, once more with a jump inperformance past the first two model sizes.To further understand model behavior, we an-alyzed the number of recalled sentences, in com-parison with the target document length. The his-tograms in Appendix F illustrate these distributionsfor each model, with color indicating the modelsize. We observe that all models recall the correctnumber of sentences, and that scale only slightlyimproves performance. A qualitative analysis re- Sentences to Recall Document Accuracy (%) OPT Sentences to Recall",
  "Parameters": "Flan-T5 : Number of documents recalled by each model in comparison with the target. Color indicates model size.Results are from models trained on our largest dataset. OPT recalls the expected number of documents. Pythiastruggles at the smaller scale, but improves as model size increases. Flan-T5 seemingly recalls a random number ofdocuments at all scales.",
  "veals that while models correctly recall the numberand type of attributes, they often err in the attributevalues, such as recalling Time: Evening insteadof Time: Morning": "Investigating Performance Discrepancies. Ourresults indicate that the ability to consolidate andaccurately recall the correct number of documentsvaries depending on the model suite, but the under-lying reasons for this discrepancy remain unclear.At a high level, these differences in performancecould be due to several factors: architectural varia-tions, the effectiveness of pre-trained weights forfine-tuning on this task, the fine-tuning hyperpa-rameters, or a combination of these elements.To investigate this further, we fine-tuned an OPT-125M, a Pythia-70M, and a Flan-T5 Small model,all with randomly initialized weights, using ourlargest dataset. We then compare their performanceagainst the pre-trained models that were fine-tunedon the dataset of the same size.Our findings reveal that the Pythia model initial-ized with random weights significantly outperformthe pre-trained weights (). This suggeststhat architectural differences are not responsiblefor the poor performance of this model. Instead,the issue lies in the capability of the pre-trainedweights to be effectively fine-tuned for this specifictask. Regarding Flan-T5, fine-tuning with randomlyinitialized weights does not appear to enhance per-formance when compared to fine-tuning the pre-trained model. This observation suggests that themodels architecture is responsible for the observeddifferences in performance.Although fine-tuning hyperparameters couldalso be a factor, we conducted a thorough search.Additionally, models in the simpler setup per-formed well and were trained with identical hy-perparameters. Conversely, in the standard setup,while models were able to memorize the trainingsamples and Q/A examples, the solutions learnedby the pre-trained Pythia-70M and Flan-T5 Smalldoes not generalize well to the validation and testQ/A, unlike the OPT model.",
  "Comprehensive Analysis": "Reflecting on our experimental observations, wecan gain insights into the causes of certain modelsfailing on our benchmark. Weve observed that thedocuments recalled by the models are typically ofthe correct length (Appendix F) and error-free (Fig-ure 6). Additionally, models trained under the sim-plified setup successfully recall information from asingle training document (). Therefore, theissue appears not to lie in the content of the recalleddocuments but rather in the quantity of documentsbeing recalled.Indeed, some models seem incapable of recallingthe correct number of documents, instead recall-ing a random number of documents (Appendix E).These models include the two smallest Pythia vari-ants and all Flan-T5 models, which correspond-ingly perform poorly on our benchmark.Interestingly, the smallest Pythia model performswell if fine-tuned starting from random weights rather than the pre-trained weights (), sug-gesting that the poor performance of the pre-trainedweights cannot be attributed to an architecturalreason. Instead, the issue appears to be with thepre-training weights failing to learn a solution thatgeneralizes to the problem of recalling the correctnumber of documents, rather than merely memo-rizing the training samples. Why this discrepancyoccurs, particularly in contrast to the larger pre-trained Pythia models, remains unclear and war-rants further research. Different hyperparameterscould potentially enable the smaller models to gen-eralize well to our problem, but it is uncertain ifthis can be achieved without severely degrading thelanguage modeling capabilities of the pre-trainedmodel.Regarding Flan-T5, given that the smallestmodel fine-tuned from scratch performs as poorlyas the one fine-tuned from pre-trained weights, theroot cause of the poor performance could be eitherarchitectural or due to improper hyperparameters.Additionally, the size of the model appears to in-fluence its performance. Since Flan-T5 follows anencoder-decoder architecture, unlike the decoder-only structures of models such as OPT and Pythia,its parameters are divided roughly equally betweenthe encoder and decoder. Consequently, the largestFlan-T5 models decoder is comparable in size tothat of the third smallest Pythia model, which coin-cides with the point where performance begins toimprove for Pythia (as seen in ). Modelswithin the Pythia suite smaller than this thresholddo not show significant performance gains. How-ever, the smallest Pythia model, when trained fromscratch, outperforms Flan-T5 under similar condi-tions. This highlights the role of scale in modelperformance, suggesting that both architectural andhyperparameter factors could hinder the emergenceof capabilities at smaller scales. Further researchwill be necessary to pinpoint the exact cause andclarify the challenges faced by these smaller mod-els.",
  "One aspect not addressed in this study is whethermodels can perform the given task while retaining": "their language modeling capabilities. Due to thesize of the models examined, repetitive fine-tuningon the training documents is necessary for themto memorize the data, which leads to overfittingon the task. Ideally, experiments would need to beconducted on much larger models, incorporatingthe training documents into the pre-training corpus,followed by standard instruction tuning. One ofthe tasks in this tuning would involve recalling alldocuments related to a given topic. This approachwould help determine if a model can accomplishthis in a manner that is useful for solving problems.Unfortunately, we currently lack the computationalresources to conduct such experiments, and hencewe leave this for future work.",
  "Information Distribution": "We observed a notable performance gap betweenthe standard and simplified setups, supporting thefindings by Prato et al. (2023). Their research indi-cates that LLMs more easily recall multiple piecesof information when this information is containedin a single training sample rather than dispersedacross multiple samples.This raises questionsabout how the distribution of topic-related infor-mation across multiple training documents affectsan LLMs ability to gauge its knowledge. Particu-larly, the impact of this distribution on the internalmechanisms of the LLM is not well understood. Numerous studies have shown that languagemodels can memorize entire passages and docu-ments within their weights, enabling them to recallthis information during inference (Carlini et al.,2020, 2022; Tirumala et al., 2022; Biderman et al.,2023; de Wynter et al., 2023; Chen et al., 2024).Consequently, the strong performance of models inthe simpler setup, where they only need to recallinformation from a single document per topic, isnot surprising. However, it remains unclear why recalling infor-mation from multiple documents presents a greaterchallenge. Specifically, how is this information en-coded within the model parameters (Wallat et al.,2020; Dai et al., 2022; Meng et al., 2022) and howdoes dispersed information affect the recalling pro-cess? Understanding these mechanisms is crucialfor improving the performance of language models,as many real-world problems necessitate recallinginformation from multiple training documents.",
  "Knowledge Awareness & Understanding": "While we have demonstrated that some LLMs pos-sess an awareness of the extent of their knowledgeconcerning the topics in our benchmark, this doesnot necessarily mean that these models can gaugetheir knowledge across any topic.Determining whether LLMs can accurately as-sess the scope of their understanding of topics fromtheir pre-training corpus requires further investiga-tion. Topics in practice could cover a wide array ofsubjects, including individuals, locations, events,and concepts. However, we believe that the specifictype of topic is likely not an influential factor.The critical element, in our view, is the breadthof these topics, which relates to the amount of infor-mation relevant for each. Our findings did not showa decline in model performance when recalling alarger number of documents. Nevertheless, this ob-servation might change if the number of documentswere significantly increased. Further research isnecessary to explore the limits and capabilities ofmodels in handling broader topics.A more profound question is the extent to whichLLMs understand the scope of their entire knowl-edge base, or at least subsets of it. Given the vastamount of information LLMs learn during training,comprehending the scope of this knowledge or itssubsets seems incredibly challenging. Yet, it wouldbe beneficial for a model to understand the extentof its own expertise.Finally, it is important to note that understand-ing the scope of ones knowledge concerning atopic does not imply an understanding of that topicitself. Whether LLMs truly comprehend the knowl-edge they have memorized is a different researchquestion from ours and is an active area of investi-gation (Bender et al., 2021; Li et al., 2022; Gurneeand Tegmark, 2023).",
  "Conclusion": "This study focused on determining whether LLMspossess an understanding of the span of their ownknowledge on specific topics. Notably, we ob-served that all models, if scaled sufficiently, knowhow many documents are authored by the same per-son. Consequently, these LLMs know how muchthey know about these individuals; otherwise, theywould sporadically recall too few or too many doc-uments.More specifically, we find that this capabilityemerges based on the models architecture, its size, the datasets size used for training, and the effec-tiveness of the pre-trained weights in learning asolution that generalizes, rather than simply memo-rizing the training samples.Overall, our research contributes to a deeper un-derstanding of the capabilities and inner workingsof LLMs. Grasping how aware LLMs are of theirown knowledge and identifying any limitations inthis regard is crucial, as this feature enhances theusefulness and trustworthiness of intelligent sys-tems. Further research is necessary to continueexploring this aspect.",
  "Ethical Considerations": "This research utilizes large language models trainedon extensive textual datasets. While such modelshave demonstrated exceptional ability in genera-tion, it is critical to highlight the ethical consider-ations that the data used for training these modelsinherently contains human biases. These, in turn,can manifest in the models outputs. As such, itis essential when deploying such models, to crit-ically evaluate their outputs, keeping in mind thelikelihood of underlying bias.",
  "Amos Azaria and Tom Mitchell. 2023. The InternalState of an LLM Knows When Its Lying. arXive-prints, page arXiv:2304.13734": "Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On thedangers of stochastic parrots: Can language modelsbe too big? Proceedings of the 2021 ACM Confer-ence on Fairness, Accountability, and Transparency. Lukas Berglund, Asa Cooper Stickland, Mikita Balesni,Max Kaufmann, Meg Tong, Tomasz Korbak, DanielKokotajlo, and Owain Evans. 2023a.Taken outof context: On measuring situational awareness inLLMs. arXiv e-prints, page arXiv:2309.00667. Lukas Berglund, Meg Tong, Max Kaufmann, MikitaBalesni, Asa Cooper Stickland, Tomasz Korbak, andOwain Evans. 2023b. The Reversal Curse: LLMstrained on A is B fail to learn B is A. arXive-prints, page arXiv:2309.12288. Stella Biderman, USVSN PRASHANTH, LintangSutawika, Hailey Schoelkopf, Quentin Anthony,Shivanshu Purohit, and Edward Raff. 2023. Emer-gent and predictable memorization in large languagemodels. In Advances in Neural Information Process-ing Systems, volume 36, pages 2807228090. CurranAssociates, Inc. Stella Biderman, Hailey Schoelkopf, Quentin Anthony,Herbie Bradley, Kyle OBrien, Eric Hallahan, Mo-hammad Aflah Khan, Shivanshu Purohit, USVSNSai Prashanth, Edward Raff, Aviya Skowron, Lin-tang Sutawika, and Oskar van der Wal. 2023. Pythia:A Suite for Analyzing Large Language ModelsAcross Training and Scaling. arXiv e-prints, pagearXiv:2304.01373.",
  "Vanessa Buhrmester, David Mnch, and Michael Arens.2021. Analysis of explainers of black box deep neu-ral networks for computer vision: A survey. MachineLearning and Knowledge Extraction, 3(4):966989": "Nicholas Carlini, Daphne Ippolito, Matthew Jagiel-ski, Katherine Lee, Florian Tramer, and ChiyuanZhang. 2022.Quantifying Memorization AcrossNeural Language Models.arXiv e-prints, pagearXiv:2202.07646. Nicholas Carlini,Florian Tramer,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, UlfarErlingsson, Alina Oprea, and Colin Raffel. 2020. Ex-tracting Training Data from Large Language Models.arXiv e-prints, page arXiv:2012.07805.",
  "Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, Gretchen": "Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. EvaluatingLarge Language Models Trained on Code. arXive-prints, page arXiv:2107.03374. Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao,He He, Jacob Steinhardt, Zhou Yu, and Kath-leen McKeown. 2023. Do Models Explain Them-selves?Counterfactual Simulatability of Natu-ral Language Explanations.arXiv e-prints, pagearXiv:2307.08678. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowd-hery, Alex Castro-Ros, Marie Pellat, Kevin Robin-son, Dasha Valter, Sharan Narang, Gaurav Mishra,Adams Yu, Vincent Zhao, Yanping Huang, AndrewDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V.Le, and Jason Wei. 2022a.Scaling Instruction-Finetuned Language Models. arXiv e-prints, pagearXiv:2210.11416. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowd-hery, Alex Castro-Ros, Marie Pellat, Kevin Robin-son, Dasha Valter, Sharan Narang, Gaurav Mishra,Adams Yu, Vincent Zhao, Yanping Huang, AndrewDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V.Le, and Jason Wei. 2022b.Scaling Instruction-Finetuned Language Models. arXiv e-prints, pagearXiv:2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training Verifiers to Solve Math Word Prob-lems. arXiv e-prints, page arXiv:2110.14168.",
  "Zhengbao Jiang, Frank F. Xu, Jun Araki, and GrahamNeubig. 2020. How Can We Know What LanguageModels Know? Transactions of the Association forComputational Linguistics, 8:423438": "Saurav Kadavath, Tom Conerly, Amanda Askell, TomHenighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, EliTran-Johnson, Scott Johnston, Sheer El-Showk,Andy Jones, Nelson Elhage, Tristan Hume, AnnaChen, Yuntao Bai, Sam Bowman, Stanislav Fort,Deep Ganguli, Danny Hernandez, Josh Jacobson,Jackson Kernion, Shauna Kravec, Liane Lovitt, Ka-mal Ndousse, Catherine Olsson, Sam Ringer, DarioAmodei, Tom Brown, Jack Clark, Nicholas Joseph,Ben Mann, Sam McCandlish, Chris Olah, andJared Kaplan. 2022.Language Models (Mostly)Know What They Know.arXiv e-prints, pagearXiv:2207.05221.",
  "Fabio Petroni, Tim Rocktschel, Patrick Lewis, An-ton Bakhtin, Yuxiang Wu, Alexander H. Miller, andSebastian Riedel. 2019. Language Models as Knowl-edge Bases? arXiv e-prints, page arXiv:1909.01066": "Ian Porada, Alessandro Sordoni, and Jackie Chi KitCheung. 2021. Does Pre-training Induce SystematicInference? How Masked Language Models AcquireCommonsense Knowledge.arXiv e-prints, pagearXiv:2112.08583. Gabriele Prato, Jerry Huang, Prasanna Parthasarathi,Shagun Sodhani, and Sarath Chandar. 2023. EpiK-eval: Evaluation for language models as epistemicmodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 95239557, Singapore. Association for Com-putational Linguistics.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. 2019. Exploring the Lim-its of Transfer Learning with a Unified Text-to-TextTransformer. arXiv e-prints, page arXiv:1910.10683. David Rein, Betty Li Hou, Asa Cooper Stickland, Jack-son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-lian Michael, and Samuel R. Bowman. 2023. GPQA:A Graduate-Level Google-Proof Q&A Benchmark.arXiv e-prints, page arXiv:2311.12022.",
  "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020": "How much knowledge can you pack into the param-eters of a language model? In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 54185426,Online. Association for Computational Linguistics. Agus Sudjianto, William Knauth, Rahul Singh, ZebinYang, and Aijun Zhang. 2020.Unwrapping TheBlack Box of Deep ReLU Networks: Interpretability,Diagnostics, and Simplification. arXiv e-prints, pagearXiv:2011.04041. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer,and Armen Aghajanyan. 2022. Memorization with-out overfitting: Analyzing the training dynamics oflarge language models. In Advances in Neural Infor-mation Processing Systems, volume 35, pages 3827438290. Curran Associates, Inc. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schel-ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-ramanian, Xiaoqing Ellen Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023. Llama 2: Open Foundationand Fine-Tuned Chat Models. arXiv e-prints, pagearXiv:2307.09288.",
  "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,Xipeng Qiu, and Xuanjing Huang. 2023. Do LargeLanguage Models Know What They Dont Know?arXiv e-prints, page arXiv:2305.18153": "Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-haylov, Myle Ott, Sam Shleifer, Kurt Shuster, DanielSimig, Punit Singh Koura, Anjali Sridhar, TianluWang, and Luke Zettlemoyer. 2022.OPT: OpenPre-trained Transformer Language Models. arXive-prints, page arXiv:2205.01068. Yukun Zhao, Lingyong Yan, Weiwei Sun, GuoliangXing, Chong Meng, Shuaiqiang Wang, ZhicongCheng, Zhaochun Ren, and Dawei Yin. 2023. Know-ing What LLMs DO NOT Know: A Simple Yet Ef-fective Self-Detection Method. arXiv e-prints, pagearXiv:2310.17918.",
  "A.1Knowledge Awareness": "Large language models are widely recognized formemorizing a substantial amount of informationduring their training (Petroni et al., 2019; Robertset al., 2020; Jiang et al., 2020; Carlini et al., 2022;AlKhamissi et al., 2022; He et al., 2024). However,it remains unclear to what extent these models un-derstand their own knowledge. Research to datehas shown that LLMs can assess, with some degreeof accuracy, whether they know the answer to agiven question (Kadavath et al., 2022; Zhao et al.,2023; Amayuelas et al., 2023; Yin et al., 2023;Liang et al., 2024).While these studies primarily evaluate themodels ability to determine if it possesses theknowledge necessary to answer a question, theydo not consider the quantity and source of thisknowledge. For instance, the question Is Jupitera planet? requires knowledge of a single fact,whereas Do you know all papers related to topicX? necessitates understanding multiple pieces ofinformation, likely derived from various trainingsamples.In essence, locating a specific piece of informa-tion within a models parameter space is differentfrom retrieving multiple pieces of information andrecognizing when the search is complete. Our re-search focuses on this latter aspect, seeking to de-termine whether LLMs comprehend the extent oftheir knowledge on specific topics.",
  "A.2Implicit Knowledge Retrieval": "At the heart of our methodology lies implicit knowl-edge retrieval. This involves prompting a modelwith a question, enabling it to retrieve knowledgestored within its parameters, and subsequently gen-erating an answer based on the retrieved informa-tion (Vinyals and Le, 2015; Chung et al., 2022a;Geva et al., 2023). Considering the black box na-ture of deep neural networks (Alain and Bengio,2016; Sudjianto et al., 2020; Buhrmester et al.,2021; Liang et al., 2021), this setup is frequentlyemployed to deduce the inner workings and capabil-ities of such models (Porada et al., 2021; Berglundet al., 2023b; Allen-Zhu and Li, 2023a,b; Berglundet al., 2023a; Madsen et al., 2024), offering valu-able insights into the knowledge and skills themodel has acquired (Hendrycks et al., 2020; Chenet al., 2021; Cobbe et al., 2021; Hendrycks et al.,2021; Rein et al., 2023). Hence we consider it to be",
  "B.1Size": "The details of each dataset used in our experimentsare provided in . The training set of eachdataset consists of all of the documents (diary en-tries), as well as the train Q/A pairs. The validationand test sets solely consist of Q/A pairs. There areno overlaps between the Q/A pairs in the training,validation and test sets. As previously mentioned,for each author, we generate 1 to 8 diary entries,where each entry contains 1 to 8 sentences (exclud-ing the title).",
  "CModel Suite Differences": "The following outlines the architectural differencesbetween the OPT, Pythia, and Flan-T5 suite of mod-els, emphasizing their unique characteristics andtraining details.Both OPT and Pythia are based on the GPT-3architecture, with only slight variations. OPT em-ploys learned positional embeddings and utilizesthe ReLU activation function (Nair and Hinton,2010). In contrast, Pythia incorporates rotary posi-tional embeddings and the GELU activation func-tion (Hendrycks and Gimpel, 2016). A notable dis-tinction in Pythias architecture is its use of parallelresidual connections, where the self-attention andfeed-forward blocks run concurrently, and their out-puts are summed along with the residual. This dif-fers from OPTs sequential arrangement, where theself-attention block is followed by the feed-forwardblock. Additionally, Pythia forgoes the applica-tion of dropout after the attention and feed-forwardblocks, unlike OPT, which applies a dropout rateof 0.1.Turning to Flan-T5, this model remains largelyfaithful to the original Transformer architec-ture (Vaswani et al., 2017), with a few key excep-tions. Layer normalization (Lei Ba et al., 2016)in Flan-T5 is applied before the residual, self-attention, and feed-forward blocks. In contrast,",
  "OPT and Pythia place the residual connections be-fore the layer normalization. Flan-T5 also does notinclude a bias term in the layer normalization andadopts relative positional embeddings": "Regarding pre-training, OPT is trained on ThePile (Gao et al., 2020) along with other datasets,whereas Pythia is exclusively trained on The Pile.Flan-T5 is a fine-tuned version of T5 (Raffel et al.,2019), with both models being trained on a mixof datasets. For more detailed information on thepre-training specifics and hyperparameters, readersare encouraged to refer to the respective papers foreach model (Zhang et al., 2022; Biderman et al.,2023; Chung et al., 2022b).",
  "DTraining Details": "We train our models until they converge by employ-ing the Adam optimizer (Kingma and Ba, 2014),which is configured with beta values of 0.9 and0.999, and an epsilon of 1e-8. No weight decay isapplied in this process. The learning rate is initiallyset to zero and then linearly increased to reach themodel-specific rate detailed in over thecourse of 3,600 steps. After this warm-up period,the learning rate is maintained constant. We set thebatch size to 32."
}