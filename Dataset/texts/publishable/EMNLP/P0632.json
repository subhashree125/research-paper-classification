{
  "Abstract": "Large Language Models (LLMs) are typicallyshipped with tokenizers that deterministicallyencode text into so-called canonical token se-quences, to which the LLMs assign probabil-ity values. One common assumption is thatthe probability of a piece of text is the prob-ability of its canonical token sequence. How-ever, the tokenization of a string is not unique:e.g., the Llama2 tokenizer encodes Tokens as[Tok,ens], but [Tok,en,s] also representsthe same text. In this paper, we study non-canonical tokenizations. We prove that, givena string, it is computationally hard to find themost likely tokenization for an autoregressiveLLM, as well as to compute the marginal prob-ability over all possible tokenizations. We thenshow how the marginal is, in most cases, in-distinguishable from the canonical probability.Surprisingly, we then empirically demonstratethe existence of a significant amount of sig-nal hidden within tokenization space. Notably,by simply aggregating the probabilities of non-canonical tokenizations, we achieve improve-ments across a range of LLM evaluation bench-marks for a variety of architectures, includingtransformers and state space models.",
  "Introduction": "Autoregressive large language models (LLMs) gen-erate text by predicting the next word sequentially.A crucial yet often overlooked step in this pro-cess is tokenization, whereby each word is bro-ken down into subwords. It allows the model togenerate text beyond what it was trained on, en-abling open-vocabulary generation. However, thisapproach also introduces a significant challenge: agiven string can be tokenized in exponentially manyways (). For example, this papers abstractcan be tokenized in more than 10267 ways underthe Llama2 vocabulary (Touvron et al., 2023).While a given string can be tokenized in multipleways, at inference time, almost all successful mod-",
  ": Exponential growth of the number of tok-enizations. The (log-scale) y-axis shows the number oftokenizations as a function of the sentence length": "ern LLMs utilize a fixed, or canonical, tokeniza-tion: a deterministic, rule-based mapping from textto token sequences (Gage, 1994). Consequently,it has become commonplace to use this token se-quence as a proxy for the underlying text. In partic-ular, the probability of the token sequence is oftenused in place of the probability of the text (e.g. forevaluation metrics like perplexity), even thoughthese quantities are not necessarily equal. To com-plicate matters, some language models are pre-trained with stochastic tokenizations (Kudo, 2018;Provilkov et al., 2020), exposing them to multi-ple ways of tokenizing the same string, with thehope of obtaining models with a more developedunderstanding of the compositionality of words.In this paper, in the context of modern LLMs,we ask whether non-canonical tokenizations of astring can provide additional signal at inferencetime, which would be lost by considering just thecanonical tokenization.To this end, we inves-tigate two natural alternatives: finding the mostlikely tokenization and marginalizing over tokeniza- tions. For example, one natural way to answer amultiple-choice question is to choose the answerwith the highest probability conditioned on thequestion (Zellers et al., 2019); instead of alwaysassuming the canonical tokenizations of the an-swers, one could compare answers based on theprobability of their most likely tokenizations, or al-ternatively the marginal probability of all possibletokenizations.We first study the problem of finding the mostlikely tokenization and show that it is NP-hard un-der some mild assumptions. As such, we proposean anytime branch-and-bound algorithm to approx-imate the most likely tokenization. We find that, fortext lengths where the branch-and-bound strategyis practical, the canonical tokenization is usuallythe most likely one.Then we ask the question of whether thereis a significant amount of probability mass con-centrated on tokenizations other than the canon-ical. We first observe that as we sample tokensequences of varying length from the LLM distri-bution unconditionally, the proportion of canoni-cal tokenizations decreases significantly as the se-quence length increases. To further investigate thisphenomenon, ideally one would need to computethe marginal probability of all tokenizations for agiven string, which we show to be #P-hard. Hence,we implement an importance sampling estimatorfor the marginal probability. Surprisingly, despitethe extremely large number of non-canonical tok-enizations, we empirically find that the estimatedmarginal probability is usually very close to thecanonical tokenizations probability.This raises our last question: does the completetokenization space add any meaningful signal at all,in addition to the canonical tokenization alone? Re-markably, we show that, even for the cases wherethere is little probability mass on non-canonicaltokenizations, they seem to carry some meaningfulsignal. Specifically, we show that for Gemma-2B(Gemma Team et al., 2024), Llama2-7B (Touvronet al., 2023) and Mamba-130M (Gu and Dao, 2024),by employing ensemble strategies for weighting dif-ferent tokenizations at inference time, we achievesignificant performance improvements on challeng-ing LLM evaluation benchmarks.",
  "Contributions.In summary, we show that:(i) while it is tempting to consider computing themarginal probability of a string, this quantity is#P-hard to compute; (ii) in fact, even computing": "the probability of the most likely tokenization isNP-hard; and (iii) while in most cases the marginalprobability of a string is practically the same asthe canonical probability, non-canonical tokeniza-tions seem to provide some signal to downstreamtasks, to the point that we achieve consistent im-provement across a range of open source modelson Q&A datasets.",
  "Related Work": "Many previous works have explored tokenizationstrategies within the LLM pipeline, and the (oftenundesirable) inductive biases they may introduce:for example, in introducing unfairness between lan-guages (Petrov et al., 2023), gender bias (Ovalleet al., 2024), and in performing arithmetic (Singhand Strouse, 2024).Some recent works haveavoided the many downsides of tokenization by em-ploying byte-level models, but either suffer fromslow decoding due to longer sequences (Yu et al.,2023), or rely on token-level models for more effi-cient generation (Wang et al., 2024). To overcomethe limitations of tokenization, prior works haveexamined (approximately) marginalizing over thedistribution of possible token sequences (Buckmanand Neubig, 2018; Cao and Rimell, 2021; Chirkovaet al., 2023). In this work, we analyze modernLLMs and consider multiple strategies for extract-ing information from tokenization space; findingthat, contrary to prior belief, the signal is presentnot in the most-likely tokenization or (approxi-mated) marginals, but rather in a mixture of canon-ical and non-canonical tokenizations.",
  "An LLM Induces a Distribution overTokenizations": "Let x = (x1, x2, . . .) denote a string (a sequence ofcharacters). A vocabulary V is a set of strings thatrepresent subwords, or tokens. A token sequencew.r.t. a vocabulary V is a sequence v = (v1, v2, . . . )where each vi V. A tokenization of string x w.r.t.a vocabulary V is a token sequence w.r.t. V suchthat the concatenation of all tokens is equal to x.Simply put, a tokenization breaks down a stringinto substrings, each recognized by the vocabu-lary. The substrings are ordered by their positionin the original string. We write v |=V x to denotethat token sequence v is a tokenization of stringx w.r.t. the vocabulary V, sometimes omitting Vwhen meaning is clear.An autoregressive LLM p defines a conditional",
  "|v|i=1 p (vi|v1, . . . , vi1) if v |=V x,0otherwise": "Most modern LLMs make use of tokenizersbased on Byte-Pair Encoding (BPE) (Gage, 1994),whereby the token vocabulary is initialized withthe character vocabulary, and a merge table is ini-tialized to be empty. The method then iterativelycounts all pairs of tokens and merges the most fre-quent pair into a new token. This new token isadded to the vocabulary and the merge rule is addedto the merge table. This process is repeated untilthe desired vocabulary size is reached. The result-ing merge table specifies which tokens are to bemerged into larger tokens, as well as the priorityof these merges. In this way, it defines a canonicaltokenization procedure as follows: first, a string issplit into its constituent characters, then, the pairof adjacent tokens with the highest priority mergerule is combined. This is repeated until no furthermerge rules from the table are applicable.BPE dropout (Provilkov et al., 2020) introducesan additional step during training: when tokenizinga word, merge rules are dropped with some prob-ability. After this dropout phase, merges proceedthe same way as BPE. This dropout phase acts asa regularization method that provides robustnessto input noise. It also means that language modelstrained with BPE dropout should assign more massto non-canonical tokenizations.",
  ": return N": "At inference time, for a string x, the tokenizeroutputs the canonical tokenization v (without anydropout), which is then evaluated by the LLM toget the canonical probability p(v, x). Note thatthis probability is one of an exponential numberof tokenization probabilities for a particular string.In fact, one can compile a Multi-valued DecisionDiagram (MDD) (Lee, 1959) that represents thiscombinatorial space for a given string tractably bydecomposing and reusing subsequences. This datastructure allows one to compute the total numberof tokenizations in linear time in the number ofedges of the diagram. Algorithm 1 shows how tocompile an MDD from a string and showsan example of an MDD compiled from the stringBird. Each node in the diagram corresponds toa position in the string, and edges from node i tonode j are labelled with the corresponding tokenxi:j = (xi, xi+1, . . . , xj). Every path going fromthe root to a terminal node is a tokenization of x.Given what we know so far, a question naturallyarises: since we can tractably represent all tok-enizations as an MDD, and given that the numberof possibilities is exponential, can we efficientlycompute the most likely tokenization of a string?And perhaps more interestingly, is it the canonicalone, as often is assumed in practice?",
  "Computing the Most LikelyTokenization is Hard": "We begin by noting that there exist simple distri-butions where finding the most likely tokenizationcan be done efficiently. For example, if we anno-tate the edges in an MDD with probabilities, thatgives us a tokenization distribution where the mostlikely tokenization is simply the MDD path withthe highest probability. By carefully modifying",
  ": Run-time for branch-and-bound over to-kenizations. The search grows exponentially with thenumber of characters in the string": "the MDD, it even becomes possible to efficientlycompute the most likely tokenization induced bya bi-gram distribution, where each token dependsonly on the previous one (Dupont and Rosenfeld,1997; Choi et al., 2020).For more complex autoregressive language mod-els, however, we unfortunately show that comput-ing the most likely tokenization is computationallyhard. We formalize this as follows.",
  "Theorem 4.2. The most-likely tokenization prob-lem is NP-complete": "Proof. (Sketch) The proof is by reduction from the3-SAT Boolean satisfiability problem (Karp, 2010).We encode the Boolean variables as possible to-kenizations of substrings such that there is a cor-respondence between the probability of the mostlikely tokenization and the existence of a satisfiyingassignment. The full proof is in Appendix B. Given the LLM training regime, a reasonable as-sumption in practice is that the canonical tokeniza-tion is (close to) the most likely tokenization. Toempirically verify this claim, we devise a branch-and-bound algorithm to search through the MDD inorder to find some tokenization whose probabilityis higher than the canonical. We do so by settingthe initial lower bound as the canonical probability,",
  ": Distribution of tokenizations for the wordTokens. An overwhelming probability mass is on thecanonical tokenization, with (an exponential number of)others sharing a miniscule portion of probability": "and then pruning paths whose partial probability isbelow this bound. We set a time budget of 1-hour,after which the search returns the best tokenizationat that point. As expected, we find that branch-and-bound is quickly overwhelmed by the numberof tokenizations as the string length grows. Find-ing the most likely tokenization this way rapidlybecomes intractable for longer strings. shows the branch-and-bound searchtime across three LLM architectures for the stringLanguage models typically tokenize text into sub-words, utilizing a learned deterministic set of mergerules to aggregate tokens. We gradually insert newwords and re-run search to visualize its scalabil-ity. Branch-and-bound always returns the canoni-cal tokenization as the best candidate, despite theexponential number of possible candidates.Not only does the canonical tokenization seemto be the most likely one for shorter text, but italso often is overwhelmingly so. showsthe tokenization distribution for the word Tokensunder the Llama2 model; there are 52 tokenizations,with the canonical taking most of the mass.Even though canonical seems to take up a ma-jority of the probability mass in these cases, ifwe look at generated text from these LLMs, asizable percentage of the (unconditionally) gen-erated tokenizations are non-canonical. shows canonicity as a function of the number oftokens generated by the language model. As gen-erated text grows larger, the probability of gen-erating non-canonical tokenizations also grows.This is surprising, as it is seemingly in contra-",
  ": Canonicity in generated text. Percentage ofcanonicity drops as more tokens are generated": "diction to earlier evidence. It turns out that, ifwe investigate these non-canonical generated se-quences in more depth, we find that a large ma-jority of such cases are non-English, with a largeportion consisting of code and languages that uti-lize unicode characters. It is nevertheless inter-esting that some non-canonical tokenizations areindeed more likely than their canonical counter-parts, with some even in grammatically correct En-glish. For example, the string x = tongueless(with denoting a whitespace) is canonically tok-enized as v = [tong,uel,ess] by Gemma, withp(v|x) 0.474; however v = [tongue,less]is a more likely tokenization according to theLLM, with p(v|x) 0.518. This gap betweenthe probability of the most likely tokenizationand canonical tokenization can be even more ex-treme. For instance, v = [Hyp,no,patu,rist]is a much more likely tokenization p(v|x) =0.9948 compared to the canonical tokenizationv = [Hyp,nop,atu,rist], with the latter tak-ing only p(v|x) = 0.0004 of the total mass.This seems to suggest that there is some massbeing attributed to non-canonical tokenizations, es-pecially over longer text. We thus raise anotherquestion: instead of using a single tokenization,could we aggregate over all tokenizations, eachweighted by their probability, effectively comput-ing the marginal probability of a given string?",
  "Theorem 5.2. The marginal string probabilityproblem is #P-hard": "Proof. (Sketch) The proof is by reduction fromthe counting version of the 3-SAT Boolean satis-fiability problem (#3-SAT), which is known to be#P-complete. We encode the Boolean variablesas possible tokens in a string, such that there isa correspondence between the number of satisfy-ing assignments of the Boolean formula and themarginal probability of the string under the LLM.The full proof can be found in Appendix B.",
  "Marginal Probability Estimation": "In light of the above hardness results, we now shiftour attention to approximating the marginal stringprobability. In particular, we will focus on esti-mators based on sequential importance sampling(Kloek and van Dijk, 1978; Geweke, 1989). In thisinstance of importance sampling, we sample tok-enizations v given a string x according to some pro-posal distribution q(v|x). Given a set of samplesv(1), . . . , v(N) from this distribution, an estimateof the marginal string probability p(x) is",
  "j=1p(vj|v1:j1),": "where p(vj|v1:j1) is the LLM next-token distri-bution.However, estimating p(x) this way re-quires rejecting all sampled token sequences wherev |= x, making the approach infeasible in practice.To address this issue, we use a modified pro-posal distribution: the 1-step look-ahead proposaldistribution, first proposed in Chirkova et al. (2023).This distribution adjusts the LLMs next-token dis-tribution at each step by checking whether the",
  "(b) Log probability difference between approximate marginal and canonical probability": ": Convergence of approximate marginal. (a) the approximate marginal string probability as a function ofthe number of samples (), compared to the canonical probability () and the true marginal (). (b) averageabsolute difference in log-likelihood between the approximate marginal and the canonical probability for differentstrings across Gemma, Llama2 and Mamba in color, with individual examples in gray (). upcoming token vj, combined with the previoustokens, forms a tokenization of a prefix of thestring x. Intuitively, we iteratively prune awayfrom the support of the distribution tokenizationsnot consistent with x. This can be done efficientlyby simply traversing the MDD compiled from thestring and masking out all tokens that are not com-patible with the labels of the outgoing edges at thecurrent node. Formally, the proposal distribution is",
  "qLA(vj|v1:j1, x) p(vj|v1:j1)vv1:j |= x1:w": "Here, vv1:j |= x1:w evaluates to 1 if v1:j forms atokenization of a prefix of string x, and 0 otherwise.Essentially, the proposal distribution is a greedyand myopic approximation of the LLM distributionover tokenizations at every step of the sequence.Interestingly, we observe that, for short stringswhere we are able to compute the true marginal,even though the proposal eventually converges tothe true marginal as the number of samples in-creases, the probability of the canonical tokeniza-tion is just as close to the true marginal. This seemsto suggest that the canonical probability is, in fact,practically the marginal probability of the stringin these cases. a shows one instance ofthe approximate marginal slowly converging to thetrue marginal as the number of samples increasesfor a single small example of the OPENBOOKQAdataset (Mihaylov et al., 2018).For longer text, we observe that, for most cases,the approximate marginal also converges close tothe canonical probability. As the number of tok-enizations to be summed out is enormous, we areunable to compute the true marginal. In none of the cases we evaluated, the approximate marginal prob-ability was meaningfully higher than the canonical.b shows the difference in log-probabilityof several marginal estimates across different archi-tectures and OPENBOOKQA strings. Notably, esti-mates that were very different from the canonicalprobability contained no canonical samples, furtherconfirming that most of the probability mass is inthe canonical tokenization.So far, we have presented empirical evidencethat seems to confirm that: (1) canonical is, inmost cases, the most likely tokenization, and (2)it carries so much of the probability mass that itis practically the marginal itself. Curiously, in anarguably contradictory twist, we experimentallyshow evidence that suggests that there exists somesignal in non-canonical tokenizations to the pointwhere we are able to achieve consistently betterdownstream performance in Q&A tasks.",
  "Non-Canonical Tokenizations inQuestion Answering": "In multiple-choice question answering, a modelis given a question (possibly with context) and isasked to choose between a number of different an-swers to the question. Typically, this is performedby evaluating the probability of each answer underthe default canonical tokenization, and selectingthe answer with the highest probability. Formally,given a question c with canonical tokenization vcand set of K answers {ai}Ki=1 with canonical tok-enizations {vai}Ki=1, the classification is given by",
  "vai|=aip(vai|vc)": "From prior discussion, we expect the approx-imate marginal to gradually converge to canon-ical. However, we empirically find that, beforeconvergence, there is a surprising increase in ac-curacy when weighting over non-canonical tok-enizations compared to the canonical baseline. Fig-ure 7 shows accuracy for the marginal approxi-mation as a function of the number of samples inthree different question answering datasets: HEL- LASWAG (Zellers et al., 2019), SOCIALIQA (Sapet al., 2019) and OPENBOOKQA (Mihaylov et al.,2018). Due to computational constraints, we onlyevaluate on randomly sampled subsets of 1000 ex-amples for each dataset.By parameter tuning the number of samples,we are able to achieve a consistent performanceincrease in accuracy, as shows. Tuningthe number of samples consisted of sampling 256samples from a 1000 examples validation hold-out subset, computing the accuracy for 256 trialsof 256-choose-k samples, and taking the k whichmaximizes average accuracy on the hold-out sub-set. This k is then used to sample that number of tokenizations in the test set. The precise values ofk are shown in in appendix.To further understand how much non-canonicaltokenizations play a role in this improvement, andfind out where the signal in tokenization comesfrom, we construct a mixture of canonical and non-canonical tokenizations. We evaluate the improve-ment in accuracy, effectively measuring how muchnon-canonicity plays a role in the downstream task.More formally, we compute",
  "(1 ) p(vai|vc, va1 va2 . . . vak),": "where 0 1 and we use vai to denote theset of all non-canonical tokenizations of ai, i.e.vai = {v : (v = vai) (v |= ai)}. When = 1,the equation above reduces to the standard canoni-cal tokenization baseline, while = 0 weighs onlynon-canonical tokenizations of the same answer.To make sure that both terms are on the same scale,we condition the distributions on the possible an-swers, yielding two classifiers over answers insteadof tokenizations.We approximate p(vai|vc, va1 . . . vak) bycomputing the (unbiased) marginal estimate over",
  "p(vai|vc, va1 . . . vak) p(vai|vc)": "The resulting mixture is then a weighted versionof the marginal p(ai|vc, a1 . . . ak) where weadjust the mass attributed to canonical accordingto parameter . This allows us to inspect how themodel behaves when mass is shoveled aroundfrom non-canonical to canonical and vice versa. shows how this mixture behaves fordifferent values of downstream, while shows the performance change when tuning inthe validation set and applying it on the test set.Precise tuned values are shown in .These experiments show that there is clear andsignificant signal in non-canonical tokenizations tothe point that we are able to achieve a consistentincrease in accuracy, suggesting that non-canonicaltokenizations do indeed retain meaningful infor-mation in LLMs, and hopefully motivating furtherresearch in this direction.",
  "Conclusion": "Modern language models make the assumption thattext is represented by a unique canonical tokeniza-tion equivalent to the text itself. We showed thatnot only is the space of possible tokenizations ex-ponential, but prove that reasoning probabilisticallyabout this space is hard. We then showed empiricalevidence that suggests that, in practice, not onlyis the canonical tokenization the most likely tok- enization, but it is also very close to the probabilityof the text itself (i.e. the marginal probability ofthe text summed over tokenizations). Despite this,we present surprising evidence of significant sig-nal in non-canonical tokenizations, thus motivatingfurther research on non-canonical tokenizations.",
  "Jacob Buckman and Graham Neubig. 2018. Neurallattice language models. Transactions of the Associa-tion for Computational Linguistics, 6:529541": "Kris Cao and Laura Rimell. 2021. You should evaluateyour language model on marginal likelihood over to-kenisations. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 21042114. Nadezhda Chirkova, Germn Kruszewski, Jos Rozen,and Marc Dymetman. 2023. Should you marginalizeover possible tokenizations? In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages112.",
  "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay": "Kale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy. Preprint, arXiv:2403.08295.",
  "C. Y. Lee. 1959. Representation of switching circuits bybinary-decision programs. The Bell System TechnicalJournal, 38(4):985999": "Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question answer-ing. In Conference on Empirical Methods in NaturalLanguage Processing. Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, JwalaDhamala, Kai-Wei Chang, Richard Zemel, AramGalstyan, Yuval Pinter, and Rahul Gupta. 2024.Tokenization matters: Navigating data-scarce tok-enization for gender inclusive language technologies.Preprint, arXiv:2312.11779.",
  "Aleksandar Petrov, Emanuele La Malfa, Philip Torr,and Adel Bibi. 2023. Language model tokenizersintroduce unfairness between languages. Advancesin Neural Information Processing Systems, 36": "Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita.2020. BPE-dropout: Simple and effective subwordregularization. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics. Maarten Sap, Hannah Rashkin, Derek Chen, RonanLe Bras, and Yejin Choi. 2019. Social IQa: Com-monsense reasoning about social interactions. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP). Associationfor Computational Linguistics.",
  "Aaditya K Singh and DJ Strouse. 2024. Tokenizationcounts: the impact of tokenization on arithmetic infrontier llms. arXiv preprint arXiv:2402.14903": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "Junxiong Wang, Tushaar Gangavarapu, Jing NathanYan, and Alexander M. Rush. 2024. Mambabyte:Token-free selective state space model.Preprint,arXiv:2401.13660": "Lili Yu, Daniel Simig, Colin Flaherty, Armen Agha-janyan, Luke Zettlemoyer, and Mike Lewis. 2023.Megabyte: Predicting million-byte sequences withmultiscale transformers. In Advances in Neural Infor-mation Processing Systems, volume 36, pages 7880878823. Curran Associates, Inc. Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics.",
  "AProblems": "For the purposes of studying the complexity ofinference problems on induced tokenization dis-tributions, we use L to denote a class (set) of au-toregressive large language models, and make theassumption that this set covers all possible autore-gressive distributions: Assumption A.1 (Expressivity of LLMs). We as-sume that L is sufficiently expressive: given anytoken sequence v = (v1, ..., vm), and sequence1, ..., m with i (0, 1) for all i, there existsp L such that p(vi|v1, ..., vi1) = i for alli = 1, ..., m. Note that we do not require that the conditionalprobability take the value 0 or 1, as this cannotbe expressed using logits. We also need to makethe (reasonable) assumption that the conditionalprobability distribution of LLMs can be computedin polynomial time: Assumption A.2 (Complexity of LLMs). We as-sume that for any p L, and any sequence oftokens v = (v1, . . . , vm), we can compute the dis-tribution p(vi|v1, ..., vi1) for any i = 1, . . . , m inpolynomial time in |v|. Now, we consider two inference problems re-lated to the induced tokenization distribution;namely, computing the most likely tokenization,and marginal string probability (a more formalstatement of Problems 4.1 and 5.1): Problem A.3 (Most Likely Tokenization). Given astring x, vocabulary V, and an autoregressive LLMp L over V, and a threshold > 0, we define themost likely tokenization problem MLT(x, V, p, )as deciding whether:",
  "Proof. We begin by showing hardness. Given aninstance of 3-SAT, we construct an instance ofMLT such that the 3-CNF is satisfiable iff themaximal probability is above a certain threshold": "Let = Kk=1 ci be a 3-CNF formula consistingof K clauses over n Boolean variables, whereck = lk,1lk,2lk,3, and lk,j is a literal. For conve-nience, we write Ik,j for the index of variable lk,jrefers to, and Pk,j to be a Boolean variable which istrue iff it is a positive literal, i.e. lk,j = aIk,j Pk,j.",
  "p(vi|v0, ..., vi1) =": "0.45if i = 0(vi = a vi = ab)0.033if i = 0(vi = a vi = ab)0.9if i < 2n(vi1 = a vi = bc)0.025if i < 2n(vi1 = a vi = bc)0.9if i < 2n(vi1 = ab vi = c)0.025if i < 2n(vi1 = ab vi = c)0.45if i < 2n(vi1 = bc vi1 = c)(vi = a vi = ab)0.033if i < 2n(vi1 = bc vi1 = c)(vi = a vi = ab)0.2if i < 2n (vi1 = d)1 0.5n+K+1if i 2n (vi = d)S(i + 1 2n, v)",
  "Trueif (v2Ik,j = a) = Pk,jFalseotherwise(5)": "We define v |= iff Kk=1 S(k, v), i.e. allclauses are satisfied. Now we claim that the 3-CNF formula is satisfiable iff maxv p(v, x) >0.5(0.45)n(0.9)n+K.We begin by noting thatall tokenizations of the string x are of the samelength 2n + K, since each abc\" sequenece mustbe split into either (ab\", c\") or (a\", bc\"), andthe ddd...\" sequence must be tokenized into K d\"tokens. The probability of any valid tokenization v",
  "i=2np(d|v1, . . . , vi1)": "Note that the remaining conditional probabili-ties are all either 0.9 or 0.025; thus, p(v, x) >0.5(0.45)n(0.9)n+K iff all of these conditionalprobabilities are 0.9. Since all of the tokens vi fori 2n (for x) are d\", this happens iff v |= , andthe 3-CNF is satisfiable. Thus MLT is NP-hard. To show NP-completeness, we note all tokeniza-tions have length 2n + K and so oracle calls to theLLM take polynomial time in n, K by AssumptionA.2. If the answer to MLT is Yes, then there existsa tokenization v with p(v, x) > t which acts asthe certificate. This certifiacte can be checked inpolynomial time; thus MLT is in NP.",
  "4if i 2n (vi = d)S(i + 1 2n, v)": "The difference between this distribution and thatin Theorem 4.2 is the last 4 cases, where the proba-bility is dependent on the number of CNF variablesn. Now, we will show that the model count ofthe CNF formula is equal to C {0, ..., 2n}iff (C 0.5)(0.45)n(0.9)n < v p(v, x) <(C + 0.5)(0.45)n(0.9)n.As before, the probability of any valid tokeniza-tion v is given by:",
  "< (C + 0.5)(0.45)n(0.9)n": "We have shown that the model count of theCNF formula is equal to C {0, ..., 2n} iff(C 0.5)(0.45)n(0.9)n < v p(v, x) < (C +0.5)(0.45)n(0.9)n.Given v p(v, x), we cancompute the (unique) value of C {0, ..., 2n} forwhich this holds by binary search, with complexityO(n). Thus we have reduced #3-SAT to MSP andso MSP is #P-hard."
}