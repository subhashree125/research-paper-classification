{
  "Abstract": "High-quality data is crucial for the pre-trainingperformance of large language models. Unfor-tunately, existing quality filtering methods relyon a known high-quality dataset as reference,which can introduce potential bias and com-promise diversity. In this paper, we proposeScalingFilter, a novel approach that evaluatestext quality based on the perplexity differencebetween two language models trained on thesame data, thereby eliminating the influenceof the reference dataset in the filtering process.An theoretical analysis shows that ScalingFilteris equivalent to an inverse utilization of scalinglaws. Through training models with 1.3B pa-rameters on the same data source processed byvarious quality filters, we find ScalingFilter canimprove zero-shot performance of pre-trainedmodels in downstream tasks. To assess the biasintroduced by quality filtering, we introducesemantic diversity, a metric of utilizing text em-bedding models for semantic representations.Extensive experiments reveal that semantic di-versity is a reliable indicator of dataset diversity,and ScalingFilter achieves an optimal balancebetween downstream performance and seman-tic diversity. 1",
  "Introduction": "The success of large language models (LLMs) issignificantly influenced by the quality and quantityof the pre-training corpus. Researchers have de-veloped various data curation pipelines to enhancedataset quality, focusing on raw web crawling, textextraction, repetition and toxic content removal,and, notably, quality filtering (Brown et al., 2020;Rae et al., 2021; Penedo et al., 2023).Quality filters aim to extract high-quality datafrom a noisy raw corpus, thereby improving the",
  ": In ScalingFilter, we assess the quality of textdocuments by their scaling characteristics with languagemodels in different sizes": "language models performance without increasingtraining costs. Existing filters are broadly classi-fied into two categories: reference-dependent andreference-free approaches. Reference-dependentmethods, such as binary classification (Brown et al.,2020; Gao et al., 2020; Chowdhery et al., 2023) andDSIR (Xie et al., 2023), filter out low-quality databy comparing it with high-quality seed datasets.While effective, these methods inevitably introducebiases present in the reference data, such as specificwriting styles or topics, thereby limiting the diver-sity and representativeness of training corpus (Sol-daini et al., 2023). In contrast, reference-free meth-ods, such as perplexity gating (Marion et al., 2023),assess data quality using predefined metrics likeperplexity scores from pre-trained models. Thesemethods mitigate the biases introduced by refer-ence datasets but encounter challenges due to theindirect relationship between absolute perplexityand document quality. This indirect relationshipinadvertently favors data with simple and repetitivecontent. Although such content is easier for mod-els to predict, it contributes minimally to learningdiversity and complexity (Wettig et al., 2024).To address these issues, we introduce a simpleyet effective quality filtering approach named Scal-ingFilter, which inversely leverages recent scalinglaws in generative modeling to assess data quality.The core idea is to analyze the perplexity differ-ences between two pre-trained models on the samedata and assess the data quality based on these dif- ferences. We find a positive correlation betweendata quality and perplexity differences by inverselyderiving Chinchilla scaling law (Hoffmann et al.,2022). In other words, given a pair of pre-trainedmodels of different sizes, documents with higherperplexity differences indicate higher quality.ScalingFilter involves utilizing the differencebetween two separate models for data quality as-sessment, effectively addressing the bias issue in-duced by relying on a single model trained on thereference data. This approach also mitigates theproblem of selecting simple and repetitive textsthat arise from overfitting to the perplexity metric,thereby enhancing data diversity and complexity.Furthermore, ScalingFilter offers a theoretical anal-ysis for using perplexity differences as a qualityindicator for data filtering by inversely derivingmodel scaling laws.Our experiments demonstrate that ScalingFilteris superior to existing methods in improving fil-tered data quality while preserving data diversity.Specifically, we employ a pair of meta-models withsizes of 124M and 774M parameters to assess theperplexity difference for each document in the rawdataset, and then select the high-quality ones us-ing a top-k strategy. We train a 1.3B model fromscratch using filtered high-quality data. We thenevaluate its zero-shot performance on downstreamtasks and assess the semantic diversity of the fil-tered dataset.The results demonstrate that ScalingFilter out-performs the unfiltered baseline and previous state-of-the-art (SoTA) quality filtering methods. Specif-ically, compared to the unfiltered baseline, Scal-ingFilter achieves a +3.09% improvement in down-stream accuracy and a +2.23 increase in seman-tic diversity. When compared with perplexity gat-ing (Marion et al., 2023; Wenzek et al., 2019), Scal-ingFilter achieves a +1.12% improvement in per-formance and a +4.7 increase in semantic diversity.In summary, the contributions of this work arethreefold: 1. We introduce quality factor, a novel metricthat correlates directly with the quality oftraining data through the lens of model scal-ing laws, offering a more precise and unbiasedapproach to data curation.",
  "Methodology": "Overview.Existing quality filtering methodsdepend on either a reference high-quality dataset orthe absolute perplexity scores of documents froma single language model, which can introduce po-tential bias or result in inferior performance. Inthis section, we will elaborate on the principles ofScalingFilter through mathematical derivation. Thecore concept of ScalingFilter lies in estimating thequality of data samples by inversely applying thescaling law. Specifically, the scaling law reveals apower-law decrease in loss with increasing modelsize or data size (Hestness et al., 2017; Kaplanet al., 2020; Hoffmann et al., 2022; Aghajanyanet al., 2023). Ultimately, the scaling law yieldsthe optimal model/data scaling-up allocation strat-egy. In other words, under the same computationalbudget (TFLOPS), it determines the optimal ratioof model size to the number of training tokens toachieve the lowest loss, represented by a modelscaling exponent a and a data scaling exponent b.Extensive experiments comparing multipledatasets with known quality differences revealedthat high-quality data increases the model scalingexponent a (Bi et al., 2024). Specifically, the ex-periments compared the early and final versions ofin-house data together with OpenWebText2 (Gaoet al., 2020), revealing that the final version ofOpenWebText2 results in the highest a, while theearly version with the poorest quality leads to thelowest a. Intuitively, a higher value of a acceleratesthe rate at which the loss decreases as the modelparameters increase. This positive relationship willbe demonstrated later. Such an observation sug-gests that high-quality data enhances logical clarityand decreases predictive difficulty after adequatetraining. Consequently, scaling up the model sizebecomes more beneficial when the compute bud-get is increased (Bi et al., 2024; Aghajanyan et al.,2023; Kaplan et al., 2020; Hoffmann et al., 2022).",
  "(b)": ": (a) A visual diagram illustrates the theoretical result that high-quality data accelerates the rate of lossdecrease as model parameters increase, resulting in larger model scaling exponents a. (b) We calculated the averageloss of GPT-2 models of different sizes on several datasets with recognized quality levels: Wikipedia, OpenWebText,and Books3 represent high-quality data, while Unfiltered CommonCrawl represents low-quality data. The resultsclosely align with the theoretical analysis shown in (a), which indicates that high-quality data accelerates the rate ofloss decrease as model parameters increase. By inversely applying this principle, ScalingFilterestimates data quality based on the rate of loss de-crease in models with a fixed parameter difference,thereby separating high-quality data from the rawdataset.To proceed, we will first define the quality factor,which is the magnitude of loss reduction. Then,starting from the formula of the scaling law, wewill demonstrate the positive correlation betweenthe quality factor and the model scaling exponenta. Furthermore, based on the positive correlationbetween a and data quality observed in (Bi et al.,2024), we can ultimately prove the positive corre-lation between the quality factor and data quality.Quality factor.We begin with defining the qual-ity factor, which we will later demonstrate to havea positive correlation with data quality. We denotethe smaller meta-model as p and the larger one asq. Both meta-models share the same architectureand are trained on the same dataset, with the onlydifference being the parameter counts: Np for pand Nq for q, with Np < Nq. Let xi be a giventext sample, and denote the quality factor of thissample as di. Then, we have:",
  "perplexity has a direct relationship with the cross-entropy loss L because PPL = 2L, indicating thatthe perplexity score is positively related to the lossL": "Quality factor is positively correlated with dataquality.Next, we will introduce the expressionof the scaling law (Hoffmann et al., 2022; Kaplanet al., 2020; Henighan et al., 2020; Aghajanyanet al., 2023) and transform it into a form involvingthe model scaling exponent a which, as we intro-duced in the overview, is known to have a positiverelationship with data quality (Bi et al., 2024).Given the number of model parameters N andtraining tokens D, the expected model loss L isformulated (Hoffmann et al., 2022) as:",
  "D(2)": "where E represents the minimal achievable loss,corresponding to the entropy of natural text. ThetermsAN andBD account for the functional ap-proximation error and the optimization or conver-gence error, respectively (Aghajanyan et al., 2023).Here, A, B, , and are hyperparameters relatedto the model architecture and the training data. Thescaling law, indicating the optimized numbers ofN and D under a given compute budget C, followsa power-law form (Kaplan et al., 2020; Hoffmannet al., 2022):",
  "Da(4)": "We focus on the relationship between expected lossL and model scaling exponent a as well as modelsize N, and thus denote L as L(a, N). Its obviousthat L decreases as N increases. We further provein Appendix A.2.1 that at a specific N0, the slopeof the tangent to the L N curve decreases asa increases (i.e., the larger the a, the steeper thetangent, as illustrated in a that l2 is steeperthan l1). Due to this monotonic relationship, we caninfer the value of a from the slope of the tangent:for a given N0, a steeper tangent (greater absolutevalue of the slope) indicates a larger a, that is:",
  "N=N0(5)": "Furthermore, we prove in Appendix A.2.2 thatthe above conclusion can be extended from thetangent slope at a given N0 to the slope of thesecant line for any given N (i.e. ki in a).Letting N = NqNp, the slope of the secant lineis always negative, and a is positively correlatedwith the negative slope of the secant line. Since thequality factor d is also positively correlated withthe negative slope of the secant line, it follows thatd is positively correlated with a:",
  "d = 2L(Np)L(Nq) = 2(L(Nq)L(Np))": "= d a(6)Based on empirical observations from (Bi et al.,2024), higher values of a are achieved with high-quality data, indicating that quality factor d is pos-itively correlated with data quality.This conclusion aligns with our practical compar-ative tests. As shown in b, we calculatedthe average loss of GPT-2 models of different sizes on several datasets with recognized quality levels:Wikipedia, OpenWebText, and Books3 representhigh-quality data, while Unfiltered CommonCrawlrepresents low-quality data, based on a randomsample of 10k documents from each dataset. Theresults align closely with the theoretical estimatesshown in a, where the high-quality datashows a steeper secant (k2 > k1) compared tolow-quality data. Its worth noting that a singlecase might deviate from the training data distribu-tion of the meta-models, leading to higher absoluteperplexity for various model sizes. Thus, relyingsolely on single perplexity as a quality criterioncan result in misjudgments. However, high-qualitydata follows a law where perplexity decreases morewith an increase in model parameters, indicating agreater perplexity difference (i.e., quality factor).Selecting high-quality data with quality factor.We have demonstrated that the quality factor candirectly characterize data quality above, so itsstraightforward to directly use it to select high-quality data from a noisy unfiltered dataset. Wecall this simple yet effective method as ScalingFil-ter, as illustrated in . Consider a unfilteredset of documents S, containing both high and low-quality documents. For each sample xi S, wecalculate the quality factor di for it. As derived pre-viously, samples with higher di are of better quality.The top-k samples are then selected based on thedesired cleaning rate to form the resulting dataset.",
  "Experiments": "In this section, we will demonstrate the effec-tiveness of ScalingFilter through extensive exper-iments. Specifically, language models trained ondata filtered by ScalingFilter consistently achievedsuperior performance across various downstreamtasks, compared to the unfiltered baseline and othercommon quality filtering approaches, highlightingthe higher quality of the data. Furthermore, by mea-suring the semantic diversity of the filtered dataset,we found that ScalingFilter effectively preservedthe diversity present in the original dataset.",
  "Data Quality Evaluation": "Setup.We begin with five CommonCrawldumps from 2019 to 2023, processed through theCCNet (Wenzek et al., 2019) pipeline, in accor-dance with (Computer, 2023). From the prepro-cessed dataset, 500 GB of text data are randomlyselected as our baseline, yielding approximately : Zero-shot downstream accuracy of models trained with different quality filters. We cover a variety oftasks and widely used datasets (Penedo et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Dey et al., 2023;Biderman et al., 2023), including sentence completion, coreference resolution, natural language inference andmultiple-choice question answering. For binary classification (Brown et al., 2020; Chowdhery et al., 2023; Touvronet al., 2023) and importance resampling (Xie et al., 2023), we leverage the best results from various referencedatasets, whereas perplexity gating (Marion et al., 2023) utilizes the larger models perplexity in our meta-models.",
  "Quality FilterHellaswagLAMBADAWinograndePIQAARCOpenbookQABoolQAvg": "Random45.4041.9651.0769.8039.8832.4056.7648.18Binary Classification48.1348.9654.3069.7541.6630.4061.3850.65Importance Resampling47.5248.3654.3868.5041.6332.6060.8050.54Perplexity Gating48.1748.9653.0469.7541.5429.6060.0050.15ScalingFilter (Ours)49.0748.4255.0970.5742.6731.4061.6851.27 125 billion tokens for additional quality filtering.In each experiment, we train a decoder-only modelwith 1.3B parameters, using the same model ar-chitecture as (Peng et al., 2023). Each model istrained on 25B tokens until performance levelsoff, according to (Hoffmann et al., 2022; Penedoet al., 2023; Marion et al., 2023), which takes ap-proximately 4 days on 4 nodes with 8 NVIDIATesla V100 GPUs. We use pre-trained GPT-2 mod-els (Radford et al., 2019) as default meta-modelsto calculate quality factors for each sample. Thesmaller and larger models have 124M and 774Mparameters, respectively. We later perform abla-tion studies to discuss impacts by the pre-trainingdata. Following (Penedo et al., 2023), we utilizethe lm-evaluation-harness library (Gao et al.,2023) to evaluate zero-shot performance across var-ious downstream tasks of each model trained ondocuments retained through specific quality filter-ing method. We encompasses a variety of tasks andwidely used datasets (Penedo et al., 2023; Brownet al., 2020; Chowdhery et al., 2023; Dey et al.,2023; Biderman et al., 2023), including sentencecompletion (Hellaswag (Zellers et al., 2019) andLAMBADA (Paperno et al., 2016)), coreferenceresolution (Winogrande (Sakaguchi et al., 2021)),natural language inference (ARC (Clark et al.,2018)), and multiple-choice question answering(PIQA (Bisk et al., 2020), OpenbookQA (Mihaylovet al., 2018), BoolQ (Clark et al., 2019)). Baselines.We compare ScalingFilter with ran-dom selection, binary classification (Brown et al.,2020; Gao et al., 2020; Chowdhery et al., 2023;Touvron et al., 2023), importance resampling (Xieet al., 2023) and perplexity gating (Marion et al.,2023). All quality filters will keep 70% of the unfil-tered documents in align with (Computer, 2023), ifnot specified otherwise. As for binary classification,we choose Wikipedia, books and OpenWebText as positive samples and unfiltered CommonCrawldocuments as negative ones, following (Du et al.,2022; Chowdhery et al., 2023). We set the shapeparameter of Pareto distribution = 9, follow-ing (Brown et al., 2020; Gao et al., 2020; Chowd-hery et al., 2023). As to importance resampling,we follow the settings in (Xie et al., 2023), withOpenWebText (Gokaslan and Cohen, 2019) as thetarget dataset. As for perplexity gating, we fol-low (Marion et al., 2023) as well as our cleaningratio, keeping documents with perplexity rangingfrom 15th to 85th percentiles, resulting in keep-ing the middle 70% documents of the unfiltereddataset. Perplexity is computed by the larger ofthe meta-models, the one with higher capacity andability.Results. shows the comparison betweenvarious data quality filter methods. In summary: On average, ScalingFilter shows a 0.62% im-provement over the widely-used binary classi-fication quality filtering method and a 0.73%improvement over importance resampling,achieving the state-of-the-art performance.",
  "Notably, for binary classification (Brown et al.,": "2020; Chowdhery et al., 2023; Touvron et al., 2023)and importance resampling (Xie et al., 2023), weuse the best results from various reference datasets,specifically OpenWebText. The results for per-plexity gating (Marion et al., 2023) use the largermodels perplexity in our meta-models for a faircomparison. Ablations concerning the referencedatasets of the aforementioned methods will bediscussed in subsequent sections. : Ablations on effects of meta-models training data within the ScalingFilter framework. The results revealthat meta-models trained on alternative datasets also showcase competitive performance, indicating that there is notan overly strong dependency on meta-models pretrained on WebText, emphasizing the robustness and flexibility ofScalingFilter variants.",
  "For model pairs trained on WebText, we directly use OpenAI GPT-2 models from HuggingFace, which is the meta-models used in the original ScalingFilterframework": "Meta-models trained on various datasets exhibitcompetitive and comparable performance.Wedetail ablation studies with meta-models trained ondifferent datasets in . Besides the meta-models trained on WebText, results of which areshown in , we trained meta-models on asubset of Wikipedia, OpenWebText, and unfilteredCommonCrawl data with no more than 25B tokens.Each dataset was used to train meta-models for 1epoch. The results demonstrate that all experimentsoutperform the baseline of random selection. Train-ing on unfiltered CommonCrawl or OpenWebTextyielded results competitive with those from otherquality filtering methods. Furthermore, training onWikipedia achieved results very close to the best,with a marginal gap of 0.15%. Ablations on different sizes of meta-models.We perform experiments to investigate impactsbrought by using pairs of meta-models with dif-ferent sizes. The results are briefly presented in. When using a pair of meta-models withrelatively small differences in the number of param-eters to estimate the quality factors of data, thereis a certain degree of performance degradation inthe downstream tasks. Reducing the size of thelarger models in meta-models from 774M to 335Mdecreases the average performance on downstreamtasks by 0.96%. Conversely, increasing the sizeof the smaller models in meta-models from 124Mto 335M results in a decrease of 1.28% in perfor-mance. This suggests that a larger parameter gapmay more effectively amplify differences in howmodels fit textual data, allowing for a more reliableassessment of the quality factor. Detailed explo-ration of this hypothesis is left as future work. Ablations on reference datasets.We also ex-amine the impacts of different reference datasetson popular quality filtering methods that rely on areference. Results are shown in . Binaryclassification using OpenWebText as the positiveclass results in the best performance, similar to importance resampling with the same dataset asa reference. This aligns with the findings in (Biet al., 2024), which confirm that OpenWebText hassuperior data quality. Binary classification with amixed dataset including OpenWebText, Wikipedia,and books yields inferior results, possibly due tothe classifiers training recipe, such as the mixingratio of the three datasets. Surprisingly, importanceresampling with Wikipedia results in similar aver-age accuracy to the random baseline, with muchbetter accuracy in ARC and BoolQ but significantlyworse performance in sentence completion taskslike Hellaswag and LAMBADA, possibly due tothe serious domain shift towards Wikipedia. Inconclusion, the choice of reference datasets has asignificant impact on the performance of qualityfilters that rely on references.",
  "Data Diversity Evaluation": "Training large language models requires diversedata. Current quality filters, by favoring text datasimilar to the reference dataset, may discard docu-ments on informal topics or from minorities, reduc-ing the trained models knowledge diversity (Wen-zek et al., 2019; Soldaini et al., 2023). How canwe assess a datasets data diversity? We introducea metric to measure a document groups semanticdiversity.Semantic diversity metric.Following (Fried-man and Dieng, 2022), we define semantic diver-sity as the exponential of the Shannon entropy ofthe semantic similarity matrixs eigenvalues. For aset of text documents x1, x2, ..., xn and a semanticsimilarity function f, we obtain a similarity matrixS, where each entry si,j = f(xi, xj). Denoting1, 2, ..., n as the eigenvalues of S/n, we definesemantic diversity as follows.",
  "(7)": ": Ablation studies on the effects of reference data. We varied the reference datasets for binary classifica-tion (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023) and importance resampling (Xie et al., 2023).The results indicate that OpenWebText is the optimal reference dataset choice for both reference-dependent qualityfiltering methods.",
  ": Positive correlation between the number ofdatasets and semantic diversity, demonstrating semanticdiversity as a reliable measure of data diversity": "A pre-trained language model extracts each doc-uments semantic embedding, using cosine similar-ity as the similarity function. In our experiments,we utilize the bge-base-en-v1.5 model (Xiaoet al., 2023) with the sentence_transformers li-brary due to its efficiency and outstanding perfor-mance in various text embedding-related retrievaland clustering tasks. Selecting a proper size of documents.Com-putational constraints prevent calculating seman-tic diversity for all documents in the dataset. Ex-periments on the unfiltered dataset help select anappropriate document count for calculating the se-mantic diversity metric. For each experiment, werandomly select n samples, calculate their semanticdiversity score, and repeat this process 10 times tocompute the average score and standard deviation.Results are displayed in . Results indicatethat semantic diversity stabilizes when the groupexceeds 10,000 samples, with a standard deviationof 0.12. We choose 10,000 samples for subsequentexperiments to balance accuracy and efficiency.",
  "Standard Deviation": "Semantic DiversityStandard Deviation : Results on the relationship between semanticdiversity and sample size. Semantic diversity stabilizesat a sample size of 10,000, with a standard deviationbelow 0.2. Therefore, we choose 10,000 as our samplesize for calculating semantic diversity, as it representsthe datasets diversity adequately while ensuring com-putational efficiency. with diverse topics or writing styles, including newsarticles (CC-News (Hamborg et al., 2017)), moviereviews (IMDB (Maas et al., 2011)), forums (Red-dit 2), Wikipedia, and crawled web pages (Open-WebText (Gokaslan and Cohen, 2019)). We ex-tracted the same number of samples from one ormore of the above datasets, creating a mixed subsetof 10,000 samples. We then averaged the relation-ship between semantic diversity and the numberof datasets (N). shows a positive corre-lation between semantic diversity and the numberof datasets (N), indicating that semantic diversityaccurately reflects data diversity within datasets. Quality filtering with quality factor keeps thediversity of the unfiltered dataset.We assessthe semantic diversity of datasets resulting fromvarious quality filtering methods. The results arepresented in . Most quality filters achieve",
  "M335M48.7747.2553.6769.7541.1232.0059.6050.31335M774M48.3245.7652.4170.1842.0530.6060.6149.99124M774M49.0748.4255.0970.5742.6731.4061.6851.27": "higher diversity than the original unfiltered dataset,likely due to the removal of a large number ofmachine-generated spams with similar semanticmeanings. The results indicate that importance re-sampling achieves the highest diversity, at 56.25,attributed to its resampling strategy. ScalingFilterresults in greater diversity compared to the mostcommonly used binary classification, thanks to itsreference-free nature. Perplexity gating reducesthe diversity of the original dataset, supporting theconclusion from (Wenzek et al., 2019) that filter-ing data based solely on perplexity thresholds canintroduce unexpected bias to data diversity.",
  "Related Work": "Quality Filtering.Pretraining data for largelanguage models often includes low-quality con-tent, such as harmful machine-generated spam oranomalous formats. To filter this data, researcherstypically score documents using linear classifiers orlanguage models, then filter based on these scores.High-quality data proxies like Wikipedia, books,and OpenWebText are commonly used. Early stud-ies, such as (Brown et al., 2020; Chowdhery et al.,2023), employed linear binary classifiers, compar-ing curated datasets to unfiltered CommonCrawldata and used noisy thresholding with Pareto ran-domness, which, while potentially enhancing diver-sity, might reduce data quality as suggested by (Xieet al., 2023). Recent studies, such as (Touvron et al.,2023), use Wikipedia as the sole positive class andapply hard thresholding, potentially limiting corpusdiversity and introducing biases. Another approachinvolves language models. For instance, (Wenzeket al., 2019) trained an n-gram model on Wikipedia and categorized documents by perplexity into head,middle, and tail, with low-perplexity documentsretained for pre-training (Computer, 2023). Otherstudies (Wenzek et al., 2019; Soldaini et al., 2023)keep all data to preserve diverse writing styles.Similarly, some filter data based on a models per-plexity, which might bias towards easily predictedtexts and discard challenging but high-quality con-tent (Marion et al., 2023). Our approach introducesa quality factor derived from two language modelsto address this issue.Scaling Laws.Scaling laws quantify how modelsize, dataset size, compute budget, and perfor-mance relate during neural network training. Ini-tial studies (Hestness et al., 2017; Kaplan et al.,2020) identified a power-law relationship amongthese factors. (Hoffmann et al., 2022) introduced aunified formula for scaling laws that incorporatesdata-dependent scaling terms. Recent studies showvariations in scaling laws across multilingual (Con-neau et al., 2019) and multimodal (Henighan et al.,2020; Cherti et al., 2023) settings. (Aghajanyanet al., 2023) meticulously analyzed this subject, de-riving varied scaling law parameters for uni-modaland mixed-modal contexts, highlighting the signifi-cant impact of data modality on scaling behaviors.This discovery suggests a hypothesis that varyingdata quality influences scaling behaviors. A recentstudy on large language model scaling laws (Biet al., 2024) confirms data quality impacts bothmodel and data scaling exponents in scaling laws.This paper demonstrates the link between scalinglaw parameters and data quality, facilitating theselection of high-quality samples based on theirscaling attributes.",
  "Conclusion": "We have presented ScalingFilter for data qualityfiltering in a reference-free manner. Starting fromthe scaling law, we demonstrate that the perplexitydifference across agnate models of different sizes(i.e. meta-models) correlates with data quality pos-itively. We select samples with higher perplexitydifference (i.e. quality factors) to form the pre-training dataset. By eliminating the bias brought",
  "Limitations": "There are still some limitations of ScalingFilter thatneed to be addressed. First, it relies on perplexitydifference between two LLMs, which may miss nu-anced aspects of text quality like factual accuracyor bias like race bias, social class bias and genderbias, etc. Second, it requires significant computa-tional resources to compute perplexity differencesfor a large dataset. Third, its applicability to otherlanguages and data-limited domains is uncertain.Future research should address these limitationsand further explore the relationship between seman-tic diversity and model performance, particularlyregarding fairness and bias. We sincerely thank Jingcheng Hu and ZhengZhang for useful discussions. This work was sup-ported by the National Natural Science Founda-tion of China (No. 62121002, U20B2047), AnhuiProvincial Science and Technology Major Project(No. 2023z020006) and the Fundamental ResearchFunds for the Central Universities. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang,Stephen Roller, Naman Goyal, Omer Levy, andLuke Zettlemoyer. 2023. Scaling laws for genera-tive mixed-modal language models. arXiv preprintarXiv:2301.03728. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-ing open-source language models with longtermism.arXiv preprint arXiv:2401.02954. Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling.In InternationalConference on Machine Learning, pages 23972430.PMLR. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Mehdi Cherti, Romain Beaumont, Ross Wightman,Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and JeniaJitsev. 2023.Reproducible scaling laws for con-trastive language-image learning. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 28182829. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. arXiv preprintarXiv:1905.10044. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457.",
  "Together Computer. 2023. Redpajama v1: An opensource recipe to reproduce llama training dataset": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2019. Unsupervisedcross-lingual representation learning at scale. arXivpreprint arXiv:1911.02116. Nolan Dey, Gurpreet Gosal, Hemant Khachane, WilliamMarshall, Ribhu Pathria, Marvin Tom, Joel Hestness,et al. 2023. Cerebras-gpt: Open compute-optimallanguage models trained on the cerebras wafer-scalecluster. arXiv preprint arXiv:2304.03208. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.Glam: Efficient scaling of language models withmixture-of-experts. In International Conference onMachine Learning, pages 55475569. PMLR. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, et al. 2024. The llama 3 herd of models. arXivpreprint arXiv:2407.21783.",
  "Dan Friedman and Adji Bousso Dieng. 2022. The vendiscore: A diversity evaluation metric for machinelearning. arXiv preprint arXiv:2210.02410": "Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation.",
  "Aaron Gokaslan and Vanya Cohen. 2019.Open-webtext corpus": "Felix Hamborg, Norman Meuschke, Corinna Breitinger,and Bela Gipp. 2017. news-please: A generic newscrawler and extractor. In Proceedings of the 15th In-ternational Symposium of Information Science, pages218223. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,Christopher Hesse, Jacob Jackson, Heewoo Jun,Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.2020.Scaling laws for autoregressive generativemodeling. arXiv preprint arXiv:2010.14701. Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-gory Diamos, Heewoo Jun, Hassan Kianinejad,Md Mostofa Ali Patwary, Yang Yang, and YanqiZhou. 2017. Deep learning scaling is predictable,empirically. arXiv preprint arXiv:1712.00409. Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks,Johannes Welbl, Aidan Clark, et al. 2022. Train-ing compute-optimal large language models. arXivpreprint arXiv:2203.15556. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv:2001.08361. Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011. Learning word vectors for sentiment analysis.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 142150, Portland,Oregon, USA. Association for Computational Lin-guistics.",
  "Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question answer-ing. arXiv preprint arXiv:1809.02789": "Denis Paperno, Germn Kruszewski, Angeliki Lazari-dou, Quan Ngoc Pham, Raffaella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, and RaquelFernndez. 2016. The lambada dataset: Word pre-diction requiring a broad discourse context. arXivpreprint arXiv:1606.06031. Guilherme Penedo, Quentin Malartic, Daniel Hesslow,Ruxandra Cojocaru, Alessandro Cappelli, HamzaAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,and Julien Launay. 2023. The refinedweb datasetfor falcon llm: outperforming curated corpora withweb data, and web data only.arXiv preprintarXiv:2306.01116. Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao,Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang,Bolin Ni, Jingcheng Hu, et al. 2023. Fp8-lm: Train-ing fp8 large language models.arXiv preprintarXiv:2310.18313.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99106": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri,Patrick LeGresley, Jared Casper, and Bryan Catan-zaro. 2019.Megatron-lm: Training multi-billionparameter language models using model parallelism.arXiv preprint arXiv:1909.08053. Shaden Smith, Mostofa Patwary, Brandon Norick,Patrick LeGresley, Samyam Rajbhandari, JaredCasper, Zhun Liu, Shrimai Prabhumoye, GeorgeZerveas, Vijay Korthikanti, et al. 2022. Using deep-speed and megatron to train megatron-turing nlg530b, a large-scale generative language model. arXivpreprint arXiv:2201.11990. Luca Soldaini, Rodney Kinney, Akshita Bhagia, DustinSchwenk, David Atkinson, Russell Authur, Ben Bo-gin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar,Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morri-son, Niklas Muennighoff, Aakanksha Naik, CrystalNam, Matthew E. Peters, Abhilasha Ravichander,Kyle Richardson, Zejiang Shen, Emma Strubell, Nis-hant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettle-moyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,and Kyle Lo. 2023. Dolma: An Open Corpus ofThree Trillion Tokens for Language Model Pretrain-ing Research. arXiv preprint. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-neau, Vishrav Chaudhary, Francisco Guzmn, Ar-mand Joulin, and Edouard Grave. 2019. Ccnet: Ex-tracting high quality monolingual datasets from webcrawl data. arXiv preprint arXiv:1911.00359.",
  "A.2.2Generalizing from tangent slope tosecant slopeIts impossible to calculate the slope of the tan-gent L": "N in the real scenario, we can only acquirethe slope of the secant line by assessing the cross-entropy loss on two models with different sizes (i.e.a pair of meta-models). Next, we will prove thatthe slope of the tangent has a positive relationshipwith the slope of the secant line. Therefore, we canbuild direct relationship between the slope of thesecant line and a.Given a pair of meta-models with Np and Nqparameters where Np < Nq, we can denote theslope of the secant line as:",
  "A.3Sampling vs. top-k selection": "Previous works (Brown et al., 2020; Gao et al.,2020; Xie et al., 2023; Wettig et al., 2024) typicallyuse sampling without replacement, selecting databased on a rating score to balance quality and di-versity. This approach often results in improveddownstream performance. We conducted exper-iments to determine whether this sampling strat-egy could enhance the downstream performanceof ScalingFilter. Following (Wettig et al., 2024),we introduced a temperature term to adjust sam-ple diversity. Here, 0 means top-k selection, Table A.3: Ablations on sampling vs. top-k selection. Note that the top-k results are identical to the originalScalingFilter results reported in . We compare top-k data selection to sampling without replacementfollowing (Xie et al., 2023; Wettig et al., 2024).",
  "A.4Ablation Study on Hyperparameters": "To further validate the robustness of ScalingFilter,we conducted ablation experiments using varioustraining hyperparameters on 1.3B models. Our fo-cus was primarily on two hyperparameters: learn-ing rate (default 2.5 104) and global batch size(default 256). We doubled the default values foreach in the ablation study. The results are presentedin Table A.2. The results indicate that an increase inglobal batch size significantly reduces performancein both settings with different quality filters, as ithalves the training steps. Conversely, increasingthe learning rate slightly affects downstream accu-racy. Overall, ScalingFilter remains robust acrossa range of training hyperparameters, consistentlysurpassing binary classification, its top competitor,as shown in .",
  "Table A.4: Time required for various quality filteringmethods on 360,000 data entries": "Although the ScalingFilter method involves agreater computational overhead, it achieves higherdownstream performance and data diversity com-pared to other methods. The model-based datafiltering approach is increasingly being adopted by the community (e.g., LLaMA 3 (Dubey et al.,2024), using LLaMA 2 to perform quality filter-ing), and model-based methods like ScalingFilterenable iteratively improving data quality throughthe training of better meta-models.There might be methods to reduce the computa-tional overhead of ScalingFilter, such as collectinga certain amount of text data with their correspond-ing quality factors and training a scorer to directlypredict the quality factor for any text data. We willleave this type of exploration for future work.",
  "A.6Qualitative Results and Analysis vs.Perplexity Gating": "There are two potential reasons why our proposedScalingFilter method can provide more complexand diverse data than the Perplexity Gating (Marionet al., 2023; Wenzek et al., 2019) method whichuse a single model to perform quality filtering.First, perplexity correlates with the similarity be-tween the text and the models training data, whichintroduces biases. For example, a model trainedon code data will show higher perplexity whenpredicting literary texts. This was discussed in Sec-tion 1, highlighting the sensitivity of perplexity tothe alignment between the targeted data and thetraining data distribution.Second, perplexity reflects the inherent complex-ity of the text. As noted in , repeatedwords like \"word word word...\" result in very lowperplexity because such patterns are easier for lan-guage models to predict. While complexity is apotential indicator of data quality, it is not alwaysdirectly related. Therefore, we aim to minimize theinterference brought by complexity when perform-ing data quality filtering.In contrast, our ScalingFilter method employs apair of meta-models trained on the same data butwith different model sizes. The perplexity com-puted by each model inherently reflects both thesimilarity to the training data and the complexityof the text. By comparing the differences in theperplexities of the two models, we effectively re-",
  "No.Text": "(a)Contains disordered formatting:Two interesting finance and investment posts by David Merkel at The AlephBlog....................\\nExcerpt from Post the First:\\nThe main thing to understand hereis that the government is not here to help you, but to milk you. The government doesnot care about you. It cares about its survival. If it cant get sufficient taxes out ofthe populace, it will use its financing arm, the central bank, to lend to it at preferentialrates, while passing on losses to the populace via inflation.\\nExcerpt from Post theSecond:\\nThe main idea is this: buy companies with better prospects than those beingsold.\\nLabels: Bloggery, Debts, Finance, government, Ideas, Inflation, investing\\nSeeingEyePeople.........................\\nHoldingon...........................................\\nOpeningpara-graphs........................\\nIke...................................\\nNumbers.................................\\nFiftyyears ago................................\\nHappiness...............................\\nRisk................................\\nAgentleman...................\\nGone................................\\nOpeningparagraphs....................\\n[TRUNCATED] (b)Contains incomplete sentences:Speaking Out Archives\\n2019, July Archives\\n#605Japan Should Seek to CreateWorld Whale-Raising Organization\\nJuly 8, 2019 Japan resumed commercial whaling in its200-mile exclusive economic zone on July 1 after its notification of the withdrawal from theInternational Whaling Commission on De...\\n#604Japan Should Wake Up to ThirdBlack Ship Arrival\\nJuly 1, 2019 Japans fate was changed by the United States twice inhistory - the arrival of Commodore Matthew Perrys black ship in 1853 and Japans defeatby the U.S. in World War I... [TRUNCATED]",
  "duce the influence of these factors, thus mitigatingthe training data bias. Consequently, ScalingFilterenhances both data diversity and quality, as demon-strated in Tables 1 and 5": "To empirically support this, we provide concreteexamples of texts that were discarded by Scaling-Filter but retained by the single-model PerplexityGating method. These examples highlight the abil-ity of ScalingFilter to filter out low-quality datathat would otherwise be retained due to the biasesinherent in Perplexity Gating. Table A.5 presentsthese examples. In Example (a), while the first part of the doc-ument contains normal text, the second part con-sists of repeated characters (\".......\"), which artifi-cially lowers the overall perplexity by reducing thecomplexity of the latter section. As a result, thisdocument would be retained by Perplexity Gating.However, this document is clearly of low quality,as it contains repeated patterns and poor formatting.Fortunately, ScalingFilter accurately discards thisdocument by eliminating the bias introduced bylow complexity.",
  "From the above examples, we can derive an in-tuitive theoretical explanation. In Example (a), the": "text exhibits low complexity, leading to low per-plexity values across both the large and small meta-models. If we were to directly use the perplexityscore for quality filtering, this would introduce acomplexity bias, as low complexity is not directlyindicative of data quality. By calculating the differ-ence in perplexity between the two models, Scal-ingFilter effectively cancels out the bias causedby complexity, yielding a quality factor that is nolonger affected by text complexity alone."
}