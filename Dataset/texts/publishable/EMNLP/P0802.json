{
  "Abstract": "Seeking answers effectively for long videosis essential to build video question answering(videoQA) systems. Previous methods adap-tively select frames and regions from longvideos to save computations. However, thisfails to reason over the whole sequence ofvideo, leading to sub-optimal performance. Toaddress this problem, we introduce a state spacelayer (SSL) into multi-modal Transformer toefficiently integrate global semantics of thevideo, which mitigates the video informationloss caused by frame and region selection mod-ules. Our SSL includes a gating unit to en-able controllability over the flow of global se-mantics into visual representations. To fur-ther enhance the controllability, we introducea cross-modal compositional congruence (C3)objective to encourage global semantics alignedwith the question. To rigorously evaluate long-form videoQA capacity, we construct two newbenchmarks Ego-QA and MAD-QA featur-ing videos of considerably long length, i.e.17.5 minutes and 1.9 hours, respectively. Ex-tensive experiments demonstrate the superior-ity of our framework on these new as wellas existing datasets. The code, model, anddata have been made available at nguyent-thong.github.io/Long_form_VideoQA.",
  "Introduction": "VideoQA has been extensively studied to developsystems to assist humans in daily activities (Grau-man et al., 2022; Lei et al., 2021), e.g., remind usersof their past actions, help users locate their belong-ings, and provide assistance with complex tasks.To implement these functions, we expect videoQAsystems to understand and extract relevant infor-mation from long-form videos with diverse objectsand complex spatial-temporal interactions.Compared with short clips, long-form videospose more challenges for videoQA. They consist",
  "*Corresponding to: Thong,": "of a higher number of objects and events. As such,comprehensively encoding information from themrequires expensive computations. Moreover, a highamount of information may be unrelated to theposed question. To address these problems, recentstudies (Bain et al., 2021; Wang et al., 2023; Gaoet al., 2023) adaptively select a subset of videoframes and visual regions associated with the ques-tion. Nevertheless, if a question necessitates a rea-soning of the entire sequence of events (e.g. video1s ), or an understanding of the overallvideo narration (e.g. video 2s ), a merehandful of selected frames or regions might notsufficiently encapsulate necessary details.To tackle these problems, we introduce a statespace layer (SSL). Before forwarding video framesto selection modules, SSL fixes long-term depen-dency patterns for integrating global informationinto visual representations. Such global informa-tion offers the selected frames the global contextwithin the video, so that they can relate to otherframes even though those frames are not selectedfor attention computation. However, a consider-able amount of unrelated global information mayflow into visual representations. Therefore, wefirst equip SSL with a gating mechanism to pro-vide more controllability over the flow of globalsemantics into visual representations, resulting inour Gated State space Multi-modal Transformer(GSMT) architecture. Furthermore, we promoteglobal semantics that is more aligned with thequestion. In particular, we introduce Cross-modalCompositional Congruence (C3) objective thatcompares visual attention with its version transi-tioned to the language basis via cross-modal at-tention, effectively measuring cross-modal congru-ence between intra-modal relations. Our rationalebehind focusing on intra-modal relations is becausevideoQA models often need to understand spatialand temporal relationships between entities andevents posed by the question (Gandhi et al., 2022),",
  "person's health deterioratesreplace the ingot of palladiumwearing the armored suitactivate the rt unit": "Question: what if the person did not have the ingot of palladium to replace the smoking one in the rt unit?Answer: The RT unit would degrade, potentially causing the person's health to deteriorate or the armored suit to malfunction. : Long-form videoQA examples, with videos taken from MAD (Soldan et al., 2022) and Ego4D (Graumanet al., 2022) datasets, respectively. Question in video 1 requires the model to reason about the relation chainof replacing ingot of palladium to activate the rt unit that powers the armored suit and protects persons health.Question in video 2 necessitates an understanding of the overall theme in video 2. thus we encourage globally informed visual repre-sentations to maintain compositional consistencybetween visual patches and question entities.Remarkably, we observe that recent long-formvideoQA works (Gao et al., 2023; Islam et al.,2024) still mostly evaluate on videos lasting at mostone minute or two, and use short-natured questionswhich necessitate watching only a short period ofvideo, i.e., about 100 seconds (Mangalam et al.,2023), to determine the answer. To more rigor-ously evaluate long-form videoQA capacity, weintroduce a construction procedure which utilizeslarge language model (LLM) to generate questionsand associated answers for egocentric and movievideos whose average lengths are 17.5 minutes and1.9 hours, respectively. Additionally, we also con-duct automatic and manual filtering to obtain high-quality questions which require watching a videoup to 1200 seconds to answer, longer than any ex-isting long-form videoQA benchmarks (Xiao et al.,2021; Wu et al., 2021; Mangalam et al., 2023).To sum up, our contributions are as follows:",
  "We propose a Gated State space Multi-modalTransformer (GSMT) with state space layer(SSL) to integrate global information into visualrepresentations for long-form videoQA": "We equip SSL with a gating mechanism toprovide controllability over the flow of globalvideo semantics and a Cross-modal Composi-tional Congruence (C3) objective to encouragequestion-aligned visual representations. We curate two new datasets with excessivelylong video lengths and long-natured questionsfor long-form videoQA. Comprehensive experi-ments on our curated and five standard datasetssubstantiate the superiority of our frameworkover various competitive baselines.",
  "Related Work": "Video question answering (videoQA). VideoQAdatasets (Xu et al., 2017; Jang et al., 2017; Tapaswiet al., 2016; Lei et al., 2018; Nguyen et al., 2024c)mostly focus on short video clips about daily ac-tivities, e.g. sports or household work. Recentworks (Gao et al., 2021; Grunde-McLaughlin et al.,2021; Wu et al., 2021) focus on complex spatial-temporal reasoning ability over longer temporallengths with causal and transition questions. Re-garding methodology, early works (Zhao et al.,2018; Li et al., 2019) present LSTM-based andGNN-based architectures to capture cross-modaland motion-appearance interaction. For example,(Xiao et al., 2023) incorporate graph modeling intoTransformer to explicitly encode the object rela-tions in videos. Due to outstanding performanceof pre-trained vision-language Transformers, Bainet al. (2021); Fu et al. (2023); Wang et al. (2023)utilize pre-trained Transformer models on down-stream videoQA tasks.Long-form video modeling. To improve the appli-cability of vision-language systems, various workshave focused on long-form video modeling for videoQA tasks. Wu and Krahenbuhl (2021) in-troduce short-term feature extraction technique andlong-term memory mechanism to alleviate redun-dant video frame processing. Lin et al. (2022b) pro-pose to compensate sparsely extracted video frameswith audio cues. Islam and Bertasius (2022) de-sign structured multi-scale temporal decoder. Gaoet al. (2023) utilize question as a guide to selectquestion-related visual segments to mitigate com-putation. Inspired by (Nguyen et al., 2021; Wuet al., 2024b) in language modeling domains, weintroduce a method to encode global semantics ofvideos to tackle information degradation problemin long-form video modeling.Cross-modal alignment. Alignment approaches,such as contrastive learning (Nguyen and Luu,2021; Nguyen et al., 2024e; Wu et al., 2023a,2024a), optimal transport (Nguyen and Luu, 2022;Wu et al., 2023b, 2024d,c), tree-based fusion(Nguyen et al., 2023c), and energy-based modeling(Nguyen et al., 2023d), have been investigated tocompare different input samples. For multimodallearning, recent cross-modal alignment methods fo-cus on establishing a shared latent space in whichsamples of different modalities can be comparedreadily (Nguyen et al., 2023b, 2024b,d,a, 2023a,2022b,a). For example, with a contrastive learn-ing formulation, CLIP (Radford et al., 2021) andALIGN (Jia et al., 2021) learn generalizable image-text representations from millions of image-textpairs. To exploit relation for cross-modal align-ment, Ren et al. (2021) use hard bijective correspon-dences between words and objects via an argmaxover the cross-modal attention matrix to optimizethe cross-modal alignment. Pandey et al. (2022)generalize the hard alignment with their soft align-ment approach, but they focus on objects within animage rather than a long video.",
  "Gated State Space Multi-ModalTransformer (GSMT)": "Our GSMT takes videos and questions as input,divides each video frame into visual patches anda question into textual words, and then forwardsvisual patches and textual words into video and textembedder to extract initial representations.Input Embedder.For video embedder, weutilize frozen pre-trained vision-language Trans-former to extract patch-level features X={x0, x1, ..., xL1}, of all T frames, where t-th frame consists of N patch-level features{xt,j}N1j=0 , hence L = NT, and x Rd. Fortext embedder, a similar frozen pre-trained vision-language Transformer is used to obtain word-levelfeatures W = {w0, w1, ..., wM1}, where w0 cor-responds to the [CLS] token and w1, ..., wM1 cor-respond to words in the question.Gated State Space Layer (Gated SSL). Inspiredby (Gu et al., 2021), we define a sequence-to-sequence map SSL(X) from a sequence of patch-level features to dS-dim hidden states, parameter-ized by learnable state matrices A RdSd, B RdSd, C RddS, and step size as:",
  "This can be written as a convolutional rep-resentationO= X,where=": "CB, CAB, ..., CAL1Bdenotes the convolu-tional kernel, the discrete convolution opera-tor, X the input sequence, O the correspondingoutput sequence. This convolution denotes thefixed global dependency pattern that facilitates thecomputation of global information among visualpatches. We use Fast Fourier Transformer (FFT)(Cooley and Tukey, 1965) to compute the convolu-tion in parallel provided that has been obtained.Computing the kernel is non-trivial since itrequires L distinct matrix powers. Instead, inspiredby (Gupta et al., 2022), we initialize A to be a",
  ".(13)": "We implement selector as a differentible Gumbel-softmax selection function.The output of theselector is a sequence of segment index B.Thereby, we extract respective segment featureswith respect to the selected segment indices, i.e.Sst = {sb | b B}.For every frame in the selected segments,we then perform cross-modal attention betweenits patch-level hidden representations H={h,j}N1j=0 , { b",
  ".(15)": "Lastly, we stack the selected patches of all selectedframes to obtain Hst = {h,j| T }N1j=0 .Multi-Modal Attention. At present, we employself-attention to produce multi-modal hidden rep-resentations that fuse the information of questionand video. In particular, we concatenate the ques-tion word-level features Q = {wi}M1i=0 , selectedsegment features Sst = {sb | b B}, and selectedpatch features Hst = {h,j| T }N1j=0 :",
  "Ego-QA": "We inherit 3k hours of 8640 egocentric videos fromthe Ego4D dataset (Grauman et al., 2022). Eachvideo is associated with about 280 dense captionsof consecutive moments. Based on these captions,we create our dataset in 2 stages, i.e. question-answer generation and data filtering.Question-answer generation. In this first stage,we concatenate a videos dense captions followingthe time order to construct its language descrip-tion. We utilize GPT-4 (Achiam et al., 2023) togenerate 20 questions per video. In our prompt, weencourage GPT-4 to avoid questions that are visu-ally biased and can be answered by a short videomoment. Then, we present the generated questionsto GPT-4 to generate the correct answer along with4 wrong answer choices.Data filtering. In the second stage, we filter outquestions that include clue words, e.g. passage,text, and description. Moreover, we also re-move questions that GPT-4 can answer withoutlooking at the concatenated narration or the ques-tion. Then, we adopt manual filtering by askingten graduate students who are native English speak-ers to ensure the veracity and temporal certificatelength for every question-answer sample. Partic-ularly, annotators are instructed to verify that 1)questions are valid and the correct answer is in-deed correct, 2) all distractor answers are incorrect,and 3) the video length to watch to determine thecorrect answer is at least 2 minutes.The filtering stage reduces the number of ad-missible questions by a factor of 4 to 5. Weaccomplish 18.8K questions for 992 videos, whichwe split into 80% train, 10% val, and 10% test.",
  "MAD-QA": "We follow the same process for Ego-QA to obtainMAD-QA by utilizing 1.2K hours of 650 videosfrom the MAD dataset (Soldan et al., 2022). Sincevideo lengths and the number of dense captions arelarger than Ego-QA, we ask GPT-4 to generate 60instead of 20 questions per video. Because GPT-4might store external knowledge about the movie,we replace the name of characters in the captionwith person_1, person_2, etc. Afterwards, weobtain 15.7K questions for 650 videos, and we splitthem into 80% train, 10% val, and 10% test.The average video lengths in Ego-QA and MAD-QA are 17.5 minutes and 1.9 hours, respectively.Moreover, the average necessary video lengths hu-mans need to watch to determine the answer forthe two datasets are respectively 1204.4 and 396.07seconds, longer than the average 100-second lengthof the recent very long-form videoQA datasetEgoSchema (Mangalam et al., 2023). We visu-alize the statistics of our datasets in Appendix E,and language prompts to generate question-answersamples in Appendix A, along with the instructionsfor human annotators in Appendix B.",
  "Standard Benchmarks": "In addition to our constructed Ego-QA and MAD-QA datasets, we follow previous works (Gao et al.,2023; Xiao et al., 2023; Wang et al., 2023) to eval-uate GSMT on four additional publicly availabledatasets for long-form videoQA: AGQA (Grunde-McLaughlin et al., 2021), NExT-QA (Xiao et al.,2021), STAR (Wu et al., 2021), Env-QA (Gao et al.,2021), and EgoSchema (Mangalam et al., 2023).AGQA (Grunde-McLaughlin et al., 2021) is avideoQA dataset for compositional spatio-temporalreasoning. As recommended by the dataset creator,we employ its v2 version, which possesses morebalanced distributions. AGQA consists of 2.27MQA pairs for 9.7K videos.NExT-QA (Xiao et al., 2021) focuses on causaland temporal reasoning. The dataset comprises5,440 videos associated with 52K questions.STAR (Wu et al., 2021) concentrates on situatedreasoning questions. The dataset provides 60Kquestions related to 22K videos clips.Env-QA (Gao et al., 2021) is curated for dynamicenvironment understanding. Env-QA contains 23Kegocentric videos collected on virtual environment",
  "Question: what if the person did not have the ingot of palladium to replace the smoking one in the rt unit?": "0. The RT unit would spontaneously improve its efficiency and generate additional power for the suit,enhancing the person's abilities 1. The lack of a new palladium ingot would trigger an internal safeguard,transforming the RT unit into an endless energy source free from toxicity 2. The RT unit would degrade, potentially causing the person's health to deteriorate or the armoredsuit to malfunction 3. The RT unit would lever the quantum fluctuations in the environment, spontaneouslygenerating a new form of energy hitherto unknown to science 4. Absence of palladium replacement wouldenable the RT unit to evolve, gaining sentience and the ability to repair and upgrade itself autonomously",
  "Implementation Details": "Our framework can be implemented on most multi-modal Transformers. To fairly compare with pre-vious works (Wang et al., 2023; Gao et al., 2023),we evaluate upon two popular types of pre-trainedmodels, i.e. CLIP (ViT/B-32) (Radford et al., 2021)and All-in-One-Base (Wang et al., 2023). We di-vide a video frame into 44 patches and send themto video embedder. For our gated SSL, we usedS = d = dh = 512, dgating = 128. For selectionmodules, we use top-k = 4, top-j = 12. For the mul-timodal attention module, we use NL = 2. Basedon validation, we employ max-pooling for all pool-ing operations, ReLU activation for , and NL = 2.",
  ": Results of videoQA on STAR": "We apply LC3 upon J(2), and observe no differ-ence between applying on J(1) and J(2). For faircomparison on AGQA, NExT-QA, STAR, Env-QA,and EgoSchema datasets, we sample 32 frames pervideo, and split them into K = 8 segments. Sincevideo lengths are longer in our EgoQA and MAD-QA datasets, we sample 128 and 8192 frames pervideo, respectively, and split into K = 8 segments.For language modality, we embed the question withthe same pre-trained model as the video embedder,and embed the answer with the pre-trained BERT-base model. We apply = 0.005 to balance thescale of LC3 and LCE.",
  "Quantitative Results": "We show our experiments on AGQA-v2, Env-QA,STAR, NExT-QA, and EgoSchema in , 2,3, 4, and 5, respectively, while revealing resultson our constructed Ego-QA and MAD-QA in Ta-ble 6. We can observe that our method achievessuperior performance over the latest methods onall datasets. In terms of the overall accuracy, weoutperform the second-best method on AGQA-v2,Env-QA, STAR, and EgoSchema, i.e. MIST-CLIP,by 1.77%, 3.76%, 1.72%, and 2.13%, respectively.Equivalently, we outperform CoVGT, which is thesecond-best method on NExT-QA, by 1.20%.Inspecting more closely, we note that our frame-work obtains more significant performance in-crease on questions that require the capacity ofreasoning among visual concepts, i.e. improving2.66% and 3.54% respectively for relation-actionand object-action on AGQA-v2, 6.25% and 4.52%respectively for causal and temporal on NExT-QA,",
  ": Effect of the position of SSL": "than those that require the ability to extract infor-mation within one frame, i.e. improving 2.26%for object-relation on AGQA-v2 and 3.34% for de-scriptive on NExT-QA. These results demonstrateour global semantics signal can address the chal-lenging long-range temporal reasoning problemsof long-form videoQA.Remarkably, existing methods demonstrate sig-nificantly low performance on our curated datasets.For example, MIST-CLIP only achieves 29.73%on Ego-QA, and 17.15% accuracy on MAD-QA,which is less than random chance. In contrast,humans obtain 80.29% and 73.21% accuracy onEgo-QA and MAD-QA, respectively. These resultssuggest that previous methods might not encom-pass sufficient information in their selected seg-ments and visual regions. Conversely, with the in-tegrated global information, our framework can en-hance videoQA performance on these challengingdatasets. However, the accuracy remains substan-tially below human performance. Future researchshould focus more on genuine long-form videoQA,where videos can extend to several hours.",
  "Ablation Study": "Gated SSL implementation. We explore the ef-fect of our gated SSL in on NExT-QA,STAR, Ego-QA, and MAD-QA datasets. As canbe observed, removing the gating unit, i.e. the SSLapproach, results in performance drops, since re-dundant and noisy information might be passed tothe visual representations. Additionally, not initial-izing state space parameters as diagonal matrices,i.e. non-diag SSL, does not remarkably impact the performance. However, the time and memorycomplexity would become O(L2), which is signifi-cantly more costly than our initialization approach.Choices for Global Semantics. We compare gatedSSL with other choices to extract global seman-tics among visual elements, i.e. self-attention andconvolution, in terms of videoQA performance in and GPU memory cost in . Ascan be observed, our gated SSL not only bringsless computational cost than self-attention but alsohigher accuracy, validating the effectiveness of itsglobal information signal. Moreover, whereas con-volution pays equivalent computational cost to ourgated SSL, its local pattern does not provide pro-ductive contextual information among visual ele-ments, resulting in lower accuracy than gated SSL.Effect of Gating Unit. We ablate the gating unitand vary the gating dimension dgating in our gatedSSL. As shown in , increasing the gatingdimension leads to higher videoQA accuracy, asmodel has more controllability towards global in-formation into visual representations. However,when the gating dimension becomes larger, the per-formance saturates and deteriorates. We posit thatthe model might become more constrained to al-low the encoding of global semantics, degeneratingto the architecture with limited global semantics.Apparently, removing gating unit results in perfor-mance drops, because irrelevant global informationfor the question could flow into visual hidden stateswithout model controllability.Effect of C3 objective. We compare our C3 align-ment objective with alternative approaches, i.e. op-timal transport (OT) (Pramanick et al., 2022) andits partial variant (POT) (Chapel et al., 2020). Asshown in , both OT and POT can polishvideoQA performance, while C3 yields the highestperformance. This shows that compositional con-sistency is important for long-form videoQA, sincethe model needs to grasp the relations among enti-ties, specifically those specified by the question.Position of State Space Layer. We replace thepenultimate multi-modal attention with SSL. Asshown in , such design choice deterio-rates the videoQA performance. The reason mightbe that since the frames and regions have alreadybeen selected, limited global information can beextracted from the video. Moreover, SSL does notexplicitly calculate dependency between tokens,thus producing little refined representations to com-pute the final answer, which has been observed byprevious work (Zuo et al., 2022). This substansti-",
  "Qualitative Results": "We visualize videoQA cases in . As canbe observed, our model can choose the correct an-swer for questions that require information over thevideo with a limited number of video segments. Weposit that due to our integrated global semantics,visual representations not only encode informationof the selected segments but also the video context,thus furnish the model with sufficient cues to ascer-tain the correct answer. In contrast, constrained tothe selected segments, previous state-of-the-art, i.e.MIST-CLIP (Gao et al., 2023), struggles in suchquestions and produces the incorrect output.",
  "Conclusion": "We introduce a Gated State space Multi-modalTransformer (GSMT) with a state space layer (SSL)to integrate global semantics of video into visualrepresentations to tackle long-form videoQA. Wefurther incorporate a gating unit to provide morecontrollability over the integrated global semanticsand a cross-modal compositional congruence (C3)objective to encourage the semantics aligned withthe question. To comprehensively evaluate long-form videoQA, we curate two long-form videoQAdatasets with excessively long video lengths andlong-natured questions. Extensive experiments onthese and standard datasets validate the superiorityof our framework.",
  "Our proposed framework has achieved promisingimprovement by integrating productive global con-text of long videos for long-form videoQA, but weconsider the following limitations as future work:": "Improve the generalizability of videoQA sys-tems: Long videos exhibit diverse content, whichmake it unlikely that the model will encounterinputs of the same distribution during inference.Since training different models for different set-tings is computationally costly, it is desirable toconstruct a long-form videoQA model that cangeneralize to myriad content. Extend our datasets to multicultural settings:Many long videos from different nations withdistinct backgrounds are prevalent in the Inter-net. Therefore, to increase the usefulness of long-form videoQA model, there is a need to construct",
  "Acknowledgement": "This research/project is supported by the NationalResearch Foundation, Singapore under its AI Sin-gapore Programme (AISG Award No: AISG3-PhD-2023-08-051T). Thong Nguyen is supported by aGoogle Ph.D. Fellowship in Natural Language Pro-cessing. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in neuralinformation processing systems, 35:2371623736. Max Bain, Arsha Nagrani, Gl Varol, and Andrew Zis-serman. 2021. Frozen in time: A joint video andimage encoder for end-to-end retrieval. In Proceed-ings of the IEEE/CVF International Conference onComputer Vision, pages 17281738. Shyamal Buch, Cristbal Eyzaguirre, Adrien Gaidon,Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. 2022.Revisiting the\" video\" in video-language understand-ing. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages29172927.",
  "Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 50465055": "Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen.2021. Env-qa: A video question answering bench-mark for comprehensive understanding of dynamicenvironments. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages16751685. Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang,and Mike Zheng Shou. 2023. Mist: Multi-modaliterative spatial-temporal transformer for long-formvideo question answering. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1477314783. Kristen Grauman, Andrew Westbury, Eugene Byrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar,Jackson Hamburger, Hao Jiang, Miao Liu, XingyuLiu, et al. 2022. Ego4d: Around the world in 3,000hours of egocentric video. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1899519012. Madeleine Grunde-McLaughlin, Ranjay Krishna, andManeesh Agrawala. 2021. Agqa: A benchmark forcompositional spatio-temporal reasoning. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1128711297.",
  "Md Mohaiminul Islam, Ngan Ho, Xitong Yang, TusharNagarajan, Lorenzo Torresani, and Gedas Bertasius.2024. Video recap: Recursive captioning of hour-long videos. arXiv preprint arXiv:2402.13250": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. InProceedings of the IEEE conference on computervision and pattern recognition, pages 27582766. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, ZaranaParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, ZhenLi, and Tom Duerig. 2021. Scaling up visual andvision-language representation learning with noisytext supervision. In International conference on ma-chine learning, pages 49044916. PMLR. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-derBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke,Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. 2017.Ai2-thor: An interactive 3d environment for visual ai.arXiv preprint arXiv:1712.05474. Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg.2018. Tvqa: Localized, compositional video ques-tion answering. In Proceedings of the 2018 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 13691379. Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.2020. Tvqa+: Spatio-temporal grounding for videoquestion answering. In Proceedings of the 58th An-nual Meeting of the Association for ComputationalLinguistics, pages 82118225.",
  "Karttikeya Mangalam, Raiymbek Akshulakov, and Ji-tendra Malik. 2023. Egoschema: A diagnostic bench-mark for very long-form video language understand-ing. arXiv preprint arXiv:2308.09126": "Cong-Duy Nguyen, Thong Nguyen, Duc Vu, and AnhLuu. 2023a. Improving multimodal sentiment anal-ysis: Supervised angular margin-based contrastivelearning for enhanced fusion representation. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 1471414724. Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, andAnh Tuan Luu. 2024a. Kdmcse: Knowledge distilla-tion multimodal sentence embeddings with adaptiveangular margin contrastive learning. arXiv preprintarXiv:2403.17486. Cong-Duy Nguyen, The-Anh Vu-Le, Thong Nguyen,Tho Quan, and Anh-Tuan Luu. 2023b. Expand bertrepresentation with visual information via groundedlanguage learning with multimodal partial alignment.In Proceedings of the 31st ACM International Con-ference on Multimedia, pages 56655673. Thong Nguyen, Yi Bin, Xiaobao Wu, Xinshuai Dong,Zhiyuan Hu, Khoi Le, Cong-Duy Nguyen, See-KiongNg, and Luu Anh Tuan. 2024b.Meta-optimizedangular margin contrastive framework for video-language representation learning.arXiv preprintarXiv:2407.03788. Thong Nguyen, Yi Bin, Junbin Xiao, Leigang Qu,Yicong Li, Jay Zhangjie Wu, Cong-Duy Nguyen,See-Kiong Ng, and Luu Anh Tuan. 2024c. Video-language understanding: A survey from model archi-tecture, model training, and data perspectives. arXivpreprint arXiv:2406.05615.",
  "Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu,See-KiongNg,andAnhTuanLuu.2022a.Vision-and-language pretraining.arXiv preprintarXiv:2207.01772": "Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Khoi MLe, Zhiyuan Hu, Cong-Duy Nguyen, See-KiongNg, and Anh Tuan Luu. 2024d.Read-pvla: Re-current adapter with partial video-language align-ment for parameter-efficient transfer learning in low-resource video-language modeling. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 38, pages 1882418832. Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Anh TuanLuu, Cong-Duy Nguyen, Zhen Hai, and Lidong Bing.2023c. Gradient-boosted decision tree for listwisecontext model in multimodal review helpfulness pre-diction. arXiv preprint arXiv:2305.12678. Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy Nguyen, See Kiong Ng, and Anh Luu. 2023d.Demaformer: Damped exponential moving averagetransformer with energy-based modeling for temporallanguage grounding. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages36353649. Thong Nguyen, Xiaobao Wu, Anh-Tuan Luu, Cong-Duy Nguyen, Zhen Hai, and Lidong Bing. 2022b.Adaptive contrastive learning on multimodal trans-former for review helpfulness predictions.arXivpreprint arXiv:2211.03524. Thong Thanh Nguyen and Anh Tuan Luu. 2022. Im-proving neural cross-lingual abstractive summariza-tion via employing optimal transport distance forknowledge distillation. In Proceedings of the AAAIConference on Artificial Intelligence, volume 36,pages 1110311111.",
  "Luu. 2024e. Topic modeling as multi-objective op-timization with setwise contrastive learning. In TheTwelfth International Conference on Learning Repre-sentations": "Rohan Pandey, Rulin Shao, Paul Pu Liang, RuslanSalakhutdinov, and Louis-Philippe Morency. 2022.Cross-modal attention congruence regularization forvision-language relation alignment. arXiv preprintarXiv:2212.10549. Shraman Pramanick, Aniket Roy, and Vishal M Patel.2022. Multimodal learning using optimal transportfor sarcasm and humor detection. In Proceedings ofthe IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 39303940. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International Con-ference on Machine Learning, pages 87488763.",
  "Shuhuai Ren, Junyang Lin, Guangxiang Zhao, RuiMen, An Yang, Jingren Zhou, Xu Sun, and HongxiaYang. 2021.Learning relation alignment forcalibrated cross-modal retrieval.arXiv preprintarXiv:2105.13868": "Mattia Soldan, Alejandro Pardo, Juan Len Alczar,Fabian Caba, Chen Zhao, Silvio Giancola, andBernard Ghanem. 2022. Mad: A scalable datasetfor language grounding in videos from movie audiodescriptions. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 50265035. Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,Antonio Torralba, Raquel Urtasun, and Sanja Fidler.2016. Movieqa: Understanding stories in moviesthrough question-answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 46314640. Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge,Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin,Guanyu Cai, Jianping Wu, Ying Shan, et al. 2023.All in one: Exploring unified video-language pre-training. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition,pages 65986608. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenen-baum, and Chuang Gan. 2021. Star: A benchmarkfor situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track (Round 2).",
  "Chao-Yuan Wu and Philipp Krahenbuhl. 2021. Towardslong-form video understanding. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 18841894": "Xiaobao Wu, Xinshuai Dong, Thong Nguyen, ChaoqunLiu, Liang-Ming Pan, and Anh Tuan Luu. 2023a. In-foctm: A mutual information maximization perspec-tive of cross-lingual topic modeling. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 37, pages 1376313771. Xiaobao Wu, Xinshuai Dong, Thong Thanh Nguyen,and Anh Tuan Luu. 2023b. Effective neural topicmodeling with embedding clustering regularization.In International Conference on Machine Learning,pages 3733537357. PMLR. Xiaobao Wu, Xinshuai Dong, Liangming Pan, ThongNguyen, and Anh Tuan Luu. 2024a.Modelingdynamic topics in chain-free fashion by evolution-tracking contrastive learning and unassociated wordexclusion. arXiv preprint arXiv:2405.17957.",
  "Xiaobao Wu, Thong Nguyen, Delvin Ce Zhang,William Yang Wang, and Anh Tuan Luu. 2024c.Fastopic:A fast, adaptive, stable, and transfer-able topic modeling paradigm.arXiv preprintarXiv:2405.17978": "Xiaobao Wu, Fengjun Pan, Thong Nguyen, YichaoFeng,Chaoqun Liu,Cong-Duy Nguyen,andAnh Tuan Luu. 2024d. On the affinity, rationality,and diversity of hierarchical topic modeling. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence, volume 38, pages 1926119269. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-SengChua. 2021.Next-qa: Next phase of question-answering to explaining temporal actions. In Pro-ceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 97779786. Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, WeiJi, and Tat-Seng Chua. 2022. Video as conditionalgraph hierarchy for multi-granular question answer-ing. In Proceedings of the AAAI Conference on Arti-ficial Intelligence, volume 36, pages 28042812.",
  "Junbin Xiao, Pan Zhou, Angela Yao, Yicong Li,Richang Hong, Shuicheng Yan, and Tat-SengChua. 2023.Contrastive video question answer-ing via video graph transformer.arXiv preprintarXiv:2302.13668": "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, HanwangZhang, Xiangnan He, and Yueting Zhuang. 2017.Video question answering via gradually refined atten-tion over appearance and motion. In Proceedings ofthe 25th ACM international conference on Multime-dia, pages 16451653. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,and Cordelia Schmid. 2021. Just ask: Learning toanswer questions from millions of narrated videos.In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 16861697. Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-pati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.Merlot reserve: Neural script knowledge throughvision and language and sound. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1637516387. Zhou Zhao, Zhu Zhang, Shuwen Xiao, Zhou Yu, JunYu, Deng Cai, Fei Wu, and Yueting Zhuang. 2018.Open-ended long-form video question answering viaadaptive hierarchical reinforced networks. In IJCAI,volume 2, page 8. Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles,Eren Manavoglu, Tuo Zhao, and Jianfeng Gao.2022. Efficient long sequence modeling via statespace augmented transformer.arXiv preprintarXiv:2212.08136.",
  "A.1Question prompt": "I want you to act as a teacher in the class called Long-term video understanding. I will providevideo narrations describing events in the time order of the video and you will generate highlydifficult and diverse questions for your students about the high-level details in the video. Youwant to test students following abilities: Ability 1: Students ability to summarize and compare long parts of the videoAbility 2: Students ability to compress information from the video rather than just listing theactions that happened in the video.Ability 3: Students ability to identify the most important parts of the video. Your questions should not mention any particular timestamps or narrations. Remember to make sure thecorrect answers to your questions do not list information from the narrations but compress themin a concise conclusion.",
  "A.2Answer prompt": "I want you to act as a teacher in the class called Long-term video understanding. I will providevideo action narrations and highly difficult and diverse questions for your students about thehigh-level details in the video. I want you to test students following abilities: Ability 1: Students ability to summarize and compare long parts of the videoAbility 2: Students ability to compress information from the video rather than justlisting the actions that happened in the video.Ability 3: Students ability to identify the most important parts of the video. I want you to create a difficult multiple-choice exam that tests above student abilities based on thequestions I just provided. Each question should have five similar open-ended but short answers,but only one should be correct. Make it very difficult for students to find the correct answeramong all the wrong answers. All answers should be closely related to what happens in the video.Make wrong answers significantly longer than correct answers. Ensure all of the correct answerscompress information from narrations them into a concise conclusion. Your answers should notmention any particular timestamps or narrations. Do not use letters for the answer choicesPrint each correct answer exactly as \"Correct answer: [full answer]\"Please print each wrong answer on a new line and print each wrong answer as \"Wrong answer 1,2,3,4: [full answer]\"",
  "The annotation we need is to say that the Question-correct answer-wrong answer set (the whole set) isgood if all these three conditions pass:": "(Condition A) Question is Answerable: The question can be answered from the video and requires morethan just a second of video to answer (so, if the answer is not present in the video or, if theanswer can be formed with just a few frames (less than say, a second) then it fails thiscondition).",
  "(Condition B) The Marked Correct Answer is correct: The \"\"correct answer\"\" is more the correct answerto the question": "(Condition C) The Marked Wrong Answers are wrong: All 4 \"\"wrong answers\"\" are less correct than the\"\"correct answer\"\" (So for example, if a wrong answer is not completely false, but simply doesnot contain all the information that the \"\" correct answer\"\" does, then it is still a fine \"\"wrong answer\"\") IF even one of the marked answer is correct, the set should be labeled as bad. (Condition D) The question is actually long-term: This is a very very important condition. We wantthe certificate for the question to be at least 30 seconds minimum. If the certificate is non-contiguous (I.e. 5 seconds at one place, 20 seconds at another, and 15 more seconds at a thirdplace) the sum of lengths of all the sub-certificates together should be more than 30 seconds.Another example is, if a question can be answered simply from a few frames of the video, thecertificate is small (and less than 30 seconds) and hence would fail this condition. Additionaldetails on how to handle certificate edge cases are provided in the annotator training throughexamples.",
  "If any of these five conditions fail we want the whole set (Question / Correct Answer / Wrong Answer)marked bad": "Optional:Since GOOD sets are so rare, in cases where it seems that a set is good but a smallpart of the above five conditions is not being met or, if one/two words weredifferent this can be a good set, please label as MAYBE and we will fix it inthe second round. Extended notes:1. In our experience, the wrong answers are made such that they differ from the correct answer insmall but crucially meaningful ways. There are many cases where a careless reading of the wronganswer might make it seem that it is correct but upon careful inspection, it will become clearthat something about the wrong answer indeed makes it wrong. While this is common, there areindeed cases where the wrong answer is also just as correct as the correct answer. In such cases,the wrong answer fails condition C, and the set is bad. 2.Roughly speaking, we expect about 20-25% of the questions that we have provided to be found as good. However, this is not necessary and the percentage can be smaller or larger depending on eachthree-minute clip.",
  ". Edge Cases:": "1. If the asked question has multiple answers and at least one of them aligns with the correct answerwhile none of them align with any of the other wrong answers, then provided that the top 5conditions are met, we can mark the set as good. 2. If two questions are very similar (even within different clips) and both are GOOD, only choose oneas GOOD and reject the other one with a comment mentioning this. We do not expect this tohappen more than 1 or 2 times in a 100.",
  "Question: Considering the sequence of activities performed, what can be inferred about the primary purpose of the video?": "0. to showcase a comprehensive home workout routine that incorporates various exercises1. to demonstrate the correct usage and maintenance of workout equipment at home2. to compare the effectiveness of different exercise techniques for physical fitness3. to highlight the role of technology and mobile devices in enhancing workout efficiency4. to emphasize the importance of rest and recovery periods during a workout session",
  "Question: Considering the characters' various activities with electronic devices, books, and games, what can be inferred about the video'smessage on leisure and entertainment?": "0. it critiques the overwhelming presence of electronic devices in daily life1. it celebrates the diversity of activities available for leisure and entertainment2. it underscores the necessity of balancing technology use with traditional activities3. it contrasts the solitary activities against group interactions as forms of leisure4. it shows a progression from individual to group activities, highlighting the importance of community",
  "Question: what if the brunette woman did not share a smile with the man at the government building?": "0. man would have still rushed to the hospital, but for a different reason1. man and woman would not have started a passionate romance at the government building2. brunette woman would have been perfectly content without sharing a smile with the man3. couple might not have formed a relationship, and the man would not have rushed to the hospital after the accident4. lack of a shared smile would have had no impact on their future interactions",
  "EDataset Statistics": "In this appendix, we provide the total number of questions and videos, and the average number of narrationsentences per video in . We also visualize the distribution of video lengths of datasets widelyused for long-form videoQA, i.e. NExT-QA (Xiao et al., 2021), STAR (Wu et al., 2021), EgoSchema(Mangalam et al., 2023), and our curated datasets MAD-QA and Ego-QA in . In addition, weshow the distribution of temporal certificate lengths, i.e. video lengths humans need to watch to determinethe answer, of standard long-form videoQA and our datasets in . As can be observed, our datasetsexhibit longer video input length and also certificate length to verify the correct answer, validating theireffectiveness to evaluate long-form videoQA performance."
}