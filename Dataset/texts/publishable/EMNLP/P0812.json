{
  "Abstract": "Although existing fashionable generation meth-ods on Incomplete Utterance Rewriting (IUR)can generate coherent utterances, they often re-sult in the inclusion of irrelevant and redundanttokens in rewritten utterances due to their inabil-ity to focus on critical tokens in dialogue con-text. Furthermore, the limited size of the train-ing datasets also contributes to the insufficienttraining of the IUR model. To address the firstissue, we propose a multi-task learning frame-work EO-IUR (Editing Operation-guided In-complete Utterance Rewriting) that introducesthe editing operation labels generated by se-quence labeling module to guide generationmodel to focus on critical tokens. Furthermore,we introduce a token-level heterogeneous graphto represent dialogues.To address the sec-ond issue, we propose a two-dimensional ut-terance augmentation strategy, namely editingoperation-based incomplete utterance augmen-tation and LLM-based historical utterance aug-mentation. The experimental results on threedatasets demonstrate that our EO-IUR outper-forms previous state-of-the-art (SOTA) base-lines in both open-domain and task-orienteddialogue. The code will be available at",
  "Introduction": "To express concisely and conveniently in multi-turn dialogues, speakers tend to use incompleteutterances which usually omit (i.e., ellipsis) or re-fer back (i.e., coreference) to the history of dia-logue context. Su et al. (2019) reported that morethan 70% of utterances exhibited the phenomenaof coreference and ellipsis, particularly in pro-droplanguages like Chinese. This presents a significantchallenge for the application of natural languageunderstanding, particularly in the context of virtualassistants and customer support systems (Hauswaldet al., 2015; Debnath et al., 2018). To address this",
  ": An example of IUR. The first two utterancesare historical utterances, u3 is an incomplete utterance,and u3 is the rewritten utterance": "issue, the task of Incomplete Utterance Rewriting(IUR) is proposed to generate complete utterancesthat can be understood by AI systems without anyadditional context. In particular, IUR is used toperform ellipsis and coreference resolution in di-alogues. Taking as an example, He inthe utterance u3 refers to Tolstoy in the historicalutterances and Anna Karenina is omitted in u3.Hence, the rewritten utterance of u3 would be u3. Most current IUR models use generation (Suet al., 2019; Zhou et al., 2019; Xu et al., 2020)or sequence labeling paradigms (Jin et al., 2022;Si et al., 2022; Chen, 2023; Du et al., 2023; Liet al., 2023a). Each paradigm has its own set ofadvantages and disadvantages. Sequence labelingmethods commonly suffer from two main issues:grammatical errors and incomplete rewritten utter-ances. These issues arise due to missing relative in-sertion positions when multiple tokens are insertedinto one position and imbalanced positive and neg-ative samples (i.e., tokens that do not need to bemodified or inserted), respectively. The generationmethods suffer from generating redundant tokensthat are irrelevant to the dialogue context due to thelack of focus on critical tokens in dialogue context(e.g., Anna Karenina and Tolstoy in u1 andu2 ). Only a few methods (Huang et al., 2021; In-oue et al., 2022) attempt to combine the above twomethods to overcome their respective shortcomings.However, they merely utilise a multi-task learningframework to combine sequence labeling and gen- eration methods. This approach lacks direct interac-tion between the two methods, in which sequencelabeling is unable to effectively guide generationto focus on the critical tokens in dialogue contextin order to avoid generating redundant tokens.Furthermore, the majority of annotated IURdatasets are relatively small in size (e.g., the sizeof CQR (Regan et al., 2019) is only 0.64K). De-spite the existence of numerous data augmentationmethods in NLP, they are designed for ordinarydocuments. The structure of dialogue is more com-plex and personal pronouns appear with greaterfrequency. However, there is no research on dataaugmentation in IUR.To address the first issue, we propose amulti-task learning framework, EO-IUR (EditingOperation-guided Incomplete Utterance Rewrit-ing), that incorporates editing operation labels gen-erated by the sequence labeling model to guide thegeneration model. This allows the decoder of thegeneration model to focus on the critical tokensin historical utterances and incomplete utterances,with the use of four types of defined editing oper-ations. Furthermore, we introduce a token-levelheterogeneous graph to represent dialogues, whichenables the model to learn the syntactic structurecorresponding to the omitted tokens and the rela-tionships between coreference-related tokens.To address the second issue, we propose a two-dimensional utterance augmentation strategy forIUR, namely editing operation-based incompleteutterance augmentation and LLM-based historicalutterance augmentation. These are used to augmentincomplete utterances and the historical utterances,respectively. The experimental results on two Chi-nese datasets and one English dataset show that ourproposed model outperforms several SOTA base-lines significantly.",
  "Related Work": "Sequence Labeling Methods A rewritten utter-ance often maintains a high degree of structural andcontent similarity with the corresponding incom-plete utterance to be rewritten. Therefore, most ofthe generated content comes from the context ofthe dialogue, leading to the emergence of numeroussequence labeling-based methods. To address theissue of low coverage during IUR using sequencelabeling, Jin et al. (2022) proposed a hierarchicalcontext marker to address this issue. Si et al. (2022)explicitly introduced semantic structured informa- tion through carefully designed inquiry templates.To better extract information from the dialoguecontext, Chen (2023) identified spans in the con-text and their order, and then combined them intorewritten utterance. Du et al. (2023) proposed amulti-granularity information capturing frameworkfor incomplete utterance rewriting. Li et al. (2023a)used a single-layer MLP architecture to mine latentsemantic information for IUR tasks.Generation MethodsThe emergence of pre-trained generation models has prompted previousstudies to investigate the potential of generationmodels in IUR. Su et al. (2019) enhanced the Trans-former framework (Vaswani et al., 2017) by incor-porating pointer networks, which allow for gener-ating utterances by copying tokens from either thedialogue history or the current utterance. Zhou et al.(2019) introduced a pretraining approach usingpseudo-parallel data and subsequently employedreinforcement learning to optimize the reward forgenerating final answers. Xu et al. (2020) intro-duced semantic role labeling into the generationprocess of IUR to provide additional information.Combination of Generation and Sequence La-beling Methods Only a few studies have focusedon the combination of generation and sequence la-beling methods. Huang et al. (2021) set the stateof each token in the incomplete utterance to be re-served, replaced, or deleted. However, they did notconsider the state of tokens in the dialogue history.Inoue et al. (2022) only focused on missing con-tent in the current utterance without consideringits position. Furthermore, although they performedjoint training for sequence labeling and generation,they did not consider the intrinsic connection be-tween these two paradigms and treated them asindependent. Our proposed method differs fromthe aforementioned approachs in that it incorpo-rates a multi-task learning framework, which en-ables the direct interaction between the sequencelabeling and generation methods. This frameworkincorporates editing operation labels generated bysequence labeling module, guiding the generationmodel and allowing the decoder to focus on thecritical tokens in dialogue context.",
  "Speaker-utterance edgeIntra-utterance edgeInter-utterance edgePseudo-coreference edge": ": Overview of our model EO-IUR, which includes utterance augmentation, construction of token-levelheterogeneous graph convolutional neural network, editing operation labeling, and editing operation-guided IUR. plete utterance, and T = {wj}Lj=1 containing Ltokens is the rewritten utterance. Providing D asinput, the task of IUR is to generate the rewritten ut-terance T by modeling the conditional probabilitydistribution P(T |D).We view IUR here as an end-to-end editingoperation-guided generation task and its overviewis shown in . Given a dialogue, we first en-code the dialogue context using the encoder of ageneration model ( 3.2), and then we construct atoken-level heterogeneous dialogue graph ( 3.3)to merge the four types of information: syntax, ut-terance, speaker, and coreference. Subsequently,we propose a multi-task learning framework, inwhich we introduce a sequence labeling moduleto label editing operations of the tokens in the di-alogue ( 3.4). Finally, we use the labels of edit-ing operations to guide rewritten utterance gener-ation. Furthermore, in order to address the issueof limited data size and short utterances, we pro-pose two strategies for augmenting the existingutterances: editing operation-based utterance aug-mentation and LLM-based historical utterance aug-mentation. The former is designed for incompleteutterances, while the latter is intended for historicalutterances ( 3.5).",
  "The encoder of BARTbase (Lewis et al., 2020) isemployed to extract features from dialogue utter-ances, which are used to initialize node embed-": "dings in our subsequent graph convolutional net-work (GCN). Specifically, for the i-th utterance,we add a special marker [Speakeri] in front ofit to represent the speaker, resulting in an inputformat of {[Speaker1] s1 [Speaker2] s2,...} torepresent the dialogue D. Subsequently, the out-put of the final layer of the encoder is extractedas the feature representation Henc RKdu of di-alogue D, where K denotes the number of inputtokens and du represents the dimension of featurerepresentations.",
  "Dialogue as a Heterogeneous Graph": "IUR involves the resolution of coreference and el-lipsis, in which syntactic structures (e.g., the omis-sion of subjects and predicates) serve as criticalcues for the comprehension of the relations amongtokens. However, most of the previous work (Haoet al., 2021; Chen, 2023) on IUR directly usedBERT (Devlin et al., 2019) as the encoder to extractdialogue features. BERT is primarily pre-trainedon textual corpora and is therefore difficult to cap-ture the intricate structural information in dialogues.To facilitate the learning of the syntactic structurecorresponding to the omitted tokens and relationsamong coreference-related tokens in dialogues, weintroduce graph neural networks to further enhancedialogue information.First, we use the dependency parsing tool spacy1",
  "to obtain a syntactic tree representation of each ut-": "terance in dialogues. The graph is represented asG = (V, E, R). Here the node set V = VuttrVspk,where Vuttr and Vspk represent the token nodes andspeaker nodes respectively; E is the set of edgesconnecting vertices in V; and R denotes the setof relational types of edges as defined below. 1)Intra-utterance edge: Given the significance ofsyntactic information for IUR, for each token inan utterance, we establish connections between ad-jacent nodes on the corresponding syntax tree. 2)Inter-utterance edge: To integrate the informationfrom each utterance with others, we connect theroot nodes of adjacent utterances syntax trees withedges. 3) Speaker-utterance edge: To integrate thespeaker information, we connect each speakerstag to the root nodes of their corresponding syntaxtrees. 4) Pseudo-coreference edge: Coreferenceinformation is crucial for IUR. However, previouswork for coreference resolution has been unableto accurately identify all coreference cases. There-fore, here we connect all pronouns in the incom-plete utterance with pronouns and nouns in all otherutterances.Subsequently, we utilise I-layer GCNs to aggre-gate the features of neighbouring nodes. The initialrepresentation of each node is the output of its cor-responding token in the encoder. Given the nodei in the layer l, the graph convolution operation isdefined as follows,",
  "(1)": "where R is the set of different types of edges men-tioned above, W (l)r Rdudu and b(l)r Rdu aretrainable parameters, Nr(i) denotes the neighborsconnected to the node i via the r-th type of edge,and represents the activation function.For each nodes features, we average them withthe corresponding representations generated by theencoder to obtain the final representation Henci=sIi Henci",
  "Editing Operation-guided IUR": "As mentioned in introduction, IUR can be specif-ically categorized into two distinct operations:coreference resolution and ellipsis resolution. Al-though existing pre-trained generation modelsdemonstrate strong generation capabilities, they donot explicitly consider the two distinct operations of replacement (for coreference) and insertion (forellipsis) in IUR (Su et al., 2019; Xu et al., 2020;Zhou et al., 2019). Consequently, existing gen-eration models frequently fail to attend to crucialinformation present in dialogue context, such asentities, and may also generate redundant tokensthat are unrelated to the dialogue.To address the aforementioned issue, we proposean editing operation-guided IUR method that fo-cuses the model on the critical tokens in dialoguecontext, thereby enabling the generation of context-related utterances. In particular, we first introduce asequence labeling module that generates the labelsof edit operations, which are then used to guide thefollowing utterance generation.Editing Operation Labeling During the processof IUR, only a very small amount of tokens indialogue context will be used, i.e., tokens involvingsubstitution and insertion. To make the model focusmore on these critical tokens, we propose a token-level sequence labeling task to generate the labelsof editing operations.The most common editing operations are inser-tion, deletion, and replacement. In IUR, there areonly two operations: insertion and replacement. Tothis end, we defined three labels: RP, NW, andIN to correspond to these two operations and NAto correspond to no operations. This is illustratedin . The labels RP and NW involvereplacement operations in coreference resolution,while IN corresponds to insertion operations inellipsis resolution.A sequence labeling module is introduced togenerate editing operations, with an illustrative ex-ample provided in . In this example, sincethe pronoun He refers to the entity Tolstoy,He is the token being replaced (corresponding tothe RP label), and Tolstoy is the token afterreplacement (corresponding to the NW label).Moreover, the entity Anna Karenina is the spanthat needs to be inserted (corresponding to the INlabel). At this point, we do not know the specificinsertion position.After the input dialogue context D is encodedand enhanced with graph neural network features,we obtain the contextual representation Henc. Wefeed this representation into a two-layer MLPwith tanh() activation function, and then pass itthrough a softmax layer to obtain the label proba-bility distribution for each token as follows,",
  "i=1yi log g( Henc, i)(3)": "where yi is the i-th label and |C| is the total number.Editing Operation-guided Utterance GenerationTo use the labels of editing operations to facilitatethe utterance generation at the decoder side, wedefine three types of key tokens, i.e., tokens cor-responding to the labels RP, NW, and IN.We hope that the decoder can pay more attention tothese tokens when generating rewritten utterances.To accomplish this, we modify the cross-attention layer between the encoder and decoder.This is done by multiplying the attention score byan influence factor, which is the sum of the proba-bilities that each input token belongs to the labelsRP, NW and IN, as shown below,",
  "attn<j,i> =exp((d + i) attn<j,i>)Kk=1 exp((d + k) attn<j,k>)(4)": "where 0 represents the MLP parameters corre-sponding to the label NA, attn<j,i> denotes theattention score of the j-th token of the output withrespect to the i-th token of the input. To prevent theattention scores from degrading to zero due to someof the influence factors i converging to zero, weadd a temperature coefficient d to them, which hasa smoothing effect. By scaling the cross-attention",
  "Utterance Augmentation": "The majority of existing IUR datasets are relativelysmall, which makes it challenging to train an ef-ficient model. Furthermore, colloquial dialoguesfrequently contain ellipsis and coreference, result-ing in brief and ambiguous utterances. To addressthe above two issues, we propose two data augmen-tation strategies to expand existing IUR datasets:editing operation-based incomplete utterance aug-mentation and LLM-based historical utterance aug-mentation.Editing Operation-based Incomplete UtteranceAugmentation In IUR, there are two cases involv-ing ellipsis resolution and coreference resolution.It is hypothesized that these two cases can be trans-formed into each other, and a strategy is proposedto augment training samples. The incomplete ut-terance is compared with the rewritten utterance toidentify the positions of ellipsis and coreference.In instances where ellipsis occurs, a pronoun isinserted to indicate coreference at that point. Con-versely, for instances of coreference, the referencetoken is deleted, thereby converting it into an ellip-sis. This approach enables bidirectional conversionbetween ellipsis and coreference.LLM-based Historical Utterance AugmentationWe also consider enhancing the dialogue history. To this end, we employ a large language model(LLM) to rewrite historical utterances without al-tering the contextual semantics. Here, we utilizethe method of in-context learning. Initially, sev-eral samples are generated through LLM in a zero-shot manner. Subsequently, the five samples withthe highest rewriting quality are manually selectedas examples for data augmentation purposes. Ap-pendix A and B respectively provide an exampleof data augmentation and the prompts used duringthe data augmentation process.",
  "Experimental Settings": "Datasets To comprehensively evaluate the effec-tiveness of our EO-IUR, following previous work(Si et al., 2022), we conducted evaluation on threepopular datasets: the Chinese open-domain dia-logue datasets REWRITE (Su et al., 2019) andRESTORATION-200K (RES200K) (Pan et al.,2019), and the English task-oriented dialoguedataset TASK (Quan et al., 2019). Specific statis-tics for the three datasets are provided in Ap-pendix C and the implementation details are listedin Appendix D. Evaluation MetricsSimilar to previous work(Chen, 2023; Li et al., 2023b), we use the fol-lowing metrics: 1) BLEUn (Papineni et al., 2002)measures the accuracy by calculating the degreeof matching of n-grams in the generated utter-ances and the reference utterances, and here we useBLEU-1 (B1), BLEU-2 (B2) and BLEU-4 (B4).2) ROUGEn (Lin, 2004) measures the degree ofoverlap of n-grams in the generated and referenceutterances, focusing on recall, and here we useROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L(RL). 3) Restoration F-scoren (Fn for short) (Panet al., 2019) measures how much missing infor-mation is added to the incomplete utterance, andhere we use F1, F2 and F3. 4) Exact Match (EM)measures how many generated utterances are com-pletely correct. Baselines We compare our model EO-IUR withthe following strong baselines: BARTbase (Lewiset al., 2020), RAST (Hao et al., 2021), SARG(Huang et al., 2021), HCT (Jin et al., 2022),QUEEN (Si et al., 2022), RAU (Zhang et al., 2022),SGT (Chen, 2023), MGIIF (Du et al., 2023) andMIUR (Li et al., 2023a).",
  "Experimental Results": "The experimental results are shown in Tables 4, 5and 6. Our EO-IUR is significantly better in com-parison with the best baselines in three datasets.Due to the different purposes of the three datasets,we report the different metrics on the respectivedatasets following previous work (Chen, 2023; Liet al., 2023a). The better performance on BLEUnand ROUGEn demonstrates that our EO-IUR iscapable of generating more accurate and less re-dundant utterances. In contrast to other text gener-ation tasks, the content of incomplete and rewrittenutterances in IUR is largely similar, making it chal-lenging to discern the effects of utterance rewritingthrough the use of BLEUn and ROUGEn. Conse-quently, the experimental results indicate that theperformance improvement of these two metrics isnot as significant as that observed in other metrics.In comparison with BLEUn and ROUGEn, ourEO-IUR achieves more significant improvementsin most Fn and EM, with the EM metric increas-ing by 9.2, 9.4, and 5.6 on the TASK, REWRITE,and RES200K datasets, respectively. The Fn andEM metrics focus on the completion of coreferen-tial and omitted information during the rewritingprocess. The substantial improvements achievedin these two types of metrics also demonstrate theeffectiveness of our EO-IUR. It is noteworthy thatthe significant improvement in EM indicates thatour EO-IUR can generate complete utterances.QUEEN and MGIIF also focused on capturingkey information, where the former employed man-ually designed rules to identify the pronouns to bereplaced and the ellipsis in incomplete utterances,while the latter focuses on utterance-level impor-tance information. In contrast to the aforemen-tioned methods, we introduce token-level editingoperations to measure the contribution of each to-ken. The experimental results in Tables 4, 5 and6 show that our EO-IUR outperforms them signif-icantly on all metrics, thereby substantiating theefficacy of our approach to selecting key tokens",
  "Analysis": "Due to the limited space available, we only reportedthe results on the TASK and the results are pre-sented in Tables 7. The results on the other twodatasets also demonstrated similar trends.Impact of Editing Operation-guided IUR Whenthe editing operation guidance is removed (i.e.,w/o ED guidance), all of the models metrics de-crease significantly, demonstrating that this guid-ance can enhance utterance generation. Further-more, when the entire editing operation labelingtask is removed directly (i.e., w/o Multi-task learn-ing), including the editing operation guidance,there is a greater decrease in model performance,indicating that training on the sequence labelingtask is also beneficial for utterance generation.To comprehensively assess the efficacy of theediting operation-guided IUR in preventing the gen-eration of redundant tokens during the rewriting process, we have collated the experimental resultson TASK. In this analysis, we define a redundanttoken as one that does not exist in the referenceutterances. Without the introduction of editing op-eration guidance to our model EO-IUR, 12.75%of the generated rewritten utterances contain re-dundant tokens. This figure is reduced to 8.25%after the incorporation of this guidance. This isdue to the fact that the labels generated by editingoperation labeling provide prior information to thegeneration model, allowing it to focus more on keytokens in dialogue context. In utterance rewriting, the use of labels to guidethe decoder is a crucial aspect of the process. Theselabels, which can be either soft or one-hot, serveas influencing factors that adjust attention scoresin the cross-attention layers. In certain instances,one-hot labels can be directly utilized as influenc-ing factors. As shown in , soft labels aredemonstrably superior to one-hot labels (i.e., One-hot label) as influencing factors. One potentialissue with using one-hot labels during training isthat it may lead to overfitting. Additionally, in-correctly predicting one-hot label during inferencecould affect the decoders generation process. An-other consideration is that, in addition to tokenscorresponding to the RP, NW, and IN la-bels, some tokens corresponding to the NA labelalso require attention during generation. There-fore, solely focusing on the former may preventthe model from considering the complete semantic Percentage(%) EO-IUR v.s. HCT EO-IUR v.s. BARTbase EO-IUR v.s. MIUR 71.3%22.0%6.7% 52.0%40.7%7.3% 49.3%44.0%6.7% WinTieLose",
  ": Human evaluation on REWRITE": "context of dialogues. Consequently, the utilizationof soft labels can facilitate the acquisition of morecomprehensive token information, effectively cir-cumventing the aforementioned issues.To assess the efficacy of our labeling approach,we directly combine RP, NW and IN into asingle label ED (i.e., Merge labels). As shownin Tables 7, our findings indicate that utilizingtwo labels results in a notable decline in perfor-mance. This phenomenon can be attributed to thefact that the four types of labels can provide morefine-grained information to distinguish differentediting operations. RP and NW correspondto the replacement operation in coreference res-olution, while IN corresponds to the insertionoperation in ellipsis resolution. These three labelscan distinguish the cases of coreference resolutionand ellipsis resolution in IUR.Impact of Utterance Augmentation We conductthe experiments of removing utterance augmenta-tion (i.e., w/o Utterance augmentation) and allmetrics drops in . The EM metric, whichmeasures the proportion of samples that are com-pletely correct after rewriting, demonstrates themost significant improvement after using data aug-mentation in comparison with the other metrics.However, existing methods may introduce minorerrors or redundant tokens during the rewriting pro-cess, resulting in a lower EM score. As previouslystated, the two data augmentation strategies em-ployed herein result in the generation of a greaternumber of training samples, thereby enhancing therobustness of the model and facilitating the avoid-ance of errors to the greatest extent possible.Impact of the Length of Incomplete Utterancesand the Number of Editing Operations Takingthe TASK dataset as an example, we evaluate theperformance of the model when the incompleteutterances are in different lengths, as shown in Ta-ble 8. It can be observed that the model demon-strates superior performance when the incomplete",
  ": Analysis of the correlation between modelperformance and the number of editing operations": "utterances are of shorter or longer duration. How-ever, it exhibits diminished performance when thelength of the incomplete utterances falls within theinterval and the interval . We ana-lyzed the data and found that most short utterances(such as \"Thank you. Goodbye.\") lacked meaning-ful content and most long utterances were gener-ally complete, which did not need to be rewritten.Incomplete utterances are primarily observed in ut-terances of intermediate length, specifically withinthe intervals and , which present agreater challenge.We also analyzed the correlation between thenumber of editing operations in the utterances andthe model performance, as shown in . It canbe observed that as the number of editing opera-tions increases, the models performance decreases.For example, when the number of editing opera-tions is 3, 4, and 5, the EM scores are 25.0, 20.0,and 0, respectively. How to address incompleteutterance rewriting in complex scenarios is alsoa research direction worthy of exploration in thefuture.We also analyze the impact of token-level het-erogeneous graph and provide the case study in theAppendix E and F.",
  ":Performance comparison to GPT-4 onREWRITE": "lines HCT, BARTbase and MIUR. A random sam-ple of 50 pieces of data from the REWRITE datasetwas distributed to three raters (graduate studentsin NLP), who were asked to select the result gen-erated by the two methods that was of superiorquality. As shown in , our method outper-forms the other three models in human evaluationsignificantly, further demonstrating its effective-ness. Furthermore, it is noteworthy that BARTbaseand our method exhibit comparable performance inmany instances, and the failure rate of our EO-IURis highest among the comparison with three models.Our observations indicated that while BARTbasegenerated numerous utterances that did not meetrewrite requirements, their fluency led to superiormanual evaluation results. In contrast, HCT andMIUR generated many utterances with grammati-cal errors due to the use of sequence labeling meth-ods, resulting in poor manual evaluation perfor-mance.",
  "Comparison with ChatGPT": "The majority of evaluations of LLMs currently fo-cus on single-turn dialogues, with minimal atten-tion paid to multi-turn dialogues. However, theIUR tasks necessitate a comprehensive grasp of theglobal and structural information present in multi-turn dialogues. To investigate the performance ofLLMs in IUR, we have conducted a comparativeanalysis between ChatGPT and our EO-IUR. Ourapproach employs the in-context learning approach,provides prompts and five examples, and utilisesGPT-4 (the version used is gpt-4-1106-preview)to generate rewritten utterances. We have providedthe prompt in Appendix G.The experimental results are shown in .Despite GPT-4 having a significantly greater num-ber of parameters than our EO-IUR, our model stillachieves superior performance under these condi-tions, particularly in terms of EM, which is almosttwice as much as GPT-4. According to our ob-servation, the results generated by GPT-4 sufferfrom \"under-rewriting\" and hallucination, that is,the rewriting is incomplete and some utterances aregenerated that is contradictory to the conversation",
  "Error Analysis": "To conduct a more in-depth study of our proposedmodel, we have statistically analyzed the exper-imental results and conducted an error analysis.Due to our use of the sum of the probabilities thateach input token belongs to the labels RP, NWand IN as an influence factor to guide the gen-eration of rewritten utterances, we speculate thatthe performance of editing operation labeling willgreatly affect the generation of rewritten utterances.Consequently, the performance of editing opera-tion labeling was evaluated using the REWRITEdataset as an illustrative example, with the EMmetric employed as the evaluation metric. Theresults revealed that the EM metric for editing op-eration labeling was 76.35, which was lower thanthat for rewritten utterance generation (79.9). Thisindicates that approximately a quarter of the gener-ated labels were incorrect, which has a significantimpact on subsequent utterance generation. Nev-ertheless, this also indicates that correct rewrittenutterances can still be generated even when somelabels in certain utterances are predicted incorrectly.It also demonstrates that the use of soft labels caneffectively alleviate the problem of model overcon-fidence and improve the robustness of our model.Conversely, other errors originate from utterancegeneration itself. Despite the guidance of the cor-rect labels, the utterance generation model still gen-erates incomplete utterances, utterances with gram-matical errors, utterances with redundant tokens.",
  "Conclusion": "In this paper, we introduce the editing operationlabels generated by the editing operation labelingto guide the generation model to focus on criticaltokens. Furthermore, we introduce a token-levelheterogeneous graph to learn the syntactic structureand the relationships among coreference-related to-kens. We also propose a two-dimensional utteranceaugmentation strategy to further boost the model.The experimental results on three datasets showthat our EO-IUR outperforms previous SOTA base-lines significantly. In the future, we intend to em-ploy large language models (LLMs) to facilitateutterance generation and to reduce the number ofredundant tokens in rewritten utterances.",
  "Limitations": "Although our proposed method effectively inte-grates the generation task with the classificationtask to leverage the strengths of both, there are stillsome drawbacks. Firstly, incorporating informa-tion from heterogeneous graphs of dialogues viagraph neural networks incurs additional computa-tional resources. On the other hand, in the processof using classification results to guide the genera-tion of rewritten utterances, there are errors in theclassification results, which may have a negativeimpact on generation. Therefore, it is worthwhileto explore how to efficiently integrate graph in-formation and better combine classification andgeneration in future research.",
  "Acknowledgements": "The authors would like to thank the three anony-mous reviewers for their comments on this paper.This research was supported by the National Natu-ral Science Foundation of China (Nos. 62276177and 62376181), and Project Funded by the PriorityAcademic Program Development of Jiangsu HigherEducation Institutions. Yunshan Chen. 2023. Incomplete utterance rewritingas sequential greedy tagging. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 72657276, Toronto, Canada. Association forComputational Linguistics.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. Bert: Pre-training of deepbidirectional transformers for language understand-ing. Preprint, arXiv:1810.04805": "Haowei Du, Dinghao Zhang, Chen Li, Yang Li, andDongyan Zhao. 2023. Multi-granularity informa-tion interaction framework for incomplete utterancerewriting. In Findings of the Association for Com-putational Linguistics: EMNLP 2023, pages 25762581, Singapore. Association for Computational Lin-guistics. Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, ZhaopengTu, and Dong Yu. 2021. RAST: Domain-robust dia-logue rewriting as sequence tagging. In Proceedingsof the 2021 Conference on Empirical Methods in Nat-ural Language Processing, pages 49134924, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Johann Hauswald, Michael A Laurenzano, YunqiZhang, Cheng Li, Austin Rovinski, Arjun Khu-rana, Ronald G Dreslinski, Trevor Mudge, ViniciusPetrucci, Lingjia Tang, et al. 2015. Sirius: An openend-to-end voice and vision personal assistant and itsimplications for future warehouse scale computers.In Proceedings of the Twentieth International Con-ference on Architectural Support for ProgrammingLanguages and Operating Systems, pages 223238. Mengzuo Huang, Feng Li, Wuhe Zou, and WeidongZhang. 2021. SARG: A novel semi autoregressivegenerator for multi-turn incomplete utterance restora-tion. In Thirty-Fifth AAAI Conference on ArtificialIntelligence, AAAI 2021, Thirty-Third Conferenceon Innovative Applications of Artificial Intelligence,IAAI 2021, The Eleventh Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2021, Vir-tual Event, February 2-9, 2021, pages 1305513063.AAAI Press. Shumpei Inoue, Tsungwei Liu, Son Nguyen, and Minh-Tien Nguyen. 2022. Enhance incomplete utterancerestoration by joint learning token extraction and textgeneration. In Proceedings of the 2022 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 31493158, Seattle, United States.Association for Computational Linguistics. Lisa Jin, Linfeng Song, Lifeng Jin, Dong Yu, and DanielGildea. 2022. Hierarchical context tagging for utter-ance rewriting.In Thirty-Sixth AAAI Conferenceon Artificial Intelligence, AAAI 2022, Thirty-FourthConference on Innovative Applications of ArtificialIntelligence, IAAI 2022, The Twelveth Symposiumon Educational Advances in Artificial Intelligence,EAAI 2022 Virtual Event, February 22 - March 1,2022, pages 1084910857. AAAI Press. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020.BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 78717880, Online. Association for Computa-tional Linguistics. Jiang Li, Xiangdong Su, Xinlan Ma, and Guanglai Gao.2023a. How well apply simple MLP to incomplete ut-terance rewriting? In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 15671576,Toronto, Canada. Association for Computational Lin-guistics. Zitong Li, Jiawei Li, Haifeng Tang, Kenny Zhu, andRuolan Yang. 2023b. Incomplete utterance rewritingby a two-phase locate-and-fill regime. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 27312745, Toronto, Canada. Associa-tion for Computational Linguistics.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Qian Liu, Bei Chen, Jian-Guang Lou, Bin Zhou, andDongmei Zhang. 2020. Incomplete utterance rewrit-ing as semantic segmentation. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 28462857,Online. Association for Computational Linguistics. Zhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou,and Xiaojiang Liu. 2019. Improving open-domaindialogue systems via multi-turn incomplete utterancerestoration. In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 18241833, Hong Kong, China. Associationfor Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Jun Quan, Deyi Xiong, Bonnie Webber, and ChangjianHu. 2019. GECOR: An end-to-end generative el-lipsis and co-reference resolution model for task-oriented dialogue. In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 45474557, Hong Kong, China. Associationfor Computational Linguistics. Michael Regan, Pushpendre Rastogi, Arpit Gupta,and Lambert Mathias. 2019. A dataset for resolv-ing referring expressions in spoken dialogue viacontextual query rewrites (cqr).arXiv preprintarXiv:1903.11783.",
  "Shuzheng Si, Shuang Zeng, and Baobao Chang. 2022": "Mining clues from incomplete utterance: A query-enhanced network for incomplete utterance rewriting.In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 48394847, Seattle, United States. Associationfor Computational Linguistics. Hui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Peng-wei Hu, Cheng Niu, and Jie Zhou. 2019. Improv-ing multi-turn dialogue modelling with utteranceReWriter. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 2231, Florence, Italy. Association for Com-putational Linguistics.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz": "Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Kun Xu, Haochen Tan, Linfeng Song, Han Wu, HaisongZhang, Linqi Song, and Dong Yu. 2020. SemanticRole Labeling Guided Multi-turn Dialogue ReWriter.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 66326639, Online. Association for Computa-tional Linguistics. Yong Zhang, Zhitao Li, Jianzong Wang, Ning Cheng,and Jing Xiao. 2022. Self-attention for incomplete ut-terance rewriting. In IEEE International Conferenceon Acoustics, Speech and Signal Processing, ICASSP2022, Virtual and Singapore, 23-27 May 2022, pages80478051. IEEE. Kun Zhou, Kai Zhang, Yu Wu, Shujie Liu, and Jing-song Yu. 2019. Unsupervised context rewriting foropen domain conversation. In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 18341844, Hong Kong,China. Association for Computational Linguistics.",
  "Prompt for historical utterance augmenta-tion": "Given a dialogue with utterances from dif-ferent speakers separated by semicolons,keep the last utterance unchanged, rewritethe historical utterances, and keep the se-mantics of the dialogue unchanged. Hereare some examples.Examples: {Examples}Input: {Input} where Examples are the five highest-quality ex-amples we have selected, and the Input is thehistorical utterances that needs to be augmented,accompanied by its corresponding incomplete utter-ance. To illustrate, the utterance Is there anythingfun in Qingdao? can be rewritten by LLM as Doyou know where is the best place to go in Qingdao?with explicit semantics in a dialogue about tourism.",
  "CDataset statistics": "The specific information of the three datasets isshown in , where Avg. Hist, Avg. Currand Avg. Rewr refer to the average numbers oftokens in the historical utterances, current utter-ances, and rewritten utterances, respectively. It canbe observed that the rate of replacement operationsin REWRITE is considerably higher (35.8%) thanin RES200K (0.15%) and TASK (12.4%). Addi-tionally, the average number of tokens in histori-cal utterances of TASK is significantly larger thanthose in the other two datasets.",
  "EImpact of Token-level HeterogeneousGraph": "When the token-level heterogeneous graph (i.e.,w/o heterogeneous graph) is removed, the BARTencoder is employed solely to represent dialogues.As illustrated in , the metrics demonstratea decline. This outcome indicates that the struc-tural information within dialogues can link thecoreference-related tokens in a dialogue and re-veal the ellipsis tokens in incomplete utterances,subsequently enhancing the performance of IUR.Additionally, experiments were conducted on fourtypes of edges, and the results demonstrated thatremoving any type of edges would result in a per-formance decrease. Specifically, removing intra-utterance edges, inter-utterance edges, speaker-utterance edges, and pseudo-coreference edges re-duced the EM metric by 1.4, 0.5, 0.2, and 0.9,respectively. These findings further substantiatethe effectiveness of these four types of edges.We observe that the removal of intra-utteranceedges and pseudo-coreference edges results in thelargest decrease relative to the removal of othertypes of edges, which highlights the importanceof these two types of edges. This is mainly due tothe fact that the intra-utterance edges incorporatesyntactic information well, allowing the model tobetter grasp what syntactic components are miss-ing in the incomplete utterance, which often corre-sponds to omitted content. Furthermore, pseudo-coreference edges can effectively fuse coreferenceinformation in the dialogue. Additionally, the pres-ence of numerous personal pronouns in the dia-logue allows for the use of such edges, which canenhance the models awareness of the coreferencerelations in the dialogue.",
  "FCase Study": "We conducted a case study to further explore theeffectiveness of our proposed method, as shown in. In the incomplete utterance, availablerestaurants refers to indian restaurants, and inthe north part of town is omitted. The phrase inthe north part of town is a prepositional phrasemodifying the two indian restaurants. In orderfor the model to correctly rewrite this utterance, it Dialogue contextSpeaker1: How about indian food in the north part of town instead?Speaker2: There are two indian restaurants in the north part of town . what price range are you looking for?Speaker1: What are the price ranges of the two available restaurants? (The incomplete utterance.)",
  "must first learn the coreference relation betweenavailable restaurants and indian restaurants, aswell as the syntactic structure that in the north partof town modifies the two indian restaurants": "We compared the results generated by MIUR,BARTbase and our proposed model.Only ourmethod generated a correct rewritten utterance,while MIUR generated an utterance with gram-matical error due to the inherent flaws of sequencelabeling methods that make it difficult to guaranteegrammatical correctness. Furthermore, MIUR alsodid not correctly identify the coreference relationbetween available restaurants and indian restau-rants. The utterance generated by BARTbase isincomplete and lacks the modifier in the northpart of town. Our method benefits from the edit-ing operation guidance and heterogeneous graphrepresentation of dialogue, enabling it to generatea correctly rewritten utterance. This is due to theability of our EO-IUR to not only learn the corefer-ence relation but also to capture the syntactic struc-ture. It is noteworthy that our EO-IUR correctlyidentifies the tags RP and NW for availableand Indian, respectively. This enables the modelto prioritize these tokens during the generation ofrewritten utterance.",
  "HSample Comparative Analysis betweenGPT-4 and EO-IUR": "Three examples are given in where GPT-4rewrites incorrectly but our method rewrites cor-rectly. For the first example, speaker A asks forKorean food, while GPT-4 rewrites it to ask for anexpensive restaurant, which we speculate is dueto the fact that GPT-4 overly relies on speaker Asfirst sentence (i.e., \"I am looking for an expensiverestaurant that serves welsh food.\"), resulting in theoutput of an incorrectly rewritten utterance. Forthe second example, speaker A asked for the phonenumber of nirala, while the utterance output byGPT-4 asked for the phone number of \"moderately"
}