{
  "Abstract": "While large visual-language models (LVLM)have shown promising results on traditional vi-sual question answering benchmarks, it is stillchallenging for them to answer complex VQAproblems which requires diverse world knowl-edge. Motivated by the research of retrieval-augmented generation in the field of naturallanguage processing, we use Dense PassageRetrieval (DPR) to retrieve related knowledgeto help the model answer questions. However,DPR conduct retrieving in natural languagespace, which may not ensure comprehensiveacquisition of image information. Thus, theretrieved knowledge is not truly conducive tohelping answer the question, affecting the per-formance of the overall system. To addressthis issue, we propose a novel framework thatleverages the visual-language model to selectthe key knowledge retrieved by DPR and an-swer questions. The framework consists of twomodules: Selector and Answerer, where bothare initialized by the LVLM and parameter-efficiently finetuned by self-bootstrapping: findkey knowledge in the retrieved knowledge doc-uments using the Selector, and then use themto finetune the Answerer to predict answers;obtain the pseudo-labels of key knowledge doc-uments based on the predictions of the An-swerer and weak supervision labels, and thenfinetune the Selector to select key knowledge;repeat. Our framework significantly enhancesthe performance of the baseline on the chal-lenging open-domain Knowledge-based VQAbenchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%.Our code ispublicly available at",
  "*Jing Liu is the corresponding author": "Dai et al., 2023). They usually use a mappingnetwork to inject visual features into the semanticspace of the large language model (Brown et al.,2020; Zhang et al., 2022; Touvron et al., 2023; vic,2023; Touvron et al., 2023) and demonstrate strongcapabilities on multimodal perception and reason-ing. Thus, they achieve significant progress in con-ventional visual question answering benchmarks(Antol et al., 2015; Goyal et al., 2017; Hudson andManning, 2019) which primarily focus on address-ing straightforward questions that only necessitatevisual perception and recognition. However, it isstill challenging for the LVLMs to answer visualquestions which require broader world knowledgeand common sense (Wang et al., 2017; Marinoet al., 2019; Schwenk et al., 2022).Motivatedbytheresearchofretrieval-augmented generation (Karpukhin et al., 2020a)in the field of natural language processing, weuse Dense Passage Retrieval (DPR) to retrieverelated world knowledge to help the modelanswer questions.However, when using DPR,we need to transform the image into texts toretrieve the related knowledge, which leads to theunderutilization of visual information. Thus, theretrieved knowledge may be unfaithful and affectsthe model performance. To address the issue, weconsider the LVLM as the knowledge selector tofind helpful knowledge from candidate retrievedknowledge by DPR. Then the selected knowledgeis fed into the LVLM to predict the answer.In this paper, we introduce a novel frameworkwhere we adopt the large visual-language modelto perform knowledge selection and question an-swering. Our framework comprises two modules:a Selector and an Answerer. We train two mod-ules by repeating the following process: the Se-lector first identifies important knowledge fromthe candidate knowledge documents retrieved bythe pre-trained retriever; then, the Answerer takesthe key knowledge documents as the input knowl- edge and is finetuned to generate the answer; next,we generate pseudo-labels of key knowledge doc-uments according to the Answerers predictionsand weak supervision labels; finally, we refine theSelector to assess the relevance of retrieved knowl-edge documents in answering the question. Thisstrategy of self-bootstrapping enhances the abilityof knowledge selection and answer generation con-sistently, enabling the model to accurately respondto knowledge-intensive questions.We conduct extensive experiments on the open-domain knowledge-based VQA benchmark (OK-VQA (Marino et al., 2019)) to validate the effective-ness of the proposed framework, where our methodlargely outperforms the baseline and achieves thestate-of-the-art performance of 62.83%, only fine-tuning 0.16% parameters with LoRA (Hu et al.,2022a). We also conduct comprehensive ablationsto validate the impact of different components ofthe proposed framework, including the Effect ofSelector and Answerer, cycle training of the frame-work, varying the number of key knowledge docu-ments, and so on.Our contributions are summarized as follows:",
  "We introduce a novel framework that lever-ages the large visual-language model to selectkey knowledge and use them to answer ques-tions, respectively": "We propose a new self-bootstrap learningmethod to train the Selector and Answerer,where the Selector chooses key knowledgedocuments for the Answerer and the Answererprovides pseudo-labels for the Selector. We achieve a state-of-the-art performance of62.83% on the OK-VQA dataset, surpassingthe previous state-of-the-art method. Notably,this improvement is achieved by fine-tuningonly 0.16% of parameters using LoRA.",
  "Related work": "LargeVisual-LanguageModels.Recently,large visual-language models (Li et al., 2023;Alayrac et al., 2022; Liu et al., 2023; Dai et al.,2023) have demonstrated remarkable visual-language understanding and reasoning capabilities,owing to the advancement of larger languagemodels (Brown et al., 2020; Zhang et al., 2022;Touvron et al., 2023; vic, 2023; Touvron et al.,2023).These methods typically consist of afrozen visual encoder (Radford et al., 2021), a visual-language connector (Li et al., 2023), and alarge language model (Chung et al., 2022; Zhanget al., 2022; vic, 2023). The models are firstlypre-trained on large-scale visual-text datasets toalign visual features to the language embeddingspace. After pretraining, the large language modelcan understand the visual details. Then, the modelis finetuned to adapt to various visual-languagetasks.In this study, we adopt BLIP2, one ofthe widely used models, as our backbone forbootstrapping knowledge selection and questionanswering with it. Knowledge-based VQA.Conventional VQAbenchmarks (Goyal et al., 2017; Hudson and Man-ning, 2019) primarily focus on basic visual percep-tion and reasoning tasks and numerous studies haveachieved promising results on these benchmarks(Anderson et al., 2017; Zhang et al., 2021; Tan andBansal, 2019; Lu et al., 2019; Li et al., 2022; Wanget al., 2022). Different from them, the knowledge-based VQA task (Wang et al., 2017; Marino et al.,2019; Schwenk et al., 2022) requires models to in-corporate diverse world knowledge to respond toquestions about visual content, which is more chal-lenging. Recent studies (Gardres et al., 2020; Wuet al., 2022; Lin and Byrne, 2022; Gui et al., 2021)have explored various open-domain world knowl-edge sources, such as ConceptNet (Speer et al.,2017), Wikipedia (Vrandecic and Krtzsch, 2014),Google Search Corpus (Luo et al., 2021). Theyretrieve the relevant knowledge documents fromthe knowledge bases and integrate them into the an-swering model to generate predictions. Except forusing explicit knowledge, some methods also takeGPT-3 (Brown et al., 2020) as an implicit knowl-edge producer. They either prompt GPT-3 within-context examples to predict answers directly(Yang et al., 2022; Hu et al., 2022b; Shao et al.,2023), or use GPT-3 to generate answer candidateswith evidence serving as textual implicit knowledgebases (Gui et al., 2021; Lin et al., 2022), leading tosignificant performance improvements. Differentfrom these approaches, we employ a large visual-language model to select key retrieved knowledgeand reason on the knowledge to answer questions.",
  "[garfield, ,garfield]garfieldSelectorAnswerer": "Question: What is a famous cartoon animal of this type?Knowledge 1: with two of the most famous voices in cartoons, both supplied by mel blanc, sylvester's sloppy \"sufferin succotash\" and tweety's baby-voiced \"i tawt i taw a puddy tatKnowledge 2: maybe one of the most widely known cat cartoon, garfield is one cat with attitude. he isn't interested in much, except lasagna, napping, lasagna, teasing the dog Knowledge k: why some of our favorite cartoon characters throughout the years have been feline in nature. maybe one of the most widely known cat cartoon, garfield is one cat with attitudeSel Prompt: Does the retrieved knowledge document provide the key information to help answer the question?Ans Prompt: Short Answer",
  "Retrievedknowledge": ": Our framework consists of two modules: a Selector and an Answerer. Selector (left) selects the top-Tknowledge documents for the Answerer (right), and the Answerer focuses on important knowledge informationto predict answers. Both modules utilize the same frozen visual module to extract image features. We train thefully connected (FC) layer and fine-tune the language model using LoRA, which amounts to only 0.16% of thetotal parameters. For detailed training procedures of the two modules, refer to Alg. 1. The original knowledge isretrieved using DPR, and for brevity, we omit the retrieval process here (details can be found in .1).",
  "Preliminaries": "Knowledge Retrieval.We adopt the Dense Pas-sage Retrieval (DPR) (Karpukhin et al., 2020b)to retrieve the knowledge documents. We trans-form the image into raw texts composed of cap-tions, objects, attributes, and OCR (Optical Char-acter Recognition). Then we compute the similar-ity scores between the query and knowledge doc-uments sim(qi, Dj) = qTi dj and exploit FAISS(Johnson et al., 2019) to index Top-k related knowl-edge documents Pi = {Pi,1, Pi,2, ..., Pi,k} for i-thquery. Large Visual-Language Model.In our work,both knowledge selection and question-answeringmodules adopt BLIP-2 (Li et al., 2023) as the back-bone. The architecture of BLIP-2 comprises afrozen image encoder (Dosovitskiy et al., 2020;Fang et al., 2023), a Q-Former (Li et al., 2023), anda pre-trained language model (Chung et al., 2022).Given an image Ii, the frozen image encoder out-puts a set of visual features {hi,1, hi,2, ..., hi,m}.Q-Former takes extracted visual features as in-put, and outputs language-aligned visual features{vi,1, vi,2, ..., vi,l}. These visual features are con-catenated with the textual word embeddings, whichare fed into the language model for generation.Through pre-training on large-scale image-caption datasets, Q-Former can effectively project visualfeatures into the feature space of the LanguageLarge Model (LLM). We freeze the visual encoderand Q-former during training. We train the fullyconnected layer and use LoRA (Hu et al., 2022a)to finetune the LLM (only finetune 0.16% of totalparameters).",
  "Selector and Answerer": "Selector.After obtaining the Top-k knowledgedocuments using DPR for the i-th sample, we aimto choose t most important knowledge documentsfrom the retrieved documents, where t is smallerthan k. As shown in , we first use the frozenimage encoder and Q-former to extract the imagefeatures Vi, where these features are extractedonce and then used by the Selector and the An-swerer. Then image features Vi are fed into theindependent fully-connected layer to obtain the vi-sual embeddings Evi . We concatenate the question,a retrieved knowledge document, and the Selec-tion prompt \"Does the retrieved knowledge docu-ment provide the key information to help answerthe question?\" into one sentence S. Next, visualembeddings Evi and the text are concatenated andfed into the LLM (Flan-T5 (Chung et al., 2022)is adopted in our work). Last, we use the proba-bility of generating the word yes as the score ofeach retrieved knowledge document Pi,j, denotedas si,j = LLM(concat(Evi , Si)), and we selecttop-t documents Pi = { Pi,1, Pi,2, ..., Pi,t} based",
  "Pi == Selector(Ii, Qi, Pi), | Pi| = t(1)": "Answerer.After obtaining the selected knowl-edge documents, we aim to reason on the knowl-edge to answer questions. As shown in , weprocess the same image features to obtain the differ-ent visual embeddings Evi via the fully-connectedlayer of the Answerer. Next, we concatenate thequestion and the knowledge into one sentence S using the template \"Question: {} Knowledge: {}Answer: \". We concatenate the visual embeddingsand the text, which are fed into the LLM with differ-ent LoRA parameters to get the answer. The modeloutputs corresponding answers based on differentdocuments. The Answerer can be conceptualizedas follows:",
  "Self-Bootstrap Learning": "To enable the Selector and Answerer to select keyknowledge and answer questions, we bootstrapthem with each other in a style of cycle training.We repeat the following process for the given i-thsample {Ii, Qi, Pi, Ai} of the training dataset: Answerer Training.We use Eq. 1 to get the se-lected knowledge documents Pi. The image Iiis fed into the frozen ViT and Q-former to ob-tain the image features Vi.We use the train-able FCans layer to output the visual embeddingsEvans,i. We concatenate the visual embedding, thequestion Qi and each selected knowledge docu-ment Pi,j to construct t triplets for the sample,where j = 1, 2, . . . , t. Then we finetune the An-swerer with LoRA under the supervision of theground truth answer Ai:",
  "Algorithm 1 Pipeline of cycle training": "Input:KB-VQA dataset D={Ii, Qi, Ai|i=1, 2, . . . , N};RetrievedknowledgedocumentsPi={P 1i , P 2i , . . . , P ki }; Ii, Qi, Pi, and Ai denoteimage, question, document set, and answer setof i-th sampleOutput: Knowledge selection model Selector;Question answering model Answererfor sample in D do Stage 1:1: Using Selector to select top-t documentsPi from the retrieved knowledge documentsPi as Eq. 12: Finetuning Answerer on {Ii, Qi, Pi, Ai}supervised by the ground-truth answer asEq. 3.Stage 2:1: Using Answerer to predict answers forretrieved knowledge documents Pi as Eq. 22: Generating to pseudo labels {yi,j} for re-trieved knowledge documents Pi as Eq. 43:FinetuningSelectoron{Ii, Qi, Pi, {yi,j}}supervisedbythepseudo label as Eq. 5.",
  "end for": "Selector Training.We first use Eq. 2 to predictanswers based on each retrieved knowledge doc-ument Pi,j. Then we assign pseudo labels to theretrieved documents according to model predic-tions and weak supervision labels (Luo et al., 2021;Lin and Byrne, 2022; Lin et al., 2023). We use\"yes\" and \"no\" as pseudo labels, where label adocument as positive knowledge if Answerer canoutput the correct answer using that document andthe document contains any of the answers in Ai.",
  "yi,j =": "yes,if ai = ai Pi,j contains an answer in Aino,else(4)After obtaining the pseudo label of each re-trieved knowledge document, we use the trainableFCsel layer to output the visual embeddings Evsel,i.we concatenate the visual embedding, the ques-tion Qi and each retrieved knowledge documentPi,j to construct k triplets for the sample, wherej = 1, 2, . . . , k. Then we finetune the Selector : Performance comparison with state-of-the-art (SOTA) methods on the OK-VQA dataset. KnowledgeSources: ConceptNet (C); Wikipedia (W); Google Search (GS); Google Images (GI). The best result in the table isbolded. The results show that our method achieves the state-of-the-art performance.",
  "ModelsLarge ModelsKtrainKtestKnowledge ResourceAcc (%)": "BAN+AN (Marino et al., 2019)---W25.6ConceptBERT (Gardres et al., 2020)---C33.7KRISP (Marino et al., 2021)---C+W38.4Visual Retriever-Reader (Luo et al., 2021)-100100GS39.2MAVEx (Wu et al., 2022)---W+C + GI39.4 PICa (Yang et al., 2022)GPT-3 (175B)--GPT-348.0TRiG(Ensemble) (Gao et al., 2022)T5-large (770M)100100W50.5KAT(Single) (Gui et al., 2021)T5-large (770M)4040W + GPT-353.1KAT(Ensemble) (Gui et al., 2021)T5-large (770M)4040W + GPT-354.4RA-VQA (Lin and Byrne, 2022)T5-large (770M)550GS54.5REVIVE(Single) (Lin et al., 2022)T5-large (770M)4040W+GPT-356.6REVIVE(Ensemble) (Lin et al., 2022)T5-large (770M)4040W+GPT-358.0PromptCap (Hu et al., 2022b)GPT-3 (175B)--GPT-360.4Prophet (Shao et al., 2023)GPT-3 (175B)--GPT-3+MCAN61.1FillingGap (Wang et al., 2023)GPT-3 (175B)--GPT-361.3SimpleBaseline (Xenos et al., 2023)LLaMA 2 (13B)--LLaMA 261.2Cola-FT (Chen et al., 2024)FLAN-T5(11B)--BLIP+OFA62.4 Flamingo (Alayrac et al., 2022)Flamingo (80B)--Pretrain57.8InstructBLIP (Dai et al., 2023)InstructBLIP Vicuna (7B)--Pretrain62.1Qwen-VL (Bai et al., 2023)Qwen-VL(Qwen-7B)--Pretrain58.6MM-Reasoner (Khademi et al., 2023)Flamingo (80B)--GPT-460.8",
  "j=1log LLMsel(yi,j|Evsel,i, Qi, P ji )": "(5)We provide the overall training pipeline in Alg. 1.Through continuous iteration, the Selector will pro-vide more crucial knowledge for the Answerer toaccurately respond to questions. Meanwhile, theimprovement in the Answerers reasoning abilitywill also result in more precise pseudo-labeling, fur-ther enhancing the Selectors discriminative power.During the inference stage, we utilize the Selec-tor to choose key knowledge, and then instruct theAnswerer to respond to questions based on thisknowledge.",
  "Dataset.We conduct extensive experiments onOK-VQA (Marino et al., 2019) to evaluate the ef-fectiveness of our method. OK-VQA is a challeng-ing open-domain knowledge-based VQA dataset": "that requires models to leverage various exter-nal knowledge sources to answer questions. Thedataset contains 14,055 questions and 14,031 im-ages, whereas the training set and testing set have9k and 5k image-question pairs, respectively. Dueto no knowledge base being provided for OK-VQA,we need to choose the proper knowledge base forthe dataset. In this paper, we adopt Google SearchCorpus (Luo et al., 2021) as the knowledge basewhich is collected in the websites using the GoogleSearch API. Evaluation Metric.We use the standard VQAmetric (Antol et al., 2015) to evaluate the perfor-mance of the model. Given the prediction of thequestion a and the groudtruth answer set A, theVQA accuracy is calculated as:",
  "Implementation Details.In our experiment, weadopt BLIP2 T5-XL (3B) (Li et al., 2023) to ini-tialize the Selector and Answerer. We freeze the": "image encoder and Q-former, with both the Se-lector and Answerer sharing the same visual mod-ule. We finetune the fully connected layer anduse LoRA (Hu et al., 2022a) to train the LLM.We use the default huggingface-PEFT setting: r=8,lora_alpha=32, lora_dropout=0.1. We use Adamas the optimizer and set the batch size to 8. Weuse the warm-up strategy which trains the modelwith an initial learning rate of 1e-4 and warm-upfactor of 0.05 for 1000 steps and then utilizes acosine annealing learning strategy with an initiallearning rate of 1e-4 and a final learning rate of0 after 10 epochs. We use top-30 knowledge doc-uments retrieved by a pre-trained DPR (Lin andByrne, 2022) as candidates for Selector and usethe selected top-5 documents from the 30 docu-ments for the Answerer to train and infer, denotedas Kcandidate = 30, Ktrain = 5, Ktest = 5. Weuse 2 Nvidia A800 GPUs (80G) for all experiments.",
  "Comparison with State-of-the-artMethods": "As shown in Tab. 1, we can see that early models(BAN+AN (Marino et al., 2019), ConceptBERT(Gardres et al., 2020), KRISP (Marino et al.,2021), Visual Retriever-Reader (Luo et al., 2021),and MAVEx (Wu et al., 2022)) have a weak perfor-mance, achieving a VQA accuracy from 25.6% to39.4%. Recently, by introducing larger models (T5-large, GPT-3, LLaMA, Vicuna) and diverse knowl-edge resources (ConceptNet, Wikipedia, GoogleWeb Search and Google Images), the performancehas a significant performance improvement, achiev-ing a VQA accuracy of 62.4%. Our method aims toaugment the reasoning ability to answer knowledge-intensive questions of the large visual-languagemodel. When directly finetuning BLIP2 T5-XLon OKVQA, the model has a low performanceof 55.44%. By introducing external knowledge,the performance has a significant performance im-provement. Different from RA-VQA-v2 (Lin et al.,2023) and PreFLMR (Lin et al., 2024), we do nottrain a multimodal retriever from scratch whichrequires expensive annotations and high computa-tional costs. We directly leverage the large visual-language model to select key knowledge from theretrieved knowledge by DPR like the process ofre-ranking. With the same knowledge resources(i.e., Google Search), our method achieves 62.8%accuracy, outperforming other state-of-the-art mod-els. It is worth noting that we do not use GPT-3 andwe only train the 0.16% parameters of the model. : Comparison of our selector with differentknowledge selection strategies. We select 5 knowledgedocuments from top-30 knowledge candidates retrievedby DPR. DPR Score refers to selecting top-5 knowl-edge based on similarity scores. Random Selectionmeans randomly selecting 5 knowledge documents from30 candidate knowledge documents. Selector denoteschoosing 5 key knowledge documents by the Selector.",
  "We conduct the ablation studies to evaluate differ-ent components of our framework on OK-VQA": "Effect of Selector.We conduct the ablation studyto evaluate the effectiveness of Selector in ourmethod. We show the results in Tab. 2. From theresults, we can observe: our framework, leveragingkey knowledge documents selected by the Selec-tor, consistently outperforms the Answerer whenusing the same number of documents retrieved byDPR. We improve the performance by 2.14% and1.88% with 1 and 5 test knowledge documents,compared to DPR-based retrieval. When using therandomly selected documents, the model performsworst. These results demonstrate that top-rankedknowledge documents based on DPR scores arenot optimal for question answering and our keyknowledge selection module can identify relevantdocuments for accurate question answering, en-suring the coherence of knowledge retrieval andquestion-answering processes. Effect of different knowledge reasoning methodsof Answerer.In Tab. 3, we present a comparisonof Answerer using different knowledge reasoningmethods. The results show that the performanceusing the strategy of voting surpasses that of con-catenating under different knowledge selection set-tings. We argue that directly combining all theknowledge documents into a lengthened documentmakes it difficult for Answerer to reason on them,which is easily influenced by noisy information. Incontrast, it is easier for Answerer to reason on each : Effect of different knowledge reasoning meth-ods of Answerer. Concatenating denotes that we com-bine the key knowledge documents into one sentenceand feed it into Answerer to predict the final answer.Voting means that we feed different key knowledge doc-uments into Answerer to predict different answers andchoose the best answer based on majority voting.",
  "document to predict the answer. Simple voting canchoose the best answer": "Effect of the self-bootstrap learning method.To evaluate the effectiveness of our self-bootstraplearning method, we compare the method with thestrategy of independent training of two modules.We finetune the Answerer with the knowledge doc-uments retrieved by DPR as the baseline. Inde-pendent training means that we take two passes inanswerer training and one pass for selector trainingon the entire dataset. Cycle training means thatwe train the answerer and selector on each batchdata of the dataset simultaneously. The results inTab. 4 show that the model with cycle training out-performs the model with independent training by3.81%. The VQA score of using independent train-ing is even lower than the baseline. These resultsdemonstrate that our cycle training method caneffectively boost the Selector and Answerer eachother, which makes the model find key knowledgedocuments and leverage the knowledge to answerquestions. Effect of different methods of pseudo-labeling.In Tab. 5, we compare the model performance withdifferent methods of pseudo-labeling. When usingthe model predictions as guidance, the model has aVQA score of 62.31%. When adding the weak su-pervision as the guidance, the models VQA scoreincreases from 62.31% to 62.83%. The resultsdemonstrate that using weak supervision labels pre-",
  "serves potentially useful documents, aiding the An-swerer in accurately answering questions": "Effect of key knowledge documents selectionranges and quantities.In Tab. 6, we evaluatekey knowledge document selection using variousnumbers of candidate documents and selected doc-uments. From the results, we have the followingfindings: (1) As the number of selected documentsincreases, the models performance improves. Thisindicates that using more documents to train andtest contributes to answering questions. (2) Us-ing more documents for training can improve theperformance a lot (the 2nd line v.s. the last line).However, using more documents for testing hasalmost no improvement (the 3rd line v.s. 4th line).(3) When the number of candidate documents in-creases, the models performance improves. The re-sult demonstrates that low-ranked documents basedon DPR scores may contain useful information forquestion answering. It is necessary for the modelto select key knowledge documents.",
  "garfield": ": Qualitative results on the test split of OK-VQA. We compared our method with a model that fine-tunesBLIP2 with knowledge ranked by DPR. The middle segment of the graph represents knowledge from variousmethods used to answer questions. On the right side of the graph, different answers are depicted when using distinctknowledge. Green and red colors indicate whether the selected final answer is correct. Effect of different knowledge documents selec-tion in Answerer fine-tuning.Tab. 7 comparesAnswerer fine-tuning with different document se-lection strategies. The results show that our frame-work performs optimally when utilizing Selector inboth Answerer training and inference. This is likelybecause the Selector provides more informative keyknowledge documents and using both Selector en-sures the consistency between the training domainand testing domain. Performance of the knowledge retrieval.Intab. 8, we evaluate our Selector in the knowledgeretrieval task. Following previous methods (Luoet al., 2021; Lin and Byrne, 2022), we adopt pseudorelevance to measure if the retrieved document isrelevant to the query due to the absence of ground-truth document. We use Recall to measure the per-formance of the the knowledge retrieval. From theresults, we can see that our Selector improves theperformance of DPR a lot. This means our Selectorcan retrieve more relevant knowledge documents",
  "In , We present a case study comparing ourmethod with a model that fine-tunes BLIP2 usingknowledge ranked by DPR. In the first case, top-": "ranked knowledge documents from DPR misguidethe model, resulting in incorrect predictions. How-ever, our methods Selector chooses key knowledgedocuments that aid in predicting correct answers.In the second case, each knowledge document fromDPR contains irrelevant information, leading to anincorrect final answer. Despite the top-1 documentfrom the Selector resulting in a wrong answer, ourmethod identifies other key knowledge documentsfor generating correct answers. Through majorityvoting, the final selected answer is correct. Thesecases demonstrate our methods ability to extractinformative knowledge from retrieved documentsto support accurate question answering.",
  "Conclusion": "In this paper, we propose a novel framework thatleverages the large visual-language model to con-struct two modules: (1) Selector for finding key re-trieved knowledge and (2) Answerer for reasoningon the knowledge to predict answers. We designa self-bootstrap learning method to improve theirabilities, where the Selector chooses key knowl-edge documents for the Answerer and the Answererprovides pseudo-labels for the Selector. Comparedwith state-of-the-art methods, our method achievesbetter performance on a challenging open-domainknowledge-based VQA benchmark (OK-VQA) andwe conduct a comprehensive analysis to evaluatethe effectiveness of our method.",
  "Limitations": "Although our framework can effectively select keyknowledge documents for answering question, it isinevitable that the knowledge still contains noise.In some cases, the model itself can answer thequestion without external knowledge, introducingextra knowledge may affect the performance. Inthe future, we can explore to dynamically selectrequired knowledge to help itself answer questions.In addition, there is a concern on the generaliz-ability of the proposed method on other domains, especially when the initial DPR model can notretrieve gold standard context. In the future, weconsider adopting a stronger multimodal retrievermodel to obtain more useful candidate knowledgedocuments, which enhances the generalizability ofour framework.",
  ".Vicuna": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and Lei Zhang.2017. Bottom-up and top-down attention for im-age captioning and visual question answering. 2018IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 60776086. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C Lawrence Zitnick, andDevi Parikh. 2015. Vqa: Visual question answering.In Proceedings of the IEEE international conferenceon computer vision, pages 24252433. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang,Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Zi-wei Liu. 2024. Large language models are visualreasoning coordinators. Advances in Neural Informa-tion Processing Systems, 36. Zhuo Chen, Jiaoyan Chen, Yuxia Geng, Jeff Z Pan,Zonggang Yuan, and Huajun Chen. 2021. Zero-shotvisual question answering using knowledge graph. InThe Semantic WebISWC 2021: 20th InternationalSemantic Web Conference, ISWC 2021, Virtual Event,October 2428, 2021, Proceedings 20, pages 146162. Springer. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Albert Li, Pascale Fung, and Steven C. H.Hoi. 2023. Instructblip: Towards general-purposevision-language models with instruction tuning.ArXiv, abs/2305.06500. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.arXiv preprintarXiv:2010.11929. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, LedellWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,and Yue Cao. 2023. Eva: Exploring the limits ofmasked visual representation learning at scale. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1935819369. Feng Gao, Qing Ping, Govind Thattai, Aishwarya Re-ganti, Ying Nian Wu, and Prem Natarajan. 2022.Transform-retrieve-generate:Natural language-centric outside-knowledge visual question answer-ing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages50675077. Franois Gardres, Maryam Ziaeefard, Baptiste Abe-loos, and Freddy Lecue. 2020.Conceptbert:Concept-aware representation for visual question an-swering. In Findings of the Association for Compu-tational Linguistics: EMNLP 2020, pages 489498. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913.",
  "Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-mann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat:A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614": "Yangyang Guo, Liqiang Nie, Yongkang Wong, YibingLiu, Zhiyong Cheng, and Mohan Kankanhalli. 2022.A unified end-to-end retriever-reader framework forknowledge-based vqa. In Proceedings of the 30thACM International Conference on Multimedia, pages20612069. Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022a. LoRA: Low-rank adaptation oflarge language models. In International Conferenceon Learning Representations.",
  "Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019.Billion-scale similarity search with gpus.IEEETransactions on Big Data, 7(3):535547": "Amita Kamath, Christopher Clark, Tanmay Gupta, EricKolve, Derek Hoiem, and Aniruddha Kembhavi.2022. Webly supervised concept expansion for gen-eral purpose vision models. In European Conferenceon Computer Vision, pages 662681. Springer. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020a. Dense passage retrieval foropen-domain question answering. In Proceedingsof the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Lin-guistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020b. Dense passage retrieval foropen-domain question answering. arXiv preprintarXiv:2004.04906. Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, andChenguang Zhu. 2023.MM-reasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 65716581, Singapore. Association forComputational Linguistics.",
  "Man Luo, Yankai Zeng, Pratyay Banerjee, and ChittaBaral. 2021.Weakly-supervised visual-retriever-reader for knowledge-based question answering.arXiv preprint arXiv:2109.04014": "Kenneth Marino, Xinlei Chen, Devi Parikh, AbhinavGupta, and Marcus Rohrbach. 2021. Krisp: Inte-grating implicit and symbolic knowledge for open-domain knowledge-based vqa. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1411114121. Kenneth Marino, Mohammad Rastegari, Ali Farhadi,and Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-tion answering benchmark requiring external knowl-edge. In Proceedings of the IEEE/cvf conferenceon computer vision and pattern recognition, pages31953204. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision. Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023.Prompting large language models with answer heuris-tics for knowledge-based visual question answering.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1497414983.",
  "Denny Vrandecic and Markus Krtzsch. 2014. Wiki-data: a free collaborative knowledgebase. Communi-cations of the ACM, 57(10):7885": "Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, andAnton Van Den Hengel. 2017. Fvqa: Fact-basedvisual question answering. IEEE transactions on pat-tern analysis and machine intelligence, 40(10):24132427. Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-chitectures, tasks, and modalities through a simplesequence-to-sequence learning framework. In Inter-national Conference on Machine Learning, pages2331823340. PMLR.",
  "Ziyue Wang, Chi Chen, Peng Li, and Yang Liu. 2023.Filling the image information gap for vqa: Promptinglarge language models to proactively ask questions.arXiv preprint arXiv:2311.11598": "Jialin Wu, Jiasen Lu, Ashish Sabharwal, and RoozbehMottaghi. 2022.Multi-modal answer validationfor knowledge-based vqa.In Proceedings of theAAAI conference on artificial intelligence, volume 36,pages 27122721. Alexandros Xenos, Themos Stafylakis, Ioannis Patras,and Georgios Tzimiropoulos. 2023. A simple base-line for knowledge-based visual question answering.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1487114877. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, XiaoweiHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 36, pages 30813089. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, JianweiYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-feng Gao. 2021. Vinvl: Revisiting visual represen-tations in vision-language models. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 55795588. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "A.1Experiments on Other Datasets": "We also evaluate our method on FVQA (Fanget al., 2023) and A-OKVQA (Schwenk et al., 2022)to demonstrate the effectiveness of our method.FVQA is a VQA dataset that mostly containsquestions requiring external knowledge to answer,and provides supporting fact triplets alongside theimage-question-answer triplets. A-OKVQA is anaugmented successor of OK-VQA, containing 25Kimage-question pairs that require broader common-sense and world knowledge to answer. Due to A-OKVQA does not provide the knowledge source,we use Wikipedia (Vrandecic and Krtzsch, 2014)as the knowledge base.As shown in Tab. 9, our method surpasses previ-ous state-of-the-art methods, which demonstratesthe effectiveness and generalization of our method.Tab. 10 shows the comparative results on the chal-lenging A-OKVQA dataset. Our method achievedcompetitive results, which demonstrates the effec-tiveness of our method."
}