{
  "Abstract": "Reasoning is most powerful when an LLM ac-curately aggregates relevant information. Weexamine the critical role of information aggre-gation in reasoning by requiring the LLM to an-alyze sports narratives. To succeed at this task,an LLM must infer points from actions, iden-tify related entities, attribute points accuratelyto players and teams, and compile key statisticsto draw conclusions. We conduct comprehen-sive experiments with real NBA basketball dataand present SPORTSGEN, a new method to syn-thesize game narratives. By synthesizing data,we can rigorously evaluate LLMs reasoningcapabilities under complex scenarios with vary-ing narrative lengths and density of information.Our findings show that most models, includingGPT-4o, often fail to accurately aggregate bas-ketball scores due to frequent scoring patterns.Open-source models like Llama-3 further sufferfrom significant score hallucinations. Finally,the effectiveness of reasoning is influenced bynarrative complexity, information density, anddomain-specific terms, highlighting the chal-lenges in analytical reasoning tasks.1",
  "Introduction": "Accurately aggregating information is essential forreasoning (Sprague et al., 2024; Wang et al., 2023a;Yasunaga et al., 2024; Hu et al., 2024c). This isespecially true when reasoning over longitudinaldata (ukasz Kidzinski and Hastie, 2021). For ex-ample, when assessing patient outcomes over yearsto determine the effectiveness of new medications,models must accurately track changes to provideinsights into trends, patterns, and potential causalrelationships. While LLMs hold significant poten-tial in reasoning, understanding how they aggregateinformation to support reasoning is key to ensuringtheir robustness in decision-making.",
  "*Work done during Yebowen Hus internship; KaiqiangSong and Sangwoo Cho were full-time researchers at TencentAI Lab, Seattle, USA at the time of this work.1": "Sports data have recently emerged as a populartesting ground for LLMs (Srivastava et al., 2023;Wang et al., 2023b; Li et al., 2024b; Yang et al.,2024c). Unlike health data, sports narratives arereadily accessible from diverse sources, e.g., ESPN,CBS Sports, BBC Sport, and Sports Illustrated.2 These narratives provide detailed event descriptionsover extended game periods, making them an excel-lent case study to explore how LLMs reason withlongitudinal data. presents a snippet froman NBA basketball game. The play-by-plays cap-ture key actions of the game, such as passes, shots,and fouls, along with timestamps and team-playeraffiliations. Our research involves using LLMs totrack scoring actions within these detailed gamedescriptions and to link entities for comprehensivegame analysis. These games include a mix of ex-pected and unexpected events, presenting uniquechallenges for reasoning tasks.LLMss reasoning abilities, including multi-hop,deductive, inductive, and abductive reasoning, havegarnered increasing attention (Hu and Shu, 2023;Shi et al., 2023; Zhao et al., 2023a; Yang et al.,2024b,a; Li et al., 2024a; Duan et al., 2024; Chu-Carroll et al., 2024). For example, Sprague et al.(2023) introduce a dataset designed to evaluate lan-guage models on multi-step reasoning with 1,000-word murder mysteries and object placement. Yanget al. (2024b) explore latent multi-hop reasoningand found evidence of its use in fact composition.Berglund et al. (2024) study the reversal curse thatLLMs trained on A is B fail to learn that B is A.Distinguishing from previous studies, our approachintroduces a quantitative aspect to reasoning withgame narratives, where the LLM must summarizekey statistics to derive conclusions.We focus on analytical reasoning in this work,which involves identifying relationships between el-ements, drawing inferences and synthesizing infor-",
  "for Analytical Reasoning": ": ESPNs NBA game play-by-play descriptions. We are particularly interested in exploring whether LLMscan perform analytical reasoning in a more focused and manageable context using divide-and-conquer strategies. mation (Savage et al., 2023). Notably, we evaluateif LLMs perform better in a more focused and man-ageable context when using a divide-and-conquerstrategy. Sports narratives, for instance, can besegmented along multiple dimensions. We exploredivision methods that simplify multi-hop reasoningor facilitate information aggregation, thereby eas-ing the load on LLMs. We further introduce a newDiscounted Cumulative Accuracy (DCA) metric.DCA differs from traditional accuracy by allowinga small margin of error. This helps us identify sce-narios where LLMs significantly hallucinate, ratherthan grounding predictions on the narratives. Ourexperiments reveal multiple limitations of state-of-the-art LLMs in using divide-and-conquer strate-gies for analytical reasoning.With data synthesis, we further stress test LLMsreasoning abilities in complex and novel scenarios.LLMs are required to handle a range of narrativesthat differ in length, complexity, and informationdensity, and may also involve a shift from naturallanguage to symbolic reasoning. Our approach,SPORTSGEN, enhances the control over narrativecomplexity when compared to human-written gamenarratives and few-shot prompting. It contributesto ongoing research in data synthesis (Veselovskyet al., 2023; Arora et al., 2023; Kang et al., 2024;Divekar and Durrett, 2024) by creating syntheticsports narratives that help evaluate the analyticalcapabilities of LLMs. Our research contributionsare summarized as follows. We investigate LLMs reasoning abilities by an-alyzing play-by-play sports narratives. This in-volves tracking scoring actions, inferring pointsfrom actions, recognizing players and teams, andsummarizing key statistics to draw conclusions.By exploring how LLMs aggregate information,we gain insights into their potential in longitudi-nal studies, such as patient health management,",
  "where the LLM must identify and interpret recur-ring patterns to make critical decisions": "We observe that LLMs struggle more with accu-rately aggregating points than assigning points toplayers and teams. Basketballs frequent scoringposes a challenge for tracking actions. Moreover,when input narratives are short and instructionsare lengthy, LLMs may overlook the narrativeand hallucinate game points. Our SPORTSGENapproach has significant implications for simu-lating complex reasoning scenarios that involvessynthesizing both text and numerical data.",
  "Related Work": "Success in reasoning hinges on effective content se-lection. Recent studies have placed a growing em-phasis on LLMs reasoning capabilities, exploringvarious types such as deductive, inductive, abduc-tive, and multi-hop reasoning (Bostrom et al., 2021;Huang and Chang, 2022; Hu and Shu, 2023; Zhaoet al., 2023a; Holliday and Mandelkern, 2024). Ef-forts to enhance LLMs reasoning abilities includeprompting, supervised fine-tuning, and adjustmentsto decoding (Wei et al., 2023; Chen et al., 2023;Zhao et al., 2023a; Burnell et al., 2023; Zheng et al.,2023; Cheng et al., 2024; Ahn et al., 2024; Wangand Zhou, 2024). In our paper, we propose a quanti-tative approach to reasoning by requiring the LLMto identify and consolidate relevant facts. Similarto how frequency indicates salient content in multi-document summarization (Fabbri et al., 2019; Maoet al., 2020; Gholipour Ghalandari et al., 2020;Lebanoff et al., 2021; Thomson et al., 2023), weconjecture that how LLMs aggregate informationis critical in addressing complex reasoning chal-lenges.Numerical reasoning is applied in financial ques-tion answering and solving mathematical wordproblems, and numerous datasets have been de- veloped for these purposes (Amini et al., 2019;Cobbe et al., 2021; Patel et al., 2021; Zhu et al.,2021; Chen et al., 2022; Liu et al., 2023; Zhao et al.,2023b; Lu et al., 2023). Our study explores a newangleinvestigating LLMs ability to analyticallysolve problems using divide-and-conquer strate-gies (Lee and Kim, 2023; You et al., 2023). Sportsdata are complex and multifaceted. Our focus is onpinpointing specific divide-and-conquer strategieswhere LLMs are particularly effective.Sports data plays a pivotal role in various lan-guage tasks, such as real-time summarization ofgames, data-to-text generation, and analysis ofcommentator bias (Wiseman et al., 2017; Edouardet al., 2017; van der Lee et al., 2017; Puduppullyet al., 2019; Merullo et al., 2019; Huang et al., 2020;Hu et al., 2024a,b). It also influences the analysisof domains like game reviews and gameplay logs,enhancing our understanding of gameplay com-mentary dynamics (Lukin, 2020; Kicikoglu et al.,2020; Gu et al., 2022; Furman et al., 2022). Build-ing on research by Hu et al. (2024b), our studyexplores how well LLMs can track scoring actionsacross lengthy narratives and accurately link enti-ties for in-depth game analysis. With SPORTSGENfor data synthesis (Veselovsky et al., 2023; Aroraet al., 2023; Divekar and Durrett, 2024), we take astep further by creating diverse narratives that varyin style, complexity, and level of detail to betterassess LLM effectiveness.",
  "Analytical Reasoning withDivide-and-Conquer Strategies": "Our research uses analytical reasoning to accuratelycalculate team points from sports narratives. Wespecifically focus on ESPNs NBA game play-by-play descriptions (). On average, an NBAgame consists of 466 plays and 6,229 tokens, withthe longest narrative containing up to 7,322 tokens.To succeed at this task, an LLM must understandthe basic rules of the game, distinguishing betweenscoring plays, such as three-pointers and field goals,and non-scoring actions like passing. Further, themodel must correctly attribute each scoring actionto the appropriate player and team, which requiresnuanced reasoning. Finally, the LLM must aggre-gate all the scored points to determine the finalscores for the teams. This process tests the LLM inthree critical areas: knowledge acquisition, pointreferencing, and effective score aggregation. Errorsin any of these areas can lead to incorrect results. Player-Centric Division.We are particularly in-terested in exploring whether LLMs can performanalytical reasoning in a more focused and manage-able context using a divide-and-conquer strategy.For instance, sports narratives can be segmentedalong multiple dimensions. In our player-centricdivision, the LLM assesses player performancebased on play-by-play descriptions. Here, the LLMreceives a list of players, without their team affilia-tions, and is tasked with tracking and summarizingeach players performance throughout the game.The final results are presented in JSON format. Fol-lowing this, we link the individual players scoreswith their respective teams to calculate the overallteam scores. This player-centric approach employsLLMs to track individual player performances, al-lowing for a direct calculation of team scores fromplayer data. It lessens the burden on LLMs to usemulti-hop reasoning for inferring team scores fromplayer scores. Batch-Centric Division.Instead of processingthe play-by-play narrative at once, we break it downinto smaller, non-overlapping batches, each con-taining K plays. In our batch-centric division ap-proach, each batch is analyzed independently bythe LLM to determine the team points for that seg-ment. This approach allows the LLM to narrow itsfocus, helping it easily identify where errors mightbe occurring. It also eases the burden on LLMs, asthey are not required to aggregate points to deter-mine total scores from batch scores; instead, thisis handled through direct calculation. We comparethis approach to monolithic processing, where anLLM processes the entire game narrative in oneattempt. Interestingly, one would expect that whenLLMs are provided with data in smaller batches,they would perform better in this more granularand manageable setting. In 5, we report notablefindings that highlight the limitations of state-of-the-art LLMs when using these divide-and-conquerstrategies.We focus on identifying specific data divisionswhere LLMs are particularly effective. Real-worlddata can be complex and multifaceted. By focus-ing on sports narratives, we aim to understand thelimitations of LLMs in reasoning and informationaggregation, since complex reasoning often requireLLMs to aggregate relevant information effectively.Our research does not introduce new methods ofreasoning. Instead, it uses chain-of-thought prompt-ing, which enables Transformers to tackle inher-",
  "Creating Data with SPORTSGEN": "We introduce SPORTSGEN, a new method that syn-thesizes sports narratives by modeling game dy-namics. SPORTSGEN offers a platform to assessLLMs reasoning capabilities in novel scenariosthat may not exist in real data. Since synthesizednarratives have not been encountered by LLMs dur-ing pretraining, they also serve as a valuable bench-mark for future LLM assessments. Previously, datasynthesis has been utilized in fine-tuning, distilling,and evaluating LLMs for tasks such as hallucina-tion detection, topic classification, sarcasm and hu-mor detection (Veselovsky et al., 2023; Tang et al.,2023; Arora et al., 2023; Kang et al., 2024; Zhanget al., 2024; Divekar and Durrett, 2024). Our workexpands on these efforts by focusing on the specificchallenges in generating game narratives.3 Setting Up Team Profiles.We create profiles fortwo basketball teams, Team T1 and Team T2, for aspecific game. Each team has five players, fillingthe roles of point guard (PG), shooting guard (SG),small forward (SF), power forward (PF), and center(C). Each team has an efficiency score E from 0 to100, which reflects the points they score per 100possessions.4 We start with real NBA teams forthese profiles, and later users can select either realor synthetic players to create fantasy teams.",
  "Turn-by-Turn Game Generation.Our SPORTS-GEN renders realistic sports narratives using a turn-by-turn approach, which mirrors the back-and-forth": "3Spatial information, such as player positions on the field,is not included in our synthesized narratives, as it falls outsideour current scope.4Efficiency scores for basketball teams are obtained fromESPN. We aim to model team effectiveness in goal attempts;future research may consider individual player metrics. dynamics seen in live sports games. In each turn,a team controls the ball and performs a series ofplays5 such as passes, shots, and fouls, which arekey actions during the game. After each turn, pos-session generally shifts to the opposing team. Eachquarter consists of alternating turns between thetwo teams, with each turn being given an allottedtime frame. For example, a turn might unfold asfollows: Lakers take possession, make a quickpass, and take a shot that misses, all within about15 seconds. Each basketball game lasts 48 minutes.Our approach continues to add turns until nearingthe end of a quarters time limit. If the ending turnexceeds this limit, it will be truncated. Building an Action Graph.When a team hasthe ball, they are on offense and use actions suchas passing, dribbling, shooting to score points. Thedefense tries to stop them by blocking, stealing, andrebounding. We refer to each sequence of these ac-tions as a turn, and characterize it using a Markovgraph. For example, a turn might start with a de-fensive rebound, followed by a missed shot, andthen conclude. In the graph, each significant actionis represented as a node, and transitions betweennodes show how team members cooperate and exe-cute their tactics. The graph begins and ends withspecial nodes that mark the start and end of a turn.Our model includes 16 key actions; an example isprovided in Appendix 5. This action graph helpsto visualize the games flow within each turn. Action Templates.Each action is linked to spe-cific templates that describe it. E.g., <player-PF>misses a 20-foot jumper. and <player-PF> failsto convert a 14-foot pull-up jump shot. correspondto the action miss. To gather these templates, wefirst search NBA data using keywords selected byexperts to identify plays that describe each action.We then provide these initial templates to GPT-4oto increase their lexical diversity and enhance the",
  "In our paper, plays and actions are used interchange-ably to describe significant game actions": "Action - make <player-SG> successfully makes a 2-foot two point shot withthe assistance of <player-SF>. <player-SF> executes a delicate finger roll layup. <player-PG> hit a 26-foot three pointer. <player-PG> executes a 21-foot step back jumpshot. <player-SG> sinks a 26-foot three-point shot with an assistfrom <player-C>. <player-SF> executes a tip shot... Action - miss <player-PG> fails to make a 25-foot standard jump shot. <player-C> fails to convert a 20-foot jumper. <player-SG> unsuccessfully attempts a three point jumper. <player-PF> misses a 20-foot jumper. <player-PG> misses a 4-foot driving layup... Action - block <player-PG> individual defensive play resulted in a personalblock (<player-SG> draws the foul). <player-PF> executes a personal block as <player-PG> drawsthe foul. <player-PG> successfully executes a personal block againstthe opponent, drawing the foul in the process. <player-PG> commits a shooting block foul and <player-G> isfouled during the play...",
  ": Each action is linked to specific templates thatdescribe it. The templates are enhanced by GPT-4o toincrease their lexical diversity and narrative quality": "narrative quality. An example of actions and theirtemplates is shown in . During generation,each template linked to a sampled action has anequal probability of being used. The transitionsbetween actions are determined by analyzing realgame narratives to calculate maximum likelihoodestimates. Controlling Narrative Complexity.We adjustthe complexity of the game by changing the num-ber of actions per turn. Longer turns can extend theoverall narrative, whereas shorter turns create morefrequent scoring opportunities, as each turn oftenresults in an attempt to score. In NBA narratives,a turn might have anywhere from 1 to 10 actions,with an average of 1.65 actions. This results in ascoring to non-scoring action ratio R = 1:3. Tomodel turn length, we employ a Gaussian distribu-tion G parameterized by the mean () and standarddeviation (), which we calibrate using NBA data.During synthesis, we vary to generate narrativeswith different turn lengths while keeping stable.This approach allows us to produce narratives withrealistic scoring ratios such as R = 1:2, 1:3, 1:4,or 1:5, effectively capturing the dynamics of actualbasketball games. Rendering the Narrative.We begin by select-ing turn lengths from the Gaussian function G (Fig-ure 2). Each turn is then rendered by tracing a paththrough the action graph that matches the desig-nated length. We evaluate these paths for qualityand reject any that are unlikely in a basketball gameaccording to a set of expert rules. The probability of a turn leading to a score is determined using theEfficiency Score (E) for the involved team. Fromthis, we sample a 1 to indicate scoring and 0otherwise. A scoring turn must include a makeaction to indicate a successful score, and we recordthe points earned by the team in the box score.We repeatedly sample from the action graph tofind a path that meets our length and scoring cri-teria. We then select a template to describe eachaction in the chosen path. Next, placeholders arereplaced with relevant team player profiles, e.g.,player-PG is replaced with the actual point guardsname. Each action is tagged with a timestamp in-dicating the time elapsed since the previous action.We calculate these time intervals using real data andcreate a Gaussian Ga for each action type. We alter-nate between the profiles of the two teams involvedin the game. This process is repeated until we havecreated enough turns to complete one game quarter,after which we proceed to generate the remainingquarters to complete the full game narrative.",
  "Experiments": "Sports data are accessible from a variety of sourcessuch as ESPN, BBC Sport, CBS Sports, and SportsIllustrated. This work utilizes the basketball play-by-plays provided by Hu et al. (2024b) consistingof 28,492 NBA games from 2002 to 2023, gatheredfrom ESPN. Due to resource constraints, we sub-sampled the original dataset to DH, correspondingto the number of human-written narrative quarters.Further, our proposed method, SPORTSGEN, gener-ates synthesized game narratives with varied scor-ing to non-scoring (S:NS) ratios including 1:2, 1:3,1:4, and 1:5. DS represents the number of gamequarters synthesized by SPORTSGEN. Accuracy.Our goal is to enable LLMs to trackand predict the total points scored by each team atthe end of a game quarter. We choose quarter-levelevaluation because current LLMs do not performwell at tracking points for full games. The accu-racy metric strictly evaluates whether the modelspredictions matches with the actual scores for eachquarter and team. Here, each team is considered asa separate data point. For human-written narratives,our dataset includes detailed box scores sourcedfrom ESPN. For our SPORTSGEN generated narra-tives, ground-truth scores are obtained during thegeneration process.",
  ": Results of top LLMs in calculating team points from sports narratives, divided by batches of 1, 3, 10, or 30plays (DnC-{1,3,10,30}), by individual players (DnC-P), and monolithic processing (Mono)": "ing numerical values, as it is strict and can unfairlypenalize LLMs. To address this, we introduce thediscounted cumulative accuracy metric, which dif-fers from traditional accuracy by allowing a smallmargin of error. DCA rewards predictions that areclose to the true value, with diminishing gains aspredictions deviate further. This approach drawsinspiration from the DCG (Jrvelin and Kekli-nen, 2002), a highly effective metric in informationretrieval. It provides a balanced evaluation by cu-mulatively adjusting rewards, allowing for a fairerassessment of a systems performance.Concretely, we denote the tolerance level as T(Eq.(1)). At zero tolerance (T = 0), the DCA metricbecomes standard accuracy. As the tolerance levelincreases, performance differences between mod-els become less pronounced, making this a moreforgiving evaluation metric. During each time stept, we apply a discounting factor of 1 t T. With a tol-erance of 10 points (T = 10), this factor decreasesgradually from 1 down to 0 in steps of 0.1.We use pt to denote the proportion of instanceswhere the prediction error is exactly t points fromthe true value (Eq.(2)). Let N represent the totalnumber of instances evaluated. For each instance,|sn sn| measures the absolute difference betweenthe predicted and actual values. If this error sur-passes T, the instance does not contribute to thescore. Higher accuracy and DCA scores indicatebetter-performing systems.",
  "Divide-and-Conquer Results": "In , we compare the analytical reasoning ca-pabilities of leading LLMs.6 Our evaluation showsthat both GPT-4o and Claude-3-Opus are effectivefor this task, with Claude-3-Opus outperformingespecially in monolithic processing Mono scenar-ios, where the entire game narrative is processed asa whole. Here, Claude-3-Opus achieves 67.20% ac-curacy and a DCA score of 93.56%. This indicatesthat the model provides exactly accurate results inover 60% of cases and is within a minimal errormargin (T = 0) in >90% of the scenarios. This supe-rior performance may stem from Claude-3-Opussadvanced logical reasoning and computational abil-ities. Of the three variants in the Claude-3 modelfamily (Haiku, Sonnet, and Opus), Opus is the mostsophisticated and costly. The expense for using thisAPI is $15 / million tokens for input and $75 / mil-lion tokens for output.On the other hand, GPT-4o excels when usingthe divide-and-conquer (DnC) strategy on human-written sports narratives. It achieves the highestaccuracy at 88.61% and a DCA score of 98.41%with a batch size of 10 (DnC-10). This suggests that 6The lineup includes both proprietary and open-sourcemodels: Llama3-8B-Instruct, Llama3-70B-Instruct (MetaAI,2024), OpenAIs GPT-4o and GPT-3.5-Turbo (OpenAI, 2024),Gemini-Pro-1.5 (GeminiTeam, 2023), and Anthropics Claude-3-Opus (Anthropic, 2024). These LLMs are available to theresearch community from March to May 2024. The modelscan perform complex analyses, handle multi-step procedures,and tackle higher-order math and reasoning tasks.",
  ": We experiment with varying tolerance levels(T = 0, 1, 3, 5, 10) to assess the impact of this parameter.Adjusting the tolerance threshold generally does notalter the relative rankings of the models": "incorporating innovative strategies like DnC canpotentially narrow the performance gaps amongtop-performing models. The open-source Llama3-70B-Inst model performs on par with GPT-4o andClaude-3-Opus. However, lesser models such asLlama3-8B-Inst, Gemini-Pro-1.5, and GPT-3.5-Turbo lag behind, with Llama3-8B-Inst deliveringthe lowest accuracy and DCA scores. Batch-Centric vs. Player-Centric Division.Wetest the performance of LLMs using different batchsizes by dividing the games play-by-plays intobatches of 1, 3, 10, or 30 plays, denoted as DnC-{1,3,10,30} in . Our findings revealed twokey insights. First, the optimal batch size variesbetween models. High-performing models suchas Claude-3-Opus, GPT-4o, and Llama3-70B-Instshow peak performance with a batch size of 10.In contrast, less robust models perform best withsmaller batches, such as 3 plays per batch. Thissuggests that finding the right balance between ac-curacy per batch and the total number of batches iscrucial for achieving optimal results.Notably, using the smallest batch size (1 play)did not yield the best performance. This was pri-marily because play-by-play data in small batcheswere often overshadowed by system messages andinstructions, resulting in models, particularly theLlama3-8B-Inst, generating hallucinated scores. In-creasing the batch size generally led to more scor-ing errors for all models. This emphasizes the needfor a strategic approach to selecting batch sizes aswell as enhancing models instruction followingabilities to improve their robustness. The DnC-Pmethod slightly enhances LLM accuracy comparedto processing the narrative as a whole (Mono),although it is not as effective as segmenting the nar-rative into smaller batches. Our findings suggestthat while LLMs excel in reasoning, their perfor-mance in information aggregation is weaker.",
  ": We compare human narratives with those ofSportsGen and few-shot prompting. Prompting gener-ates narratives deviating from typical basketball games": "Accuracy vs. DCA.In , we experimentwith varying tolerance levels (T = 0, 1, 3, 5, 10) toassess the impact of this parameter. We note thatadjusting the tolerance threshold generally does notalter the relative rankings of the models. However,increasing the tolerance tends to narrow the perfor-mance gap among different models. Importantly,accuracy scores alone may not be the most suitablemetric for evaluation. Lower-performing modelsoften result in accuracy in the low single digits.For example, the model Gemini-Pro-1.5 reports anaccuracy of just 4.58%. Its performance remainsthe lowest even when tolerance is increased to 10points (T=10), resulting in the lowest DCA score of18.56%. Such findings highlight Gemini-Pro-1.5sconsiderably weaker analytical reasoning capabili-ties even under tolerance settings.",
  "Human Versus Synthetic Narratives": "In , we compare human-written sports nar-ratives with those generated by SportsGen and afew-shot prompting approach. Due to resourcelimits, we set DH=400 for human narratives andDS=480 for each synthetic narrative set acrossall experiments. Few-shot prompting, detailed inAppendix A, uses existing play-by-plays to guideGPT-4o in creating synthesized game quarter. Thismethod can result in repetitive scoring actions, witha high S:NS Ratio of 1:0.4, deviating from typicalbasketball games. In contrast, SPORTSGEN offersenhanced controllability and conveniently gener-ates box scores during generation. We adjust theturn length to produce games with varying infor-mation densities, featuring S:NS ratios of 1:2, 1:3,1:4, and 1:5. With an S:NS ratio of 1:3, it mirrorsthe ratio found in actual human narratives and re-sulting in an average of 1,700 tokens per narrative.This makes SPORTSGEN a more practical optionfor creating realistic sports narratives.In , we report accuracy and DCA scoresfor SPORTSGEN narratives with a 1:3 S:NS ratio,utilizing three strategies: DnC-10 (dividing the nar-",
  ": (Left) Synthesized narratives with varyingS:NS ratios. (Middle) Narratives grouped by # of scor-ing actions. (Right) Narratives grouped by # of tokens": "rative into batches of 10 actions), DnC-P (divisionby individual players), and monolithic processing(Mono). We observe that accuracy for synthetic nar-ratives tends to be lower than for NBA narratives.This may be due to the models greater familiaritywith NBA narratives, which may be included in thepretraining data, compared to our novel syntheticnarratives. Further, our use of GPT-4o enhanced ac-tion templates introduces more lexical variety thanthe typical NBA narrative, creating a more chal-lenging task for the models in accurately trackingteam scores. Overall, Claude-3-Opus achieves thehighest performance across all settings when mea-sured by DCA scores, suggesting its effectivenesswhere some margin of error is permissible. Qualitative Evaluation. We conduct a qualitativecomparison between SPORTSGEN and real NBAnarratives. A human expert or GPT-4 review 100narrative snippets from each source, with each snip-pet containing 10 plays. The evaluation focuses on(a) the naturalness of the language and (b) the logi-cal coherence of the narratives. Evaluators choosetheir preferred narrative or opt for a tie, also provid-ing justifications. We shuffle the order of pairs toreduce bias in this process, i.e., it is not guaranteedthat Snippet A Snippet B when (A, B) is pre-sented to the human or system. The results showthat our human expert prefers SportsGen narratives39% of the time, tied 9%, and favors NBA narra-tives 52% of the time. GPT-4s analysis was quitesimilar, with 39% preference for SportsGen, 7%ties, and 54% for NBA narratives. These insightssuggest that while SportsGen is aligning with hu-man narrative standards, some gaps remain.",
  "Frequent scoring presents a challenge for LLMs intracking and aggregating team points, especially": "during intense periods, e.g., the last quarter of a bas-ketball game, where scoring spikes. In this study,we examine how models handle game quarters withvarying levels of information density. We applythree settings: (a) four synthesized narrative setswith S:NS ratios of 1:5, 1:4, 1:3, and 1:2, where1:2 represents a higher proportion of relevant in-formation (see , left); (b) we group syn-thesized narratives into four quartiles based on thetotal number of scoring actions (middle); and (c)for comparison, we also group narratives into fourquartiles based on their length, measured in tokencount (see , right).In , we report DCA scores for Llama3-8B-Inst and Llama3-70B-Inst to see how LLMsperform across various information densities. Wenote a decrease in performance as the models pro-cess inputs from low to high levels of informationdensity, particularly in Llama3-70B-Inst, whichexhibits the most noticeable decline at high S:NSratios, such as 1:2 or 1:3. When analyzing narra-tives by length, the models exhibit a more grad-ual performance decrease. Our analysis indicatesthat a models analytical reasoning capabilities arelinked to the quantity and density of relevant facts,echoing Greg Kamradts needle-in-a-haystack test,where the challenge is to retrieve a single fact froman LLMs extensive context window.7 OriginalScrambled FictionalSymbolic DCA (%) Claude3Opus GPT4o Llama370BInst Llama38BInst GPT3.5Turbo GeminiPro1.5",
  "7github.co/gkamradt/LLMTest_NeedleInAHaystack": "alter the context to convert the original narratives toa more symbolic format. Original are syntheticnarratives using NBA team profiles; Scrambledmismatch NBA players and teams; Fictional re-places NBA player names with imaginary onesfrom FIFA; and Symbolic substitutes players withlabels like Player 1, 2, 3.... These changes arenot expected to impact the models ability to under-stand scoring actions or attributing them to teams.The actions described in play-by-plays remain un-changed as they characterize relationships betweensymbols, such as Player 1 assists Player 2. Thismethod makes the game input less reliant on naturallanguage and more on symbolic elements.We examine the models ability to calculate teampoints using synthetic narratives and present theDCA scores in . Our findings indicate thatreplacing original players with fictional names orindices can variably impact model performance.Models such as Llama3-8b-Inst and Gemini-Pro-1.5 score too low for us to draw definitive conclu-sions. Even high-performing models such as GPT-4o and Llama3-70b-Inst experience some declinein performance. Claude-3-Opus has demonstratedgreater resilience than GPT-4o when faced withaltered contexts. These results suggest that whileLLMs have substantial reasoning skills, they relyon natural language context to perform well; thus,symbolic reasoning has room for improvement.",
  "Conclusion": "We investigate LLMs analytical reasoning abilities,using divide-and-conquer strategies to determinewhere they perform best in analyzing sports nar-ratives. With SPORTSGEN, we further generate avariety of narratives that vary in style, complexity,and detail to better assess LLM performance. Ourresearch not only paves the way for future LLMassessments but also sets an understanding of howLLMs may handle complex, longitudinal data.",
  "Limitations": "This study offers important insights into how LLMshandle sports narratives and there are a few limita-tions to consider. The use of synthesized narrativesfrom SPORTSGEN, while valuable for controlledexperimentation, may not capture all the nuancesof natural game commentary. Differences betweenour synthetic data and real-world narratives couldinfluence how well findings generalize to naturalsettings. Additionally, our analysis is confined to basketball, a sport with frequent scoring events,which may not be representative of other sportsthat feature different dynamics and scoring pat-terns. Therefore, the broader applicability of ourresults to such diverse contexts is yet to be explored.Our future work may explore qualitative aspectssuch as strategic analysis or player performancethat go beyond numerical scores. While acknowl-edging these limitations, our study still enrichesthe discussion about applying advanced reasoningtechniques to sports narratives and sets the stagefor future research in this exciting area.",
  "Anthropic. 2024.Introducing the next generationof claude. Accessed on: Mar 4, 2024": "Simran Arora, Brandon Yang, Sabri Eyuboglu, AvanikaNarayan, Andrew Hojel, Immanuel Trummer, andChristopher R. 2023. Language models enable sim-ple systems for generating structured views of hetero-geneous data lakes. Preprint, arXiv:2304.09433. Lukas Berglund, Meg Tong, Max Kaufmann, MikitaBalesni, Asa Cooper Stickland, Tomasz Korbak,and Owain Evans. 2024. The reversal curse: Llmstrained on \"a is b\" fail to learn \"b is a\". Preprint,arXiv:2309.12288.",
  "Ryan Burnell, Han Hao, Andrew R. A. Conway, andJose Hernandez Orallo. 2023. Revealing the structureof language model capabilities": "Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song,Xiaoyang Wang, Dong Yu, and Jianshu Chen.2023. Skills-in-context prompting: Unlocking com-positionality in large language models.Preprint,arXiv:2308.00304. Zhiyu Chen, Wenhu Chen, Charese Smiley, SameenaShah, Iana Borova, Dylan Langdon, Reema Moussa,Matt Beane, Ting-Hao Huang, Bryan Routledge, andWilliam Yang Wang. 2022.Finqa: A dataset ofnumerical reasoning over financial data. Preprint,arXiv:2109.00122.",
  "Abhishek Divekar and Greg Durrett. 2024. Synthesizrr:Generating diverse datasets with retrieval augmenta-tion. Preprint, arXiv:2405.10040": "Quyet V. Do, Tianqing Fang, Shizhe Diao, ZhaoweiWang, and Yangqiu Song. 2024. ConstraintChecker:A plugin for large language models to reason oncommonsense knowledge bases. In Proceedings ofthe 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 714731, St. Julians, Malta.Association for Computational Linguistics. Jinhao Duan, Renming Zhang, James Diffenderfer,Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin,Mohit Bansal, Tianlong Chen, and Kaidi Xu. 2024.Gtbench: Uncovering the strategic reasoning limita-tions of llms via game-theoretic evaluations. Amosse Edouard, Elena Cabrio, Sara Tonelli, and NhanLe-Thanh. 2017. Youll never tweet alone: Buildingsports match timelines from microblog posts.InProceedings of the International Conference RecentAdvances in Natural Language Processing, RANLP2017, pages 214221, Varna, Bulgaria. INCOMALtd. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, andDragomir Radev. 2019. Multi-news: A large-scalemulti-document summarization dataset and abstrac-tive hierarchical model. In Proceedings of the 57thAnnual Meeting of the Association for ComputationalLinguistics, pages 10741084, Florence, Italy. Asso-ciation for Computational Linguistics. Gregory Furman, Edan Toledo, Jonathan Shock, and JanBuys. 2022. A sequence modelling approach to ques-tion answering in text-based games. In Proceedingsof the 3rd Wordplay: When Language Meets GamesWorkshop (Wordplay 2022), pages 4458, Seattle,United States. Association for Computational Lin-guistics. Vedant Gaur and Nikunj Saunshi. 2023. Reasoning inlarge language models through symbolic math wordproblems. In Findings of the Association for Com-putational Linguistics: ACL 2023, pages 58895903,Toronto, Canada. Association for Computational Lin-guistics.",
  "GeminiTeam. 2023. Gemini: A family of highly capa-ble multimodal models. Preprint, arXiv:2312.11805": "DemianGholipourGhalandari,ChrisHokamp,Nghia The Pham, John Glover, and Georgiana Ifrim.2020. A large-scale multi-document summarizationdataset from the Wikipedia current events portal.In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages13021308, Online. Association for ComputationalLinguistics. Yi Gu, Shunyu Yao, Chuang Gan, Josh Tenenbaum, andMo Yu. 2022. Revisiting the roles of text in textgames. In Findings of the Association for Computa-tional Linguistics: EMNLP 2022, pages 68676876,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics.",
  "Yebowen Hu, Kaiqiang Song, Sangwoo Cho, XiaoyangWang, Hassan Foroosh, Dong Yu, and Fei Liu. 2024a.Can large language models do analytical reasoning?Preprint, arXiv:2403.04031": "Yebowen Hu, Kaiqiang Song, Sangwoo Cho, XiaoyangWang, Hassan Foroosh, Dong Yu, and Fei Liu. 2024b.Sportsmetrics: Blending text and numerical data tounderstand information fusion in llms.Preprint,arXiv:2402.10979. Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu,Daoan Zhang, Hassan Foroosh, Dong Yu, and Fei Liu.2024c. Define: Enhancing llm decision-making withfactor profiles and analogical reasoning. Preprint,arXiv:2410.01772.",
  "Hong Jin Kang, Fabrice Harel-Canada, Muham-mad Ali Gulzar, Violet Peng, and Miryung Kim.2024.Human-in-the-loop synthetic text data in-spection with provenance tracking.Preprint,arXiv:2404.18881": "Osman Doruk Kicikoglu, Richard Bartle, Jon Chamber-lain, Silviu Paun, and Massimo Poesio. 2020. Ag-gregation driven progression system for GWAPs. InWorkshop on Games and Natural Language Process-ing, pages 7984, Marseille, France. European Lan-guage Resources Association. Logan Lebanoff, Bingqing Wang, Zhe Feng, and Fei Liu.2021. Modeling endorsement for multi-documentabstractive summarization. In Proceedings of theThird Workshop on New Frontiers in Summarization,pages 119130, Online and in Dominican Republic.Association for Computational Linguistics. Soochan Lee and Gunhee Kim. 2023. Recursion ofthought: A divide-and-conquer approach to multi-context reasoning with language models. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 623658, Toronto, Canada.Association for Computational Linguistics.",
  "Stephanie M. Lukin, editor. 2020. Workshop on Gamesand Natural Language Processing. European Lan-guage Resources Association, Marseille, France": "Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, andJiawei Han. 2020. Multi-document summarizationwith maximal marginal relevance-guided reinforce-ment learning. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 17371751, Online. As-sociation for Computational Linguistics. Jack Merullo, Luke Yeh, Abram Handler, Alvin Gris-som II, Brendan OConnor, and Mohit Iyyer. 2019.Investigating sports commentator bias within a largecorpus of American football broadcasts. In Proceed-ings of the 2019 Conference on Empirical Methods",
  "Ratish Puduppully, Li Dong, and Mirella Lapata. 2019": "Data-to-text generation with entity modeling. In Pro-ceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 20232035, Florence, Italy. Association for ComputationalLinguistics. Thomas Savage, Ashwin Nayak, Robert Gallo, EkanathRangan, and Jonathan H Chen. 2023. Diagnosticreasoning prompts reveal the potential for large lan-guage model interpretability in medicine. Preprint,arXiv:2308.06834.",
  "Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri,and Greg Durrett. 2024. Musr: Testing the limitsof chain-of-thought with multistep soft reasoning.Preprint, arXiv:2310.16049": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R. Brown, Adam Santoro, Aditya Gupta,Adri Garriga-Alonso, Agnieszka Kluska, AitorLewkowycz, Akshat Agarwal, Alethea Power, AlexRay, Alex Warstadt, Alexander W. Kocurek, AliSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish,Allen Nie, Aman Hussain, Amanda Askell, AmandaDsouza, Ambrose Slone, Ameet Rahane, Ananthara-man S. Iyer, Anders Andreassen, Andrea Madotto,Andrea Santilli, Andreas Stuhlmller, and AndrewDai. 2023. Beyond the imitation game: Quantifyingand extrapolating the capabilities of language models.Preprint, arXiv:2206.04615.",
  "Craig Thomson, Ehud Reiter, and Barkavi Sundararajan.2023. Evaluating factual accuracy in complex data-to-text. Computer Speech & Language, 80:101482": "Chris van der Lee, Emiel Krahmer, and Sander Wubben.2017. PASS: A Dutch data-to-text system for soc-cer, targeted towards specific audiences. In Proceed-ings of the 10th International Conference on NaturalLanguage Generation, pages 95104, Santiago deCompostela, Spain. Association for ComputationalLinguistics. Veniamin Veselovsky, Manoel Horta Ribeiro, AkhilArora, Martin Josifoski, Ashton Anderson, andRobert West. 2023. Generating faithful synthetic datawith large language models: A case study in compu-tational social science. Preprint, arXiv:2305.15041.",
  "Xuezhi Wang and Denny Zhou. 2024. Chain-of-thoughtreasoning without prompting": "Zhe Wang, Petar Velickovic, Daniel Hennes, NenadTomaev, Laurel Prince, Michael Kaisers, YoramBachrach, Romuald Elie, Li Kevin Wenliang, Fed-erico Piccinini, William Spearman, Ian Graham,Jerome Connor, Yi Yang, Adri Recasens, MinaKhan, Nathalie Beauguerlange, Pablo Sprechmann,Pol Moreno, Nicolas Heess, Michael Bowling, DemisHassabis, and Karl Tuyls. 2023b. Tacticai: an ai assis-tant for football tactics. Preprint, arXiv:2310.10553. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Preprint,arXiv:2201.11903. Sam Wiseman, Stuart Shieber, and Alexander Rush.2017. Challenges in data-to-document generation.In Proceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing, pages22532263, Copenhagen, Denmark. Association forComputational Linguistics.",
  "Michihiro Yasunaga, Xinyun Chen, Yujia Li, PanupongPasupat, Jure Leskovec, Percy Liang, Ed H. Chi,and Denny Zhou. 2024. Large language models asanalogical reasoners. Preprint, arXiv:2310.01714": "Haoxuan You, Rui Sun, Zhecan Wang, Long Chen,Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang,and Shih-Fu Chang. 2023. IdealGPT: Iteratively de-composing vision and language reasoning via largelanguage models. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1128911303, Singapore. Association for Computa-tional Linguistics. Dongxu Zhang, Varun Prashant Gangal, Barrett MartinLattimer, and Yi Yang. 2024. Enhancing hallucina-tion detection through perturbation-based syntheticdata generation in system responses. In Proceedingsof the 62th Annual Meeting of the Association forComputational Linguistics. Association for Compu-tational Linguistics.",
  "Hongyu Zhao, Kangrui Wang, Mo Yu, and HongyuanMei. 2023a. Explicit planning helps language modelsin logical reasoning": "Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan,Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang,Rui Zhang, and Arman Cohan. 2023b. Docmath-eval:Evaluating numerical reasoning capabilities of llmsin understanding long documents with tabular data.Preprint, arXiv:2311.09805. Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and DennyZhou. 2023. Take a step back: Evoking reasoningvia abstraction in large language models. Preprint,arXiv:2310.06117. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, ChaoWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, andTat-Seng Chua. 2021. Tat-qa: A question answeringbenchmark on a hybrid of tabular and textual contentin finance. Preprint, arXiv:2105.07624.",
  "AExample Prompts": "In this section, we present sample prompts used inour experiments. Particularly, few-shot promptinguses existing play-by-plays to guide GPT-4o increating synthesized game quarters. This methodtends to result in repetitive scoring actions. 0.4483 0.2672 0.0517 0.0345 0.0603 0.0086 0.0172 0.0172 0.0086 0.3822 0.0068 0.0701 0.4129 0.0105 0.0111 0.0473 0.0120 0.0089 0.0123 0.0018 0.0946 0.0726 0.1594 0.0367 0.0042 0.0003 0.0029 0.0006 0.0092 0.0005 0.0008 0.0002 0.0002 0.1837 0.0553 0.0254 0.0090 0.0007 0.0007 0.0007 0.1847 0.3163 0.0057 0.0105 0.0009 0.0005 0.0005 0.0003 0.3158 0.0406 0.2041 0.0193 0.0033 0.0166 0.0086 0.0066 0.0007 0.0040 0.0013 0.9818 0.0091 0.1315 0.0854 0.0035 0.0254 0.0023 0.0012 0.0035 0.0012 0.0012 0.0845 0.1997 0.0046 0.0092 0.0061 0.0015 0.3429 0.0113 0.3715 0.0287 0.0305 0.0028 0.0042 0.0038 0.0136 0.0061 0.0005 0.0028 0.0682 0.0682 0.0909 0.0245 0.2820 0.2725 0.0559 0.0068 0.0286 0.0150 0.0041 0.0082 0.0027 0.0014 0.4043 0.2979 0.1064 0.0213 0.0426 0.1064 0.0432 0.0378 0.0703 0.0081 0.0014 0.0027 0.0667 0.1333 0.0286 0.0095 0.1290 start vs end defensiverebound makefoul miss team rebound offensive charge turnover badpass offensiverebound kicked ball violation timeout defensive goaltending violation personal foul traveling block : An example action graph created from NBA narratives. When a team has the ball, they are on offense anduse actions such as passing, dribbling, shooting to score points. The defense tries to stop them by blocking, stealing,and rebounding. We refer to each sequence of these actions as a turn, and characterize it using a Markov graph. Inthe graph, each significant action is represented as a node, and transitions between nodes show how team memberscooperate and execute their tactics. The node vs denotes a matchup between two players, such as Nikola Jokic vs.Anthony Davis, indicating their direct competition in key moments of the game. The graph begins and ends withspecial nodes that mark the start and end of a turn.",
  "SportsGen Narratives (S:NS Ratio = 1:3)": "<Example 1>3:20Zach LaVine secured the defensive rebound.3:20Nikola Vucevic performs a dunk with an assist from Lonzo Ball.2:59LaMelo Ball fails to make a 17-foot jump shot.2:56Cody Martin secures an offensive rebound.2:38Mark Williams shoots and scores a flagrant 1 free throw.2:34Mark Williams committed a personal foul.2:33Lonzo Ball secures a defensive rebound.2:19DeMar DeRozan misses a 20-foot jumper.2:17Miles Bridges secures a defensive rebound.2:14Mark Williams fails to make a 17-foot pullup jump shot. <Example 2>3:00Jonathan Kuminga executes a tip shot.2:57Franz Wagner secured the defensive rebound.2:46Cole Anthony hit a 26-foot three pointer.2:39Stephen Curry fails to make a 29-foot three-point shot.2:38Golden State Warriors offensive team rebounded the ball.2:37Franz Wagner secured the defensive rebound.2:22Cole Anthony fails to make a 10-foot two-point shot.2:19Stephen Curry secures the defensive rebound.2:00Stephen Curry fails to connect on a 27-foot three-point shot.1:57Wendell Carter Jr. fails to make a 17-foot pullup jump shot.",
  "Few-Shot Prompting Narratives": "<Example 1>2:30Clint Capela makes 2-foot layup (Trae Young assists)2:15Jaylen Brown makes 3-point jumper (Derrick White assists)2:00Dejounte Murray makes 20-foot jumper1:45Jayson Tatum makes 18-foot jumper1:30Trae Young makes 3-point jumper1:15Kristaps Porzingis makes 10-foot jumper1:00Dejounte Murray makes 18-foot jumper0:45Jaylen Brown makes 2-foot layup (Jayson Tatum assists)0:30Trae Young makes 3-point jumper0:15Jayson Tatum makes 20-foot jumper <Example 2>8:13Rui Hachimura defensive rebound8:00DAngelo Russell makes 20-foot jumper7:45Devin Vassell makes 25-foot three point jumper (Tre Jones assists)7:30LeBron James makes driving layup7:15Keldon Johnson makes 10-foot jumper7:00Anthony Davis makes 15-foot jumper6:45Victor Wembanyama makes 12-foot jumper6:30Austin Reaves makes 25-foot three point jumper (DAngelo Russell assists)6:15Devin Vassell misses 20-foot jumper6:13Anthony Davis defensive rebound",
  "To determine the total points scored by each team, we need to examine the play-by-playdescriptions and identify which team scored the points in each play. Heres the breakdown:": "1. Jakob Poeltl makes layup (Keldon Johnson assists) - San Antonio Spurs score 2 points.2. Gordon Hayward makes 15-foot pullup jump shot - Charlotte Hornets score 2 points.3. Gordon Hayward makes free throw 1 of 1 - Charlotte Hornets score 1 point.4. Terry Rozier makes 25-foot three point jumper (Mason Plumlee assists) - Charlotte Hornetsscore 3 points.5. Jalen McDaniels makes layup (Terry Rozier assists) - Charlotte Hornets score 2 points.6. Devin Vassell makes free throw 1 of 2 - San Antonio Spurs score 1 point.7. Devin Vassell makes free throw 2 of 2 - San Antonio Spurs score 1 point.8. Mason Plumlee makes alley oop dunk shot (James Bouknight assists) - Charlotte Hornets score2 points.9. Gordon Hayward makes free throw 1 of 2 - Charlotte Hornets score 1 point.10. Gordon Hayward makes free throw 2 of 2 - Charlotte Hornets score 1 point.",
  "To determine the total points scored by each team, Ill analyze the play-by-play descriptionsand identify the scoring plays": "Charlotte Hornets:1. Gordon Hayward makes 15-foot pullup jump shot (5:53) - 2 points2. Terry Rozier makes 25-foot three point jumper (5:32) - 3 points3. Jalen McDaniels makes layup (5:06) - 2 points4. Mason Plumlee makes alley oop dunk shot (4:43) - 2 points5. Gordon Hayward makes free throw 1 of 2 (4:24) - 1 point6. Gordon Hayward makes free throw 2 of 2 (4:24) - 1 pointTotal points for Charlotte Hornets: 2 + 3 + 2 + 2 + 1 + 1 = 11 points San Antonio Spurs:1. Jakob Poeltl makes layup (6:10) - 2 points2. Devin Vassell makes free throw 1 of 2 (4:55) - 1 point3. Devin Vassell makes free throw 2 of 2 (4:55) - 1 pointTotal points for San Antonio Spurs: 2 + 1 + 1 = 4 points",
  "System Message": "Your task is to generate detailed play-by-play descriptions for one quarter of an NBA game,capturing the dynamic flow of the game. This includes player actions (e.g., shooting, blocking,stealing, etc.), key moments (e.g., turnovers, fouls, timeouts), player substitutions, and more.Please ensure the narrative is engaging and reflects the pace and intensity of an NBA game.",
  "Generate a full quarter of an NBA game using the detailed player-team affiliations andplay-by-play data provided. Ensure the following requirements are met:": "1. Typically, a quarter of NBA games feature 70 to 150 plays, with 20 to 40 being scoringplays, and a scoring density between 0.25 and 0.35.2. A teams efficiency score, showing potential points per 100 possessions,can impact game outcomes.3. Adhere to the example play-by-play description format provided.4. Ensure a natural and logical progression between plays.5. Separate each column in the play-by-play data using a tab character.",
  "#Example Play-by-Play Descriptions:": "TimeTeamPlay12:00Team-2player-3 vs. player-14 (player-13 gains possession)11:46Team-2player-12 makes alley oop layup (player-14 assists)11:26Team-1player-2 makes 2-foot layup (player-3 assists)11:08Team-2player-12 makes driving layup10:43Team-1player-6 misses 21-foot step back jumpshot10:41Team-1player-2 offensive rebound10:37Team-2player-13 misses 25-foot three point jumper10:34Team-2player-15 defensive rebound10:27Team-2player-14 shooting foul10:27Team-2player-7 misses free throw 1 of 210:27Team-1team-1 offensive team rebound10:27Team-2player-12 makes free throw 2 of 210:14Team-2player-18 misses 23-foot three point jumper10:11Team-1player-6 defensive rebound..."
}