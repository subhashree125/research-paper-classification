{
  "Abstract": "Visual representation learning has been a cor-nerstone in computer vision, involving typicalforms such as visual embeddings, structuralsymbols, and text-based representations. De-spite the success of CLIP-type visual embed-dings, they often lack access to world knowl-edge critical for visual reasoning. In this work,we propose Visual Table, a novel form of vi-sual representation tailored for visual reason-ing. Visual tables are constructed as hierarchi-cal descriptions of visual scenes, featuring ascene description and multiple object-centricdescriptions covering categories, attributes, andknowledge. Thanks to the structural and tex-tual formats, visual tables offer unique prop-erties over mere visual embeddings, such asexplainability and controllable editing. Further-more, they deliver instance-level world knowl-edge and detailed attributes that are essentialfor visual reasoning. To create visual tables,we develop a generator trained on the datasetwith collected, small-scale annotations. Ex-tensive results on 11 visual reasoning bench-marks demonstrate that the generated visualtables significantly outperform previous struc-tural and text-based representations.More-over, they consistently enhance state-of-the-art multi-modal large language models acrossdiverse benchmarks, showcasing their poten-tial for advancing visual reasoning tasks. Ourcode is available at",
  "equal contributions, corresponding author": "Internet (Radford et al., 2021; Jia et al., 2021).Beyond visual embeddings, symbolic and struc-tured visual representations (e.g., scene graph) (Xuet al., 2017; Krishna et al., 2017) exhibited ad-vantages across domains, such as vision-languagetasks (Teney et al., 2017; Hudson and Manning,2019a; Zhong et al., 2020), video and 3D sceneunderstanding (Yang et al., 2023a; Armeni et al.,2019; Wald et al., 2020), and robotics (Rana et al.,2023; Gu et al., 2023; Kalithasan et al., 2023).More recently, some works have strived to con-vert visual scenes into text-based representations(e.g., image captions) (Hu et al., 2022; Yang et al.,2022; Shao et al., 2023; Khademi et al., 2023), trig-gering the reasoning capability of large languagemodels (LLMs) (Ouyang et al., 2022; Zhang et al.,2022; Touvron et al., 2023; Chiang et al., 2023).Among these visual representations, CLIP-typevisual embeddings (Radford et al., 2021), learnedfrom image-text pairs, have dominated many visiontasks. Their success can be attributed to robust gen-eralization in encoding visual attributes (e.g., visualappearance (Yang et al., 2023b; Pratt et al., 2023;Yan et al., 2023), visual relations (Zhao et al., 2023;Li et al., 2024; Zhong et al., 2023; Momeni et al.,2023)). However, lacking external world knowl-edge (e.g., object affordance, background knowl-edge of named entities), these visual embeddingslead to sub-optimal performance in complex rea-soning tasks. Consider the example at the left topof . While CLIP embeddings can effectivelycapture visual attributes (e.g., recognizing a personwith a beard), they still struggle to answer questionsthat necessitate critical world knowledge beyondthe image itself (Marino et al., 2019; Schwenk et al.,2022; Chen et al., 2023c; Yue et al., 2023).To this end, we propose Visual Table, a novelform of visual representation designed for visualreasoning. As shown in the left-bottom of , avisual table is presented as hierarchical text descrip-tions for visual scenes, and comprises a scene de- Knowledge from Our Visual Table: Currency: Used as a medium of exchange, legal tender in the United States, a historical figure important to the nation's history Portrait: Represents Abraham Lincoln, the 16th President of the United States, known for leading the country through the Civil War and issuing the Emancipation Proclamation Questions: Who is the person in the image? What can the thing shown in the image be used for? What is the face value of this banknote? Knowledge from LLMs or Knowledge Bases: Currency: A system of money used in a particular country or region for transactions, serving as a medium of exchange Portrait: Typically a painting, photograph, or other artistic medium, that depicts the likeness of a person or group, often focusing on their facial features and expressionsInput Image",
  "Visual Table Generator": "Scene description: This is an image of a US five-dollar bill featuring the portrait of Abraham Lincoln. Category: Currency; Attribute: Paper material, features a portrait, textual inscriptions, security threads, and symbols; Knowledge: Used as a medium of exchange, legal tender in the United States, and bears the likeness of a historical figure important to the nation's history. Category: Portrait; Attribute: Depicts a male with a beard, wearing a black suit and a bow tie; Knowledge: Represents Abraham Lincoln, the 16th President of the United States, known for leading the country through the Civil War and issuing the Emancipation Proclamation. Multi-modal Large Language Model with Generated Visual Tables",
  "Related Work": "Visual Representation Learning. Early worksfocus on training image classifiers using labor-intensive image labels (Krizhevsky et al., 2012; Si-monyan and Zisserman, 2015; Szegedy et al., 2015;He et al., 2016). As an augmentation, object andattribute labels are used to enhance visual represen-tation (Anderson et al., 2018; Zhang et al., 2021).To reduce the annotation cost, self-supervised learn-ing (He et al., 2020; Chen et al., 2020; Grill et al.,2020; He et al., 2022) is proposed to match thevisual representation of different views from thesame image. Moving forward, vision-language pre-training (Radford et al., 2021; Jia et al., 2021; Guet al., 2021; Zhong et al., 2022; Li et al., 2022) isproposed to match web-collected visual-text pairs,exhibiting generalizable capability on diverse vi-sual recognition tasks. Compared to these visualembeddings, our visual tables additionally encodeworld knowledge critical for visual reasoning, sup-port interpretability to humans and LLMs, andenable controllable editing.Beyond implicit embeddings, structural andsymbolic representations, such as image scenegraphs (Xu et al., 2017; Zellers et al., 2018; Tanget al., 2020; Shi et al., 2021; Zhong et al., 2021),have attracted significant attention. These worksaim to abstract visual scenes into concise repre-sentations, demonstrating special benefits in di-verse domains, including vision-language mod-eling (Yu et al., 2021; Pan et al., 2022; Mitraet al., 2023; Herzig et al., 2023) and various down-stream tasks (Hudson and Manning, 2019a; Zhonget al., 2020; Ji et al., 2020; Hughes et al., 2022;Kalithasan et al., 2023). Resembling the conceptof structural representations, visual tables are pre- sented in the hierarchical text, yet deliver richersemantics through free-form language.Another line of works explores text-based visualrepresentation (Hu et al., 2022; Wang et al., 2023b;Yang et al., 2022; Gui et al., 2021; Lin et al., 2022;Shao et al., 2023; Fu et al., 2023b; Khademi et al.,2023; Wang et al., 2022; Hakimov and Schlangen,2023). These methods typically convert visual in-puts into text (e.g., image captions, object tags),then retrieve knowledge from knowledge bases(e.g., Wikipedia (Vrandecic and Krtzsch, 2014),ConceptNet (Liu and Singh, 2004)) and/or frozenLLMs (e.g., GPT3), and finally perform text reason-ing using frozen LLMs with in-context examples.Unlike these works, our single generator modellearns to compress a comprehensive visual knowl-edge base, without the need for manually selectingoff-the-shelf vision libraries (e.g., image caption-ers, object detectors), external knowledge sources,or high-quality in-context examples. Multi-modal Large Language Models. MLLMsharness LLMs to empower reasoning on multi-modal tasks, typically on visual question answer-ing (VQA) (Li et al., 2023b; Fu et al., 2023a;Yin et al., 2023; Alayrac et al., 2022; Dai et al.,2023; Zhu et al., 2023; Chen et al., 2023a; Baiet al., 2023; IDEFICS, 2023; Huang et al., 2024; Liet al., 2023a). These methods usually learn layersthat connect visual encoder (Radford et al., 2021)and LLMs (Touvron et al., 2023; Chiang et al.,2023). Building on top of MLLMs, some worksseek to improve the quality of instruction-followingdata (Chen et al., 2023b; Wang et al., 2023a) or toenhance object perception by introducing controlsignals (Jain et al., 2023), while the others explorechain-of-thoughts idea (Kojima et al., 2022; Weiet al., 2022; Zhang et al., 2023; Zheng et al., 2024;Mitra et al., 2023). In parallel, we focus on visualrepresentation learning and the resulting visual ta-bles serve as inputs of MLLMs and above methods.",
  "Visual Table": "We introduce Visual Table, a new form of vi-sual representation, constructed in hierarchical text.Given an image I, a visual table VT = g(I) iscreated by a generator g. VT consists of a scenedescription and multiple object-centric descriptionsthat encompass categories, attributes, and knowl-edge. Thanks to structural and textual formats,visual tables support interpretability to humans andLLMs, and enable controllable table editing. More- Visual Encoder (CLIP) Connector (MLP) Large Language Model (Vicuna) Based on the given image, generate the visual table that follows the following JSON format:",
  "Train": ": An overview of learning a visual table generator and its application on MLLMs. Left: We design a promptto collect visual table annotations on a small scale of images. These annotations are used to train our visual tablegenerator which consists of a frozen visual encoder, a vision-language connector, and a pre-trained LLM. Right:Our generator is employed to generate visual tables for the images in downstream tasks. With the generated visualtable either as standalone or as additional visual representations, LLM performs reasoning to output text response. over, they capture visual scenes at a level of granu-larity (e.g., instance-level knowledge) and richness(e.g., free-form language). These characteristicsrender visual tables potentially beneficial to a broadspectrum of vision tasks. provides an overview of visual table gener-ation and its application on image reasoning usingMLLMs. We first collect a small number of visualtable annotations (Sec. 3.1) and then train a visualtable generator g (Sec. 3.2). The trained generatoris able to automatically produce visual tables forany input images in downstream tasks. We evalu-ate the efficacy of our generated visual tables ondiverse benchmarks where visual tables serve asvisual representations (Sec. 3.3).",
  "Training Data Collection": "To create visual table annotations, a straightfor-ward solution is to ask human annotators to labelimages with the required visual table content. De-spite high quality, training human annotators andcleaning their annotations can be expensive andcumbersome. To mitigate these burdens, we opt foran alternative solution by leveraging powerful foun-dation models with near-human capabilities, suchas GPT4V (OpenAI, 2023). We design a detailedprompt to ensure the annotations closely align withour requirements, resulting in a small scale of vi-sual tables, i.e., on 61K images (Lin et al., 2014).Prompt design. We design a prompt consistingof performing four tasks on input images: scenedescription generation, object category recognition,attribute description generation, and knowledge de- scription generation. Without losing generality, weprovide basic definitions and guidelines for eachtask to ensure versatile descriptions. For example,we define object affordance as the functions sup-ported by objects and remind that affordance mightbe altered case by case, due to deformed shape,unreliable materials, and so on.At the end of the prompt, we specify that theoutput should be in JSON format, i.e., a nested dic-tionary. This structure design enables controllablemanipulation on visual tables, such as studyingthe effects of removing table components. The fullprompt and the collected visual tables can be foundin the appendix. Statistics. We collect a visual table per image,in total 61K images from the COCO dataset (Linet al., 2014). These images are selected based onthe scheme that an image should be associated withat least two instruction-following responses in theLLaVA-Instruct-158K dataset (Liu et al., 2023a).On average, there are 458 text tokens in each visualtable after the tokenization process.",
  "p(Ta|I, Tinstruct) = Li p(ti|h(I), Tinstruct,<i, Ta,<i), (1)": "where L denotes sequence length, denotes train-able parameters (e.g., connector and LLM), h repre-sents frozen visual encoder, Tinstruct,<i and Ta,<iare text tokens of instructions and answers beforethe current prediction token ti, respectively. Training and Inference. Our generator is trainedon the collected visual table annotations and partialtraining data from LLaVA-1.5 (Liu et al., 2023a).Specifically, there are three training stages: (1)Visual-language alignment: With visual encoderand LLM frozen, connector is trained on 595Kimage-text pairs by instructing the model to gen-erate captions. (2) Instruction fine-tuning: Withvisual encoder frozen, connector and LLM aretrained on 199K GPT-generated instruction-tuningdata. (3) Supervised fine-tuning: With visual en-coder frozen, connector and LLM are trained onour 61K visual table annotations. Once trained, ourgenerator can automatically generate a visual tableas the visual representation of any given image.Note that, we avoid using any human annotationsfrom visual question answering (VQA) datasetsduring training, i.e., 467K VQA instances that wereused to optimize VQA performance of MLLMs.This strategy aims to minimize the biases fromVQA tasks, thereby allowing the resulting visualtables to potentially benefit a broader spectrum ofvisual tasks beyond VQA.",
  "Application on Image Reasoning": "With instance-level descriptions of attributes andknowledge, visual tables essentially create compre-hensive databases for individual visual scenes. Inthis work, we focus on its application in image rea-soning tasks and assess its impact on performancewithin the context of MLLMs.Specifically, visual tables are first generated forthe images. We then re-train the MLLMs according to their original training methodologies, using thegenerated visual tables as visual representations.We choose the LLaVA-1.5 as our main testbed. Themodel training follows its original pipeline, exceptthat at its second training stage (instruction-tuningstage), the LLM module takes the generated visualtables VT = g(I) as the input, and learns to predictnext token:",
  "p(Ta|I, Tinstruct) = Li p(ti|g(I), h(I), Tinstruct,<i, Ta,<i). (2)": "We highlight that our visual tables g(I) are textformats and thus can function as standalone visualrepresentations, without using visual embeddingsh(I). In this scenario, the generated visual tablescan be directly processed by pre-trained LLMswithout requiring the first training stage for visual-language embedding alignment. Our experimentsdemonstrate that using only visual tables alreadyachieves strong performance across diverse bench-marks, while combining both visual representationsleads to further performance improvements.",
  "Experiments": "In this section, we first introduce our implementa-tion details, benchmarks, and evaluation protocols,and then present our results, including the com-parison with traditional text-based representations,the comparison with SOTA MLLMs, the ablationstudy, the case study, and the human study. Implementation Details. We provide implemen-tation details for our visual table generator and theMLLM with visual tables. For visual table gen-erator, we adopt the same architecture of LLaVA-1.5 (Liu et al., 2023a), consisting of CLIP ViT-L/14@336px (Radford et al., 2021) as the visualencoder, Vicuna-13B (Chiang et al., 2023) as theLLM, and a two-layer MLP as the connector. Itis initialized by the LLaVA-1.5-13B model pre-trained by excluding VQA data, and further fine-tuned on 61K visual table annotations for 3 epochs,with batch size as 128, learning rate as 2e-5, andthe optimizer as AdamW. For the MLLM with vi-sual table, we adopt the same training pipeline asLLaVA-1.5, except that during the second trainingstage, our model is fine-tuned with the generatedvisual tables as additional visual representations.",
  "MethodLLM#PT#ITMM-VetLLaVAWMMMUMMBMMVPPOPEVizWizSQAIGQAVQAv2VQAT": "Representation: EBLIP-2 (Li et al., 2023c)V-13B129M-22.485.319.661.041.041.042.5InstructBLIP (Dai et al., 2023)V-7B129M1.2M26.236.034.560.549.250.1InstructBLIP (Dai et al., 2023)V-13B129M1.2M25.678.933.463.149.550.7Shikra (Chen et al., 2023a)V-13B600K5.5M58.877.4IDEFICS-9B (IDEFICS, 2023)L-7B353M1M48.235.538.450.925.9IDEFICS-80B (IDEFICS, 2023)L-65B353M1M54.536.045.260.030.9Qwen-VL (Bai et al., 2023)Q-7B1.4B50M38.235.267.159.378.863.8Qwen-VL-Chat (Bai et al., 2023)Q-7B1.4B50M60.638.968.257.578.261.5LLaVA-1.5 (Liu et al., 2023a)V-7B558K665K30.581.933.964.320.785.950.066.862.078.558.2LLaVA-1.5 (Liu et al., 2023a)V-13B558K665K35.486.739.567.733.385.953.671.663.380.061.3 Representation: TVicuna-VTV-7B665K28.782.033.261.919.381.252.467.355.373.451.1Vicuna-CapV-13B665K23.079.239.162.112.073.351.369.548.461.448.0Vicuna-DCapV-13B665K27.177.637.461.413.383.451.669.351.768.549.6Vicuna-SGV-13B665K28.177.036.559.311.382.351.068.952.067.849.3Vicuna-VTV-13B665K30.782.539.662.726.781.955.470.056.174.053.8 Representation: E + TLLaVA-VTV-7B558K665K31.885.034.368.024.086.550.567.963.179.859.7LLaVA-CapV-13B558K665K36.388.940.669.432.086.453.871.963.580.360.7LLaVA-DCapV-13B558K665K36.786.438.668.930.786.952.671.263.480.360.7LLaVA-SGV-13B558K665K36.186.840.669.230.087.357.571.863.180.361.1LLaVA-VTV-13B558K665K39.889.141.969.436.787.157.472.664.080.761.2 : Comparison with text-based representations and MLLMs. E, T and E + T denotes the visualrepresentations as visual embeddings, text-based representations (Cap: Short Caption; Dcap: Detailed Caption;SG: Scene Graph; VT: Visual Table) and their concatenation, respectively. #PT/#IT denotes the number of samplesin the stage-one/two training, respectively. V-7B/13B: Vicuna-7B/13B (Chiang et al., 2023); L-7B/13B: LLaMA-7B/13B (Touvron et al., 2023); Q-7B/13B: Qwen-7B/13B (Bai et al., 2023). Bold values refer to the best resultswithin each group. Visual table largely outperforms previous text-based representations and is the only textrepresentation that can consistently enhance SOTA MLLMs across diverse benchmarks. standing in the distance, partially obscured by theterrain are identified in the 6th and 7th examples,respectively, thereby facilitating the correct answer.Overall, visual tables not only offer interpretabil-ity to humans but also thoroughly describe the de-tailed objects in visual scenes, providing preciseattributes and rich knowledge. This comprehen-sive representation consistently enhances SOTAMLLMs across diverse benchmarks, as demon-strated in our experiments.",
  "marks that evaluate reasoning ability, while GQAfocuses more on visual attributes. TextVQA evalu-ates the OCR reasoning capability": "Evaluation Protocols. We adopt two widely-usedprotocols: (1) Exact-matching protocol matchesthe predicted answer string and ground-truthstring (Goyal et al., 2017; Hudson and Manning,2019b). (2) GPT-assisted protocol relies on GPTmodels to measure the correctness of the predicted,open-ended answer, given the question and ground-truth answer (Yu et al., 2023; Liu et al., 2023b).",
  "Tab. 1 shows the results of typical text-based repre-sentations, our visual tables, and recent MLLMs": "Setup.The exact-matching evaluation is uti-lized for academic VQA benchmarks, includingPOPE, VizWiz, ScienceQA, VQA-v2, GQA, andTextVQA. This evaluation protocol is also appliedto MMBench and MMVP benchmarks due to theirmultiple-choice settings. We use GPT-assisted eval-uation for the remaining benchmarks, includingMM-Vet (open-ended VQA, using the official GPT-4 evaluation server), LLaVA-Bench (open-endedVQA, using GPT-3.5-1106), and MMMU (open-ended VQA, with 855 VQA paris sampled from itsoriginal val split, using GPT-3.5-1106).",
  "Baselines. Visual table is designed as a structuraltext-based representation. We thus compare it with": "the commonly-used text-based representations, in-cluding: (1) Cap: We generate short captions us-ing BLIP2-OPT-2.7B (Li et al., 2023c), a captionerfrom the widely-used BLIP model family. LLaVA-1.5 also employs a captioner from this family tocreate its training data. (2) DCap: We use the samepre-trained LLaVA-1.5 model as our generator toproduce detailed captions since it already learnedimage captioning during training. The differenceis that our generator is further fine-tuned on visualtable annotations. (3) SG: Inspired by (Mitra et al.,2023), we utilize the same pre-trained LLaVA-1.5model as our generator to produce scene graphs,including visual attributes and visual relationships.Both scene graphs and visual tables are structuralrepresentations, while visual tables have richer se-mantic descriptions and cover object knowledge.With these text-based representations ready, wereplace visual tables and re-train MLLMs. Whenused as standalone visual representation, with-out the need for visual embeddings or the needfor visual-language alignment training, they aredenoted as Vicuna-Cap/DCap/SG, respectively.When combined with visual embeddings, they aredenoted as LLaVA-Cap/DCap/SG, respectively.Further, to validate whether visual tables canenhance existing MLLMs that have already exhib-ited incredible results on visual reasoning, we in-corporate baselines of recent MLLMs, e.g., BLIP-2 (Li et al., 2023c), InstructBLIP (Dai et al., 2023),Shikra (Chen et al., 2023a), IDEFICS (IDEFICS,2023), Qwen-VL (Bai et al., 2023) and LLaVA-1.5 (Liu et al., 2023a). Comparison with Text-based Representations.As Tab. 1 shows, no matter in Vicuna-VT setting orLLaVA-VT setting, visual tables significantly out-perform traditional text-based representations (e.g.,+2.6 on MM-Vet, +5.5 on LLaVA-Bench, +3.1 onMMMU, +15.4 on MMVP over Vicuna-SG). More-over, it is worth noting that visual table (LLaVA-VT) is the only text-based representation that canconsistently improve the base model LLaVA-1.5across benchmarks. These results reveal that, un-like previous text-based representations, visual ta-bles stand out as robust visual representations ca-pable of generalization across various scenarios. Consistent Improvements over SOTA MLLMs.Despite the challenge of further enhancing existingMLLMs, which have already demonstrated remark-able performance in visual reasoning, visual tables(LLaVA-VT) consistently achieve improvements",
  ": Ablation study on visual table components.We probe the components by editing visual tables andre-training our MLLM. Bold/underlined values refer tothe best/second-best results": "over SOTA MLLMs across diverse benchmarks(e.g., +4.4 on MM-Vet, +2.4 on LLaVA-Bench,+2.4 on MMMU, +3.4 on MMVP). These consis-tent enhancements across various benchmarks vali-date the generality of our visual table design andcan be attributed to the valuable information pro-vided by visual tables, such as world knowledge,which visual embeddings lack. Textual Representations vs. Visual Embeddings.It is not surprising that visual embeddings performbetter than text-based representations (LLaVA-1.5 vs. Vicuna-Cap/DCap/SG/VT). However, itis worth recognizing that text representations bynature offer unique advantages over visual em-beddings. For instance, textual formats can bedirectly utilized by LLMs without the need forvisual-language alignment (e.g., Vicuna-VT), offer-ing a solution for domains where alignment data isscarce, such as 3D vision. Textual formats are alsointerpretable to humans, facilitating applicationsthat require human-machine interaction. Moreover,by explicitly representing object knowledge in text,visual tables can support research such as embodiedAI that requires an understanding of object affor-dance (Gu et al., 2023). Overall, we believe thattext-based representations are worth exploring andour work suggests a promising avenue of represent-ing visual scenes in structural and rich text.",
  "Ablation Study": "Benefiting from the structural design, the compo-nents in visual tables can be effortlessly edited.This enables ablation experiments to probe the im-pacts of removing certain components in visualtables. We use the edited visual tables as visual rep-resentations and re-train MLLMs for comparisons. Setup. In the ablation study, we utilize 61K anno-tated visual tables to inspect the impacts of eachcomponent since they follow strict JSON formats.All MLLMs are first trained on 595K image-textpairs, and then fine-tuned on 177K instruction-following data with the edited visual tables as in- Question: Please explain the meme in detail.GT Answer: The meme features a dog lying on a cyan wooden floor, looking exhausted or unenthusiastic. The meme humorously captures the feeling of a lack of motivation and energy that is often associated with Mondays ... LLaVA: The image features a small dog lying on a wooden floor The caption reads \"Monday. Just Monday The dog appears to be relaxed and enjoying its time on the wooden surface. The scene captures a moment of leisure and contentment for the dog, as it spends its day indoors. LLaVA-VT: The image features a small dog lying on a wooden floor, appearing to be sleepy or tired. The dog is positioned in the center of the scene The meme is a play on words, with the phrase \"Just Monday\" written below the dog. This cleverly implies that the dog is expressing its feelings about the start of the week, as if it were a human. The meme is likely meant to evoke a sense of humor and relatability, as many people might feel tired or unenthusiastic about starting a new week. Generated Visual Table: Scene: an image of a dog lying; text that says \"MONDAY. JUST MONDAY.\"; suggesting a feeling of dread or reluctance associated with the start of the work week.Object \"Dog\": lying down with its head resting on the floor, possibly sad; often reflect the emotions of their owners, indicating a state of tiredness. Object \"Text\": added to express a sentiment about the start of the week; the use of text in images is a common way to convey additional messages or emotions that complement the visual content, it suggests a feeling of reluctance or negativity associated with the beginning of the work week. Object \"Wooden floor\": ...",
  "Question: What is the name of the colony shown? A. Maryland B. New Hampshire C. Rhode Island D. VermontGT Answer: BLLaVA: CLLaVA-VT: B": "Generated Visual Table: Scene: This is a map showing the eastern seaboard of the United States, with states highlighted in green. Object \"Map\": displays the eastern seaboard of the United States, with states outlined and shaded in green, overlaid on a base of water bodies and landforms; focus on the political boundaries of the states. Object \"States\": The states are outlined and shaded in green, indicating some sort of highlighting or grouping. The states are: Maine, New Hampshire, Vermont, Massachusetts, Rhode Island, Connecticut, New York, New Jersey, Delaware, Maryland, Virginia, North Carolina, South Carolina, Georgia, and Florida; States are the primary units of government in the United States. Object \"Water bodies\": shown in blue and include the Atlantic Ocean, the Gulf of Mexico, and various bays, straits, and inlets; also play a role in defining state boundaries along coastlines. Object \"Landforms\": ...",
  "Question: How many people are sitting down?GT Answer: oneLLaVA: There are two people sitting down in the image.LLaVA-VT: There is one person sitting down, an older man": "Generated Visual Table: Scene: An elderly man is sitting on a couch in a home setting, smiling as a young woman stands beside him, gently assisting or comforting him. Object \"Elderly man\":sitting, smiling, appears content; receiving care or companionship. Object \"Young woman\": standing, assisting or comforting the elderly man; engaged in a supportive or nurturing role. Object \"Couch\": ... Object \"Cushion\": ... Object \"Wall decoration\": ... Object \"Window\": ...",
  "Question: Which action is performed in this image? A. pushing car B. snowboarding C. biking through snow D. shoveling snowGT Answer: ALLaVA: BLLaVA-VT: A": "Generated Visual Table: Scene: A person is engaged in a physical activity, possibly pushing or lifting a car in a snowy parking lot. Object \"Person\": Engaged in a strenuous activity, wearing winter clothing, appears to be exerting effort; The person is likely attempting to move a vehicle that is stuck in the snow, which is a common problem during winter. Object \"Car\": Partially covered in snow, appears to be a sedan; The car is immobile, likely due to being stuck in the snow, cars can become immovable when their tires spin on ice or snow without enough traction. Object \"Snow\": Covers the ground; It can create hazardous conditions for vehicles if not properly managed. Object \"Parking lot\": Covered with snow, has multiple parking spaces, some occupied by vehicles; provide a designated place for vehicles to park, they require maintenance such as snow removal to ensure accessibility. Object \"Trees\": ... Object \"Fence\": ... Object \"Sign\": ...",
  ": Visualization of visual reasoning examples. For simplicity, we visualize partial visual tables that relate tothe question, with attribute and knowledge separated by ;": "puts. GPT-assisted evaluation is used for bench-marks: GQA (open-ended VQA with 398 VQApairs sampled from test-dev split, using GPT-3.5-1106), MMVP (open-ended VQA, using the GPT-3.5-1106), MM-Vet (open-ended VQA, using theirofficial GPT-4 evaluation server), and MMMU(open-ended VQA with 855 VQA paris sampledfrom its original val split, using GPT-3.5-1106). Results. Tab. 2 presents the results of our abla-tion study. Compared to the baseline model (row1) that takes CLIP visual embeddings as the onlyvisual representations, scene descriptions (row 2)improve the performance on MMVP (+2.0) andMM-Vet (+5.6), yet bring limited benefits on GQA(+0.1) and worse performance on MMMU (-1.0).These results suggest that scene descriptions canprovide useful information but cannot robustlybenefit wide benchmarks. When compared withrow 2, adding attributes (row 3) largely improvesthe performance on GQA (+1.7), and adding knowl-edge (row 4) significantly improves the results onMM-Vet (+2.5) and MMMU (+4.1). These resultsalign with intuition since GQA highlights objectattributes while MM-Vet and MMMU heavily relyon knowledge to answer the questions.Combining all components, full visual tables(row 5) achieve either the best or the second-best re-sults across all benchmarks, striking a good balance.Notably, full visual tables (row 5) largely outper-forms scene descriptions (row 2), even though bothare annotated by GPT4V. These results validate thenecessity of all components and the consistent per-formance improvements stem from our design of",
  "Visualization and Case Study": "Benefiting from the textual formats, visual tablesallow humans to interpret and inspect what infor-mation they provide to support visual reasoning,as illustrated in . For simplicity, we showpartial visual tables, i.e., the evidence that supportsthe visual question answering. We highlight thewrong answers in red, the correct answers in blue,and the supporting evidence from visual tables ingreen, respectively. More examples can be foundin the appendix.Take the top-left image as an example. Ourmodel can correctly describe the theme of the im-age as sleepy or tired, instead of the answer re-laxed and enjoying its time with opposite seman-tics from the LLaVA model. Our correct answer isattributed to the generated visual table. It identifiesthe dog as possibly sad, provides the knowledgeof the dog instance as often reflect the emotionsof their owners, indicating a state of tiredness, andexplicitly offers the world knowledge the begin-ning of the work week based on the text MON-DAY. JUST MONDAY. Moreover, visual tablescan provide discipline knowledge, such as geo-graphic knowledge in the top-right example, theeastern seaboard of the United States and Thestates are: Maine, New Hampshire, ....Besides knowledge, visual tables can preciselyrecognize object attributes. For instance, El-derly man: sitting; Young woman: standing, as-sisting or comforting the elderly man is identified",
  ": Human study to measure the quality of ourgenerated visual tables versus collected annotations": "in the bottom-left example, thereby facilitating thecorrect answer. Further, visual tables explicitlyrecord the perceived visual objects and their knowl-edge, reducing the chance of hallucinations dur-ing LLM reasoning. Consider the bottom-rightimage. Person: attempting to move a vehicle thatis stuck in the snow, which is a common problemduring winter in the visual table can avoid thewrong answer of snowboarding which also oftenhappens during snowy days.In summary, while being interpretable to hu-mans, visual tables thoroughly describe the de-tailed objects in visual scenes and provide preciseattributes and rich knowledge, thereby consistentlyimproving SOTA MLLMs across diverse bench-marks, as demonstrated in experiments.",
  "Human Study": "In , we conduct a human study to measurethe quality of our generated visual tables versus theannotations from GPT4V. This study is done with100 randomly sampled images from the MM-Vetdataset and 3 volunteers. The results indicate thatour generated visual tables have reasonable qualityand can be further improved. The detailed obser-vations are summarized as follows: (1) They arecomparable on the natural images captured in dailylife. (2) Annotations are oftentimes better at rec-ognizing the text characters (e.g., tables, diagrams,equations), while 38% of our sampled images con-tain major texts. This indicates that the generatedvisual tables can be further improved by addingmore text-intensive images into training data. (3)Annotations tend to output lengthy descriptions,however, some information in long descriptionsis irrelevant to the image theme (e.g., minor char-acters in corners and even wrong OCR results, ex-tended knowledge far beyond the images, excessivespeculation about the deep meaning). In contrast,our generated visual tables tend to describe factualinformation in images. Discussion.We utilize the foundation modelGPT4V to create visual table annotations. Theseannotations can alternatively be sourced from hu-man annotators, albeit at a higher cost and withlengthy post-processing. This annotation process is designed to create general representations of visualscenes, applicable to a wide range of downstreamtasks beyond VQA. Importantly, neither humanannotators nor foundation models have access toany downstream task-specific information, such asthe particular question associated with an imagein the VQA task. Therefore, the improvementsobserved in downstream performance, as well asthe distinctive structural and textual properties, areattributed to the design of the visual tables, inde-pendent of the method used to collect annotations.",
  "Conclusion": "In this paper, we propose Visual Table a newvisual representation presented in structured text.Using the collected dataset with visual table an-notations, we learn a generator to produce high-quality visual tables for arbitrary input images. Be-yond visual embeddings, visual tables support in-terpretability to humans and LLMs, enable control-lable editing, and meanwhile, offer rich descrip-tions of attributes and knowledge for each object,thereby facilitating visual reasoning. Accordingto extensive experiments, the resulting visual ta-bles exhibit superior performance than previoustext-based representations, and demonstrate consis-tent improvements over the SOTA MLLMs acrossdiverse benchmarks. We believe our study hasshowcased visual tables as robust and generaliz-able visual representations, laying the groundworkfor future research concerning visual reasoning. Limitations. When training visual table genera-tor, we exclude the VQA annotations to minimizethe biases in VQA tasks. However, our generatormight still inherit undesired biases from the train-ing datasets and the collected annotations. Besides,visual tables require computation during genera-tion and downstream tasks. A more efficient wayto generate and utilize visual tables can be furtherexplored in future research. Acknowledgements. This work was supportedby National Key R&D Program of China (ProjectNo. 2022ZD0161200, 2022ZD0161201). Thiswork is also supported by Hong Kong ResearchGrant Council - Early Career Scheme (Grant No.24200223) and Hong Kong Innovation and Tech-nology Commission Project No. ITS/228/22FP.This work was also partially funded by the Centrefor Perceptual and Interactive Intelligence (CPII)Ltd under the Innovation and Technology Commis-sion (ITC)s InnoHK. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and Lei Zhang.2018. Bottom-up and top-down attention for imagecaptioning and visual question answering. In Pro-ceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 60776086.IEEE.",
  "Anthropic. 2024. The claude 3 model family: Opus,sonnet, haiku": "Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir RZamir, Martin Fischer, Jitendra Malik, and SilvioSavarese. 2019. 3D scene graph: A structure forunified semantics, 3d space, and camera. In Proceed-ings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 56645673. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966.",
  "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023a. Shikra: Unleashingmultimodal llms referential dialogue magic. arXivpreprint arXiv:2306.15195": "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-ghui He, Jiaqi Wang, Feng Zhao, and DahuaLin. 2023b.Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprintarXiv:2311.12793. Ting Chen, Simon Kornblith, Mohammad Norouzi, andGeoffrey Hinton. 2020. A simple framework forcontrastive learning of visual representations. In In-ternational conference on machine learning, pages15971607. PMLR. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-ravit Changpinyo, Alan Ritter, and Ming-Wei Chang.2023c. Can pre-trained vision and language modelsanswer visual information-seeking questions? arXivpreprint arXiv:2302.11713. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality.",
  "Boyang Li, Pascale Fung, and Steven Hoi. 2023. In-structblip: Towards general-purpose vision-languagemodels with instruction tuning.arXiv preprintarXiv:2305.06500": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023a. Mme: A comprehensive evaluationbenchmark for multimodal large language models.arXiv preprint arXiv:2306.13394. Xingyu Fu, Sheng Zhang, Gukyeong Kwon, Pramu-ditha Perera, Henghui Zhu, Yuhao Zhang, Alexan-der Hanbo Li, William Yang Wang, Zhiguo Wang,Vittorio Castelli, Patrick Ng, Dan Roth, and BingXiang. 2023b. Generate then select: Open-ended vi-sual question answering guided by world knowledge.In Findings of the Association for ComputationalLinguistics: ACL 2023, pages 23332346, Toronto,Canada. Association for Computational Linguistics. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913. Jean-Bastien Grill, Florian Strub, Florent Altch,Corentin Tallec,Pierre H Richemond,ElenaBuchatskaya, Carl Doersch, Bernardo Avila Pires,Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar,et al. 2020. Bootstrap your own latent: A new ap-proach to self-supervised learning. arXiv preprintarXiv:2006.07733. Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Kr-ishna Murthy Jatavallabhula, Bipasha Sen, AdityaAgarwal, Corban Rivera, William Paul, Kirsty El-lis, Rama Chellappa, et al. 2023. Conceptgraphs:Open-vocabulary 3d scene graphs for perception andplanning. arXiv preprint arXiv:2309.16650.",
  "Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-mann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat:A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614": "Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey PBigham. 2018. Vizwiz grand challenge: Answeringvisual questions from blind people. In Proceedings ofthe IEEE conference on computer vision and patternrecognition, pages 36083617. Sherzod Hakimov and David Schlangen. 2023. Imagesin language space: Exploring the suitability of largelanguage models for vision & language tasks. InFindings of the Association for Computational Lin-guistics: ACL 2023, pages 1419614210, Toronto,Canada. Association for Computational Linguistics. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-otr Dollr, and Ross Girshick. 2022. Masked autoen-coders are scalable vision learners. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1600016009. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, andRoss Girshick. 2020. Momentum contrast for un-supervised visual representation learning. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 97299738. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 770778. Roei Herzig, Alon Mendelson, Leonid Karlinsky, As-saf Arbelle, Rogerio Feris, Trevor Darrell, and AmirGloberson. 2023. Incorporating structured represen-tations into pretrained vision & language models us-ing scene graphs. arXiv preprint arXiv:2305.06343.",
  "Jitesh Jain, Jianwei Yang, and Humphrey Shi. 2023.VCoder: Versatile Vision Encoders for MultimodalLarge Language Models. arXiv": "Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan CarlosNiebles. 2020. Action Genome: Actions as com-positions of spatio-temporal scene graphs. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1023610247. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, ZaranaParekh, Hieu Pham, Quoc V Le, Yunhsuan Sung,Zhen Li, and Tom Duerig. 2021.Scaling up vi-sual and vision-language representation learning withnoisy text supervision. In International Conferenceon Machine Learning (ICML). Namasivayam Kalithasan, Himanshu Singh, VishalBindal, Arnav Tuli, Vishwajeet Agrawal, Rahul Jain,Parag Singla, and Rohan Paul. 2023. Learning neuro-symbolic programs for language guided robot ma-nipulation. In IEEE International Conference onRobotics and Automation. Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, andChenguang Zhu. 2023.MM-reasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 65716581, Singapore. Association forComputational Linguistics. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual Genome: Connecting language andvision using crowdsourced dense image annotations.International Journal of Computer Vision (IJCV),123(1):3273.",
  "Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR)": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Eval-uating object hallucination in large vision-languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 292305. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. 2014.Microsoft coco:Common objects in context. In ECCV, pages 740755. Springer. Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu,Chenguang Zhu, and Lu Yuan. 2022. Revive: Re-gional visual representation matters in knowledge-based visual question answering. Advances in NeuralInformation Processing Systems, 35:1056010571.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b. Visual instruction tuning. In Advancesin Neural Information Processing Systems": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, et al. 2023c. Mm-bench: Is your multi-modal model an all-aroundplayer? arXiv preprint arXiv:2307.06281. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems. Kenneth Marino, Mohammad Rastegari, Ali Farhadi,and Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-tion answering benchmark requiring external knowl-edge. In Proceedings of the IEEE/cvf conferenceon computer vision and pattern recognition, pages31953204.",
  "Xuran Pan, Tianzhu Ye, Dongchen Han, Shiji Song, andGao Huang. 2022. Contrastive language-image pre-training with knowledge graphs. Advances in NeuralInformation Processing Systems, 35:2289522910": "Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi.2023.What does a platypus look like? generat-ing customized prompts for zero-shot image classi-fication. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1569115701. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International Con-ference on Machine Learning (ICML). Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. 2023. Say-plan: Grounding large language models using 3dscene graphs for scalable task planning. In 7th An-nual Conference on Robot Learning. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision, pages 146162. Springer. Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023.Prompting large language models with answer heuris-tics for knowledge-based visual question answering.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1497414983. Jing Shi, Yiwu Zhong, Ning Xu, Yin Li, and Chen-liang Xu. 2021.A simple baseline for weakly-supervised scene graph generation. In Proceedingsof the IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 1639316402.",
  "Karen Simonyan and Andrew Zisserman. 2015. Verydeep convolutional networks for large-scale imagerecognition. In ICLR": "Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 83178326. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-manet, Scott Reed, Dragomir Anguelov, DumitruErhan, Vincent Vanhoucke, and Andrew Rabinovich.2015. Going deeper with convolutions. In Proceed-ings of the IEEE conference on computer vision andpattern recognition, pages 19. Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi,and Hanwang Zhang. 2020. Unbiased scene graphgeneration from biased training. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 37163725. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Damien Teney, Lingqiao Liu, and Anton van Den Hen-gel. 2017. Graph-structured representations for vi-sual question answering. In Proceedings of the IEEEconference on Computer Vision and Pattern Recogni-tion (CVPR), pages 19.",
  "Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zux-uan Wu, and Yu-Gang Jiang. 2023a. To see is tobelieve: Prompting gpt-4v for better visual instruc-tion tuning. arXiv preprint arXiv:2311.07574": "Zhenhailong Wang, Manling Li, Ruochen Xu, LuoweiZhou, Jie Lei, Xudong Lin, Shuohang Wang, ZiyiYang, Chenguang Zhu, Derek Hoiem, et al. 2022.Language models with image descriptors are strongfew-shot video-language learners. Advances in Neu-ral Information Processing Systems, 35:84838497. Ziyue Wang, Chi Chen, Peng Li, and Yang Liu. 2023b.Filling the image information gap for VQA: Prompt-ing large language models to proactively ask ques-tions. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 28742890,Singapore. Association for Computational Linguis-tics. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017. Scene graph generation by iterative mes-sage passing. In Proceedings of the IEEE conferenceon computer vision and pattern recognition (CVPR),pages 54105419. An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, ZexueHe, Yujie Lu, William Yang, Jingbo Shang, and Ju-lian McAuley. 2023. Learning concise and descrip-tive attributes for visual recognition. In Proceedingsof the IEEE/CVF International Conference on Com-puter Vision (ICCV). Jingkang Yang, Jun CEN, Wenxuan Peng, Shuai Liu,Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, QifengChen, and Ziwei Liu. 2023a. 4d panoptic scene graphgeneration. In Thirty-seventh Conference on NeuralInformation Processing Systems. Yue Yang, Artemis Panagopoulou, Shenghao Zhou,Daniel Jin, Chris Callison-Burch, and Mark Yatskar.2023b.Language in a bottle: Language modelguided concept bottlenecks for interpretable imageclassification. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 1918719197. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, XiaoweiHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 36, pages 30813089.",
  "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, XingSun, Tong Xu, and Enhong Chen. 2023. A survey onmultimodal large language models. arXiv preprintarXiv:2306.13549": "Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, HuaWu, and Haifeng Wang. 2021. Ernie-vil: Knowledgeenhanced vision-language representations throughscene graphs. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 35, pages 32083216. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and LijuanWang. 2023. Mm-vet: Evaluating large multimodalmodels for integrated capabilities. arXiv preprintarXiv:2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu:A massive multi-discipline multimodal understand-ing and reasoning benchmark for expert agi. arXivpreprint arXiv:2311.16502. Rowan Zellers, Mark Yatskar, Sam Thomson, and YejinChoi. 2018. Neural motifs: Scene graph parsing withglobal context. In Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR), pages 58315840. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, JianweiYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-feng Gao. 2021. Vinvl: Revisiting visual representa-tions in vision-language models. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 55795588. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,George Karypis, and Alex Smola. 2023.Multi-modal chain-of-thought reasoning in language mod-els. arXiv preprint arXiv:2302.00923": "Long Zhao, Liangzhe Yuan, Boqing Gong, Yin Cui, Flo-rian Schroff, Ming-Hsuan Yang, Hartwig Adam, andTing Liu. 2023. Unified visual relationship detec-tion with vision and language models. arXiv preprintarXiv:2303.08998. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, andSibei Yang. 2024. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in lan-guage models. Advances in Neural Information Pro-cessing Systems, 36. Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, andYin Li. 2021. Learning to generate scene graph fromnatural language supervision. In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 18231834. Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, andYin Li. 2020. Comprehensive image captioning viascene graph decomposition. In Proceedings of theEuropean Conference on Computer Vision (ECCV),pages 211229. Springer. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-yuan Li, Noel Codella, Liunian Harold Li, LuoweiZhou, Xiyang Dai, Lu Yuan, Yin Li, et al. 2022.Regionclip: Region-based language-image pretrain-ing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages1679316803. Yiwu Zhong, Licheng Yu, Yang Bai, Shangwen Li,Xueting Yan, and Yin Li. 2023. Learning procedure-aware video representation from instructional videosand their narrations. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1482514835.",
  "AVisual Table Annotation Collection": "To collect visual table annotations, we create a de-tailed prompt for GPT4V to ensure the annotationsclosely align with our requirements, as illustratedin .Without loss of generality, we design visual ta-bles to cover common components in visual en-vironments. For instance, the scene descriptioncovers time, location, and event, while object de-scriptions detail the attributes (e.g., color, material,relationships among objects) and world knowledge(e.g., a basic understanding of the physical worldand social life).",
  "BAdditional Implementation Details": "We provide details for response prompts ofMLLMs in Tab. 4. Specifically, we follow LLaVA-1.5 (Liu et al., 2023a) to design the prompts fordifferent benchmarks, in consideration of the set-tings of benchmarks. For example, we instruct theMLLM to answer a single word or phrase when thebenchmark adopts exact matching evaluation (i.e.,VQAv2, GQA, TextVQA, POPE). Similarly, weinstruct the MLLM to answer open-ended answersif the benchmark adopts GPT-assisted evaluation(i.e., MM-Vet, LLaVA-Bench, MMMU), to answerthe choice letter if the benchmark is evaluated in amulti-choice setting (i.e., MMBench, SQA-IMG,MMVP), and to answer Unanswerable when theimages are unable to provide sufficient information(i.e., VizWiz where many images are blurred).",
  "CAdditional Experiment Results": "In addition to Tab. 1 in the main paper, we showmore results in Tab. 5 of in appendix. The ex-periment settings are the same as our main paper,including benchmarks, baselines, and evaluationprotocols. The only difference is that we addition-ally provide results for our MLLMs in 7B size, i.e.,Vicuna-VT-7B and LLaVA-VT-7B. Vicuna-VT-7B. Same as the trend in our main pa-per, visual tables are more effective representationsthan previous text-based baselines. For example,even if using an LLM with a smaller size, Vicuna-VT-7B can even outperform the baselines that uti-lize 13B LLMs (e.g., +5.0 on LLaVA-Bench, +2.6on MMBench, +8.0 on MMVP, +5.6 on VQA-v2over Vicuna-SG). Again, these strong results val-idate our design of visual tables, providing richknowledge and precise attributes. LLaVA-VT-7B. Our model LLaVA-VT-7B consis-tently outperforms the base model LLaVA-1.5-7Bacross diverse benchmarks. Specifically, our modelperforms better not only on knowledge-intensivebenchmarks (e.g., +1.3 on MM-Vet, +3.1 onLLaVA-Bench, +1.1 on SQA), but also on attribute-intensive benchmarks (e.g., +3.4 on MMVP, +1.1on GQA) and general benchmarks (e.g., +3.7 onMMBench, +1.3 on VQA-v2). These promisingresults again demonstrate that our generated visualtables work as generalizable visual representations,thereby facilitating complex visual reasoning.",
  "DAdditional Visual Reasoning Examples": "With the textual formats, visual tables allow hu-mans to interpret and inspect how they support vi-sual reasoning, as illustrated in . For each im-age, we visualize the question, the ground-truth an-swer, the answer from the LLaVA-1.5-13B model,the answer of our MLLM with visual tables, andour generated visual tables. For simplicity, weshow partial visual tables, i.e., the evidence thatsupports the visual question answering. We high-light the wrong answers in red, the correct answersin blue, and the supporting evidence from visualtables in green, respectively.Instance-level knowledge are provided by vi-sual tables. For instance, the knowledge of USfive-dollar bill featuring the portrait of AbrahamLincoln and The Starry Night by Vincent vanGogh is given by our visual table for the 3rd and4th examples, respectively. Moreover, our visual ta-ble can also provide discipline knowledge, such asthe geographic knowledge in the 5th example, theeastern seaboard of the United States and Thestates are: Maine, New Hampshire, Vermon, ....Besides knowledge, visual tables can preciselyrecognize the object attributes. For instance, El-derly man: sitting; Young woman: standing, as-sisting or comforting the elderly man, and Ante-lope: four animals with brownish-gray fur; Giraffe:",
  "Input Prompt": "You are an AI visual assistant that can analyze a single image. Given an image, you need to perform the task of scene description. And then, you need to identify each object in the image. For each object, you need to perform 3 tasks: object category recognition, attribute description generation, and knowledge description generation. Scene description:1. Based on the given image, please provide a short and concise description for the scene in the image, such as the location, the time of the day (e.g., morning, evening), the event, and so on. Object category recognition:1. Based on the given image, please recognize the category for each object in the scene. 2. Please cover as many objects as possible. The objects should cover not only the salient objects, but also the other objects such as the small objects, the objects in the background, the objects that are partially occluded, and so on. Attribute description generation:1. Based on the given image, please generate the visual attributes for each object. 2. Visual attributes characterize the objects in images. They can be OCR characters on the object, spatial relations to surrounding objects, action relations to surrounding objects, relative size compared to surrounding objects, color, geometry shape, material, texture pattern, scene environment, motion or dynamics of objects, and so on.3. Specially, if possible, the visual attributes could be the emotions (e.g., surprised, angry), age (e.g., young, elderly), and so on. Knowledge description generation:1. Based on the given image, please describe the knowledge for each object.2. The knowledge includes object affordance, commonsense knowledge, background knowledge, and so on. 3. Object affordance is defined as the functions supported by the objects. For example, what the objects can be used for? Note that the affordance might be altered case by case, due to deformed shape, unreliable materials, and so on.4. Commonsense knowledge is defined as basic understandings and assumptions about daily life, human behavior, and the natural world. It also includes understanding social norms, basic cause-and-effect relationships, and simple reasoning about daily situations.5. Background knowledge is defined as the knowledge of named entities, such as celebrities, ceremonies, festivals, and so on. Output format:The output content should follow the following JSON format. {\"scene description\": \"\", \"objects\": [{\"object category\": \"\", \"attribute description\": \"\", \"knowledge description\": \"\"}, ......, {\"object category\": \"\", \"attribute description\": \"\", \"knowledge description\": \"\"}]}. Directly output the JSON without any other content. The output MUST follow JSON format. {\"scene description\": \"The image is a simple illustration representing a marine food web, showcasing the relationships between various marine animals and plants.\",\"objects\": [ {\"object category\": \"Blue whale\", \"attribute description\": \"Large, blue and gray in color, streamlined body shape\", \"knowledge description\": \"Marine mammal, known to be the largest animal on Earth, feeds on krill and other small sea creatures\"}, {\"object category\": \"Killer whale\", \"attribute description\": \"Black and white coloration, distinctive dorsal fin\", \"knowledge description\": \"Also known as orca, a toothed whale that is a top predator, feeds on various marine animals including seals and fish\"}, {\"object category\": \"Seagull\", \"attribute description\": \"White with gray wings, flying bird with a short beak\", \"knowledge description\": \"Seabird that feeds opportunistically, often found scavenging and eating fish or invertebrates\"}, {\"object category\": \"Leopard seal\", \"attribute description\": \"Spotted gray coat, large body with a reptile-like head\", \"knowledge description\": \"One of the top Antarctic predators, known for its diverse diet including fish, penguins, and even other seals\"}, {\"object category\": \"Penguin\", \"attribute description\": \"Black and white plumage, flightless bird with an upright stance\", \"knowledge description\": \"Adapted to marine life, feeds primarily on fish and krill and is a prey item for seals and killer whales\"}, {\"object category\": \"Elephant seal\", \"attribute description\": \"Large, thick-bodied seal with brownish skin color\", \"knowledge description\": \"Named for their large size and males' trunk-like snouts, they feed on fish and squid, and can dive deep\"}, {\"object category\": \"Fish\", \"attribute description\": \"Typical streamlined fish shape, blue with scales\", \"knowledge description\": \"General representation of fish, diverse group that can feed on plankton or be prey for larger marine animals\"}, {\"object category\": \"Krill\", \"attribute description\": \"Small, red-orange, shrimp-like creature\", \"knowledge description\": \"Crustacean vital to the marine food chain; a primary food source for many marine animals, from fish to whales\"}, {\"object category\": \"Squid\", \"attribute description\": \"Elongated body with tentacles, often depicted in brown color\", \"knowledge description\": \"Cephalopods known for their mobility, intelligence, and as a food source for predators like seals and whales\"}, {\"object category\": \"Crab\", \"attribute description\": \"Red-orange color, characterized by a round shell and pincers\", \"knowledge description\": \"Crustaceans with a varied diet; some species may feed on seaweed, mollusks, and detritus\"}, {\"object category\": \"Zooplankton\", \"attribute description\": \"Tiny, varied shapes, often transparent or with visual cues for microscopic details\",\"knowledge description\": \"Small and often microscopic organisms drifting in water, a crucial component of the marine food web\"}, {\"object category\": \"Phytoplankton\", \"attribute description\": \"Illustrated as small green organisms with simple structures\", \"knowledge description\": \"Microscopic plants or plant-like organisms in aquatic environments, they perform photosynthesis and are a base food source\"}, {\"object category\": \"Seaweed\", \"attribute description\": \"Green, plant-like with leafy fronds\", \"knowledge description\": \"Marine algae that provide habitat and food for various marine creatures, also play a role in oxygen production\"}]}",
  "Input Image": "Visual Table Annotation{\"scene description\": \"The image depicts a pastoral scene in a dry, arid environment, likely a desert or semi-desert region during the day. A herder is tending to a flock of goats, possibly in the morning or midday judging by the shadows and the bright sky.\",\"objects\": [ {\"object category\": \"Person\", \"attribute description\": \"Adult male, wearing traditional attire, standing amidst the flock, holding an object that might be a staff or tool\", \"knowledge description\": \"The person is a goat herder, responsible for the care, guidance, and protection of the livestock. The traditional attire suggests a cultural or regional dress code, and the staff or tool is typically used to assist in herding.\"}, {\"object category\": \"Goats\", \"attribute description\": \"Various sizes, predominantly black and white, with long hair, some with horns, scattered across the foreground and midground of the image\",\"knowledge description\": \"Goats are domesticated animals commonly raised for their meat, milk, and wool. They are known for their hardiness and ability to graze on sparse vegetation, which makes them well-suited for this environment.\"}, {\"object category\": \"Fence\", \"attribute description\": \"Wire fence in the background, enclosing an area\", \"knowledge description\": \"The fence likely serves to contain livestock or demarcate property boundaries within the pastoral area.\"}, {\"object category\": \"Shack\", \"attribute description\": \"Small, rudimentary structure with a flat roof in the background\", \"knowledge description\": \"The structure may serve as storage for supplies, shelter for the herder, or a place to keep the goats during inclement weather or at night.\"}, {\"object category\": \"Mountains\", \"attribute description\": \"Bare mountains visible in the far background under a clear blue sky\",\"knowledge description\": \"The mountains contribute to the geography of the region and may be part of a larger range that influences the local climate and ecosystem.\"}]} {\"scene description\": \"This is an image of the Mona Lisa, a famous Renaissance painting by Leonardo da Vinci. The painting depicts a woman with an enigmatic smile, seated in front of a distant landscape.\",\"objects\": [ {\"object category\": \"Painting\", \"attribute description\": \"The subject is a woman with dark hair, a subtle smile, attire from the Renaissance period, and a mountainous landscape in the background.\",\"knowledge description\": \"The Mona Lisa painting is one of the most recognized and valuable paintings in the world, housed in the Louvre Museum in Paris. It is celebrated for the subject's mysterious smile and the artist's use of sfumato, a technique that creates a soft transition between colors and tones.\"}]}",
  "EAdditional Examples of GeneratedVisual Tables": "In the following tables, we visualize our generatedvisual tables. As these examples show, our visualtables can thoroughly encode the visual scenes inhierarchical text, by offering concise scene descrip-tions, accurate object categories, precise object at-tributes, and rich instance-level object knowledge. Question: Please explain the meme in detail.GT Answer: The meme features a dog lying on a cyan wooden floor, looking exhausted or unenthusiastic. The text at the top of the meme reads \"MONDAY,\" and at the bottom, it says \"JUST...MONDAY.\" The meme humorously captures the feeling of a lack of motivation and energy that is often associated with Mondays, represented by the dog's pose and expression. LLaVA: The image features a small dog lying on a wooden floor, possibly a deck, with a caption above it. The caption reads \"Monday. Just Monday. . .\" The dog appears to be relaxed and enjoying its time on the wooden surface. The scene captures a moment of leisure and contentment for the dog, as it spends its day indoors. LLaVA-VT: The image features a small dog lying on a wooden floor, appearing to be sleepy or tired. The dog is positioned in the center of the scene, occupying a significant portion of the image. The floor has a blue color, which adds to the overall ambiance of the scene. The meme is a play on words, with the phrase \"Just Monday\" written below the dog. This cleverly implies that the dog is expressing its feelings about the start of the week, as if it were a human. The meme is likely meant to evoke a sense of humor and relatability, as many people might feel tired or unenthusiastic about starting a new week. Generated Visual Table: Scene: an image of a dog lying; text that says \"MONDAY. JUST MONDAY.\"; suggesting a feeling of dread or reluctance associated with the start of the work week.Object \"Dog\": lying down with its head resting on the floor, possibly sad; often reflect the emotions of their owners, indicating a state of tiredness. Object \"Text\": added to express a sentiment about the start of the week; the use of text in images is a common way to convey additional messages or emotions that complement the visual content, it suggests a feeling of reluctance or negativity associated with the beginning of the work week. Object \"Wooden floor\": ...",
  "Question: Which scene category matches this image the best? A. manufactured home B. campus C. badlands D. field/cultivatedGT Answer: BLLaVA: DLLaVA-VT: B": "Generated Visual Table: Scene: appears serene and is likely taken in a park or educational institution setting. Object \"Building\": Red brick exterior, multiple windows , rectangular shape, two-story height; The building's architecture suggests it could be part of a school or institutional complex, designed for educational or administrative purposes. Object \"Sky\": ... Object \"Tree\": ...Object \"Lawn\": ... Object \"Shrub\": ...",
  "Question: What is the face value of this banknote?GT Answer: five <OR> 5LLaVA: The face value of this banknote is $1.LLaVA-VT: The face value of this banknote is five dollars": "Generated Visual Table: Scene: an image of a US five-dollar bill featuring the portrait of Abraham Lincoln. Object \"Currency\": Paper material, features a portrait, textual inscriptions, security threads, and symbols; Used as a medium of exchange, legal tender in the United States, and bears the likeness of a historical figure important to the nation's history. Object \"Portrait\": ...; Abraham Lincoln, the 16th President of the United States. Object \"Text\": Inscribed with ... \"The United States of America\"; Conveys the value and denomination of the currency, as well as the country of issue. Object \"Security thread\": ...; Serves as a counterfeit deterrent feature to enhance the security of the currency. Object \"Emblem\": An eagle and shield, symbols of the United States;Symbolizes the country's strength and heritage, commonly used in government and military insignia. Question: Can you give a short introduction to this painting?GT Answer: The Starry Night is an oil-on-canvas painting by the Dutch Post-Impressionist painter Vincent van Gogh. Painted in June 1889, it depicts the view from the east-facing window of his asylum room at Saint-Rmy-de-Provence, just before sunrise, with the addition of an imaginary village. It has been in the permanent collection of the Museum of Modern Art in New York City since 1941, acquired through the Lillie P. Bliss Bequest. Widely regarded as Van Gogh's magnum opus, The Starry Night is one of the most recognizable paintings in Western art. LLaVA: The painting is a beautiful depiction of a night sky filled with stars, creating a captivating and serene atmosphere. The scene also features a mountainous landscape, adding depth and dimension to the artwork. The combination of the starry night sky and the mountainous terrain creates a sense of tranquility and wonder, inviting the viewer to appreciate the beauty of nature. LLaVA-VT: The painting is a famous artwork by Vincent Van Gogh, titled \"The Starry Night.\" It features a nighttime scene with a village and a church in the foreground, surrounded by a beautiful starry sky. The painting is known for its vivid colors and the artist's unique style, which captures the essence of the night and the tranquility of the village. Generated Visual Table: Scene: This is a painting depicting a night scene with a starry sky. The painting is known as 'The Starry Night' by Vincent van Gogh. Object \"Starry sky\": The sky is filled with bright stars and swirling patterns; associated with tranquility and the vastness of the universe. Object \"Moon\": upper right of the painting; key element in the painting providing light and contrast. Object \"Tree\": left side of the painting, reaching towards the sky; used in art to represent life and growth, as well as to add contrast and depth to a scene. Object \"Houses and buildings\": ...; depict a rural or small town setting, providing a sense of human habitation and community. Object \"Mountain\": visible in the background behind the village; add a sense of scale and grandeur to a landscape painting. Object \"Church tower\": ...",
  "Question: Do these animals all have the same type?GT Answer: NoLLaVA: YesLLaVA-VT: No": "Generated Visual Table: Scene: an image of a savanna grassland during the day, with a group of antelopes in the foreground and a giraffe in the background. Object \"Antelope\": Four animals with brownish-gray fur, standing in tall dry grass; Antelopes are herbivorous mammals known for their speed and agility. Object \"Giraffe\": Tall animal with a long neck and legs, standing in the distance, partially obscured by the terrain; the tallest land animals. Object \"Grass\": ... Object \"Sky\": ...",
  "Question: Is there a snowboard in the image?GT Answer: YesLLaVA: NoLLaVA-VT: Yes": "Generated Visual Table: Scene: A person snowboarding down a snowy slope with visible tracks from previous skiers or snowboarders. Object \"Person\": Wearing a red jacket, black pants, and is in a bent posture while snowboarding; engaged in snowboarding, a winter sport that involves descending a snow-covered slope while standing on a snowboard attached to the rider's feet. Object\"Snowboard\": Partially visible under the person's feet; a piece of equipment used for snowboarding. Object \"Snow\": Covers the ground, appears to be compacted with visible tracks; provides the necessary surface for snowboarding and skiing. Object \"Track\": ...",
  "gas exchange, taking in oxygen and expelling carbon dioxide. They are essential": "for breathing and are a key part of the respiratory system.\"},{\"object category\": \"X-ray label\", \"attribute description\": \"CharactersJ.8H, likely indicating a code or identifier for the X-ray image, placed in thetop right corner\", \"knowledge description\": \"X-ray labels are used for patientidentification and to help organize and retrieve medical images in a radiology",
  "for cells and is transported through the bloodstream.\"},": "{\"object category\": \"Cytoplasm\", \"attribute description\": \"Visible withinthe connective tissue cells, labeled Cytoplasm\", \"knowledge description\":\"The cytoplasm is the gelatinous interior of a cell, where metabolic processes occur, and contains various organelles and enzymes necessary for cell function.\"},{\"object category\": \"Labels\", \"attribute description\": \"Text labels in En-glish, Extracellular fluid, Connective tissue cells, Glucose, Cytoplasm\",",
  "celled microorganisms. They have a cell wall, genetic material in form of DNA,": "and they replicate through a process that includes replication and transcription.\"},{\"object category\": \"Plasmid\", \"attribute description\": \"Small, circularDNA molecule within the bacterial cell, visible replication sites indicated byarrows\", \"knowledge description\": \"Plasmids are extra-chromosomal, self-replicating genetic elements that are present in many bacteria. They can carry genes that confer antibiotic resistance or other beneficial traits for the bacteria.\"},{\"object category\": \"Foreign DNA\", \"attribute description\": \"Linearmolecule, outside the bacterial cell, being taken up by the cell\", \"knowledge",
  "insights, while the stars may represent success or acclaim.\"},": "{\"object category\": \"Light bulb\", \"attribute description\": \"White, six-sided,above the persons head inside the thought bubble\", \"knowledge description\":\"The light bulb is a universal symbol for an idea or a flash of inspiration. It is often used to represent creativity and the process of thinking.\"},{\"object category\": \"Star\", \"attribute description\": \"Yellow, five-pointed,inside the thought bubble\", \"knowledge description\": \"Stars are often used to"
}