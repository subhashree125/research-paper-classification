{
  "Abstract": "Legal case retrieval (LCR) aims to providesimilar cases as references for a given factdescription. This task is crucial for promot-ing consistent judgments in similar cases, ef-fectively enhancing judicial fairness and im-proving work efficiency for judges. However,existing works face two main challenges forreal-world applications: existing works mainlyfocus on case-to-case retrieval using lengthyqueries, which does not match real-world sce-narios; and the limited data scale, with currentdatasets containing only hundreds of queries,is insufficient to satisfy the training require-ments of existing data-hungry neural mod-els.To address these issues, we introducean automated method to construct syntheticquery-candidate pairs and build the largest LCRdataset to date, LEAD, which is hundreds oftimes larger than existing datasets. This dataconstruction method can provide ample train-ing signals for LCR models. Experimental re-sults demonstrate that model training with ourconstructed data can achieve state-of-the-artresults on two widely-used LCR benchmarks.Besides, the construction method can also beapplied to civil cases and achieve promisingresults. The data and codes can be found in",
  "Introduction": "Legal case retrieval (LCR) aims to search for histor-ically relevant cases based on a given fact descrip-tion (Bench-Capon et al., 2012; Bhattacharya et al.,2022; Locke and Zuccon, 2022; Yu et al., 2022;Sansone and Sperl, 2022). This task can help legalprofessionals, such as judges and lawyers, improvework efficiency by providing past cases as refer-ences for current judgments. Thus, it plays a crucialrole in promoting judicial fairness by facilitatingsimilar cases receiving similar judgments.",
  ": An example for legal case retrieval, where thekey facts are in blue": "Different from open-domain retrieval, LCR de-mands a complex understanding of case detailsand necessitates models equipped with legal knowl-edge to generate knowledge-rich case representa-tions (Xiao et al., 2023; Sun et al., 2023a). Asshown in , models are required to recog-nize that the severity of injury rather than the lo-cation of injury is the key factor in assessing therelevance of given candidates to the query. Recentyears have seen significant efforts by scholars to im-prove the performance of LCR, including introduc-ing additional knowledge features (Bhattacharyaet al., 2022; Yao et al., 2022; Sun et al., 2023a) anddesigning LCR-oriented pre-training objectives (Liet al., 2023a; Ma et al., 2023).However, despite these advancements, the real-world application of LCR still faces the followingchallenges: (1) Asymmetric Retrieval. Existingmethods mostly focus on symmetric retrieval set-tings with lengthy fact descriptions for both queriesand candidates. In contrast, real-world user queriesoften consist of only a few sentences describingkey details. This inconsistency between applicationand training scenarios results in sub-optimal per-formance. (2) Limited Data. Another challengeis the limited data scale, as legal data annotation requires highly skilled and experienced annotators,making it time-consuming and labor-intensive. Ex-isting LCR datasets contain only a few hundredqueries (Ma et al., 2021; Li et al., 2023b), com-pared to tens of thousands in open-domain retrievaldatasets (Bonifacio et al., 2021; Qiu et al., 2022;Xie et al., 2023). Besides, most retrieval methodsrely heavily on data-hungry neural models, makingthe construction of large-scale, high-quality legalretrieval data a key to enhancing LCR performance.To address these issues, this paper proposes amethod for automatically constructing high-quality,synthetic legal retrieval datasets for model training.Specifically, given a case candidate, we employ alarge-scale generative language model to first ex-tract key facts, and omit entities, including namesand places. Then, based on the anonymous keyfact, we require the model to generate a brief andcoherent description of the case, which is regardedas the search query. In this way, the generatedquery is short and contains only a few sentences.Additionally, to improve data diversity and enablethe model to retrieve relevant cases even whenkey facts are not entirely consistent, we employ aknowledge-driven data augmentation strategy. Foreach query, we select the case that is most similarfrom the perspective of charges, related legal arti-cles, and prison term, from the entire corpus as theaugmented positive candidate.This approach enables us to rapidly build thelargest LCR dataset, LEAD, with over 100K query-candidate pairs and without any manual annotation,surpassing existing LCR datasets by a hundred-fold. To verify the effectiveness of our method, wetrain dense passage retrieval models with LEADand compare the model with several competitivebaseline models, on two widely-used criminal LCRbenchmarks. The experimental results demonstratethat models trained with our enriched high-qualitycase retrieval data can achieve state-of-the-art per-formance in LCR tasks. Besides, the proposedframework for data generation can be easily ap-plied to civil case retrieval, and achieve satisfyingperformance. The code and data in our paper willbe released to promote the development of LCR.",
  "Related Work": "Legal Case Retrieval. Legal case retrieval is achallenging task that requires a deep understandingof legal documents. The task entails models identi-fying the most legally relevant cases within candi- date documents concerning a given query case.Earliest work for LCR attempt to employ tradi-tional retrieval models, including, BM25 (Robert-son and Zaragoza, 2009) and TF-IDF (Aizawa,2003), for legal retrieval (Zeng et al., 2007). Withthe development of deep learning, many effortshave been devoted into designing neural architec-tures to enhance long textual representation (Belt-agy et al., 2020; Shao et al., 2020), interpretabil-ity (Yu et al., 2022; Sun et al., 2023b), legal knowl-edge enriched representation (Abolghasemi et al.,2022; Ma et al., 2024; Xiao et al., 2023; Sun et al.,2022; Yao et al., 2022). Due to the lack of a large-scale LCR dataset, these researches mainly focuson the re-ranking phrase, overlooking the signif-icance of dense passage retrieval (DPR) for highrecall rate (Karpukhin et al., 2020). To elevate thedata scarcity issues, some researchers explore theself-supervised pre-training for legal DPR. For in-stance, SAILER (Li et al., 2023a) adopts an asym-metric encoder-decoder architecture, integratingvarious pre-training objectives to encode rich se-mantic information across tasks. CaseEncoder (Maet al., 2023) leverages fine-grained legal provi-sions to select relevant and irrelevant cases for eachquery, thus improving the quality of training data.In this paper, we find that our data constructionmethods can further facilitate the LCR performanceby scaling the high-quality instances for LCR.Dataset for LCR. High-quality data lies in thecore of existing data-hungry neural models forLCR. However, due to the highly skilled and ex-perienced annotators required for legal data anno-tation, existing LCR datasets only contain a fewhundred queries. For example, LeCaRD (Ma et al.,2021) consists of a total of 107 queries, each with100 candidate documents, but only 30 of thesedocuments have been manually annotated for rele-vance. LeCaRDv2 (Li et al., 2023b) contains 800queries, with only 30 documents per query anno-tated for relevance. CAIL2022-LCR is the competi-tion dataset of the Challenge of AI in Law (CAIL).Compared to these datasets, open-domain retrievaldatasets have hundreds of times more queries, suchas T2Ranking (Xie et al., 2023) with 307k queries,and mMarco-Chinese (Bonifacio et al., 2021) with516k queries. The lack of large-scale data hindersthe development of LCR.Data Augmentation for Information RetrievalData augmentation aims to increase the amount oftraining data by heuristically generating new datainstances based on existing data. In the context",
  ": The illustration of the data construction process": "of information retrieval, data augmentation is typi-cally applied to generate new queries, positive andnegative examples. For example, the Inverse ClozeTask (ICT) (Lee et al., 2019) randomly selects a to-ken span from a text segment to serve as the query,while the remaining tokens form the key. This isthe opposite of the Cloze Task, where the remain-ing tokens are used as the query and the sampledtoken span serves as the candidate. This approachhas been proven effective in pre-training(Changet al., 2020; Sachan et al., 2021).Additionally, the use of in-batch negatives is amethod to expand negative examples. For a givenquery, the negatives are generated from the positiveexamples of other queries within the same batch.This method typically requires a larger batch sizeto generate more negatives for a query (Chen et al.,2020) and has been widely applied in open-domainretrieval scenarios (Lee et al., 2019; Karpukhinet al., 2020; Izacard et al., 2022).Recently, researchers have also utilized LLMsto synthesize data for training embedding models.For instance, Chen et al. (2024) used GPT-3.5 togenerate questions for collected passages, whileWang et al. (2024) employed GPT-4 to first createtask types and then construct queries, positive docu-ments and hard negative documents based on thesetasks. These models have set new state-of-the-artresults on multiple benchmarks.",
  "Data Construction": "To address the challenges of asymmetric retrieval,queries in the training dataset should align withreal-world user queries, which are often charac-terized by brevity and conciseness. As shown in, we propose an automatic method to gen-erate queries based on case facts. We will introducethe details about the data generation in this section.",
  "Query Generation": "Key Events Extraction. As all case documentsare manually written by judges, there are many de-tails and viewpoints contained in these documents,such as the names of every participant, their re-lationships, and the court discussion about eachevent. However, in real life, considering users un-familiarity with legal knowledge, the queries theysearch often only include key factual events. Toget the short queries as real-world user queries,we extract key information from the facts of legalcases gathered from online sources. Then, to dothis efficiently, automatically, and at a large scale,our approach leverages a generative method basedon open-source, large-scale language models. Weemploy an LLM to generate queries for our dataset.During the generation process, the model is firstrequired to compress provided case facts into con-cise case descriptions, which only retain essentiallegal events. To guide the model, we furnish it witha task description and two illustrative exampleswithin the prompt, ensuring effective and accuratequery generation. The specific prompt is providedin appendix A.1. Anonymization. In the previous step, we alsoinstruct LLM to remove entities such as personalnames, locations, and dates from the cases. How-ever, we found that approximately 30% of casesstill contain these entities, which are typically irrel-evant to the key events and do not affect the finaljudgment. Besides, the shared entities betweenqueries and candidates would provide a shortcutto the models, leading models trained on this dataassign high relevance scores to the queries and can-didates with the same entities and overlook criticallegal events. Therefore, we implement a strategyto anonymize these entities. Specifically, we uti-",
  ": Details of statistics of existing LCR datasets. The COLIEE dataset does not annotate the correspondingcharges for the cases, so this table does not provide such information": "lize DeepTHULAC1 for part-of-speech tagging ofqueries. Subsequently, specific information suchas personal names, company names, locations, andtime within the queries are replaced with seman-tically equivalent content. For instance, personalnames are replaced with random usual names. Thisapproach enables the model to better grasp the re-lationships between queries and key information,thereby enhancing the effectiveness of retrieval.With the key events extraction and anonymiza-tion, we can generate a relevant query for every can-didate case. The query-candidate pairs can serveas the training signals for LCR models.",
  "Knowledge-Driven Augmentation": "Through the aforementioned method, we can con-struct large-scale query-candidate pairs that containthe same key facts. However, in real applications,we usually cannot find cases that are completelyidentical to the query. Therefore, to enable themodel to handle a diverse range of queries in real-world scenarios, we further propose a knowledge-driven data augmentation method.Unlike open-domain information retrieval, in theLCR domain, it is not appropriate to judge whethertwo cases are similar based solely on the factualdetails of the case. The legal articles applicableto the case and the judgment results are also im-portant (Li et al., 2023c). Therefore, for a givenquery-candidate pair, we select the cases with simi-lar legal articles and prison terms to the candidateas the augmented positive candidate. Specifically,we extract the main and ancillary legal articles fromthe Reason section of the case. Here, the main le-gal articles refer to those detailing specific charges,such as Article 133 from the Chinese Criminal Law,which defines and sets sentencing standards for thecrime of traffic accidents. The ancillary legal arti-cles refer to those outlining the impact of certainfacts on sentencing, such as Article 67 from the Chi-nese Criminal Law, which defines self-surrender and its influence on the final sentencing. The con-tent of these two articles is provided in appendixA.5. Additionally, we extract the charges and spe-cific prison terms of the final judgment, such asdeath penalty and imprisonment, from the Judg-ment section. These extracted elements serve asthe basis for positive augmentation.Next, for each candidate case in the dataset, weidentify a related case in which the main legal arti-cles match those of the original candidate case, andthe additional legal articles as well as prison termsare as similar as possible. This process results ina new positive example. This positive exampleis legally related to the original case, but becausethey are two completely different cases, it ensuresthat there is no overlap in the factual details. Thisprocess leads to a dataset that has been augmentedwith positive examples.",
  "Construction Details": "We collect 6.6 million criminal cases from ChinaJudgment Online 2. Initially, we exclude criminalruling documents (containing only content relatedto commutation) and retain only criminal judgmentdocuments. Subsequently, we filter out cases withfacts shorter than 100 Chinese characters, as themajority of criminal cases fall within this range.Using regular expressions, we match and extract in-formation such as charges, legal articles, and judg-ments from the cases, eliminating those where suchcontent couldnt be extracted via rules. In the end,there are about 2 million cases remained. Fromthis pool, we randomly select 100 thousand casesto generate queries for each charge. Then, for eachof these 100 thousand cases, we search for themost similar cases from the initial 2 million usingcharges, legal articles, and judgments as criteria, toaugment new positive examples.",
  "case pairs, which is several hundred times largerthan other LCR datasets available, and capableof supporting the training of existing data-hungrydense passage retrieval models": "DiversityIn LCR, the diversity of charges cangreatly benefit the performance of case retrieval.To achieve this, we carefully select cases acrossdifferent charges and set a maximum threshold forthe number of cases per charge. In total, we cover210 charges, ensuring a diverse range of case de-scriptions. Additionally, in our prompts for querygeneration, we include two examples to help themodel learn how to generate queries. Since thegenerated queries are easily affected by examples,each time a query is generated, the examples in theprompt are randomly selected from a set of exam-ples to ensure diversity in the generated queries.Its worth noting that our data constructionmethod is automated and doesnt rely on manualannotation. This makes it highly efficient for appli-cation to any criminal case with a clear structure.As a result, the datasets size and coverage can beexpanded rapidly, not limited solely to the numbersmentioned. In section 4.6, we also apply the samemethod to generate data from civil cases. Due to the asymmetric nature of our dataset, theaverage query length is only 79 characters, which ismore close to the real-world applications. Specificexamples in the dataset can be found in , andwe present the statistics of our constructed datasetand other widely-used LCR datasets in .",
  "Model Training": "In this paper, we mainly focus on dense passageretrieval for legal cases. We adopt a dual-encoderarchitecture for all models. This involves sepa-rately encoding the query and the candidate casesto obtain query embeddings and candidate caseembeddings and calculating the cosine similaritybetween them as the final similarity score.The training is conducted in an in-batch negativesetting (Karpukhin et al., 2020). In the in-batchnegative setting, for each query in a batch with Ntraining pairs, the negative examples are the pos-itives of the other queries in the same batch, i.e.,N-1 negative examples. However, when we use thenewly identified positive examples from the dataset,some negatives may share the same charges, legalarticles, or judgments with the positives, leadingto false negatives that can impact the model train-ing. To address this, during training, we set the",
  "Datasets and Metrics": "In this paper, we focus on legal asymmetric re-trieval, but existing datasets with human-annotatedlabels focus on symmetrical retrieval, where thequeries are lengthy cases. Therefore, to better as-sess the models performance in asymmetric re-trieval, we adopt our method to simplify the querycases in benchmarks into a short version automat-ically. To ensure the high quality of evaluationbenchmarks, we manually check the generatedqueries, ensuring that the queries do not changethe key events. Specifically, we employ GPT-4 togenerate the short version of queries and conductquality testing by one of the authors. For case-to-case retrieval, we utilize the original datasetswithout query generation.We adopt LEAD for training,and adopttwowidely-useddatasetsforevaluation:(1) LeCaRD (Ma et al., 2021) is a widely-used LCR evaluation dataset, which contains 107queries annotated by several legal practitioners.(2) CAIL2022-LCR 3 official testing set is fur-nished by the CAIL2022 organization, structuredsimilarly to LeCaRD. We test our models on stage2 of CAIL2022.In both datasets, each queryhas 100 candidate cases, but only 30 of themare manually annotated. The annotations rangefrom 0 (Both key facts and key circumstancesare irrelevant) to 1 (Key facts are irrelevant butkey circumstances are relevant), 2 (Key facts arerelevant but key circumstances are irrelevant),and 3 (Both key facts and key circumstances arerelevant). We only consider the annotated cases,and regard cases marked as 3 as relevant.As a retrieval task, we report normalized dis-counted cumulative gain (NDCG@10, NDCG@20,NDCG@30), Precision (P@5, P@10), and MeanAverage Precision (MAP). These evaluation met-rics align with those used in LeCaRD, aimingto provide a comprehensive understanding of themodels performance across various aspects.",
  "We compare our model with several competitivebaselines, including:": "Traditional Retrieval Model: (1) BM25 (Robert-son and Zaragoza, 2009) utilizes exact word match-ing to score documents based on their term frequen-cies and document lengths.Pretrained Models: (1) Chinese BERT is anadaptation of the original BERT model (Devlinet al., 2018) for the Chinese. (2) Lawformer (Xiaoet al., 2021) is the first Chinese legal pre-trainedmodel based on the longformer model (Beltagyet al., 2020). (3) SAILER (Li et al., 2023a) is astructure-aware pre-trained model for LCR, whichemploys an asymmetric encoder-decoder architec-ture for pre-training.Data Augmentation Method: (1) Inverse Clozetask (ICT) (Lee et al., 2019) is a data augmenta-tion method for retriever pre-training, which ran-domly samples a span from a text segment as thequery, while the remaining context as the candi-date. (2) CaseEncoder (Ma et al., 2023) constructsLCR data with fine-grained legal article informa-tion, which assumes that similar cases should con-tain similar legal articles. (3) BGE-M3 (Chen et al.,2024) is trained on large-scale synthetic and labeleddata, showing strong generalization performance.Fine-Tuned Models: (1) T2Ranking (Xie et al., 2023) is a large-scale retrieval dataset in the open-domain. We directly utilize an open-source dual-encoder checkpoint, fine-tuned on the T2Rankingdataset as our baseline model. (2) GTE-Qwen1.5-7B-instruct (Li et al., 2023d) is based on a largelanguage model of 7B parameters and harnessesa multi-stage contrastive learning, demonstrat-ing broad applicability across various NLP tasks.(3) LeCaRD / CAIL2022 Train refers to the mod-els trained with the instances contained in LeCaRDor CAIL2022. Details are provided in appendixA.2. As one benchmark is used for training, weonly present the results of this model on the otherbenchmark.",
  "Implementation Details": "During evaluation, we employ a truncation strat-egy for lengthy candidates. Specifically, when thelength of a candidate case exceeds the maximumsequence length of the utilized models, we truncatethe case into multiple segments. Subsequently, weindividually calculate the similarity score betweeneach segment and the query, ultimately selectingthe maximum similarity score as the final score forthe candidate case.The training batch size is set as 128 and the en-coders are trained for up to 80 epochs with a learn- 0.000.250.500.751.00 0.45 0.50 0.55 0.60 LeCaRD P@5P@10MAP",
  ": Comparison of model performance with andwithout false negative masking": "ing rate of 1e-5 using Adam, linear scheduling withwarm-up, and dropout rate 0.1. The maximum in-put sequence length was set to 2048. Additionally,our model reported in utilizes positive aug-mentation data at a ratio of 70%. That is, 30% ofthe query-candidate pairs in the dataset consist ofqueries paired with their original cases, while theremaining 70% of query-candidate pairs comprisesimplified queries paired with cases newly identi-fied using the method outlined in .2. Werandomly select 2048 samples from the dataset asthe development set, with the rest used for training.",
  "Main Result": "The overall results are presented in . Fromthe results, we can observe that: (1) Our modeloutperforms all baselines on both benchmarks bya large margin, achieving state-of-the-art perfor-mance.It indicates that using larger-scale andmore comprehensive LCR data can greatly ben-efit task performance, which emphasizes the im-portance of developing data augmentation methodsfor LCR. (2) The traditional method, BM25, canoutperform many models. Especially, BM25 canbeat the models finetuned on T2Ranking, which",
  ": The results on the CAIL2019-SCM dataset": "consisting millions of open-domain retrieval in-stances. It proves that LCR task is challengingand directly employing open-domain models cannot achieve satisfactory results. That is becauseLCR requires the models to capture not only se-mantic relevance but also legal element relevance.(3) Compared to the pre-trained models, our modeltrained with LEAD can achieve siginificant perfor-mance improvements. The pre-training for LCRusually involves millions of cases and days of pre-training, which is computationally expensive. Itshows the potential of scaling high-quality data forLCR, which can avoid expensive pre-training andyield superior performance. (4) Our model can con-sistently outperform the data augmentation modelsand fine-tuned models. The existing data augmenta-tion method can not generate high-quality data forLCR. Besides, existing open-domain data cannotbenefit LCR performance, and the scale of exist-ing manually annotated LCR datasets like LeCaRDcannot fulfill the requirements of training dense re-trieval models, highlighting the importance of datascale rather than quality. Our proposed method toautomatically construct large-scale data is effectivein high-quality data generation.We also extend thebase model to LLM and train with our constructeddata, as presented in appendix A.4. Error AnalysisWe have noticed the followingerrors: although our model is trained to handleshort queries, it still struggles to identify the mostrelevant cases when the description of the case isonly a few words long (e.g., A killed B with a",
  ": The results of our model trained on LEAD and baseline models on CAIL2022-LCR under the traditionalcase-to-case symmetric retrieval setting": "hammer then escaped.). At this point, among themost relevant cases, there are sometimes one ortwo cases with completely different charges (e.g.,as hit-and-run). We assume that its still difficult forthe model to generate a vector representation of thelegal elements contained in overly short queries.Additionally, When two cases have many tokensin common, the model may overscore their similar-ity. For example, when retrieving medical malprac-tice cases, our model sometimes incorrectly ranksDUI (driving under the influence) cases highly be-cause both types of cases often involve many con-centration units (mg/mL).",
  "Ablation Study": "We adopt a knowledge-driven data augmentationstrategy for dataset construction. In this subsection,we conduct an ablation study to explore the impactof augmented positive examples.Proportion of Augmented Candidates. Weadopt a knowledge-driven data augmentation strat-egy to make the query-candidate pairs with similarlegal elements but diverse legal events. In this para-graph, to verify the effectiveness of the data aug-mentation, we conduct experiments with varyingproportions of augmented positive examples withinthe dataset. Specifically, we present the results withthe proportions as {0.00, 0.35, 0.700, 1.00}. Theresults are shown in .From the results, we can observe that: (1) Com-pared with models without data augmentation (0%),models trained with further data augmentation canachieve significant performance improvements forboth two datasets and all metrics. It indicates thatthe knowledge-driven data augmentation methodscan effectively match similar cases from the entirecorpus and benefit the diversity of LEAD. (2) The optimal performance is achieved at 70% and whenthe proportion reaches 100%, the model perfor-mance drops. This suggests that retaining a certainproportion of original cases as positive candidatesis effective for LCR. We believe this is becausethese data instances help reduce the distance be-tween simplified queries and original cases in thevector representation space, allowing the modelto better comprehend the meaning of simplifiedqueries in asymmetric retrieval scenarios. Addi-tionally, since the queries and the positive casesin this portion of the data come from the samecases, they have high semantic similarity, whichalso encourages the model to generate similar vec-tor representations for semantically similar cases.False Negative Masking. We adopt the in-batchnegative sampling strategy to increase the scale ofnegative sampling. However, this training strategywill inevitably introduce false negative noises. Toaddress this challenge, we adopt a false negativemasking strategy, where the cosine similarity ofnegative candidates with the same charges is setto during the training process. In this para-graph, we evaluate the effects of false negativemasking strategy, with the results presented in Ta-ble 3. We can find that removing the false negativemasking strategy significantly deteriorates modelperformance on both datasets. This suggests thatduring the training process, many negative exam-ples are indeed related to the query, and ignoringthem can mitigate such interference.",
  "Our method to automatically construct LCRdatasets is flexible and can be easily extended toany case. Existing LCR works usually focus oncriminal cases and overlook civil cases, which are": "more relevant to our daily lives. In this subsec-tion, we construct a civil case retrieval dataset withthe same construction method. Specifically, thejudgment results of civil cases are more complexthan criminal cases, and the knowledge-driven dataaugmentation strategy cannot be applied to civilcases. Therefore, here we present the results withno further candidate augmentation. Finally, wegenerate 77k query-candidate pairs for civil cases.We utilize CAIL2019-SCM (Xiao et al., 2019) asthe benchmark, which comprises 3036 triplets forthe private lending cases, each consisting of threecases: A, B, and C. The task is to determine whichof case, B or C, is more similar to A. We report theaccuracy of several models that are not limited tocriminal cases, and our model in . Despiteusing only simplified queries and their correspond-ing original cases as training data, our model canachieve the best performance on this test set. Thisdemonstrates that simple asymmetric retrieval datacan also enable the model to understand legal ele-ments, validating the robustness of our approach.",
  "Case-to-Case Symmetric Retrieval": "In this paper, we mainly focus on asymmetric LCRand our large-scale dataset can also benefit the tra-ditional case-to-case symmetric retrieval setting.In this subsection, we evaluate the models in thetraditional setting. The results are shown in Ta-ble 5. From the results, we can observe that (1) Ourmodel still outperforms other models by a largemargin, indicating that our constructed asymmetricretrieval dataset is not only effective for asymmet-ric retrieval tasks but also performs excellently intraditional case retrieval scenarios. This suggeststhat our model effectively learns to identify similarlegal elements through augmented positive exam-ples. (2) The baseline models can achieve superiorperformance on the asymmetric retrieval setting.That is because the lengthy query can provide moredetailed information for models to retrieve simi-lar cases. The short queries require the modelsto associate the key events and legal knowledgeto capture relevance between the query and candi-dates, which presents a great challenge for existingmodels. Therefore, we encourage the communityto devote more efforts to asymmetric LCR.",
  "Limitations": "In this paper, we discuss the limitations of this pa-per: (1) We construct a large-scale synthetic LCRdataset for Chinese cases. Our method is language-agnostic and can also be applied to cases in othercountries, which is worth exploring in the future.(2) We only fine-tune our model with LCR syn-thetic data. In the future, we can combine it withopen-domain synthetic data to train an embeddingmodel capable of multi-task applications.",
  "Acknowledgement": "This work is supported by the National Scienceand Technology Major Project (2022ZD0160502),the National Natural Science Foundation of China(62106126), the National Social Science Fund ofChina (21AZD143), the Guoqiang Institute, Ts-inghua University, Tsinghua-Toyota Joint ResearchFund, Beijing Advanced Innovation Center for Fu-ture Blockchain and Privacy Computing. Amin Abolghasemi, Suzan Verberne, and Leif Az-zopardi. 2022.Improving bert-based query-by-document retrieval with multi-task optimization. InProceedings of ECIR, volume 13186 of Lecture Notesin Computer Science, pages 312. Springer.",
  "Longformer: The long-document transformer. CoRR,abs/2004.05150": "Trevor J. M. Bench-Capon, Michal Araszkiewicz,Kevin D. Ashley, Katie Atkinson, Floris Bex, FilipeBorges, Danile Bourcier, Paul Bourgine, Jack G.Conrad, Enrico Francesconi, Thomas F. Gordon,Guido Governatori, Jochen L. Leidner, David D.Lewis, Ronald Prescott Loui, L. Thorne McCarty,Henry Prakken, Frank Schilder, Erich Schweighofer,Paul Thompson, Alex Tyrrell, Bart Verheij, Dou-glas N. Walton, and Adam Z. Wyner. 2012. A history",
  "Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-ing Yang, and Sanjiv Kumar. 2020. Pre-training tasksfor embedding-based large-scale retrieval. In Pro-ceedings of ICLR. OpenReview.net": "Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, DefuLian, and Zheng Liu. 2024. BGE m3-embedding:Multi-lingual, multi-functionality, multi-granularitytext embeddings through self-knowledge distillation.CoRR, abs/2402.03216. Ting Chen, Simon Kornblith, Mohammad Norouzi, andGeoffrey E. Hinton. 2020. A simple framework forcontrastive learning of visual representations. In Pro-ceedings of ICML, volume 119 of Proceedings of Ma-chine Learning Research, pages 15971607. PMLR.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. BERT: pre-training ofdeep bidirectional transformers for language under-standing. CoRR, abs/1810.04805": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He,Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,Yuxiang Huang, Weilin Zhao, Xinrong Zhang,Zhen Leng Thai, Kai Zhang, Chongyi Wang, YuanYao, Chenyang Zhao, Jie Zhou, Jie Cai, ZhongwuZhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li,Zhiyuan Liu, and Maosong Sun. 2024. Minicpm: Un-veiling the potential of small language models withscalable training strategies. CoRR, abs/2404.06395. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2022. Unsupervised dense in-formation retrieval with contrastive learning. TMLR,2022. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,and Wen-tau Yih. 2020. Dense passage retrieval foropen-domain question answering. In Proceedings ofEMNLP, pages 67696781. Association for Compu-tational Linguistics. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.2019. Latent retrieval for weakly supervised opendomain question answering. In Proceedings of ACL,pages 60866096. Association for ComputationalLinguistics. Haitao Li,Qingyao Ai,Jia Chen,Qian Dong,Yueyue Wu, Yiqun Liu, Chong Chen, and Qi Tian.2023a.SAILER: structure-aware pre-trained lan-guage model for legal case retrieval. In Proceedingsof SIGIR, pages 10351044. ACM.",
  "Yixiao Ma, Yueyue Wu, Qingyao Ai, Yiqun Liu, YunqiuShao, Min Zhang, and Shaoping Ma. 2024. Incorpo-rating structural information into legal case retrieval.ACM Trans. Inf. Syst., 42(2):40:140:28": "Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai,and Yiqun Liu. 2023. Caseencoder: A knowledge-enhanced pre-trained model for legal case encoding.In Proceedings of EMNLP, pages 71347143. Asso-ciation for Computational Linguistics. Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoqiaoShe, Jing Liu, Hua Wu, and Haifeng Wang. 2022.Dureader-retrieval: A large-scale chinese benchmarkfor passage retrieval from web search engine. In Pro-ceedings of EMNLP, pages 53265338. Associationfor Computational Linguistics.",
  "Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu,and Maosong Sun. 2021. Lawformer: A pre-trainedlanguage model for chinese legal long documents. AIOpen, 2:7984": "Chaojun Xiao, Zhiyuan Liu, Yankai Lin, and MaosongSun. 2023. Legal knowledge representation learning.In Representation Learning for Natural LanguageProcessing, pages 401432. Springer Nature Singa-pore Singapore. Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, CunchaoTu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang,Xianpei Han, Heng Wang, Jianfeng Xu, et al. 2019.Cail2019-scm: A dataset of similar case matching inlegal domain. arXiv preprint arXiv:1911.08962. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv,Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li,Haitao Li, Yiqun Liu, and Jin Ma. 2023. T2ranking:A large-scale chinese benchmark for passage ranking.In Proceedings of SIGIR, pages 26812690. ACM. Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu,Lei Hou, Cunchao Tu, Juanzi Li, Yun Liu, WeixingShen, and Maosong Sun. 2022. LEVEN: A large-scale chinese legal event detection dataset. In Find-ings of ACL, pages 183201. Association for Compu-tational Linguistics. Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong,Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022.Explainable legal case matching via inverse optimaltransport-based rationale extraction. In Proceedingsof SIGIR, pages 657668. ACM.",
  "A.1Data Construction Details": "To generate concise case descriptions from casefacts, we employ a large-scale generative languagemodel, for query generation. The input instruc-tions and a sample case description, along with itsoriginal case fact, are shown in .The generated case description retains all the le-gal elements from the original case fact while omit-ting the rest of the content. The original case fact,being part of a court judgment, contains a plethoraof details to comprehensively describe the casesproceedings. However, including these details aspart of a real-world user query is redundant.",
  "A.2Experimental Details": "Training with LeCaRDLeCaRD training setannotates 30 cases for relevance to each query.When constructing the dataset, for each queryQi, all cases with a relevance score of 3 are des-ignated as {Pi1, Pi2, ..., Pin}, while the remain-ing cases are designated as {Ni1, Ni2, ..., Nim}.If m < n, then m n cases are randomly se-lected from the 70 unannotated cases to form{Ni(m+1), Ni(m+2), ..., Nin}. Each training datumconsists of one query, one positive case, and onenegative case, denoted as (Qi, Pij, Nij), wherei = 1, 2, ..., 107 and j = 1, 2, ..., n. This processresults in a training set of size 1,112. The remainingimplementation details are same as those describedin .3. Existing datasets usually containlimited annotated pairs and cannot fulfill the re-quirements for the training of data-hungry neuralmodels.",
  "A.3Addition Experiment Result": "We also conducted experiments on the originalLeCaRD dataset under the traditional case-to-casesymmetric retrieval setting, and the results areshown in . Here, we present the resultsof all baseline models and the models trained onLEAD with different proportions of augmentedpositive examples.From the results, we can observe that similarto the results on the CAIL2022-LCR dataset, ourdataset, LEAD can significantly benefit the per-formance of traditional case-to-case symmetric re-trieval.",
  "A.4.1Implementation Details": "LLM is typically trained on the Next Token Predic-tion task, utilizing causal attention and Last TokenPooling strategy. To adapt the model into an Em-bedding Model, we first modified it to bidirectionalattention and Mean Pooling strategy.We employed the open-source generative lan-guage model MiniCPM (Hu et al., 2024). For ourtraining setup, we set the batch size to 128 andtrained the model for up to 10 epochs with a learn-ing rate of 1e-4 using Adam, linear scheduling. Thesoftmax score was set to 0.2. Due to computationalconstraints, we limited the sequence length to 512and employed LoRA (Hu et al., 2022) with a rankof 16. Additionally, we enabled mixed precisiontraining with bfloat16. We did not use the falsenegative masking strategy here. A.4.2Main resultAs shown in , although MiniCPM is a gen-erative language model, the results of training itdirectly with LCR data still significantly surpassthe strongest baseline, SAILER. This demonstratesthe powerful potential of scaling models in LCR.By incorporating data from other domains, we cantrain large models that perform exceptionally wellacross multiple tasks.",
  "A.5Articles of the Criminal Law of thePeoples Republic of China": "Article 67[General Voluntary Surrender] If, after commit-ting a crime, the offender voluntarily surrendersand truthfully confesses their crime, it is consideredvoluntary surrender. For offenders who voluntarilysurrender, a lighter or mitigated punishment maybe imposed. If the crime is minor, the punishmentmay be waived.[Special Voluntary Surrender] If a criminal sus-pect, defendant, or convict under compulsory mea-sures truthfully confesses to other crimes not yetknown to the judicial authorities, it is consideredvoluntary surrender.Even if a criminal suspect does not meet theconditions for voluntary surrender specified in theprevious two paragraphs, a truthful confession oftheir crime can lead to a lighter punishment; ifthe truthful confession prevents particularly severe consequences, a mitigated punishment may be im-posed.Article 133[Traffic Accident Crime] Violating traffic andtransportation regulations resulting in a major ac-cident that causes serious injury, death, or signifi-cant property damage shall be punished by impris-onment of up to three years or criminal detention.If the offender flees the scene of the accident orif there are other particularly egregious circum-stances, the punishment shall be imprisonment ofthree to seven years. If fleeing the scene results ina persons death, the punishment shall be impris-onment of seven years or more.Article 133-1 [Dangerous Driving Crime] Driv-ing a motor vehicle on the road under any of thefollowing circumstances shall be punished by crim-inal detention and a fine:(1) Racing in a particularly egregious manner;(2) Driving a motor vehicle while intoxicated;(3) Seriously exceeding the passenger limit orthe speed limit while engaged in school bus orpassenger transport services;(4) Violating safety management regulations forthe transport of hazardous chemicals, thereby en-dangering public safety.If the owner or manager of the motor vehicleis directly responsible for the actions specified initems (3) and (4) of the preceding paragraph, theyshall be punished according to the preceding para-graph.If the actions specified in the preceding two para-graphs also constitute other crimes, the more se-vere punishment shall apply.Article 133-2 [Obstructing Safe Driving Crime]Using violence against or forcibly taking control ofthe operating equipment of the driver of a publictransportation vehicle in operation, thereby inter-fering with the normal operation of the vehicle andendangering public safety, shall be punished byimprisonment of up to one year, criminal detention,or control, and may also be fined.If the driver of a public transportation vehiclein operation abandons their post, fights with oth-ers, or assaults others, thereby endangering publicsafety, they shall be punished according to the pre-ceding paragraph. System:As a legal expert, you are capable of extracting keyelements from the basic information of a case.User:I have a dataset for searching cases by case. How-ever, the basic information of the cases in my datasetis too long. I will send you the basic informationof these cases, please help me simplify them, andgreatly shorten their length while retaining key legalelements. You can remove non-key names, locations,etc., but do not delete important elements for casejudgments.Assistant:I understand your requirement.User:In August 2015, XX Co., Ltd. obtained the right touse state-owned construction land in XX Lake areaof XX District through public auction, and developedthe \"XX\" project. The defendants Alice, Bob, Char-lie, David, and Edward are villagers of XX Village,XX Street Office, XX District, within the scope ofthe project. The demolition and compensation withinthe scope of the land has been implemented by theManagement Committee of XX in 2014. After the\"XX\" project started construction on July 13, 2016,Alice, Bob, and others proposed to the constructionparty to contract part of the project such as excava-tion of retaining walls and earthwork excavation offoundation piles. Because they did not have the con-struction qualification, the project developer did notagree. From July 20 to August 12, 2016, Alice, Bob,Charlie, David, and Edward convened at \"XX\" and\"XX\" Tea House to discuss obstructing the construc-tion, and raised 70,000 yuan for the obstruction fund.At the same time, Bob, Charlie, David, and othersused WeChat groups and phone calls to invite andmobilize more than 20 villagers from their villageand their family members to obstruct the constructionof the \"XX\" construction site by methods such aslocking the gate, pulling the power switch, insulting,and standing on construction machinery, causing theconstruction site to be unable to proceed normally.The economic loss caused by the obstruction duringthe construction period was appraised by XX DistrictPrice Certification Center as 124,530 yuan.Assistant:A company obtained the right to use constructionland through auction and compensation has been im-plemented, but villagers nearby conspired to obstructconstruction, raising a fund of 70,000 yuan and mo-bilizing more than 20 villagers to repeatedly obstructthe construction site, causing an economic loss ap-praised at 124,530 yuan.",
  ": Prompt and an example for generating shortqueries, where specific information such as names andlocations have been anonymized": "Query Case 1:The defendant, on a certain day while passing througha neighborhood, stole a motorcycle worth approxi-mately 3,000 yuan. Subsequently, the defendant rodethe motorcycle to another city, intending to sell itto someone, but was apprehended on the spot by theowner. The stolen motorcycle has been recovered andreturned to the victim. The defendant has confessedto their crime.Positive Case 1:The Peoples Procuratorate of Hanshan County ac-cuses: On the evening of September 24, 2017, the de-fendant Li Jun walked to the entrance of the old trans-portation bureau dormitory lane opposite HanshanNo. 2 Middle School, and stole the Jixiangshi brandtwo-wheeled electric bike parked there by reconnect-ing the electric wire. The next evening, the defendantLi Jun rode the stolen electric bike to the ShanghaiQiqiang Electric Bike Shop located at Wangmei Roadin Hanshan County for sale. Since the price nego-tiation with the shop owner was not successful, hethen hid the electric bike under the building of HanCity River and River Water Conservancy Construc-tion and Installation Co., Ltd. The appraisal price ofthe stolen electric bike was 1760 yuan. On October 1,2017, the defendant Li Jun was arrested at his homein Motang Village, Chengbei Administrative Village,Huanfeng Town, Hanshan County by the HanshanCounty Public Security Bureau. On October 2, 2017,the Hanshan County Public Security Bureau returnedthe stolen vehicle to the victim Mao.Query Case 2:Defendant Alice was driving a car while intoxicated,rear-ending another vehicle and causing propertydamage. Alice was determined to be fully respon-sible. Alices blood alcohol content exceeded thelegal limit.Positive Case 2:After investigation, it was found that on January 20,2012, at around 8:10 PM, the defendant, Yu, haddinner and drank alcohol with friends. After drink-ing, he drove the vehicle with license plate ShaanxiAWB062 home. While driving north along Ming-guang Road and approaching the intersection withFengcheng 8th Road, he failed to brake in time andcollided with the rear end of the vehicle with licenseplate Shaanxi AFU210, driven by Guo Guangcheng,who was waiting at the traffic light. This causedGuos vehicle to rear-end the vehicle in front, withlicense plate Shaanxi A05V90, driven by Zhao Ming,resulting in a traffic accident involving damage toall three vehicles. The public security authorities ap-prehended the defendant, Yu, at the scene. The roadtraffic accident report determined that the defendant,Yu, was fully responsible for the accident, while GuoGuangcheng and Zhao Ming bore no responsibility.It was determined that the defendant, Yu, had a bloodalcohol concentration of 180.51 mg/100 ml. Furtherinvestigation revealed that on February 2, 2012, thedefendant, Yu, paid Zhao Ming 12,000 yuan for ve-hicle repairs. On February 10, 2012, the defendantcompensated Guo Guangcheng 65,000 yuan, afterwhich Guo Guangcheng transferred ownership of thevehicle with license plate Shaanxi AFU210 to Yu."
}