{
  "Abstract": "Large Language Models (LLMs) have thepromise to revolutionize computing broadly,but their complexity and extensive training dataalso expose significant privacy vulnerabilities.One of the simplest privacy risks associatedwith LLMs is their susceptibility to member-ship inference attacks (MIAs), wherein an ad-versary aims to determine whether a specificdata point was part of the models training set.Although this is a known risk, state of the artmethodologies for MIAs rely on training multi-ple computationally costly shadow models,making risk evaluation prohibitive for largemodels. Here we adapt a recent line of workwhich uses quantile regression to mount mem-bership inference attacks; we extend this workby proposing a low-cost MIA that leveragesan ensemble of small quantile regression mod-els to determine if a document belongs to themodels training set or not. We demonstratethe effectiveness of this approach on fine-tunedLLMs of varying families (OPT, Pythia, Llama)and across multiple datasets. Across all sce-narios we obtain comparable or improved ac-curacy compared to state of the art shadowmodel approaches, with as little as 6% of theircomputation budget. We demonstrate increasedeffectiveness across multi-epoch trained targetmodels, and architecture miss-specification ro-bustness, that is, we can mount an effective at-tack against a model using a different tokenizerand architecture, without requiring knowledgeon the target model. 1",
  "*These authors contributed equally to this work1Our code is available at": "support, they often require fine-tuning on propri-etary, domain-specific datasets to improve theirperformance and relevance. This process, how-ever, can inadvertently expose sensitive informa-tion. MIAs witness this vulnerability by reliablydetermining whether a specific data point was partof the training dataset or not, thereby potentiallyrevealing personal or proprietary information.Membership inference attacks are used to au-dit the privacy of trained models, and successfulexternal attacks can lead to breaches of confiden-tiality, financial loss, and erosion of user trust. Fine-tuning can amplify these risks, as models trained onsmaller, specialized datasets are more susceptibleto memorizing and revealing specific data points,and specialized datasets not found on the open inter-net can contain sensitive user information. Recentstudies have found that credible privacy attacks canbe mounted against modern LLMs (Carlini et al.,2021, 2022; Mattern et al., 2023). However, it isrelatively uncommon to routinely assess fine-tunedLLMs for MIA risk, as current state of the art MIAsrequire the training of several shadow models models that are, ideally, identical in nature to themodel under attack in terms of architecture, train-ing data distribution, and hyperparameters (Shokriet al., 2017; Carlini et al., 2022; Sablayrolles et al.,2019; Watson et al., 2021). The result is that mount-ing such an attack the building block of an audit is substantially more expensive than training theLLM in the first place.To circumvent the high computational costs ofmounting a shadow-model-based attack, recentworks such as (Bertran et al., 2024; Tang et al.,2024) has attempted to directly reduce the cost ofhypothesis-testing style membership inference at-tacks by replacing shadow models with a quantileregression step that directly estimates the feature-conditional quantile of a score function from dataknown not to have been used in model training,where the target quantile of the score distribution directly corresponds to the false positive rate of theattack. The quantile regression approach is compu-tationally attractive because it only requires train-ing a single model (rather than multiple shadowmodels), and the architecture of the quantile regres-sion model need not be related to (and indeed canbe much simpler than) the architecture of the modelunder attack. Prior work has only demonstrated theeffectiveness of this approach in relatively simpleclassification settings (Bertran et al., 2024) and forsmall diffusion models (Tang et al., 2024). In thispaper we extend this line of quantile-regressionbased attacks to large language models. We brieflysummarize our contributions: We propose the use of low-cost regression en-sembles to launch MIAs against LLMs. Eachmodel in our ensemble can be significantlysmaller and therefore cheaper to train than themodel under attack, and need not use the sametokenizer or belong to the same model family.To exemplify this, we use Pythia-160m (Bider-man et al., 2023) and OPT-125m (Zhang et al.,2022) architectures to attack Pythia, Llama(Touvron et al., 2023), and OPT models up to7b parameters. We investigate performance across a varietyof scoring objectives and across architectureand tokenizer. Overall, we find that our resultsare robust to the scoring function and the cho-sen architecture, in contrast to shadow modelbased approaches which are much more sensi-tive to architecture choices. We demonstrate the effectiveness of our ap-proach at launching MIAs against LLMs inthe low false positive rate regime on the chal-lenging single epoch training setting on AGNews, WikiText, and XSum datasets (Zhanget al., 2015; Merity et al., 2016; Narayan et al.,2018). Our approach robustly outperformsother baselines, with as little as 6% of thecompute required on the larger architectures.",
  "True Positive Rate": "Ours: AUC=0.976LiRA (160m): AUC=0.733LiRA (410m): AUC=0.834LiRA (1.4b): AUC=0.936LiRA (2.8b): AUC=0.973LiRA* (160m): AUC=0.941LiRA* (410m): AUC=0.973LiRA* (1.4b): AUC=0.987LiRA* (2.8b): AUC=0.989Loss: AUC=0.856min-k: AUC=0.882zlib: AUC=0.792neighbor: AUC=0.571 : Comparing true positive rates vs false positiverates of our method with LiRA variants and marginalbaselines with different scoring functions on XSumwhere target model is Pythia-6.9b. LiRA* representsLiRA with fixed variance estimate. Results for LiRAare obtained with 4 shadow models from Pythia familywith varying sizes. Results for our method are obtainedwith ensemble of 5 quantile regression models finetunedfrom Pythia-160m.",
  "Because shadow models are designed to producesamples from the null or alternative hypothesis dis-": "tributions, shadow-model-based methods requireknowledge of the model architecture and trainingprocess so as to be able to replicate the entire train-ing pipeline of the model under attack. This ad-ditionally makes training each shadow model asexpensive as training the model under attack. Mo-tivated by these limitations, Bertran et al. (2024)proposed a quantile regression based attack whichreframes the hypothesis testing problem over therandomness of the training and test data, rather thanover the randomness of the model training. Bertranet al. (2024) evaluate their method for classificationmodels, and Tang et al. (2024) extends this methodto attack simple diffusion models. Our work is adirect extension of this line.",
  "Membership Inference on LLMs": "Because shadow model approaches require repli-cating the model training process many times, theyare prohibitively expensive to mount on large lan-gauge models. As a result many recent works onmembership inference attacks on LLMs focus onproposing more effective scoring functions that canbe applied without calibration (i.e., using marginalthresholds). Examples include local curvature ofthe loss by comparing samples with neighboringtexts (Mattern et al., 2023), conditioned score on asubset of high perplexity tokens (Shi et al., 2024),and re-normalizing loss by compression length un-der zlib (Carlini et al., 2021), among others (Longet al., 2018; Watson et al., 2021). While thesemethods produce results with minimal computa-tional costs, their performance lags behind thosethat learn a calibration function.",
  "Memorization in LLMs": "Memorization is a related but different concernthan vulnerability to membership inference, oftenmotivated by copyright infringement. Definitionsof memorization are still being actively explored.There are have been attempts to define memoriza-tion through prompting language models to re-gurgitate text with varying types of prefixes (Car-lini et al., 2021),(Nasr et al., 2023),(Carliniet al., 2023), counterfactual notions of memoriza-tion (Zhang et al., 2023), and adversarial compres-sion rate of text (Schwarzschild et al., 2024).",
  "Here we provide a detailed description of our at-tack, starting with the general idea of how score-based membership inference attacks are designed,": "and followed with a technical description of howwe design our low-cost ensemble attack.We follow a standard setup of membership in-ference attacks in which the adversary has queryaccess to an LLM f trained on an unknown datasetDpriv in terms of log likelihoods per-token for an ar-bitrary input sequence, sampled from a documentdistribution D. Each sample x Dpriv consistsof a document or sentence, usually split into to-kens x = {xi} by a model-specific tokenizer. Themodel f outputs a probability distribution of thenext token xi conditioned on the preceding tokensequence x<i = x1, . . . , xi1. We let f(xi | x<i)denote the likelihood of token xi assigned by modelf, conditioned on the preceding tokens. A modelf is usually fine-tuned on a dataset Dpriv by mini-mizing negative log likelihood:",
  "i[n] log f(xi | x<i), (1)": "with n the number of tokens in x.Because of this, training samples are potentiallymemorized, or are more likely under the modelsdistribution than other, similar samples that mightbe equally likely under the sampling distribution.A membership inference attack is a hypothesistest that exploits this tendency by using a test statis-tic (score) derived from queries to f that aims todetermine whether a document x is a member ofthe training set Dpriv or not. We cast this as distin-guishing between a null and alternative hypothesis:H0 : x DH1 : x Dpriv. We restrict our attention to membership inferenceattacks that define a test statistic (score) s(x; f).These attacks determine if this input-score pair(s(x; f), x) is likely under the null hypothesis, andaccuse a document of being part of the privatedataset if this test fails (i.e. rejects the null). Welldenote s(x; f) = s(x) = s when clear from con-text. A score is any function computable givenaccess to the model f and target point x. The inten-tion is to choose a score that takes systematicallyhigher values for x Dpriv. Examples of suchscores are discussed in 2.2; for scores that are com-puted per token, we take the per document score tobe the token-averaged score.The adversarys goal is to learn an attack func-tion Af : X {0, 1} that implements the hypoth-esis test described above. The works discussedhere follow a common thread by implementing theadversary as",
  "Af(x) = 1[s(x) q(x)],(2)": "where the threshold q(x) is sometimes referredto as difficulty calibration. Attacks are differ-entiated based on their choice of score function,and their choice of threshold function. For ex-ample (Yeom et al., 2018) uses negative log like-lihood as their score function, and a constant(marginal) threshold function. (Sablayrolles et al.,2019; Watson et al., 2021; Carlini et al., 2022)use shadow models (Shokri et al., 2017) to de-termine a suitable per-example threshold.No-tably, (Carlini et al., 2022) proposes an offlinetest that models the score distribution of a doc-ument under H0 as N((x), (x)2), where themean and variance are the empirical mean andvariances of the score function computed acrossall shadow models that do not include x in theirtraining set, the threshold function q(x) is thencomputed as a quantile of the normalized scoredistribution q(x) = 1(1 )(x) + (x) with a target false positive rate and 1 the inverseCDF of a standard distribution. There are otherscore functions such as min-k (Shi et al., 2024),zlib entropy (Carlini et al., 2022), and neighbor-hood comparison attack (Mattern et al., 2023) thatcan be viewed as adaptive threshold methods. Inthis work we choose to characterize them as non-adaptive scores since this characterization enablesfurther refinement using shadow models and quan-tile regression.",
  "Quantile Regression": "The recent work of (Bertran et al., 2024) proposedto do away with shadow models by instead learn-ing a quantile regression model to directly pre-dict (a quantile of) the score function for publicdata by minimizing pinball loss for the target quan-tile. Here we instead build our parametric regres-sion model as a pair of functions : X R, : X R+ that respectively predict the meanand standard deviation of the score distribution un-der the null hypothesis. Given a dataset Dpub Dand a family of regression models r R we mini-mize either",
  "PB(1)((x) + (x), s)].(4)": "Where in the first scenario we learn the mean andstd of the distribution by minimizing negative loglikelihood of a normal distribution, and on the latterwe directly learn the median and (1) quantile ofthe distribution using pinball loss2; the (1) quan-tile corresponds to a point falling below 1 standarddeviation above the mean of a standard Gaussian,and thus is chosen as a natural target for a robustestimate of standard deviation of the score distribu-tion. The second objective we propose shares someof the advantages of the parametric negative loglikelihood approach (only two outputs are neededto model the score distribution), but instead relieson robust quantile estimators that can be used to de-rive mean and standard deviation of the distributionunder the Gaussian assumption.",
  "In both settings, the quantile threshold is com-puted as q1(x) = 1(1 )(x) + (x) with a target false positive rate and 1 the inverseCDF of a standard distribution": "To decide which of these objectives providesa more suitable base model, we choose the onewith the smallest pinball loss at the target falsepositive rate measured on public data, similarly to(Bertran et al., 2024). We note that this statistic canbe computed without access to private data. Therelative performance of these objectives varies bydataset, with both producing strong results.",
  "Ensemble of Quantile Regression Models": "The work in (Tang et al., 2024) on MIAs againstdiffusion models used a bootstrapping approach inwhich multiple small quantile regression modelsvoted on whether to accuse a point of membershipin the training set. Our preliminary experimentsshowed that using the entire dataset per ensem-ble produced better results than bootstrapping, sohere we instead choose to leverage the entire pub-lic data Dpub by using deep ensembles of (weak)learners as in (Lakshminarayanan et al., 2017). Wetreat each model in the ensemble as a uniformly-weighted mixture model, and compute the meanand variance of the ensemble as",
  "m[M]2m(x)+2m(x)2(x). (6)": "This methodology allows us to leverage moreof the available public samples on each individualmodel of the ensemble, compared to a bootstrapapproach, where roughly 63% of the samples areused per ensemble3. This approach averages themean and std of the models, and then computesthe appropriate quantile given these averaged pa-rameters, as opposed to the voting approach usedin (Tang et al., 2024) in which each model of theensemble votes on the membership of a document.",
  "Datasets": "We conducted experiments on three public datasetsacross different domains: AG News (Zhang et al.,2015), WikiText-103 (Merity et al., 2016), andXSum (Narayan et al., 2018). On WikiText-103,we sampled around 22.5% of the full dataset andexcluded examples that contain less than 25 char-acters. On XSum, we took the original article asthe text samples. On each dataset, we split the datasamples into two halves, where one half is usedto fine-tune language models and is regarded asthe private dataset. The other half is regarded asthe public dataset and further split into two sets,which we name as public-train and public-test re-spectively. Public-train set was used to train quan-tile regression models for our method and shadowmodels for LiRA while the public-test set was usedas a holdout set for testing. Membership inferenceattacks were evaluated on the union of private andpublic-test splits. shows the split sizes andstatistics on sample length for each dataset.",
  "Baselines": "To evaluate the performance of the proposedmethod, we compared it with different score func-tion baselines without difficulty calibration, includ-ing loss attack (Yeom et al., 2018), min-k% (Shiet al., 2024), zlib entropy (Carlini et al., 2021) andneighborhood comparison attack (Mattern et al.,2023). We also conducted extensive comparisonagainst LiRA (Carlini et al., 2022) variants that usevariable and fixed variance estimates.",
  "Implementation Details": "We fine-tuned target language models on the privatesplit of each dataset for 3 epochs with Adam with alearning rate of 5 105 and batch size of 64; weused HuggingFace public checkpoints as a startingpoint for all models. Unless otherwise noted, wereport MIA results on the first epoch of training,since this represents the most challenging scenariowhere each sample in the private set is only seenonce by the target model; and use document-leveltoken-averaged negative log likelihood as our basescoring function. Shadow models used in LiRAexperiments were trained on sampled subsets ofthe public-train split of each dataset with identicalsettings as the target language models. Quantileregression models were trained on the public-trainsplit of the dataset for 4 epochs with Adam with alearning rate of 2105 and batch size of 128. Westored snapshots of the quantile regression modelat integer epochs and picked the snapshot with thebest evaluation loss on a holdout set sampled fromthe public-train split. All experiments were con-ducted on a machine with 8 V100 GPUs.",
  "%": "P-22.455.194.97 3.50 17.9234.2528.731.131.061.05 1.37 48.7933.2345.941.936.984.53 1.89 62.7170.8759.38P-63.357.087.46 3.68 17.6533.7834.171.091.111.12 1.30 50.7240.7258.522.70 11.10 6.55 2.07 50.8776.9972.68O-21.452.833.30 2.47 16.3026.7521.411.061.031.05 1.10 33.7423.2226.981.343.502.79 1.44 49.7756.0036.43O-62.655.146.20 3.36 19.7431.7433.790.931.041.08 1.12 34.7836.7453.691.836.885.51 1.70 32.8371.9167.70L-74.156.377.59 3.187.5125.4539.381.681.681.62 1.424.6217.3461.873.71 14.34 5.43 1.814.6945.2881.46 : True positive rates (%) at 0.1% and 1% false positive rate (FPR) of different membership inference methodson the three datasets. P-2, P-6, O-2, O-6, L-7 correspond to Pythia-2.8b, -6.9b, OPT-2.7b, -6.7b, and Llama-7b,respectively. LiRA* represents LiRA with fixed variance. All LiRA results are obtained with 4 shadow models. Theshadow models are Pythia-2.8b models for Pythia models, and OPT-2.7b models for OPT and Llama models. size affects the performance of our method. Finallywe study different factors affecting the privacy risksof fine-tuned models including target model sizesand training epochs and how these impact differentMIA methods. Additional experiments measuringrobustness to score function selection are presentedin Appendix A.",
  "Comparison with Baselines": "shows the performance of our proposedmethod and baselines on AG News, WikiText andXSum. We compute the true positive rates at 0.1%and 1% false positive rates of all methods. Dueto compute limits, we trained 4 shadow modelsfor each setting4 and did not train shadow modelsfrom exactly the same pretrained model for largermodels. For Pythia-6.9b and OPT-6.7b, we usedPythia-2.8b and OPT-2.7b as shadow models corre-spondingly. For Llama-7b, we picked OPT-2.7b asthe shadow model architecture as it showed betterperformance compared to Pythia-2.8b with LiRAmethods. The results for our method were obtainedusing an ensemble of 5 models.We observe that loss, min-k%, zlib entropy,and neighborhood comparison attacks performpoorly, especially on the more challenging Wiki-Text dataset where there is a great variety in topicand text length among the samples. This highlightsthe importance of per-sample calibration in achiev-ing high performance in low false positive regime.In our experiments across all datasets, ourmethod shows performance comparable to the twoLiRA variants. It achieves the best performanceamong all methods at 0.1% FPR across all datasetsand model families. This illustrates the effective-",
  "We note that since these shadow models are trained ex-clusively on public data, we use the offline LiRA version andeach shadow model counts as an out sample": "ness and robustness of our method in the low falsepositive rate regime. LiRA achieves strong per-formance at 1% FPR across datasets when targetmodels and shadow models are derived from ex-actly the same pretrained model. For Pythia-6.9band OPT-6.7b, LiRA methods are outperformed byour method on AG News and WikiText. This islikely due to the mismatch in model sizes betweenshadow models and target models, necessitated byLiRAs very large computational requirements. ForLlama-7b, where shadow models are from a differ-ent model family, LiRA methods are outperformedby our method across all three datasets by a largemargin. These results demonstrate the favorableperformance of our method compared to LiRA withfew shadow models especially when it is impracti-cal to leverage shadow models that share the samearchitecture or size with the target model architec-ture due to limited compute or lack of information.The following section details the exact computa-tion costs of each attack. Extended results on ROCcurves are presented in Appendix B.",
  "Scalability of our Attack": "shows a performance comparison betweenour method and LiRA at different shadow andregression model sizes when the target model isPythia-6.9b. For LiRA methods, performance gen-erally improves with the size of shadow models,which is unsurprising since there is less differencebetween target and shadow models. The trend isparticularly evident for LiRA with per-sample vari-ance, while LiRA with fixed variance is more stableacross different shadow model sizes. This indicatesthat the variance estimate in LiRA is significantlymore sensitive to shadow model sizes comparedto mean estimate, at least on this particular sce-nario. To achieve competitive results with LiRA onchallenging datasets such as WikiText, it would bebest to use shadow models of similar sizes as targetmodel. In contrast, our method achieves high per-formance even when the size of the target modelis significantly larger than the regression model.Additional analysis on performance by regressionmodel size of our method is shown in Appendix D. shows a comparison on time requiredto train a single model for the attacks on XSum,Pythia-160m regression models for our method andPythia models up to 6.9b for LiRA. Our methodrequires only 6% of the compute time required forLiRA with 4 Pythia-2.8b shadow models and 1.5%of the time would be required if Pythia-6.9b shadowmodels were used.",
  "Cross Family Performance": "In , we show a comparison of our methodwith LiRA on WikiText where the model familyvaries among target model and attacker models. Inthe experiments with Pythia-6.9b and OPT-6.7b astarget models, we observe that both our method andLiRA performs better when the target model andshadow models are from the same model family ingeneral. However, the performance of our method is less influenced by the difference in model fami-lies. In fact, our method with mismatched modelfamilies is able to outperform LiRA with matchedmodel families in the experiments. In the experi-ments with Llama-7b, we observe a dramatic degra-dation in the performance of LiRA methods. Inconstrast, our method is able to achieve relative sta-ble performance as measured by TPR at 1% FPRwith different choices of model families. Nonethe-less, a significant difference in TPR at 0.1% FPR isobserved for our method, signifying the difficultyof maintaining competitive performance at lowerfalse positive regime when target model architec-ture is not exactly known.",
  "Effect of Ensemble Size": "shows the results achieved by our methodusing varying ensemble configurations on threedatasets, where the target model is Pythia-6.9b. Weobserve that performance improves when ensem-ble size increases in general. The variance in truepositive rates from different runs of our methodtends to decrease when the ensemble size increases.The performance at 1% FPR stabilizes for ensem-ble size 5 while there is some fluctuation at 0.1%FPR. This can be explained by the fact the thresh-old corresponding to 0.1% FPR is more sensitiveto noise as the number of samples in considerationis less. shows the distribution of standarddeviation of z-scores computed from different runsof our method with varying ensemble sizes. Withan increased ensemble size, we observe variance inthe computed z-scores from different runs reducesamong the samples from the three datasets. As aresult, the noise in our prediction is reduced, whichleads to better performance from ensembling. 0.06 0.07 0.08 0.09",
  "Effect of Training Epoch and Model Size": "The preceding results all showed MIA performanceagainst a single epoch of fine-tuning, since that isthe harder setting to attack5. In this section, westudy the how training epochs and size of targetmodel affects privacy risk captured by differentmethods. shows results when the target modelis OPT-6.7b with varying epochs of fine-tuning.All the methods achieve higher true positive rateswhen the target model is trained with more epochs,indicating an increase in the privacy risks associ-ated. This finding is consistent with the findingsin (Duan et al., 2024). While the performance ofsimple score-function-based methods are relativelypoor on models trained for one epoch, they becomemore competitive when the number of epochs in-creases. Among the methods with per-sample cal-ibration, our method consistently achieves betterperformance at 0.1% FPR and comparable perfor-mance at 1% FPR to variants of LiRA.",
  "Conclusion": "We have developed a membership inference at-tack methodology for large language models thatis more computationally efficient than the priorstate of the art by almost two orders of magnitudewithout sacrificing effectiveness in all experi-ments the accuracy of our attack either exceeds oris comparable to that of LiRA. These efficiency im-provements are especially important when MIAsare intended to be used as a routine privacy auditingprocedure for large deployed models, as shadowmodel attacks which are several times more expen-sive to run than the training of the target modelitself become prohibitive.",
  "Broader Impact": "Advancements in membership inference attacks(MIAs) for large language models (LLMs) are im-portant for improving privacy auditing and com-pliance. By improving the efficiency of MIAs,our work helps auditors more routinely evaluatedeployed models for privacy properties (or lackthereof). By making privacy leakage more easilymeasurable, we hope our work encourages privacyto become a first-order design desiderata in large-scale machine learning.Improved MIAs of course also increase the riskof external attacks. Thus in the short run, work onprivacy attacks (including ours) can increase theprivacy risk of deployed models. Nevertheless, webelieve that in the long run exposing privacy risk isan essential step to mitigating it.",
  ". Scalable membership inference attacks viaquantile regression. Advances in Neural InformationProcessing Systems, 36": "Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling.In InternationalConference on Machine Learning, pages 23972430.PMLR. Nicholas Carlini, Steve Chien, Milad Nasr, ShuangSong, Andreas Terzis, and Florian Tramer. 2022.Membership inference attacks from first principles.In 2022 IEEE Symposium on Security and Privacy(SP), pages 18971914. IEEE. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,Katherine Lee, Florian Tramer, and Chiyuan Zhang.2023. Quantifying memorization across neural lan-guage models. In The Eleventh International Confer-ence on Learning Representations. Nicholas Carlini,Florian Tramer,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, UlfarErlingsson, et al. 2021. Extracting training data fromlarge language models. In 30th USENIX SecuritySymposium (USENIX Security 21), pages 26332650. Michael Duan, Anshuman Suri, Niloofar Mireshghallah,Sewon Min, Weijia Shi, Luke Zettlemoyer, YuliaTsvetkov, Yejin Choi, David Evans, and HannanehHajishirzi. 2024. Do membership inference attackswork on large language models?arXiv preprintarXiv:2402.07841. Cynthia Dwork, Frank McSherry, Kobbi Nissim, andAdam Smith. 2006. Calibrating noise to sensitiv-ity in private data analysis. In Theory of Cryptog-raphy: Third Theory of Cryptography Conference,TCC 2006, New York, NY, USA, March 4-7, 2006.Proceedings 3, pages 265284. Springer.",
  "Cynthia Dwork and Aaron Roth. 2014. The algorith-mic foundations of differential privacy. Foundationsand Trends in Theoretical Computer Science, 9(34):211407": "Nils Homer, Szabolcs Szelinger, Margot Redman, DavidDuggan, Waibhav Tembe, Jill Muehling, John VPearson, Dietrich A Stephan, Stanley F Nelson, andDavid W Craig. 2008. Resolving individuals con-tributing trace amounts of dna to highly complexmixtures using high-density snp genotyping microar-rays. PLoS genetics, 4(8):e1000167. Bargav Jayaraman, Lingxiao Wang, Katherine Knip-meyer, Quanquan Gu, and David Evans. 2021. Revis-iting membership inference under realistic assump-tions. Proceedings on Privacy Enhancing Technolo-gies, 2021(2). Balaji Lakshminarayanan, Alexander Pritzel, andCharles Blundell. 2017. Simple and scalable pre-dictive uncertainty estimation using deep ensembles.Advances in neural information processing systems,30. Yunhui Long, Vincent Bindschaedler, Lei Wang, DiyueBu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, andKai Chen. 2018. Understanding membership infer-ences on well-generalized learning models. arXivpreprint arXiv:1802.04889. Justus Mattern, Fatemehsadat Mireshghallah, ZhijingJin, Bernhard Schoelkopf, Mrinmaya Sachan, andTaylor Berg-Kirkpatrick. 2023. Membership infer-ence attacks against language models via neighbour-hood comparison. In Findings of the Association forComputational Linguistics: ACL 2023, pages 1133011343, Toronto, Canada. Association for Computa-tional Linguistics.",
  "Stephen Merity, Caiming Xiong, James Bradbury, andRichard Socher. 2016. Pointer sentinel mixture mod-els. In International Conference on Learning Repre-sentations": "Shashi Narayan, Shay B Cohen, and Mirella Lapata.2018. Dont give me the details, just the summary!topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 17971807. Milad Nasr,Nicholas Carlini,Jonathan Hayase,Matthew Jagielski, A Feder Cooper, Daphne Ippolito,Christopher A Choquette-Choo, Eric Wallace, Flo-rian Tramr, and Katherine Lee. 2023. Scalable ex-traction of training data from (production) languagemodels. arXiv preprint arXiv:2311.17035. Alexandre Sablayrolles, Matthijs Douze, CordeliaSchmid, Yann Ollivier, and Herv Jgou. 2019.White-box vs black-box: Bayes optimal strategies formembership inference. In International Conferenceon Machine Learning, pages 55585567. PMLR.",
  "Avi Schwarzschild,Zhili Feng,Pratyush Maini,Zachary C Lipton, and J Zico Kolter. 2024. Rethink-ing llm memorization through the lens of adversarialcompression. arXiv preprint arXiv:2404.15146": "Weijia Shi, Anirudh Ajith, Mengzhou Xia, YangsiboHuang, Daogao Liu, Terra Blevins, Danqi Chen, andLuke Zettlemoyer. 2024. Detecting pretraining datafrom large language models. In The Twelfth Interna-tional Conference on Learning Representations. Reza Shokri, Marco Stronati, Congzheng Song, and Vi-taly Shmatikov. 2017. Membership inference attacksagainst machine learning models. In 2017 IEEE sym-posium on security and privacy (SP), pages 318.IEEE. Shuai Tang, Zhiwei Steven Wu, Sergul Aydore, MichaelKearns, and Aaron Roth. 2024. Membership infer-ence attacks on diffusion models via quantile regres-sion. In International Conference on Machine Learn-ing. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Lauren Watson, Chuan Guo, Graham Cormode, andAlexandre Sablayrolles. 2021. On the importance ofdifficulty calibration in membership inference attacks.In International Conference on Learning Representa-tions. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, andSomesh Jha. 2018. Privacy risk in machine learning:Analyzing the connection to overfitting. In 2018IEEE 31st computer security foundations symposium(CSF), pages 268282. IEEE. Chiyuan Zhang, Daphne Ippolito, Katherine Lee,Matthew Jagielski, Florian Tramr, and Nicholas Car-lini. 2023. Counterfactual memorization in neurallanguage models. Advances in Neural InformationProcessing Systems, 36:3932139362. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "AComparison of Scoring Functions": "In previous sections, we have compared our methodwith baselines leveraging different scoring func-tions, now we explore the performance of differentscoring functions when per-sample based calibra-tion is applied. shows a comparison ofdifferent scoring functions and their counterpartswhere calibration is applied through LiRA and ourmethod. We observe that while min-k and zlib en-tropy typically improves over loss attack, their per-formance advantage is not necessarily maintainedwhen calibrated using LiRA or our method. Thiscan potentially be attributed to the original designintent of these scoring functions, which were de-signed to not require (as much) calibration, andtherefore benefit less from it. For instance, un-calibrated zlib entropy performs quite differentlyfrom the base loss attack, but their calibrated perfor-mances under LiRA are near identical. Calibrationusing our method has the best performance in mostcases, especially at lower false positive rate. Still,we noticed overall higher training losses on our re-gression models when using zlib entropy or min-kscore as the scoring function, which may explainthe worse performance with zlib entropy on XSum.",
  "CVarying Sizes of Regression Models": "Here we study how the size of quantile regres-sion models affects the performance of our method. shows a performance comparison of ourmethod using Pythia models of varying sizes forregression when the target model is Pythia-6.9b.We observe an improvement of true positive ratesat 0.1% and 1% FPR as regression model size in-creases. In our experiments, we observed lower",
  "Ours (70m)5.7430.5411.48 49.2430.30 65.12Ours (160m)6.9534.1719.29 58.5236.92 72.68Ours (410m)7.8440.5922.00 63.8940.38 78.24": ": True positive rates (%) at 0.1% and 1% falsepositive rates on the three datasets; the target model isPythia-6.9b. LiRA results are obtained with 4 shadowmodels. LiRA* represents LiRA with fixed variance.Results for our method are obtained using ensemblesof 5 models from finetuning Pythia models of differentsizes. We observe an improvement of true positive ratesat 0.1% and 1% FPR as regression model size in-creases. In our experiments, we observed lowerregression loss when training with larger mod-els, which may explain the improvement in per-formance.",
  "EExtended Comparison with LiRAUsing Increased Number of ShadowModels": "Here we present extended comparison with LiRAvariants using increased number of shadow models. shows a comparison of our method withLiRA on WikiText where the model family variesamong target model and attacker models. We ob-serve improved performance of LiRA with variablevariance with the increased shadow models. Ourmethod is able to achieve competitive results witha fraction of the time required for preparing the at-tacker models and consistently outperforms LiRAwhen the target model is Llama-7b.",
  "Ours (P-160m)19.29 58.5213.03 45.9811.28 56.743.4Ours (O-125m)18.03 61.1914.48 53.6917.9661.873.4Ours (P-410m)22.00 63.8913.92 54.3118.95 61.8411.4Ours (O-350m)22.5566.7414.84 56.9517.00 63.7411.4": ": True positive rates (%) at 0.1% and 1% FPRwith different target models on the WikiText datasetalong with the total time to prepare the shadow modelsor regression models. P-2.8b, -160m, -410m and O-2.7b, -125m, -350m correspond to Pythia-2.8b, -160m,-410m and OPT-2.7b, -125m, -350m, respectively. LiRAresults are obtained with 8 shadow models from thePythia and OPT families. LiRA* represents LiRA withfixed variance. Results for our method are obtainedusing ensembles of 5 regression models."
}