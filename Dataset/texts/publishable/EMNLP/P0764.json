{
  "Abstract": "We introduce a new database of cognatewords and etymons for the five main Romancelanguages (Romanian, Italian, Spanish, Por-tuguese, French), the most comprehensive oneto date with over 19,000 entries. We proposea strong benchmark for the automatic recon-struction of protowords for Romance languagesby applying a series of machine learning mod-els and features on these data. The best re-sults reach 90% accuracy in predicting the pro-toword of a given cognate set, surpassing ex-isting state-of-the-art results for this task andshowing that computational methods can bevery useful in assisting linguists with protowordreconstruction.",
  "Introduction and Related Work": "Protoword reconstruction, consisting of recreat-ing the words in a proto-language from their de-scendants in daughter languages, is central to thestudy of language evolution. As the foundationof historical linguistics (Campbell, 2013; Malloryand Adams, 2006) and the basis for linguistic phy-logeny (Atkinson et al., 2005; Alekseyenko et al.,2012; Dunn, 2015; Brown et al., 2008), protowordreconstruction offers important pieces of informa-tion concerning the geographical and chronologi-cal dimensions of ancient communities (Heggarty,2015; Mallory and Adams, 2006), at the sametime, allowing an insight into the cognitive andcultural world of our ancestors. The traditional pro-cess of reconstructing ancient languages consists ofthe \"comparative grammar-reconstruction\" method(Chambon, 2007; Buchi and Schweickard, 2014),and the etymological data thus obtained can be usedas a source on human prehistory, corroborating thearchaeological inventory (Heggarty, 2015), and pro-viding the basis for linguistic paleontology (Epps,2014). The reconstruction of a word automaticallyimplies a reconstruction of the surrounding reali-ties, both natural and socio-cultural. For example, the presence in different Indo-European languagesof obviously related words for beech or salmonallowed the reconstruction of words from Proto-Indo-European and thus information about the ele-ments of nature present in the immediate vicinity ofthe Indo-Europeans could be extracted. In the ab-sence of any clear documentary or archaeologicaldata, these lexical clues allowed the geographicalidentification of the Indo-European homeland, alsofacilitating the chronology of successive waves ofseparation of Indo-European languages from thecommon trunk.In the case of Romance languages, although themother tongue - Latin - is attested, its presence inwritten texts is not an exhaustive source for linguis-tic, social, and historical analysis of the communitythat spoke it. It is now generally accepted that thespoken language represented a different diastatic,diaphasic, and diamesic variety from written lan-guage, used by the few educated people who de-cided to express themselves in writing (Wright,2002). The Latin language that we reconstructfrom words inherited in Romance languages isthus the only concrete and reliable living varietyof the language from which Romance languagesoriginate, whether we call it oral/ vulgar Latin orProto-Romance. We will opt here for the name\"Proto-Romance\" when we refer to the languagefrom which the Romance languages originate, asthis corresponds to the concept of protolanguageand protoword (Buchi and Schweickard, 2014).Furthermore, there are still numerous clearlycognate words present in several Romance lan-guages, whose etymon is not attested in Latin (norin any other language from which it might havebeen borrowed). For example, in the case of It.trovare find, Fr. trouver, Cat. trobar, etymolo-gists have hotly debated over the decades whetherone should reconstruct the protoform *tropare or*turbare (Georgescu and Georgescu, 2020). A se-ries of cognates attested in all Romance geograph-",
  "ical areas, like Rom. nca moreover, It. anche,Old Fr. anc, Cat. anc etc., has triggered over 15etymological hypotheses over the last century, stillwithout a generally accepted solution": "Although etymologists interest in reconstruct-ing the protolanguages has risen over the years,they still encounter numerous gaps when using ex-clusively the classical, manual methods (Buchi andSchweickard, 2010, 2020). As the task of pro-toword reconstruction plays an important role inhistorical linguistics, studies have gone beyondthe comparative method in an attempt to auto-mate the process (Atkinson, 2013; Oakes, 2000;Bouchard-Ct et al., 2013; Ciobanu and Dinu,2019). However, the task has been recognizedas difficult and challenging. Computational pro-toword reconstruction is a fairly new direction ofstudy, and consequently even state of the art ap-proaches have limitations. Complete automationof the reconstruction process is still a desideratum.Oakes (2000) proposed two systems (Jakarta andPrague) that, combined, cover the steps of the com-parative method for protolanguage reconstruction,and several other approaches to reconstruct pro-towords computationally had been attempted previ-ously (Hewson, 1973; Lowe and Mazaudon, 1994;Kondrak, 2002). The work of computational biol-ogists such as Alexandre Bouchard-Ct, RussellGray, Robert McMahon, and Mark Pagel, and co-authors took the protoword reconstruction one stepfurther by applying methods from computationalbiology to the problem of the reconstruction of lan-guage history, often in collaboration with linguists(Pagel, 1999; Pagel et al., 2013; Bouchard-Ctet al., 2009; Bouchard-Ct et al., 2013). In recentyears, researchers have introduced new methods forprotoword reconstruction, based on modern compu-tational techniques (for example, CRF, transform-ers, RNN, deep learning) (Ciobanu and Dinu, 2018;Sims-Williams, 2018; Meloni et al., 2021; Fourrier,2022; List et al., 2022; He et al., 2023a; Akavarapuand Bhattacharya, 2023; Kim et al., 2023). Thecomputational methods are limited today by 1) theavailable data (sparse, inconsistent) and 2) by theinsufficiency of linguistic knowledge embedded inthe systems. The latest computational results on Romanceprotoword reconstruction, in particular, are re-ported on the database of (Meloni et al., 2021),which contains 8,799 cognates set in Latin, Ital-ian, Spanish, Portuguese, French, and Romanian (not all full cognates set). This is a revision of thedataset of (Dinu and Ciobanu, 2014) (used withvery good results in (Ciobanu and Dinu, 2018))with the addition of cognates scraped from Wik-tionary.Starting with these remarks, our main contribu-tions are:1.We introduce a comprehensive Romancedatabase for protoword reconstruction by process-ing RoBoCoP (Dinu et al., 2023), the largest Ro-mance cognate-borrowing database obtained fromelectronic dictionaries with etymological informa-tion of Romanian, Italian, Spanish, Portuguese, andFrench.2. We propose a strong benchmark for automaticprotoword reconstruction, by applying a set of ma-chine learning models (using various feature setsand architectures) on any cognate set of Romancelanguages.The rest of the paper is organized as follows:In we present the database that we havecreated and offer details about the processing stepsinvolved; in we introduce our approachfor the automatic protoword reconstruction, alongwith methodological details; the results of our pro-posed experiments are fleshed out in ;and a comprehensive error analysis is describedin . The last section is dedicated to finalremarks.",
  "Data": "A major inconvenience in Historical Linguistics ingeneral, and in computational approaches of pro-toword reconstruction in particular is the scarcityof available data. Nonetheless, in the last few years,several initiatives have been undertaken in this di-rection. (Ciobanu and Dinu, 2018) developed adatabase of Latin protowords, further expanded by(Meloni et al., 2021) with Wiktionary data. Re-cently, this dataset was extensively used for sev-eral studies (Kim et al., 2023; He et al., 2023b;Akavarapu and Bhattacharya, 2023). In 2023, Dinuet al. (2023) published the most comprehensivedatabase of Romance related words, named RoBo-Cop. It contains cognates and etymons in fiveRomance languages: Italian, Spanish, Portuguese,Romanian, and French. It has already been usedwith good results on prominent historical linguistictasks such as cognate identification (Dinu et al.,2023), cognate-borrowings discrimination (Dinuet al., 2024b), and determining the borrowing di-",
  "The ProtoRom Database": "Starting with the RoBoCoP database (Dinu et al.,2023), in order to obtain cognate sets with commonetymons in the five Romance languages, we filteredout the words with Latin etymology. We then cre-ated maximal tuples of words in the Romance lan-guages with the same etymon (< wLi >, e), whereLi are all the languages among the five where theetymon e engendered a word, and wLi are the cor-responding words in each of the languages dis-cussed. In cases where multiple words in Li derivefrom the same etymon e, we created multiple tu-ples (< wLi >, e) with all possible combinationsof cognate words < wLi > and the same etymon e.For an example of such a case see .We curated the obtained data, with the help oflinguists. In the process, we discarded sets thatcontained irrelevant or erroneous information, e.g.:erroneous lexical forms (e.g. Lat. videre see -It. vedere - Fr. voir - Ro. videa (correct: vedea);included a verb form in any mood other than theinfinitive (e.g. Lat. videre - Sp. veas (subjunctive)/ viendo (gerundive) / etc.); retained the reflexiveform of a verb (e.g. Lat. ponere put - It. porre -Sp. ponerse (poner + reflexive pronoun se), etc.);or contained words derived on Romance ground(e.g. Lat. dens tooth - It. dente - Ro. dint,os (=dinte + suff.-os), etc.).We were able to apply manual corrections forall these errors for the smaller subset of entries inthe database that have a cognate in each of the fivelanguages. For the rest of the full database Pro-toRom, we applied a semi-automatic correction bylemmatizing the cognate words, using the defaultlemmatizers1 implemented in the spaCy2 libraryfor each of the Romance languages. In all exper-",
  "1using the models, for each language L2": "iments described in the rest of the paper, we usethe lemmas of the cognates instead of the originalforms found in the dictionary.In addition to the correct series thus retained,we integrated the database created by Reinheimer-Rpeanu (2001), a high quality collection of cog-nate series manually selected from the etymologi-cal dictionaries of each Romance language, someof which still not digitized (which probably ex-plains why certain cognate sets from this collectionwere not among ones in the RoBoCoP database).We thus obtained a new database of cognate sets.The proposed database contains a total 39,973full or partial cognate sets along with their etymons.For the experiments in this paper, we focus onthe 19,222 entries with at least 2 cognates. Wechoose this subset in order to ensure the robustnessof our experiments, focusing on Latin etymons thatengendered at least two cognates in two differentlanguages, and we ignore the entries with only onecognate for a given etymon. Going further, thisrestricted dataset will be referred to as ProtoRom3.A cognate set is composed of a tuple of words indifferent languages with a common etymon, wherethe tuple can be either a full set of 5 cognates or apartial set of 2 to 4 cognates, where the cognate inone or more of the languages is missing (the Latinetymon did not produce an attested word in theselanguages according to our sources).There are 1,245 full cognate sets in the database,the rest being partial cognate sets. To facilitatedistinguish- ing between the two settings, we namethe first one ProtoRom-all5, and the second oneProtoRom. When we leave out one of the lan-guages, we can obtain more full sets of 4-tuples(sets with at least 4 cognates) as follows: 1,480 ifwe leave out Italian, 2,493 if we leave out French,1,489 when we leave out Portuguese, 1,504 whenwe leave out Spanish, and 1,946 by leaving outRomanian. The statistics detailing the number ofpartial cognate sets in all combinations are shownin .ProtoRom is the largest database of cognate setsfor Romance languages so far, significantly exceed-ing the widely used database for this task (Meloniet al., 2021), containing 8,799 cognate sets of Ro-manian, French, Italian, Spanish, Portuguese wordsand the corresponding Latin form (which, in turn,is an extension of Ciobanu and Dinu (2018)s orig-",
  "Experimental Setting": "For our experimental trials, we consider two set-tings: In the first one, we limit our dataset to onlythe full cognate sets (i.e. 5-tuples of cognates fromeach of the five languages, that originate from thesame Latin etymon), while in the second one weconsider all cognate sets (with at least two cognatesfrom different languages, per etymon, as previ-ously mentioned). The second setting uses the fullbreadth of our proposed dataset (ProtoRom-all5),whereas the first one is a strict subset (ProtoRom). Data splitting.In order to train and validate ourmodels, we split our datasets into 80% : 10% :10% train-dev-test subsets. Because of the natureof the cognate sets, generating a language-levelstratified split is a non-trivial task. Since a Latinetymon can produce more than one reflex in a givenlanguage, we end up with i max(1, nLi) cognatesets for a given etymon, where nLi is the numberof reflexes generated by that etymon in languageLi.We propose a random split methodology thatachieves the following properties: A Latin etymonand all of its cognate sets are not allowed to bepart of more than one split; the raw number ofcognate sets (i.e. entries in the dataset) follows the80 : 10 : 10 distribution; the distribution of uniqueLatin etymons is also 80 : 10 : 10; for each of thefive languages; and computing the distribution ofunique reflexes in that language yields the sameratio across the splits. In other words, if we onlykeep the Latin etymons and their reflexes in onlyone language, we obtain a monolingual task withthe same 80 : 10 : 10 split.In order to perform these splits, we construct foreach Latin etymon a 5-dimensional vector (nLi)i,using the previous definition of nLi. In order toobtain a split of ratio 0 < p < 1, we want to selectsuch vectors that, when summed together, equalp(NLi)i, where NLi is the total number of uniquereflexes from language Li. In other words, we facea task equivalent to a five-dimensional knapsackproblem, which is not feasible given the large totalcapacities. Considering that these vectors containparticularly small values, and are somewhat uni-formly distributed, plus the large capacities that we have to fill, we are able to randomly select etymonsand their associated cognate sets and add them toany of the three splits, as long as they fit. Thisapproach yields the original split distribution withsome small deviations (< 1%).Also note that after splitting the ProtoRom-all5dataset, containing only the full cognate sets, wecan use it as a starting point for splitting the rest ofthe ProtoRom dataset, thus ensuring that no train-ing examples from one setting leaks into the vali-dation of the other one. Features.The proposed approaches can be splitinto two main categories: models for reconstructingthe orthographical representation of the protowordsusing the orthographical form of modern cognates,and models that reconstruct the phonemic repre-sentation from phonetic transcriptions of moderncognates. Our extracted dataset essentially pro-vides the necessary examples for the former, whilefor the latter we employ the eSpeak4 library to au-tomatically generate the phonemic representations.",
  "Models": "We use a variety of machine learning models, in-cluding classical, neural, and transformer-based(pretrained and trained from scratch for the task).We include methods used in previous papers onthe topic and evaluate them on our larger datasetin order to provide a benchmark for the task ofprotoword reconstruction for Romance languages.We experiment with a variety of models, includ-ing pre-trained large language models (LLMs) andcurrent state-of-the-art models for protoword recon-struction with various architectures (probabilisticRNN, character-level transformer) adapted to ournew database, as well as original solutions. In thisway, we aim to provide a benchmark for the task ofprotoword reconstruction. CRF + rerankingWe used an approach that re-lies on conditional random fields (CRFs), basedon the method proposed by Ciobanu and Dinu(2018). Firstly, we applied a sequence labelingmethod that produces the form of the Latin an-cestors, for each modern language. The modernwords are the sequences, and their characters arethe tokens. We used character n-grams from theinput words as features. We employed pairwise se-quence alignment (Needleman and Wunsch, 1970)between modern words and protowords to obtain",
  "It-Ro-Es: 2,913Fr-Ro-Es: 3,503Fr-Es-Pt: 2,311Ro-Es-Pt: 2,919Pt: 5,202-Pt: 1,489": ": Number of cognate sets that are descendants from the same Latin word, for each language combination.x-y means the number of cognate sets for languages x and y; x-y-z means the number of cognate sets for languagesx,y, and z; x means how many descendants are from Latin for language x; -x means the number of cognate sets forall languages except x. the labels for each token. Secondly, we definedseveral ensemble methods to take advantage of theinformation provided by all languages, in order toimprove performance. We employed fusion meth-ods based on the ranks in the n-best lists and theprobability estimates provided by the individualclassifiers for each possible production, in order tocombine the outputs of the classifiers (n-best listof possible protowords) and to leverage informa-tion from all modern languages. For each wordin the productions list, we multiply the rank of itwith the confidence score given by the CRF modelfor each language; we sum up the multiplicationscores for each word in the list and then rerank theproductions based on these results. Probabilistic LSTMWe conducted experimentsusing a combination of recurrent neural networkswith different dynamic programs and expectation-maximization techniques, as described in He et al.2023b. The overall system can be split in twostages: a) a modelling stage, where we model theevolution of words by making small character-leveledits to the ancestral form; for each language in thestudy, the distribution over newly created words iscomputed; b) an expectation-maximization stage,where the ancestral form is inferred; using wordssampled from the posterior distribution, the ex-pected edit count is computed and further usedby the character-level recurrent neural network inorder to optimize the next round of samples; the fi-nal reconstruction is the maximum likelihood wordforms. This model requires a full tuple of cog-nates to be passed as input, so we only computeresults for experiments on the ProtoRom-all5 set.Like the original authors, we only apply this modelon the phonemic forms of words, since the prob-ability distributions of edit operations used in thealgorithm rely on a set of manually set features foreach phoneme that are not similarly available fororthographical characters. Character-level transformerThe next experi-ments conducted in this research are based on thetransformer model, proposed by Kim et al. 2023.Some critical changes in the architecture weremade in order to be able to accept our samplesformat: multiple modern word sequences (one foreach language) correspond to a single protoformsequence. A positional encoding is applied to eachindividual modern word sequence before concate-nation. An additive language embedding is appliedto the token embeddings alongside the positionalencoding in order to make a difference betweeninput tokens of different languages. Pre-trained LLM (Flan-T5)We finally evalu-ate the capabilities of pretrained Large LanguageModels (LLMs) to solve our task. While LLMsare currently obtaining state-of-the-art performanceacross NLP tasks, our specific goal is unlike usualtasks included in benchmarks or in training datafor LLMs, and it is strongly multilingual (includ-ing one dead language), so we suspect it might bea difficult task for an LLM. We choose to use apretrained model and fine-tune it on our own train-ing data in order to increase its chances to performwell. We use a \"base\" variant of the Flan-T5 model(Chung et al., 2024), and fine-tune the model us-ing instructions including the prompt: \"What is theetymon given the following cognates:\", followedby a list of cognate and language pairs formattedas \"< Li >: < wi >\" and separated by new lines,where the list of cognate words wi is their respec-tive languages Li can be arbitrarily long (from 2to 5 cognates, in the case of our experiments). Forevaluation, we attempt to generate multiple out-put sequences, which are used as a ranking for theetymon prediction.One limitation of pretrained LLMs that we can-not overcome through fine-tuning is its alphabet,which contains mostly characters in the Latingraphical alphabet, which means that we can only use this model with othographical features. Us-ing phonemic features would require retraining themodel from scratch and we would lose the benefitof pertaining which is usually the strong point ofLLMs.",
  "Results": "The previously described methods have beenapplied on both ProtoRom and ProtoRom-all5datasets, using the orthographical form of the cog-nates and Latin etymon, or alternatively the auto-generated phonemic representations (where themodels were able to accommodate them).We also provide a comprehensive human evalu-ation of the results. Linguists from our team man-ually analyzed the entire list of results, and wepresent the most significant observations regard-ing the models successes and failures. The lin-guists did not correct the protoforms proposed bythe models, but only evaluated and commented onthem in relation to current knowledge in the fieldof historical linguistics.The metrics used include accuracy, (normalized)edit distance, and Covi, with i {1, 5, 10}, whichstands for an extended version of the accuracy met-ric, where a correct prediction is one where themodel found the correct etymon within the firsti etymons predicted by our method (this metricis computed for models that are able to output aranked list of predictions - Flan-T5 and CRF-basedmodels).",
  "ProtoRom-all5 Results": "Results obtained on the ProtoRom-all5 set areshown in . In terms of accuracy (or Cov1),the best results are obtained using the orthograph-ical forms, with the CRF-rerank model, reaching60.4%. From the perspective of the Covi metrics,it is remarkable that the CRF-rerank model obtainsa Cov10 score above 82%.The experiments using the phonemic forms pro-duce weaker results, with the best accuracy reach-ing 55.8% in the top 1 predictions scenario. Nev-ertheless, the CRF approach is able to achieve anaccurracy close to 80% when we consider the top10 best ranked predictions.The probabilistic RNN models achieve very poorperformances, reaching a mean edit distance of3.11 when trained on the phonemic representations.",
  "ProtoRom Results": "The best accuracy when training the orthographicalmodels is achieved in this scenario by the Trans-former model, closely surpassing 73% ().As for the Covi metrics, the Flan model remark-ably obtains a Cov10 accuracy score of 85.4%, andan edit distance of 0.23.Similarly to the previous scenario, the experi-ments using the phonemic forms produce weakerresults, with the best accuracy reaching 66.8% viathe Transformer model. These results represent acollection of baselines for protoword reconstruc-tion using our proposed dataset configurations.We believe the higher accuracy observed on thefull dataset is simply due to the larger amount ofavailable data. While ProtoRom-all5 is a subsetthat contains only complete cognate sets from eachof the five studied languages (totaling 1,245 sets)the ProtoRom dataset includes sets of two, three, orfour cognates, resulting in significantly more sets(19,222). This larger dataset allowed the modelsto learn more phonetic correspondences, therebyimproving the reconstruction process. Even thoughthey are not full sets of five cognates, the additionalcognate sets in the full database seem to help themodels learn more about their protowords. Thislearning process is closely similar to the humanmethod of learning: with more examples, linguistscan be more certain of particular correspondencesor phonetic changes and can apply them in thereconstruction with much greater confidence.",
  "Error analysis": "This section is dedicated to a deeper dive into qual-itatively quantifying the errors produced by thepreviously proposed models. Our objective is sepa-rating purely wrong predictions from \"near misses\",which may still provide value for linguists for thereasons discussed below.The error analysis was manually conducted bythe linguists from our team, who specialize in Ro-mance languages. They did not modify the proto-forms provided by the models in any way. Theironly intervention was to distinguish forms thatwere genuinely erroneous from those whose differ-ences from the dictionary form were either insignif-icant or represented a correct adjustment to thereality of Latin pronunciation. In the final quantita-tive analysis, forms in this category were thereforeincluded in the list of correct predictions withoutany changes to their structure.",
  "Transformer470.98/0.16": ": Reported results for protoword reconstruction on the ProtoRom-all5 dataset via orthographical representa-tions (Gr) and via phonemic representations (Ph), respectively. We report the reconstruction accuracy along with themean edit distance (Edit) and mean normalized edit distance (NEdit). The Covi values for the edit distances arecomputed by selecting the minimum distance between the true etymon and the top i predictions, then averaging overthese minima for all of the test examples. For the Flan and CRF models, we look at the top 1, 5, and 10 predictionswhen computing these metrics.",
  ": Similar to we report the same evaluations when using the complete ProtoRom dataset": "Through analyzing the errors, we have identifiedsome patterns that typically reflect either an insuf-ficient number of examples to support a particularphonetic change or the irregularity of the changeitself. For example, the short tonic /u/ developsinto Spanish /o/ in half of the cases, while it re-mains /u/ in the other half. In such scenarios, themodel may not know which phonetic treatment thecognates underwent and might choose the wrongvariant. Similarly, in cases of phonetic accidents,which are by nature irregular and unpredictable, themodel cannot reconstruct the pre-accident form. In-stead, it reconstructs the intermediate form betweenthe classical word and its Romance descendants.Identifying and systematizing these errors can helpimprove future results by broadening the input withinformation related to sound changes. Before analysing the errors, a few preliminarypoints should be made. Romance lexicography asa whole is graphocentric - it considers the written,classical Latin (CL) lexical variants as the basisfor the Romance vocabulary, even though it goeswithout saying that vernacular languages, oral parexcellence, developed from an oral language, in ourcase Proto-Romance (PR) (Chambon, 2007). In thelatest methodology used in Romance etymology,developed within the DRom project (Buchi andSchweickard, 2014), the etymological identifica-tion is based strictly on the comparative grammar - reconstruction method, starting from the lexicalforms that were used uninterruptedly in Romancelanguages. The lexemes attested in Classical Latinare only a written correlate, possibly further evi-dence of the existence of the form obtained by themethods of comparative historical linguistics.In the light of these considerations, we find thatsome of the reconstructed variants classified as er-rors should actually be considered as positive re-sults and evidence that the machine could workat the same level as a linguist applying traditionalmethods. By positive results instead of errors wemean cases - not a few - where the machine recon-structed exactly the phonetic form valid for oralLatin, at the expenses of the standard orthographi-cal form as it is lemmatized in classical Latin dic-tionaries.Cases where the word obtained and the one givenby the dictionary did not completely match wereautomatically considered as errors, although some-times it was not a mistake as such. Therefore, thereare a number of protoforms which, although theyappear in the list as inadvertences, are variants thatshould be taken into account with full attention bylinguists. Some are no more wrong than the formin the dictionary, some are closer to the actual oralform than those provided by lexicographers, whilesome are exactly the form that historical linguistswould have reconstructed using traditional meth- ods based on the sound laws of each language (wediscuss each case below). Therefore, protoformsobtained by the automatic methods proposed hereare sometimes preferable to the lemmatized ones,and this is the most important thing we can expectfrom the machine.Below, we provide a list of situations categorizedas errors, but where the the automatic protoword re-construction is either comparable or better than theversion proposed by the dictionary, as it representsexactly the linguistic variant we should consider asintermediate between classical Latin and Romancelanguages. Protowords ending in -um instead of standard-us (lupum instead of lupus). The differencebetween the endings -us / -um did not properlyexist in Proto-Romance, as the final consonant-s/-m was no longer pronounced. Thus, if theetymological dictionaries provide the classi-cal nominative form lupus as an etymon forRo. lup, It. lupo, Fr. loup etc., but the com-puter reconstructs lupum this latter variantis more correct from a grammatical point ofview, since in general nouns are inherited fromthe accusative form (in our case ending in -um) and not from the nominative (ending in-us). Moreover, if it reconstructs lupu, thisform is even more correct, being the real one,that reflects the pronunciation in the spokenlanguage. The automatically reconstructed protoformsreflect phonetic features specific to Proto-Romance: monophthongation (au > o, e.g.CL auca vs PR oca; > e, e.g. pna vspena; > e, e.g. hsitare vs esitare); re-duction of geminate consonants (addictus vsadictum); loss of the initial or intervocalic /h/(hsitare vs esitare; cohrente vs coerente);phonetic adaptation of Greek loanwords to theLatin pronunciation (y > i, e.g. CL byzanti-nus vs PR bizantinus, the aspirate consonantsbecome oclusive, th > t (CL citharoedu vsPR citaredu), ph > f (CL phalange vs PRfalange); assimilations (CL admonere vs PRammonire); simplification of consonant clus-ters (CL sculptore vs PR scultore, temptarevs tentare, unctura vs untura); changes in thepronunciation of vowels (CL guttu vs PR gotu,misculare vs mescolare, siccare vs sec(c)are,occidere vs ucidere, calcea vs calcia).",
  "Certain reconstructed etyma retain accidental": "phonetic changes that must be presupposed fora particular geolinguistic area (Sp. queso, Pt.queixo imply the metathesis PR caesu insteadof CL caseu, Ro plop, It. pioppo, Sp. chopolead to the protoform with metathesis plopu,correctly identified by the machine, instead ofCL populus), or for the global PR variety (Ro.doamna, It. donna, Sp. doa, lead to the syn-copated protoform domna, reconstructed bythe machine, instead of CL domina, registeredin lexicography). The automatically reconstructed protoformsmay mirror morphologic changes that under-lie the subsequent Romance developments:nouns of the 5th declension undergo a shift tothe 1st declension (CL canities vs PR canitia,species vs specia); verbs shifting from middle-passive to the active voice (CL renasci vs PRrenascere). The computer has reconstructed the obliquecase forms representing the basis from whichthe Romance nouns were inherited (nomina-tive flos vs oblique case flore- > Ro. floare, It.fiore, Fr. fleur, etc.; civitas vs civitate > Ro.cetate, Sp. ciudad, etc.), or the plural insteadof the singular form, when the Romance lex-emes descend from the former (sg. capitiumvs pl. capitia > Sp. cabeza, Pt. cabea). The real errors in the experiments we developedstem primarily from lexicographic omissions ormistakes, as well as in the imprecise methodol-ogy employed by the Ibero-Romance dictionariesconsulted, namely the lack of any distinction be-tween inherited and borrowed Latin words (Buchiand Dworkin, 2019). This latter inaccuracy leadsto a misinterpretation of the phonetic correspon-dences by the computer, given that only the in-herited words, not the borrowed ones, underwentregular sound change. Therefore, if we put togetherRo. roata, Sp. rueda, Pt. roda, with Ro. rotat,ie, Sp.rotacion, Pt. rotao, the computer will not be ableto correctly infer the correspondence t/d/d andwill confuse it with t/t/t, also assuming the seriesd/d/d. Therefore, some reconstructions, especiallyin the case of words circumscribed only to Ibero-Romance languages, could not take this sound lawinto account (e.g., on the basis of Sp. miedo, Pt.medo, the computer could not reconstruct metus,but proposed medus, which is wrong). This kindof shortcomings will be easily overcome in the future, firstly by clearly establishing, in the Pro-toRom database, the inheritance-borrowing distinc-tion, and secondly by extending the input providedto the computer with a number of basic phoneticlaws. Revised performance scores.Looking at thebest reported predictions, we can apply the lin-guistic observations stated in the previous sectionand count which wrong predictions can be actuallyconsidered acceptable errors. Thus using these re-covered predictions, the best models scores wouldchange as follows:",
  "Conclusion": "In this paper, we built a new dataset for automaticprotoword reconstruction, consisting of 19, 222cognate sets from five Romance languages (Roma-nian, Italian, Spanish, Portuguese, French). This isto date the largest database of its kind, surpassingits predecessor which totals 8, 799 cognate sets.We also proposed a series of comprehen-sive benchmarks ranging from deep-learning ap-proaches, using LLMs and Transformer-based ar-chitectures, to more classical algorithms such asCRFs, some of which achieved performances ofmore than 85% accuracy when allowing multiplegenerated reconstructions.An in-depth linguistic analysis of the erroneousreconstructions was also performed using the pre-dictions of the best performing models. This at-tempt shed some light on the various categories ofmistakes, out of which several could be consideredacceptable. When ignoring the aforementionedacceptable errors, we were able to surpass 90%accuracies. We consider this an important distinc-tion, since in our view similar tools should aimat assisting linguists in their scientific endeavours.Raw metrics are useful to compare computational methods, but, in order to assess their usability, amore qualitative inspection of the results shouldbe performed. We hope through our research toincentivize further analysis.As for future work, we are looking into an addi-tional refinement of the current cognate sets, butalso extending the database with more examples,including properly validated monolingual Latin re-flexes that were excluded from our experiments forrobustness sake. We also intend to expand past theproposed benchmarks with more novel approaches,relying on both the proposed dataset and the addi-tional contents of its parent database, RoBoCoP.",
  "Limitations": "One limitation of the current work stems fromthe automatic generation of the phonetic repre-sentations via a third-party library (eSpeak). Al-though this approach was employed successfullyin previous studies, the quality of the generatedphonemes has a higher variance when comparinghigh-resourced languages to lower-resourced ones(such as Romanian, or even Latin).Also, in this study we used the generated pho-netic forms without any extra preprocessing steps,in order to have a representation of the pronunci-ation that is as accurate as possible. Removingphonetic markers (such as stress markers) fromthese representations may turn the generation taskinto a somewhat easier one, since currently the pho-netic models are tasked with predicting the stressedsounds too.In terms of resources, existing LLMs are mostlytargeting orthographical texts, making any reason-able attempt at generating phonetic ones very diffi-cult.",
  "We want to thank the reviewers for their usefulsuggestions and Diana Grigore, Cosmin Petrescu,Ioana Pintilie for their help in developing the algo-rithms": "V. S. D. S. Mahesh Akavarapu and Arnab Bhattacharya.2023. Cognate transformer for automated phonologi-cal reconstruction and cognate reflex prediction. InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 68526862. Association for Computational Linguistics. Alexander V. Alekseyenko, Quentin D. Atkinson,Remco Bouckaert, Alexei J. Drummond, MichaelDunn, Russell D. Gray, Simon J. Greenhill, PhilippeLemey, and Marc A. Suchard. 2012. Mapping theorigins and expansion of the Indo-European languagefamily. Science, 337:957960. Quentin Atkinson, Geoff Nicholls, David Welch, andRussell Gray. 2005. From words to dates: waterinto wine, mathemagic or phylogenetic inference?Transactions of the Philological Society, 103(2):193219.",
  "Quentin D Atkinson. 2013.The descent of words.Proceedings of the National Academy of Sciences,110(11):41594160": "Alexandre Bouchard-Ct, Thomas L. Griffiths, andDan Klein. 2009. Improved reconstruction of pro-tolanguage word forms. In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (NAACL 2009),pages 6573, Boulder, Colorado. Association forComputational Linguistics. Alexandre Bouchard-Ct, David Hall, Thomas L. Grif-fiths, and Dan Klein. 2013. Automated reconstruc-tion of ancient languages using probabilistic mod-els of sound change. Proc. Natl. Acad. Sci. USA,110(11):42244229. Cecil H Brown, Eric W Holman, Sren Wichmann, andViveka Velupillai. 2008. Automated classification ofthe worlds languages: a description of the methodand preliminary results. Language Typology andUniversals, 61(4):285308.",
  "Jean-Pierre Chambon. 2007. Remarques sur la gram-maire compare-reconstruction en linguistique ro-mane (situation, perspectives). Mmoires de la So-cit de linguistique de Paris, 15:5772": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Alina Maria Ciobanu and Liviu P. Dinu. 2018. Ab ini-tio: Automatic Latin proto-word reconstruction. InProceedings of the 27th International Conference onComputational Linguistics (COLING 2018), pages16041614, Santa Fe, New Mexico, USA. Associa-tion for Computational Linguistics.",
  "Alina Maria Ciobanu and Liviu P. Dinu. 2019. Auto-matic identification and production of related wordsfor historical linguistics. Computational Linguistics,45(4):667704": "Liviu P. Dinu and Alina Maria Ciobanu. 2014. Buildinga dataset of multilingual cognates for the Romanianlexicon. In Proceedings of the Ninth InternationalConference on Language Resources and Evaluation,LREC 2014, Reykjavik, Iceland, May 26-31, 2014,pages 10381043. European Language ResourcesAssociation (ELRA). Liviu P. Dinu, Ana Sabina Uban, Alina MariaCristea, Anca Dinu, Ioan-Bogdan Iordache, SimonaGeorgescu, and Laurentiu Zoicas. 2023. Robocop:A comprehensive Romance borrowing cognate pack-age and benchmark for multilingual cognate iden-tification. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 76107629. Association for ComputationalLinguistics. Liviu P. Dinu, Ana Sabina Uban, Anca Dinu, Ioan-Bogdan Iordache, Simona Georgescu, and LaurentiuZoicas. 2024a. It takes two to borrow: a donor and arecipient. whos who? In Findings of the Associationfor Computational Linguistics, ACL 2024, Bangkok,Thailand and virtual meeting, August 11-16, 2024,pages 60236035. Association for ComputationalLinguistics. Liviu P. Dinu, Ana Sabina Uban, Ioan-Bogdan Iordache,Alina Maria Cristea, Simona Georgescu, and Lauren-tiu Zoicas. 2024b. Pater incertus? there is a solution:Automatic discrimination between cognates and bor-rowings for Romance languages. In Proceedings ofthe 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Evalu-ation (LREC-COLING 2024), pages 1265712667,Torino, Italia. ELRA and ICCL.",
  "Paul Heggarty. 2015. Prehistory through language andarchaeology. In The Routledge Handbook of Histori-cal Linguistics, pages 598626. Routledge": "John Hewson. 1973. Reconstructing prehistoric lan-guages on the computer: The triumph of the elec-tronic neogrammarian. In COLING 1973 Volume 1:Computational And Mathematical Linguistics: Pro-ceedings of the International Conference on Compu-tational Linguistics. Young Min Kim, Kalvin Chang, Chenxuan Cui, andDavid R. Mortensen. 2023. Transformed protoformreconstruction. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 2438,Toronto, Canada. Association for Computational Lin-guistics."
}