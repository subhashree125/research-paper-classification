{
  "Abstract": "Vision-language models (VLMs) like CLIPhave demonstrated remarkable applicabilityacross a variety of downstream tasks, includingzero-shot image classification. Recently, theuse of prompts or adapters for efficient transferlearning (ETL) has gained significant attentionfor effectively adapting to downstream tasks.However, previous studies have overlooked thechallenge of varying transfer difficulty of down-stream tasks. In this paper, we empirically an-alyze how each ETL method behaves with re-spect to transfer difficulty. Our observationsindicate that utilizing vision prompts and textadapters is crucial for adaptability and general-izability in domains with high difficulty. Also,by applying an adaptive ensemble approachthat integrates task-adapted VLMs with pre-trained VLMs and strategically leverages moregeneral knowledge in low-difficulty and lessin high-difficulty domains, we consistently en-hance performance across both types of do-mains. Based on these observations, we pro-pose an adaptive ensemble method that com-bines visual prompts and text adapters withpre-trained VLMs, tailored by transfer diffi-culty, to achieve optimal performance for anytarget domain. Upon experimenting with ex-tensive benchmarks, our method consistentlyoutperforms all baselines, particularly on un-seen tasks, demonstrating its effectiveness.",
  "Introduction": "Vision-languagemodels(VLMs),suchasCLIP (Radford et al., 2021) and ALIGN (Jia et al.,2021), have demonstrated remarkable applicabilityacross various downstream tasks such as imageclassification. A distinctive feature of these VLMsfor image classification is their ability to classifyunseen classes that have not been encounteredduring pre-training through zero-shot inference,which is not possible to traditional vision models.",
  "equal contribution": "The primary challenge of VLMs for downstreamtasks is to excel in classifying both seen and un-seen class sets. In the context of VLM classifica-tion tasks, the ability to accurately classify seenclass sets is termed adaptability, while the capa-bility to extend this proficiency to unseen classsets is referred to as generalizability. To boostthese abilities, recent research has introduced effi-cient transfer learning (ETL) methods to fine-tuneVLMs. One strategy involves the use of soft prompttuning (Zhou et al., 2022b,a; khattak et al., 2023;Khattak et al., 2023). Another research direction in-volves adapter-style tuning (Gao et al., 2023; Zhanget al., 2022; Zhu et al., 2023b) either by adjust-ing specific parameters or employing cache-basedtechniques. These approaches empower VLMs toswiftly adapt to new tasks using only a few samples(i.e. few-shot image classification task).However, previous approaches do not considera significant factor for adapting to downstreamtasks: varying transfer difficulty (Yu et al., 2023).This refers to the challenge of adapting pre-trainedVLMs according to the target domain. For instance,transferring pre-trained VLMs to specific fine-grained domains, such as FGVC Aircraft, is morechallenging than transferring to general coarse-grained domains. In a real-world scenario, it is hardto predict the specific target task and domain thatwill emerge. Therefore, without investigating howeach type of ETL behaves in response to differentlevels of transfer difficulty and applying an adap-tive method based on this investigation, the resultfor each target domain can be suboptimal. Someworks manually train models differently for eachdataset (Gao et al., 2023; Zhang et al., 2022), butthis approach is not feasible in real-world scenariosas prior knowledge for the target task is not given.",
  "or": ": Overview of APEX compared to the conventional ETL methods. APEX exhibits two key differences: (a):Firstly, APEX integrates prompt tuning for the visual encoder and a linear adapter for the text encoder, each tailoredto the specific properties of their respective modalities, which performs better on high-difficulty domains. (b):Secondly, APEX integrates an adaptive coefficient within the text encoder to strategically balance pre-adapter andpost-adapter features to properly combine task-specific knowledge and general VLMs knowledge based on transferdifficulty. A detailed explanation, including notations and the algorithm, can be found in and Appendix B. for each modality on multiple domains with vary-ing transfer difficulty, revealing four key findings.Firstly, we find that visual prompt tuning (VPT)generalizes better to unseen classes compared totext prompt tuning (TPT) in cases of high-difficultydomains, as TPT tends to overfit on base classes forthese domains. ( Obs. 1). This occurs because, inhigh-difficulty domains, the class separability of vi-sual features from a visual encoder is low, causingTPT to overly adapt in classifying these challeng-ing features ( Obs. 2). Moreover, text adapter (TA)can significantly boost the adaptability of VPT, re-sulting in high adaptability and generalizability,especially for highly difficult domains ( Obs. 3).However, fine-tuning with adapters could compro-mise generalizability in easier domains. Our lastobservation is that combining pre- and post-adapterfeatures to leverage pre-trained VLMs knowledgecan address this concern with a proper balance be-tween them. For instance, using more pre-adapterfeatures can maintain generalizability in easier do-mains. The ideal balance depends on the domainsdifficulty, highlighting the need to adjust the en-semble coefficient accordingly ( Obs. 4). Based on our observations, we present aAPEX (text Adapter, visual Prompt, and adaptiveEnsemble for cross(X)-modality) that utilizes anadaptive ensemble with VPT and TA. Specifically,we use the combination of VPT and TA, whichhave shown high generalizability and adaptabilityfor high-difficulty domains, as shown in Obs. 1-3((a)). Also, motivated by Obs. 4, we employan adaptive ensemble approach that determines the optimal ensemble coefficient for each domain byusing the distances to learned classes in pre-trainedVLMs to estimate transfer difficulty ((b)).This adaptive ensemble controls the level of adapta-tion, by primarily utilizing task-specific knowledgewith adapted VLMs for high-difficulty domains butleveraging general knowledge for low-difficultydomains, as pre-trained VLMs already possess suf-ficient ability and prevent an overfitting from ex-cessive adaptation. With this, our method acts asa difficulty-agnostic solution, enabling the modelto effectively adapt to all target domains regard-less of transfer difficulty. In summary, our maincontributions are: We investigate prompt tuning and adapter tuningmethods to understand their effectiveness acrossdomains with varying transfer difficulties. Ourfindings reveal that the efficacy of each methodwith each modality varies across different oftransfer difficulty, with notable performance ofVPT and TA for high-difficulty domains. We propose APEX, which utilizes VPT and TAfor tuning and employ an adaptive ensemble ap-proach to optimally leverage the general knowl-edge of VLMs for each domain. The ensemblescoefficient is adaptively determined by the dis-tances to learned classes, serving as an estimateof transfer difficulty.",
  "Here, we provide a brief overview of the back-ground related to our method. For a detailed expla-nation with more related works is in Appendix E": "Zero-shot CLIP.CLIP (Radford et al., 2021) isdesigned for creating visual features based on nat-ural language guidance. The CLIP model can per-form zero-shot inference, classifying an image intoone of C possible classes without additional train-ing. This is achieved by calculating the cosine simi-larity between an visual feature z, derived from thevisual encoder, and the text features of each class{ti}Ci=1, which are obtained from the text encoder.For processing the image, let us define the visualencoder as V, which comprises LV layers, denotedas {Vi}LVi=1. The encoder takes patch embeddingsE0 RMdv as input, which are obtained by di-viding the image I into M fixed-size patches. Patchembeddings Ei is then fed into the (i + 1)th trans-former block (Vi+1) along with a learnable class([CLS]) tokens ci. This process is sequentially car-ried out through all LV transformer blocks, formu-lated as follows:",
  "Pr(y = i|z, t) =exp (sim(z, ti)/)Cj=1 exp (sim(z, tj)/), (5)": "where sim(, ) indicates cosine similarity and is the learned temperature of CLIP. We can alsointerpret the text features as a classifier (Gao et al.,2023; Zhang et al., 2022), where ti is the classifierweight for class i.Prompt Tuning for CLIP.To enable prompttuning (Zhou et al., 2022a; khattak et al., 2023;Zhu et al., 2023a; Khattak et al., 2023), we re-place the Eq. (1) and Eq. (3) by newly introducingbV and bT learnable tokens { P ki Rdv}bVk=1 and",
  "[Pj,Wj]=Tj([Pj1,Wj1]) j =JT +1,. . . ,LT": "Here, we train the visual and text prompt for thefirst JV and JT layers of corresponding encoders.Adapter-style Tuning for CLIP.To enableadapter-style tuning, we replace Eq. (2) and Eq. (4)by introducing ImgAdapt and TxtAdapt whichare shallow stacking networks upon the frozenCLIP model (Gao et al., 2023; Zhang et al., 2022;Zhu et al., 2023b).",
  "z = ImgProj(cLV), z = ImgAdapt(z) (8)t = TxtProj(wNLT ), t = TxtAdapt(t) (9)": "3Motivating ObservationsHere, we analyze the behavior of visual and textencoders depending on different tuning methodsand transfer difficulty of target domains within theframework of ETL. To accomplish this, we beginby categorizing domains based on their relativetransfer difficulty (RTD), which is a metric firstdefined by Yu et al. (2023). Definition 1 (Relative Transfer Difficulty (Yu et al.,2023)). Let f() and g() be random classifierswhere the precision of each equals 1/C, and zero-shot CLIP, respectively. Also, Precf and Precg de-note the precision of classifiers f and g. Then, RTDis formulated as follows:",
  "C Precg": "Under this metric, we identify EuroSAT, DTD, andFGVC Aircraft as the three most challenging do-mains, while ImageNet, SUN397, and StanfordCars are recognized as the three easiest domains.We will primarily focus on these six domains toclearly demonstrate the impact of RTD on VLMsbehavior. To assess adaptability and generalizabil-ity, we train the CLIP-B/16 utilizing each prompt",
  "Observation 2. Low class separability of visualfeatures is the primary reason for the overfitting ofTPT on high RTD": "Class separability is a critical factor in deter-mining the transferability of a source model to atarget domain (Pndy et al., 2022). To determinethe class separability of visual features, we use theratio of intra- to inter-class cosine similarities (Ohet al., 2021; Zhu et al., 2023b). demonstratesthat the ratio is higher in domains with lower RTD,which are considered easier, and lower in morechallenging datasets with higher RTD. These find-ings suggest that the class separability highly cor-relates with transfer difficulty, strongly influencingthe overfitting risk of TPT on high RTD domains.To see how class separability affects TPT, wefurther explore the visual features and predictions",
  ": Comparison of the combined effectiveness ofprompt tuning and adapter-style tuning. Easy\" refersto three domains with low RTD, and Challenge\" refersto three domains with high RTD": "of zero-shot CLIP and TPT. As shown in ,EuroSAT, which exhibits a high RTD, shows lowerclass separability compared to SUN397 that has alower RTD. Furthermore, in EuroSAT, when TPTattempts to classify visual features with low classseparability, its performance for novel classes islower than zero-shot CLIP. This is because TPTtries to fit the decision boundary, represented as dot-ted lines, to features that are challenging to classifyby solely adjusting classifier weights with multiplestacks of learnable prompts. This underscore thesignificance of separable visual features, a factorclosely linked to VPT. Consequently, this leads tosignificant overfitting, where the decision boundaryof one class overlaps with others. Conversely, withvisual features that exhibit high class separability,TPTs predictions are more accurate than those ofzero-shot CLIP as it can easily determine the bet-ter decision boundary.These results underscore thesignificance of separable visual features, a factorclosely linked to VPT.",
  "Observation 3. TA effectively enhances adaptabil-ity with a low risk of overfitting when employedwith VPT, especially on higher RTD datasets": "shows that while TA and VPT each ex-hibit less adaptability than TPT alone, together theyoutperform across all categories, signifying bothhigh adaptability and generalizability. This advan-tageous combination is particularly significant forhigher RTD, while the performance improvementin novel categories with lower RTD is marginal.This synergy occurs because VPT enhances theclass separability in visual features, allowing thelinear transformation of classifier weights to suf-fice for adaptation, as depicted in . TA simplymodifies the features of the pre-trained text encoder,preventing overconfidence in the decision bound-ary, especially for domains with high RTD and lowclass separability. In addition, we conduct exper-iments using a combination of TPT and a visual",
  "adapter (VA). However, this combination provesless effective than integrating VPT and TA, fur-ther emphasizing the importance of visual featureseparability": "Observation 4. By modulating the influence ofTA through an ensemble of pre-adapter and post-adapter features, each with a domain-specific coef-ficient, we can significantly improve generalizationin low RTD domains while maintaining high per-formance in high RTD domains. While combining VPT and TA has great synergyin high RTD domains, utilizing TA can result in theloss of some general knowledge from the originalCLIP, which is crucial for domains with low RTD.This is evident in Tab. 1, as navely using VPT andTA together may lead to a degradation in perfor-mance on novel classes in domains with low RTD.This is because for low RTD, a lot of tasks withinthe domain need to lie in the region of generalknowledge, as illustrated in (b). But the train-ing of a TA creates a task-specific boundary whichmay not be optimal for other tasks within the samedomain. In domains with high RTD, task-specificknowledge gained from adapters can also enhanceperformance on unseen tasks, as the general knowl-edge is often insufficient for these domains.This degradation in domains with low RTD canbe mitigated by diminishing the influence of TA. : Comparison of accuracy (%) on novel classesbetween zero-shot CLIP, without an ensemble, an en-semble with fixed coefficient, and an ensemble withoptimal coefficient. We determine the fixed coefficientas 0.4, based on average novel performance.",
  "t = TxtAdapt(t) + (1 ) t.(10)": "As Tab. 1 illustrates, the ensemble method im-proves performance in domains with low RTD.However, using pre-adapter features can yield sub-optimal outcomes in more challenging domains.For instance, performance on EuroSAT drops from77.73% to 75.87% when is set as a fixed coef-ficient, as domains with high RTD demand morefrom TA. By optimally setting for each domain,we consistently outperform zero-shot CLIP acrossall domains by effectively combining general andtask-specific knowledge tailored to each domainsneeds. Observing this optimal coefficient, we notethat that more challenging domains typically re-quire a higher coefficient. These findings highlightthe necessity of a method to calculate an adaptivecoefficient of ensemble, which would modulate TAactivation according to domain and its RTD.",
  ": The relationship between class distance andoptimal for each domain used in Eq. (10) and": "following the pre-trained text encoder. While ex-isting adapter-style methods (Zhang et al., 2022;Zhu et al., 2023b; Gao et al., 2023) rely on manu-ally optimized text prompts for different datasets,we use learnable text prompts just for the inputbecause manually creating prompt templates foreach domain in the real world is challenging. Thelearnable text prompts are unnecessary if manualprompts are already well-formed, which is furtherexplained in .We extract the visual feature z using Eq. (6)and Eq. (2) and the text feature t using Eq. (7)with JT=1 and Eq. (9). We apply linearadapter parameterized as matrix A and bias b forTextAdapter in Eq. (9) rather than using bottle-neck structure (Zhang et al., 2022; Gao et al., 2023)based on our results in . Our adapter can beformulated as follows:",
  ": A concept figure for calculating the adaptivecoefficient eval for ensemble upon its class distance": "where eval is the ensemble coefficient for a targetclass at evaluation and teval is the final represen-tation for that class. With this ensemble approach,for domains with high RTD, the model relies on theadaptability and generalizability of VPT and TA.Conversely, for domains with low RTD, it leveragesgeneral knowledge from the pre-trained model toavoid excessive adaptation. To determine the optimal eval for each class,which estimates transfer difficulty and acts as a con-troller for adaptation, we employ a non-parametricmethod based on the distance between the text fea-tures of the evaluation class and the classes learnedduring training. This approach is based on the as-sumption that in domains with high RTD, classfeatures are typically less separable in the text em-bedding space, similarly to their separability in theimage embedding space. Hence, domains like Eu-roSAT exhibit low class distances, while those withlow RTD, such as Stanford Cars, display high classdistances. shows that the optimal , used inEq. (10) and Tab. 1, is highly correlated with thedistance between class features. This tendency sug-gests that eval based on the distance between classfeatures can effectively represent transfer difficulty. Moreover, instead of applying a single eval forall classes, we adopt a class-wise approach. Thisis because, within the same domain, target featuresconsidered as out-of-task should rely more on thegeneral knowledge of pre-trained VLMs, whereasfeatures closer to the learned classes should lever-age more task-specific knowledge. With regard tothis, we adaptively set eval by comparing the textfeature of the evaluation class with the features ofthe learned classes, as illustrated in . Specif-ically, we calculate both the average and nearestdistances between the evaluation class and the C",
  "eval = exp (davgeval) 1(dnneval>),": "where is a scaling factor. The equation indi-cates a preference for pre-adapter features whenthe text feature distance from learned classes islarge, and for trained TA when it is small. Thecondition of dnneval > , where is a small valueset at 0.05, serves to treat an evaluation class thatis very similar to the base class as identical. Thisadaptive eval enables flexible use of general andtask-specific knowledge. Moreover, since text em-beddings are usually pre-calculated (Radford et al.,2021), this adaptive coefficient incurs only a minorcomputational overhead. Vision Ensemble.Additionally, to further im-prove the performance by leveraging more generalknowledge of the pretrained VLMs, we can alsoemploy an ensemble technique for the visual en-coder that combines the visual feature of the pre-trained VLM (z) with the task-adapted VLMs (z)as follows:",
  "Experimental Setup": "Datasets.We evaluate APEX on the three mostcommonly used transfer learning tasks: base-to-novel generalization, cross-dataset evaluation, anddomain generalization. For all the few-shot exper-iments except domain generalization, we followCoCoOp (Zhou et al., 2022a) which uses 11 im-age recognition datasets. The datasets cover multi-ple recognition tasks including ImageNet (Denget al., 2009) and Caltech101 (Fei-Fei et al.,2004) which consists of generic objects; Oxford-Pets (Parkhi et al., 2012), Stanford Cars (Krauseet al., 2013), Flowers102 (Nilsback and Zisserman,2008), Food101 (Bossard et al., 2014), and FGVCAircraft (Maji et al., 2013) for fine-grained classifi-cation, SUN397 (Xiao et al., 2010) for scene recog-nition, UCF101 (Soomro et al., 2012) for actionrecognition, DTD (Cimpoi et al., 2013) for textureclassification, and EuroSAT (Helber et al., 2017)which consists of satellite images. For the domaingeneralization benchmark, we use ImageNet as asource dataset and use ImageNet-A (Hendryckset al., 2019), ImageNet-R (Hendrycks et al., 2020),ImageNet-Sketch (Wang et al., 2019), and Ima-geNetV2 (Recht et al., 2019) as out-of-domaindatasets. Experimental Details.We use multiple base-lines for comparison with our methods in ex-periments. These include the standard zero-shotCLIP (Radford et al., 2021), CLIP-Adapter (Gaoet al., 2023), CoCoOp (Zhou et al., 2022a) andMaPLe (khattak et al., 2023). We also considerProGrad (Zhu et al., 2023a), which uses gradient",
  "APEX72.0064.70 48.48 50.68 76.76 60.16": "alignment for prompt learning. When reporting re-sults, we have reproduced all the experiments, aswe observe that the values are highly dependenton the random seed. Instead of taking the averageresults from three seeds, as done in previous works(khattak et al., 2023), we use the average of 20seeds to determine the final value for base-to-noveland the average of 5 seeds for cross-evaluation anddomain-generalization. Additionally, we found thatusing the Adadelta optimizer (Zeiler, 2012) yieldsbetter results, so we have reproduced the experi-ments with Adadelta. More experimental detailscan be found in the Appendix A.",
  "Main Results": "Base-to-Novel Generalization.In this scenario,the datasets are evenly divided into base and novelcategories. The model is trained on the base classesusing 16 shots and is subsequently tested on boththe base and novel classes. As indicated in Ta-ble 2, APEX consistently outperforms the best ofthe previous methods in average accuracy acrossall datasets, with a margin of 16%. In particular,our method exhibits superior performance in novelclasses on all datasets, demonstrating APEXs en-hanced generalizability. The exceptions are OxfordPets and FGVC Aircraft, where the performanceis already exceptionally high and low, respectively.This improvement is especially notable in domainswith high RTD, such as EuroSAT (+15.84%) and",
  "DTD (+3.90%). Additionally, the APEX methodalso shows superior performance in base categories,highlighting the high adaptability of our approach": "Cross-dataset Evaluation.We train the modelto generalize across different domains by using across-dataset evaluation task. Specifically, we firsttrain the model on the ImageNet dataset and thentransfer it to the 10 other datasets. sum-marizes that APEX shows the best overall perfor-mance compared to existing baselines. Our pro-posed method achieves the best performance on7 out of 11 tasks. This demonstrates APEXs ef-fectiveness, especially in difficult situations whereboth the task and domain are unseen. Domain Generalization.We assess the capabil-ity of APEX to generalize to out-of-distribution databy training on the source dataset, ImageNet, andsubsequently testing on various modified versionsof ImageNet. Our method does not achieve a largemargin of superiority since our adaptive ensembleis primarily designed to enhance performance innovel classes. Nonetheless, our method still sur-passes all baseline models on average accuracy inthis domain generalization task.",
  "In this section, we provide ablation experiments onAPEX. Full results are detailed in Appendix C": "Effect of Ensemble.We have conducted a com-ponent analysis of two adaptive ensemble tech-niques of APEX, focusing on (1) the text encoderand (2) the visual encoder. The results, as shownin , reveal that the ensembling of the textencoder is crucial for enhancing performance. Con-versely, ensembling the visual encoder results in aminor yet consistent improvement. The text ensem-ble notably achieves substantial improvements indomains with low RTD, implying that task-specificknowledge is primarily acquired through TA. Over-all, employing both ensemble techniques leads tothe most improvement regardless of RTD.",
  ": Comparison of the accuracy of base, novel,and their harmonic mean using low-rank linear adapterand bottleneck layer of non-linear adapter (Gao et al.,2023)": "et al., 2022) utilize the bottleneck layer (He et al.,2016) which shrinks and re-expands the featuredimensions to improve efficiency. Similarly,we utilize low-rank matrix factorization thatA = UV where V, U Rdldr with dr < dl toimprove the parameter efficiency. showsthat although TAs performance diminishes withdecreasing dimension dr, average accuracy withfew parameters (dr = 32) still achieves perfor-mance comparable to ProGrad (Zhu et al. 2023a;+0.72%). Moreover, the linear adapter consistentlyoutperforms the non-linear adapter (Gao et al.,2023) across all values of dr, motivating us to usea linear adapter in our proposed APEX.",
  "Conclusion": "We propose APEX to address the challenges of con-ventional prompt and adapter-style ETL methodsfor VLMs. Our approach incorporates two key com-ponents based on our observations: (1) using VPTand TA for exploiting the property of each modalityand (2) adaptive ensemble coefficient in the infer-ence stage. We empirically demonstrate the supe-rior performance of APEX, consistently achievinga better performance than the previous methods.",
  "Acknowledgements": "This work was supported by Institute of Informa-tion & communications Technology Planning &Evaluation (IITP) grant funded by the Korea gov-ernment (MSIT) (No.2019-0-00075, Artificial In-telligence Graduate School Program (KAIST), 5%),Institute of Information & communications Tech-nology Planning & Evaluation (IITP) grant fundedby the Korea government(MSIT) (No. RS-2024-00457882, AI Research Hub Project), 5%), and In-stitute of Information & communications Technol-ogy Planning & Evaluation (IITP) grant funded bythe Korea government(MSIT) (No.2022-0-00641,XVoice: Multi-Modal Voice Meta Learning, 90%).",
  "Limitation": "We focus on two types of ETL, prompt tuning andadapter-style tuning, for VLMs for vision-languageunderstanding tasks such as CLIP, EVA-CLIP, andCoCA-CLIP. While our extensive analyses providevaluable insights, our paper primarily centers onunderstanding tasks, with opportunities for furtherexploration in vision-language generation taskssuch as BLIP (Li et al., 2022a) and LLaVA (Liuet al., 2024). Additionally, though we focus on twomain representative ETL methods, further analy-ses could be conducted on other ETL methods likeLoRA (Hu et al., 2022) and IA3 (Liu et al., 2022).We leave these aspects for future work but wishto emphasize the comprehensive exploration pro-vided by our study on the two representative ETLmethods for VLMs. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Fei-Fei Li. 2009. Imagenet: a large-scalehierarchical image database. pages 248255": "Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-ing generative visual models from few training ex-amples: An incremental bayesian approach tested on101 object categories. 2004 Conference on ComputerVision and Pattern Recognition Workshop, pages 178178. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma,Rongyao Fang, Yongfeng Zhang, Hongsheng Li, andYu Qiao. 2023. Clip-adapter: Better vision-languagemodels with feature adapters. International Journalof Computer Vision, pages 115. Shashank Goel, Hritik Bansal, Sumit Bhatia, RyanRossi, Vishwa Vinay, and Aditya Grover. 2022. Cy-clip: Cyclic contrastive language-image pretraining.Advances in Neural Information Processing Systems,35:67046719. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 770778. Patrick Helber, Benjamin Bischke, Andreas R. Dengel,and Damian Borth. 2017. Eurosat: A novel datasetand deep learning benchmark for land use and landcover classification. IEEE Journal of Selected Topicsin Applied Earth Observations and Remote Sensing,12:22172226. Dan Hendrycks, Steven Basart, Norman Mu, SauravKadavath, Frank Wang, Evan Dorundo, Rahul De-sai, Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo,Dawn Xiaodong Song, Jacob Steinhardt, and JustinGilmer. 2020.The many faces of robustness: Acritical analysis of out-of-distribution generalization.2021 IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 83208329. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-hardt, and Dawn Xiaodong Song. 2019. Natural ad-versarial examples. 2021 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR),pages 1525715266. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, ZaranaParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, ZhenLi, and Tom Duerig. 2021. Scaling up visual andvision-language representation learning with noisytext supervision. In International conference on ma-chine learning, pages 49044916. PMLR.",
  "Menglin Jia, Luming Tang, Bor-Chun Chen, ClaireCardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In EuropeanConference on Computer Vision (ECCV)": "Muhammad Uzair khattak, Hanoona Rasheed, Muham-mad Maaz, Salman Khan, and Fahad Shahbaz Khan.2023. Maple: Multi-modal prompt learning. In TheIEEE/CVF Conference on Computer Vision and Pat-tern Recognition. Muhammad Uzair Khattak, Syed Talal Wasim, Muza-mmal Naseer, Salman Khan, Ming-Hsuan Yang, andFahad Shahbaz Khan. 2023. Self-regulating prompts:Foundational model adaptation without forgetting. InProceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 1519015200.",
  "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021": "The power of scale for parameter-efficient prompttuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 30453059, Online and Punta Cana, Domini-can Republic. Association for Computational Lin-guistics. Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022a. Blip: Bootstrapping language-imagepre-training for unified vision-language understand-ing and generation. In International Conference onMachine Learning, pages 1288812900. PMLR. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu HongHoi. 2021. Align before fuse: Vision and languagerepresentation learning with momentum distillation.Advances in neural information processing systems,34:96949705. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang,Jianwei Yang, Chunyuan Li, Yiwu Zhong, LijuanWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.2022b. Grounded language-image pre-training. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1096510975. Yanghao Li, Haoqi Fan, Ronghang Hu, ChristophFeichtenhofer, and Kaiming He. 2023.Scalinglanguage-image pre-training via masking. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2339023400. Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mo-hta, Tenghao Huang, Mohit Bansal, and Colin Raffel.2022. Few-shot parameter-efficient fine-tuning is bet-ter and cheaper than in-context learning. In Advancesin Neural Information Processing Systems.",
  "Haohan Wang, Songwei Ge, Eric P. Xing, andZachary Chase Lipton. 2019. Learning robust globalrepresentations by penalizing local predictive power.In Neural Information Processing Systems": "Jianxiong Xiao, James Hays, Krista A. Ehinger, AudeOliva, and Antonio Torralba. 2010. Sun database:Large-scale scene recognition from abbey to zoo.2010 IEEE Computer Society Conference on Com-puter Vision and Pattern Recognition, pages 34853492. Hantao Yao, Rui Zhang, and Changsheng Xu. 2023.Visual-language prompt tuning with knowledge-guided context optimization. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 67576767. Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu,Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,Xin Jiang, and Chunjing Xu. 2022. FILIP: Fine-grained interactive language-image pre-training. InInternational Conference on Learning Representa-tions.",
  "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,and Lucas Beyer. 2023.Sigmoid loss forlanguage image pre-training.arXiv preprintarXiv:2303.15343": "Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao,Kunchang Li, Jifeng Dai, Yu Qiao, and HongshengLi. 2022. Tip-adapter: Training-free adaption of clipfor few-shot classification. In Computer Vision ECCV 2022, pages 493510, Cham. Springer NatureSwitzerland. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, andZiwei Liu. 2022a.Conditional prompt learningfor vision-language models. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1681616825.",
  "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, andZiwei Liu. 2022b. Learning to prompt for vision-language models. International Journal of ComputerVision, 130(9):23372348": "Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Han-wang Zhang. 2023a. Prompt-aligned gradient forprompt tuning. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages1565915669. Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou,Dong Wang, Bin Zhao, and Peng Gao. 2023b. Notall features matter: Enhancing few-shot clip withadaptive prior refinement.In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 26052615.",
  "AImplementation Details": "As explained in , we utilize the ViT-B/16model as the CLIP image encoder and a standardGPT2-like structure with an End Of Text (EOT) to-ken as the classification token for the text encoder.To implement APEX, we use visual prompts forall layers, setting JV = 12 for base-to-novel gen-eralization and JV = 3 for cross-evaluation anddomain generalization. The text prompt is appliedonly to the shallow prompt, and therefore, JV = 1for all experiments. The number of prompts foreach layer, bV and bT , is set to 2. The initial textprompt is fixed as a photo of a\", and the visualprompts are initialized with a zero-mean Gaussiandistribution with a standard deviation of 0.02. Thematrix term of the text adapter is initialized withan identity matrix, and the bias vector is initializedwith a zero vector.Fortraining,weusetheAdadeltaopti-mizer (Zeiler, 2012) with a learning rate of 0.15and a cosine learning rate scheduler. The batch sizeis set to 16, and we train for 15 epochs, except forImageNet, where we train for 5 epochs. As in pre-vious works, we apply augmentation techniques ofrandom cropping and flipping. The scaling factor, used for calculating eval, is set to 4.0. In theSGD experiments presented in Appendix C, weadopt a batch size of 16 and epochs of 30 and 5for ImageNet, along with a learning rate of 0.0015and a cosine learning rate scheduler. The augmen-tation and scaling factors are set the same as in theAdadelta experiments.For reproducing baselines, we use the Adadeltaoptimizer with a learning rate of 0.25, selected aftera grid search with values [0.1, 0.15, 0.2, 0.25, 0.3].The rest of the settings remain the same as in theoriginal papers. Results with their original con-figurations using SGD optimizer are listed in Ap-pendix C. All our experiments were conducted ona single NVIDIA RTX 3090.",
  "CAdditional Experiments": "C.1Ablation on Adaptive Ensemble illustrates the complete results of the com-ponent analysis of the adaptive ensemble. We onlydisplay results for novel classes, as these ensem-ble components do not affect the results for baseclasses, given that eval is set to 1.0 for seenclasses. AThe ensemble of the text encoder is cru-cial as its removal leads to a significant perfor-mance drop in domains with low RTD, such asStanford Cars and SUN397. This demonstrates thatmoderating TA with an adaptive ensemble helpsto leverage both task-specific knowledge and gen-eral VLMs knowledge effectively. The ensembleon the visual encoder offers marginal improvement,but combining both still yields the most superiorperformance on average.",
  "Average74.6176.1974.8376.76": "datasets. This parameter-efficient approach exhibitsrelative robustness in performance, even outper-forming MaPLe (khattak et al., 2023) for rank64 (+0.32%) on average. These encouraging re-sults have led us to adopt the linear adapter for thetext encoder. Furthermore, we observe that initial-izing the adapter with an identity matrix improvesperformance, a strategy that can be explored morethoroughly in future work.",
  "C.3Full Results on Manual Text Prompts": "presents the detailed results for each datasetusing manual prompts, which are summarized in. The manual prompts, designed for eachdataset as described in (Gao et al., 2023; Zhanget al., 2022), appear to underperform comparedto other methods. This suggests that they maynot be the optimal choice for every dataset, andthat designing these prompts manually is challeng-ing. In contrast, just ensembling multiple manualprompts (Radford et al., 2021) works significantlybetter, indicating that optimal prompts may existamong these manual options. This finding also im-plies that utilizing improved manual prompts cansubstantially enhance performance, potentially re-placing shallow prompts. Shallow prompt tuningfor the text input yields the best results, demonstrat-ing its effectiveness and flexibility. Therefore, weadopt this approach for our main results.",
  "C.5Comparison with More Baselines": "Due to the page limit, we present a comparisonwith additional baselines for base-to-novel gener-alization experiments in , which are notincluded in . These include training withVPT, TPT, and a combination of VPT and TPT.We also compare our method with the recently pro-posed PromptSRC (Khattak et al., 2023), whichemploys various regularization techniques such asself-consistency loss and Gaussian averaging. Ourmethod outperforms all these baselines in termsof harmonic mean and demonstrates particularlyhigh performance for novel classes. Compared to PromptSRC, our method significantly outperformsin novel classes of high RTD domains, such as Eu-roSAT (+8.39%) and DTD (+4.22%), while main-taining comparable performance in other domains.Notably, our method achieves these results witha simpler training approach, without the need fornumerous manual prompts for SRC loss, and withfewer hyperparameters, unlike the many requiredby PromptSRCs regularization techniques. Addi-tionally, our method surpasses the simpler base-lines of naive training using VPT, TPT, and theircombination, highlighting the effectiveness of ourconfiguration design and adaptive ensemble.",
  "Average on11 datasets ImageNet Caltech101 OxfordPets StanfordCarsFlowers102 Food101 FGVCAircraft SUN397 DTD EuroSAT UCF101": "Opt. manual prompt (Zhang et al., 2022)Base84.1576.6498.1595.0580.7597.4589.3542.9281.2483.0293.9387.10Novel75.2469.0094.3397.0475.3277.6691.2836.4277.6057.5971.7479.70HM79.1772.6296.2096.0377.9486.4490.3039.4079.3868.0181.3583.24 Ens. (60 manual prompts) (Radford et al., 2021)Base84.0276.4898.1595.0980.7097.3789.5642.5681.4682.6293.0187.18Novel76.1770.2493.9396.4475.8877.1691.2035.6478.3659.4580.3579.21HM79.7073.2395.9995.7678.2286.0990.3738.7979.8869.1586.2283.00 Shallow prompt (APEX)Base83.9977.1298.1895.1180.5397.4789.6042.6981.1782.4592.8386.74Novel76.7671.1095.0697.2775.0877.5892.0635.2178.9863.8079.8978.37HM80.0473.9996.5996.1877.7186.4090.8138.5980.0671.9485.8882.34 ble. However, adding TPT to VPT and TA does notenhance performance, especially in high RTD sce-narios, as evidenced by decreased performance inDTD (-4.98%) and EuroSAT (-6.78%) compared toconfigurations without TPT. While combining TPTwith VA demonstrates reasonable performance, itis not as effective as the combination of VPT andTA. This highlights the importance of class separa-bility of visual features achieved through multiplestacks of prompts. Overall, the configuration ofAPEX outperforms the other setups.",
  "C.7Ablation on": "presents the results of an ablation study onthe hyperparameter , which is used to calculateeval. A higher leads to a lower eval, indicat-ing greater reliance on the general knowledge ofVLMs, which is beneficial for domains with lowRTD, and vice versa. As observed, the performancein domains with low RTD, such as Stanford Carsand SUN397, tends to improve with a higher .However, the optimal performance for difficult do-mains like Aircraft and DTD is achieved with values between 1.0 and 3.0. Not all domains fol-low this tendency since eval is calculated on aclass-wise basis, as demonstrated in the case ofEuroSAT. Interestingly, except for the value of 2.0,our method demonstrates robustness to variationsin , as it does not significantly affect the aver-age performance. Overall, setting to 4.0 yieldsthe best performance, and therefore, this value hasbeen selected for the final results.",
  "C.8Ablation on": "presents the comprehensive results of theablation study on a fixed , which is used in and Eq. (10). The same is applied uniformlyacross all classes and is set as a fixed value forboth the visual and text encoders. This is doneto determine the correlation between and thedomain, along with its transfer difficulty. Similar to Section C.7, domains with high RTD, such asEuroSAT, require a higher value to perform wellcompared to domains with low RTD, like StanfordCars. These findings support the necessity for anadaptive ensemble that is closely aligned with RTD.",
  "C.9Shallow Prompt": "Although we observe that TPT leads to overfit-ting, we employ one-layer learnable text promptsto enhance real-world practicality. com-pares the performance of manually optimizedprompts (Gao et al., 2023; Zhang et al., 2022), theensemble of manual prompts (Radford et al., 2021),and shallow prompts. The shallow prompt methodoutperforms manual prompts, proving its effective-ness. However, manual prompts, particularly whenensembled, also show comparable performance toshallow prompts, suggesting that well-designedmanual prompts can be an effective alternative.",
  "C.10Results on Different VLMs": "We validate our approach using different back-bones: EVA-CLIP (Sun et al., 2023) and CoCa (Yuet al., 2022). displays the results us-ing these two backbones, where we compare ourmethod with both zero-shot and naive prompt tun-ing approaches that combine VPT and TPT. Asobserved, APEX consistently outperforms the aver-age results in terms of harmonic mean, regardlessof the model used. Specifically, with EVA-CLIP,our method demonstrates superior performance forboth base and novel classes. In the case of themost challenging domain, EuroSAT, our methodsignificantly enhances performance compared tothe zero-shot accuracy for novel classes (+18.46%).A similar improvement of 8.85% on EuroSAT isobserved with CoCa. However, in terms of novelclasses, the average performance of zero-shot tun-ing is superior for CoCa. This could be attributedto the larger patch size of this backbone, whichmight increase the risk of overfitting on the vision",
  "D.3Results on 6 datasets": "We also present extended results in ,which include data from three additional datasets:ImageNet, SUN397, and DTD. For ImageNet andSUN397, which already exhibit high class separa-bility, we note that all methodsTPT, VPT, andtheir combinationyield similar performance dif-ferences. However, the results for DTD indicatea tendency for TPT to overfit to the base classes.This observation is consistent with the findings pre-sented in .",
  "Shots": "EuroSAT TPTVPT+TPTVPT : Extended results for . All results indifferent datasets show similar trends that indicate VPTyields a smaller discrepancy in performance betweenbase and novel categories, suggesting a reduced risk ofoverfitting compared to TPT. (Radford et al., 2021) facilitates this by adoptingcontrastive learning with a large-scale dataset of400 million images. ALIGN(Jia et al., 2021) fur-ther improves upon this by scaling up the datasetwith more noisy image-text pairs. FILIP (Yao et al.,2022) enables finer-grained alignment between twomodalities and GLIP (Li et al., 2022b) improvesvisual grounding and object detection using VLMs.CoCa (Yu et al., 2022) employs both captioning andcontrastive losses, thereby integrating the modelcapabilities of contrastive approaches like CLIPwith those of generative methods. CyCLIP (Goel et al., 2022) employs cyclic loss to ensure geo-metric consistency, while FLIP (Li et al., 2023)enhances VLMs through masking techniques. EVA-CLIP (Sun et al., 2023) implements various trainingtechniques, such as different attention mechanismsand optimizers, to further improve CLIPs perfor-mance. Additionally, SigLIP (Zhai et al., 2023) re-places the softmax loss with sigmoid loss, enablingmore efficient pretraining with smaller batch sizes.There is also a line of research focused onencoder-decoder or decoder-only architectures.BLIP (Li et al., 2022a) facilitates both encodingand decoding by training with three objective func-tions, utilizing synthetic data and data filtering.ALBEF (Li et al., 2021) employs a strategy ofalignment before applying cross-attention, com-bined with a momentum update. Flamingo (Alayracet al., 2022) enables few-shot inference in vision-language tasks through architectural innovations,using vision-language prompts. Prompt TuningEfficient tuning using softprompts, originating in the domain of naturallanguage processing, has gained a lot of atten-tion (Lester et al., 2021). This approach has alsobeen applied in the vision-language domain toadapt to downstream tasks. CoOp (Zhou et al.,2022b) was the first to apply learnable promptsfor CLIP model, replacing manual prompts foreach domain. ProDA (Lu et al., 2022) observesthat these text prompts can be viewed as a distri-",
  "Opt. manual prompt (Zhang et al., 2022)84.1575.2479.17Ens. (60 manual prompts (Radford et al., 2021))84.0276.1779.70Shallow prompt83.9976.7680.04": "bution and proposes prompt distributional learningfor higher quality results. CoCoOp (Zhou et al.,2022a) conditions text prompts on images to pre-vent overfitting to base classes. KgCoOp (Yao et al.,2023) regularizes by minimizing the discrepancybetween learned and manual prompts. UPT (Zanget al., 2022) examines both VPT (Jia et al., 2022)and text prompts, proposing a unified approach togenerate visual and textual prompts from the samearchitecture. MaPLe (khattak et al., 2023) employsthe alignment of visual and text prompts for im-provement with deep prompts, while DCP (Liuet al., 2023) uses an attention mechanism for thisalignment. There is also a line of research aimedat preventing the forgetting of general knowledge.ProGrad (Zhu et al., 2023a) aligns gradient direc-tions to preserve general knowledge, and Prompt-SRC (Khattak et al., 2023) utilizes multiple regular-ization losses with Gaussian aggregation of modelweights to prevent forgetting. Adapter-style TuningAdapter-style tuning hasbeen extensively explored as an alternative toprompt tuning. CLIP-Adapter (Gao et al., 2023)was the first proposed method in this area, utilizinga two-layer MLP structure with ReLU nonlinearityin between. Additionally, it incorporates a residualconnection to preserve general knowledge. For im-proved efficiency, Tip-Adapter (Zhang et al., 2022)employs a cache-based model to save the featuresand labels of few-shot samples, using them to pre- dict test outcomes without further training. Thisapproach also facilitates better fine-tuning by us-ing the cache as initial training points for furtherrefinement. Differently, Task Residual (Yu et al.,2023) adopts a unique strategy by simply adding aresidual or bias term vector for each class, reducingreliance on pre-trained features. Zhu et al. (2023b)enhances cache-based models through prior refine-ment, which involves selecting important featuresfor the cache-based model.",
  "RTD4.95 1031.84 1011.42 102": "Algorithm 2 Pseudo-Algorithm for Adaptive Infer-ence of APEXRequire: Pretrained visual encoder V, Pretrainedtext encoder T , Learned vision prompts P,Learned shallow text prompts P0,Learnedadapter parameterized by matrix A and b, TheC classes for base category {1, . . . , C}, TheCeval candidate classes for evaluation {C +1, . . . , C + Ceval},"
}