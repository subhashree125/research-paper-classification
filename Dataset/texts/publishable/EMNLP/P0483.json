{
  "Abstract": "Conventional Knowledge Graph Reasoning(KGR) models learn the embeddings of KGcomponents over the structure of KGs, but theirperformances are limited when the KGs areseverely incomplete. Recent LLM-enhancedKGR models input KG structural informationinto LLMs. However, they require fine-tuningon open-source LLMs and are not applicable toclosed-source LLMs. Therefore, in this paper,to leverage the knowledge in LLMs withoutfine-tuning to assist and enhance conventionalKGR models, we propose a new three-stagepipeline, including knowledge alignment, KGreasoning and entity reranking. Specifically, inthe alignment stage, we propose three strate-gies to align the knowledge in LLMs to theKG schema by explicitly associating uncon-nected nodes with semantic relations. Basedon the enriched KGs, we train structure-awareKGR models to integrate aligned knowledgeto original knowledge existing in KGs. In thereranking stage, after obtaining the results ofKGR models, we rerank the top-scored entitieswith LLMs to recall correct answers further.Experiments show our pipeline can enhancethe KGR performance in both incomplete andgeneral situations.",
  "Introduction": "Knowledge Graph (KG) is widely used to storeenormous human knowledge or objective facts inthe real world. Conventional embedding-basedKGR models learn structural embeddings for KGcomponents. Recently, path-based KGR modelsexploit the logical knowledge underlying the pathsconnecting the head and tail. All these models treatentities and relations as symbolized identificationswithout actual semantics and thus heavily rely onreasoning over the KG structures. However, evenfull-size KG datasets cannot fully cover the massivereal-world knowledge and suffer from incomplete-ness, which naturally restricts KGR performances.",
  "* Corresponding Author": ": (a) Conventional KGR models reason overoriginal KGs, suffering from incompleteness. (b) Ourproposed pipeline without fine-tuning includes threesteps: align LLMs to the KG schema (the aligned edgesare in red), reason over the enriched KGs and rerank theresults with LLMs. Our pipeline achieves better results. Although LLMs show exciting abilities, it is achallenge for them to singly act as entity reason-ers for KGR task due to the huge KG entity space.Tan et al. (2023) further proves that matching theprediction of LLMs with entity names by postpro-cessing could easily fail. Recently, KGT5 (Sax-ena et al., 2022) and CSProm-KG (Chen et al.,2023a) have explored to learn KG structure byfine-tuning LLMs. However, on the one hand, forclosed-source LLMs like ChatGPT, we can notaccess the parameters and thus can not combineits knowledge with KGs by fine-tuning; on theother hand, fine-tuning open-source LLMs, suchas LLAMA3-70B 1, for a single task is relativelyexpensive. Therefore, how to assist KGR by in-corporating the rich knowledge in LLMs and thestructured information in KGs without fine-tuning",
  "becomes a remaining problem": "Relying on the instruction-following capabilityof LLMs, we propose to use LLMs from two viewsto enhance KGR performance without fine-tuning.First, many entity pairs in KGs lack necessary se-mantic relations because of the incompleteness ofKGs. From the view of knowledge alignment, wealign the knowledge in LLMs to the KG schema tomitigate the incompleteness of KGs before reason-ing and then add the aligned knowledge into KGs inthe form of edges, which preserves KG structuresand enriches KG connections. Formally, we input apair of entities into LLM and have it predict their re-lation. Based on the enriched KGs, we can adopt ar-bitrary structure-aware KGR models to conduct theentity prediction task. Second, after obtaining KGreasoning results, from the view of entity reranking,we leverage LLMs to rerank the top-scored entitiesof KGR models for further recalling the correctanswers. Finally, these two views of using LLMsto enhance KGR performance are not exclusive andtogether form our proposed three-stage pipeline forKGR: alignment, reasoning and reranking. Moreover, in the alignment stage, we presentthree knowledge alignment strategies, includingthe closed domain strategy, the open domain strat-egy and the semi-closed domain strategy. Theyrepresent three kinds of approaches for inducingknowledge in LLMs to be outputted according tothe KG schema. Specifically, to directly align theknowledge to the manually predefined relationswhile constructing KGs, the closed domain strat-egy constrains LLMs to select one of the prede-fined relations in the form of multiple-choice ques-tions. Since the relations between entities in thereal world go beyond the predefined ones, the opendomain strategy does not restrict the output con-tent, making less loss of information from LLMs.To provide explainable knowledge alignment forhumans, in the semi-closed domain strategy, wemap the output of LLMs in the open domain backto the predefined relations by semantic matching. To verify the effectiveness of our pipeline in in-complete and general situations, we conduct exper-iments on WN18RR and FB15K-237 with differentsparse-level and full-size versions. Additionally,we compare the accuracy and stability of the threealignment strategies to illustrate the quality of thegenerated relations. We further demonstrate thediverse influences of aligned edges on the origi-nal knowledge by analysing the LLMs output in the case study, which reveals that, when applyingthe open domain knowledge alignment, LLMs gen-erate correct and fine-grained semantics beyondthe predefined KG relations. This may explain themechanism of performance enhancement.In summary, our contributions are tri-fold:",
  "Conventional KG Reasoning": "Traditional KGR models can be categorized intoembedding-based and path-based models (Lianget al., 2022). The embedding-based models encodethe KG entities and relations into low-dimensionrepresentations. RotatE (Sun et al., 2019) uses arotation-based method with complex-valued em-beddings. Tucker Decomposition is first introducedin KGR by TuckER (Balazevic et al., 2019). Then,HAKE (Zhang et al., 2020) models the semantichierarchy based on the polar coordinate space andHousE (Li et al., 2022) involves a novel parameter-ization based on Householder transformations. Thebackbone of path-based models is reinforcementlearning (Das et al., 2018). MultiHopKG (Linet al., 2018) does multihop reasoning and providesKG paths to support predictions. CURL (Zhanget al., 2022) separates the KGs into differentclusters according to the entity semantics and thenfine-grains the path-finding procedure into two-level. JOIE (Hao et al., 2019) models all triples inthe same zero-curvature Euclidean space, omittingthe hierarchical and cyclical structures of KGs.CAKE (Niu et al., 2022) further extracts common-sense entity concepts from factual triples and canaugment negative sampling by jointing common-sense and conducting fact-view link prediction.",
  "By modelling KGR task as a sequence-to-sequenceproblem, GenKGC (Xie et al., 2022) and KG-S2S (Chen et al., 2022) utilize encoder-decoder": "pre-trained language models to generate targetentity names. Lee et al. (2023) unifies KG factsinto linearized sentences and guides LLMs tooutput the answers in texts directly. Followingthem, fine-tuning open-source LLMs by fusingthe accessible KG structures for the KGR taskhas enjoyed lots of interest. KG-LLaMA (Yaoet al., 2023) makes the first step to applyingLLaMA (Touvron et al., 2023) in KG linkprediction by instruction tuning. KoPA (Zhanget al., 2023c) further leverages prefix tuning andprojects KG embeddings into textual token space.",
  "Exploration of LLMs without Fine-tuning": "By prompting LLMs, MPIKGC (Xu et al., 2024)generates descriptions of components in the KGsand sends the enriched information into description-based KGR models. However, MPIKGC is basedon description-based KGR models and can not dealwith unconnected entities, which we can handle.KICGPT (Wei et al., 2023) reranks the top retrievedentities, but it is centred on prompt engineering andfocuses on analysing the effect of several designedknowledge prompts on the ranking quality. Besides,the KGR models KICGPT used are unoptimized.Our proposed pipeline is centred on optimizingKGR models and focuses on assisting reasoningfrom two perspectives: alignment and reranking.",
  "Methodology": "In this section, we describe the concrete implemen-tation methodology of the new pipeline withoutfine-tuning. First, we propose three knowledgealignment strategies and the corresponding waysto convert the textual output of LLMs into KGschema. Second, we train conventional structure-aware KGR models over the enriched KGs. Finally,we further leverage LLMs to rerank the top-scoredentities of KGR models, recalling correct answers.",
  "Knowledge Alignment": "To obtain the knowledge related to the queried twoentities in LLMs, we induce the output of LLMs viadifferent prompts. Considering the trade-off of theKG schema and the flexible but controllable outputof LLMs, we propose the following three align-ment strategies, which explicitly enrich KGs withthe knowledge in LLMs in three different manners.The prompts are shown in Appendix B. We findwhether neighbour edges of entities are includedin prompts has little effect on the output of LLMs. 3.1.1Closed Domain StrategyThe test-like format of multiple-choice questionsis generally used in the evaluation of the ability ofLLMs in the fields of law (Cui et al., 2023), health-care (Wang et al., 2023a) and finance (Zhanget al., 2023a). In this alignment strategy, we utilizeLLMs to select the most likely relation for thehead and tail entities. Specifically, we add thenames of predefined KG relations to the promptsas candidates and explicitly instruct LLMs togenerate the capital letter before the correct option.LLMs are induced to fully conform to the originalKG schema; thus, their knowledge is aligned withKGs at both the semantic and structural levels. 3.1.2Open Domain StrategyActually, the relations between different entitiesare diverse and fine-grained. However, researchersabstract the KG relations into several representativeones for unification and convenience during the KGconstruction. We aim to leverage the knowledgein LLMs relevant to the KG domains between twoentities to augment the omitted information.Specifically, in the open domain strategy, weadopt prompts in the form of short answer ques-tions to induce knowledge in LLMs. We do notrestrict their output to necessarily follow the prede-fined KG relations and only imply what aspects ofknowledge LLMs should focus on. The descriptionof KG domains in prompts ensures that LLMs donot generate aimlessly. All the outputs are addedinto KGs as enriched relations on edges, withoutdiscarding any semantic information in LLMs. 3.1.3Semi-Closed Domain StrategyIn the closed domain strategy, LLMs directlygenerate the option, so we have no insight into howLLMs understand the KG relations and why LLMsmake the final decision. As for the open domainstrategy, the output of LLMs exactly reflects theknowledge about the two entities. However, LLMsare unable to voluntarily abstract these concreterelations into the structural format as humans do.Therefore, the semi-closed domain knowledgealignment strategy arises, where we map the outputof LLMs in the open domain strategy back to theKG schema. Specifically, we leverage Sentence-BERT (Reimers and Gurevych, 2019) to calculatethe semantic similarity between the output and allthe predefined relations. The output is eventuallyconverted to the relation with the highest similarityscore. This alignment strategy provides an inter-",
  "KG Reasoning": "In the closed domain strategy and semi-closed do-main strategy, since we align the knowledge inLLMs to the predefined KG relations, we do notneed to modify the modelling way of conventionalstructure-aware KGR models. In the open domainstrategy, since the aligned knowledge is added toKGs as sentences, we use word2vec (Mikolov et al.,2013) to initialize the embeddings for words in allthe output of LLMs and update them while training.Specifically, we take the mean of all embeddingsof words in the corresponding sentence as the em-bedding of an enriched KG edge. In this way, thedownstream KGR models can be trained over theenriched KGs and take advantage of two forms ofknowledge in LLMs and KGs at the same time.Based on the predicted entities of KGR modelsover the enriched KGs, we further improve the per-formance in the next entity reranking stage.",
  "Entity Reranking": "After the reasoning of the KGR models, we willget a list of entities sorted by the scores calculatedby scoring functions. Traditional structure-awareKGR models mainly reason over the KG connec-tions. In this stage, we recall the correct answersusing the reranking ability of LLMs based on thepredicted entities of KGR models. Specifically, weinput the names of top-k candidate entities withthe highest scores into prompts (see Appendix C)and utilize the knowledge in LLMs to rerank thembased on the probability of semantically holding.Therefore, the entity reranking stage further im-proves KGR performance by leveraging semanticknowledge along with structural prediction results.",
  "Experimental Setup": "We adopt the gpt-3.5-turbo version of ChatGPTbecause of its flexibility and shorter API call time.We also deploy LLAMA3-70B in one 24G TeslaV100 as the representative of open-source LLMs.For each dataset, the ratio of new facts enrichedby LLMs and existing facts in the original datasetis 1:10, i.e., 8684 new facts for the four versions ofWN18RR and 27212 new facts for the four versions",
  ": Statistics of our datasets with full-size anddifferent sparse-level versions by randomly retaining": "of FB15K-237. Specifically, for each fact to beadded, we make a single LLM call and process theLLM response to the corresponding form in eachknowledge alignment strategy.In the sparse datasets, we randomly select entitypairs which are not connected. Note that, to avoidthe information leakage of the KG connections,there is no requirement for these entity pairs tobe connected or not in the corresponding full-sizeKGs. In addition, besides predefined relations, wealso allow LLMs to generate or select no relationin the corresponding alignment strategy.For enriched edges, we include all the generatedanswers into KGs without filtering, even thoughsome of them may conflict with the KG groundtruth. The reason is that what we are interestedin is the full picture and unprejudiced knowledgeof LLMs, so any sort of LLM output evaluationcan not be introduced. In other words, regardlessof the answers of LLMs being right or wrong, itis a manifestation of its knowledge and should beconsidered in the downstream KG reasoning.The maximum token length of input texts is lessthan 4096. The generated maximum token lengthis set to 128. For ChatGPT, the temperature param-eter is set to 0.3 in the knowledge alignment stagewhich can increase diversity and set to 0 in the en-tity reranking stage which can guarantee reliability.In the entity reranking stage, we rerank top-k en-tities with k {10, 20}. The optimal k is 20 inall datasets. For WN18RR, the optimal alignmentstrategy is in the open domain. For FB15K-237, theoptimal alignment strategy is in the closed domain.",
  ": Overall results of our pipeline under the optimal settings in FB15K-237. The best results are in bold": "versality of the knowledge stored in LLMs for KGsin a variety of incomplete situations, besides full-size dataset WN18RR (WN18RR-100%), we con-struct three sparse versions, i.e., WN18RR-10%,WN18RR-40% and WN18RR-70%, by randomlyretaining 10%, 40% and 70% triples of WN18RR.The same goes for the dataset FB15K-237. Thestatistics of all the datasets are listed in .",
  ": KGR performance and our proposed three knowledge alignment strategies under ChatGPT in four versionsof FB15K-237. Numbers in bold are the best results of the three alignment strategies": "fore reasoning (Alignmnet, Reasoning) and entityreranking after reasoning (Reasoning, Reranking)can individually improve reasoning performance.Concatenating these two views, our pipeline (Align-mnet, Reasoning, Reranking) obtains the best per-formance enhancement. The improvements in full-size datasets indicate that LLMs provide additionalinformation beyond the well-constructed structuralknowledge in KGs. In sparse datasets, KGR mod-els suffer from limited training data, whereas ourpipeline achieves considerable and consistent en-hancement. The gaps in sparse datasets are greaterthan those in full-size datasets, illustrating our ef-fectiveness under incomplete situations. Further-more, ChatGPT and LLAMA3-70B show com-parable results, confirming the amazing abilitiesof our pipeline together with recent open-sourceLLAMA3-70B and closed-source ChatGPT.",
  "In this section, we compare the different impacts ofthe three knowledge alignment strategies in detail": "In and 5, the lower bounds are the KGRresults without alignment. The upper bounds arethe highest results obtained by randomly addingedges with ground truth to KGs and running KGRmodels multiple times. In full-size datasets, theselected entity pairs do not have golden labels, sowe can not acquire the upper performance bounds. Combining all the results in and 5, com-pared to the lower bounds, there is performance en-hancement in all three knowledge alignment strate-gies for both RotatE and MultiHopKG. This resultsuggests that explicitly enriching KGs by aligningknowledge in LLMs to KG schema does translatethe knowledge into performance enhancement.All the results can not exceed the upper boundsbecause there is still some deviation between thetwo forms of knowledge in LLMs and KGs. For the two kinds of KG datasets, the resultsof three knowledge alignment strategies show dif-ferent trends. In , the improvement in theopen domain strategy is the most prominent, fol-lowed by the improvement in the semi-closed do-main strategy, and the performance improvement in the closed domain strategy is relatively unap-parent. By analysing the output content of LLMsand KG schema, we find that there are only elevenhigh-level relations in WN18RR, and LLMs cangenerate more detailed descriptions of semanticsbetween words in the open domain. In , thetrend of the three alignment strategies for FB15K-237 is the opposite of the trend for WN18RR. Thebest performance is achieved with the closed do-main. The reason may be that the LLM outputcontents in the open domain strategy for FB15K-237 have much redundant knowledge about thetwo entities themselves rather than the expectedrelations between them. Therefore this informa-tion becomes noise that needs to be handled. Incontrast, having the LLM output aligned with theKG schema in the closed and semi-closed domainavoids this situation.",
  "Accuracy of Knowledge Alignment": "To intuitively illustrate the effectiveness of theknowledge in LLMs, we calculate the accuracyof the three knowledge alignment strategies fromthe perspective of relation prediction. Specifically,when there is a golden label of the relation in KGs,we check if LLMs pick up the correct option (au-tomatic evaluation in the closed and semi-closeddomain strategies) or if the output and the golden la-bel semantically overlap (manual evaluation in theopen domain strategy). When there are no goldenlabels, we make judgments based on the real world. From , we find all the accuracy ratesof ChatGPT directly answering relations betweenentities are relatively high, which is the source ofeffectiveness of our proposed knowledge alignment.The accuracy is also stable in the same alignmentstrategy at different sparsity levels. This indicatesknowledge in LLMs is well induced according tothe KG schema in our experiments. Moreover,for relatively abstract relations in WN18RR, thehighest accuracy is achieved in the open domainstrategy, while for relatively concrete relations inFB15K-237, the highest accuracy is achieved in theclosed domain strategy. These two phenomena areconsistent with the performance enhancement in.5. The semi-closed domain strategy losessome information in the process of transforminglinguistic forms for the sake of interpretability, andthus achieves the median accuracy in all datasets. 10%30%50%100% 0.3 0.4 0.5 0.6 0.7 0.8",
  "Stability of Knowledge Alignment": "The stability of knowledge alignment seeks to eval-uate whether enriching KGs by aligning LLMs withKG schema in the three strategies will impact theoriginal knowledge stored in KGs. We introducethe Knowledge Stability (KS@k) metric, indicat-ing the ratio of entities that are correctly predictedby KGR models both before alignment and afteralignment. We calculate KS@k as follows:",
  "KS@k = rank (Alignment, Reasoning) k rank (Reasoning) k,": "where rank (Reasoning) k signifies thecount of rank value under k predicted by KGR mod-els before alignment, i.e., original KGR results; rank (Alignment, Reasoning) k denotesthe count of rank value under k predicted by KGRmodels after alignment, i.e., enhanced KGR results.The insight is that if the score rankings of cor-rect answers in this dataset maintain less than kafter alignment, the aligned knowledge in the threealignment strategies is stable. However, for somespecific queries, the prediction may be worse due tothe introduced wrong facts, resulting in our pipelinechanging its prediction from a correct answer to awrong one and then KS@k declines.In , we employ the number of alignededges ranging from 2% to 10%, with an interval of1%, and measure stability by KS@3 for RotatE. We observe that the closed and semi-closed domains,which add predefined relations into KGs, have sta-ble performance for both datasets. However, theopen domain strategy sees varying degrees of de-cline. We attribute this to KGR models payingmore focus on the diverse output and then resultingin the dilution of original KG knowledge.",
  "Case Study of the Aligned Knowledge": "To further explore the positive and negative influ-ence of LLM output in the open domain strategyon different datasets, we list some typical outputof ChatGPT and LLAMA3-70B in Appendix Dand carry out error analysis in Appendix E. Wefind the LLM output usually goes beyond the pre-defined KG relations and provides fine-grainedinformation. However, LLMs may also provide\"redundant correct information\" as shown below.Positive Influence. LLMs in the open domainusually generate the relationship in plain and accu-rate language, without using professional linguisticvocabulary. For instance, LLMs output Tubercu-losis is a type of infectious disease, which is inline with the definition of hypernym. We visual-ize the embeddings of predefined KG relations andkeywords generated in the open domain strategylearned by RotatE. shows two cases whichexplicitly illustrate their positions in the embed-ding space. Close points in the space indicate thatRotatE successfully captures their similar seman-tics and then these newly generated words are wellintegrated into the KG schema. The eleven prede-fined relations can be seen as abstractions of theconcrete output of LLMs. Therefore, KGR modelsindeed understand and benefit from our proposedopen-domain knowledge alignment strategy.Negative Influence. In contrast, although theLLM output is consistent with the objective world,it may contain redundant correct information. InFB15K-237, when asked about the relation be-tween Robert Ridgely and USA, besides cor-rectly answering Robert Ridgely was an Amer-ican, ChatGPT and LLAMA3-70B also outputhis occupation, which is a redundant entity prop-erty. This redundant correct information wouldsomewhat interfere with the downstream training.Compared with the open domain strategy, align-ing knowledge in LLMs with the KG schema ofFB15K-237 in the other two strategies introducesless noise. Therefore, in summary, LLMs con-sistently improve the KGR performance under all : The positions of the predefined relations inWN18RR and keywords generated by ChatGPT in theopen domain alignment strategy in the embedding space.We can see the predefined relations have overlappingand more delicate semantics, which LLMs realize.",
  "Effects of Reranking Entity Numbers": "shows conspicuous performance enhance-ment of LLMs as rerankers, which suggests theeffectiveness of our proposed pipeline. The sparserthe datasets, the more significant the enhancementof the entity reranking stage and the top-20 sce-nario gives better results than the top-10 scenariobecause LLMs have more chances to recall correctanswers from candidates.These results provethat after the knowledge alignment stage, LLMscan further enhance the KGR performance basedon the semantic differences between candidateentities. Moreover, LLAMA3-70B and ChatGPThave competitive overall results (Hits@3) and per-formance improvement (Imp.) in all the datasets,showing the generalizability of our pipeline.",
  "Limitations and Future Work": "During the use of LLMs, we cannot anticipatewhether the output is valuable before the call ofLLMs, resulting in the quality of each answer ofLLMs can not be controlled. Moreover, the erroranalysis in also shows that there are someimperfections in the output of LLMs. Therefore,in the future, we can add a module to make furthercorrections using the ability of KGR models whileKG reasoning.Additionally, our proposed pipeline is scalable.The rapidly evolving RAG technology (Gaoet al., 2024) may further improve the quality ofknowledge alignment and reranking.We alsohope the pipeline can inspire more thinking abouthow to utilize closed-source LLMs to enhance theperformance of other KG-related tasks from the per-spectives of knowledge alignment and reranking.",
  "Ethics Statement": "In this paper, we use datasets WN18RR andFB15K-237, including eight versions of them. Thedata is all publicly available. Our task is knowledgegraph reasoning, which is performed by findingmissing entities given existing knowledge. Thiswork is only relevant to NLP research and will notbe put to improper use by ordinary people. Weacknowledge the importance of the ACM Code ofEthics and totally agree with it. We ensure thatthis work is compatible with the provided code, interms of publicly accessed datasets and models.Risks and harms of LLMs include the genera-tion of harmful, offensive, or biased content. Thesemodels are often prone to generating incorrect in-formation, sometimes referred to as hallucinations.The ChatGPT used in this paper was licensed underthe terms of OpenAI. We are not recommendingthe use of our proposed pipeline for alignment orranking tasks with social implications, such as jobcandidates or products, because LLMs may exhibitracial bias, geographical bias, gender bias, etc., inthe reasoning results. In addition, the use of LLMs",
  "in critical decision-making sessions may pose un-specified risks": "Ivana Balazevic, Carl Allen, and Timothy Hospedales.2019. TuckER: Tensor factorization for knowledgegraph completion. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 51855194, Hong Kong, China. As-sociation for Computational Linguistics. Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam.2022.Knowledge is flat: A Seq2Seq generativeframework for various knowledge graph comple-tion. In Proceedings of the 29th International Con-ference on Computational Linguistics, pages 40054017, Gyeongju, Republic of Korea. InternationalCommittee on Computational Linguistics. Chen Chen, Yufei Wang, Aixin Sun, Bing Li, and Kwok-Yan Lam. 2023a. Dipping PLMs sauce: Bridgingstructure and text for effective knowledge graph com-pletion via conditional soft prompting. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 1148911503, Toronto, Canada. Associ-ation for Computational Linguistics. Zhongwu Chen, Chengjin Xu, Fenglong Su, ZhenHuang, and Yong Dou. 2023b. Incorporating struc-tured sentences with time-enhanced bert for fully-inductive temporal relation prediction. In Proceed-ings of the 46th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR 23, page 889899, New York, NY,USA. Association for Computing Machinery. Zhongwu Chen, Chengjin Xu, Fenglong Su, ZhenHuang, and Yong Dou. 2023c. Meta-learning basedknowledge extrapolation for temporal knowledgegraph. In Proceedings of the ACM Web Conference2023, WWW 23, page 24332443, New York, NY,USA. Association for Computing Machinery. Zhongwu Chen, Chengjin Xu, Fenglong Su, ZhenHuang, and Yong Dou. 2023d. Temporal extrapo-lation and knowledge transfer for lifelong temporalknowledge graph reasoning. In Findings of the As-sociation for Computational Linguistics: EMNLP2023, pages 67366746, Singapore. Association forComputational Linguistics. Alla Chepurova, Aydar Bulatov, Yuri Kuratov, andMikhail Burtsev. 2023.Better together: Enhanc-ing generative knowledge graph completion with lan-guage models and neighborhood information.InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 53065316, Singapore.Association for Computational Linguistics.",
  "Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu,Ben He, Xianpei Han, and Le Sun. 2023. Mitigatinglarge language model hallucinations via autonomousknowledge graph-based retrofitting": "Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun,and Wei Wang. 2019. Universal representation learn-ing of knowledge bases by jointly embedding in-stances and ontological concepts. In Proceedingsof the 25th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining, KDD 19,page 17091719, New York, NY, USA. Associationfor Computing Machinery. Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, FredMorstatter, and Jay Pujara. 2023. Temporal knowl-edge graph forecasting without knowledge using in-context learning. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 544557, Singapore. Associationfor Computational Linguistics. Rui Li, Jianan Zhao, Chaozhuo Li, Di He, Yiqi Wang,Yuming Liu, Hao Sun, Senzhang Wang, WeiweiDeng, Yanming Shen, Xing Xie, and Qi Zhang. 2022.HousE: Knowledge graph embedding with house-holder parameterization. In Proceedings of the 39thInternational Conference on Machine Learning, vol-ume 162 of Proceedings of Machine Learning Re-search, pages 1320913224. PMLR. Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenx-uan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, andFuchun Sun. 2022. Reasoning over different types ofknowledge graphs: Static, temporal and multi-modal.arXiv preprint arXiv:2212.05767. Xi Victoria Lin, Richard Socher, and Caiming Xiong.2018. Multi-hop knowledge graph reasoning withreward shaping. In Proceedings of the 2018 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 32433253, Brussels, Belgium.Association for Computational Linguistics. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, HongyuLin, Xianpei Han, Le Sun, and Hua Wu. 2022. Uni-fied structure generation for universal informationextraction. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 57555772, Dublin,Ireland. Association for Computational Linguistics.",
  "Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean. 2013. Efficient estimation of word representa-tions in vector space": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer. 2022. Rethinking the role of demonstrations:What makes in-context learning work? In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1104811064,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Guanglin Niu, Bo Li, Yongfei Zhang, and Shiliang Pu.2022. CAKE: A scalable commonsense-aware frame-work for multi-view knowledge graph completion. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 28672877, Dublin, Ireland. As-sociation for Computational Linguistics. Guanglin Niu, Yongfei Zhang, Bo Li, Peng Cui, Si Liu,Jingyang Li, and Xiaowei Zhang. 2019. Rule-guidedcompositional representation learning on knowledgegraphs.In AAAI Conference on Artificial Intelli-gence. Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language models as knowl-edge bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, China. Associationfor Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics. Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.2022. Sequence-to-sequence knowledge graph com-pletion and question answering.In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 28142828, Dublin, Ireland. Association forComputational Linguistics.",
  "Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and JianTang. 2019. Rotate: Knowledge graph embedding byrelational rotation in complex space. In InternationalConference on Learning Representations": "Yiming Tan, Dehai Min, Y. Li, Wenbo Li, Na Hu, Yon-grui Chen, and Guilin Qi. 2023. Can chatgpt replacetraditional kbqa models? an in-depth analysis of thequestion answering performance of the gpt llm fam-ily. In International Workshop on the Semantic Web. Kristina Toutanova and Danqi Chen. 2015. Observedversus latent features for knowledge base and textinference. In Proceedings of the 3rd Workshop onContinuous Vector Space Models and their Composi-tionality, pages 5766, Beijing, China. Associationfor Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models.",
  "Haochun Wang, Chi Liu, Sendong Zhao, Bing Qin,and Ting Liu. 2023a. Chatglm-med. In GitHub": "Xiao Wang, Weikang Zhou, Can Zu, Han Xia, TianzeChen, Yuansen Zhang, Rui Zheng, Junjie Ye,Qi Zhang, Tao Gui, et al. 2023b. Instructuie: Multi-task instruction tuning for unified information extrac-tion. arXiv preprint arXiv:2304.08085. Yanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok.2023. KICGPT: Large language model with knowl-edge in context for knowledge graph completion. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 86678683, Singapore.Association for Computational Linguistics. Xin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, HuiChen, Feiyu Xiong, Mosha Chen, and Huajun Chen.2022. From discrimination to generation: Knowl-edge graph completion with generative transformer.In Companion Proceedings of the Web Conference2022, WWW 22, page 162165, New York, NY,USA. Association for Computing Machinery. Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhi-hong Zhu, Tong Xu, Xiangyu Zhao, Yefeng Zheng,and Enhong Chen. 2024. Multi-perspective improve-ment of knowledge graph completion with large lan-guage models.In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1195611968, Torino, Italia.ELRA and ICCL.",
  "Liang Yao, Jiazhen Peng, Chengsheng Mao, and YuanLuo. 2023. Exploring large language models forknowledge graph completion": "Denghui Zhang, Zixuan Yuan, Hao Liu, Xiaodong lin,and Hui Xiong. 2022. Learning to walk with dualagents for knowledge graph reasoning. Proceedingsof the AAAI Conference on Artificial Intelligence,36(5):59325941. Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, WeiDai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu,Zhiqiang Liu, Zhoufan Zhu, Anbo Wu, Xin Guo,and Yun Chen. 2023a. Fineval: A chinese financialdomain knowledge evaluation benchmark for largelanguage models.",
  "BPrompts for knowledge alignment": "Currently, many works are trying to explore howto incorporate structural information stored in KGsinto the knowledge in LLMs (Chepurova et al.,2023). They either explicitly linearize the neigh-bourhood edges and use LLMs as answer genera-tors, or fine-tune LLMs by incorporating the struc-tured KG embedding into the input of LLMs. Asmentioned in the introduction, our motivation isthe opposite of the recent papers. We want to fig-ure out whether the knowledge stored in the LLMsitself can be aligned with the predefined schema ofKGs. Therefore, for our designed prompts inputinto LLMs, we should not introduce any structuralinformation, such as neighbourhoods, paths or sub-graphs. Following the conclusions of (Min et al.,2022), we design several prompts and select thebest in our experiments.To make LLMs better understand the semanticsof relations, we randomly choose some triple exam-ples of relations and expect LLMs to capture theirmeanings. We also include a description of KGdomains, since the relations are highly correlatedwith it., , and rep-resent four prompts in our proposed knowledgealignment settings for two datasets.",
  "EError analysis": "We analyse the incorrect output of LLMs in theopen domain. Errors fall into the following threecategories: error type 1) generating fabricated ormisplaced facts (hallucination of LLMs); error type2) outputting not related for those entity pairsthat should have relations; and error type 3) out-putting incorrectly formatted or meaningless sen-tences. We show some cases in the . Theseinconsistencies can be solved through further in-consistency detection and knowledge consistentalignment (Wan et al., 2024; Guan et al., 2023;Zhang et al., 2023b).",
  "FExperimental detail": "Our experiments use one 24G Tesla V100 GPUwith Pytorch 1.8. The KG reasoning process needs3h to 12h, depending on the sparsity level of thedatasets. The implementation code of KGR modelsis obtained from their original papers. We use theoptimal parameter reported in the original papersand code. All the results were mean values frommultiple runs.The keys of ChatGPT API were bought fromthe official channel. Each call time was about 0.5sto 2s. All the input and output of ChatGPT is inEnglish. The collection of the output of ChatGPTwas done by the authors. Since the used datasetsare well constructed, there are no offensive contentand identifiers. While collecting the output of Chat-GPT, we still manually checked to anonymise theoffensive content and identifiers in the output byremoving them.",
  "GDifferences between our work withworks of KG construction and worksintroducing the external knowledge": "Currently, KG construction is mainly based on theability of LLMs to extract the given text. For in-stance, Univeral IE (Lu et al., 2022) was fine-tunedfor different information extraction domains re-spectively; InstructionUIE (Wang et al., 2023b)further utilized instruction fine-tuning to unify mul-tiple information extraction tasks at the output sideand achieved better performance. On the other hand, in order to understand if LLMs contain real-world knowledge, given head entities and relations,LAMA (Petroni et al., 2019) answered queriesstructured as \"fill-in-the-blank\" cloze statementsto answer tail entities and found that LLMs canrecover missing entities to some extent. In contrast,our proposed pipeline leveraged LLMs to answermissing relations and enriched KGs with predictedrelations by LLMs, leading to the improvementof KGR models. However, constructing KG withLLMs is not our ultimate goal, our goal is to makethe enriched new edges serve KGR models better.In order to conduct KGC tasks, the researchersexplored several ways of enriching KG informa-tion, including, KG text descriptions (Chen et al.,2023b), lifelong reasoning (Chen et al., 2023d),and the use of Meta-Learning (Chen et al., 2023c).Since LLMs are currently believed to contain awealth of real-world knowledge, we want to knowwhether the knowledge of LLMs is effective forKGR models, and thus this exploratory work pro-posed two ways of using LLMs to help KGR, andformed a pipeline: firstly, introducing the knowl-edge of LLMs into KGs, and then using the knowl-edge of LLMs to rerank the results of KGR. Notethat currently only LLMs can provide both twocapabilities together for the KGR task.",
  "HThe choice of three knowledgealignment strategies": "The results of our proposed strategies are relatedto the abstraction degree of the relations in KGs.We try to give a qualitative description: For KGswith abstract relations, Open Domain Strategy isthe best; for KGs with concrete relations, ClosedDomain Strategy is the best. Specifically, from Sec-tion 4.8, we find 1) the relations in WN18RR arehigh-dimensional and abstract (for instance: hyper-nym, derivationally related from). 2) the relationsin FB15K-237 are concrete and non-subdivisible(for instance: award, has nationality). Because ofthe powerful generative and understanding capa-bilities of LLMs, for WN18RR, the LLM outputusually goes beyond the predefined KG relationsand provides fine-grained information, leading tothe best performance in the Open Domain Strategy.In contrast, FB15K-237 contains detailed relations,so it is the best choice to correspond the knowledgeof LLMs to the original relations in FB15K-237.",
  "contributions to the field of bird study in the country": ": Some cases of ChatGPT and LLAMA3-70B output in the two datasets. Golden Label is the predefined KGrelations. It is very interesting to note that there are indeed two Americans, both named Robert Ridgely. One is anactor and the other is an ornithologist. So ChatGPT and LLAMA3-70B both correctly complete the relationshipbetween the two entities. Meanwhile, they also have the same problem: providing \"redundant correct information\"."
}