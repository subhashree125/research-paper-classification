{
  "Abstract": "Rotary Position Embedding (RoPE) is an effi-cient position encoding approach and is widelyutilized in numerous large language models(LLMs). Recently, a lot of methods have beenput forward to further expand the context win-dow based on RoPE. The core concept of thosemethods is to predefine or search for a set offactors to rescale the base frequencies of RoPE.Nevertheless, it is quite a challenge for exist-ing methods to predefine an optimal factor dueto the exponential search space. In view ofthis, we introduce PSC (Phase Shift Calibra-tion), a small module for calibrating the fre-quencies predefined by existing methods. Withthe employment of PSC, we demonstrate thatmany existing methods can be further enhanced,like PI, YaRN, and LongRoPE. We conductedextensive experiments across multiple modelsand tasks. The results demonstrate that (1)when PSC is enabled, the comparative reduc-tions in perplexity increase as the context win-dow size is varied from 16k, to 32k, and upto 64k. (2) Our approach is broadly applica-ble and exhibits robustness across a variety ofmodels and tasks. The code can be found at",
  "Introduction": "Large-scale language models (LLMs) have shownimpressive results across a variety of natural lan-guage processing (NLP) applications. For instance,OpenAI has shown that GPT-4 (OpenAI, 2023) canperform at a level comparable to humans in a rangeof professional tasks. Additionally, open-sourcemodels such as LLaMA2 (Touvron et al., 2023b)and Mistral (Jiang et al., 2023) have made signifi-cant contributions to the advancement and practicalapplication of LLMs in both research and indus-try. However, one significant challenge that LLMsface is handling tasks that require processing longcontext, such as responding to questions based onmultiple documents and summarizing lengthy texts such as books. In these scenarios, the perplexityof the responses can increase substantially, leadingto a notable decrease in the performance of LLMs.Therefore, equipping LLMs with long-range abilityhas become a critical and pressing issue for bothacademic and commercial sectors.An intuitive method is to fine-tune a pre-trainedTransformer with a longer context length. Nev-ertheless, there are two limitations: first, modelstrained in this manner adapt to long context lengthsvery slowly (Chen et al., 2023b); second, fine-tuning updates all model parameters is memory-inefficient which prevents the model from adaptingto a large context length (Chen et al., 2023c).Optimizing position encodings is another ma-jor direction for extending the context window ofLLMs (Jin et al., 2024). The original Transformer(Vaswani et al., 2017) that serves as the core com-ponent of LLMs uses sinusoidal functions of vari-ous frequencies to enhance the models extrapolatecapability. It could be regarded as an absolute po-sition encoding mechanism. Since then, relativepositional encoding techniques such as RoPE (Suet al., 2021) and ALiBi (Press et al., 2022) havefurther increased the length extrapolation of Trans-formers. Despite the effectiveness, many existingpre-trained LLMs that use these positional encod-ing methods exhibit weak extrapolation capabilities.For example, LLaMA (Touvron et al., 2023a) with2048 predefined context size explodes perplexitymetric when the input texts length is larger than4096 (Chen et al., 2023b).Recently, new positional encoding schemes havebeen proposed to overcome such limitations. (Chenet al., 2023b) and (kaiokendev, 2023) show thatthe effective context size could be extended bymodifying RoPE via Position Interpolation, whichhas a much smaller upper bound than the extrap-olated method and is more stable (Chen et al.,2023b).Neural Tangent Kernel (NTK) theoryshows that its difficult for multilayer perceptron (MLP) to learn high-frequency information in alow-dimensional domain. Therefore, NTK-basedmethods take the high-frequency information intoaccount (block97, 2023b,a; emozilla, 2023). Fur-thermore, YaRN hypothesizes that previous meth-ods lead to a closer embedding distribution andremedy the issue by using different interpolatingschemes at different frequencies (Peng et al., 2023).The shared characteristic of previous methods isthat they utilize predefined frequency rescale fac-tors. Some algorithms leverage optimal methods toestimate optimal frequencies directly, such as Lon-gRoPE (Ding et al., 2024) and CLEX (Chen et al.,2023a). However, due to the exponential searchspace complexity, it is challenging for those meth-ods to estimate an optimal frequency; they alsoneed heavy searching cost, for instance, it costsLongRoPE nearly 3 days to search an optimal fre-quency for a 256k context window using an A100GPU. While existing techniques for encoding posi-tional information are adept at handling long-rangedependencies, they often depend on fixed patternsor necessitate extensive searches within large pa-rameter spaces.As a result, adapter-based ap-proaches such as LoRA (Hu et al., 2022) have beenutilized to further enhance performance. Nonethe-less, these methods still face limitations, primarilydue to the low rank of the adapter weights (Bi-derman et al., 2024) and the inherently high-ranknature of long-context tasks. In this work, we introduce Phase Shift Calibra-tion to assist position encoding methods to improvetheir long-range capabilities. The main idea is thatwe propose a module to calibrate the predefinedfrequency to approximate the optimal frequency.To this end, we first present that there is a rotarytransformation between the actual frequencies andthe optimal frequencies. The transformation canbe represented as a block diagonal matrix. It isfull-rank if the predefined frequencies are far fromthe optimal ones. Hence, it is challenging for low-rank adapter methods such as LoRA to learn thetransformation. To remedy this issue, we introducea calibration module into the base model, whichapproximates the rotary transformation matrix andhelps calibrate the predefined frequencies to theideal position. We conduct extensive experimentsacross different LLMs, position encoding schemes,and various long-context tasks. The results demon-strate the effectiveness of our methods.",
  "Preliminaries and Related Work": "Rotary Position Embedding (RoPE).Trans-former models leverage positional information toexploit the order of tokens within texts. In ourresearch, we concentrate on Rotary Position Em-bedding (RoPE) (Su et al., 2021) and its deriva-tives. RoPE acts as the positional encoding tech-nique used across various Large Language Mod-els (LLMs), such as the LLaMA (Touvron et al.,2023a) and the Mistral model (Jiang et al., 2023).Given a sequence of N word embeddings {xi}Ni=1,where xi is a d-dimensional vector and d is the di-mension of the embedding. RoPE applies a rotarytransformation to each query/key embedding in apairwise manner. Take d = 2 for example, RoPEconverts each vector into the query vector and keyvector via a transformation in a complex space:",
  "so": "Low-rank Adaption.LoRA (Hu et al., 2022)posits that the weight adjustments in pre-trainedmodels are characterized by a low intrinsic rankduring adaptation. Given a pre-trained weight ma-trix W Rdk, it is updated with a low-rankdecomposition W + W = W + BA, whereB Rdr, A Rrk, and r min(d, k). Dur-ing training, W remains fixed, while A and B aretrainable.",
  "Phase Shift": "Let denote the optimal frequency for long con-text extension of a large language model, thefrequency predefined or estimated by some algo-rithms, such as PI or LongRoPE. It is challengingto predefine a frequency that is exactly equal to due to the exponential search space. The sub-optimal frequencies cause the sin / cos values tomove out of the ideal position, as shown in . As a result, there exists a rotary transformationbetween the ideal position encoded embeddingsand the actual embeddings:",
  "Let W = Rd,mWq and I denote the identity ma-": "trix, then fq = RW = W + ( R I)W. Whenlow-rank adapter methods such as LoRA are em-ployed to finetune the model, we need to utilizetwo low-rank matrices A and B to approximatethe additional matrix. Specifically, BALoRA ( R I)W.Approximating the matrix becomes difficult ifthe pre-established frequencies are not ideal. Forinstance, if none of the pre-established frequen-cies are optimal, then R I becomes a matrix offull rank since it is a block diagonal matrix withall non-zero elements, while BA remains a low-rank matrix. The accuracy of the LoRA weightapproximation may be compromised due to thisdiscrepancy in rank. Moreover, even if only a sin-gle frequency is suboptimal, the rank of RI doesnot become a small number. Taking LLaMA-2 asan example, each layer of LLaMA-2 contains 32attention heads. If there is only one suboptimalfrequency, the rank of R I could reach 32. Incontrast, the LoRA method typically utilizes a low-rank matrix with a rank that does not exceed 16 inpractical applications.Beyond the matter of rank inconsistency, thediversity in the distribution of frequencies, initialphases, and the norms of the embeddings leadsto a sophisticated mapping among attention lay-ers, thereby increasing the complexity of the fine-tuning procedure.",
  "Phase Shift Calibration (PSC)": "Drawing inspiration from the ResNet (He et al.,2016) in the field of computer vision, we pro-pose a phase shift calibration module to tacklethis issue. demonstrates the key com-ponents of our approach. We posit that the em- bedding can be divided into two components: oneis the base embedding, which LoRA can effec-tively learn; and the other is shift embedding, whichshould be acquired separately. This shift embed-ding arises from the phase shift discussed in thepreceding section. To be specific, fq (xm, m) fq (P(xm) xm + xm, m), where P presents atwo-layer Multilayer Perceptron (MLP) composedof a learnable block diagonal matrix and is theelement-wise production.In practice, since the frequencies of RoPE areorganized block-wise instead of pair-wise (Wolfet al., 2020), we hence design a head-wise blockdiagonal matrix. More specifically:",
  "P(x) = 2 (W2 (1 (W1x))) ,(13)": "where W1 and W2 are block diagonal matriceswith each block size Rdhdh, and dh is the sizeof single head dimension. For LLaMA and Mis-tral model, dh = 128, our approach incorporatesonly a small set of parameters (< 1%), thereforeit is parameter efficient. 1 and 2 are activationfunctions, we set 1 = SiLU and 2 = 1 2Tanh.There could be two forms of phase shift calibra-tion according to its position: (1) pre-calibrationwith the form fq(P(xm) xm + xm, m) appliesphase shift calibration before the position encod-ing module; (2) post-calibration which form isPfq(xm, m)+ 1 fq(xm, m) applies phaseshift calibration after the position encoding module.In the experimental section, we will compare thetwo forms, and the results show that the positioningof the calibration mechanism affects performancedistinctly. Additionally, our approach is remark-ably straightforward to implement. Algorithm 1shows the Pytorch-like style of our method.",
  "Experimental Settings": "Model.We conduct experiments on LLaMA-2 and Mistral with various position encode ap-proaches. In addition, we assess our approachby utilizing several well-known publicly availablemodels, including Together.ai (Together.ai, 2023),CodeLlama (Rozire et al., 2023), and LongLoRA(Chen et al., 2023c). Datasets.In order to comprehensively and metic-ulously analyze our technique, we employ severaldatasets to train and assess our context-extendedmodel. We initially carry out experiments by utiliz-ing a small dataset sampled from the RedPajama(Computer, 2023) dataset, and the length of eachtext in the sampled dataset is greater than 4K. Wealso utilize the PG19 (Rae et al., 2020) train splitdataset chunked into 64k segments for training.While conducting the evaluation, we use the PG19validation split and the Proof-pile (Azerbayev et al.,2022) test split. Details are shown in the appendix.",
  "sliding window method from (Press et al., 2022)with S=256 is adopted": "We initially present the evaluation results on theLLaMA-2 model and its context window exten-sions using various approaches in , , and . We extend LLaMA-2 with diverseposition encoding schemes such as PI, YaRN, andLongRoPE. When fine-tuning, we employ LoRAwith a rank of 8. We can notice that the fine-tunedmodels show lower perplexity than the non-fine-tuned ones. Phase shift calibration can enhance allthe base position encoding schemes. It even booststhe performance of the optimal-based method Lon-gRoPE. The possible reason might be that the scalefactor search space is exponential, which makes itdifficult to search for an ideal frequency, and theobjective signal may be too sparse as only 5 PG19texts are used to guide the search. More signifi-cantly, by comparing , , and , we can notice that the advantage of applyingphase shift calibration becomes greater as the ex-tended context window changes from 16k to 64k.The reason perhaps is that as the context windowincreases, the largest possible rescale factor alsoincreases. In other words, the frequency solutionspaces are enlarged, which makes it even more dif-ficult to predefine an ideal frequency. With phaseshift calibration, the frequencies are pre-calibratedto an ideal position. We also incorporate the phase shift calibrationmodule into several well-known publicly availablemodels, like Together.ai, CodeLlama, and Lon-gLoRA. We fine-tune the enhanced model usingthe PG19 dataset and assess it on the Proof-piledataset. presents the outcomes, and wecan note that phase shift calibration enhances Lon-gLora and YaRN more prominently than it does forTogether and CodeLlama. This may be due to thatthe Together and CodeLlama are pre-trained andfine-tuned with full parameter updates, while theremaining ones utilize the LoRA-like method. 0k4k8k12k16k20k24k28k32k36k Prompt Length (Tokens) 0% 20% 40% 60% 80% 100%",
  "Retrieval Accuracy": "LLaMA-2 7BLLaMA-2 7B (extended YaRN)LLaMA-2 7B (PI FT)LLaMA-2 7B (PI PSC)LLaMA-2 7B (YaRN FT)LLaMA-2 7B (YaRN PSC) : A comparison of passkey retrieval accuracyfor context-augmented Large Language Models (LLMs).\"Extend YaRN\" indicates that the model incorporatesYaRN without undergoing fine-tuning. \"FT\" denotesthat the models have been fine-tuned using LoRA (r=8),while \"PSC\" signifies that the models have been fine-tuned with the phase shift calibration module activated.(The graphs for LLaMA-2 7B (PI PSC) and LLaMA-2(YaRN PSC) coincide as they exhibit the same results:with 100% accuracy up to 34k.) Passkey Retrieval.The passkey retrieval taskproposed by (Mohtashami and Jaggi, 2023) gaugesa models effective context window size. This taskaims to require a model to fetch a simple passkeyfrom a large set of useless tokens. In our assess-ment, we conduct 10 iterations of the passkey re-trieval task with the context window sizes rangingfrom 2k to 36k. The random passkey is positionedat a random location that is uniformly distributedamong the collection of the tokens. The prompttemplate is presented in the appendix.The comparison of retrieval accuracy with vari-ous approaches is presented in . We can no-tice that the accuracy of the LLaMA-2 base modeldrops instantly to 0 when the sequence length goesbeyond its pre-trained context window length. Al-though extending the context window using YaRNwithout fine-tuning can raise the accuracy beyondthe 4k pre-trained context size, the accuracy islower and the performance is less stable compared",
  ": Sliding window perplexity (S=256) of ten 128k Proof-pile documents over various models": "to the fine-tuning-based models. With fine-tuning,position encoding methods such as PI and YaRNcan significantly enhance the retrieval accuracy.However, the accuracy becomes unstable as theevaluated context length gets closer to the contextwindow size. For example, at 32k, the accuracy ofLLaMA-2 7B (YaRN FT) drops to 90%, while at33k, the accuracy of LLaMA-2 7B (PI FT) drops to90%. Both LLaMA-2 7B (PI PSC) and LLaMA-27B (YaRN PSC) show a 100% retrieval accuracyup to a 34k context length when the phase shiftcalibration module is enabled. Standard BenchmarksWe assess differentmethods in comparison with the original LLaMA-2 model by using the Hugging Face Open LLMLeaderboard (Face, 2023). Specifically, the Lan-guage Model Evaluation Harness library (Gao et al.,2023) is utilized to carry out the evaluation. We em-ploy 25-shot ARC-Challenge (Clark et al., 2018),10-shot HellaSwag (Zellers et al., 2019), 5-shotMMLU (Hendrycks et al., 2021), and 0-shot Truth-fulQA (Lin et al., 2021).The experiments aim to assess the degradationof model performance along with the context-extended window.We compare different mod-els equipped with the phase shift calibration mod-ule with the relevant baselines and the originalLLaMA-2 model.The results are summarizedin .We can notice that models armedwith the phase shift calibration show compara-ble performance to the related baselines.PSCcan even outperform the related baselines. Forinstance, LongLoRAPSC outperforms LongLoRAon all datasets, TogetherPSC attains the second-bestperformance on the MMLU dataset. Even more no-table, YaRNPSC even achieves the best performanceon the TruthfulQA dataset, with the accuracy per-",
  "formance increased by 0.85%": "Long Context BenchmarksWe also evaluatedour method using the L-Eval benchmarks (An et al.,2024). L-Eval is a comprehensive evaluation suitedesigned to assess long-context language modelsacross multiple sub-tasks. Our experiments wereperformed on the Llama2-7B model, utilizing thePI method both with and without PSC. The resultsare detailed in . From these results, weobserve that enabling PSC contributes to an im-provement in the average L-Eval score at both 16kand 64k contexts. However, the average score at64k is noted to be lower than at 16k. This discrep-ancy may be attributed to the increase in perplexityas the context window expands. Additionally, theaverage length of many datasets is shorter than 16k,which could influence the performance at 64k.",
  "Ablation Study": "In this section, we present ablation studies on thephase shift calibration modules. We aim to addressthe following questions: (1) Since the phase shiftcalibration module introduces a few additional pa-rameters, can a LoRA with a large rank outperformthe PSC module? (2) What is the effectivenessof the phase shift calibration module at differentpositions of the base model? (3) What is the perfor-mance of the phase shift calibration with respect tothe number of fine-tuning steps? More Parameters.We fine-tune the base modelwith different ranks and position encoding meth-ods and assess the performances. The results arepresented in . Several discoveries are ap-parent. First, with phase shift calibration, we canobtain stable improvements across various tokenlengths at different ranks. Second, increasing therank size of LoRA leads to almost no performance",
  "Pre-calibration vs Post-calibration.We evalu-ate the effectiveness of the phase shift calibrationmodule at different positions. In this experiment,we only update the parameters of the PSC while": "keeping the other parameters frozen. The resultsare summarized in . We have several keyfindings with these results. First, by comparing itwith , we can observe that the phase shiftcalibration itself can improve the perplexity of themodels. When combined with LoRA, it can furtherenhance the performance. Second, applying thephase shift calibration before the position encodingmethod is better than applying it after the positionencoding method. The possible reason is that theposition encoding method introduces complex non-linear distortion to the query/key embeddings.",
  ": Computational overhead with the PI positionencoding": "Ablation on Fine-tuning Steps.We present therelationship between perplexity and fine-tuningsteps for the Mistral-7B model extended to a 32Kcontext window on the Proof-pile test set. As Fig-ure 4 indicates, the perplexity drops rapidly to 2.15at step 500, and then gradually converges to 2.11at step 2000. Further fine-tuning the model fromstep 2000 does not lead to any further improvement.Thus, a stopping criterion can be implemented toconserve computational resources. Calculating theperplexity for the entire dataset is computationallyexpensive. Instead, we might opt to sample a sub-set of documents to approximate the perplexity,using this estimation as our stopping criterion. Ad-ditionally, setting a baseline number of steps andapplying the stopping criterion only after surpass-ing this baseline can further alleviate the computa-tional burden linked with perplexity calculations.For models tailored to specific domains, employingdomain-specific metrics as stopping criteria canbe a judicious approach, offering a more preciseevaluation of the models effectiveness within thatparticular context.",
  "Conclusion": "In this work, we present PSC: Phase Shift Calibra-tion, an approach for calibrating the existing ex-tended position encoding methods. We first presentthat there is a rank inconsistency issue when the pre-defined frequencies are not optimal. A phase shiftcalibration module is designed to remedy this issue.We conduct extensive experiments on various tasks,and the results show that PSC is compatible withvarious context extension methods, including in-terpolation, mixing of interpolation/extrapolation,and search-based techniques. With PSC, the long-range abilities of LLMs can be further enhanced.Moreover, our method only introduces a few moreparameters (< 1%), which is parameter-efficient.This work thus supports many natural language pro-cessing tasks that require long-range capabilities.We discuss several promising future works in theappendix.",
  "Limitations": "This paper introduces a phase shift calibration mod-ule to the base model to further enhance the per-formance of existing position encoding methods.Since the introduced phase shift calibration modulecontains a small set of trainable parameters, ourmethod requires fine-tuning of the enhanced mod-els and needs a bit more GPU memory than simplyfine-tuning with LoRA.",
  "Tri Dao. 2023. FlashAttention-2: Faster attention withbetter parallelism and work partitioning": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,and Christopher R. 2022. FlashAttention: Fast andmemory-efficient exact attention with IO-awareness.In Advances in Neural Information Processing Sys-tems. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,and Mao Yang. 2024.Longrope: Extending llmcontext window beyond 2 million tokens. Preprint,arXiv:2402.13753.",
  "Hugging Face. 2023. Open llm leaderboard": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep Residual Learning for Image Recog-nition. In Proceedings of 2016 IEEE Conference onComputer Vision and Pattern Recognition, CVPR16, pages 770778.",
  "Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Xiaodong Song, andJacob Steinhardt. 2021. Measuring massive multitasklanguage understanding. ICLR, abs/2009.03300": "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-rank adaptation oflarge language models. In International Conferenceon Learning Representations. Albert Qiaochu Jiang, Alexandre Sablayrolles, ArthurMensch, Chris Bamford, Devendra Singh Chap-lot, Diego de Las Casas, Florian Bressand, Gi-anna Lengyel, Guillaume Lample, Lucile Saulnier,Lelio Renard Lavaud, Marie-Anne Lachaux, PierreStock, Teven Le Scao, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2023. Mis-tral 7b. ArXiv, abs/2310.06825.",
  "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,and Timothy P. Lillicrap. 2020. Compressive trans-formers for long-range sequence modelling. ICLR": "Baptiste Rozire, Jonas Gehring, Fabian Gloeckle,Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, I. Evtimov, Joanna Bitton, Manish PBhatt, Cristian Cantn Ferrer, Aaron Grattafiori, Wen-han Xiong, Alexandre Defossez, Jade Copet, FaisalAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,Thomas Scialom, and Gabriel Synnaeve. 2023. Codellama: Open foundation models for code. ArXiv,abs/2308.12950.",
  "Together.ai. 2023. Llama-2-7b-32k": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. ArXiv. Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava,Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-tian Cantn Ferrer, Moya Chen, Guillem Cucurull,David Esiobu, Jude Fernandes, Jeremy Fu, WenyinFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,Naman Goyal, Anthony S. Hartshorn, Saghar Hos-seini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, YinghaiLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, KalyanSaladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, R. Subramanian, Xia Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023b. Llama 2: Open foundationand fine-tuned chat models. ArXiv, abs/2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, pages 59986008.",
  "A.1Settings": "Training.For training, we employ the AdamWoptimizer (Loshchilov and Hutter, 2019) with 1 =0.9 and 2 = 0.95. We utilize a learning rate of2104 when training on the sampled RedPajamadataset, and 2 105 otherwise. The weight decayis set to zero, and a linear warmup of 20 steps isapplied. All experiments are conducted using theTransformers (Wolf et al., 2020) framework, andFlash Attention 2 (Dao et al., 2022; Dao, 2023)is utilized to optimize memory usage. For a faircomparison, all models are trained for 3000 stepson 4 A800 GPUs. We set the batch size to thevalue that maximizes GPU memory utilization andadopt a gradient accumulation step size of 4. Whentraining LongRoPE, we add three additional rescalefactors corresponding to PI, NTK, and YaRN to theinitial population. Evaluation.When training our model with theRedPajama dataset, we evaluate our method by us-ing the PG19 validation split. We pick 10 randomsamples from the PG19 validation split with at least96k tokens. When we train our model on the PG19train split dataset chunked into 64k segments, weevaluate the model using the Proof-pile (Azerbayevet al., 2022) test split. Likewise, we select 10 ran-dom samples from Proof-pile with at least 128ktokens. Passkey prompt.To measure the effective con-text window size, we utilize the prompt employedby existing literature (Mohtashami and Jaggi, 2023;Chen et al., 2023c; Ding et al., 2024). The promptis shown as follows:",
  "Passkey prompt": "There is an important info hidden inside alot of irrelevant text. Find it and memorizethem.I will quiz you about the importantinformation there.The grass is green. The sky is blue. Thesun is yellow. Here we go. There and backagain. (repeat M times)The pass key is <PASS KEY>. Rememberit. <PASS KEY> is the pass key.The grass is green. The sky is blue. Thesun is yellow. Here we go. There and backagain. (repeat N times)What is the pass key? The pass key is",
  "A.2More Experiments": "Mistral 7B.We also extend the Mistral 7B v0.1model (Jiang et al., 2023), which is another famousopen-source model. We extend Mistral with YaRN(Peng et al., 2023) to 32k and perform an ablationstudy on the phase shift calibration module. Fortraining, we use a small dataset sampled from theRedPajama (Computer, 2023) dataset with tokenlength 4k. we utilize a constant learning rate2 104 with a linear warmup of 20 steps. Wefine-tune the models for 3000 steps. We evalu-ate the models using Proof-pile (Azerbayev et al.,2022) test split and 10 documents with token length 128k are sampled. The results are describedin . We can observe that with the phaseshift calibration module enabled, the performanceof long-range abilities gets further improved uponYaRN. LLaMA-2 13B.In addition, we assess our ap-proach on the LLaMA-2 13B model (Touvronet al., 2023b). The models are fine-tuned withsampled documents from RedPajama (Computer,2023) dataset. Each document has token length 4k. We set the learning rate as 2 104 and usea linear warmup of 20 steps. Both PI (Chen et al.,2023b) and YaRN (Peng et al., 2023) are employedin our evaluation. shows the results. Theresults exhibit similar performance improvementas the evaluations on the LLaMA-2 7B model. Itdemonstrates our method is compatible with vari-ous LLMs and position encoding approaches.",
  "A.3Initial Phase and Norm distribution": "The RoPE and its extensions consider each pair(x, y) in the embeddings as a complex number. Andperform a rotary transformation on each pair. Duethe complicated distribution of (x, y), it is challeng-ing to predefine a set of frequencies to conduct therotary transforms. We show the initial phase andnorm distributions of some sampled (x, y) pairsfrom different layers and heads in , Fig-ure 7, , , which have complicateddistributions of phase and norm."
}