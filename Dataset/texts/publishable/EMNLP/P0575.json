{
  "Abstract": "Large language models optimized with tech-niques like RLHF have achieved good align-ment in being helpful and harmless. However,post-alignment, these language models oftenexhibit overconfidence, where the expressedconfidence does not accurately calibrate withtheir correctness rate. In this paper, we de-compose the language model confidence intothe Uncertainty about the question and the Fi-delity to the answer generated by languagemodels. Then, we propose a plug-and-playmethod, UF Calibration, to estimate the con-fidence of language models. Our method hasshown good calibration performance by con-ducting experiments with 6 RLHF-LMs on fourMCQA datasets. Moreover, we propose twonovel metrics, IPR and CE, to evaluate the cal-ibration of the model, and we have conducteda detailed discussion on Truly Well-CalibratedConfidence for large language models. Ourmethod could serve as a strong baseline, andwe hope that this work will provide some in-sights into the model confidence calibration.",
  "Introduction": "Large language models (LLMs) acquire vast worldknowledge and demonstrate powerful capabilitiesthrough pre-training (Brown et al., 2020; OpenAI,2023; Bubeck et al., 2023; Sun et al., 2024). Withtechnologies like RLHF (Ouyang et al., 2022) andRLAIF (Bai et al., 2022; Lee et al., 2023), largelanguage models can become more helpful andharmless to align with human preferences (Askellet al., 2021). However, how to build a more honestsystem has not yet been fully discussed. An honestmodel should have a certain understanding of theboundary of its knowledge, that is, knowing what itdoes not know (Yin et al., 2023; Yang et al., 2023b;Zhou et al., 2024). A plausible method is utilizing",
  "* Work done during internship at Meituan. Corresponding author": "the calibrated confidence to estimate the knowl-edge boundary of language models. For pre-trainedlanguage models, the per-token logit can alreadybe considered a well-calibrated confidence score,which implies that pre-trained language models(mostly) know what they know (Kadavath et al.,2022).However, recent studies have indicated thatlanguage models optimized with techniques likeRLHF will exhibit issues of overconfidence (Linet al., 2022a; Kadavath et al., 2022; OpenAI, 2023;He et al., 2023; Zhao et al., 2023; Tian et al., 2023;Xiong et al., 2023). This issue could be reflectedin Multiple-Choice Question Answering (MCQA)tasks, where the probability of RLHF-LMs gen-erating a token and the likelihood of that tokenbeing the correct answer are not well-calibrated.For example, an answer provided by RLHF-LMswith 95% confidence does not mean that there isa 95% probability that the answer is correct. Thisphenomenon may be due to the optimization objec-tive of RLHF, which is to make the model generateresponses aligned with human preferences ratherthan fitting answers that appear more frequently inthe corpus during the pre-training stage.To alleviate the issue of miscalibration, previouswork focuses on two perspectives: the logit-basedmethod and the verbalization-based method. Logit-based methods are usually post-hoc.We needto find a higher temperature (usually above 2.0),known as Temperature-Tuning (Guo et al., 2017),to make the distribution of the models token logitsmoother for mitigating overconfidence (Kadavathet al., 2022; He et al., 2023). The verbalization-based method usually requires prompt engineeringto elicit the models confidence, and it also necessi-tates the model to have strong Self-Awareness (Linet al., 2022a; Tian et al., 2023; Yin et al., 2023). Ag-gregating the models logit-based and verbalization-based confidence can also calibrate the model con-fidence to some extent (Xiong et al., 2023).",
  ": In four different MCQA datasets, our methodhas demonstrated good calibration effects, meaning it issufficiently close to the y = x curve. The experimentaldata is derived from GPT-3.5-Turbo": "As shown in and Appendix Tabel 6,by replacing the models answer with All otheroptions are wrong., we can assess whether themodel had high fidelity to its previously given an-swer. Inspired by this phenomenon, we decomposethe language model confidence into two dimen-sions: the Uncertainty about the question and theFidelity to the answer generated by language mod-els. First, if the answers provided by languagemodel are consistent under multiple samplings, itindicates that language model has lower uncertaintyregarding that question. Thus, we could utilizethe information entropy of the frequency distribu-tion of sampled answers to calculate the modelsuncertainty about a question. Second, we designa novel method to estimate the models fidelityto each of its sampled answers. Last, the uncer-tainty regarding question Q and the fidelity to theanswer ai together determine the models confi-dence. As shown in , our proposed UFCalibration achieved good calibration across differ-ent MCQA datasets. Meanwhile, UF Calibrationdoes not require knowledge of the models per-token log-probability, making it broadly applicableto various Black-box RLHF-LMs, which do notprovide the per-token log-probability.To have a closer look at the calibration of modelconfidence, we propose two novel metrics for evalu-ating and observation: 1) Inverse Pair Ratio (IPR),which is the proportion of inverse pairs in the Reli-ability Diagram. This metric could reflect whetherthe model is well-calibrated from the perspectiveof the monotonicity of the Reliability Diagram. Ifthe reliability diagram is monotonic, it indicatesthat the average accuracy of low-confidence an- swers is always lower than that of high-confidenceanswers. 2) As shown in , we find thatas the number of model parameters increases, lan-guage models still tend to consistently express un-certainty within certain fixed ranges. Thus, wedesign the Confidence Evenness (CE) to observeto the uniformity of the density of each bar in thereliability diagram. Our experimental results indi-cate that, after calibration, even within the samedataset, there is a significant difference in the confi-dence of the answers provided by language modelsfor different questions. We summarize our maincontributions as follows:",
  "|A|j=1 exp(logitaj/t),(1)": "where t is the sampling temperature of languagemodels and |A| is the size of candidate answer setA. Recent studies indicate that good calibrationcan be achieved by adjusting the temperature ofRLHF-LMs (Kadavath et al., 2022; He et al., 2023).However, temperature-scaling (Guo et al., 2017)often requires higher temperatures, such as above2.0 (Kadavath et al., 2022), which might cause theoutputs of the language models to become too ran-dom. When the probabilities for model-generatedtokens are inaccessible, a straightforward solution",
  "LLM": ": If the models choice of answer changes afterreplacing the content of its previous selected option withAll other options are wrong, it could be consideredthat the models fidelity to its previous answer is nothigh enough. is to deploy sampling and use the frequency of thesampled result to estimate the probability of gen-erating this token. For instance, given a questionQ, we could sample K times to acquire a set ofanswers A containing N distinct answers, and eachanswer with an associated frequency ni. The prob-ability of the model generating answer ai can beestimated by ni",
  "Verbalization-based Method": "However, some commercial models, such as Chat-GPT and Claude, usually do not provide per-token logits.Benefiting from instruction fine-tuning(Chung et al., 2022; Zhang et al., 2023),language models could generate responses corre-sponding to the input instructions. Another intu-itive method is to prompt large language modelsto provide their verbalized confidence along withtheir responses as follows (Jiang et al., 2021; Lin",
  "(Answer, Conf) = LLM(Question),(3)": "This method requires the model to have a strongability to follow instructions and strong self-awareness (know whether it knows something ornot (Yin et al., 2023)). Accordingly, verbalizedconfidence can be a floating-point number between0 and 1, i.e., 0.8. And it can be linguistic ex-pressions, i.e., Almost Certain, About Even,Unlikely. Although this method is quite easyto implement, we find various different LMs al-ways tend to output some fixed high confidenceexpressions, as show in .",
  "Eliciting the Fidelity of Answers": "As shown in , for question Q and a can-didate answer (ai, oi), where the option index isai and the content is oi, we simply replace oi withAll other options are wrong., and then query themodel again. If the model has high fidelity to thepreviously selected answer (ai, oi), it should select(ai, All other options are wrong.) in the subse-quent round of inquiry rather than any other option.If language models select other options, we removethe newly selected option to ensure that there isonly one All other options are wrong in candi-date options. By repeating this process until themodel selects All other options are wrong, wecan establish a hierarchical fidelity chain C, suchas \"ACD\". This implies that when all optionsare available, the model will prefer to select optionA. However, if option A is excluded, the modelwill tend to choose option C, which indicates thatthe models fidelity to option A is not high enough.Accordingly, if the chain C has only one element,",
  "(1-Uncertainty)Fidelity(D)": ": Our proposed UF Calibration, which requires at most two phases to invoke the model. In the Samplingphase, for black-box models, similar to the Sampled method, we need to sample 10 times. For white-box models, asingle invocation is sufficient. In the eliciting the fidelity phase, the model needs to be invoked approximately 2 to 3times to generate a fidelity chain, as show in . such as A, this suggests that the models fidelityto option A is high enough, which can, to a cer-tain extent, reflect the models confidence. Corre-spondingly, for a hierarchical fidelity chain C, weassign a fidelity weight to each element from rightto left. For example, for the ith element di fromthe right, we simply set its weight as i. Therefore,the normalized fidelity of the ith element ai can becalculated as follows:",
  "|C|i=1 i ,(4)": "where we usually set as 2. As shown in ,the answer set A might include multiple differentanswers. Consequently, we sequentially replacethe candidate answer in A with All other optionsare wrong. to elicit different hierarchical fidelitychains, as depicted in the second step of .The fidelity score of each element ai in every hierar-chical fidelity chain Cj can be calculated using (4).Thus, the models fidelity of answer ai can be calcu-lated by the weighted average fidelity score acrossdifferent hierarchical chains. Since the hierarchi-cal fidelity chain is elicited by greedy decoding,the frequency of occurrence of different chains isconsistent with the frequency of occurrence of thefirst element a|C| from left to right. Therefore, thefrequency Psampled(a|C|) can be viewed as a proxyfor the probability Psampled(Cj) of different hierar-chical fidelity chains to calculate the overall fidelity",
  "Uncertainty Estimation": "As shown in .1, through sampling, wecan obtain the frequency of each answer generatedby the model and use it to estimate the genera-tion probability of each answer token. Previousworks (Kadavath et al., 2022; OpenAI, 2023) haverevealed that RLHF-LMs often exhibit overconfi-dence in token generation probability, especiallyin the temperature range we commonly use, suchas between 0 and 1.0. However, these probabili-ties could still reveal, to some extent, the modelsconfidence regarding the current question Q. Forinstance, if the distribution of Psampled is flatter,it indicates that the language model has more sig-nificant uncertainty regarding the question Q. Anintuitive method is calculating the information en-tropy of the distribution Psampled to estimate themodels uncertainty about question Q as follows:",
  "Ours0.1270.0830.75792.8330.0890.0830.90680.3240.1090.0830.92583.8660.0420.0440.76487.515": ": Experimental results derived from GPT-3.5-Turbo and GPT-4-Turbo. For each column in the table, thecloser the color is to blue, the better the calibration. And the closer it is to orange, the worse the performance. Wealso have bolded the best results, and for the second-best results, we have added an underline beneath them.",
  "Experimental Setting": "Dataset.We have conducted experiments onfour MCQA datasets to verify the effectivenessof our proposed confidence estimation method.ARC (Clark et al., 2018) is a dataset of 7,787grade-school-level questions. We use the test splitof the ARC-Challenge with 1,172 questions forour experiments. MMLU (Hendrycks et al., 2021)is a dataset designed to measure knowledge ac-quired during pretraining and covers 57 subjects.",
  "To reduce the cost of API calls, we sampled 1": "8 ofthe data for testing for each subject. Common-SenseQA (Talmor et al., 2019) is a dataset for com-monsense question answering, and we use the vali-dation split with 1,221 questions for experiments.TruthfulQA (Lin et al., 2022b) is a dataset that con-tains 817 questions designed to evaluate languagemodels preference to mimic some human false-hoods. All the experiments are conducted under a0-shot setting. Metrics.We utilize multiple metrics to evaluate.We bin the predictions from the model by their con-fidence and report the ECE (expected calibrationerror). We also report the Brier Score of differentmethods in . In this paper, we also definestwo novel metrics to evaluate the calibration. Thefirst one is IPR (Inverse Pair Ratio), which is usedto measure the monotonicity of the reliability di-agram. If the reliability diagram is monotonic, itindicates that the average accuracy of answers withlow confidence is lower than the average accuracyof answers with high confidence.",
  "C2K,(8)": "where IP is the inverse pair number in the reliablediagram, and K is the bin number with a densitylarger than 0. We found that as the number ofmodel parameters increases, the accuracy of themodel improves across various datasets. However,language models still tend to consistently expressuncertainty within certain fixed ranges, and ECE",
  "In this paper, we adopt 10 equal-size bins to calcu-late ECE10, IPR10 and CE10. We also report theaccuracy on these benchmarks to measure whethercalibration reduces the accuracy": "Baselines.We compared our approach with dif-ferent baselines for eliciting the confidence of lan-guage model. First, we reproduced the Verb andLing method proposed by Tian et al. (2023). TheVerb method involves prompting the model tooutput a floating-point number between 0 and 1to represent its confidence immediately after pro-viding an answer (Tian et al., 2023; Lin et al.,2022a). The Ling method entails having the lan-guage model express its confidence level in naturallanguage (Tian et al., 2023). Since commercialmodels like ChatGPT do not provide per-token log-its, we employed a sampling technique to estimatethe probability of token generation, referred to asthe Sampled method. Unless otherwise specified,the Sampled method involves sampling 10 times.For open-source models like LLaMA2-Chat, we di-rectly use the probability of token generation asthe measure of the language models confidence,which we refer to as the Token method. We alsocompare the Conformal Prediction Baseline pro-posed by Kumar et al. (2023) with our UF calibra-tion in Appendix D.1. All the prompt templates weuse are shown in Appendix E.",
  ") Our proposed method demonstrates a clear im-provement over the various baselines in terms ofthree metrics: ECE10, IPR10, and CE10, whichdemonstrates the effectiveness of our method": "2) The Verb and Ling methods might, to someextent, impair the language models accuracyon multiple-choice question answering tasks,which might be caused by more complicated in-structions. Additionally, since the Ling methodis more complex, it has a greater impact on theoverall accuracy than the Verb method. 3) Similar to the conclusion from Tian et al. (2023),the calibration of the Verb method tends to bebetter than that of the Ling method. This is be-cause the linguistic expressions used in the Lingmethod are based on human psychology. How-ever, the confidence represented by the sameexpression may have a gap between humans andmodels and among different models and differ-ent sentences might mean the same thing (Kuhnet al., 2023). 4) The CE10 of the Verbalization-based Methodis relatively low, which suggests that languagemodels tends to prefer outputting expressions ofcertain confidence, such as Highly Likely,0.8 and 0.9. This phenomenon can also ex-plain why the ECE10 of the Verbalization-basedMethod improves when the overall average ac-curacy of the model is between 70-90%. : Our proposed method achieved well-calibrated results across all temperatures. The experimental resultsare derived from LLaMA2-13B-Chat. The results from Baichuan2-13B-Chat are presented in Appendix .",
  "Ablation Study": "As shown in , removing Uncertainty andonly relying on Fidelity to estimate the modelsconfidence, we can also achieve comparativelybetter calibration than other methods. This phe-nomenon indicates that our proposed method re-flects the language models Fidelity to its an-swers very well. Meanwhile, it is difficult to es-timate the models confidence only depending onUncertainty. As mentioned in 3.3, Uncertaintyis designed for measuring the models uncertaintyregarding the question Q, rather than its confidencefor a particular answer. In the section 3.2, we utilize(4) to calculate the language models normalizedfidelity in a hierarchical fidelity chain, where is a hyper-parameter. The larger the value of ,the lower the estimated fidelity for answers closerto the end of the fidelity chain. Our experimentsin indicate that setting to around 2 is arelatively appropriate choice for the fidelity estima-tion process. If is too large, the ECE10 will alsoincrease, which will cause the issue of overconfi-dence of our estimated confidence.",
  "ducted a detailed discussion of a research question:What kind of Confidence is Truly Well-Calibrated?": "Temperature-ScalingIn the main experiments,we evaluate various methods using a constant tem-perature of 1.0. In this section, we will explorethe influence of sampling temperature on the per-formance of different methods. As illustrated inFigures 4 and 7, our proposed calibration methodconsistently achieves the lowest expected calibra-tion error across all temperatures, showing remark-able robustness to temperature variations. This isbecause, in eliciting model fidelity, our methodalways employs Greedy Decoding rather than Sam-pling. Thus, the hierarchical chains we obtain areusually consistent across different sampling temper-atures. In contrast, the expected calibration error ofLogit-based Methods is usually affected by temper-ature. For the Sampling method with limited sam-pling budgets, the lower the temperature, the moresignificantly the diversity of the sampled results : Reliability diagrams of Baichuan2-13B-Chat on ARC-Challenge. In these diagrams, the darker the color,the higher the density. The reliability diagrams of other models we evaluated are shown in Appendix Figures 1312. will decrease, exacerbating the overconfidence oflanguage models. For the Token Method, the im-pact of temperature on its calibration shows a trendof first increasing and then remaining relativelystable or first increasing and then decreasing.This is because we could directly utilize (1) to es-timate the confidence of each option, and if thetemperature is too low (i.e., 0.1), it will lead to theconfidence of a large number of options approach-ing zero. This phenomenon might contribute toreducing expected calibration error, but it does notnecessarily indicate that the models confidence iswell-calibrated. The Verbalization-based methodis less affected by temperature, which indicatesthat the expressions which language models preferto output are relatively consistent across differenttemperatures. Parameter-ScalingAs shown in , weevaluate the calibration of various methods at dif-ferent parameter scales on the LLaMA2-Chat seriesmodels. Our proposed method exhibits good cali-bration across different amounts of model parame-ters. With the size of model parameters increasing,the calibration of the Verbalization-based methodand the Logit-based method is improving. Thisphenomenon indicates that as the scale of model pa-rameters increases, the models Self-Awareness isimproving. However, the relatively high expectedcalibration error suggests that language models stillhave issues with overconfidence. Truly Well-Calibrated ConfidencePreviouswork mainly evaluates the calibration of languagemodels through ECE. This section will discuss theresearch question: What Kind of Confidence isTruly Well-Calibrated?. demonstrates thecalibration of various methods. From the calibra-tion perspective, we hope that the confidence andaccuracy relationship is close to the curve y = x.Thus, we need to reduce the ECE by calibratingconfidence. Meanwhile, we hope that the reliabil-ity diagram should be as monotonic as possible toensure that the accuracy of the results generated with low confidence is lower than that of the resultswith high confidence. Therefore, we propose theInverse Pair Ratio (IPR) to evaluate monotonicity.From the perspective of building a more honestsystem, we hope the models confidence shouldbe distributed across different confidence intervals.For example, if a language model has an overallaccuracy of 75% on the TruthfulQA dataset andthe confidence of each question from the languagemodel is always 75%, its ECE and IPR would be 0.And we find that different models tend to expressconfidence within a fixed interval. In this case, wethink that the confidence may not necessarily be atruly well-calibrated confidence because we couldnot exclude some low-confidence results based onthe confidence from the language model. Althoughthe prior distribution of the models confidence isunknown, our confidence estimation method findsthat language models have different confidencefor different questions. Thus, we propose a met-ric called Confidence Evenness (CE) to measurewhether the model confidence always is locatedin a fixed interval. We believe ECE, IPR, and CEevaluate calibration from different perspectives andthere is a trade-off between these three metrics. Wesuggest that truly well-calibrated confidence shouldachieve a balance among ECE, IPR, and CE, ratherthan over-optimizing any of them.",
  "Conclusion": "In this paper, we decompose the language modelconfidence into the Uncertainty about the questionand the Fidelity to the answer generated by lan-guage models. Through the decomposition, wepropose a plug-and-play method, UF CALIBRA- TION, to calibrate the confidence of language mod-els. Through experiments with 6 RLHF-LMs on 4multiple-choice question answering benchmarks,our method exhibits good calibration. Besides, wepropose two novel metrics, IPR and CE, to eval-uate the calibration of language models. Finally,we conduct a detailed discussion on Truly Well-",
  "Limitations": "Although our method has shown good calibration,it is mainly applicable to scenarios where the setof answers is known, i.e., multiple-choice questionanswering, text classification, sentiment classifica-tion, and preference labeling in RLHF. Elicitingthe models fidelity in open-ended generation sce-narios is a direction worth exploring. Meanwhile,our method involves multiple invocations of lan-guage models, and how to estimate the probabilitydistribution of tokens generated by the languagemodel with as few callings as possible remains tobe studied.",
  "This work was supported by the National NaturalScience Foundation of China (No. 62441602). Thecomputations in this research were performed usingthe CFFF platform of Fudan University": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, Nelson El-hage, Zac Hatfield-Dodds, Danny Hernandez, Jack-son Kernion, Kamal Ndousse, Catherine Olsson,Dario Amodei, Tom Brown, Jack Clark, Sam Mc-Candlish, Chris Olah, and Jared Kaplan. 2021. Ageneral language assistant as a laboratory for align-ment. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Carol Chen, Catherine Olsson, Christo-pher Olah, Danny Hernandez, Dawn Drain, DeepGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemi Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel R. Bow-man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, andJared Kaplan. 2022. Constitutional ai: Harmlessnessfrom ai feedback.",
  "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind": "Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. In Advancesin Neural Information Processing Systems, vol-ume 33, pages 18771901. Curran Associates, Inc. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,and Yi Zhang. 2023. Sparks of artificial general in-telligence: Early experiments with GPT-4. ArXivpreprint arXiv:2303.12712. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. ArXiv,abs/1803.05457.",
  "Association for Computational Linguistics, 9:962977": "Saurav Kadavath, Tom Conerly, Amanda Askell, TomHenighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, EliTran-Johnson, Scott Johnston, Sheer El-Showk,Andy Jones, Nelson Elhage, Tristan Hume, AnnaChen, Yuntao Bai, Sam Bowman, Stanislav Fort,Deep Ganguli, Danny Hernandez, Josh Jacobson,Jackson Kernion, Shauna Kravec, Liane Lovitt, Ka-mal Ndousse, Catherine Olsson, Sam Ringer, DarioAmodei, Tom Brown, Jack Clark, Nicholas Joseph,Ben Mann, Sam McCandlish, Chris Olah, and JaredKaplan. 2022. Language models (mostly) know whatthey know.",
  "Semantic uncertainty: Linguistic invariances foruncertainty estimation in natural language genera-tion. In The Eleventh International Conference onLearning Representations": "Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu,David Bellamy, Ramesh Raskar, and Andrew Beam.2023.Conformal prediction with large languagemodels for multi-choice question answering. arXivpreprint arXiv:2305.18404. Harrison Lee, Samrat Phatale, Hassan Mansoor, ThomasMesnard, Johan Ferret, Kellie Lu, Colton Bishop,Ethan Hall, Victor Carbune, Abhinav Rastogi, andSushant Prakash. 2023. Rlaif: Scaling reinforcementlearning from human feedback with ai feedback.",
  "OpenAI. 2023. Gpt-4 technical report": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744.Curran Associates, Inc. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,Qinyuan Cheng, Xiangyang Liu, Hang Yan, YunfanShao, Qiong Tang, Shiduo Zhang, Xingjian Zhao,Ke Chen, Yining Zheng, Zhejian Zhou, RuixiaoLi, Jun Zhan, Yunhua Zhou, Linyang Li, XiaoguiYang, Lingling Wu, Zhangyue Yin, Xuanjing Huang,Yu-Gang Jiang, and Xipeng Qiu. 2024. Moss: Anopen conversational large language model. MachineIntelligence Research. Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsenseknowledge. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 41494158, Minneapolis, Minnesota. Asso-ciation for Computational Linguistics. Katherine Tian, Eric Mitchell, Allan Zhou, ArchitSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,and Christopher Manning. 2023. Just ask for cali-bration: Strategies for eliciting calibrated confidencescores from language models fine-tuned with humanfeedback. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 54335442, Singapore. Association for Com-putational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, JieFu, Junxian He, and Bryan Hooi. 2023. Can llmsexpress their uncertainty? an empirical evaluation ofconfidence elicitation in llms": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,Dong Yan, Fan Yang, Fei Deng, Feng Wang, FengLiu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Ji-aming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, MangWang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei-dong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li,Tianyu Li, Wei Cheng, Weipeng Chen, XiangrongZeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, XinYu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li,Youxin Jiang, Yuchen Gao, Yupeng Zhang, ZenanZhou, and Zhiying Wu. 2023a. Baichuan 2: Openlarge-scale language models.",
  "AThe Computation Cost of ElicitingFidelity": "In this section, we will display the average lengthof the fidelity chains for different models acrossvarious datasets in the . Since we deploygreedy decoding during the process of elicitingfidelity,the average length of the fidelity chain isequal to the average number of requests. At thesame time, it should be noted that, when elicitingthe Fidelity Chain, only 1 token needs to be gener-ated. Therefore, the average length of the fidelitychain can also be regarded as the average numberof tokens generated.",
  "Acc.77.35%46.62%": ": We found that if the option chosen by the model in the first round is replaced with \"All other options arewrong,\" the model then chooses \"All other options are wrong\" in the second round. In this case, the accuracy of themodels first-round choice is significantly higher compared to when it chooses other options in the second round.The results are derived from TruthfulQA.",
  "CWhy could CE be used as a metric?": "As mentioned in section 4.2, we found that Lan-guage models tend to prefer outputting expressionsof certain confidence, such as Highly Likely, 0.8,and 0.9. In the table 7, we have counted the oc-currence of different confidence levels for variousmodels on different datasets to demonstrate themodels preference for certain confidence levelswhen using the Verb and Ling method.We also notice that as the model parametersincreased, the accuracy of the model improved,but the language models preference for certainconfidence levels do not change and even becamestronger. Therefore, we introduced the ConfidenceEvenness to assess whether the models confidenceis overly concentrated in certain intervals.Can existing metrics (such as ECE) capture this phenomenon? There is an example: on Common-SenseQA, as the parameters of Llama2-Chat in-creasing, the accuracy rises from 51% to 70%,and the ECE using the Ling method decrease from0.385 to 0.189. But the 70B model shows a strongerpreference for outputting a confidence of 0.9. Fo-cusing solely on the ECE metric cannot fully ob-serve the changes in model preferences. Fortu-nately, this phenomenal could be reflected by theCE metrics. Another extreme case is if models of varyingparameter sizes always output a 0.9 confidencelevel, and as the model size increases, the aver-age accuracy just shifts from 70% to 90%, thenthe ECE would drop to 0. If we only use exist-ing metrics for observation, we might concludethat the model with the largest parameters has thestrongest self-awareness. However, by evaluating the CE metric across different models, we can iden-tify a potential preference in how models expressconfidence. Its ECE becoming 0 might just coin-cidentally be because the average accuracy on acertain dataset equals the confidence level it prefersto output. Therefore, we believe the CE metricprovides a new perspective for observing modelconfidence calibration.Finally, it should be noted that we believe anover-concentration of model confidence in a par-ticular value or interval is not conducive to usingmodel confidence as a simple metric to filter outlow-confidence answers.",
  "D.1Compared with Conformal Prediction": "We reproduce Conformal Prediction for RLHF-LMs (Kumar et al., 2023) in our dataset and set-ting. Specifically, for each dataset, we select 50%samples as the calibration set and the other sam-ples as the test set. We also set the error rate to = 0.1 meaning the prediction answer set has a90% probability of containing the correct answer.We then calculate the conformal scores in the cali-bration set, where the specific calculation formulais Score = 1 max SoftmaxScore. For the testset, we take the 1 quantile of the conformalscores from the calibration set as the threshold q.During the testing stage, for a given sample, it isonly added to the prediction set if its generatedprobability is greater than or equal to 1 q. Foreach sample in the prediction set, we consider itsconfidence to be (1 ) (SoftmaxScore). asshown in the following table 8, our proposed UFCalibration still demonstrates good calibration com-pared to conformal prediction for RLHF-LMs. Itis also important to note that conformal predictionrequires a calibration set to determine a thresholdto build a prediction set. However, our method isa plug-and-play approach that can accurately esti-mate the models confidence without requiring anyprior knowledge.",
  "order the options, obtaining 10 different probabilitydistributions Pi. The temperature of the languagemodel was set to 1.0, and the final probability dis-tribution was P = 1": "1010i=1 Pi.The results are shown in . From the exper-imental results, it can be seen that the performanceof these two methods is comparable. However, itis important to note that CAPE-ENUM requiresknowledge of the logits generated by the modelfor each token. For Black-Box models, multiplesamples are needed to obtain Pi, and the overalltime complexity is O(M K), where K representsthe number of permutations, and M represents thenumber of samples needed to obtain a probabilitydistribution Pi. Moreover, obtaining an accuratePi, usually requires a large M, which also leads toan increase in computational cost.",
  "D.3Candidate-Aware UF Calibration": "For some questions like Which of the followinganswers is better?, after replace some options withAll other options are wrong, the remaining op-tions are still reasonable. For example, Whichof the following animals has the largest volume?.We find that these types of questions may appear inthe ARC-Challenge. To address issues, we proposeCandidate-Aware UF Calibration, which will in-troduce all the candidate answers in the promptwhen utilizing our UF Calibration, even if the cur-rently selectable options are only a subset of these.Therefore, the models prompt template could bechanged to: The question is: [current question].Candidate answers: [all candidate answers]. Fromthe options below, please select the option youagree with the most: [options for this round]. An-swer:. We tested Candidate-Aware UF Calibrationon three Llama2-Chat models. Experimental re-sults from show that Candidate-Aware UFCalibration still demonstrates performance similarto UF Calibration. This also partially validates that\"All of the other options are incorrect\" is a validapproach for quantifying fidelity.",
  ": Comparing calibration results of CAPE (Jiang et al., 2023) and our proposed method": "fidence, while ot represents whether it is the correctanswer (0 indicating an incorrect answer, 1 indicat-ing a correct answer). In , we present theBrier Scores of various baselines and our proposedmethod. It can be seen that our method still exhibitsgood calibration, especially for closed-source mod-els such as GPT-3.5-Turbo, GPT-4 Turbo.",
  "MethodPrompt Template": "Verb (Tian et al., 2023)Provide your best guess and the probability that it is correct(0.0 to 1.0) for the following question. Give ONLY the guess andprobability, no other words or explanation. For example:\\nGuess:<most likely option, without any extra commentary whatsoever;justtheoption>\\nProbability:<theprobabilitybetween0.0and1.0thatyourguessiscorrect,withoutanyextracommentary whatsoever; just the probability!>\\nThe question is:{question}\\nOptions:\\n{choices}Answer: Ling (Tian et al., 2023)Provide your best guess for the following question, and describehow likely it is that your guess is correct as one of the followingexpressions:{EXPRESSION_LIST}.Give ONLY the guess and yourconfidence, no other words or explanation. For example:\\n\\n Guess:<most likely guess, as short as possible; not a complete sentence,justtheguess!>\\nConfidence:<descriptionofconfidence,without any extra commentary whatsoever; just a short phrase!>\\nThe question is: {question}\\n Options:\\n{choices}Answer: SampledProvidetheoptionyouagreewithmostforthefollowingquestion. Give ONLY the option of the answer, no other words orexplanation. For example:\\nAnswer: <most likely option, withoutany extra commentary whatsoever; just the option>\\nThe questionis: {question}\\nOptions:\\n{choices}Answer: TokenProvidetheoptionyouagreewithmostforthefollowingquestion. Give ONLY the option of the answer, no other words orexplanation. For example:\\nAnswer: <most likely option, withoutany extra commentary whatsoever; just the option>\\nThe questionis: {question}\\nOptions:\\n{choices}Answer: OursProvidetheoptionyouagreewithmostforthefollowingquestion. Give ONLY the option of the answer, no other words orexplanation. For example:\\nAnswer: <most likely option, withoutany extra commentary whatsoever; just the option>\\nThe questionis: {question}\\nOptions:\\n{choices}Answer:"
}