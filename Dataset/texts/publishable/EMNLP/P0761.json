{
  "Abstract": "In-Context Learning (ICL) enables large lan-guage models (LLMs) to achieve rapid taskadaptation by learning from demonstrations.With the increase in available context lengthof LLMs, recent experiments have shown thatthe performance of ICL does not necessarilyscale well in many-shot (demonstration) set-tings. We theoretically and experimentally con-firm that the reason lies in more demonstrationsdispersing the model attention from the query,hindering its understanding of key content. In-spired by how humans learn from examples,we propose a training-free method FOCUSICL,which conducts triviality filtering to avoid atten-tion being diverted by unimportant contents attoken-level and operates hierarchical attentionto further ensure sufficient attention towardscurrent query at demonstration-level. We alsodesign an efficient hyperparameter searchingstrategy for FOCUSICL based on model per-plexity of demonstrations. Comprehensive ex-periments validate that FOCUSICL achievesan average performance improvement of 5.2%over vanilla ICL and scales well with many-shot demonstrations.",
  "Introduction": "The rapid development of large language models(LLMs) has facilitated the emergence and enhance-ment of their In-Context Learning (ICL) abilities(Wei et al., 2022a; Dong et al., 2023). As a training-free method, ICL can achieve fast model adapta-tion on specific tasks based on several demonstra-tions prefixed to the query, formally denoted asICL(response|demos, query). Intuitively, moredemonstrations can help LLMs better understandthe task and increase the likelihood of findingdemonstrations that aid in responding queries, thusleading to better performance. Theoretically, a sim-ilar conclusion can be drawn. Previous studies(Dai et al., 2023; Irie et al., 2022; von Oswald",
  ": The average model attention for query is dis-persed by the increased number of demonstrations, caus-ing inadequate understanding of query": "et al., 2023; Akyrek et al., 2023) have theoreti-cally inferred that ICL can be viewed as an implicitfinetuning process, with demonstrations analogousto training samples. On this basis, as finetuninghas been validated to comply with the scaling law(Hernandez et al., 2021) where performance in-creases with the number of training samples, theperformance of ICL should also positively corre-lates with the number of demonstrations, which hasbeen experimentally verified by previous studies(Bertsch et al., 2024; Duan et al., 2023).However, with the increase in available contextlength of LLMs (Reid et al., 2024), some stud-ies (Zhao et al., 2023; Agarwal et al., 2024) ob-serve counterexamples when scaling the demonstra-tion numbers from few-shot to many-shot. Agar-wal et al. (2024) finds that the optimal numberof demonstrations for six out of eleven bench-marks is not the maximum number they have tested.Our experimental results () also indicatethat the model performance might decline withincreased demonstrations when applying ICL, ex-hibiting an inverse-scaling phenomenon (McKen-zie et al., 2023). These findings indicate that LLMsare not stable many-shot learners. To understand this gap, we revisit the derivationof Dai et al. (2023) that formally equates ICL withfinetuning and identify that their approximation ofstandard attention operation as linear attention op-eration will ignore the competition for attentionbetween demonstrations and the query when gen-erating the response. Since this approximation iskey to the equivalence of ICL and finetuning, wehypothesize that the reason why ICL does not ad-here to the scaling law like finetuning is that moredemonstrations can divert attention away from thequery. Inadequate attention and understanding ofthe query can naturally lead to inferior response. Toverify our hypothesis, we first conduct experimentsconfirming that increasing the number of demon-strations does lead to a decrease in model attentiontowards queries (). We further experimentby adding blank spaces within the demonstrationsand confirm that: the more blank spaces added, themore attention towards queries distracted by blanks,resulting in lower response accuracy (). Inspired by the way humans benefit from ignor-ing irrelevant contents and integrating insights frommultiple examples when solving problems, we pro-pose FOCUSICL to avoid the attention dispersionissue faced by ICL. Specifically, at the token-level,FOCUSICL conducts triviality filtering by adap-tively masking unimportant tokens of demonstra-tions based on attention distribution, allocatingthe attention to more important contents. At thedemonstration-level, FOCUSICL performs hierar-chical attention mechanism by dividing demonstra-tions into multiple batches and respectively con-ducting intra-batch and inter-batch attention oper-ations. The limited demonstration number withineach batch ensures sufficient attention to the query,while inter-batch attention integrates the benefitsfrom a larger number of demonstrations. We fur-ther introduce an efficient hyperparameter search-ing strategy for FOCUSICL according to modelperplexity of demonstrations. Our experiments across three LLMs on fivebenchmarks confirm that FOCUSICL achieves anaverage performance improvement of 5.2% overICL by avoiding attention dispersion, with lowerinference overhead. This demonstrates the effec-tiveness, efficiency, and generalizability of FOCU-",
  "SICL. Furthermore, we observe that FOCUSICLachieves performance scaling with the number ofdemonstrations by maintaining attention on criti-cal parts, making demonstration number a possible": "scaling direction for LLM-based AGI. Finally, wepropose a unified perspective to understand the di-vergent phenomena observed in previous studies,where more demonstrations lead to either improved(Bertsch et al., 2024) or deteriorated (Agarwal et al.,2024) performance in ICL. Based on experimen-tal results, we conclude that the performance ofICL initially benefits but subsequently suffers frommore demonstrations. The weaker the model andthe closer the relationship between samples, thelater the sweet spot for the number of demonstra-tions occurs.Our contributions are summarized as follows:",
  "where Sampling() denotes certain samplingstrategy and Cat[] denotes the operation of con-catenation": "Scaling Demonstration NumberDue to restric-tions on context window (2048 4096), earlystudies (Brown et al., 2020; Lu et al., 2022) on ICLare limited to few-shot scenarios where they gener-ally observe gains from more demonstrations. Asthe context window expands recently, counterex-amples occur. Agarwal et al. (2024) finds that thebest performance of Gemini 1.5 Pro is achievedunder settings where demonstration number is notthe maximum one tested in over half of the bench-marks. Zhao et al. (2023) discoveries that increas-ing the number of demonstrations does not nec-",
  "Approximating ICL as Finetuning": "Since Dai et al. (2023) derives that ICL is formallyequivalent to finetuning, with demonstrations anal-ogous to training samples, we decide to revisit theirderivation process below to explore why finetun-ing satisfies scaling laws (Hernandez et al., 2021)while ICL does not. FinetuningLet W 0, W FT Rdoutdin bethe initialized parameter matrix and the update ma-trix, and x Rdin be the input representation. Theoutput of certain linear layer optimized by gradientdescent can be formulated as follows:",
  "x = xW 0 + xW F T(2)": "ICLFor each attention head of M, let hi Rdin be the representation of the ith input to-ken, W q, W k, W v be the projection matricesfor computing the queries, keys and values. Wedenote hidemosW k, hidemosW v, hiqW k,hiqW v as Dk, Dv, Qk, Qv, respectively. Togenerate r, the output of hr can be derived below:",
  "(3)": "Dai et al. (2023) approximate the standard atten-tion to linear attention by removing the softmaxoperation for ease of qualitative analysis. SincehrW qQvQk is the attention result in the zero-shot learning (ZSL) setting and hrW qDvDk isthe extra outcome from demonstrations, they aredenoted as hrW ZSL and hrW ICL respectively.Comparing Eq. (3) with Eq. (2), we can understandICL as finetuning by treating the W ICL gener-ated from demonstrations as the W FT generatedfrom training samples.",
  "j exphrW qQk": "j(6)With the existence of (hr) in Eq. (5), an increasein the number of demonstrations will lead to alarger (hr), thereby decreasing the model atten-tion towards q. At the same time, ICL does notnecessarily adhere to the scaling law as it is nolonger formally equivalent to finetuning. There-fore, we hypothesize that more demonstrationscan divert model attention from the key con-tents (query), leading to possible performancedecrease. Spaces Added per Demonstration 0.0034 0.0036 0.0038 0.0040 0.0042 0.0044 Attention per Token of q Accuracy (%)",
  "Experimental Evidence for Hypothesis": "To validate our hypothesis, we first investigatewhether the model attention towards the query de-creases with the increase of demonstration num-ber. To avoid potentially unreliable results causedby data contamination (Jiang et al., 2024), our ex-ploratory experiments are conducted with longchat-7b-v1.5 (Li et al., 2023a) (32k context window)on the proposed COUNTA benchmark (See detailsin 5.1), which requires the model to Count thenumber of character A in the five candidates. Asshown in , the average attention weight ofmodel towards each token in the query decreases byscaling up the demonstration number, correspond-ing to Eq. (5).We further explore how the models lack of at-tention towards the query affects the quality of theresponse. Specifically, we add several blank spacesat the end of each demonstration. This format main-tains the ICL paradigm and the meaningless blankspaces will not introduce additional information.As shown in , we find that more blankspaces disperse the model attention towards thequery similar to the demonstrations, which in turnleads to a decline in accuracy. Based on the experi-ments above, we have confirmed our hypothesis.",
  "Triviality Filtering": "Humans benefit from selectively ignoring irrele-vant parts (trivialities) of demonstrations to avoidattention dispersion. In contrast, the standard at-tention mechanism of LLMs fails to completelyignore (assign zero attention weight to) trivialitiesand leverage the prior that the tokens of query aregenerally important, for which we propose trivialityfiltering operation. To predict response r for givenquery q, in each attention layer, we first calculatethe attention scores s as follows:",
  "INF, si sindex and i demos0, else": "hr = softmax(s + mask(s)) Cat[Dv; Qv](8)where hr is the outcome of hr. By applying triv-iality filtering operation, useless parts of demon-strations are assigned zero attention weights thusLLMs can focus on leveraging relevant contentsof the demonstrations to solve the current query.To achieve a broad impact, apart from r, we alsoapply triviality filtering operation on tokens be-long to responses of demonstrations by autoregres-sively treating {(qi, ri)}k1i=1 as demonstrations of(qk, rk), k [2, N].",
  ": Input details of FOCUSICL": "amples separately and then integrate the insights toavoid distracting attention by focusing on too manyexamples simultaneously. Motivated by this, we in-troduce hierarchical attention mechanism for LLMsto learn from many-shot demonstrations while fo-cusing on current query. We first split the demon-strations into T batches, where each one comprisesB consecutive demonstrations. Without editingthe token order, we change the position indexes toensure that each batch is logically adjacent to thequery (). To ensure that batches are mutu-ally invisible to each other, we use a mask matrix,allowing us to parallelly apply intra-batch attentionwithin each batch i and query as follows: hir, si = TrivialityFiltering Att(hjbatchiq)(9)By controlling the batch size B, we can ensurethat the model maintains enough attention towardsthe query within each batch. To further integrateinsights from different batches, we conduct inter-batch attention as follows:",
  "Hyperparameter Searching": "To efficiently find suitable values of filtering thresh-old p and batch size B for different LLMs andtasks, we propose a hyperparameter searching strat-egy as shown in Algorithm 1. By treating qi ascurrent query and S1:i1 as demonstrations, themodel perplexity 1 (ppl) of ri can reflect the LLMscapability when demonstration number is i 1(lower ppl indicates better performance). Thus, wechoose the p that yields the lowest average ppl andB that first leads an increasing trend in ppl as ourhyperparameter choices. We generally set Sp as",
  "DetailsWe conduct experiments with threewidely used long-context LLMs: LONGCHAT-7B-": "V1.5-32K (Li et al., 2023a), VICUNA-7B-V1.5-16K (Zheng et al., 2023) and LLAMA-3-8B-INSTRUCT (AI@Meta, 2024). We choose the max-imum available number of demonstrations for eval-uation based on the 40 GB memory of the A100GPU (). The hyper parameter searchingresults are listed in . We use random sam-pling decoding strategy (T=0.1) and report the out-comes averaged over 5 runs (randomly selectingdemonstrations) for credible results.",
  ": Accuracy (%) of LLAMA-3-8B-INSTRUCTwith compared methods across benchmarks": "BaselinesUnder most settings, EARLYSTOP out-performs ICL, consistent with the observations ofAgarwal et al. (2024) and Zhao et al. (2023) thatmore demonstrations does not necessarily lead tobetter performance. Compared to EARLYSTOPwhich avoids the negative impact of attention dis-persion by not introducing more demonstrations,STRUCTICL leverages all the given demonstra-tions through structured input to achieve slightlybetter performance. OursHowever, due to the lack of insights into thereasons behind performance degradation of ICLwith more demonstrations, the baselines fail tomaintain the model attention on critical input partswhile fully leveraging all demonstrations. In con-trast, by introducing triviality filtering operationand hierarchical attention mechanism to achievethe above vision, FOCUSICL outperforms the com-pared baselines, achieving an average of 5.2% (3.31points) performance improvement over ICL acrossthree LLMs. The results of the T-test also indi-cate that FOCUSICL is significantly superior tobaselines, with a p-value less than 0.05. This vali-dates the effectiveness and generalizability of FO-",
  "CUSICL": "AblationsWe also report the performance ofonly performing triviality filtering operation as anablation study. The results show that FOCUSICLbenefits 1.29 points improvement from the trivial-ity filtering operation and 2.02 points improvementfrom the hierarchical attention mechanism. EfficiencyBy performing hierarchical attentionmechanism, demonstrations between differentbatches does not need direct interactions, whichcan save a significant amount of inference overhead.Assuming each demonstration has an average of Ltokens, the overhead of attention operation betweenN demonstrations for ICL is:",
  "Scaling with More Demonstrations": "The recent significant advancements in LLMsmainly stem from scaling up in dimensions ofmodel size and training data size. However, giventhe limitations of computation resource and dataproduction speed, we are in eager need of exploringother potential scaling dimensions to continuouslyenhance the performance of LLMs. As shown in, the demonstration number is not a stablescaling dimension when applying ICL, as the per-formance can sometimes exhibit an inverse-scalingphenomenon with more demonstrations. In con-trast, FOCUSICL enables LLMs to become stablemany-shot learners by directing their attention toimportant contents, thereby achieving good scala-bility in the dimension of demonstration number.It should be noted that we find the advantageof FOCUSICL over ICL continues to grow as thenumber of demonstrations increases. This meansthat if we have more resources to conduct experi-ments with more demonstrations, the advantage ofFOCUSICL over ICL can be larger.",
  "To gain a deeper understanding of the workingmechanism of FOCUSICL, we explore it from as-pects of attention and hidden state distributions,following the experimental settings in 3.3": "Attention DistributionThe primary purpose ofFOCUSICL is to prevent the model attention frombeing scattered by the increased demonstrations,thereby ensuring a proper understanding of key con-tents. Therefore, we observe the attention weightsallocated by the model towards the query as thenumber of demonstrations increases. As shownin , by ignoring unimportant parts of thedemonstrations and introducing the hierarchical at-tention mechanism, FOCUSICL consistently main-tains sufficient attention towards the query. Hidden States DistributionWe further investi-gate the distribution of the hidden states of the lastinput token at the penultimate model layer throughPrincipal Component Analysis (PCA). Intuitively,the distribution of the hidden states of the last to-ken mainly depends on the current problem to besolved and should be independent of the demonstra-tion number. However, as shown in , wefind that the hidden states of ICL change systemati-cally with an increasing number of demonstrations,whereas FOCUSICL does not exhibit such behav-ior. We think that the systematic decline in atten-tion towards the query in ICL with an increasingnumber of demonstrations continuously affects thehidden states during response generation, therebyimpacting the quality of the generated response. Incontrast, FOCUSICL avoids this issue by maintain-ing sufficient attention to the query as shown above,ultimately benefiting from more demonstrations.",
  "Further Discussion": "Based on our existing insights and experimentalresults, we attempt to understand the divergent phe-nomena of ICL observed in previous studies wheremore demonstrations sometimes lead to better per-formance, while sometimes the opposite occurs.We think the main reason leading to the above phe-nomena comes from the double-edged sword effectof learning from more demonstrations: on the onehand, they can help the model better understandthe task and increase the likelihood of finding use-ful knowledge; on the other hand, they might alsodistract the model, leading to insufficient attentionand understanding of current query. We considerthat two aspects can influence the balance betweenthe two effects: Weak models require more demonstrations tounderstand the task.As shown in , weobserve that the optimal number of demonstrationsfor LONGCHAT-7B-V1.5-32K is greater comparedto the other two models across most benchmarks.Considering that its performance is also the worst,we believe the reason for the aforementioned situa-tion is that weaker models require more demonstra-tions to help them better understand the task. More demonstrations are needed when theyhave a closer relationship.We also notice thatthe LLMs are more demonstration-hungry onCountA compared to other benchmarks as shownin . We analyze that the correlation be-tween samples in other benchmarks is relatively",
  ": Average model attention towards token of qwith varying demonstration numbers": "weak, and even a single demonstration is sufficientto clarify the task format. In contrast, the demon-strations in CountA are closely related, collectivelydetermining what the task definition is. In thisscenario, LLMs cannot discern the complete taskinformation if only given a few demonstrations. Tosum up, when the samples are closely related, themodel needs more demonstrations to analyze thecorrelations among them, so as to better understandand complete the task.",
  "From an objective perspective , we think there aretwo main limitations of this paper:": "1. Although we have extended the demonstra-tion number to nearly or even beyond 100,due to computational resource limitations, weare unable to conduct experiments with largerdemonstration numbers. We will further ver-ify the applicability of FOCUSICL with largerdemonstration numbers in the future. 2. This work primarily discusses LLMs that ap-ply the standard transformer decoder architec-ture. We look forward to further exploringthe scaling performance with the demonstra-tion number and the applicability of FOCU-",
  "AI@Meta. 2024. Llama 3 model card": "Ekin Akyrek, Dale Schuurmans, Jacob Andreas,Tengyu Ma, and Denny Zhou. 2023. What learn-ing algorithm is in-context learning? investigationswith linear models. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,Matthew R Gormley, and Graham Neubig. 2024. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, JianfengGao, and Yejin Choi. 2020. PIQA: reasoning aboutphysical commonsense in natural language. In TheThirty-Fourth AAAI Conference on Artificial Intelli-gence, AAAI 2020, The Thirty-Second Innovative Ap-plications of Artificial Intelligence Conference, IAAI2020, The Tenth AAAI Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2020, NewYork, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the AI2 reasoning challenge. CoRR,abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems. CoRR, abs/2110.14168. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, ShumingMa, Zhifang Sui, and Furu Wei. 2023. Why can GPTlearn in-context? language models secretly performgradient descent as meta-optimizers. In Findings ofthe Association for Computational Linguistics: ACL2023, Toronto, Canada, July 9-14, 2023, pages 40054019. Association for Computational Linguistics.",
  "reasoning capabilities from llms by leveraging neg-ative data. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 38, pages 1859118599": "Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan,Xinglin Wang, Bin Sun, Heda Wang, and Kan Li.2024b. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. In The TwelfthInternational Conference on Learning Representa-tions, ICLR 2024, Vienna, Austria, May 7-11, 2024.OpenReview.net. Hui Liu, Wenya Wang, Hao Sun, Chris Xing Tian,Chenqi Kong, Xin Dong, and Haoliang Li. 2024.Unraveling the mechanics of learning-based demon-stration selection for in-context learning.CoRR,abs/2406.11890. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 80868098. Association for Computational Linguistics. Ian R. McKenzie, Alexander Lyzhov, Michael Pieler,Alicia Parrish, Aaron Mueller, Ameya Prabhu, EuanMcLean, Aaron Kirtland, Alexis Ross, Alisa Liu,Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauff-man, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh,Max Weiss, Sicong Huang, The Floating Droid, TomTseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang,Zhengping Zhou, Najoung Kim, Samuel R. Bowman,and Ethan Perez. 2023. Inverse scaling: When biggerisnt better. CoRR, abs/2306.09479. Machel Reid, Nikolay Savinov, Denis Teplyashin,Dmitry Lepikhin, Timothy P. Lillicrap, Jean-BaptisteAlayrac, Radu Soricut, Angeliki Lazaridou, OrhanFirat, Julian Schrittwieser, Ioannis Antonoglou, Ro-han Anil, Sebastian Borgeaud, Andrew M. Dai, KatieMillican, Ethan Dyer, Mia Glaese, Thibault Sotti-aux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, James Molloy, Jilin Chen, MichaelIsard, Paul Barham, Tom Hennigan, Ross McIl-roy, Melvin Johnson, Johan Schalkwyk, Eli Collins,Eliza Rutherford, Erica Moreira, Kareem Ayoub,Megha Goel, Clemens Meyer, Gregory Thornton,Zhen Yang, Henryk Michalewski, Zaheer Abbas,Nathan Schucher, Ankesh Anand, Richard Ives,James Keeling, Karel Lenc, Salem Haykal, SiamakShakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-man Ring, Stephen Spencer, Eren Sezener, and et al.2024. Gemini 1.5: Unlocking multimodal under-standing across millions of tokens of context. CoRR,abs/2403.05530.",
  "Computational Linguistics: Human Language Tech-nologies, NAACL 2022, Seattle, WA, United States,July 10-15, 2022, pages 26552671. Association forComputational Linguistics": "Bin Sun, Yitong Li, Fei Mi, Weichao Wang, YiweiLi, and Kan Li. 2023. Towards diverse, relevant andcoherent open-domain dialogue generation via hybridlatent variables. In Thirty-Seventh AAAI Conferenceon Artificial Intelligence, AAAI 2023, Thirty-FifthConference on Innovative Applications of ArtificialIntelligence, IAAI 2023, Thirteenth Symposium onEducational Advances in Artificial Intelligence, EAAI2023, Washington, DC, USA, February 7-14, 2023,pages 1360013608. AAAI Press. Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge.In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, NAACL-HLT 2019, Minneapolis, MN, USA,June 2-7, 2019, Volume 1 (Long and Short Papers),pages 41494158. Association for ComputationalLinguistics. Johannes von Oswald, Eyvind Niklasson, Ettore Ran-dazzo, Joo Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. 2023. Trans-formers learn in-context by gradient descent. In In-ternational Conference on Machine Learning, ICML2023, 23-29 July 2023, Honolulu, Hawaii, USA, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 3515135174. PMLR. Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan,Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024.Integrate the essence and eliminate the dross: Fine-grained self-consistency for free-form language gen-eration. In Proceedings of the 62nd Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), ACL 2024, Bangkok, Thailand,August 11-16, 2024, pages 1178211794. Associa-tion for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-hery, and Denny Zhou. 2023.Self-consistencyimproves chain of thought reasoning in languagemodels. In The Eleventh International Conferenceon Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. 2022a. Emer-gent abilities of large language models. Trans. Mach.Learn. Res., 2022.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,": "and Denny Zhou. 2022b. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems35: Annual Conference on Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New Orleans,LA, USA, November 28 - December 9, 2022. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, andLingpeng Kong. 2023. Compositional exemplarsfor in-context learning. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 3981839833.PMLR. Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang,Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024a.Poor-supervised evaluation for superllm via mutualconsistency.In Findings of the Association forComputational Linguistics ACL 2024, pages 1161411627.",
  "Peiwen Yuan, Shaoxiong Feng, Yiwei Li, XinglinWang, Boyuan Pan, Heda Wang, and Kan Li. 2024b.Batcheval:Towards human-like text evaluation.CoRR, abs/2401.00437": "Peiwen Yuan, Xinglin Wang, Shaoxiong Feng, BoyuanPan, Yiwei Li, Heda Wang, Xupeng Miao, and KanLi. 2024c. Generative dense retrieval: Memory canbe a burden. In Proceedings of the 18th Conferenceof the European Chapter of the Association for Com-putational Linguistics, EACL 2024 - Volume 1: LongPapers, St. Julians, Malta, March 17-22, 2024, pages28352845. Association for Computational Linguis-tics. Peiwen Yuan, Xinglin Wang, Jiayi Shi, Bin Sun, andYiwei Li. 2023. Better correlation and robustness: Adistribution-balanced self-supervised learning frame-work for automatic dialogue evaluation. In Advancesin Neural Information Processing Systems 36: An-nual Conference on Neural Information ProcessingSystems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023.",
  "Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shu-jian Huang, and Xinyu Dai. 2023. Dynamic demon-strations controller for in-context learning. CoRR,abs/2310.00385": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judgingllm-as-a-judge with mt-bench and chatbot arena. InAdvances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023.",
  "To investigate the influence of hyperparameters, wereport the results of LONGCHAT-7B-V1.5-32K onGSM8K benchmark with varying hyperparametersettings": "Filtering ThresholdAs shown in , withthe increase of filtering threshold p, the models per-formance first improves and then declines. This isbecause, when p is relatively small, the model ben-efits from ignoring unimportant content and focus-ing its attention on more beneficial parts. However,when p becomes larger, the model might overlookpotentially useful information in the demonstra-tions, leading to a decrease in performance. Batch SizeAs shown in , a similar in-verted U-shaped curve phenomenon occurs whenscaling the batch size while maintaining the over-all demonstration number fixed. As the batch sizedecreases from 80, the model attention to the querycontinues to increase, which can lead to a certainimprovement in model performance.However,when the batch size is too small, the model mayfail to fully understand the task definition due toexcessive lack of interaction between demonstra-tions, consistent with the findings of Bertsch et al.(2024).Luckily, through our proposed hyperparametersearching strategy, we can efficiently attain suitablehyperparameters for the given tasks and LLMs.",
  "When we identify tokens that are unhelpful for an-swering the current query through attention, TRIV-": "IALITY directly masks them to prevent the modelsattention from being distracted. Another more intu-itive approach is to filter out demonstrations withminimal attention. We compared these two meth-ods, and the results are shown in the . Itcan be seen that TRIVIALITY, which operates at afiner granularity at the token level, achieves betterresults.Additionally, we conducted the following experi-ments to further validate the motivation that tokenswith low attention are unimportant and should bemasked. We set the following settings below onCountA with LONGCHAT-7B-V1.5-32K:",
  "Randomly masking 40% of tokens": "The experimental results in demonstratethe following: compared to No Masking, randomlymasking reduces accuracy from 79.04% to 35.00%.Masking high-attention tokens leads the model torepeatedly output the word nobody, indicating aloss of problem-solving ability. Conversely, mask-ing low-attention tokens significantly improves per-formance.To further analyze the underlying reasons, wecalculated the models perplexity across differentsettings. We found that random masking and mask-ing high-attention tokens significantly increasemodel perplexity, likely due to the loss of criti-cal token information. In contrast, masking low-attention tokens decreases model perplexity, indi-cating that filtering trivial tokens based on poste-rior attention information helps the model performtasks more confidently.",
  "A.3FOCUSICL with DemonstrationsRetrieval": "Previous research (Rubin et al., 2022; Liu et al.,2024; Ye et al., 2023) have shown that selectingdemonstrations relevant to the current query canenhance the performance of ICL. We investigatedwhether combining FOCUSICL with demonstra-tion retrieval could yield better results. For simplic-ity, we used BERT embeddings rather than othercomplex retrieval methods (Yuan et al., 2024c) to retrieve the most relevant demonstrations. We thencompared the experimental results using both ICLand FocusICL, as shown in . Retrievingrelevant demonstrations resulted in a 1.13% im-provement for ICL and a 1.53% enhancement forFocusICL. This improvement is likely attributedto the hierarchical attention mechanisms ability tomore effectively utilize demonstrations with sub-stantial informative content.",
  "CInverse-scaling Phenomena withGemini": "Due to the limitations of computational resourcesand the unavailability of closed-source models, ourexperiments are primarily conducted on 7-8B opensource LLMs. However, by utilizing APIs, weadditionally explore the performance changes ofmore powerful models as the number of demon-strations increased, further validating the general-izability of the argument that LLMs are not stablemany-shot learners. We choose to experiment withGEMINI 1.5 PRO for its long available context win-dow (1M tokens). We test GEMINI 1.5 PRO onMATH benchmark (Hendrycks et al., 2021), whichcontains 7 subsets with 5 difficulty levels that canthoroughly evaluating the math reasoning abilitiesof LLMs. We use greedy searching decoding strat-egy with and report the outcomes averaged over5 runs for credible results. As shown in ,obvious inverse-scaling phenomenon appears in5 out of 7 subsets, with Precalculus and Interme-diate Algebra as exceptions. This validates thegeneralizability of the argument that LLMs are notstable many-shot learners. Meanwhile, we observethat across different difficulty levels, GEMINI 1.5PRO presents similar performance changing trends. clearly shows such phenomenon. Thisindicates that the task difficulty does not affects theoptimal demonstration number of certain task."
}