{
  "Abstract": "Adapting Large Language Models (LLMs) foragent tasks is critical in developing languageagents. Direct Preference Optimization (DPO)is a promising technique for this adaptationwith the alleviation of compounding errors, of-fering a means to directly optimize Reinforce-ment Learning (RL) objectives. However, ap-plying DPO to multi-turn tasks presents chal-lenges due to the inability to cancel the par-tition function. Overcoming this obstacle in-volves making the partition function indepen-dent of the current state and addressing lengthdisparities between preferred and dis-preferredtrajectories. In this light, we replace the pol-icy constraint with the state-action occupancymeasure constraint in the RL objective andadd length normalization to the Bradley-Terrymodel, yielding a novel loss function namedDMPO for multi-turn agent tasks with theoret-ical explanations. Extensive experiments onthree multi-turn agent task datasets confirm theeffectiveness and superiority of the DMPO loss.",
  "Introduction": "Developing generalist agents capable of solvingcomplex tasks has been a central goal in the arti-ficial intelligence community (Reed et al., 2022;Team et al., 2024). Recently, Language agents (Yaoet al., 2022b) emerge as a prominent research direc-tion, leveraging the considerable potential of LargeLanguage Models to address intricate tasks involv-ing instruction following (Ouyang et al., 2022),action planning (Huang et al., 2022), and tool uti-lization (Schick et al., 2024). Nevertheless, thesubstantial disparity between the pretraining taskof Large Language Models and the requirements ofagent tasks suggests significant potential for futureadvancements in language agent capabilities.Behavioral Cloning (BC) (Pomerleau, 1991) is afrequently employed approach to bridge the do-",
  ": Illustration of DMPO loss, which directly opti-mizes the RL objective by maximizing the likelihood ofthe preferred trajectory over the dispreferred trajectory": "main gap by fine-tuning LLMs through expertagent trajectories. Recent endeavors in BC (Chenet al., 2023; Zeng et al., 2023; Yin et al., 2023)involve the Supervised Fine-tuning of LLMs onoptimal state-action pairs. Although these methodsenable swift adaptation of LLMs to agent tasks,BC is notably susceptible to compounding errors minor errors of the learner accumulate along inter-actions between the agent and environment, leadingto performance deterioration in non-deterministicenvironments (Ross et al., 2011).In alleviating compounding errors, Direct Pref-erence Optimization (Rafailov et al., 2024b) hasdemonstrated remarkable success in the single-turnpreference alignment task due to its simple imple-mentation and robustness. DPO optimizes RL ob-jectives by maximizing the likelihood of preferredresponses over dis-preferred responses, mitigatingthe need for continuous interaction with the en-vironment and the training instability commonlyassociated with traditional RL algorithms (Chris-tianos et al., 2023; Liang et al., 2024). Althoughthere has been an initial endeavor to apply the DPOloss on LLMs for agent tasks (Song et al., 2024),it encounters suboptimal performance, as it is tai-lored specifically for the single-turn bandit settingand is ill-suited for multi-turn agent tasks. This work aims to develop a robust loss func-tion capable of directly optimizing RL objectivesin multi-turn scenarios.The crux of this pur-suit involves eliminating the partition function inthe Bradley-Terry (BT) model (Bradley and Terry,1952; Christiano et al., 2017). This entails ensuringthe partition functions independence from the cur-rent state and neutralizing the impact of the lengthdisparity between preferred and dis-preferred tra-jectories. To achieve this, we substitute the policyconstraint with the state-action occupancy measure(SAOM) (Johnson et al., 2000) constraint in theRL objective and introduce length normalizationinto the BT model. These adjustments culminatein the development of a new and simple loss func-tion DMPO for multi-turn agent tasks. As shownin , DMPO directly optimizes the RL ob-jective by maximizing the likelihood of preferred(\"win\") trajectory over dis-preferred (\"lose\") trajec-tory. Notably, the SAOM constraint has advantagesin mitigating compounding errors compared to thepolicy constraint (Xu et al., 2020; Ghasemipouret al., 2020). Furthermore, the derivation offers atheoretical rationale for the efficacy of the lengthnormalization technique in DPO loss (Meng et al.,2024).To summarize, our contributions are threefold:",
  "In this section, we first introduce the in-contextlearning methods and fine-tuning methods of lan-guage agents and then review the literature inpreference-based RL": "In-Context LearningInspired by the superiorin-context learning capabilities of LLMs (Achiamet al., 2023), researchers have designed various in-struction prompts for LLMs, equipped with mem-ory modules (Zhang et al., 2024), toolkits (Qu et al., 2024), and various workflows (Sumers et al., 2023),to build language agents for various real-world do-mains. ReAct (Yao et al., 2022b) incorporates CoTreasoning (Wei et al., 2022) into action generation.Reflexion (Shinn et al., 2024) and PROMST (Chenet al., 2024) refine the prompt using environmentfeedback. However, these in-context learning meth-ods fail to fully exploit the potential of LLMs, sincemost LLMs are not specifically trained for agenttasks. This work focuses on adapting the LLMs toagent tasks through fine-tuning. Agent TuningRecent studies, including Fire-Act (Chen et al., 2023), AgentTuning (Zeng et al.,2023), Lumos (Yin et al., 2023), MIMIR (Denget al., 2024), AUTOACT (Qiao et al., 2024), and-UMi (Shen et al., 2024) supervised fine-tuningLLMs with self-instruct or expert trajectories. How-ever, such BC approaches suffer from compound-ing errors when interacting with dynamic envi-ronments. Taking a step further, Pangu (Chris-tianos et al., 2023) and CMAT (Liang et al., 2024)utilize RL technologies to further fine-tune theLLMs, which may result in a complex and unsta-ble training procedure. To simplify the procedure,ETO (Song et al., 2024) and EMMA (Yang et al.,2024) directly employ the DPO loss (Rafailov et al.,2024b) to optimize the RL objective for the agenttask. Nevertheless, the DPO loss is designed forsingle-turn bandit settings and is ill-suited for multi-turn scenarios. Along this line, this work extendsthe DPO loss in multi-turn scenarios and derivesthe DMPO loss. Preference-Based RLIn multi-turn scenarios,preference-based RL typically starts by explicitlylearning a reward function from preference dataand then optimizing it (Frnkranz et al., 2012;Christiano et al., 2017; Hejna III and Sadigh, 2023;Shin et al., 2021). However, this two-stage learningprocess presents challenges regarding training effi-ciency and instability. This work instead presents asingle-stage policy learning approach using DMPOloss that directly optimizes a policy to satisfy pref-erences. While IPL (Hejna and Sadigh, 2024) andCPL (Hejna et al., 2023) share a similar idea withour work in eliminating the reward learning stage,their loss functions are limited to trajectory pairsof equal length, significantly restricting their appli-cability.",
  "Task Description": "The agent task can be formulated as a Markovdecision process (MDP). A MDP is a 5-tuple(S, A, T , R, ), where S denotes the state space,A denotes action space, T denotes dynamic transi-tion function S A S, R denotes reward func-tion S A , and [0, 1) is the discountfactor. The goal for the agent is to choose actionsat each time step that maximize the expected futurediscounted reward ET1t=0 tr(st, at), whereT is the trajectory length.In the language agent setting (Christianos et al., 2023), the state space and action space are bothsubsets of the language space.For the initialstate s0 S, it contains the task instruction andprompt. At each time step t, LLMs generate ac-tion at according to the policy (at|st) with theparameter . Then the environment will returndynamic feedback ot and transport the state intost+1. Note that the new state st+1 is just a simplecombination of st, at, and ot, and the trajectory = (s0, a0, s1, a1, , sT , aT ).",
  "DKL[(at|st)||ref(at|st)],(1)": "where E is the expectation function, DKL[||] de-notes the KL divergence between two distributions,ref denotes a reference policy, and the is a pa-rameter controlling the deviation from the basereference policy ref. The DPO loss is tailored forthe single-turn preference alignment setting, wherethe trajectory length (T) is limited to 1.Notably, the reward function is learned throughthe Bradley-Terry (BT) model (Bradley and Terry,1952; Christiano et al., 2017):",
  ": Illustration of expert trajectories and trajec-tories learned under the constraints of policy and state-action occupancy measure": "where P() denotes the probability and the coeffi-cient (1 )/(1 T ) is used to normalize theprobability distribution.First, we will provide an intuitive explanationof how the SAOM constraint can reduce the com-pounding error. In imitation learning, the conven-tional SFT learning objective aims to minimize theKL divergence between the expert policy and thecurrent policy:",
  "= max E(s,a)dE[log((a|s)],(7)": "where E is the expert policy and dE is the SAOMwith policy E. As shown in , the trajec-tories learned under policy constraints are suscep-tible to significant compounding error. This vul-nerability stems from the fact that expert datasetsare unable to comprehensively cover all possiblestates. Consequently, the SFT loss leads the modelto choose random actions in states that are not repre-sented in the expert datasets. As a result, the modelgradually deviates from the expert trajectories afterthe initial error, illustrating the phenomenon knownas compounding error.To alleviate the compounding error, subsequentimitation learning research such as (Abbeel and Ng,2004; Ghasemipour et al., 2020; Ho and Ermon,2016) employ the SAOM constraint:",
  "min E(s,a)dE[D()(d(a|s)||dE(a|s))],(8)": "where different approaches utilize different distri-bution distance measures D(). The strength ofSAOM constraint lies in its ability to steer actionselection towards distributions that closely mimicexpert state-action pairs, especially in unexploredstates within the expert datasets. Illustrated in Fig-ure 2, at state s2, policy constraints lead the modelto choose actions uniformly, whereas SAOM con-straints aim to lead the model toward actions that",
  "r(s, a)),(10)": "where represents the optimal policy, Z is thepartition function that normalizes the probability.Its noteworthy that as d(s, a) is a function of(s, a) pairs, normalizing it results in the partitionfunctions Z being independent of the current states. Consequently, Z remains constant for all (s, a)pairs, providing us with the opportunity to elimi-nate them. Easily, we can rearrange Eq (10) into:",
  "k=0(awk |swk )P(swk+1|swk , awk ),(15)": "where P(s0) represents the probability of the ini-tial state s0 and P(sk+1|sk, ak) denotes the tran-sition functions. In general, obtaining the SAOMd(st, at) is challenging because we do not knowthe transition function P(sk+1|sk, ak) in dynamicenvironments. However, in Eq (14) we simplycalculate the ratio between the current SAOMd(st, at) and the reference SAOM dref (st, at).It is important to note that the transition functionremains consistent for both, allowing for cancella-tion. By substituting the Eq (15) into Eq (14), wecan obtain the DMPO loss function:",
  "Observation 4.0.2. If the reward (l) of dispre-ferred trajectory is estimated higher by the policy, the weight [( l) ( w)] will be larger": "LengthNormalizationExplanationInSimPO (Meng et al., 2024), the effectivenessof the length normalization technique was em-pirically demonstrated.However, a theoreticalexplanation was not provided.Our derivationshows that it assists in eliminating the partitionfunction. Without length normalization in Eq (13),a length-dependent bias term arises in the BTmodel, degrading model performance as thedisparity in trajectory lengths between preferredand dispreferred samples increases. Further DiscussionAs discussed in .2,the optimal solution to the RL objective in Eq (9)takes the form shown in Eq (10). However, it iscontended that achieving the optimal solution maynot always be feasible when dealing with an arbi-trary reward function r(s, a) within the context of",
  ": Statistics of three agent datasets. Train, Test-Seen, and Test-Unseen refer to the number of tasksin each set respectively": "a language agent setting. This limitation arises dueto the definition of the new state st+1 as a compos-ite of st, at, and ot, which introduces an inherentconstraint on the transition function between states.In general, in multi-turn dynamic environments,no loss function can rigorously optimize the RLobjective, and the DMPO loss serves as a good ap-proximation. In many cases, the DMPO loss canprecisely optimize the RL objective in Eq (9).",
  "Experiments": "In this section, we conduct extensive experimentson three agent tasks to demonstrate the effective-ness of the proposed DMPO loss function. Our ex-periments aim to address the following questions: RQ1: Can the DMPO loss function exhibit ro-bustness to noisy training trajectories data and mit-igate compounding errors? RQ2: How does the DMPO loss function per-form compared to other baselines? RQ3: What is the impact of the discount factor and the trajectory length on the DMPO loss?",
  "DatasetsFollowing prior work (Song et al.,": "2024), we conduct experiments on three representa-tive agent datasets, including WebShop (Yao et al.,2022a), ScienceWorld (Wang et al., 2022), andALFWorld (Shridhar et al., 2020b). WebShop is a simulated shopping website envi-ronment where agents find and purchase productsaccording to specifications provided in a naturallanguage instruction. The final reward r is calculated based on how closely the purchasedproducts match the specified criteria. ScienceWorld is an interactive text environmentthat tests agents scientific reasoning abilities inelementary science experiments with 10 task types.The final reward r is computed based onthe number of subgoals the agent successfully ac-complishes within each task. ALFWorld is a simulated text-based environment that enables agents to complete embodied house-hold tasks from the ALFRED benchmark (Shridharet al., 2020a). The final binary rewards signify thecompletion status of the task.All three environments can be formally de-scribed as MDP and conducted by language agents.The statistical details of our datasets are outlinedin . Following (Song et al., 2024), in ad-dition to the in-distribution seen test sets, bothScienceWorld and ALFWorld include unseen testsets that include out-of-distribution tasks. Theseadditional test sets enable us to evaluate the gener-alization capabilities of different agents. Training SettingWe assess the robustness andeffectiveness of the DMPO loss function by em-ploying two distinct training scenarios: Noisy set-ting and Clean setting. Following (Song et al.,2024), we adopt the experts trajectories as the\"win\" trajectories to form preference trajectory datain both noisy setting and clean setting. Initially, weutilize the LLMs, which have been fine-tuned withexpert trajectories, to generate new trajectories onthe training set. We observe that the LLMs havea tendency to generate trajectories with repeatedactions or meaningless words. In the noisy setting,these noisy trajectories are used as \"lose\" trajecto-ries for preference data. Conversely, in the Cleansetting, we eliminate the noisy trajectories and em-ploy the remaining ones as \"lose\" trajectories forpreference data. Parameter SettingsIn this work, we utilize twodifferent base models Llama-2-7B-Chat (Touvronet al., 2023) and Mistral-7B-Instruct-v0.2 (Jianget al., 2023) to build language agents. Follow-ing (Song et al., 2024), we utilize the AdamWoptimizer. When supervised fine-tuning the basemodels to get the reference model, we set the batchsize to 64. The learning rate is selected from {1e-5,2e-5, 3e-5} with 3% warm up and a cosine sched-uler. When refining the agents with DMPO lossfunction, we set the batch size to 32 and tune thehyperparameters and within the ranges of {0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 } and {0.1, 0.2,0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99} respectively.We conduct all experiments on 8 NVIDIA A100GPUs.",
  "Noisy Setting Results (RQ1)": "In the noisy setting, we utilize the noisy trajectoriesas \"lose\" trajectories for preference data to inves-tigate the robustness of the DMPO loss function.As shown in , we evaluate the DMPO lossfunction with two different base models on tworepresentative agent tasks and observe that: In all Unseen test sets and most Seen test setsfor both base models, the DMPO loss function out-performs the DPO loss function. This superioritystems from DMPO assigning greater importance toinitial state-action pairs, prioritizing high-qualityexpert actions from the early stages, and reducingthe influence of noisy \"lose\" actions in later stages.This mitigates the influence of noise, endowing themodel with enhanced generalization capabilities.Meanwhile, the DPO loss is not appropriate formulti-turn settings and cannot cancel out the parti-tion function in the BT model, thereby resulting inits inferior performance. The performance of Mistral-7B-Instruct-v0.2 is significantly better than that of Llama-2-7B-Chaton Scienceworld and AlfWorld. This observationsuggests a positive correlation between the effec-tiveness of the base model and its performanceenhancement after fine-tuning for agent tasks usingthe DMPO loss function.",
  "In clean setting, we filter out the noisy trajectoriesand select high-quality trajectories as the \"lose\" tra-jectories for preference data, enabling us to utilizethe DMPO loss function fully": "BaselinesFollowing (Song et al., 2024), we com-pare our models trained by DMPO loss functionwith the following representative baselines.1)Base: default LLM without tuning. 2) SFT: LLMfine-tuned through supervised learning on experttrajectories. 3) Best-of-N: This approach involvesusing an SFT-based agent for sampling and se-lecting the trajectory with the highest reward outof N samples.Here, N is specified as 10.4)RFT (Rejection sampling Fine-Tuning) (Yuan et al.,2023): This approach augments the expert trajec-tory dataset by incorporating successful trajecto-ries and subsequently trains the agent on the aug-mented dataset. 5) PPO (Proximal Policy Opti-mization) (Schulman et al., 2017) directly optimizeRL objectives to maximize the cumulative rewards.6) ETO (Exploration-based Trajectory Optimiza-tion) (Song et al., 2024) iteratively explores the en-vironment to enhance the training preference dataand utilizes DPO loss to learn from preference data. ResultsBased on the Llama-2-7B-Chat model,we show the comparison results under clean settingin . Notably, we observe that: All fine-tuning methods significantly outperformthe base model on both datasets, with improve-ments of at least 49%. On Webshop, they even sur-pass the performance of advanced closed-sourceLLMs. This underscores the significant gap be-",
  ": The effect of hyperparameter on the relativeperformance of the model trained with DMPO loss onthe WebShop dataset in both noisy and clean settings": "tween the pre-training tasks of LLMs and the agenttasks. By fine-tuning LLMs, language agents ex-hibit substantial potential for improvement. The model trained using DMPO loss achievedoptimal performance on both datasets, highlightingthe effectiveness of DMPO loss in learning frompreference data. The improvement over the SFTmodel suggests that DMPO reduces the compound-ing errors, resulting in higher rewards. The model trained using DMPO loss exhibitssubstantial performance improvements comparedto the noisy setting, achieving an average increaseof 5.2% on Webshop and 11.3% on Scienceworld.This highlights the importance of selecting high-quality \"lose\" trajectories in constructing prefer-ence data, as opting for such trajectories yieldssuperior performance.",
  "Ablation Study (RQ3)": "Hyperparamter AnalysisTo verify the impactof reweight function (t, T) in Eq (17), we tunethe the hyperparameter on WebShop and presentthe results in . Our findings reveal that bothbase models achieve optimal performance with asmaller in the noisy setting and a larger in theclean setting. According to Eq (17), a smaller implies that the DMPO loss assigns reduced weightto the state-action pairs in later steps. This indi-cates that DMPO can balance the impact of noiseby adjusting the parameter . When faced withnoisy \"loss\" trajectories, selecting a smaller can",
  "help alleviate noise impact. Conversely, when deal-ing with high-quality \"loss\" trajectories, a largergamma can be selected to better learn strategiesfrom the state-action pairs in later steps": "Length AnalysisTo examine the impact of tra-jectory length on model performance, we con-ducted an experiment by categorizing the noisytrajectories into three groups based on their maxi-mum length. We ensure that the number of prefer-ence data in each group is the same. As shown in, we observe that the performance of themodel trained with DPO loss function decreasesrapidly as the length of noisy \"loss\" trajectoriesincreases. In contrast, the model trained with theDMPO loss function exhibits robustness againstnoisy \"loss\" trajectory length. This is attributed tothe length normalization employed in the DMPOloss, which mitigates the influence of inconsistentlengths between \"win\" and \"lose\" trajectories.",
  "Conclusion": "In this work, we propose a simple and robust lossfunction DMPO loss, which directly optimizes theRL objective for multi-turn agent tasks. By sub-stituting the policy constraint with the SAOM con-straint and introducing the length normalizationinto BT model, we eliminate the partition functionin the BT model and derive the DMPO loss func-tion. The SAOM constraint has played a pivotalrole in mitigating compounding errors. Meanwhile,this derivation offers a theoretical rationale for theefficacy of the length normalization technique. Ex-tensive experiments on three agent datasets demon-strate the effectiveness of DMPO loss, highlightingits capability to reduce compounding errors and itsresilience to trajectory length disparity.",
  "Limitation": "This paper primarily focuses on issues when fine-tuning LLMs on the agent tasks and derives a sim-ple and robust loss function. However, our studyhas several limitations: 1) We solely concentrate onturn-wise task formulation which results in sparserewards for LLMs. Exploring token-wise task for-mulation as suggested in (Rafailov et al., 2024a)would be a valuable avenue for future investigation.2) The experiments in this work are conducted us-ing 7B-sized models on simulated datasets. Futureexperiments on larger models and datasets can pro-vide stronger validation of our conclusions.",
  "Ethical Considerations": "In this paper, we present a new DMPO loss functionfor refining LLMs in agent tasks, without bringingforth additional ethical dilemmas. We utilize pub-licly accessible data while conscientiously steeringclear of sensitive information. Additionally, theuse of LLMs could perpetuate unnoticed societalbiases. We suggest thorough risk assessments andadvise users to be mindful of the potential riskslinked to model deployment.",
  "Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-tic, Shane Legg, and Dario Amodei. 2017. Deepreinforcement learning from human preferences. Ad-vances in neural information processing systems, 30": "Filippos Christianos, Georgios Papoudakis, MatthieuZimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen,Khyati Khandelwal, James Doran, Xidong Feng, Ji-acheng Liu, et al. 2023. Pangu-agent: A fine-tunablegeneralist agent with structured reasoning. arXivpreprint arXiv:2312.14878. Chunyuan Deng, Xiangru Tang, Yilun Zhao, HanmingWang, Haoran Wang, Wangchunshu Zhou, ArmanCohan, and Mark Gerstein. 2024. Mimir: A stream-lined platform for personalized agent tuning in do-main expertise. arXiv preprint arXiv:2404.04285.",
  "Jonathan Ho and Stefano Ermon. 2016. Generativeadversarial imitation learning. Advances in neuralinformation processing systems, 29": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, andIgor Mordatch. 2022. Language models as zero-shotplanners: Extracting actionable knowledge for em-bodied agents. In International conference on ma-chine learning, pages 91189147. PMLR. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,",
  "Rafael Rafailov, Joey Hejna, Ryan Park, and ChelseaFinn. 2024a.From r to q*:Your languagemodel is secretly a q-function.arXiv preprintarXiv:2404.12358": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2024b. Direct preference optimization: Your lan-guage model is secretly a reward model. Advancesin Neural Information Processing Systems, 36. Scott E. Reed,Konrad Zolna, Emilio Parisotto,Sergio Gmez Colmenarejo, Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,Jackie Kay, Jost Tobias Springenberg, Tom Eccles,Jake Bruce, Ali Razavi, Ashley Edwards, NicolasHeess, Yutian Chen, Raia Hadsell, Oriol Vinyals,Mahyar Bordbar, and Nando de Freitas. 2022. Ageneralist agent. Trans. Mach. Learn. Res., 2022. Stphane Ross, Geoffrey Gordon, and Drew Bagnell.2011. A reduction of imitation learning and struc-tured prediction to no-regret online learning. In Pro-ceedings of the fourteenth international conferenceon artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2024.Toolformer: Language models can teach themselvesto use tools. Advances in Neural Information Pro-cessing Systems, 36.",
  "Daniel Shin, Daniel S Brown, and Anca D Dragan.2021. Offline preference-based apprenticeship learn-ing. arXiv preprint arXiv:2107.09251": "Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik Narasimhan, and Shunyu Yao. 2024. Re-flexion: Language agents with verbal reinforcementlearning. Advances in Neural Information Process-ing Systems, 36. Mohit Shridhar, Jesse Thomason, Daniel Gordon,Yonatan Bisk, Winson Han, Roozbeh Mottaghi, LukeZettlemoyer, and Dieter Fox. 2020a.Alfred: Abenchmark for interpreting grounded instructions foreveryday tasks. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 1074010749. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2020b. Alfworld: Aligning text and em-bodied environments for interactive learning. arXivpreprint arXiv:2010.03768.",
  "Theodore R Sumers, Shunyu Yao, Karthik Narasimhan,and Thomas L Griffiths. 2023.Cognitive ar-chitectures for language agents.arXiv preprintarXiv:2309.02427": "SIMA Team, Maria Abi Raad, Arun Ahuja, CatarinaBarros, Frederic Besse, Andrew Bolt, Adrian Bolton,Bethanie Brownfield, Gavin Buttimore, Max Cant,Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune,Adrian Collister, Vikki Copeman, Alex Cullum,Ishita Dasgupta, Dario de Cesare, Julia Di Trapani,Yani Donchev, Emma Dunleavy, Martin Engelcke,Ryan Faulkner, Frankie Garcia, Charles Gbadamosi,Zhitao Gong, Lucy Gonzalez, Kshitij Gupta, KarolGregor, Arne Olav Hallingstad, Tim Harley, SamHaves, Felix Hill, Ed Hirst, Drew A. Hudson, JonyHudson, Steph Hughes-Fitt, Danilo J. Rezende, MimiJasarevic, Laura Kampis, Nan Rosemary Ke, ThomasKeck, Junkyung Kim, Oscar Knagg, Kavya Koppa-rapu, Andrew K. Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cus-sons, Loic Matthey, Siobhan Mcloughlin, PiermariaMendolicchio, Hamza Merzic, Anna Mitenkova,Alexandre Moufarek, Valria Oliveira, Yanko GitahyOliveira, Hannah Openshaw, Renke Pan, AneeshPappu, Alex Platonov, Ollie Purkiss, David P. Re-ichert, John Reid, Pierre Harvey Richemond, TysonRoberts, Giles Ruscoe, Jaume Sanchez Elias, TashaSandars, Daniel P. Sawyer, Tim Scholtes, Guy Sim-mons, Daniel Slater, Hubert Soyer, Heiko Strath-mann, Peter Stys, Allison C. Tam, Denis Teplyashin,Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Mar-cus Wainwright, Jane X. Wang, Zhengdong Wang,Daan Wierstra, Duncan Williams, Nathaniel Wong,Sarah York, and Nick Young. 2024.Scaling in-structable agents across many simulated worlds.CoRR, abs/2404.10179. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, XiaoLiu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning:Enabling generalized agent abilities for llms. arXivpreprint arXiv:2310.12823": "Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen,Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. A survey on the memory mecha-nism of large language model based agents. arXivpreprint arXiv:2404.13501. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-ing llm-as-a-judge with mt-bench and chatbot arena.Preprint, arXiv:2306.05685.",
  "DPO": "Task Instruction: I want green modern velvetdining chairs for the dining room, and pricelower than 200.00 dollarsThought 1: I should search for green modernvelvet dining chairsfirst.Action 1: search[green modern velvet diningchairs]Observation 1: <searching results>Thought 2:The chair with the codeB08TW9LTLM seems to be a green modernvelvet dining chair within the price range. Ishould click on it to get more information.Action 2: click[B08TW9LTLM]Observation 2: <product details>Thought 3: The item has options for differentquantities and colors. I should select the quan-tity that matches my needs.Action 3: clickObservation 3: <product details>Thought 4: Now I should click on the BuyNowbutton to purchase the item.Action 4: click[Buy Now]Reward: 0.33",
  "DMPO": "Task Instruction: I want green modern velvetdining chairs for the dining room, and pricelower than 200.00 dollarsThought 1: I need to find green modern velvetdining chairs that are less than $200.00, so Illstart by searching for that specific description.Action 1: search[green modern velvet diningchairs]Observation 1: <searching results>Thought 2: There are several options, but manyare over my budget. The ZHENGHAO Mod-ern Velvet Dining Chairs Set of 2 for $173.99seems to fit my criteria, even though its slightlyover my budget. Ill click on it to see more de-tails.Action 2: click[B07R9KFCKY]Observation 2: <product details>Thought 3: This set of chairs is available inemerald green, which is the color I want. Illselect this color option.Action 3: click[emerald green]Observation 3: <searching results>Thought 4: Now I need to decide if I want aset of 2 or a set of 4. Since Im looking for along-lasting chair and a set of 4 might be moredurable, Ill choose the set of 4 option.Action 4: click[set of 4]Observation 4: <searching results>Thought 5: Ive selected the color and the set ofchairs I want, and the price is slightly over mybudget. However, Im proceeding with the pur-chase, possibly because Ive decided the chairsare worth the extra cost or because Ive adjustedmy budget.Action 5: click[buy now]Reward: 1.0",
  "BMT-Bench": "In this section, we evaluate and compare the models trained with DMPO vs DPO on various datasetsusing MT-bench (Zheng et al., 2023), and the results are presented in .The analysis of win rates presented in the table indicates that DMPO consistently outperforms DPOacross all training datasets on the MT-bench. Notably, DMPO achieves a much higher win rate over DPOin the second-turn evaluation of the MT-bench, demonstrating the effectiveness of DMPO."
}