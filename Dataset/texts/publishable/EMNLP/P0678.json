{
  "Abstract": "ContrastiveLanguage-ImagePre-training(CLIP) has significantly improved performancein various vision-language tasks by expandingthe dataset with image-text pairs obtainedfrom the web.This paper further exploresCLIP from the perspectives of data andmodel architecture. To mitigate the impactof the noise data and enhance the quality oflarge-scale image-text data crawled from theinternet, we introduce a diverse descriptiongeneration framework that can leverage LargeLanguage Models (LLMs) to combine andrefine information from web-based image-textpairs, synthetic captions, and detection tags.Additionally, we propose RWKV-CLIP, thefirst RWKV-driven vision-language repre-sentation learning model that combines theeffective parallel training of transformers withthe efficient inference of RNNs.Extensiveexperiments across different model scalesand pre-training datasets demonstrate thatRWKV-CLIP is a robust vision-language repre-sentation learner and it achieves state-of-the-artperformanceacrossmultipledownstreamtasks, including linear probing, zero-shotclassification,andzero-shotimage-textretrieval. To facilitate future research, the codeand pre-trained models are released at",
  "Introduction": "The proliferation of mobile networks and socialplatforms has greatly accelerated the large-scaleproduction of image-text pairs (Yang et al., 2020;Yu et al., 2020). This unprecedented abundanceof data has established the foundation for vision-language pre-training.Contrastive Language-Image Pre-training (CLIP) employs two distinctunimodal encoders for images and text, utilizing acontrastive loss, a highly effective mechanism forrepresentation learning. Having been pre-trained",
  "on extensive image-text pairs collected from theinternet, CLIP demonstrates strong transferabilityand has been widely applied across various do-mains (Zhou et al., 2023; Yao et al., 2023)": "In recent years, many large-scale image-textdatasets collected from the internet have been re-leased. LAION400M (Schuhmann et al., 2021) iscreated for research purposes and it contains 400million image-text pairs curated using the CLIPmodel. Building on this, LAION5B (Schuhmannet al., 2022), which consists of 5.85 billion CLIP-filtered image-text pairs, successfully replicatesand fine-tunes basic models such as CLIP. How-ever, using the CLIP model to filter web-basedimage-text pairs still retains a considerable pres-ence of noisy data. To improve data quality, Data-Comp (Gadre et al., 2024) employs various strate-gies such as basic filtering, CLIP score filtering,and text&image-based filtering. However, inher-ent characteristics of internet data, such as abstract",
  "In recent years, the Transformer (Vaswani et al.,": "2017) model has been extensively applied in large-scale representation learning, yielding significantperformance improvements across multiple down-stream tasks (Acosta et al., 2022; Kirillov et al.,2023; Wang et al., 2023b), including image clas-sification (Dosovitskiy et al., 2020; Wang et al.,2023a), text generation (Brown et al., 2020), andspeech recognition (Radford et al., 2023). Despitethese achievements, the quadratic computationalcomplexity inherent in Transformer limits its ca-pacity to effectively process high-resolution imagesand long sequences, posing a substantial challengeto its broader applicability across varied domains. In this paper, we design a framework for gener-ating diverse descriptions. Following ALIP (Yanget al., 2023), we first use the OFA (Wang et al.,2022) model to generate synthetic descriptions con-sistent with image content. However, constrainedby the training data, OFA can only partially iden-tify coarse-grained object categories. Therefore,we introduce an open-set image tagging modelRAM++ (Huang et al., 2023) to capture more de-tailed and precise semantic information from im-ages. By leveraging LLMs, we synthesize andrefine information from web-based texts, syntheticcaptions, and detection tags.Additionally, in-spired by RWKV (Peng et al., 2024) and Vision-RWKV (Duan et al., 2024), we propose RWKV-CLIP, the first RWKV-driven vision-language rep-resentation learning model that combines the ef-fective parallel training of Transformers with theefficient inference of RNNs.Extensive experi-ments across various model scales and pre-trainingdatasets demonstrate that RWKV-CLIP is a robustand efficient vision-language representation learner.The main contributions of this paper are summa-rized as follows: We introduce a diverse description genera-tion framework, which can leverage LLMs tosynthesize and refine information from web-based texts, synthetic captions, and detectiontags to produce more accurate and semanti-cally enriched descriptions.",
  "Vision-Language Representation Learning": "As the milestone in vision-language representa-tion learning, CLIP (Radford et al., 2021) hasgarnered unparalleled interest due to its remark-able zero-shot recognition capability and outstand-ing transfer performance. Subsequently, a signif-icant amount of enhancement works (Yang et al.,2024; An et al., 2024, 2023) based on CLIP havebeen proposed. SLIP (Mu et al., 2022) combinesself-supervised learning with CLIP pre-training toachieve significant performance improvements. De-CLIP (Li et al., 2022b) employs multi-view su-pervision across modalities and nearest-neighborsupervision from similar pairs to enhance represen-tation learning efficiency. FILIP (Yao et al., 2022)refines contrastive loss to learn fine-grained repre-sentations for image patches and sentence words.UniCLIP (Lee et al., 2022) boosts data efficiencyby integrating contrastive loss across multiple do-mains into a single universal space. HiCLIP (Genget al., 2023) enhances cross-modal alignment byincorporating hierarchy-aware attention into bothvisual and language branches of CLIP. ALIP (Yanget al., 2023) introduces a gating mechanism to re-duce the influence of noisy pairs using syntheticdata. Different from the above methods, this paperconducts further exploration of both the data andmodel architecture, proposing a diverse descrip-tion generation framework and introducing RWKV-CLIP, the first RWKV-driven vision-language rep-resentation model.",
  "Text Agumentation": "With the success of LLMs in Natural LanguageProcessing (NLP), there is growing interest in lever-aging LLMs to enhance text descriptions in large-scale image-text pairs. LaCLIP (Fan et al., 2023)explores different strategies to generate rewrite ex-amples and uses the in-context learning ability ofLLMs to rewrite text within image-text datasets.However, the hallucination issue of LLMs and re-liance on limited samples to guide the rewritingprocess can still introduce significant noise. To",
  ": The architecture of our proposed diverse description generation framework": "address this, CapsFusion (Yu et al., 2024) gener-ates synthetic captions for each image and utilizesChatGPT to merge raw texts and synthetic cap-tions, creating a dataset with one million instruc-tions for LLaMA fine-tuning. Despite this, captiongeneration models such as OFA (Wang et al., 2022)and BLIP (Li et al., 2022a) are limited by theirtraining data and can only identify a restricted setof coarse-grained object categories. In this paper,we introduce the open-set image tagging modelRAM++ (Huang et al., 2023) to assign semanticdetection tags to each image. Beneficial from de-tection tags, more semantic information can beintroduced from images, which in turn further con-strains LLMs and mitigates hallucinations.",
  "Receptance Weighted Key Value": "RWKV (Peng et al., 2023) is first proposed in NLP,it addresses memory bottleneck and quadratic scal-ing in Transformers through efficient linear scal-ing while retaining expressive characteristics likeparallelized training and robust scalability. Re-cently, Vision-RWKV (Duan et al., 2024) suc-cessfully transferred the RWKV from NLP to vi-sion tasks, outperforming ViT in image classifica-tion with faster processing and reduced memoryconsumption for high-resolution inputs. PointR-WKV (He et al., 2024) demonstrates leading per-formance across various downstream tasks, surpass-ing Transformer- and Mamba-based counterparts inefficiency and computational complexity. Further-more, Diffusion-RWKV (Fei et al., 2024) adaptsRWKV for diffusion models in image generation tasks, achieving competitive or superior perfor-mance compared to existing CNN or Transformer-based diffusion models. However, these methodshave only validated RWKV in specific downstreamtasks, and the potential of RWKVs to replace ViTsin vision-language representation learning remainsunverified.",
  "Diverse Description Generation": "The architecture of our proposed diverse descrip-tion generation framework is illustrated in .To mitigate the effects of mismatched image-textpairs, following ALIP (Yang et al., 2023), we firstadopt the OFAbase model to generate a syntheticcaption for each image. The synthetic captionsexhibit a high degree of semantic alignment withthe image, facilitating alignment across differentmodal feature spaces. However, constrained bythe training data, OFAbase can recognize a limitednumber of object categories and tends to producecaptions with a simplistic sentence structure. Tocapture finer-grained semantic information withinimages, we incorporate the open-set image taggingmodels RAM++ (Huang et al., 2023) to extract",
  ": The architecture of RWKV-CLIP, which consists of M and N RWKV-driven blocks followed by anaverage pooling layer": "object detection tags for each image.To assess the viability of our approach, followingCapsFusion (Yu et al., 2024), we initially leverageChatGPT to combine information from raw texts,synthetic captions, and detection tags. However,the time and computational effort involved is pro-hibitive. Therefore, we construct an instructiondataset based on ChatGPT interactions and fine-tuned the open-source LLaMA3 with this dataset.After that, we leverage the fine-tuned LLaMA3model for large-scale inference. Specifically, weselect 70K image-text pairs from YFCC15M thathave more than 10 detection tags. Then, we inputthe raw texts, synthetic captions, and detection tagsof these data into ChatGPT to get instruction re-sponses. The details of the instruction prompt areprovided in the supplementary material.After obtaining the instruction dataset, we utilizethe LLaMA Factory (Zheng et al., 2024) to finetunethe LLaMA3-8B and leverage vLLM (Kwon et al.,2023) to accelerate large-scale inference.",
  "RWKV-CLIP": "In this section, we propose RWKV-CLIP, a ro-bust and efficient RWKV-driven vision-languagerepresentation learner. Inspired by CLIP (Rad-ford et al., 2021) and Vision-RWKV (Duan et al.,2024), RWKV-CLIP adopts a dual-tower architec-ture with a block-stacked encoder design like theTransformer (Vaswani et al., 2017), where eachblock consists of a spatial mixing and a channel mixing module. The overview architecture of ourproposed RWKV-CLIP is shown in .Input Augmentation. Based on our proposed di-verse description generation framework, we canobtain three types of text: raw text Tr, syntheticcaption Ts, and generated description Tg. To im-prove the robustness of the model, we randomlyselect a text from [Tr, Ts, Tg] as the augmentationfor text inputs:",
  "aug(T) = Sample([Tr, Ts, Tg]).(1)": "Meanwhile, the input image I RHW3 is trans-formed into HW/p2 patches, where p is the patchsize.Spatial Mixing. The input text aug(T) and imageI are passed through the spatial mixing module,which acts as an attention mechanism and performsglobal attention computation of linear complexity.Specifically, the input data is shifted and enteredinto four parallel linear layers to obtain multi-headvectors Gsx, Rsx, Ksx, V sx :",
  "T = Concat(T1, T2),(4)": "where {G, R, K, V, w}, denotes learn-able vectors, I is the quad-directional shift vectorin the image, i.e., I1 = x[h 1, w, 0 : C/4], I2 =x[h + 1, w, C/4 : C/2], I3 = x[h, w 1, C/2 :3C/4], I4 = x[h, w + 1, 3C/4 : C], T is the bi-directional shift in the text i.e., T1 = [w 1, 0 :C/2], T2 = [w + 1, C/2 : C], where h, w, Cpresent the number of height, width, and channel.These shift functions enhance feature interaction atthe channel level, enabling a focus on neighboringtokens. Specifically, the bi-directional shift ensuresforward and backward interaction of text tokenswithout increasing additional FLOPs. To avoid afixed learned vector, a new time-varying decay wxis calculated as follows:",
  "wsx = x + (1 (Lerpw(x))) x,wsx = ( wsx), wsx = exp ( exp( wsx)) ,(5)": "where x {I, T}, is a learnable vector, Mi, Mjare learnable weight matrices. The function isused to obtain learned vectors by inexpensivelyaugmenting inputs with additional offsets. wsx andwsx are middle values of wsx during the calculationprocess. This process allows each channel of wx tovary based on a mix of the current and prior tokensx.Subsequently, wsx, Rsx, Ksx, V sx are used to com-pute the global attention result wkvt via a lin-ear complexity bidirectional attention mechanism.This result is then multiplied by (Gsx), function-ing as a gate mechanism to control the output Osx:",
  "Experimental Settings": "Pre-training Datasets. We train our model onthe YFCC15M dataset, which is a subset ofYFCC100M (Thomee et al., 2016) filtered byDeCLIP (Li et al., 2022b).To further verifythe effectiveness and generalizability of RWKV-CLIP, following ALIP (Yang et al., 2023), werandomly select subsets of 10M and 30M fromthe LAION400M (Schuhmann et al., 2021). Wethen conduct a series of experiments with differentmodel scales and pre-training datasets.ImplementationDetails.ConsistentwithALIP (Yang et al., 2023), we employ OFAbaseto generate synthetic captions.The instructiondataset is constructed using ChatGPT-35-turbo,and we fine-tune LLaMA3-8B to enhance thegeneration of diverse descriptions. We employAdamW (Loshchilov and Hutter, 2019) as the opti-mizer, initialized with a learning rate of 1e3 and aweight decay of 0.2. The parameters 1 and 2 areset to 0.9 and 0.98, respectively. The input imagesize is 224224, and the input text sequence lengthis truncated or padded to 77. The temperature pa-rameter is initialized to 0.07. We train RWKV-CLIP for 32 epochs with a batch size of 4096 on 8NVIDIA A100(80G) GPUs. We meticulously reg-ulate the parameters and FLOPs of RWKV-CLIPto ensure the fairness of the experimental compari-son. Please refer to the supplementary material formore detailed parameters, FLOPs, and settings ofRWKV-CLIP.",
  "Experimental Results": "LinearProbe.Buildinguponpreviousworks (Yang et al., 2023; Li et al., 2022b;Geng et al., 2023), we use RWKV-CLIP asa feature extractor and train only a logisticregression classifier.Tab. 1 details the linearprobe performance across 10 downstream datasets,as referenced in ALIP (Yang et al., 2023).RWKV-CLIP achieves a significant performanceimprovement ranging from 1.9% 11.1% overthe baseline models, outperforming ALIP in 8of the 10 datasets.The observed performanceimprovements are primarily due to two mainfactors: (1) Our proposed description generationframework effectively synthesizes and refinesinformation from web-based texts,syntheticcaptions, and detection tags, producing moreaccurate and semanti- cally enriched descriptions.(2) RWKV-CLIP exhibits superior representationlearning capabilities compared to ViT-basedmodels.Zero-shot Image-text Retrieval.In Tab. 2,we compare our method with state-of-the-artapproaches in zero-shot image-text retrieval onFlickr30k and MSCOCO. RWKV-CLIP achievesnew state-of-the-art results on all evaluationmetrics.Specifically, RWKV-CLIP achieves76.0%/57.6% I2T/T2I retrieval Recall@1 onFlickr30K, surpassing ALIP by 5.5%/8.7%. Sim- ilarly, significant improvements of 3.5%/4.7%in I2T/T2I retrieval Recall@1 are observed forRWKV-CLIP on MSCOCO. This exceptionalimage-text retrieval capability indicates that therepresentations learned by RWKV-CLIP are robustand exhibit enhanced cross-modal alignment. Zero-shot Classification. We present the zero-shot classification performance across 11 datasets.To ensure fair comparisons, we use the sameprompt templates and class names as establishedin ALIP (Yang et al., 2023) and SLIP (Mu et al.,2022). As shown in Tab. 3, RWKV-CLIP achievesan average performance improvement of 2.6% 14.4% over baseline models. Notably, our modeloutperforms ALIP in 10 out of the 11 datasets, withsignificant enhancements on instance discrimina-tion datasets such as Food101, and ImageNet. Thisimprovement is mainly due to the diverse descrip-tions generated by our framework, providing morefine-grained semantic information. Zero-Shot Robustness Evaluation. In Tab. 4, wepresent a robustness evaluation comparing ALIPand RWKV-CLIP. Our results show that RWKV-CLIP consistently outperforms ALIP in terms ofrobustness across all datasets with an average im-provement of 2.0%. These experimental resultsestablish the RWKV-driven model as a robust rep-resentation learner.",
  "Ablation Study": "Effectiveness of Model and Data Scaling. To eval-uate the effectiveness of RWKV-CLIP on modeland data scaling, we conduct experiments on ran-domly selected subsets of 10M and 30M fromLAION400M. For a more comprehensive compari-son, we report the linear probe performance on 26downstream datasets. As shown in , RWKV-CLIP significantly improves performance acrossdifferent model scales and pre-training datasets.These results demonstrate the robustness and ex-tensibility of RWKV-CLIP. Detailed experimentalresults can be found in the supplementary material.Comparision Analysis with CapsFusion. To fur-ther demonstrate the performance differences be-tween our proposed diverse description generationframework and CapsFusion, we used CapsFusion-LLaMA to rewrite the YFCC15M dataset based onraw texts and synthetic captions. We then trainedRWKV-CLIP using texts generated by our frame-work and CapsFusion. As shown in Tab. 5, ourframework achieves a 0.9% and 2.1% improve-ment in the average linear probe and zero-shotclassification performance, respectively. This im-provement is primarily due to the detection tagsintroducing more semantic information from im-ages, which further constrains LLMs and reduceshallucinations (as shown in ).Ablation on Different Types of Text. We conductablation experiments on different categories of text,the average linear probe results on 10 datasets andthe average zero-shot classification accuracy on 11datasets are shown in Tab. 6. Synthetic captions",
  ": Statistical analysis of raw texts, syntheticcaptions, and generated diverse descriptions on theYFCC15M": "dence of mismatched image-text pairs in raw texts,which can adversely affect representation learning.As shown in , our analysis of cosine simi-larity (computed by CLIP-L14) and token countsacross different text types reveals that syntheticcaptions and generated diverse descriptions havehigher average similarity and token counts thanraw texts. Furthermore, despite these advantages,raw texts achieve superior zero-shot classificationresults, mainly due to the constraints imposed bythe prompt template.",
  "Ablation on Model Architecture": "In Tab. 7, base on text augmentation, we performan ablation study combining RWKV and Trans-former architectures. Compared with TransformerIand TransformerT , the integration of RWKVI andTransformerT achieves a 2.7% improvement onlinear probe but the zero-shot classification per-formance declines by 10.8%. This reduction isprimarily due to the poor compatibility betweenthe RWKV and Transformer architectures. Con-versely, the combination of RWKVI and RWKVTyields improvements of 3.2% and 2.7% in linearprobe and zero-shot classification, respectively, in-dicating that RWKV outperforms Transformer invision-language representation learning.Analysis of Feature Embedding. To understand",
  ": Ablation on model architecture": "what makes RWKV-CLIP effective, we randomlyselect 250 image-text pairs from YFCC15M andvisualize the modality gaps of ALIP and RWKV-CLIP. Specifically, each image and its correspond-ing text are encoded into embedding space andreduced to two dimensions using UMAP (McInneset al., 2018). As shown in , we found that therepresentations learned by RWKV-CLIP exhibitclearer discriminability within the same modal-ity. Additionally, compared to ALIP, RWKV-CLIPdemonstrates closer distances in the image-textmodality space, indicating superior cross-modalalignment performance.",
  "Conclusion": "In this paper, we further explore CLIP from theperspectives of data and model architecture. We in-troduce a diverse description generation frameworkthat can leverage Large Language Models (LLMs)to combine and refine information from web-basedimage-text pairs, synthetic captions, and detec-tion tags. Besides, we propose RWKV-CLIP, thefirst RWKV-driven vision-language representation learning model that combines the effective paralleltraining of transformers with the efficient infer-ence of RNNs. Our method demonstrates superiorperformance across various model scales and pre-training datasets on different downstream tasks. Wehope that our work provides insights into vision-language representation learning models.",
  "Limitations": "Our proposed framework for diverse descriptiongeneration leverages the existing caption genera-tion model and detection tags model, both of whichcan directly influence the quality of the final gen-erated descriptions. Furthermore, due to limita-tions in computational resources, this study onlyexecutes experiments at tens of millions of scalesof image-text pairs. Conducting experiments at abillion-scale necessitates substantial computationalresources.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Li Fei-Fei. 2009. Imagenet: A large-scalehierarchical image database. In CVPR": "AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformers forimage recognition at scale. ICLR. Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu,Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, JifengDai, and Wenhai Wang. 2024. Vision-rwkv: Effi-cient and scalable visual perception with rwkv-likearchitectures. arXiv:2403.02308.",
  "Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Wei-wei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie,Yaqian Li, and Lei Zhang. 2023.Open-set im-age tagging with multi-grained text supervision.arXiv:2310.15200": "Justin Johnson, Bharath Hariharan, Laurens VanDer Maaten, Li Fei-Fei, C Lawrence Zitnick, andRoss Girshick. 2017. Clevr: A diagnostic datasetfor compositional language and elementary visualreasoning. In CVPR. Douwe Kiela, Hamed Firooz, Aravind Mohan, VedanujGoswami, Amanpreet Singh, Pratik Ringshia, andDavide Testuggine. 2020. The hateful memes chal-lenge: Detecting hate speech in multimodal memes.In NeurIPS. Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-YenLo, et al. 2023. Segment anything. In CVPR, pages40154026.",
  "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,and CV Jawahar. 2012. Cats and dogs. In ICCV": "Bo Peng, Eric Alcaide, Quentin Gregory Anthony,Alon Albalak, Samuel Arcadinho, Stella Biderman,Huanqi Cao, Xin Cheng, Michael Nguyen Chung,Leon Derczynski, Xingjian Du, Matteo Grella, Kran-thi Kiran GV, Xuzheng He, Haowen Hou, Przemys-law Kazienko, Jan Kocon, Jiaming Kong, BartomiejKoptyra, Hayden Lau, Jiaju Lin, Krishna Sri IpsitMantri, Ferdinand Mom, Atsushi Saito, GuangyuSong, Xiangru Tang, Johan S. Wind, Stanisaw Woz-niak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, andRui-Jie Zhu. 2023. RWKV: Reinventing RNNs forthe transformer era. In EMNLP. Bo Peng, Daniel Goldstein, Quentin Anthony, AlonAlbalak, Eric Alcaide, Stella Biderman, EugeneCheah, Teddy Ferdinan, Haowen Hou, PrzemysawKazienko, et al. 2024.Eagle and finch: Rwkvwith matrix-valued states and dynamic recurrence.arXiv:2404.05892. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In ICML.",
  "Bowen Wang, Liangzhi Li, Yuta Nakashima, and Ha-jime Nagahara. 2023a.Learning bottleneck con-cepts in image classification. In CVPR, pages 1096210971": "Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-chitectures, tasks, and modalities through a simplesequence-to-sequence learning framework. In ICML. Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, LeweiLu, Hongsheng Li, et al. 2023b. Internimage: Ex-ploring large-scale vision foundation models withdeformable convolutions. In CVPR, pages 1440814419.",
  "A.2Detail Instruction Prompt": "The prompt used to input ChatGPT is present inthe following:\"Please merge the information from the given rawtext and the synthetic caption with the help of thehighly relevant detection tags. The raw captionoffers detailed real-world information, yet it suf-fers from flaws in sentence structure and grammar.The synthetic caption exhibits impeccable sentencestructure but often lacks in-depth real-world de-tails and may contain false information. The highlyrelevant detection tags are provided to enrich thesemantic information of the raw caption, whilesome are redundant and noisy. You are a greatinformation integration and summary expert, youare also good at enriching semantic information.Ensure a well-structured sentence while retainingthe detailed real-world information provided inthe raw caption. Avoid simply concatenating thesentences and avoid adding external informationto describe. Correctness and simplify sentencesfinally. Raw caption:<raw caption>, synthetic cap-tion:<synthetic caption>, and highly relevant de-tection tags:<detection tags>\".",
  "B.1Downstream Datasets": "To comprehensively demonstrate the performanceof RWKV-CLIP, we compared the linear probe re-sults of RWKV-CLIP and ALIP across 26 datasets.These datasets include Food101 (Bossard et al.,2014), CIFAR10 (Krizhevsky et al., 2009), CI-FAR100 (Krizhevsky et al., 2009), Birdsnap (Berget al., 2014), SUN397 (Xiao et al., 2010), Stan-ford Cars (Krause et al., 2013), FGVC Air-craft (Maji et al., 2013), VOC2007 (Everingham,2007), DTD (Cimpoi et al., 2014), Pets (Parkhiet al., 2012), Caltech101 (Fei-Fei et al., 2004),Flowers102 (Nilsback and Zisserman, 2008),MNIST (LeCun et al., 1998), SLT10 (Coateset al., 2011), EuroSAT (Helber et al., 2019), RE-SISC45 (Cheng et al., 2017), GTSRB (Stallkampet al., 2012), KITTI (Geiger et al., 2012), Coun-try211 (Radford et al., 2021), PCAM (Veelinget al., 2018), UCF101 (Soomro et al., 2012), Ki-netics700 (Carreira et al., 2019), CLEVR (Johnsonet al., 2017), Hateful Memes (Kiela et al., 2020),SST2 (Radford et al., 2021), ImageNet (Deng et al.,2009). Details on each dataset and the correspond-ing evaluation metrics are provided in Tab. 12.",
  "B.2Detail Linear Probe Results": "Following ALIP, we conduct experiments on ran-domly selected subsets of 10M and 30M from theLAION400M dataset. For a comprehensive com-parison, we report the linear probe performance on26 downstream datasets. The complete experimen-tal results are shown in Tab.11. RWKV-CLIP-B/32outperforms ALIP-ViT-B/32 2.6% and 1.4% whentraining on LAION10M and LAION30M, respec-tively. Additionally, RWKV-CLIP-B/16 also sur-",
  "C.2Cross Modal Alignment Analysis": "To evaluate the performance of the cross-modalalignment of RWKV-CLIP, we random select 50samples from YFCC15M and visualize the cross-modal cosine similarity matrix in . We ob-serve that the diagonal of the RWKV-CLIP ma-trix is significantly clearer compared to ALIP, indi-cating that the representations learned by RWKV-CLIP exhibit greater distinctiveness and improvedcross-modal alignment capability.",
  "piece of wood": ": Comparison of generated text using our proposed diverse description generation framework vs. CapsFu-sion. Hallucinations are highlighted in red, and additional semantic information is highlighted in green. CIFAR 10 & CIFAR 100a photo of a {label}.a blurry photo of a {label}.a black and white photo of a {label}.a low contrast photo of a {label}.a high contrast photo of a {label}.a bad photo of a {label}.a good photo of a {label}.a photo of a small {label}.a photo of a big {label}.a photo of the {label}.a blurry photo of the {label}.a black and white photo of the {label}.a low contrast photo of the {label}.a high contrast photo of the {label}.a bad photo of the {label}.a good photo of the {label}.a photo of the small {label}.a photo of the big {label}.",
  "Food101a photo of {label}, a type of food": "Caltech101a photo of a {label}.a painting of a {label}.a plastic {label}.a sculpture of a {label}.a sketch of a {label}.a tattoo of a {label}.a toy {label}.a rendition of a {label}.a embroidered {label}.a cartoon {label}.a {label} in a video game.a plushie {label}.an origami {label}.art of a {label}.graffiti of a {label}.a drawing of a {label}.a doodle of a {label}.a photo of the {label}.a painting of the {label}.the plastic {label}.a sculpture of the {label}.a sketch of the {label}.a tattoo of the {label}.the toy {label}.a rendition of the {label}.the embroidered {label}.the cartoon {label}.the {label} in a video game.the plushie {label}.the origami {label}.art of the {label}.graffiti of the {label}.a drawing of the {label}.a doodle of the {label}. Stanford Carsa photo of a {label}.a photo of the {label}.a photo of my {label}.i love my {label}!a photo of my dirty {label}.a photo of my clean {label}.a photo of my new {label}.a photo of my old {label}. DTDa photo of a {label} texture.a photo of a {label} pattern.a photo of a {label} thing.a photo of a {label} object.a photo of the {label} texture.a photo of the {label} pattern.a photo of the {label} thing.a photo of the {label} object.",
  "SUN39a photo of a {label}.a photo of the {label}": "ImageNeta bad photo of a {label}.a photo of many {label}.a sculpture of a {label}.a photo of the hard to see {label}.a low resolution photo of the {label}.a rendering of a {label}.graffiti of a {label}.a bad photo of the {label}.a cropped photo of the {label}.a tattoo of a {label}.the embroidered {label}.a photo of a hard to see {label}.a bright photo of a {label}.a photo of a clean {label}.a photo of a dirty {label}.a dark photo of the {label}.a drawing of a {label}.a photo of my {label}.the plastic {label}.a photo of the cool {label}.a close-up photo of a {label}.a black and white photo of the {label}.a painting of the {label}.a painting of a {label}.a pixelated photo of the {label}.a sculpture of the {label}.a bright photo of the {label}.a cropped photo of a {label}.a plastic {label}.a photo of the dirty {label}.a jpeg corrupted photo of a {label}.a blurry photo of the {label}.a photo of the {label}.a good photo of the {label}.a rendering of the {label}.a {label} in a video game.a photo of one {label}.a doodle of a {label}.a close-up photo of the {label}.a photo of a {label}.the origami {label}.the {label} in a video game.a sketch of a {label}.a doodle of the {label}.an origami {label}.a low resolution photo of a {label}.the toy {label}.a rendition of the {label}.a photo of the clean {label}.a photo of a large {label}.a rendition of a {label}.a photo of a nice {label}.a photo of a weird {label}.a blurry photo of a {label}.a cartoon {label}.art of a {label}.a sketch of the {label}.a embroidered {label}.a pixelated photo of a {label}.itap of the {label}.a jpeg corrupted photo of the {label}.a good photo of a {label}.a plushie {label}.a photo of the nice {label}.a photo of the small {label}.a photo of the weird {label}.the cartoon {label}.art of the {label}.a drawing of the {label}.a photo of the large {label}.a black and white photo of a {label}.the plushie {label}.a dark photo of a {label}.itap of a {label}.graffiti of the {label}.a toy {label}.itap of my {label}.a photo of a cool {label}.a photo of a small {label}.a tattoo of the {label}."
}