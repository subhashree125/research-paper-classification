{
  "Abstract": "Reinforcement Learning from Human Feed-back significantly enhances Natural LanguageProcessing by aligning language models withhuman expectations. A critical factor in thisalignment is the strength of reward models usedduring training. This study explores whetherstronger reward models invariably lead to betterlanguage models. In this paper, through exper-iments on relevance, factuality, and complete-ness tasks using the QA-FEEDBACK datasetand reward models based on Longformer, weuncover a surprising paradox: language mod-els trained with moderately accurate rewardmodels outperform those guided by highly ac-curate ones. This challenges the widely heldbelief that stronger reward models always leadto better language models, and opens up newavenues for future research into the key factorsdriving model performance and how to choosethe most suitable reward models.",
  "Introduction": "Language models (LMs) have made remarkableprogress, achieving close-to-human capabilities ina wide range of tasks (Shen et al., 2017; Radfordet al., 2019; Brown et al., 2020; Su et al., 2022;Achiam et al., 2023; Yuan et al., 2024). While tradi-tional fine-tuning has been effective, it often suffersfrom exposure bias, where models are trained onground truth data rather than their own predictions,leading to inconsistencies during generation (Shenet al., 2019; Wang and Sennrich, 2020). Addition-ally, fine-tuning lacks the ability to optimize forsequence-level rewards, limiting its effectiveness incapturing complex, human-like preferences (Zhuet al., 2024). RLHF addresses these limitationsby incorporating feedback from humans, allow-ing models to generate more contextually relevantand aligned outputs (Stiennon et al., 2020; Ouyanget al., 2022; Su et al., 2024; Madaan et al., 2024).",
  "*Corresponding authors": "It is commonly assumed that higher accuracyin reward models enhances language model per-formance because these models provide precisefeedback during training (Chaudhari et al., 2024).This perspective suggests that accurate feedbackdirectly improves the effectiveness of LMs, espe-cially in complex tasks like machine translationand question answering (Bai et al., 2022).In this paper, we conducted extensive exper-iments using the QA-FEEDBACK dataset (Wuet al., 2024).Reward models based on Long-former (Beltagy et al., 2020) were evaluated fortheir binary classification accuracy in predictingtask relevance, factuality, and completeness. Toensure fair evaluation, the performance of LMstrained with these reward models was assessed us-ing independent high-accuracy models tailored toeach task. Surprisingly, our findings reveal a para-dox: LMs achieve their best performance not withthe most accurate reward models, but with those ofmoderate accuracy (Casper et al., 2023), challeng-ing the prevailing assumption that higher rewardmodel accuracy directly correlates with improvedoutcomes. This result raises important questionsabout the relationship between reward model ac-curacy and language model performance in RLHF,warranting further investigation.The main contributions of this study include:",
  "Demonstrating that moderate reward modelaccuracy and balanced training lead to betterlanguage model performance, contradictingthe assumption that higher accuracy is invari-ably beneficial": "Providing insights into reward dynamics,revealing that moderately accurate rewardmodels offer more task-appropriate rewards,which are intuitively more beneficial for train-ing LMs than those provided by the most ac-curate models. Analyzing KL divergence trends, showing thatmoderately accurate reward models facilitatea balanced and stable training process, pro-moting better generalization and challengingthe notion that higher accuracy alone ensuresoptimal training outcomes.",
  "Motivation and Problem Settings": "Motivation.Findings indicate that the strength ofreward models in RLHF does not consistently cor-relate with improved language model performance,challenging the assumption that stronger rewardmodels always lead to better outcomes (Casperet al., 2023).Understanding the dynamic rela-tionship between reward model accuracy and lan-guage model performance is essential for optimiz-ing RLHF in complex NLP tasks (Ouyang et al.,2022). This study posits that there exists an op-timal range of reward model accuracy that max-imizes language model performance (Wu et al.,2024). Therefore, the primary aim of this researchis to identify this optimal range and examine itsimplications for various NLP applications. Problem Settings.This study investigates the ef-fect of reward model strength on language modelperformance in RLHF, focusing on tasks that eval-uate the factuality, relevance, and completenessof generated text (Wu et al., 2024). Specifically,reward model strength is defined by binary classifi-cation accuracy on test sets (Wu et al., 2024), andlanguage model performance is measured usinghigh-accuracy, independent reward models.Formally, for a language model trained withRLHF, this study analyzes how the reward modelsclassification accuracy (SRM) and the number oftraining steps () affect language model perfor-mance (PLM) (Qin et al., 2024). This relationshipis mathematically represented by:",
  ": Training steps and accuracy ranges for reward mod-els by task type": "Datasets.The QA-FEEDBACK dataset (Wuet al., 2024), derived from the ASQA dataset (Stel-makh et al., 2022), is used for this study. Thisdataset focuses on generating long-form answersto ambiguous factual questions in an open-domainsetting. The data is split into 3,853/500/948 fortraining, validation, and testing, requiring the gen-eration of detailed answers from multiple knowl-edge passages (Min et al., 2020). Hyperparameter Settings.We follow the hyper-parameter settings recommended by Wu et al. (Wuet al., 2024), whose configuration has been specifi-cally designed and empirically validated for RLHFtasks involving QA-feedback. These settings areselected to ensure an optimal trade-off betweenmodel performance and training stability, based onprior experimental findings. For a detailed descrip-tion of all hyperparameters used in the experiments,please refer to Appendix D. Training and Evaluation Paradigm.Followingcommon practice (Schulman et al., 2017), we beginby fine-tuning LMs, followed by applying RLHFusing Proximal Policy Optimization (PPO). In ad-dition, a separate instance of the T5-base modelwas specifically initialized as the value model forthe PPO algorithm. Finally, we evaluate the trainedLMs using three independent, highly accurate re-ward models, which assess various aspects of theLMs outputs, including relevance, factuality, and",
  ": Summary of independent high-accuracy rewardmodels used for evaluation": "A common pitfall in performing RLHF is rewardgaming, where LMs maximize rewards in unin-tended ways, such as finding shortcuts in genera-tion that attain high reward scores from the rewardmodels, yet misalign with human preferences (Panget al., 2022). To mitigate this, we following (Wuet al., 2024) and set a KL threshold. When thedivergence between the current policy and the ref-erence policy exceeded this threshold, the trainingprocess was interrupted. This approach ensuredthat the model did not deviate excessively from thereference policy, effectively reducing the likelihoodof reward manipulation.",
  "Are High-Accuracy and Deeply TrainedReward Models Always the Best?": "Setup.Building on the Basic ExperimentalSetup, reward models for relevance, factuality, andcompleteness from the QA-FEEDBACK datasetwere used in PPO training. Performance was as-sessed at regular intervals, and top-performing in-stances were identified and visualized in three-dimensional plots. Results.Figures 1 to 3 show that optimal lan-guage model performance is achieved using rewardmodels with moderate accuracy and an appropriatenumber of trained steps. For the relevance task, theT5-small model performed best with moderatelyaccurate reward models, effectively mitigating therisk of overfitting. Similarly, the results for fac-tuality emphasized the importance of maintainingbalanced reward model accuracy to prevent overfit-ting and ensure reliable outcomes. These findingssuggest that overly accurate reward models canresult in overfitting, which impairs the generaliza-tion ability of LMs. These trends were consistentacross the T5-base and T5-large models, furthersupporting the conclusion that moderate accuracyin reward models strikes the best balance betweentraining stability and performance. Detailed resultsfor T5-base and T5-large are available in the Ap-",
  "pendix A": "RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 0.700 RM Accuracy 0.45 0.50 0.55 0.60 0.65 0.70 LM Performance 0.45 0.50 0.55 0.60 0.65 RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 RM Accuracy 0.40 0.44 0.48 0.52 0.56 0.60 0.64 0.68 0.72 LM Performance",
  ": 3D surface plot evaluating relevance ratios for T5-small. Optimal performance was achieved with reward modelshaving moderate accuracy": "RM Trained Steps 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 RM Accuracy 0.75 0.80 0.85 0.90 0.95 1.00 LM Performance 0.775 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 RM Trained Steps 0.64 0.66 0.68 0.70 0.72 0.74 0.76 RM Accuracy 0.75 0.78 0.81 0.84 0.87 0.90 0.93 0.96 0.99 1.02 LM Performance",
  "How Do Best and Most Accurate RewardModels Differ?": "Setup.This evaluation utilized three models: T5-small, T5-base, and T5-large, to compare thebest-performing and most accurate reward mod-els across relevance, factuality, and completenesstasks. The analysis focused on understanding thedifferences in reward behavior during training foreach model. While the primary analysis in this sec-tion is based on the T5-small model, similar trendswere observed with the T5-base and T5-large mod-els, whose results are provided in the appendix forreference. Results.Figures 4, 5, and 6 illustrate the dis-tinct strategies of the best-performing reward mod-els compared to the most accurate models usingthe T5-small model. For the relevance task, thebest-performing reward model provided higher andmore variable rewards (), indicating anaggressive approach that likely stimulated the gen-eration of more relevant outputs. In the factualitytask, this model maintained higher mean rewardswith less variability (), promoting factualaccuracy. Conversely, for the completeness task, itemployed a conservative strategy with lower aver-age rewards but greater variability ().",
  "Analysis.Moderately accurate best-performingreward models typically align rewards with task re-quirements. In both relevance and factuality tasks,these models provide higher and more varied re-": "wards, thus encouraging the generation of morerelevant and accurate outputs. This variability al-lows LMs to explore a broader range of responses,improving the quality of the generated text. Con-versely, in completeness tasks, a conservative strat-egy with lower average rewards but greater vari-ability helps ensure thorough and comprehensivetext evaluation. The trends observed in T5-smallmodels are consistent with those seen in T5-baseand T5-large models, further supporting the conclu-sion that moderate accuracy in reward models ef-fectively balances overfitting and underfitting. De-tailed results for T5-base and T5-large can be foundin the Appendix B.",
  "How Do Best and Most Accurate RewardsImpact Models?": "Setup.This section evaluates the impact of re-ward models on the training dynamics of T5-small,T5-base, and T5-large models in relevance, factu-ality, and completeness tasks, with a focus on KLdivergence trends to assess stability and adaptabil-ity. While the results presented here focus on theT5-small model, similar trends were observed forthe T5-base and T5-large models, whose results areprovided in the appendix. KL Divergence and Its Role in RLHFKL diver-gence (Kullback-Leibler divergence) is a measureof how one probability distribution P diverges froma second, expected probability distribution Q (Kull-back and Leibler, 1951). It is commonly used inreinforcement learning to constrain the differencebetween the current policy and a reference policyduring training. Mathematically, KL divergence isdefined as:",
  "(2)": "In the context of RLHF, KL divergence serves asa regularization term to prevent the trained policyfrom deviating excessively from the reference pol-icy. This constraint helps to stabilize the trainingprocess by reducing the chance of reward hackingor reward gaming, where the model could exploitthe reward system without truly improving perfor-mance (Pang et al., 2022).",
  "Results.Comparing KL divergence trends re-vealed significant differences in how LMs alignedwith the training data. For the relevance task, the": "best reward model resulted in consistently lowerKL divergence and variance, indicating stable align-ment (). In the factuality task, the best re-ward model exhibited higher mean KL divergencebut lower variance, suggesting a consistent yet var-ied alignment process (). For the complete-ness task, the best reward model showed highermean and variance in KL divergence, indicating aflexible approach suitable for evaluating complextexts ().",
  ":Completeness task KL divergence (T5-smallmodel): training steps vs. KL divergence (left), mean andvariance of rewards (right)": "Analysis.Best-performing reward models, whichare typically of moderate accuracy, create a bal-anced training environment that facilitates both sta-bility and adaptability. In relevance and factualitytasks, these models encourage stable learning, en-hancing the relevance and accuracy of outputs. Forthe completeness task, the flexibility in handling complex texts is demonstrated by higher variancein KL divergence. The observed trends in T5-smallmodels were consistent with those seen in T5-baseand T5-large models, further validating the conclu-sion that moderate accuracy in reward models ef-fectively balances overfitting and underfitting. De-tailed results for T5-base and T5-large models canbe found in the Appendix C.",
  "Conclusion and Future Work": "This study demonstrates that LMs trained withmoderately accurate reward models in RLHFachieve optimal performance, challenging the con-ventional belief that higher accuracy is always morebeneficial. The results show that moderately accu-rate reward models offer more task-aligned feed-back and foster a balanced, stable training process,promoting better generalization.This researchhighlights the limitations of relying exclusively onhighly accurate reward models, as excessive focuson accuracy may lead to suboptimal outcomes. Infuture work, it will be crucial to further explore thepotential overfitting of reward models, particularlyin their ability to generalize to out-of-distribution(OOD) tasks. Techniques such as regularization,data augmentation, and explicit OOD evaluationwill be key areas of investigation to enhance the ro-bustness of reward models across diverse scenariosand ensure their effectiveness in guiding LMs inbroader, more complex NLP tasks.",
  "Limitations": "Dataset Constraints.The conclusions are drawnfrom the QA-FEEDBACK dataset (Wu et al., 2024),which is specialized in generating long-form re-sponses to factual inquiries. This focus may limitthe generalizability of the results, necessitating val-idation across various datasets, including those per-taining to conversational and question-answeringcontexts. Model Scope.The evaluation utilized T5 modelsof different scales for initial validation (Raffel et al.,2020). Future investigations should incorporatemore complex models, such as Llama2 (Touvronet al., 2023), to gain deeper insights and verify therobustness of the proposed methodologies across abroader range of model architectures.",
  "Reward Model Variations.This study did notexplore the impact of different reward model sizes": "and architectures on RLHF performance. The re-ward models used were based on a single archi-tecture, which may limit the applicability of thefindings. Future research should systematicallyinvestigate how variations in reward model size,capacity, and design affect the learning process,generalization, and overall RLHF performance, par-ticularly in diverse NLP tasks. Understanding theinfluence of these factors will be crucial for devel-oping more robust and scalable reward models thatcan generalize across a wider range of applications.",
  "Acknowledgements": "We thank Xingluan (AI Cloud computing service),EIT and IDT High Performance Computing Cen-ter for providing computational resources for thisproject. This work is supported by 2035 Key Re-search and Development Program of Ningbo Cityunder Grant No.2024Z127. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.2022. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXivpreprint arXiv:2204.05862.",
  "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.Longformer: The long-document transformer. arXivpreprint arXiv:2004.05150": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020. Language models are few-shot learn-ers. Preprint, arXiv:2005.14165. StephenCasper,XanderDavies,ClaudiaShi,Thomas Krendl Gilbert, Jrmy Scheurer, JavierRando, Rachel Freedman, Tomasz Korbak, DavidLindner, Pedro Freire, et al. 2023. Open problemsandfundamentallimitationsofreinforcementlearning from human feedback.arXiv preprintarXiv:2307.15217. Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Mura-hari, Tanmay Rajpurohit, Ashwin Kalyan, KarthikNarasimhan, Ameet Deshpande, and Bruno Castroda Silva. 2024. Rlhf deciphered: A critical analysisof reinforcement learning from human feedback forllms. arXiv preprint arXiv:2404.08555. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361.",
  "Solomon Kullback and Richard A Leibler. 1951. Oninformation and sufficiency. The annals of mathe-matical statistics, 22(1):7986": "Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, YangYu, Ruoyu Sun, and Zhi-Quan Luo. 2023. Remax: Asimple, effective, and efficient reinforcement learningmethod for aligning large language models. In Forty-first International Conference on Machine Learning. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,et al. 2024. Self-refine: Iterative refinement withself-feedback. Advances in Neural Information Pro-cessing Systems, 36.",
  "Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, YuwenChen, Zilin Zhu, Yang Yu, and Jie Zhou. 2022. Welm:A well-read pre-trained language model for chinese.arXiv preprint arXiv:2209.10372": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Chaojun Wang and Rico Sennrich. 2020. On exposurebias, hallucination and domain shift in neural ma-chine translation. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 35443552. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, AlaneSuhr, Prithviraj Ammanabrolu, Noah A Smith, MariOstendorf, and Hannaneh Hajishirzi. 2024. Fine-grained human feedback gives better rewards for lan-guage model training. Advances in Neural Informa-tion Processing Systems, 36.",
  ": 3D surface plot evaluating factuality ratios for T5-base. Optimal performance was achieved with reward modelshaving moderate accuracy": "RM Trained Steps 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 RM Accuracy 0.75 0.80 0.85 0.90 0.95 1.00 1.05 LM Performance 0.75 0.80 0.85 0.90 0.95 1.00 RM Trained Steps 0.64 0.66 0.68 0.70 0.72 0.74 0.76 RM Accuracy 0.72 0.76 0.80 0.84 0.88 0.92 0.96 1.00 1.04 LM Performance",
  ": 3D surface plot evaluating factuality ratios forT5-large. The best performance was seen with reward modelsof moderate accuracy": "RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 0.700 RM Accuracy 0.50 0.55 0.60 0.65 0.70 0.75 0.80 LM Performance 0.55 0.60 0.65 0.70 0.75 0.80 RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 RM Accuracy 0.48 0.52 0.56 0.60 0.64 0.68 0.72 0.76 0.80 0.84 LM Performance",
  ": 3D surface plot evaluating relevance ratios for T5-base. Optimal performance was achieved with reward modelshaving moderate accuracy": "RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 0.700 RM Accuracy 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 LM Performance 0.2 0.3 0.4 0.5 0.6 0.7 0.8 RM Trained Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 RM Accuracy 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 LM Performance"
}