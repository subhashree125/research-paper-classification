{
  "Abstract": "Long-term memory is significant for agents,in which insights play a crucial role. How-ever, the emergence of irrelevant insight andthe lack of general insight can greatly under-mine the effectiveness of insight. To solve thisproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embod-ied agent designed to improve LLMs plan-ning and decision-making ability by summa-rizing and utilizing insight effectively acrossdifferent scales. MSI achieves this through theexperience selector, insight generator, and in-sight selector. Leveraging a three-part pipeline,MSI can generate task-specific and high-levelinsight, store it in a database, and then userelevant insight from it to aid in decision-making. Our experiments show that MSI out-performs another insight strategy when plan-ning by GPT3.5. Moreover, We delve into thestrategies for selecting seed experience and in-sight, aiming to provide LLM with more usefuland relevant insight for better decision-making.Our observations also indicate that MSI ex-hibits better robustness when facing domain-shifting scenarios.",
  "Introduction": "Creating agents that can make autonomous deci-sions in the environment has always been a promis-ing and interesting research direction. (Significant-Gravitas, 2023; Sun et al., 2023) With the emer-gence of ChatGPT and GPT-4 (Achiam et al.,2023), large language models (LLMs) have trans-formed from specialized models to a general modelthat can complete multiple types of tasks, henceit can make decisions for agents. (Xi et al., 2023;Yang et al., 2024; Wang et al., 2023b). This typeof agent will transform multi-modal informationinto natural language as short-term memory. Itthen prompts large language models with short-term memory and long-term memory to plan and",
  ": Example of insight summarizing and utiliz-ing. MSI will summarize the insights in multi-scaleand utilize insights by selecting based on the task.DB=Database": "make decisions. With these capabilities, the agentcan generate a series of actions that are executablewithin a given environment. (Yao et al., 2023; Parket al., 2023; Gao et al., 2023; Zheng et al., 2023)Insight1, as a form of long-term memory, hasgradually become a crucial part of guiding LLMplanning and decision-making. (Shinn et al., 2023;Zhao et al., 2023; Fu et al., 2024; Wang et al.,2023a; Xi et al., 2023; Zeng et al., 2024). Rela-tive to other long-term memory such as examples,insight is more concise and higher-level. Althoughprevious work has proposed a method of usingLLM to summarize and utilize insights (Zhao et al.,2023), it either provides LLM with too many ir-relevant insights or can not summarize the high-level insights, as shown in . The formercan interfere with decision-making (Liu et al.,",
  "In this paper, \"insight\" refers to \"the knowledge acquiredthrough multiple observations of facts or events\"": ": The overall pipeline for the MSI-agent to com-plete a task. MSI Memory refers to the part that dealswith insight. In MSI Memory, Experience Selection andInsight Generation will summarize historical experienceinto insights, while Insight Selection will select insightsto assist the executor in completing future tasks. 2023a; Chen et al., 2023; Ren et al., 2023; Donget al., 2023), while the latter may result in a lack ofhigh-level prior information to assist in decision-making. (Wen et al., 2023; Majumder et al., 2023;Wang et al., 2023c). Therefore, providing mod-els with comprehensive and related insights to thecurrent task has become important.To address these challenges, we proposed Multi-Scale Insight Agent (MSI-Agent), an embodiedagent designed to summarize and utilize insightseffectively. Inspired by Expel (Zhao et al., 2023),MSI collects the task background, user queries,agents plans, environmental feedback, and exe-cution results as \"experience\" from a series oftraining tasks. These experiences are then orga-nized into the successful experience set or success-failure experience pairs set via an experience selec-tor. Subsequently, an insight generator summarizesmulti-scale insights based on the organized expe-rience(s). Through this method, both high-leveland fine-grained insight can be generated.During task execution, the insight will pass aninsight selector to filter out the irrelevant insightand the remaining insight prompts the executor toformulate plans and execute tasks within a givenenvironment. The overall pipeline for the MSIagent to complete a task is illustrated in ,while the architecture of the insight part in MSI isdetailed in .This solution effectively mitigates the issueshighlighted earlier. By allowing classifying andselecting insights, MSI ensures that the LLM is notoverwhelmed with irrelevant insights. Simultane-ously, the multi-scale insights generation providesa nuanced understanding at various levels, address- ing the challenge of high-level insights summariza-tion. As a result, MSI stands as a robust solution,offering contextual and comprehensive insights tai-lored to enhance decision-making capabilities.In summary, our contributions are as follows:(1) We proposed MSI, an embodied agent thatcan create and utilize multiple scales of insights,greatly improving the alignment between insightsand tasks. (2) We designed 3 useful modules among experi-ence selection, multi-scale insight generation, andtask-related insight selection, shielding the noisecaused by irrelevant insights.(3) We got the SOTA results in the TEACh TfDbenchmark with GPT3.5 and beat another insightmechanism in the Alfworld. Whats more, ourexperiment comprehensively investigates the se-lection strategies of seed experiences and insightsunder various approaches and has proven that theMSI can enhance the robustness of insight utiliza-tion facing domain shifting.",
  "Embodied AI": "Embodied AI focuses on leveraging multi-model in-formation for decision and execution of actions. Di-verging from traditional reinforcement learning ap-proaches (Schulman et al., 2017), current researchendeavors employ language models as decision-makers for action decisions. Specifically, the modeltransforms information from non-natural languagemodalities into natural language through a modalitytransformer (Inoue and Ohashi, 2022; Sarch et al.,2023), using natural language information as inputto guide the Large Language Model in decision-making (Song et al., 2023; Singh et al., 2023, 2022;Suglia et al., 2021; Fu et al., 2024). Some methodsinvolve fine-tuning the language model to map lan-guage inputs to action sequences at different hierar-chical levels (Zhang et al., 2022; Zheng et al., 2022;Koshti and Bhavsar, 2023), while others prompt afrozen LLM to predict action plans, relying on theinstruction-following and context-learning proper-ties of the LLM to simulate new tasks during test-ing (Wu et al., 2023; Sarch et al., 2023; Song et al.,2023; Singh et al., 2023, 2022; Dong et al., 2024a).By relying on action(s) generated by the model, therobot can accomplish the designated tasks in theenvironment. : Pipeline of MSI Memory. The Insight Summarization part will summarize the historical task experience,while the Insight Utilization part will select relative insights to help the agent decide on future work. In the InsightGeneration part, we will continuously update the insight database based on the training task experience (pair).We will freeze the database after updating insight with all training tasks. It should be noted that only some taskgenerates environment insights (aligning with 3.3). Env=environment",
  "LLM Long-term Memory": "When making decisions, humans often recall pastcases to assist in decision-making. Due to the lim-ited input length, the LLM Agent cannot receive in-finite historical experiences. Therefore, efficientlyutilizing existing success/failure experiences be-comes crucial. The LLM Long-term Memory isdesigned to address this challenging issue (Zhaoet al., 2023; Wen et al., 2023; Majumder et al.,2023; Qian et al., 2024). Currently, the LLM AgentMemory operates in two modes: example memoryand insight memory. Example memory involvesmanually crafting experience examples that weresuccessful in tasks. During usage, similar exam-ples are retrieved based on the current task, usingmethods such as vectors or BM25, to prompt thelarge language model (Wang et al., 2023a; Wenet al., 2023; Dong et al., 2024b; Song et al., 2023;Zhong et al., 2023). Insight memory, on the otherhand, summarizes success/failure experiences intoinsights through the LLM. When new tasks occur,the insights are directly input as a part of the promptinto the LLM for helping planning and decision-making. (Majumder et al., 2023; Zhao et al., 2023).",
  "Method": "Figures 2 and 3 illustrate our approach. Initially,utilizing historical task data (train set), we employthe task execution module to collect a sufficientnumber of experiences. (3.1) These experiencesare then subjected to the experience selector, whichidentifies experiences/experience pairs suitable forgenerating insights. (3.2) Subsequently, the multi- scale insights will be generated and stored in theinsight database. (3.3) When a new task arises, weretrieve relevant sights from the database based onpredefined rules. (3.4) These insights, along withtask background, and user queries, are provided tothe task execution module to facilitate execution.We refer to the process from experience collectionto insight generation as insight summarization, andthe subsequent insight selection and task executionas insight utilization.",
  "Experience Generation": "As shown in , we regard training data ashistory tasks. For each history task, the execu-tor leverages LLM to generate a plan based ontask background and user queries. Subsequently,the robot employs first-order logic to decomposethe plan into atomic actions (e.g., moving forward,picking up objects) and execute them in an envi-ronment. In some tasks or cases, the executor mayreplan based on the environment feedback. Uponcompletion, task background, user queries, agentsplans, environmental feedback, and execution re-sults are stored as experiences for summarization.Detailed information can be found in Appendix A.",
  "Multi-Scale Insight Generation": "Multi-Scale Insight We categorize the insightsinto several scales. For all tasks, we will gener-ate general scale and subtask scale insights. Ifthe task provides a specific environment category(for example, kitchen), we will also generate envi-ronment scale insights. General insight refers tothe knowledge required for all tasks, which shouldbe high-level. Environment insight pertains to theknowledge needed in a specific environment, andsubtask insight involves the understanding of exe-cuting particular subtasks. The overall pipeline canbe seen in s Insight Generation module.Insight Generation We initialize the insightdatabase to be empty. Whenever a seed experi-ence merges, we select all insights in the order ofgeneral, subtask.2 as a pool of candidate experiencefor updating.Subsequently, we prompt the LLM with tem-plates containing the candidate insight, all expe-rience information, and descriptions of 5 atomicactions: adding, removing, editing, agreeing onan insight, and moving an insight between scales,requesting the LLM to update the insight databasethrough these atomic actions (Zhao et al., 2023).For subtask insight, we also require the LLM to ad-ditionally generate a subtask name correspondingto the insights. 3 After the LLM generation is complete, we up-date the insight database in the order of general,environment (if have), and subtask, according tothe atomic actions.Align with Expel, we also employ a scoringmechanism in insight generation. Specifically, each 2If there is a specific environment category in the task, wewill select environment and subtask insight that is consistentwith the experiences environment category, and the order isgeneral, environment, and subtask3The prompt of Insight Generation can be seen in Ap-pendix C insight receives an initial score of 2 when an \"add\"or \"move\" action is executed, the score increases by1 for an \"agree\" action, remains unchanged for an\"edit\" action, and decreases by 1 for a \"remove\" ac-tion. An insight is discarded when its score reacheszero.",
  "Multi-Scale Insight Selection": "Similar to the generation process, we use generaland subtask insights2 as candidate insights. Forsubtask insights, we adopt two modes for furtherselection:Hashmap indexing: We extract all subtasknames from the subtask insight database, combinethem with user queries, and provide them to theLLM, requiring the LLM to return all task namesrelated to the user query. Subsequently, we con-sider all insights under returned subtask names asthe subtask insights for this user query. The promptof hashmap subtask selection can be seen in Ap-pendix D",
  "Experiment": "We evaluate MSI on the 2 benchmarks5: TEAChTfD benchmark (Padmakumar et al., 2022) andAgentBench Alfworld benchmark (Shridhar et al.,2020; Liu et al., 2023b). Our experiments are de-signed to address the following research questions(RQs): RQ1: Does MSI outperform other insightsmethods? RQ2: What kind of seed experience se-lection strategy should be chosen when facing dif-ferent insight generation strategies and tasks? RQ3:What kind of insight selection strategy should beadopted for different future tasks? RQ4: How doesthe robustness of the MSI system evolve with thedomain shifts?",
  "Max(Lpredx,Lrefx)xp Lrefx(5)": "SCN and GCN refer to the success conditionnumber and goal condition number respectively,Lpred refers to the step used to execute the task bythe executor while Lref refers to the step used toexecute the task by a human annotator, p refers tothe distribution of the datasets and x is the sampleof the distribution of the datasets.For Alfworld, we calculate the SRACC metric.",
  "Executor": "TEACh We use HELPER (Sarch et al., 2023) as theTEAChs executor. HELPER (Sarch et al., 2023)is an executor framework built on top of TEACh.As shown in , it provides the task back-ground, user query (i.e., the dialogue), and otherrelevant information to the LLM in a fixed format,allowing the LLM to generate a piece of code asthe plan(Chen et al., 2021) and create a sequenceof subtasks to guide the robot. Initially, the robotwill walk around the environment to observe andobtain a spatial plan map that includes informationabout the objects it has observed, as well as its lo-cation (Blukis et al., 2022). At each time step, therobot receives an RGB image through its camera. Itwill then determine an atomic action based on theimage, location, and subtask, and execute it in thesimulation environment. (Sarch et al., 2023; Zhanget al., 2022) If the execution fails, the robot willcall upon the VLM model (Li et al., 2023) to pro-vide the most likely reason for the failure based onthe image and attempt a second try or replan (Yaoet al., 2022; Shinn et al., 2023). In the MSI, we in-clude the environment, dialogue, planned subtasks,actual executed subtasks, and the VLM-providedfailure reasons during replanning as part of the ex-perience. (Note that: The EXPERIENCE in theprompt refers to insight in the paper. ) Alfworld We use AgentBench as the Alfworldsexecutor. AgentBench (Liu et al., 2023b) is ex-ecutor frameworks with ReAct format (Yao et al.,2022), Alfworld is one of its subtask. As shownin , AgentBench provides the task back-ground (as shown below), user query (i.e., the dia-logue), and other relevant information to the LLMin a fixed format, allowing the LLM to generate athought and an action (as the plan) in each turn. Af-ter the actions execution, the environment will givethe feedback to the agent and the agent will replananother action based on feedback and new thoughtsuntil the task is finished. In the MSI, we includethe task background, user query, and all thought-action-observations in the task as the experience.The introduction of HELPER and AgentBench canbe seen in Appendix A",
  "Hyperparameter": "Our insight generation and decision-making com-ponents are aligned with Expel. We have cho-sen ChatGPT (gpt-3.5-turbo-1106) as the LLMfor selecting insight subtasks. GPT-4 (gpt-4-1106-preview) as the LLM for insight generation. Dur-ing the experience selection phase, we use text-embedding-ada-002 to establish a vector library forfailed experiences for retrieval purposes.TEACh We have chosen ChatGPT (gpt-3.5-turbo-1106) as the decision-maker for planning.The settings for experience memory enhancement,PreCheck, Correction, and locator are all alignedwith HELPER. Due to the time limitation and bud-get, we do not use GPT4 as the decision-maker forplanning.Alfworld We have chosen ChatGPT (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) asthe decision-maker for planning. The examples areall aligned with AgentBench.",
  "Baseline": "For TEACh, We consider the following baselines:Fine-Tune Based Model:Episodic Trans-former (E.T.) (Padmakumar et al., 2022) is anend-to-end multimodal transformer that can pre-dict the action by language inputs like dialogue andimages in the environment. Jarvis (Zheng et al.,2022) and FILM (Min et al., 2022) use a multi-modal transformer to predict subgoals and trans-form them into atomic actions by rules. DANLI(Zhang et al., 2022) uses an LM to encode languageinputs to high-level subgoals and uses a PDDLmodel (Lamanna et al., 2021) to transform sub-",
  "MSI12.70 (2.60)13.66 (8.72)14.54 (3.70)10.08(6.35)": ": Trajectory from Dialogue (TfD) evaluation on the TEACh validation set. Trajectory length weightedmetrics are included in ( parentheses ). SR = success rate. GC = goal condition success rate. The results with come from (Sarch et al., 2023). We use ChatGPT as the LLM in LLM Agent-Based Model. We reproduce theHELPER in HELPER line and apply Expel in TEACh. Both Expel and MSI use pair mode to generate insight.",
  ": AgentBench Alfworld results. We reproduceall results via AgentBenchs framework. Both Expeland MSI use pair mode to generate insights": "goals, object states, and spatial maps into an atomicaction. It also has a strategy to replan atomic actionwhen facing errors in atomic action.LLM Agent-Based Model: HELPER (Sarchet al., 2023) uses LLM to transform all informationinto a code and uses a code parser to parse the codeinto subgoals. Expel (Zhao et al., 2023) presentsa pipeline to generate schemes and experience aslong-term memory. Different from the originalsetting in Expel, our pair mode uses success-failpairs between different tasks instead of betweenreflexion (Shinn et al., 2023) steps.For Alfworld, We consider the following base-lines: Act-only (Yao et al., 2022), ReAct (Yaoet al., 2022) and Expel (Zhao et al., 2023)",
  "We select only those experiences generated by GPT3.5with SRACC=1 for MSI and Expel to generate insights.Therefore, the insights should generally align with SRACC": "Based Models but above Fine-Tune Based Models.This may be because many irrelevant insights inthe prompts lead to decreased performance. De-spite the Expel summarizing experience based ontraining data, its effectiveness is inferior to thatof HELPER, which uses one-shot examples di-rectly. Conversely, MSIs success rate in both INDand OOD tasks is over 40% higher than that ofHELPER, indicating that the Multi-Scale Insightsummarization and utilization method can providetask-relevant insights to assist the model in makinginference decisions. Alfworld The results of MSI on Alfworld aredisplayed in . Both insight mechanismsgain positive effects on ReAct-based agents. Theenhancement effect on the performance throughMSI insight is approximately twice that of Expelinsight (20 vs 10 in GPT4-dev and 4 vs 2 in GPT4-std) which indicates the performance of MSI ismeaningful over Expel. As a result, MSI insight can improve an agentsplanning and decision-making ability in bothsingle-turn plans (TEACh) and multi-turn plans(Alfworld). This showcases its extensive versatilityand potential applications across different contexts. Cases comparison: illustrates thedecision-making processes and insights examplesused by HELPER, Expel, and MSI when complet-ing the task of slicing tomatoes and plating them.It can be observed that HELPER incorrectly marksthe landmark of Tomato as the location \"Counter-Top\" in the one-shot example, instead of Toaster,causing a failure in finding the tomato and thusfailing the task.In contrast, MSI successfully : An example of 3 plans dealing with a specific task in TEACh. (A) The original tasks user query, weomit some responses. (B) Plan to finish the task without experience. (C) Expel insights example (D) MSI insightsexample(E) Plan to finish the task with Expel. (F) Plan to finish the task with MSI. We omit most of the insights inExpel and MSI due to the length limitation. marks the landmark, even though it uses the sameone-shot example where the Tomato landmark ismarked as CounterTop. This is because MSI hasa subtask insight that guides the model on howto ensure accurate positioning when the dialogueincludes \"near another object.\" This reflects the ef-fectiveness of insight generation to a certain extent.Although Expel also has insight that assists themodel in locating objects, and its decision-makingfor plate location is correct, irrelevant yet similarinsight has influenced its judgment. For example,the insight marked in red in the figure may lead theLLM to mistakenly believe that it needs to generatecode strictly following the dialogue sequence andthat the executor needs to further slice the tomatoslices. On the contrary, MSIs insight prompts themodel to first determine the order of the steps, andsince there are no examples in the general insight,it also reduces the LLMs susceptibility to interfer-ence from irrelevant variables.",
  "Experience Select Strategy (RQ2)": "shows the results of the two strategies un-der two long-term memory methods. From theperspective of the optimization goal of insights(i.e. SRACC), Expel performs 8.28% and 8.99%on HELPER IND and OOD data when using in-sights summarized from successful experiencesalone compared to using success-failure pairs with 9.94% and 11.60% respectively. In contrast, MSIperforms better when summarizing insights fromsuccess-failure pairs rather than just successful ex-periences, the former reaches 12.70% and 14.54%in HELPER IND and OOD data while the lat-ter only gains 10.65% and 13.39%. AlfworldsGPT3.5 version has the same trend in .The reason for this outcome may be that Expelsmethod of summarizing and utilizing insights pro-vides the LLM with many fine-grained insights thatare problematic yet related to the issue or irrelevantinsights(as shown in the red part of ), lead-ing to decreased accuracy.Conversely, when MSI summarizes the insights,it does so at multiple scales and only selects a por-tion for actual use by the LLM. This approachseparates general insights with strong generalityfrom fine-grained insights, ensuring that when theLLM uses insights from success-failure pairs, itcan benefit from the strong generality of generalinsights while reducing the interference of irrele-vant fine-grained insights through selective insightuse. Due to this characteristic of MSI, its effec-tiveness in summarizing and utilizing insights fromsuccess-failure experience pairs is better than usingsuccessful experiences alone.",
  "Insights Select Strategy (RQ3)": "shows the comparison of multi-scale in-sights versus only general insights used under twodifferent Insight Select Strategies. In most cases,the use of multi-scale insights provides a strongerimprovement to LLM planning than the use ofgeneral insights alone. However, when dealingwith OOD problems in pair mode, the general in-sights gain 14.86% in TEACh and 20% in Alfworld,which outperforms the multi-scale insights resultof 14.54% and 16% respectively. This may be dueto task-specific insights summarized in-domain notaligning with OOD tasks, resulting in fine-grainedmismatches. Pair mode is more susceptible to fine-grained mismatches, which is why using only gen-eral insights can be more helpful to model decision-making than using multi-scale insights. Consistentwith the conclusions of .4, the effective- ness of MSI when summarizing insights in pairmode is always better than in success mode. presents the impact of two differentmethods of refining task-specific insights on LLMdecision-making in TEACh.Across both datatypes, results using hashmap pair retrieval are over20% higher on Success Rate (SR) than those usingvector similarity retrieval (from 10.05% to 12.70%in IND and 11.43% to 14.54% in OOD). This isbecause vector similarity retrieval may introduceirrelevant insights, as shown in . If thetask is \"water plants with a bowl\", the top threeinsights retrieved by vector similarity are classifiedas \"Water Plant\", \"Retrieve and Prepare\" and \"Pre-pare Beverage\". The first two seem to align withthe task requirements, while the third is unrelated.The \"Prepare Beverage\" can be retrieved becausethe word bowl is in the task whose semantic spaceis associated with cooking, leading to the retrievalof irrelevant insights. This also explains why themethod of vector similarity retrieval, used to re-trieve schemes as examples, cannot be employedwhen utilizing insight.The results from Tables 4 and 5 collectively il-lustrate the strategy for selecting insight:The agent system needs to first determinewhether the current task aligns with the seed taskexperiences for insight generation. If there is noalignment, then only general insights in the MSIshould be used to assist LLM decision-making.Conversely, if there is alignment, multi-scale in- : The robustness of agents when facing domainshifting. Dashed lines indicate baseline scores withoutinsight or with random scheme shuffling across threedomains. Solid lines show scores after sequential in-sight summarization: first, kitchen experiences informinsight; then living room experiences update it; finally,bedroom experiences refine it, with corresponding re-sults displayed under each domain.",
  "Robustness in Domain Adaptation (RQ4)": "Agents can adjust to new environments by con-stantly updating their insights repository. However,the distribution of new tasks may differ from thatof old tasks that have already been summarizedinto insights, which can lead to \"catastrophic for-getting\" of old tasks when the insights undergodomain transfer, resulting in decreased model per-formance on old tasks. Therefore, it is crucial tohave robust agents for Domain Adaptation. illustrates the robustness of MSI andExpel under domain shifting in TEACh. We fedthe training data into the insight summarizer inthe order of environments: kitchen, living room,and bedroom, unlike the original MSI and Expel,which shuffle the training data before input. Weselected the kitchen task in the valid unseen set as\"original domain tasks\" for testing. insights sum-marized solely on kitchen data are more beneficialin assisting the model with decision-making in thekitchen. However, as new OOD data is introduced,the model insights a degree of forgetting, leadingto a decline in performance on kitchen tasks. Com-pared to Expel, which declines 2.11% after summa-rizing the living room and bedroom scheme, MSIshows a smaller degree of performance decline andfaster convergence with only a decline of about",
  "Conclusion": "In this paper, we propose MSI, which is capable ofsummarizing and utilizing multi-scale insights toenhance the decision-making ability of embodiedagents. MSI can assist agents in making higher-quality decisions and is better equipped to handleinsight distribution shifting that may occur withcontinuous insight updating.Our experiments demonstrate that for MSI,success-failure experience pairs are better seed datafor insights, while the strategy for insight selectionneeds to be determined based on a comprehensiveassessment of the future task distribution and thedistribution of tasks for which insights have beensummarized.It sets a new state-of-the-art result for the TEAChusing agents based on ChatGPT as the foundationand beat another insight mechanism in the Alf-world. We believe our work contributes new in-sights into the summarization, storage, and utiliza-tion of long-term memory, especially insights.",
  "Limitations": "While MSI achieves significant improvements overexisting baselines, there are still directions to ex-plore for future work.(1) Although the General and Subtask scale canbe used in all tasks, the environment scale can onlybe used in some embodied scenarios. In the future,we will expand the idea of multi-scale insight bydesigning different scales in other tasks.(2) We only explore one type of long-term mem-ory, insight. In the future, we will explore thecombination of different types of long-term mem-ory.",
  "Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.2023.Benchmarking large language models inretrieval-augmented generation.arXiv preprintarXiv:2309.01431": "Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374. Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang,Yunsen Xian, and Weiran Xu. 2023.Bridgingthe kb-text gap: Leveraging structured knowledge-aware pre-training for kbqa. In Proceedings of the32nd ACM International Conference on Informa-tion and Knowledge Management, CIKM 23, page38543859, New York, NY, USA. Association forComputing Machinery. Guanting Dong, Keming Lu, Chengpeng Li, TingyuXia, Bowen Yu, Chang Zhou, and Jingren Zhou.2024a. Self-play with execution feedback: Improv-ing instruction-following capabilities of large lan-guage models. CoRR, abs/2406.13542. Guanting Dong, Yutao Zhu, Chenghao Zhang, ZechenWang, Zhicheng Dou, and Ji-Rong Wen. 2024b. Un-derstand what LLM needs: Dual preference align-ment for retrieval-augmented generation.CoRR,abs/2406.18676.",
  "Kushal Koshti and Nidhir Bhavsar. 2023. Interaction isall you need? a study of robots ability to understandand execute. arXiv e-prints, pages arXiv2311": "Leonardo Lamanna, Luciano Serafini, Alessandro Saetti,Alfonso Gerevini, and Paolo Traverso. 2021. On-line grounding of pddl domains by acting and sens-ing in unknown environments.arXiv preprintarXiv:2112.10007. Chunyuan Li, Zhe Gan, Zhengyuan Yang, JianweiYang, Linjie Li, Lijuan Wang, and Jianfeng Gao.2023. Multimodal foundation models: From spe-cialists to general-purpose assistants. arXiv preprintarXiv:2309.10020, 1(2):2. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2023a.Lost in the middle:How lan-guage models use long contexts.arXiv preprintarXiv:2307.03172.",
  "So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, andYonatan Bisk. 2022. Dont copy the teacher: Dataand model challenges in embodied dialogue. arXivpreprint arXiv:2210.04443": "Aishwarya Padmakumar, Jesse Thomason, Ayush Shri-vastava, Patrick Lange, Anjali Narayan-Chen, Span-dana Gella, Robinson Piramuthu, Gkhan Tr, andDilek Hakkani-Tr. 2022. Teach: Task-driven em-bodied agents that chat. In Thirty-Sixth AAAI Con-ference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Ar-tificial Intelligence, IAAI 2022, The Twelveth Sym-posium on Educational Advances in Artificial In-telligence, EAAI 2022 Virtual Event, February 22- March 1, 2022, pages 20172025. AAAI Press. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Mered-ith Ringel Morris, Percy Liang, and Michael S Bern-stein. 2023. Generative agents: Interactive simulacraof human behavior. In Proceedings of the 36th An-nual ACM Symposium on User Interface Softwareand Technology, pages 122. Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, XinCong, Yankai Lin, Yesai Wu, Zhiyuan Liu, andMaosong Sun. 2024. Investigate-consolidate-exploit:A general strategy for inter-task agent self-evolution. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne XinZhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,and Haifeng Wang. 2023.Investigating the fac-tual knowledge boundary of large language mod-els with retrieval augmentation.arXiv preprintarXiv:2307.11019.",
  "Significant-Gravitas. 2023. Autogpt": "Ishika Singh, Valts Blukis, Arsalan Mousavian, AnkitGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: Generating situated robot task plans usinglarge language models. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA),pages 1152311530. IEEE. Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti,Jonghyun Choi, Aniruddha Kembhavi, and RoozbehMottaghi. 2022. Ask4help: Learning to leveragean expert for embodied tasks. Advances in NeuralInformation Processing Systems, 35:1622116232. Chan Hee Song, Jiaman Wu, Clayton Washington,Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023.Llm-planner: Few-shot grounded planning for em-bodied agents with large language models. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 29983009. Alessandro Suglia, Qiaozi Gao, Jesse Thomason,Govind Thattai, and Gaurav Sukhatme. 2021. Em-bodied bert: A transformer model for embodied,language-guided visual task completion.arXivpreprint arXiv:2108.04927.",
  "Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md RizwanParvez, and Graham Neubig. 2023c. Learning tofilter context for retrieval-augmented generation": "Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma,Pinlong Cai, Min Dou, Botian Shi, Liang He, andYu Qiao. 2023. Dilu: A knowledge-driven approachto autonomous driving with large language models.arXiv preprint arXiv:2309.16292. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lep-ert, Andy Zeng, Shuran Song, Jeannette Bohg, Szy-mon Rusinkiewicz, and Thomas Funkhouser. 2023.Tidybot: Personalized robot assistance with largelanguage models. arXiv preprint arXiv:2305.05658. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, YiwenDing, Boyang Hong, Ming Zhang, Junzhe Wang,Senjie Jin, Enyu Zhou, et al. 2023. The rise andpotential of large language model based agents: Asurvey. arXiv preprint arXiv:2309.07864.",
  "HELPER executor prompt": "You are an adept at translating human dia-logues into sequences of actions for house-hold robots. Given a dialogue between a<Driver> and a <Commander>, you shouldwrite a Python program to be executed by ahousehold robot that could finish all tasksin the conversation.{API}Write a script using Python and the Inter-actionObject class and functions definedabove that could be executed by a house-hold robot.Experience you have summarized in thepast:{EXPERIENCE}{RETRIEVED_EXAMPLES}Adhere to these stringent guidelines:1. Use only the classes and functions de-fined previously. Do not create functionsthat are not provided above.2. Make sure that you output a consistentplan. For example, opening of the same ob-ject should not occur in successive steps.3. Make sure the output is consistent withthe proper affordances of objects. For exam-ple, a couch cannot be opened, so your out-put should never include the open() functionfor this object, but a fridge can be opened.4. The input is dialogue between <Driver>and <Commander>. Interpret the dialogueinto robot actions. Do not output any dia-logue.5. Object categories should only be chosenfrom the following classes: ShowerDoor,Cabinet, CounterTop, Sink, Towel, Hand-Towel, TowelHolder, SoapBar, ToiletPa-per, ToiletPaperHanger, HandTowelHolder,SoapBottle, GarbageCan, Candle, Scrub-Brush, Plunger, SinkBasin, Cloth, Spray- Bottle, Toilet, Faucet, ShowerHead, Box,Bed, Book, DeskLamp, BasketBall, Pen,Pillow, Pencil, CellPhone, KeyChain, Paint-ing, CreditCard, AlarmClock, CD, Laptop,Drawer, SideTable, Chair, Blinds, Desk,Curtains, Dresser, Watch, Television, News-paper, FloorLamp, RemoteControl, House-Plant, Statue, Ottoman, ArmChair, Sofa,DogBed, BaseballBat, TennisRacket, Vac-uumCleaner, Mug, ShelvingUnit, Shelf,StoveBurner, Apple, Lettuce, Bottle, Egg,Microwave, CoffeeMachine, Fork, Fridge,WineBottle, Spatula, Bread, Tomato, Pan,Cup, Pot, SaltShaker, Potato, PepperShaker,ButterKnife, StoveKnob, Toaster, Dish-Sponge, Spoon, Plate, Knife, DiningTable,Bowl, LaundryHamper, Vase, Stool, Cof-feeTable, Poster, Bathtub, TissueBox, Foot-stool, BathtubBasin, ShowerCurtain, TV-Stand, Boots, RoomDecor, PaperTowel-Roll, Ladle, Kettle, Safe, GarbageBag, Ted-dyBear, TableTopDecor, Dumbbell, Desk-top, AluminumFoil, Window, LightSwitch,AppleSliced, BreadSliced, LettuceSliced,PotatoSliced, TomatoSliced6. You can only pick up one object at a time.If the agent is holding an object, the agentshould place or put down the object beforeattempting to pick up a second object.7. Each object instance should instantiatea different InteractionObject class even iftwo object instances are the same object cat-egory.Follow the output format provided earlier.Think step by step to carry out the instruc-tion.Write a Python script that could be executedby a household robot for the following:dialogue: {command}Python script:",
  "AgentBench executor prompt": "Interact with a household to solve a task.Imagine you are an intelligent agent in ahousehold environment and your target is toperform actions to complete the task goal.At the beginning of your interactions, youwill be given the detailed description of thecurrent environment and your goal to ac- complish. For each of your turn, you will begiven a list of actions which you can chooseone to perform in this turn. You shouldchoose from two actions: THOUGHTrCTION. If you choose THOUGHT, youshould first think about the current conditionand plan for your future actions, and thenoutput your action in this turn. Your outputmust strictly follow this format:THOUGHT:your thoughts.ACTION: your next action; If you choose CTION, you shoulddirectly output the action in this turn.Your output must strictly follow this for-mat:CTION: your next action . After youreach turn, the environment will give youimmediate feedback based on which youplan your next few steps. if the environmentoutput Nothing happened, that means theprevious action is invalid and you shouldtry more options. Here is some experienceyou summarized before: {experience}Reminder:1. the action must be chosen from the givenavailable actions. Any actions except pro-vided available actions will be regarded asillegal.2. Think when necessary, try to act directlymore in the process.\"",
  "BBenchmark infromation": "TEACh The TEACh dataset (Padmakumar et al.,2022) is constructed on over 120 different AI2-THOR simulation environments (Kolve et al., 2017)and encompasses more than 2000 embodied intelli-gence tasks aimed at completing household chores.These environments can be categorized into fourhyper-environments: kitchen, living room, bed-room, and bathroom. The training set consists of1482 data points, encompassing all four types of en-vironments. The valid seen set is built with 181 datapoints across the four environments, with all simu-lation environments having appeared in the trainingset. In contrast, the valid unseen set is constructedwith 612 data points in three types of environments:kitchen, living room, and bedroom, based on simu-lation environments that have not been previouslyencountered in the training set. Therefore, we con-sider the valid unseen set as out-of-domain (OOD) data and the valid seen set as in-domain (IND) data.Our tests are conducted on the Trajectory from Dia-logue (TfD) benchmark (Padmakumar et al., 2022),where the agent receives multiple rounds of inter-active dialogue between a commander and a driver.The model must analyze the entire dialogue andmake a series of decisions to complete all tasksmentioned in the dialogue.Alfworld The Alfworld dataset (Shridhar et al., 2020) encompasses more than 4000 embodied in-telligence tasks aimed at completing householdchores. These tasks can be categorized into sixhyper-task: \"pick and place\", \"pick clean thenplace\", \"pick heat then place\", \"pick cool thenplace\", \"look at obj\", and \"pick two obj\". We justselect 20 successful experiences in each hyper-task.We use the AgentBench (Liu et al., 2023b) for eval-uation, it contains 20 data points in the dev set and50 data points in the std set. Aligned with Alfworld,we consider the std set as out-of-domain (OOD)data and the dev set as in-domain (IND) data.",
  "Pair-Mode Insight Generation Prompt": "You are an advanced reasoning agent thatcan add, edit, move or remove rules fromyour existing ruleset, based on formingnew critiques of past task trajectories.The ruleset has three parts, GENERALRULES, ENVIRONMENT RULES andTASK RULES. GENERAL RULES refersto rules that could used in all environment(Kitchens, LivingRooms, Bedrooms, andBathrooms) and task. ENVIRONMENTRULES refers to rules that could used inall task in {env}. TASK RULES refers torules that could used in a specific task. Youwill be given two previous task trials withinstruction:{instruction}One trial is successful, and the other isunsuccessful. Here are the two previoustrials to compare and critique:",
  "Failed Trajectories:{Failed Trajectories}": "Succeeded Trajectories:{Succeeded Trajectories}Here are the EXISTING RULES:GENERAL RULES:{general rules}ENVIRONMENT RULES:{environment rules}TASK RULES:{task rules}By examining and contrasting to the suc-cessful trial, and the list of existing rules,you can perform the following operations:add, edit, remove, move or agree so that thenew rules are HIGH LEVEL critiques of thefailed trial or proposed way of Thought in3 parts, so they can be used to avoid simi-lar failures when encountered with differentquestions in the future. Have an emphasison critiquing how to perform better Thoughtand Action.Follow the below format:GENERAL RULES:<OPERATION><RULENUMBER>:<RULE>ENVIRONMENT RULES:<OPERATION><RULENUMBER>:<RULE>TASK RULES:<OPERATION><RULENUMBER>:<RULE>The rule number should increase betweenparts, for example if there is 4 general rulesthe first environment rule number should be5.The available operations are:AGREE(if the existing rule is strongly relevantfor the task), REMOVE(if one existingrule is contradictory or similar/duplicatedto other existing rules), EDIT (if anyexisting rule is not general enough or canbe enhanced, rewrite and improve it), ADD(add new rules that are very different fromexisting rules and relevant for other tasks.),MOVE(move rules between different leveland reshape the rules if the rules are notgeneral in all enviroment(for GENERALRULES) or task(for GENERAL RULES or EMVIRONMENT RULES)). Each needsto CLOSELY follow their correspondingformatting below:AGREE <EXISTING RULE NUMBER>:<EXISTING RULE>REMOVE <EXISTING RULE NUMBER>:<EXISTING RULE>EDIT <EXISTING RULE NUMBER>:<NEW MODIFIED RULE>ADD <NEW RULE NUMBER>: <NEWRULE>MOVE <EXISTING RULE NUMBER>:<RESHAPED RULE>.(for example if youwant to move a rule in environment ruleswith id 12 to task rules, you should useMOVE 12:<RESHAPED RULE> in taskrules part)Note1: MOVE command will remove therules by number and add new rules in thepart it present in and ADD command willadd new rules in the part it present in.Note2:If you believe some rules in generalrule part can not be used in the {env}, youshould just remove that rules instead ofmove it.Note3:In task rules part, there may sometask irrelevant with the trail now, DO NOTremove themIn the TASK RULES part, you should spec-ify the task name in the <RULE> withthe following format:<RULE CONTENT>(TASK: <TASK NAME>), the length oftask name should be less than 20 charac-ters and the number of task should less than20.Do not mention the trials in the general rulesbecause they should be GENERALLY AP-PLICABLE. Each rule should be conciseand easy to follow.Remember this robot can only generatepython script. The execute subgoal and er-ror log are gained from another robot whichthis robot can not communite. So each rulesshould focus on helping robot to plan andgenerate better python script to solve thequestion based on ONLY dialogue. And op-eration can be used MULTIPLE times. Doat most 4 operations in each parts (whichmeans the max operation number in 3 partsis 4x3=12) and each existing rule can only",
  "Success-Mode Insight Generation Prompt": "You are an advanced reasoning agent thatcan add, edit, move or remove rules fromyour existing ruleset, based on forming newcritiques of past task trajectories. The rule-set has three parts, GENERAL RULES, EN-VIRONMENT RULES and TASK RULES.GENERAL RULES refers to rules thatcould used in all environment (Kitchens,LivingRooms, Bedrooms, and Bathrooms)and task. ENVIRONMENT RULES refersto rules that could used in all task in {env}.TASK RULES refers to rules that couldused in a specific task. You will be givensuccessful task trials with instruction:{instruction}Here are the trials:{Succeeded Trajectories} Here are the EXISTING RULES:GENERAL RULES:{general rules}ENVIRONMENT RULES:{environment rules}TASK RULES:{task rules}By examining the successful trials, and thelist of existing rules, you can perform thefollowing operations: add, edit, remove,move or agree so that the new rules areHIGH LEVEL insights of the successful tri-als or proposed way of Thought in 3 parts,so they can be used as helpful tips to differ-ent questions in the future. Have an empha-sis on tips that help the agent perform betterThought and Action.",
  "Follow the below format:GENERAL RULES:<OPERATION><RULENUMBER>:<RULE>ENVIRONMENT RULES :<OPERATION><RULENUMBER>": ":<RULE>TASK RULES:<OPERATION><RULENUMBER>:<RULE>The rule number should increase betweenparts, for example if there is 4 general rulesthe first environment rule number should be5.The available operations are:AGREE(if the existing rule is strongly relevantfor the task), REMOVE(if one existingrule is contradictory or similar/duplicatedto other existing rules), EDIT (if anyexisting rule is not general enough or canbe enhanced, rewrite and improve it), ADD(add new rules that are very different fromexisting rules and relevant for other tasks.),MOVE(move rules between different leveland reshape the rules if the rules are notgeneral in all enviroment(for GENERALRULES) or task(for GENERAL RULES orEMVIRONMENT RULES)). Each needsto CLOSELY follow their correspondingformatting below:AGREE <EXISTING RULE NUMBER>:<EXISTING RULE>REMOVE <EXISTING RULE NUMBER>:<EXISTING RULE>EDIT <EXISTING RULE NUMBER>:<NEW MODIFIED RULE>ADD <NEW RULE NUMBER>: <NEWRULE>MOVE <EXISTING RULE NUMBER>:<RESHAPED RULE>.(for example if youwant to move a rule in environment ruleswith id 12 to task rules, you should useMOVE 12:<RESHAPED RULE> in taskrules part)Note1: MOVE command will remove therules by number and add new rules in thepart it present in and ADD command willadd new rules in the part it present in.Note2:If you believe some rules in generalrule part can not be used in the {env}, youshould just remove that rules instead ofmove it.Note3:In task rules part, there may sometask irrelevant with the trail now, DO NOTremove them",
  ": The insights task-specific level under 3 sources. (0 for general insight and 10 for task-specific insight)": "In the TASK RULES part, you should spec-ify the task name in the <RULE> withthe following format:<RULE CONTENT>(TASK: <TASK NAME>), the length oftask name should be less than 20 charac-ters and the number of task should less than20.Do not mention the trials in the general rulesbecause they should be GENERALLY AP-PLICABLE. Each rule should be conciseand easy to follow.Remember this robot can only generatepython script. The execute subgoal and er-ror log are gained from another robot whichthis robot can not communite. So each rulesshould focus on helping robot to plan andgenerate better python script to solve thequestion based on ONLY dialogue. And op-eration can be used MULTIPLE times. Doat most 4 operations in each parts (whichmeans the max operation number in 3 partsis 4x3=12) and each existing rule can onlyget a maximum of 1 operation so just findthe most important rules to operate. Do notoperate rules in other parts. Below are theoperations you do to the above list of EX-ISTING RULES",
  "Insight Selection Prompt in Hashmap Index": "You are a task selector trying to select taskcategories.A household robot have just summarizedsome experience, and each experiencebelongs to a task category.Now this robot is facing a new task, basedon a dialogue between a <Driver> and a<Commander>, but this robot do not knowwhich experience should be used in thistask.You should select task categories related to",
  "FInsight High-Level Rate": "In the table 6, we compared the task-specific de-gree of three different insight sources in Alfworld,where 0 points are completely general (applicableto all tasks), 10 points are completely task-specific(can only be used for one specific task), and inter-mediate scores represent the degree to which theycan be used for some tasks.We have manually created three examples, eachin the format:(insight, thought, score).For each example, the scores are respectively0, 5, and 10. We have then asked the model (gpt-4-turbo-2024-04-09) to derive the score in a COTmanner.We can observe that the distribution of Expelis relatively uniform, the distribution of MSI Tasktends to be around 7 points, while the distributionof MSI General leans towards 0-1 points.This demonstrates that MSI indeed distinguishesbetween general insight and task-specific insight,and that task-specific insight is more targeted to-wards specific tasks.",
  "Prompt of Rating Insights Level": "prompt: You will be given an experienceabout houseworking, your task is to judgewhether the experience is a general rule (alltasks in housework can be used) or a task-related rule. You should think step by stepand give a score of 0-10, 0 means this expe-rience is a general rule, and 10 means thisexperience is a task-related rule. Here areexamples:"
}