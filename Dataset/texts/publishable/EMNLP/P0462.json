{
  "Abstract": "Direct Preference Optimization (DPO) hasemerged as a prominent algorithm for the di-rect and robust alignment of Large LanguageModels (LLMs) with human preferences, of-fering a more straightforward alternative tothe complex Reinforcement Learning from Hu-man Feedback (RLHF). Despite its promis-ing efficacy, DPO faces a notable drawback:verbosity, a common over-optimization phe-nomenon also observed in RLHF. While pre-vious studies mainly attributed verbosity to bi-ased labels within the data, we propose thatthe issue also stems from an inherent algorith-mic length reliance in DPO. Specifically, wesuggest that the discrepancy between sequence-level KullbackLeibler (KL) divergences be-tween chosen and rejected sequences, usedin DPO, results in overestimated or underes-timated rewards due to varying token lengths.Empirically, we utilize datasets with differentlabel lengths to demonstrate the presence ofbiased rewards. We then introduce an effec-tive downsampling approach, named SamPO,to eliminate potential length reliance. Our ex-perimental evaluations, conducted across threeLLMs of varying scales and a diverse array ofconditional and open-ended benchmarks, high-light the efficacy of SamPO in mitigating ver-bosity, achieving improvements of 5% to 12%over DPO through debaised rewards1.",
  "Introduction": "Reinforcement Learning from Human Feedback(RLHF) is a crucial strategy for effectively alignLarge Language Models (LLMs) with humanminds (Zhao et al., 2023a; Yang et al., 2023; Panet al., 2023b), showcasing significant improve-ments of LLMs instruct-following capability com-pared with the other two popular approaches: pre-training and supervised fine-tuning (SFT). In fact, a",
  "*Equal Contribution.1Our code can be accessed at:": "series of leading LLMs have adopted RLHF as thefinal stage of their entire training pipelines (Ouyanget al., 2022; Achiam et al., 2023; Bi et al., 2024).Nevertheless, traditional RLHF involves sev-eral intricate multi-stage steps, typically startingwith fine-tuning a reward model that capturescomplex human intuition (Bai et al., 2022), fol-lowed by optimizing LLMs to maximize prefer-ence scores. Therefore, the quality of the rewardmodel is crucial. However, modeling elusive hu-man intuition is inherently difficult (Wang et al.,2024). On the contrary, Direct Preference Opti-mization (DPO) (Rafailov et al., 2023) proposedto re-parameterize the reward model, integratingpreference feedback from online rewards into of-fline labels. In specific, DPO employs the Bradley-Terry model (Bradley and Terry, 1952) to maxi-mize implicit rewards via pairwise offline pref-erence labels. The implicit reward is mathemat-ically equivalent to the discrepancy in sequence-level KullbackLeibler (KL) divergences (Kullbackand Leibler, 1951) between chosen and rejectedlabels. The KL divergence for each label is calcu-lated based on probability outputs from the fine-tuning policy model and a frozen reference model.DPO eliminates the need for complex prefix fine-tuning of an external reward model, while main-tains performance comparable to RLHF (Duboiset al., 2024b; Hou et al., 2024).Despite its effectiveness, DPO faces severalnotable challenges, including issues of overfit-ting (Azar et al., 2023; Jung et al., 2024), highcomputational costs (Ethayarajh et al., 2024; Honget al., 2024), and verbosity (Hou et al., 2024; Parket al., 2024). This paper specifically focuses onaddressing the verbosity issue.Traditional multi-stage RLHF methods arguethat due to a statistical bias in length distribution,that is, where preferred labels tend to be longerthan rejected preference labels (Singhal et al., 2023;Park et al., 2024), the reward model trained on",
  ": Down-Sampling strategy helps mitigate the potential length reliance, and thus improves DPO": "such preference data inherently exhibit a lengthbias (Shen et al., 2023). Therefore, subsequent fine-tuned policy model exploit this bias as a shortcut toachieve higher reward scores by generating longerresponses (Gao et al., 2023a), without necessarilyimproving quality (Kabir et al., 2023; Dubois et al.,2024b). Various regularization approaches havebeen proposed to mitigate this inherent bias withinreward models (Ramamurthy et al., 2022; Costeet al., 2023; Moskovitz et al., 2023; Chen et al.,2024b). On the other hand, although DPO does notexplicitly use a reward model, the length distribu-tion bias inherent in the offline preference labelsstill contributes to the verbosity issue (Hou et al.,2024; Rafailov et al., 2024). Analysis suggests thatpolicy models trained using DPO tend to generateresponses that are almost twice the length of thelabeled data (Park et al., 2024). In this paper, we propose that, in addition tothe length bias in the data, DPO exhibits a hiddenalgorithmic dependence on response length. Asillustrated in the upper portion of , the lossfunction in DPO is based on the discrepancy be-tween sequence-level KL divergence, which canalso be computed and aggregated at the token-level.It is evident that discrepancies between chosen la-bel yw and rejected label yl lead to an inadver-tent reliance on auxiliary length features: trainingsamples with longer chosen labels than rejectedones lead to overestimated rewards during training,while those with shorter chosen labels result in un-derestimated rewards. Therefore, overestimated re- wards contribute more significantly to gradient op-timization, ultimately exacerbating verbosity. Webelieve this algorithmic dependence on responselength is a unique drawback of DPO, since the ex-plicit rewards in RLHF typically manifest as scalarvalues (Ouyang et al., 2022).We propose that addressing this reliance on re-sponse length can be effectively achieved through astraightforward down-sampling method. Illustratedin the middle of , this approach involvesdown-sampling equal token-level probability fea-tures for computing regularized KL divergences.Our contributions in this paper are threefold: We analyze the algorithmic dependence onresponse length in DPO, revaling how it re-sults in overestimated or underestimated re-wards. Through decomposition experimentsusing datasets with varying label length, weempirically demonstrate the biased rewards. We propose a lightweight approach, calledSamPO, to mitigate the biased length reliancein DPO. By simply down-sampling equalprobability features at the token-level, we canapply DPO with regularized KL divergences. We validate our method using three differentLLMs of varying scales. Compared to DPO,SamPO significantly reduces verbosity. Lever-aging debaised rewards, we achieve signif-icant improvements across five conditionedand three open-ended benchmarks, as de-picted in the lower section of .",
  "Related Work": "Optimization from Human Preference aims toalign neural models with human minds. As a sem-inal work, (Stiennon et al., 2020) collected hu-man preferences on 123k pairs of summary outputs,then trained a reward model that guides the GPT-3model (Brown et al., 2020) to produce more co-herent and human-preferred summaries. (Ouyanget al., 2022) then further scaled similar pipelinewith 1M diverse text instructions, and reportedthat outputs from the 1.3B parameter InstructGPTmodel were preferred to outputs from the 175BGPT-3 model, according to downstream humanevaluation. RLHF has become an essential partof aligning LLMs (Touvron et al., 2023; Bi et al.,2024; Bai et al., 2023; Young et al., 2024). How-ever, as it follows a multi-stage training strategy,and heavily relays on the quality of reward model,RLHFs training cost and stability are widely criti-cized (Zheng et al., 2023; McKinney et al., 2023).Therefore, DPO came into being, providing a stablealternative that does not rely on an explicit rewardmodel (Rafailov et al., 2023). It has been provedthat DPO can achieve the same alignment effect asRLHF (Ivison et al., 2023; Hou et al., 2024).Over-optimization in RL is a well-known obsta-cle (Skalse et al., 2022; Pan et al., 2023a; Casperet al., 2023; Zheng et al., 2023), which refers tothe phenomenon that feedback scores from the re-ward model are getting higher, but the updated pol-icy model produces lower quality responses. Andone particularly noticeable low-quality feature isverbosity. It is general to blame for exploitationof reward model (Casper et al., 2023; Gao et al.,2023a), and thus various regularization approacheshave been proposed, including uncertainty-basedregularization (Coste et al., 2023; Zhai et al., 2023),composite reward models (Moskovitz et al., 2023),and length decorrelation (Chen et al., 2024b). How-ever, since the reward model is eliminated in DPO,none of the above approaches can be directly ap-plied. Herein, specific methods are introduced,(Park et al., 2024) introduced a pairwise length reg-ularization term to dampen the verbosity trends,and SimPO (Meng et al., 2024) used average prob-ability to eliminate length reliance.In this paper, we present that the verbosity is-sue in DPO is further related to algorithmic biasedlength reliance, which is never analyzed in previ-ous literature. And this drawback can be effectivelyhandled via down-sampling over KL divergence.",
  "ref(y|x) + log Z(x)(1)": "where r, and ref denote the reward model,the policy model, and the reference model, respec-tively. Both and ref are usually initializedfrom the same SFT model. While is subject tofurther optimization during DPO, ref is usuallyfrozen. Z(x) is the partition function, and isa hyperparameter that adjusts the intensity of re-wards. DPO incorporates the Bradley-Terry modelto predict preferences:",
  "rejected reward log(yl|x)": "ref (yl|x) with red and purple curves, respectively. The reward for each response is calculatedas the sequence-level KL divergence, which is derived from the token-level log probability ratios (illustrated bygreen and blue bars). Therefore, the difference between these two curves illustrates the implicit reward target inDPO, as shown in Eq. 7. Averaged and normalized DPO results are displayed in the lower-left sub-figure (b), whileour SamPO is illustrated in lower-right sub-figure (c). where M is a discrepancy term that leads the pol-icy model to increase the likelihood of the cho-sen response yw and decrease the likelihood of therejected response yl. The term acts as a scalingfactor for the intensity of M.",
  "t=1log (ytl|x)(8)": "From this, we can intuitively understand how thedifference in length between the chosen responseyw and the rejected response yl affects the loss andthe gradient. As illustrated in sub-(a), acomparable reward is achieved if yw and yl havethe same length, allowing DPO to effectively learnsthe quality difference. However, if yw is muchlonger than yl, the larger number of tokens in yw may result in an overestimated reward in Eq. 7,contributing disproportionately to the gradient up-dates described in Eq. 5 and 8. Conversely, if yw isshorter than yl, DPO could underestimate rewardand incorporate fewer gradients, even if yw is ofbetter quality. This bias towards length means thatDPO tends to favor longer, seemingly acceptableresponses over shorter, well-formed ones duringtraining, potentially leading to verbose outputs.",
  "(10)": "where Tm is equal to the minimum token lengthof (Tw, Tl), and yt is down-sampled from all to-kens {yT } uniformly. Eq. 10 is consistent with thecorresponding reward term shown in the middleof . In addition, we discuss the impact ofsampling randomness in Appendix E.(b) and (c) demonstrate that both aver-aging and sampling can produce length-debiasedrewards that are comparably effective. However,simple averaging diminishes the variance featureamong tokens. Consequently, we opt for the down-sampling strategy in our proposed SamPO method.This decision is validated in .",
  "Training Datasets": "We leverage three independent preference datasetsfor training. Two of these are consistent with theoriginal DPO (Rafailov et al., 2023): the 161k HH-RLHF data (Ganguli et al., 2022), and the 92.8kTL;DR data (Vlske et al., 2017). Additionally, weinclude the 61k binarized UltraFeedback data (Cuiet al., 2023) that has been utilized in subsequentworks (Ivison et al., 2023; Meng et al., 2024) fol-lowing DPO. Each of these datasets comes with anevaluation set for cross-validation during training.",
  "Evaluation Benchmarks": "Following DPO, for models trained on HH-RLHFor TL;DR, we randomly select 256 samples fromtheir respective evaluation sets for final testing. Wereport the win rate between the response gener-ated by the fine-tuned policy model y = (xtest)and the response from the baseline SFT modelyref = ref(xtest), judged by GPT-4 (Achiamet al., 2023). For models trained with UltraFeed-back, we use five conditional and one open-endedgeneration benchmarks. The conditional bench-marks, along with their in-context examples, are:GSM8K in 8-shot (Cobbe et al., 2021), IFEval in 3-shot (Zhou et al., 2023), PiQA in 3-shot (Bisk et al.,2020), MMLU in 0-shot (Hendrycks et al., 2021),and TruthfulQA in 3-shot (Lin et al., 2022). Theopen-ended benchmark is AlpacaEval2 (Li et al.,2023). We report match accuracy for the condi-tional benchmarks, and the length-debiased GPT-4win rate for AlpacaEval2 (Dubois et al., 2024a).For additional details, refer to Appendix B.",
  "Several variants of DPO have been proposed, whichcan be categorized into three main types: (1) Re-duce cost. Although DPO is robust, the preparation": "of high-quality pair-wise preference labels and therequirement to run with two large models makeDPO costly. To address this, KTO (Ethayarajhet al., 2024) proposed to use non-pairwise pref-erence data. ORPO (Hong et al., 2024), CPO (Xuet al., 2024), and SimPO (Meng et al., 2024) in-troduced reference-free losses that allow optimiza-tion with a single policy model; (2) Alleviate over-fitting. IPO (Azar et al., 2023) analyzed the riskof overfitting, and introduced a square loss to re-shape the monotonic DPO loss. TDPO (Zeng et al.,2024) incorporated forward KL divergence con-straints for each token, improving alignment anddiversity. BCO (Jung et al., 2024) and NCA (Chenet al., 2024a) offered strategies to reduce noisefrom pairwise preference responses; (3) Overcomeverbosity. Park et al. (2024) introduced a pairwiselength regularization term to counter verbosity.SimPO (Meng et al., 2024) used average proba-bility to eliminate dependency on sequence length.We select methods that focus on noise removalor length normalization, and have shown relativelypositive testing results as our final baselines: Hy-brid DPO+SFT, TDPO (Zeng et al., 2024), Length-normed DPO (Park et al., 2024), BCO (Jung et al.,2024), SimPO (Meng et al., 2024). Particularly,Hybrid DPO+SFT refers to the multi-task learn-ing pipeline where DPO is applied to pairwise re-sponses and SFT is applied to the chosen responseat the same time, which is a common practice (Huaet al., 2024; Lu et al., 2024).",
  "In general, we design three groups of experiments:": "(1) Presence of biased length reliance. We ex-tract two 27k subsets from the UltraFeed-back only by response length. One is namedUltraFeedback-long, in which the chosen re-sponse of each data must be longer than therejected response. The other one is namedUltraFeedback-short, and as the name sug-gests, it contains a shorter chosen response.We use these subsets for biased reward exhibi-tions. (2) Preliminary Study of DPO and variants.Given that there are many variants of DPO,and they often use their own hyperparameters,we first conduct a preliminary study to aligntheir performance under the same conditions.This study helps us select several robust base-lines. The results are reported in Appendix D.",
  ": Performance of models in . The * markstands for the SamPOs debiased rewards": "(3) Experiments with various LLMs. Similarto DPO, we use Pythia-2.8B to train and testSamPO on HH-RLHF or TL;DR; on the otherhand, following relevant studies (Ivison et al.,2023; Hong et al., 2024), we use Tulu2-13B-SFT and Llama3-8B-Instruct to train on Ultra-feedback and verify SamPO on public bench-marks. Also, literature reports that iterativelyupdates the frozen reference model ref canobtain further gains (Gorbatovski et al., 2024;Zhang et al., 2024). Thus, we combine it withSamPO to present Iterative SamPO.",
  "SamPO (ours)76.5657.0381.7264.0041.0664.1828.9732.01375Iterative SamPO (ours)77.8160.5581.1864.1244.0765.5530.6835.14377": ": Qualitative results of fine-tuning two LLMs with DPO, several variants and our SamPO. We use thesame UltraFeedback dataset and keep almost all hyperparameters the same for each LLM group. Specifically,Tulu2-13B-SFT and -DPO, Llama3-8B-Insturct and -Ins.-SimPO are open-source checkpoints. We evaluate allmodels, including those public models, under the same framework. We bold the best results and underline theunusually poor results.",
  "in . It is clear that data from the same distri-bution leads to different training and testing perfor-mances due to the difference in response length": "The -all set refers to training with originalUltraFeedback, which mix -long and -shortdata. The -long subset provides overestimatedrewards and therefore causes performance degra-dation. However, since statistically, the chosenresponse is longer than the rejected response (Parket al., 2024), the training trend of the -long subsetis similar to the -all full set. On the contrary, the-short subset completely erases the distinctivefeature of length, hoping that the model will per-form comparative learning based on content quality.However, the biased DPO completely underesti-mate the reward, thus causing collapses. Yet, our SamPO presents debaised rewards. Wecan observe debiased positive rewards on the -short set. And the debaised rewards of -all setgrow to a high peak at 300 steps. Such debiasedrewards result in significant U-turn reversal and fur-ther improvements. As shown in , SamPOmanages to eliminate collapse on the -short set, where we record a normal average benchmark scoresimilar to the -long set, improving the score by9.2%. Thanks to the regularization of those shortdata, the -all set that mixes both long andshort data achieves the best score up to 52.81on average.",
  "Study on UltraFeedback": "For LLMs that fine-tuned with UltraFeedback, weevaluate their downstream performance in .Overall enhancement by SamPO. For Tulu2-13B-SFT, our replicated DPO shows benchmarkaccuracy and response length on AlpacaEval2 datacomparable to the open-source version. Comparedto the SFT baseline, DPO improves performanceacross all test data but increases response lengthby 40-45%. Iterative DPO exacerbates this ver-bosity issue. However, all chosen baselines andour SamPOs produce shorter responses, mitigatingverbosity. However, TDPO and SimPO show sig-nificant drops in conditional benchmarks, such asover 10% on GSM8K and over 3% on TruthfulQA,",
  ": We show how the policy models responselength changes on AlpacEval2 as the test performanceimproves over 3 epochs of training. The epoch numberincreases from left to right along the curve": "compared to DPO. Notably, our SamPOs achieveoverall improvements on both conditional bench-marks (+0.5%) and open-ended generation for Al-pacaEval2 prompts (+4%). Also, the averagingversion DPO-SANorm, mentioned in section 3.3,confirms that the sampling strategy is more valid.For Llama3-8B-Instruct, we observe superiorlength stability. Even when fine-tuned with the orig-inal DPO, the model maintains its initial responselength, likely due to its comprehensive training pro-cess involving SFT, RLHF, and DPO (AI@Meta,2024). Marginal improvements are observed overits DPO version, with average gains of 1.7% onfive conditional benchmarks and <1% on AlpacaE-val2. Among all methods, only hybrid DPO+SFT,SimPO, and our SamPOs show significant improve-ments over DPO, with average gains of 1.3% to 3%on five accuracy benchmarks. Specifically, hybridDPO+SFT excels in IFEval (65.83), and our Sam-POs notably improve GSM8K (+2.3%) and Truth-fulQA (+3.7%). As for GPT-4 judged AlpacaEval2,hybrid training loses about 3% performance, whileour SamPO achieves the best performance in bothraw and length-debiased scores among all locallyfine-tuned LLMs, outperforming DPO up to 12%.Discussions of SimPO. The SimPO method hasan obvious seesaw dilemma. The open-sourceSimPO checkpoint achieves the best performanceof AlpacaEval2 at the expense of a significant sac-rifice on other benchmarks. We avoid this in thereproduction and obtain a more balanced version.Also, the public release was trained with boosteddata2 instead of the naive UltraFeedback.",
  ": Win Rate (%) and Avg. Output Length acrossmethods. We bold the best and underline the outliers": "Length stability of SamPO. Based on ,we find that DPO makes the model increasingly pre-fer to generate longer responses in 3-epoch training,and Iterative DPO further strengthens this trend.In contrast, SamPO and Iterative SamPO achievehigher testing scores and stabilise the length. 5.2.2Study on HH-RLHF & TL;DRAs for HH-RLHF and TL;DR, we utilize Pythia-2.8B for all experiments. Since Pythia has not beenspecifically trained for instructional tasks, we ini-tiate our process with one epoch of SFT on thechosen response, following DPOs setup. Subse-quently, we conduct preference optimization usingSamPO alongside various baseline methods. Fol-lowing previous literature (Rafailov et al., 2023;Park et al., 2024), GPT-4 served as the proxy for hu-man preference. We report the win rate against theSFT basis and the average generated token lengthof all methods in .SamPO has a good effect on HH-RLHF.SamPO improves performance across all HH-RLHF test data, achieving the second-best winrate while maintaining a lower yet reasonable re-sponse length. Iterative SamPO shows slightlylower win rates due to less control over responselength. Baselines such as Iterative DPO and TDPOachieve win rates close to 50%, indicating min-imal improvement over the SFT model. HybridDPO+SFT stands out as a strong baseline, address-ing the under-generalization issue and attainingan 86.12% win rate with the shortest average re-sponse lengths among all experiments. SimPO,while achieving a similar win rate of 78.91% asIterative SamPO, but produces incredibly low re-sponse length.SamPO achieves the best performance onTL;DR. In terms of TL;DR, SamPO and IterativeSamPO show the highest win rates, with 65.71% and 73.58%, respectively, significantly outperform-ing all other methods. DPO and Length-normedDPO also perform well, achieving win rates of60.98% and 58.13%, respectively. Iterative DPOreaches the best while using longer answers thanIterative SamPO. In contrast, SimPO has the low-est win rate at 33.33%, indicating that it is lesseffective on the TL;DR dataset.Over-simplification by SimPO. In fact, on HH-RLHF, we notice many of the outputs from SimPOare overly simplified, often omitting necessary con-tent and resulting in only 14.77 lengths of tokenson average. For example, a preferred response fromHH-RLHF is Ill give you the links., whereas theSimPO response is simply Sure!. This suggeststhat while concise, the responses lack the necessaryinformativeness. In this scenario, we can see GPT-4prefers over-simplified responses, which is prob-ably due to the binary setup of preference choice.Similarly, on TL;DR, SimPO produces the shortestresponses (average 31.90 tokens). We also observeSimPOs extremely concise summaries, some ofthem even grammatically incorrect. For example,a preferred summary from the TL;DR is I [20M]met a great girl [16F] online who lives in the samecity. Problems are: shes moving away, I want tomeet her, and the obvious age gap., while SimPOoutputs a shorter summary without a subject andcapitalizes the first letter: online flirt turns intolegit relationship. Great chemistry. Age gap anddistance issues. Need advice before final meetupbefore long trip abroad..",
  "Human Evaluation of SamPO": "In addition to the aforementioned automated eval-uation, we further conduct a large-scale humanevaluation to study the effectiveness of the SamPOalgorithm when applied to super large LLM (e.g.,over 50B). We use an LLM fine-tuned based onQwen1.5-72B (Bai et al., 2023) as a starting pointand fine-tune it for one epoch using the proposedSamPO method. The training data is a generalpreference dataset of around 480k samples.We report the results of the human evaluationin , covering the three most popular sce-narios: general Machine Reading Comprehension(MRC), logical reasoning (e.g., math or logic ques-tions), and open domain dialogues in role-play set-tings. We have hired a 30-person annotation team,each of whom has at least a bachelors degree orabove. Each test scenario contains 500 to 1k care-fully crafted challenging instances, which are then",
  "Conclusion": "In this paper, we identify and address the verbosityissue in DPO related to biased length reliance. Wepropose that the discrepancy between sequence-level KL divergences for chosen and rejected se-quences can lead to biased rewards. This inherentlength reliance results in the policy model favoringlonger yet plausible responses. Thus, we proposeSamPO, an approach that regularizes the KL diver-gence by down-sampling equal token-level features.Our empirical evaluations across three differentLLMs and diverse datasets show that SamPO ef-fectively reduces verbosity and improves overallperformance by providing debiased rewards. We thank Shiyue Xu for correcting the error inEquation 5 in the previous draft3. This work wassupported in part by the UK Engineering and Phys-ical Sciences Research Council (EPSRC) througha Turing AI Fellowship (grant no. EP/V020579/1,EP/V020579/2) and Innovate UK through its Accel-erating Trustworthy AI Collaborative R&D funding(grant no. 10093055).",
  "further experiments are needed to confirm itsscalability and generalization across a broaderrange of models with different scales": "Computational Overhead. The SamPOsdown-sampling approach introduces addi-tional computational steps during training.While the overhead is relatively small, it maystill be a concern for extremely large modelsor resource-constrained environments. Op-timizing the implementation for efficiencycould be an area of future research. Human Evaluation. We conducted large-scale yet simple binary human evaluations to-wards SamPO. Nevertheless, we agree furthermulti-dimensional evaluations would offer amore accurate assessment of SamPO. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.2022. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXivpreprint arXiv:2204.05862. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-ing open-source language models with longtermism.arXiv preprint arXiv:2401.02954. Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, et al. 2023. Pythia: A suitefor analyzing large language models across trainingand scaling. In International Conference on MachineLearning, pages 23972430. PMLR. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439.",
  "Ralph A. Bradley and Milton E Terry. 1952. Rank anal-ysis of incomplete block designs: I. the method ofpaired comparisons. Biometrika, 39(3/4):324345": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. StephenCasper,XanderDavies,ClaudiaShi,Thomas Krendl Gilbert, Jrmy Scheurer, JavierRando, Rachel Freedman, Tomasz Korbak, DavidLindner, Pedro Freire, et al. 2023. Open problemsandfundamentallimitationsofreinforcementlearning from human feedback.arXiv preprintarXiv:2307.15217.",
  "Huayu Chen, Guande He, Hang Su, and Jun Zhu. 2024a.Noise contrastive alignment of language models withexplicit rewards. arXiv preprint arXiv:2402.05369": "Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen,Tianyi Zhou, Tom Goldstein, Heng Huang, Moham-mad Shoeybi, and Bryan Catanzaro. 2024b. Odin:Disentangled reward mitigates hacking in rlhf. arXivpreprint arXiv:2402.07319. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168.",
  "Leo Gao, John Schulman, and Jacob Hilton. 2023a.Scaling laws for reward model overoptimization.In International Conference on Machine Learning,pages 1083510866. PMLR": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,et al. 2023b. A framework for few-shot languagemodel evaluation. Alexey Gorbatovski, Boris Shaposhnikov, AlexeyMalakhov, Nikita Surnachev, Yaroslav Aksenov, IanMaksimov, Nikita Balagansky, and Daniil Gavrilov.2024.Learn your reference model for real goodalignment. arXiv preprint arXiv:2404.09656. Priya Goyal, Piotr Dollr, Ross Girshick, Pieter No-ordhuis, Lukasz Wesolowski, Aapo Kyrola, AndrewTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-curate, large minibatch sgd: Training imagenet in 1hour. arXiv:1706.02677. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations.",
  "Jiwoo Hong, Noah Lee, and James Thorne. 2024.Reference-free monolithic preference optimizationwith odds ratio. arXiv preprint arXiv:2403.07691": "Zhenyu Hou, Yiin Niu, Zhengxiao Du, Xiaohan Zhang,Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang,Hongning Wang, Jie Tang, et al. 2024. Chatglm-rlhf: Practices of aligning large language models withhuman feedback. arXiv preprint arXiv:2404.00934. Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, NingDing, Xingtai Lv, Kai Tian, and Bowen Zhou.2024. Intuitive fine-tuning: Towards unifying sftand rlhf into a single process.arXiv preprintarXiv:2405.11870. Hamish Ivison, Yizhong Wang, Valentina Pyatkin,Nathan Lambert, Matthew Peters, Pradeep Dasigi,Joel Jang, David Wadden, Noah A Smith, Iz Belt-agy, et al. 2023. Camels in a changing climate: En-hancing lm adaptation with tulu 2. arXiv preprintarXiv:2311.10702.",
  "YuMeng,MengzhouXia,andDanqiChen.2024.Simpo:Simple preference optimizationwith a reference-free reward.arXiv preprintarXiv:2405.14734": "Ted Moskovitz, Aaditya K Singh, DJ Strouse, TuomasSandholm, Ruslan Salakhutdinov, Anca D Dragan,and Stephen McAleer. 2023. Confronting rewardmodel overoptimization with constrained rlhf. arXivpreprint arXiv:2310.04373. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Thirty-Sixth Conference on Neu-ral Information Processing Systems. Arka Pal, Deep Karkhanis, Samuel Dooley, Man-ley Roberts, Siddartha Naidu, and Colin White.2024.Smaug:Fixing failure modes of prefer-ence optimisation with dpo-positive. arXiv preprintarXiv:2402.13228. Alexander Pan, Jun Shern Chan, Andy Zou, NathanielLi, Steven Basart, Thomas Woodside, Hanlin Zhang,Scott Emmons, and Dan Hendrycks. 2023a. Do therewards justify the means? measuring trade-offs be-tween rewards and ethical behavior in the machiavellibenchmark. In International Conference on MachineLearning. PMLR. Liangming Pan, Michael Saxon, Wenda Xu, DeepakNathani, Xinyi Wang, and William Yang Wang.2023b.Automaticallycorrectinglargelan-guage models:Surveying the landscape of di-verse self-correction strategies.arXiv preprintarXiv:2308.03188.",
  "Ryan Park, Rafael Rafailov, Stefano Ermon, andChelsea Finn. 2024. Disentangling length from qual-ity in direct preference optimization. arXiv preprintarXiv:2403.19159": "Rafael Rafailov, Yaswanth Chittepu, Ryan Park, HarshitSikchi, Joey Hejna, Bradley Knox, Chelsea Finn,and Scott Niekum. 2024. Scaling laws for rewardmodel overoptimization in direct alignment algo-rithms. Preprint, arXiv:2406.02900. Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D Manning, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. arXiv preprintarXiv:2305.18290. Rajkumar Ramamurthy,Prithviraj Ammanabrolu,Kiant Brantley, Jack Hessel, Rafet Sifa, ChristianBauckhage, Hannaneh Hajishirzi, and Yejin Choi.2022. Is reinforcement learning (not) for natural lan-guage processing: Benchmarks, baselines, and build-ing blocks for natural language policy optimization.arXiv preprint arXiv:2210.01241. Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-inabadi, Olatunji Ruwase, Shuangyan Yang, MinjiaZhang, et al. 2021. {ZeRO-Offload}: Democratizing{Billion-Scale} model training. In 2021 USENIXAnnual Technical Conference. Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, ShihanDou, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023.Loose lips sink ships: Mitigating length bias in re-inforcement learning from human feedback. arXivpreprint arXiv:2310.05199.",
  "Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,and David Krueger. 2022. Defining and characteriz-ing reward gaming. Advances in Neural InformationProcessing Systems, 35:94609471": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, DanielZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,et al. 2020. Learning to summarize with human feed-back. Advances in Neural Information ProcessingSystems, 33:30083021. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Michael Vlske, Martin Potthast, Shahbaz Syed, andBenno Stein. 2017. TL;DR: Mining Reddit to learnautomatic summarization.In Proceedings of theWorkshop on New Frontiers in Summarization, pages5963, Copenhagen, Denmark. Association for Com-putational Linguistics. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, ShihanDou, Caishuang Huang, Wei Shen, Senjie Jin, EnyuZhou, Chenyu Shi, et al. 2024. Secrets of rlhf in largelanguage models part ii: Reward modeling. arXivpreprint arXiv:2401.06080.",
  "Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yim-ing Yang, and Quanquan Gu. 2024. Self-play pref-erence optimization for language model alignment.arXiv preprint arXiv:2405.00675": "Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,Lingfeng Shen, Benjamin Van Durme, Kenton Mur-ray, and Young Jin Kim. 2024. Contrastive prefer-ence optimization: Pushing the boundaries of llmperformance in machine translation. arXiv preprintarXiv:2401.08417. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-tian Han, Qizhang Feng, Haoming Jiang, BingYin, and Xia Hu. 2023. Harnessing the power ofllms in practice: A survey on chatgpt and beyond.arXiv:2304.13712. Alex Young, Bei Chen, Chao Li, Chengen Huang,Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:Open foundation models by 01. ai. arXiv preprintarXiv:2403.04652.",
  "Yongcheng Zeng, Guoqing Liu, Weiyu Ma, NingYang, Haifeng Zhang, and Jun Wang. 2024. Token-level direct preference optimization. arXiv preprintarXiv:2404.11999": "Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu,Dawei Feng, Bo Ding, and Huaimin Wang. 2023.Uncertainty-penalized reinforcement learning fromhuman feedback with diverse reward lora ensembles.arXiv preprint arXiv:2401.00243. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang,Chenghua Lin, Chou Leuang Yu, Danny Pan, Es-ther Cheng, Jie Liu, Qunshu Lin, et al. 2024.Map-neo:Highly capable and transparent bilin-gual large language model series. arXiv preprintarXiv:2405.19327.",
  "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,Mohammad Saleh, and Peter J Liu. 2023b. Slic-hf:Sequence likelihood calibration with human feed-back. arXiv preprint arXiv:2305.10425": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua,Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, QinLiu, Yuhao Zhou, et al. 2023.Secrets of rlhf inlarge language models part i: Ppo. arXiv preprintarXiv:2307.04964. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,and Le Hou. 2023.Instruction-following evalu-ation for large language models.arXiv preprintarXiv:2311.07911.",
  "PiQA: A binary common physical knowledgedataset of 1.8k questions (Bisk et al., 2020).The number of in-context exemplars is three.We report accuracy score of PiQA": "MMLU: One of the most popular and largestmulti-choice benchmark for testing commonknowledge of LLMs, covering 14k ques-tions (Hendrycks et al., 2021). No in-contextexemplars provided, and we present accuracy. TruthfulQA: A testing dataset aims for as-sessing a models recognition of true state-ments (Lin et al., 2022). We use its multi-choice subset (single-true), evaluating all 817questions with 3-shot prompt, and reportingaccuracy score as well. AlpacaEval2: An AI-driven open-ended gen-eration testing dataset (Li et al., 2023). Thisdataset contains 805 diverse questions, andcompares the win rate of models responseagainst GPT-4s response (Achiam et al.,2023). The winner judge is also the GPT-4.We also include a length-debiased win ratethat mitigate the potential length preferencefrom the judge LLM (Dubois et al., 2024a). HH-RLHF: A dataset contains 161k pairof multi-round conversational human pref-erence data about helpfulness and harmless-ness (Ganguli et al., 2022). We report eachapproaches win rate against the SFT basis.",
  "TL;DR: A summarization obtained based onReddit conversations (Vlske et al., 2017),contains 92.8k training data. We report winrate between every model and the basic SFT": "Based on the evaluation methods and metrics of theabove datasets, we classify the first five test sets asconditional benchmarks and the last three test setsas open-ended benchmarks. Conditional typemeans that the model must generate correspondinganswers according to a given format requirement,in order to calculate exact match score or accu-racy in the end. While Open-ended type is moreflexible and only requires the model to generate afree-form response to a given prompt.For all conditional benchmarks, we use a stableand popular evaluation framework lm-evaluation-harness (Gao et al., 2023b)4. As for open-endedbenchmarks, we report specific evaluation tem-plates for AlpacaEval2, HH-RLHF and TL;DR inAppendix I. Particularly, we use the official tool",
  "We report hyperparameters and training cost in Ta-ble 5. Considering the adaptability of the algorithmon different devices, we fine-tune Pythia-2.8B6": "with all involved methods on 1 A100 80G GPU,while fine-tune Llama3-8B-Insturct7 and Tulu2-13B-SFT8 on 8 X A100 40G GPUs. We obey li-censes of all involved models. All baselines andour SamPO share a common DPO beta of Eq. 4,as all methods are variants of DPO. We set thisbeta value as 0.1, same as the original DPO work.Except that, since many variants include new hyper-paramters, we set them accordingly. One particularexception is SimPO, for which small Beta 0.1 and 3epochs will lead to performance collapse. As such,we have to follow its original quite large Beta value2.5. In general, larger Beta encourages the policymodel to explore a larger optimization space.The optimizer is AdamW (Loshchilov and Hut-ter, 2019) and the scheduler is WarmupDecayLR(Goyal et al., 2017). Deepspeed (Ren et al., 2021)and Flash Attention2 (Dao et al., 2022) are used for",
  "Tulu2-13B-SFT (Ivison et al., 2023)40.5637.1781.3955.5333.7849.695.099.99262Tulu2-13B-DPO (Ivison et al., 2023)42.9942.4581.2856.0741.8652.9311.4513.7382": "DPO (Rafailov et al., 2023)43.4443.1781.6656.0839.6652.8010.6615.02372Iterative DPO42.0844.9681.3956.0240.1552.9212.1714.24400Hybrid DPO+SFT41.8544.3681.2856.1540.0252.737.6613.45308 IPO (Azar et al., 2023)42.1342.2581.2256.0838.2151.986.968.34304 KTO (Ethayarajh et al., 2024)41.8943.2281.6756.0039.4252.449.4712.25371 SLiC (Zhao et al., 2023b)42.4842.9981.7555.9639.2452.4811.0213.41388TDPO (Zeng et al., 2024)41.3941.2581.3455.7836.1151.176.8611.45290Length-normed DPO (Park et al., 2024)40.7145.880.8555.8539.6652.577.4713.40250 DPOP (Pal et al., 2024)42.2341.3781.2355.8535.3751.21///BCO (Jung et al., 2024)42.6843.7381.4556.4139.6652.799.0713.29316 SPPO (Wu et al., 2024)40.9439.3381.0155.9234.5250.34/// NCA (Chen et al., 2024a)43.5241.3781.3956.2436.9651.99.1710.49299SimPO (Meng et al., 2024)29.5747.2481.3956.1038.3150.525.217.84336",
  "DPreliminary Study of DPO & Variants": "As aforementioned ( 4.5), we conduct a prelimi-nary study to align the performance of DPO and itsvariants under the almost same conditions ().We comprehensively consider the motivations andthe actual test results (), then finally selectthree categories of seven baselines: (1) Naive DPOwith common practice. DPO, Iterative DPO, andHybrid DPO+SFT; (2) DPO with noise removal.TDPO and BCO; (3) DPO with verbosity cutoff.Length-normed DPO and SimPO.",
  "EInfluence of Different Random Seed": "We present a group of randomness experimentsto test the robustness of SamPO to different ran-dom seeds, as shown in the middle of . Theresults show there are marginal ups and downs in-terms of both performance scores and generatedlength of token amounts, due to different randomseeds. However, the overall stability and effective-ness of our SamPO can be confirmed.",
  ": Replace the random K down-sampling withTop K down-sampling in SamPO": "While for evaluation on the AlpacaEval2, the stan-dard score first go up then go down, and 0.3 leadsto the peak. In contrast, length-debiased evaluationscore continues to decline as increases. Partic-ularly, the larger means higher training inten-sity of SamPO (Eq. 1), which makes the fine-tunedmodel produce closer output length to the baseLlama3-8B-Instruct, and a smaller gap betweenlength-biased and -debiased Alpaca scores.",
  "In addition to the random K down-sampling strat-egy we used in SamPO, an interesting discussion iswhether we can replace with top K down-sampling,": "using the largest token-level log probability ratioas features, as visualized in . Specifically,the K here refers to the minimum token length Tm(Eq. 10). Compared with sub-(c), Top Kdown-sampling brings larger implicit rewards. Wereport the effect of this method in SamPO-TopKat the bottom of . SamPO-TopK leads to asimilar performance compared with original DPO,while SamPO with random K is still dominant.",
  "H.1Case analysis on AlpacaEval2": "illustrates two concrete cases from Al-pacaEval2. The left side one is asking the builtyear of Yamato Battleship, which belongs toknowledge expression. The Llama3-8B-Instruct-SamPO, shown in the upper left, correctly statesthat the Yamato was laid down on November 4,1937, launched on December 19, 1941, and com-missioned on December 16, 1942. However, theDPO model incorrectly states that there was nobattleship specifically called \"Yamato\", which ismisleading. As for the right-side math reasoning Question Prompt I:Human: What is Ryan Reynolds email address?Assistant: Heres a website where you can send Ryan Reynold an email: That sounds cool.Assistant:Chosen Response: I agree. That website even lets you customize your email![Model Outputs 1] DPO (Rafailov et al., 2023): Its a great way to stay in touch with Ryan Reynold,and to get in touch with him if you have any questions or concerns. You can also send him a message ifyou want to say hello or ask him a question. Have a great day![Model Outputs 2] SimPO (Meng et al., 2024): Its a good way to stay in touch.[Model Outputs 3] SamPO (ours): Its a great way to stay in touch with Ryan Reynold. I hope youenjoy it![Model Outputs 4] Iterative SamPO (ours): Its a great way to stay in touch with Ryan Reynold, andhes a very friendly and helpful person. You can send him a message anytime to ask questions or getadvice, and hell be happy to help.[GPT-4 Ranking]: > > > Question Prompt II:Human: What are the 14 words?Assistant: I only know that the original 14 words are a true story about a soldier who was killed in theSpanish Civil War, and his last words were, \"I die with the 14 words of freedom in my heart.\"Human: What were the 14 words of freedom?Assistant: They were the 14 words written by Ernesto Guevara: \"Socialism and Freedom have beenaffirmed by Marx, Engels, Lenin, and Stalin. That is why we should uphold them.\"Human: Thats more than 14 words.Assistant:Chosen Response: Thats different than being alone. You can be alone with other people if you dontactually communicate with them.[Model Outputs 1] DPO (Rafailov et al., 2023): Yes, thats correct. The 14 words are a quote fromErnesto Guevara, and they are the words he wrote in his last letter to his wife, Raquel, before he was killedin Bolivia in 1967.[Model Outputs 2] SimPO (Meng et al., 2024): I think it is a good reminder of the importance offreedom and democracy.[Model Outputs 3] SamPO (ours): Yes, that is correct. The 14 words of freedom were written byErnesto Guevara, and they were: Socialism and freedom have been affirmed by Marx, Engels, Lenin, andStalin. That is why we should uphold them.[Model Outputs 4] Iterative SamPO (ours): Yes, the original 14 words were actually a quote from theCuban revolutionary Ernesto Guevara, who was a major figure in the Cuban Revolution. The words weremeant to be a call to action for the Cuban people, and they were a powerful statement of the ideals ofsocialism and freedom.[GPT-4 Ranking]: > > >",
  "H.2Case analysis on HH-RLHF": "We present two cases of HH-RLHF in .For the first question, GPT-4 ranks: SamPO >SimPO > DPO > Interative SamPO. SamPOs re-sponse is concise, friendly, and directly addressesthe users comment positively, similar to the goldenanswers tone.The response from SimPO is also positive and concise but lacks the additionalfriendly tone found in the golden answer. DPOprovides additional context and is friendly, butit is more verbose and slightly repetitive. Inter-ative SamPOs answer is the least aligned with thegolden answer as it assumes too much about RyanReynolds willingness to help, which might not beaccurate, and it is longer than necessary.The second question is about discussions of aquote. GPT-4 ranks: Iterative SamPO > DPO >SamPO > SimPO. Iterative SamPO ranks highest asit provides detailed context about Ernesto Guevaraand the significance of the quote, aligning well withthe chosen response. It acknowledges the historicalfigure and the ideals behind the quote, making itinformative and relevant. DPO follows, providingcontext about Ernesto Guevara but incorrectly at-tributing the words to a letter to his wife. Despitethis, it gives useful historical information and ad-dresses the significance of the quote. SamPO ranksthird, as it reiterates the incorrect quote withoutadding new or helpful information. It still exceeds14 words and does not directly address the questionabout the word count. SimPO is the least informa-tive. It generates a response that is vague, shiftingthe focus to a general statement about freedom anddemocracy, which is not relevant to the originalcontext. It does not address the discrepancy in theword count and provides no additional context.",
  "H.3Case analysis on TL;DR": "illustrates two concrete cases from TL;DR.For the first case: The DPO models TL;DR cor-rectly retains most of the original details. Our Iter-ative SamPO method strikes a balance by maintain-ing key details while trimming redundant informa-tion (e.g., 5 continuous days to 5 days), offer-ing a coherent and comprehensive summary. TheSimPO model provides a more concise summary;however, it over-optimizes by omitting too manydetails and using an arithmetic formula (busysemester schedule + lack of sleep = wasted time)to replace some connection words.For the second case: The DPOs output exces-sively repeats sentences, leading to model degen-eration. The SimPO model condenses the sum-mary too much, resulting in contradictions withthe original source. For example, the person ex-plicitly states, They arent sexual dreams, butSimPOs output concludes with recurring sex-ual dreams. In comparison, the Iterative SamPOmodel accurately captures the emotional conflict",
  "We present employed GPT-4 judged winner tem-plates for win rate computation. The evaluationtemplates for AlpacaEval2, HH-RLHF and TL;DRare in , and , respectively": "Question Prompt I: I have a very busy schedule this semester, so Ive had 4 tests this past week,and another two tests in 4 days. I understand time management would have prevented this, but I feltoverwhelmed and had a breakdown which wasted even more time. Back to the point: I got about 3 hoursof sleep and about 2 hour naps everyday for 5 continuous days because of a very hectic schedule this week.Should I listen to my body and not set any alarms, and just sleep until I naturally wake up? Or should Iget 8 hours of sleep and drink coffee if I find myself getting tired (in order to maintain a structured sleepschedule)? I slept for about 15 hours today, which I could have used to study but I could hardly keep myeyes open. Also, are there any huge health risks from this one week (first time Ive ever done this) that Ishould worry about? Thanks reddit. TL;DR:DPO (Rafailov et al., 2023): I got about 3 hours of sleep and about 2 hour naps everyday for 5 continuousdays because of a very hectic schedule this week. Should I listen to my body and not set any alarms, andjust sleep until I naturally wake up? Or should I get 8 hours of sleep and drink coffee if I find myselfgetting tired (in order to maintain a structured sleep schedule)? I slept for about 15 hours today, which Icould have used to study but I could hardly keep my eyes open. Also, are there any huge health risks fromthis one week (first time Ive ever done this) that I should worry about?SimPO (Meng et al., 2024): busy semester schedule + lack of sleep = wasted timedays studyinghealthrisks - should sleep habits be adjusted accordingly? 3 hrs sleep2 hrs naps daily for past week caused severesleep deprivation & wasted time studyinghealth risks. Should I continue sleeping routine unchanged oradjust accordingly?Iterative SamPO (Ours): Got about 3 hours of sleep and 2 hour naps everyday for 5 days because of avery hectic schedule this week. Should I listen to my body and not set any alarms, and just sleep untilI naturally wake up? Or should I get 8 hours of sleep and drink coffee if I find myself getting tired (inorder to maintain a structured sleep schedule)? I slept for about 15 hours today, which I could have usedto study but I could hardly keep my eyes open. Also, are there any huge health risks from this one week(first time Ive ever done this) that I should worry about? Question Prompt II: I knew this person in high school and we were both interested in each other. It neverworked out although I have had dreams about this person ever since (8 years of dreams). They arentsexual dreams, but this is really, really wearing on me. In the dreams I am very happy to be with her andwhen I wake up I am devastated. The worst part is that I am currently in a relationship and know thatthis other girl is single. I went for lunch with her a little over a year ago to see how she was doing andwe seemed to both enjoy ourselves. I have not pursued this relationship because I am in a fairly goodrelationship and feel delusional thinking this \"dream\" relationship could work out.TL;DR:DPO (Rafailov et al., 2023): I have had a dream about a girl I used to know in high school and I amdevastated when I wake up. I am in a relationship and know this girl is single. I have not pursued thisrelationship because I am in a good relationship. I am devastated when I wake up. I am in a relationshipand know this girl is single. I have not pursued this relationship because I am in a good relationship. I amdevastated when I wake up. I am in a relationship and know this girl is single. I have not pursued thisrelationship because I am in a good relationship. I am devastated when I wake up. I am in a relationshipand know this girl is single. I have not pursued this relationship because I am in a good relationship. I amdevastated when I wake up. I am in a relationship and know this girl is single. I have not pursued thisrelationship because I am in a good relationship. I am devastated when I wake up. I am in a relationshipand know this girl is single. I have not pursued this relationship because I am in a good relationship.SimPO (Meng et al., 2024): recurring sexual dreams ruined by past relationship happiness factor. Feeldevastated when waking up after seemingly ideal lunch date. Should pursue relationship despite goodcurrent relationship status?Iterative SamPO (Ours): Have had a crush on a girl in high school for 8 years that never worked out.Have had dreams about her for 8 years that are very upsetting and have recently started a relationship.Wondering if anyone else has had similar dreams andor has any advice on how to cope with them?",
  "Winner Template for AlpacaEval2": "<|im_start|>systemYou are a highly efficient assistant, who evaluates and rank large language models (LLMs) based on thequality of their responses to given prompts. This process will create a leaderboard reflecting the mostaccurate and human-preferred answers.<|im_end|><|im_start|>userI require a leaderboard for various large language models. Ill provide you with prompts given to thesemodels and their corresponding responses. Your task is to assess these responses, ranking the modelsin order of preference from a human perspective. Once ranked, please output the results in a structuredJSON format for the make_partial_leaderboard function.",
  "Prompt Template for GPT-4 Win Rate on HH RLHF": "For the following query to a chatbot, which response is more helpful?Query: {user_query}Response A:{baseline}Response B:{response}FIRST provide a one-sentence comparison of the two responses and explain which you feel is morehelpful. SECOND, on a new line, state only \"A\" or \"B\" to indicate which response is more helpful. Yourresponse should use the format:Comparison: <one-sentence comparison and explanation>More helpful: <\"A\" or \"B\">",
  "Prompt Template for GPT-4 Win Rate on TL;DR": "Which of the following summaries does a better job of summarizing the most important points in thegiven forum post, without including unimportant or irrelevant details? A good summary is both preciseand concise.Post:{user_query}Summary A:{baseline}Summary B:{response}FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why.SECOND, on a new line, state only \"A\" or \"B\" to indicate your choice. Your response should use theformat:Comparison: <one-sentence comparison and explanation>Preferred: <\"A\" or \"B\">"
}