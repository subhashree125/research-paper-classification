{
  "Abstract": "We introduce iterative retrieval, a novel frame-work that empowers retrievers to make iterativedecisions through policy optimization. Find-ing an optimal portfolio of retrieved items isa combinatorial optimization problem, gener-ally considered NP-hard. This approach pro-vides a learned approximation to such a solu-tion, meeting specific task requirements under agiven family of large language models (LLMs).We propose a training procedure based on re-inforcement learning, incorporating feedbackfrom LLMs.We instantiate an iterative re-triever for composing in-context learning (ICL)exemplars and apply it to various semantic pars-ing tasks that demand synthesized programsas outputs. By adding only 4M additional pa-rameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterativeretriever, outperforming previous methods inselecting ICL exemplars on semantic parsingdatasets such as SMCALFLOW, TREEDST,and MTOP. Additionally, the trained iterativeretriever generalizes across different inferenceLLMs beyond the one used during training.",
  "Introduction": "A significant emergent capability of large languagemodels (LLMs) is in-context learning (ICL; Brownet al., 2020), which facilitates few-shot learning. InICL, a set of exemplars1 is usually provided to buildthe mapping relationship between inputs and out-puts. These exemplars can either be hand-craftedand fixed or retrieved from a training set. However,if retrieving from the dataset, the retrievers used insuch applications are typically off-the-shelf models(e.g., Contriever (Izacard et al., 2022)) that do notconsider interactions among retrieved items when Johns Hopkins University. Performed while interning atMicrosoft. Google. Performed while at Microsoft.1 An exemplar is a tuple of input and output, demonstratingthe mapping relationship between the two.",
  ": Above: ICL under a single retriever call. Be-low: ICL under our proposed iterative retriever": "multiple targets are required, nor the specific char-acteristics of the inference LLMs and downstreamtask requirements. Research (Gao et al., 2021; Liuet al., 2022; Lu et al., 2022, i.a.) has shown thatICL is sensitive to both the exemplars providedand their order within prompts. Off-the-shelf re-trievers, which generally rank items based solelyon semantic similarity (Lee et al., 2019; Reimersand Gurevych, 2019a, i.a.), do not ensure optimalconditions for either criterion, leading to subopti-mal performance in downstream LLM generation.Hence, there is a need for a retriever capable ofconstructing a portfolio of items tailored to achieveoptimal generation with LLMs.We propose iterative retrieval to address thisproblem. Unlike traditional retrievers that performa single call to obtain a list of similar items orderedby their similarities, iterative retrieval involves a se-quence of retrieval calls, each using different queryvectors. This makes the retriever stateful, maintain-ing an internal state. The process can be likened tonavigating the encoding space of exemplars, witheach step adjusting direction based on previouslyselected exemplars, thus building a trajectory ofexemplar selections.This approach can be formulated as Markov de-cision processes (MDPs). At each step, the actiontaken by the retriever is a retrieval call that fetches (potentially multiple) documents from the datasetD.2 The policy is trained to optimally select ex-emplars at each step so that the overall trajectorymaximizes the reward, leading to better ICL perfor-mance. By leveraging the LLMs as environments,we create simulators that allow a policy to roll outin the environment and receive feedback on theeffectiveness of the composed prompts, measuredby a reward (metric). Thus, exemplar selection andprompt composition can be framed as policy opti-mization aimed at maximizing rewards, which canbe addressed through reinforcement learning.We situate our study in in-context semantic pars-ing due to its difficulty, popularity, and practicalvalue.3 We instantiate an iterative retriever and in-vestigate the performance of policy learning underthis setup. Our contributions include:",
  "Overview of an Iterative Retriever": "We consider the problem of in-context learning(ICL): given a dataset D = {(, )} of exem-plars, a retriever retrieves a sequence of exem-plars () based on input query and generate theanswer based on the distribution LM(|; ()).This retriever : X D retrieves an orderedlist (of length ) of exemplars for the LM. Thegoal of the retriever is to select a sequence ofexemplars ((, ))1 such that the probabilityof the expected output is maximized:",
  "() = arg top(,)D(, (, ))(2)": "Prior work has differed on the choice of the scor-ing function : BM25 (Roy et al., 2023), coverage(Gupta et al., 2022), etc. However, such methoddid not model the interaction between the retrievedexemplars and the language model. We propose aniterative version, where we create a retrieval state, and for each step one exemplar (, ) D isretrieved. This is an approximation to the optimiza-tion problem in Equation 1.",
  "The reward function : D D R funnelssignal from the LLM back to the retriever. It willbe discussed in 4": "By situating our proposed iterative retriever un-der this RL scenario, we can utilize all sorts of RLtechniques to train this retriever from the environ-ment, which is the LLM itself. In the next section,we instantiate a neural iterative retriever and situateit under a common task, namely semantic parsing,under this ICL framework.",
  "J4Z8Jz/Iz2g/ehO9i07X1KhS9rwkGxFd/gbd5snH</latexit>D": ": ICL prompt construction for an example in SMCALFLOW. Above: ICL with BM25 as the retriever. Below:An instance of our iterative retriever. BM25 retrieves examples that overlaps lexically with the query, whereas thetrained iterative retriever is better at retrieving structurally similar exemplars since it is trained to maximize theprobability of the LM generating the reference parse.",
  "((, )|) exp(Q(s) Fenc()/)(5)": "where Q : R R maps a state vector s toa query vector q, Fenc : R is a textembedder that maps a text sequence into a vector,and is a temperature hyperparameter. In ourexperiments, Fenc is initialized with the weightsof Contriever (Izacard et al., 2022), a general-purpose text embedder trained for retrieval.",
  "where the encoded vector of the retrieved exem-plar is passed to the GRU to update the state.4": "Note that the only additional parameters we in-cluded in this neural iterative retriever is the statetransition model, where we instantiate as a GRU.This is different from a regular retriever, wherea single retrieval call to the training set () =arg max(,)D q Fenc() is made. The iterativeretriever navigates the encoding space of exem-plars, adjusting the query vector q at each stepbased on previously selected exemplars , thussteering the search process to find new candidates. demonstrates the process of such an it-erative retriever. This stateful design allows foroptimized retrieval results through iterative inter-actions, incorporating signals from both external",
  "Training": "Environment SimulatorTo construct feedback(or reward) from the underlying LLMs, we treatLLMs as environments where actions are per-formed and evaluated.We design an iterativeprompting schedule within this LLM environmentto simulate the process of iterative retrieval andcorresponding ICL prompt execution. At each step, the current sequence of chosen exemplars, , isturned into an LLM prompt using a predefined tem-plate,5 then used for LLM generation. This sched-ule effectively simulates the real-world scenario ofprompting LLMs, allowing us to observe variousexecution dynamics, such as generated hypothesesand their probabilities. Reward DesignTechnically, if the final task met-ric were available, it can be used directly as thereward to optimize for. However, such a reward isoften too coarse to reflect differences in partiallycorrect results. For example, if the metric is exactmatch, which is common in semantic parsing tasks,the reward would simply be the Kronecker delta(, ), yielding 1 only if the prediction exactlymatches the reference , and 0 otherwise.Given that the LLM simulator provides access tothe probabilities of generated sequences,6 we em-ploy a more general reward design that is not task-specific. Our reward leverages the LM completionprobability of the reference sequence LM(|)(Shin et al., 2021; Shi et al., 2023), which capturessubtle changes in the likelihood of the LM generat-ing the target sequence with respect to changes inthe input . In ICL, more exemplars typically resultin better performance before reaching saturation.Inspired by Zhang et al. (2022), We further refinethe reward to reflect the increase in the likelihoodof the reference given the prompt. This is aproxy value that measure how much this exemplarcontribute to generating the reference parse. Thisdesign encourages the model to select exemplarsthat most significantly contribute to the final result 5 Refer to Appendix A.3 for the template used in this work.6 This is generally accessible in many LLM inferenceimplementations such as vLLM (Kwon et al., 2023). ForOpenAI-style APIs, this can be accessed using the echoparameter.",
  "where = ( |)old ( |) is a probability ratio be-": "tween action 8 performed against the current pol-icy and old policy old at state , clip() clips to be within (1 , 1 + ) and is the advantage.Advantage at step describes how much bet-ter it is to take a specific action at state , overrandomly selecting an action according to (|).To compute it, besides the neural model defined in3, we follow common practice in reinforcementfrom human feedback (RLHF; Huang et al., 2024)to add a single linear layer to serve as a state-valuefunction () = vs that maps states to values. Gen-eralized advantage estimate (GAE; Schulman et al.,2016) is then used for variance-reduced advantageestimation atop the learned state-value function:",
  "State Transition InitializationWe found thatinitializing the state transition function as an iden-tity function (i.e., states do not change at all:": "7 We experimented with various other RL algorithms (in-cluding policy gradient (Sutton et al., 1999) and advantageactor critic (A2C; Mnih et al., 2016)) and found that PPO isthe most stable one for our scenario.8 describes that the action of an iterative retriever is toretrieve an exemplar from a candidate set, hence D.",
  "(, ) = ) helps stabilizing training. This canbe achieved in GRU by initializing the bias of theupdate gate to = : this renders the GRU notforgetting and not updating": "Sampling & Collecting ExperienceIn a singleretrieval step, the retriever selects an exemplar froma candidate set, with the policy (|) defining aprobability distribution over candidates D. Inthis RL simulation, it is crucial to sample differentactions at each step to enable the model to explorevarious trajectories and benefit from those that yieldhigher rewards. However, constructing the entiredistribution and sampling from it at each step iscomputationally infeasible, especially when thenumber of candidates exceeds 100K. Furthermore,these distributions often exhibit a long-tailed nature,where many candidates have low scores, suggestingthat a significant portion of candidates may be lesssimilar and potentially less useful for ICL.To address these challenges and reduce the com-putational cost of sampling trajectories while man-aging the trade-offs between exploration and ex-ploitation, we propose a stratified sampling (strata) method to construct a modified policy thatcontains candidates.To start, we construct a buffer with top- exem-plars retrieved with Equation 5. Retain the top/ samples in the policy. Split the rest into( 1) strata, and sample / from each. Com-bine all these selected exemplars and renormal-ize these scores with softmax (with temperaturerenorm). This method enables the model to focuson more promising candidates while still allowingfor exploration (see for an illustration).During training, experience replay (Lin, 1992)is employed to improve training efficiency. To col-lect experience, we run inference with the currentpolicy fixed on several training examples to gen-erate trajectories. At each step, information such as policy, reward, and value is recorded. These tra-jectories are stored in a replay buffer, then shuffledand split into mini-batches for policy optimization.This approach allows the same experiences to bereplayed multiple times, reducing the number ofrequired simulation runs.",
  "Experimental Setup": "DatasetsWe validate our pilot iterative retrieverfor ICL on a set of semantic parsing datasets,namely SMCALFLOW (Andreas et al., 2020),TREEDST (Cheng et al., 2020), and MTOP (En-glish portion only; Li et al., 2021), following theBenchClamp benchmark (Roy et al., 2023). Sam-ples of representations are shown in . Forstatistics, see Appendix A.1. BaselinesWe compare our iterative retriever(henceforth denoted as ITERR) with a range of off-the-shelf retrievers, including BM25 (Robertsonand Zaragoza, 2009) and a dense encoder, Con-triever (Izacard et al., 2022). Additionally, webenchmark against two strong baselines from priorwork on improving exemplar selection: EPR (Rubin et al., 2022) is an efficient exemplarretrieval method for in-context learning (ICL)that leverages a scoring LM to label positiveand negative training examples, then using thisdataset for a contrastively learned dense retriever.",
  "CEIL (Ye et al., 2023) uses determinantal pointprocesses (DPPs) to model the interaction be-tween the given input and in-context exemplars": "For the EPR baseline, we replace the base denseretrieval encoder with Contriever instead of S-BERT (Reimers and Gurevych, 2019b) for faircomparison. Following Ye et al. (2023), we use thetrained EPR model as initialization for CEIL. Simi-larly, the same EPR checkpoint is used to initializethe text encoder in ITERR. Note that in ITERR, wefreeze the weights of the EPR encoder and onlytrain the GRU-based state transition function, pol-icy network, and value network, resulting in 4Mmore parameters compared to the original Con-triever (110M 114M).For retrievers without iterative capabilities, wetake only the top- retrieved items and keepingtheir original ranks. For EPR, CEIL, and ITERR,we selected the best performing model checkpointson the validation set. All generation is run with 10exemplars; i.e. = 10. Generation with LLMsThe inference LLMis essential for executing input prompts to gen-erate responses.In our experiments, we useLlama-2-7b (Touvron et al., 2023) to build theenvironment simulator and train the policy usingits signals. With the learned policy, we investi-gate both intra-family and inter-family generaliza-tion by replacing the inference LLMs. For modelswithin the same Llama-2 family, we explore vari-ous model sizes and finetuned versions, includingCodeLlama-70b-Instruct (Rozire et al., 2023),a model further fine-tuned for code generation. Forinter-family experiments, we choose Mistral-7b(Jiang et al., 2023). For decoding configurations,we consistently use beam search with beam size 3and sampling temperature 0.5.",
  "HyperparametersPlease refer to Appendix A.2": "Evaluation MetricsWe follow prior work inevaluating semantic parsing (Roy et al., 2023),where exact match at (EM@) is used. Exactmatch results for top- decoded hypotheses reflectsbeam search decoding used in LLMs, where multi-ple parsing results are generated simultaneously.However, EM is a stringent metric, penalizingeven minor mismatches. For instance, a parse witha substructure reordered differently (a && b) fromthe reference (b && a) is still correct but wouldscore zero under EM. This is problematic in se-mantic parsing, where target parses are composi-tional, making it important to assess the correct-ness of substructures. Since SMCALFLOW andTREEDST involve deeply nested structures, wealso adopt SMatch (Cai and Knight, 2013), follow-ing Chen et al. (2023), to evaluate performanceon substructures. SMatch is designed to evaluateAMRs (Langkilde and Knight, 1998). Generatedcode can be transformed to AMRs by treating eachfunctions return value as an entity and each argu-ment to a function as a value, where the parametername is the relation. See Appendix B for details.",
  "Results & Analyses": "We evaluate the performance of different retrieversby comparing their downstream ICL performanceon semantic parsing (). ITERR outperformsall baselines across three datasets on all metrics.The gain in EM is intuitive since it aligns withthe training objective, which involves the probabil-ity of generating target parses. The improvementin SMatch indicates that ITERR optimizes retrieval",
  "ITERR63.970.971.0---": ": Comparison of our approach, ITERR againstbaselines. EM@ denotes exact match at top-; P,R and F denote precision, recall, and F1 score re-spectively. Experiment results are run with 10 exem-plars in the prompt, averaged over 3 inference runs, andsignificance tests using paired -test confirm that theimprovements over Contriever, EPR, and CEIL are sta-tistically significant ( < 0.05).",
  "results to improve compositionality to some extent,even with a simple objective.9": "Generalization across Inference LLMsITERRbenefits from interactive training with an underly-ing LLM. While training incurs costs, these can beminimized by training only once, ideally using asmaller LM. Hence in this section we investigatethe generalization capabilities of ITERR trainedwith a smaller LM , but used for generation undera larger LM .In the following experiments, ITERR is trainedwith Llama-2-7b as the environment, but used for(a) intra-family LMs: variants within the Llama-2model family; and (b) inter-family LMs: Mistral(Jiang et al., 2023) from a different model family.We follow the setup described in 6, substitutingonly the LLM. As shown in , ITERR signif-icantly outperforms (> 1% gain) baselines for 75%of the settings and is comparable to a prior strongbaseline (within 1% in absolute performance) for15% of settings, demonstrating its generalization 9 While a more dedicated reward design, such as incorpo-rating various linearizations of target structures, might furtherenhance ITERRs performance. This work focuses on demon-strating the frameworks effectiveness rather than dedicatedlyoptimizing for a specific task design. : Performance comparisons on using various LLMs for inference (top row: SMCALFLOW; mid: TREEDST;bottom: MTOP). Our ITERR used in these experiments are trained with Llama-2-7b but performs retrieval of ICLexemplars used on other LLMs. within and beyond its own model family.In intra-family generalization, performance met-rics improve with larger model sizes, and ITERRconsistently outperforms all baselines. This im-provement is most evident with larger models suchas Llama-2-70b and CodeLlama-70b-Instruct.For inter-family generalization, ITERR maintainsits advantage across datasets, though this is lesspronounced than within the same model family.This is expected, as the signal from LLM simulatoris more representative for models sharing the samepre-training procedure. Notably, with Mistral, Con-triever performs worse than BM25 on MTOP, butITERR still shows improvement. This suggests thatITERR, comprising a frozen EPR and additionalGRU layers, can learn task-specific abilities notpresent in the vanilla EPR. ICL & Number of ExemplarsWe investigatedhow the performance of ITERR changes with thenumber of exemplars ({1, , 10}) used for ICLon the SMCALFLOW dataset (). ITERRconsistently outperforms baseline models acrossvarious metrics and numbers of exemplars, withone exception for the EM@3 metric when using6 exemplars. This aligns with our training objec-tive, where actions that boost performance at eachstep receive higher advantages. ITERR achieves comparable performance with fewer exemplars.CEIL shows a similar trend in EM, but itsSMatch performance lags significantly, indicatingpoorer quality in near-miss predictions comparedto ITERR. Practically, this means our method al-lows for a trade-off between performance and cost,enabling effective ICL with fewer exemplars andreducing the number of tokens processed by LLMs.",
  "Stratified sampling52.375.7": ": Results on ablation study. EPR intializationindicates the model is trained from Contriever insteadof a EPR finetuned checkpoint. + Transformer decoderreplaces GRU with a Transformer decoder. Stratifiedsampling replaces the stratified sampling described in with sampling directly from the buffer. We further conduct ablation study on compo-nents of an iterative retriever, focusing on theSMCALFLOW dataset and use Llama-2-7b whilechanging the configuration of the iterative retriever.Results are reported in . EM@1 EM@3 70.0 72.5 75.0 77.5 CalFlowV2 SMatch EPRCEILIterR",
  ": Performance comparisons across the various numbers of exemplars used for ICL": "EPR InitializationAlthough we follow priorwork in using EPR as initialization for Fenc, ouriterative retriever is agnostic to the choice of baseencoders for similarity search. Even without EPRinitialization, our training procedure still improvesperformance against Contriever ( 1% gain underContriever, but 6% gain under EPR). We seethat ITERR benefits more when using EPR initial-ization, significantly outperforming the baselines.We hypothesize that this advantage stems from twosources: (1) EPR is fine-tuned on the target dataset,making it more domain-specific; (2) EPR restruc-tures the action space, subsequently enhancing sam-ple efficiency in RL training. State Transition with Transformer DecoderIn3, we parameterize the state transition function inthe iterative retriever with a GRU. To explore alter-natives, we conducted an ablation experiment by re-placing the GRU with a more powerful Transformerdecoder, configured with 3 layers, 1024 hiddendimensions, with learnable positional encodings.Despite the increased expressiveness of the Trans-former decoder, we observed a performance drop.During training, employing the warmup technique(Xiong et al., 2020) led to a trivial solution wherethe policy learned to predict a nearly fixed trajec-tory across test examples. Disabling the warmupstabilized the training but did not improve perfor-mance. Developing a stabilized approach to trainthe Transformer decoder as a state encoder is be-yond the scope of this work, as our focus is ondemonstrating the overall framework of iterativeretrieval rather than optimizing a specific model forthe state transition function. Notably, even with theless expressive GRU, our iterative retriever success-fully learns a policy that retrieves a more optimizedsequence of ICL exemplars.",
  "Additional Related Work": "LLMs as Environment in RLLu et al. (2023)used policy gradient to learn a dense retriever forICL exemplar retrieval, but the state does not con-tain previously selected examples, and thus is not it-erative and unable to model exemplar order. Zhanget al. (2022) used -learning RL for ICL exemplarreordering, with a similar reward design like ours.However, the proposed method does not extend toexemplar retrieval, since the policy space is toolarge to be handled by -learning. Few-shot Semantic ParsingFew-shot semanticparsing using LLMs has shown impressive capa-bilities in understanding new examples with min-imal training data (Shin et al., 2021; Shin andVan Durme, 2022). However, these parsers oftenstruggle with generalization and fail to parse unob-served local structures due to their limited accessto information encoded through exemplars (Bo-gin et al., 2022). To this end, recent research hasexplored various approaches to improving exem-plar selection. EPR (Rubin et al., 2022) used aproxy LM to score outputs from an unsupervisedretriever, enabling better training of a dense re-triever. Oren et al. (2021), Gupta et al. (2022),and Levy et al. (2023) emphasize learning to selectexemplars based on particular criteria, such as di-versity measures and coverage of local structures,to enhance compositional generalization. Whilethese approaches have shown performance im-provements in semantic parsing tasks, these arehighly based on heuristics constructed from re-searchers experience. Our approach could be seenas an automated version (through RL) of seeking",
  "information useful for semantic parsing": "Multi-hop Question AnsweringThere has beenrelated work in iterative retrieval in multi-hop QA.Qi et al. (2019) iteratively generates text queriesbased on paragraphs retrieved, thus can be consid-ered as a discrete, non-neural version of our MDP.Asai et al. (2020); Xiong et al. (2021) trained adense, neural iterative retriever with oracle paths asdirect supervision. Additionally, IRCoT (Trivediet al., 2023) utilizes an iterative retriever to per-form chain-of-thought (CoT) in LLMs, in whichthe agent is the LLM, thus not trained.",
  "Conclusion": "We proposed iterative retrievers that iterativelybuilds a prompt to perform in-context learning.Such retrievers are framed as Markov decision pro-cesses and trained via policy optimization fromLLM feedback, where the policy directs which ex-emplar to append to the existing exemplar sequence.Experiments on semantic parsing demonstrated per-formance gain of iterative retrievers over variousdatasets and state-of-the-art baselines, showing thatthey are able to construct prompts that improves in-context learning and downstream LLM generation.",
  "Limitations": "In our instantiation of the iterative retriever, at eachstep a single exemplar is retrieved. One could en-vision multiple exemplars being retrieved at eachstep, thus making the RL trajectory shorter. Thiscould make RL training easier and inference faster.Our reward design depends on a particular lin-earization of the target structure. A more structuredreward function may exhibit better training behav-ior and lead to better performance.The encoder for queries in the iterative retrieveris frozen in our current setup. A trainable queryencoder that receives feedback from LLMs may bedesired, but we left that for future work.While we believe that semantic parsing / codegeneration is one of the most useful but challengingtask for LLMs, as such is a representative task forICL research, we have not tested the effectivenessof iterative retrievers under other LLM tasks.",
  "Jacob Andreas, John Bufe, David Burkett, CharlesChen, Josh Clausman, Jean Crawford, Kate Crim,Jordan DeLoach, Leah Dorner, Jason Eisner, Hao": "Fang, Alan Guo, David Hall, Kristin Hayes, KellieHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, DanKlein, Jayant Krishnamurthy, Theo Lanman, PercyLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-Govern, Aleksandr Nisnevich, Adam Pauls, DmitrijPetters, Brent Read, Dan Roth, Subhro Roy, JesseRusak, Beth Short, Div Slomin, Ben Snyder, StephonStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-drei Vorobev, Izabela Witoszko, Jason Wolfe, AbbyWray, Yuchen Zhang, and Alexander Zotov. 2020.Task-oriented dialogue as dataflow synthesis. Trans-actions of the Association for Computational Linguis-tics, 8:556571. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,Richard Socher, and Caiming Xiong. 2020. Learn-ing to retrieve reasoning paths over wikipedia graphfor question answering. In 8th International Confer-ence on Learning Representations, ICLR 2020, AddisAbaba, Ethiopia, April 26-30, 2020. OpenReview.net. Ben Bogin, Shivanshu Gupta, and Jonathan Berant.2022. Unobserved local structures make composi-tional generalization hard. In Proceedings of the2022 Conference on Empirical Methods in Natu-ral Language Processing, pages 27312747, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Shu Cai and Kevin Knight. 2013. Smatch: an evaluationmetric for semantic feature structures. In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 748752, Sofia, Bulgaria. Associationfor Computational Linguistics. Yunmo Chen, William Gantt, Tongfei Chen, AaronWhite, and Benjamin Van Durme. 2023. A unifiedview of evaluation metrics for structured prediction.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1286812882, Singapore. Association for Computa-tional Linguistics. JianpengCheng,DevangAgrawal,HctorMartnez Alonso, Shruti Bhargava, Joris Driesen,Federico Flego, Dain Kaplan, Dimitri Kartsaklis,Lin Li, Dhivya Piraviperumal, Jason D. Williams,Hong Yu, Diarmuid Saghdha, and Anders Johannsen. 2020. Conversational semantic parsingfor dialog state tracking.In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 81078117,Online. Association for Computational Linguistics.",
  "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021": "Making pre-trained language models better few-shotlearners. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 38163830, Online. Association for Computa-tional Linguistics. Shivanshu Gupta, Sameer Singh, and Matt Gardner.2022.Structurally diverse sampling for sample-efficient training and comprehensive evaluation. InFindings of the Association for Computational Lin-guistics: EMNLP 2022, pages 49664979, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Shengyi Huang, Michael Noukhovitch, Arian Hosseini,Kashif Rasul, Weixun Wang, and Lewis Tunstall.2024. The N+ implementation details of RLHF withPPO: A case study on tl;dr summarization. CoRR,abs/2403.17031. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2022. Unsupervised dense in-formation retrieval with contrastive learning. Trans.Mach. Learn. Res., 2022. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-lez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-ory management for large language model servingwith pagedattention. In Proceedings of the 29th Sym-posium on Operating Systems Principles, SOSP 2023,Koblenz, Germany, October 23-26, 2023, pages 611626. ACM.",
  "Irene Langkilde and Kevin Knight. 1998. Generationthat exploits corpus-based statistical knowledge. In": "36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics, Volume 1, pages704710, Montreal, Quebec, Canada. Association forComputational Linguistics. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.2019. Latent retrieval for weakly supervised opendomain question answering. In Proceedings of the57th Annual Meeting of the Association for Computa-tional Linguistics, pages 60866096, Florence, Italy.Association for Computational Linguistics. Itay Levy, Ben Bogin, and Jonathan Berant. 2023. Di-verse demonstrations improve in-context composi-tional generalization. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 14011422, Toronto, Canada. Association for Computa-tional Linguistics. Haoran Li, Abhinav Arora, Shuohui Chen, AnchitGupta, Sonal Gupta, and Yashar Mehdad. 2021.MTOP: A comprehensive multilingual task-orientedsemantic parsing benchmark. In Proceedings of the16th Conference of the European Chapter of the Asso-ciation for Computational Linguistics: Main Volume,pages 29502962, Online. Association for Computa-tional Linguistics.",
  "Long Ji Lin. 1992. Self-improving reactive agents basedon reinforcement learning, planning and teaching.Mach. Learn., 8:293321": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2022.Whatmakes good in-context examples for GPT-3?InProceedings of Deep Learning Inside Out (DeeLIO2022): The 3rd Workshop on Knowledge Extrac-tion and Integration for Deep Learning Architectures,pages 100114, Dublin, Ireland and Online. Associa-tion for Computational Linguistics. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,and Ashwin Kalyan. 2023. Dynamic prompt learn-ing via policy gradient for semi-structured mathe-matical reasoning. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages80868098, Dublin, Ireland. Association for Compu-tational Linguistics. Volodymyr Mnih, Adri Puigdomnech Badia, MehdiMirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,David Silver, and Koray Kavukcuoglu. 2016. Asyn-chronous methods for deep reinforcement learning.CoRR, abs/1602.01783.",
  "Inbar Oren, Jonathan Herzig, and Jonathan Berant. 2021": "Finding needles in a haystack: Sampling structurally-diverse training sets from synthetic data for compo-sitional generalization. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 1079310809, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, andChristopher D. Manning. 2019. Answering complexopen-domain questions through iterative query gen-eration. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages25902602, Hong Kong, China. Association for Com-putational Linguistics. Nils Reimers and Iryna Gurevych. 2019a. Sentence-bert:Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Nat-ural Language Processing, EMNLP-IJCNLP 2019,Hong Kong, China, November 3-7, 2019, pages 39803990. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019b. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics.",
  "Stephen E. Robertson and Hugo Zaragoza. 2009. Theprobabilistic relevance framework: BM25 and be-yond. Found. Trends Inf. Retr., 3(4):333389": "Subhro Roy, Samuel Thomson, Tongfei Chen, RichardShin, Adam Pauls, Jason Eisner, and Benjamin VanDurme. 2023. Benchclamp: A benchmark for eval-uating language models on syntactic and semanticparsing. In Advances in Neural Information Pro-cessing Systems 36: Annual Conference on NeuralInformation Processing Systems 2023, NeurIPS 2023,New Orleans, LA, USA, December 10 - 16, 2023. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,Wenhan Xiong, Alexandre Dfossez, Jade Copet,Faisal Azhar, Hugo Touvron, Louis Martin, Nico-las Usunier, Thomas Scialom, and Gabriel Synnaeve.2023. Code llama: Open foundation models for code.CoRR, abs/2308.12950.",
  "Weijia Shi, Sewon Min, Michihiro Yasunaga, MinjoonSeo, Rich James, Mike Lewis, Luke Zettlemoyer, andWen-tau Yih. 2023. REPLUG: retrieval-augmentedblack-box language models. CoRR, abs/2301.12652": "Richard Shin, Christopher Lin, Sam Thomson, CharlesChen, Subhro Roy, Emmanouil Antonios Platanios,Adam Pauls, Dan Klein, Jason Eisner, and BenjaminVan Durme. 2021. Constrained language modelsyield few-shot semantic parsers. In Proceedings ofthe 2021 Conference on Empirical Methods in Natu-ral Language Processing, pages 76997715, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Richard Shin and Benjamin Van Durme. 2022. Few-shot semantic parsing with language models trainedon code. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 54175425, Seattle, United States.Association for Computational Linguistics. Richard S. Sutton, David A. McAllester, Satinder Singh,and Yishay Mansour. 1999. Policy gradient methodsfor reinforcement learning with function approxima-tion. In Advances in Neural Information ProcessingSystems 12, [NIPS Conference, Denver, Colorado,USA, November 29 - December 4, 1999], pages 10571063. The MIT Press. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,and Ashish Sabharwal. 2023. Interleaving retrievalwith chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings ofthe 61st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),pages 1001410037, Toronto, Canada. Associationfor Computational Linguistics. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng,Shuxin Zheng, Chen Xing, Huishuai Zhang, YanyanLan, Liwei Wang, and Tie-Yan Liu. 2020. On layernormalization in the transformer architecture. In Pro-ceedings of the 37th International Conference onMachine Learning, ICML 2020, 13-18 July 2020, Vir-tual Event, volume 119 of Proceedings of MachineLearning Research, pages 1052410533. PMLR. Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, JingfeiDu, Patrick S. H. Lewis, William Yang Wang, YasharMehdad, Scott Yih, Sebastian Riedel, Douwe Kiela,and Barlas Oguz. 2021. Answering complex open-domain questions with multi-hop dense retrieval. In9th International Conference on Learning Represen-tations, ICLR 2021, Virtual Event, Austria, May 3-7,2021. OpenReview.net. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, andLingpeng Kong. 2023. Compositional exemplarsfor in-context learning. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 3981839833.PMLR. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-tive example selection for in-context learning. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 91349148, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics.",
  "BSMatch Evaluation": "For evaluation of semantic parse or code genera-tion on partial results, we utilize SMatch (Cai andKnight, 2013). Generated code can be transformedto AMRs by treating each functions return valueas an entity and each argument to a function as avalue, where the parameter name is the relation.An example is given below.Consider the following parse in SMCALFLOW,expressed in Lisp:"
}