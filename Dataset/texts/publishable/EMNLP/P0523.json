{
  "Xuanchang Zhang1, Zhuosheng Zhang2,, Hai Zhao3,4,5,*": "1College of Zhiyuan, Shanghai Jiao Tong University2School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University3Department of Computer Science and Engineering, Shanghai Jiao Tong University4Key Laboratory of Shanghai Education Commission for Intelligent Interactionand Cognitive Engineering, Shanghai Jiao Tong University5Shanghai Key Laboratory of Trusted Data Circulation and Governance in Web3{zxcsamzxc, zhangzs}@sjtu.edu.cn,",
  "Abstract": "Despite the rapid progress of large languagemodels(LLMs),theirtaskperformanceremains sensitive to prompt design. Recentstudies have explored leveraging the LLM itselfas an optimizer to identify optimal promptsthat maximize task accuracy. However, whenevaluating prompts, such approaches heavilyrely on elusive manually annotated gold labelsto calculate task accuracy for each candidateprompt, which hinders its generality. To over-come the limitation, this work proposes GLaPE,a gold label-agnostic prompt evaluation methodto alleviate dependence on gold labels. GLaPEis composed of two critical aspects:self-consistency evaluation of a single promptand mutual-consistency refinement acrossmultiple prompts. Experimental results on 8widely-recognized reasoning tasks demonstratethat GLaPE can produce more effectiveprompts, achieving performance comparableto those derived from manually annotated goldlabels. Analysis shows that GLaPE providesreliable evaluations aligned with accuracy,even in the absence of gold labels.Codeis publicly available at",
  "Introduction": "As the integration of large language models (LLMs)into natural language processing tasks has becomeimperative in recent years (Achiam et al., 2023;Scao et al., 2023; Chowdhery et al., 2022; Touvronet al., 2023), the sensitivity of the performanceof LLMs to prompts has garnered significantattention (Pezeshkpour and Hruschka, 2023; Loyaet al., 2023).While traditional soft prompttuning methods (Li and Liang, 2021; Liu et al.,2022; Lester et al., 2021; Qin and Eisner, 2021) *Corresponding authors. This research was supported bythe Joint Research Project of Yangtze River Delta Science andTechnology Innovation Community (No. 2022CSJGG1400)and National Natural Science Foundation of China (No.62406188). demonstrate effectiveness in guiding the LLM toperform desired tasks, they encounter limitationswhen applied to private LLMs, such as GPT-4(OpenAI, 2023). This situation necessitates theexploration of effective strategies for optimizingprompts without requiring gradient updates.Recent studies (Yang et al., 2023; Zhouet al., 2022) have unveiled a noteworthy strategy,where the LLM itself acts as the optimizer toseek the prompt that maximizes task accuracy.Specifically, OPRO (Yang et al., 2023) provides anintriguing avenue for prompt optimization basedon a gold label evaluation recipe (a).The optimization commences with an initialprompt, then iteratively evaluates existing promptsand generates novel prompts based on priorassessments.However, a significant caveatemerges as these studies heavily rely on manuallyannotated gold labels. Concretely, the gold label,representing the ideal output, serves as a crucialingredient for evaluating and refining prompts.Nevertheless, the acquisition of such gold labelsposes a formidable obstacle (Huang et al., 2023;Stechly et al., 2023), introducing complexityand hindering the widespread implementationand generality of these optimization techniques.Therefore, exploring alternative methodologiesbecomes mandatory to address these challengesand improve the efficiency of prompt evaluationand optimization for LLMs.To address the limitations, this work proposesa gold label-agnostic prompt evaluation (GLaPE)method to identify prompts that facilitate consistentand accurate answers. Instead of relying on goldlabels, GLaPE evaluates prompts based on twocritical aspects: self-consistency evaluation andmutual-consistency refinement. Inspired by Wanget al. (2022), we first consider a naive solution byutilizing self-consistency (SC) as the evaluationmetric instead of accuracy, as correct answersgenerally exhibit higher SC than incorrect ones.",
  "(a) Accuracy Evaluation": "Question: Oscar has 24 lollipops and eats 2 on his way to school. He passes 14 outto his friends. He buys twice as many lollipops on his way home as he gave to hisfriends. He eats 3 more that night and 2 more in the morning. How many lollipopsdoes Oscar have?Gold Label (Answer): 31Prompt1: By carefully analyzing all aspects of the situation, the optimal solutionbecomes crystal clear.Responses: 31, 31, 31, 31, 31, 31, 31, 31, 31, 31 Score: 100.0Prompt2: After thorough examination and careful consideration, the optimalsolution becomes clear.Responses: 19, 31, 31, 31, 31, 31, 31, 31, 36, 36 Score: 100.0Prompt3: Lets think about this logically.Responses: 31, 33, 33, 36, 36, 36, 36, 36, 36, 36 Score: 0.0Prompt4: Lets approach this problem systematically.Responses: 19, 31, 31, 31, 33, 33, 36, 36, 36, 36 Score: 0.0Prompt5: By carefully analyzing all the available data, the optimal solutionbecomes unequivocally evident.Responses: 8, 8, 11, 19, 19, 31, 31, 36, 36, 36 Score: 0.0",
  "(b) Our GLaPE Method": "Question: Oscar has 24 lollipops and eats 2 on his way to school. He passes 14 outto his friends. He buys twice as many lollipops on his way home as he gave to hisfriends. He eats 3 more that night and 2 more in the morning. How many lollipopsdoes Oscar have?Gold Label (Answer): 31Prompt1: By carefully analyzing all aspects of the situation, the optimal solutionbecomes crystal clear.Responses: 31, 31, 31, 31, 31, 31, 31, 31, 31, 31Score: 87.9Prompt2: After thorough examination and careful consideration, the optimalsolution becomes clear.Responses: 19, 31, 31, 31, 31, 31, 31, 31, 36, 36Score: 81.8Prompt3: Lets think about this logically.Responses: 31, 33, 33, 36, 36, 36, 36, 36, 36, 36Score: 50.0Prompt4: Lets approach this problem systematically.Responses: 19, 31, 31, 31, 33, 33, 36, 36, 36, 36Score: 45.7Prompt5: By carefully analyzing all the available data, the optimal solutionbecomes unequivocally evident.Responses: 8, 8, 11, 19, 19, 31, 31, 36, 36, 36Score: 44.2",
  "New PromptsNew Prompts": ": Sketch of prompt optimization utilizing the LLM as an optimizer (Yang et al., 2023), featuring distinctprompt evaluation metrics based on: (a) accuracy or (b) our proposed GLaPE. The texts are favorably read in colors.Blue: gold label, Yellow: most frequent answer, Green: high score, Red: low score, Purple: prompt evaluation. However, we will show that SC alone may notalways yield accurate evaluations, since SC doesnot always align well with accuracy and canoverestimate prompts that produce incorrect butconsistent answers.To mitigate this, we thenpropose a complementary approach named mutual-consistency refinement across multiple prompts.This approach penalizes inconsistent scores basedon SC across prompts that produce the sameanswers.By doing so, the refinement processeffectively identifies prompts that demonstrate highSC but result in incorrect answers, leading to morereliable evaluation scores. illustrates ourGLaPE method.Building on our GLaPE evaluation strategy,we then develop a gold label-agnostic promptoptimization method. Specifically, we substitutethe accuracy evaluation method in OPRO withour GLaPE method (b).Experimentalresults on 8 widely-recognized reasoning tasksdemonstrate that GLaPE can produce more effec-tive prompts, achieving performance comparable tothose derived from manually annotated gold labels.Our key contributions are as follows:(i) This work studies a gold label-agnostic prompt evaluation method to alleviate dependenceon gold labels, which allows prompt evaluation inmore realistic scenarios when human-annotateddataset is unavailable.To the best of ourknowledge, this work is the first to study gold label-agnostic prompt evaluation for LLMs.(ii) We propose a novel prompt evaluationapproach named GLaPE, which consists of self-consistency evaluation of a single prompt andmutual-consistency refinement across multipleprompts. GLaPE helps LLMs optimize effectiveprompts that are comparable with those derivedfrom manually annotated gold labels.(iii) We elicit the analysis of why the widely-used SC approach fails at our evaluation taskand figure out an effective mutual-consistencyrefinement approach to mitigate the challenge.",
  "Related Work": "Prompt Optimization In the domain of LLMs(Achiam et al., 2023; Scao et al., 2023; Chowdheryet al., 2022; Touvron et al., 2023), promptengineering plays a crucial role in guiding modelsto generate desired outputs across diverse tasks(Pezeshkpour and Hruschka, 2023; Loya et al., 2023). Consequently, optimizing prompts becomesparamount for enhancing the performance andefficiency of LLMs. Various soft prompt tuningmethods (Li and Liang, 2021; Liu et al., 2022;Lester et al., 2021; Qin and Eisner, 2021) havebeen explored in previous research to optimizeprompts for open-source LLMs. However, thesemethods encounter challenges when applied toprivate LLMs,where accessing gradients isinfeasible.Consequently, diverse gradient-freeprompt optimization techniques (Zhou et al., 2022;Pan et al., 2023; Ye et al., 2023) have been explored.Recent works (Yang et al., 2023) have embracedan iterative process for gradient-free promptoptimization, commencing from an initial promptand iteratively assessing existing prompts whilegenerating new ones based on prior evaluations.Nevertheless, these iterative prompt optimizationmethods heavily depend on gold labels forprompt evaluation. Relying on gold labels notonly limits the generalizability of these promptoptimization methods but may also introduceother potential issues (Huang et al., 2023; Stechlyet al., 2023). In our work, we propose a novelgold label-agnostic prompt evaluation methodand subsequently present a unique approach tooptimize prompts for LLMs without the constraintsassociated with conventional gold label reliance.Prompt Selection Prompt selection tasks aimto identify the optimal prompt among candidatesfor a given task, representing an alternativeapproach to prompt optimization. Recent studieshave delved into probability-based evaluationmethods, utilizing diverse metrics such as mutualinformation (Sorensen et al., 2022), entropy (Luet al., 2021), and perplexity (Gonen et al., 2022).In contrast to these probability-centric assessments,our proposed evaluation approach exclusively relieson the output, making it applicable to private LLMswhere only the output is accessible.",
  "Task Formulation": "Existing studies on prompt design (Yang et al.,2023; Zhou et al., 2022) generally adhere to atwo-stage paradigm in an iterative manner: (i)evaluate the prompt, analogous to calculating theloss function and gradient in soft prompt tuning;(ii) optimize the prompt, analogous to the gradientdescent process in soft prompt tuning.We formulate the two stages on top of the widely- used question-answering (QA) task defined byQA pairs (Q, A), where each pair comprises aninput Q and its corresponding expected output A.We introduce the prompted model as M and anevaluation function f. Our objective is to determinethe optimal natural language instruction prompt.To begin with, we define the meta-prompt asthe input to for prompt optimization. As the upperblock shown in , a meta-prompt containsthree parts. The first part is a problem description.The second part is an optimization trajectory,includes past solutions and their evaluation scores.The third part is the optimization instruction forgenerating new candidate prompts.Then, we describe the process of obtaining theoptimization trajectory. In each iteration, the LLMgenerates a candidate prompt to the QA task. Weconcatenate each question Q with the candidateprompt to form the prompted input [Q; ]. Then,the prompted input is feed to the model to obtainthe response M ([Q; ]). We evaluate the goodnessof candidate prompt based on the evaluationfunction f, e.g., calculating the accuracy betweeneach pair of M ([Q; ]) and the labeled answer Ain previous studies. Then the candidate promptalong with the evaluation score is added to thetrajectory for the next iteration.The optimization process terminates when theLLM is unable to propose new prompts withbetter evaluation scores, or a maximum numberof optimization steps has reached.",
  "Self-consistency": "Here, we adopt the definition of self-consistencyproposed by Wang et al. (2022). We sample nresponses (r1, , rn) from the LLM using thesame prompt.The final answer is determinedby a voting mechanism, where the most frequentresponse a is selected as the answer.Self-consistency is the frequency of a in all n responses,which can be formulated as:",
  "-20.0": ": The schematic representation of our GLaPE method integrating self-consistency (SC) evaluation andmutual-consistency (MC) refinement. This sketch illustrates how our method assesses the prompts in ;computation details are provided in Appendix A.2. Notably, we observed that prompt3, as indicated by the redmarker, produces an incorrect answer with high self-consistency (70%). Through the mutual-consistency refinement,our GLaPE score experiences a decrease of 20.0, rendering it more discernible when compared to prompt1 andprompt2. The texts are favorably read in colors of background. Blue: self-consistency, Purple: mutual-consistencyrefinement, Green: answer 31 (gold label), Orange: answer 36, Yellow: GLaPE metric. the generalization of existing prompt optimizationmethods. Furthermore, we ultimately expect LLMsto solve problems for which answers are not alreadyknown. Therefore, when optimizing prompts toenhance performance, gold labels are not readilyavailable. Thus, it is imperative to find a gold label-agnostic prompt evaluation method.",
  "For a gold label-agnostic prompt evaluationmethod,itisessentialtorelyexclusivelyon the responses and identify patterns withinthem.Building on the findings of Wang et al": "(2022), which demonstrate that selecting the mostfrequently response enhances accuracy, we aim toinvestigate whether SC correlates with accuracy.To this end, we experiment by utilizing theprompt Lets think step by step. proposed byKojima et al. (2022). We calculated the averageself-consistency of correct, incorrect, and overallanswers and presented the results in . Weobserve a significant superiority in the average",
  ": SC-Accuracy Graph for Prompts.Eachprompt is represented as a point on the graph, wherethe x-coordinate signifies self-consistency and the y-coordinate signifies accuracy": "self-consistency of correct answers compared toincorrect ones. A more specific example is shownin . We see that the average SC of correctanswers (answer 31) significantly surpasses thatof incorrect ones.This observation indicatesthat the self-consistency of responses may reflectaccuracy. Thus, it is possible to evaluate promptsbased on the SC of the responses and incorporatethis method in prompt optimization.However, we also find that there exists disparitybetween SC and accuracy when using SC as thesole evaluation metric. This disparity happens toPrompt 3 as shown in . Concretely, Prompt3 yields an incorrect answer (answer 36) but hasa high SC of 70.0. By taking the GSM8K dataset asthe testbed, we computed both the self-consistencyand accuracy for a group of prompts. Consequently,we draw each prompt as a point in .Given the observed fluctuations in the line, itis apparent that self-consistency does not alignrigorously with accuracy. Additionally, we findthat the Spearman correlation coefficient betweenSC and accuracy is relatively low, as shown in thefirst line of . Therefore, relying on self-consistency alone proves insufficient in offeringa comprehensive representation of accuracy inprompt evaluation and optimization.So far, we show that SC alone may notalways yield accurate evaluations, since SC does not always align well with accuracy and canoverestimate prompts that produce incorrect butconsistent answers. Therefore, it deserves a morein-depth investigation to mitigate the side effectsof the overestimated prompts by SC. Beyondexamining individual prompt responses, we cananalyze relationships between different prompts.",
  "Mitigating the Challenge withMutual-consistency (MC) Refinement": "Although the performance of a single prompt isonly related to its responses, we leverage otherprompts for better evaluation in the absence of agold label.Specifically, we infer the gold label from otherprompts and then refine the SC evaluation of thesingle prompt. shows that correct answersexhibit higher self-consistency (SC), allowing usto predict answer correctness by analyzing theaverage SC of all prompts producing it. In ,we can predict that the answer \"31\" is more likelyto be correct, while the answer \"36\" is not, asthe average SC of \"31\" is 87.5, whereas thatof \"36\" is 46.7. This prediction further aids inrefining evaluation of each prompt. For an incorrectanswer, we should lower the evaluation score ofprompts with elevated SC, towards the average. In, since the average SC of answer 36 is46.7 while prompt 3 has an elevated SC of 70.0, theevaluation score of prompt 3 should be lowered.This refinement mitigate the SC evaluation ofoverestimated prompts.In summary, we predict the correctness of ananswer by its average SC and refine each SCtowards this average. This aligns the evaluationof prompts producing the same answer.Based on our pivot study above, we find thatcombining SC and MC is effective for achievinggold label-agnostic prompt evaluation.",
  "In light of the discussions in , wepropose GLaPE, a gold label-agnostic promptevaluation approach. GLaPE is composed of two": "critical aspects: self-consistency evaluation of asingle prompt and mutual-consistency refinementacross multiple prompts. The overall procedure isillustrated in Figure as depicted in .For formal description purposes, we assumethere are N different prompts and denote theevaluation score for each prompt i as fi. Amongmultiple samplings of M prompted with ([Q; i]),the answer is ai and the self-consistency is ci, asdefined in .2.Self-consistency Evaluation:We evaluateprompts based on the self-consistency of theiranswers by minimizing the loss function:",
  "Ltotal = Lself + (1 ) Lrefine,(4)": "whereweightsthecontributionofself-consistency evaluation and mutual-consistencyrefinement in the evaluation process. Based on pre-liminary experiments (detailed in Appendix A.1),we set = 0.5.We obtain the ultimate evaluations f1, , fNby minimizing the loss function Ltotal.Weinitialize fi with ci for simplicity and utilize thedefault gradient descent method to find the optimalsolution with a learning rate of 0.05.",
  "Experiment Setup": "Datasets. Our experiments were conducted on 8benchmark datasets to evaluate the performanceof our gold label-agnostic prompt evaluation andoptimization method. We selected five datasetsspecifically focused on arithmetic reasoning:AddSub (Hosseini et al., 2014), AQuA (Linget al., 2017), GSM8K (Cobbe et al., 2021), Multi-Arith (Roy and Roth, 2015), and SVAMP (Patel et al., 2021).Additionally, we included theMATH dataset (Hendrycks et al., 2021), whichis extremely challenging and comprehensive, totest our methods efficacy on particularly difficultbenchmarks.Furthermore, we expanded ourevaluation to commonsense reasoning benchmarks,such as Big-Bench Date (bench authors, 2023)and StrategyQA (Geva et al., 2021), to assess theperformance of GLaPE in varied contexts.Prompt Optimization. We implemented theOPRO method proposed by Yang et al. (2023)using the prompt shown in of their paper.This technique utilizes an LLM to evaluate existingprompts, generating improved prompts based onthe obtained evaluation scores.We chose thisapproach due to its adaptability; alternative metricscan easily replace evaluation scores in the meta-prompt of optimization. This flexibility facilitatesthe seamless execution of our gold label-agnosticprompt optimization experiments. Due to time andfinancial limitations, we conducted both the OPROand GLaPE-based methods for 16 iterations each,generating 8 prompts per iteration.LLM Backbone. In both the evaluation andoptimization phases, we employed GPT-3.5-turbo-0613, which was the latest version of GPT-3.5-turbo.For prompt evaluation, we empiricallyset the temperature to 0.7 and generated 10outputs using chain-of-thought prompting (Weiet al., 2023). For prompt optimization, defaulthyperparameters and meta-prompt from Yang et al.(2023) were applied.",
  "Main Results": "shows the main results on the 8 benchmarkdatasets.GLaPE is able to produce effectiveprompts, achieving performance comparable tothose derived from manually annotated gold labelssuch as OPRO. The results suggests that ourGLaPE can function as a robust metric, akinto accuracy.We also compared our methodwith other recent prompt optimization methodsfor private LLMs; these results are detailed inAppendix A.3, providing additional evidence toverify the generality of GLaPE.",
  "GLaPE-based (Ours)Lets approach this problem with an innovative,revolutionary, and groundbreaking solution.25.9": ": Optimization results (optimal prompt and corresponding accuracy) of our GLaPE-based prompt optimizationmethod and OPRO (Yang et al., 2023) across various datasets. Notably, Our optimal prompt is determined byselecting the prompt with the highest GLaPE score. Initially, on the GSM8K dataset, we comparedprompt optimization outcomes using two distinctevaluation methods: self-consistency assessmentand GLaPE. As shown in , GLaPE-basedprompt optimization results in a superior promptcompared to that obtained through confidenceassessment.This observation suggests thatincorporating mutual-consistency refinement to rectify confidence evaluation enhances the efficacyof prompt optimization.Furthermore, we incorporated the Spearmancorrelation coefficient1 into our study, wherein ahigher coefficient signifies a stronger correlationbetween variables. This quantitative assessment",
  ": SC-Accuracy Graph v.s. GLaPE-Accuracy Graph for Prompts on Multiarith": "was employed to juxtapose GLaPE with the solelySC-based evaluation regarding the correlationwith accuracy.Our analysis concentrated onprompts within the optimization trajectory in theexperiment in .2, to mitigate unnecessarycomputational costs. As shown in , theSpearman coefficient between GLaPE and accuracyexceeds that of self-consistency across all datasets.Additionally,we utilized the visualizationmethod introduced in .1 to depict theprompts of the optimization trajectory in a graph(). In a, we observe a fluctuatingline, whereas in b, a consistently increasingline is evident.Both of the scrutiny indicatethat our mutual-consistency refinement methodsignificantly mitigates the disparity between self-consistency and accuracy.",
  "Generalizability Across LLMs": "In the experiment of 6.2, we only optimizeprompts for GPT-3.5-turbo. To further assess thegeneralizability of our method on models otherthan GPT-3.5-turbo, we conducted experimentson three widely used open-source models ofdifferent sizes:Mistral-7B, Llama3-8B, andGemma2-9B. As shown in , our GLaPE-based prompt optimization approach consistentlydiscovers prompts that outperform the baselinesand are competitive with the OPRO method acrossall models.",
  "Rethink on Gold Label-agnosticPrompt Optimization": "Our amalgamation of self-consistency evaluationand mutual-consistency refinement facilitates theidentification of prompts leading to correct answers.However, we also observe a diminished Spearmancorrelation coefficient between our GLaPE andaccuracy on the AQuA dataset and StrategyQAdataset, as depicted in .Given thesuboptimal performance, we shift to reflect on theintrinsic restriction posed by the LLM. As stated in.1, in scenarios where all prompts resultin consistent but inaccurate answers, our evaluationmay fail to identify the error. Without access toexternal resources, discerning the consistent errorsbecomes challenging. We illustrate some examplequestions in the Strategy dataset in , wherealmost all prompts lead to a incorrect answer withextremely high self-consistency during our promptoptimization.By excluding these problematicquestions, we observe a significant improvementin the Spearman correlation coefficient, as detailedin Appendix A.4.Notably, Our GLaPE evaluation method hasbeen shown effective in challenging datasets, too.Our GLaPE evaluation method aligns well withaccuracy on the rigorous MATH dataset, as shownin .This alignment can be attributedto the significant disparity in self-consistencybetween correct and incorrect answers, as depictedin .Conversely, on the StrategyQAdataset, the self-consistency levels of correct andincorrect answers are remarkably similar. Thissimilarity arises from the inherent errors within theLLM itself, which precludes effective distinctionbetween these answers without access to the goldstandard labels.",
  "Conclusion": "This work presents an innovative gold label-agnostic prompt evaluation methodology that oper-ates in the absence of gold labels. By integratingself-consistency evaluation and mutual-consistencyrefinement, our evaluation demonstrates a strongcorrelation with accuracy.Subsequently, we Example 1: Was Iggy Pop named after his father?Label: Yes.LLM Answer & Explanation: No. Iggy Pop, whosereal name is James Newell Osterberg Jr., was notnamed after his father. His fathers name was JamesNewell Osterberg Sr., so they shared the same firstname but not the same nickname. Iggy Pop got hisstage name from his first band, The Iguanas, and latershortened it to Iggy.Example 2: Is Great Pyramid of Giza the last wonderof its kind?Label: Yes.LLM Answer & Explanation: No. The GreatPyramid of Giza is one of the Seven Wonders of theAncient World. It is the only wonder from that list thatstill exists today. However, there have been other listsof wonders created throughout history. For example,the New7Wonders Foundation conducted a global pollin 2007 to determine the New Seven Wonders of theWorld.",
  "Limitations": "First, in , we outlined the challenges facedby our GLaPE method in accurately assessing theinherent error of LLM itself. In future research,innovative approaches could be explored to identifythe consistent mistakes.Another limitation inour current evaluation methodology is that weutilize a singular digital score as the assessment,which fails to furnish comprehensive informationregarding the prompts.Consequently, futureresearch could augment the granularity of promptevaluations, incorporating other assessments, likenatural language feedback, to address this shortfall.",
  "BIG bench authors. 2023. Beyond the imitation game:Quantifying and extrapolating the capabilities oflanguage models. Transactions on Machine LearningResearch": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,Parker Barnes,Yi Tay,Noam Shazeer,Vinodkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick,Andrew M. Dai, Thanumalayan SankaranarayanaPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling languagemodeling with pathways. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did AristotleUse a Laptop? A Question Answering Benchmarkwith Implicit Reasoning Strategies.Transactionsof the Association for Computational Linguistics(TACL).",
  "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. arXivpreprint arXiv:2101.00190": "Wang Ling, Dani Yogatama, Chris Dyer, and PhilBlunsom. 2017.Program induction by rationalegeneration: Learning to solve and explain algebraicword problems.In Proceedings of the 55thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 158167,Vancouver, Canada. Association for ComputationalLinguistics. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam,Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 6168. Manikanta Loya, Divya Sinha, and Richard Futrell.2023. Exploring the sensitivity of llms decision-making capabilities: Insights from prompt variationsand hyperparameters. In Findings of the Associationfor Computational Linguistics:EMNLP 2023.Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2021. Fantastically orderedprompts and where to find them:Overcomingfew-shot prompt order sensitivity. arXiv preprintarXiv:2104.08786.",
  "Guanghui Qin and Jason Eisner. 2021. Learning howto ask: Querying lms with mixtures of soft prompts.arXiv preprint arXiv:2104.06599": "Subhro Roy and Dan Roth. 2015.Solving generalarithmetic word problems. In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 17431752, Lisbon,Portugal. Association for Computational Linguistics. Teven Le Scao, Angela Fan, Christopher Akiki,Ellie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2023.Bloom:A 176b-parameter open-access multilingual language model. Taylor Sorensen, Joshua Robinson, Christopher Rytting,Alexander Shaw, Kyle Rogers, Alexia Delorey,Mahmoud Khalil, Nancy Fulda, and David Wingate.2022. An information-theoretic approach to promptengineering without ground truth labels.InProceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers). Association for ComputationalLinguistics.",
  "Kaya Stechly, Matthew Marquez, and SubbaraoKambhampati. 2023. Gpt-4 doesnt know its wrong:An analysis of iterative prompting for reasoningproblems. arXiv preprint arXiv:2310.12397": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2022. Self-consistency improves chainof thought reasoning in language models.arXivpreprint arXiv:2203.11171.",
  "A.1Preliminary Experiments": "In this section, we discuss two crucial hyperparam-eters used in our experiments.The first is the balance weight , which balancesSC evaluation and MC refinement as described inEquation 4. We tested values of 0.25, 0.5, 0.75,and 1.0, with results detailed in . An optimalbalance was achieved at = 0.5, emphasizing the",
  "A.3Further Comparison of PromptOptimization Methods": "To emphasize the efficacy of our method, weconducted additional comparisons between ourGLaPE method and other recent prompt opti-mization approaches for private LLMs, includingAPE (Zhou et al., 2022), APO (Pryzant et al., 2023),and PE2 (Ye et al., 2023). The results are presentedin . These comparisons demonstrate thatGLaPE is not only competitive but also exceeds theperformance of other existing supervised methodsin various cases.",
  "A.4Spearman Correlation Coefficients onCleaned Datasets": "It is imperative to recognize that our methodologyevaluates prompts on individual questions, andthe evaluation score of a prompt across theentire dataset is derived from the sum of itsevaluation scores on each question. Consequently,inaccuracies in evaluations for questions statedin can significantly compromise theeffectiveness of the overall dataset evaluation,particularly on challenging datasets. To gauge theimpact of challenging questions on our GLaPE,we exclude questions for which no prompt resultsin a correct answer with a self-consistency level",
  "AddSubAQuABig-Bench DateGSM8KMultiArithSVAMPStrategyQAMATH": "Cleaned Dataset 0.61(+0.17) 0.40(+0.36)0.94(+0.06)0.69(+0.20) 0.93(+0.05) 0.81(+0.12) 0.41(+0.13) 0.61(+0.14)Control Group0.42(-0.07) -0.01(-0.05)0.86(-0.02)0.40(-0.09) 0.84(-0.04) 0.61(-0.08)0.16(-0.02)0.46(-0.01)Original Dataset0.440.040.880.490.880.690.180.47 : Comparison of Spearman correlation coefficients () before and after excluding challenging questions thatsurpass the intrinsic capabilities of LLM. Evaluation of the control group is conducted by randomly selecting 10subsets of the original dataset, and the average Spearman correlation coefficient is computed. greater than 50% from the dataset. The cleaneddataset was then compared to a control group,consisting of an equally large subset of the originaldataset, to mitigate the influence of dataset sizebias. On the initial dataset, the control group, andthe cleaned dataset, we calculate the Spearmancorrelation coefficient.In , the Spearman correlation coefficienton the cleaned dataset demonstrates a considerableimprovement compared to that on the originaldataset or control group.This improvementunderscores the pronounced adverse influence ofintricate questions on our evaluation process."
}