{
  "Abstract": "Pretrained language models (LMs) encode im-plicit representations of knowledge in their pa-rameters. However, localizing these representa-tions and disentangling them from each otherremains an open problem. In this work, weinvestigate whether pretrained language mod-els contain various knowledge-critical subnet-works: particular sparse computational sub-graphs that can, if removed, precisely suppressspecific knowledge the model has memorized.We propose a multi-objective differentiablemasking scheme that can be applied to bothweights and neurons to discover such subnet-works and show that we can use them to pre-cisely remove specific knowledge from modelswhile minimizing adverse effects on the behav-ior of the original model. We demonstrate ourmethod on multiple GPT2 variants, uncoveringhighly sparse subnetworks (98%+ sparsity) thatare critical for expressing specific collections ofrelational knowledge. When these subnetworksare removed, the remaining network maintainsmost of its initial abilities but struggles to rep-resent the suppressed knowledge.1",
  "Introduction": "Large-scale language models (LLMs) encode largeamounts of relational knowledge (Petroni et al.,2019; Carlini et al., 2023; Liu et al., 2023), whichthey transfer to successfully adapt to downstreamtasks (Wang et al., 2019b,a). Following this suc-cess, considerable research focuses on better un-derstanding the extent to which LLMs capture thisknowledge (Liu et al., 2019a; Safavi and Koutra,2021; Da et al., 2021; Huang et al., 2022). Inthese works, relational triplets (e.g., (car, IsA,vehicle)) are converted to natural language (e.g.,A car is a vehicle.) before being presentedto a model. Key tokens in these input sequences are",
  "Model Behavior Objectives": ": Knowledge-critical subnetworks are necessaryfor expressing target knowledge triplets (TARGETKG)in LMs. When removed, the remaining model no longerexpresses the specific triplets, but maintains its abilityto express other relational knowledge (CONTROLKG)and its language modeling abilities (CONTROLLM). masked, and the model demonstrates its knowledgeof the relations by recovering these tokens.With the body of work studying LLMs as knowl-edge bases, a subset of works focuses on where andhow this knowledge may be encoded by the modelsthat capture it. The answer to these questions couldpotentially facilitate the development of more ef-fective finetuning methods, which can be useful forrectifying factual errors made by language mod-els, updating models with evolving knowledge, andpreventing ethically undesirable behavior.Considerable work in model probing (Belinkovand Glass, 2019; Durrani et al., 2020; Antverget al., 2022; Belinkov, 2022) and mechanistic in-terpretability (Geva et al., 2021, 2022b,a) exploresthese questions, discovering hidden representations,neurons, and layers that are responsible for the ex-pression of knowledge from these systems. How-ever, these works typically do not localize the knowledge accessing behavior to individual pa-rameters. Another line of work in model editingexplores whether knowledge in the model can bechanged (De Cao et al., 2021; Dai et al., 2022; Haseet al., 2023b; Mitchell et al., 2022a,b; Meng et al.,2022, 2023; Hase et al., 2023a; Gupta et al., 2023;Jang et al., 2023; Chen et al., 2023). However,the goal of these methods is also typically not toprecisely localize the parameters responsible forexpressing knowledge, but instead to broadly editmodel parameters such that a new desired behavioroverwrites the models preference for the old one.In this work, we hypothesize that any piece of re-lational knowledge expressed by a language modelis encoded by a limited subset of its parameters. Wesearch for these parameters by identifying sparsesubnetworks that, when removed, suppress themodels ability to express the knowledge of interestwhile not affecting other abilities of the model. Asthe model cannot express target knowledge withoutthese subnetworks, we refer to them as knowledge-critical. In , we illustrate this concept when the weights marked with a red cross are re-moved from the original network, the expressionof the triplet (cafe, IsA, restaurant) is sup-pressed, whereas other triplets are not.To discover knowledge-critical subnetworks, wepropose training differentiable masks over weightsor neurons of the original pretrained model, suchthat the mask can identify and remove a knowledge-critical subnetwork for the targeted knowledgegraph.Specifically, we train the mask to: (1)suppress the expression of the target knowledgetriplets, (2) maintain the ability to express genericrelational knowledge and language, and (3) removeonly a minimal subset of weights.After train-ing, the remaining pruned model can no longerexpress the target knowledge, but maintains its per-formance on other behaviors, thereby identifyingthe knowledge-critical subnetwork as the maskedportion of the original model.Our results across multiple target knowledgegraphs (constructed from WordNet and Concept-Net) and LLMs at multiple scales (from the familyof GPT2 models) show that weight maskingconsistently identifies sparse subnetworks (an av-erage sparsity of 98.6%) that satisfy our objec-tives. When these subnetworks are removed, theremaining models perplexity on the target knowl-edge associated with the subnetwork largely in-creases (an average relative perplexity increase of253% - 5589% for different GPT2 models), indicat- ing that the expression of the target knowledge issuccessfully suppressed. However, the remainingnetworks ability to model generic relational knowl-edge and natural language negligibly changes. Fi-nally, in a study on CommonsenseQA, we showthat once these subnetworks are removed, modelsfinetuned using parameter-efficient methods strug-gle with questions that require the knowledge en-coded by the removed subnetworks.",
  "Related Work": "LLMs as Knowledge BasesOur work buildson prior research that demonstrates the knowledgememorization abilities of large language models(LLMs; Carlini et al., 2021; AlKhamissi et al.,2022). Multiple studies have shown that LLMsencode various types of knowledge (Liu et al.,2019a; Chen and Gao, 2022; Safavi and Koutra,2021; Huang et al., 2022). In these works, para-metric knowledge in LLMs is typically expressedby conditioning on a natural language context tocomplete or infill a sequence that expresses theknowledge (Petroni et al., 2019; Jiang et al., 2020;Shin et al., 2020; Cao et al., 2021a; Zhong et al.,2021; Qin and Eisner, 2021; Liu et al., 2023; Yuet al., 2023). Other methods also fine-tune mod-els to create an interface to parametric knowledge(Bosselut et al., 2019; Roberts et al., 2020; Jianget al., 2021; Hwang et al., 2021). In contrast, ourwork investigates where knowledge is encoded byLLMs and localizes the critical subnetworks forexpressing these facts. Function-Specific SubnetworksMethodologi-cally, our work draws inspiration from studies thatidentify task-specific subnetworks in neural net-works. Perhaps most known, Frankle and Carbin(2019) propose the Lottery Ticket Hypothesis, show-ing that learned subnetworks could achieve test ac-curacy similar to that of original networks. Otherworks prune subnetworks for the purpose of effi-cient finetuning (Mallya et al., 2018; Zhao et al.,2020; Sanh et al., 2020; Guo et al., 2021), or iden-tifying function-specific subnetworks (Cao et al.,2021b; Sanh et al., 2020; Zhang et al., 2021; Csor-ds et al., 2021). Identifying function-specific sub-networks also leads to useful applications, such asdisentangling representations to reduce model sus-ceptibility to spurious correlations (Zhang et al.,2021), probing models for linguistic properties(Cao et al., 2021b; De Cao et al., 2020), identifyingand removing a toxic behavior or bias (Li et al., 2024; Chintam et al., 2023), and finding subnet-works specialized for different languages (Foroutanet al., 2022). Most similar to our work is that of Renand Zhu (2022), which learns coarse subnetworksthat encoded large portions of ConceptNet. We alsoadopt a differentiable weight masking scheme, butuse it to identify highly sparse subnetworks criticalfor particular expressions of knowledge. Mechanistic InterpretabilityMechanistic inter-pretability tackles the problem of understandingmodel behavior by reverse-engineering computa-tions performed by transformer models. Elhageet al. (2021) discovered algorithmic patterns andframeworks in simplified transformer models. Fol-lowing this, researchers discovered induction heads(Olsson et al., 2022), i.e., specific attention headsinvolved in in-context learning in LLMs. Similarly,with interventions on attention and MLP sublayers,Geva et al. (2023) identified critical points wherethe model propagates information, as well as theinternal mechanism for attribute extraction. Otherwork focuses on knowledge tracing and localiza-tion in model parameters for the goal of modelediting (Dai et al., 2022; Meng et al., 2022, 2023;Gupta et al., 2023; Hernandez et al., 2024). Acti-vation patching with corrupted tokens (Meng et al.,2022) or corrupted prompts (Wang et al., 2023) usecausal intervention to identify activations responsi-ble for flipping the models output. In contrast, ourwork focuses on preserving the original model toprecisely locate individual model weights responsi-ble for expressing a given set of target knowledgewithout counterfactuals. Our work is closer to pathpatching (Goldowsky-Dill et al., 2023) and auto-matic circuit discovery (Conmy et al., 2023) to lo-calize behaviors in network subgraphs but focusesspecifically on identifying subnetworks associatedwith knowledge relationships. Our work is alsosimilar to Lo et al. (2024), which shows that mod-els can re-learn removed concepts via neurons. Incontrast, we focus on individual parameter pruning.",
  "K = {(h1, r1, t1), ...(hn, rn, tn)}": "where h and t are head and tail entity nodes, re-spectively, and r is the relation that holds betweenthe two entities. To input relational knowledge toan LM, triplets are verbalized using a natural lan-guage template. For example, the triplet (house,IsA, building), can be verbalized with the tem-plate {article} {h} is {article} {t} as Ahouse is a building. A typical way to promptfor knowledge is to mask the tail entity A houseis a ___ (Petroni et al., 2019). To approximatean autoregressive models confidence in a giventriplet, we compute a distribution over the missingtoken and calculate the perplexity of the model forthe correct token building. Differentiable Weight Masking for Function-Specific Parameter SearchTo localize param-eters that are critical for modeling specific knowl-edge, we learn a binary mask over each networkparameter. For a language model f(x, ) with pre-trained parameters that takes as input x, we learna set of binary parameters m {0, 1}|| that iselement-wise multiplied with the frozen , suchthat our subnetwork is formulated as f(x, m ).Similar to other binary mask learning methods (Caoet al., 2021b; Sanh et al., 2020), our method mod-els each parameter mask mi with the concrete (i.e.,Gumbel-Softmax) distribution, a differentiable ap-proach to learn continuous mask scores si from real-valued parameters li R (Maddisonet al., 2017; Jang et al., 2017):",
  "Mask GranularityDiscovering subnetworks re-quires selecting the granularity of the parameter": "mask, reflecting the granularity at which we hy-pothesize separable knowledge representations canbe discovered in the model. Most prior work se-lects neurons (Elhage et al., 2022) or layers (Zhouet al., 2023) as the basic structural unit for localiz-ing model behaviors. While these representationshave been shown to encode knowledge behaviors(Dai et al., 2022; Lo et al., 2024), they are per-haps too broad for reliably disentangling specificknowledge, as they are typically polysemantic (i.e.,they jointly encode multiple behaviors; Olah et al.,2020). Conversely, localizing knowledge repre-sentations as an unconstrained combination of in-dividual parameters is likely more separable, butmay be noisy, as many parameters may be largelyredundant, and individual parameters may sufferfrom overfitting. With no clear choice, in this work,we explore both parameter-level and neuron-levelmasking to provide complementary insights formechanistic knowledge localization.",
  "Methodology": "This section defines our methodology for discover-ing knowledge-critical subnetworks using differen-tiable weight or neuron masking.NotationWe define a subnetwork as in 3:f(x, m ), where is the set of parameters ofthe network f and m is the mask over a portion ofthat networks parameters. To learn a mask overneurons, we jointly mask all the weights connect-ing to the same neuron. We assume a target set ofknowledge KT K (TARGETKG) for which wewant to identify the critical parameters.",
  "Knowledge-Critical Subnetworks": "Our goal is to find knowledge-critical subnetworks:the essential parameters to express a given set oftarget knowledge. When knowledge-critical sub-networks are removed, the expression of the targettriplets should be suppressed, and the expressionof irrelevant triplets should be unaffected. SuppressionFor f(x, m ) to be critical inexpressing KT , its removal from the original net-work should also suppress the models ability to ex-press the knowledge in KT . More formally, the in-versely masked subnetwork (i.e., remaining model),f(x, m ), where m = 1 m, should havedifficulty expressing KT . We define this as thesuppression criterion, as it encourages that the re-maining model cannot represent knowledge in KT .If we find such a disentanglement, we consider that",
  "the pretrained model heavily relies on the removedsubnetwork to perform a task related to KT": "MaintenanceHowever, if only optimized forsuppression, our method may discover subnet-works that are critical to all expressions of knowl-edge, or all expressions of coherent sequencesof language. As the model should retain its ini-tial capacities, we also define maintenance cri-teria for knowledge-critical subnetworks. Theyshould: (1) not affect the models ability to ex-press other relational knowledge KC = K \\ KT(CONTROLKG), and (2) not affect the models orig-inal language modeling abilities (CONTROLLM).These criteria are referred to as maintenance-KGand maintenance-LM, respectively. SparsityFinally, we aim to keep the knowledge-critical subnetwork as sparse as possible to discoverthe parameters that predominantly encode the ex-pression of KT . Without imposing a high sparsitylevel, parameters unrelated to the expression of KTor KC might persist within the subnetwork.",
  "To learn a weight mask for knowledge-critical sub-networks, we define a joint objective that optimizesfor the criteria defined above": "Suppression LossTo fulfill the suppression cri-terion, the remaining model, f(x, m ), shouldbe less confident in the expression of knowledge inKT . We propose to minimize the KL divergencebetween the remaining models predicted distribu-tion over possible tail entities of a knowledge tripletand a uniform reference distribution UV over thetokens in the models vocabulary. For x KT :",
  "Lsuppress = DKL(UV f(x, m ))(3)": "Maintenance LossesAs there are multiple waysa model could learn to suppress the expression KT ,namely (1) suppressing all knowledge that is inthe same format or (2) suppressing all languageexpressions, we define two regularization objec-tives. To encourage the rest of the model to keep itsoriginal performance on the control knowledge KCand a standard language modeling dataset DLM,we calculate the KL divergence of f(x, m )with the pretrained models distribution f(x, ) asa reference. Thus, for any x KC or x DLM:",
  "fruit36113712381.6---sun36113612387.5---swimming40144015517.8---": ": Statistics on sampled KGs and their verbalization. The graph statistics show the number of tripletsand the unique number of heads, tails, and relations. The average perplexity is calculated with the gold tail tokencross-entropy loss. The perplexity for certain KGs in the Medium, Large and XL columns are not included as we donot evaluate them in our study on model scale. Sparsity RegularizationTo promote the subnet-work containing only parameters critical for mod-eling TARGETKG, we encourage sparsity by min-imizing the average subnetwork density (i.e., sig-moid of the masking parameters li from Eq. 1):",
  "Experimental Setup": "Models & TrainingTo test whether our methodcan scale to various model sizes, we discoverknowledge subnetwork masks for GPT2-small,(117M parameters, 12 layers), GPT2-medium,(345M parameters, 24 layers), GPT2-large, (774Mparameters, 36 layers), and GPT2-XL. (1.5B pa-rameters, 42 layers; Radford et al., 2019). Duringmask learning, we do not mask the embedding,language modeling head, layer-normalization, andbias parameters,2 and only learn masks for the top50% of transformer layers.3 Further implementa-tion details on masking, hyperparameter, and check-point selection are in Appendix B. 2Prior work has not observed an advantage to maskingthese components for general tasks (Zhao et al., 2020).3Multiple layer-wise analyses have shown that the firstlayers of transformer LMs encode low-level linguistic featuresthat may be a prerequisite for knowledge modeling (Tenneyet al., 2019; Liu et al., 2019a). We also perform a maskedlayer choice study that confirms this intuition (Appendix C).",
  "DatasetsTo create TARGETKG and CON-": "TROLKGs, we sample hypernym triplets fromWordNet (Miller, 1995), as well as triplets from theLAMA subset of ConceptNet (Speer et al., 2017;Petroni et al., 2019). For simplicity, we only usetriplets with single-token tail entities. We sample 7TARGETKGs for WordNet, and 3 for ConceptNet(statistics shown in ) by randomly selectingan initial node and sampling knowledge triplets byperforming 3-hop random walks in both the par-ent and child direction of the KG. To create CON- TROLKG, we prioritize not leaking TARGETKGcounterfactuals and having a shared CONTROLKGacross different TARGETKGs, and remove fromthe complete KG any triplet that shares the sameentities as the union of the TARGETKGs shownin . For all triplets, to suppress and main-tain knowledge that the model is already confidentabout, we select the verbalization for each tripletwith the lowest perplexity on the tail token. For theCONTROLLM dataset, we use WikiText-2 (Merityet al., 2017). We refer to CONTROLKG and CON-",
  "(47.5)Neuron Masking94.922422.071.95.48720.229.4Random Weights99.121.014.61.511.45.5Random Neurons94.9110.770.411.235.928.5": ": Subnetwork discovery for GPT2-small, averaged over three seeds and seven KGs for WordNet, andthree KGs for ConceptNet. PPL = PPL(f(x, m )) - PPL(f(x, )) and similarly for Rank results. The valuesin parenthesis are the average metric (PPL or Rank) for the pretrained model (i.e., the base from which the iscomputed). The arrows (,) show the desired direction for the metric. Random is an average of randomly maskedbaselines at the same sparsity levels as the discovered knowledge-critical subnetworks for each KG-seed pair. the remaining and original models. For the sup-pression and maintenance-KG criteria, we calcu-late PPL using the loss on the masked tail entityfor triplets in the TARGETKG and CONTROLKGdatasets. For a knowledge-critical subnetwork, weexpect PPL and Rank values to be high forTARGETKG and low for CONTROLKG. For themaintenance-LM criterion, we calculate PPL asthe average perplexity on all tokens in a sequence,which should be low if removing the critical subnet-work does not affect the models general languagemodeling ability.4 For the sparsity criterion, wecalculate the percentage of parameters that werenot pruned. The denominator is the number ofmasked parameters, meaning the total size of denselayers in the upper half of the model. Ideally, thesparsity should be as high as possible to keep themajority of parameters (i.e., near 99%). BaselineWe use weight and neuron masking tolocalize knowledge-critical subnetworks. As a con-trol baseline, we create randomly masked modelsat the same sparsity level as the knowledge-criticalsubnetwork. If the discovered subnetwork is crit-ical for expressing TARGETKG, then removing arandom subnetwork at the same weight or neuronsparsity should yield lower corruption for express-ing TARGETKG (i.e., lower PPL) than remov-ing the critical subnetwork. Similarly, if the crit-ical subnetwork successfully preserves the main-tenance criteria, a random subnetwork should bemore likely to prune useful weights for expressingCONTROLKG and CONTROLLM, which shouldlead to a higher PPL on maintenance datasets.",
  "We first evaluate the degree to which discoveredsubnetworks are knowledge-critical": "Weight-masked SubnetworksIn , we ob-serve that across seven different knowledge graphs(TARGETKGs) and three random seeds, the sub-networks found with weight masking consistentlyachieve a notably high sparsity (> 98%).5 For thesuppression criterion, we notice a high PPL onTARGETKG for both approaches, meaning that theperplexity of the remaining model on TARGETKGis significantly higher than the pretrained modelsperplexity. In contrast, removing a random subnet-work at the same sparsity yields a smaller perplex-ity increase, meaning the discovered subnetworksare significantly more critical for expressing TAR- GETKG. At the same time, we find little changein perplexity on the maintenance datasets for re-lational knowledge (CONTROLKG) and languagemodeling (CONTROLLM), demonstrated by thenegligible PPL on both datasets and the smallRank value on CONTROLKG. 6 We note that anegative PPL here may result from the remain-ing model slightly overfitting to the CONTROLKGdistribution, although it is never too significant.We observe similar results for knowledge-critical subnetworks for larger models. For three 5 provides individual KG results for the averagedweight masking results in .6Note that the lower average PPL of CONTROLKG com-pared to TARGETKG is due to CONTROLKG being larger,which minimizes the impact of outliers and reduces averageperplexity.",
  ": Ablation study for the multi-objective loss on GPT2-small using weight masking, with [min, max]boundaries, averaged across three KGs and two seeds": "TARGETKGs: communication, representation,and location, we observe an average increase inTARGETKG perplexity of 256 for GPT2-medium,5780 for GPT2-large, 536 for GPT2-XL, and anegligible maintenance PPL (). Neuron-masked SubnetworksOn the otherhand, neuron masking does not reliably fulfill theconditions of discovering knowledge-critical sub-networks. While removing neuron-masked subnet-works yields greater suppression of TARGETKGthan weight masking, it also significantly impactsCONTROLKG PPL and Rank (more than ran-domly removing neurons at the same sparsity), in-dicating that other behaviors of the model are notrobustly maintained. They also tend to be lesssparse, frequently keeping 5% of the parametersof the original model.7 We hypothesize that thisobservation is potentially related to neuron superpo-sition (Elhage et al., 2022), where the neurons thatrepresent TARGETKG cannot be fully disentangledfrom representations that encode general relationalknowledge. While weights may also be polyseman-tic, they are more fine-grained, potentially encod-ing knowledge in a more separable manner. Ablation StudyAs our method relies on a jointobjective combining multiple loss functions, weperform an ablation study of the loss terms pre-sented in 4.2 for weight masking and remove eachobjective (i.e., No Suppression, No Maintenance-KG, No Maintenance-LM) to validate whetherthese losses accomplish their goals.8 In , weobserve that the suppression loss is necessary toincrease TARGETKG perplexity (and suppress theknowledge). Without it, the model only optimizesfor retaining CONTROLKG, and generalizes thisimprovement to TARGETKG as well (as indicatedby the negative PPL). We also find that removing",
  "the maintenance losses significantly affects CON-": "TROLKG and CONTROLLM perplexity differences.Without these controls, our method learns to sup-press the knowledge from the model by suppress-ing general abilities. The suppression objective,a minimization of the KL divergence between theoutput distribution and a uniform distribution, af-fects the prediction of tail entities for all relationalknowledge rather than affecting only TARGETKG.We present additional ablations related to varyingthe training objectives in Appendices B (varyingi in Eq. 6) and F (adding additional loss terms). Paraphrase GeneralizationTo assess whetherour subnetworks generalize to other verbalizationsof TARGETKG and CONTROLKG, we evaluate thepruned models on 20 other distinct relation para-phrases that are not used during training. Specifi-cally, we vary the tokens representing the relationand the format of the head and tail entities whilestill ensuring grammatical correctness.9 For weightmasking, our conclusions do not change when us-ing other prompt styles, as seen in . Inter-estingly, the PPL for CONTROLKG paraphrasesis sometimes lower than for the format used fortraining, likely because the starting perplexity ishigher on other templates,10 and the maintenanceof CONTROLKG generalizes to a greater degreeon these suboptimal templates. The neuron mask-ing approach generalizes well to TARGETKG para-phrases, but poorly for CONTROLKG templates,reinforcing our previous observations.",
  ": Composing subnetworks with GPT2-small.Individual stands for the individual subnetwork removalaverage across the same three seeds and KGs": "the parameter space of the original model. For threeWordNet TARGETKGs and three random seeds, wefind that GPT2-small subnetworks are relativelydenser in the first and final masked transformerblocks. For weight masking, more density is ob-served in the attention sublayers (). Inter-estingly, much of the density of the subnetworksin the attention sublayers is tied to individual atten-tion heads (), supporting prior conclusionsthat particular attention heads encode semantic re-lationships (Clark et al., 2019; Geva et al., 2023).However, despite being dense around similar por-tions of the model across different TARGETKGsand random seeds, the subnetworks are quite dis-tinct. When we calculate the Jaccard similarity(i.e., IoU) of the individual parameters across sub-networks for different random seeds for the sameTARGETKG, the result is quite low on averagefor weight-masked subnetworks (3-4%) thoughhigher for the final attention output sublayer (10-12%) indicating the knowledge-critical subnet-works are quite disjoint, even when discovered bysuppressing the same information ().Neuron masking led to a much higher density inthe second feedforward layers of the transformerblocks and attention layers (). We findthat the IoU of neuron-masked subnetworks arealso 10 higher (34-44%; ), partiallydue to their reduced sparsity, but also perhaps in-dicating that neuron masking yields more uniquesubnetworks across seeds, though they are also lessreliably knowledge-critical. SubnetworkCompositionHowever,eventhough knowledge-critical subnetworks acrossrandom seeds may be disentangled, composingthem (and removing them jointly) amplifies thesuppression effect. As shown in , whenwe compose subnetworks for GPT2-small as aunion of three random seed masks for the sameTARGETKG, the suppression effect increasessignificantly, by a factor of 6 (far more thanremoving additional random parameters fromthe remaining model; ).While thissuppression is accompanied by a degradation inthe maintenance criteria (30-40 PPL on CON- TROLKG instead of near 0), the absolute differenceis far smaller. Composing neuron-masked subnet-works yields similar trends, though we observe twointeresting patterns. First, the intersection of thesesubnetworks produces a subnetwork that satisfiesthe maintenance criteria to be knowledge-critical,though at the cost of reducing suppression. Second,neuron-masked compositions yield monotonicchanges in suppression and maintenance scores assparser composition methods are used. Furtheranalyses on seed-based and knowledge-basedvariance across discovered subnetworks are inAppendix I and J, respectively. Subnetwork SensitivityFinally, we investigatewhether discovered subnetworks are structurallysensitive. Specifically, we perform a sensitivityanalysis of the recorded metrics as we iterativelyexpand or contract the subnetwork (by adding orremoving parameters). As we add parameters tothe subnetwork (i.e., remove parameters from theremaining model), we measure the change in TAR- GETKG PPL. In this case, a sudden drop inPPL would indicate that the discovered subnet-work is spurious. In Appendix , we observethat expanding the discovered subnetwork in smallamounts does not significantly recover the modelsability to express TARGETKG, providing furtherevidence that the subnetworks are not arbitrarilydiscovered, but rather have meaningful knowledge-expressing structure within the larger model. Weprovide more experimental details in Appendix G.",
  "Downstream Task Transfer": "If a subnetwork is truly knowledge-critical, its re-moval should harm a pretrained language modelsability to transfer to a downstream task requiringthe knowledge encoded by the subnetwork. Totest this hypothesis, we finetune a model on the CommonsenseQA benchmark (Talmor et al., 2019)after removing a relevant knowledge-critical sub-network. We use the in-house splits from Lin et al.(2019), with a development set of 1241 and an ini-tial test set of 1221 questions. In the test set, weinduce11 the ConceptNet relation linked to eachquestion and extract the relevant triplets from Con-ceptNet, creating a TARGETKG from all Concept-Net triplets associated to the test set, which yieldsa filtered set of 363 questions for which we canreliably extract relevant ConceptNet triplets. Weuse these relevant triplets as TARGETKG and theremaining distinct triplets in the LAMA subset ofConceptNet as CONTROLKG to learn a knowledge-critical subnetwork using either weight and neuronmasking for GPT2-small. Then, we apply differentfinetuning methods to the remaining model afterremoving the critical subnetwork, using the sametraining set. We compare finetuning the remain-ing masked model (Weight Mask, Neuron Maskin ) to the performance of finetuning thefull pretrained model (Full), as well as a randomlymasked model at the same sparsity as the masked-weight subnetwork (Random Mask). We reportresults across three random seeds in . For all finetuning methods, we find that the re-maining model with weight masking has similaraccuracy to the pretrained model on the develop-ment split and a close accuracy for the overall testset. However, we observe a consistent significantperformance drop on the filtered subset after fine-tuning (average drop of 7.3%; head tuning barelybetter than selecting a random answer on a 5-choiceMCQA task), indicating that the model strugglesto transfer knowledge associated with TARGETKGduring fine-tuning. Interestingly, in less parameter-efficient finetuning methods, this drop does notpersist when the neuron-masked subnetwork is re-moved, suggesting that knowledge is either stilltransferred or recovered over the course of finetun-ing (Lo et al., 2024). In addition, for both headtuning and LoRA (Hu et al., 2022) with weightmasking, we find that if we randomly split the fil-tered TARGETKG, one halfs knowledge-criticalmask does not affect the accuracy of the other halfas significantly as its own (see Appendix E for de-tails), indicating the performance drop is indeedspecific to the pruned knowledge.",
  "Conclusion": "In this paper, we conceptualize knowledge-criticalsubnetworks, sparse computational subgraphswithin larger language models that are respon-sible for expressing specific knowledge relation-ships.We discover these subnetworks using amulti-objective differentiable masking approachthat jointly optimizes a criterion designed to sup-press the expression of target knowledge whenknowledge-critical subnetworks are removed froma language model, and maintenance criteria that en-sure the language model retains its initial capacityto model other relational knowledge and generallanguage. Our results show that when knowledge-critical subnetworks are removed, a model losesits ability to express the knowledge encoded in thesubnetwork, and to transfer it when finetuned ondownstream tasks requiring the knowledge.",
  "Acknowledgements": "We thank Mohammadreza Banaei, Syrielle Montar-iol, Debjit Paul, Khai Loong Aw, Badr AlKhamissi,Silin Gao, Yifan Hou, Beatriz Borges, Yu Fei, andAngelika Romanou for their helpful discussionsand feedback on our manuscript. We also gratefullyacknowledge the support of the Swiss National Sci-ence Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Center for Imaging, Sony GroupCorporation, and the Allen Institute for AI.",
  "We discuss the limitations of our proposed methodand conducted experiments on three axes: data,model, and hyperparameter. We emphasize that the": "data used for our experiments are limited to En-glish only. As English is a high-resource language,additional challenges could arise when reproduc-ing our method in a low-resource language (e.g.,finding a rich lexical database like WordNet). Weidentify the lack of diverse pretrained languagemodel architectures and language modeling ob-jectives as the main model limitation. We havetested our method on the billion scale but did notexpand our scope to larger models with differentarchitectures (for example, in the 7B scale). Wealso limit the analysis to models trained with anautoregressive language modeling objective in con-trast to text-to-text models such as T5 (Raffel et al.,2020) or Masked-Language-Modeling models suchas RoBERTa (Liu et al., 2019b). Finally, the hyper-parameter search detailed in the Appendix, whilenot exhaustive, provides sufficient evidence to sup-port the validity of the selected range. To findmore precise knowledge-critical subnetworks, fu-ture methods may need to take this hyperparametersearch further.",
  "Ethics Statement": "In this study, we concentrate on relational knowl-edge, but the technique of identifying subnetworkscould be used in mitigating bias within models.Likewise, this method of finding subnetworks mayalso inadvertently lead to the elimination of criti-cal ethical or factual knowledge from a languagemodel, resulting in a model that could generate of-fensive content and misinformation. For example,there exists a backdoor attack method against deepneural networks that builds on top of the identifi-cation and editing of subnetworks (Qi et al., 2021).Therefore, caution should be exercised when apply-ing the identification and removal of subnetworksto models used in essential applications.",
  "Yoshua Bengio, Nicholas Lonard, and Aaron Courville.2013. Estimating or propagating gradients throughstochastic neurons for conditional computation": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-tanya Malaviya, Asli Celikyilmaz, and Yejin Choi.2019. COMET: Commonsense transformers for auto-matic knowledge graph construction. In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics, pages 47624779, Flo-rence, Italy. Association for Computational Linguis-tics. Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-ong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021a.Knowledgeable or educated guess? revisiting lan-guage models as knowledge bases. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 18601874, Online.Association for Computational Linguistics.",
  "Steven Cao, Victor Sanh, and Alexander Rush. 2021b": "Low-complexity probing via finding subnetworks. InProceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 960966, Online. Association for Computa-tional Linguistics. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,Katherine Lee, Florian Tramer, and Chiyuan Zhang.2023. Quantifying memorization across neural lan-guage models. In The Eleventh International Confer-ence on Learning Representations. Nicholas Carlini,Florian Tramer,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, Ul-far Erlingsson, Alina Oprea, and Colin Raffel. 2021.Extracting training data from large language models.",
  "Zeming Chen and Qiyue Gao. 2022. Probing linguisticinformation for logical inference in pre-trained lan-guage models. Proceedings of the AAAI Conferenceon Artificial Intelligence, 36(10):1050910517": "Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyil-maz, and Antoine Bosselut. 2023. Reckoning: Rea-soning through dynamic knowledge encoding. InAdvances in Neural Information Processing Systems,volume 36, pages 6257962600. Curran Associates,Inc. Abhijith Chintam, Rahel Beloch, Willem Zuidema,Michael Hanna, and Oskar van der Wal. 2023. Iden-tifying and adapting transformer-components respon-sible for gender bias in an English language model.In Proceedings of the 6th BlackboxNLP Workshop:Analyzing and Interpreting Neural Networks for NLP,",
  "pages 379394, Singapore. Association for Compu-tational Linguistics": "Kevin Clark, Urvashi Khandelwal, Omer Levy, andChristopher D. Manning. 2019. What does BERTlook at? an analysis of BERTs attention. In Pro-ceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP,pages 276286, Florence, Italy. Association for Com-putational Linguistics. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch,Stefan Heimersheim, and Adri Garriga-Alonso.2023. Towards automated circuit discovery for mech-anistic interpretability. In Advances in Neural Infor-mation Processing Systems, volume 36, pages 1631816352. Curran Associates, Inc. Rbert Csords, Sjoerd van Steenkiste, and JrgenSchmidhuber. 2021. Are neural nets modular? in-specting functional modularity through differentiableweight masks. In International Conference on Learn-ing Representations.",
  "Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, andAntoine Bosselut. 2021. Analyzing commonsenseemergence in few-shot knowledge models. In 3rdConference on Automated Knowledge Base Construc-tion": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, BaobaoChang, and Furu Wei. 2022. Knowledge neurons inpretrained transformers. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 84938502, Dublin, Ireland. Association for ComputationalLinguistics. Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-ing factual knowledge in language models. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 64916506, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics. Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz,and Ivan Titov. 2020.How do decisions emergeacross layers in neural models? interpretation withdifferentiable masking. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 32433255, On-line. Association for Computational Linguistics. Nadir Durrani, Hassan Sajjad, Fahim Dalvi, andYonatan Belinkov. 2020. Analyzing individual neu-rons in pre-trained language models. In Proceed-ings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages48654880, Online. Association for ComputationalLinguistics. Nelson Elhage, Tristan Hume, Catherine Olsson,Nicholas Schiefer, Tom Henighan, Shauna Kravec,Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,Carol Chen, Roger Grosse, Sam McCandlish, JaredKaplan, Dario Amodei, Martin Wattenberg, and",
  "Christopher Olah. 2022. Toy models of superpo-sition. Transformer Circuits Thread": "Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly,Nova DasSarma, Dawn Drain, Deep Ganguli, ZacHatfield-Dodds, Danny Hernandez, Andy Jones,Jackson Kernion, Liane Lovitt, Kamal Ndousse,Dario Amodei, Tom Brown, Jack Clark, Jared Ka-plan, Sam McCandlish, and Chris Olah. 2021. Amathematical framework for transformer circuits.Transformer Circuits Thread. Negar Foroutan, Mohammadreza Banaei, Rmi Lebret,Antoine Bosselut, and Karl Aberer. 2022. Discov-ering language-neutral sub-networks in multilinguallanguage models. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 75607575, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics.",
  "Jonathan Frankle and Michael Carbin. 2019. The lotteryticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on LearningRepresentations": "Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual associa-tions in auto-regressive language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Association for Computational Linguis-tics. Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, ShovalSadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.2022a. LM-debugger: An interactive tool for inspec-tion and intervention in transformer-based languagemodels. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing:System Demonstrations, pages 1221, Abu Dhabi,UAE. Association for Computational Linguistics. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-berg. 2022b. Transformer feed-forward layers buildpredictions by promoting concepts in the vocabularyspace. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Process-ing, pages 3045, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 54845495, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "Demi Guo, Alexander Rush, and Yoon Kim. 2021": "Parameter-efficient transfer learning with diff prun-ing. In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages48844896, Online. Association for ComputationalLinguistics. Anshita Gupta, Debanjan Mondal, Akshay Sheshadri,Wenlong Zhao, Xiang Li, Sarah Wiegreffe, and NiketTandon. 2023. Editing common sense in transform-ers. In Proceedings of the 2023 Conference on Em-pirical Methods in Natural Language Processing,pages 82148232, Singapore. Association for Com-putational Linguistics. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghan-deharioun. 2023a. Does localization inform editing?surprising differences in causality-based localizationvs. knowledge editing in language models. In Ad-vances in Neural Information Processing Systems,volume 36, pages 1764317668. Curran Associates,Inc. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-nitsa Kozareva, Veselin Stoyanov, Mohit Bansal, andSrinivasan Iyer. 2023b. Methods for measuring, up-dating, and visualizing factual beliefs in languagemodels. In Proceedings of the 17th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 27142731, Dubrovnik,Croatia. Association for Computational Linguistics.",
  "Circuit breaking: Removing model behaviors withtargeted ablation": "Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.2016. Commonsense knowledge base completion.In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 14451455, Berlin, Germany.Association for Computational Linguistics. Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and XiangRen. 2019. KagNet: Knowledge-aware graph net-works for commonsense reasoning. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 28292839, Hong Kong,China. Association for Computational Linguistics. Nelson F. Liu, Matt Gardner, Yonatan Belinkov,Matthew E. Peters, and Noah A. Smith. 2019a. Lin-guistic knowledge and transferability of contextualrepresentations. In Proceedings of the 2019 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long and Short Pa-pers), pages 10731094, Minneapolis, Minnesota.Association for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing.ACM Comput. Surv., 55(9).",
  "Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.2017. The concrete distribution: A continuous relax-ation of discrete random variables. In InternationalConference on Learning Representations": "Arun Mallya, Dillon Davis, and Svetlana Lazebnik.2018. Piggyback: Adapting a single network to mul-tiple tasks by learning to mask weights. In Proceed-ings of the European Conference on Computer Vision(ECCV), pages 6782. Kevin Meng, David Bau, Alex Andonian, and YonatanBelinkov. 2022. Locating and editing factual asso-ciations in gpt. In Advances in Neural InformationProcessing Systems, volume 35, pages 1735917372.Curran Associates, Inc.",
  "Chris Olah, Nick Cammarata, Ludwig Schubert, GabrielGoh, Michael Petrov, and Shan Carter. 2020. Zoomin: An introduction to circuits. Distill": "Catherine Olsson, Nelson Elhage, Neel Nanda, NicholasJoseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,Danny Hernandez, Scott Johnston, Andy Jones, Jack-son Kernion, Liane Lovitt, Kamal Ndousse, DarioAmodei, Tom Brown, Jack Clark, Jared Kaplan,Sam McCandlish, and Chris Olah. 2022. In-contextlearning and induction heads. Transformer CircuitsThread. Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language models as knowl-edge bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, China. Associationfor Computational Linguistics.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Siyu Ren and Kenny Zhu. 2022.Specializing pre-trained language models for better relational reason-ing via network pruning. In Findings of the Associ-ation for Computational Linguistics: NAACL 2022,pages 21952207, Seattle, United States. Associationfor Computational Linguistics.",
  "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020": "How much knowledge can you pack into the param-eters of a language model? In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 54185426,Online. Association for Computational Linguistics. Tara Safavi and Danai Koutra. 2021. Relational WorldKnowledge Representation in Contextual LanguageModels: A Review.In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 10531067, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics.",
  "Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020": "BERTnesia: Investigating the capture and forgettingof knowledge in BERT. In Proceedings of the ThirdBlackboxNLP Workshop on Analyzing and Interpret-ing Neural Networks for NLP, pages 174183, On-line. Association for Computational Linguistics. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel Bowman. 2019a. Superglue: A stickierbenchmark for general-purpose language understand-ing systems.In Advances in Neural InformationProcessing Systems, volume 32. Curran Associates,Inc. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019b.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In Interna-tional Conference on Learning Representations. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-pretability in the wild: a circuit for indirect objectidentification in GPT-2 small. In The Eleventh Inter-national Conference on Learning Representations. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,Michael Zeng, and Meng Jiang. 2023.Generaterather than retrieve: Large language models arestrong context generators. In The Eleventh Inter-national Conference on Learning Representations. Xiongyi Zhang, Jan-Willem van de Meent, and ByronWallace. 2021. Disentangling representations of textby masking transformers. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 778791, Online and PuntaCana, Dominican Republic. Association for Compu-tational Linguistics. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-rich Schtze. 2020. Masking as an efficient alterna-tive to finetuning for pretrained language models. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 22262241, Online. Association for Computa-tional Linguistics.",
  "Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021": "Factual probing is [MASK]: Learning vs. learningto recall. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 50175033, Online. Associationfor Computational Linguistics. Kankan Zhou, Eason Lai, Wei Bin Au Yeong, KyriakosMouratidis, and Jing Jiang. 2023. ROME: Evaluat-ing pre-trained vision-language models on reasoningbeyond visual common sense. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 1018510197, Singapore. Associationfor Computational Linguistics.",
  "TARGETKGTo gather small connected TAR-": "GETKGs, we randomly select an initial node andsample knowledge triplets by walking a depth ofthree up (parent direction) and down (child di-rection) in the respective KG. Given a seed nodesuch as representation in WordNet12 or fruitin ConceptNet, we sample relations by perform-ing a 3-hop random walk. For example, for thefruit KG shown in , we start from theseed concept fruit. In the first depth, we retrieve(fruit, ReceivesAction, eaten) and (wine,MadeOf, fruit). In the next depth, we retrieve(champagne, IsA, wine), and so forth for all pos-sible relations. Note that we only sample relationswith a single-token tail entity.Once this connected KG is sampled, we applytwo filtering processes.The first one enforcesmany-to-one relationships in KT to avoid headentities with multiple tails. The second filteringprocess reduces the tail-entity imbalance to avoidover-fitting to a small set of tokens. For this, wecount the frequency of the tail tokens in the sam-pled graph and keep at most a quartile amount oftriplets with shared tail entities.Finally, we verbalize TARGETKG graph withthe formats that give the lowest perplexity on thepretrained model. We try various relation-specificverbalization templates per knowledge triplet andpick the one that yields the lowest tail-token per-plexity.For example, in the representationgraph, while the model had lower perplexitywith the template {h} is a kind of {t}for the triplet (representation.n.02,IsA, 12In WordNet, a word sense is represented by its lemma,syntactic category, and sense ID (e.g., in map.n.01, n for nounand 01 for sense ID). We omit this naming convention fromthe main paper tables for readability.",
  ": Examples of KG triplets, and the best GPT-2 small verbalization for WordNet and ConceptNet": "creation.n.02), it also had lower perplexity withthe template A {h} is a {t} for the triplet(chart.n.02, IsA, map.n.01). Note that thiscan change for each model size, such as GPT2-small, medium, large and XL. CONTROLKGTo create CONTROLKG, we pri-oritize not leaking TARGETKG counterfactualsand having a shared CONTROLKG across differentTARGETKGs. Therefore, we remove from the com-plete KG (e.g., for ConceptNet TARGETKGs, thecomplete LAMA subset of ConceptNet) any tripletthat shares the same entities as the union of theTARGETKGs shown in . For all KG verbal-izations, to remove and maintain knowledge thatthe model is already confident about, we pick thebest scoring verbalization for each triplet amongseveral prompt styles and filter out those that yieldan individual PPL higher than a threshold. Fortesting, we use held-out triplets.",
  "BTraining and EvaluationImplementation": "Mask ImplementationAs mentioned in 5, dur-ing mask learning, we do not mask the embedding,language modeling head, layer-normalization, andbias parameters. We also only learn masks for thetop 50% of the transformer layers. We initializethe mask parameters such that, in the first forwardpass, each model parameter has a starting maskingprobability of (li) = 0.45, meaning the search isexpected to start with an empty knowledge-criticalsubnetwork (i.e., a subnetwork mask of zeros) anda fully-connected inverse subnetwork (i.e., the fullmodel). Results on a hyperparameter search forinitialization can be found in . Moreover,for the randomly masked baseline, we mask eachmodule (e.g., MLP module at layer 8) at the samesparsity as the corresponding module in the criti-cal subnetwork, which means that the masking isnot uniformly done across all layers. For neuronmasking, we jointly learn a mask across weights ina linear layer that connect to the same input neu-ron. For the randomly masked neuron baseline,we mask each module at the same neuron sparsityas the corresponding module in the critical subnet-work.",
  ": Selection limit for each success criteria": "AdamW optimizer. For equation 6, we set 1 = 1.5and 2 = 3 = 1 in all of our our experiments. Toencourage the subnetwork to be sparser, we sched-ule 4 to start at 2 and increase linearly after 50%of the training until it reaches 3. For GPT2-small,we use a single GPU setting to run the mask train-ing for 40,000 steps. For GPT2-medium and large,we use a three GPU distributed setting and run themask training for 50,000 steps. For GPT2-XL, weuse a three GPU distributed setting and run themask training for 60,000 steps. Software and HardwareWe primarily use Py-Torch13 and Huggingface Transformers14 to imple-ment the masking method. Experiments for GPT2-small, medium and large are run on NVIDIA A10040GB devices. Experiments for GPT2-XL are runon NVIDIA A100 80GB devices. Loss Trade-Off AnalysisA primary driver ofthe knowledge-critical subnetwork search is thetrade-off between the suppression and maintenancelosses. To validate our i choices, we run a min-imal experiment on giving importance to one ob-jective at a time for two TARGETKGs and onerandom seed. Specifically, when we set any oneof the weights in Eq. 6 to a value of 3, we setthe value of the rest to 1. As seen in , wefind that giving more weight to the suppressionloss finds checkpoints with higher perplexity dif-ferences on TARGETKG while simultaneously sat-isfying the maintenance criteria. Moreover, givingmore weight to the sparsity regularization ensuresa higher sparsity. These results support the i hy-perparameters we use in all of our experiments, asdescribed above. DataloadersAs each TARGETKG is small, ateach gradient step, the model sees the completegraph. Therefore, the TARGETKG batch size isthe same as the number of triplets (see ). Incontrast, CONTROLKG and CONTROLLM datasetshave thousands of entries in total. To balance thelearning and make it more efficient, we create a",
  "Masked LayerPercentageSparsityTARGETKGCONTROLKGCONTROLLM# ofChoiceMasked()PPL()PPL ()PPL ()checkpoints": "0-11100%95.6 [94.9, 96.6]242.7 [-26.6, 1254.3]11.8 [6.7, 15.9]1.3 [1.0, 1.7]1.13-1175%97.3 [94.4, 98.3]669.7 [-8.7, 2119.7]-1.4 [-8.3, 10.8]1.0 [0.5, 2.9]76.96-1150%98.6 [97.1, 99.2]870.4 [38.7, 2665.1]0.4 [-2.6, 4.0]0.5 [0.3, 1.0]104.19-1125%99.2 [98.0, 99.7]1185.0 [62.3, 4787.0]4.2 [0.1, 9.5]0.4 [0.0, 0.8]103.2 : Subnetwork discovery results for different percentages of upper layers masked in GPT-2 small,averaged over four KGs and two seeds with [min, max] values denoted in brackets. The arrows (,) show thedesired value for the metric. dynamic cyclical training dataloader that samples anew batch at each step without replacement. Whenthe dataloader reaches the end of the dataset, itrestarts with a new ordering. Please refer to Ta-ble 10 for the exact batch sizes. Best Checkpoint SelectionWe iteratively selectthe best checkpoint, starting with strict criteria onthe maintenance datasets and gradually looseningthem. We check whether any checkpoints satisfythe first set of criteria limits shown in .The checkpoints need to have a TARGETKG PPLabove the mentioned floor and maintenance PPLbelow the mentioned ceiling. If the set of check-points retrieved is empty, we select from the nextset of limits. If none of the iterations are successful,we pick the last checkpoint as the best one.",
  "CMasked Layer Choice Study": "Layer-wise model probing analyses have shownthat the first layers of transformer language modelsencode representations crucial for low-level linguis-tic tasks and features that may be a prerequisite forknowledge modeling (Tenney et al., 2019; Liu et al.,2019a). Researchers have also shown that knowl-edge is not only contained in the final few layers(Wallat et al., 2020). Therefore, for our datasets,we investigate how masking different percentagesof upper dense layers can affect the success criteriadefined for a knowledge-critical subnetwork. Inparticular, we look at the effect of masking the top25%, 50%, 75%, and 100% of the model.In , we observe that masking all denselayers in transformer blocks (100%) can affect themaintenance criteria significantly. CONTROLKGperplexity difference is smaller when maskingfewer layers, confirming that lower layers may haveimperative representation to knowledge modeling.As the values for the different criteria are similarfor masking the top 25% and 50%, we use the top50% masking approach to increase the masking",
  "In this section, we provide additional metrics forsubnetwork discovery results and non-aggregatedresults for the randomly masked baseline": "Minimum & Maximum BoundariesIn additionto the average PPL and Rank presented in Ta-ble 2, we add minimum and maximum boundariesto all of the results in and 15. We also pro-vide log probability differences LogProb similarto how PPL is calculated. We observe in the same trend as PPL. On average, removing thesubnetwork increases the rank of the gold tail tokenand decreases the log probability. In contrast, therandomly masked baseline does not increase theTARGETKG rank significantly and does not main-tain CONTROLKG rank to the same extent as thecritical subnetwork. Model ScaleWe include the individual KG re-sults for larger models in . While individ-ual results on GPT2-medium are not as sparse andeffective as the small and large variants, it is stillmore significant than randomly masking the modelat the same sparsity. Randomly Masked BaselineWe provide thenon-aggregated randomly masked baseline resultsfor GPT2-small in and for larger models in. We notice that KGs where the pretrainedmodel perplexity is already low (see ) seemnot to be as affected by a random subnetwork re-moval as those that have a higher initial perplexity.",
  "WordNet": "building98.4 [97.4, 99.3]5.8 [3.0, 10.2]14.8 [3.0, 26.2]2.8 [1.0, 5.2]communication99.2 [99.0, 99.3]5.0 [-2.4, 10.1]4.6 [0.3, 7.6]1.2 [1.0, 1.4]change98.4 [98.0, 99.1]33.9 [25.7, 43.3]24.2 [16.2, 36.2]2.0 [1.3, 2.6]statement98.2 [96.3, 99.2]15.6 [-0.3, 34.6]0.0 [-3.5, 3.4]3.3 [1.2, 6.8]location99.0 [98.8, 99.1]18.8 [-15.8, 55.8]0.2 [-7.5, 4.6]1.6 [1.3, 1.9]representation98.1 [97.1, 98.8]48.8 [11.4, 80.3]46.2 [30.6, 66.2]3.0 [1.6, 4.5]magnitude99.0 [98.6, 99.3]41.9 [21.1, 70.4]12.3 [-0.2, 29.2]1.6 [0.8, 2.0]",
  "Random Weights99.1 [99.0, 99.2]21.0 [13.7, 29.4]14.6 [12.4, 17.2]1.5 [1.3, 1.7]Average99.1 [99.0, 99.2]636.4 [276.8, 888.4]2.8 [2.3, 3.2]0.2 [0.2, 0.3]": ": Subnetwork discovery results for GPT-2 small with weight masking, averaged over three seeds with[min, max] values denoted in brackets. PPL = PPL(f(x, m )) - PPL(f(x, )). The arrows (,) show thedesired value for the metric. Random is an average of randomly masked baselines at the same sparsity levels as thediscovered knowledge-critical subnetworks for each KG-seed pair. questions are not explicitly annotated with a rela-tion. However, they were constructed with Con-ceptNet such that each questions head conceptrelates to four of the tail answers with the samerelation. This does not apply to the fifth answer, ascrowd workers created them. Therefore, to retrievethe relations, we iterate through the questions andcheck if any relations with the question head con-cept and correct tail answer exist in the LAMA andCommonsense Knowledge Base Completion sub-sets of ConceptNet (Li et al., 2016; Petroni et al.,2019). If it does and has only one relation, wechoose that relation. If it has multiple relations, wetake the union of relations between the head con-cept and the distractor tail answers and intersectthat with the correct tail triplets. If the intersectionis a set larger than one element, we choose onerelation at random. Out of the 1221 test questions,only 572 have a single-token correct answer, andwe could only find the corresponding relation to363 questions, which is our filtered test set.For the MCQA head, we use the HuggingfaceDouble Heads model.15In addition to the lan-guage modeling head, this model adds a parallelmultiple-choice classification head. The MCQAhead takes as input the last sequence output. Tofinetune the MCQA model, we use three kinds offine-tuning. The first one is Head Tuning, in whichthe model parameters are frozen, but the MCQAhead is not. The second method is LoRA (Hu et al., 2022), which is a parameter-efficient fine-tuning method. Similar to the head tuning method,LoRA freezes the model parameters and insteadinserts trainable rank decomposition parameters ineach transformer layer. We use a rank of 16 forall LoRA experiments. Finally, we also try FullFinetuning, in which all model parameters aretuned. To remove a subnetwork, we manually setthe knowledge-critical parameters to 0. Therefore,the value of these parameters can change duringfull finetuning. In addition, we also verify whether learning amask for one randomly selected half of the filteredtest set (Half 1) corrupts downstream task transferfor a distinct half (Half 2), where there are no tripletoverlaps. We find in that, on average, theaccuracy on the triplets the mask was trained for isless by 3.6% than the held-out half.",
  "Random Neurons94.9 [93.9, 95.4]110.7 [60.7, 177.5]70.4 [51.5, 107.2]11.2 [9.1, 14.7]Average94.9 [93.9, 95.4]22422.0 [11669.2, 31616.9]71.9 [61.4, 77.5]5.4 [4.8, 6.4]": ": Subnetwork discovery results for GPT-2 small with neuron masking, averaged over three seeds with[min, max] values denoted in brackets. PPL = PPL(f(x, m )) - PPL(f(x, )). The arrows (,) show thedesired value for the metric. Random is an average of randomly masked baselines at the same sparsity levels as thediscovered knowledge-critical subnetworks for each KG-seed pair.",
  "xlog(f(x, m ))(7)": "In , we compare subnetworks extracted inthis manner (i.e., Expression-only) with those ofour main method, as well as those of a combina-tion of these objectives: Lfinal + 5Lexpress. Inter-estingly, we find that the Expression-only settingcan learn a mask for a highly sparse subnetwork,which, when removed from the full model, alsosignificantly increases perplexity on TARGETKG.However, this subnetwork also struggles to main-tain perplexity on CONTROLKG, indicating it mayencode abilities crucial for expressing any set of re-lational knowledge. Adding the expression loss toour joint objective mitigates this issue, but reducessubnetwork sparsity by a significant margin (4%),indicating that the Expression-only loss may dis-cover spurious subnetworks that are not actuallyknowledge-critical they are not responsible forthe expression of the knowledge when they are en-tangled in the full model, though their parametersmay compute a function that expresses it.",
  "GSpurious Subnetworks Test": "We hypothesize that a spurious subnetwork wouldcause the remaining network from which it was re-moved to re-gain the ability to express TARGETKGif the subnetwork was randomly expanded (i.e.,PPL on TARGETKG would drop as more param-eters are removed from f(x, m )). Meanwhile, if removing the critical subnetwork is not a spuri-ous solution to suppress the TARGETKG, then theremaining model would generally still fail to recog-nize TARGETKG, even as more parameters wererandomly removed, leading PPL to rise or staythe same. To verify this hypothesis, we remove fur-ther parameters from the remaining model. Startingfrom the knowledge-critical subnetwork sparsity,we randomly remove parameters at intervals of0.5%. We run this iterative process of removing pa-rameters with five different random seeds. We alsotest whether the mask has found a spurious solutionto achieve the maintenance criteria by adding backparameters, though with smaller intervals of 0.1%,as the starting sparsity level is typically high. In , we observe that removing more pa-rameters in small amounts does not significantlyrecover expressing TARGETKG. As a baseline, weplot the effect on PPL of removing further pa-rameters from remaining models with randomlyremoved subnetworks of the same sparsity. Inter-estingly, for the maintenance datasets, PPL forboth datasets increases as we remove parametersfrom the remaining model. When we add back pa-rameters, we do not see a linear recovery to PPL= 0. Instead, we observe an initial phase of in-crease followed by a phase of decrease as the modelreturns to its original state (i.e., a PPL of zeroat 100% sparsity). This effect can be explained bythe fact that our subnetwork had been optimized tokeep these abilities, and has been slightly overfitfor maintenance, though not for suppression. Thus,",
  "Random11.4 [2.1, 20.0]5.5 [4.2, 7.3]0.0 [-0.1, 0.0]-0.1 [-0.1, -0.1]Average636.1 [331.7, 1164.9]1.6 [1.4, 1.8]-0.9 [-1.2, -0.4]0.0 [0.0, 0.0]": ": Subnetwork discovery rank and log probability results for GPT-2 small with weight masking,averaged over three seeds. Metric = Metric(f(x, m )) - Metric(f(x, )) for Rank and LogProb. Random is anaverage of randomly masked baselines at the same sparsity levels as the discovered knowledge-critical subnetworksfor each KG-seed pair. Note that non-zero values may be rounded to 0.0 as we round to one decimal place. IndividualKG results for the random baseline are in 17.",
  "HStructural Analysis": "In this section, we investigate the structure of the re-moved knowledge-critical subnetworks by lookingat their relative density across different layer types(), and more specifically, across differentattention heads () and the Wq, Wk, andWv matrices in attention sublayers (. Thedensity is calculated relatively, meaning accordingto the particular sublayers size. The model used isGPT2-small.Layer depth-wise, we observe that the subnet-work is consistently most dense around the first andfinal masked transformer blocks, which are layers7 and 12 in . Specifically, layer type-wise,we find that knowledge-critical subnetworks aremost dense in the attention sublayers for layer 7and layer 12 (Attn-Out and Attn-Wq, Wk, Wv).In addition, we have not found any completecolumns or rows that were dense in the criticalsubnetworks. This means no input or output neu-ron features get completely removed when thecritical subnetwork is removed. Therefore, themasked region may not be working to zero-out theknowledge by turning specific features off, whichwould counter the prevailing view that neuron-level changes are necessary for mechanistic interven-tions (Dai et al., 2022; Meng et al., 2022).When we investigated attention heads and Wq,Wk, and Wv masks in detail for 3 KGs and 3 seeds,we found that head 10 in layer 7, and heads 1 and9 in layer 12 are significantly dense. Moreover,the Wv mask is consistently the most dense acrossthe three attention Wq, Wk, and Wv masks. There-fore, while the subnetworks do not have a signifi-cant IoU, as demonstrated by the seed-based (Ap-pendix I) and the KG-based analyses (Appendix J),the subnetworks still tend to be dense in similarlayer types at similar layer depths.",
  "IRandom Seed-Based Analysis": "We investigate the stability of subnetwork discov-ery under random seed variance for GPT2-small.We also explore whether composing subnetworksfrom different seeds could increase the suppressioneffect while still fulfilling the rest of the successcriteria. Seed-based VariancePrior work shows that sub-networks identified under distinct random seedsmay differ with a large variance (Csords et al.,2021). We inspect how subnetworks from the bestcheckpoints for three random seeds overlap for anindividual TARGETKG. We use Jaccard similar-ity, or intersection over union (IoU), as the overlapmetric. In , we plot a Venn diagram of",
  "Random99.3 [99.2, 99.4]1.6 [1.0, 2.4]0.1 [-0.6, 1.2]0.1 [0.1, 0.2]Average99.3 [99.2, 99.4]536.5 [257.0, 789.9.6]3.2 [2.8, 3.6]0.0 [0.0, 0.1]": ": Subnetwork discovery results on larger models per KG with weight masking, averaged over two seeds.Random is an average of randomly masked baselines at the same sparsity levels as the discovered knowledge-criticalsubnetworks for each KG-seed pair. Individual KG results for the random baseline are in . parameter overlap for each knowledge graph. Wecan see that, on average, when using IoU, onlyaround 3.7% of the unioned subnetwork param-eters overlap across the three seeds (3.76% forlocation, 3.8% for communication, and 3.5% forrepresentation), meaning the subnetworks iden-tified under different random seeds vary, whichcomplies with prior works analysis. Across layers,the IoU is also similarly low with a higher over-lap for the final attention layer masks (10%) asshown in . Subnetwork CompositionWe combine masksof three seeds in their intersection, their floral in-tersection (intersection unioned with each inter-section of two seeds), and overall union to mea-sure the effect on PPL for TARGETKG, CON- TROLKG, and CONTROLLM. We average the re-sults over three KGs (representation, location,and communication).In , we observe that removing the inter-section and floral intersection of the subnetworksdoes not increase TARGETKG PPL. On the otherhand, removing the union of the subnetworks in-creases the TARGETKG perplexity difference sig-nificantly larger than the original results. However,combining the subnetworks and removing them in-creases PPL on maintenance datasets more thanusing an individual seeds subnetwork, as seen inthe original results. We note that the increase inthe PPL on maintenance datasets matches the increase we get when removing an equally sparserandom subnetwork (see ). Therefore, itmay be possible to naively combine subnetworks;however, they may not guarantee the maintenancecriteria to the same extent. A future idea could beto continue optimizing for the subnetwork mask byinitializing it as the union of the subnetworks to seeif more robust suppression can be achieved.",
  "JKnowledge-Based Analysis": "This section examines the overlap of subnetworksacross different KGs for the same seed with GPT2-small. This contrasts with the previous section thatstudies the overlap of subnetworks across differentseeds for the same KG. Similarly, we use Jaccardsimilarity, or intersection over union (IoU), as theoverlap metric. We also explore whether compos-ing subnetworks for different KGs from the sameseed could suppress all of the TARGETKGs. Knowledge-based VarianceIn , weplot a Venn diagram of parameter overlap for eachseed across different TARGETKGs. On average,when using IoU, only around 3.56% of the unionedsubnetwork parameters overlap across the threeseeds (4.08% for seed 735, 4.01% for seed 1318,and 2.65% for seed 84). Across layers, the IoUis also similarly low with a significantly higheroverlap for the final attention layer masks (12%)as shown in .",
  ": Subnetwork discovery results on the randomly masked baseline for GPT2-small weight masking,averaged over three seeds": "Subnetwork CompositionWe combine masksof three KGs for the same seed in their intersection,their floral intersection (intersection unioned witheach intersection of two KGs), and overall unionto measure the effect on PPL for TARGETKG,CONTROLKG, and CONTROLLM. We average theresults over three seeds (735, 1318, and 84).Similar to the findings in composing subnet-works for different seeds, shows that com-posing subnetworks for different KGs increasesthe PPL on TARGETKG when using their union.However, removing the union of the subnetworksalso has higher perplexity differences on mainte-nance datasets than using an individual KGs sub-network, as seen in the original results. Once again,this PPL increase on the maintenance datasetsmatches the difference we would observe usingan equally sparse random subnetwork. Therefore,while subnetworks of different KGs may be com-posable to fortify the suppression effect, they maynot guarantee the maintenance criteria to the sameextent as the individual subnetworks.",
  "Removed Subnetwork Sparsity": "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 ControlLM PPL for communication Original SparsityFound SubnetworkRandomly Masked : Removing and adding parameters to the remaining GPT2-small model, averaged over five seeds, withstandard deviation depicted as the filled area around the average curves. The x-axis is the removed subnetworksparsity. The y-axis is the PPL = PPL(f(x, m )) - PPL(f(x, )) for the different datasets. Vertical dashedlines show the original sparsity of the critical subnetwork. The darker curve is the outcome starting from the criticalsubnetwork, whereas the lighter curve is from a randomly masked model at the same sparsity.",
  ": Composing subnetworks across KGs with GPT2-small and weight masking, averaged across threeseeds. Original stands for the individual subnetwork removal average across the same three seeds and KGs": "representation-735 representation-1318 representation-84 location-735 location-1318 location-84 communication-735 communication-1318 communication-84 L12 FF-2 L12 FF-1 L12 Attn-Out L12 Attn-Wq, Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 L9 Attn-Out L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 Attn-Out L8 Attn-Wq, Wk, Wv L7 FF-2 L7 FF-1 L7 Attn-Out L7 Attn-Wq, Wk, Wv 2.31.83.32.01.51.41.41.11.2 1.91.32.91.41.01.01.00.70.8 2.82.63.72.72.52.52.52.32.4 1.31.22.31.21.01.11.00.81.0 1.10.92.01.00.70.70.70.50.6 1.10.82.20.90.60.60.60.40.5 0.40.31.20.30.20.20.20.10.2 0.80.61.30.60.50.50.50.30.4 0.80.71.70.70.50.50.50.40.5 1.00.82.10.80.50.50.60.40.5 0.40.31.50.40.20.20.20.10.2 0.90.71.60.80.60.60.60.50.5 0.90.81.90.80.60.60.70.50.5 1.20.92.40.90.60.60.70.50.5 0.60.52.40.80.30.70.40.20.3 1.11.12.21.10.90.90.90.80.9 1.31.12.91.10.80.80.90.60.7 1.61.13.31.10.70.70.90.60.6 1.20.83.40.80.50.50.70.40.9 1.91.53.31.41.11.11.31.01.1 2.21.74.31.61.21.21.51.01.1 2.81.94.91.91.31.31.51.01.1 3.92.77.32.31.71.62.31.71.6 4.53.26.12.72.22.12.82.22.1",
  "Density % for all layer masks": "0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 : Average module mask density with weight masking, for different KGs ( representation, location,and communication) and seeds. Reported in percentage (%). The brighter the color, the higher the removed maskdensity. representation-735 representation-1318 representation-84 location-735 location-1318 location-84 communication-735 communication-1318 communication-84 L12 FF-2 L12 FF-1 L12 Attn-Out L12 Attn-Wq, Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 L9 Attn-Out L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 Attn-Out L8 Attn-Wq, Wk, Wv L7 FF-2 L7 FF-1 L7 Attn-Out L7 Attn-Wq, Wk, Wv 6.35.05.74.93.34.65.96.05.3 3.94.24.23.53.33.54.33.63.3 6.15.75.76.86.66.26.16.25.9 17.8 15.1 15.6 17.7 16.7 17.6 17.1 15.2 11.1 2.93.02.62.63.02.83.13.02.6 2.62.12.53.02.53.12.52.61.8 0.30.00.10.30.10.30.30.30.0 0.80.30.50.30.40.30.80.40.5 4.14.44.13.74.04.03.84.13.3 2.31.21.61.82.12.22.52.53.1 0.81.01.00.30.40.70.80.80.5 2.11.22.00.50.41.41.00.51.4 4.14.03.64.23.83.73.13.52.9 2.92.12.21.61.31.41.71.32.0 2.53.12.30.80.90.90.72.22.9 3.94.63.60.10.00.40.33.45.3 4.34.43.94.44.74.54.14.13.7 3.05.54.26.55.97.28.36.56.4 0.80.71.01.01.31.21.61.00.8 0.71.81.22.32.02.52.62.11.6 5.96.86.85.45.35.06.26.76.0 3.14.03.13.95.14.76.95.34.8 24.9 21.6 21.5 17.6 18.6 16.7 14.1 19.9 20.4 25.8 23.6 25.1 18.9 19.8 18.6 17.1 23.0 21.1",
  "Layers": "4.52 0.18 0.41 0.65 0.67 0.15 0.43 0.16 3.720.20.75 0.44 0.77 0.17 0.43 0.18 0.24 0.760.20.74 0.32 0.85 0.45 0.64 0.660.10.44 1.72 0.33 0.72 0.19 0.25 0.29 0.52 0.990.9 0.90.38 1.02 0.53 1.05 1.27 0.19 1.781.10.84 1.46 0.62 2.14 0.71 1.29 1.73 1.32 0.82 0.89 0.38 2.67 1.47 0.57 1.67 2.92.821.10.82 3.17 2.78 1.45 3.52 3.079.11.04 2.18 communication-735",
  ": Venn diagrams for parameter overlap of three subnetworks identified under three different randomseeds with weight masking, for each KG representation, location, and communication": "549,120 360,960301,056 386,304 112,896 370,176 1,100,544 seed-735seed-1318 seed-84 Parameter overlap for different seeds and same KG \"communication.n.02\" 246,528 359,424 157,440 294,912 304,128 150,528 1,230,336 seed-735seed-1318 seed-84 Parameter overlap for different seeds and same KG \"location.n.01\" 471,552476,160 217,344 320,256 347,136274,176 1,144,320 seed-735seed-1318 seed-84 Parameter overlap for different seeds and same KG \"representation.n.02\"",
  ": Venn diagrams for parameter overlap of three subnetworks identified under three different randomseeds with input neuron masking, for each KG representation, location, and communication": "representation location communication L12 FF-2 L12 FF-1 L12 Attn-Out L12 Attn-Wq, Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 L9 Attn-Out L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 Attn-Out L8 Attn-Wq, Wk, Wv L7 FF-2 L7 FF-1 L7 Attn-Out L7 Attn-Wq, Wk, Wv 0.060.040.04 0.040.040.04 0.10.110.11 0.070.130.13 0.050.050.05 0.030.030.03 0.010.010.01 0.030.030.03 0.020.020.02 0.020.020.01 0.010.010.01 0.040.040.05 0.020.020.02 0.020.020.02 0.010.020.01 0.030.040.04 0.020.020.02 0.020.020.01 0.010.020.02 0.030.050.05 0.030.030.02 0.030.020.02 0.050.030.06 0.050.040.05",
  ": Jaccard similarity of different seed masks for the same KG with weight masking, (representation,location, and communication). The brighter the color, the higher the Intersection over Union": "representation location communication L12 FF-2 L12 FF-1 L12 Attn-Out L12 Attn-Wq, Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 L9 Attn-Out L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 Attn-Out L8 Attn-Wq, Wk, Wv L7 FF-2 L7 FF-1 L7 Attn-Out L7 Attn-Wq, Wk, Wv 0.440.360.45 0.360.370.35 0.510.580.53 0.480.50.29 0.350.360.32 0.250.360.24 0.00.50.0 0.120.20.22 0.340.40.29 0.20.550.4 0.230.330.38 0.170.080.19 0.270.40.29 0.230.20.11 0.370.750.07 0.220.00.0 0.250.40.31 0.210.390.4 0.070.50.29 0.090.360.35 0.320.470.35 0.160.30.34 0.420.520.39 0.50.660.5",
  ": Jaccard similarity of different seed masks for the same KG with input neuron masking,(representation, location, and communication). The brighter the color, the higher the Intersection overUnion": "478,547336,76581,022 265,315 60,33942,471 53,895 representation.n.02 location.n.01 communication.n.02 Parameter overlap for different KGs and the same seed \"735\" 380,563255,06552,010 195,874 41,35533,752 40,030 representation.n.02 location.n.01 communication.n.02 Parameter overlap for different KGs and the same seed \"1318\" 1,021,277223,95779,177 199,509 69,420 30,622 44,280 representation.n.02 location.n.01 communication.n.02 Parameter overlap for different KGs and the same seed \"84\"",
  ": Venn diagrams for parameter overlap of three subnetworks identified under three different KGswith weight masking, for each seed 735, 1318, and 84": "L12 FF-2 L12 FF-1 L12 Attn-Out L12 Attn-Wq, Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 L9 Attn-Out L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 Attn-Out L8 Attn-Wq, Wk, Wv L7 FF-2 L7 FF-1 L7 Attn-Out L7 Attn-Wq, Wk, Wv 0.060.050.05 0.050.050.04 0.150.130.12 0.120.120.06 0.050.050.03 0.030.030.02 0.010.010.0 0.040.040.03 0.020.020.02 0.020.020.01 0.020.010.01 0.050.060.04 0.030.020.02 0.020.020.01 0.010.010.01 0.050.050.03 0.030.020.02 0.020.020.01 0.020.020.01 0.050.050.03 0.030.030.02 0.030.020.02 0.030.040.02 0.040.040.03"
}