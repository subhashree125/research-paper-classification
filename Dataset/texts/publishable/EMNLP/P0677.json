{
  "Abstract": "Recent studies have shown that distributed ma-chine learning is vulnerable to gradient inver-sion attacks, where private training data canbe reconstructed by analyzing the gradients ofthe models shared in training. Previous attacksestablished that such reconstructions are possi-ble using gradients from all parameters in theentire models. However, we hypothesize thatmost of the involved modules, or even theirsub-modules, are at risk of training data leak-age, and we validate such vulnerabilities in var-ious intermediate layers of language models.Our extensive experiments reveal that gradientsfrom a single Transformer layer, or even a sin-gle linear component with 0.54% parameters,are susceptible to training data leakage. Ad-ditionally, we show that applying differentialprivacy on gradients during training offers lim-ited protection against the novel vulnerabilityof data disclosure.1",
  "Introduction": "As the requirement for training machine learningmodels on large-scale and diverse datasets inten-sifies, distributed learning frameworks have risenas an effective solution that balances both the needfor intensive computation and critical privacy con-cerns among edge users. As a prime example,Federated Learning (FL) (McMahan et al., 2016)preserves the privacy of participants by retainingeach clients data on their own devices, while onlyexchanging essential information, such as modelparameters and updated gradients. Nonetheless,recent research (Zhu et al., 2019; Dang et al., 2021;Balunovic et al., 2022) has demonstrated the pos-sibility of reconstructing client data by a curious-but-honest server or clients who have access to thecorresponding gradient information.Specifically, two types of methods have beenproposed to extract private textual training data:",
  ": To reconstruct training data, prior attacks (a)typically require access to gradients from the wholemodel, while our attack (b) uses partial model gradients": "(i) gradient matching method (Zhu et al., 2019)align gradients from the presumed data with themonitored gradients; and (ii) analytical reconstruc-tion techniques (Dang et al., 2021; Gupta et al.,2022) deduce the used tokens by analyzing the gra-dient patterns, such as the presence of non-zeroembedding gradients that are correlated to thoseused tokens.This study examines whether models are morevulnerable than realized by framing a researchquestion:Can private training data be recon-structed using gradients from partial intermediateTransformer modules? This setting is motivated byseveral realistic scenarios. First, Parameter Freez-ing (Gupta et al., 2022) offers a straightforward de-fense against reconstruction attacks targeting spe-cific layers, e.g., the embedding. Second, layer-wise training strategies are adopted to meet thediverse needs of: (i) transfer learning for domainadaptation (Chen et al., 2020; Saha and Ahmad,2021), (ii) personalized federated learning for man-aging heterogeneous data (Mei et al., 2021) and (iii)enhancing communication efficiency (Lee et al.,2023). Generally, existing attacks require access togradients from either (i) all deep learning modulesor (ii) the word embedding/last linear layer; giventhe above defenses, this is not always practical.In this work, we challenge the premise that gra- dients from all layers are necessary to reconstructtraining data. We demonstrate the feasibility ofreconstructing text data from gradients of varyinggranularities, ranging from multiple Transformerlayers down to a single layer, or even its singlelinear component (e.g., individual Attention Query,Key modules), as depicted in . Addition-ally, we investigate the impact of differential pri-vacy on gradients (Abadi et al., 2016) as a defenseand find our attacks remain effective, without sig-nificant degradation of model performance. Ourstudy motivates further research into more effec-tive defense mechanisms for distributed learningapplications.",
  "Related Work": "DistributedLearning.Distributedlearning,such as Federated Learning (FL), is a growing fieldaimed at parallelizing model training for better ef-ficiency and privacy (Gade and Vaidya, 2018; Ver-braeken et al., 2020; Froelicher et al., 2021). FL ishighly valuable for privacy preservation by remain-ing sensitive data local as participants computegradients on their devices and sharing updates viaa central server (McMahan et al., 2017).However, the shared gradients introduce an at-tack surface that allows malicious participants, orthe curious-but-honest server, to reconstruct thetraining data. The gradient assets available to at-tackers may vary depending on the use of parameterfreezing defense (Gupta et al., 2022) or layer-wisetraining strategies (Lee et al., 2023). Gradient Inversion Attack (GIA).Recent stud-ies (Zhu et al., 2019; Zhao et al., 2020; Yanget al., 2023; Zhang et al., 2024) have investigateddata leakage in distributed learning, known as Gra-dient Inversion Attack (Zhang et al., 2023; Duet al., 2024). One strategy is the analytical-basedapproach, which identifies correlations betweenthe gradients and model parameters to retrieveused training tokens. RLG (Dang et al., 2021),FILM (Gupta et al., 2022), DECEPTICONS (Fowlet al., 2022) and DAGER (Petrov et al., 2024) havedemonstrated the use of gradients from specific lay-ers, such as last linear and embedding layers, forreconstruction. While these works present effectiveattacks, parameter freezing (Gupta et al., 2022) canaddress threats from specific layers, and the methodof DECEPTICONS assumes malicious parametermodification.Alternatively, optimization-based method iter- atively refines randomly generated data to matchreal data by minimizing the distance between theirgradients across layers.Pioneering works likeDLG (Zhu et al., 2019), InvertGrad (Geiping et al.,2020), and TAG (Deng et al., 2021) employed Eu-clidean (L2), Cosine, and combined Euclidean andManhattan (L1) distances for data reconstruction.LAMP (Balunovic et al., 2022) advances these at-tacks with selective initialization, embedding reg-ularization loss, and word reordering. Wu et al.(2023) and Li et al. (2023) build on the frame-works of DLG or LAMP while incorporating addi-tional information, such as auxiliary datasets. Ourstudy also adheres to optimization-based strategybut shows that partial gradients alone can revealprivate training data.Recent work has explored reconstructing pri-vate training data from single gradient modules,but with different focuses from ours. Wang et al.(2023) employs gradient decomposition in a syn-thetic two-layer network to recover training ex-amples, though applying this method to practicaldeep networks requires idealized conditions such astransparent parameter manipulation and controlledactivations (e.g., tanh, sigmoid). Li et al. (2023)extends this approach to recover hidden representa-tions from BERTs pooler layer, guiding optimiza-tion via the LAMP method that leverages all gra-dients. DAGER (Petrov et al., 2024) uses the firsttwo self-attention gradients of transformer-basedlanguage models for reconstruction, locating validtokens by determining whether their embeddingvectors lie within the subspace of the first layersgradient, then combining their embeddings to ver-ify their occurrence with the second layers gradientto establish exact sequences. Unlike these methods,our research focuses on identifying vulnerabilitiesacross all intermediate modules to gradient inver-sion attacks. Defense against GIA.To mitigate the risk ofGIA, two defense strategies have been explored:i) encryption-based methods, which disguise thereal gradients using techniques such as Homomor-phic Encryption (Zhang et al., 2020) and Multi-Party Computation (Mugunthan et al., 2019), andii) perturbation-based methods, including gradi-ent pruning (Zhu et al., 2019), adding noise throughdifferential privacy (Balunovic et al., 2021), orthrough learned perturbations (Sun et al., 2021;Fan et al., 2024). The former approach incurs addi-tional computational costs (Fan et al., 2024), while the latter may face challenges in achieving a bal-ance in the privacy-utility trade-off. In this work,we compare the vulnerabilities associated with at-tacking partial gradients versus the previously fullyexposed setting under the defense of differentialprivacy on gradients (Abadi et al., 2016).",
  "In this section, we first introduce the threat modeland then present the attack methodology, whichenables the use of intermediate gradients in Trans-former layers to reconstruct training data": "Threat Model.The attack operates in distributedtraining environments where the server distributesinitial model weights and clients submit gradientsderived from their local data. Potential attackers, ei-ther participating as clients or as curious-but-honestservers, can access shared parameters and inter-cept gradient communications. Unlike the assetsin prior studies that permitted access to full gra-dients, our research focuses on scenarios whereattackers only observe partial gradients. The goalof these attackers is to reconstruct the private textdata received by other clients or servers in training. Attack Strategy.Inspired by the gradient match-ing strategy, which involves minimizing the dis-tance between gradients generated from randomlysampled data and the true gradients to iterativelyalign the dummy input with the real one, we for-mulate our optimization objective as,",
  "M is a non-empty subset of the available mod-ules {q, k, v, o, f, p} within each layer": "We detail these notations and their involved num-ber of parameters in . The lowest ratio is0.54% for attacking a single linear module. Wand W represent the target gradient and the de-rived dummy gradient, respectively. D serves as adistance measurement in optimization. We extendthe use of cosine distance, following prior stud-ies (Balunovic et al., 2022; Geiping et al., 2020),",
  "Experimental Settings": "Datasets.We examine reconstruction attackson three classification datasets: CoLA (Warstadtet al., 2019), SST-2 (Socher et al., 2013), andRotten Tomatoes (Pang and Lee, 2005), follow-ing previous studies (Balunovic et al., 2022; Liet al., 2023) to evaluate reconstruction performance.We conduct experiments on 10 batches randomlysampled from each dataset separately, reportingaverage results across different test scenarios. Models.We conduct experiments on BERTBASE,BERTLARGE (Devlin et al., 2019), and TinyBERT(Jiao et al., 2020), following previous studies (Denget al., 2021; Balunovic et al., 2022; Li et al., 2023).We also adopted a BERTFT model, which involves fine-tuning BERTBASE for two epochs before at-tacks. For the word reordering step after recon-struction, we utilized a customized GPT-2 languagemodel trained by Guo et al. (2021) as an auxiliarytool, same as the setting used in LAMP. Evaluation Metrics.We evaluate the efficacyof our attack using ROUGE-1, ROUGE-2, andROUGE-L (ROUGE, 2004; Lhoest et al., 2021),corresponding to unigram, bigram, and longestcommon sub-sequence, respectively. The F-scoresof these metrics are reported. Attack Setup.We adapt the open-source imple-mentation of LAMP (Balunovic et al., 2022) toserve as both the basis framework and the baseline,as LAMP remains the state-of-the-art method fortext reconstruction. We employ identical hyper-parameters as LAMP for fair comparisons. Theengineering contribution of our work is that we im-plemented a gradient extraction process to obtainpartial gradients from modules at varying desiredgranularity. Defense Setup.We employ DP-SGD (Abadiet al., 2016; Yousefpour et al., 2021), adjustingthe noise multiplier while maintaining a clip-ping bound C as 1.0. To assess noise effects, wetrain a BERTBASE model on the SST-2 dataset for2 epochs, evaluating utility changes with the F1-score and MCC (Matthews, 1975; Chicco and Ju-rman, 2020). We set the delta to 2 105, andexplore noise multipliers from 0.01 to 0.5.2",
  "Results and Analysis": "Attack Results.We present the ROUGE-Lscores for experiments on various gradient gran-ularities, including results for Transformer layersin , Attention modules across all layers in, and FFN modules in . These testsused the BERTBASE model on the CoLA datasetwith a batch size of 1. Further results on ROUGE-1and ROUGE-2 scores for CoLA are shown in Fig-ure 5, SST-2 in , and Rotten Tomatoesin . More results on different models andlarger batch sizes (B = 2, 4) are provided in and in Appendix A, along with severalreconstruction examples presented in inAppendix B.By inspecting , we observe that usinggradients from Transformer layers WT achieves",
  ": Results across varying FFN Modules": "performance comparable to the baseline, whichuses all gradients. Additionally, merely using gra-dients from a single layer Wt,i still yields decentattack scores, with layers 6 to 9 achieving resultscomparable to the baseline while using only 6.47%of model parameters. These results demonstratethat each layer is vulnerable to the reconstructionattack, with the middle layers hold highest risk. presents the results of the attack fromindividual modules in the Attention Blocks acrossall layers. Most modules facilitate attack perfor-mance above 50%, while the Query and Key mod-ules achieve relatively higher attack performance.Similarly, middle layers achieve the best perfor-mance; surprisingly, Wk,4, Wk,8, Wq,5 andWo,6 achieve performance almost equivalent tothe baseline, while using only 0.54% of its param-",
  "eters. Similar results can be observed from FFNmodules, as demonstrated in": "Defense Results.We deploy differential privacyvia DP-SGD (Abadi et al., 2016) to counter datareconstruction attacks. In our SST-2 dataset ex-periments, we apply varying noise levels () from0.01 to 0.5 under privacy budgets () of 2 108 to 1.87. Noise levels above 0.5 were not exploreddue to a significant drop in the MCC metric from0.773 to 0, compromising model utility. The resultsare detailed in . With the increase in noise,we observe a considerable decline in model util-ity. This observation supports our conclusion thatwhile DP-SGD offers limited protection against theattack, it significantly impacts model utility, as theMCC and F1-Score metrics decreased substantiallywith a privacy budget of 1.87.",
  "Conclusion": "In this study, we investigate the feasibility of re-constructing training data using partial gradients ina Transformer model. Our extensive experimentsdemonstrate that all modules within a Transformerare vulnerable to such attacks, leading to a muchhigher degree of privacy risk than has previouslybeen shown. Our examination of differential pri-vacy as a defense also indicates that it is not suffi-cient to safeguard private text data from exposure,inspiring further efforts to mitigate these risks.",
  "We identify several opportunities for further im-provement": "We conduct our experiments in the context ofclassification tasks; however, we propose that thetask of language modeling could serve as an addi-tional viable application scenario. Our preliminaryvalidation revealed promising outcomes. However,we did not scale the test volume or involve themdue to limitations in computational resources. We evaluate the defense effectiveness of DP-SGD and find that it is not adequate to mitigate therisk without significant model utility degradation.However, we believe that other techniques, such asHomomorphic Encryption and privacy-preservedmulti-party communication, could potentially re-duce such a risk of privacy leakage.Nonethe-less, these techniques often impose a significantoverhead on system communication and substan-tial computational resources. Therefore, we advo-cate for further research into the defense strategiesthat can enhance the systems resilience more effi-ciently. In our experiments, we set the batch sizes to1, 2, 4 and the average sentence length is less than25 words, resulting in a total tokens involved be-ing smaller than in generic industrial settings. Weconsider exploring larger batch sizes and longer se-quences as future research directions, which werediscussed in the related work, DAGER (Petrovet al., 2024). While our experiments adhered to controlled set-tings similar to previous work such as LAMP, TAG,DLGaimed at identifying foundational vulnera-bilities in Transformer models, widely used in real-istic applicationswe also recognize the value of",
  "Ethics Statement": "In this study, we delve into the vulnerabilities ofevery module in Transformer-based models againstdata reconstruction attacks. Our investigation seeksto evaluate the resilience of cutting-edge Trans-former models when they are trained in a dis-tributed learning setting and face malicious recon-struction attacks. Our findings reveal that eachcomponent is susceptible to these attacks.Our research suggests that the risk of databreaches could be more significant than initially es-timated, emphasizing the vulnerability of the entireTransformer architecture. We believe it is crucialto disclose such risks to the public, encouragingthe research community to take these factors intoaccount when developing secure systems and ap-plications, and to promote further research intoeffective defense strategies.",
  "Acknowledgement": "We would like to express our appreciation to theanonymous reviewers for their valuable feedback.This research was undertaken with the assistanceof resources from the National Computational In-frastructure (NCI Australia), an NCRIS enabled ca-pability supported by the Australian Government.We also express our gratitude to SoC IncentiveFund, FSE strategic startup grant and HDR Re-search Project Funding for supporting both traveland research. Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-dan McMahan, Ilya Mironov, Kunal Talwar, andLi Zhang. 2016. Deep learning with differential pri-vacy. In Proceedings of the 2016 ACM SIGSAC con-ference on computer and communications security,pages 308318. Mislav Balunovic, Dimitar Dimitrov, Nikola Jovanovic,and Martin Vechev. 2022. Lamp: Extracting textfrom gradients with language model priors.Ad-vances in Neural Information Processing Systems,35:76417654.",
  "Davide Chicco and Giuseppe Jurman. 2020. The advan-tages of the matthews correlation coefficient (mcc)over f1 score and accuracy in binary classificationevaluation. BMC genomics, 21(1):113": "Trung Dang, Om Thakkar, Swaroop Ramaswamy, RajivMathews, Peter Chin, and Franoise Beaufays. 2021.Revealing and protecting labels in distributed training.Advances in Neural Information Processing Systems,34:17271738. Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, ChaoShang, Hang Liu, Sanguthevar Rajasekaran, and Cai-wen Ding. 2021. Tag: Gradient attack on transformer-based language models. In The 2021 Conference onEmpirical Methods in Natural Language Processing. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "Jiacheng Du, Jiahui Hu, Zhibo Wang, Peng Sun,Neil Zhenqiang Gong, and Kui Ren. 2024. Sok: Gra-dient leakage in federated learning. arXiv preprintarXiv:2404.05403": "Mingyuan Fan, Yang Liu, Cen Chen, Chengyu Wang,Minghui Qiu, and Wenmeng Zhou. 2024. Guardian:Guarding against gradient leakage with provable de-fense for federated learning. In Proceedings of the17th ACM International Conference on Web Searchand Data Mining, pages 190198. Liam H Fowl, Jonas Geiping, Steven Reich, Yuxin Wen,Wojciech Czaja, Micah Goldblum, and Tom Gold-stein. 2022. Decepticons: Corrupted transformersbreach privacy in federated learning for languagemodels. In The Eleventh International Conferenceon Learning Representations. David Froelicher, Juan R Troncoso-Pastoriza, ApostolosPyrgelis, Sinem Sav, Joao Sa Sousa, Jean-PhilippeBossuat, and Jean-Pierre Hubaux. 2021. Scalableprivacy-preserving distributed learning. Proceedingson Privacy Enhancing Technologies, 2021(2):323347.",
  "easy is it to break privacy in federated learning? Ad-vances in Neural Information Processing Systems,33:1693716947": "Chuan Guo, Alexandre Sablayrolles, Herv Jgou, andDouwe Kiela. 2021. Gradient-based adversarial at-tacks against text transformers. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 57475757. Samyak Gupta, Yangsibo Huang, Zexuan Zhong,Tianyu Gao, Kai Li, and Danqi Chen. 2022. Recov-ering private text in federated learning of languagemodels. Advances in Neural Information ProcessingSystems, 35:81308143. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, XiaoChen, Linlin Li, Fang Wang, and Qun Liu. 2020.Tinybert: Distilling bert for natural language under-standing. In Findings of the Association for Computa-tional Linguistics: EMNLP 2020, pages 41634174. Sunwoo Lee, Tuo Zhang, and A Salman Avestimehr.2023. Layer-wise adaptive model aggregation forscalable federated learning.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 37, pages 84918499. Quentin Lhoest, Albert Villanova del Moral, YacineJernite, Abhishek Thakur, Patrick von Platen, SurajPatil, Julien Chaumond, Mariama Drame, Julien Plu,Lewis Tunstall, Joe Davison, Mario ako, Gun-jan Chhablani, Bhavitvya Malik, Simon Brandeis,Teven Le Scao, Victor Sanh, Canwen Xu, NicolasPatry, Angelina McMillan-Major, Philipp Schmid,Sylvain Gugger, Clment Delangue, Tho Matus-sire, Lysandre Debut, Stas Bekman, Pierric Cis-tac, Thibault Goehringer, Victor Mustar, FranoisLagunas, Alexander Rush, and Thomas Wolf. 2021.Datasets: A community library for natural languageprocessing. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing: System Demonstrations, pages 175184, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Jianwei Li, Sheng Liu, and Qi Lei. 2023. Beyond gradi-ent and priors in privacy attacks: Leveraging poolerlayer inputs of language models in federated learning.In International Workshop on Federated Learning inthe Age of Foundation Models in Conjunction withNeurIPS 2023.",
  "H Brendan McMahan, Eider Moore, Daniel Ramage,and Blaise Agera y Arcas. 2016. Federated learn-ing of deep networks using model averaging. arXivpreprint arXiv:1602.05629, 2:2": "Yuan Mei, Binbin Guo, Danyang Xiao, and WeigangWu. 2021. Fedvf: Personalized federated learningbased on layer-wise parameter updates with vari-able frequency. In 2021 IEEE International Perfor-mance, Computing, and Communications Conference(IPCCC), pages 19. IEEE. Vaikkunth Mugunthan, Antigoni Polychroniadou, DavidByrd, and Tucker Hybinette Balch. 2019. Smpai: Se-cure multi-party computation for federated learning.In Proceedings of the NeurIPS 2019 Workshop onRobust AI in Financial Services, volume 21. MITPress Cambridge, MA, USA. Bo Pang and Lillian Lee. 2005. Seeing stars: exploitingclass relationships for sentiment categorization withrespect to rating scales. In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 115124.",
  "Sudipan Saha and Tahir Ahmad. 2021. Federated trans-fer learning: concept and applications. IntelligenzaArtificiale, 15(1):3544": "Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 conference on empiri-cal methods in natural language processing, pages16311642. Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang,Hai Li, and Yiran Chen. 2021. Soteria: Provabledefense against privacy leakage in federated learningfrom representation perspective. In Proceedings ofthe IEEE/CVF conference on computer vision andpattern recognition, pages 93119319.",
  "Alex Warstadt, Amanpreet Singh, and Samuel R Bow-man. 2019. Neural network acceptability judgments.Transactions of the Association for ComputationalLinguistics, 7:625641": "Ruihan Wu, Xiangyu Chen, Chuan Guo, and Kilian QWeinberger. 2023. Learning to invert: Simple adap-tive attacks for gradient inversion in federated learn-ing. In Uncertainty in Artificial Intelligence, pages22932303. PMLR. Haomiao Yang, Mengyu Ge, Dongyun Xue, KunlanXiang, Hongwei Li, and Rongxing Lu. 2023. Gradi-ent leakage attacks in federated learning: Researchfrontiers, taxonomy and future directions. IEEE Net-work. Ashkan Yousefpour, Igor Shilov, Alexandre Sablay-rolles, Davide Testuggine, Karthik Prasad, ManiMalek, John Nguyen, Sayan Ghosh, Akash Bharad-waj, Jessica Zhao, et al. 2021. Opacus: User-friendlydifferential privacy library in pytorch. In NeurIPS2021 Workshop Privacy in Machine Learning. Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang,Feng Yan, and Yang Liu. 2020. BatchCrypt: Efficienthomomorphic encryption for Cross-Silo federatedlearning. In 2020 USENIX Annual Technical Confer-ence (USENIX ATC 20), pages 493506. USENIXAssociation. Rui Zhang, Song Guo, Junxiao Wang, Xin Xie, andDacheng Tao. 2023. A survey on gradient inversion:Attacks, defenses and future directions. In Proceed-ings of the Thirty-First International Joint Confer-ence on Artificial Intelligence, pages 5678685. Zhuo Zhang, Jintao Huang, Xiangjing Hu, JingyuanZhang, Yating Zhang, Hui Wang, Yue Yu, QifanWang, Lizhen Qu, and Zenglin Xu. 2024. Revisitingdata reconstruction attacks on real-world dataset forfederated natural language understanding. In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 1408014091.",
  ": The comparison of reconstruction attacks using different gradient modules on CoLA dataset and BERTBASEmodel (B = 1)": "We observe three key points: (i) gradients from all Transformer layers perform comparably to thebaseline, which uses gradients from all deep learning modules, (ii) gradients from specific modules, suchas Attention Query, Attention Key, and FFN Output, can achieve equivalent or superior performance tothe baseline, and (iii) gradients from the middle layers outperform those from the shallow or final layers.",
  "BExamples of Reconstruction": "Some reconstructed examples are presented in . For each example, we provide results usingparameter gradients from different modules: the baseline LAMP method, which utilizes gradients fromall layers, and our method, which employs gradients specifically from the 9th Transformer layer, the 5thAttention Key module, and the 7th FFN Output module, respectively. We observe that the reconstructionperformance of using only one Transformer layer or even a single module could recover inputs withcoherent pieces. Their performance was comparable to the baseline method using full gradients."
}