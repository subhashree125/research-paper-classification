{
  "Abstract": "In the real world, documents are organized indifferent formats and varied modalities. Tradi-tional retrieval pipelines require tailored docu-ment parsing techniques and content extractionmodules to prepare input for indexing. This pro-cess is tedious, prone to errors, and has infor-mation loss. To this end, we propose DocumentScreenshot Embedding (DSE), a novel retrievalparadigm that regards document screenshotsas a unified input format, which does not re-quire any content extraction preprocess andpreserves all the information in a document(e.g., text, image and layout). DSE leveragesa large vision-language model to directly en-code document screenshots into dense represen-tations for retrieval. To evaluate our method,we first craft the dataset of Wiki-SS, a 1.3MWikipedia web page screenshots as the corpusto answer the questions from the Natural Ques-tions dataset. In such a text-intensive documentretrieval setting, DSE shows competitive effec-tiveness compared to other text retrieval meth-ods relying on parsing. For example, DSE out-performs BM25 by 17 points in top-1 retrievalaccuracy. Additionally, in a mixed-modalitytask of slide retrieval, DSE significantly out-performs OCR text retrieval methods by over15 points in nDCG@10. These experimentsshow that DSE is an effective document re-trieval paradigm for diverse types of documents.Model checkpoints, code, and Wiki-SS collec-tion are released at",
  "Introduction": "Information retrieval systems help users access ex-ternal information from documents in varied modal-ities, including text, images, charts, and tables. Asshown in (a), existing document retrievalparadigms typically process these modalities sep-arately. For example, traditional lexical retrieverBM25 (Robertson and Zaragoza, 2009) or neuralretrievers such as DPR (Karpukhin et al., 2020) relyon extracted text contents from documents. Recent multimodal retrieval (Yang et al., 2023; Wei et al.,2023) leverage both processed text and image unitsto broaden the scope of retrieval, thus supportingtext-image tasks.However, the existing retrieval paradigms lacka unified encoding process across modalities, lead-ing to two underlying issues. Firstly, preprocess-ing is not a trivial effort. Specialized processingis required to handle various document types andcontent modalities, and they are often imperfect.For instance, HTML files in the wild can presentsignificant complexity due to their varied struc-tures, making it difficult for a single tool to parseall information accurately. Similarly, slides andPDFs often require OCR models to extract text andhandle other content types like tables and figuresseparately (Huang et al., 2022; Tanaka et al., 2023).Managing these diverse modalities separately istedious, and precisely dealing with the long-taileddocument appearances in the real world is oftenimpractical. Secondly, this process breaks theoriginal appearance of the document, disruptingits visual context and layout integrity. The visualpresentation of a document can convey essentialinformation that is difficult to capture through con-tent extraction alone. For example, in addition tothe contents of texts and images, the size and posi-tion of these elements in a document may encodethe importance of the information they contain (Xuet al., 2020; Huang et al., 2022).To tackle the aforementioned issues, we intro-duce Document Screenshot Embedding (DSE), anew information retrieval paradigm that unifies thevaried formats and modalities in a single form fordirect document encoding and indexing: screen-shots. In contrast to using various tools to extracttexts and images from documents in different for-mats, screenshots are easy to obtain and all the in-formation in the documents are visually preserved.As illustrated in (b), DSE directly en-codes the screenshot of any given document into HTML parser, OCR, etc. (a) (b) : Comparison between (a) existing document retrieval paradigm and (b) our proposed paradigm. DSEbypasses the document parsing and content extraction process, directly encoding the original appearance ofdocuments with multimodal contents into a dense representation for indexing a dense representation through a large vision-language model. During search, a users queryis encoded by a language model to locate the near-est document embeddings. We conduct empiricalstudies to demonstrate that DSE is effective fordocument retrieval. Specifically, we conduct exper-iments on two types of document retrieval settings:text-intensive and text-image-mixed. For the text-intensive case, we collect 1.3 million Wikipediaweb page screenshots as our corpus and fine-tunea large vision-language model as a bi-encoder toconduct dense retrieval on questions in the NQdataset (Kwiatkowski et al., 2019). Experimentalresults show that DSE outperforms the traditionaltext-based retrieval method BM25 by 17 points intop-1 retrieval accuracy on NQ questions and iscompetitive with text-based dense retrieval meth-ods in a text-oriented evaluation. This experimentindicates that DSE can sufficiently encode the tex-tual information in a screenshot. As for the image-text mixed setting, we use slide retrieval. We turnthe existing SlideVQA (Tanaka et al., 2023) datasetinto an open-domain retrieval setting, where mod-els are required to retrieve relevant slides from apool of 50k slides for given questions. Results showthat DSE outperforms all text-based retrieval meth-ods which rely on OCR (including BM25 and densetext retrieval) by over 15 points in nDCG@10.",
  "Neural Document Retrieval": "Traditional document retrieval methods such as TF-IDF and BM25 (Robertson and Zaragoza, 2009)represent text as bag-of-words representations andconduct efficient search over an inverted index.Recent neural retrieval methods represented byDPR (Karpukhin et al., 2020), proposed to finetunepretrained neural networks such as BERT (Devlinet al., 2019) to encode query and document sepa- rately into dense semantic vectors in a bi-encoderarchitecture. The effectiveness of text dense re-triever has been boosted in recent years by var-ious training strategies such as data augmenta-tion (Xiong et al., 2021; Lin et al., 2023; Xiao et al.,2023), pretraining (Izacard et al., 2021; Gao andCallan, 2022; Wang et al., 2023), distillation (Linet al., 2021; Ren et al., 2021) and instruction tun-ing (Su et al., 2023; Asai et al., 2023). With thegrowth of large language models, finetuning anLLM-based text encoder demonstrated further im-provement in both in-domain and out-domain re-trieval effectiveness (Ma et al., 2024; Wang et al.,2024; Muennighoff et al., 2024; Lee et al., 2024).Besides text retrieval, prior multi-modal retrievalstudies (Wei et al., 2023; Koukounas et al., 2024)have explored retrieval across various combinationsof text and image inputs for queries and documents.These approaches aim to bridge the gap betweendifferent modalities, enabling more comprehensiveretrieval systems. Existing text and multi-modal re-trieval works assume that the datasets are well pre-processed, where text and image data are carefullyextracted and organized for model inputs. However,this is not always true in real-world scenarios wheredocuments are often unstructured and diverse. Inthis work, we consider the document retrieval tasksthat begin with the original look of documents.",
  "Large Vision-Language Model": "Large language models (LLMs) like GPT-4 (Ope-nAI, 2024) and LLaMA (Touvron et al., 2023),pre-trained on massive corpora and fine-tuned tofollow user instructions, have shown success in var-ious natural language generation tasks (Wei et al.,2022). Recent advancements have integrated vi-sion capabilities into LLMs, enabling them to pro-cess both text and images simultaneously. Com-mercial models like GPT-4V (OpenAI, 2024) andopen-source models such as LLaVA (Liu et al., 2023) exhibit strong performance. Building uponLLaVA, recent works such as LLaVA-NEXT (Liuet al., 2024a), Idefics2 (Laurenon et al., 2024),and Phi-3-vision (Abdin et al., 2024) have furtherimproved performance. They enable the processingof higher-resolution images and handle more chal-lenging vision-language tasks, such as OCR (Liuet al., 2024a,b). Inspired by the capabilities oflarge vision-language models, our work pioneersits application in document retrieval tasks.",
  "Document Retrieval Datasets": "Commonly used text retrieval datasets such asMS MARCO (Bajaj et al., 2018), Wikipedia-NQ (Karpukhin et al., 2020), and BEIR (Thakuret al., 2021) are released in well-preprocessed textcontents. Similarly, multi-modal retrieval datasetslike AToMIC (Yang et al., 2023) and m-BEIR (Weiet al., 2023) have text and images extracted fromtheir sources and separately stored.On the other hand, existing datasets designedfor question-answering tasks based on documentimages include DocVQA (Mathew et al., 2021),VisualMRC (Tanaka et al., 2021), WebSRC (Chenet al., 2021), and InfographicVQA (Mathew et al.,2022). These datasets contain document imagespaired with questions, focusing on reading compre-hension evaluation where a ground truth documentimage is provided for each question. Besides, theimage pools in these datasets are relatively small,comprising only a few thousand images.Therefore, to fairly evaluate multi-modal doc-ument retrieval in a large scale, we craft a text-intensive image corpus called Wiki-SS, containing1.3 million Wikipedia page screenshots. Addition-ally, we convert SlideVQA (Tanaka et al., 2023)dataset, a visual QA dataset, into an open-domainslide retrieval dataset, consisting of 50K slides.",
  "Task Definition": "Given a query Q and a corpus C consisting of doc-uments {D1, D2, ..., Dn}, the task of documentretrieval is to identify the k documents that aremost relevant to the query Q, with k n. Thisrelevance is determined using a similarity metricSim(Q, D) R. Note that in this work, the screen-shotted document is a complete information snip-pet (e.g. a web article, a PDF page). This is differ-ent from some of the previous retrieval work, wherethe term document denotes arbitrary information",
  "Document Screenshot Embedding": "We adopt a bi-encoder architecture for dense re-trieval, where a document screenshot and user textquery are encoded into dense vectors using a vi-sion and text encoder, respectively. We can naivelyapply the vision and text encoders from CLIP (Rad-ford et al., 2021) to our task; however, in our exper-iment, we observe that the vision encoder cannotencode screenshots with more fine-grained informa-tion; thus, we propose to use large vision languagemodels as the document screenshot encoder. Visual EncoderWhen a document screenshotD is provided, it is first processed by a vision en-coder Ev to generate a sequence of latent represen-tations. The length of the sequence is determinedby the image tokenizer of the vision encoder. Wetakeclip-vit-large-patch14-3361 as an ex-ample. Any given screenshot is first converted toan image with 336 336 pixels and then dividedinto 24 24 patches (i.e., 576 patches in total),each of which consists of 14 14 pixels. Eachpatch is flattened and mapped to a patch embed-ding with a trainable linear projection. The patchembeddings are encoded into latent representationswith a vision encoder. However, if a screenshotcontains many texts (e.g., Wikipedia webpage), the576 patch latent embeddings may not capture thefine-grained textual information in the screenshot. Vision Language ModelTo address the aboveissue, we leverage a large vision language model,Phi-3-vision,2 which uses the same image tokenizerfrom clip-vit-large-patch14-336 but can rep-resent an image with more patches by cropping itinto sub-images. For example, given a screenshot,we can choose to divide it into (Cx24)(Cy24)patches. The given screenshot is converted to animage with (Cx 336) (Cy 336) pixels andcropped into Cx Cy sub-images, each of whichhas 336 336 pixels. Similarly, each sub-image isencoded into 576 patch latent representations inde-pendently. Note that Phi-3-vision further convertsthe whole screenshot into 336 336 pixels andencodes them into an additional 576 patch latent",
  "El": ": Overview of DSE encoder architecture. DSE adopts a bi-encoder architecture, where the document towerencodes the document screenshot into dense vector by taking vision input and the query tower encodes the query bytaking text input. Document and query encoders share the same language model. representations to capture the global information,resulting in (Cx Cy + 1) 576 patch latent rep-resentations in total, as depicted in left side of Fig-ure 2. Also, every four patch latent representationsare concatenated and projected into one embeddingfor language model inputs. This process yields(Cx Cy + 1) 576 4 patch latent embeddings asthe input for the language model El. In .3, we will show that encoding a screenshot intomore patch latent embeddings (increasing Cx andCy) helps capture more fine-grained information inthe screenshot but sacrifices screenshot documentencoding efficiency.The encoded patch latent embeddings are con-catenated with a text prompt as the input to thesubsequent language model: <s><img> What isshown in this image?</s>. Here, the <img> tokenis a special placeholder token and is replaced bythe sequence of patch latent embeddings from thevision encoder. To aggregate sequence informationusing a language model with uni-directional atten-tion, following previous work in text retriever (Maet al., 2024), we use the embedding of the end-of-sequence token </s> from the last hidden state asthe document screenshot embedding:",
  "Di{D+}DNexp(Sim(Q, Di)/),": "where D+ denotes the positive document. DN rep-resents a set of negative documents that are irrele-vant to the query Q, including hard negatives andin-batch negatives. is a temperature parameter setto 0.02 in our experiments. Note that we only con-sider text queries, which are directly input to thelanguage model using template f<s>{query}</s>and the last hidden state of </s> is used as thequery embedding, Vq = El(Q).",
  "Web-Page Retrieval": "DatasetWe construct the Wiki-SS dataset, us-ing the Selenium Python toolkit3 to access EnglishWikipedia pages through URLs and automaticallytake screenshots. The screenshots are taken witha window size of 980 980 pixels to ensure ade-quate coverage of the core content. The screenshotcreation process is conducted over a span of fourdays, from May 20 to May 23, 2024. Note thatstoring the entire collection of Wikipedia screen-shots would require over 2TB of storage in PNGformat. In order to make Wiki-SS more manage-able for research purposes, we downsize the corpus by filtering out the web pages which are consid-ered easy negative samples for all the questionsin the train, dev and test sets from Natural Ques-tions (Kwiatkowski et al., 2019). Specifically, weperform BM25 search for each question to retrievethe top 50 documents over the text corpus. The re-trieved documents are pooled together as our finalcorpus. Note that we concatenate each questionand its corresponding ground truth answers as aquery for BM25 search. Although BM25 is a rela-tively weak retriever, including the target answer inthe query for lexical search ensuring that positiveand hard negative documents for each question areincluded in the downsized corpus. As a result, weobtain a collection of 1,267,874 Wikipedia screen-shots for our experiments.To compare with text-based retrieval baselines,we create a text version Wikipedia collection whichmirrors the collection of Wiki-SS. Given the signif-icant updates and changes to Wikipedia pages overtime, the existing Wikipedia dumps (Karpukhinet al., 2020; Izacard et al., 2024) cannot beused as a fair comparison. Thus, we re-processthe Wikipedia text contents based on the May20, 2024 dump4 using Wikipedia parsing toolmwparserfromhell. For each document in the textcorpus, we use the first 500 words of each docu-ment, mirroring the corpus in Wiki-SS, where eachscreenshot covers only the first-page content. Formore details, please see Appendix A.1. Training DataWe create the training data by tak-ing the questions in the NQ train split as queriesand using BM25 to retrieve the top-50 relevant doc-uments over the text corpus for each question. Adocument candidate (either in screenshot or text)is considered positive when the corresponding textcontains the answers for the question. Otherwise,the document is considered a hard negative candi-date. We drop the training example if either thepositive or negative candidate list is empty, result-ing in 49,095 training examples of triplets of query,positive documents and hard negative documents. EvaluationWe evaluate the in-domain effec-tiveness of retrievers using the 3,610 NQ testset questions.Consistent with previous prac-tices in evaluating retrieval effectiveness on QAdatasets (Karpukhin et al., 2020), we use top-kretrieval accuracy as the metric. A question is con-sidered correctly answered if one of the candidate documents contains an exact match of the answerstring in the corresponding text content. The orig-inal NQ datasets contain both short answers andlong answers for the questions (Kwiatkowski et al.,2019), we follow the same method for computingexact match accuracy as Karpukhin et al. (2020),where the short answers are the target.",
  "DatasetThe original SlideVQA (Tanaka et al.,": "2023) data is designed for document visual ques-tion answering. It contains 14.5k QA pairs and 52kslide images in total. The images contain varioustext formats, layouts, and visual content such asplots and charts. Given a question, the original taskis to select the most relevant slides among the samedeck with up to 20 slides and then answer the ques-tion based on the selected slides. The documentselection process is in the form of reranking andclassification. In order to support the evaluation ofdocument retrieval, we modify the SlideVQA to anopen-domain retrieval task, where the task is to re-trieve k most relevant slide from the entire pool ofslide images. After our processing (e.g. removingthe slides that fail to download, and questions thatdo not have evidence slides available), SlideVQA-open contains 50,714 slide images (screenshots)in its corpus. We also create a corresponding text-based corpus for comparison with text retrievers us-ing pytesseract OCR toolkit to extract text fromevery slide deck. Training DataWe create the training data basedon the original train split of SlideVQA, the an-notated evidence slides for a given question areconsidered positive documents, and the other slideswithin the same deck are considered as hard nega-tive documents. This process leads to 10,290 train-ing examples in total. EvaluationWe construct the SlideVQA-openevaluation set using the 2,136 questions in the testset of SlideVQA. We evaluate the models retrievaleffectiveness using nDCG@10 and Recall@10. Inthe following sections, mentions of SlideVQA referto the open-domain retrieval setup.",
  ": Supervised retrieval effectiveness comparison. DSE and CLIP directly encode document screenshots whilethe other text-based retrieval models encode the extracted text from documents": "recognized for its effective and efficient trade-offin performance. To train the model, we employmemory-efficient techniques such as LoRA (Huet al., 2022), FlashAttention (Dao, 2024), and Deep-Speed (Rasley et al., 2020). The model is trainedwith a batch size of 128 for one epoch on Wikipediawebpage retrieval and trained with a batch size of64 for two epochs for slide retrieval. The modelweights are shared between the language modelsfor document screenshot and query encoding. Inboth tasks, each training query is paired with onepositive document and one hard negative document.We set (Cx, Cy) = (4, 4) by default; that is, thedocument screenshots are resized to 1344 1344pixels and cropped into 4 4 sub-images. Thetraining process is conducted on two A100 80GBGPUs. During inference, the embeddings are in-dexed using a Flat Faiss index (Douze et al., 2024)for exact nearest neighbor search.",
  "Baselines": "We compare DSE against the following documentretrieval methods based on text input: (1) BM25:a traditional text retriever based on lexical repre-sentation. (2) DPR: we follow the same setting asthe DPR work (Karpukhin et al., 2020), initializ-ing dense retriever with BERT-base, and finetun-ing the model on our training data based on textinput. (3) E5: similar to DPR, we finetune theunsupervised E5-base model (Wang et al., 2022),which has BERT further pretrained with contrastivelearning based on web data. (4) Phi-3: we use thesame model initialization and configuration as DSEbut only fine-tune the component of the languagemodel as a text-based dense retriever. Additionally,we compare the fine-tuned CLIP model, whose im-age encoder is also initialized by ViT-large (thesame as DSE) but only supports a fixed length ofpatch sequence; i.e., (Cx, Cy) = (1, 1). Please see",
  "Supervised Retrieval Effectiveness": "presents the models retrieval effective-ness in the supervised setting, where models arefine-tuned on NQ or SlideVQA training queriesand evaluated on the corresponding evaluation set.For the Wikipedia webpage retrieval task, DSEdemonstrates significant improvements over the tra-ditional text-based retrieval method BM25. Specifi-cally, DSE achieves 46.2% and 77.6% in top-1 andtop-20 retrieval accuracy, which are 17 points and10 points higher than BM25, respectively. This indi-cates that DSE can effectively encode text-intensivedocuments in the format of screenshots for retrieval.When compared with neural text retrieval meth-ods, DSE outperforms smaller model DPR andperforms on par with E5. Phi-3, which uses thesame language model as DSE (with 4 billion pa-rameters), achieves approximately 4 points highertop-1 retrieval accuracy than DSE. This suggeststhat existing vision language models still cannotfully capture the text content in a screenshot.In the slide retrieval task, where the documentsinclude a mix of text and visual content, we ob-serve DSE significantly outperforms (i.e., over 15points in both nDCG@10 and Recall@10) all thetext retrieval baselines that rely on OCR contentextraction. This highlights the risk of informationloss in the content extraction step, where OCR isonly able to extract text content, thereby losing thevisual elements of the documents. Notably, DPR, aneural retrieval method, fails to outperform BM25in this task. This may be due to the varied layoutsof slides, which pose additional challenges for textcontent extraction and result in noisy text input",
  ": Zero-shot retrieval effectiveness comparison.Models are trained on Wiki-SS with NQ questions andevaluated on TriviaQA questions and slide retrieval task": "for text neural retrieval fine-tuning. By contrast,DSE bypasses the stage of text content extractionand directly encodes document screenshots, whichpreserves more information for retrieval.Finally, DSE outperforms CLIP even thoughthey use the same backbone of the vision trans-former to digest the document screenshots. ForNQ, DSE surpasses CLIP by 11.1 points in top-1accuracy, and for SlideVQA, DSE achieves 12.6points higher in nDCG@10. We contribute the ef-fectiveness gain to the large vision-language modelencoder, which as we will show in .3,has the capacity to handle more fine-grained in-formation in a screenshot and possibly enhancedsemantic understanding.To further explore the integration of text and vi-sual information, we examined the hybrid retrievalresults combining text-based and screenshot-basedmethods, as shown in Appendix A.2. The results in-dicate that combining CLIP with text-based modelsyields notable performance improvements in theSlideVQA task. However, DSE still outperformssuch case in mixed modality scenario, demonstrat-ing its capability to encode both fine-grained vi-sual details and textual content directly in a singlepipeline. As the hybrid approach is not a single,unified pipeline that directly encodes the documentinput, we leave the hybrid results in Appendix.",
  "Zero-Shot Retrieval Effectiveness": "In this section, we further evaluate the generaliza-tion capability of DSE. Specifically, we apply themodels fine-tuned on NQ questions to retrieve an-swers for TriviaQA questions (Joshi et al., 2017)over the Wiki-SS (or the corresponding Wiki text)corpus, assessing their ability to generalize acrossdifferent query distributions. Additionally, we eval-uate the NQ fine-tuned models on the SlideVQAdataset to examine cross-task generalization. (1, 1) (2, 2) (3, 3) (4, 4) : A snapshot of a Wikipedia webpage dividedby different numbers of patches (red small squares). Asthe number of patches increases, each patch can capturemore fine-grained text information in the screenshot.(Cx, Cy) means the image are divided into Cx Cysub-images; then converted into (Cx 24) (Cy 24)patches. See more detail in .2 and . As shown in , on TriviaQA, the text re-triever based on LLM (i.e., Phi-3) achieves thebest zero-shot effectiveness with a top-1 retrievalaccuracy of 57.1%. Both DPR and CLIP showlower zero-shot effectiveness, being outperformedby BM25 by approximately 10 points. In contrast,DSE achieves a top-1 retrieval accuracy of 50.3%,which is 3 points higher than BM25. This indicatesthat DSE has relatively good zero-shot effective-ness across different query distributions but withroom for improvement.On the slide retrieval task, we observe that DSEshows the best effectiveness among all. Specif-ically, DSE outperforms BM25 by 8 points interms of nDCG@10, while all the other text-basedmethods underperform BM25. This result showsthat even though DSE is only fine-tuned on theWikipedia webpage retrieval task, where text is themain content, it is still able to encode documentinformation beyond text. This demonstrates the po-tential of DSE in handling diverse document typesand tasks without needing task-specific training.",
  "Impacts of Patch Sequence Length": "As discussed in .2, each screenshot iscropped into Cx Cy sub-images and encodedas a sequence of patches. Thus, increasing thenumber of crops yields a more lengthy patch inputsequence, which incurs more computation cost fordocument encoding. On the other hand, increasingthe number of crops results in patches with morefine-grained visual information, as illustrated in. In the setting of (Cx, Cy) = (1, 1), eachpatch contains multiple words, while in the settingof (Cx, Cy) = (4, 4), a single letter is covered bytwo patches. This leads to a trade-off between theefficiency and quality of document encoding. Westudy this trade-off by training DSE with different",
  "(Cx, Cy)Number of Sub-Images": ": Trade-off between effectiveness and efficiencyof DSE with varying numbers of crops for input images.The inference speed is measured on a single H100 GPUwith BF16 precision and FlashAttention enabled. numbers of crops and evaluate the corresponding re-trieval effectiveness and document encoding speed(Doc/sec) on the Wiki-SS task for NQ questions.We plot the efficiency and effectiveness in Fig-ure 4. When cropping the image into 4 4 sub-images for more fine-grained patch encoding, thetop-10 retrieval accuracy increases from 62.0% to73.7%, indicating that finer granularity helps themodel better understand and encode the documentscreenshot. However, this comes at the cost of com-putational efficiency. As the number of sub-imagesincreases, the sequence length of the models in-put grows, resulting in longer encoding times. Thedocument encoding speed decreased from 12.2 doc-uments per second with 1 1 sub-images to 4.3documents per second with 4 4 sub-images asinput. Finally, the experiment suggests that using(Cx, Cy) = (2, 2) or (3, 3) offers a good trade-offbetween retrieval effectiveness and computationalefficiency of document encoding.",
  "Case Study": "We conducted a case study to illustrate whether thefine-tuned embeddings effectively utilize the coresemantic information in the screenshots. presents the attention visualization of two examplesfrom Wiki-SS and SlideVQA. We used the Phi-3-vision model fine-tuned on NQ as the backboneand extracted the multi-head attention of the lasttoken embedding to the image patches at the finallayer. The image patches contain both global andlocal features: Global features are tokenized fromthe resized full image input (336 336), while lo-cal features are derived from crops when the imageis resized to 1344 1344 and then cropped into4 4 sub-images before encoding. For both exam- : Case study on two examples in Wikipedia andSlideQA. We visualize the multi-head attention fromthe fine-tuned embedding to the image patches at thelast layer. GLOBAL-HEAD is the attention head to thecoarse image features (336336), while the LOCAL-HEAD is the attention head to more fine-grained imagefeatures after cropping (16336336). ples, the global attention heads appear to focus ongeneral information, such as images, logos, titles,and sections. In contrast, the local attention headsconcentrate on finer details in the screenshots, suchas individual letters and keywords, which are cru-cial for retrieval. This qualitative evidence suggeststhat DSE can effectively capture information fromvarious modalities within the screenshots.",
  "Importance of Visual Integration": "In the mixed-modality retrieval task, both contentextraction errors and the lack of visual context areinherent challenges in OCR-based methods, whichour proposed DSE method can overcome. We con-ducted an error analysis of failure cases from thePhi-3 text retriever in the SlideVQA task, whereDSE retrieved relevant documents within the top10 results, but Phi-3 did not. We categorized theerrors into two groups: (1) documents that couldbe answered using text alone, suggesting OCR er-rors, and (2) documents requiring additional visualcontext, indicating that missing the visual elementsled to retrieval failures. In our manual review of 50cases, 22 could be resolved with correct text extrac-tion, while 28 required visual context. This analy-sis supports our claim that traditional OCR-basedmethods suffer from content extraction errors andloss of visual integration, while DSE successfullyaddresses these issues by integrating all modalities.",
  "Answer": ": Examples of Top-1 retrieval results from DSE for NQ test set questions that are being consideredirrelevant because an exact match for the answer was not found in the corresponding extracted text body. However,the exact answer can be found in the tables covered by the screenshots. answer string in the retrieved documents. However,such evaluation only calculates the exact answermatches within the main text body. This could re-sult in an underestimation of DSEs effectivenessif the answer appears in the content beyond themain text body, such as images, captions, or tables.To investigate this potential underestimation, werandomly select 50 questions from the test set ofNQ where DSEs top-1 retrieved documents arejudged irrelevant while the purely text-based Phi-3counter-part deems them positive. We manuallyexamine the corresponding screenshots retrievedby DSE and discover that 7 out of 50 samples areactually false negatives. In other words, the ex-act answer in these cases could be found in theimage captions or tables within the screenshots asillustrated in . This indicates DSEs capa-bility to capture information in other areas besidesthe main texts that contain important clues for thedocument representation.",
  "Conclusion": "In this paper, we introduce DSE, a novel informa-tion retrieval paradigm that leverages screenshots tosimplify the document retrieval process. By circum-venting traditional preprocessing steps and directlyencoding documents with a vision-language model,DSE offers a unified approach to handling varieddocument modalities. We empirically show that DSE outperforms traditional retriever and OCR-based methods on varied document retrieval tasks,such as webpage and slide retrieval. This highlightsthe potential of DSE to improve document retrievalin a range of real-world applications.By integrating DSE with a large vision-languagemodel (VLM) generator, it leads to a promisingvisual-based retrieval augmented generation (V-RAG) paradigm. In this paradigm, DSE retrievesdocument screenshots, which the VLM generatorcan process directly for generation, without need-ing separate text or image extraction. This createsan end-to-end document intelligence system thateliminates the need for content extraction. We hopeour work opens future research into improving V-RAG with better retrieval methods, enabling moreefficient and seamless multi-modal information re-trieval and generation.",
  "Limitations": "This work has several limitations that warrant fur-ther exploration. Firstly, while we evaluated DSEon Wikipedia webpage retrieval and slide retrievaldatasets, there remains a gap in its effectivenessfor more general-purpose document retrieval tasks,such as those involving PDFs or web pages withhighly varied structures and content. Future workcan consider multi-task training across diverse doc-ument types and content. Additionally, combin- ing our method with extracted text and image con-tents could make DSE more versatile for generalretrieval tasks. Secondly, our current approach re-lies solely on supervised fine-tuning. However,research in text retrieval has shown that contrastivepretraining can significantly improve retriever ef-fectiveness. Investigating whether such pretrain-ing methods can enhance DSEs performance is apromising direction for future research. Thirdly,the reliance on visual data introduces challengesin environments where such data is of low quality.Blurry or low-resolution screenshots may degradethe effectiveness of DSE. Conversely, processingvery high-resolution images can reduce computa-tional efficiency. We leave further explore the bal-ance of image quality and computational efficiencyas future work.",
  "This work complies with the ACL Ethics Policy.We declare that there are no ethical issues in thispaper, to the best of our knowledge": "We sincerely thank Xilun Chen, Xinyu Shi, XinyuZhang, Dawei Zhu, and the anonymous reviewersfor their invaluable feedback and insightful sugges-tions. We also extend our appreciation to Jheng-Hong Yang, Dongfu Jiang, and Yobo Wang fortheir helpful discussions on technical questions.This research was supported in part by the Nat-ural Sciences and Engineering Research Council(NSERC) of Canada and Microsoft via the Accel-erating Foundation Models Research program. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-min Bao, Harkirat Behl, Alon Benhaim, MishaBilenko, Johan Bjorck, Sbastien Bubeck, Qin Cai,Martin Cai, Caio Csar Teodoro Mendes, WeizhuChen, Vishrav Chaudhary, Dong Chen, DongdongChen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,Matthew Dixon, Ronen Eldan, Victor Fragoso, DanIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,Abhishek Goswami, Suriya Gunasekar, EmmanHaider, Junheng Hao, Russell J. Hewett, JamieHuynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Ma-houd Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-den, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,Arindam Mitra, Hardik Modi, Anh Nguyen, BrandonNorick, Barun Patra, Daniel Perez-Becker, ThomasPortet, Reid Pryzant, Heyang Qin, Marko Radmi-lac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,Olli Saarikivi, Amin Saied, Adil Salim, Michael San-tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,Swadheen Shukla, Xia Song, Masahiro Tanaka, An-drea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,Yu Wang, Rachel Ward, Guanhua Wang, PhilippWitte, Haiping Wu, Michael Wyatt, Bin Xiao, CanXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-wen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technicalreport: A highly capable language model locally onyour phone. arXiv:2404.14219. Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen,Gautier Izacard, Sebastian Riedel, Hannaneh Ha-jishirzi, and Wen-tau Yih. 2023. Task-aware retrievalwith instructions. In Findings of the Association forComputational Linguistics: ACL 2023, pages 36503675, Toronto, Canada. Association for Computa-tional Linguistics. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-drew McNamara, Bhaskar Mitra, Tri Nguyen, MirRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,and Tong Wang. 2018.MS MARCO: A humangenerated machine reading comprehension dataset.arXiv:1611.09268. Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, DanyangZhang, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021.WebSRC: A dataset for web-based structural readingcomprehension. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 41734185, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "Matthijs Douze, Alexandr Guzhva, Chengqi Deng, JeffJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazar,Maria Lomeli, Lucas Hosseini, and Herv Jgou.2024. The Faiss library. arXiv:2401.08281": "Luyu Gao and Jamie Callan. 2022. Unsupervised cor-pus aware language model pre-training for dense pas-sage retrieval. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 28432853,Dublin, Ireland. Association for Computational Lin-guistics. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.2023. Tevatron: An efficient and flexible toolkit forneural retrieval. In Proceedings of the 46th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 23,page 31203124, New York, NY, USA. Associationfor Computing Machinery. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-rank adaptation oflarge language models. In International Conferenceon Learning Representations. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, andFuru Wei. 2022. LayoutLMv3: Pre-training for doc-ument AI with unified text and image masking. InProceedings of the 30th ACM International Confer-ence on Multimedia, MM 22, page 40834091, NewYork, NY, USA. Association for Computing Machin-ery. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021.Unsupervised denseinformation retrieval with contrastive learning.arXiv:2112.09118. Gautier Izacard, Patrick Lewis, Maria Lomeli, LucasHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and EdouardGrave. 2024. Atlas: few-shot learning with retrievalaugmented language models. J. Mach. Learn. Res.,24(1). Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer. 2017. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611, Vancouver,Canada. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781,Online. Association for Computational Linguistics. Andreas Koukounas, Georgios Mastrapas, Michael Gn-ther, Bo Wang, Scott Martens, Isabelle Mohr, SabaSturua, Mohammad Kalim Akram, Joan FontanalsMartnez, Saahil Ognawala, Susana Guzman, Maxi-milian Werk, Nan Wang, and Han Xiao. 2024. JinaCLIP: Your CLIP model is also your text retriever.arXiv:2405.20204. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral Questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466.",
  "Hugo Laurenon, Lo Tronchon, Matthieu Cord, andVictor Sanh. 2024.What matters when buildingvision-language models? arXiv:2405.02246": "Chankyu Lee, Rajarshi Roy, Mengyao Xu, JonathanRaiman, Mohammad Shoeybi, Bryan Catanzaro, andWei Ping. 2024. NV-Embed: Improved techniquesfor training llms as generalist embedding models.arXiv:2405.17428. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and XilunChen. 2023. How to train your dragon: Diverse aug-mentation towards generalizable dense retrieval. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 63856400, Singapore.Association for Computational Linguistics. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021. In-batch negatives for knowledge distillationwith tightly-coupled teachers for dense retrieval. InProceedings of the 6th Workshop on RepresentationLearning for NLP (RepL4NLP-2021), pages 163173,Online. Association for Computational Linguistics.",
  "Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li,Xucheng Yin, Cheng lin Liu, Lianwen Jin, and XiangBai. 2024b. On the hidden mystery of OCR in largemultimodal models. arXiv:2305.07895": "Xueguang Ma, Kai Sun, Ronak Pradeep, Minghan Li,and Jimmy Lin. 2022. Another look at DPR: Repro-duction of training and replication of retrieval. InAdvances in Information Retrieval, pages 613626,Cham. Springer International Publishing. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, andJimmy Lin. 2024. Fine-tuning LLaMA for multi-stage text retrieval. In Proceedings of the 47th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 24,page 24212425, New York, NY, USA. Associationfor Computing Machinery. Minesh Mathew, Viraj Bagal, Rubn Tito, DimosthenisKaratzas, Ernest Valveny, and C.V. Jawahar. 2022.InfographicVQA. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vi-sion (WACV), pages 16971706.",
  "OpenAI.2024.GPT-4technicalreport.arXiv:2303.08774": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. arXiv:2103.00020. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,and Yuxiong He. 2020. DeepSpeed: System opti-mizations enable training deep learning models withover 100 billion parameters. In Proceedings of the26th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, KDD 20,page 35053506, New York, NY, USA. Associationfor Computing Machinery. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-RongWen. 2021. RocketQAv2: A joint training methodfor dense passage retrieval and passage re-ranking.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages28252835, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics.",
  "Stephen E. Robertson and Hugo Zaragoza. 2009. Theprobabilistic relevance framework: BM25 and be-yond. Found. Trends Inf. Retr., 3:333389": "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.Smith, Luke Zettlemoyer, and Tao Yu. 2023. Oneembedder, any task: Instruction-finetuned text em-beddings. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 11021121,Toronto, Canada. Association for Computational Lin-guistics. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, TakuHasegawa, Itsumi Saito, and Kuniko Saito. 2023.SlideVQA: A dataset for document visual ques-tion answering on multiple images.Proceedingsof the AAAI Conference on Artificial Intelligence,37(11):1363613645.",
  "VisualMRC: Machine reading comprehension on doc-ument images. Proceedings of the AAAI Conferenceon Artificial Intelligence, 35(15):1387813888": "Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In Thirty-fifth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track (Round 2). Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023.LLaMA:Open and efficient foundation language models.arXiv:2302.13971.",
  "Liang Wang, Nan Yang, Xiaolong Huang, Binx-ing Jiao, Linjun Yang,Daxin Jiang, RanganMajumder, and Furu Wei. 2022.Text embed-dings by weakly-supervised contrastive pre-training.arXiv:2212.03533": "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,Linjun Yang, Daxin Jiang, Rangan Majumder, andFuru Wei. 2023. SimLM: Pre-training with repre-sentation bottleneck for dense passage retrieval. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 22442258, Toronto, Canada.Association for Computational Linguistics. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,Rangan Majumder, and Furu Wei. 2024. Improv-ing text embeddings with large language models. InProceedings of the 62nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1189711916, Bangkok, Thai-land. Association for Computational Linguistics.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighoff. 2023. C-Pack: Packaged resourcesto advance general Chinese embedding.arXiv:2309.07597": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In International Conference on LearningRepresentations. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, FuruWei, and Ming Zhou. 2020. LayoutLM: Pre-trainingof text and layout for document image understanding.In Proceedings of the 26th ACM SIGKDD Interna-tional Conference on Knowledge Discovery & DataMining, KDD 20, page 11921200, New York, NY,USA. Association for Computing Machinery. Jheng-Hong Yang, Carlos Lassance, Rafael SampaioDe Rezende, Krishna Srinivasan, Miriam Redi,Stphane Clinchant, and Jimmy Lin. 2023. Atomic:An image/text retrieval test collection to support mul-timedia content creation. In Proceedings of the 46thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR23, page 29752984, New York, NY, USA. Associa-tion for Computing Machinery.",
  "A.1Wiki-SS Context Length": "For Wiki-SS, the screenshot size is consistently980 980. However, it is challenging to ensurethat the text version exactly matches the text bodyin the screenshot. We manually check a sample of50 Wikipedia pages where the main content bodywas longer than what was covered in the first-pagescreenshot. The maximum number of words cov-ered in these cases was 492, with an average of453. Therefore, we set the truncation length to 500words to ensure that the text version does not con-tain less content than the screenshot so that DSEdoes not benefit from having more main text con-tent when demonstrating its effectiveness.Additionally, we examined the sensitivity of thetext-based retriever regarding input length in theWikipedia retrieval task. The table below showsthe effectiveness of the text retriever E5 on theWiki-NQ task with varying input lengths:",
  "A.2TextScreenshot Hybrid Search": "We evaluate whether combining text-based inputmethods with visual-based input can improve re-trieval performance. Specifically, we examine theeffectiveness of a hybrid search pipeline that in-corporates both text-based retrieval and screenshot-based retrieval. The hybrid approach is achievedby interpolating the similarity scores of the rankingresults from the two retrievers (Ma et al., 2022).As shown in , the hybrid CLIP and Phi-3 search significantly improves performance onthe SlideVQA task, outperforming each methodindividually, with notable gains in nDCG@10 andRecall@10. This suggests that integrating text andvisual information provides a more comprehen-sive understanding, enhancing retrieval in mixed-modality scenarios, especially when CLIP alonestruggles with text content. However, for the text-intensive task (Wiki-SS), we find that hybrid mod-els do not offer significant improvements. However,for the text-intensive Wiki-SS task, hybrid modelsshow minimal improvement indicating limited ben-efits of hybrid input in such context.Interestingly, while the hybrid approach boostsperformance for mixed-modality tasks, DSE alonestill outperforms the hybrid CLIP + Phi-3 model,highlighting its ability to directly integrate bothtext and visual information effectively."
}