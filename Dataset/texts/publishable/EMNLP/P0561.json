{
  "Abstract": "Large vision-language models (LVLMs) haverecently dramatically pushed the state of theart in image captioning and many image un-derstanding tasks (e.g., visual question answer-ing). LVLMs, however, often hallucinate andproduce captions that mention concepts thatcannot be found in the image. These hallucina-tions erode the trustworthiness of LVLMs andare arguably among the main obstacles to theirubiquitous adoption. Recent work suggests thataddition of grounding objectivesthose thatexplicitly align image regions or objects to textspansreduces the amount of LVLM halluci-nation. Although intuitive, this claim is notempirically justified as the reduction effectshave been established, we argue, with flawedevaluation protocols that (i) rely on data (i.e.,MSCOCO) that has been extensively used inLVLM training and (ii) measure hallucinationvia question answering rather than open-endedcaption generation. In this work, in contrast,we offer the first systematic analysis of the ef-fect of fine-grained object grounding on LVLMhallucination under an evaluation protocol thatmore realistically captures LVLM hallucinationin open generation. Our extensive experimentsover three backbone LLMs reveal that ground-ing objectives have little to no effect on objecthallucination in open caption generation.",
  "Introduction": "Large Vision-Language Models (LVLMs) have re-cently displayed impressive image understandingabilities (Li et al., 2023a; Liu et al., 2023c; Baiet al., 2023; Fini et al., 2023; OpenAI, 2023; Anilet al., 2023, inter alia). Their widespread adop-tion, however, is hindered by object hallucinationin which the LVLMssimilar to general hallu-cination of LLMs (Zhang et al., 2023b)inventobjects (or attributes of or relations between ob-jects) not present in the image.A range of methods have recently been proposedto address LVLM hallucination such as modified decoding strategies (Leng et al., 2023; Huang et al.,2023), post-hoc removal of hallucinated content(Yin et al., 2023; Zhou et al., 2023), or reinforce-ment learning (Sun et al., 2023; Zhao et al., 2023b;Gunjal et al., 2023; Yu et al., 2023). Most of theseapproaches, however, either increase inference costor need expensive additional training and/or data,impeding their ubiquitous applicability.A recent line of work (Chen et al., 2023b; Youet al., 2023; Pramanick et al., 2023) has suggestedthat including grounding objectivese.g., basedon referring expressions (Kazemzadeh et al., 2014)where textual descriptions of image regions have tobe grounded to the respective parts of the imageinto the LVLM training reduces object hallucina-tion. The claim is intuitive: region-level objectivesdemand finer-grained image understanding thanthe global image captioning (de facto the maintraining objective of LVLMs), as demonstrated invisiolinguistic compositionality (Bugliarello et al.,2023). Such objectives should thus, intuitively, dis-courage models from generating content they can-not ground in the image. Intuition aside, the empir-ical support for the claim that grounding objectivesreduce LVLM hallucination is weak and mainlylimited to question-answering (QA) style of evalu-ation in which the model is explicitly asked aboutexistence of objects in an image (Li et al., 2023b);we argue that this evaluation protocol poorly alignswith real-world free-form text generation tasksprimarily open image captioningfor which thereis no empirical evidence yet that object groundingreduces hallucination. Contributions. In this work, we perform the firstcomprehensive analysis of the effects that ground-ing objectives have on LVLM object hallucinationin open (i.e., free-form) image captioning, address-ing the shortcomings of existing hallucination eval-uation protocols. Concretely, we measure the ef-fect of adding two popular grounding objectives asadditional objectives to standard image captioning- based training of LVLMs: (1) the referring expres-sions (RE) objective asks the model to generate thebounding box of the region that corresponds to atextual description and vice versa; whereas (2) thegrounded captioning (GC) objective demands thatthe model generates image descriptions with inter-leaved (relative coordinates of) bounding boxes formentioned objects. We then compare the extent ofhallucination for LVLM variants trained with andwithout these grounding objectives. To this end,we compare the hallucination measures based onquestion answering (QA) (Li et al., 2023b) againstfree-form metrics for open captioning (Rohrbachet al., 2018; Jing et al., 2023). Critically, observingthat (1) existing evaluation measures and proto-cols (Rohrbach et al., 2018; Li et al., 2023b) relyon MSCOCO (Lin et al., 2014) and (2) MSCOCOdata is part of the training mix for most LVLMs, weargue that existing measures are likely to underesti-mate LVLM hallucinate; we thus extend our hallu-cination evaluation protocol to out-of-distributiondata that LVLMs will not have seen in training. Findings. Our experiments with three differentLLM backbones show that, under a sound eval-uation protocol, including grounding objectivesreferring expressions and grounded captioningtoLVLM training has little to no effect on objecthallucination, both in QA-based evaluation andopen-ended captioning. Enforcing generation ofgrounded captions at inference time, on the otherhand, slightly reduces object hallucinations but theeffect is small and comes at the cost of (slight)reduction in caption detailedness. A qualitative in-spection of grounded captions also confirms thatforcing model to generate a bounding box for men-tioned objects most often does not prevent it fromhallucinating content. In sum, we find that ground-ing objectives fail to meaningfully reduce LVLMhallucination, calling for novel methodological pro-posals towards hallucination reduction.",
  "Grounding Objectives in LVLMs": "Grounding objectives seek to align natural lan-guage expressions with regions in the image. Theseobjectives either take image regions as input, in theform of a bounding box and predict correspond-ing language expressions or produce such regionsas output. Many recent LVLMs have been trainedwith grounding tasks in their training mix alongsidestandard tasks like captioning and VQA (Liu et al.,2023b; Bai et al., 2023; Wang et al., 2023b); other models have been designed specifically for expres-sion grounding and trained with grounding objec-tives only (Chen et al., 2023b; You et al., 2023;Pramanick et al., 2023; Zhang et al., 2023a; Penget al., 2023; Chen et al., 2023a; Zhao et al., 2023a). Objectives.Our investigation focuses on the twoarguably most popular grounding objectives, com-monly part of LVLM training: referring expres-sions (Kazemzadeh et al., 2014) and grounded cap-tioning (Plummer et al., 2015).Referring expressions is the standard groundingobjective, included in training of nearly all LVLMs.Given a natural language description (of a region),the model has to ground it to the correct imageregion. As is common practice, we also use theinverse task, that is, generation of the natural lan-guage description for the given image region.Grounded captioning is the task of generatingan image caption in which the locations of regionsfor mentioned objects are interleaved in the caption(see for examples). In theory, such explicitgrounding is expected to result in closer adherenceto the image content and reduce hallucinations.Other grounding objectives have been proposedfor LVLMs training, such as question answeringwith image regions in the input or output (Zhu et al.,2016); these, however, are outside the scope of ourstudy, because we focus on the effects of groundingon hallucination primarily in free-form captioning. Encoding regions.Different approaches existfor representing image regions for the LVLMs.Most commonly, regions are represented as bound-ing boxes using either (relative) coordinates inplain text (Liu et al., 2023b; Chen et al.,2023b; Bai et al., 2023; Wang et al., 2023b)(e.g., [0.10, 0.05, 0.64, 1.00]; the coordinates aretreated as text and tokenized with the tokenizer ofthe corresponding LLM) or with learned embed-dings that correspond to a fixed-size rasterizationof the image (Peng et al., 2023; You et al., 2023;Pramanick et al., 2023). In this work, we adopt theformer region representation, i.e., relative coordi-nates as text, as this avoids introducing additionaltrainable parameters to the model.",
  ". VQA veries facts2. Validate with human annotation": ": CHAIR and FaithScore are used to measure hallucinations in open caption generation with LVLMs.CHAIR relies on human object annotation (over a fixed set) to identify objects and check if they are hallucinated.FaithScore first uses an LLM to convert captions into facts which are then verified by a VQA model. ation, usually image captioning (Rohrbach et al.,2018; Wang et al., 2023a; Jing et al., 2023). The lat-ter is arguably more indicative of models tendencyto hallucinate in the wild (i.e., in various real-world applications) but it is also a more difficultsetup for automatic evaluation. In contrast, QA-based evaluation is straightforward, but an untestedproxy for actual hallucination in generative tasks. QA-Based Hallucination Evaluation. POPE (Liet al., 2023b) is the de facto standard benchmarkfor QA-based hallucination evaluation. Relyingon images annotated with objects from MSCOCO(Lin et al., 2014), the benchmark consists of yes/noquestions about object existence (Is there X in theimage?). The negative questionsabout objectsnot in the imageare generated in three differ-ent ways using: i) objects randomly selected fromthe total pool of objects that exist in the dataset(random); ii) the most frequently annotated ob-jects in the dataset (popular); iii) objects with highco-occurrence to the images actual objects (ad-versarial), as co-occurrence statistics are a com-mon cause of hallucinations (Rohrbach et al., 2018;Biten et al., 2022; Li et al., 2023b; Zhou et al.,2023). The performance metric is accuracy, i.e.,the percentage of correctly answered questions. Open Hallucination Evaluation. We focus on twopopular meatrics for quantifying hallucination inopen caption generation: CHAIR (Rohrbach et al.,2018) and FaithScore (Jing et al., 2023), illustratedin ). The two metrics identify hallucinationin different ways: by complementing them with oneanother, we mitigate the risk of our findings merelybeing an artifact of a single (imperfect) evaluationmetric. Both metrics can also indirectly quantifyhow informative and descriptive the generated cap-tions are. As our result will show (5), there existsa tradeoff between faithfulness/hallucination and informativeness of the captions. We thus argue thatthe hallucination metrics should be contextualizedwith the measures of informativeness: factually cor-rect but uninformative captions are as undesired ascaptions with hallucinated information. CHAIR detects hallucinated objects using the setof 80 object classes from MSCOCO (Lin et al.,2014) with which the images are annotated. Wordsfrom the captions are matchedusing exact stringmatchingagainst the class names, augmentedwith synonyms. The resulting list of matched ob-jects is then cross-referenced against the gold listof annotated objects and all matched but not anno-tated objects are considered hallucinations. Twoscores are produced over the dataset: (1) CHAIRidivides the total number of hallucinated objectsacross all captions with the total number of de-tected objects; (2) CHAIRs is the proportion ofimages in the dataset for which the caption con-tains at least one object hallucination. CHAIRsis less than ideal for longer captions as they aremore likely to contain at least one hallucination;such a binary caption-level measure would hidepotentially substantial differences in hallucinationrates between models. Because of this, we adoptonly CHAIRi in this work. Following Zhai et al.(2023a), we additionally report the average num-ber of matched objects per caption as well as thegold object coverage (i.e., the average percentageof annotated objects mentioned in the caption) asmeasures of caption informativeness. CHAIR unfortunately comes with two majorshortcomings. First, it is based on MSCOCO im-ages and object annotations which are widely usedin a range of derivative datasets leveraged for train-ing LVLMs (Goyal et al., 2017; Kazemzadeh et al.,2014; Mao et al., 2016; Liu et al., 2023c). Thismakes LVLMs a priori less likely to hallucinateon MSCOCO images, which means that CHAIR is likely overly optimistic about (i.e., it underesti-mates) the amount of LVLM hallucination in thewild. We thus propose to extend CHAIR to anout-of-distribution dataset, one that ideally alsocomes with a larger set of object classes. Sec-ond, CHAIR relies on exact string matching be-tween caption words and synonym sets of the ob-ject classes. Adapting vanilla CHAIR based onstring matching to a larger set of object classeswould, however, require significant manual effort,as one would have to (1) create a curated list of syn-onyms for all new classes (without overlap betweenrelated classes) to correctly account for recall and(2) inspect examples and create special rules foredge cases to limit false positives (e.g., add babyX synonyms to all animal classes X in order notto falsely match the person class). Addressingboth issues simultaneously, we propose semanticmatching between the caption and object classes asan alternative to string matching for large sets of ob-ject classes. Our extension, dubbed CHAIR-MEN(from CHAIR with Matching using Embeddingsof Noun phrases) (1) extracts all noun phrases fromthe generation,1 (2) embeds the extracted phrasesas well as classes names with a pretrained sentenceencoder (Reimers and Gurevych, 2019)2 and (3)makes matching decisions based on cosine simi-larity between obtained embeddings: to each nounphrase, we assign (i) the class amongst the imagesobjects with the most similar embedding, if cosineexceeds a threshold t1, (ii) the class amongst theother objects (i.e., not present in the image) with themost similar embedding, if cosine exceeds a thresh-old t2, or otherwise (iii) no object class. Matchingfirst only against the images objects makes falsenegatives from a semantically related object notin the image less likely. We calibrate the thresh-olds (t1 = 0.73, t2 = 0.78) by trying to match thescores that vanilla CHAIR produces on MSCOCO,as an established measure for that dataset.FaithScore (Jing et al., 2023), a model-based hal-lucination metric, is designed with finer-grainedevaluation in mind: it does not only consider ob-jects/entities but also other aspects that modelscan hallucinate about (specifically: color, relation,count, and other attributes), without the need forhuman annotation. FaithScore computation is a 2-stage process that relies: (1) on an LLM to extractatomic facts from the generated text, phrasing",
  "With spaCy v3 EN_CORE_WEB_SM2BAAI/BGE-BASE-EN-V1.5 (Xiao et al., 2023)": "them as statements (e.g., There is a man) thefactuality of which, in the context of the image, isthen (2) verified with a VQA model (question: Isthe following statement correct?). The final scoreis then simply the proportion of positive answersgiven by the VQA model. We additionally reportthe average number of facts produced by the LLMas a measure of informativeness of generated cap-tions. The original work of Jing et al. (2023) relieson GPT-4 to extract facts but this is too expensivefor our evaluation; instead, we use a smaller LLM3 after verifying that it successfully follows task in-structions. We use OFA (Wang et al., 2022) as theVQA model for FaithScore, as it is much faster andonly marginally less accurate than Llava-1.5 (Liuet al., 2023b) according to Jing et al. (2023). Caption Quality Metrics. Next to the hallucina-tion measures, we add the following two standardmetrics to monitor how grounding objectives af-fect the general caption quality: CIDEr (Vedan-tam et al., 2015) is a measure based on n-gramoverlap with a set of reference captions. CLIP-Score, a reference-free metric, is the cosine simi-larity between the image and caption embeddings,produced by a CLIP model (Radford et al., 2021a).4",
  "Experimental Setup": "We comprehensively analyze the effect of ground-ing objectives on LVLM hallucination. For the sakeof transferability and robustness of our findings, ourexperimental core, namely the model architectureand training procedure, follows established prac-tices as closely as possible. All model instancesare trained according to the same protocol, that is,we control for everything other than the effect ofgrounding, i.e., inclusion/exclusion of groundingdata during training. We primarily focus on measur-ing hallucination in open-ended image captioningas this, we argue, better reflects LVLMs hallucina-tion in real-world applications; for completenessand comparison of evaluation protocols, we alsoperform the QA-based evaluation with POPE. Webenchmark LVLMs for hallucinations in two differ-ent caption generation scenarios: (1) in standardimage captioning, with expected caption length of1-2 sentences (as in MSCOCO), and (2) groundedimage captioning (with standard length), where theLVLM is explicitly prompted to interleave region",
  "coordinates into the caption. In the Appendix B,we also provide results for long (i.e., detailed, de-scriptive) caption generation": "Evaluation Datasets.Despite the previouslymentioned shortcomings, MSCOCO (Lin et al.,2014) remains the primary dataset for evaluatingLVLM hallucination in the literature, both with QA-based and free-form generation metrics/protocols(Rohrbach et al., 2018; Li et al., 2023b).Wethus include MSCOCO but complement it withthe Objects365 (O365) (Shao et al., 2019) datasetwhich comes with a much larger inventory of ob-ject classes (365 classes in total, including the 80MSCOCO classes) and, consequently, more objectannotations per image. We evaluate on 5000 and5386 images from test portion of MSCOCO andvalidation portion of O365, respectively.5 For thePOPE evaluation, we generate two new test setsfrom O365, each with 1500 examples (matchingMSCOCO POPE): O365/COCO uses only the 80classes from MSCOCO, and O365/non-COCOutilizes the remaining 285 classes. LVLM Architecture. We adopt the typical LVLMarchitecture: (1) images are encoded by an imageencoder, (2) projected by an alignment module intothe LLM embedding space, and (3) prepended tothe embeddings of textual tokens (Liu et al., 2023b).For the alignment module, we adopt as default theprojection by Chu et al. (2024), which uses a 2-layer MLP followed by a pooling layer. We alsoexperiment with a resampler (Li et al., 2023a; Baiet al., 2023; Alayrac et al., 2022), which learns toencode the visual information from the image in aset of trainable query embeddings; specifically, weuse a 3-layer perceiver-resampler (Alayrac et al.,2022) with 32 query tokens. We leverage the Ope-nAI CLIP ViT-L/14-224 (Radford et al., 2021b)as the image encoder. We experiment with threedifferent LLM backbones: Vicuna 1.5 7B (Chi-ang et al., 2023), Llama-3 8B (instruct) (AI@Meta,2024), and Phi-3-mini (Abdin et al., 2024). TheLLM parameters are frozen and 4-bit quantized(Dettmers et al., 2023); instead of direct LLM up-dates, we learn the LoRA adapters (Hu et al., 2022)for all parameter matrices of the LLM. 5WehaveadditionallyconsideredOpenImages(Kuznetsova et al., 2020), Visual Genome (VG) (Krishnaet al., 2017), and LVIS (Gupta et al., 2019) as datasets withgold object annotations but ultimately decided against theirinclusion due to insufficient object coverage in annotations(i.e., not all objects are annotated in every image).",
  "Pre-Training. We pre-train the alignment moduleand only the alignment module (all other parameterfrozen)on image-caption data. For this, we usethe 560k examples from Liu et al. (2023b)": "Training Mix. LVLMs are generally instruction-trained on a mix of tasks and datasets. The mix weadopt reflects the main goal of our study: to iso-late the effect of grounding objectives on LVLMshallucination. We thus include the following tasks:1. Standard image captioning: we train on theMSCOCO captions (400k examples);2. Long captioning: we use LLAVA-DETAILED(Liu et al., 2023c) with 23k long captions gener-ated by GPT-4 on the basis of (short) MSCOCOreference captions and gold object annotations;3. VQA: we select from VQAv2 (Goyal et al., 2017)all 170k yes/no questions. VQA is only added tothe training mix for the sake of QA-based halluci-nation evaluation with POPE;6 4. Referring expressions (see 2): we combineRefCOCO (Kazemzadeh et al., 2014; Mao et al.,2016) (320k examples) and Visual Genome (Kr-ishna et al., 2017) (we sample 320k examples);5. Grounded captioning (see 2): we use Flickr30k-Entities (Plummer et al., 2015) (150k examples).We name our LVLM model variants based ontheir respective training mix. The Base LVLM hasbeen trained only on non-grounding tasks (1-3); ad-dition of the referring expressions and groundedcaptioning tasks is indicated with +RE and +GC, re-spectively. For brevity, we provide further trainingand inference details in the Appendix A. By de-fault, we use the pooled MLP projection from Chuet al. (2024) for all models. Additionally, we train aVicuna-based model with the perceiver-resampler,which we denote with (Perc).",
  "Results": "We now report the observed hallucination effectsunder both protocols: in free-form captioningand in QA-based hallucination evaluation (as indi-cated by the POPE metric/protocol). The reportedCHAIR results correspond to our CHAIR-MENvariant; we report the results obtained with thevanilla CHAIR based on string matching in Ap-pendix C. We did not separately optimize hyperpa-rameters for each LLM and will thus refrain fromtheir mutual performance comparison; instead, for",
  "each of the three LLMs, we analyze how inclusionof grounding objectives affects their hallucination": "Referring Expressions. Before we test the effectsof grounding on free-form and QA-based halluci-nation, we first analyze if the two grounding objec-tives are mutually compatible. Concretely, we testhow the models trained with grounding objectives(+RE, and +RE+GC) perform on one of the ground-ing tasks itself. In other words, we test if and howwell models explicitly trained with grounding ob-jectives learn to ground expressions and whetherthe two grounding objectives are mutually benefi-cial. The results for expression grounding (one ofthe two RE tasks: given the description, provide thebounding box) are shown in . The metric isprecision@50, that is, the proportion of exampleswhere the intersection between the predicted andgold bounding box contains at least 50% of theirunion. The results indicate that adding groundedcaptioning (+GC) consistently and substantially im-proves the performance for all three LLMs: this strongly suggests that the two grounding objectivesare mutually compatible. Vicuna-based model withthe perceiver-resampler (Perc) aligner consider-ably underperforms the (default) MLP aligner; wesuspect that this is because the (pre-)training datawas insufficient for it to learn to properly encodepositional information. QA Hallucinations with POPE. sum-marizes the hallucination results according to theQA-based evaluation protocol with POPE. Overall,both grounding objectives, referring expressions(+RE) and grounding captions (+GC) fail to con-sistently and non-negligibly improve performance,i.e., reduce hallucination. While their combination+RE+GC greatly improves grounding capabilitiesover +RE alone for all LLMs (), the same isnot true for QA-based hallucination reduction (i.e.,POPE), pointing to the lack of causal link betweenobject grounding and hallucination reduction. Standard Captions. displays the perfor-mance of our LVLM variants on standard imagecaptioning. We observe consistently, for all testedmodels on both evaluation datasets, that groundingobjectives (i.e., their inclusion or exclusion) havelittle to no effect on performance: all models learnto generate proper captions in the MSCOCO style,with 10 words on average and of similar generalquality, as captured by the caption quality met-rics (CIDEr, CLIPScore). The metrics that capturecaption detailness (coverage, number of objects &atomic facts) also show little difference betweenthe models. Most importantly, the same is truefor hallucination metrics CHAIRi and FaithScore,",
  "confirming that there is no positive transfer fromgrounding to hallucination reduction": "Grounded Captions. Previous results establishthat training on grounding objectives does not re-duce hallucination in open caption generation. Wenext test whether forcing the model to generategrounded captions at inference can reduce halluci-nation. Intuitively, prompting the model to producegrounded captions should encourage it to generateonly objects contained in the image. The resultsin show that generating grounded captionsindeed results in some hallucination reduction, butthe effect is rather small. Reduction is more promi-nent on Objects365 where the baseline hallucina-tion rate is higher than on MSCOCO. On the flipside, generating grounded captions at inferenceslightly reduces their informativeness too (i.e., weobserve fewer objects and atomic facts in the gen-erated captions). A closer qualitative inspection(see 6) reveals that LVLMs trained with ground-ing objectives still incorrectly describe objects orfabricate them entirely.",
  ": Qualitative examples of Vicuna +RE+GCfor standard and grounded captioning. Hallucinationsare underlined in red. Predicted bounding boxes arevisualized in the image and marked in the caption": "example, the model fully hallucinates a womanalong with a bounding box for her. In the secondexample, the second elephant bounding box ispositionally correct in that it points to an animal,but that animal is a rhino. In the third example, sim-ilarly, the bounding box correctly contains an applebut the attribute peeled is hallucinated. These",
  "Related Work": "Large Vision-Language Models. LVLMs are es-sentially Large Language Models (LLMs) (Brownet al., 2020; Touvron et al., 2023; OpenAI, 2023;Jiang et al., 2023) extended to understand visualinput. Recent models have shown an impressive un-derstanding of images (OpenAI, 2023; Anil et al.,2023; Li et al., 2023a; Dai et al., 2023a; Liu et al.,2023c; Bai et al., 2023; Fini et al., 2023; Zhu et al.,2023; Laurenon et al., 2023; Geigle et al., 2023;Wang et al., 2023b) and a range of models havebeen proposed specifically for grounding and refer-ring (Chen et al., 2023b; You et al., 2023; Praman-ick et al., 2023; Zhang et al., 2023a; Peng et al.,2023; Chen et al., 2023a; Zhao et al., 2023a). Measuring Object Hallucinations. A range ofhallucination metrics have been proposed: CHAIR(Rohrbach et al., 2018) identifies hallucinated ob-jects by checking captions (via string matching)against a set of annotated objects (i.e., MSCOCO).Wang et al. (2023a) fine-tune an LLM to identifyhallucinatory captions through comparison withreference captions; FaithScore (Jing et al., 2023),a reference-free approach, uses an LLM to extractverifiable facts and then tests these facts with aVQA model. POPE (Li et al., 2023b) indirectlymeasures hallucination with questions about objectexistence: while a good test of image understand-ing , which may indicate the extent of models ten-dency to hallucinate, it is not a direct measure ofhallucination in open-ended captioning. Hallucination Mitigation.A range of ap-proaches have been proposed to mitigate hallu-cination: Biten et al. (2022); Dai et al. (2023b);Zhai et al. (2023a) propose adaptions to the train-ing data and objectives. Liu et al. (2023a); Gunjalet al. (2023); Zhao et al. (2023b); Yu et al. (2023)use reinforcement-learning methods to reduce hal-lucinations in model output. Leng et al. (2023);Huang et al. (2023) propose (training-free) decod-ing methods that mitigate hallucinations. Zhou et al.(2023); Yin et al. (2023) create pipeline approachesthat post-hoc clean the generated text from hallu-cinated content. Finally, for QA hallucinations, re-searchers have created robust instruction data (Liuet al., 2023a), VQA examples (Hu et al., 2023), andadditional benchmarks (Lu et al., 2023).",
  "Conclusion": "Object hallucination remains one of the main obsta-cles to wide-range adoption of LVLMs. Prior worksuggested that grounding objectives like referringexpressions reduce hallucination but the empiricalsupport for this claim is confined to QA-based eval-uation. In this work, we carried out an in-depthanalysis of the effects that grounding objectives inLVLM training have on their hallucination in openimage captioning. Our extensive experiments withthree backbone LLMs show that there is no causallink between improved object grounding (via ob-jectives like referring expressions) and hallucina-tion reduction: this observation is true both underQA-based and open captioning hallucination evalu-ation protocols. Finally, we observe that explicitlyprompting LVLMs to generate grounded captionsat inference can slightly reduce hallucination but atthe expense of reduced caption informativeness.",
  "Limitations": "There are two main limitations to our analysis.First, while we aim for a comprehensive analy-sis of the effects of different training objectivesand task mixes on downstream hallucination, thereare a number of modeling decisions that we hadto fix (i.e., we could not explore other variants)primarily w.r.t. to the architecture of the LVLMdue to a limited computational budget. One could,inter alia, consider a different image encoder, addi-tional or larger LLMs, and/or alignment modulesother than the MLP or perceiver-resampler. Addi-tionally, due to our limited computational budget,we train our models on less data and for fewersteps than a lot of other work that trains LVLMs(e.g. Chen et al. (2023b); Liu et al. (2023b); Baiet al. (2023)); we thus cannot rule out that a reduc-tion in hallucination due to grounding objectivesmight emerge at some larger scale of groundingtraining.Second, our findings are (modulo anecdotal ev-idence from manual qualitative analysis of a lim-ited number of examples) based on reliance onimperfect automatic metrics. While this is a com-mon practice in related work as well, we increasethe likelihood of the robustness of our findingsand conclusions by employing two mutually com-plementing hallucination quantification metrics,CHAIR and FaithScore (see 3), as well as addi-tionally proposing a semantic extension to CHAIR(CHAIR-MEN, see 3).",
  "Acknowledgements": "This work was supported by the Alcatel-LucentStiftung and Deutsches Stiftungszentrum throughthegrantEquitablyFairandTrustworthyLanguage Technology (EQUIFAIR, Grant Nr.T0067/43110/23). This work was in part supportedby the Alexander von Humboldt Foundation. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari,Harkirat Behl, Alon Benhaim, Misha Bilenko, Jo-han Bjorck, Sbastien Bubeck, Martin Cai, CaioCsar Teodoro Mendes, Weizhu Chen, VishravChaudhary, Parul Chopra, Allie Del Giorno, Gus-tavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter,Amit Garg, Abhishek Goswami, Suriya Gunasekar,Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Ma-houd Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Chen Liang, Weishung Liu, EricLin, Zeqi Lin, Piyush Madan, Arindam Mitra, HardikModi, Anh Nguyen, Brandon Norick, Barun Patra,Daniel Perez-Becker, Thomas Portet, Reid Pryzant,Heyang Qin, Marko Radmilac, Corby Rosset, Sam-budha Roy, Olatunji Ruwase, Olli Saarikivi, AminSaied, Adil Salim, Michael Santacroce, Shital Shah,Ning Shang, Hiteshi Sharma, Xia Song, MasahiroTanaka, Xin Wang, Rachel Ward, Guanhua Wang,Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu,Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu,Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang,and Xiren Zhou. 2024. Phi-3 Technical Report: AHighly Capable Language Model Locally on YourPhone. _eprint: 2404.14219.",
  "AI@Meta. 2024. Llama 3 Model Card": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Brock, Aida Nematzadeh, Sahand Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karen Simonyan. 2022.Flamingo:a Visual Language Model for Few-Shot Learning.CoRR, abs/2204.14198.ArXiv:2204.14198. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Gemini: A Fam-ily of Highly Capable Multimodal Models. CoRR,abs/2312.11805. ArXiv: 2312.11805. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023.Qwen-VL: A FrontierLarge Vision-Language Model with Versatile Abili-ties. CoRR, abs/2308.12966. ArXiv: 2308.12966.",
  "Computer Vision, WACV 2022, Waikoloa, HI, USA,January 3-8, 2022, pages 24732482. IEEE": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020.Language Models are Few-Shot Learners.arXiv:2005.14165 [cs]. ArXiv: 2005.14165. Emanuele Bugliarello, Laurent Sartran, AishwaryaAgrawal, Lisa Anne Hendricks, and Aida Ne-matzadeh. 2023. Measuring Progress in Fine-grainedVision-and-Language Understanding. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), ACL 2023, Toronto, Canada, July 9-14, 2023,pages 15591582. Association for ComputationalLinguistics. Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, PengLi, Maosong Sun, and Yang Liu. 2023a. Position-Enhanced Visual Instruction Tuning for MultimodalLarge Language Models. CoRR, abs/2308.13437.ArXiv: 2308.13437.",
  "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023b. Shikra: Unleash-ing Multimodal LLMs Referential Dialogue Magic.CoRR, abs/2306.15195. ArXiv: 2306.15195": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, ShuangXu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu,Xinyang Lin, Bo Zhang, and Chunhua Shen. 2024.MobileVLM V2: Faster and Stronger Baseline forVision Language Model.CoRR, abs/2402.03766.ArXiv: 2402.03766. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023a.InstructBLIP: Towards General-purposeVision-Language Models with Instruction Tuning.CoRR, abs/2305.06500. ArXiv: 2305.06500. Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pas-cale Fung. 2023b. Plausible May Not Be Faithful:Probing Object Hallucination in Vision-LanguagePre-training. In Proceedings of the 17th Conferenceof the European Chapter of the Association for Com-putational Linguistics, EACL 2023, Dubrovnik, Croa-tia, May 2-6, 2023, pages 21282140. Associationfor Computational Linguistics.",
  "Agrim Gupta, Piotr Dollr, and Ross B. Girshick. 2019": "LVIS: A Dataset for Large Vocabulary Instance Seg-mentation. In IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2019, Long Beach,CA, USA, June 16-20, 2019, pages 53565364. Com-puter Vision Foundation / IEEE. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-Rank Adaptationof Large Language Models.In The Tenth Inter-national Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022. Open-Review.net.",
  "Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and ZhenbangSun. 2023. CIEM: Contrastive Instruction Evalua-tion Method for Better Instruction Tuning. CoRR,abs/2309.02301. ArXiv: 2309.02301": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, WeimingZhang, and Nenghai Yu. 2023. OPERA: Alleviat-ing Hallucination in Multi-Modal Large LanguageModels via Over-Trust Penalty and Retrospection-Allocation.CoRR, abs/2311.17911.ArXiv:2311.17911. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7B. CoRR, abs/2310.06825. ArXiv: 2310.06825.",
  "Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia,and Xinya Du. 2023. FAITHSCORE: EvaluatingHallucinations in Large Vision-Language Models.CoRR, abs/2311.01477. ArXiv: 2311.01477": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara L. Berg. 2014. ReferItGame: Referringto Objects in Photographs of Natural Scenes.InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2014, October 25-29, 2014, Doha, Qatar, A meetingof SIGDAT, a Special Interest Group of the ACL,pages 787798. ACL. Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,Caiming Xiong, and Richard Socher. 2019. CTRL:A Conditional Transformer Language Model for Con-trollable Generation. CoRR, abs/1909.05858. ArXiv:1909.05858. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A. Shamma,Michael S. Bernstein, and Li Fei-Fei. 2017. VisualGenome: Connecting Language and Vision UsingCrowdsourced Dense Image Annotations.Int. J.Comput. Vision, 123(1):3273. Place: USA Pub-lisher: Kluwer Academic Publishers. Alina Kuznetsova, Hassan Rom, Neil Alldrin, JasperR. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, ShahabKamali, Stefan Popov, Matteo Malloci, AlexanderKolesnikov, Tom Duerig, and Vittorio Ferrari. 2020.The Open Images Dataset V4. Int. J. Comput. Vis.,128(7):19561981. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-zalez, Hao Zhang, and Ion Stoica. 2023. EfficientMemory Management for Large Language ModelServing with PagedAttention. In Proceedings of the29th Symposium on Operating Systems Principles,SOSP 2023, Koblenz, Germany, October 23-26, 2023,pages 611626. ACM. Hugo Laurenon, Lucile Saulnier, Lo Tronchon, StasBekman, Amanpreet Singh, Anton Lozhkov, ThomasWang, Siddharth Karamcheti, Alexander M. Rush,Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023.OBELISC: An Open Web-Scale Filtered Datasetof Interleaved Image-Text Documents.CoRR,abs/2306.16527. ArXiv: 2306.16527. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li,Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decod-ing. CoRR, abs/2311.16922. ArXiv: 2311.16922. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.Hoi. 2023a. BLIP-2: Bootstrapping Language-ImagePre-training with Frozen Image Encoders and LargeLanguage Models. CoRR, abs/2301.12597. ArXiv:2301.12597.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023c.Visual Instruction Tuning.CoRR,abs/2304.08485. ArXiv: 2304.08485": "Ilya Loshchilov and Frank Hutter. 2019. DecoupledWeight Decay Regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Jiaying Lu, Jinmeng Rao, Kezhen Chen, XiaoyuanGuo, Yawen Zhang, Baochen Sun, Carl J. Yang, andJie Yang. 2023. Evaluation and Mitigation of Ag-nosia in Multimodal Large Language Models. CoRR,abs/2309.04041. ArXiv: 2309.04041. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L. Yuille, and Kevin Murphy. 2016.Generation and Comprehension of Unambiguous Ob-ject Descriptions. In 2016 IEEE Conference on Com-puter Vision and Pattern Recognition, CVPR 2016,Las Vegas, NV, USA, June 27-30, 2016, pages 1120.IEEE Computer Society.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei. 2023.Kosmos-2: Grounding Multimodal Large LanguageModels to the World. CoRR, abs/2306.14824. ArXiv:2306.14824": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,Juan C. Caicedo, Julia Hockenmaier, and SvetlanaLazebnik. 2015.Flickr30k Entities: CollectingRegion-to-Phrase Correspondences for Richer Image-to-Sentence Models. In 2015 IEEE InternationalConference on Computer Vision, ICCV 2015, Santi-ago, Chile, December 7-13, 2015, pages 26412649. Shraman Pramanick, Guangxing Han, Rui Hou, SayanNag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang,Rama Chellappa, and Amjad Almahairi. 2023. Jackof All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model._eprint: 2312.12423. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021a. Learn-ing Transferable Visual Models From Natural Lan-guage Supervision. In Proceedings of the 38th In-ternational Conference on Machine Learning, ICML2021, 18-24 July 2021, Virtual Event, volume 139 ofProceedings of Machine Learning Research, pages87488763. PMLR. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021b. Learn-ing Transferable Visual Models From Natural Lan-guage Supervision. arXiv preprint, abs/2103.00020._eprint: 2103.00020. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Nat-ural Language Processing, EMNLP-IJCNLP 2019,Hong Kong, China, November 3-7, 2019, pages 39803990. Association for Computational Linguistics. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2018. Object Hallu-cination in Image Captioning. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, Brussels, Belgium, October 31- November 4, 2018, pages 40354045. Associationfor Computational Linguistics. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng,Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.2019.Objects365: A Large-Scale, High-QualityDataset for Object Detection. In 2019 IEEE/CVFInternational Conference on Computer Vision, ICCV2019, Seoul, Korea (South), October 27 - November2, 2019, pages 84298438. IEEE. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,Chunyuan Li, Yikang Shen, Chuang Gan, Liang-YanGui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,and Trevor Darrell. 2023.Aligning Large Multi-modal Models with Factually Augmented RLHF.CoRR, abs/2309.14525. ArXiv: 2309.14525. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. LLaMA: Openand Efficient Foundation Language Models. CoRR,abs/2302.13971. ArXiv: 2302.13971. Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. 2015. CIDEr: Consensus-based image de-scription evaluation. In IEEE Conference on Com-puter Vision and Pattern Recognition, CVPR 2015,Boston, MA, USA, June 7-12, 2015, pages 45664575.IEEE Computer Society. Junyang Wang, Yiyang Zhou, Guohai Xu, PengchengShi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, MingYan, Ji Zhang, Jihua Zhu, Jitao Sang, and HaoyuTang. 2023a. Evaluation and Analysis of Halluci-nation in Large Vision-Language Models. CoRR,abs/2308.15126. ArXiv: 2308.15126. Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022. OFA: Unifying Ar-chitectures, Tasks, and Modalities Through a SimpleSequence-to-Sequence Learning Framework. In In-ternational Conference on Machine Learning, ICML2022, 17-23 July 2022, Baltimore, Maryland, USA,volume 162 of Proceedings of Machine LearningResearch, pages 2331823340. PMLR. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu,Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.2023b. CogVLM: Visual Expert for Pretrained Lan-guage Models.CoRR, abs/2311.03079.ArXiv:2311.03079.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighof. 2023. C-Pack: Packaged ResourcesTo Advance General Chinese Embedding. CoRR,abs/2309.07597. ArXiv: 2309.07597": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, HaoWang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,and Enhong Chen. 2023. Woodpecker: HallucinationCorrection for Multimodal Large Language Models.CoRR, abs/2310.16045. ArXiv: 2310.16045. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2023. Ferret: Referand Ground Anything Anywhere at Any Granularity.CoRR, abs/2310.07704. ArXiv: 2310.07704. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, YifengHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-TaoZheng, Maosong Sun, and Tat-Seng Chua. 2023.RLHF-V: Towards Trustworthy MLLMs via Behav-ior Alignment from Fine-grained Correctional Hu-man Feedback.CoRR, abs/2312.00849.ArXiv:2312.00849. Bohan Zhai, Shijia Yang, Xiangchen Zhao, ChenfengXu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Man-ling Li, Tan Yan, and Xiangjun Fan. 2023a. HallE-Switch: Rethinking and Controlling Object ExistenceHallucinations in Large Vision Language Models forDetailed Caption. CoRR, abs/2310.01779. ArXiv:2310.01779.",
  "Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang,Jiashi Feng, and Bingyi Kang. 2023a. Bubogpt: En-abling visual grounding in multi-modal llms. CoRR,abs/2307.08581": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, XiaoyiDong, Jiaqi Wang, and Conghui He. 2023b. Be-yond Hallucinations: Enhancing LVLMs throughHallucination-Aware Direct Preference Optimization.CoRR, abs/2311.16839. ArXiv: 2311.16839. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, LinjunZhang, Zhun Deng, Chelsea Finn, Mohit Bansal, andHuaxiu Yao. 2023. Analyzing and Mitigating Ob-ject Hallucination in Large Vision-Language Models.CoRR, abs/2310.00754. ArXiv: 2310.00754. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023. MiniGPT-4: Enhanc-ing Vision-Language Understanding with AdvancedLarge Language Models. CoRR, abs/2304.10592.ArXiv: 2304.10592. Yuke Zhu, Oliver Groth, Michael S. Bernstein, andLi Fei-Fei. 2016. Visual7W: Grounded QuestionAnswering in Images. In 2016 IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR2016, Las Vegas, NV, USA, June 27-30, 2016, pages49955004. IEEE Computer Society.",
  "ATraining and Details": "All models were trained on a single NVIDIARTX3090s card, with training duration ranging be-tween 2-4 GPU days, depending on the trainingtask mix. We train for one epoch (on the concatena-tion of corpora from all tasks, as all tasks arefromthe low-level technical point of viewinstancesof causal language modeling, i.e., next token pre-diction) with AdamW optimizer (Loshchilov andHutter, 2019) and a cosine schedule. For LoRA,we set r = 64, = 128. During pre-training,where only the parameters of the alignment mod-ule are updated, we use batch size 32, learning rate0.001, and weight decay 0. For training on the taskmix, we use learning rate 2e-4, weight decay 0,and batch size 16/32/64 for Vicuna/Phi-3/Llama-3(achieved with gradient accumulation). For generation (i.e., inference), we use greedydecoding with a repetition penalty (Keskar et al.,2019) of 1.15 to avoid degenerative repetitions inlong caption generation. We use one fixed promptper task (see ) both in training and at infer-ence (for the subset of tasks on which we evaluate). We encode bounding boxes with 2 signif-icantdigits(,e.g.,[0.10, 0.05, 0.64, 1.00]).For grounded captions where multiple bound-ing boxes are needed (e.g.,for somethinglikethreezebras),wefollowPlummeret al. (2015) and combine the coordinateswith semicolons in the same brackets (, e.g.,[0.10, 0.05, 0.64, 1.00; 0.50, 0.15, 0.64, 1.00]).Ifwe would have more than three boxes in brackets,we instead create a single bounding box coveringall boxes to limit the final sequence length.",
  "BLong Captions": "shows long captioning results. For brevity,we only report the results for Objects365 withCHAIR(-MEN): for MSCOCO and FaithScore theresults are qualitatively the same. Overall, the dif-ferences between model variants are negligible sim-ilar to the standard captions. The grounding objec-tives (+RE and +GC) thus does not seem to affectlong captions. This again questions the extent towhich improved fine-grained image understandingfrom grounding actually transfers to hallucinationreduction in open generation.",
  "CCHAIR and CHAIR-MEN": "We report results based on our CHAIR-MEN ap-proach in the main paper. In the following, wecompare them against vanilla CHAIR results basedon the string matching method. In , we re-port string-matching CHAIR results for MSCOCO,which can be compared to (standard cap-tions), (grounded captions), and (long captions).We find that results with CHAIR-MEN arehighly proportional to CHAIR. This validatesCHAIR-MEN as an alternative approach for identi-fying hallucinated objects and opens up the exten-sion to other datasets like Objects365."
}