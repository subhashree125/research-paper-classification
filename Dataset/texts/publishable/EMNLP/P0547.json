{
  "Abstract": "Text style transfer (TST) is crucial in naturallanguage processing, aiming to endow text witha new style without altering its meaning. Inreal-world scenarios, not all styles have abun-dant resources. This work introduces TWIST(reusing Transferable Weight Increments forStyle Text generation), a novel framework tomitigate data scarcity by utilizing style featuresin weight increments to transfer low-resourcestyles effectively. During target style learning,we derive knowledge via a specially designedweight pool and initialize the parameters for theunseen style. To enhance the effectiveness ofmerging, the target style weight increments areoften merged from multiple source style weightincrements through singular vectors. Consid-ering the diversity of styles, we also designeda multi-key memory network that simultane-ously focuses on task- and instance-level in-formation to derive the most relevant weightincrements. Results from multiple style trans-fer datasets show that TWIST demonstratesremarkable performance across different back-bones, achieving particularly effective resultsin low-resource scenarios.",
  "Introduction": "Text style transfer (TST) is a significant area innatural language processing, aiming to endow textwith a new style without altering its meaning. Nu-merous studies have been successfully applied tosentiment transfer (Luca, 2016; Lai et al., 2022; Liet al., 2018), text formalization (Rao and Tetreault,2018; Jain et al., 2019), writing style imitation (Zhuet al., 2023; He et al., 2020; Riley et al., 2021), androle-specific dialogue scripts creation (Xu et al.,2023a; Niu and Bansal, 2018).As a sequence-to-sequence generation task, TSToften faces the problem of parallel data scarcity.However, annotating style-specific data is often",
  "Corresponding Author.Equal Contribution": "labor-intensive. Despite efforts to address this chal-lenge, recent studies still face significant limita-tions. i) Self-supervised Pre-training (Riley et al.,2021; Xu et al., 2023a), leverage large amounts ofstyle corpus for self-supervised pre-training in thelatent space, while performances with a lack of cre-ativity or formulaic text easily fall short comparedto supervised methods (Lai et al., 2022; Sudhakaret al., 2019). ii) In-context Learning (Shao et al.,2023; Wang et al., 2024), utilize the powerful capa-bilities of ChatGPT-4 (OpenAI, 2023) in few-shotlearning, while the stability is affected by high ac-curacy in prompt design. Additionally, there is noguarantee that a single basis of the learned simplexwill correspond to a target attribute such as dialectdue to a lack of scalability. iii) Synthetic DataGeneration (Suzgun et al., 2022; Chen and Huang,2024), utilize closed-source models to generatelarge synthetic datasets, which is hard to guaranteethe quality of synthetic data and may lead to bias.These issues undermine the efficacy and hinder thepractical applications of TST. This paper aims to exploit the limited and au-thentic datasets with supervised signal guidanceto achieve robust and scalable performance. Tothis end, we propose a supervised method TWIST(reusing Transferable Weight Increments for StyleText generation), which unleashes the potential toextract knowledge from known styles for unseenstyles. More specifically, TWIST is a two-stageframework. In the preparation stage, we employLow-Rank Adaptation (Hu et al., 2021) (LoRA) totrain weight increments for source styles. Then,these weight increments containing task-specificknowledge are stored in a source weight pool,which is designed for simultaneously capturingtask- and instance-level information. After inter-nal iterations, the weight pool can export the mostrelevant weight increments in a key-value format.We adaptively handle variable styles by focusingon dual-level authentic information. In the opti- mization stage, TWIST initializes partial parame-ters specific to target style by reusing weight incre-ments, thereby reducing data dependency. Inspiredby Ilharco et al. (2023), we involve the weightedsummation of parameter matrices to derive ap-propriate initial weights. In addition, we employSingular Value Decomposition (SVD) to extracta small subset of parameters from source weightmatrices, which are then injected into the initializa-tion matrix. We retain the top-q singular values andtheir corresponding singular vectors, achieving aneffect similar to sparse matrices. This reduces inter-ference between weight matrices, making mergingmore effective.Experiments demonstrate the remarkable effi-cacy and stability of TWIST on widely recognizedbenchmarks, achieving state-of-the-art (SOTA) per-formance across various backbone models, includ-ing T5 (Ruder et al., 2019) and LLaMA-2 (Touvronet al., 2023). Compared to other baselines relyingon small-scale models, TWIST based on T5-Largedemonstrates superior performance with only 10%training data. Notably, the performance based onLLaMA-2-7B surpasses other fine-tuned methodsand achieves performance comparable to powerfulChatGPT4 (OpenAI, 2023).Our contributions are summarized as follows:I. We propose a new perspective to address datascarcity in text style transfer. To the best of ourknowledge, this is the first work to explore learninggeneral knowledge from diverse source styles fortransferring to target styles.II. We introduce a model-agnostic frameworkwhere the proposed weight pool module is reusableand scalable by focusing on task- and instance-levelinformation. Despite requiring additional computa-tional resources, all weight increments are shared,enabling flexible and sufficient reuse of informa-tion from various styles.III. Experiments across different backbone net-works demonstrate that TWIST enhances param-eter initialization, mitigating the impact of datascarcity. Our study showcases its generalizability,achieving significant results in low-resource stylescenarios and notable improvements in commonlyencountered high-resource styles.",
  "LoRA Fine-tuned Model": "Fine-tuning (Yosinski et al., 2014) is a machinelearning technique where a pre-trained languagemodel (PLM) is further trained on a task-specificdataset, adapting its general knowledge to the newtask. The most common practice for learning a newtask t T involves fine-tuning all parameters (Heet al., 2015) of a PLM on the target task trainingdata {(x, y)}. Given the pre-trained parameters,fine-tuning scheme leads to a specialized model tby optimizing:",
  "Transfer Source Weight for Target Style": "In addition to the efficiency of LoRA parame-ters, we further explore the potential of LoRA astransferable parameters.A typical approach isinterpolation-based methods (Finn et al., 2017),which aim to learn better parameter initializationto adapt to unseen tasks, essentially learning to ini-tialize efficiently. They merge all model weightsas follows: t = 0 + Ss=1 s, where isa hyper-parameter, and S is the number of sourcetasks. s can be implemented in various ways,such as Adapter (Houlsby et al., 2019) or Prompttuning (Li et al., 2022). We chose LoRA to re-duce the networks depth. The performance ofabovementioned methods is affected by parameterinterference (Yadav et al., 2023). Asai et al. (2022);Peng et al. (2024) used weighting or attention op-erations to mitigate this impact, showing superiorperformance in comprehension tasks. However, forgeneration tasks requiring continuous word predic-tion, the direct addition of weights amplifies thenegative impact of parameter interference.",
  "Problem Setup": "Text Style Transfer aims to alter the style of inputtext x to produce output text y while preserving theoriginal content. This task is formulated by model-ing the conditional probability Pr(y|x; ), wherex = {w1, . . . , wtx} denotes the sequence of tokensfrom the source style, and y = {z1, . . . , zty} rep-resents the sequence of tokens in the target style.Here, x and y are token sequences from vocabularyV , and tx and ty the sequence lengths. However,the generalization capability of parameters is of-ten influenced by the number of supervised pairs. Thus, we consider a set S = {s1, . . . , sS} ofsource style transfer tasks that are independent ofthe target style t. Each task sn = (xni , yni )kni=1 con-sists of kn tuples where xni is input text from thesource style and yni is the corresponding output textin the target style. From these tasks, we extract a setof weights {1s, . . . , Ss } containing specificstyle knowledge from the source. For an unseen tar-get style t, our goal is to maximize the probabilityPr(yt|xt; t), where initial t = 0+Ss ss.We hypothesize that such knowledge transfer canenhance the generalization capability of the targetstyle t in low-resource scenarios. Therefore, howto effectively transfer and reuse s is crucial ob-jective of our work.",
  "Method": "Our proposed method consists of two stages (de-picted in ): In the preparation stage, weconstruct a source weight pool ( 3.1) to storereusable weight increments. We use a LoRA-basedPEFT method to train weight increments indepen-dently for each source style in 3.1.1. Subse-quently, these weight increments are clustered in 3.1.2 and stored in a specially designed multi-keymemory network in 3.1.3. Finally, 3.1.4 de-scribes a retrieval scheme that considers task-leveland instance-level information. In the optimizationstage ( 3.2), we discuss transferring the retrievedinformation to initialize the model more efficiently.",
  ": Framework of our method": "3.1.1Source Weight Increments Pre-trainingWe first obtain source weight increments s for acollection of source tasks {s1, . . . , sS}, where S isthe number of source tasks. In practice, the datasetsfor style transfer tasks are typically high-resourceto enable effective knowledge extraction.Each source weight increment is trained onlyonce and can be transferred to different targettasks.We perform the extraction using themethod described in 2.1, aiming to obtain ={1, . . . , S}. During training, only s =AsBs for specific task s is updated by maximizingthe likelihood as follows:",
  "maxs Pr(y|x; 0, s)(3)": "3.1.2Clustering Source Weight IncrementsWe construct the weight pool to identify simi-larities between source tasks for more effectivestyle knowledge transfer. In particular, throughthe spectral clustering algorithm, we categorizethe source weight increments into several clus-ters C G, where G is a weighted undirectedgraph.Specifically, each input is treated as anode p, and the weight between node pi and pj iswi,j = 1/(1+pipj). To ensure each cluster hassufficient nodes, we employ the min-max cut strat-egy to segment G, resulting in C = {C1, . . . , CC},where C is the number of clusters. When retriev-ing weight increments, it is better to identify thesuitable weight cluster and select the most relevant",
  "Adaptive Knowledge Retrieval": "Firstly, we initialize a set of learnable parame-ters with a semi-orthogonal matrix (Saxe et al.,2014), following Wang et al. (2022). Next, givena tokenized input x, we use BERT, denoted asfBERT (Devlin et al., 2019), to extract its seman-tic features. This maps the original text x to ahidden feature space, generating the query vectorq. Mathematically, q = fBERT(x) (x Rlc,q Rd), where l represents the sequence length.To maintain consistency, fBERT remains frozen atall stages.Optimizing weight key k. To reduce the op-timization cost of the weight key k, we use theK-nearest neighbors algorithm to calculate the co-sine similarity between the query and keys forretrieving the top-W most similar keys K ={k1 , . . . , kW } for each query, where W S.The weight keys k K are optimized toalign with the input instance distribution as fol-lows: k = k + k cos(q, k) , where isthe learning rate and cos() denotes cosine simi-larity. To avoid local optima, we also introducea random key masking scheme proposed by Penget al. (2024). This approach reduces the retrieverstendency to over-prioritize specific keys, encourag-ing better allocation of attention to other elements.Optimizing cluster key kC. To compute thecluster key kC, we take the geometric mean of theweight keys k within the cluster. The formula",
  "n , where n is thenumber of weight keys in the cluster and ki repre-sents the individual weight keys within the cluster": "Adaptive Retrieval.For an unseen target task tand its instance data {(xi, yi)Ni=1}t (X, Y)t, weuse a task-level query qtask and an instance-levelquery qins to adaptively retrieve the weight incre-ments. The task-level query aims to capture overallinformation relevant to the specific target task andis designed as a vector qtask Rc (Vu et al., 2022).However, due to the diversity and limitations ofthe resources in the pool, to enhance the robust-ness of retrieval, we design an instance-level querycalculated as follows: qins = 1 NxX fBERT(x).For each source weight increment s P,we use qtask and qins to lookup its correspondingcluster key and source key respectively. The Re-trieval Score Rs between s and instance x iscalculated as follows:",
  "Reusing LoRA-based Weight Increments": "Previous methods for merging weight often en-counter the issue of dense parameters, particularlyin the complex tasks (Yadav et al., 2023), e.g. gen-eration tasks. Using interpolation methods directlyleads to instability in initialization. One feasibleapproach is to prune AsBs (Zhang et al., 2023);however, initial experiments in indicate thatpruning results in significant performance degrada-tion on interpolation methods. Therefore, we usedsingular value decomposition (SVD) to extract asmall subset of parameters from the task-specificLoRA matrix. These parameters are combined withRs and injected into the model requiring initializa-tion.Specifically, we first perform SVD for AsBs ,i.e. AsBs= UssVs , where Us Rdoutr and Vs Rdinr are orthogonal matrices, ands Rrr is a diagonal matrix with its diag-onal entries sorted from highest to lowest. Weselect the top-q singular values and their corre-sponding singular vectors to reduce the numberof parameters.This involves constructing sub-matrices: U(q)s Rdoutq, V(q)s Rdinq, and(q)s Rqq containing the first q columns of Us,Vs, and the top q singular values along its diago-nal, respectively. We then approximate the originalproduct AsBs as U(q)s (q)s V(q)s. This reducesthe number of parameters from r (dout + din) toq (dout + din + 1), because now we only need tostore the top-q singular values and their correspond-",
  "s=1Rs U(q)s (q)s V(q)s(6)": "Discussion.Selecting q much smaller than r cangreatly improve computational efficiency. How-ever, the choice of q is crucial. If q is too close tor, the approximation will retain more informationfrom the original LoRA matrix, but it will lose thesparsity benefit, and parameter interference mightrise. On the other hand, if q is too small, importantinformation may be lost, negatively impacting theinitialization and overall model performance. 3.2.1Training for Target Style GenerationThe model is expected to learn unseen target tasks,and for each target task tk, we use the learned pa-rameters 0 + kt to initialize the weight matrixfor the target task. Given the dataset Dk of tk, thelearning objective function is defined as:",
  "Experiment Setup": "Datasets.We chose the following four datasetsfrom nine different styles: YELP (Luca, 2016) in-cludes parallel sentences of positive and negativereviews. GYAFC (Rao and Tetreault, 2018) pro-vides sentences of formal and informal expressions.Shakespeare (Zhu et al., 2023) contains the worksof Shakespeare. Genshin (Xu et al., 2023a) isbased on game roles and contains six sub-datasets:Xiangling, Hutao, Mona, Diluc, Venti, and Noelle.The data usage is shown in . Source and Target Tasks.In all experiments,the target task remains unseen for any source task.Practically, we rotate through different target styles.If Shakespeare is selected as the target style, weremove its corresponding weight increment from",
  "Implementation Details": "Baselines.For T5-Large, the selected base-lines include: Fine-tune (FT) T5-Large (Ruderet al., 2019), CrossAligned (Lai et al., 2022),Fine-tune BART-Large (Lewis et al., 2019),Delete&Retrieve (Li et al., 2018), B-GST (Sud-hakar et al., 2019), BTTS (Xu et al., 2023b) andTextSETTR (Riley et al., 2021). For LLaMA2-7B, the selected baselines include: QLoRA-basedFine-Tuning (QLFT) LLaMA2-7B (Dettmers et al.,2023), QLoRA-based Fine-Tuning ChatGLM2-6B (GLM et al., 2024), Few-shot (FS) Alpaca-7B (Taori et al., 2023), Few-shot Claude-31 (An-thropic, 2024) and Few-shot ChatGPT-4 2 (Ope-nAI, 2023). The prompt used by Few-shot Tuningis shown in Appendix C. Evaluation Metrics.We use automatic metricsto assess attribute control, such as style accuracy(ACC) and content preservation (CP) after trans-fer. To estimate the output style, we follow themethod proposed by Riley et al. (2021) and traina classifier for the specific style on the training set.More details can be found in Appendix B. For con-tent preservation, we calculate Self-BLEU usingSacreBLEU (Post, 2018), following Sudhakar et al.(2019) and Xu et al. (2020a). Additionally, wereport the \"G-score\" (the geometric mean of ACCand CP), following (Xu et al., 2018). Parameter Settings.For experiments based onT5-Large, we utilized 2 NVIDIA RTX4090 GPUs.The learning rate was set to 2102, with a weightdecay of 0.01 and a batch size of 8. We ran themodel for 50 epochs with 100 warmup steps. Forexperiments involving LLaMA2-7B, we employed",
  "The specific version is claude-3-sonnet-20240229.2The specific version is gpt-4-1106": "2 NVIDIA A100 GPUs. The learning rate was ad-justed to 2 104, with a weight decay of 0.01and a batch size of 2. These experiments were con-ducted over 3 epochs with 0.03 warmup steps. Thekey optimization learning rate was 1 103. Allexperiments used AdamW (Loshchilov and Hutter,2019) as the optimizer.",
  "Main Results": "Results on T5.We use T5-Large as the back-bone model for comparison with other small-scalePLM methods. The training data consists of thefull dataset, and the results are presented in Ta-ble 1, where indicates that we folded the re-sults for this task. Detailed results are providedin . TWIST demonstrates stronger gener-alization capabilities than directly fine-tuning T5-Large (Ruder et al., 2019). Benefiting from betterinitialization, it shows significant improvements inlow-resource writing styles and role dialogue styles,with increases of 20.6% and 6.1%, respectively.On average, compared to the previous best methodDelete&Retrieve (Li et al., 2018), TWIST showsan improvement of 12.5%. On formality transfer,B-GST (Sudhakar et al., 2019) outperformed it bya margin of 4.8%. Results on LLaMA2.The comparison resultsbased on LLaMA2-7B are shown in , withunfolded results provided in . Comparedto the direct fine-tuning method (Dettmers et al.,2023), TWIST achieves an overall improvement of8.3%. We achieve a noticeable content preserva-tion (CP) lead compared to some few-shot methodsusing carefully designed prompts. Regarding styleaccuracy, we achieve comparable results to the pow-erful closed-source model ChatGPT-4 (OpenAI,2023), trailing by 1.9% and 3.0%.",
  "Results on Low-resource Setting": "In the low-resource setting, we use only a smallnumber of training instances from the target task.We perform a secondary sampling of the target taskdataset to obtain a subset of {1%, 2%, 5%, 10%} ofthe full training instances. We conducted three ran-dom samplings in total to reduce the randomnessof the experiment.a and b show the performanceof small-scale model-based methods on the Shake-speare writing style and dialogue styles of specific 1%2%5%10% T5-large CrossAligned Delete&Retrieve B-GST TextSETTR Ours G-score",
  ": Comparison under various shots of instances": "roles in Genshin. Due to the better initialization ca-pability of our method, it adapts more effectively tolow-resource scenarios. However, as the amount ofdata increases, the advantage gradually diminishes.Additionally, the BLUE line and RED line in Fig-ure 2c represent two different tasks: writing styleand role dialogue style transfer. The performanceis similar to that of another backbone network.",
  ": Comparison under various parameters": "Power of Scale.As shown in , ourmethod benefits from the size of the parameterscale, but the improvement brought by increasingthe parameters is not absolute. Test style transfertask requires a comprehensive consideration of con-tent preservation and style accuracy. One of thechallenges is balancing content and style. From ourexperience, increasing the parameters can lead tomore diverse expressions and redundant represen-tations. This redundancy can affect the calculationof ScareBLEU, impacting content preservation atthe data scale level.",
  "MethodsCP ACCGCP ACCGCP ACCGCP ACCGCP ACCG": "FT T5-Large56.7 52.0 54.3 53.2 71.5 61.7 15.9 62.1 31.4 53.1 55.6 54.3 44.7 60.3 50.4FT BART-Large 48.9 55.2 52.0 50.3 70.1 57.9 12.4 72.6 30.4 49.8 55.3 52.5 40.3 63.3 48.2CrossAligned2.968.2 14.1 21.5 68.2 38.3 4.853.7 16.1 14.7 57.5 28.9 11.0 61.9 24.3Delete&Retrieve 56.9 49.4 53.0 56.2 73.4 64.2 14.0 56.8 28.2 54.1 52.7 53.3 45.3 58.1 49.7B-GST54.2 60.2 57.1 63.5 80.0 71.3 17.1 69.5 34.5 30.7 60.1 42.9 41.4 67.5 51.5TextSETTR54.4 44.9 49.4 42.7 75.6 56.8 14.2 78.6 33.4 32.5 52.6 41.3 36.0 62.9 45.2BTTS54.7 53.7 54.2 52.7 75.9 63.4 16.4 74.6 34.9 30.7 50.7 39.4 38.6 63.7 48.0",
  ": Ablation study based on LLaMA2-7B": "better initialization for training. In the third rowof , we ablated the LoRA parameter matrixand adopted random initialization, which resultedin a decline in overall performance. Combined withc, a good initialization can indicate boththe upper and lower limits of the ability, especiallyin low-resource scenarios.In 2.2, we chose LoRA to avoid increasing thenetworks depth. In Appendix D.1, we transfer themethod to adapters and prompts, further analyzingthe differences between LoRA and these methods.Additionally, LoRA can be applied to different pa-rameter parts of the network. In Appendix D.2, wefound that the FFN layer may effectively improvestyle accuracy. In this work, we applied weightincrements to the output of the attention layer witha higher G-Score.",
  "ically, we replaced U(q)s (q)s V(q)sin Equation 6": "with AsBs . The initial results indicate a signifi-cant performance improvement.Furthermore, we analyzed how the selection ofq works. We first conducted comparative exper-iments on a single task using the LoRA and theSVD-based LoRA method. In the comparison, weused efficient tuning without additional modules.The experimental results in b show thatas q increases, the performance of the SVD-basedmethod gradually approaches that of the LoRAmethod. When q exceeds 16, the performancestabilizes and can be considered equivalent to theLoRA method. Subsequently, we retested the afore-mentioned task using the SVD-based Interpolationmodule. a shows that the models over-all performance after interpolation surpasses thatof the method on the right. However, as we dis-cussed in 3.2, increasing q does not necessarilylead to performance improvement. Our findingsalign with Jiang et al. (2024), indicating that param-eters influence each other during interpolation. Al-though increasing q can enhance the performanceof a single task, suggesting that more information iscontained within the parameters, the effectivenessof simply increasing information density dimin-ishes when the total parameter count remains thesame. By reducing q, we can make the parameter",
  "space sparser, thereby reducing interference andimproving the models overall performance": "Effects of Multi-key Retrieval.The first row of shows the results of removing the retrievalscore, i.e. Rs = 1. The significant performancedrop highlights the importance of Adaptive Re-trieval for retrieving the most relevant knowledge.Adaptive Retrieval allows dynamic selection of themost suitable weight increments through query-keymatching. The following row shows the resultsof ablating instance-level queries, specifically qins and ks . We only used task-level weight incrementsfor model initialization. The performance declineindicates that incorporating instance-level featuresindeed helps transfer the most useful knowledge tospecific instances in the target task.",
  ": Human evaluation on three metrics": "To supplement automatic metrics, we conductedhuman evaluations by sampling 50 instances fromeach dataset. The baselines used for comparisonare: QLFT LLaMA2-7B (Dettmers et al., 2023),FS Alpaca-7B (Taori et al., 2023), and FS ChatGPT-4 (OpenAI, 2023). Five participants were enlistedto assess each model based on three criteria: (1)strength of style transfer, (2) semantic integrity, and(3) sentence fluidity. Performances were rankedfrom best-1 to worst-4. presents the rank-ings of methods, as determined by participant feed-back. In the Style Accuracy and Fluency assess-ment, participants seemed to struggle with makinga clear choice; however, TWIST significantly out-performed the others in terms of Content.",
  "Visualization": "Stylistic Features.We visualize the stylistic fea-tures via 2D UMAP (McInnes et al., 2020) in Fig-ure 5. We selected 12 features for visualizationdimensions: these include three stylistic features,i.e., quantities of punctuation, the number of sen-tences, and the number of words (Zhu et al., 2023);and nine less-correlated vertical style types (Kangand Hovy, 2021), i.e., Humorous, Polite, Formal,Romantic, Gender, Dominance, Exciting, Sadness,",
  ": 2D-Visualization of Stylistic Feature": "and Offense. The first set of features is determinedstatistically, while the latter are evaluated usinga classifier (Kang and Hovy, 2021). The resultsclearly illustrate that transformed texts (in BLUE)are distinctly different from the original text style(in RED) and closely align with the supervisedtarget results (in GOLDEN). Task Similarity.To visually represent the rela-tionships and similarities among all generated keyvectors, we present a similarity matrix visualizationin . C represents the clustered key kC, andothers are weight key k. The visualization showsthat keys in the same cluster typically exhibit highersimilarity. The keys are roughly divided into fiveclusters, with the cluster containing C2 represent-ing the formality transfer task in two scenarios. Thekeys are approximately divided into two clustersfor role dialogue style transfer. Through manualinspection, we found that roles in the cluster con-taining C4 tend to be more aggressive, with shortersentences and more varied interjections and punc-tuation. Conversely, roles in the cluster containingC5 tend to be more conservative, using more formallanguage. YELPC1rYELP EM C2FR Shaks. C3 HuTao C4 XiangL.Venti MonaC5DilucNoelle YELPC1EM rYELPC2XiangL. C4 HuTao C3 Shaks. FRMona VentiC5DilucNoelle 0.0 0.2 0.4 0.6 0.8 1.0 C1 C2 C3 C4 C5",
  "Related Work": "Text Style Transfer and Style Categories.TSTaims to endow the text with a new style withoutaltering its meaning (Riley et al., 2021). From theholistic definition of style, current work can be di-vided into two paradigms (Zhu et al., 2023). Thefirst paradigm decouples explicit style from content,as in Xu et al. (2023a), which added an adversarialloss function. (Syed et al., 2020) randomly droppedinput words and reconstructed the input for eachauthor separately. The second paradigm avoids ex-plicitly decoupling style from content. Dai et al.(2019) added extra style embeddings in the input.Yi et al. (2021) employs generative flow techniquesto extract stylistic features from instances of styles.Previous studies primarily focused on simple styletransfer tasks, like formality (Rao and Tetreault,2018) or sentiment transfer (Xu et al., 2020b; Sud-hakar et al., 2019). These tasks achieved satisfac-tory results via word-level transfers. Recently, Xuet al. (2023a) defined the cross-style as a compoundstyle, such as personality (Shao et al., 2023) andwriting styles(Zhu et al., 2023; Tao et al., 2024). Parameter-Efficient Fine-Tuning and Weight In-crements.PEFT is an efficient method for fine-tuning large models by adjusting only a subset ofparameters. LoRA applies low-rank decompositionto weight matrices and updates the model with in-cremental parameters (Hu et al., 2021). Adapter in-troduces small, trainable modules into each layer ofthe pre-trained model (Syed et al., 2020). Prompt-tuning fine-tunes the model efficiently by modi-fying only the input (Lester et al., 2021). Prefix-tuning adds a continuous prefix to the input layerthat adapts to tasks through adjustment (Li andLiang, 2021).Weight Increments refer to thesubset of parameters modified in PEFT, contain-ing the most relevant knowledge for specific TSTtasks (Yang et al., 2023; Horvitz et al., 2024). Ourgoal is to retrieve reusable knowledge from incre-ments to support low-resource style learning.",
  "Limitations": "In this paper, we introduce TWIST, a method forstyle transfer in text generation. However, ourmethod introduces an additional retrieval frame-work, which may increase computational and mem-ory costs. This overhead is relatively minor com-pared to the resource demands of large models usedfor inference. Further quantitative analysis of thisweakness is provided in Appendix E. In addition,using LoRA as the format of weight incrementsmay not be optimal and introduces extra param-eters, and the tunable parameter quantity variesacross different parts of the model. We further an-alyze this in Appendix D.1. Lastly, our currentevaluation of TWIST has been limited to English-language tasks. We recognize the importance ofassessing its performance in other languages to un-derstand its broader applicability and intend to testTWIST on non-English tasks in future work.",
  "Ethics Statement": "TWIST enhances the ability to transfer style in en-vironments with limited data resources. All experi-ments were conducted using widely-used generaldatasets, which are unlikely to contain harmful con-tent. However, using role styles may carry certainrisks, as malicious actors could potentially exploitTWIST for activities such as fraud. Additionally,the styles can be influenced by the datasets usersprovide, which might contain harmful content, bi-ases, or privacy issues.For human evaluation, we enlisted five nativeEnglish speakers as volunteers to assess the gen-erated texts. These volunteers have a backgroundin literary creation, enabling them to distinguishdifferent text styles. We did not ask for personalinformation or collect any private data from thevolunteers. This research was supported by the National Nat-ural Science Foundation of China (No.62076059),the Science and Technology Joint Project of Liaon-ing province (2023JH2/101700367), and the Funda-mental Research Funds for the Central Universities(N2424010-7). Osmar Zaiane gratefully acknowl-edges the funding from Natural Sciences and Engi-",
  "Model-agnostic meta-learning for fast adaptation ofdeep networks": "Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-hui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Han-lin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, JiadaiSun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang,Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, LucenZhong, Mingdao Liu, Minlie Huang, Peng Zhang,Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang,Shulin Cao, Shuxun Yang, Weng Lam Tam, WenyiZhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiao-tao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, XinyueYang, Xixuan Song, Xunkai Zhang, Yifan An, YifanXu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai,Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang,Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024.Chatglm: A family of large language models fromglm-130b to glm-4 all tools.",
  "Bohao Peng, Zhuotao Tian, Shu Liu, Mingchang Yang,and Jiaya Jia. 2024. Scalable language model withgeneralized continual learning": "Matt Post. 2018. A call for clarity in reporting BLEUscores. In Proceedings of the Third Conference onMachine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computa-tional Linguistics. Sudha Rao and Joel Tetreault. 2018. Dear sir or madam,may I introduce the GYAFC dataset: Corpus, bench-marks and metrics for formality style transfer. pages129140, New Orleans, Louisiana. Association forComputational Linguistics.",
  "Prateek Yadav, Derek Tam, Leshem Choshen, ColinRaffel, and Mohit Bansal. 2023. Ties-merging: Re-solving interference when merging models": "Kexin Yang, Dayiheng Liu, Wenqiang Lei, BaosongYang, Mingfeng Xue, Boxing Chen, and Jun Xie.2023.Tailor: A soft-prompt-based approach toattribute-based controlled text generation. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 410427, Toronto, Canada. Associa-tion for Computational Linguistics. Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and MaosongSun. 2021. Text style transfer via learning style in-stance supported latent space. In Proceedings of theTwenty-Ninth International Conference on Interna-tional Joint Conferences on Artificial Intelligence,pages 38013807.",
  "A.1Key-Value Memory Networks": "We will provide a clearer explanation of the mo-tivation and optimization process. Here, we willelucidate several key concepts, namely Key-valueMemory Neural Networks (KV-MemNNs) (Milleret al., 2016b).Specifically, KV-MemNN draws inspirationfrom the idea of key-value storage, allowing flexi-ble storage and retrieval of relevant information. Itconsists of two main components: 1. Key-valueMemory: A programmable memory unit designedto store information in key-value pairs. In our work,this corresponds to [k, ], where k represents keyvector and represents weight increment. can be considered a form of value vector. Pleasenote that key is not synonymous with weightincrement. They operate on different dimensionsand carry distinct meanings, stored in pairs. 2.Memory Access Mechanism: A learnable atten-tion mechanism quickly retrieve relevant informa-tion from memory, denoted as Equation 5. Duringtraining, the optimization goal is to learn effective storage and access of information in memory toaccomplish the given task. In our work, in the KVretrieval mechanism, we determine the most rele-vant keys to the current query by computing thecosine similarity between the key vector and queryvector via k = k + k cos(q, k). Subsequently,we retrieve the value vectors corresponding to thesekeys. This similarity-based retrieval method formsthe foundational assumption of KV-MemNN andunderpins information retrieval. How this retrievalmethod can return the most relevant information?Through learning optimization based on neural net-work frameworks, we achieve this goal, which mo-tivates our training to optimize initial k and .",
  "A.2Style Representation and Retrieval": "We further elaborate on the motivation behind thisapproach.As discussed, learned knowledge isrepresented as a collection of weight increments{1, . . . , S}, where each i is associatedwith a key vector ki Rd (for i = 1, . . . , S).These key vectors approximate the centroids oftheir respective task distributions pi, forming key-value pairs [ki, i]. The optimization processaligns the task distributions pi with the correspond-ing ki and i. For example, given a datasetbased on Shakespeares works (X, Y), each inputxn X is transformed by BERT into a query vec-tor qshaken Rd. Our objective is to compute a keyvector kshake that maximizes its similarity with allqshaken, representing the semantic centroid of theShakespearean style.The pair [kshake, shake] represents the finalkey-value pair, where kshake encapsulates theShakespearean style at a semantic level, andshake reflects the style at the weight level. Thispairing is manually specified.During querying, TWIST identifies the mostrelevant pairs by calculating correlations betweenthe query and key vectors. The pre-trained modelis then re-parameterized using the correspondingvalue () as specified in Equation 5. Given aShakespearean sentence xtest1 and a sentence in anunrelated style xtest2, we transform them into queryvectors qtest1 and qtest2, respectively. After align-ing the centroids, the similarity between kshake andqtest1 will exceed the similarity between kshake andqtest2, as kshake captures the high-dimensional se-mantic features averaged across all Shakespeareanqueries qshake.In summary, we do not compute the similarity between the weight increment and the query vector,as they are fundamentally different representations.Instead, we compute the similarity between thekey vector and the query vector. The key vectorrepresents the averaged characteristics of queriesspecific to a training style, and when a query ex-hibits high similarity to a key vector, it suggests astrong correlation between the query and the style.",
  "CPrompt Design for Few-shot Methods": "The current closed-source LLMs demonstrate out-standing few-shot performance, but this depends oncarefully crafted prompt engineering. In ,we have designed a series of prompt schemes aimedat fully harnessing the potential of these large mod-els. We selected 100 sets of examples and usedthe classifier for style verification. The resultsare shown in . In the main text, we se-lected prompt 3 as the comparison baseline, whichachieved the highest style accuracy.",
  ": Additional Parameter Parts": "as shown in . We selected three distinctparts: the embedding layer, the attention layer, andthe feed-forward layer. In our ablation experiments,we adjusted the rank size as much as possible to en-sure that the number of trainable parameters acrossthe three groups remained consistent. shows that different parameter partsslightly impact performance. However, using the at-tention layer as the additional parameter was morestable overall. Moreover, while the feed-forwardlayer sometimes achieves higher style accuracy, ittends to reduce content preservation.",
  "D.4Different Combinations of r and q": "Referring to the work of (Dettmers et al., 2023),higher r values lead to a greater number of learn-able parameters and potentially improved perfor-mance. Through extensive experiments, we foundthat the performances are not sensitive to r (unlessvery low), whereas q tends to be more sensitive.Overall, we obtained some empirical guidelinesin the above . Larger or lower values of",
  "16.1 62.7 31.8 51.5 49.2 50.382.240.29 16.2 74.7 34.8 54.9 59.3 57.1164.480.58 17.7 78.5 37.3 55.0 60.1 57.5328.961.16 17.2 78.1 36.7 55.2 57.9 56.5": "42.240.29 16.1 63.2 31.9 50.5 48.1 49.384.480.58 16.5 75.4 35.3 54.8 58.4 56.6168.961.16 17.8 80.7 37.9 55.3 60.2 57.73217.922.32 17.5 78.3 37.0 55.1 58.2 56.66435.844.65 17.9 78.5 37.5 56.1 56.9 56.5 44.480.58 16.0 63.1 31.8 50.5 48.7 49.688.961.16 16.5 77.6 35.8 55.1 60.2 57.61617.922.32 17.9 81.0 38.1 55.0 60.9 57.93235.844.65 18.1 81.2 38.3 55.2 61.1 58.16471.689.30 17.8 78.7 37.4 56.1 58.9 57.5128143.3618.60 17.9 78.5 37.5 55.9 59.7 57.8",
  "EConsumption of Additional Calculation": "The additional overhead mainly consists of twoparts: first, the generation and retrieval key processrequires extra time; Second, the weight pool neces-sitates additional storage for these weights, whichconsumes memory resources. Time Analysis.First, we measure the time takento generate each key in , using the samesettings as in 4.2. The detailed results are shownin . Additionally, we measure the timerequired to retrieve the weight increments for thecorresponding tasks, as shown in .",
  "FAgreement Measure of Annotators": "We conducted preliminary experiments before se-lecting participants to assess the agreement mea-sure between annotators, and we will make thesedata publicly available. To evaluate the reliabil-ity and robustness of our score-comparison anno-tation scheme, we calculated the Inter-AnnotatorAgreement (IAA) using Krippendorffs (Art-stein, 2017). We tested the annotators on differentdatasets according to three metrics: style accuracy,content consistency, and sentence fluency, to mea-sure the IAA. The comparison method employedwas head-to-head (win-tie-lose), with each annota-tor completing comparisons for 100 instances. Theresults are as follows in .",
  "LayerCP ACCGCP ACCGCP ACCGCP ACCGCP ACCGCP ACCG": "Embedding56.3 74.2 64.6 57.9 73.2 65.1 59.2 76.8 67.4 56.7 74.1 64.8 54.5 69.8 61.7 56.4 64.7 60.4Feed Forward 56.8 75.0 65.3 57.6 74.9 65.7 58.7 79.3 68.2 57.2 73.8 65.0 53.2 70.9 61.4 56.2 69.9 62.7Attention57.2 75.1 65.5 58.6 74.8 66.2 60.3 78.5 68.8 58.5 74.5 66.0 55.7 70.6 62.7 56.8 70.4 63.2",
  "MethodsCP ACCGCP ACCGCP ACCGCP ACCGCP ACCGCP ACCG": "QLFT LLaMA2 53.5 68.7 60.6 56.9 68.2 62.3 53.2 61.2 57.1 54.8 60.2 57.4 54.5 61.8 58.0 56.2 63.8 59.9QLFT ChatGLM 44.3 67.7 54.7 46.9 66.8 56.0 44.7 61.8 52.6 46.5 61.8 53.6 45.5 64.3 54.1 47.7 63.9 55.2FS Alpaca39.2 71.5 52.9 41.3 70.5 54.0 40.2 65.9 51.5 43.2 67.8 54.1 41.2 71.4 54.2 43.7 68.7 54.8FS ChatGPT-442.3 79.3 57.9 44.7 75.4 58.1 45.2 80.2 60.2 39.6 73.2 53.8 40.9 75.9 55.7 45.0 73.5 57.5FS Claude-342.0 78.5 57.4 44.4 74.9 57.6 44.7 78.8 59.3 40.0 72.7 53.9 40.9 75.5 55.6 44.9 73.0 57.2",
  "BaselinePrompt": "Prompt 1System:[Role][Background][Personality]User:This is a dialogue attributed to the character [role]:[Golden Text 1][Golden Text 2][......][Golden Text n]In light of the [role]s background, distinctive personality traits, and other relevant information, coupledwith the given dialogue, you are tasked with precisely emulating the stylistic and tonal aspects of [role]sspeech. The text you are required to mimic is as follows: [Original Text]Please follow the requirements:[Requirements]Assistant:[Generated Text] Prompt 2User:There are two paragraphs from different characters. Rewrite the target paragraph according to thespeaking style, personality, mood, word usage and punctuation of the origin paragraph. Pay attention toimitating the speaking style of the first paragraph as much as possible and use different words. Here isan example.Original Paragraph:[Original Text 1]Target Paragraph:[Golden Text]The text you are required to rewrite is as follows: [Original Text 2]Please follow the requirements:[Requirements]Assistant:[Generated Text] Prompt 3Supervised History:System:[Role][Background][Personality]User:In light of the [role]s background, distinctive personality traits, and other relevant information,coupled with the given dialogue, you are tasked with precisely emulating the stylistic and tonal aspectsof [role]s speech. The text you are required to mimic is as follows: [Original Text]Please follow the requirements:[Requirements]User:[Original Text 1]Assistant:[Golden Text 1]User:[Original Text 2]Assistant:[Golden Text 2]......User:[Original Text n]Assistant:[Golden Text n]Generating:User:[Original Text]Assistant:[Generated Text]",
  "Use LoRA-based parameter efficient method to obtain source weight increments s = AsBs via Eq. 3": "Use singular value decomposition to approximate AsBs as UssVsInitialize weight key ks and calculate query via q = fBERT(xi)Use K-nearest neighbors algorithm to obtain the top-W most similar keys K = {k1 , . . . , kW }Update source weight key k K via k = k + k cos(q, k)Obtain key-value pair [ks ; s]endendUse spectral clustering algorithm to obtain cluster C = {C1, . . . , CC} and ns Cn"
}