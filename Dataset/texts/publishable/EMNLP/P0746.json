{
  "Abstract": "Language models strongly rely on frequencyinformation because they maximize the likeli-hood of tokens during pre-training. As a con-sequence, language models tend not to gen-eralize well to tokens that are seldom seenduring training.Moreover, maximum like-lihood training has been discovered to giverise to anisotropy: representations of tokensin a model tend to cluster tightly in a high-dimensional cone, rather than spreading outover their representational capacity. Our work introduces a method for quantifyingthe frequency bias of a language model by as-sessing sentence-level perplexity with respectto token-level frequency. We then present amethod for reducing the frequency bias of alanguage model by inducing a syntactic priorover token representations during pre-training.Our Syntactic Smoothing method adjuststhe maximum likelihood objective function todistribute the learning signal to syntacticallysimilar tokens. This approach results in bet-ter performance on infrequent English tokensand a decrease in anisotropy. We empiricallyshow that the degree of anisotropy in a modelcorrelates with its frequency bias.",
  "Introduction": "Humans possess a remarkable ability to quickly un-derstand the meaning of unknown words, givencontextual cues.Consider the sentence, theGolden Gate Bridge has been obnebulated everymorning this week, limiting visibility of the Pa-cific Ocean. For many readers, obnebulated isprobably not a familiar term, but we are likelyto infer that 1) it is almost certainly a verb be-cause of the -ed suffix and occurrence after per-fective and passive auxiliaries, and 2) its meaningrelates to visibility and climatic conditions.1 The",
  ": Illustration of the BLiMP frequency biascalculation used to evaluate a models reliance on fre-quency statistics when making predictions. The exam-ple BLiMP values are from a baseline RoBERTa model": "ability to integrate unknown words based on syn-tactic and semantic context is essential for robustlanguage understanding and still poses a signifi-cant challenge for language models. Nevertheless,Pre-trained Transformer Language Models (PLMs)have proven tremendously capable of solving awide array of language processing tasks (Touvronet al., 2023; Chowdhery et al., 2023).Part of the success of PLMs can be attributedto the pre-training objective. Despite variations inarchitecture, the vast majority of language mod-els are pre-trained to maximize the log-likelihoodof a word, given the surrounding context (Devlinet al., 2019; Brown et al., 2020; Chowdhery et al.,2023; Touvron et al., 2023). As language use ischaracterized by a Zipfian distribution (Zipf, 1935),language models are exposed to frequent tokensexponentially more often than infrequent ones dur-ing pre-training. Consequently, the representationsof these frequent tokens are optimized based on exponentially more learning signals than those oflow-frequency tokens. It has been shown that max-imum likelihood objectives lead to representationdegeneration in English language models becauseinfrequent tokens are pushed into a narrow mani-fold of the representational space (Gao et al., 2019).This representation degeneration problem is linkedto the broader problem of anisotropy: the hiddenstates of a language model tend to cluster togetherinto a small cone-shaped subspace, rather than overtheir full representational capacity (Arora et al.,2016a; Ethayarajh, 2019; Gao et al., 2019). Aslanguage model evaluation is based on cumulativeevaluation scores that conceal how well a modelprocesses infrequent words, the disparities in therepresentational space are difficult to assess.Conventional language modeling approaches re-quire large model sizes to effectively capture long-tail vocabulary distributions, limiting the scala-bility of these methods (Feldman, 2020; Havivet al., 2023). In this work, we propose SyntacticSmoothing: a syntactically-guided label smooth-ing approach to improve the representation of infre-quent tokens in language models without resortingto perpetual increases of model and training datasize. We smoothly distribute the backpropagationsignal over syntactically similar tokens using a sim-ilarity metric based on part-of-speech (POS) tagdistributions. Using this method, tokens that areseldom seen during training benefit from the morefrequent updates of tokens that occur in similarsyntactic functions. We evaluate our method usinga new metric for quantifying the frequency biasof language models (illustrated in fig. 1) and findthat Syntactic Smoothing reduces both the fre-quency bias and the degree of anisotropy in a smallEnglish language model. We further explore therelationship between anisotropy and frequency biasand their effect on downstream performance.",
  "Related Literature": "Through maximum likelihood training, languagemodels implicitly learn to encode token frequencystatistics. This training process gives rise to a fre-quency bias in models that constrains their abilityto generalize to infrequent tokens. In this section,we begin by reviewing literature that discusses thechallenges of generalizing linguistic knowledgeto infrequent tokens.We then examine recentwork that links the impact of token frequency toanisotropy in the models representational space.",
  "Generalization to Infrequent Tokens": "Current approaches to language modeling relyheavily on the memorization of infrequent tokensto perform well on downstream tasks (Feldman,2020).Recent analytical work has shown thatcertain layers of transformer models implicitlystore memorized long-tail data (Haviv et al., 2023;Kobayashi et al., 2023). Feldman and Zhang (2020)demonstrate that models memorize atypical ex-amples to achieve the highest accuracy on long-tailed data samples. This memorization hack, how-ever, has only been shown to work well with over-parameterized models (Belkin et al., 2019). Whilethese studies present various metrics to evaluatememorization, these metrics do not capture howmemorization impacts generalized linguistic un-derstanding within the models. In our work, weaddress this gap by developing a metric that quanti-fies the extent of this frequency bias in relation tomodels linguistic abilities.Language use follows a Zipfian distribution,meaning that many tokens appear infrequently.Standard training objectives often require largemodels and noisy datasets with sufficient long-tail samples for effective generalization (Zhengand Jiang, 2022). However, improving general-ization without excessive scaling can be achievedby training models with inductive priors that lever-age linguistic information. On the lexical level, theintegration of morphological and orthographic in-formation during representation learning has beenexplored to obtain more fine-grained word embed-dings (Salle and Villavicencio, 2018; Vulic et al.,2017; Cotterell and Schtze, 2015; Bhatia et al.,2016; Botha and Blunsom, 2014). To improvesyntactic generalization, the objective function hasbeen enriched with auxiliary tasks, such as predict-ing constituency labels (Wang et al., 2023), hyper-nyms (Bai et al., 2022), dependency tags (Cui et al.,2022), and POS tags (Diehl Martinez et al., 2023).Some approaches have also shown promising re-sults on rare word performance by constructingtoken embeddings that consider a words surfaceform and surrounding context (Schick and Schtze,2019, 2020).",
  "While frequency bias and generalization capabili-ties can be observed by analyzing model behavioron inputoutput patterns, representational analy-ses indicate that these phenomena are linked to the": "distribution of token representations. Languagemodels trained as likelihood maximizers have beenshown to yield degenerate representations for raretokens (Gao et al., 2019). Throughout training, in-frequent tokens are disproportionately pushed inthe negative direction of most hidden states, result-ing in their clustering together irrespective of theirsemantic or syntactic properties. This clusteringbehavior leads to anisotropy: rather than occupyinga large region of the representational space, tokenrepresentations lie along a narrow manifold (Gaoet al., 2019; Ethayarajh, 2019). 2.2.1Defining AnisotropyAnisotropy is defined as the inverse of isotropy:1 I(v()). A representational space is isotropicif all the vector directions are distributed uniformly,meaning no particular direction is favored over an-other.",
  "I(v()) Eij(1 cos(v(wi), v(wj)))(2)": "Here, wi and wj are two tokens sampled from thevocabulary, and cos is defined as taking the cosinesimilarity of the two word representations for wiand wj.Despite its prevalence, the impact of anisotropyon a models language understanding abilities re-mains unclear.Some studies suggest that re-ducing anisotropy improves performance on non-contextual benchmarks, sentence comparison tasks,and multilingual benchmarks (Bis et al., 2021;Su et al., 2021; Rajaee and Pilehvar, 2022).Conversely, other research indicates that higheranisotropy might enhance semantic clustering tasksand that reducing anisotropy does not uniformlyimprove performance on common NLU tasks (Ait-Saada and Nadif, 2023; Ding et al., 2022). Fur-thermore, the relationship between anisotropy and maximum likelihood training has been questioned.Some researchers argue that isotropy exists in localmanifolds of contextual word representations (Caiet al., 2020), while others contend that anisotropyarises from the learning dynamics of the queryand key attention matrices in transformer models(Godey et al., 2024). 2.2.2Reducing AnisotropyExisting methods to reduce anisotropy broadly fallinto three categories. The first group of approachestransforms the hidden states of language modelsto remove semantically uninformative directionsand to preserve the dimensions of maximal isotropy(Arora et al., 2016b; Mu and Viswanath, 2018; Rau-nak et al., 2019; Su et al., 2021; Bis et al., 2021).This intervention style is based on the assump-tion that the top singular dimensions of pre-trainedword representations encode frequency statisticsrather than semantic or lexical information (Mu andViswanath, 2018). The second category of methodsintroduces novel training objectives and regular-ization terms that reduce the effects of anisotropy(Gong et al., 2018; Gao et al., 2019; Wang et al.,2019). This type of approach places an inductivebias on representations that push the embeddingsof frequent and infrequent words to occupy a sim-ilar semantic space. The third set of approachesexplores different training paradigms to directlyminimize anisotropy, such as using normalizingflow models (Li et al., 2020) or manipulating thegradients used in maximum likelihood models (Yuet al., 2022) While frequency bias and anisotropy are preva-lent in language modeling, quantifying their effectsand understanding their impact on generalization,particularly for infrequent words, remains an openarea of research. Our paper introduces a novelmethod for improving the representation of infre-quent tokens by integrating linguistic information.Moreover, we hypothesize that adjusting the learn-ing process to better represent infrequent tokenswill also reduce anisotropy, as these two phenom-ena are interconnected.",
  "Frequency Bias": "We investigate frequency effects using a zero-shot test of grammatical capability known asBLiMP: The Benchmark of Linguistic MinimalPairs (Warstadt et al., 2020). BLiMP comprises 67datasets (or subtasks), each consisting of 1,000 pairs of grammatical and ungrammatical sentencesthat differ only with respect to a specific linguisticcharacteristic (covering syntax, morphology, andsemantics). Language models are tasked with as-signing a higher likelihood to the grammatical sen-tence. The grammatical generalization capabilitiesof a language model are often summarized by aver-aging the accuracies achieved across the 67 BLiMPtasks. While random guessing scores 0.5, state-of-the-art models have achieved scores of 0.87 whentrained on large datasets, and models trained on the10M-word BabyLM dataset have achieved scoresup to 0.80 (Warstadt et al., 2023).BLiMP is carefully balanced to ensure individ-ual tokens occur equally in both sentence types.However, within a single pair, there may be an im-balance in average token frequency: For instance,the sentence Graces piano teachers are known hasa log frequency of 8.35 while its associated mini-mal pair Graces piano teachers are replied has alog frequency of 6.20. We hypothesize that despitethe minimal difference in BLiMP pairs, modelstrained in a typical manner will be biased by tokenfrequency when determining grammatical accept-ability. Our goal is to quantify how language model per-formance differs between BLIMP pairs with largepositive frequency differences (where the correctsentence has more frequently occurring tokens) andwith large negative frequency differences (wherethe correct sentence has much less frequently oc-curring tokens). We do so in two steps.First, for each BLIMP sentence pair, we cal-culate the average (natural log) frequency of thediffering tokens. Frequencies of individual tokensare computed with respect to a models trainingdata; for instance, in the example above the tokenknown has a log frequency of 8.35 in the trainingdata. Sentence pairs are then ranked by the relativedifference in these average frequencies, where pos-itive values indicate a higher average frequency forthe acceptable sentence. These relative differencesform a distribution, as shown in the middle plot offig. 1.Then, we compute the BLiMP score usingpseudo log-likelihood (Salazar et al., 2020) forBLIMP pairs in the upper and lower thirds of therelative frequency difference distribution. We ex-clude the middle third, as these represent pairs withminimal frequency differences (see the frequencyplot for details). We define a models frequency bias as the difference between the two BLiMPscores. The entire process is illustrated in fig. 1.In practice, we find that standard transformerlanguage models, such as OPT-125M (Zhang et al.,2022), RoBERTa-base (Liu et al., 2019), and T5-base (Raffel et al., 2020), exhibit a frequency biasas high as 13.7%. Our goal is to develop a modelthat can attain a frequency bias close to zero whileattaining a high BLiMP score: that is, a modelthat makes determinations on the grammatical ac-ceptability of sentences based solely on relevantlinguistic aspects, rather than relying on possiblymisleading statistical artifacts of the training data.",
  "Syntactic Smoothing": "We hypothesize that transformer language modelsexhibit a strong frequency bias due to their max-imum likelihood training objective, which limitsinfrequent tokens from receiving useful learningsignals and thus hinders their ability to effectivelyencode linguistic information. To address this, wepropose at each learning step to backpropagate thelearning signal of a target token to all other tokensserving similar syntactic roles; this benefits infre-quent tokens that appear less often in the trainingdata.Syntactic Smoothing implements this strat-egy by distributing a portion of every update signalto all syntactically similar tokens using a syntac-tic similarity metric (operationalized below). Thisresults in the representation of infrequent tokens ap-proaching the average representation of all tokensthat serve a similar syntactic function; e.g., therepresentation of a niche word like obnebulatedwould encode its syntactic role as a verb.Our method consists of two components; (1) asimilarity metric that uses part-of-speech distribu-tions as a coarse proxy for syntactic similarity, and(2) an adjustment to the loss function to smooth thebackpropagation signal over syntactically similartokens during pre-training.",
  "Syntactic Similarity Score": "The syntactic similarity between two tokens can bemeasured in multiple ways, e.g., by using surfacefeatures, dependency labels, or even the predictionsof a teacher language model (Hinton et al., 2015).Here, we present a simple measure that acts as acoarse approximation for syntactic similarity: weconsider two tokens to be similar if they have asimilar distribution of part-of-speech tags in the training set.We evaluate the syntactic similarity between to-kens prior to training, as a one-off preprocessingstep over the entire training set. First, we use thepart-of-speech (POS) tagger from the NLTK pack-age (Bird et al., 2009) to assign each word in thetraining set to one of 12 universal POS tags, basedon its given context (Petrov et al., 2012).2 We thentokenize the training data into sub-word tokens andassign each token the POS tag corresponding tothe word it belongs to in each instance. As wordscan take on a different part of speech dependingon the context, we count the number of times eachtoken in our vocabulary V appears as each POS tagin the training data, producing a 12-valued vector.This results in a matrix M RV 12 containingthe distribution over POS tags for each token. Fi-nally, we can compute the similarity of two tokensVi and Vj using the cosine similarity of their POSdistributions:",
  "Mi Mj": "Note that while in this paper we define syntac-tic similarity via cosine similarity, any real-valueddistance metric or divergence can be used. Thesimilarity function does not need to be symmetric,although we note that symmetric functions providecomputational advantages as only half the valuesneed to be computed and stored. Also, note that ourmethodology does not depend on a specific choiceof POS tagger.We provide the POS distributions and similaritydistributions for the example tokens blind andthe in fig. 2. Notice that the occurs almostexclusively as a determiner and is not similar tomany other tokens, whereas blind occurs as anoun, verb, adjective, and adverb and has a highsimilarity to more than half the other tokens in thevocabulary.",
  "V k=0 s(i,k) otherwise(3)": "where , the smoothing parameter, determines theproportion of the error signal reserved for the cor-rect word and s is our part-of-speech similaritymetric. We experiment with different values for ,noting that = 0 is the standard likelihood max-imization task. We also investigate the use of apacing function that linearly decreases so thatat the start of training the majority of the signalis propagated to other syntactically similar tokensand by the end of training nearly all of the errorsignal is sent to the correct token to ensure that themodel still optimizes perplexity.In practice, we also find it beneficial to apply atemperature scaling function to the syntactic simi-larity distribution. Thus, rather than using the rawsyntactic similarity scores, s(i, j), in eq. (3), weuse the temperature-scaled similarity scores:",
  "Experimental Setup": "Our experiments focus on smaller language modelsand datasets due to computational constraints andthe particular challenges of generalizing to uncom-mon instances under resource-constrained trainingconditions (Warstadt et al., 2023; Diehl Martinezet al., 2023). DataWe use the dataset published as train-ing data for the BabyLM challenge at the 2023CoNLL workshop (Warstadt et al., 2023). It con-tains roughly 10 million tokens sampled from pre-existing datasets, covering a wide range of domainsincluding transcribed speech (both adult-directedand child-directed), movie subtitles, Wikipedia ar-ticles, and books. The dataset was constructed tobe similar to the input received by children 56%comes from transcribed speech and 40% comesfrom sources intended for children. ModelWe use a small 8-layer encoder-styleRoBERTa model with pre-layer normalization(Huebner et al., 2021).We report the hyper-parameter settings we use throughout all experi-ments in table 3 (appendix A) and computationalrequirements in appendix B. We use a BPE tok-enizer (Sennrich et al., 2016) with a vocabularysize of 8192 as recommended in previous work(Diehl Martinez et al., 2023). EvaluationWe evaluate the BLiMP frequencybias of our models, as defined in section 3, on theevaluation set of BLiMP. To compute anisotropywe use the formulation defined in eq. (2); We sam-ple 1,000 pairs of random word tokens with theirsurrounding context from the training set, and com-pute the cosine similarity of their hidden repre-sentation at each of the 8 layers of the RoBERTamodel. To obtain a models final anisotropy value,we average the anisotropy scores across the 8 lay-ers. Additionally, we finetune and evaluate eachmodel on two downstream sentence-level tasks,COLA (Warstadt et al., 2019) and SST-2 (Socheret al., 2013), as well as two language inferencetasks, MNLI (Williams et al., 2018) and QNLI (Ra-jpurkar et al., 2016; Wang et al., 2018).",
  ". Label Smoothing: The base model trainedwith label smoothing (Szegedy et al., 2016)": "We train a baseline with a low-level of smooth-ing ( = 0.2) and a mid-level of smoothing( = 0.5). Note that Syntactic Smoothingcan be seen as a linguistically-guided versionof the standard label smoothing approach, inwhich the learning signal is distributed to alltokens uniformly. OurModelsWetrainourmodelswithSyntacticSmoothing using the same two values as the label smoothing baselines to facilitatecomparison. We also run variants using the linearpacing function presented in section 4.2 whichlinearly decreases the smoothing from an initialvalue of to zero across training.For thesevariants, we use the same two values of smoothing,as well as an additional high value of = 0.8giving a total of five SyntacticSmoothingvariants.3",
  "Results": "Our results are summarized in table 1. We find thatour method reduces frequency bias while retain-ing strong language modeling capabilities. At thesame time, we observe that the models with thelowest frequency bias also demonstrate the lowestanisotropy. We then extend our analysis beyondthe specific phenomenon of frequency bias andanisotropy by examining the impact of SyntacticSmoothing on the linguistic generalization capabil-ities of the model and its downstream performanceafter finetuning. Finally, we find that an alternativesyntactic scoring metric leads to similar results asthe cosine-based definition.",
  "We conduct analyses to inspect the learning dynam-ics of our method and its effect on frequency biasand anisotropy in more detail": "Syntactic Smoothing reduces frequency bias.We find that all four pre-trained models exhibitstrong frequency bias (see fig. 3); they are morelikely to incorrectly prefer ungrammatical sen-tences if they contain tokens that occur more fre-quently during training. This confirms our hypothe-sis that the evaluation of generalization capabilitiesis obfuscated by frequency effects. 3We do not include unpaced Syntactic Smoothing witha high value of as initial experiments found that distributingsuch a high proportion of the learning signal away from thecorrect token leads to high perplexity and poor downstreamperformance.",
  "Low2.939.773.270.784.969.779.2Mid-0.233.872.171.983.567.279.4Paced Low7.439.971.970.585.270.080.4Paced Mid5.734.572.371.884.068.278.9Paced High5.231.072.270.583.767.779.1": ": We report bias (), anisotropy (), BLiMP () score, and accuracy or correlation scores () on twodownstream sentence-level tasks COLA and SST-2 and two downstream language inference tasks MNLI andQNLI for our MLM baseline, two label smoothing (LS) baselines, and five Syntactic Smoothing (SyS) variants.SyS-P variants use linear pacing to reduce the smoothing factor to zero over training.",
  ": Frequency bias plotted for the three opensource pre-trained models, our base model, the twolabel smoothing (LS) baselines and our two SyntacticSmoothing (SyS) models": "By contrast, the two SyntacticSmoothingvariants successfully reduce the frequency bias.The frequency bias is almost completely removedin the case of the Mid variant, which distributesexactly half of the training signal to syntacti-cally similar tokens.We further observe thatthe Label Smoothing baselines also reduce biasbut to a lesser extent than the correspondingSyntactic Smoothing models with the same de-gree of smoothing. : Anisotropy learning dynamics plotted for thebaseline RoBERTa model, the two label smoothing (LS)baselines and our Syntactic Smoothing (SyS) models.Values in parentheses indicate the degree of smoothing. Syntactic Smoothing reduces anisotropy.Asshown in table 1, Syntactic Smoothing reducesanisotropy over both the base model and labelsmoothing baselines.4 Label smoothing reducesanisotropy, but not to the same extent as ourSyntactic Smoothing models. To better under-stand how anisotropy develops in a model, wecompute the models anisotropy scores at eightcheckpoints during training, as shown in fig. 4. Wefind that a greater degree of smoothing leads to agreater reduction in anisotropy for our SyntacticSmoothing variants (it is less clear if this is thecase for label smoothing), supporting our hypothe-sis that syntactic initialization helps promote betterrepresentation learning across the models vocabu-lary. We also find that the pacing method leads tolower anisotropy than the flat method, with SyS-P(High) achieving the lowest anisotropy throughout.Over the course of training, we observe a con-sistent double-dip trend: an initial dip followedby a sudden rise, followed by a second slow de-crease in anisotropy. The Syntactic Smoothingmodels do not see as large a sudden rise, maintain-ing a lower anisotropy throughout. To examinethe learning dynamics in more detail, we also plotthe evolution of the anisotropy across several lay-ers of our baseline model and the SyS-P (High)variant, given in fig. 5. Two observations standout. The anisotropy of all layers in the SyntacticSmoothing model is lower than in the correspond-ing layers in the baseline model across the entirelearning process. In both the baseline model andthe Syntactic Smoothing model, earlier layershave lower anisotropy; this finding agrees with the 4Note that we do not compute the anisotropy for the threeopen-source pre-trained models (OPT, RoBERTa, T5) becausethese models use different architectural configurations thanthe models we train (e.g., larger hidden dimensions). same observation made by Ethayarajh. Notably, inthe final layercommonly used for sentence rep-resentations in downstream tasksthe anisotropyof the Syntactic Smoothing model remains con-sistently low and does not increase significantlyduring training, in contrast to the drastic fluctua-tion observed in the baseline model. : Anisotropy learning dynamics plotted for thebaseline model and the paced Syntactic Smoothingmodel with high smoothing, across some of the modelslayers. We highlight the difference in anisotropy of thefinal layer across the two models at the end of training. Frequency bias and anisotropy are correlated.For each model, we compute the models frequencybias and anisotropy at multiple training stages. Weplot the learning dynamics of anisotropy and fre-quency bias in fig. 6, only including the points after50% of training has been completed to avoid thenoisy first dip observed in the anisotropy dynamicsabove. We find a positive Pearson correlation of0.73 and a polynomial goodness-of-fit R2 score of0.63 between these two metrics.It is also evident that the pacing approach re-introduces frequency bias towards the end of train-ing, as the degree of smoothing is linearly reducedto zero. It is noteworthy that the final anisotropyand bias are lower than the baseline model, andcompleting training without any smoothing may bebeneficial for downstream tasks, as explored in thenext section.",
  "Effects of Smoothing on DownstreamTasks": "While our method primarily aims to enhance therepresentation of infrequent tokens, we sought toinvestigate the potential for improvement in stan-dard evaluation measures, given the limited num-ber of affected test instances. Nonetheless, weobserve that all the Syntactic Smoothing mod-els, as well as the label smoothing models, achievebetter BLIMP scores than our baseline model (see : Pairs of anisotropy, and frequency bias forthe baseline RoBERTa model, the two label smoothingbaselines and our Syntactic Smoothing models. Thearrows indicate increasing training progress (startingafter 50% of training has completed). table 1). These results suggest that methods thatsmooth label distributions, whether through a syn-tactic prior or a simpler uniform smoothing ap-proach, enhance the representation of all tokens,including the more frequent ones.We had concerns that softening the frequencybias with our method might lead to degraded per-formance in downstream tasks for which frequencycan be a strong proxy. As a control condition,we finetune our model on two sentence-level tasks(COLA, SST-2) and two language inference tasks(MNLI and QNLI), both of which are part of theGLUE (Wang et al., 2018) benchmark. We findthat none of the Syntactic Smoothing objectivesresult in substantial performance degradation onthese NLU tasks (see the last four columns of Ta-ble table 1), and in fact note that for some tasks,such as SST-2, the Syntactic Smoothing modelsyield uniform increases in performance.5",
  ":Results for bias (), anisotropy (), andBLiMP () score for Syntactic Smoothing (SyS) mod-els that use a Jensen Shannon-based [JS] definition ofthe similarity metric": "where KL(Mi, Mj) is the Kullback-Leibler diver-gence between the POS distributions, Mi and Mj,for the vocabulary items Vi and Vj.Summarized in table 2, we note that the effectof using a Jensen Shannon-based definition of thesimilarity metric yields a similar (albeit slightlysmaller) decrease in frequency bias and anisotropy,as compared to the standard cosine-based definitionof the similarity metric.",
  "Conclusion": "Our work studies the phenomenon of frequencybias in language models that degrades the perfor-mance of these models on tokens infrequently ob-served during training. We develop a novel methodfor quantifying the degree to which a languagemodel prefers grammatically incorrect sentencesthat contain frequent tokens over grammaticallycorrect sentences containing infrequent tokens. Weintroduce a new training approach, SyntacticSmoothing, that distributes the backpropagationsignal to syntactically similar tokens.Using acoarse approximation of syntactic similarity basedon part-of-speech tags, we show that this approachcan remove the frequency bias without degradingbroader language understanding. We also find thatreductions in frequency bias are strongly correlatedwith reductions in a models anisotropy. Our find-ings provide a novel angle through which to ob-serve the role of anisotropy in language modeling.",
  "Ethical Impact": "Studying long-tail data comes with some knownethical concerns. Previous research has found thatnames of female and non-white persons tend tofall in the long-tail of many datasets which can re-sult in less efficient neural representations of thesenames compared to names of male and white per-sons(Wolfe and Caliskan, 2021). Our paper does not directly study whether the methods we developaffect these implicit biases, although we would sus-pect that our approach might help remove some ofthese biases (without further experimentation this,however, remains a risk of our work).Along similar lines, we also do not conduct athorough analysis to determine whether the curatedBabyLM training set we use contains offensivedata or uniquely identifies individuals.For anoverview of the pre-processing steps that were doneto remove harmful data from the BabyLM corpora,we refer the reader to the BabyLM proceedings(Warstadt et al., 2023).We also note that the use of large-scale black-boxLLMs makes studying infrequent token representa-tions and their downstream effects more difficult.Our use of smaller LMs helps increase transparencyand facilitates the reproducibility of our method byresearch groups with small computational budgets.",
  "Limitations": "Our methods use English-only data, and thus as-sume an English-centric notion of word functions.For the syntactic information, we use the POS tagsprovided by the NLTK tagger. As this tagger wastrained on a separate dataset, this may suggest ourmethod relies on additional data in order to bestrepresent infrequent words. However, in initial ex-periments with an unsupervised tagger trained onlyon the 10M-word dataset, we achieved similar re-sults. Additionally, the models we experiment withare all relatively small and, while we assume thatour results can be scaled up to larger architectures,our limited computational resources do not allowus to collect empirical evidence. In future work, weplan to further explore the impact of SyntacticSmoothing on models with autoregressive archi-tectures and larger training datasets. We also hopefuture work will apply our method to more lan-guages, possibly leveraging unsupervised POS tag-gers for these languages, and evaluate the effect ofSyntactic Smoothing on different downstreamtasks (particularly tasks with irregular vocabularyfrequency distributions).",
  "Acknowledgements": "The experiments reported in this paper were per-formed using resources provided by the CambridgeService for Data Driven Discovery (CSD3) op-erated by the University of Cambridge ResearchComputing Service, provided by Dell EMC and Intel using Tier-2 funding from the Engineeringand Physical Sciences Research Council (capitalgrant EP/T022159/1), and DiRAC funding fromthe Science and Technology Facilities Council.Richard Diehl Martinez is supported by the GatesCambridge Trust (grant OPP1144 from the Bill& Melinda Gates Foundation). Zbulon Gorielyswork is supported by The Cambridge Trust. LisaBeinborns work is partially supported by the DutchNational Science Organisation (NWO) throughthe VENI program (Vl.Veni.211C.039). AndrewCaines and Paula Buttery are supported by Cam-bridge University Press & Assessment. Mira Ait-Saada and Mohamed Nadif. 2023.Isanisotropy truly harmful? a case study on text cluster-ing. In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 11941203, Toronto, Canada.Association for Computational Linguistics. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,and Andrej Risteski. 2016a. A latent variable modelapproach to PMI-based word embeddings. Transac-tions of the Association for Computational Linguis-tics, 4:385399.",
  "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016b.A simple but tough-to-beat baseline for sentence em-beddings. In International Conference on LearningRepresentations": "He Bai, Tong Wang, Alessandro Sordoni, and Peng Shi.2022. Better language model with hypernym classprediction. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 13521362, Dublin,Ireland. Association for Computational Linguistics. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and SoumikMandal. 2019.Reconciling modern machine-learning practice and the classical biasvariancetrade-off. Proceedings of the National Academy ofSciences, 116(32):1584915854. Parminder Bhatia, Robert Guthrie, and Jacob Eisenstein.2016. Morphological priors for probabilistic neuralword embeddings. In Proceedings of the 2016 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 490500, Austin, Texas. Associa-tion for Computational Linguistics.",
  "Xingyu Cai, Jiaji Huang, Yuchen Bian, and KennethChurch. 2020. Isotropy in the contextual embeddingspace: Clusters and manifolds. In International Con-ference on Learning Representations": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Ryan Cotterell and Hinrich Schtze. 2015. Morpholog-ical word-embeddings. In Proceedings of the 2015Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 12871292, Denver,Colorado. Association for Computational Linguis-tics.",
  "Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.2022. Lert: A linguistically-motivated pre-trainedlanguage model. arXiv preprint arXiv:2211.05344": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Richard Diehl Martinez, Hope McGovern, ZebulonGoriely, Christopher Davis, Andrew Caines, PaulaButtery, and Lisa Beinborn. 2023. CLIMB curricu-lum learning for infant-inspired model building. InProceedings of the BabyLM Challenge at the 27thConference on Computational Natural LanguageLearning, pages 8499, Singapore. Association forComputational Linguistics.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.Distilling the knowledge in a neural network. arXivpreprint arXiv:1503.02531": "Philip A. Huebner, Elior Sulem, Fisher Cynthia, andDan Roth. 2021. BabyBERTa: Learning more gram-mar with small-scale child-directed language. In Pro-ceedings of the 25th Conference on ComputationalNatural Language Learning, pages 624646, Online.Association for Computational Linguistics. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, andKentaro Inui. 2023. Transformer language modelshandle word frequency in prediction head. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 45234535, Toronto, Canada.Association for Computational Linguistics. Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang,Yiming Yang, and Lei Li. 2020. On the sentenceembeddings from pre-trained language models. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 91199130. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A robustly optimized BERT pretrainingapproach. arXiv preprint arXiv:1907.11692.",
  "Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012": "A universal part-of-speech tagset. In Proceedingsof the Eighth International Conference on LanguageResources and Evaluation (LREC12), pages 20892096, Istanbul, Turkey. European Language Re-sources Association (ELRA). Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551. Sara Rajaee and Mohammad Taher Pilehvar. 2022. Anisotropy analysis in the multilingual BERT embed-ding space. In Findings of the Association for Com-putational Linguistics: ACL 2022, pages 13091316,Dublin, Ireland. Association for Computational Lin-guistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ questions formachine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392, Austin,Texas. Association for Computational Linguistics.",
  "Vikas Raunak, Vivek Gupta, and Florian Metze. 2019": "Effective dimensionality reduction for word embed-dings. In Proceedings of the 4th Workshop on Rep-resentation Learning for NLP (RepL4NLP-2019),pages 235243, Florence, Italy. Association for Com-putational Linguistics. Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-trin Kirchhoff. 2020. Masked language model scor-ing. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages26992712, Online. Association for ComputationalLinguistics. Alexandre Salle and Aline Villavicencio. 2018. Incor-porating subword information into matrix factoriza-tion word embeddings. In Proceedings of the Sec-ond Workshop on Subword/Character LEvel Models,pages 6671, New Orleans. Association for Compu-tational Linguistics.",
  "Timo Schick and Hinrich Schtze. 2019. Attentive mim-icking: Better word embeddings by attending to infor-mative contexts. arXiv preprint arXiv:1904.01617": "Timo Schick and Hinrich Schtze. 2020. Rare words:A major problem for contextualized embeddings andhow to fix it by attentive mimicking. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 34, pages 87668774. Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words withsubword units. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 17151725,Berlin, Germany. Association for Computational Lin-guistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics.",
  "Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.2021. Whitening sentence representations for bet-ter semantics and faster retrieval.arXiv preprintarXiv:2103.15316": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,Jon Shlens, and Zbigniew Wojna. 2016. Rethinkingthe inception architecture for computer vision. InProceedings of the IEEE conference on computervision and pattern recognition, pages 28182826. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ivan Vulic, Nikola Mrkic, Roi Reichart, Diarmuid Saghdha, Steve Young, and Anna Korhonen. 2017.Morph-fitting: Fine-tuning word vector spaces withsimple language-specific rules. In Proceedings of the55th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages5668, Vancouver, Canada. Association for Compu-tational Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel Bowman. 2018. GLUE:A multi-task benchmark and analysis platform for nat-ural language understanding. In Proceedings of the2018 EMNLP Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP, pages353355, Brussels, Belgium. Association for Com-putational Linguistics.",
  "Language model pre-training with linguistically mo-tivated curriculum learning": "Alex Warstadt, Aaron Mueller, Leshem Choshen, EthanWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-quera, Bhargavi Paranjabe, Adina Williams, TalLinzen, and Ryan Cotterell. 2023. Findings of theBabyLM challenge: Sample-efficient pretraining ondevelopmentally plausible corpora. In Proceedingsof the BabyLM Challenge at the 27th Conference onComputational Natural Language Learning, pages134, Singapore. Association for Computational Lin-guistics. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.Bowman. 2020. BLiMP: The benchmark of linguis-tic minimal pairs for English. Transactions of theAssociation for Computational Linguistics, 8:377392.",
  "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-man. 2019. Neural network acceptability judgments.Transactions of the Association for ComputationalLinguistics, 7:625641": "Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume1 (Long Papers), pages 11121122, New Orleans,Louisiana. Association for Computational Linguis-tics. Robert Wolfe and Aylin Caliskan. 2021. Low frequencynames exhibit bias and overfitting in contextualizinglanguage models. In Proceedings of the 2021 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 518532, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon. 2022.Rare tokens degenerate all tokens: Improving neuraltext generation via adaptive gradient gating for raretoken embeddings. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 2945,Dublin, Ireland. Association for Computational Lin-guistics. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "BComputational Requirements": "We purposefully train a small-scale LM for ourexperiments. The total amount of the trainableparameters in our model is 12,750,336. Each ofour experiments trains for approximately 14-20GPU hours, using a server with one NVIDIA A10080GB PCIe GPU, 32 CPUs, and 32 GB of RAMfor all experiments. Below, we report a subset ofthe output of the lscpu command: Architecture:x86_64CPU op-mode(s):32-bit, 64-bitAddress sizes:46 bits physical,48 bits virtualByte Order:Little EndianCPU(s):32On-line CPU(s) list: 0-31Vendor ID:GenuineIntelModel name:Intel(R) Xeon(R)Silver 4210R CPU@ 2.40GHzCPU family:6Model:85Thread(s) per core:1Core(s) per socket:1Socket(s):8Stepping:7BogoMIPS:4800.11",
  "CWord Class Versus Word FrequencyAnalysis": "Broadly, we find that content words, primarilynouns, are over-represented in low-frequency to-kens. We moreover, find that the syntactic distribu-tion across POS tags changes considerably whencomparing the top 100 and bottom 100 most andleast frequently occurring tokens. This analysissuggests that poor performance on infrequent to-kens has a particularly strong effect on a modelsinability to correctly model specialized noun vo-cabulary items.",
  "DBLiMP Data Filtering": "We filter the BLiMP data to only focus on pairsof sentences where one set of tokens has been re-placed by another set and ignore sentence pairsthat only differ in the order of tokens. We alsoremove pairs where tokens have only been addedto one sentence, rather than replaced. This filteringonly removes 15% of BLiMP pairs and 9 of the 67subtasks from consideration."
}