{
  "Abstract": "Large Vision-Language Models (LVLMs) haveshown remarkable performance on many visual-language tasks. However, these models stillsuffer from multimodal hallucination, whichmeans the generation of objects or content thatviolates the images. Many existing work de-tects hallucination by directly judging whetheran object exists in an image, overlooking theassociation between the object and semantics.To address this issue, we propose Hierarchi-cal Feedback Learning with Vision-enhancedPenalty Decoding (HELPD). This frameworkincorporates hallucination feedback at both ob-ject and sentence semantic levels. Remarkably,even with a marginal degree of training, this ap-proach can alleviate over 15% of hallucination.Simultaneously, HELPD penalizes the outputlogits according to the image attention windowto avoid being overly affected by generated text.HELPD can be seamlessly integrated with anyLVLMs. Our experiments demonstrate that theproposed framework yields favorable resultsacross multiple hallucination benchmarks. Iteffectively mitigates hallucination for differentLVLMs and concurrently improves their textgeneration quality. 1",
  "Introduction": "Large Language Models (LLMs) (Brown et al.,2020; OpenAI, 2023; Touvron et al., 2023a,b),guided by human instruction, have demonstratedimpressive performance in numerous Natural Lan-guage Processing (NLP) tasks (Qin et al., 2023).In light of the success of LLMs, researchers as-pire to integrate the powerful capabilities of LLMsinto multimodal domains, consequently introduc-ing Large Vision-Language Models (LVLMs) (Liet al., 2023a; Dai et al., 2023; Zhu et al., 2023;Ye et al., 2023; Liu et al., 2023b). Particularly,",
  "*Corresponding author.1Code is available at": "Describe the content of the image in detail. The image features a squirrel jumping inthe air, leaping over snow-covered ground.It is likely that this action was taken toavoid dangerous predators or catch foodhidden beneath it.This action takes placeduring winter, with new snowflakes coveringeverything around, except for a few spotsof grass and trees that can be seen onboth sides. This scene happens after thesquirrel's jump across an open area, whereeverything, including roofs, walls, andfences, is covered by thick layers of snow.",
  ": A case of LVLM hallucination. The partsmarked in red are, in fact, hallucinations. The partsmarked in blue would be mistaken for hallucinations bydetection methods that focus only on objects": "GPT-4 (OpenAI, 2023) has been endowed with thecapability to engage in complex, image-based dia-logues with humans, while also being proficient inresolving a series of visual-language tasks.Despite the fact that LVLMs have achieved quiteconsiderable results on various tasks, problemswith these models have gradually emerged. Withinthese problems, the hallucination (Rohrbach et al.,2018; Li et al., 2023c) has attracted significant at-tention. This is a phenomenon that LVLMs tend togenerate content contradictory to the image, suchas non-existent objects. In order to alleviate thisphenomenon, many explorations have been car-ried out in recent work (Li et al., 2023c; Wanget al., 2023; Liu et al., 2023a; Zhou et al., 2023;Zhai et al., 2023; Lee et al., 2023; Huang et al.,2023; Wang et al., 2024). CoVe (Dhuliawala et al.,",
  "(c) MiniGPT-4": ": Attention visualization of LVLMs. For thesame input, each image represents the attention matrixof a specific LVLM generation instance. Red indicatesthe attention of the image, while green represents thephenomenon of Over-trust\" in the generated text. 2023) proposes a Chain-of-Verification method, itfirst generates verification questions, then executesthem to check for hallucination, and finally gets arevised response. Additionally, some approachesaim to alleviate the hallucination from the decod-ing strategy (OBrien and Lewis, 2023; Chuanget al., 2023; Huang et al., 2023). Dola decoding(Chuang et al., 2023) is a strategy of contrastingthe mature layer and the immature layer of themodel, followed by the determination of the nexttoken based on the differences in logits. Opera(Huang et al., 2023) is a recently proposed decod-ing method that employs an Over-trust Penalty todetermine the occurrence of hallucination, and uti-lizes a Retrospection-Allocation rollback mecha-nism for decoding. However, most of the existing work focuses onalleviating object-level hallucinations. During thisprocess, some methods excessively concentrate onwhether the generated objects exist in the image,neglecting the association between these objectsand the semantics of the whole sentence. As illus-trated in , words marked in red, such astrees, are real object hallucinations. Solely con-sidering the presence of objects, the parts markedin blue, such as predators and food, would alsobe defined as hallucinations. Nevertheless, com-bined with context and semantics, such a definitionis deemed inappropriate. Meanwhile, as indicatedby the green box in , Over-trust (Huanget al., 2023) does exist in LVLMs, which means cer-tain generated tokens receive excessive attention,leading to a subsequent generation that deviatesfrom the image. We initially assumed that insuffi- cient focus on the visual part of the input might beone cause of this phenomenon. However, furtherobservation of the attention matrix of these mod-els reveals strong focus on the visual input (seered boxes in ). This indicates that consid-ering the over-trust penalty only accounts for theimpact of the text, additional focus on the image istherefore required to balance it.Based on the observations above, we proposeHELPD, a novel LVLM framework that utilizesHierarchical FeEdback Learning with Vision-enhanced Penalty Decoding. We note the neces-sity of integrating both the inherent properties ofan object and semantic meaning to determine thepresence of hallucination. Thus, we propose the hi-erarchical feedback learning, which only requires asmall amount of training, and we add this feedbackmechanism at the end of the training period. Onthe one hand, the collection of objects is extractedfrom the sampled sentences and label sentences,and the object-level feedback is obtained throughthe comparison of the object sets. On the otherhand, leveraging the powerful few-shot inferencecapabilities of GPT-4, we conduct a semantic com-parison to obtain sentence-level feedback.The manner of sampling constitutes a crucialcomponent in the hierarchical feedback learningprocess. Opera decoding (Huang et al., 2023) pre-dicts the next word by subtracting the over-trustpenalty score from the logits, where the penaltyscore is computed based on the attention windowof the generated text, disregarding the potent influ-ence of visual attention. Consequently, we proposethe Vision-Enhanced Penalty Decoding, which in-corporates visual attention into the penalty scorecomputation and makes the final logits place moreemphasis on the image input. This approach ef-fectively mitigates an over-reliance on the textualmodality during the decoding process, and en-hances the influence of visual modality, therebyalleviating the hallucination.Our contributions can be summarized as follows: We propose a hierarchical feedback learn-ing method, incorporating object-level andsentence-level hallucination feedback. It canmitigate the occurrence of hallucinations withonly a minimal amount of training.",
  "Large Vision-Language Models (LVLMs)": "Owing to the success of Large Language Models(LLMs) (Brown et al., 2020; OpenAI, 2023; Tou-vron et al., 2023a,b; Chung et al., 2022; Zeng et al.,2023; Sun et al., 2021; Yang et al., 2023) in manyNatural Language Processing (NLP) tasks (Qinet al., 2023), many researchers have endowed itwith multimodal perception capabilities. Amongthese, Large Vision-Language Models (LVLMs)(Li et al., 2023a; Dai et al., 2023; Zhu et al., 2023;Ye et al., 2023; Liu et al., 2023b; OpenAI, 2023;Peng et al., 2023; Anil et al., 2023) have shownparticularly notable performance.LVLMs primarily consist of three components: avisual encoder, a modality alignment module, anda Large Language Model (LLM) (Yin et al., 2023;Zhang et al., 2024). Visual encoders include Vi-sion Transformers (ViT) (Dosovitskiy et al., 2021),CLIP ViT (Radford et al., 2021), and others (Brocket al., 2021; Fang et al., 2023). Specifically, ViTsplits images into patches, which are then inputinto Transformer blocks through linear mappingfor feature learning. Since there exits a modalitygap between the visual encoders and the LLMs, themodal alignment module is required as a bridge.Models such as Flamingo (Alayrac et al., 2022),BLIP-2 (Li et al., 2023a), and InstructBLIP (Daiet al., 2023) apply the Q-former, a method thatextracts visual features in a query-based mannerby employing a set of learnable vectors. Anothermore direct method involves using a linear inter-face for modality alignment. For instance, LLaVA(Liu et al., 2023b) employs a linear layer to mapimages to the textual embedding space.",
  "Hallucination in LVLMs": "Multimodal hallucination is a significant challengefaced by LVLMs, severely impairing the reliabilityand robustness of these models. It typically mani-fests as generating content that is inconsistent withthe image or contradicts common sense. Gener-ally, hallucinations can be divided into IntrinsicHallucination and Extrinsic Hallucination. Intrin-sic hallucination refers to the generation of content that conflicts with the input. On the other hand,extrinsic hallucination represents the generation ofadditional content that does not actually exist, suchas objects not present in the image.Recently, numerous efforts have been dedicatedto the elimination of multimodal hallucination (Liet al., 2023c; Wang et al., 2023; Liu et al., 2023a;Zhou et al., 2023; Zhai et al., 2023; Lee et al., 2023;Huang et al., 2023; Wang et al., 2024; Gunjal et al.,2024; Zhao et al., 2023). CHAIR (Rohrbach et al.,2018) is an early proposed metric for evaluatingobject hallucinations in image captioning tasks. Itassesses the degree of hallucination by calculat-ing the proportion of objects that appear in thegenerated descriptions but not in the image itself.POPE (Li et al., 2023c) introduces a polling-basedobject probing evaluation method, which assessesthe degree of hallucination based on the responsesto questions like Is there a <object> in the im-age? that are posed based on the objects. CoVe(Dhuliawala et al., 2023) introduces a chain-of-verification that considers its own responses andself-corrects hallucination. Liu et al. (2023a) con-ducts visual instruction tuning on LVLMs with thenewly proposed LRV-Instruction dataset to mitigatehallucinations.",
  "Hierarchical Feedback Learning": "Vision-enhanced Penalty Decoding Vision penalty Over-trust penalty InstructionUSER: example_1: 'Sample sentence_1'; 'Label sentence_1'.Score:x.x GPT4: example_n: 'Sample sentence_n'; 'Label sentence_n'.Score:y.ycurrent: 'Sample sentence'; 'Label sentence'. SCORE: ... ... sentence-level hallucination score : This diagram illustrates the framework of HELPD. The Hierarchical Feedback Learning detectshallucination by obtaining object-level feedback from comparing object sets extracted from sampled and labelsentences, and sentence-level feedback through semantic comparison using GPT-4s few-shot inference capabilities.To improve the effectiveness of sampling, the Vision Penalty Decoding augments the over-trust penalty score with avision-enhanced penalty score, making the final logits closer to the image. object set Ssam = {obj1, obj2, . . . , objm} andthe label object set Slab = {obj1, obj2, . . . , objn},where m and n represent the number of objects.Subsequently, we calculate the F1 score of thesetwo sets as the object-level feedback scores Robj:",
  "Precision + Recall(3)": "Sentence-level feedback is obtained through thefew-shot inference capability of GPT-4. We pro-vide a detailed evaluation method and pre-annotateseveral sentence pairs as context (see Appendix Afor detail) to instruct it on discerning hallucinationfrom semantics. The score ranging from 0 to 1,returned by GPT-4, is defined as Rsen.Given that Rsen and Robj are non-differentiable,they cannot be directly incorporated into trainingusing the gradient method. Inspired by (Suttonet al., 1999), we introduce the reinforce algorithmto handle this problem. Specifically, based on thetokens sampled, we first retrieve their correspond-ing log probabilities from the original logits:",
  ",(4)": "where i is the index within the batch, j is the indexwithin the sequence length, Ai,j is the correspond-ing sampled action, and k is the index within thevocabulary of size V . A hyperparameter is setto determine the relative importance of the twotypes of feedback mentioned above (see Equation(5)). By summing the product of the feedback andthe corresponding log probabilities of the actions,we obtain a negative weighted log-likelihood loss.To prevent the loss from infinitely increasing withthe number of actions, the total loss is divided bythe number of sampled actions to yield the lossfunction for reinforce algorithm, denoted as LRL:",
  "LRL,otherwise,(8)where x is the generated token": "OverallPenalty over-trustscore matrix vision-enhanced score matrix generated text window vision attention window : The illustration of Vision-enhanced PenaltyDecoding. The total penalty is composed of the vi-sion penalty and the over-trust penalty. The over-trustpenalty is computed based on the generated text (the up-per region), while the vision penalty is computed fromthe vision attention window (the lower area).",
  "Vision-enhanced Penalty Decoding": "With the hierarchical feedback learning, we can ef-fectively detect object-level and sentence-level hal-lucinations and correct them through back propaga-tion. To obtain the sampled sentences, we need tosample each token from the models logits. Basedon our analysis of the attention matrix, we proposethe Vision-enhanced Penalty Decoding based onOpera (Huang et al., 2023).Over-Trust Logit Penalty.First, we provide abrief description of the original process. It sets alocal window of length h for the attention matrix,where h represents the length of the current gener-ated sequence. Upon obtaining such a lower trian-gular matrix, it pads the upper triangular part withzeros and scales up the values to avoid excessivelysmall values. Subsequently, it conducts column-wise multiplication on this matrix and select themaximum value of the vector as the over-trustpenalty (h). Finally, it subtracts this penaltyfrom the original logits to predict the next token.Vision-enhanced Penalty Decoding.As illus-trated in , we note that the over-trustpenalty establishes a local window whose size islimited to the length of the generated text. Thisapproach can effectively extract over-trust patternsfrom past tokens, but it inadvertently amplifies themodels reliance on the text modality, thereby pas-sively diminishing its focus on images.To foster a greater focus on images during thesampling process, we set an additional local win-dow Whl beyond the local window of the over-trustpenalty, as shown in , purposed for storingthe image components within the attention matrix:",
  "Whl = {wi}hi=1,s.t. wi = {i,j}lj=1,(9)": "where h is the length of the over-trust penalty win-dow, l means the length of the visual input withinthe attention matrix, and i,j represents the atten-tion weight from ith token to jth token. Subse-quently, we conduct the column-wise multiplica-tion on the Whl to obtain a vector of column-wisescores, which represents the accumulated attentionvalues of image:",
  "Hallucination Benchmarks": "CHAIR.Caption Hallucination Assessment withImage Relevance (CHAIR) (Rohrbach et al., 2018)is an evaluation metric employed for assessing hal-lucination in image captioning, and it is often usedto evaluate LVLMs. CHAIR obtains scores for thedegree of hallucination by calculating what propor-tion of objects generated are actually in the imageaccording to the ground truth sentences and objectsegmentations. Specifically, it computes the hallu-cination at both instance level (defined as CHAIRi)and sentence level (defined as CHAIRs):",
  ": Results of LVLMs under three evaluation settings of POPE on the validation set of MSCOCO. Yesdenotes the proportion of answering Yes to the given question. w/ ours means the application of HELPD": "POPE.POPE (Li et al., 2023c) converts halluci-nation assessment into asking the model to answera series of true or false questions about whether anobject is present in the image. Specifically, given animage set and the object annotations contained ineach image, POPE will construct a series of triplesconsisting of images, questions, and answers. Itconsiders three polling strategies by sampling theobjects randomly, from popular objects, and amongthose frequently co-occurring objects, respectively.Finally, POPE involves 3K questions for the cap-tions of 500 images and uses the Accuracy, Preci-sion, Recall, and F1 scores for evaluation.GAVIE.GPT4-Assisted Visual Instruction Eval-uation (GAVIE) (Liu et al., 2023a) is an approachto measure the hallucination without the need forhuman-annotated ground-truth answers. GPT-4takes the generated captions with bounding boxcoordinates as the image content and compares hu-man instructions and model response. Then, askGPT-4 to score the answers based on two criteria:(1) Accuracy: whether the response hallucinateswith the image content. (2) Relevancy: whetherthe response directly follows the instruction. It iscomposed of 1k questions and uses accuracy andrelevancy for evaluation.MMHal-Bench.MMHal-Bench (Sun et al.,",
  "Implementation Details": "We randomly select 5,000 images from the train-ing sets of MSCOCO 2014 (Lin et al., 2014) andFlickr30k (Plummer et al., 2017).Given thateach image corresponds to multiple short cap-tions, we prompt GPT-4 to synthesize a longer cap-tion for each image based on these short captions(see Appendix A). Then, we employ LoRA-tuning(Hu et al., 2022) and deepspeed zero stage 3 toconduct minimal training on LLaVA-1.5-7b andmPLUG-Owl2-7b for 1 epoch. We use the AdamW(Loshchilov and Hutter, 2019) optimizer for opti-mization purposes. The learning rate and weightdecay are set to 0.0001 and 0.1, respectively. Dur-",
  ": Evaluation results on GAVIE. The metricscores of Relevancy and Accuracy are from 0 to 10. w/ours means the application of HELPD": "ing the training process, we initiate a warm-up ratioof 0.03, after which we apply the cosine scheduleto decay the learning rate. We set to 0.6. The val-ues of c for LLaVA-1.5 and mPLUG-Owl2 are setto 0.7 and 0.8. Each model requires approximately4 hours to train with 2 NVIDIA 3090 24Gb GPUs.",
  "Main Results": "In general, it is noticeable that the application ofHELPD with various LVLMs is able to enhancetheir performance across different evaluation met-rics, compared to the original LVLMs.Upon examining the results from the POPEbenchmark, as detailed in , it is evidentthat the hierarchical feedback learning has led toenhancements in the accuracy, precision, and F1score.This suggests that our proposed frameworkcan provide effective hallucination detection andfeedback during training, combining object entitiesand semantic information to guide the model in en-hancing its ability to discern hallucinated objects.As shown in , from the score of CHAIRsand CHAIRi, it is evident that with the help of",
  ": Detailed performance of LVLMs on theeight categories in MMHAL-Bench, where Overallindicates the averaged performance across all categories.w/ ours means the application of HELPD": "HELPD, both mPLUG-Owl2 and LLaVA-1.5 havedemonstrated varying degrees of hallucination re-duction. Specifically, the trained mPLUG-Owl2,under various decoding methods, is able to reducethe CHAIRs by an average of 19.4 and the CHAIRiby an average of 5.4. This indicates that our pro-posed framework can effectively mitigate the gener-ation of hallucinations, whether at the instance levelor the sentence level. Moreover, it can be observedthat the trained LVLMs do not exhibit significantfluctuations in the length of generated text. In mostcases, LLaVA-1.5 can even increase the averagegenerated length by 1.1 to 4.0. This illustrates thatHELPD, while enhancing the anti-hallucination ca-pabilities of LVLMs, does not excessively interferewith the generated length. shows the performance of differentLVLMs on the GAVIE benchmark, which asksGPT-4 to pretend to be a smart teacher and scores(0-10) the answers according to the image contentand instructions. The trained models achieve im-provements in both accuracy and relevancy. Specif-ically, the trained mPLUG-Owl2 attains scores of8.88 and 6.12 in relevancy and accuracy respec-tively, surpassing the provided baseline models.This demonstrates that our proposed frameworkcan aid LVLMs in more directly following instruc-tions, and the generated responses are more accu-rate concerning the image content.Detailed performance of LVLMs on the eight cat-egories in MMHAL-Bench is shown in . Itis evident that the trained models surpass their cor-responding baseline models in performance acrossall eight question categories, and achieve a score ofover 3 in five categories, including Object attribute",
  "Further Analysis": "Break-down Study of Hierarchical FeedbackLearning.As illustrated in .1. In orderto detect hallucinations of different granularitiesduring training and provide feedback for parame-ter updates, we introduce hallucination feedbackat both the object and sentence levels. To verifywhether these two types of feedback contribute tothe mitigation of hallucination, we conduct abla-tion experiments, and the results are presented in. Both object-level and sentence-level feed-back can aid in alleviating hallucination, makingthe generated text adhere more closely to instruc-tions and rendering it more accurate concerningthe image content. It can also be observed that,compared to object-level feedback, sentence-levelfeedback can more effectively enhance the modelsability to resist hallucination. We hypothesize thatthis is because object-level feedback is more uncon-trollable, such as possible omissions in the processof object extraction, or score reductions due tothe presence of synonyms. However, the sentence-level feedback generated by prompting GPT-4 caneffectively compensate for the deficiencies of theobject-level feedback, thereby enhancing the over-all performance of hierarchical feedback learning. The Timing of Incorporating Hierarchical Feed-back Learning.To investigate at which stageof training the integration of hierarchical feed-back learning can better enhance the models anti-hallucination capabilities, we also conduct an ab-lation study on the hyperparameter c. The experi-mental results of LLaVA-V1.5 on the random set ofPOPE are shown in , with more details avail-able in the Appendix B. It indicates that LLaVA-",
  ": Ablation results on the decoding strategy.We exhibit the average F1-score computed on random,popular, and adversarial splits of POPE": "V1.5 exhibits fewer hallucinations when c = 0.7,while mPLUG-Owl2 performs better when c = 0.8.Therefore, we default to assigning c = 0.7 forLLaVA-V1.5 and c = 0.8 for mPLUG-Owl2.Different Decoding Strategy.Based on the ob-servations of the attention matrix, we propose thevision-enhanced penalty decoding based on opera.To validate its effectiveness, we conduct an abla-tion study on LVLMs. The experimental results areshown in and . As can be observed,compared to the baseline decoding strategy, thevision-enhanced penalty decoding demonstrates su-perior performance on benchmarks such as POPEand CHAIR, and has a smaller impact on the lengthof the generated text. It should be noted that thisdecoding strategy pays more attention to the hallu-cination performance of long texts.",
  "Conclusion": "In this paper, we aim to alleviate hallucinations inLarge Vision-Language Models (LVLMs), and pro-pose the HELPD framework, which employs thehierarchical feedback learning for small amounts oftraining on the model. To enhance attention to thevisual modality, we also propose a vision-enhancedpenalty decoding strategy. To evaluate the effec-tiveness of our approach, we conduct evaluationson numerous benchmarks. Experimental resultsdemonstrate that our proposed framework effec-tively mitigates hallucination for different LVLMswithout impacting sentence length and concurrentlyimproves their text generation quality. Future workcould focus on a more comprehensive evaluationof hallucination at different granularities.",
  "Ethics Statement": "The data (Lin et al., 2014; Plummer et al., 2017)used in our work is all drawn from open-sourcedatasets. The data and text involved in our researchdo not involve private information and social is-sues. This research is supported by the National Natu-ral Science Foundation of China (No.62476127,No.62106105), the Natural Science Foundationof Jiangsu Province (No.BK20242039), the CCF-Baidu Open Fund (No.CCF-Baidu202307), theCCF-Zhipu AI Large Model Fund (No.CCF-Zhipu202315), the Fundamental Research Fundsfor the Central Universities (No.NJ2023032), theScientific Research Starting Foundation of Nan-jing University of Aeronautics and Astronautics(No.YQR21022), and the High Performance Com-puting Platform of Nanjing University of Aeronau-tics and Astronautics. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, Roman Ring, Eliza Rutherford, SerkanCabi, Tengda Han, Zhitao Gong, Sina Samangooei,Marianne Monteiro, Jacob L. Menick, SebastianBorgeaud, Andy Brock, Aida Nematzadeh, SahandSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,Oriol Vinyals, Andrew Zisserman, and Karn Si-monyan. 2022. Flamingo: a visual language modelfor few-shot learning. In Advances in Neural In-formation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022. Peter Anderson, Basura Fernando, Mark Johnson, andStephen Gould. 2016.SPICE: semantic proposi-tional image caption evaluation. In Computer Vision -ECCV 2016 - 14th European Conference, Amsterdam,The Netherlands, October 11-14, 2016, Proceedings,",
  "Part V, volume 9909 of Lecture Notes in ComputerScience, pages 382398. Springer": "Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Gemini: A fam-ily of highly capable multimodal models. CoRR,abs/2312.11805. Satanjeev Banerjee and Alon Lavie. 2005. METEOR:an automatic metric for MT evaluation with improvedcorrelation with human judgments. In Proceedingsof the Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization@ACL 2005, Ann Arbor, Michigan, USA,June 29, 2005, pages 6572. Association for Compu-tational Linguistics. Andy Brock, Soham De, Samuel L. Smith, and Karen Si-monyan. 2021. High-performance large-scale imagerecognition without normalization. In Proceedings ofthe 38th International Conference on Machine Learn-ing, ICML 2021, 18-24 July 2021, Virtual Event,volume 139 of Proceedings of Machine LearningResearch, pages 10591071. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, YoonKim, James R. Glass, and Pengcheng He. 2023. Dola:Decoding by contrasting layers improves factualityin large language models. CoRR, abs/2309.03883": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, Albert Web-son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-gun, Xinyun Chen, Aakanksha Chowdhery, SharanNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,Yanping Huang, Andrew M. Dai, Hongkun Yu, SlavPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, AdamRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.2022. Scaling instruction-finetuned language models.CoRR, abs/2210.11416.",
  "Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning.CoRR,abs/2305.06500": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,Roberta Raileanu, Xian Li, Asli Celikyilmaz, andJason Weston. 2023. Chain-of-verification reduceshallucination in large language models.CoRR,abs/2309.11495. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. In 9th International Conferenceon Learning Representations, ICLR 2021, VirtualEvent, Austria, May 3-7, 2021. OpenReview.net. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, LedellWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,and Yue Cao. 2023.EVA: exploring the limitsof masked visual representation learning at scale.In IEEE/CVF Conference on Computer Vision andPattern Recognition, CVPR 2023, Vancouver, BC,Canada, June 17-24, 2023, pages 1935819369.IEEE. Alessandro Favero, Luca Zancato, Matthew Trager, Sid-dharth Choudhary, Pramuditha Perera, AlessandroAchille, Ashwin Swaminathan, and Stefano Soatto.2024. Multi-modal hallucination control by visualinformation grounding. CoRR, abs/2403.14003. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023. MME: A comprehensive evaluationbenchmark for multimodal large language models.CoRR, abs/2306.13394. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the V in VQAmatter: Elevating the role of image understanding invisual question answering. In 2017 IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR2017, Honolulu, HI, USA, July 21-26, 2017, pages63256334. IEEE Computer Society. Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. De-tecting and preventing hallucinations in large visionlanguage models. In Thirty-Eighth AAAI Conferenceon Artificial Intelligence, AAAI 2024, Thirty-SixthConference on Innovative Applications of ArtificialIntelligence, IAAI 2024, Fourteenth Symposium onEducational Advances in Artificial Intelligence, EAAI2014, February 20-27, 2024, Vancouver, Canada,pages 1813518143. AAAI Press.",
  "Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, WeimingZhang, and Nenghai Yu. 2023. OPERA: alleviatinghallucination in multi-modal large language modelsvia over-trust penalty and retrospection-allocation.CoRR, abs/2311.17911. Alina Kuznetsova, Hassan Rom, Neil Alldrin, JasperR. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, ShahabKamali, Stefan Popov, Matteo Malloci, Tom Duerig,and Vittorio Ferrari. 2018. The open images datasetV4: unified image classification, object detection,and visual relationship detection at scale. CoRR,abs/1811.00982.",
  "Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-joon Seo. 2023. Volcano: Mitigating multimodalhallucination through self-feedback guided revision.CoRR, abs/2311.07362": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.Hoi. 2023a. BLIP-2: bootstrapping language-imagepre-training with frozen image encoders and largelanguage models. In International Conference onMachine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 1973019742.PMLR. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,Jason Eisner, Tatsunori Hashimoto, Luke Zettle-moyer, and Mike Lewis. 2023b. Contrastive decod-ing: Open-ended text generation as optimization. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pages 1228612312. Association for Computa-tional Linguistics. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023c. Eval-uating object hallucination in large vision-languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 292305. Association for Computational Lin-guistics.",
  "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick. 2014. Microsoft COCO:common objects in context. In Computer Vision -ECCV 2014 - 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part V, volume 8693 of Lecture Notes in ComputerScience, pages 740755. Springer. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, YaserYacoob, and Lijuan Wang. 2023a. Mitigating hal-lucination in large multi-modal models via robustinstruction tuning. arXiv preprint arXiv:2306.14565,1(2):9.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei. 2023.Kosmos-2: Grounding multimodal large languagemodels to the world. CoRR, abs/2306.14824": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,Juan C. Caicedo, Julia Hockenmaier, and SvetlanaLazebnik. 2017.Flickr30k entities:Collectingregion-to-phrase correspondences for richer image-to-sentence models. Int. J. Comput. Vis., 123(1):7493. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, JiaaoChen, Michihiro Yasunaga, and Diyi Yang. 2023.Is chatgpt a general-purpose natural language pro-cessing task solver?In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2023, Singapore, Decem-ber 6-10, 2023, pages 13391384. Association forComputational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the 38th InternationalConference on Machine Learning, ICML 2021, 18-24July 2021, Virtual Event, volume 139 of Proceedingsof Machine Learning Research, pages 87488763.PMLR.",
  "Language Processing, Brussels, Belgium, October 31- November 4, 2018, pages 40354045. Associationfor Computational Linguistics": "Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,Weibao Gong, Jianzhong Liang, Zhizhou Shang,Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, HaoTian, Hua Wu, and Haifeng Wang. 2021. ERNIE3.0: Large-scale knowledge enhanced pre-trainingfor language understanding and generation. CoRR,abs/2107.02137. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,Chunyuan Li, Yikang Shen, Chuang Gan, Liang-YanGui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,and Trevor Darrell. 2023. Aligning large multimodalmodels with factually augmented RLHF.CoRR,abs/2309.14525. Richard S. Sutton, David A. McAllester, Satinder Singh,and Yishay Mansour. 1999. Policy gradient methodsfor reinforcement learning with function approxima-tion. In Advances in Neural Information ProcessingSystems 12, [NIPS Conference, Denver, Colorado,USA, November 29 - December 4, 1999], pages 10571063. The MIT Press. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288.",
  "the 2022 Conference on Empirical Methods in Natu-ral Language Processing, EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pages59565965. Association for Computational Linguis-tics": "Junyang Wang, Yiyang Zhou, Guohai Xu, PengchengShi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, MingYan, Ji Zhang, Jihua Zhu, Jitao Sang, and HaoyuTang. 2023.Evaluation and analysis of halluci-nation in large vision-language models.CoRR,abs/2308.15126. Lei Wang, Jiabang He, Shenshen Li, Ning Liu, andEe-Peng Lim. 2024. Mitigating fine-grained halluci-nation by fine-tuning large vision-language modelswith caption rewrites. In MultiMedia Modeling - 30thInternational Conference, MMM 2024, Amsterdam,The Netherlands, January 29 - February 2, 2024, Pro-ceedings, Part IV, volume 14557 of Lecture Notes inComputer Science, pages 3245. Springer. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,Dong Yan, Fan Yang, Fei Deng, Feng Wang, FengLiu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu,Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, LeiSu, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma,Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,Peidong Guo, Ruiyang Sun, Tao Zhang, TianpengLi, Tianyu Li, Wei Cheng, Weipeng Chen, Xian-grong Zeng, Xiaochuan Wang, Xiaoxi Chen, XinMen, Xin Yu, Xuehai Pan, Yanjun Shen, YidingWang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yu-peng Zhang, Zenan Zhou, and Zhiying Wu. 2023.Baichuan 2:Open large-scale language models.CoRR, abs/2309.10305.",
  "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, XingSun, Tong Xu, and Enhong Chen. 2023.A sur-vey on multimodal large language models. CoRR,abs/2306.13549": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,Yufei Xue, Jidong Zhai, Wenguang Chen, ZhiyuanLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.GLM-130B: an open bilingual pre-trained model. InThe Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. OpenReview.net. Bohan Zhai, Shijia Yang, Xiangchen Zhao, ChenfengXu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Man-ling Li, Tan Yan, and Xiangjun Fan. 2023. Halle-switch: Rethinking and controlling object existencehallucinations in large vision language models fordetailed caption. CoRR, abs/2310.01779.",
  "B.1More Experimental Results Compared toExisting Methods": "For POPE benchmark in , compared to ex-isting methods (Liu et al., 2023a; Favero et al.,2024; Yang et al., 2024), our approach performswell on both the random and popular test sets, butfalls short in terms of accuracy on the adversar-ial test set. This is because the method by Zhouet al. (2023) uses multiple similar images for com-parison, enabling multi-angle judgment of the testproblem. In contrast, our proposed method requiresless computational resources and is able to surpassthe performance in terms of F1 score. While forCHAIR in , compared to existing work,we are able to demonstrate better performance.",
  "B.3Further Analysis About the Quality of theGenerated Text": "Considering that interventions in the decoding pro-cess can impact the quality of the generated text,we also conduct experiments to measure the impactcaused by the vision-enhanced penalty decoding.Evaluation was carried out on the generated textfrom CHAIR, using the BLEU (Papineni et al.,2002), ROUGE-L (Lin, 2004), METEOR (Baner-jee and Lavie, 2005), and SPICE (Anderson et al.,2016) metrics, with the experimental results pre-sented in . It can be observed that our pro-posed vision-enhanced penalty decoding performscomparably to beam search in terms of text gen-eration metrics, without demonstrating excessivedecline. It even surpasses beam search on BLEU1,BLEU4, and ROUGE-L. As can also be seen from, compared to beam search and opera decod-ing, our proposed method is able to maintain thelength of the generated sentences as well. This elu-cidates that our method can maintain the quality ofthe generated text while mitigating hallucinations.Additionally, to verify whether HELPD can miti-gate hallucinations while preserving general capa-bilities, we conduct evaluations on VQA-v2 (Goyalet al., 2017) and MME (Fu et al., 2023). As shownin and , LVLMs with HELPDcan maintain relatively stable performance acrossvarious metrics, demonstrating that the frameworkdoes not significantly impair the models founda-tional abilities.",
  "We show some generation cases in , 9, and10": "You will be presented with two pieces of text that describe the sameimage. First text comes from a dataset and can be considered as thelabel, and the other is generated by a model. Your task is to comparethese two pieces of text and evaluate them from the perspective ofhallucination. If the contents described by the two pieces of text arecompletely consistent and there is no hallucination, please give ascore of 1. If hallucination is present in the model-generated text, i.e., itdescribes content not present in the label text, please give a scorecloser to 0. Note that the lower the score, the less obvious thehallucination. The score range is between 0 and 1. Please only providea score and do not provide reason. Output example 1:Dataset text (Label): Label 1;Model generated text: Text 1;score: score 1 Output example 2:Dataset text (Label): Label 2;Model generated text: Text 2;score: score 2 Output example 3:Dataset text (Label): Label 3;Model generated text: Text 3;score: score 3 Output format:Dataset text (Label): Model generated text: score:",
  ": Prompt for generating sentence-level feedback score": "Given the following five captions of the same image, please combinethem into a single, comprehensive caption that includes all theinformation, especially objects, without any repetition: Output example 1:Caption 1: caption_1;Caption 2: caption_2;Caption 3: caption_3;Caption 4: caption_4;Caption 5: caption_5;Combined caption 1: response_1 Output example 2:Caption 1: caption_1;Caption 2: caption_2;Caption 3: caption_3;Caption 4: caption_4;Caption 5: caption_5;Combined caption 2: response_2 Output example 3:Caption 1: caption_1;Caption 2: caption_2;Caption 3: caption_3;Caption 4: caption_4;Caption 5: caption_5;Combined caption 3: response_3 Output format:Caption 1:Caption 2:Caption 3:Caption 4:Caption 5:Combined caption:",
  ": Experimental results on MME benchmark": "Describe the content of the image in detail. The image features a lion and a cow running in a meadow. They are allin motion, one chasing the other, running together in their naturalhabitat. In this wild scene of natural beauty, the two animals seem to rush at full speed. Additionaly, there are some other animals in thedistance, such as horses, creating a natural atmosphere. This photograph depicts a lion and a cow running in a meadow. Theyare engaged in a chase, each animal exploring their naturalsurroundings in pursuit or flight. Amidst this wild display of naturalsplendor, the two creatures appear to interact, charging full-tilt towardsan object of interest. mPLUG-Owl2 mPLUG-Owl2 w/ HELPD",
  ": An illustrative case is presented to compare the output of mPLUG-Owl2 and mPLUG-Owl2 withHELPD": "Describe the content of the image in detail. The image showcases a young woman standing alone on the bank ofan outdoor body of water, possibly a river or a lake. She holds a fishingrod, seemingly enjoying her time as she casts her line into the water.The area is surrounded by trees, adding to the serenity of the scene.With no other individuals are present in this area. The image features a young woman standing on the bank of an outdoorbody water, such as river or lake. She is holding onto her fishing rodand appears to be enjoying herself while casting for something in thewaterside area near trees that surround it. There are several otherpeople visible around this scene who may also have joined togetherwith others nearby. LLaVA-1.5w/ HELPD LLaVA-1.5",
  ": An illustrative case is presented to compare the output of LLaVA-v1.5 and LLaVA-v1.5 withHELPD": "Describe the content of the image in detail. The image features a man with other players in white and blue clothing,paddling an oar-powered boat through rough water. They are wearingprotective gear such as helmets on their head to ensure safety, whilenavigating the rapids of this challenging course with precisionskillfulness required for competitive kayaking. Additionally, there are alot of spectators on both sides cheering for them. The image features a man in white and blue clothing, paddling an oar-powered boat through rough water. The man is wearing protective gearsuch as helmets to ensure safety, while navigating the rapids of thischallenging course with precision skillfulness required for competitivekayaking. mPLUG-Owl2 mPLUG-Owl2 w/ HELPD"
}