{
  "Abstract": "Code-switching (CS) is the process of speakersinterchanging between two or more languageswhich in the modern world becomes increas-ingly common. In order to better describe CSspeech the Matrix Language Frame (MLF) the-ory introduces the concept of a Matrix Lan-guage, which is the language that provides thegrammatical structure for a CS utterance. Inthis work the MLF theory was used to developsystems for Matrix Language Identity (MLID)determination. The MLID of English/Mandarinand English/Spanish CS text and speech wascompared to acoustic language identity (LID),which is a typical way to identify a language inmonolingual utterances. MLID predictors fromaudio show higher correlation with the textualprinciples than LID in all cases while also out-performing LID in an MLID recognition taskbased on F1 macro (60%) and correlation score(0.38). This novel approach has identified thatnon-English languages (Mandarin and Spanish)are preferred over the English language as theML contrary to the monolingual choice of LID.",
  "Introduction": "Code-switching (CS) is the process of speakersswitching between two or more languages in spo-ken or written language (). Spoken CSdata is scarce, and thus models for processing CSspeech often yield poor performance in comparisonto monolingual models. Given that in many coun-tries CS is widespread (e.g. India, South Africa,Nigeria) (Diwan et al., 2021; Ncoko et al., 2000;Rufai Omar, 1983), it is essential to develop sys-tems for understanding and modelling CS speech.One of the critical tasks in analyzing code-switchedspeech is determining the matrix language (ML), orthe dominant language, which serves as the struc-tural framework for the utterance. Accurate identi-fication of ML is essential for various applicationsas well as sociolinguistic studies.",
  ". The System Morpheme Principle - all systemmorphemes which have grammatical relationsexternal to their head constituent will comefrom ML": "The morphemes as units within the MLF frame-work were first introduced by Myers-Scotton in1997 (Myers-Scotton, 1997) and were split intocontent and system morphemes. Some commonexamples of system morphemes are quantifiers,possessives and tense/aspect determiners, whilecontent morphemes include nouns, pronouns, ad-jectives, verbs and prepositions.Matrix language identity (MLID) is the iden-tity of the language providing the grammaticalframe for the utterance and it can be defined forboth monolingual and CS utterances. Moreover,the existence of ML implies a certain token distri-bution following the System Morpheme Principle(Myers-Scotton, 2002) which is highlighted in fur-ther Myers-Scotton works in the 4-M model. Over-all, MLID provides insight into the grammatical properties of the utterance and a computational im-plementation of an MLID would be able to reducethe amount of manual annotation.In this paper three MLID systems for CS textand audio were implemented. MLF theory for-mulates The Morpheme Order Principle and theSystem Morpheme Principle, which were imple-mented into three systems for MLID determina-tion from text (P1.1, P1.2 and P2) and from au-dio (1.1, 1.2 and 2). Anextensive correlation analysis and comparison ofan MLID determination system and a traditionalacoustic language identities (LID) were carried out.Recognised MLID and LID from CS texts and au-dio were compared to ground truth ML annotationand the quality of ML recognition was measuredin terms of F1 macro and Matthews CorrelationCoefficient (MCC). To conclude the findings, thedistributions of textual LIDs were compared to thetextual MLID distributions of the CS data.The remainder of the paper is structured as fol-lows. The next section reviews the relevant re-search presented previously on MLF and LID inCS text and speech. The third section providesa detailed description of the methods used. Thisis followed by a section on experiments, whichprovides information on datasets, detailed imple-mentation, experiment descriptions as well as adiscussion of results. Conclusions summarise andcomplete the paper.",
  "Related work": "MLF theory has rarely been used to automaticallyanalyse speech or text. Up until now it was onlyused for text augmentation (Bhat et al., 2016; Yil-maz et al., 2018; Lee et al., 2019) or for LanguageModel (LM) adaptation for code-switching. Forexample additional grammatical information wasused during LM construction (Adel et al., 2015;Soto and Hirschberg, 2019) or a self-supervisedtraining procedure was set up which encouragedgeneration of CS utterances (Chang et al., 2019).MLID classification for CS text was carried out inBullock et al. (2018) where the ML was identifiedbased on the token and system POS majorities.Simultaneously in the speech processing domaina common technique to separate languages in CSis LID. LID of a whole CS utterance may be per-formed when CS is regarded as a separate language(Mary N J et al., 2020), in this case the componentperforms both LID and CS detection. A multi- lingual ASR system with an utterance-wise LIDcomponent as an auxiliary task was tested for CSutterances in Toshniwal et al. (2018) but the modelwas not able to generate CS text as a result. LIDand language segmentation (LIS) systems makedecisions based on similarity to the data they weretrained on (Muralikrishna et al., 2021) and, to thebest of our knowledge, no study was done to de-termine if pretrained LID/LIS are able to predict adominant language in a CS utterance.Neither MLID nor LID were previously used forCS analysis. Furthermore, ML determination prin-ciples were never fully implemented and compared.However, statistical methods were introduced be-fore (Guzmn et al., 2017) which can assess thenature of CS. Among the statistical methods onlythe M-index (Multilingual Index) quantifies therepresentation of the languages in multilingual cor-pora. While the M-index is useful to learn aboutthe balance of the token LIDs, it might be insuffi-cient to learn about the utterance LID and MLIDdistributions.The above indicates that theoretical methods toidentify ML from text exist but previously therewas only one attempt to determine MLID whichwas not based on the two ML determination prin-ciples. Furthermore, there are no existing MLIDpredictors from audio to the best of our knowl-edge. Therefore the objective of this study is toadvance technologies for multilingual understand-ing and analysis by describing the implementation,comparison and performance of automatic MLIDpredictors from CS audio and text based on the MLdetermination principles from the MLF theory.",
  "Principles for ML determination": "The MLF theory principles mentioned above haveto be implemented in order to compare the main lan-guage recognised by an LID system to the MLID.Each of them provides estimates for MLID but isconditioned by different evidence in the utterance.The Morpheme Order Principle is separated intotwo implementations of the 1st Principle (P1.1 andP1.2). The implementations of the principle deduc-tion as per MLF theory are described below.",
  ": Detection error tradeoff (DET) curve for possible log values. Thin diamond is the default value oflog = 0, thick diamond - result of log estimation, red star - ground truth log": "rent implementation operates on the level of words.Suppose there is a CS utterance y of length , then(y, l) = ((, ), (1, 1), .., (, ), (, )) aremorphemes with corresponding language ID la-bels, is an empty morpheme and is an emptylanguage morpheme tag from an empty language . If ((, ), .., ( , )) constitute a word where0 < < < + 1, that < < | = 2and 1, +1 1, then the language of thecontext 1 is the ML while 2 is the embeddedlanguage. For example in post blog Mandarin will be ML since it is a context forEnglish singleton insertions.",
  ": Pipeline of the morpheme order-based princi-ple for ML determination P1.2": "In P1.2 the second part of the Morpheme OrderPrinciple is implemented which postulates that themorpheme order is determined by the ML. For ex-ample, in speak clear enough theEnglish translation of the auxiliary Mandarin verbwill never appear at the end of an utterance in English, signifying that Mandarin is ML in thisutterance. Assume languages (1, 2) arepresent in a bilingual utterance where are all lan-guages, then the original CS utterance y can betranslated into two monolingual utterances y1 andy2. y1 and y2 are obtained from the original ut-terance y by a Neural Machine Translation (NMT)systems 1 and 2. Consider an LM (|y)which is used to provide a probability of the utter-ance belonging to a language , then given the twolanguages 1 and 2 classification leads to:",
  "Principle 2: The system word principle": "From the examples in it is evidentthat there exists an overlap of content/systemmorphemes duality with the traditional con-tent/function words opposition defined in linguis-tics, although they are not equivalent and the tra-ditional classifications also not strictly distinguish-able. Therefore in the implementation of the 2Principle (P2) for ML determination instead ofcontent/system morpheme duality (Myers-Scotton,2002) a content/function Part of Speech (POS) du-ality is considered.System POS are identified, namely determiners,auxiliaries, subordinating conjunctions and coor-dinating conjunctions, while the rest of POS areconsidered as content POS. ML is determined in aCS utterance if one of the participating languagesprovided function POS for the utterance and theother language did not. The language that pro-vided the function POS is determined as the ML.Although CS POS taggers exist (Feng et al., 2022;Bansal et al., 2022) none of them are available inopen-source and since training a POS tagger is nota goal of this work a monolingual POS tagger isused instead. For example in the utterance imokay with the determiner the is used andtherefore ML is determined as English.",
  "CS data": "CS spoken language corpora SEAME (120 hours)and Miami (35 hours) are used for analysis andacoustic MLID training. Agreement analysis is car-ried out for CS utterances from the SEAME andMiami corpora and monolingual SEAME and Mi-ami utterances are used for estimating the scalingfactor (). The monolingual subsets ofSEAME and Miami are also used for training themapping from the LID outputs to English, Man-darin and Spanish posteriors (). Mandarincharacters in the SEAME corpus are word seg-mented which is helpful when applying P1.1, aprinciple that operates with words. All the intro-duced principles require morpheme-level LID tagannotation which is available for Miami and is au-tomatically determined for SEAME based on thescript (latin vs logographic). Finally, additionalMLID-annotated 91 CS Miami utterances wereused to measure the quality of MLID predictionfrom text and audio. The annotated MLID labelswere assigned to the CS utterance transcriptions onthe basis of determiner-noun-adjective complexes(Parafita Couto and Gullberg, 2017).",
  "Applying ML Principles to utterancetranscriptions": "P1.1 ML only applies to utterance transcriptionswith singleton insertions, therefore resulting in asmall data coverage: only for 36% (SEAME) and60% (Miami) of the CS data the ML is determined.In P2 POS tags are computed for constituent mono-lingual islands (segments) of a CS utterance usinga pretrained CNN-based POS tagger (Svenstrupet al., 2017). P2 covered 31% (SEAME) and 58%(Miami) of all of the CS examples. Furthermore,a baseline MLID determiner from text is imple-mented which is based on the token LID countfollowing Bullock et al. (2018). Examples of run-ning the resulting principles implementations ispresented in the . The implementation of P1.2 includes three com-ponents: a Machine Translation (MT) system, apseudomorpheme tokeniser and a language model(LM). CS utterances are translated word by wordusing Wiktionary5 to preserve the token order. TheEnglish and Spanish LMs are trained on the to-kenised English and Spanish Callhome datasets.The tokenisation was carried out using a stemmerwhere stem and affix would be separated. For theMandarin Callhome dataset separate characters areregarded as morphemes. The two Transformer-based (Vaswani et al., 2017) LMs with 2 lay-ers, 2 attention heads per layer are trained for 25epochs with negative log-likelihood loss on one3080 Nvidia GPU for 1 hour. Validation and testperplexities for the three languages are presentedin .",
  "Valid48.9794.9857.76Test57.6198.1652.30": "Moreover, a preliminary experiment is carriedout to evaluate if the trained LMs have the abilityto detect the original word order (WO) among itspermuted variants (up to 20 word permutations).The sequence of tokens for which the probabilitywas the highest was chosen as the predicted originalWO. Comparing the sequence with chosen WO tothe original WO leads to 37% accuracy for SEAMEand 60% for Miami. : Outcomes of estimation. \"- MCC\" is thecorrelation measured between the MLID determined bythe unscaled P1.2 approach and MLID labels from otherprinciples (+ true MLID labels for Miami). \"+ MCC\"are the correlation measurements with the scaled P1.2.",
  "- MCC0.310.330.360.080.41+ MCC0.360.310.380.090.37": "Outputs of the pre-trained monolingual LMshave different probability distributions, therefore,as described in , the factor is used toallow for scale changes. is derived from expecta-tions of the probabilities yielded on monolingualexamples and their translations following Equation4. As a result of estimation the MCC of SEAMEP1.1/P1.2, Miami P1.1/P1.2 and Miami P2/P1.2has increased (). Additionally, a \"true\" value is calculated using ground truth MLID forMiami and P1.1 and P2 MLID for SEAME, andthey are compared to the estimated . DET plotsand highlighted thresholds in demonstratethat by using the estimated the amount of FalsePositives (FP) and False Negatives (FN) becomesmore balanced for SEAME. For Miami the es-timation does not lead to more balanced FP and : Experimental results for SEAME. First three columns and last three rows (P1.1, P1.2 and P2) refer tothe ML determination principles from text. \"Coverage\" row presents the percentage of all CS examples beingprocessed. \"% English\" row displays the percentage of utterances recognised as \"English\" LID or MLID. MCCBaseline refers to the word LID majority implementation (Bullock et al., 2018). \"\" is a pretrained LID system,\"\" column is a mapping trained on monolingual utterances from SEAME. 1.1, 1.2 and2 are trained mappings similar to but trained on CS data and labels generated from transcriptionsby corresponding principles. 1.1, 1.2 and 2 contain correlation values with the target MLIDdetermined from text (italic) and correlations with other MLID targets.",
  "Language Identification": "If one assumes that is a \"dominant\" language thatmost acoustically resembles the spoken CS utter-ance, then a conventional LID system can be usedas an ML determiner. An ECAPA-TDNN (Desplan-ques et al., 2020) model pretrained on Voxlingua-107 (Valk and Alume, 2020) was used to automati-cally detect the dominant language from audio data( and 8, column ). The ECAPA-TDNNmodel was trained to recognise a large number oflanguages. In order to limit the models to binarytask a mapping function was trained from the out-puts based on a fully-connected neural network(Multi-Layer Perceptron, MLP) classifier.Themapping function is trained to map 107 languageoutput posteriors to the binary output of the lan-guages participating in CS. LID is a challengingtask for accented data such as monolingual subsetsfrom SEAME and Miami but still achieves 82%and 79% F1-macro respectively on cross-validationamong 5 splits.",
  "ML identification from audio": "One can train an MLP mapping model using theLID posterior distribution to also predict P1.1, P1.2and P2 from audio. Due to the different coveragerates of P1.1, P1.2 and P2 of the CS data the amountof training data would vary greatly: 16582 for P1.1,43068 for P1.2 and 23868 for P2. The resultingsystems will be further referred to as 1.1,",
  "MCC P1.110.030.09MCC P1.20.0310.1MCC P20.090.11": "P1.1, P1.2 and P2 were applied to CS text data andthe agreement analysis is presented in and for SEAME and Miami respectively in thefirst three columns. P1.1 and P2 have to meet cer-tain conditions to be applied, therefore they do nothave full coverage of CS data: 36% and 31% forSEAME, 60% and 58% for Miami. Measuring thecorrelations only for the utterances for which theMLID is determined is performed to measure theagreement of the implementations of the linguistic : Experimental results for Miami. \"\" column is a mapping trained on monolingual utterances fromMiami. \"F1-macro true\" and \"MCC true\" are the metric values when comparing the outputs of the systems to groundtruth ML annotation for Miami.",
  "MCC Baseline0.990.280.670.590.810.830.420.8MCC P1.110.380.810.450.420.850.430.82MCC P1.20.3810.090.260.340.350.530.34MCC P20.810.0910.70.860.870.510.82": "principles. Higher correlation indicates that theprinciples in agreement may be used to generateground truth MLID labels for downstream tasks.Calculating MCC with unknown labels leads to ex-tremely low correlation due to a large portion ofunknown labels which makes it impossible to as-sess the slight changes in correlation of the labeleddata (Tables 9 and 10). Among the three principles P1.1 and P2 have thegreatest correlation (0.82 for SEAME and 0.81 forMiami), P1.1/P1.2 demonstrates less correlation(0.36 and 0.38), while the least correlation is ob-served between P1.2 and P2 (0.31 and 0.09). P1.1and the baseline have almost identical behaviorwhich is expected (0.99 and 1.0), whereas less cor-relation is observed of the baseline with P2 (0.69and 0.67) and P1.2 (0.28 and 0.28).",
  "Correlation of P1.1/P1.2/P2 and theacoustic LID/MLID": "The ML determined from CS text is compared tothe LID computed from the corresponding audio.The procedure for the LID experiments is describedin the previous subsection. Columns 4 and 5 inTables 6 and 8 show the amount of correlation be-tween MLID derived from text and the recognisedLID classes. The same columns for Miami in also include F1 macro and MCC for an annotatedMLID subset. Training on the monolin-gual utterances seems to increase the MCC (from0.27 to 0.35) but decrease the F1 macro (56% from53%) for the CS Miami data. Suppose a conventional LID system determinesthe dominant language in a CS audio based on themajority of time the language is spoken. Then thetrue annotation may be approximated by countingthe textual token LIDs in a CS utterance (Baseline).However, correlation analysis shows that systems are better predictors of the token LID ma-jority (columns 4-5 vs 6-8 row MCC Baseline inTables 6 and 8). Further experimentation comprises of compar-ing 1.1, 1.2 and 2 withP1.1, P1.2 and P2. Upon observing the results forSEAME data 1.2 leads to overall highestvalue out-of-domain MCC scores (0.47+0.4=0.87)for textual principles P1.1 and P2. A similar in-spection of the Miami results shows the biggestMCC scores for 1.1 (0.35+0.87). For theannotated subset of Miami data 1.2 leadsto the biggest F1 macro among all systems (60%),while 2 leads to the biggest MCC score(0.38). : Distributions of languages in CS corpora. Utterance level LID for monolingual subsets is in the \"UtteranceLID\" row, token level LID for CS is in the \"Token LID\" row and utterance level textual ML for CS are in rowsP1.1/P1.2/P2.",
  ": Correlations between acoustic and outputs and textual P1.1, P1.2 and P2 for CSMiami data": "Lastly, the MCC scores between the textual andacoustic MLID determiners are summed up forevery / approach (Figures 3 and 4). Re-sults show that the correlation of the proposed ap-proaches with the MLID is higher than in systems in all occasions apart from 1.2 forCS Miami data. The latter is due to the word orderof the English and Spanish languages being similarin contrast to English/Mandarin CS. This leads tothe morpheme order having a better discriminationpower in the case of English/Mandarin CS than inEnglish/Spanish CS.",
  "P1.1/P1.2/P2 distribution analysisAt the last step of analysis the distributions oflanguages are measured on utterance level LID": "for monolingual (Utterance LID row in ),token level LID for CS (Token LID row in Ta-ble 11) and utterance level textual MLID for CS(P1.1/P1.2/P2 rows in ). The numbers re-veal that although the majority of the monolingualutterances are English in both corpora (54% forSEAME and 68% for Miami), it is not the pre-ferred ML when CS occurs in the utterance for allprinciples. The token LID distribution also doesnot seem to be correlated with the choice of theML in these corpora. In SEAME there seems tobe a strong preference towards using Mandarin asan ML (77%) when EL insertions are single words(P1.1). The preference is not as strong for Span-ish in the CS Miami subset (55%) but it is still abig difference in comparison to the monolingualdistributions (32%). P1.2 and P2 show a similardistribution of MLIDs with the numbers indicatingthe preference of speakers to use the non-Englishlanguage as the grammatical frame for a CS utter-ance.",
  "Conclusion": "To the best of our knowledge this is the first workthat precisely carries out the Matrix Language (ML)determination of a code-switched (CS) utterancebased on the Matrix Language Frame (MLF) the-ory and that compares Matrix Language Identity(MLID) to acoustic Language Identity (LID). Threemethods for ML determination in text and audioare implemented using the ideas and the conceptsof the MLF theory (Myers-Scotton, 1997). An ex-tensive correlation analysis of the MLID systemsfrom text and speech is carried out. A pretrainedLID system is adapted to the data by train-ing a mapping function , while also map-ping functions 1.1, 1.2, 2for MLID are trained. consistently outper-forms for ML determination from audio based on Matthews Correlation Coefficient (MCC). Com-paring the results to the ground truth ML annotationshows that the trained 1.2 and 2outperform in terms of F1-macro and MCCrespectively. Finally, this approach reveals thatdespite English dominating as the utterance LIDfor the monolingual utterances, non-English (Man-darin or Spanish) languages set the grammaticalframe for CS utterances.The proposed approaches can be used for ac-curate automatic analysis of CS text and audio.It can provide insight into the nature of CS forwhole datasets but also separate speakers and evenutterances. Further work will explore the useful-ness of the MLID implementations in Natural Lan-guage Processing and Automatic Speech Recogni-tion (ASR) applications, namely in language anddialogue modelling and also in end-to-end multi-task ASR the MLID component will be used as apart of the ASR setup. Additionally, further devel-opment of P2 is required where the system mor-phemes would be automatically determined from agiven set of CS data rather than using a closed setof POS tags.",
  "Limitations": "The main limitation of the method is related to dataavailability: there is limited ML-annotated CS dataopenly available to date. Therefore it is problem-atic to assess the quality of ML classification. MLidentity can be determined in CS data using theP1.1 but the principle can only be applied in caseof singleton EL insertions. Since there is no MLannotation, correlation was measured for most ofthe experiments which is difficult to assess. Fi-nally, although providing valuable insight into theCS data, the usefulness of the method is yet to betested in NLP and ASR applications.",
  "Brecht Desplanques, Jenthe Thienpondt, and Kris De-muynck. 2020. ECAPA-TDNN: Emphasized chan-nel attention, propagation and aggregation in TDNNbased speaker verification.In Interspeech 2020.ISCA": "Anuj Diwan, Rakesh Vaideeswaran, Sanket Shah,Ankita Singh, Srinivasa Raghavan, Shreya Khare,Vinit Unni, Saurabh Vyas, Akash Rajpuria, Chi-ranjeevi Yarra, et al. 2021. Multilingual and code-switching asr challenges for low resource indian lan-guages. arXiv preprint arXiv:2104.00235. Yukun Feng, Feng Li, and Philipp Koehn. 2022. To-ward the limitation of code-switching in cross-lingualtransfer. In EMNLP 2022, pages 59665971, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics."
}