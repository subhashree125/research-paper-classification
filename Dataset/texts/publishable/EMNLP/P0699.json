{
  "Abstract": "Large Language Models (LLMs) have demon-strated capability in instruction induction,generating instructions from demonstrations(input-output pairs). However, existing meth-ods often rely on large datasets or numerous ex-amples, which is impractical and costly in real-world scenarios. In this work, we propose alow-cost, task-level framework called INDUCT-LEARN. It induces pseudo instructions from afew demonstrations and a short phrase, addinga CoT process into existing demonstrations.When encountering new problems, the learnedpseudo instructions and demonstrations withthe pseudo CoT process can be combined intoa prompt to guide the LLMs problem-solvingprocess. We validate our approach on the BBH-Induct and Evals-Induct datasets, and the re-sults show that the INDUCT-LEARN frameworkoutperforms state-of-the-art methods. We alsoexhibit cross-model adaptability and achievesuperior performance at a lower cost comparedto existing methods.",
  "Introduction": "Although Large Language Models (LLMs) excel atadapting to unseen tasks through in-context learn-ing (Brown et al., 2020; Wei et al., 2022a; Chenet al., 2023; Wei et al., 2023), their inferential pro-cess is often implicit, making it challenging toexplicitly understand the underlying mechanisms.Honovich et al. (2023) reveal LLMs potential forinstruction induction, where they infer rules frominput-output pairs to create natural language in-structions, offering hope for mitigating the chal-lenge of implicit reasoning.Following these insights, Zhou et al. (2022), Sunet al. (2023), Zhang et al. (2023), Chen et al. (2024),Hsieh et al. (2024) innovated in utilizing input-output pairs for generating diverse prompts, aimedat selecting optimal prompts for downstream mod-els. These works generally assume the availabil- Task Content: The purpose of this task is to determine the final coordinate(x, y) after a series of movements in a 2D space. The movements can includesteps forward, backward, and turns (90 degrees left or right).Operational Steps: 1. Initialize the current position to (0, 0) and the current direction to north.2. Process each movement in the input sequence: a. If the movement is \"take X steps forward\", move X steps in the currentdirection and update the current position accordingly. [...]3. After processing all movements, return the final coordinate (x, y). [Q] If you take 1 step backward, then take 2 steps forward, then take 5 stepsbackward, then turn 90 degrees left, what coordinate are you at? Initial Position: (0, 0) Initial Direction: North (0 degrees)Movement 1: Take 1 step backward [...] New Position: (0, -1) New Direction: North (0 degrees)[...]Movement 4: Turn 90 degrees left [...]Since we've processed all movements, our final coordinate is (0, -4)....",
  "Test Quesiton": "[Task Name] 2d movement [Q] If you turn 90 degrees right, then take 2 steps backward, what coordinateare you at? [A] [Q] If you turn 90 degrees right, then take 2 steps backward, what coordinateare you at?[A] Let's refer to the 'Operational Steps' and think step by step. Initial Position: (0, 0) Initial Direction: North (0 degrees) Movement 1: Turn 90 degrees rightWe'll turn 90 degrees clockwise, so our new direction is east.New Position: (0, 0) New Direction: East (90 degrees)[...] Since we've processed all movements, our final coordinate is (-2, 0).So, the answer is (-2, 0). (0, -2)",
  "Pseudo Instructions": "[Q] If you take 1 step backward, then take 2 steps forward, then take 5 stepsbackward, then turn 90 degrees left, what coordinate are you at? [A] (0, -4)... : Comparison of model output from standardprompting and INDUCT-LEARN prompting. INDUCT-LEARN framework generates pseudo instructions andpseudo CoT demonstrations to help the model answerquestions more effectively.",
  "ity of substantial external resources, like extensivetraining datasets, for acquiring numerous demon-strationsa process that can incur signicant costsfor producing prompts. Moreover, most of these": "methods require an initial prompt (instruction) toperform prompt optimization.However, this differs from the situation of gen-eral users. Typical users usually only possess lim-ited data (a few examples) and are unwilling to in-vest signicant costs in prompt optimization usingthese methods. Moreover, these approaches opti-mize prompts for specic target models, and theoptimized prompts may not be effective for othermodels. Furthermore, general users often lack suf-cient prompting skills to communicate with LLMs(Desmond and Brachman, 2024; Mishra and Nouri,2022; Zamrescu-Pereira et al., 2023), making itchallenging to write good initial instructions.We propose a low-cost, task-level INDUCT-LEARN framework that can effectively improveperformance using only a few demonstrations anda very simple short phrase to limit the scope. Thisframework rst induces task-level pseudo instruc-tions based on the provided task-specic demon-strations and corresponding short phrase. Thesepseudo instructions encapsulate the Task Content,Input and Output Format, and Operational Steps.Then, similar to human inductive learning, themodel is asked to practice and infer specic ex-amples using its self-induced pseudo instructions,rewriting the demonstrations into pseudo CoT ex-amples. When encountering new questions, thelearned pseudo instruction and CoT demonstrationscan be combined into a prompt to guide the LLMsthinking and solve the problem.Our validation on custom datasets BBH-Inductand Evals-Induct shows that our proposed low-costINDUCT-LEARN framework achieves performancecomparable to carefully crafted human instruc-tions and signicantly outperforms state-of-the-artframeworks. Notably, powerful LLMs using theINDUCT-LEARN framework can even surpass theperformance of human-written instructions. Fur-thermore, our experiments demonstrate that theINDUCT-LEARN framework exhibits cross-modeladaptability, meaning that the prompt generated byone model can be effectively used by other mod-els to improve their performance. In terms of cost,we can outperform state-of-the-art frameworks at alower cost.",
  "PINDUCT = LM(I, D | P)(2)": "where P is the meta prompt we design to guidethe LLM to simulate an experts analysis, induc-ing instructions from the given demonstrations D.PINDUCT is the outcome of this step, containing fourimportant components: Task Content, which is adescription of this task; Input Format and OutputFormat, which describe the format of the input-output pairs (xi, yi), to guide the LLM in preciselyunderstanding the input and generating the accu-rate output; and Operational Steps, which providestep-by-step guidance to answer the given input xi.",
  "Models": "In our experiments, we conduct evaluations oneight instruction-tuned models across three series,including both open-source models and commer-cial APIs.For open-source models, we adoptMistral-7B (Jiang et al., 2023), Mixtral-8x7B(Jiang et al., 2024), Mixtral-8x22B, Llama 3 8B,and Llama 3 70B (Meta, 2024). On the commer-cial side, we select Gemini 1.0 Pro (Gemini Team,2023), Gemini 1.5 Flash, and Gemini 1.5 Pro (Gem-ini Team, 2024).",
  "Datasets": "To evaluate the capability of LLMs in inferringinstructions from demonstrations without explicithints, we adopt BBH-Induct and Evals-Induct.Both datasets are sufciently challenging and donot perform well when LLMs are used in zero-shotsettings without given instructions. BBH-InductWe introduce a modication of theBIG-Bench-Hard (BBH) dataset (Suzgun et al.,2022) called BBH-Induct, specically designedto evaluate the instruction induction capabilities ofLLMs. Unlike the original BBH dataset, which con-tains explicit hints in each instance, BBH-Induct re-moves information that is unrealistic for real-world cases where only input-output pairs are available,and the LLM is expected to solve problems with-out explicit instructions. To address this issue, weeliminate the prex and sufx templates in the ques-tions. Due to space constraints, the constructionprocess is shown in Appendix A. Each BBH-Inducttask contains approximately 96 to 600 examples,amounting to a total of 5,161 instances. For de-tailed descriptions of the tasks, refer to the BBHtask descriptions in (Suzgun et al., 2022). Evals-InductWe adopt another challengingdataset: Evals-Induct, derived from the OpenAIEvals project1. This project collects tasks that evenGPT-4 struggles with. We select 24 suitable chal-lenge tasks in Evals-Induct. Similar to BBH-Induct,we include only question-answer pairs without ad-ditional information as demonstrations, aiming touse Evals-Induct to evaluate the induction capa-bility of LLMs. LLMs are required to performinductive reasoning across multiple examples andgenerate suitable instructions that are benecial foranswering the questions. Each Evals-Induct taskhas approximately 91 to 378 examples, totaling3,683 instances. For detailed information on thetasks, please refer to in Appendix.",
  "INDUCT(Ours)": "Llama 3 8B30.2845.5138.9643.49-2.029.1921.0415.2418.21-2.83Llama 3 70B59.4570.3059.3769.93-0.3718.8336.5224.2247.9111.39Mistral 7B37.5841.3040.2637.59-3.7112.7820.8713.9616.32-4.55Mixtral 8x7B38.9748.3240.5144.71-3.6112.6425.6821.2025.22-0.46Mixtral 8x22B45.5758.6145.9167.018.416.9636.8331.9240.503.67Gemini 1.0 Pro46.2157.6450.3853.55-4.0916.1336.6419.5824.23-12.41Gemini 1.5 Flash42.6256.6954.2765.098.418.1742.8725.9544.441.57Gemini 1.5 Pro52.4469.4562.8178.969.5122.4249.3837.5051.131.75 : Results of instruction generation experiment across models and tasks in zero-shot settings. We evaluateexact match accuracy (%) and calculate the mean of all the tasks of both BBH-Induct and Evals-Induct. The bestresults of each setting are marked in bold, and underlines denote the best performance excluding human instructions. denotes the performance gap between INDUCT(Ours) and human instructions (Ours - Human). SP denotes ShortPhrase.",
  "only 9 demonstrations and utilizes up to 3 CoTresults in the inference scenario": "Experimental SetupsTo optimize experimentalefciency and stability, we utilize the API servicesof all eight LLMs, including Google, Groq2, andTogether AI3. Details such as model version andsafety settings are listed in Appendix B. Due toresource constraints, experiments with Gemini 1.5Pro and Mixtral 8x22B are conducted on a ran-dom sample of 25 instances per task per dataset.All other models are evaluated on the completedatasets.",
  "Quality of Instruction Generation": "We conduct experiments in the zero-shot setting toevaluate the quality of different instructions. Giventhe instructions generated by different methods andthe question, we calculate the accuracy of the re-sults that LLMs generate. According to ,PINDUCT signicantly outperforms the SOTA base-line Self-Discover (Zhou et al., 2024) in 15 out of16 settings, with the only exception being the Mis-tral 7B model on BBH-Induct dataset. Comparedto instructions given by human experts, PINDUCTachieves a comparable performance, with a minorloss in the BBH-Induct dataset (3 vs. 5) and a tiein the Evals-Induct dataset.Notably, we observe that our PINDUCT benetsmore as the models scale increases.For theLlama 3 series, as the model scale increases from8B to 70B, PINDUCT improves by 26.44% (from43.49% to 69.93%), while human instruction im-proves by 24.79 (from 45.51% to 70.30%). As the model scale grows, PINDUCT gradually closesthe gap with human instruction. Hence, the gap be-tween PINDUCT and human instruction narrows from-2.02% (45.51% minus 43.49%) to -0.37% (70.3%minus 69.93%). There is a similar trend in the Mis-tral and Gemini series. For the Mixtral 8x22B,Gemini 1.5 Flash, and Gemini 1.5 Pro models,PINDUCT even surpassed human instructions. Thisnding aligns with the observations of Honovichet al. (2023) that larger and more capable modelsexhibit stronger instruction induction abilities.We can conclude that our PINDUCT is signicantlybetter than human expert instructions in modernpowerful LLMs.",
  "Effect of LEARN Stage": "In order to measure the effectiveness of our gener-ated pseudo CoT process, we conduct a 3-shot ex-periment across different models and instructions.The rst row of each model in representsthe 3-shot performance without CoT, and the sec-ond row represents the 3-shot performance with ourpseudo CoT PLEARN. We can see that in most set-tings, adding our PLEARN benets the performance.Specically, 23 out of 24 settings in BBH-Inductand 18 out of 24 settings in Evals-Induct show im-provement.Moreover, compared to SOTA baseline, ourPINDUCT-LEARN outperforms Self-Discover in mostof the settings after adding the pseudo CoT pro-cess. It only loses in the Llama 3 8B and Mistral7B models of BBH-Induct and the Gemini 1.0 Promodel of Evals-Induct. This aligns with our pre-vious ndings, showing that our method benetsmore as the model becomes stronger. Furthermore,Our PINDUCT-LEARN method also achieved perfor-mance comparable to human instructions and even",
  "(b) Cross-model Improvement (Evals-Induct)": ": Comparison of INDUCT (Ours) and Short Phrase + 0-shot under different cross-model settings, where theprompt induction model and task execution/inference model are different. Accuracy is expressed as a percentage.Darker green indicates greater performance improvement with INDUCT (Ours), while darker red indicates greaterperformance decline. The comparison is conducted on the (a) BBH-Induct and (b) Evals-Induct datasets.",
  "Cross-Model Performance": "We present the cross-model adaptability and effec-tiveness of PINDUCT_LEARN in . Specically,the gure shows the performance difference (i.e.,improvement) between using our PINDUCT_LEARNand the original Short Phrase + 3-shot setting.Cross-model refers to using PINDUCT_LEARN gener- ated by one model to perform tasks with anothermodel. For example, we use the Gemini 1.5 Promodel to generate P Gemini1.5ProINDUCT_LEARN, and then useP Gemini1.5ProINDUCT_LEARNas the prompt for the Mixtral8x22B model to perform tasks. Compared to Mix-tral 8x22B directly using Short Phrase, the perfor-mance improves by 42.03.",
  "(h) Gemini 1.5 Pro": ": The accuracy difference distribution across 24 Eval-Induct tasks for 8 models using our Induct-LearnPrompt compared to the Short Phrase under 3-shot settings. Tasks are sorted by accuracy difference from low tohigh, indicating that tasks to the right benet more from our method. Improvements are marked in green, whileperformance declines are highlighted in red.",
  "Our experiments indicate that larger LLMs bene-t more signicantly from the proposed INDUCT-LEARN framework compared to smaller models": "INDUCT stageThis observation aligns with nd-ings by Honovich et al. (2023), who noted thatlarger models exhibit stronger instruction inductionabilities. Therefore, in the INDUCT stage, largermodels possess enhanced comprehension capabili-ties, enabling them to generate higher-quality taskinstructions PINDUCT from a limited number of ex-amples, resulting in better performance. LEARN stageIn the LEARN stage, as shown inEquation 4, we lter out the incorrect results toobtain DLEARN, the set of demonstrations wherethe models predicted output yj matches the groundtruth yj. We dene the Pass Rate (PR) as follows:",
  "i=1PRi(9)": "where M is the number of tasks and PRi is thepass rate of the i-th task. shows the macro pass rates across var-ious models and datasets. We can observe thatthe larger the model in each series, the higher themacro pass rate. Specically, the strongest models(i.e., Llama 3 70B, Mixtral 8x22B, and Gemini 1.5Pro) achieve the highest macro pass rate in theirrespective series. On the other hand, a low macropass rate may occur in situations where the num-ber of generated CoT demonstrations is less than",
  "Different Number of Demonstrations": "To comprehensively evaluate the impact of thenumber of demonstrations N during the INDUCTstage and the number of demonstrations k usedin the LEARN stage, we conducted additional ex-periments by sampling instances from each taskin the BBH-Induct dataset. For both open-sourceand closed-source models, we initially consideredselecting the models with the highest parametercounts. However, due to the high cost of Gemini1.5 Pro, we ultimately selected the Mixtral 8x22Band Gemini 1.5 Flash models for our experiments.Specically, for the Mixtral 8x22B model, we sam-pled 25 instances per task, consistent with our pre-vious experiments. For the Gemini 1.5 Flash model,we sampled 50 instances per task. INDUCT Stage shows that as N in-creases, performance continuously improves. Thisindicates that more demonstrations help induce(generate) better instructions, thereby enhancingthe models performance. In practice, consider-ing our paper mainly focuses on giving a lim-ited amount of training data (X, Y pairs withoutCoT), and best utilize this data to improve LLMsquestion-answering capabilities, we chose to set Nto 9 for subsequent experiments. LEARN Stage demonstrates that whenN is xed at 9, an increase in k consistently leadsto improved performance across all models. Thissuggests that a greater number of model-generateddemonstrations, augmented with Chain-of-Thought(CoT), can signicantly enhance the models rea-soning abilities. : Comparison of average accuracy and number of LLM requests across 8 models under an extreme conditionwhere a single task with only one instance is executed. The congurations compared are Short Phrase + 0-shot,Short Phrase + 3-shot, Self-Discover + 0-shot, Self-Discover + LEARN, INDUCT(Ours), and INDUCT-LEARN on theBBH-Induct dataset.",
  "Meta Prompt Ablation": "To prevent LLMs from being sensitive to the givenprompt, we conducted an ablation study to assessthe consistency of our proposed INDUCT-LEARNframework. We used GPT-4o to rewrite the origi-nal meta prompt, and these rewritten prompts areshown in Figures 10 and 13 in Appendix D.6. Weselected the Gemini 1.5 Flash model as the rep-resentative and sampled 50 data points from eachtask in the BBH-Induct dataset for analysis. Theseexperiments were conducted using the INDUCT+ 0-shot setup. indicates that differentmeta-prompts do not signicantly impact the per-formance of our INDUCT Stage.",
  "In scenarios where the same task is executed multi-ple times, prompt generation costs can be largely": "ignored. To comprehensively assess potential costs,we analyze the worst-case scenario where onlya single task is executed once, making the costof prompt generation a signicant factor. Giventhat different tasks and models have varying tokenamounts, and even different models may use differ-ent tokenization methods, we chose to standardizevariables by analyzing costs based on the number ofrequests. For detailed request calculations, pleaserefer to in Appendix. As shown in , our Induct-Learn methoddemonstrates signicant efciency and accuracyadvantages. For the 0-shot and PLEARN settings,INDUCT-LEARN requires fewer requests than Self-Discover but provides higher average accuracy inalmost all cases. In almost all settings (except Mis-tral 7B), our PINDUCT only requires one additionalrequest to signicantly improve performance overa single request for Short Phrase + 0-shot/3-shot.Notably, our methods advantages increase withstronger models. For example, with powerful mod-els like Llama 3 70B, Mixtral 8x22B, Gemini 1.5Flash, and Gemini 1.5 Pro, our PINDUCT (requir-ing only 2 requests) can outperform Self-Discoverwith PLEARN (requiring 13 requests). These resultsindicate that our Induct-Learn method improvesaccuracy without signicantly increasing computa-tional costs.",
  "Related Work": "Inductive Abilities of LLMsInductive reason-ing, a key cognitive ability, has been extensivelystudied in the context of LLMs (Liu et al., 2024;Yu et al., 2023). Yang et al. (2022) introduceda task for inferring natural language rules fromfacts, demonstrating LLMs inductive capabilities.Zhu et al. (2023) leveraged these capabilities togenerate rules from examples, which were thenused as input prompts. Qiu et al. (2024) and Hanet al. (2023) conrmed that LLMs, particularlyGPT-4, can derive rules from input-output pairs,performing on par with humans on classic propertyinduction tasks. Honovich et al. (2023) showcasedLLMs generating instructions from input-outputexamples, termed instruction induction. Our ex-perimental results support the observation that theinductive ability of LLMs increases with their scale,with larger models generating better instructionsthat lead to greater performance improvements indownstream tasks. Instruction GenerationBuilding on Honovichet al. (2023), several studies have focused on gen-erating instructions using LLMs. Some researchersproposed methods to train LLMs to generate in-structions from input examples and labels, suchas reverse learning (Ye et al., 2022), generatingand ranking candidate instructions (Zhou et al.,2022; Zhang et al., 2023), and iteratively reninginstructions by combining input-output examples(Sun et al., 2023; Chen et al., 2024). Addition-ally, some studies have proposed methods to itera-tively optimize prompts, such as using genetic evo-lution (Xu et al., 2022; Fernando et al., 2023; Yangand Li, 2023; Cui et al., 2024; Guo et al., 2024)or gradient-based approaches (Tang et al., 2024;Pryzant et al., 2023). However, these methods of-ten require substantial computational resources. Incontrast, INDUCT-LEARN focuses on generatinga single instruction and its CoT, improving ef-ciency and reducing resource usage. Our resultsindicate that our methods prompts possess cross-model capabilities rather than being tailored forspecic models. LLM ReasoningCoT prompts enable large lan-guage models to generate intermediate reasoningsteps, guiding and explaining solutions (Wei et al.,2022b; Kojima et al., 2022; Nye et al., 2021).Zhang et al. (2022) leveraged datasets to select di-verse samples and generate CoT examples. Wang et al. (2022) extracted multiple reasoning chainsduring decoding and chose the most consistent one.(Wang et al., 2023) introduced a planning step be-fore answering questions, followed by solving theproblem according to the plan. In contrast, ourapproach operates at the task level. The INDUCTstage generates PINDUCT from a small number ofdemonstrations for a given task, followed by theLEARN stage, which converts the demonstrationsinto CoT examples. As a result, our method doesnot require a training set, unlike the aforementionedapproaches. Moreover, when encountering newquestions, only a single request is needed.",
  "Conclusion": "In this work, we proposed The INDUCT-LEARNframework that leverages the instructional induc-tion capabilities of LLMs to enhance model per-formance in low-resource settings.By induc-ing pseudo instructions from limited input-outputpairs and a short guiding phrase, the frameworkguides the model in rewriting demonstrations intopseudo chain-of-thought examples. When encoun-tering new problems, the INDUCT-LEARN prompt,which includes the learned pseudo instructions andpseudo CoT demonstrations, guides the LLMs rea-soning and problem-solving approach. Empiricalevaluations on two datasets demonstrate the effec-tiveness of our framework, outperforming state-of-the-art methods. The cross-model adaptabil-ity exhibited by the INDUCT-LEARN frameworkfurther highlights its practical utility, as promptsgenerated by one model can effectively enhancethe performance of others. Our work underscoresthe potential of harnessing the instructional induc-tion capabilities of LLMs to develop cost-effectivesolutions for improving model performance in real-world, low-resource scenarios. We open up newavenues for exploring the integration of inductivelearning strategies with the capabilities of LLMs,paving the way for more efcient and accessibleapproaches to enhancing model performance.",
  "Limitations": "Dependence on Instruction-Following ModelsLLMs employing the INDUCT-LEARN frameworkrequire instruction tuning. This allows them tofollow the directives of the Meta Prompt duringthe INDUCT stage, inducting PInduct from input-output pairs. Models lacking instruction tuning areunlikely to complete this task successfully. Necessity of Large-Scale ModelsOur experi-mental results indicate that the ability for instruc-tion induction improves with the strength of themodel. To achieve results surpassing human in-struction, more powerful LLMs are required to gen-erate PINDUCT-LEARN.",
  "This work was supported by National Science andTechnology Council, Taiwan, under grants NSTC112-2634-F-002-005 -, and Ministry of Education(MOE) in Taiwan, under grants NTU-113L900901": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, T. J. Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler,Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.ArXiv,abs/2005.14165. Lichang Chen, Jiuhai Chen, Tom Goldstein, HengHuang, and Tianyi Zhou. 2024. InstructZero: Ef-cient instruction optimization for black-box largelanguage models. In Proceedings of the 41st Inter-national Conference on Machine Learning, volume235 of Proceedings of Machine Learning Research,pages 65036518. PMLR. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen,and Hsin-Hsi Chen. 2023. Self-ICL: Zero-shot in-context learning with self-generated demonstrations.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1565115662, Singapore. Association for Computa-tional Linguistics. Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,Damien Lopez, Kamalika Das, Bradley Malin, andKumar Sricharan. 2024. Phaseevo: Towards uniedin-context prompt optimization for large languagemodels. ArXiv, abs/2402.11347.",
  "Simon J. Han, Keith Ransom, Andrew Perfors, andCharles Kemp. 2023. Inductive reasoning in humansand large language models. Cognitive Systems Re-search, 83": "Or Honovich, Uri Shaham, Samuel R. Bowman, andOmer Levy. 2023. Instruction induction: From fewexamples to natural language task descriptions. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 19351952, Toronto, Canada.Association for Computational Linguistics. Cho-Jui Hsieh, Si Si, Felix Yu, and Inderjit Dhillon.2024. Automatic engineering of long prompts. InFindings of the Association for Computational Lin-guistics ACL 2024, pages 1067210685, Bangkok,Thailand and virtual meeting. Association for Com-putational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de Las Casas,Emma Bou Hanna, Florian Bressand, GiannaLengyel,Guillaume Bour,Guillaume Lample,Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. ArXiv, abs/2401.04088. Albert Qiaochu Jiang, Alexandre Sablayrolles, ArthurMensch, Chris Bamford, Devendra Singh Chap-lot, Diego de Las Casas, Florian Bressand, Gi-anna Lengyel, Guillaume Lample, Lucile Saulnier,Lelio Renard Lavaud, Marie-Anne Lachaux, PierreStock, Teven Le Scao, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2023. Mis-tral 7b. ArXiv, abs/2310.06825.",
  "Meta. 2024. Introducing meta llama 3: The most capa-ble openly available llm to date": "Swaroop Mishra and Elnaz Nouri. 2022. Help me think:A simple prompting strategy for non-experts to createcustomized content with models. In Annual Meetingof the Association for Computational Linguistics. Maxwell Nye, Anders Andreassen, Guy Gur-Ari,Henryk Michalewski, Jacob Austin, David Bieber,David Dohan, Aitor Lewkowycz, Maarten Bosma,David Luan, Charles Sutton, and Augustus Odena.2021. Show your work: Scratchpads for interme-diate computation with language models.ArXiv,abs/2112.00114. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-guang Zhu, and Michael Zeng. 2023. Automaticprompt optimization with \"gradient descent\" andbeam search. In Conference on Empirical Methodsin Natural Language Processing. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar,Valentina Pyatkin, Chandra Bhagavatula, BailinWang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xi-ang Ren. 2024. Phenomenal yet puzzling: Testinginductive reasoning capabilities of language modelswith hypothesis renement. In The Twelfth Inter-national Conference on Learning Representations,ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-Review.net.",
  "Hong Sun, Xue Li, Yi Xu, Youkow Homma, Qin-hao Cao,Min man Wu,Jian Jiao,and De-nis Xavier Charles. 2023.Autohint: Automaticprompt optimization with hint generation. ArXiv,abs/2307.07415": "Mirac Suzgun, Nathan Scales, Nathanael Scharli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsinChi, Denny Zhou, and Jason Wei. 2022. Challengingbig-bench tasks and whether chain-of-thought cansolve them. In Annual Meeting of the Association forComputational Linguistics. Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, SiyuanLu, Yaliang Li, and Ji-Rong Wen. 2024. Unleashingthe potential of large language models as promptoptimizers: An analogical analysis with gradient-based model optimizers. ArXiv, abs/2402.17564. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, YunshiLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In An-nual Meeting of the Association for ComputationalLinguistics.",
  "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Huai hsin Chi, and Denny Zhou. 2022.Self-consistency improves chain of thought reasoning inlanguage models. ArXiv, abs/2203.11171": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-gatama, Maarten Bosma, Denny Zhou, Donald Met-zler, Ed Huai hsin Chi, Tatsunori Hashimoto, OriolVinyals, Percy Liang, Jeff Dean, and William Fedus.2022a. Emergent abilities of large language models.ArXiv, abs/2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, andDenny Zhou. 2022b. Chain of thought promptingelicits reasoning in large language models. ArXiv,abs/2201.11903. Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-bert Webson, Yifeng Lu, Xinyun Chen, HanxiaoLiu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.Larger language models do in-context learning dif-ferently. ArXiv, abs/2303.03846. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-gang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps:Genetic prompt search for efcient few-shot learn-ing. In Conference on Empirical Methods in NaturalLanguage Processing. Heng Yang and Ke Li. 2023. Instoptima: Evolution-ary multi-objective instruction optimization via largelanguage model-based instruction operators. In Con-ference on Empirical Methods in Natural LanguageProcessing.",
  "Fei Yu, Hongbo Zhang, and Benyou Wang. 2023. Natu-ral language reasoning, a survey. ACM ComputingSurveys": "J.D. Zamrescu-Pereira, Richmond Y. Wong, BjoernHartmann, and Qian Yang. 2023. Why johnny cantprompt: How non-ai experts try (and fail) to designllm prompts. In Proceedings of the 2023 CHI Confer-ence on Human Factors in Computing Systems, CHI23, New York, NY, USA. Association for ComputingMachinery. Zhihan Zhang, Shuohang Wang, Wenhao Yu, YichongXu, Dan Iter, Qingkai Zeng, Yang Liu, ChenguangZhu, and Meng Jiang. 2023. Auto-instruct: Auto-matic instruction generation and ranking for black-box language models. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 98509867, Singapore. Association for Com-putational Linguistics.",
  "AConstruction of BBH-Induct Dataset": "shows an example in the movie_recom-mendation task, where each question in BBH startswith Find a movie similar to, explicitly guiding themodel. In our BBH-Induct dataset, we remove thisinstruction to better reect real-world scenarios. [Task Instruction]Recommend movies similar to the given list of movies.[Question]Find a movie similar to Minority Report, Total Recall,Inside Out, Forrest Gump:Options:(A) Phenomena(B) Lilting(C) Catwoman(D) Edge of Tomorrow",
  "BDetailed Experimental Congurations": "Model VersionThe models used in this paperinclude several versions from Llama, Mistral, Mix-tral, and Gemini, each associated with specicAPI services. Llama 3 8B (llama3-8b-8192) andLlama 3 70B (llama3-70b-8192) were both ac-cessed through Groq. Mistral 7B (Mistral-7B-Instruct-v0.3) and Mixtral 8x22B (Mixtral-8x22B-Instruct-v0.1) were provided by To-gether AI, while Mixtral 8x7B (mixtral-8x7b-32768) was also accessed via Groq.In addi-tion, the Gemini series, which includes Gem-ini 1.0 Pro (gemini-1.0-pro-001), Gemini 1.5Flash (gemini-1.5-flash-001), and Gemini 1.5Pro (gemini-1.5-pro-001), were utilized throughGoogles API service. Safety SettingsWe disable safety settings forGemini to prevent Googles API from refusing torespond.The following harm categories wereset to BLOCK_NONE: HARASSMENT, HATE_SPEECH,SEXUALLY_EXPLICIT, and DANGEROUS_CONTENT.",
  "Mj=1 [|DLEARN,j| k]": "Mwhere M is the total number of tasks, DLEARN is theset of demonstrations generated during the LEARNstage, and k represents the number of requireddemonstrations. The formula sums 1 for each taskwhere |DLEARN,j| k, and 0 for subtasks wherethis condition is not met. presents the ll rates for various modelson the BBH-Induct and Evals-Induct datasets. Ahigher ll rate indicates that the model can producesufcient examples for effective few-shot learningin the LEARN stage. As observed, larger models,such as Llama 3 70B, Mixtral 8x22B, and Gemini1.5 Pro, demonstrate signicantly higher ll ratescompared to smaller models. This indicates thatlarger models are better at generating the neces-sary number of high-quality CoT demonstrations,leading to more robust few-shot learning.",
  "2d_movementEvaluate if GPT can keep track of its position and orientation while randomwalking on a 2D grid in rst-person POV": "anagramsGiven an anagram (word whose letters has been randomly shufed), nd theoriginal word. The dataset only takes 12+ character long words. The promptcould include a list of 100+ candidates (the anagram being one of those) whichwould make the guess easier for a human being but oddly enough, the accuracyis lower with this input.",
  "determinantAsks GPT only the answer of the determinant from 1x1 to 4x4 matrices": "forth_stack_simTests the models ability to keep track of a stack of numbers given a set of ANSForth words. The model is asked to respond to a series of numbers and wordswith the resulting stack representation. The words used in the tests are arithmeticoperators: +, -, *, / and stack operators: drop, swap, rot, over, dup, 2over, 2drop,2swap, 2dup, nip. The prompts and expected results on the stack are all less than15 numbers and words long. guess_the_singerThis evaluation measures the models ability to identify a singer or band byanalyzing the rst 10 words of a song. To ensure the evaluations fairness andfocus, we have excluded songs with multiple singers/bands and included onlythose published before 2021. To test the models performance, we provide itwith three potential choices and evaluate its accuracy in selecting the correct one.",
  "partially_solved_crossword_cluesThis evaluates the models ability to nd the answer to crossword clues, whensome of the letters of the answer are already provided": "points_on_line100 sets of vector coordinates in the form of (x, y, z), (x, y, z), with an ideal centrecoordinate. The coordinates have a random start position of (-10, -10, -10) to (10,10, 10) and a furthest maximum distance from origin per-component of 20. Allpositions are in steps of 0.01 for ease of readability and human understanding.",
  "recurrence-relationThis will evaluate the models performances when calculating the runtime ofrecurrence relations": "resistor_ohm_calculatorTests the models ability to calculate the resistance (in ohms) based on the colorsof resistors four bands. This eval doesnt test the models ability to calculatethe minimum/maximum value based on the tolerance. This eval doesnt test themodels ability to calculate the resistance of a resistor with ve or six bands. smiles_to_formulaConversion of SMILES (Simplied Molecular Input Line Entry System) strings a widely used ASCII string notation for molecular structures to the corre-sponding molecular formula (the types and numbers of atoms in the molecule).",
  "AVERAGE9.1415.852.7018.6933.515.1412.9118.266/8": ": Accuracy comparison for INDUCT-LEARN (Ours) and Short Phrase + 3-shot across various tasks in theBBH-Induct dataset. The Win Rate column represents the proportion of times INDUCT-LEARN (Ours) outperformsthe Short Phrase. Bold indicates a win rate greater than 0.5. You are an expert in the eld of NLP (Natural Language Processing), possessing exceptional data observation andanalysis skills. Your expertise includes extracting signicant rules from complex datasets and formulating precise taskinstructions based on these rules. Currently, you are focusing on analyzing a specic set of examples, deriving insights to formulate a clear and detailedtask description. This task description will serve as an instruction set, guiding the execution of the related tasks.",
  ": Meta Prompt": "You are an authority in the realm of Quantum Physics, known for your exemplary skills in data observation andtheoretical analysis. Your expertise lies in extracting signicant principles from intricate datasets and creating precisetask instructions based on these principles. Presently, you are concentrating on examining a specic set of examples to draw insights for formulating a lucid andcomprehensive task description. This description will serve as a set of guidelines directing the accomplishment of relatedtasks.",
  ": Meta Prompt 1 (Rewritten)": "You are an expert in the eld of Environmental Science, known for your outstanding observational and analytical abilities.Your skillset includes identifying key patterns in complex environmental data and crafting accurate task instructionsbased on these observations. Currently, you are concentrating on examining a particular set of examples to derive insights and create a comprehensivetask description. This task description will guide the completion of related assignments.",
  ": Meta Prompt 2 (Rewritten)": "You are an expert in the eld of materials science, possessing exceptional skills in data observation and analysis. Yourexpertise includes extracting signicant rules from intricate datasets and formulating precise task instructions based onthese rules. Presently, you are concentrating on analyzing a specic set of examples, deriving insights to create a clear andcomprehensive task description. This task description will act as a guide, directing the execution of related tasks.",
  "Ours01012": ": Breakdown of request usage from . Note that for Short Phrase, we use the human-provideddemonstrations in the dataset. For Self-Discover and Ours, we use the demonstrations generated after the LEARNstage. [Task Name]Object Counting [Question]I have a lettuce head, two yams, a cabbage, a stalk of celery, three heads ofbroccoli, a garlic, an onion, a carrot, a cauliflower, and a potato. [Answer]13 [Question]I have a cauliflower, a stalk of celery, a cabbage, and a garlic. [Answer]4 [Question]I have a car, and a toaster. [Answer]2 [Question]I have two cows, a snail, a fish, a mouse, a snake, and a frog. [Answer]7 [Question]I have four oranges, a peach, a plum, three apples, three grapes, tworaspberries, a nectarine, a strawberry, a blackberry, and a banana. [Answer]18 [...] You are an expert in the field of NLP (Natural Language Processing),possessing exceptional data observation and analysis skills. Yourexpertise includes extracting significant rules from complex datasetsand formulating precise task instructions based on these rules. Currently, you are focusing on analyzing a specific set of examples,deriving insights to formulate a clear and detailed task description.This task description will serve as an instruction set, guiding theexecution of the related tasks. The task description should include the following elements: Task Content: Clearly define the purpose of the task and the specificactivities required to be completed. Input Format: Provide detailed descriptions of the types of dataaccepted, their formats, and how to process these data effectively. Output Format: Clearly outline the expected result types and formats,including any necessary standards or specifications. Operational Steps: Detail the specific step-by-step proceduresrequired to complete the task. Please write the [Task Instruction] concisely and clearly to ensure it iseasily understandable and followable by users. Meta Prompt (System Prompt)Short Phrase and Demonstrations (User Prompt) Task Instruction: Object Counting Task Content: Count the total number of objects mentioned in a given text. Input Format: Text: A sentence or short paragraph listing various objects. Format: Plain text. Processing: Identify individual objects mentioned. Account for quantifiers (e.g., \"two,\" \"three,\" \"a\"). Output Format: Type: Integer. Format: A single number representing the total count of objects. Operational Steps: 1. Read the input text. 2. Identify all nouns or noun phrases that represent distinct objects. 3. Sum the number of each object, considering any preceding quantifiers. 4. Return the total count as a single integer.",
  ": INDUCT Stage: A Case Example in BBH-Induct": "[Question]I have a lettuce head, two yams, a cabbage, a stalk of celery,three heads of broccoli, a garlic, an onion, a carrot, acauliflower, and a potato. [Answer]Let's refer to the 'Operational Steps' and think step by step. Task Instruction: Object Counting Task Content: Count the total number of objects mentioned in a given text. Input Format: Text: A sentence or short paragraph listing various objects. Format: Plain text. Processing: Identify individual objects mentioned. Account for quantifiers (e.g., \"two,\" \"three,\" \"a\"). Output Format: Type: Integer. Format: A single number representing the total count of objects. Operational Steps: 1. Read the input text. 2. Identify all nouns or noun phrases that represent distinct objects. 3. Sum the number of each object, considering any preceding quantifiers. 4. Return the total count as a single integer. Please follow the instructions in \"Operational Steps\" and perform step-by-step deduction.In the last line, output the answer according to the rules in \"Output Format\". Pseudo Instructions (System Prompt) Demonstrations (User Prompt) 1. Read the input text: \"I have a lettuce head, two yams, a cabbage, a stalk of celery, three heads of broccoli, a garlic, an onion, a carrot, a cauliflower, anda potato.\" 2. Identify all nouns or noun phrases that represent distinct objects: lettuce head yams cabbage stalk of celery heads of broccoli garlic onion carrot cauliflower potato 3. Sum the number of each object, considering any preceding quantifiers: 1 lettuce head 2 yams 1 cabbage 1 stalk of celery 3 heads of broccoli 1 garlic 1 onion 1 carrot 1 cauliflower 1 potato Total: 1 + 2 + 1 + 1 + 3 + 1 + 1 + 1 + 1 + 1 = 13 4. Return the total count as a single integer: Pseudo Chain-of-Thought (COT) Example LEARN Stage (1)",
  ": LEARN Stage: A Case Example in BBH-Induct": "Task Instruction: Object Counting Task Content: Count the total number of objects mentioned in a given text. Input Format: Text: A sentence or short paragraph listing various objects. Format: Plain text. Processing: Identify individual objects mentioned. Account for quantifiers (e.g., \"two,\" \"three,\" \"a\"). Output Format: Type: Integer. Format: A single number representing the total count of objects. Operational Steps: 1. Read the input text. 2. Identify all nouns or noun phrases that represent distinct objects. 3. Sum the number of each object, considering any preceding quantifiers. 4. Return the total count as a single integer. Please follow the instructions in \"Operational Steps\" and perform step-by-step deduction. In the last line, output the answer according to the rules in \"OutputFormat\". [Question]I have a lettuce head, two yams, a cabbage, a stalk of celery, three heads of broccoli, a garlic, an onion, a carrot, a cauliflower, and a potato. [Answer]Let's refer to the 'Operational Steps' and think step by step. 1. Read the input text: [...] 2. Identify all nouns or noun phrases that represent distinct objects: lettuce head yams [...] potato 3. Sum the number of each object, considering any preceding quantifiers: 1 lettuce head 2 yams [...] 1 potato Total: 1 + 2 + 1 + 1 + 3 + 1 + 1 + 1 + 1 + 1 = 13 4. Return the total count as a single integer: [...] [Question]I have a piano, a flute, and four trombones.[Answer]Let's refer to the 'Operational Steps' and think step by step. INDUCT-LEARN PINDUCT-LEARN and New Question 1. Read the input text: \"I have a piano, a flute, and four trombones.\" 2. Identify all nouns or noun phrases that represent distinct objects: \"piano\" \"flute\" \"trombones\" 3. Sum the number of each object, considering any preceding quantifiers: \"a piano\" = 1 \"a flute\" = 1 \"four trombones\" = 4 Total: 1 + 1 + 4 = 6 4. Return the total count as a single integer: Model Output: Answering Question",
  ": INDUCT-LEARN framework: A Case Example in BBH-Induct": "[Task Name]2d movement [Question]If you take 5 steps forward, then take 4 steps backward, then turn 90 degreesleft, then take 4 steps backward, then take 3 steps backward, then take 1 stepforward, then take 1 step backward, then take 4 steps backward, then take 4steps backward, then take 2 steps backward, what coordinate are you at? [Answer](17, 1) [Question]If you take 1 step backward, then take 2 steps forward, then take 5 stepsbackward, then turn 90 degrees left, what coordinate are you at? [Answer](0, -4) [...] You are an expert in the field of NLP (Natural Language Processing),possessing exceptional data observation and analysis skills. Yourexpertise includes extracting significant rules from complex datasetsand formulating precise task instructions based on these rules. Currently, you are focusing on analyzing a specific set of examples,deriving insights to formulate a clear and detailed task description.This task description will serve as an instruction set, guiding theexecution of the related tasks. The task description should include the following elements: Task Content: Clearly define the purpose of the task and the specificactivities required to be completed. Input Format: Provide detailed descriptions of the types of dataaccepted, their formats, and how to process these data effectively. Output Format: Clearly outline the expected result types and formats,including any necessary standards or specifications. Operational Steps: Detail the specific step-by-step proceduresrequired to complete the task. Please write the [Task Instruction] concisely and clearly to ensure it iseasily understandable and followable by users. Meta Prompt (System Prompt)Short Phrase and Demonstrations (User Prompt) Task Instruction: 2d_movement Task Content:The purpose of this task is to determine the final coordinate (x, y) after a series of movements in a 2D space. The movements can include steps forward,backward, and turns (90 degrees left or right). Input Format:The input will be a sequence of movements, where each movement is separated by \"then\". The movements can be one of the following: \"take X steps forward\" or \"take X steps backward\", where X is a positive integer. \"turn 90 degrees left\" or \"turn 90 degrees right\". Output Format:The output should be a coordinate in the format (x, y), where x and y are integers. Operational Steps: 1. Initialize the current position to (0, 0) and the current direction to north (0 degrees).2. Process each movement in the input sequence: a. If the movement is \"take X steps forward\", move X steps in the current direction and update the current position accordingly. b. If the movement is \"take X steps backward\", move X steps in the opposite direction of the current direction and update the current position accordingly. c. If the movement is \"turn 90 degrees left\", turn 90 degrees counterclockwise and update the current direction accordingly. d. If the movement is \"turn 90 degrees right\", turn 90 degrees clockwise and update the current direction accordingly.3. After processing all movements, return the final coordinate (x, y). Note: The current direction can be represented as an angle in degrees (0-360), where 0 degrees is north, 90 degrees is east, 180 degrees is south, and 270degrees is west.",
  ": INDUCT Stage: A Case Example in Eval-Induct": "[Question]EXERCISE: If you take 1 step backward, then take 2 stepsforward, then take 5 steps backward, then turn 90 degrees left,what coordinate are you at? [Answer]Let's refer to the 'Operational Steps' and think step by step. Task Instruction: 2d_movement Task Content:The purpose of this task is to determine the final coordinate (x, y) after a series ofmovements in a 2D space. The movements can include steps forward, backward, andturns (90 degrees left or right). Input Format:The input will be a sequence of movements, where each movement is separated by\"then\". The movements can be one of the following:[...] Output Format:The output should be a coordinate in the format (x, y), where x and y are integers. Operational Steps: 1. Initialize the current position to (0, 0) and the current direction to north (0 degrees).2. Process each movement in the input sequence: a. If the movement is \"take X steps forward\", move X steps in the current direction andupdate the current position accordingly. [...]3. After processing all movements, return the final coordinate (x, y). Note: The current direction can be represented as an angle in degrees (0-360), where 0degrees is north, 90 degrees is east, 180 degrees is south, and 270 degrees is west. Pseudo Instructions (System Prompt) Demonstrations (User Prompt) Let's break down the movements step by step: Initial Position: (0, 0)Initial Direction: North (0 degrees) Movement 1: Take 1 step backwardSince we're moving backward, we'll subtract 1 from the current y-coordinate (since we're initially facing north).New Position: (0, -1)New Direction: North (0 degrees) Movement 2: Take 2 steps forwardSince we're moving forward, we'll add 2 to the current y-coordinate (since we're still facing north).New Position: (0, 1)New Direction: North (0 degrees) Movement 3: Take 5 steps backwardSince we're moving backward, we'll subtract 5 from the current y-coordinate (since we're still facing north).New Position: (0, -4)New Direction: North (0 degrees) Movement 4: Turn 90 degrees leftWe'll turn 90 degrees counterclockwise, so our new direction is west (270 degrees).New Position: (0, -4)New Direction: West (270 degrees) Since we've processed all movements, our final coordinate is (0, -4). Pseudo Chain-of-Thought (COT) Example LEARN Stage (1)",
  ": LEARN Stage: A Case Example in Eval-Induct": "Task Content: The purpose of this task is to determine the final coordinate(x, y) after a series of movements in a 2D space. The movements can include steps forward, backward, and turns (90 degrees left or right).Operational Steps: 1. Initialize the current position to (0, 0) and the current direction to north.2. Process each movement in the input sequence: a. If the movement is \"take X steps forward\", move X steps in the current direction and update the current position accordingly. [...]3. After processing all movements, return the final coordinate (x, y). [Question]If you take 1 step backward, then take 2 steps forward, then take 5 steps backward, then turn 90 degrees left, what coordinate are you at? [Answer] Let's refer to the 'Operational Steps' and think step by step. Let's break down the movements step by step: Initial Position: (0, 0)Initial Direction: North (0 degrees) Movement 1: Take 1 step backwardSince we're moving backward, we'll subtract 1 from the current y-coordinate (since we're initially facing north).New Position: (0, -1)New Direction: North (0 degrees) Movement 2: Take 2 steps forwardSince we're moving forward, we'll add 2 to the current y-coordinate (since we're still facing north).New Position: (0, 1)New Direction: North (0 degrees) Movement 3: Take 5 steps backwardSince we're moving backward, we'll subtract 5 from the current y-coordinate (since we're still facing north).New Position: (0, -4)New Direction: North (0 degrees) Movement 4: Turn 90 degrees leftWe'll turn 90 degrees counterclockwise, so our new direction is west (270 degrees).New Position: (0, -4)New Direction: West (270 degrees) Since we've processed all movements, our final coordinate is (0, -4).[...] [Q] If you turn 90 degrees right, then take 2 steps backward, what coordinate are you at?[A] Let's refer to the 'Operational Steps' and think step by step. INDUCT-LEARN PINDUCT-LEARN and New Question Let's break down the movements step by step: Initial Position: (0, 0)Initial Direction: North (0 degrees) Movement 1: Turn 90 degrees rightWe'll turn 90 degrees clockwise, so our new direction is east (90 degrees).New Position: (0, 0)New Direction: East (90 degrees) Movement 2: Take 2 steps backwardSince we're moving backward, we'll subtract 2 from the current x-coordinate (since we're facing east).New Position: (-2, 0)New Direction: East (90 degrees) Since we've processed all movements, our final coordinate is (-2, 0). So, the answer is (-2, 0). Model Output: Answering Question"
}