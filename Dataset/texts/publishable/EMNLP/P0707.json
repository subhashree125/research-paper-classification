{
  "Abstract": "The increase in parameter size of multimodallarge language models (MLLMs) introducessignificant capabilities, particularly multimodalin-context learning, where MLLMs enhancetask performance without updating pre-trainedparameters. However, this effectiveness hingeson the appropriate selection of in-context exam-ples, a process currently biased towards visualdata, overlooking textual information. Moreimportantly, the area of supervised retrieversfor retrieval of multimodal in-context learning,crucial for optimal in-context example selec-tion, continues to be uninvestigated. Our studyprovides an in-depth evaluation of the impactof textual information on the unsupervised se-lection of in-context examples in multimodalcontexts, uncovering a notable sensitivity ofretriever performance to the employed modali-ties. Based on the above finding, we introducea novel supervised MLLM prompt retrieverMSIER that leverages a trained retriever basedon MLLMs confidence to select examples,which enhances multimodal in-context learningefficiency. This approach is validated throughextensive testing across three different tasks,demonstrating the methods effectiveness. Ad-ditionally, we investigate the influence ofmodalities on our supervised retrieval methodstraining and explore the transferability of thesupervised prompt retriever. This explorationpaves the way for future advancements, high-lighting the potential for refined in-contextlearning in MLLMs through the strategic use ofmultimodal data. The public code is availableat",
  "MLLM": ": An overview of multimodal in-context exam-ple retrieval: This process involves receiving an imageor an image-text query from the test dataset, and thenusing a retrieval mechanism to find similar examplesin a training dataset. These examples and the originalquery (collectively called the prompt) are then inputtedinto a MLLM to generate the output. high performance on downstream tasks without anyupdate of parameters that require extra computingresources for the further training process. To be spe-cific, the primary advantage of in-context learninglies in its capacity to assimilate and adapt to newinformation from its context. This is achieved byutilizing a series of question-and-answer pairs fromthe training dataset, known as in-context exam-ples, which enable learning without necessitatingupdates to the models parameters. Significantly,recent developments in multimodal large languagemodels (MLLMs), which are constructed basedon the foundations of large language models, havesimilarly exhibited capabilities in in-context learn-ing (Alayrac et al., 2022; Peng et al., 2023; Zhaoet al., 2023; Yin et al., 2023), termed as multimodalin-context learning (M-ICL).A notable advantage of M-ICL lies in its ability to utilize a singular model across a variety of lan-guage comprehension tasks. Nevertheless, (Chenet al., 2023a; Wu et al., 2023; Ye et al., 2023) re-vealed that the efficacy of this approach in down-stream applications can differ significantly, influ-enced by the selection of in-context examples. Thisrevelation has fueled interest in the concept of in-context example retrieval, as illustrated in . In this process, training examples (memory) fora given test instance (query) are selected for theprompt, guided by a certain similarity metric.In contrast to Natural Language Processing(NLP) (Dong et al., 2023; Coda-Forno et al., 2023),where the primary input is text, multimodal taskspredominantly utilize a single image (image-textpair) as input and the memory for retrieval consistsof multimodal examples. Current methodologiesfor unsupervised retrieval of in-context examples inmultimodal large language models (MLLMs) pre-dominantly focus on visual information(Alayracet al., 2022; Yang et al., 2022; Gu et al., 2023),thereby overlooking the significance of linguisticdata. This oversight prompts inquiries regardingthe impact of textual content on retrieval processesand whether the inclusion of additional modalitiescould potentially enhance the efficacy of MLLMsin-context learning capabilities. Moreover, the su-pervised retrieval framework for M-ICL has re-ceived minimal attention so far.In this work, our attention is centered on M-ICL,an emerging concept in the MLLMs field with lim-ited existing research on its practical application.Specifically, we undertook a thorough investiga-tion into the effects of textual information for re-trieval of in-context examples in M-ICL. Throughthis research, we have developed a supervised re-triever that aligns with existing knowledge fromMLLMs. This framework achieves high efficiencyand demonstrates excellent transferability acrossvarious datasets and sizes of MLLMs. The contri-butions of this paper are summarized as follows: We present the first thorough analysis into therole of textual modal in both unsupervised andsupervised retrieval of in-context examples forM-ICL and demonstrate that the addition oftextual modal plays a crucial role in improvingM-ICL performance.",
  "By considering both visual and textual in-formation when selecting in-context exam-ples, we design a Multimodal Supervised In-": "context Examples Retrieval (MSIER) frame-work with more efficient example selectionperformance by using a foundation MLLMscorer to evaluate the relevance and suitabilityof potential examples from multimodal data. Extensive experiments on three typical multi-modal tasks demonstrate the high efficiency ofour constructed supervised retrieval methodthat achieves the best performance. Besides,we further provide valuable insights regardingthe importance and transferability of the su-pervised method in selecting highly relevantin-context examples for M-ICL.",
  "Related Work": "In-context Examples Retrieval in NLP and CVThe field of natural language processing has iden-tified that the choice of in-context examples sig-nificantly influences performance, as evidencedby (Agrawal et al., 2022) and (Min et al., 2022b).Furthermore, the construction of these in-contextexamples, often referred to as prompts, includingaspects such as relevance and diversity of retrievedexamples, has been reported to impact performanceas well. This understanding has guided the com-munity toward investigating optimal methods forselecting effective in-context examples for largelanguage models. In their study, (Liu et al., 2021)posited that effective in-context examples shouldbear semantic similarity to query sentences. Basedon this hypothesis, they advocated for the selec-tion of the nearest neighbors in the training set, asdetermined by a sentence encoder like RoBERTa(Liu et al., 2019). (Rubin et al., 2022) initiallyemployed an unsupervised approach to gatheringpotential candidates, from which the most suitableexamples were selected using a supervised method.Correspondingly, (Zhang et al., 2023) exploredthe application of a supervised retriever within thescope of visual in-context learning, demonstratingenhanced performance outcomes. Nevertheless, thesignificance of textual information is overlookedwithin unsupervised and supervised retrievals forM-ICL, and the impact of various modalities on thedevelopment of the supervised retriever continuesto be an area requiring investigation.Multimodal Retrieval-augmented GenerationRetrieval augmentation for multimodal models en-hances their capabilities by infusing externally re-trieved information into their workflow. (Chenet al., 2022) proposed MuRAG, which accesses an external multimodal memory to augment languagegeneration. (Yasunaga et al., 2023) presented thefirst retrieval-augmented multimodal model thatenables a base model to refer to relevant text andimages fetched from external memory. (Ramoset al., 2023) introduced a new approach to imagecaptioning that generates sentences given the inputimage and retrieved captions. The retrieval processin RAG is designed to retrieve supplementary infor-mation to enhance the context for text generationand to produce more informed and contextuallyrich outputs. On the other hand, the retrieval ofin-context examples in M-ICL aims to find exam-ples that demonstrate how to perform a given task,providing explicit guidance to the model on howto generate its output. However, the role of tex-tual information in the task of in-context exampleretrieval has remained unexplored so far.In-context Learning In-context learning wasfirst proposed by (Brown et al., 2020) in their pa-per introducing GPT-3. It is a significant depar-ture from the traditional learning method based onstochastic gradient descent and does not involveany parameter updates: the model needs to be pro-vided with a few examples of the task as part of thecontext for text generation. The size of the modeland training data were thought to be key to traininga model with in-context learning capabilities. Morerecently, there has been more research on the exactcauses of in-context learning. (Min et al., 2022a)has proposed MetaICL, a meta-training frameworkto elicit in-context learning capabilities in text-onlylanguage models. MetaICL conditions each exam-ple with related in-context examples during train-ing. In the context of MLLMs, ICL has been ex-tended to more modalities, leading to MultimodalICL(Alayrac et al., 2022; Li et al., 2023; Chen et al.,2023b).",
  "Multimodal In-Context Learning": "Within the realm of multimodal tasks such as im-age captioning, a pre-trained MLLM utilizes an im-age and its corresponding caption as an in-contextexample to delineate requirements of the task essentially instructing the model on the expectedform of input and output. Subsequently, when pre-sented with a new image, the model can producea more precise caption based on these learned pat-terns. Crucially, M-ICL maintains the parametersof the pre-trained model unchanged, thereby offer- ing a more resource-efficient approach for tacklingdownstream tasks.Regularly, given a memory (training dataset)D = [xi, yi]Ni=1 comprising N pairs of multimodal-ities information and their corresponding labels,alongside a query example xq from the test datasetand the pre-trained model f, the process of in-context learning can be described as follows:",
  "yq = f([Cp, xq])(1)": "where the context prompt Cp consists of a sequenceof input-output pairs from D, e.g., the input is animage and the output is its caption for the cap-tioning task. Specifically, prompt Cp serves as acontextual guide, directing the model to generatethe optimal yq corresponding to query xq, whileavoiding any alterations to the parameters of thelarger model.",
  "Importance of text information forunsupervised MLLM retrieval": "As for the framework of the Multimodal Unsu-pervised In-context Examples Retrieval (MUIER)which is a prevalent retrieval methodology, ouranalysis initially focuses on the contribution of var-ious modalities. Traditionally, existing MLLMsthat present outstanding M-ICL capability have pri-marily utilized image data to facilitate M-ICL tasks.For instance, Flamingo (Alayrac et al., 2022) em-ployed the Retrieval-based In-Context Example Se-lection (RICES) strategy (Yang et al., 2022) for theidentification of appropriate in-context examples,predominantly assessing the similarity between thequery image and the images stored in the memory(training dataset). Nonetheless, this approach over-looks the significant role of textual information,which indeed influences the efficacy of M-ICL.To validate the effectiveness of text-augmentedMUIER, we performed a comprehensive compari-son across diverse configurations of unsupervisedretrievers, encompassing three main settings specif-ically designed for the image captioning task: (1)Q-I-M-I (Query-Image-Memory-Image) indicatesthe case where only image information of image-text pairs preserved in the memory was applied forthe retrieval of context. As described in RICES,we only consider the cosine similarity betweenthe query image and the memory image based onvision features extracted by the unsupervised re-triever. (2) Q-I-M-IT describes the standard settingfor MUIER by pairing the memory image with its",
  "POS": ": Overview of the MSIER Method: The fundamental principle involves assessing the in-context learningperformance for each source instance, thereafter identifying and choosing those instances exhibiting the mostfavorable or least favorable outcomes. These selected instances are then utilized to form a dataset, categorized aseither positive or negative, which is essential for the facilitation of contrastive learning. The examples with highCIDEr scores (corresponding to low NLL loss during the scoring process) are selected as positive samples. Number of shots 90.0 92.5 95.0 97.5 100.0 102.5 105.0 107.5 CIDEr Q-I-M-IQ-I-M-IT",
  ": The introduction of textual information in theunsupervised method leads to a higher M-ICL perfor-mance across all numbers of shots, demonstrating theimportance of text modality": "corresponding text label. Specifically, when we usean image as a query, we will calculate its similaritybetween the query image iq and the memory imageim as well as its caption cm, which means the finalsimilarity S = cos(iq, im) + cos(iq, cm). presents the M-ICL performance in dif-ferent unsupervised retriever settings, given thesame query (Q) and memory (M) configuration.Compared with the standard setting, the Q-I-M-IT setting exerts a positive impact on the M-ICLperformance to varying degrees, which verifies thesubstantial influence of textual content on M-ICLperformance.",
  "Multimodal Supervised Prompt Retriever": "The unsupervised methodology depicted earlier isnot explicitly designed for multimodal in-contextlearning applications. Instead, its efficacy relieson the preliminary training phase of the feature ex-traction mechanism, where the objective functionutilized may not align with the demands of multi-modal in-context learning scenarios. In contrast,we propose a novel strategy that is based on super-vised in-context examples retrieval, assuming theavailability of labeled training data as shown in Fig-ure 2. The primary aim of this approach is to refinethe original retriever to ensure that the chosen in-context example(s) contribute effectively towardsenhancing the log-likelihood maximization.It is important to acknowledge that evaluatingeach candidate within the dataset for MLLM train-ing entails a significant expenditure of time andcomputational resources. This is attributed to theextensive size of existing MLLMs and the substan-tial overhead associated with preprocessing imagedata. Consequently, in alignment with the method-ologies outlined in (Rubin et al., 2022), we initiallynarrowed down the Top-N (N=50) candidates byassessing the cosine similarity of multimodal infor-mation through the application of the unsupervisedretriever. Drawing upon the findings presented in3.2, our approach integrates both visual and tex-tual data for the identification of Top-N examples,which are subsequently employed in the trainingof our proposed multimodal in-context examples retriever.Scoring In the process of scoring retrieved candi-dates for each training instance via an unsupervisedretriever, we initially devise effective prompts asillustrated in following (Awadalla et al.,2023). The MLLM scorer is furnished with a spe-cific image or image-text prompt as input and pro-ceeds to make predictions for the designated orangesegment. These predictions are subsequently lever-aged to compute the NLL loss, which serves asthe performance metric for the given in-contextinstance. Consequently, the candidates are reorga-nized based on their scores to facilitate the subse-quent selection of positive and negative samplesfor contrastive learning.Training Adopting the contrastive learningframework as mentioned by (Rubin et al., 2022),we introduced a query encoder designed to pro-cess inputs comprising image or image-questionpairs, and a context encoder tasked with handlingcandidate prompts represented as image-promptpairs, for feature extraction aimed at subsequentoptimization. Both encoders are initialized withvision encoders and text encoders of CLIP model(Radford et al., 2021). During each iteration, aminibatch B is selected from the training dataset tofine-tune the CLIP model. Furthermore, for everyinstance within the assembled memory B with Nimage-text pair, both a positive and a negative ex-ample are independently sampled from the Top-Kpositive and negative candidates. This samplingstrategy is implemented to develop a similarity met-ric. Specifically, this metric should ensure that fora given test example xq, it exhibits similarity totraining instances that facilitate the decoding of yq.The formulation of the contrastive loss is executedas follows:",
  ": Impact of texts on proposed MSIER method.T denotes the Training setting and E denotes theEvaluation setting": "retrieval, yet the impact of textual data on MSIERhas not been investigated. To thoroughly assess thecontribution of text in the training of supervisedretrievers, we conducted a detailed comparison oftwo scenarios involving fine-tuned CLIP models,distinguishing them based on their approach to in-corporating textual information within the imagecaptioning task.We decoupled the training and evaluation set-tings to investigate the significance of textual datain our proposed supervised retrieval framework.Specifically, we employed distinct query-memoryconfigurations during the retrievers training phaseand evaluation phase. As depicted in ,two training configurations Q-I-M-I and Q-I-M-IT were implemented, and their performancewas assessed under two corresponding settings toensure an equitable comparison. Within the con-text of the Q-I-M-I setting, textual data during thetraining phase has a negligible influence on the re-trievers performance. In contrast, incorporatingtextual information during the training of the super-vised multimodal retriever markedly enhances itseffectiveness, resulting in a significant performanceimprovement.",
  "Datasets": "Following (Alayrac et al., 2022), we focus on threerepresentative multimodal tasks and the detailsabout the datasets used for these tasks as follows.MS COCO (Lin et al., 2015) for image captioning,OK-VQA (Marino et al., 2019) for Visual QuestionAnswering, and HatefulMemes (Kiela et al., 2021)for rank classification. Accuracy on the test split",
  ": Prompts used for different tasks. Orange parts are used for calculating scores for each candidate in memory": "is measured for OK-VQA. In MSCOCO, evalua-tion utilizes CIDEr scores (Vedantam et al., 2015)on the Karpathy-test split. For HatefulMemes, theAUC ROC is calculated. All results are averagedwith three runs. For further details of these down-stream tasks and corresponding datasets, please re-fer to Appendix A.3 and A.4. The experiments con-ducted herein utilize the OpenFlamingo-3B frame-work (Awadalla et al., 2023). Comprehensive de-tails regarding OpenFlamingo can be found in Ap-pendix A.2.",
  "Compared Methods": "The analysis evaluates various methodologies, in-cluding the Random baseline, RICES (Q-I-M-I), Q-T-M-T textual variant, Multimodal UnsupervisedIn-context Examples Retrieval (MUIER) using allmultimodality aspects, and the proposed approach:Multimodal Supervised In-context Examples Re-trieval (MSIER) enhancing CLIPs dual encoders.MSIER undergoes 30 epochs of training with theAdamW optimizer (Kingma and Ba, 2017), warm-ing up to a peak learning rate 1e-5, subject to re-duction using the cosine annealing rule. For fur-ther details of these methods, please refer to Ap-pendix A.1.",
  "Main Results": "As shown in , compared with Random andRICES, MUIER method shows further enhance-ments, underscoring the benefits of integrating im-ages and text in a retrieval strategy without explicitsupervision. This approach enables the model tobetter understand in-context examples, with thetextual content offering additional direction, thusimproving the retrieval of more relevant examples.MSIER method outperforms all with a significantrise in performance, evidenced by a 5.52 increasein the CIDEr score of MS COCO dataset. This sug-gests that a refined retrieval mechanism, informedby the foundational knowledge of multimodal largelanguage models (MLLMs) and tailored to selectcorrect image-text pairings, yields superior learn-ing outcomes. Furthermore, MSIER with only 4",
  "Further Analysis": "How does the MSIER improve M-ICL specif-ically?To address this query, we employed amultimodal analysis of the in-context examplesidentified by MUIER and MSIER as depicted in. Our examination concentrates on im-age captioning by selecting one example from theMS COCO dataset where the number of shots is4. Concretely, the columns from left to right delin-eate the retrieved in-context example with RICES,MUIER, and MSIER separately. The blue rows arethe retrieved multimodal in-context examples, thatis, a pair comprising input and output, whereas thesubsequent orange rows present the query imagealongside the models prediction. In the given fig-ure, examples identified by MSIER exhibit a closersemantic similarity to the queries than RICES andMUIER. Consequently, the MLLM made a moreaccurate prediction that includes the caption \"achocolate covered peanut\", which is not capturedby previous predictions using RICES and MUIERmethods for retrieving in-context examples.How does the order of in-context examples af-",
  "MSIERMUIERRICES": ": Multimodal in-context examples retrieved by RICES, MUIER and MSIER. In each grid, the first fourblue rows contain the prompt while the orange row contains the query image and prediction. Further outcomes aredetailed in the Appendix B. fect M-ICL? To understand if changing the orderof multimodal in-context examples makes a differ-ence, we fixed the number of in-context examplesto 3, and evaluate all possible permutations. Asshown in , the standard deviation is gener-ally small, so the order is not a concern as long ashigh-quality examples are chosen.How is the transferability of proposedMSIER? The compositional characteristics of nat-ural language and images are general, meaning theretriever may exploit similar knowledge in differ-ent tasks or scoring MLLMs. This motivates usto explore whether the proposed MSIER trainedon one dataset or MLLM scorer can be directlytransferred to others without further tuning. This isa practical research question as training a retrieverfor each dataset or MLLM scorer can be costly inreal applications.",
  "Datasets To measure the transferability of su-pervised retrievers between datasets, we eval-uated the performance of MSIER trained on": "OK-VQA and HatefulMemes datasets on MSCOCO dataset with different numbers of shotsof retrieved in-context examples. The resultin manifests some transferability ofour proposed MSIER between multiple mul-timodal datasets. Enhanced performance ofMSIER with training on the OK-VQA datasetsuggests that data form and volume signifi-cantly influence MSIERs effectiveness. MLLM Scorer Regarding the impact ofMLLM scorer on the transferability ofMSIER, our methodology involves deployinga retriever trained utilizing the OpenFlamingo-3B model as the scoring model, subsequentlyapplied to the inference processes of theOpenFlamingo-9B model. The outcomes in demonstrate that the MSIER approach,when utilizing the 3B model as a scorer, man-ifests superior transferability and outperformsthe MUIER-9B method. This implies that su-pervised retrieval methods, initially trained on smaller-scale models, retain effectivenessupon application to larger models, obviatingthe necessity for separate training for largermodels and thus enhancing the cost-efficiencyof inference processes in larger-scale models.",
  "Ablation Study": "This section presents an extensive ablation studybased on the OKVQA and MS COCO datasets toverify the best setting for our proposed modelscomponents.Impact of Different Modality Encoders inCLIP Unlike ICL retrieval in NLP and CV, M-ICLretrieval employs multiple encoders in its back-bone retriever to extract features from differentmodalities. To investigate the roles of the text andimage encoders within the base CLIP model inour proposed MSIER framework, we selectivelyfreeze one encoder during MSIER training. demonstrates that the update on image encoderhas a more significant effect on M-ICL retrievalperformance improvement because freezing the im-age encoder results in a notable performance dropcompared to the original MSIER (1.39 ). In con-trast, freezing only the text encoder yields similaror even improved retrieval performance, suggestingthat the original trained MSIER may have overfit-ted to some degree on the text encoder.Impact of Number of Candidates We proceedto assess the performance of MSIER across a rangeof candidate numbers. In MSIER, we passed thecandidates to a scoring LM and label the top-K andthe bottom-K as positive and negative examples",
  ": Comparison of M-ICL performance of differentsettings of K for MSIER on OK-VQA dataset": "Impact of Textual Information in RetrievedIn-context Examples The role of textual informa-tion during the evaluation of M-ICL remains unex-plored for our proposed MSIER. In , we findthat compared to the standard scenario, the replace-ment of texts in the in-context examples hugelyimpacts the M-ICL performance. To be specific,employing a mask on half of the captions throughthe random selection method results in a significantdecrease in the CIDEr score. Correspondingly, theside effect of the removal of texts decreases fromMUIER method to MSIER method compared withrandom retrieval.",
  ": Comparison of M-ICL performance of randomselection, MUIER, and MSIER method with maskedtext in-context examples on MS COCO dataset": "Impact of Backbone Retriever To investigatewhether using a different backbone model could im-prove performance, we further evaluated retrievalmethods, MUIER and MSIER, on the image cap-tioning benchmark by utilizing an alternative back-bone: EVA-02-CLIP (Sun et al., 2023), a model enhances the original CLIP model with advancedpre-training techniques. The results, reported in, show that MUIER and MSIER methodsemploying EVA-02-CLIP(-L/14) as the backboneretriever demonstrated better performance. Thissuggests that the EVA-02-CLIP model offers re-trieval benefits for the M-ICL of the OpenFlamingomodel, with improved feature extraction for bothtextual and visual information. Furthermore, theproposed MSIER method exhibited a correspond-ing performance improvement when using the en-hanced retriever, explicitly validating MSIERs ex-cellent transferability and ability to achieve bet-ter results as multimodal encoders advance. Addi-tional experiments can be found in Appendix A.5.",
  "Conclusion": "In this study, we conducted an extensive evalua-tion of textual informations role in unsupervisedand supervised in-context example retrieval formultimodal in-context learning.We introducea novel MSIER methodology that incorporatesMLLMs self-contained information to train a su-pervised prompt retriever. Our experiments acrossthree multimodal tasks demonstrate that integrat-ing text modality and the foundational knowledgeof MLLMs significantly enhances the efficiencyof example selection, yielding substantial improve-ments over existing approaches. Moreover, MSIERdemonstrates high efficiency by outperformingbenchmarks with far fewer examples, while alsoexhibiting strong transferability by remaining ef-fective for larger models after training on smallerones, thereby improving cost-efficiency for large-scale inference. Future research could delve intooptimizing inter-modal interactions, leveraging ourfindings for more effective multimodal in-contextlearning retrieval strategies.",
  "Our research focuses exclusively on the integrationof visual and textual data for multimodal in-context": "learning. Given the expanding use of additionalmodalities (such as video and audio) in this domain,the development of a comprehensive framework ca-pable of consolidating these varied modalities intoa unified representation is increasingly imperative.We observed that enhancements in multimodalin-context learning (M-ICL) performance on theMS COCO dataset surpass those on other datasets.This discrepancy may stem from the necessity formore meticulously constructed prompts in visualquestion answering (VQA) and rank classificationtasks, which demand more than mere questions orcaptions for effective in-context example retrieval.Moreover, both diversity and relevance play crit-ical roles in the selection of in-context examplesfor M-ICL. Future investigations might, therefore,benefit from examining how the relationships be-tween retrieved examples can inform the design ofan improved supervised retriever for M-ICL.",
  "Sweta Agrawal, Chunting Zhou, Mike Lewis, LukeZettlemoyer, and Marjan Ghazvininejad. 2022. In-context examples selection for machine translation": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Brock, Aida Nematzadeh, Sahand Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karen Simonyan. 2022.Flamingo: a visual language model for few-shotlearning.",
  "Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, NanningZheng, Jian-Guang Lou, and Dongmei Zhang. 2023.How do in-context examples affect compositionalgeneralization?": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, Je-nia Jitsev, Simon Kornblith, Pang Wei Koh, GabrielIlharco, Mitchell Wortsman, and Ludwig Schmidt.2023. Openflamingo: An open-source framework fortraining large autoregressive vision-language models.arXiv preprint arXiv:2308.01390. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei. 2023.Kosmos-2: Grounding multimodal large languagemodels to the world": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the 38th InternationalConference on Machine Learning, volume 139 ofProceedings of Machine Learning Research, pages87488763. PMLR.",
  "Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. 2023.What makes good examples for visual in-contextlearning?": "Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang,Wenjuan Han, and Baobao Chang. 2023. Mmicl: Em-powering vision-language model with multi-modalin-context learning. WanrongZhu,JackHessel,AnasAwadalla,Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,Youngjae Yu, Ludwig Schmidt, William Yang Wang,and Yejin Choi. 2023. Multimodal c4: An open,billion-scale corpus of images interleaved with text.",
  "The Q-T-M-T variant, an adaptation of Q-I-M-I, elects in-context examples predicated ononly textual similarity": "The standard unsupervised approach usedin this research, termed Multimodal Un-supervised In-context Examples Retrieval(MUIER), employs readily accessible fea-tures to identify the most similar examples viasearch, based on all multimodality aspects. Multimodal Supervised In-context ExamplesRetrieval (MSIER) represents our secondaryproposal, which involves the enhancement ofCLIPs dual encoders. CLIP ViT-L-14 is ap-plied for all experiments. This enhancementprocess is facilitated through direct optimiza-tion to improve M-ICL performance. Thesupervised model undergoes training for 30epochs utilizing the AdamW optimizer. Theinitial learning rate is established at 0.00001,subject to reduction according to the cosineannealing rule.",
  "A.2Multimodal Large Language Model": "The architecture of OpenFlamingo encompassesa fixed large language model featuring a decoder-only configuration (for instance, MPT ), accom-panied by a static visual encoder (such as CLIP-ViT (Radford et al., 2021)), which is succeededby a trainable perceiver resampler. To facilitatethe integration of visual and linguistic data, train-able cross-attention layers are strategically inter-spersed among the pre-trained language model lay-ers. OpenFlamingo underwent pre-training on the LAION-2B (Schuhmann et al., 2022) and Multi-modal C4 (Zhu et al., 2023) datasets and its per-formance is on par with that of Flamingo (Alayracet al., 2022), demonstrating its competitive edge.Some well-known MLLMs such as LLaVA andMinigpt-4 do not discuss their in-context learn-ing ability or present any experimental results onin-context learning in their papers. In contrast,the Flamingo model is one of the first and mostrenowned MLLMs to include experiments on in-context learning in its paper. Since the originalFlamingo model is not open-sourced, we opt forthe open-source version, OpenFlamingo, as thebackbone. We use its in-context learning perfor-mance on three representative downstream tasks inits paper for a fair comparison.",
  "A.3Downstream Tasks": "Image Captioning is a task where a modelgenerates a textual description of an image.It combines elements of computer vision andnatural language processing to interpret thecontents of an image and articulate them in hu-man language. The challenge is recognizingthe objects within the image and describingtheir attributes and the relations between themin a coherent sentence structure. Visual Question Answering (VQA) In thistask, a model is given an image along witha natural language question about the image,and it must provide an appropriate answer.To provide an appropriate answer, the modelmust understand both the visual content ofthe image and the semantics of the questiontext. It often involves aspects of object de-tection, attribute recognition, and languageunderstanding to correctly answer questionssuch as \"What color is the car?\" or \"How manypeople are in the room?\" Rank Classification is a task that involvesordering or prioritizing a set of items or enti-ties based on specified criteria, which may bederived from multiple input modalities liketext and images. For example, in a multi-modal setting, this could involve analyzingtext and images together to rank products inan e-commerce setting based on relevanceto a search query or user preference. Morebroadly, rank classification could involve anytask where items need to be ordered or pri-",
  "A detailed explanation of used datasets correspond-ing to evaluated downstream tasks is provided:": "MSCOCO is a large-scale dataset for objectdetection, segmentation, and captioning. Itprovides diverse images with complex scenesand multiple objects in context. Annotationsinclude object segmentation, recognition, andimage captions. The dataset contains approxi-mately 330K images, with 1.5 million labeledinstances across 80 object categories. We uti-lize Karparthys split: training (83K images),validation (5K images), and test (5K images)sets. OK-VQA is a dataset designed for open-ended Visual Question Answering that re-quires external knowledge beyond image con-tent. It features questions demanding multi-modal knowledge, combining visual cues withgeneral world knowledge. The dataset com-prises over 14,000 images sourced from theMSCOCO dataset, with around 14,000 ques-tions spanning 10 categories. We leverage itsstructured format, which includes a balancedmix of 9,009 training and 5,046 testing ques-tions, to assess the capability of models tointegrate visual understanding with externalknowledge sources. HatefulMemes A dataset constructed for thedetection and classification of hate speech inmultimodal content. It uniquely combines tex-tual and visual elements to challenge modelsin understanding complex, nuanced expres-sions of hate speech. The dataset consistsof 10,000+ meme images, annotated for hatespeech detection, with a distribution of 8,500for training and 3,000 for testing. The Hate-ful Memes dataset provides a significant chal-lenge in discerning subtle contextual cues andcultural references, requiring advanced multi-modal analysis capabilities.",
  "In our main experiments, MUIER and MSIER uti-lized all modalities to calculate similarity scores": "and directly selected the most similar items asin-context examples. Recently, some newly pro-posed retrieval methods have emerged, such asMixed Modality In-Context Example Selection(Chen et al., 2023a), which uses visual featuresto retrieve a set of top-N candidate examples andthen employs textual information to retrieve thefinal in-context examples (with varying numbersof examples, such as 4, 8, 16, or 32). We alsovalidated the efficiency of MSIER in this scenario,and shows that, although we did not traina new MSIER specifically designed for the newretrieval method, it still demonstrated better perfor-mance on the VQA task. This further highlightsthe generality of our proposed MSIER framework.",
  "MMICES-CLIP32.9633.6035.8936.1334.65MMICES-MSIER33.0834.3635.9236.8435.05": ":Comparison of M-ICL performance ofMMICES with CLIP as retriever, and MMICES withMSIER as retriever on MS COCO dataset. We chosethe top 50 candidates for further selection of in-contextexamples in this experiment. Impact of Backbone Retriever We further eval-uated the retrieval methods MUIER and MSIER us-ing a different backbone: ALIGN (Jia et al., 2021),a model pre-trained on noisy image-text pairs usingcontrastive learning. The results, reported in , show that the MUIER and MSIER methods didnot demonstrate explicit advantages over randomselection. This suggests that the ALIGN modeldoes not offer retrieval benefits for the M-ICL ofthe OpenFlamingo model. This can be attributed tothe fact that OpenFlamingo utilizes the CLIP modelas its vision encoder, resulting in poor contextualinformation from examples primarily retrieved bythe ALIGN model, leading to little enhancement inM-ICL performance."
}