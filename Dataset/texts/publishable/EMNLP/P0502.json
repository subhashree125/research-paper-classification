{
  "Abstract": "Recent advancements in State Space Models(SSMs) have attracted significant interest, par-ticularly in models optimized for parallel train-ing and handling long-range dependencies. Ar-chitectures like Mamba have scaled to billionsof parameters with selective SSM. To facilitatebroader applications using Mamba, exploringits efficiency is crucial. While token reductiontechniques offer a straightforward post-trainingstrategy, we find that applying existing meth-ods directly to SSMs leads to substantial per-formance drops. Through insightful analysis,we identify the reasons for this failure and thelimitations of current techniques. In response,we propose a tailored, unified post-training to-ken reduction method for SSMs. Our approachintegrates token importance and similarity, thustaking advantage of both pruning and merging,to devise a fine-grained intra-layer token re-duction strategy. Extensive experiments showthat our method improves the average accu-racy by 5.7% to 13.1% on six benchmarks withMamba-2 compared to existing methods, whilesignificantly reducing computational demandsand memory requirements.1",
  "Introduction": "There are growing research interests and efforts inSSMs in recent years. Building on the foundationlaid by the Kalman filter model (Kalman, 1960),SSMs have evolved to address long-range depen-dencies and are optimized for parallel training. Sev-eral works (Gu et al., 2021a,b, 2022; Gupta et al.,2022; Dao and Gu, 2024) have proposed SSM-based models capable of processing sequence dataacross a variety of tasks and modalities.A notable recent contribution, Mamba (Gu andDao, 2023a), integrates time-varying parameters",
  "*Equal contribution.1Code available at": "into SSMs, allowing the model to selectively prop-agate or forget information. Additionally, Mambaintroduces a hardware-aware parallel algorithmthat accelerates both training and inference. Un-like quadratic attention mechanisms, which be-come prohibitively expensive with longer sequencelengths, Mambas subquadratic-time architecture ismore efficient and better suited for handling longsequences. The exceptional scaling performance ofMamba underscores its potential as an effective al-ternative to the Transformer model (Vaswani et al.,2017) for generative language modeling tasks.In line with existing research efforts aimed atenhancing the efficiency of Transformer models(Shen et al., 2024b,c; Zhan et al., 2021), explor-ing the efficiency of SSMs is crucial for facilitat-ing real-time applications. While weight pruningand quantization are prevalent techniques for opti-mizing Transformer models (Vaswani et al., 2017;Yang et al., 2023; Zhang et al., 2022), token re-duction (Rao et al., 2021; Pan et al., 2021; Yuanet al., 2021; Renggli et al., 2022) has proven ef-fective in improving Transformer efficiency due tothe token length dimension or number of token isindependent of the model architecture.Given that SSM blocks also process input tokenssimilarly to Transformer models, applying existingstate-of-the-art (SOTA) token reduction techniques(Liang et al., 2022; Cao et al., 2023; Bolya et al.,2023) to SSMs appears to be a straightforwardpost-training approach to enhance their efficiency,especially when scaling to billions of model param-eters. This can achieve faster serving and lowerpeak memory usage, facilitating the wider deploy-ment of large-scale SSMs like Mamba. However,as illustrated in , this application of tokenreduction to SSMs, while offering some benefitsof faster inference with fewer tokens, results insignificant performance drops.In this paper, after applying existing Transformertoken reduction techniques to SSMs and observing their failures, we conduct an insightful analysis tounderstand the patterns and reasons for their fail-ures on SSMs. Based on our analysis, we proposea unified post-training token reduction method forSSMs to preserve performance and improve effi-ciency. We first employ a decoupling strategy thatcomputes the importance of each token and classi-fies them into two sets: less important tokens andmore important tokens. Following this, we devisea fine-grained intra-layer token reduction strategyfor the hidden states and residual connections ofMamba. Our approach uses a hybrid token reduc-tion strategy (combining and taking advantagesof pruning and merging) on hidden state tokens,meticulously designed to balance preserving essen-tial information and eliminating redundancy. Ourunified strategy can be generalized to other modelarchitectures like Transformers. In summary, themain contributions of our work are as follows: We observe the failure of directly applying to-ken reduction techniques from Transformers toSSMs, and we conduct an insightful analysis toinvestigate the patterns of token reduction strate-gies and the possible reasons for their failures. We are the first to propose a unified post-trainingtoken reduction method designed for SSMs. Thisstrategy leverages insights from both token prun-ing and token merging, and incorporates the to-ken importance and similarity evaluation. Zero-shot evaluations on various SSMs demon-strate the effectiveness of our method, improvingaverage accuracy by 5.7% to 13.1% on six bench-marks with Mamba-2, and by 6.5% to 15.1% withMamba compared to baseline methods. Mean-while, our method significantly reduces compu-tational demands and memory requirements.",
  "State Space Models.SSMs (Gu and Dao, 2023b;": "Mehta et al., 2022; Wang et al., 2023) are emerg-ing architecture designs for sequence-to-sequencetransformation. The design has the strength tomodel complex systems by focusing on how theinput, output, and state variables evolve over time.Mamba-2 (Dao and Gu, 2024) propose state spaceduality to design a new architecture whose corelayer is a refinement of selective SSM. S4ND(Nguyen et al., 2022) is the first work that ap-plies the state space mechanism to visual tasksand shows the potential to achieve competitive performance with ViTs (Dosovitskiy et al., 2020).ViM (Zhu et al., 2024) proposes a novel visionbackbone with bidirectional selective SSM. The ac-complishments demonstrate the potential of SSMsas an emerging foundation model family. Token Reduction.Token reduction is an effec-tive strategy to enhance computational efficiencyby reducing the number of processed tokens orpatches (Modarressi et al., 2022; Huang et al., 2022;Nawrot et al., 2022; Wang and Yu, 2023; Konget al., 2023; Zhan et al., 2024). It enables sig-nificant acceleration without requiring additionalweights or specialized hardware, aiming to selec-tively retain the most informative tokens. Severalinnovative approaches have been developed forTransformers. For example, EViT (Liang et al.,2022) uses the attentiveness of the [CLS] tokenwith respect to other tokens to identify the mostimportant tokens. DynamicViT (Rao et al., 2021)and SPViT (Kong et al., 2022) add layers that em-ploy the Gumbel-Softmax trick to selectively pruneless informative tokens. Agile-Quant (Shen et al.,2024a) leverage the activation-aware token pruningtechnique to reduce the outliers for LLMs. ToMe(Bolya et al., 2023) measures dot product similaritybetween token keys to determine redundancy andmerge accordingly. PuMer (Cao et al., 2023) pro-posed a token reduction framework for large-scaleVLMs with text-informed pruning and modality-aware merging strategies to progressively reducethe tokens of input image and text.However, the dynamics of information flowbetween tokens and the learning mechanisms inmodels like Mamba (Gu and Dao, 2023b) remainlargely unexplored. The absence of attention layersin Mamba makes current token reduction methodsineffective. Furthermore, the inclusion of the SSMmodule prevents the effective use of existing tokenreduction methods.",
  ":Performance of applying token pruning(EViT) and merging (PuMer) methods on Mamba-2.8B,showcasing significant drop in accuracy": "Mamba (Gu and Dao, 2023b) represents a dis-crete version of the continuous system for SSMsand incorporates a timescale parameter to facil-itate the transformation of continuous parameterswith the zero-order hold (ZOH) as A = exp(A),and B = (A)1(exp(A) I) B. After ob-taining the discretized A and B, the discretizationof Equation (1) can be rewritten as,",
  "Analysis of Reasons Behind the Failure ofToken Reduction on SSMs": "Due to the SSMs reliance on a sequential strategyfor token computation, the previous token reduc-tion strategies highlighted in do not yieldeffective results. In this section, we delve into thereasons why directly applying SOTA token pruningor merging method fails on SSMs. Failure of token pruning on SSMs.ExistingSOTA token pruning methods for Transformers,such as Token Filtering (Berchansky et al., 2023),Agile-Quant (Shen et al., 2024a), and EViT (Lianget al., 2022), typically involve sorting all tokens inthe current layer based on an importance evalua-tion criterion, and then removing the less importanttokens. As shown in (a), after we directlyimplement post-training token pruning (EViT) toreduce 20% of the overall FLOPS for Mamba-2.8B,there is a dramatic drop in average accuracy on zero-shot evaluation. This performance drop isintroduced by pruning certain tokens with unrecov-erable information loss, although the pruned tokensare less important based on a heuristic importancemetric. This information loss is gradually ampli-fied during the sequence computations process ofEquation (2) and (3) in SSMs. Failure of token merging on SSMs.On theother hand, linguistic contexts often contain re-dundant tokens, which do not add significantcontextual depth to the models understanding.ToMe (Bolya et al., 2023) introduces a bipartitetoken merging strategy for vision Transformers.Following this, initiatives like PuMer (Cao et al.,2023) extend this strategy to vision-language mod-els, merging redundant tokens in linguistic modelcomponents and their vision counterparts at thesame time. However, as shown in (b), ap-plying this bipartite token merging strategy directlyto SSMs proves ineffective. The strategy uniformlypartitions the tokens in the current layer into twogroups, and merges tokens in one group into theother group, disregarding the inherent value (ortoken importance) of each token. Thus, certain im-portant tokens may be merged into other tokens.Given the critical role of important tokens in se-quence computations using Equation (3) in SSMs,overlooking the inherent significance of tokens andthus removing important tokens can lead to substan-tially different y in Equation (3) and thus severeperformance degradation.",
  "Motivation": "From the analysis presented, we conclude that thefailure of token pruning in SSMs comes from theloss of crucial information due to token removal.Meanwhile, the failure of token merging in SSMscan be attributed to the neglect of token importance.This oversight can result in a more significant dropin accuracy compared to pruning, underscoring thecritical role of token importance in the modelsperformance. Therefore, our objective is to com-bine token importance and similarity as guidancefor a unified token reduction method (combiningpruning and merging). We aim to develop a morefine-grained reduction strategy to handle the com-putation sensitivity of selective SSMs, ensuringthat the reduction process maintains model accu-racy and efficiency simultaneously.",
  "y SSM(A, B, C)(x),(4)": "where the hidden states y RBND is the out-put of SSM (see Equation (3)). The token sequenceoutput of the lth layer can be obtained as Tl LinearT y + Tl1. To evaluate the importance ofeach token, we first extract the hidden states yfrom the SSM layer, denoted as y RBND.The hidden states represent the intermediate repre-sentations of the tokens after passing through theSSM layer. To quantify the importance of eachtoken, we compute the sum of the y across thelast dimension, which corresponds to the featuredimension D. The SSMs architecture, with itshigh-dimensional channel space, allows for a finer-granularity analysis of attention across numerouschannels. Unlike Transformers that produce a sin-gle attention matrix per head, SSMs exploit theirextensive channel capacity for a more detailed at-tention distribution, enhancing the models abilityto discern subtle features and interactions among to-kens. Thus, we aggregate the clipped values acrossall channels for each token to evaluate token impor-tance as follows,",
  "Unified Token Reduction by TokenImportance Classification": "To achieve token reduction, it is important to derivea token importance classification strategy that ef-fectively differentiates between less important andmore important tokens. However, it is challengingto directly classify thousands of tokens in real-timedue to high complexity. To overcome this, we fur-ther leverage the token importance evaluation asin Equation (5), and employ a decoupling strategy.The strategy initially computes the importance ofeach token, followed by classification based on thisobtained importance. After that, we perform uni-fied token reduction (UTR) and leverage multipledesign choices to enable effective and fine-grainedstrategies. illustrates our proposed ap-proach. The steps of our method are as follows: 1. Calculate token importance with Equation (5).2. Classify the tokens into set MA and MB basedon their importance. At the end, N/2 less im-portant tokens are assigned to set MA, with therest N/2 more important tokens to set MB.",
  "Design Choices": "Intra-layer token reduction design.We delvedeeper into our intra-layer token reduction designtailored for SSMs, targeting the hidden states andresidual connections. Our approach employs thehybrid token reduction strategy on hidden statetokens, meticulously designed to strike a balancebetween preserving essential information and elim-inating redundancy. By discerning the contextualsignificance of each token, this strategy focuseson removing tokens with minimal contextual rele-vance, thus enhancing the overall informationalflow of the SSM module.This design choicenot only preserves but also amplifies the high-contextual tokens. Residual connections are crucialfor maintaining the integrity of information fromthe last layer. Therefore, we aim to preserve asmuch residual information as possible through ourtoken merging method. The final design is shownin the design choices part in . Empiricalresults support our fine-grained design, demonstrat-ing that reducing tokens with our method in thehidden state and residual connection areas effec-tively preserves the performance of SSMs. Hierarchical token reduction procedure.Weapply a hierarchical method to reduce tokens acrossmultiple layers. Tokens reduced in one layer are fur-ther reduced in subsequent layers, balancing overallefficiency and performance. Reducing tokens ineach layer can cause high overhead, as token im-portance between adjacent layers is often similar.Thus, it is unnecessary to reduce tokens at everylayer. Furthermore, reducing tokens in earlier lay-ers yields greater computational savings, but theselayers cannot fully capture token importance. In",
  "Implementation Details": "We implement our method based on PyTorch(Paszke et al., 2019) for scientific computationsand HuggingFace (Wolf et al., 2019) for managingmodels. We use Mamba models to test the effective-ness of our method. Our approach covers a varietyof Mamba models, with Mamba-2-2.7B, Mamba-2-1.3B, Mamba-2.8B and Mamba-1.4B. We evaluatethe task performance on multiple common sensereasoning datasets including LAMBADA (Papernoet al., 2016), HellaSwag (Zellers et al., 2019),PIQA (Bisk et al., 2020), Arc-easy (Clark et al.,2018), Arc-challenge (Clark et al., 2018), andWinoGrade (Sakaguchi et al., 2021). Perplexityon LAMBADA dataset and average accuracy on allmentioned datasets are provided. All experimentsare conducted on a NVIDIA A100 80GB GPU. Reduction locations.We adopt the hierarchicaltoken reduction procedure. For Mamba2-2.7B andMamba-2.8B, we perform all methods in the layers; for Mamba2-1.3Band Mamba-1.4B, we perform all methods in the layers. We use a fixedcompression ratio for each prune layer. Evaluation Details.The evaluation of perplexity(PPL) and average accuracy are adjusted to accountfor the reduction in the number of output due totoken reduction. The target label logits are adjustedaccordingly. For example, when the output tokenreduction rate is m%, the label logits are also re-duced to their first 1 m% logits to calculate thePPL and average accuracy properly. Baselines.We compare our method with PuMer(Cao et al., 2023) and EViT (Liang et al., 2022).PuMer, which includes a dedicated text token re-duction module, can be directly adopted in ourstudy. For EViT, originally designed for visionTransformers, we configure it to ensure a fair com-parison in our evaluation.",
  "Quantitative Evaluation": "Evaluation on Mamba-2.As shown in ,for Mamba-2 models (1.3B and 2.7B), our methodconsistently achieves better performance than allbaselines (PuMer and EViT) with non-marginalimprovements under the same FLOPS reductionratios. For Mamba-2-1.3B, our method achievessignificantly lower PPL and higher accuracy onalmost all downstream datasets, with an average ac-curacy 10% (54.6% v.s. 44.2% from EViT) higherthan the best baseline under 20% FLOPS reduction.For Mamba-2-2.7B, our method outperforms base-lines on various benchmarks with wide margins,achieving an average accuracy 13.1% higher thanthe best baseline under 30% FLOPS reduction.",
  ": Main results of post-training performance on Mamba-1.4B and Mamba-2.8B. We compare with baselinemethods and evaluate them on six benchmarks under 10%, 20%, and 30% FLOPS reduction": "Evaluation on Mamba.As demonstrated in Ta-ble 2, for Mamba models (1.4B and 2.8B), wecan make similar observations that our method out-performs all baselines with non-marginal improve-ments in terms of PPL and accuracy on multiplebenchmarks. Our method maintains a low PPLwhile baselines can hardly keep a reasonable PPL(such as our 23.97 PPL v.s. 9785 from EViT un-der 20% FLOPS reduction for Mamba-2.8B). Ouraverage accuracy is significantly higher than base-lines, such as our 53.4% over 41.1% from EViT forMamba-1.4B under 20% FLOPS reduction. Summary.For SSMs such as Mamba, our pro-posed method consistently demonstrates better per-formance in terms of PPL and average accuracyacross various levels of FLOPS reduction com-pared with baselines. PuMer and EViT fail to main-tain high performance due to the reasons discussedin .2. After an insightful investigation ofthe reasons for failure and a comprehensive designto combine the advantages of pruning and merging,our unified method can effectively and efficientlyprune tokens in SSMs without significant perfor-mance degradation.",
  ": Ablation study of token importance metric withour unified token merging and pruning design": "Clip (the max function in Equation (5)), and withClip, along with their impacts on LAMBADA PPLand average accuracy across six tasks (as in Ta-ble 2). The results show that Clip achieves thelowest PPL of 17.96 and the highest average accu-racy of 58.7% for Mamba-2-2.7B, outperformingother metrics. For Mamba-2.8B, though Clip hasa slightly higher PPL, its average accuracy is thehighest 57.6%. This analysis underscores the im-portance of the proposed token importance metricin enhancing model accuracy and efficiency. Reduction location analysis.The choice of to-ken reduction location impacts model performance. presents the ablation study of reductionlocation on Mamba-2-2.7B under a 20% FLOPSreduction. Notably, the configuration with reduc-tion layers at achievesthe lowest PPL 17.96 and the highest 58.7% aver-",
  ": Ablation study of reduction location on Mamba-2-2.7B under 20% overall reduction of FLOPS": "age accuracy, demonstrating the effectiveness ofthis specific reduction strategy. In contrast, deeperreduction layers, such as ,result in higher PPL and lower average accuracy,indicating that deeper layers do not always yieldbetter results. Token reduction at earlier layerscan lead to higher computation efficiency withoutsacrificing accuracy significantly. Different design choices.For hidden states andresidual connections, we can apply pruning, merg-ing, or our hybrid token reduction with differentcombinations of pruning and merging (denoted byq). We conduct ablation studies to find the optimalq configuration for both hidden states and resid-ual connections. presents experiments onthe Mamba-2-2.7B model under a 30% FLOPSreduction. The results indicate that the combina-tion of q = 0.5 for hidden states and mergingonly for residual connections achieves the lowest40.61 PPL and the highest 54.7% average accu-racy, highlighting its effectiveness in this context.Furthermore, combining pruning and merging withq = 0.5 for hidden states consistently outperformspruning-only or merging-only strategies. Notably,even our basic method using importance classifica-tion (M-only & M-only Acc. 54.0%) outperformsexisting methods (PuMer Acc. 36.4% and EViTAcc. 41.6%) by a large margin.",
  "Efficiency Results": "We evaluate the GPU peak memory usage ofMamba-2.8B and Mamba-2-2.7B when generat-ing 2048 tokens with a batch size 96 under variousFLOPS reduction ratios. As illustrated in ,the GPU peak memory reduction for Mamba-2.8Bcan reach up-to 14.4%, 27.7%, and 40.0%, under10%, 20%, and 30% FLOPS reduction, respectively.For Mamba-2-2.7B, it can reduce the peak mem-ory by 11.4%, 20.3%, 30.6% when reducing 10%,20%, and 30% FLOPS, respectively. 33.5 40.4 55.8 47.8 Mamba-2.8BMamba-2-2.7B GPU Peak Memory (GB) 38.1 43.8 54.9 48.7 Base10%20%30%",
  ": Ablation study of different design choices onMamba-2-2.7B under 30% overall reduction of FLOPS": "Further, our proposed method can lead to prac-tical inference acceleration with higher modelthroughput, as shown in . The through-put can be improved by 1.07, 1.17, and 1.29for Mamba-2.8B, and 1.10, 1.22, and 1.37for Mamba-2-2.7B, when reducing 10%, 20%, and30% FLOPS, respectively. The throughput mea-surements are collected with a batch size 16 by gen-erating 100 tokens with a prompt length of 2048.More details and efficiency results of other modelscan be found in Appendix A.",
  "Conclusion": "In this paper, we introduced a unified post-trainingtoken reduction method for SSM architectures likeMamba. We addressed the limitations of existingtoken reduction techniques by combining tokenimportance and similarity to create a fine-grainedreduction strategy. Our method includes multipledesign choices for effective intra-layer optimiza-tions. Experiments show significant reductions incomputational demands and peak memory usage,while maintaining competitive accuracy, outper-forming baseline methods on benchmarks. 1.001.101.221.37 Mamba-2-2.7B Throughput (token/s) 10%Base 30%20% 1.00 1.071.171.29 Mamba-2.8B Throughput (token/s) 10%Base 30%20%",
  "Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng,Wei Niu, Mengshu Sun, Bin Ren, Minghai Qin, HaoTang, and Yanzhi Wang. 2022. Spvit: Enabling fastervision transformers via soft token pruning. ECCV": "Zhenglun Kong, Haoyu Ma, Geng Yuan, Mengshu Sun,Yanyue Xie, Peiyan Dong, Xin Meng, Xuan Shen,Hao Tang, Minghai Qin, et al. 2023. Peeling theonion: Hierarchical reduction of data redundancy forefficient vision transformer training. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 37, pages 83608368. Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song,Jue Wang, and Pengtao Xie. 2022. EVit: Expeditingvision transformers via token reorganizations. In In-ternational Conference on Learning Representations.",
  "Piotr Nawrot, Jan Chorowski, Adrian ancucki, andEdoardo M Ponti. 2022.Efficient transform-ers with dynamic token pooling.arXiv preprintarXiv:2211.09761": "Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs,Preey Shah, Tri Dao, Stephen Baccus, and Christo-pher R. 2022. S4nd: Modeling images and videos asmultidimensional signals with state spaces. Advancesin neural information processing systems, 35:28462861. Bowen Pan, Rameswar Panda, Yifan Jiang, ZhangyangWang, Rogerio Feris, and Aude Oliva. 2021.Ia-red2: Interpretability-aware redundancy reduction forvision transformers. Advances in Neural InformationProcessing Systems, 34:2489824911. Denis Paperno, Germn Kruszewski, Angeliki Lazari-dou, Quan Ngoc Pham, Raffaella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, and RaquelFernndez. 2016. The lambada dataset: Word pre-diction requiring a broad discourse context. arXivpreprint arXiv:1606.06031. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kpf, EdwardYang, Zach DeVito, Martin Raison, Alykhan Tejani,Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-jie Bai, and Soumith Chintala. 2019. PyTorch: animperative style, high-performance deep learning li-brary. Curran Associates Inc., Red Hook, NY, USA. Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu,Jie Zhou, and Cho-Jui Hsieh. 2021. Dynamicvit: Ef-ficient vision transformers with dynamic token sparsi-fication. Advances in neural information processingsystems, 34:1393713949.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99106": "Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong,Zhengang Li, Ming Lin, Chao Wu, and Yanzhi Wang.2024a. Agile-quant: Activation-guided quantizationfor faster inference of llms on the edge. In Proceed-ings of the AAAI Conference on Artificial Intelligence,volume 38, pages 1894418951. Xuan Shen, Zhenglun Kong, Changdi Yang, ZhaoyangHan, Lei Lu, Peiyan Dong, Cheng Lyu, Chihhsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu,Miriam Leeser,Pu Zhao,and Yanzhi Wang.2024b. EdgeQAT: Entropy and Distribution GuidedQuantization-Aware Training for the Accelerationof Lightweight LLMs on the Edge. arXiv preprintarXiv:2402.10787.",
  "Xuan Shen, Pu Zhao, Yifan Gong, Zhenglun Kong,Zheng Zhan, Yushu Wu, Ming Lin, Chao Wu, XueLin, and Yanzhi Wang. 2024c.Search for Ef-ficient Large Language Models.arXiv preprintarXiv:2402.10787": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Hongwei Wang and Dong Yu. 2023. Going beyondsentence embeddings: A token-level matching algo-rithm for calculating semantic textual similarity. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 563570. Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, LindaLiu, Mohamed Omar, and Raffay Hamid. 2023. Se-lective structured state-spaces for long-form videounderstanding. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 63876397. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771. Changdi Yang, Pu Zhao, Yanyu Li, Wei Niu, Jiex-iong Guan, Hao Tang, Minghai Qin, Bin Ren, XueLin, and Yanzhi Wang. 2023. Pruning parameteriza-tion with bi-level optimization for efficient semanticsegmentation on the edge. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1540215412. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, YujunShi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, andShuicheng Yan. 2021. Tokens-to-token vit: Trainingvision transformers from scratch on imagenet. In Pro-ceedings of the IEEE/CVF international conferenceon computer vision, pages 558567.",
  "machine really finish your sentence? arXiv preprintarXiv:1905.07830": "Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, WeiNiu, Yushu Wu, Tianyun Zhang, Malith Jayaweera,David Kaeli, Bin Ren, Xue Lin, and Yanzhi Wang.2021.Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and PruningSearch. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), pages48214831. Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu,Zichong Meng, Hangyu Zheng, Xuan Shen, StratisIoannidis, Wei Niu, Pu Zhao, and Yanzhi Wang. 2024.Exploring Token Pruning in Vision State Space Mod-els. Preprint, arXiv:2409.18962. Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao,Tianlong Chen, Mingyi Hong, Yanzhi Wang, andSijia Liu. 2022. Advancing model pruning via bi-level optimization. Advances in Neural InformationProcessing Systems, 35:1830918326. Lianghui Zhu, Bencheng Liao, Qian Zhang, XinlongWang, Wenyu Liu, and Xinggang Wang. 2024. Vi-sion mamba: Efficient visual representation learningwith bidirectional state space model. arXiv preprintarXiv:2401.09417.",
  ": Additional results of post-training performance on Mamba-2-2.7B. We compare with LTMP and evaluatethem on six benchmarks under 10%, 20%, and 30% FLOPS reduction": "The throughput of token generation for Mamba-1.4B and Mamba-2-1.3B using the proposedmethod are also collected under the same config-uration in .4, as illustrated in .With our optimization, the throughput can be im-proved by 1.08, 1.15, and 1.26 for Mamba-1.4B, and 1.10, 1.19, and 1.35 for Mamba-2-1.3B, when reducing 10%, 20%, and 30% FLOPS,respectively.",
  "A.3More Results": "We compared our method with LTMP (Bonnaerensand Dambre, 2023), a simple token pruning andmerging method designed for Vision Transformer.Our method outperforms LTMP in six benchmarksunder same FLOPS reduction by a large margin, asshown in . The results emphasizing that thesimple combination of token pruning and mergingfrom Transformer is inadequate for SSMs."
}