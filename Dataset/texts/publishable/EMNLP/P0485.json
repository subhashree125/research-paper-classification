{
  "Abstract": "The performance of Large Language Models(LLMs) is substantially influenced by the pre-training corpus, which consists of vast quan-tities of unsupervised data processed by themodels. Despite its critical role in model per-formance, ensuring the quality of this data ischallenging due to its sheer volume and the ab-sence of sample-level quality annotations andenhancements. In this paper, we introduce Dec-orateLM, a data engineering method designedto refine the pretraining corpus through datarating, tagging and editing. Specifically, Deco-rateLM rates texts against quality criteria, tagstexts with hierarchical labels, and edits textsinto a more formalized format. Due to the mas-sive size of the pretraining corpus, adopting anLLM for decorating the entire corpus is less ef-ficient. Therefore, to balance performance withefficiency, we curate a meticulously annotatedtraining corpus for DecorateLM using a largelanguage model and distill data engineering ex-pertise into a compact 1.2 billion parametersmall language model (SLM). We then applyDecorateLM to enhance 100 billion tokens ofthe training corpus, selecting 45 billion tokensthat exemplify high quality and diversity for thefurther training of another 1.2 billion parameterLLM. Our results demonstrate that employingsuch high-quality data can significantly boostmodel performance, showcasing a powerful ap-proach to enhance the quality of the pretrainingcorpus.",
  "*Equal contribution.Corresponding author": "et al., 2023). The backbone of these models effec-tiveness lies in their training processes, specificallyin the quality and composition of their pre-trainingcorpora (Penedo et al., 2023; Le Scao et al., 2023).Traditionally, LLMs are pre-trained on vast datasetscomposed of billions of tokens harvested from di-verse text sources.Data quality is of vital importance for trainingLLM (Zhou et al., 2024). However, acquiring high-quality data is a formidable challenge due to thesheer volume and unstructured nature of it.The reliance on large-scale unsupervised dataleads to the inclusion of numerous low-quality textswithin the training data. This infusion of poor-quality data can adversely affect the models learn-ing processes, resulting in performance deficienciesand limitations in their applicability. However, theexisting methods for curating and enhancing thequality of such datasets are often inadequate. Theytypically lack the capacity to scale to the size re-quired while maintaining or improving data quality,primarily due to the absence of fine-grained anno-tations and the impracticality of manual oversight.Addressing these challenges requires innovativeapproaches that can scale with the data require-ments of LLMs while ensuring enhancements indata quality. This paper introduces DecorateLM, acomprehensive data engineering methodology de-signed to refine the pretraining corpus through asystematic \"decorating\" process. The term \"dec-orating\" in this context refers to a series of pro-cesses aimed at enriching the data with additionalmetadata, improving its structure, and ensuring itsrelevance and quality.DecorateLM employs a three-phase strategy toaccomplish these goals. The first phase, rating,involves evaluating texts against a predefined setof quality criteria. These criteria are designed toassess the educational value, expertise, fact andtrivia, reasoning level, scarcity, structural format,story-likeness and subjectivity of texts. The second",
  "DecorateLM": ": We utilize GPT-4 to assemble an annotated training corpus and integrate data engineering expertise intoDecorateLM. DecorateLM is then used to process 100 billion tokens from the raw corpus, sampling 45 billion tokensusing its rating and tagging capabilities to create what we refer to as the Decorated corpus. We further enhance theDecorated corpus by applying DecorateLMs editing features, making it more suitable for LLM training. phase, tagging, categorizes the texts using a hierar-chical label system that reflects the content of thedata. This labeling enhances data management andretrieval efficiency, a key aspect of iterative trainingprocesses. The final phase, editing, involves revis-ing and standardizing texts to meet higher linguisticstandards of formality and clarity.To implement this methodology effectively, wecurate a specialized training corpus using pre-trained LLMs to preprocess and initially rate po-tential data samples. This approach leverages themodels capabilities to perform initial assessmentsat scale. We then distill our data engineering exper-tise into a small language model (SLM)whichis optimized for more detailed and nuanced dataprocessing tasks. We name this SLM as the Deco-rateLM. Using DecorateLM, we enhance 100 bil-lion tokens from our initial datasets, selecting 45billion tokens that exhibit optimal quality and di-versity. These tokens are subsequently used to trainLM to demonstrate DecorateLMs effectiveness.The results from our study underscore the sub-stantial benefits of using high-quality, well-curateddata in training LLMs. Not only do these resultsdemonstrate improved model performance, but theyalso suggest that DecorateLM offers a scalable andeffective solution to one of the most pressing issuesin modern AIenhancing the quality of training",
  "Related Work": "In recent years, the quality and selection of datafor training language models receive considerableattention. Researchers propose various methodolo-gies to assess, select, and improve high-quality data,with the goal of enhancing both the performanceand efficiency of models (Elazar et al., 2023; Long-pre et al., 2023; Xie et al., 2023; Li et al., 2024).Data Annotation and Rating.QuRating,DEITA, and ALPAGASUS are employed for dataannotation, each utilizing distinct methodologies toenhance training via refined rating scores (Wettiget al., 2024; Liu et al., 2023; Chen et al., 2023).Phi-1 and MoDS use GPT-4 and DeBERTa to im-prove educational data and precise data selection,accelerating learning and fine-tuning (Gunasekaret al., 2023; Du et al., 2023).Domain Diversity in Data. INSTAG introducesa detailed tagging system for diverse SFT data, im-proving MT-Bench scores with less data (Lu et al.,2023). Phi-1.5 extends Phi-1 by adding syntheticdata across multiple domains in a textbook style (Liet al., 2023b).Data Optimization for Model Training. Stud-ies show that models can perform well with smallerdatasets and less computing. WRAP maintains per- : The Spearman correlations between model ratings and ground truth of validation set. Specifically, thex-axis represents the ground truth rating scores of the data. The y-axis represents the prediction rating scores ofGPT-4 and DecorateLM after evaluating the validation set. Rating scores generated by GPT-4 are more discrete andinaccurate compared to DecorateLM. educational value expertise fact and trivia reasoning level scarcity story-likeness structural format subjectivity educational value expertise fact and trivia reasoning level scarcity story-likeness structural format subjectivity 1.000.500.600.720.250.320.600.17 0.501.000.380.610.440.040.29-0.03 0.600.381.000.550.190.480.600.25 0.720.610.551.000.360.270.560.20 0.250.440.190.361.00-0.060.01-0.09 0.320.040.480.27-0.061.000.370.66 0.600.290.600.560.010.371.000.18 0.17-0.030.250.20-0.090.660.181.00 0.0 0.2 0.4 0.6 0.8 1.0 : Spearman correlation coefficients between var-ious rating criteria. The correlations align with intuitiveexpectations. For instance, data with higher educationalvalue often exhibits enhanced reasoning levels, which,in turn, enhances their comprehensibility. formance with fewer resources on the C4 dataset,and TinyStories uses simple vocabulary for quickerlearning (Maini et al., 2024; Eldan and Li, 2023).Additionally, Phi-3 uses a two-stage training withweb and synthetic data to improve reasoning andspecialized skills (Abdin et al., 2024).",
  "Framework": "In this section, we detail the methodology of Deco-rateLM, which is designed for sample-level anno-tation and enhancement. The framework of Dec-orateLM consists of three distinct phases: rating,tagging, and editing. During the rating phase, Dec-orateLM assigns numeric scores to a text based on predefined quality dimensions. In the taggingphase, DecorateLM predicts hierarchical tags atthree levels for the text. In the editing phase, Dec-orateLM rephrases the text to present alternativenarratives, thereby facilitating the models acquisi-tion of core knowledge from varied perspectives.The training pipeline of DecorateLM incorpo-rates both a teacher model and a student model.The teacher model, which is larger, excels in pro-cessing detailed instructions related to text qual-ity. However, its slower processing speed limitsits practicality for annotating or editing extensivepretraining corpora. To address this, knowledgefrom the teacher model is distilled into a more com-pact student model to enhance efficiency. Distinctdistillation strategies are employed for each of thethree phases. The rating and tagging phases, whichinvolve processing the entire raw corpus and gen-erating concise annotations, exhibit similar input-output dynamics. Consequently, DecorateLM isconfigured to manage these two phases concur-rently to optimize efficiency, instead of leveragingtwo separate models. For the editing phase, a sepa-rate distillation process is implemented to distill theknowledge required for effective rephrasing intoanother model of DecorateLM.",
  "Rating": "High-quality training data is crucial for develop-ing powerful language models. However, the idealproperties that constitute an optimal training cor-pus remain challenging to characterize compre-hensively. To achieve robust language understand-ing and generation capabilities, language modelsshould be trained on high-quality data meticulously : Word cloud of tags. The size of each tag isproportional to its frequency in the annotated dataset.Tags are color-coded based on their levels: first-leveltags in dark blue, second-level tags in medium blue, andthird-level tags in light blue.",
  "curated based on diverse criteria that capture theessential and abstract qualities of natural languagetexts": "Criteria.To assess the quality of texts, we defineeight evaluative criteria that quantitatively measurethe contributions of a text to model training frommultiple perspectives. For each criterion, data sam-ples are assigned a quantitative score, enabling anobjective evaluation across the various criteria. 1. Educational Value evaluates whether the con-tent is suitable for educational purposes,specifically its utility in textbooks. It assessesthe clarity, detail, and comprehensibility ofexplanations and guiding principles.",
  ". Subjectivity focuses on content with personalopinions and conversations": "Annotated Dataset Construction.In alignmentwith the established criteria, we annotate a set ofcarefully selected samples using GPT-4 to form theannotated dataset. Considering the inaccuracy ofLLMs in assigning precise quality scores (Zhenget al., 2024), we adopt a pairwise comparisonmethod.Inspired by QuRating (Wettig et al., 2024), this work employs the Bradley-Terry (B-T)model (Bradley and Terry, 1952) to derive prefer-ence probabilities from pairwise comparisons. Allprompts used in the rating phase are displayed inAppendix A.1. Subsequently, we normalize theseprobabilities by sorting them and applying a lineartransformation to map them onto a uniform ratingscale from 0 to 100, thereby establishing the finalscores for each criterion. Analysis.Upon acquiring the meticulously cu-rated annotated dataset, we proceed to train Deco-rateLM, with the training details provided in Ap-pendix B.1. A validation set is segregated prior totraining. DecorateLM is employed to assign scoresto each data sample. For a fair comparison, we alsouse GPT-4 to assign numeric scores to these sam-ples. Then we compute the Spearman correlationcoefficient between the model-provided scores andthe ground truth annotation from the B-T model.As depicted in , GPT-4, untrained for therating task, demonstrates inferior scoring perfor-mance compared to DecorateLM.In the analysis presented in , we com-pute the Spearman correlation coefficients betweenvarious rating criteria. The results reveal a modestpositive correlation across most pairs of criteria,",
  "Tagging": "The quality of the pretraining corpus is initiallyassessed through rating criteria. However, thesecriteria alone are insufficient for ensuring diversityin the pretraining samples and for the fine-grainedselection of data. Tagging pretraining data into abroad spectrum of topics and fields can ensure di-versity within the training corpus. Furthermore, astructured tagging system facilitates the targeted en-hancement of the model by incorporating data thataddress specific areas, consequently improving themodels performance in particular domains. Next,we introduce our hierarchical tagging system. Tags Design.To systematically categorize thepretraining dataset, we first clearly define 21 pri-mary categories that cover a wide range of humanknowledge, from Natural Sciences to Social Events.We then expand this framework by engaging GPT-4, which serves as a human expert, in a two-stepiterative dialogue process. The first dialogue iter-ation yields 255 second-level tags. For the third-level tags, we inform GPT-4 of each first-level cat-egory along with its corresponding second-level",
  "tags, prompting the model to generate a total of793 specific third-level tags under the second-levelcategories. The details and prompts are in Ap-pendix A.2": "Analysis.We present the result of the tag treein and the word cloud of the tag tree in. To access the tag prediction performance,we manually re-annotated the existing validationsplit set with tags at the first, second, and third lev-els. We then compare the accuracy of DecorateLMand GPT-4 using this newly re-annotated validationset. As shown in , DecorateLM achievesperformance comparable to that of GPT-4.",
  "Editing": "The process of rating and tagging extracts valuabledata from the pretraining corpus. Despite undergo-ing a rigorous cleaning pipeline, even high-qualitydata sourced from the internet may still retain somenoise. Inspired by the work of (Maini et al., 2024),we propose to enhance the utilization of this high-quality data by rephrasing it based on the intrinsicattributes of the samples. By transforming the datainto different verbal forms, we aim to preserve thecore information diversity of the pertaining stagewhile being as clean as the SFT-stage dataset.",
  ": Distribution of first-level tags across differentdatasets, arranged in descending order by frequency inthe decorated corpus": "Annotated Dataset Construction.We begin byselecting 10,000 data samples, each containing be-tween 50 and 2048 tokens, to create a noisy dataset.We observe that this noisy dataset continues to ex-hibit issues such as unclear expressions, lack ofnatural language fluency, and mixed topics that arenot fully resolved by standard cleaning methods.This noisy dataset is rephrased using GPT-4 basedon prompts in Appendix A.3. Analysis.Due to the absence of a comprehen-sive metric for evaluating rephrased text againstthe original text, we design several custom met-rics and use human evaluation to quality-check therephrased texts. For each evaluation metric, wecompare the rephrased outputs of DecorateLM andGPT-4, with human annotators rating each outputas a win, lose, or tie. The evaluation metrics areas follows: Enhanced Clarity, which determinesthe texts increased conciseness and clearer expres-sion; Text Fluency, which assesses the smoothnessand readability of the text; Term Precision, whichchecks the retention of specialized terminology;Logical Coherence, which examines the consis-tency of causal and logical relationships within the",
  "The Final Decorated Corpus": "After we train the DecorateLM on the curated an-notated dataset, we proceed to decorate the pre-training corpus. Specifically, we select five largepre-training datasets including Common CrawlChn (CC-CN), Dolma, C4, The Pile, and BaiduWiki (BD-Wiki). Due to limited resources, we onlysample a volume of 100 billion tokens from thesedatasets.For the rated and tagged corpus, as shown in Fig-ure 5, the English datasets, Dolma and The Pile, ex-hibit relatively high ratings and low cross-entropy,making them relatively ideal training corpora thatare high-quality and well-balanced across domains.In contrast, the Chinese datasets, BD-wiki and CC-CN, exhibit lower ratings and higher cross-entropy,indicating shortcomings in overall quality and datadistribution. This also underscores the necessityof using DecorateLM to improve the quality of thenon-English corpus. For the tagging result alone, the analysis of the distribution of these datasetsacross the first-level labels is illustrated in . Regarding the effectiveness of editing on theDecorated Corpus, the original and edited textsare assessed using the perplexity metric with theCCNet model (Wenzek et al., 2019). The results,shown in , indicate a significant reductionin perplexity following the editing process. Thisimprovement suggests that the editing effectivelyorganizes the data in a manner that is more con-ducive to learning by models, ensuring enhancedcomprehensibility and learnability.",
  "Experiment Setup": "We train the same SLM, MiniCPM-1.2B, used asthe backbone for DecorateLM, aiming to improveits performance. MiniCPM-1.2B follows the multi-stage training pipeline (Hu et al., 2024). The stabletraining stage utilizes a constant learning rate untilthe decay stage, where the learning rate decreasesrapidly. During the decay stage, the loss reductionaccelerates significantly. This stage is deemed suit-able for ablation studies on different data due toits substantial loss reduction and short training du-ration. We leverage the last checkpoint before thedecay stage to reprocess the decay with both theraw and decorated corpora. Performance is eval-uated against a wide range of publicly availablebenchmarks.",
  "Experiments on Rating": "Given the rating of each test sample, we can se-lect each sample with a probability determined bythese ratings (Wettig et al., 2024). We explore twosampling methods.The first method, referred to as Separate Cri-terion Sampling, follows the approach proposedby (Wettig et al., 2024). Specifically, each crite-rion is given a weight that represents its relativeimportance. The sampling method begins from thecriterion with the highest weight to the lowest one.The transition between criteria happens when thesampled data from the dimension satisfies its prede-termined corpus proportion. Within each criterion,data is sampled according to the following weight 1.The ratings for the i-th data point in t-th criterion",
  "t,(2)": "where the parameter kt represents the relative sig-nificance of each rating dimension.For both Rat. (Sep.)with weights and Rat.(Agg.) with kt, the main method assigns a weightof 0.2 to the dimensions of Educational Value,Expertise, Fact and Trivia, and Reasoning Level,while the four remaining dimensions are each as-signed a weight of 0.05 according to the authorsprior knowledge of the data quality.In practice, we sample 58.5B tokens but onlyuse 45B tokens among them as the high-qualitydata. This has a similar effect as increasing thetemperature of sampling in (Wettig et al., 2024).",
  "(3)": "where NX=x represents the number of instancewhose belong to tag x at tag level X. The ex-ponents , , are similar to what is suggestedby (Lample and Conneau, 2019) to tune the distri-bution to be smooth or concentrated.For the combined method of Rat. (Agg) & Tag. ,we calculate the sampling weights by multiplyingthe weights of Rat. (Sep.) and Tag..Domain Coverage Criterion (Avg.(DC)).To demonstrate the improvements brought bymaking the domain more balanced through tag-ging, we construct a domain coverage criterion",
  ": Comparison of benchmark performance across different strategies": "by averaging the accuracy scores of 6 taskswithin the following 5 domains. Sports domainis represented by SportQA (Xia et al., 2024)dataset. Medicine domain is represented by MedM-CQA (Pal et al., 2022) and MedQA-USMLE (Jinet al., 2021) datasets. Law domain is represented byJECQA (Zhong et al., 2020) dataset. Natural sci-ences domain is represented by SciQ (Welbl et al.,2017) dataset. Finance domain is represented byOpenFinData dataset1.",
  "to the baseline. Rat. (Agg.) improves almostall tasks and achieves an overall average scoreincrease of 2.4 points, which is greater thanRat. (Sep.)": "Tagging: The Tag. method shows a slight im-provement over the baseline in overall bench-marks and achieves a significant 4.3-point in-crease on the Domain Coverage benchmark.The Rat. (Agg) & Tag.method has com-parable overall performance to Rat. (Agg),with an additional 2-point improvement onAvg.(DC). Moreover, to validate the effec-tiveness of domain filtering, we evaluate anMMLU-oriented tagging model, as depictedin . The model targets 20 specificMMLU subtasks, enhancing their samplingprobability. It demonstrates improvement in15 of these 20 tasks compared to the Tag.method, thereby affirming the efficacy of thetagging system in modifying domain compo-sition for targeted reinforcement. Editing: Integration of the Editing methodsignificantly enhances model performance ondownstream tasks. Edit. increases the averagescore by 2.3 percentage points compared tothe baseline, demonstrating its effectiveness",
  "in rephrasing training data": "Rating and Editing:Rat. (Agg.)&Edit.emerges as the best-performing method, en-hancing the average score by 4.1 pointsrelative to the baseline and demonstrat-ing improvements across all tasks.Rat.(Agg.)&Tag.&Edit. attains the highest scoreon Avg. (DC) and maintains excellent per-formance in other tasks, suggesting that theintegration of tagging with rating and editingexpands the models knowledge base withoutsubstantially compromising depth.",
  "Conclusion": "In this paper, we present DecorateLM, a dataengineering method designed to refine the pre-training corpus through data rating, tagging andediting. DecorateLM employs a dual-training strat-egy, wherein two student models with 1.2 B pa-rameters are trained: one designed for rating andtagging, and the other focused on editing. Our ex-periments show that introducing rating and editingin data corpus significantly enhances data qualityby improving the overall performance of SLM onvarious existing benchmarks. Furthermore, our em-pirical study verifies that the implemented taggingstrategy achieves a more balanced distribution ofcategories within the training dataset. This equi-librium in categorization enables a more thoroughcomprehension of SLM proficiency across diversedomains. These encouraging results underscorethe importance of training data quality in fully ex-ploiting the capabilities of Large Language Models,thereby suggesting several compelling avenues forfuture research.",
  "Limitations": "Our study, while enhancing the quality of data ef-fectively, is subject to several limitations. Firstly,the biases present in GPT-4 may be reflected in thefine-tuning data used for DecorateLM, potentiallycausing DecorateLM to inherit these biases Addi-tionally, due to computational and time constraints,we limit our model training to 1.2 billion parametermodels using high-quality data. The generalizabil-ity of our findings would benefit from replicationwith larger language models and a wider range ofdatasets. Thirdly, our investigation is confined totraining models during the decay stage using theDecorated Corpus. An additional dimension to ourwork would involve creating a dataset of 1.1 trilliontokens with DecorateLM, followed by training amodel from scratch on this enlarged dataset, whichwe believe represents an important direction forfuture research.Moreover, although DecorateLM performs wellin filtering data from large-scale web data, its abil-ity to handle more specialized domains still re-quires improvement. The classification and label-ing of the diverse content of the real world by hu-mans are challenging to fully capture with a three-layer labeling system. Future research could ex-plore a more granular labeling system to enhancethe models precision and breadth in professionalfields. Lastly, while DecorateLM considered bothEnglish and Chinese, it did not take other languagessuch as French and Russian into account, whichmay limit its generalizability to other languages.An additional limitation lies in the current ap-proach to sampling, which may not adequately cap-ture the nuanced relationships between ratings andtaggings across various tasks. Therefore, futureresearch should explore a wider array of samplingstrategies for rating and tagging to assess their im-pact on task performance more comprehensively.",
  "Ethical Considerations": "As we develop DecorateLM, we recognize the in-herent risk of introducing or magnifying biaseswithin our datasets. The training process, while in-tended to refine and improve data accuracy, couldinadvertently perpetuate biases present in the origi-nal data. This raises significant ethical concerns, asbiased data can lead to unfair outcomes in decision-making processes that rely on our enhanced train-ing data. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-rat Behl, et al. 2024. Phi-3 technical report: A highlycapable language model locally on your phone. arXivpreprint arXiv:2404.14219. Jacob Austin, Augustus Odena, Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.Program synthesis with large language models. arXivpreprint arXiv:2108.07732.",
  "Ralph Allan Bradley and Milton E Terry. 1952. Rankanalysis of incomplete block designs: I. the methodof paired comparisons.Biometrika, 39(3/4):324345": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, KalpaGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-vasan, Tianyi Zhou, Heng Huang, et al. 2023. Al-pagasus: Training a better alpaca with fewer data.arXiv preprint arXiv:2307.08701. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168.",
  "Ronen Eldan and Yuanzhi Li. 2023. Tinystories: Howsmall can language models be and still speak coherentenglish? arXiv preprint arXiv:2305.07759": "Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, CaioCsar Teodoro Mendes, Allie Del Giorno, SivakanthGopi, Mojan Javaheripi, Piero Kauffmann, Gustavode Rosa, Olli Saarikivi, et al. 2023. Textbooks are allyou need. arXiv preprint arXiv:2306.11644.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, and Ja-cob Steinhardt. 2021. Measuring mathematical prob-lem solving with the math dataset. arXiv preprintarXiv:2103.03874. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, GanquCui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxi-ang Huang, Weilin Zhao, et al. 2024.Minicpm:Unveiling the potential of small language modelswith scalable training strategies.arXiv preprintarXiv:2404.06395. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, JunleiZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024.C-eval: A multi-level multi-discipline chinese evalua-tion suite for foundation models. Advances in NeuralInformation Processing Systems, 36. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,Hanyi Fang, and Peter Szolovits. 2021. What diseasedoes this patient have? a large-scale open domainquestion answering dataset from medical exams. Ap-plied Sciences, 11(14):6421.",
  "Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. arXiv preprintarXiv:1901.07291": "Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2023.Bloom:A 176b-parameter open-access multilingual language model. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, HaiZhao, Yeyun Gong, Nan Duan, and Timothy Bald-win. 2023a. Cmmlu: Measuring massive multitasklanguage understanding in chinese. arXiv preprintarXiv:2306.09212. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi,Matt Jordan, Samir Gadre, Hritik Bansal, EtashGuha, Sedrick Keh, Kushal Arora, et al. 2024.Datacomp-lm: In search of the next generation oftraining sets for language models. arXiv preprintarXiv:2406.11794.",
  "Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, AllieDel Giorno, Suriya Gunasekar, and Yin Tat Lee.2023b. Textbooks are all you need ii: phi-1.5 techni-cal report. arXiv preprint arXiv:2309.05463": "Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, andJunxian He. 2023.What makes good data foralignment?a comprehensive study of automaticdata selection in instruction tuning. arXiv preprintarXiv:2312.15685. Shayne Longpre, Gregory Yauney, Emily Reif, Kather-ine Lee, Adam Roberts, Barret Zoph, Denny Zhou,Jason Wei, Kevin Robinson, David Mimno, et al.2023. A pretrainers guide to training data: Measur-ing the effects of data age, domain coverage, quality,& toxicity. arXiv preprint arXiv:2305.13169. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-yang Lin, Chuanqi Tan, Chang Zhou, and JingrenZhou. 2023. # instag: Instruction tagging for analyz-ing supervised fine-tuning of large language models.In The Twelfth International Conference on LearningRepresentations. Pratyush Maini, Skyler Seto, He Bai, David Grangier,Yizhe Zhang, and Navdeep Jaitly. 2024. Rephrasingthe web: A recipe for compute and data-efficient lan-guage modeling. arXiv preprint arXiv:2401.16380. Philipp Moritz, Robert Nishihara, Stephanie Wang,Alexey Tumanov, Richard Liaw, Eric Liang, MelihElibol, Zongheng Yang, William Paul, Michael I Jor-dan, et al. 2018. Ray: A distributed framework foremerging {AI} applications. In 13th USENIX sym-posium on operating systems design and implementa-tion (OSDI 18), pages 561577. Ankit Pal, Logesh Kumar Umapathi, and Malaikan-nan Sankarasubbu. 2022. Medmcqa: A large-scalemulti-subject multi-choice dataset for medical do-main question answering. In Conference on health,inference, and learning, pages 248260. PMLR.",
  "Chen Qian, Xin Cong, Cheng Yang, Weize Chen,Yusheng Su, Juyuan Xu, Zhiyuan Liu, and MaosongSun. 2023. Communicative agents for software de-velopment. arXiv preprint arXiv:2307.07924": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Luca Soldaini, Rodney Kinney, Akshita Bhagia, DustinSchwenk, David Atkinson, Russell Authur, Ben Bo-gin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,et al. 2024. Dolma: An open corpus of three tril-lion tokens for language model pretraining research.arXiv preprint arXiv:2402.00159. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta,Adri Garriga-Alonso, et al. 2022.Beyond theimitation game: Quantifying and extrapolating thecapabilities of language models.arXiv preprintarXiv:2206.04615. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu,Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng,Weiwei L, Rui Hu, Chenxia Li, Liu Yang, XilinLuo, Xuejie Wu, Lunan Liu, Wenjun Cheng, PengCheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xi-aokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun,Yifu Chen, Yongyi Peng, Xiaojuan Liang, ShuichengYan, Han Fang, and Yahui Zhou. 2023. Skywork:A more open bilingual foundation model. Preprint,arXiv:2310.19341.",
  "Alexander Wettig, Aatmik Gupta, Saumya Malik, andDanqi Chen. 2024. Qurating: Selecting high-qualitydata for training language models. arXiv preprintarXiv:2402.09739": "Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang,Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jian-gang Luo, Liang Xu, et al. 2021. Yuan 1.0: Large-scale pre-trained language model in zero-shot andfew-shot learning. arXiv preprint arXiv:2110.04725. Haotian Xia, Zhengbang Yang, Yuqing Wang, RhysTracy, Yun Zhao, Dongdong Huang, Zezhi Chen,Yan Zhu, Yuan-fang Wang, and Weining Shen.2024.Sportqa: A benchmark for sports under-standing in large language models. arXiv preprintarXiv:2402.15862.",
  "Liang Xu, Xuanwei Zhang, and Qianqian Dong.2020.Cluecorpus2020:A large-scale chinesecorpus for pre-training language model.ArXiv,abs/2003.01355": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36. Haoxi Zhong, Chaojun Xiao, Cunchao Tu, TianyangZhang, Zhiyuan Liu, and Maosong Sun. 2020. Jec-qa: a legal-domain question answering dataset. InProceedings of the AAAI conference on artificial in-telligence, volume 34, pages 97019708. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,and Nan Duan. 2023. Agieval: A human-centricbenchmark for evaluating foundation models. arXivpreprint arXiv:2304.06364. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, et al. 2024. Lima: Less is more for align-ment. Advances in Neural Information ProcessingSystems, 36.",
  "Prompt Template": "Compare which text {criterion}Your judgement should not be influenced by thelanguage the text is written in, the length of the textand the order in which the texts are presented.If the texts have similar quality, you should stillmake a relative judgement and choose the label ofthe preferred text.You must respond with format:\"Choice: 1 or 2\\nWhy: reason of choice\"",
  "Generate Structural Format Data": "You are tasked with generating text data that has clearand organized formatting structures. Some structuralformat are list, markdown, headings and subheadings,table, json, html, xml, latex, columnar formats etc.The data should maintain a coherent structure with or-ganized sections, numbering, tables, code formatting,hierarchical structure, outlines or other organizationaltemplates where appropriate. You should not includeall of the formats in one data. One data can mix ofone, two or three formats.You can add various knowledge and facts into data tomake data more informative and longer.Please generate 3 lengthy and informative exam-ples about <topic> showcasing different formattingstyles and content. Split examples with <split>",
  "Prompt Template For Summary": "Your objective is to summarize the providedtext: [begin] {instance} [end], within 100 words,including the relevant information for the use case inthe summary as much as possible.The summary will represent the input data forclustering in the next step.Be concise and clear.Do not add phrases like \"This is the summary of\" or\"Summarized text:\"...Do not include any line breaks in the summary.Provide your answer in English only.Your comprehensive output should mirror thisstructure: {{\"summary\": \"\"}}.",
  "Prompt Template For First-level Tagging": "You are an advanced tagging system designed to iden-tify the most pertinent theme within a given text pas-sage: [begin] {instance} [end].Your role is to analyze the text meticulously andchoose the most fitting tag from the predefined list:Natural Sciences, Humanities and Social Sciences,Industrial Manufacturing, Medical and Health, Agri-culture and Forestry, Energy and Mining, Financeand Real Estate, Education, Transportation, Technol-ogy and Internet, Law, Military, Travel and Tourism,Entertainment, Arts and Culture, Emotional Psychol-ogy, Fashion and Beauty, Sports, Home and Lifestyle,Public Administration, and Social Events.Your task is to determine the single most relevant tagthat encapsulates the primary theme of the text.Your selection should be substantiated with a detailedexplanation, elucidating why this tag is the most accu-rate representation of the texts central subject matter.Your output should follow this structure: {{\"tag\":\"Selected Tag\", \"explanation\": \"Provide a detailedexplanation in English on why this is the most fittingtag.\"}}.",
  "Prompt Template For Second-level AndThird-level Tagging": "You are an advanced tagging system designed to cat-egorize a given text passage related to the first leveltag \"{first_level_tag}\" into specific second and third-level tags within a predefined hierarchy.Here is the tag hierarchy for the \"{first_level_tag}\"category in json format: {tag_tree}Here is the given text passage: [begin] {instance}[end].Your task is to analyze the text snippet above and as-sign the most fitting second-level and third-level tags,ensuring both tags align within the same hierarchicalpath.The output should precisely reflect the main focusof the text, justifying why these tags are the mostsuitable choices.Your output should follow this structure: {{\"sec-ond_level_tag\":\"Selected Second Level Tag\",\"third_level_tag\": \"Selected Third Level Tag\", \"ex-planation\": \"Provide a detailed explanation in Englishon why these tags accurately represent the texts corecontent.\"}}.",
  "B.1Details of rating and tagging model": "We employ MiniCPM-1.2B (Hu et al., 2024) asour base model. Utilizing the previously proposedrating and tagging methodologies, we collect rat-ing and three-level tagging of 30,000 training datasamples and subsequently apply supervised fine-tuning to the MiniCPM-1.2B with a learning rateof 0.00125 and total batch size of 480 every it-eration. The fine-tuning process is conducted onthree machines, each equipped with eight NvidiaA100 GPUs. We implement an decay step every120 iterations and a warm-up phase of 3 iterations,yielding distilled rating and tagging models. Weobserve that only 200 steps are needed to fine-tunethe model to its optimal performance in rating andtagging.",
  "B.2Details of editing model": "Similar to the rating and tagging model, we uti-lize the previously proposed editing method andcollect 10,000 data samples with rephrased con-tent by GPT-4.Subsequently, we apply super-vised fine-tuning to MiniCPM-1.2B with the samemethod and hyperparameters as the rating and tag-ging model, yielding an editing model. We observethat fine-tuning the model for optimal performancein editing tasks requires 600 steps, a notably highernumber compared to the steps needed for the rat-ing and tagging model. This increased demand fortraining iterations likely reflects the greater com-plexity and difficulty associated with editing tasks.",
  "C.1Cost Analysis": "Utilizing the vLLM framework (Kwon et al., 2023)and Ray (Moritz et al., 2018), we facilitate the gen-eration of synthetic data across distinct phases withvarying processing efficiencies on a single NvidiaA100 GPU. In the rating and tagging phase, theMiniCPM-1.2B model processes 16 million tokensper hour, requiring approximately 6,250 GPU hoursto generate 100 billion tokens. Conversely, in theediting phase, the same model configuration pro-cesses 12.5 million tokens per hour, necessitatingaround 8,000 GPU hours for the production of anequivalent volume of tokens.",
  "D.1Experimental Details": "We employ the pre-decay version of MiniCPM-1.2B, pre-trained on a corpus comprising 800 bil-lion tokens, as our base model.For training,the Decorated Corpus and additional high-qualitydatasets are utilized. The base model undergoesa decay process over 20,000 steps with a learn-ing rate of 0.01 and a batch size of 1200 tokensper iteration, distributed across 10 machines, eachequipped with eight A100-80GB GPUs. A decaystep is implemented every 5000 iterations.",
  "D.2Evaluation Details": "The overall evaluation utilizes the open-source toolUltraEval3. The underlying inference and accelera-tion use the open-source framework vLLM (Kwonet al., 2023), and the dataset includes com-monly used datasets: C-Eval (Huang et al., 2024)and CMMLU (Li et al., 2023a) for Chineseknowledge, AGI-Eval (Zhong et al., 2023) forWorld Knowledge, MMLU (Hendrycks et al.,2020) for English knowledge, HumanEval (Chenet al., 2021) and MBPP (Austin et al., 2021)for coding, GSM8K (Cobbe et al., 2021) andMATH (Hendrycks et al., 2021) for mathematics,",
  ": Comparison of rare domain benchmark performance across different strategies": "and BBH (Srivastava et al., 2022) for logic reason-ing, and ARC-E (Clark et al., 2018), ARC-C (Clarket al., 2018)for commonsense reasoning, and Trivi-aQA (Joshi et al., 2017) for Reading Comprehen-sion. Additionally, we conduct the Domain Cov-erage (DC) benchmark to evaluate the models ca-pability across various domain-specific knowledgebases. The DC Benchmark includes datasets suchas SportQA (Xia et al., 2024) for sports, MedM-CQA (Pal et al., 2022) and MedQA-USMLE (Jinet al., 2021) for medicine, JECQA (Zhong et al.,2020) for law, SciQ (Welbl et al., 2017) for naturalsciences, and OpenFinData4 for finance.",
  "Case 1": "Text:Photodynamic diagnosis (PDD) is available at somecentres. PDD makes cancer cells visible under violetlight. This makes abnormal growths (tumours) easierto find and remove. More complete removal reducesthe risk of cancer coming back.PDD is used in urol-ogy to test for bladder cancer. A catheter is insertedthrough the urethra and into the bladder. A solutionof 5-aminolaevulinic acid or hexaminolaevulinic acidis passed through the catheter into the bladder. Thenthe catheter is taken out immediately. Cancer cells inthe bladder process the chemicals in the solution andbecome fluorescent under violet light. No side-effectsor complications have been reported for PDD.Rating: Educational Value:99Expertise:92Fact and Trivia:92Scarcity:85Reasoning Level:93Story-likenss:55Structural format:91Subjectivity:25Tagging:First Level Tag:Medical And HealthSecond Level Tag:Medical ProceduresThird Level Tag:Diagnostic ProceduresEditing:Photodynamic diagnosis (PDD) is accessible at cer-tain facilities, a technique that enables the visual-ization of cancer cells under violet light, therebyenhancing the detection and removal of irregulari-ties, such as tumours, with increased precision. Thismethod further minimizes the likelihood of cancercells recurring. In the field of urology, PDD is uti-lized for the identification of bladder cancer. Theprocedure involves the insertion of a catheter throughthe urethra into the bladder. Within the catheter, asolution containing 5-aminolaevulinic acid or hex-aminolaevulinic acid is delivered. Following this,the catheter is promptly removed. It is observed thatcancer cells within the bladder respond to these chem-icals, converting them into fluorescent particles underthe influence of violet light. Due to its safe and effec-tive operation, no adverse effects or issues have beennoted associated with PDD.",
  "Case 2": "Text:Tron (TRX) is an ambitious project which aimsto own the bragging rights regarding the enter-tainment industry.To make that a reality, a dayhardly ends...Tron (TRX) is undoubtedly an ambi-tious project in the crypto community, and it aimsto own the bragging rights of the entertainment in-dustry. However, they are...Before I start explaininghow Tron (TRX) Mainnet, Odyssey, is enhancing thechances of Tron adoption already, I need to explainsome crucial thoughts with you....What will be the price of Tron (TRX) in five years from now? Tron isone of the top cryptocurrencies in the world (alwaysin the spotlight). The cryptocurrencys mainnet isnow live, and on its independence day, it...Pornhub,one of the largest X-rated video sites, expands itsvirtual currency payment options announcing theywill be soon accepting Tron (TRX). In recent times,Tron...Can Tron (TRX) become one of the top 5 cryp-tos in the market now?In recent days, Tron (TRX)has been experiencing one of the biggest tractions ofthe market, a fact that without a doubt is helping thetoken...Rating:Educational Value:45Expertise:54Fact and Trivia:80Scarcity:10Reasoning Level:51Story-likenss:60Structural format:55Subjectivity:80Tagging:First Level Tag:Technology and InternetSecond Level Tag:Entertainment and MediaThird Level Tag:Digital MediaEditing:Tron (TRX) embodies a visionary endeavor to dom-inate the boundaries of the entertainment sector, agoal that promises to be achieved on a nearly per-petual basis. In pursuit of this ambitious objective,Tron (TRX) continually strives to enhance its pres-ence. In the realm of cryptocurrency, Tron (TRX)stands out for its bold ambitions, vying for controlover the entertainment industrys prestige. Despiteits aspirations, Tron (TRX) is already making stridesin securing a larger user base, thanks to its innova-tive Mainnet, Odyssey. This initiative is pivotal inlaying the groundwork for Trons broader adoption.As we reflect on the potential future value of Tron(TRX) in the coming five years, it is noteworthy thatTron holds a prominent position among the worldscryptocurrencies, drawing significant attention. Thecryptocurrencys flagship network has commencedoperations, and its founding day marks an importantmilestone for its development. The entertainmentsectors leading platform, Pornhub, announces a ex-pansion into virtual currency payments, revealingplans to begin accepting Tron (TRX) as a paymentmethod shortly. This move highlights the growingacceptance and adoption of Tron (TRX) within thedigital landscape. Furthermore, there exists a dis-cussion surrounding the potential for Tron (TRX) tosurpass other top 5 cryptos in the market, with recentmarket trends clearly indicating its robust growth andwidespread acceptance.",
  "Case 3": "Text:Gown, $12,900, Valentino, Bal Harbour Shops andDesign District; diamond stud earrings, price uponrequest, elanjewels.us. Jennifer Hudson, emotion iseverything. Its how she breathes life into a charac-ter. Its how she makes a song explode. And itswhysince Hudson was a childpeople are drawnto her talent like a moth to a flame. Well, wait un-til you see her newest film. Larkin coat, $6,770, byErdem at Saks Fifth Avenue, Bal Harbour Shops,Brickell City Centre and Dadeland Mall; satin Bulletbodysuit, $350, by Fleur du Mal at Intermix, Bal Har-bour Shops, Brickell City Centre and Lincoln Road; Kimmy belt, $625, at Isabel Marant, Design District;printed velvet trousers, $900, by Paco Rabanne atThe Webster, Bal Harbour Shops and South Beach;Ellabrita strass sandal 105, $1,150, by Ren Caovillaat Neiman Marcus, Bal Harbour Shops and Shops atMerrick Park; diamond earrings, price upon request,at elanjewels.us. Gown, $25,000, Valentino, Bal Har-bour Shops and Design District; feather boa, $3,990,Loewe, Design District.Rating:Educational Value:10Expertise:2Fact and Trivia:51Scarcity:1Reasoning Level:11Story-likenss:50Structural format:36Subjectivity:63Tagging:First Level Tag:Fashion and BeautySecond Level Tag:Fashion ApparelThird Level Tag:Formal AttireEditing:A gown from Valentino, Bal Harbour Shops and De-sign District, retails for $12,900. Additionally, dia-mond stud earrings are available at a custom price,with the source being elanjewels.us. Jennifer Hud-son is celebrated for her profound emotional depth,capturing the essence of her characters and elevat-ing musical pieces to new heights. Her innate talent,which has attracted a multitude of fans since shewas a young age, is poignantly depicted in her latestcinematic venture. Erdems Larkin coat, priced at$6,770, is showcased at Saks Fifth Avenue, along-side other collections in Bal Harbour Shops, BrickellCity Centre, and Dadeland Mall. The Bullet body-suit, priced at $350, features a satin material by Fleurdu Mal at Intermix, along with other merchandisein these same locations. A belt by Kimmy, pricedat $625, is available at Isabel Marant in the DesignDistrict. For a more contemporary look, a printedvelvet trouser, priced at $900, by Paco Rabanne isoffered at The Webster in Bal Harbour Shops andSouth Beach. Elenabritas Ellabrita strass sandal 105,priced at $1,150, is designed by Ren Caovilla andavailable at Neiman Marcus, Shops at Merrick Park,and additional retailers. Diamond earrings, once re-quested, can be purchased from elanjewels.us. Agown from Valentino, priced at $25,000, is availablefrom Bal Harbour Shops and Design District, while afeather boa, priced at $3,990, adds a distinctive touchto Loewes designs in the Design District."
}