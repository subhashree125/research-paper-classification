{
  "Abstract": "Leveraging large language models (LLMs) forvarious natural language processing tasks hasled to superlative claims about their perfor-mance. For the evaluation of machine transla-tion (MT), existing research shows that LLMsare able to achieve results comparable to fine-tuned multilingual pre-trained language mod-els. In this paper, we explore what transla-tion information, such as the source, reference,translation errors and annotation guidelines,is needed for LLMs to evaluate MT quality.In addition, we investigate prompting tech-niques such as zero-shot, Chain of Thought(CoT) and few-shot prompting for eight lan-guage pairs covering high-, medium- and low-resource languages, leveraging varying LLMvariants. Our findings indicate the importanceof reference translations for an LLM-basedevaluation. While larger models do not nec-essarily fare better, they tend to benefit morefrom CoT prompting, than smaller models. Wealso observe that LLMs do not always providea numerical score when generating evaluations,which poses a question on their reliability forthe task. Our work presents a comprehensiveanalysis for resource-constrained and training-less LLM-based evaluation of machine transla-tion. We release the accrued prompt templates,code and data publicly for reproducibility1.",
  "Introduction": "Recent surge in the use of large language mod-els (LLMs) for natural language processing (NLP)tasks like question answering (Kocon et al., 2023;Tan et al., 2023) has taken strides, and significantlyimproved their applications to other downstreamtasks such as machine translation (MT), text sum-marization, information retrieval and etc., due toadvancements in natural language understandingcapabilities, contextual awareness, and a versatile",
  "*Both authors contributed equally to this work.1": "knowledge base (Kocmi and Federmann, 2023b;Zhu et al., 2023; Zhang et al., 2024).For automatic evaluation of MT quality, tradi-tional approaches use metrics such as BLEU (Pap-ineni et al., 2002), BLEURT (Sellam et al., 2020)or BERTScore (Zhang* et al., 2020) to compareMT output with a reference translation. Whenreferences are not available, quality estimation(QE) methods such as fine-tuning multilingual pre-trained language models (PTLMs) on human evalu-ation data like Direct Assessment (DA) scores (Gra-ham et al., 2013) are often used to predict estimatedscores to approximate human evaluation (Speciaet al., 2018). Recent studies leverage promptingtechniques and instruct LLMs to output a scorefor translation quality, claim to achieve promisingresults (Kocmi and Federmann, 2023b,a).However, there exists no systematic explorationof what translation information LLMs need forquality evaluation, and whether different promptingtechniques, such as Chain-of-Thought (CoT) (Weiet al., 2024) or few-shot prompting, can help boostthe performance of LLMs. To that end, we conductthis investigation to systematically explore the abil-ity of LLMs in quality evaluation in a training-lessscenario. Our contributions can be summarized as: We investigate what translation informa-tion, i.e., source, reference, translation errorsand annotation guidelines LLMs need to eval-uate translation for 8 language pairs coveringhigh-, medium- and low-resource languages.",
  "We compare our prompting methods with fine-tuning of encoder-based multilingual PTLMsand find LLM performance still lags behind": "Our analyses of the results on various prompttemplates indicate that references are impor-tant for accurate translation evaluation withLLMs, and while larger models are not al-ways better, they tend to benefit more fromCoT prompting than smaller model variants. The rest of the paper is structured as follows: discusses relevant work in quality evalua-tion, while introduces the dataset we uti-lize for this work. describes the prompt-ing methods and the baselines with the experimen-tal setup. Results and discussion are presented in. concludes our study and out-lines future directions.",
  "Related Work": "Traditional automatic MT quality evaluation met-rics such as BLEU, BLEURT and BERTScorecompare the MT output to one or several refer-ences, whilst metrics like Translation Error Rate(TER) (Snover et al., 2006) are based on the num-ber of edits required for MT output to become ref-erence, and neither takes semantic variations intoaccount.Training supervised machine learning systemson human-annotated data based on metrics suchas DA or Multi-dimensional Quality Metrics(MQM) (Lommel et al., 2014) can help predicttranslation quality without any references (De-oghare et al., 2023b). Ranasinghe et al. (2020b,2021) proposed the TransQuest framework to uti-lize the source text and MT output only and fine-tune XLM-RoBERTa-large (Conneau et al., 2020)to predict a DA score as an estimation of translationquality. COMET (Rei et al., 2020; Stewart et al.,2020; Rei et al., 2022b) was proposed initially toincorporate references along with the source andMT output to train multilingual PTLMs for qualityevaluation, but later it also supported reference-lessevaluation. Wan et al. (2022) proposed a unifiedtranslation evaluation framework that could includesource or reference or both as input for quality eval-uation. Various approaches achieved promisingresults in the QE shared task of the Conference onMachine Translation (WMT) (Specia et al., 2020,2021; Zerva et al., 2022; Blain et al., 2023), how-ever, most require supervision and training (De-oghare et al., 2023a; Kanojia et al., 2021).The advent of LLMs prompted its applicationto translation quality evaluation. Kocmi and Fed-ermann (2023b) proposed a zero-shot prompting technique, called GEMBA for DA score predic-tion using GPT-4 (OpenAI et al., 2024), claim-ing LLMs can achieve performance comparableto state-of-the-art models fine-tuned on multilin-gual data. Based on the GEMBA prompt, Fernan-des et al. (2023) proposed to use LLMs for bothDA score prediction and error categorization viafine-tuning to achieve more fine-grained evaluation.Previous research focused on whether LLMs canbe better translation evaluators than state-of-the-art models. To the best of our knowledge, onlyHuang et al. (2024) investigated how LLMs lever-age the source and reference for quality evaluation.However, they only perform zero-shot promptingfor three language pairs. Our work comprehen-sively examines factors such as translation errorsand annotation guidelines across eight languagepairs, eight prompt templates, and three differentprompting techniques.",
  "Data": "We utilized the DA score prediction data releasedwith WMT22 QE shared task (Zerva et al., 2022).This dataset includes the source (mainly fromnews articles), MT output (from different MT en-gines) and (post-edited) human references for eightlanguage pairs, i.e., English-German (EN-DE),English-Marathi (EN-MR), English-Chinese (EN-ZH), Estonian-English (ET-EN), Nepali-English(NE-EN), Romanian-English (RO-EN), Russian-English (RU-EN) and Sinhala-English (SI-EN).For each source-MT pair, the dataset contains aDA score ranging from 0 to 100, rated by humanannotators for quality assessment.To include annotated errors in the MT outputinto our prompts, we obtained word-level QE datafrom WMT22, where tokens of the MT output havesequence labels with either OK or BAD indi-cating translation quality at word level. This datasetalso involves the above 8 language pairs, whichcontains the source, MT output and the tags fortranslation quality. For each MT output, we ex-tracted the tokens that were tagged as BAD aserror words.Since source-MT segments from sentence-levelQE data might differ from those of word-level, wecompared each source-MT pair in the two datasetsand used the overlapping as the main resource ofour research. It includes source, MT output, refer-ence translations and error words for the 8 languagepairs, covering high-, medium- and low-resource languages. We present different prompt templatesin .2 to selectively include source, refer-ence and error words to test what translation infor-mation LLMs need for quality evaluation.We split the data into training, validation, andtest sets in proportions of 80%, 10%, and 10%respectively. We inferenced with LLMs on thetest set to obtain evaluation results. Training andvalidation sets were used to sample examples forfew-shot learning (see .4). The size ofthe test set for each language pair can be seen in.",
  "Baselines": "shows our baseline results using Tran-sQuest and COMET. Since TransQuest modelswere fine-tuned on data from each language pairwith the exception of EN-MR, the Spearman cor-relation scores of these reference-less models, arehigher than those of reference-based COMET mod-els. Except EN-DE and EN-MR, the correlationscores for most language pairs are relatively high.",
  "Zero-shot Prompting": "For LLMs to predict translation quality, our promptincludes 1) instructions to perform the task such asScore the following translation, and 2) translationinformation such as source or reference.Since Kocmi and Federmann (2023b) haveshown that their prompt template can achieve state-of-the-art performance using GPT-4, we mainlyfollowed their template to create our prompt in-struction as shown in . We used it as ourbase template and changed slightly different trans-lation information to test what is needed for LLMsto evaluate MT quality. We constructed promptTemplate 1 containing source + MT output as",
  "COMET also supports reference-less evaluation": "translation information, Template 2 MT output+ reference, Template 3 source + MT output +reference (exact GEMBA prompt), Template 4source + MT output + error words, Template 5source + MT output + reference + error words.We augmented the base prompt with summa-rized guidelines used during human evaluation, asTemplate 6 to test if this could help LLMs evaluateMT quality. These guidelines instruct evaluatorsto give a DA score by considering multiple fac-tors including accuracy, contextual understanding,grammar, syntax and overall readability.",
  "CoT Prompting": "Apart from the translation information and guide-lines added in the prompt, we also tested whetherCoT prompting could improve LLMs performanceby utilizing reasoning-based steps for quality eval-uation. We devised Template 7 which includestwo-step prompts to score MT quality, as shownin . In the first prompt, we give transla-tion information (including source, MT output andreference) to the LLM and ask it to analyze stepby step where the machine translation is differentfrom the reference. In the second prompt, we in-struct the LLM to score the machine translationbased on its previous output, i.e., the analysis ofmachine translation based on reference. Instructionto output a score in JSON format is given to ensureit produces the score first, like other templates.",
  "Few-shot Learning": "In addition to zero-shot and CoT prompting, wealso added 5 examples based on Template 3, toshow how human annotators score machine trans-lations from 0 100. We split the training andvalidation sets into 5 buckets for each language pairaccording to the score ranges of 0 20, 21 40,41 60, 61 80, 81 100. We randomly sampled1 example from each range. The selected 5 exam-ples for each language pair were given before theinstruction for scoring as a prefix (see ) ofthe base prompt in . We call this promptTemplate 8.",
  "Model Selection": "We chose 6 models from a variety of open-sourceLLMs according to their size, popularity and typesuch as mixture of expert (MoE) (Shazeer et al.,2017) and dense models, and based on our com-pute capability. For 7-billion-parameter models,we selected Llama-2-7B from Meta (Touvron et al., Score the following translation from {source_lang} to {target_lang} with re-spect to the {source/human_reference/error_words} on a continuous scale from0 to 100, where score of zero means \"no meaning preserved\" and score of onehundred means \"perfect meaning and grammar\".{translation_information}Score:",
  ": Base Prompt Template": "Prompt 1:You are going to evaluate the quality for {language_pair} translation. You needto think step by step. First read the following source, machine translation andreference translation. Analyze where the machine translation is different fromthe reference translation.Source: {source_sentence}Machine translation: {target_sentence}Reference translation: {reference_translation}Prompt 2:A large language model did an evaluation of machine translation quality for the{source_language} sentence, which is given as below: {output_from_Prompt1}Based on your analysis, score the machine translation quality on a continuousscale from 0 to 100, where score of zero means \"no meaning preserved\" andscore of one hundred means \"perfect meaning and grammar\". Provide the scorestrictly in JSON format.",
  ": Prompt Template 7": "2023), Gemma-7B from Google (Gemma Teamet al., 2024) and OpenChat3.5 which was trainedwith mixed-quality data using Conditional Rein-forcement Learning from Human Feedback (Wanget al., 2024). For 13-billion-parameter models, weopted for Llama-2-13B and Qwen1.5-14B whichwas specifically tested on a diverse set of 12 lan-guages and showed impressive multilingual capa-bilities (Bai et al., 2023). We also included theMixtral-8x7B model (Jiang et al., 2024) as ourMoE model, but due to the limit of our computecapability, we used the activation-aware weightquantized (AWQ) version (Lin et al., 2024). For all6 selected models, we used the instruction-tunedversion, i.e., the chat model, for zero-shot, CoT andfew-shot inference. Additionally, we experimentedwith TowerLLM (Alves et al., 2024) for EN-ZH viaHuggingFace3, but results are not discussed in thepaper because the model output is mostly identicalto the input for most instances (see Appendix A).",
  "for different LLM variants. We used vLLM (Kwonet al., 2023) to save inference time. Detailed set-tings for hyperparameters, formatting and evalua-tion metrics are provided below": "HyperparametersWe chose the default hyper-parameter settings in vLLM for all our experiments,i.e., 0.8 as temperature4, 0.95 for top_p. The inputsequence length was chosen as 1024 for zero-shotand CoT inference and 3000 for few-shot inference. FormattingAs chat models were fine-tuned oncertain formats to interact with humans, it is sug-gested to use the specific format that was used totrain the model while inferencing. As vLLM doesnot support formatting natively, we formatted allour prompt templates before they were fed intothe models based on the format of each model in.5.",
  ": Spearman achieved by models using Tran-sQuest and COMET on each language pair (LP)": "1904) was used to evaluate how the predicted scoresare correlated with the (mean of) human annotatedscores. However, not all LLMs would output ascore for all the instances. Sometimes, LLMs failedto score the input translation. In such cases, wedropped these instances (denoted as D in )during the process of correlation calculation, butthey were noted as a metric for robustness.",
  "Comparing results among LLMsWe observethat OpenChat3.5 achieved the highest Spearman": "correlation scores for most language pairs, despitehaving only 7 billion parameters roughly half thesize of Llama-2-13B and Qwen1.5-14B. It excellednot only in Spearman scores but also in consistentlyproviding valid quality evaluation scores, with veryfew dropped rows. Among the 6 models, Llama-2 (both the 7 and 13 billion variants) performedpoorly in generating evaluations with valid scores.Many rows were dropped, and the Spearman scoreswere low, indicating a weak correlation with thetrue scores. The MoE model, Mixtral-8x7B-AWQ,did not outperform OpenChat3.5 on most languagepairs and prompt templates for our task. Comparing with the baselinesWe find thatmodels fine-tuned for each language pair by Tran-sQuest, performed much better than the zero-shotprompting results for all language pairs. For somelanguage pairs like ET-EN, NE-EN and RO-EN,our best zero-shot prompting results (Template 6of OpenChat3.5 in ) were comparable to thereference-based models fine-tuned on multilingualdata using COMET. For some other pairs like SI-EN, our best zero-shot prompting results were evenslightly better than the COMET models. Comparison among TemplatesWhen we fixthe model variable as OpenChat3.5, we can com-pare the performance of different prompt templates.Looking at the OpenChat3.5 results in , weobserve that LLM performance is generally betterwhen the source and reference are included in theprompt, as in Templates 3, 5, and 6, compared toprompts without them, such as Templates 1 and2. This pattern holds true for other LLMs suchas the LLama-2 models and Gemma, as shown inthe table. Notably, the Spearman scores are obvi-ously higher when the source is incorporated intothe prompt, as seen by comparing Templates 2 and3. This suggests that the source is an essentialcomponent for evaluating MT quality using LLMs,contrary to the results in Huang et al. (2024), whoindicate that the source provides a negative impact.Our results (on Templates 4, 5, 6) suggest thatincluding error words and annotation guidelinesdoes not consistently help LLMs evaluate MT qual-",
  ": Density plots of the predicted (red) and true (mean) DA scores (blue) for medium- and low-resourcelanguage pairs i.e., RO-EN (left) and SI-EN (right)": "ity across different language pairs when comparedto using just the plain GEMBA prompt (Template3). For most language pairs like EN-ZH, ET-EN,RU-EN, and SI-EN, Template 3 had the highestcorrelation with human judgments. However, re-moving reference translations (Template 4) clearlylowered correlation scores, highlighting their im-portance for accurate MT evaluation.Although incorporating error words does notseem to improve performance, they are surpris-ingly useful in helping LLMs provide scores intheir outputs. As shown in , there are fewerdropped rows when using Templates 4 and 5, whichinclude error words. Outputs from Templates 4 and5 are the most stable across models, unlike othertemplates that are more model-dependent. Results among different language pairsForhigh-resource language pairs like EN-DE and EN-ZH, correlation scores tend to be lower than thoseof medium- and low-resource pairs such as NE-EN,RO-EN, and RU-EN. This pattern holds true acrossmost models, including the fine-tuned ones fromTransQuest and COMET. To further investigate the reasons, we selectedEN-DE and EN-ZH as high-resource languagepairs, and RO-EN and SI-EN as medium- and low-resource language pairs. We plotted the distribu-tions of the predicted (from OpenChat3.5) vs truescores (mean of all annotators) as shown in Fig-ures 4 and 5. For high-resource language pairs,the predictions are skewed towards higher DAscores. Well-trained MT systems, due to abundantresources, tend to produce high-quality translations,leading to higher DA scores. However, LLM-basedevaluation systems may amplify these imbalanceddistributions and are more likely to predict scoreswithin the high range. In contrast, for medium- and low-resource lan-guage pairs, there are fewer resources for trainingMT systems. As a result, low-quality translations(with low DA scores) are better represented thanin high-resource pairs. Quality evaluation systemscan better recognize low-quality translations andproduce a more balanced score distribution. Thisimbalance in the score representation could be thereason why predicted DA scores for high-resourcelanguages are less correlated with true scores than",
  "CoT and Few-shot Inference": "Tables 4 and 5 show results of CoT (Template 7)and 5-shot inference (Template 8) together withthe results of Template 3 for the 6 selected LLMs.Dropped rows for the two templates are presentedin . Both Templates 7 and 8 were builtupon Template 3, i.e., including the source, MToutput and reference. We expect the model per-formance to be improved when more reasoningsteps or evaluation examples were given. However,for 7 billion parameter variants, CoT promptingresulted in worse performance, as Spearman corre-lation scores of Template 7 were obviously lowerthan those of Template 3. For the larger 13 billionparameter variants, results were mixed for differ-ent language pairs. For language pairs such asEN-DE and EN-MR, CoT prompting improved theperformance in the prediction of DA scores. Thisindicates that CoT may work better on larger mod-els than smaller models. While CoT promptingdid not consistently improve model performanceas measured by the Spearman correlation scores, itshows relatively more consistent output than otherprompt templates. suggests that fewer rowswere dropped when using Template 7, especially for Llama-2 models.Interestingly, 5-shot inference results are not bet-ter than zero-shot results, posing a question oncontext utilization by LLMs. Performance varieson the LLMs and the specific language pairs. Thiscould relate to the language data available for train-ing these LLMs, as well as the quality of the evalua-tion examples chosen for different languages pairs.",
  "Discussion": "Based on our results, Template 3, which includesthe source, MT output and reference, but excludeserror words and detailed guidelines, performedthe best in terms of Spearman correlation scores.Prompting with CoT and few-shot learning mayyield better results for larger models, but more ex-periments are needed to confirm this.While larger language models often performbetter, our results show that a 7-billion param-eter model outperformed other models for mostlanguage pairs. Surprisingly, even much smallerCOMET models fine-tuned on multilingual data,rather than data for specific language pairs, usuallyoutperformed our LLM prompting results. How-ever, due to the high computational cost, we couldnot test models with 70 billion or more parameters.Different models excel at various language pairs",
  ": Dropped rows for Template 7 (T7) and Template 8 (T8), i.e., the CoT and few-shot prompt templates,using various LLMs for each language pair (LP)": "while struggling with others. Even for a singlemodel, performance fluctuates across different lan-guage pairs.This variability could stem fromwhether a language is considered high-resource,but further research is necessary to understand theunderlying causes.Our experiments with prompting LLMs for trans-lation evaluation reveal that these models are ofteninconsistent in generating numerical scores. Inmost cases, LLMs tend to generate scores accom-panied by lengthy and unstructured explanations.While using regular expressions for extraction canbe helpful, it is not always reliable. For models likeLlama-2, we observed numerous instances whereLLMs failed to produce a valid score. Our em-pirical findings demonstrate that employing CoTprompting or incorporating error words into theprompt can enhance the consistency of the modeloutputs.",
  "Conclusion and Future Work": "In this paper, we explored what translation infor-mation is needed for LLMs to evaluate MT qual-ity. We conducted a comprehensive investigationinto different prompting techniques such as zero-shot, CoT and few-shot prompting using differenttranslation information for 8 language pairs and 6LLMs of different sizes and types. Our findingssuggest that the source, MT output and referenceare essential compared to other information suchas translation errors for quality evaluation. Largermodels may not necessarily perform better thansmaller models, but CoT prompting works betteron larger than smaller model variants. We alsoobserve that LLMs do not always provide a nu-merical score when generating evaluations, whichmakes their assessments less reliable. For futureresearch, we plan to explore whether fine-tuningLLMs could improve their performance in quality evaluation. We also plan to thoroughly investigateerror explainability of LLMs using MQM and otherfine-grained error identification techniques. Thesefuture studies can inform downstream error correc-tion through automatic post-editing, contributingto a more comprehensive evaluation and correctionframework.",
  "Limitations and Ethical Considerations": "Our results were achieved on a limited number ofLLMs which are mostly smaller than 14 billionparameters due to the constraints of our compu-tational capabilities. Larger models may performdifferently in this translation evaluation task. Theexamples used in the few-shot scenario were ran-domly sampled since we do not have the knowledgeto prepare good-quality examples for all languagepairs. Results might be different if these exampleswere carefully chosen by native speakers.Our experiments in the paper were conductedsolely on publicly available datasets as describedin , requiring no ethical approval.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei": "Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen Technical Report. Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M.Guerreiro, Diptesh Kanojia, Jos G. C. de Souza,Beatriz Silva, Tnia Vaz, Yan Jingxuan, FatemehAzadi, Constantin Orasan, and Andr Martins. 2023.Findings of the WMT 2023 shared task on qualityestimation. In Proceedings of the Eighth Conferenceon Machine Translation, pages 629653, Singapore.Association for Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Sourabh Deoghare, Paramveer Choudhary, DipteshKanojia,Tharindu Ranasinghe,Pushpak Bhat-tacharyya, and Constantin Orasan. 2023a. A multi-task learning framework for quality estimation. InFindings of the Association for Computational Lin-guistics: ACL 2023, pages 91919205, Toronto,Canada. Association for Computational Linguistics. Sourabh Deoghare, Diptesh Kanojia, Fred Blain,Tharindu Ranasinghe, and Pushpak Bhattacharyya.2023b. Quality estimation-assisted automatic post-editing. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 16861698,Singapore. Association for Computational Linguis-tics. Patrick Fernandes, Daniel Deutsch, Mara Finkel-stein, Parker Riley, Andr Martins, Graham Neubig,Ankush Garg, Jonathan Clark, Markus Freitag, andOrhan Firat. 2023. The devil is in the errors: Leverag-ing large language models for fine-grained machinetranslation evaluation. In Proceedings of the EighthConference on Machine Translation, pages 10661083, Singapore. Association for Computational Lin-guistics. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open Models Based on Gemini Research and Tech-nology. Preprint, arXiv:2403.08295. Yvette Graham, Timothy Baldwin, Alistair Moffat, andJustin Zobel. 2013. Continuous measurement scalesin human evaluation of machine translation. In Pro-ceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 3341,Sofia, Bulgaria. Association for Computational Lin-guistics. Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Ji-ajun Chen, and Shujian Huang. 2024. Lost in theSource Language: How Large Language ModelsEvaluate the Quality of Machine Translation. In Find-ings of the Association for Computational LinguisticsACL 2024, pages 35463562, Bangkok, Thailandand virtual meeting. Association for ComputationalLinguistics. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of Experts. Preprint, arXiv:2401.04088. Diptesh Kanojia, Marina Fomicheva, Tharindu Ranas-inghe, Frdric Blain, Constantin Orasan, and LuciaSpecia. 2021. Pushing the right buttons: Adversarialevaluation of quality estimation. In Proceedings ofthe Sixth Conference on Machine Translation, pages",
  ", Online. Association for Computational Lin-guistics": "Tom Kocmi and Christian Federmann. 2023a. GEMBA-MQM: Detecting translation quality error spans withGPT-4. In Proceedings of the Eighth Conferenceon Machine Translation, pages 768775, Singapore.Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023b. Largelanguage models are state-of-the-art evaluators oftranslation quality. In Proceedings of the 24th An-nual Conference of the European Association for Ma-chine Translation, pages 193203, Tampere, Finland.European Association for Machine Translation. Jan Kocon, Igor Cichecki, Oliwier Kaszyca, MateuszKochanek, Dominika Szydo, Joanna Baran, JulitaBielaniewicz, Marcin Gruza, Arkadiusz Janz, KamilKanclerz, Anna Kocon, Bartomiej Koptyra, Wik-toria Mieleszczenko-Kowszewicz, Piotr Mikowski,Marcin Oleksy, Maciej Piasecki, ukasz Radlinski,Konrad Wojtasik, Stanisaw Wozniak, and Prze-mysaw Kazienko. 2023. ChatGPT: Jack of all trades,master of none. Information Fusion, 99:101861. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-zalez, Hao Zhang, and Ion Stoica. 2023. EfficientMemory Management for Large Language ModelServing with PagedAttention. In Proceedings of the29th Symposium on Operating Systems Principles,SOSP 23, page 611626, New York, NY, USA. As-sociation for Computing Machinery. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,Xingyu Dang, Chuang Gan, and Song Han. 2024.AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration.InProceedings of Machine Learning and Systems, vol-ume 6, pages 87100. Arle Richard Lommel, Aljoscha Burchardt, and HansUszkoreit. 2014. Multidimensional Quality Metrics:A Flexible System for Assessing Translation Qual-ity. Tradumtica: tecnologies de la traducci, 0:455463. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and BarretZoph. 2024. GPT-4 Technical Report. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Tharindu Ranasinghe, Constantin Orasan, and RuslanMitkov. 2020a. TransQuest at WMT2020: Sentence-level direct assessment. In Proceedings of the FifthConference on Machine Translation, pages 10491055, Online. Association for Computational Lin-guistics. Tharindu Ranasinghe, Constantin Orasan, and RuslanMitkov. 2020b. TransQuest: Translation quality esti-mation with cross-lingual transformers. In Proceed-ings of the 28th International Conference on Com-putational Linguistics, pages 50705081, Barcelona,Spain (Online). International Committee on Compu-tational Linguistics. Tharindu Ranasinghe, Constantin Orasan, and RuslanMitkov. 2021. An exploratory analysis of multilin-gual word-level quality estimation with cross-lingualtransformers. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 2: ShortPapers), pages 434440, Online. Association forComputational Linguistics. Ricardo Rei, Jos G. C. de Souza, Duarte Alves,Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,Alon Lavie, Luisa Coheur, and Andr F. T. Martins.2022a. COMET-22: Unbabel-IST 2022 submissionfor the metrics shared task. In Proceedings of theSeventh Conference on Machine Translation (WMT),pages 578585, Abu Dhabi, United Arab Emirates(Hybrid). Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020. COMET: A neural framework for MTevaluation. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 26852702, Online. Associationfor Computational Linguistics. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro,Chrysoula Zerva, Ana C Farinha, Christine Maroti,Jos G. C. de Souza, Taisiya Glushkova, DuarteAlves, Luisa Coheur, Alon Lavie, and Andr F. T.Martins. 2022b. CometKiwi: IST-unbabel 2022 sub-mission for the quality estimation shared task. InProceedings of the Seventh Conference on MachineTranslation (WMT), pages 634645, Abu Dhabi,",
  "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "BLEURT: Learning robust metrics for text genera-tion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78817892, Online. Association for ComputationalLinguistics. NoamShazeer,AzaliaMirhoseini*,KrzysztofMaziarz*, Andy Davis, Quoc Le, Geoffrey Hinton,and Jeff Dean. 2017.Outrageously Large NeuralNetworks: The Sparsely-Gated Mixture-of-ExpertsLayer.In International Conference on LearningRepresentations. Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul. 2006. A study of trans-lation edit rate with targeted human annotation. InProceedings of the 7th Conference of the Associationfor Machine Translation in the Americas: TechnicalPapers, pages 223231, Cambridge, Massachusetts,USA. Association for Machine Translation in theAmericas.",
  "Charles Spearman. 1904. The Proof and Measurementof Association between Two Things. The AmericanJournal of Psychology, 15:72101": "Lucia Specia, Frdric Blain, Marina Fomicheva, Er-ick Fonseca, Vishrav Chaudhary, Francisco Guzmn,and Andr F. T. Martins. 2020. Findings of the WMT2020 shared task on quality estimation. In Proceed-ings of the Fifth Conference on Machine Translation,pages 743764, Online. Association for Computa-tional Linguistics. Lucia Specia, Frdric Blain, Marina Fomicheva,Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary,and Andr F. T. Martins. 2021. Findings of the WMT2021 shared task on quality estimation. In Proceed-ings of the Sixth Conference on Machine Translation,pages 684725, Online. Association for Computa-tional Linguistics.",
  "Lucia Specia, Carolina Scarton, and Gustavo HenriquePaetzold. 2018.Quality Estimation for MachineTranslation. Spinger, Cham, Germany": "Craig Stewart, Ricardo Rei, Catarina Farinha, and AlonLavie. 2020. COMET - deploying a new state-of-the-art MT evaluation metric in production. In Pro-ceedings of the 14th Conference of the Associationfor Machine Translation in the Americas (Volume 2:User Track), pages 78109, Virtual. Association forMachine Translation in the Americas. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. 2023. Can ChatGPTReplace Traditional KBQA Models? An In-DepthAnalysis of the Question Answering Performanceof the GPT LLM Family. In The Semantic Web ISWC 2023, pages 348367, Cham. Springer NatureSwitzerland. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint, arXiv:2307.09288. Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang,Boxing Chen, Derek Wong, and Lidia Chao. 2022.UniTE: Unified translation evaluation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 81178127, Dublin, Ireland. Associationfor Computational Linguistics. Guan Wang, Sijie Cheng, Xianyuan Zhan, XiangangLi, Sen Song, and Yang Liu. 2024. OpenChat: Ad-vancing Open-source Language Models with Mixed-Quality Data. In The Twelfth International Confer-ence on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2024. Chain-of-thought prompt-ing elicits reasoning in large language models. InProceedings of the 36th International Conference onNeural Information Processing Systems, NIPS 22,Red Hook, NY, USA. Curran Associates Inc. Chrysoula Zerva, Frdric Blain, Ricardo Rei, PiyawatLertvittayakumjorn, Jos G. C. de Souza, SteffenEger, Diptesh Kanojia, Duarte Alves, ConstantinOrasan, Marina Fomicheva, Andr F. T. Martins, andLucia Specia. 2022. Findings of the WMT 2022shared task on quality estimation. In Proceedingsof the Seventh Conference on Machine Translation(WMT), pages 6999, Abu Dhabi, United Arab Emi-rates (Hybrid). Association for Computational Lin-guistics.",
  ": Pearsons r and Kendalls correlation scores achieved using Templates 1-8 (T1-8) on various open-sourceLLMs for each language pair (LP)": "Model input (before formatting):Score the following translation from English to Chinese with respect to thehuman reference on a continuous scale from 0 to 100, where score of zero means\"no meaning preserved\" and score of one hundred means \"perfect meaningand grammar\".\\nEnglish source: The last conquistador then rides on withhis sword drawn.\\nChinese human reference: \\nChinese translation: .\\nScore:Model output:<|im_start|>user\\nScore the following translation from English to Chinese withrespect to the human reference on a continuous scale from 0 to 100, wherescore of zero means \"no meaning preserved\" and score of one hundred means\"perfect meaning and grammar\". \\nEnglish source: The last conquistador thenrides on with his sword drawn.\\nChinese human reference: \\nChinese translation: .\\nScore:<|im_end|>\\n<|im_start|>assistant\\n."
}