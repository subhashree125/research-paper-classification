{
  "Abstract": "We study the presence of heteronormative bi-ases and prejudice against interracial romanticrelationships in large language models by per-forming controlled name-replacement experi-ments for the task of relationship prediction.We show that models are less likely to pre-dict romantic relationships for (a) same-gendercharacter pairs than different-gender pairs; and(b) intra/inter-racial character pairs involvingAsian names as compared to Black, Hispanic,or White names. We examine the contextual-ized embeddings of first names and find thatgender for Asian names is less discernible thannon-Asian names. We discuss the social impli-cations of our findings, underlining the needto prioritize the development of inclusive andequitable technology.",
  "Introduction": "Identifying romantic relationships from a givendialogue presents a challenging task in natural lan-guage understanding (Jia et al., 2021; Tigunovaet al., 2021). The perceived gender, race, or eth-nicity of the speakers, often inferred from theirnames, may inadvertently lead a model to predicta relationship type that conforms to conventionalsocietal views. We hypothesize that, when predict-ing romantic relationships, models may mirror het-eronormative biases (Pollitt et al., 2021; Vsquezet al., 2022) and prejudice against interracial ro-mantic relationships (Lewandowski and Jackson,2001; Miller et al., 2004) present in humans andsociety. Heteronormative biases assume and favortraditional gender roles, heterosexual relationships,and nuclear families, often marginalizing other gen-der expressions, sexuality, and family dynamics. Inthe US, legal protections for interracial and gaymarriages were not achieved nationwide until 1967",
  "*These authors contributed equally to this work": ":Sample conversation from DDRel (Jiaet al., 2021) dataset and relationships predicted byLlama2-7B when characters are replaced by names withdifferent-gender and same-gender. LLM tends to pre-dict differently despite the same conversation. and 2015, respectively. These relationships con-tinue to face prejudice and discrimination in thepresent days (Buist, 2019; Knauer, 2020; Zambelli,2023; Pittman et al., 2024; Daniel, 2024).In this paper, we consider the task of predict-ing romantic relationships from dialogues in moviescripts to study whether LLMs make such predic-tions based on the demographic attributes associ-ated with a pair of character names, in ways that re-flect heteronormative biases and prejudice againstinterracial romantic relationships. For instance, shows a conversation between a femaleand a male spouse pair, for which Llama2-7B pre-dicts a romantic relationship when the names in theconversation are replaced with a pair of different-gender names, but predicts a non-romantic relation-ship when replaced by same-gender names.Ideally, name-replacement should not signifi-cantly alter the predictions of a fair and robustmodel, as the utterance content plays a more sub-stantial role in language understanding, despite thepotential interdependence between utterances andoriginal names. Different predictions suggest thata model may be prone to overlooking romantic re-lationships that diverge from societal norms, thusraising ethical concerns. Such behavior would indi-cate that language models inadequately representcertain societal groups (Blodgett et al., 2020), po-tentially exacerbating stigma surrounding relation- ships (Rosenthal and Starks, 2015; Reczek, 2020)and sidelining underrepresented groups (Nozzaet al., 2022; Felkner et al., 2023).Through controlled character name-replacementexperiments, we find that relationships between(a) same-gender character pairs; and (b) intra/inter-racial character pairs involving Asian names areless likely to be predicted as romantic. These find-ings reveal how some LLMs may stereotypicallyinterpret interactions between people, potentiallyreducing the recognition of non-mainstream rela-tionship types. While prior work studies genderand racial biases by identifying stereotypical at-tributes of individuals (Cao et al., 2022; Chenget al., 2023; An et al., 2023), this paper investigatesthe role of gender and race in LLMs inferencesabout relationships between two individuals usinga relationship prediction dataset (Jia et al., 2021).",
  "Experimental Setup": "We define the following task.Given a conver-sation C which consists of a sequence of turns((S1, u1), (S2, u2), . . . , (Sn, un)) between charac-ters A and B, where Si {SA, SB} indicatesthat the speaker of an utterance (ui, i {1 : n})is either A or B, the task is to identify the rela-tionship represented as a categorical label from apre-defined set. We carry out controlled name-replacement experiments by prompting LLMs(zero-shot) to predict the relationship type betweenA and B given C.",
  "DatasetWe use the test set of DDRel (Jia et al.,": "2021) which consists of movie scripts from IMSDb,with annotations for relationship labels between thecharacters according to 13 pre-defined types (Ta-ble 3 in appendix). We consider Lovers, Spouse,or Courtship predictions as romantic and the restas non-romantic. For our experiments, we use 327instances of the test set in which characters origi-nally have different genders (manually annotated)because the test set has no dialogues between same-gender characters with the romantic label. We dis-cuss the limitations of this study due to data sourcerepresentation issues at the end of this paper. Prompt SelectionAs LLMs are sensitive toprompts (Min et al., 2022), we experimented withseveral prompt formulations on the original data(test set) for accuracy, and selected the prompt (see in appendix) resulting in the highest ac-curacy which was closest to scores reported byothers (Jia et al., 2021; Ou et al., 2024). We notethat our prompt selection is done prior to runningthe name-replacement experiments. EvaluationWe compare the average recall ofpredicting romantic relationships across differentgender assignments and races/ethnicities. We studyrecall as we hypothesize heteronormative and in-terracial relationship biases would manifest as low(romantic) recall for same-gender and interracialgroups. For completeness, we also report the meanprecision, F1, and accuracy scores in D.",
  "Studying the Influence of Gender Pairings": "We ask whether the models are equally likely torecognize romantic relationships for character pairsof varying gender assignments and if this behavioris the same across different races. We hypothesizethat models are prone to heteronormative bias andare more likely to predict romantic relationshipsfor contrastive gender assignments. To test this,we collect 30 names per race,2 dividing them into10 non-linearly segmented bins that cover gender-neutral names (shown in ) based on thepercentage of population that has been assigned asfemale at birth. Detailed name inclusion criteriaand data sources are elaborated in C.1. We replacethe original name-pair in each conversation withall pairs of distinct names per race.As dialogues may reveal gender identities (e.g.,sir, maam, father, etc.), we manually identifya subset (271 instances) where such explicit cuesare absent (to the best of our judgement) to mini-mize gender information leakage and avoid explicitgender inconsistency between the dialogue and thegender associated with the replaced name. In thesedialogues, gendered pronouns typically refer to athird person who is not part of the conversation. Asa result, they do not reveal the speakers genderidentity. However, pronouns can indicate the sex-ual orientation of a speaker (e.g., Betty: You dolove him, dont you?). Such cues, along with otherimplicit cues about gender identity that are harderto detect, may confound our analysis. However, our",
  "Except for Hispanic wherein we did not get any names in5 10% bin and only 1 name in 25 50% bin": "0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.560.550.590.580.610.590.690.680.640.72 0.560.520.570.580.590.580.650.660.630.69 0.610.580.630.600.640.620.700.690.670.72 0.580.570.590.600.620.590.670.660.630.70 0.610.600.640.620.650.620.690.690.660.69 0.590.560.610.600.610.600.660.640.620.67 0.660.640.670.660.670.640.690.660.650.67 0.670.650.680.660.680.640.670.670.640.67 0.640.610.650.630.650.610.650.630.620.65 0.700.660.700.680.680.660.680.670.650.68 MaleNeutralFemale MaleNeutralFemale Asian (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.600.610.610.600.660.700.650.740.810.79 0.600.600.600.600.650.690.640.710.790.76 0.620.620.620.610.660.710.650.730.790.76 0.610.610.600.610.640.680.630.700.760.74 0.670.660.660.640.600.650.640.670.740.71 0.710.690.700.670.660.750.670.690.740.71 0.650.640.660.630.650.680.620.660.700.68 0.740.700.720.680.670.690.650.640.680.62 0.800.770.780.750.730.730.690.670.670.65 0.780.740.740.720.690.710.660.620.660.61 MaleNeutralFemale Black (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.590.600.620.710.760.680.800.850.85 0.590.600.620.690.720.640.770.810.81 0.620.640.660.700.750.680.780.830.84 0.660.650.660.670.660.710.760.75 0.740.720.730.670.680.690.700.750.72 0.650.640.650.680.700.680.730.800.79 0.760.750.760.710.680.720.780.760.72 0.820.820.820.760.750.790.770.830.78 0.820.800.810.740.700.780.730.770.75 MaleNeutralFemale Hispanic (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.520.620.560.600.730.620.760.840.800.86 0.610.690.630.630.760.660.780.840.800.87 0.570.650.590.610.730.620.730.830.770.83 0.620.640.610.640.750.640.760.840.770.85 0.730.770.730.740.730.680.700.750.690.76 0.610.650.610.630.680.610.660.750.690.77 0.740.770.710.740.680.650.660.700.630.69 0.830.840.810.830.750.730.690.750.660.69 0.810.810.750.770.690.680.630.650.600.62 0.860.860.810.820.740.750.670.670.630.61 MaleNeutralFemale White (Recall) 0.55 0.60 0.65 0.70 0.75 0.80 0.85 : Recall of predicting romantic relationships from Llama2-7B for subset of the dataset where charactersoriginally have different genders. Horizontal and vertical axes denote % female of the name replacing an originallyfemale and male character name from the dialogue. The upper-triangle (lower-triangle) shows the scores whennames are replaced preserving (swapping) the genders of characters names as-is in the original conversation. Weconsider the names with lesser % female as male names for determining gender preservation for name-replacement.",
  "Studying Intra/Inter-Racial Pairings": "We examine whether the models exhibit preju-dice against interracial romantic relationships whenmaking predictions.We collect another set of80 first names that are both strongly race- andgender-indicative, evenly distributed among fourraces/ethnicities and two genders (details describedin C.2). We perform pairwise name-replacementsusing these 80 names for the 327 test samples to an-alyze the relationship predictions among differentintra/inter-racial name pairs.We defer details related to full prompt used andmodel output parsing to A.",
  "Findings": "Same-gender relationships are less likely to bepredicted as romantic than different-genderones.We observe a significant variation in recallof romantic relationship predictions from Llama2-7B (see ) for name-replacements involvingdifferent (top-right, and bottom-left)- versus same-gender pairs. This reveals that the model conser-vatively predicts romantic relationships when boththe characters have names associated with the samegender (top-left both male; bottom-right bothfemale). However, the precision across all racesranges between 0.78 0.84 (see in ap-pendix). Such (relatively) low difference indicatesthat, while the model makes precise romantic pre-dictions across all gender assignments and races,romantic predictions are more likely for contrastivegender assignments. Higher recall () forboth female (bottom-right) replacements than both male (top-left) across all races indicates a poten-tial stronger heteronormative bias against bothmale than both female pairs. This could poten-tially be an effect of associating female names withromantic relationships as indicated by higher recallfor female-neutral than male-neutral pairs. To testthis hypothesis, we substitute one speakers namewith a male, female or neutral name while keepingthe other anonymized (substituting with X). Wefind that name pairs containing one female nametend to have higher recall than those containing onemale name ( in appendix). This could eitherbe due to a stronger association of female nameswith romantic relationships in general, or strongerheteronormative bias against male-male romanticrelationships if models are (effectively) marginaliz-ing probabilities over the anonymous character. Apossible explanation for the former is that womentend to be portrayed only as objects of romancein fictional works, e.g., as popularly evidenced bythe failure of many movies to pass the Bechdeltest (Agarwal et al., 2015).The smaller gap in the recall between bothfemale (bottom-right) name-replacements anddifferent-gender (top-right and bottom-left) onesfor Asian and Hispanic as compared to White andBlack may result from models inability to discerngender from Asian and Hispanic names as accu-rately as for White and Black names. Figures 6and 7 (appendix) show similar trends for Llama2-13B and Mistral-7B, respectively. The unnaturalness of movie scripts with nameand gender substitutions could, in theory, pro-vide an alternative explanation for the observedbiases, but the evidence shows this is not thecause.As female characters may speak differ- AsianBlackHispanicWhite",
  ": Recall of predicting romantic relationshipsfrom Llama2-7B for subset of the dataset where charac-ters have different genders and are replaced with namesassociated with different races/ethnicities": "ently from male characters, our name-replacementscan introduce statistical inconsistency between thegender associated with a character name and thestyle or content of the lines they speak, potentiallyconfounding our observations. However, compa-rable recall between name-replacements that pre-serve the gender (upper-triangle; specifically top-right) associated with the original speakers andthe swapped variants (lower-triangle; specificallybottom-left) in , indicates that swappingboth characters genders has minimal impact onmodels performance in the conversations we used.Hence, we conclude the potential inconsistency be-tween gender and linguistic content is not a majorconfounding factor. Character pairs involving Asian names havelower romantic recall; however, we do notfind strong evidence against interracial pairings.While Llama2-7B has similar precision of predict-ing a romantic relationship across all racial pairs(0.80 0.82, shown in in appendix), Fig-ure 3 shows name pairs involving at least one Asianname have significantly lower recall. Noticeably,the recall is the lowest (0.68) when both charac-ter names are associated with Asian. Althoughthere are variations in recall values among differentracial setups, we do not observe disparate differ-ences between interracial and intraracial name pairsfor non-Asian names. Results for Llama2-13B andMistral-7B, shown respectively in and 10in the appendix, demonstrate a similar trend thatAsian names lead to substantially lower recall val-ues. Such systematically worse performance onAsian names potentially perpetuates known algo-rithmic biases (Chander, 2016; Akter et al., 2021;Papakyriakopoulos and Mboya, 2023).",
  "We perform additional experiments to understandthe observed model behavior": "Why does a model tend to predict fewer roman-tic relationships for racial pairings that involveAsian names?Although we select names foreach race that have strong real-world statistical as-sociations with one gender, we hypothesize that lowrecall on pairs with one or more Asian names maybe due to models inability to discern gender fromAsian names. To test this hypothesis, we retrievethe contextualized embeddings from Llama2-7Bfor each first name (collected in 2.2) occurrencein 15 romantic and 15 non-romantic random dia-logues. We obtain 209, 800 embeddings, which areused to train logistic regression models that classifythe gender or race associated with a name (detailsin A). As we compare the average classificationaccuracy (across 5 different train-test splits) againsta majority baseline, we observe, in , thatgender could be effectively predicted for non-Asianname embeddings, and the embeddings are distin-guishable by race for all races/ethnicities in a One-vs-All setting. However, Asian name embeddingsencode minimal gender information, decreasing thelikelihood of a model leveraging the inferred gen-der identity when making relationship predictionsthat reflect heteronormative biases. Does gender association have a stronger influ-ence on models prediction than race/ethnicity?We hypothesize that models tendency to asso-ciate gender with names influences their relation-ship predictions. To test this, we substitute nameswith generic placeholders (X and Y) to geta baseline where a model has no access to char-acter names (more details in B). After name-replacements, any deviation from these results (Ta-ble 2) would indicate that a model exploits theimplicit information from first names.In Fig-ure 2, multiple settings have recall values thatsignificantly differ from those in the anonymized",
  ":Evaluation scores for anonymous name-replacements (character replaced with X or Y) fordifferent models under study. These results depict themodels performance solely based on the context": "setting (0.6887). This disparity suggests name-replacements introduce gender information thatsignificantly influences the model behavior. Suchtrends are less prominent for Asian names due tothe models apparent inability to distinguish genderinformation in Asian names (). By contrast,racial information encoded in first names exerts alesser impact. Non-Asian heterosexual intra/inter-racial pairs give rise to similar recall in .We thus do not observe strong prejudice againstinterracial romantic relationships here.",
  "Social Implications": "It has been a prolonged and arduous struggle torecognize and accept gay marriages in the US (An-dersen, 2016; Duberman, 2019). Legal recogni-tion of these relationships remains a challengein many other countries (Lee and Ostergard Jr,2017; Chia, 2019; Ramdas, 2021). Even withinthe US, LGBTQIA+ people still encounter discrim-ination (Buist, 2019; Knauer, 2020; Naylor, 2020).We believe heteronormative biases we have ob-served could impact various downstream LLM usecases, potentially causing both representational andallocational harms (Blodgett et al., 2020). For ex-ample, when LLMs are used for story generationbased on social media posts as the premise (Teet al., 2018; Li et al., 2024a), the life events ofmembers of the LGBTQIA+ community may beoverlooked or misrepresented. If LLMs struggle torecognize same-gender romantic relationships, theymay further marginalize the LGBTQIA+ commu-nity by diminishing their social visibility and rep-resentation. In addition, such model behavior mayresult in uneven allocation of resources or opportu-nities. Consider an online advertising system that promotes low-interest home loans for married cou-ples based on social media interactions. A modelunable to identify same-gender marriages wouldexclude these couples from the promotion. There-fore, building inclusive technology that respectsminority rights is essential.",
  "Related Work": "Prior works (Wang et al., 2022; Jeoung et al., 2023;Sandoval et al., 2023; Wan et al., 2023; An et al.,2023, 2024; Nghiem et al., 2024) show that lan-guage models often treat first names differently,even with controlled input contexts, due to factorslike frequency and demographic attributes associ-ated with names (Maudslay et al., 2019; Shwartzet al., 2020; Wolfe and Caliskan, 2021; Czarnowskaet al., 2021; An and Rudinger, 2023). Our workuses models interpretations of gender associatedwith first names to reveal heteronormative biasesin some LLMs.Further, NLP systems often fail in interpretingvarious social factors (e.g., social norms, cultures,and relations) of language (Hovy and Yang, 2021).One such factor of interest is the representationof social relationships in these systems, includingpower dynamics (Prabhakaran et al., 2012), friend-ship (Krishnan and Eisenstein, 2015), and romanticrelationships (Seraj et al., 2021). Recently, Stewartand Mihalcea (2024) show failure of popular ma-chine translation systems in translating sentencesconcerning relationships between nouns of same-gender. Leveraging the task of relationship predic-tion and using an existing dataset (Jia et al., 2021),our work contributes to the assessment of socialrelationship-related biases in LLMs arising fromgender and race associations with first names.",
  "Conclusion": "Through controlled name-replacement experi-ments, we find that LLMs predict romantic rela-tionships between characters based on the demo-graphic identities associated with their first names.Specifically, relationship predictions between same-gender and intra/inter-racial character pairs involv-ing Asian names are less likely to be romantic.Our analysis of contextualized name embeddingssheds light on the cause of our findings. We alsohighlight the social implications of this potentiallyharmful model behavior for the LGBTQIA+ com-munity. We urge advocates to build technology thatrespects the rights of marginalized social groups.",
  "Limitations": "Prompt sensitivity and in-context learning.LLMs are sensitive to prompt formats (Min et al.,2022; Li et al., 2024b) therefore the accuracy of pre-dictions may vary within or across models. Whilewe had experimented with several prompts beforeconverging to the one we use (gave the best predic-tion accuracy on the original dataset as well as closeto that reported in Jia et al. (2021)), future workmay investigate the impact of different prompt for-mulations and if in-context learning can help inreducing the influence of biases on the downstreamtasks. Inadequate coverage of names associated withdifferent identities.We recognize that our paperhas limitations regarding the number of races andgenders studied. This is due to the unavailability ofdata sources to compile a sufficiently large numberof names strongly associated with a wide range ofunderrepresented races and gender identities. Linguistic usage might be significantly differentin same-gender romantic relationships.Thetest set we have utilized (Jia et al., 2021) doesnot contain dialogues between same-gender char-acter pairs in romantic relationships. As a con-sequence, we lack conversations that effectivelydepict interactions between same-gender partners.We acknowledge this limitation in our data source.However, in cases where same-gender partners ex-hibit behavior similar to different-gender couples,our results indicate that LLMs tend to demonstrateheteronormative biases in the intersection of theseinteraction styles. Conversations might contain implicit gender-revealing cues.While we ensure consistency be-tween gender associated with an utterance (basedon how a male speaks vs a female) and the gen-der associated with a name by only consider-ing the conversations that do not have explicitgender-revealing cues as described in 2.1, we ac-knowledge the possibility of the presence of im-plicit gender-revealing cues which is harder todetect.However, we believe that our findingsstand valid even if the implicit cues are presentas demonstrated by comparable recall betweenname-replacements that preserve the gender (upper-triangle; specifically top-right) associated with theoriginal speaker and the swapped variants (lower-triangle; specifically bottom-left) in . We",
  "Ethical Considerations": "Inconsistency between self-identification and de-mographic attributes associated with a name.Our categorization of names into subgroups ofrace/ethnicity and gender is based on real-worlddata as we observe a strong statistical associa-tion between names and demographic attributes(race/ethnicity and gender). However, it is cru-cial to realize that a person with a particular namemay identify themselves differently from the ma-jority, and we should respect their individual pref-erences and embrace the differences. We have at-tempted to accommodate diverse possibilities inself-identification by incorporating gender-neutralnames into our experimental setup. While thereis still ample room for improvement in address-ing this issue, we have taken a step forward inpromoting the inclusion of additional forms of self-identification in ethical NLP research. Ethical concerns about the task of relation-ship prediction.Predicting interpersonal rela-tionships from conversations may require accessto private and sensitive data. If no proper con-sent from a user is obtained, using personal datacould lead to serious ethical and legal concerns.Although building systems that identify the rela-tionship type between speakers could contributeto the development of AI agents that better under-stand human interactions, it is crucial to be trans-parent about what data is collected and how it isprocessed in such systems. Even if data privacy isproperly handled when using a model to predictrelationship types, people often exercise cautionwhen revealing romantic relationships. Therefore,the deployment of an NLP system to identify suchrelationships should be disclosed to users who maybe affected, and any predictions should remain con-fidential unless the users consent is obtained forpublic disclosure.",
  "Acknowledgements": "We would like to thank the anonymous reviewersfor their valuable feedback. Rachel Rudinger issupported by NSF CAREER Award No. 2339746.Any opinions, findings, and conclusions or recom-mendations expressed in this material are thoseof the author(s) and do not necessarily reflect theviews of the National Science Foundation. Apoorv Agarwal, Jiehan Zheng, Shruti Kamath, Sri-ramkumar Balasubramanian, and Shirin Ann Dey.2015. Key female characters in film have more totalk about besides men: Automating the Bechdel test.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 830840, Denver, Colorado. Association forComputational Linguistics. Shahriar Akter, Grace McCarthy, Shahriar Sajib, KatinaMichael, Yogesh K. Dwivedi, John DAmbra, andK.N. Shen. 2021. Algorithmic bias in data-driveninnovation in the age of ai. International Journal ofInformation Management, 60:102387. Haozhe An, Christabel Acquaye, Colin Wang, ZongxiaLi, and Rachel Rudinger. 2024. Do large languagemodels discriminate in hiring decisions on the ba-sis of race, ethnicity, and gender? In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 386397, Bangkok, Thailand. Association forComputational Linguistics. Haozhe An, Zongxia Li, Jieyu Zhao, and RachelRudinger. 2023. SODAPOP: Open-ended discov-ery of social biases in social commonsense reasoningmodels. In Proceedings of the 17th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 15731596, Dubrovnik,Croatia. Association for Computational Linguistics. Haozhe An and Rachel Rudinger. 2023. Nichelle andnancy: The influence of demographic attributes andtokenization length on first name biases. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 388401, Toronto, Canada. Associationfor Computational Linguistics.",
  "Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem,and Jordan Lee Boyd-Graber. 2024b. Pedants: Cheapbut effective and interpretable answer equivalence": "Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, andSimone Teufel. 2019. Its all in the name: Mitigatinggender bias with name-based counterfactual data sub-stitution. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages52675275, Hong Kong, China. Association for Com-putational Linguistics. Suzanne C. Miller, Michael A. Olson, and Russell H.Fazio. 2004. Perceived reactions to interracial ro-mantic relationships: When race is used as a cueto status. Group Processes & Intergroup Relations,7(4):354369. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer. 2022. Rethinking the role of demonstrations:What makes in-context learning work? In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1104811064,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics.",
  "Lorenda A Naylor. 2020. Social equity and LGBTQrights: Dismantling discrimination and expandingcivil rights. Routledge": "Huy Nghiem, John Prindle, Jieyu Zhao, and HalDaum III. 2024. \" you gotta be a doctor, lin\": Aninvestigation of name-based bias of large languagemodels in employment recommendations.arXivpreprint arXiv:2406.12232. Debora Nozza, Federico Bianchi, Anne Lauscher, andDirk Hovy. 2022. Measuring harmful sentence com-pletion in language models for LGBTQIA+ individ-uals. In Proceedings of the Second Workshop onLanguage Technology for Equality, Diversity and In-clusion, pages 2634, Dublin, Ireland. Associationfor Computational Linguistics. Jiao Ou, Junda Lu, Che Liu, Yihong Tang, FuzhengZhang, Di Zhang, and Kun Gai. 2024. Dialogbench:Evaluating llms as human-like dialogue systems. InProceedings of the 2024 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(Volume 1: Long Papers), pages 61376170.",
  "Patricia S. Pittman, Claire Kamp Dush, Keeley J. Pratt,and Jen D. Wong. 2024. Interracial couples at risk:Discrimination, well-being, and health. Journal ofFamily Issues, 45(2):303325": "Amanda M Pollitt, Sara E Mernitz, Stephen T Russell,Melissa A Curran, and Russell B Toomey. 2021. Het-eronormativity in the lives of lesbian, gay, bisexual,and queer young people. Journal of Homosexuality,68(3):522544. Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab. 2012. Predicting overt display of power in writ-ten dialogs. In Proceedings of the 2012 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 518522, Montral, Canada. As-sociation for Computational Linguistics.",
  "Evan TR Rosenman, Santiago Olivella, and KosukeImai. 2023. Race and ethnicity data for first, middle,and surnames. Scientific Data": "Lisa Rosenthal and Tyrel J Starks. 2015. Relationshipstigma and relationship outcomes in interracial andsame-sex relationships: Examination of sources andbuffers. Journal of Family Psychology, 29(6):818. Sandra Sandoval, Jieyu Zhao, Marine Carpuat, and HalDaum III. 2023. A rose by any other name wouldnot smell as sweet: Social bias in names mistrans-lation. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 39333945, Singapore. Association for Com-putational Linguistics. Sarah Seraj, Kate G Blackburn, and James W Pen-nebaker. 2021. Language left behind on social mediaexposes the emotional and cognitive costs of a roman-tic breakup. Proceedings of the National Academy ofSciences, 118(7):e2017154118. Vered Shwartz, Rachel Rudinger, and Oyvind Tafjord.2020. you are grounded!: Latent name artifacts inpre-trained language models. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 68506861,Online. Association for Computational Linguistics. Ian Stewart and Rada Mihalcea. 2024. Whose wifeis it anyway? assessing bias against same-genderrelationships in machine translation.In Proceed-ings of the 5th Workshop on Gender Bias in Natu-ral Language Processing (GeBNLP), pages 365375,Bangkok, Thailand. Association for ComputationalLinguistics. Robee Khyra Mae J. Te, Janica Mae M. Lam, and EthelOng. 2018. Using social media posts as knowledgeresource for generating life stories. In Proceedings ofthe 32nd Pacific Asia Conference on Language, Infor-mation and Computation, Hong Kong. Associationfor Computational Linguistics. Anna Tigunova, Paramita Mirza, Andrew Yates, andGerhard Weikum. 2021. PRIDE: Predicting Rela-tionships in Conversations. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 46364650, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Juan Vsquez, Gemma Bel-Enguix, Scott Thomas An-dersen, and Sergio-Luis Ojeda-Trueba. 2022. Hetero-Corpus: A corpus for heteronormative language de-tection. In Proceedings of the 4th Workshop on Gen-der Bias in Natural Language Processing (GeBNLP),pages 225234, Seattle, Washington. Association forComputational Linguistics. Yixin Wan, George Pu, Jiao Sun, Aparna Garimella,Kai-Wei Chang, and Nanyun Peng. 2023. kellyis a warm person, joseph is a role model: Genderbiases in LLM-generated reference letters. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 37303748, Singapore.Association for Computational Linguistics. Jun Wang, Benjamin Rubinstein, and Trevor Cohn.2022.Measuring and mitigating name biases inneural machine translation. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages25762590, Dublin, Ireland. Association for Compu-tational Linguistics. Robert Wolfe and Aylin Caliskan. 2021. Low frequencynames exhibit bias and overfitting in contextualizinglanguage models. In Proceedings of the 2021 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 518532, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "We present additional information about our exper-imental setup": "ModelsWe use recently introduced two popularlanguage models for testing our hypothesis, namelyLlama (Touvron et al., 2023) (7B, 13B chat), andMistral-7B (Jiang et al., 2023). Each model usesnucleus sampling (Holtzman et al., 2019) with de-fault parameters, a temperature of 0, and a maxi-mum generation length of 512. Each experimentover 327 test instances takes 30mins for Llama2-7B, 1hr for Llama2-13B, and 25mins forMistral-7B. We ran 870 experiments per race (560for Hispanic) for studying gender bias and 1600experiments (400 per race-pair) for racial bias. Computing Evaluation ScoresWe first com-pute precision, recall, F1, and accuracy scores foreach name-pair-replacement and report the averagescores for each name-pair bin, and each race-pairfor studying the influence of gender, and race asso-ciated with names, respectively. Dataset Statistics presents the frequencyof each relationship label along with romantic andnon-romantic categories used for the purpose ofthis study, in the test split of DDRel (Jia et al., 2021)dataset. Out of 327 conversations with different-gender characters in the dataset, 271 do not containexplicit gender information.",
  "PromptsWe provide the prompt template usedin our experiments in": "Parsing Outputs from LLMsWe observe incon-sistencies in the outputs predicted by LLMs despiteclear instructions regarding formatting. We use reg-ular expressions to extract the JSON outputs andthe predictions from them. We consider invalidoutputs (i.e., non-pre-defined class) from LLMs asa separate class (invalid) for evaluation purposesacross all experiments. Logistic Regression for Name EmbeddingsWequantitatively study the amount of gender infor-mation encoded in these embeddings by training alogistic regression model, separately for each race,to classify the gender associated with a name, usingembeddings of 70% of names in a race as the train-ing set and the remaining as the test set. Similarly,we train a logistic regression model to conduct aOne-vs-All\" classification for each race. We con-trol the train and test set in the racial setup to have",
  ": Frequency of relationship types in the test splitof DDRel dataset (Jia et al., 2021)": "a balanced number of positive and negative sam-ples by down-sampling the instances from otherraces (1/3 from each other race). We repeat thelogistic regression training with 5 different randomtrain-test splits. We set the random state of the lo-gistic regression model to 0 and maximum iterationto 1000. In , we report the average resultsacross 5 runs with their standard deviation.",
  "B.2One Name Is Anonymized": "We substitute one name and keep the otheranonymized to analyze the impact of one charac-ters gender on romantic relationship predictionsindependent of the second. We replace one namewith a male, female or a neutral name either pre-serving or swapping the original gender of the non-anonymized name while keeping the other nameanonymized. Male, neutral, and female names be-long to 0 25, 25 75, and 75 100% bins,respectively. We report the recall scores for ro-mantic relationship prediction (same/swapped) fordifferent models in .",
  "C.1First Names Used to Study the Influenceof Gender Pairing": "We first collect names that have frequency over 200and have more than 80% of the population havingthat name identify themselves as a particular race(Asian, Black, Hispanic, and White) from Rosen-man et al. 2023. Then, we partition these namesinto 10 non-linearly segmented bins (shown in Fig-ure 2) based on the percentage of population that has been assigned as female at birth using statis-tics from the Social Security Application dataset(SSA3). We randomly sample 3 names per bin to-taling to 30 names per race4 for performing thereplacements. We consider names belonging to aspectrum of female gender associations to ensurecoverage of gender-neutral names.We list all the names used in this set of experi-ments. We include the percentage of the populationassigned female gender at birth in parentheses. AsianSeung (0.00%),Quoc (0.00%),Dat(0.00%), Nghia (2.30%), Thuan (2.40%), Thien(2.70%), Hoang (6.40%), Sang (6.60%), Jun(9.60%), Sung (13.50%), Jie (17.30%), Wei(21.80%),Hyun (39.00%),Khanh (41.90%),Wen (44.60%), Hien (51.70%), An (54.80%), Ji(61.40%), In (80.80%), Diem (88.60%), Quyen(88.90%), Ling (91.30%), Xiao (91.50%), Ngoc(92.40%), Su (95.40%), Hanh (95.60%), Vy(97.00%), Eun (98.30%), Trinh (100.00%), Huong(100.00%) BlackDeontae(0.00%),Antwon(0.10%),Javonte (1.00%), Dejon (2.90%), Jamell (3.40%),Dijon(4.60%),Dashawn(5.80%),Deshon(6.20%), Pernell (8.30%), Rashawn (10.10%),Torrance (13.20%), Semaj (22.60%), Demetris(25.60%), Kamari (33.60%), Amari (42.00%),Shamari (56.10%), Kenyatta (57.10%), Ivory(59.30%), Chaka (76.20%), Ashante (89.40%),",
  "Unique(89.90%),Kenya(92.20%),Nikia(93.80%), Akia (94.30%), Kenyetta (95.50%),Shante (96.40%), Shaunta (97.00%), Laquandra(100.00%), Lakesia (100.00%), Daija (100.00%)": "HispanicNestor (0.00%), Fidel (0.00%), Raul(0.60%), Leonides (2.70%), Yamil (4.50%), Reyes(10.80%),Cruz (13.10%),Neftali (14.90%),Noris (38.10%), Nieves (62.40%), Guadalupe(72.60%), Ivis (75.00%), Monserrate (78.20%),Ibis (82.60%), Johanny (89.40%), Elba (91.50%),Matilde (93.40%),Rocio (96.90%),Lucero(97.30%), Cielo (97.50%), Lucila (100.00%), Zu-leyka (100.00%), Yaquelin (100.00%) WhiteZoltan (0.00%),Leif (0.10%),Jack(0.40%), Ryder (3.30%), Carmine (3.40%), Haden(4.10%), Tate (5.30%), Dickie (5.50%), Logan(7.40%), Parker (17.50%), Sawyer (20.90%), Hay-den (22.50%), Dakota (29.70%), Britt (38.30%),Harley (41.70%), Campbell (53.90%), Barrie(56.10%), Peyton (61.90%), Kelley (88.00%),Jodie (88.20%), Leigh (88.70%), Clare (90.90%),Rylee (92.20%), Meredith (94.70%), Baylee(97.00%), Lacey (97.30%), Ardith (97.70%),Kristi (99.80%), Galina (100.00%), Margarete(100.00%)",
  "C.2First Names Used to Study the Influenceof Intra/Inter-racial Pairing": "By referencing Rosenman et al. 2023 and the SSAdataset again, we collect another set of both race-and gender-indicative first names with a minimumfrequency of 200, applying a threshold of 90% forthe percentage of the population assigned either fe-male or male at birth. For race threshold, we set itto be 90% for Asian, Black, and Hispanic, and 70%for White. Although we choose a lower thresholdfor White to account for the phenomenon of nameAnglicization (Zhao and Biernat, 2019), we stillobtain empirical results that strongly indicate thesenames are represented differently from names as-sociated with other races/ethnicities. In total, weobtain 80 names that are evenly distributed amongfour races/ethnicities and two genders. We replacename-pairs while preserving the gender associatedwith the names in the original dialogue.",
  "DAdditional Results": "We report the results for Llama2-13B (Figures 6and 9) and Mistral-7B (Figures 7 and 10). We alsoreport the F1 and accuracy scores for Llama2-7B,for completeness, in and 8. We observesimilar trends as Llama2-7B discussed in the mainbody of the paper. 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.840.820.820.820.820.810.810.810.80 0.830.840.830.830.810.840.810.810.820.81 0.820.820.810.810.800.810.800.800.800.80 0.820.820.820.820.810.810.800.800.810.80 0.810.820.810.810.810.810.800.800.800.80 0.820.820.810.810.800.820.800.800.810.80 0.800.800.790.790.800.800.790.790.800.79 0.800.800.790.790.790.790.790.800.800.79 0.810.810.800.800.800.810.800.800.800.80 0.790.790.790.790.790.790.790.790.790.79 Asian (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.830.820.820.800.800.800.800.800.80 0.820.820.810.810.800.800.800.790.800.80 0.830.810.810.820.800.800.800.800.800.80 0.820.820.810.810.800.800.800.790.790.79 0.810.810.800.800.810.800.800.790.790.79 0.800.800.790.800.810.790.790.790.790.79 0.800.800.800.800.800.800.800.790.790.79 0.800.790.790.790.800.790.790.790.790.79 0.800.790.800.790.790.790.790.780.790.79 0.800.790.790.790.790.790.790.790.790.79 Black (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.820.810.810.800.810.800.800.80 0.830.820.810.800.790.810.800.790.80 0.810.810.800.800.790.800.800.790.79 0.790.790.790.790.790.790.790.79 0.800.790.790.790.790.790.790.790.79 0.800.810.800.800.800.800.800.790.80 0.800.790.790.790.780.790.790.790.79 0.790.800.790.790.790.790.790.790.79 0.790.790.790.790.790.790.790.790.79 Hispanic (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.830.810.820.800.810.790.790.800.79 0.810.800.790.790.790.800.790.790.790.79 0.820.810.810.810.790.810.790.790.800.79 0.810.810.800.800.790.810.790.790.790.80 0.800.800.800.790.800.800.790.800.790.79 0.810.800.810.800.800.800.800.790.800.79 0.790.790.790.790.780.790.790.790.800.79 0.800.800.790.790.790.790.790.790.790.79 0.800.800.790.790.790.790.790.790.790.80 0.800.790.790.790.790.790.790.800.790.79 White (Precision) 0.79 0.80 0.81 0.82 0.83 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.670.660.690.680.690.690.740.730.710.76 0.670.640.680.680.680.680.720.720.710.74 0.690.680.710.690.710.700.750.740.720.76 0.680.670.680.690.700.680.730.720.710.75 0.700.690.710.700.720.700.740.740.720.74 0.690.670.690.690.690.690.720.710.700.73 0.720.710.730.720.730.710.730.720.720.73 0.720.710.730.720.730.710.730.730.710.73 0.710.690.710.700.710.700.720.700.700.72 0.740.720.740.730.730.720.730.720.710.73 Asian (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.690.700.700.690.720.750.710.760.800.79 0.690.690.690.690.720.740.710.750.790.78 0.710.700.700.700.720.760.720.760.800.78 0.700.700.690.690.710.740.710.740.780.76 0.730.730.720.710.690.710.710.730.760.75 0.750.740.740.730.720.770.720.730.770.75 0.720.710.720.700.710.730.700.720.740.73 0.770.740.750.730.730.730.710.710.730.70 0.800.780.780.770.760.760.730.720.730.71 0.790.770.770.750.730.750.720.690.720.69 Black (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.690.690.700.750.780.740.790.820.82 0.680.690.700.740.750.710.780.800.80 0.700.710.720.750.770.730.790.810.81 0.720.720.720.720.720.750.780.77 0.770.750.760.720.730.740.740.770.75 0.720.710.720.730.740.740.760.790.79 0.780.770.770.750.730.750.790.770.75 0.810.810.800.780.770.790.780.810.78 0.810.800.800.760.740.780.760.780.77 Hispanic (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.640.710.660.690.760.700.770.810.800.83 0.700.740.700.700.780.720.780.820.800.83 0.670.720.680.690.760.700.760.810.780.81 0.700.710.690.710.770.710.770.820.780.82 0.760.780.760.770.760.730.740.770.740.77 0.690.720.690.700.730.690.720.770.740.78 0.770.780.750.760.730.710.720.740.700.73 0.820.820.800.810.770.760.740.770.720.73 0.800.800.770.780.740.730.700.710.680.70 0.830.830.800.810.770.770.730.730.700.69 White (F1) 0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.560.550.570.570.580.570.620.610.590.64 0.560.530.560.570.560.570.600.600.590.62 0.580.560.590.570.590.590.620.620.600.63 0.570.560.570.580.580.570.600.600.590.62 0.580.570.590.580.600.580.620.610.600.62 0.570.550.570.570.570.580.600.590.580.60 0.600.580.600.590.600.590.610.590.590.60 0.600.590.610.590.600.580.600.600.590.60 0.590.570.590.580.590.580.590.580.580.59 0.620.590.620.610.600.590.610.590.590.60 Asian (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.580.580.580.580.600.620.590.630.680.67 0.580.570.570.570.590.620.580.620.670.65 0.590.580.580.580.600.630.590.630.670.65 0.580.580.570.580.590.610.580.610.650.63 0.610.600.600.590.570.590.590.600.640.62 0.630.610.610.600.600.640.600.600.640.61 0.590.590.600.580.590.610.580.590.610.60 0.640.610.620.600.600.610.590.580.600.57 0.680.650.660.640.630.640.600.590.600.58 0.660.640.640.620.600.620.580.560.590.56 Black (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.580.580.580.630.650.620.680.700.70 0.570.580.590.610.630.590.660.680.68 0.580.590.600.620.640.610.670.690.69 0.590.590.590.590.590.620.650.65 0.640.620.630.590.600.610.620.640.63 0.590.590.590.610.620.610.640.670.67 0.660.640.650.620.600.630.670.650.63 0.690.690.680.650.640.670.650.690.66 0.690.680.680.640.610.660.630.660.64 Hispanic (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.540.600.550.580.640.580.650.700.670.71 0.580.610.570.580.650.600.660.700.670.71 0.560.600.560.570.630.580.630.690.660.69 0.580.590.570.580.640.590.650.700.660.71 0.640.660.640.640.640.610.620.650.610.65 0.580.600.570.580.610.570.600.640.610.66 0.640.660.620.630.590.590.590.620.580.61 0.700.700.680.690.640.630.610.640.590.61 0.680.680.640.660.610.600.580.590.560.57 0.720.710.680.690.640.640.600.610.570.57 White (Accuracy) 0.550 0.575 0.600 0.625 0.650 0.675 0.700",
  ": Precision, F1-score and Accuracy plots for romantic predictions from Llama2-7B model": "0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.880.880.870.870.870.860.830.850.850.84 0.880.850.850.870.870.860.840.840.850.84 0.860.860.840.860.860.850.830.830.840.83 0.870.880.860.870.880.870.850.860.860.85 0.880.880.860.880.860.870.850.850.860.85 0.860.860.850.860.860.860.840.840.850.84 0.840.850.840.850.850.850.850.850.860.85 0.850.860.850.860.870.870.860.860.860.85 0.860.870.850.870.870.870.850.850.860.85 0.850.850.840.860.870.860.860.850.860.85 Asian (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.880.880.900.890.860.860.850.840.840.84 0.870.850.880.860.850.860.840.830.830.83 0.880.870.900.880.850.860.850.830.830.83 0.870.860.890.870.850.860.840.830.840.84 0.860.850.870.850.840.850.850.830.840.84 0.860.840.860.850.850.850.850.840.860.85 0.850.850.860.860.850.850.850.840.860.85 0.840.830.850.840.830.840.840.830.840.84 0.850.830.840.840.840.860.860.860.870.87 0.840.830.850.840.840.850.860.850.870.86 Black (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.910.890.870.860.850.850.850.830.83 0.890.890.860.850.840.850.830.830.83 0.870.860.840.840.840.840.830.820.83 0.850.840.840.850.840.860.840.84 0.860.850.850.870.860.860.860.850.84 0.850.850.850.860.850.850.850.830.84 0.850.850.830.850.860.840.840.840.85 0.830.840.820.850.850.840.860.830.84 0.830.830.830.840.850.840.860.840.85 Hispanic (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.900.890.880.880.860.870.840.840.850.83 0.900.870.860.860.840.860.840.830.850.82 0.890.880.860.870.830.850.840.820.840.82 0.890.860.860.850.830.840.830.820.850.81 0.870.840.840.840.830.850.850.840.840.84 0.890.870.870.870.860.870.860.840.840.84 0.860.840.830.840.840.860.860.860.860.85 0.830.820.810.820.840.850.870.860.850.85 0.850.850.850.840.840.850.860.860.850.85 0.830.820.810.820.830.830.840.850.840.84 White (Precision) 0.82 0.84 0.86 0.88 0.90 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.280.290.330.310.290.320.350.340.340.35 0.290.290.330.310.300.320.340.340.330.34 0.310.310.350.330.320.340.380.350.360.37 0.300.290.330.320.300.310.340.330.330.34 0.280.290.320.300.290.310.330.320.320.33 0.300.300.320.300.300.300.330.320.310.32 0.330.310.360.330.310.310.330.320.320.32 0.330.310.350.320.310.320.330.320.320.32 0.310.310.340.320.300.310.330.320.330.32 0.330.310.360.330.310.320.320.320.320.32 Asian (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.310.310.290.310.330.340.340.350.370.36 0.310.310.300.310.340.340.340.360.370.37 0.290.290.270.280.320.320.330.340.350.36 0.290.290.280.280.310.320.320.330.340.34 0.310.320.310.310.320.310.320.330.320.33 0.310.310.310.300.310.320.300.310.310.31 0.320.320.320.310.310.300.300.300.300.30 0.330.330.330.320.320.310.310.310.310.30 0.340.330.340.320.310.300.300.290.280.28 0.340.340.340.330.320.300.300.290.280.28 Black (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.230.260.290.330.340.330.370.420.40 0.250.280.310.330.340.320.360.390.38 0.280.310.330.350.350.340.380.420.42 0.320.330.340.300.310.300.330.32 0.320.320.330.300.290.300.280.330.31 0.300.310.330.310.300.320.310.350.34 0.340.330.350.290.280.300.290.310.28 0.380.380.400.330.330.340.320.360.33 0.370.360.390.310.300.320.280.320.29 Hispanic (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.260.280.290.310.360.310.370.400.370.42 0.270.290.300.320.370.320.370.400.380.42 0.270.290.290.310.350.300.350.390.350.40 0.290.310.310.330.350.320.350.400.370.39 0.330.340.340.340.340.300.320.340.330.34 0.280.290.280.300.310.280.300.330.310.33 0.330.330.320.330.310.290.310.310.300.31 0.360.370.360.370.320.310.300.290.300.26 0.340.350.340.350.320.300.300.310.320.30 0.390.380.370.370.310.320.290.260.290.23 White (Recall) 0.250 0.275 0.300 0.325 0.350 0.375 0.400 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.430.430.480.460.440.470.490.490.480.50 0.440.440.470.460.440.460.480.480.480.49 0.450.460.490.480.460.480.520.500.500.51 0.440.440.470.460.450.460.490.480.480.48 0.430.430.460.450.440.450.480.470.470.47 0.440.440.470.450.440.450.470.460.460.46 0.470.460.500.470.450.460.480.460.460.46 0.470.460.490.470.460.470.470.470.470.46 0.450.450.490.470.440.460.470.460.470.46 0.470.460.500.470.450.460.470.460.470.46 Asian (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.460.450.440.460.480.480.480.500.510.50 0.450.450.440.450.490.480.480.500.510.51 0.430.430.420.420.460.470.470.490.490.50 0.440.430.420.430.460.460.460.470.480.49 0.460.460.450.450.470.460.460.470.470.48 0.460.450.450.440.450.460.450.460.460.45 0.460.460.460.450.460.450.440.440.440.44 0.470.470.470.460.470.450.450.450.450.44 0.480.480.480.470.450.440.440.430.430.42 0.480.480.490.470.460.450.440.440.430.42 Black (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.370.410.430.480.490.470.510.550.54 0.390.420.450.480.480.460.500.530.52 0.420.450.470.490.490.490.520.550.55 0.460.470.480.440.450.440.480.47 0.470.470.470.450.440.450.420.470.45 0.440.450.470.450.450.460.450.490.48 0.480.470.490.430.420.440.440.450.43 0.520.520.530.480.470.490.460.510.48 0.510.500.530.460.450.470.420.460.43 Hispanic (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.410.420.430.460.510.460.510.540.510.56 0.420.430.440.460.510.460.510.540.520.56 0.420.430.430.460.490.450.490.530.490.54 0.430.450.450.470.490.460.490.530.500.53 0.470.480.480.490.480.450.470.480.470.48 0.420.430.420.450.450.420.450.470.450.47 0.470.470.460.480.450.440.450.450.450.45 0.510.510.500.510.460.460.450.440.440.40 0.480.490.480.490.460.450.450.450.460.44 0.530.520.500.510.450.460.430.400.430.36 White (F1) 0.375 0.400 0.425 0.450 0.475 0.500 0.525 0.550 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.410.410.430.420.410.430.440.440.430.44 0.410.400.430.420.410.420.430.430.430.43 0.420.420.440.430.420.430.450.440.440.44 0.410.410.430.420.420.420.440.430.440.43 0.410.410.420.420.410.420.430.430.430.43 0.410.410.430.410.410.410.420.420.420.42 0.420.420.440.430.410.420.430.420.420.42 0.430.420.440.430.420.430.430.430.430.42 0.420.420.440.430.410.420.430.420.430.42 0.430.420.440.430.420.420.420.420.430.42 Asian (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.430.420.420.420.430.430.430.440.440.44 0.420.410.410.420.430.440.430.440.440.44 0.410.400.400.400.420.420.430.430.430.43 0.410.400.400.400.420.420.420.420.430.43 0.420.420.420.410.420.420.420.420.420.43 0.420.410.420.410.410.420.410.410.420.41 0.420.420.420.420.420.410.410.410.410.41 0.420.420.420.420.420.410.410.410.410.41 0.430.420.430.420.410.410.410.400.400.40 0.430.430.430.420.420.410.410.410.400.40 Black (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.380.400.410.430.430.430.450.480.47 0.390.410.420.430.430.420.440.460.45 0.400.420.420.430.430.430.450.470.47 0.420.420.430.400.410.410.430.42 0.430.420.430.410.410.410.400.430.41 0.410.410.420.420.410.420.410.430.43 0.430.430.430.400.400.400.400.410.40 0.450.450.460.430.420.430.420.440.43 0.450.440.460.410.410.420.400.410.40 Hispanic (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.400.400.410.420.450.420.450.470.450.48 0.400.410.410.420.450.420.440.460.460.47 0.400.410.400.420.430.410.440.460.440.46 0.410.410.420.420.430.420.430.450.440.45 0.430.430.430.430.420.410.420.430.420.43 0.400.410.400.420.410.400.410.420.410.42 0.430.420.420.430.410.410.420.410.410.41 0.440.440.430.440.420.420.410.410.410.39 0.430.440.430.430.410.410.410.410.420.41 0.450.450.440.440.410.410.400.380.400.36 White (Accuracy) 0.38 0.40 0.42 0.44 0.46",
  ": Precision, Recall, F1-score and Accuracy plots for romantic predictions from Llama2-13B model": "0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.840.840.840.850.840.830.830.850.840.84 0.850.840.850.840.840.830.830.840.830.84 0.850.840.840.840.840.840.830.850.840.84 0.840.850.830.860.840.830.820.840.830.84 0.850.840.840.840.840.830.820.850.830.83 0.850.840.830.840.840.830.820.840.820.83 0.860.840.840.840.840.830.830.830.830.83 0.840.830.830.840.830.820.830.830.820.83 0.850.840.830.840.830.830.830.830.830.83 0.850.840.840.840.840.830.830.830.830.83 Asian (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.840.840.840.840.840.840.830.840.830.84 0.840.840.840.840.840.830.840.830.820.83 0.840.840.840.830.830.830.820.830.830.83 0.830.840.830.820.830.830.830.830.830.83 0.840.840.840.840.830.830.840.830.820.82 0.840.840.840.840.820.830.830.820.820.82 0.840.840.840.850.840.840.840.830.820.83 0.840.840.840.840.830.830.820.820.820.81 0.840.830.840.850.820.820.840.820.810.82 0.830.820.840.840.830.830.840.830.830.82 Black (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.840.840.830.840.840.830.830.83 0.840.840.840.840.840.840.830.830.83 0.840.840.840.830.830.830.820.830.82 0.840.840.840.810.810.810.820.78 0.860.840.850.830.830.820.820.830.81 0.850.840.830.840.830.820.810.830.82 0.840.830.830.810.830.830.810.830.81 0.850.850.840.840.840.840.830.830.82 0.830.840.820.820.830.830.820.830.82 Hispanic (Precision) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.830.830.830.840.830.820.820.820.820.83 0.830.840.830.830.830.820.820.830.820.83 0.840.840.840.840.830.830.830.830.840.84 0.850.850.850.840.840.840.830.820.830.84 0.860.840.840.840.830.840.820.830.830.83 0.850.840.850.850.840.830.830.830.830.84 0.840.850.840.840.830.840.830.830.830.82 0.840.840.840.830.830.830.830.820.830.82 0.840.820.830.830.820.840.820.810.830.82 0.830.820.830.820.820.820.820.810.820.80 White (Precision) 0.80 0.81 0.82 0.83 0.84 0.85 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.180.210.200.190.200.210.230.210.210.24 0.200.220.210.200.210.210.220.210.210.23 0.210.220.220.210.220.220.240.220.230.24 0.190.200.210.190.200.200.210.200.210.22 0.210.220.220.210.220.220.220.220.210.23 0.200.210.220.210.210.210.220.210.210.22 0.230.220.230.210.220.210.220.210.210.22 0.210.200.210.200.200.200.200.200.200.20 0.220.220.220.210.210.220.210.200.210.21 0.240.230.240.220.230.220.220.210.220.22 Asian (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.190.190.200.190.210.230.230.260.270.28 0.190.200.200.190.210.230.220.260.270.27 0.200.200.200.190.210.230.220.250.260.26 0.180.190.190.170.190.200.200.220.220.23 0.200.200.200.190.200.210.200.210.210.21 0.220.230.220.200.210.220.200.210.220.21 0.220.210.210.190.200.200.190.200.190.19 0.240.240.240.210.200.210.200.190.190.18 0.270.260.260.230.210.210.190.190.180.17 0.280.270.270.220.220.220.200.180.180.16 Black (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.160.180.200.210.240.240.270.250.33 0.170.190.210.220.240.260.280.270.35 0.200.210.210.210.220.240.230.240.28 0.220.220.210.200.210.200.210.22 0.230.230.220.200.200.200.190.210.21 0.240.250.230.210.210.220.190.210.22 0.260.280.240.210.190.200.160.210.18 0.240.260.240.210.220.220.210.230.23 0.310.350.270.230.220.220.180.220.19 Hispanic (Recall) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.160.200.170.190.240.190.240.240.240.29 0.190.210.200.210.240.210.240.240.240.26 0.180.200.180.200.250.200.240.250.250.28 0.190.210.200.220.240.200.230.230.230.25 0.230.230.240.240.230.220.210.200.220.22 0.190.210.190.210.220.190.210.210.210.24 0.240.240.240.230.210.210.180.170.180.17 0.250.240.260.240.210.210.160.150.170.15 0.240.240.240.230.210.210.180.170.190.18 0.270.260.270.250.210.230.160.140.170.13 White (Recall) 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.290.340.330.310.330.330.350.340.340.37 0.330.340.340.320.330.330.350.330.330.36 0.340.350.350.330.350.350.370.350.360.37 0.310.320.330.320.330.330.340.320.330.35 0.340.350.350.340.340.340.350.350.340.36 0.330.340.340.330.340.340.340.330.330.35 0.360.350.360.330.350.340.350.330.340.34 0.340.330.340.320.330.320.320.320.320.33 0.340.350.350.330.340.340.330.330.330.34 0.370.360.380.340.360.350.350.330.350.35 Asian (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.310.320.320.310.340.360.350.400.400.42 0.320.320.320.310.330.360.350.390.400.41 0.320.320.320.300.340.360.340.380.400.40 0.300.310.300.270.310.320.320.340.350.36 0.330.330.330.300.330.340.330.340.330.34 0.350.360.350.330.330.350.330.340.340.34 0.350.340.340.300.330.330.310.320.310.31 0.380.370.370.340.330.340.320.310.310.29 0.400.390.400.360.330.340.320.300.300.28 0.420.410.410.350.340.350.320.300.290.27 Black (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.270.290.320.340.370.370.400.390.47 0.290.310.340.350.370.400.420.400.49 0.330.330.340.330.350.370.360.370.41 0.340.340.330.320.330.320.330.34 0.360.360.350.320.320.320.310.340.33 0.370.390.360.330.330.350.310.340.34 0.400.420.370.330.310.320.260.330.29 0.380.400.370.340.350.350.330.360.35 0.450.490.410.360.340.340.290.350.31 Hispanic (F1) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.270.320.290.310.370.310.380.370.370.43 0.320.340.320.330.370.330.370.370.370.40 0.290.320.290.320.390.320.370.380.380.42 0.310.340.320.350.370.330.360.360.360.39 0.370.370.370.370.360.350.330.330.340.34 0.300.330.320.340.350.310.340.330.340.37 0.380.370.370.360.330.330.300.280.300.28 0.380.370.390.370.330.340.270.250.280.25 0.370.370.380.360.340.340.300.290.310.29 0.410.390.410.380.340.360.270.240.280.22 White (F1) 0.225 0.250 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.330.360.350.340.350.350.360.350.350.37 0.340.350.350.340.350.350.360.350.350.36 0.350.360.360.350.360.360.360.360.360.37 0.340.350.350.340.350.340.350.350.350.36 0.350.360.360.350.350.350.350.360.350.36 0.350.350.350.350.350.350.350.350.350.36 0.370.360.360.350.360.350.350.350.350.35 0.350.350.350.340.340.340.340.340.340.34 0.360.360.360.350.350.350.350.340.350.35 0.370.370.370.360.360.360.360.350.350.35 Asian (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.340.340.340.340.350.360.360.380.380.39 0.340.350.340.340.350.360.360.380.380.38 0.340.350.340.330.350.360.350.370.380.38 0.330.340.330.320.330.340.340.350.350.36 0.350.350.350.330.340.350.350.350.350.35 0.360.360.360.350.350.350.340.350.350.35 0.360.350.350.340.350.350.340.340.330.34 0.370.370.370.350.340.350.340.340.330.32 0.390.380.380.360.350.350.340.330.330.32 0.390.380.390.360.350.360.340.330.330.31 Black (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.320.330.340.350.370.370.380.380.42 0.330.340.350.360.370.390.390.380.44 0.350.350.350.350.360.360.360.360.39 0.360.350.350.340.340.340.340.34 0.370.360.360.340.340.340.330.350.34 0.370.380.360.350.340.350.330.350.35 0.380.390.360.340.340.340.310.350.32 0.370.390.370.350.360.360.350.360.35 0.410.440.380.360.350.350.320.360.33 Hispanic (Accuracy) 0-2 2-5 5-10 10-25 25-50 50-75 75-90 90-95 95-98 98-100 % Female 0.320.340.330.340.360.340.370.360.360.40 0.340.350.340.350.360.340.360.360.360.38 0.330.350.330.340.380.340.370.370.370.40 0.340.350.350.360.370.350.360.360.360.38 0.370.370.370.370.360.360.350.340.350.35 0.340.350.340.350.360.340.350.350.350.37 0.370.370.370.360.350.350.330.320.330.32 0.370.370.380.370.350.350.320.310.320.31 0.370.360.370.360.350.350.330.320.340.33 0.390.380.390.370.350.360.320.300.320.29 White (Accuracy) 0.30 0.32 0.34 0.36 0.38",
  "Accuracy": "0.8400 0.8425 0.8450 0.8475 0.8500 0.8525 0.8550 0.26 0.28 0.30 0.32 0.34 0.40 0.42 0.44 0.46 0.48 0.40 0.41 0.42 0.43 0.44 0.45 : Precision, Recall, F1, and Accuracy of predicting romantic relationships from Mistral-7B for subsetof the dataset where characters have different genders and are replaced with names associated with differentraces/ethnicities."
}