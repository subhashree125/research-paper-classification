{
  "Abstract": "Perceiving and understanding non-speechsounds and non-verbal speech is essential tomaking decisions that help us interact withour surroundings. In this paper, we proposeGAMA, a novel General-purpose Large Audio-Language Model (LALM) with AdvancedAudio Understanding and Complex Reason-ing Abilities. We build GAMA by integratingan LLM with multiple types of audio repre-sentations, including features from a customAudio Q-Former, a multi-layer aggregator thataggregates features from multiple layers of anaudio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augmentsit with audio understanding capabilities. Next,we propose CompA-R (Instruction-Tuning forComplex Audio Reasoning), a syntheticallygenerated instruction-tuning (IT) dataset withinstructions that require the model to performcomplex reasoning on the input audio. Weinstruction-tune GAMA with CompA-R to en-dow it with complex reasoning abilities, wherewe further add a soft prompt as input withhigh-level semantic evidence by leveragingevent tags of the input audio.Finally, wealso propose CompA-R-test, a human-labeledevaluation dataset for evaluating the capabili-ties of LALMs on open-ended audio question-answering that requires complex reasoning.Through automated and expert human evalu-ations, we show that GAMA outperforms allother LALMs in literature on diverse audio un-derstanding tasks by margins of 1%-84% anddemonstrates state-of-the-art performance ondeductive reasoning and hallucination evalu-ation benchmarks. Further, GAMA IT-ed onCompA-R proves to be superior in its complexreasoning capabilities.",
  "It is not possible to infer whatthe man might be announcingwithout additional context": "The train is approaching a station, indicated bythe increasing volume of the horn and the bellringing, suggesting it's about to stop or arrive atthe station for a passenger pickup or drop off.The wind noise suggests an open outdoorenvironment near the tracks. A mans voice canbe heard in the background, making anannouncement.",
  "Existing LALMs": ": Comparison of existing LALMs (LTU by Gonget al. (2024) here) and GAMA. With improved audio under-standing abilities (via diverse audio feature integration) andtraining on our proposed CompA-R, GAMA can provide moredetailed captions of input audio and is also able to answerquestions regarding it that demand complex reasoning. spoken language understanding tasks, like auto-matic speech recognition, have had a long historyof benefiting from language comprehension with(L)LMs (Watanabe et al., 2018; Hu et al., 2024), theability to improve the perception and understand-ing of non-speech sounds and non-verbal speechthrough language has been less explored (fromhereon we refer to these kinds of audios or sound asaudio in the paper). Beyond visual and languageperception, the ability to understand audio is unar-guably important and necessary for autonomousagents to interact with the world.ContrastiveLanguageAudioPre-training(CLAP) (Elizalde et al., 2023a) was one of thefirst Audio-Language Models (ALM) to improveaudio understanding through a language interface.Following this, several attempts have been made toimprove CLAP and its reasoning abilities (Ghoshet al., 2024b).On the other hand, Deshmukhet al. propose Pengi, a pre-trained decoder-onlyLLM coupled with an audio-encoder, that cansolve all kinds of audio tasks by framing them as open-ended text-generation tasks.Similarly,Large Audio Language Models (LALMs) likeLTU (Gong et al., 2024) and SALMONN (Tanget al., 2024) follow a similar architecture andattempt to solve audio tasks by empowering themodel with instruction following capabilities (Weiet al., 2022). Specifically, all audio tasks are firstframed in instruction-response pairs. The modelis then fine-tuned on these pairs to learn audioreasoning and, thereby, instruction following.As an emergent ability, these models also showremarkable capabilities in open-ended questionanswering by reasoning over the input audio.However, two significant problems still persist: (1)All these models employ simple connection mod-ules between the audio encoder and the languagedecoder to enable the latter with audio under-standing capabilities. This hinders comprehensivemultimodal connection and alignment, therebyincreasing the risk of hallucinations and leadingto suboptimal performance (Liu et al., 2023a).(2) Complex reasoning with LALMs is stillunder-explored. While these models excel at audioevent detection (in various forms like captioning,event classification, etc.) and information-seekingquestions (e.g., close-ended audio questions likeHow many birds are squawking?), they fail toprovide a faithful response for questions involvingcomplex reasoning like Identifying the context oflaughter and its relationship with the automotivesounds in the recording. Draw a conclusion on thepossible scenario occurring.. We define complexreasoning for LALMs in .2 and showexamples in and .Main Contributions. Our primary contributionsare as follows: A Novel LALM. We introduce GAMA, anLALM with advanced audio understandingand complex reasoning abilities. To improveaudio perception and understanding abilities,we propose integrating an LLM with multi-ple types of audio features that encode di-verse aspects of information about the inputaudio. Specifically, we couple the output fea-tures from an Audio Q-Former and an AudioSpectrogram Transformer (AST) (Gong et al.,2021), where the AST is further equipped withan aggregation module. While the Audio Q-Former possesses impressive semantic gener-alization capabilities (Li et al., 2023), the ASTpossesses strong knowledge of surface-level audio properties. Additionally, inspired by thefact that different layers in audio models learnaudio information at different scales (Singlaet al., 2022), the aggregation module aggre-gates the features from multiple layers ofAST, which helps encode diverse knowledge.Both representations are passed through MLPlayers that connect these features into theword embedding space before adding themas the prefix. As a result, GAMA possessesimproved audio understanding capabilities bymoving away from the simple coupling of au-dio encoders and linear layers commonly em-ployed as connection modules to align theaudio and textual modalities, which generallysuffer from comprehensive multimodal align-ment (Liu et al., 2023a). GAMA is first fine-tuned on a large-scale audio-language corpus,and the resulting model outperforms all othermodels on standard audio and music under-standing benchmarks. A Novel Instruction Tuning Dataset. To en-dow an LALM with complex reasoning abili-ties, we propose CompA-R, a dataset synthet-ically generated with multi-aspect informa-tion and human-written in-context examples.Specifically, we prompt GPT to synthesize aninstruction-response pair by guiding it withvarious metadata related to the audio. A Novel Evaluation Dataset. To evaluate anLALMs complex reasoning abilities, we de-velop CompA-R-test, a human-labeled bench-mark. Specifically, CompA-R-test evaluatesan LALM on open-ended AQA that demandscomplex reasoning over the audio. GAMA-IT(GAMA fine-tuned on CompA-R) shows sig-nificant improvements on CompA-R-test overall other baselines from literature.",
  "Related Work": "Large Multi-Modal and Audio-Language Mod-els.Prior to the exploration of LLMs as effi-cient reasoners, encoder-based multi-modal lan-guage models, trained to learn a shared space be-tween language and other modalities, have showngreat promise. For example, CLAP, inspired byCLIP (Radford et al., 2021) in vision, showed state-of-the-art performance on audio-language taskslike retrieval, zero-shot classification, etc.",
  "Finetune": ": Illustration of GAMA. The GAMA architecture involves integrating various audio features with a text-only (pre-trained)LLM. The audio features are added as a prefix to the text instruction (by the user), and the LLM responds using a text output.We feed GAMA with 3 different types of audio features: 1 The Multi-Layer Aggregator that takes as input features from thelast and multiple other layers of the AST and calculates cross-attention to output a feature that encodes diverse surface featuresfrom audio. 2 The pre-trained Audio Q-Former takes as input the last layer features of AST and outputs a feature that encodesthe audio into a semantically rich and generalized space. 3 After fine-tuning, we instruction-tune this model on our proposedCompA-R and further feed the model with a soft prompt with audio tags from AST (with an added classification head). Thisadditional information serves as high-level semantic knowledge to improve complex reasoning abilities. LLMs pre-trained at an incredible scale withthe next token prediction objective implicitly com-press world knowledge in their parameters (Zhaoet al., 2023). These models learn general-purposerepresentations, which can then be aligned withthe desired response characteristics (Zhang et al.,2023). Instruction Tuning (IT), the process of fine-tuning an LLM with instruction-response pairs, hasproved to be one of the most popular forms ofalignment. Recent work shows that LLMs can alsobe instruction-tuned for multi-modal alignment.LLaVa (Liu et al., 2024), a pioneering work onmulti-modal vision-language alignment, showedthat fine-tuning an LLM on visual instruction-response pairs with additional vision features asprefix can endow the model with visual reasoningand understanding abilities. Several works follow-ing LLaVa improve aspects of LVLMs and haveachieved impressive performance on several vision-language tasks (Zhang et al., 2024). On the otherhand, LALMs like LTU and SALMONN showedimpressive performance on several audio-languagetasks by reasoning over the audio. Though thesemodels extensively evaluate several closed- andopen-ended tasks, their ability to perform complexreasoning is largely under-explored. Instruction Tuning and Complex Reasoning. IT-based alignment has also shown significant im-provements for LLMs on Natural Language Un-derstanding tasks, unlocking impressive capabil-ities (Bubeck et al., 2023), suggesting that fine-tuning is key to building and improving LLM-based agents. Very recently,(Xu et al., 2024)and (Cui and Wang, 2024) show that well-curatedIT data can improve various reasoning capabilitiesin LLMs, like logical, mathematical, complex rea-soning, etc. More specifically, IT teaches LLMsbetter and more effective methods to reason abouta problem, presented in the input instruction (likestep-by-step reasoning (Kojima et al., 2022)).",
  "illustrates the architecture of GAMA. GAMAbuilds on the same base architecture proposed in": "prior works (Gong et al., 2024) but introduces sev-eral novel components for improving audio percep-tion. More specifically, we feed the pre-trainedLLM with features from multiple audio encoders,including a pre-trained Audio-Q-Former and a pre-trained AST that encode diverse audio knowledge.Additionally, unlike prior work, we do not just usethe last layers of the AST but couple it with a multi-layer aggregator that takes features from multiplelayers as input and outputs a feature that is aware ofvarious low-level and high-level properties of theinput audio. Finally, to endow the model with effec-tive complex reasoning abilities, we employ ASTagain to extract high-level semantic knowledge, i.e.,audio event tags, as supplementary information.",
  "Audio Spectrogram Transformer (AST)": "Audio Spectrogram Transformer (AST), was oneof the first attempts to model audio signals witha pure Transformer network. We employ an ASTmodel fine-tuned on the AudioSet dataset. ASThas been employed as an audio encoder and a fea-ture extractor in a wealth of prior works due to itshigh informativeness (Gong et al., 2023, 2024). Toextract the last-layer features, we drop the audioclassification head and employ it only for eventclassification for soft prompts.",
  "Audio Q-Former": "Motivation. Our primary goal is to integrate GAMAwith an audio encoder that possesses strong se-mantic generalization capabilities for any input au-dio. Prior work has extensively explored CLAP-style training for learning audio-language encoders.However, other methods and architectures haverarely been explored. As a more powerful alter-native, we explore the Q-Former architecture pro-posed by (Li et al., 2023).Architecture. The architecture of our Audio Q-Former is based on the Querying Transformer pro-posed in Li et al. (2023), which is initialized fromBERT (Devlin et al., 2018) and has Q queryingtokens. We employ AST as the audio encoder (inplace of the ViT-based vision encoder) and keepthe rest of the architecture the same. Similar tothe original implementation, we train the model intwo stages. For the first stage, we solve three tasks,namely the Audio-Text Matching loss, the Audio-Grounded Text Generation loss, and the Audio-Text Contrastive Learning loss. For the secondstage, we employ LLaMa-2 7B as the language de-coder and solve the language-modeling loss. For training, we use 2.5M+ audio-caption pairs (de-tailed in Section E.2). For architectural details, werefer our readers to Li et al. (2023).Training with Caption Augmentation. Addition-ally, due to the lack of large-scale audio captionpairs, we adopt a caption-augmentation method-ology to augment the existing audios with diverseadditional captions. More specifically, we instructan LLM to generate k rewrites of the original cap-tion. We employ two different prompts that rewritethe input caption with two different objectives:Prompts. For Prompt 1, our primary aim is thatthe resultant rewrite should describe each acous-tic event in the caption similarly but more vividly.These augmentations help the model learn variousdistinctive characteristics of the audio concepts cor-responding to the acoustic events. For Prompt 2,our primary aim is such that the resultant rewriteshould describe each acoustic event in the cap-tion differently from the original caption. Theseaugmentations aid the model in understanding thediverse linguistic expressions that can describe asingle audio concept. We show examples below:(more examples in ): (1) Original Caption: Someone made a coolvocal for a dubstep track.(1) Rewritten Caption by Prompt 1: A capti-vating vocal performance ignites the dubsteptrack, delivering a hypnotic and enthrallingsound that reverberates through the air.(1) Rewritten Caption by Prompt 2: The dub-step track features a slick, stylish vocal per-formance that adds a layer of sophisticationto its heavy beats and basslines.(2) Original Caption: Someone eating crispsand talking.(2) Rewritten Caption by Prompt 1: Crunchycrisps mingle with the sound of a lively con-versation, creating a cozy and intimate atmo-sphere.(2) Rewritten Caption by Prompt 2: Thecrunch of crisps and the rustle of papers cre-ate a cozy, intimate atmosphere, accompaniedby the gentle hum of a conversation. During training, for each audio sample, wechoose the original caption with a probability p =0.4 or one of the rewritten versions (with a proba-bility 1 p), where each rewritten caption has anequal probability of selection. Both instrcutionsare provided in Appendix B. We employ LLaMa-2-13B (Touvron et al., 2023) with human-written",
  "Multi-Layer Aggregator": "Motivation. To extract additional details aboutthe input audio, we devise a multi-layer aggrega-tor that integrates multi-level hidden features ofthe pre-trained AST. Although AST has a globalreception field in all layers, different layers learn au-ditory information at different scales (Singla et al.,2022), i.e., the middle layers encode more genericfeatures (e.g., basic sounds, textures), while deeperlayers capture high-level concepts (e.g., speechintonations, complex sound patterns). By aggre-gating these features, the multi-layer aggregatoroutputs features that encode a more holistic andfine-grained understanding of the audio. Thus, ourmulti-layer aggregator makes fine-grained auditoryknowledge more likely to be learned while training.Architecture.Our multi-layer aggregator is atransformer-style network consisting of two trans-former layers for aggregating the hidden featuresof the audio encoder. Given the hidden featuresAj and Ak from the middle layers in the audio en-coder, the aggregation module uses two blocks tosequentially integrate the former two features withthe last layer feature Ai. Each block B is composedof self-attention, cross-attention, and Feed-forwardnetwork (FFN) arranged in a sequential manner. Fi-nally, the output features A is generated as follows,",
  "Soft Prompt": "Motivation. Though models like AST and AudioQ-Former have shown much promise in audio tasks,a major problem still exists: real-world audio gen-erally has multiple and overlapping acoustic events,and understanding all such events from model fea-tures proves to be inherently complex (Ghosh et al.,2024b). This eventually leads to sub-optimal per-formance for complex reasoning, where the explicitknowledge of plausible acoustic events in the au-dio can improve model responses. Thus, to im-prove fine-grained audio perception capabilities,we augment GAMA with high-level semantic under-standing of the input audio. To do this, we employan off-the-shelf audio model to extract high-level semantic knowledge, i.e., audio event tags, as sup-plementary information. However, as audio eventclassification is not a solved problem, errors in tagpredictions are inevitable. Thus, to mitigate thepotential adverse effects of inaccurate predictions,we are inspired by prompt tuning to introduce asoft prompting technique that enables the model toutilize the embedded tags within the instructionsadaptively.Architecture. shows an example of howwe design our soft prompt together with an instruc-tion. Specifically, we construct a fixed instructiontemplate where we add the audio event tags alongwith the soft prompt, where the soft prompt is atrainable vector. In contrast to standard prompttuning, where the model activations are generallysteered towards completing the task for which theprompt is optimized, in our version the direction isspecified by a tailored input sentence, Accordingto <hint>, you are allowed to use or partially usethe following tags:, and <hint> will be replacedby the soft prompt. This design allows us to selectvaluable information from tags adaptively ratherthan serving a specific task, as seen in standardprompt tuning methods. We only employ the softprompt in the instruction tuning for complex rea-soning step and not in the fine-tuning step. Weprovide a rationale in Appendix C.1.",
  "CompA-R": "Motivation. We define complex reasoning as thecapability of an LALM to understand the inputaudio, every individual acoustic event in the au-dio, and reason the corresponding scene in whichthe audio might have occurred, such that it caninfer nuanced relationships between them and itsunderlying contexts, thereby enabling it to drawsophisticated conclusions. We design CompA-Rwith the primary goal of endowing LALMs withcomplex reasoning abilities. We are motivated bythe primary finding that current SOTA LALMscan only perform well in prompts that require de-scribing the audio (e.g., Describe the audio) orreasoning-based prompts where identifying theacoustic events present in the audio would suffice for a faithful response (e.g., What type of video canthis audio be used for dubbing?). However, whenposed with complex reasoning questions, thesemodels often hallucinate or fail to provide a faithfulresponse (see ). Inspired by a wealth of priorwork that shows how IT on well-curated datasetscan align model behaviors for the execution ofnovel skills like reasoning and complex problemsolving (Xu et al., 2024), we propose a system-atic multi-stage pipeline to synthesize instruction-response pairs for CompA-R. CompA-R trains amodel to engage in complex reasoning by queryingit with instructions that cannot be directly inferredby identifying individual audio events and wouldrequire analyzing each event and its context in rela-tion to other scene elements and world knowledge. Synthesis Pipeline.We employ the AudioSet-strong subset to synthesize CompA-R. Our datasynthesis pipeline consists of 3 stages: i) CaptionGeneration. To generate a caption that is awareof both the audio and the visual scene, we feedGPT-4 with multiple types of information about theaudio and its corresponding video, similar to Auto-ACD (Sun et al., 2023). These include a captionof the middle frame of the video generated usingBLIP-2 (Li et al., 2023), objects in the frame identi-fied using Grounding DINO (Liu et al., 2023c), im-age labels for the frame using the ImageNet (Denget al., 2009) ontology obtained from CLIP, environ-ment context using PlaceCNN (Zhou et al., 2017),caption of the audio obtained using RECAP (Ghoshet al., 2024a) and audio event tags using the Au-dioSet ontology obtained from AST. Finally, weprompt GPT-4 to aggregate these descriptions intoa comprehensive caption. ii) Dataset Synthesis.We pass the generated caption together with theground-truth acoustic event information and theircorresponding time slices to GPT-4. We promptGPT-4 with 3 human-written exemplars (which arerandomly sampled from a pool of 50 exemplars)to synthesize an instruction-response pair. The ex-emplars and prompt are designed such that the syn-thesized instructions demand complex reasoning.We synthesize a total of 25000 instruction-responsepairs. iii) Human Verification. We discard instruc-tions due to untended noise and hallucinations. We,the authors of this paper, manually verify a subsetof CompA-R corresponding to 500 unique audiosfor creating the test set, i.e., CompA-R-test. Theremainder of the synthesized dataset is used as thetraining set. We describe the process and annota-",
  "Audio": ": Pipeline for synthesizing CompA-R. For an audioin the AudioSet-strong dataset, we first use the audio andits corresponding video to generate a caption (described in.2). This caption is then fed into GPT-4 togetherwith the ground-truth time slices for each event in the audio(available for AudioSet-strong). We then instruct GPT-4 togenerate instruction-response pairs where the instruction isaccompanied by human-written exemplars (prompt in ).To generate the test set, we perform expert human verificationfor a subset of the generated dataset.",
  "Training": "Fine-tuning. We fine-tune GAMA on the OpenAQAtraining set released by Gong et al. (2024). We usea faction of all the instances due to the unavailabil-ity of the entire AudioSet and resource constraints.Dataset details are provided in Appendix H.1. Ad-ditionally, we augmented OpenAQA with 4 moredatasets, including MusicCaps, MusicQA, NSynth,and Magna, to improve its music understandingcapabilities. For fine-tuning, we follow the ex-act same 4-stage method proposed by Gong et al.(2024) where all parameters of all encoders aretrainable, and we train only the LoRA modules ofthe LLM. We request our readers to refer to Gonget al. (2024) for more details.Instruction Tuning on CompA-R. Post fine-tuning, we instruction-tune GAMA on CompA-Rto endow it with complex reasoning abilities. Fol-lowing common conventions (Liu et al., 2023b),we fine-tuned only the LoRA modules. We callthe Instruction Tuned GAMA as GAMA-IT. Al-though fine-tuning on AQA also endows GAMAwith instruction-following capabilities, CompA-Rdiffers in the nature of training instances (therebythe capabilities it endows), and thus, we differen-tiate with such a naming convention for ease ofreading.",
  "GAMA82.685.678.381.8": ": (Left) Comparison of GAMA with other LALMs on deductive reasoning benchmark (Deshmukh et al., 2024). (Right)Comparison of GAMA with other LALMs on objet hallucination benchmark (Kuan et al., 2024). We present scores for POPE-styleevaluation w/ random sampling. For both benchmarks, we compare Acc (Accuracy), P (Precision), R (Recall) and F1 scores.",
  "Experimental Setup": "Hyper-parameters. For the fine-tuning stage, wefollow the exact same hyper-parameter setup pro-posed by Gong et al. (2024). However, we scaledown our batch sizes to 4, 2, 2, and 2 (due to com-putational constraints) with an effective batch sizeof 256 in all stages. For Instruction Tuning, weemploy a batch size of 2, an effective batch size of256, and a learning rate of 1e-4. For both trainingand evaluation, we sampled audio at 16kHz. Ablations. To demonstrate the efficacy of indi-vidual components of GAMA, we evaluate the fol-lowing ablations: (i) w/ CLAP: We replace theAudio Q-Former with a CLAP model, fine-tunedwith a similar experimental setup as the Audio Q-Former. Since CLAP does not require AST featuresand works stand-alone, the AST features were nolonger fed to CLAP in this ablation. All other com-ponents are kept identical. (ii) w/ Last Layer Feats:We replace the Multi-Layer Aggregator with thelast-layer features of the AST. Thus, the LLM onlyreceives the last layer features of AST as input,and the AST features were not passed through theMulti-Layer Aggregator. All other componentswere kept identical. (iii) w/ Last Layers Feats +Transformer: Identical to the previous ablation, but now the last layer features of AST are passedthrough an additional 4-layer transformer networkbefore it is fed to the LLM. (iv) w/o AST & Ag-gregator: We remove AST and Multi-Layer Aggre-gator features, i.e., the model does not receive thedirect AST features in any form. Thus, the LLMis now only fed with features from Q-Former andthe Soft Prompt. Note that the Q-Former and theSoft Prompt still use the AST to obtain audio fea-tures and labels. All other components were keptidentical. (v) w/o Soft Prompt: We remove the softprompt features as input. All other componentswere kept identical. Baselines. We compare GAMA with i) generation-based LALMs: LTU, Qwen-Audio, SALMONN,Pengi and AudioGPT. We only employ the originalcheckpoints open-sourced by the authors and donot re-train the models due to compute constraints(except LTU, which we retrain on our version ofOpenAQA, the same batch size as GAMA, andwith LLaMa-2 as the LLM). We do not comparewith Audio Flamingo (Kong et al., 2024) as thecheckpoint was not available at the time of writ-ing the paper, and we are constrained by computefor training it from scratch. ii) audio-language en-coders: CLAP by Wu* et al. (2023b) and Elizalde",
  ": Comparison of GAMA with other baselines on open-ended AQA on OpenAQA, complex open-ended AQA on CompA-R-test and Dense Captioning on 500 instances from AudioCaps and Clotho": "et al. (2023b), CompA-CLAP (Ghosh et al., 2024b),AudioCLIP (Guzhov et al., 2021) and Audio Q-Former. For dense captioning and close- and open-ended AQA, we evaluate using GAMA-IT. For allother tasks, we evaluate using the only fine-tunedversion of GAMA (rationale in Appendix C). Evaluation Datasets and Metrics. Evaluationmetrics used for all evaluation datasets are men-tioned in and detailed statistics about eachdataset is mentioned in Section H.2.For clas-sification, zero-shot evaluation refers to datasetsGAMA that have never been seen during training;weak zero-shot evaluation refers to datasets GAMAthat have not been seen in training but are sourcedfrom the same project as part of the training data,and seen datasets refer to datasets GAMA has beentrained on. Similar to Deshmukh et al. (2023);Gong et al. (2024), we first caption the audio andretrieve the most similar label using SentenceBERT.We employ either accuracy (Acc), Micro-F1 (Mi-F1), or Mean Average Precision (mAP) for classifi-cation evaluation. For captioning, we also proposedense captioning, which evaluates a model for itscapability to identify every event in the audio andthe context of its occurrence with respect to otherevents in the audio (more in ). For evalua-tion, we randomly select a subset of 500 samplesfrom AudioCaps and Clotho. We also employ hu-man evaluation for OpenAQA, CompA-R-test, anddense captioning. For human evaluation, we askhuman annotators to score the caption on a scaleof 1-5 and report the score averaged across the 3.More details on recruitment and background of an-notators can be found in Appendix D. Finally, dueto human evaluation being prohibitively expensive,we also propose an automated evaluation methodol-ogy for complex open-ended AQA on CompA-R-test. We evaluate model responses using text-only",
  "Results and Analysis": "Quantitative Results. compares GAMAwith other baselines on the foundational au-dio processing tasks of classification and cap-tioning.For zero-shot classification evalua-tion on VocalSound (VS) (Gong et al., 2022),TUT 2017 (TUT) (Mesaros et al., 2018), Bei-jing Opera (BJO) (Tian et al., 2014), GTZAN(GTZ) (Park et al., 2022) and Medley-solos-DB(MDB) (Lostanlen et al., 2018), GAMA outperformsour baselines by 2%-67%. For weak zero-shot eval-uation on ESC-50 (Piczak, 2015) and DCASE2017Task 4 (DCASE) (Mesaros et al., 2017), GAMA out-performs our baselines by 1%-66%. Finally, forin-domain evaluation on VGGSound (VGG) (Chenet al., 2020), FSD50K (FSD) (Fonseca et al., 2021),AudioSet (AS) (Gemmeke et al., 2017) and NSynth(NS) (Engel et al., 2017) GAMA outperforms ourbaselines by 1%-84%. GAMA sees the steepest dropin performance when the AST and Aggregator areremoved (i.e., only Auio Q-Former is employed). presents a comparison between GAMAand other LALMs on the audio entailment taskintroduced by Deshmukh et al. (2024), which eval-uates the deductive reasoning abilities of LALMs.GAMA achieves the highest overall F1 score, outper-forming all other models, with SALMONN beingits closest competitor. On the right side of ,GAMA is compared to other LALMs in terms of ob-ject hallucination performance. Using POPE-styleevaluation with random sampling, GAMA demon-strates the best results, achieving the highest accu-racy and F1 score among all models. compares GAMA-IT with other base-",
  "Question: Considering the correlation of the man's speech and the music, infer his possible connection to themusic playing in the scene": "The male speaker's speech adds a sense of authority and control to the event's atmosphere, as well as a senseof urgency or importance to the crowd's reactions. The speech also provides a contrast to the music and crowdreactions, creating a dynamic and engaging soundscape that draws the listener's attention and emotions. The male speaker is interspersed with the music and crowd reactions to create a dynamic and engagingatmosphere at the event. His speech adds energy and excitement to the event, keeps the audience engaged andinvolved, and helps create a memorable and enjoyable experience for all attendees.",
  "LTUSALMONNPengi": ": Qualitative comparison of GAMA with other baselines on instances from CompA-R-test. Both instances challengean LALM with a question about the input audio that requires advanced understanding and complex reasoning regarding theaudio and its individual events. GAMA is able to provide faithful and accurate responses through improved audio perception andreasoning capabilities. HE refers to scores assigned by human evaluators. Note that the video is only provided for illustrationpurposes and not provided as input to the LALM. lines on AQA (open-ended and complex open-ended) and dense captioning. GAMA outperformsall our baselines on all settings. GAMA shows abso-lute improvement of 4% - 50% on OpenAQA, 8%- 58% on CompA-R-test and 8% - 30% on DenseCaptioning. Similar to the tasks in , per-formance on benchmarks suffers the most whenwithout the Audio Q-Former (when only the ASTand Aggregator are employed). Audio Q-Formerproves to be especially effective (over employingCLAP) in AQA.Qualitative Results. compares GAMA-ITagainst other LALMs from literature with instancesfrom CompA-R-test. All models compared by de-fault possess audio chat or open-ended AQA capa-bilities. GAMA-IT is able to provide more faithfulresponses that are both correct and preferred moreby humans. We provide additional comparisons inFigs. 8, 9, 10, 11, 12, and our demo page: (wherewe also show comparisons of dense captioning).",
  "Conclusion": "In this paper, we propose GAMA, an LALM withimproved audio perception abilities. We integratean LLM with multiple types of audio representa-tions, which are responsible for providing diverseknowledge about the input audio. GAMA fine-tunedon a mixture of open-source datasets outperformsprior audio-language models by significant marginson 16 datasets spanning 4 tasks. Next, we proposeCompA-R, an instruction-tuning dataset that wesynthesize using a robust pipeline for endowing anLALM with complex reasoning abilities. GAMA IT-",
  "GAMA and our experimental setup have several lim-itations, including:": "For the scope of our experiments, we do notevaluate and compare music understandingextensively. We do not do this as we do nottrain GAMA on diverse and large-scale musicdatasets. We also acknowledge that it is possi-ble to employ the GAMA architecture for com-prehensive music understanding if trained onlarge-scale music understanding datasets. Aspart of future work, we plan to release a music-only version of GAMA, similar to Gardner et al.(2024).",
  ". SoundBible - Free Sound Clips, Sound Bites, andSound Effects. Accessed: 25 September 2023": "Andrea Agostinelli, Timo I. Denk, Zaln Borsos,Jesse Engel, Mauro Verzetti, Antoine Caillon,Qingqing Huang, Aren Jansen, Adam Roberts, MarcoTagliasacchi, Matt Sharifi, Neil Zeghidour, and Chris-tian Frank. 2023. Musiclm: Generating music fromtext. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. Bert: Pre-training of deepbidirectional transformers for language understand-ing. arXiv preprint arXiv:1810.04805": "Konstantinos Drossos, Samuel Lipping, and TuomasVirtanen. 2020. Clotho: An audio captioning dataset.In ICASSP 2020-2020 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 736740. IEEE. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-mail, and Huaming Wang. 2023a.Clap learningaudio concepts from natural language supervision.In ICASSP 2023-2023 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 15. IEEE. Benjamin Elizalde, Soham Deshmukh, Mahmoud AlIsmail, and Huaming Wang. 2023b. Clap learningaudio concepts from natural language supervision.In ICASSP 2023 - 2023 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 15. Jesse Engel, Cinjon Resnick, Adam Roberts, SanderDieleman, Mohammad Norouzi, Douglas Eck, andKaren Simonyan. 2017. Neural audio synthesis ofmusical notes with wavenet autoencoders. In Inter-national Conference on Machine Learning, pages10681077. PMLR. Eduardo Fonseca, Xavier Favory, Jordi Pons, FredericFont, and Xavier Serra. 2021.Fsd50k: an opendataset of human-labeled sound events. IEEE/ACMTransactions on Audio, Speech, and Language Pro-cessing, 30:829852.",
  "Joshua P Gardner, Simon Durand, Daniel Stoller, andRachel M Bittner. 2024. LLark: A multimodal foun-dation model for music": "Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman,Aren Jansen, Wade Lawrence, R Channing Moore,Manoj Plakal, and Marvin Ritter. 2017. Audio set:An ontology and human-labeled dataset for audioevents. In 2017 IEEE international conference onacoustics, speech and signal processing (ICASSP),pages 776780. IEEE. SreyanGhosh,SonalKumar,ChandraKiranReddy Evuru, Ramani Duraiswami, and DineshManocha. 2024a.Recap:Retrieval-augmentedaudio captioning.In ICASSP 2024 - 2024 IEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 11611165. Sreyan Ghosh, Ashish Seth, Sonal Kumar, UtkarshTyagi, Chandra Kiran Reddy Evuru, RamaneswaranS, S Sakshi, Oriol Nieto, Ramani Duraiswami, andDinesh Manocha. 2024b. Compa: Addressing thegap in compositional reasoning in audio-languagemodels. In The Twelfth International Conference onLearning Representations.",
  "Yuan Gong, Hongyin Luo, Alexander H. Liu, LeonidKarlinsky, and James R. Glass. 2024. Listen, think,and understand. In The Twelfth International Confer-ence on Learning Representations": "Yuan Gong, Andrew Rouditchenko, Alexander H. Liu,David Harwath, Leonid Karlinsky, Hilde Kuehne,and James R. Glass. 2023. Contrastive audio-visualmasked autoencoder. In The Eleventh InternationalConference on Learning Representations. Yuan Gong, Jin Yu, and James Glass. 2022. Vocal-sound: A dataset for improving human vocal soundsrecognition. In ICASSP 2022-2022 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 151155. IEEE.",
  "Andrey Guzhov, Federico Raue, Jrn Hees, and AndreasDengel. 2021. Audioclip: Extending clip to image,text and audio": "Yuchen Hu, CHEN CHEN, Chao-Han Huck Yang,Ruizhe Li, Chao Zhang, Pin-Yu Chen, and EnsiongChng. 2024. Large language models are efficientlearners of noise-robust speech recognition. In TheTwelfth International Conference on Learning Repre-sentations. Rongjie Huang, Mingze Li, Dongchao Yang, Jia-tong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2024.Audiogpt: Understanding and generating speech, mu-sic, sound, and talking head.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 2380223804. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,and Gunhee Kim. 2019. Audiocaps: Generating cap-tions for audios in the wild. In Proceedings of the2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long andShort Papers), pages 119132. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213.",
  "Chun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee.2024. Understanding sounds, missing the questions:The challenge of object hallucination in large audio-language models. arXiv preprint arXiv:2406.08402": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR. Samuel Lipping, Parthasaarathy Sudarsanam, Konstanti-nos Drossos, and Tuomas Virtanen. 2022. Clotho-aqa: A crowdsourced dataset for audio question an-swering. In 2022 30th European Signal ProcessingConference (EUSIPCO), pages 11401144. IEEE.",
  "Karol J Piczak. 2015. Esc: Dataset for environmentalsound classification. In Proceedings of the 23rd ACMinternational conference on Multimedia, pages 10151018": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Yaman Kumar Singla, Jui Shah, Changyou Chen, andRajiv Ratn Shah. 2022. What do audio transform-ers hear? probing their representations for languagedelivery & structure. In 2022 IEEE InternationalConference on Data Mining Workshops (ICDMW),pages 910925. IEEE.",
  "Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie.2023. A large-scale dataset for audio-language repre-sentation learning. arXiv preprint arXiv:2309.11500": "Changli Tang, Wenyi Yu, Guangzhi Sun, XianzhaoChen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and ChaoZhang. 2024. SALMONN: Towards generic hearingabilities for large language models. In The TwelfthInternational Conference on Learning Representa-tions. Mi Tian, Ajay Srinivasamurthy, Mark Sandler, andXavier Serra. 2014. A study of instrument-wise on-set detection in beijing opera percussion ensembles.In 2014 ieee international conference on acoustics,speech and signal processing (icassp), pages 21592163. IEEE. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Automatic musical genre classification of audio sig-nals": "Shinji Watanabe, Takaaki Hori, Shigeki Karita, TomokiHayashi, Jiro Nishitoba, Yuya Unno, Nelson En-rique Yalta Soplin, Jahn Heymann, Matthew Wiesner,Nanxin Chen, et al. 2018. Espnet: End-to-end speechprocessing toolkit. arXiv preprint arXiv:1804.00015. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V Le. 2022. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations. Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*,Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023a.Large-scale contrastive language-audio pretrainingwith feature fusion and keyword-to-caption augmen-tation. In IEEE International Conference on Acous-tics, Speech and Signal Processing, ICASSP. Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*,Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023b.Large-scale contrastive language-audio pretrainingwith feature fusion and keyword-to-caption augmen-tation. In IEEE International Conference on Acous-tics, Speech and Signal Processing, ICASSP. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, QingweiLin, and Daxin Jiang. 2024. WizardLM: Empow-ering large pre-trained language models to followcomplex instructions. In The Twelfth InternationalConference on Learning Representations.",
  "Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong,Dan Su, Chenhui Chu, and Dong Yu. 2024. Mm-llms: Recent advances in multimodal large languagemodels. arXiv preprint arXiv:2401.13601": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-wei Zhang, Fei Wu, et al. 2023. Instruction tuningfor large language models: A survey. arXiv preprintarXiv:2308.10792. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models.arXiv preprintarXiv:2303.18223. Bolei Zhou, Agata Lapedriza, Aditya Khosla, AudeOliva, and Antonio Torralba. 2017. Places: A 10million image database for scene recognition. IEEETransactions on Pattern Analysis and Machine Intel-ligence.",
  "CGAMA-IT vs GAMA and EvaluationChoices": "GAMA is first fine-tuned on OpenAQA and theninstruction-tuned on CompA-R for complex rea-soning.We call the instruction-tuned versionGAMA-IT. We do not evaluate GAMA-IT on gen-eral tasks like classification and vanilla caption-ing 1. GAMA-IT is aligned to generate detaileddescriptions as part of the complex reasoning stage,and we found a lack of metrics and methods thatcan faithfully evaluate such descriptions for classi-fication or captioning. For example, the retrieval-based classification evaluation method, employedextensively in prior work, including ours, uses aSentence-BERT to retrieve the label closest to thedescription for classification evaluation. Duringour preliminary analysis, we found that Sentence-BERT, which just performs retrieval using semanticmatching, is unable to faithfully retrieve the correctlabel despite the caption mentioning the label asan audio event. We further investigated CLAP asour retrieval model for evaluation and found thatit suffers from the same limitations. We attributethis to the detailed and dense nature of the descrip-tions and the fact that these models only focus onhigh-level semantic meaning for retrieval. Our ini-tial experiments show that LLM prompting servesas a feasible alternative for automatic evaluation(beyond human evaluation) using such dense de-scriptions, but due to the lack of resources and aformal framework, we leave this as part of futureresearch.",
  "C.1Soft Prompts": "We employ the soft prompt only in the instructiontuning stage for learning complex reasoning andnot in the fine-tuning step. We do this for 2 rea-sons: (i) Fine-tuned GAMA is only expected to solvegeneric audio tasks like classification, captioning,etc. Thus, we hypothesize that such high-levelsemantic cues are not necessary for effective andoptimal performance. (ii) Since fine-tuning is doneon a large-scale dataset and acoustic event classifi-cation is far from accurate, our soft prompt methodmight add unwanted noise to the training process,thereby leading to sub-optimal performance. Onthe contrary, our instruction-tuning stage, which isdone on relatively low-resource data and is onlyresponsible for aligning a model for complex rea-",
  "DAdditional Details: Human Study": "Background and Recruitment for Dense Cap-tioning and CompA-R-test Evaluation. We re-cruit 3 professionals for human evaluation of densecaptioning and CompA-R-test evaluation.Allthese 3 professionals come with at least a Ph.D.in Engineering or Sciences and were asked to useheadphones to first analyze the audio and thenjudge the response quality. The authors of this pa-per gave these annotators 5 examples of responsesand the corresponding judgments. The work wasdone voluntarily and not paid. We refrain from re-cruiting crowd raters as prior research has noticeddiscrepancies in evaluation by them (Gudibandeet al., 2023). More precisely, they have been shownto possess a tendency to rate an answer with a highscore only by visualizing the style of answeringand not the exact factual information making upthe response.All 3 human annotators score the response be-tween 1-5, and we report scores averaged acrossthe 3. Prior to evaluation, all annotators were givenat least 10 examples from the authors of the paperof generations and their corresponding scores. Forevaluation, only the audio was provided to themwith software that could play the audio and hasfields to input the scores.Background and Recruitment for OpenAQA.Since the size of OpenAQA is relatively larger thanCompA-R-test, we perform evaluation on AmazonMechanical Turk similar to Gong et al. (2024).Evaluation was done with a total of 267 uniquehuman evaluators and each generation was scoredby 2 evaluators. The same software was used forevaluation as CompA-R-test.",
  "E.1Audio Q-Former Training Details": "Pre-training Hyper-parameter. For Stage 1 oftraining, we employ a training batch size of 192,an initial learning rate of 1e-4, a minimum learningrate of 1e-5, and a warm-up learning rate of 1e-6.We do cosine decay as the learning rate schedulingtechnique. We do warmup for 5000 steps. Stage 1was pre-trained on 8 A6000 GPUs for 100 epochs.For Stage 2 of training, we keep the exact samesettings as Stage 1 but change the batch size to 128. Fine-tuning. For zero-shot audio classificationevaluation, we fine-tune the Audio Q-Former afterStage 1 pre-training on the same corpus presentedin and using the same Stage 1 objective.The only difference in the fine-tuning step is thatwe train the AST model, which is otherwise keptfrozen in the pre-training stage.Fine-tuning Hyper-parameter. For fine-tuning,we again use the same hyper-parameter setting asStage 1 pre-training but use a batch size of 64.",
  "Dataset#Audio-Caption Pairs": "Audio Set (Gemmeke et al., 2017) 21591364Free Sound (Fonseca et al., 2022) 3259020VGGSound (Chen et al., 2020) 4185161AudioSet Strong (CompA Version) (Ghosh et al., 2024b) 5108311MACS (Morato and Mesaros, 2021) 614400BBC (BBC, 2018) 731201AudioCaps (Kim et al., 2019) 848649Clotho (Drossos et al., 2020) 918735SONISS (Sonniss Limited, 2022) 101602Musical Instrument (Agostinelli et al., 2023) 117990SoundBible (sou, 2023) 121232WavText5K (Deshmukh et al., 2022) 134347MusicCaps (Agostinelli et al., 2023) 142645GTZAN (Tzanetakis et al., 2001) 156014Medley-solos (Lostanlen et al., 2019) 16732",
  "FBaseline Details": "AudioCLIP. (Guzhov et al., 2022) AudioCLIP isan extension of the CLIP model that can handle au-dio in addition to text and images by incorporatingthe ESResNeXt audio model in the CLIP frame-work. It was trained on the AudioSet dataset, whichcontains millions of audio clips with correspondinglabels. CLAP. (Elizalde et al., 2023a) CLAP (ContrastiveLanguage-Audio Pre-training), similar to CLIP, isan audio-language model trained with contrastivelearning between audio data and their correspond-ing natural language descriptions. Representationsare obtained from audio encoders and text encoders.Wu* et al. (2023b) further extend this using a fea-ture fusion mechanism and keyword-to-captionaugmentation into the model design to further en-able the model to process audio inputs of variablelengths and enhance performance.CompA-CLAP. (Elizalde et al., 2023a) CompA-CLAP, an extension to CLAP, is trained on com-pletely open-sourced datasets and further fine-tuned using specific algorithms and datasets to im-prove compositional reasoning.Pengi. (Deshmukh et al., 2023) Pengi was one ofthe first efforts to achieve general-purpose audio un-derstanding through free-form language generationwith transfer learning. Precisely, Pengi integratesan audio encoder with a decoder-only pre-trainedlanguage model (LM) where the audio featuresserve as a prefixes for the LM during response gen-eration. Following this, similar to our evaluationstrategy, they prompt the model to caption the inputaudio and calculate the similarity between the cap-tion and the ground-truth audio label for zero-shotclassification. LTU. (Gong et al., 2024) As a concurrent work toPengi, took a step forward and showed that substi-tuting the pre-trained language model with an LLMcan induce an LALM with reasoning capabilities.Precisely, they achieved this by integrating an audioencoder to LLaMA (Touvron et al., 2023) and fine-tuning the model on close-ended and open-endedinstruction-tuning datasets. Finally, beyond justclose-ended tasks, they also evaluate their modelson open-ended reasoning tasks and show superiorperformance compared to baselines.AudioGPT. (Huang et al., 2024) Different fromPengi and LTU, AudioGPT differs in how the au-dio models and LLMs are integrated for complet- ing audio tasks. More specifically, different fromend-to-end training and alignment, they integrate aclosed-source model (ChatGPT) with a pre-trainedaudio model, already capable of completing the re-quired task, using a modality-transfer transformer. The integration or interaction between the twomodels is accomplished using the prompts. Ad-ditionally, AudioGPT is capable of solving moretasks, which include human verbal speech, beyondjust non-verbal speech like Pengi and LTU.SALMONN. (Tang et al., 2024) SALMONN fol-lows a similar architecture to LTU and Pengi anddoes prefix conditioning with an LLM. However,in addition to an audio encoder, they also integratea speech encoder for speech or verbal audio under-standing. Precisely, the audio and speech featuresare concatenated before feeding them as prefixesto the LLM. SALMONN shows unique reasoningcapabilities over speech inputs overlayed with non-verbal audio.Qwen-Audio. (Chu et al., 2023) Qwen fol-lows a similar architecture to LTU, Pengi, andSALMONN, i.e., adding audio features as prefixto the model, and additionally employs a novelmulti-task learning formulation for pre-training.More specifically, they append specific tags to spe-cific parts of the instruction-response text pairs andtrain the model on diverse speech, non-speech, andmusic tasks. Post-pre-training, similar to GAMA,employs an instruction-tuning stage for alignment.The resultant model, Qwen-Audio-Chat, is ableto respond to respond to diverse queries about theinput speech and audio.",
  "G.1Annotation and Annotator Details": "As mentioned earlier, CompA-R was cleaned andCompA-R-test was verified by the paper authorsthemselves. To preserve anonymity, we briefly pro-vide some details about the authors. All authorsof the paper are either enrolled in or have gradu-ated from a graduate degree (MS and/or Ph.D.).All authors have at least 2 years of professionalresearch experience at a academic or industry lab.Their research experience spans across speech, au-dio and language processing. This provides themwith adequate knowledge to faithfully complete theprocess.For CompA-R-test verification, after at least 3authors verified the test set, with proper rationales(which they were also asked to provide) the lead author cross-verified all instances. The verificationwas done manually on local laptops and no kind ofapplication was used which was made specificallyfor this. More details will be provided on camera-ready.",
  "H.3Other Details": "Model Parameters: GAMA has a total of 7Bparameters.Out of this, LLaMA-2-7B has 32transformer-encoder layers and 6.7B parameters,the Audio Q-Former has 280M parameters, andour LoRA modules introduce 4.2 M learnable pa-rameters for fine-tuning. The AST used in ourexperiments (audio-encoder of CAV-MAE (Gonget al., 2023)) has 85M parameters with 12transformer-encoder layers, 768-hidden-state, and12 attention-heads. Compute Infrastructure: All our experiments areconducted on four NVIDIA A6000 GPUs. TrainingGAMA required four days of continuous training.Training GAMA-IT requires 4 hours of training.Pre-training Audio Q-Former requires 7 days eachfor stages 1 and 2.",
  ": The statistics of the CompA-R-test dataset": "Implementation Software and Packages: We im-plement all our models in PyTorch 22 and use theHuggingFace 23 implementations of T5large and theoriginal implementation of HTSATtiny24.Forourbaselines,weusetheoriginalGitHub repository provided by the authors:LAION-CLAP 25, CompA-CLAP 26, CLAP 27,Wav2CLIP28, AudioCLIP29, MMT30, ML-ACT31, Pengi32,LTU33, AudioGPT34,SALMONN 35, Qwen-Audio 36.Potential Risks. GAMA might encode biases fromthe pre-trained LLM or during its fine-tuning stage.Additionally, Audio Q-Former used as a backbonefor audio-to-text/music generation, might generatesynthetic audio that is misused.",
  "I will provide you with 2 different types of information about a 10-second audio clip:": "1. A list where each comma-separated element indicates the individual events occurring in the audio at various time segments. For example, '(Speech-0.0-0.64)' would mean human speech between 0.0 second to 0.64 second.2. A caption of the audio describing in a brief and abstract manner the scene in which the audio takes place. I want you to act as a Prompt Generator. According to the event information and the caption, design some instructions and corresponding responses. Theinstruction should be designed in a way such that it can be answered only from the audio without the caption and any other detail provided. Theinstruction should involve one or more hops of complex knowledge and complex reasoning based on the scene created by the audio and the correspndingcaption. Ensure that the knowledge and reasoning chains in the instructions are precise and sufficiently challenging, to the extent that only well-educated people and experts in the respective field can provide adequate responses. The instructions must meet the following conditions:1. Do NOT use phrases like 'according to the caption' in both the questions and answers; you should ask and answer as if you were observing the imageby yourself.2. The questions and answers should be as diverse as possible.3. Please don't ask some simple questions about the intensity of the audio or the gender speaking the utterance; your questions must involve someknowledge.4. Your instructions should not be answered directly based on the image and your instructions. Instead, it requires the test-taker to carefully observethe image and have a deep knowledge of the content within the image in order to answer correctly.5. If a question cannot be answered, please do not ask. Come up with 3 diverse instructions for the knowledge topics above with different language styles and accurate answers. The instructions should containinterrogative sentences and declarative sentences. The answers should be less than 30 words.",
  "Output format, which is a list of jsons:": "[{'Instruction': instruction example, 'Answer': answer example, 'Knowledge topic': The specific knowledge topic}, {'Instruction': instruction example,'Answer': answer example, 'Knowledge topic': The specific knowledge topic}, ]Here are some examples of inputs and outputs: Input list of audio events: ['(Speech-0.0-0.64)', '(Mechanisms-0.0-10.0)', '(Dog-0.221-0.547)', '(Dog-0.803-0.966)', '(Generic impact sounds-0.885-1.129)', '(Tick-0.99-1.083)', '(Dog-1.432-1.665)', '(Speech-1.537-4.901)', '(Dog-1.921-2.119)', '(Dog-2.456-3.202)', '(Dog-3.434-3.597)', '(Dog-4.016-4.121)', '(Dog-4.936-5.39)', '(Generic impact sounds-5.204-5.611)', '(Dog-5.774-5.972)', '(Speech-5.984-6.787)', '(Tick-6.508-6.636)', '(Dog-6.717-8.266)', '(Generic impact sounds-7.649-8.277)', '(Laughter-8.347-9.488)', '(Dog-9.767-10.0)']Caption: A baby cries while a woman laughs, creating a joyful and lively atmosphere in a domestic setting. Output list of jsons: [{'Instruction': 'Analyze the sounds in the audio and determine the most likely cause of the laughter heard towards the end ofthe recording. Consider the potential interactions between the different sound sources and their temporal overlaps.' , 'Answer': 'The laughter likelyresults from the playful interaction between the dogs and the baby, as indicated by the overlapping sounds of dogs and the baby's presence.','Knowledge topic': 'Human and Animal Behavior Interpretation'}, {'Instruction': 'From the given audio, infer the type of domestic setting depicted inthe scene. Base your inference on the variety and sequence of sounds, particularly focusing on the interaction between the human speaking, the dogbarking, and other background noises that may be there.', 'Answer': 'The setting is likely a home with an active family environment, evidenced by thecontinuous presence of dogs, speech, and everyday household sounds.', 'Knowledge topic': 'Environmental Acoustics and Domestic Soundscapes'},{'Instruction': 'Considering the duration and placement of speech and laughter in the audio, infer the possible emotional dynamics between thespeakers. How do these elements interact to shape the scene's atmosphere?', 'Answer': 'The scene likely transitions from a more chaotic or lively moodand finally to a more joyful and relaxed atmosphere.'}] Input list of audio events: ['(Insect-0.0-0.724)', '(Mechanisms-0.0-9.777)', '(Female speech, woman speaking-0.737-1.434)', '(Bird vocalization, birdcall, bird song-1.243-1.775)', '(Insect-2.376-3.182)', '(Female speech, woman speaking-3.386-3.509)', '(Insect-4.397-5.23)', '(Dog-7.906-8.78)','(Surface contact-8.603-9.654)']Caption: 'Birds chirp in the distance as a dog barks, creating a lively atmosphere in a peaceful outdoor setting.'Output list of jsons: [{'Instruction': 'What time of day this scene is likely set in?.' , 'Answer': 'The concurrent presence of insect and bird soundssuggests a natural, outdoor environment, possibly during early morning or evening when such wildlife is typically active.', 'Knowledge topic':'Environmental Sound Analysis and Wildlife Behavior'}, {'Instruction': 'Analyze the presence and timing of the dog's barking in the latter part of theaudio. Considering the preceding sounds and infer the dog's possible reaction or behavior in this context.', 'Answer': 'The dog's barking following thepeaceful nature sounds and speech could indicate a response to a new stimulus, possibly a visitor or an animal in the area.', 'Knowledge topic':'Animal Behavior Analysis in Diverse Sound Environments'}, {'Instruction': 'Deduce the woman's likely activity or purpose in this setting.', 'Answer':'The woman might be engaging in an outdoor activity like gardening or bird-watching.','Knowledge topic': 'Human activity recognition through sceneanalysis' }] Input list of audio events: ['(Music-0.0-10.0)', '(Male singing-0.0-10.0)', '(Male speech, man speaking-0.354-1.364)', '(Male speech, man speaking-7.674-10.0)', '(Crowd-7.681-10.0)']Caption: 'A basketball bounces while music plays, and a man speaks in an indoor stage environment.'Output list of jsons: [{'Instruction': 'Considering the presence of crowd sounds towards the end of the audio, deduce the nature of the event takingplace. How do the elements of music, singing, and speech suggest the type of event and audience involvement?', 'Answer': 'The event seems to be a liveperformance or concert, with the crowd's reaction indicating an engaged and responsive audience, typical in such settings.', 'Knowledge topic': 'EventAtmosphere Analysis'}, {'Instruction': 'Given the continuous presence of music and male singing throughout the audio, analyze the role of the man'sspeech in shaping the atmosphere of the scene. How does his speech, interspersed with music and singing, contribute to the overall environment?','Answer': 'The man's speech likely serves as commentary or narration, adding a personal or interactive element to the musical performance, enhancingthe audience's engagement.', 'Knowledge topic': 'Music and Speech Dynamics'}, {'Instruction': 'Identify the genre of music being played and explain howit complements the atmosphere of the indoor stage environment.', 'Answer': 'The genre is likely upbeat or energetic, enhancing the lively ambiance of asports or performance event in an indoor setting.','Knowledge topic': 'Music Genre Detection and Scene Analysis'}]",
  "Complex AQA Evaluation Prompt": "Please act as an impartial judge and evaluate the quality of the response provided with respectto the details provided. You will rate the quality of the response on multiple aspects, such asHelpfulness, Clarity, Factuality, Depth and Engagement. The response has been provided by an AIagent for a query related to an input audio, which the agent can perceive. You will be providedwith 4 kinds of information for evaluating the response:1. A list where each comma-separated element indicates the individual events occurring in theaudio at various time segments. For example, '(Speech-0.0-0.64)' would mean human speechbetween 0.0 second to 0.64 second.2. A caption of the audio describing in a brief and abstract manner the scene in which theaudio takes place.3. The question asked to the AI agent related to the audio.4.An answer provided by an expert judge which you can consider as a reference.5. An answer by the AI agent.",
  "##Query: {query}": "## Evaluate### Aspects - Helpfulness: Rate the response based on how well it addresses the users query about the audioand provides a relevant answer. A score of 5 indicates the answer fully aids the user, while a1 suggests it offers little to no help. - Clarity: Rate the response based on how well-structured it is, with ideas presented in aclear and coherent manner. A high score of 5 means the answer is clear and logicallystructured, while a 1 suggests a disjointed or confusing reply. - Correctness: Evaluate the correctness or accuracy of the response provided with respect tothe information provided to you. A perfect 5 indicates the response is entirely correct andaccurate, while a 1 suggests it has significant errors or has not provided an answer to thequestion asked at all. - Depth: Determine the level of detail and thoroughness in the response. A score of 5 meansthe answer delves deeply into the aspects of the input image for answering the question, whilea 1 indicates it barely scratches the surface. ### Format### Given the query and the extra information about the audio provided (the caption and comma-separated list of individual individual events), please rate the quality of the output byscoring it from 1 to 5, individually on **each aspect**. You are allowed to use all 3information provided to you about the audio, in any way you want, to judge the response. Now, please output your scores in the following json format by filling in the placeholders in[]. { 'helpfulness': { 'reason': '[your rationale]', 'score': '[score from 1 to 5]' }, 'clarity':{ 'reason': '[your rationale]', 'score': '[score from 1 to 5]' }, 'correctness': { 'reason':'[your rationale]', 'score': '[score from 1 to 5]' }, 'depth': { 'reason': '[your rationale]','score': '[score from 1 to 5]' }, 'engagement': { 'reason': '[your rationale]', 'score':'[score from 1 to 5]' } }",
  "Caption Evaluation Prompt": "I want you to act as a Caption Evaluator. I will provide you with an audio caption generated byan AI agent. The agent was asked to generate a dense and detailed caption of the audio. Toevaluate the caption, I will provide you with 2 different types of information about the 10-second audio clip: 1. A list where each comma-separated element indicates the individual events occurring in theaudio at various time segments. For example, '(Speech-0.0-0.64)' would mean human speechbetween 0.0 second to 0.64 second.2. A scene caption of the audio describing in a brief and abstract manner the scene in whichthe audio takes place. Using these two pieces of information, assign a score of 1-10 to thecaption, where 1 is the lowest score and 10 is the highest score. Your evaluation should bebased on the detailedness, correctness, and bluntness of the caption. Return a JSON with asingle key 'score', where the value of the key is the score. Here are the details: {query}",
  "The woman is likely bathing her child, as indicated by the sound of a bathtub filling up with water": "Question: Given the variety of sounds including mechanisms, generic impact sounds, and water splashes,determine the most probable activity occurring during the audio clip. Consider the presence of baby laughterin your analysis. The most probable activity occurring during the audio clip is a family or group of people having fun together,possibly playing with babies or toddlers. The mechanical sounds could be from a playground or other outdoorequipment. Based on the variety of sounds including mechanisms, generic impact sounds, and water splashes, as well asthe presence of baby laughter, the most probable activity occurring during the audio clip is likely a bath or awater-related activity.",
  "The skill level of the tap dancer cannot be inferred from the audio clip": "The skill level of the tap dancer can be inferred from the nature and arrangement of sounds. If the sounds arecomplex and intricate, it suggests that the dancer is highly skilled. On the other hand, if the sounds are simpleand repetitive, it suggests that the dancer may not be as skilled.",
  "Question: Determine the type of power tool being used in this woodworking process based on the availablesounds. Consider the mechanisms sounds": "Based on the available sounds, it is difficult to determine the exact type of power tool being used. However,the sharp and metallic sound suggests that a circular saw or a hand drill might be present in the scene. Based on the available sounds, it is likely that a circular saw is being used in this woodworking process. Thecircular saw is a power tool that uses a spinning circular blade to cut wood, and the sound of the blade cuttingthrough the wood can be heard in the audio. Additionally, the sound of the motor and other mechanisms ofthe circular saw can also be heard in the audio.",
  "Based on the sound of acceleration and revving, it is likely that the engine type is a high-performance orsports car engine": "The audio contains the sound of an engine revving and accelerating. Based on this, it is likely that the enginetype is a high-performance gasoline engine, such as a V8 or a turbocharged four-cylinder. The sound of theengine revving and accelerating is a common characteristic of high-performance engines, which are designedto produce a lot of power and torque. The sound of the engine can vary depending on the specific make andmodel of the vehicle, but the general characteristics of a high-performance engine can be identified by thesound of the engine revving and accelerating.",
  ": \"a high-pitched, piercing wail\"2: \"a persistent, rhythmic sobbing\"": "1. A tiny voice pierced the stillness of the night, demanding attention with its high-pitched piercing wail.1. In the midst of the bustling market, a high-pitched wail rose above the chatter, signaling a tiny dissenter amongus.2. The persistent, rhythmic sobbing of a newborn punctuates the nights silence.2. Amidst the rustling leaves, a babys rhythmic sobs weave an unexpected lullaby.",
  ": \"a low, gurgling growl\"2: \"a grumbling undercurrent\"": "1. As she leaned in for the kiss, a low, gurgling growl betrayed her pre-date fasting.1. In the midst of the exam, a low, gurgling growl echoed from her stomach, punctuating the tension.2. As the classroom fell into a hushed anticipation of the next lecture, a grumbling undercurrent betrayed thestudents skipped breakfast.2. Amidst the solemnity of the library, a grumbling undercurrent served as a comical reminder of lunchtimesapproach.",
  "ESCC50siren1: \"Continuous, shrill alarm\"2: \"Ear-splitting, cyclic blare\"": "1. The night was pierced by the continuous, shrill alarm of the siren, signaling an urgency that couldnt be ignored.1. The relentless, shrill alarm of the siren wove through the corridors of the hospital, prompting swift movementsfrom the staff.2. The ear-splitting, cyclic blare cut through the night as the ambulance raced down the street.2. The ear-splitting, cyclic blare of the air raid siren cast a shadow of dread over the city.",
  "sheep1: \"bleating\",2: \"baaing\"": "1. In the tranquil meadow, a chorus of woolly creatures serenaded the dawn with their gentle bleating.1. The shepherd smiled as the flocks bleating echoed through the valley, signaling a return to the fold.2. A chorus of baaing accompanied the farmer as he made his morning rounds in the misty fields.2. Under the starry sky, the gentle baaing of the flock blended with the whispers of the night.",
  ": \"Melodious and silvery, carrying alight, airy tune that seems to float onthe breeze.\",2: \"Clear and resonant, with a puretone that sings above the orchestra likea bird in the morning sky": "1. The flutes melody weaved through the garden, mingling with the rustle of leaves.1. Amidst the hustle of the market, the silvery flute tune danced above the clamor, a ribbon of tranquility.2. The flutes crystalline melody soared above the orchestra, a lark greeting the dawn.2. Amidst the rustle of the forest, the flutes song danced through the leaves, pure and high.",
  ": \"Thumping, providing a rhythmicpulse that can drive the beat of mu-sic.\",2: \"Booming, with a powerful, per-vasive quality that can reverberatethrough a space.\"": "1. The bass pulsated through the dance floor, a heartbeat synchronizing every dancers move.1. Amidst the serene silence of the night, the bass from the distant festival throbbed like a gentle earthquake.2. The bass pulsated like a second heartbeat, filling the room with its unyielding presence.2. As the bassline dropped, it seemed to command the very air, a force unseen yet unforgotten.",
  ": \"an abrupt, resonant boom that star-tles anyone nearby\"2: \"a sharp, impactful smack as twohard surfaces collide with force\"": "1. The mailbox lid clapped shut, a resonant signal marking the departure of the days correspondence.1. The oven doors heavy thud resonated in the kitchen, a prelude to the aroma of freshly baked bread.2. The kitchen was filled with the aroma of spices and the sharp smack of dough being forcefully thrown onto thecountertop.2. In the crisp morning air, the sharp smack of the newspaper hitting the doorstep announced the arrival of dailynews.",
  ": \"Clanging and clattering\"2: \"Metallic clinking and clunking\"": "1. A symphony of clanging and clattering announces the busy bustle of a restaurant kitchen in full swing.1. The rhythmic clanging and clattering of pots and pans punctuate the air as grandma orchestrates her holiday feast.2. The metallic clinking and clunking heralded the start of the dinner rush in the bustling restaurant kitchen.2. A symphony of metallic clinking and clunking rose from the sink as grandma washed up after the family feast.",
  "TUT Ur-banbus1: \"a deep, rumbling engine\", \"2\": \"thelow, steady hum of the diesel motor\"": "1.The city pulse beats with a deep, rumbling engine, heralding the arrival of the morning commute.1. A gentle giant purrs in the stillness of dawn, its deep, rumbling engine announcing the start of a journey.2. Market stalls buzz with life, their vibrant colors and smells underscored by the buss diesel hum rolling down theavenue.2. Leaves rustle in the autumn breeze, a natural chorus to the buss diesel motor humming along the cobblestonepath.",
  "residentialarea": "1: \"The symphony of childrens laugh-ter and chatter fills the air, punctuatedby the occasional bark of a dog and thehum of lawn mowers in the distance.\"2: \"A serene hush blankets the neigh-borhood, broken occasionally by thesoft whoosh of passing cars and therustle of leaves stirred by a gentlebreeze.\", 1. The neighborhood comes alive with the melody of playful banter and the sporadic chorus of canines.1. Amidst the gentle drone of distant lawn mowers, the air vibrates with juvenile mirth and convivial exchanges.2. The neighborhood rests under a tranquil silence, punctuated now and then by the whisper of tires on asphalt andthe soft dance of leaves in the wind.2. Calmness envelops the streets, save for the faint hum of vehicles gliding by and the tender shuffling of foliage inthe zephyrs caress.",
  ": \"a steady humming\"2: \"a low, monotonous droning\"": "1. The room filled with the steady humming of the air conditioner as they focused intently on their chess match.1. A steady humming enveloped the library, where pages turned almost in rhythm with the air conditioningsconstant song.2. The air conditioners low, monotonous droning became the unlikely lullaby for a midsummers nap.2. Amid the quiet study hall, the air conditioners low, monotonous droning was a steady companion to the studentsfocused brows.",
  ": \"A loud, sharp crack that echoesthrough the air.2: \"A thunderous boom that startlesand reverberates.\"": "1. The nights silence shattered with a loud, sharp crack echoing through the air.1. A burst of sudden, sharp noise split the tranquil afternoon, reverberating off the canyon walls.2. A thunderous boom startles a flock of birds into the sky, their wings flapping frantically against the silence thathad just been.2. The nights silence was shattered by a boom, reverberating through the alleyways and causing stray cats to scurry.",
  ": \"a high-pitched, sharp chirp\"2: \"a soft, repetitive squeal\"": "1. In the moonlit barn, a tiny silhouette pauses to release its high-pitched, sharp chirp, disturbing the stillness of thehay-strewn loft.2. Amidst the rustling leaves, a diminutive creature contributes its sharp chirp to the dusk chorus, a minusculesoloist in natures vast orchestra.3. A soft, repetitive squeal punctuated the silence of the old attic.4. The cheese plate on the kitchen counter became the stage for a soft, repetitive squeal.",
  ": \"a rhythmic series of sharp clicks\"2: \"a steady clatter of keys strikingpaper\"": "1. Fingers dance across keys, a rhythmic series of sharp clicks punctuating the silence of the library.1. In the attic, a story unfolds to the staccato beat of a rhythmic series of sharp clicks.2. Each steady clatter of keys striking paper weaves a tapestry of words, painting stories on the blank canvas.2. In the dimly lit corner of the library, the rhythmic dance of metallic hammers against the page composes a silentsymphony.",
  "A man speaks followed by the sound of shuffling cards in asmall room": "1. A deep, resonant voice fills the small room, accompanied by the soft shuffle of cards as they change hands,creating an intimate and deliberate atmosphere.2. The sound of a mans voice echoes through the small space, punctuated by the subtle rustle of cards as they areshuffled and arranged, invoking a sense of purposeful deliberation.3. A deep voice speaks, followed by the subtle shuffle of cards, creating an intimate and anticipatory atmosphere inthe small room.4. The gentle rustle of cards breaks the silence, punctuated by a mans voice, evoking a sense of anticipation andprivate reflection in the cozy space.",
  "A person strums an acoustic guitar, creating melodic music withthe sound of a bell ringing in the background": "1. Soothing melodies flow from the acoustic guitar, harmonizing with the soft chime of a distant bell, crafting apeaceful ambiance.2. The acoustic guitars strings vibrate with grace, weaving a melodic tapestry that intertwines with the gentle ringof a bell, transporting the listener to a serene realm.3. The gentle strumming of an acoustic guitar weaves a melodic tapestry, intertwined with the soft chime of abackground bell, creating a soothing and harmonious atmosphere.4. The rhythmic plucking of an acoustic guitar crafts a lively and uplifting melody, complemented by the delicateringing of a background bell, transporting the listener to a serene and joyful realm.",
  "Dogs bark while people talk in the background, creating a livelyatmosphere in a field": "1. Lively chatter and joyful barks fill the air, capturing the playful spirit of a sunny day in a field.2. The rhythmic sounds of dogs barking and people talking blend together, creating a vibrant and lively ambiance inthe open field.3. The chatter of people and the joyful barks of dogs fill the air, creating a vibrant and lively atmosphere in the field.4. The sound of playful dogs and lively conversation fills the field, evoking a sense of happiness and energy.",
  "A mans voice is heard speaking over a radio as a vehicle passesby in the background": "1. A clear, crisp voice pierces the airwaves, intertwining with the distant hum of a vehicle, creating an engagingaudio experience.2. The mans voice on the radio blends seamlessly with the subtle rumble of a passing vehicle, forming a captivatingauditory tapestry.3. A voiceover speaks over a radio, complemented by the distant hum of a vehicle passing by, creating a dynamicand engaging audio experience.4. A mans voice broadcasts over the radio, intertwining with the subtle rumble of a vehicle in the background,forming a captivating audio landscape.",
  "A woman speaks while a bird chirps in the background, creatinga tranquil atmosphere in a natural setting": "1. A gentle voice echoes through the forest, harmonizing with the chirping of birds, creating a soothing ambiance.2. The sound of a gentle voice blends seamlessly with the melodic chirping of birds, transporting the listener to aserene natural setting.3. The womans gentle voice blends with the soothing chirps of a bird, creating a serene ambiance reminiscent of apeaceful afternoon in nature.4. The womans words are accompanied by the melodic chirping of a bird, transporting the listener to a calming andpicturesque outdoor setting.",
  "Water rushes as people talk in the background near a hot spring,creating a serene ambiance": "1. Soothing waters create a peaceful ambiance, punctuated by the gentle chatter of people nearby, as if they areharmonizing with the soothing sounds of the hot spring.2. The calm trickle of water creates an intimate atmosphere, with the soft murmur of voices in the backgroundadding a sense of connection and tranquility to the space.3. A soothing, babbling sound fills the air as people converse near a steaming hot spring, creating a tranquilatmosphere.4. The gentle gurgling of water intertwines with the chatter of people in the background, crafting a peaceful andrelaxing ambiance.",
  "Soft music plays in the background as a speech is heard faintly,creating a calm and peaceful atmosphere": "1. A soothing melody floats in the background, complementing the faint speech, creating a tranquil ambiance.2. The soft strains of music blend with the subtle speech, fostering a sense of serenity and calmness in theatmosphere.3. Soothing tunes fill the air, complemented by a gentle speech, creating an atmosphere of tranquility and serenity.4. Mellow music and soft speech blend together, crafting a calming environment that soothes the senses.",
  "A car engine revs up and then slows down, creating a vroomsound, as the vehicle accelerates in the audio": "1. The cars engine purrs and then decelerates, emitting a smooth and powerful vroom sound as it shifts gears,creating a dynamic and energizing atmosphere.2. The vehicles engine roars to life, producing a bold and intense vroom sound as it speeds up, then gradually slowsdown, immersing the listener in a thrilling and exhilarating experience.3. The cars engine purrs powerfully, then decelerates, creating a smooth and steady vroom sound as the vehiclegains speed.4. The cars engine roars to life, building momentum with a series of sharp vroom sounds before shifting gears andslowing down.",
  "Background music plays softly as the theme music graduallyfades in, creating a melodic ambiance in an arena/performancesetting": "1. The arena comes alive with a subtle, soothing melody that gradually builds in intensity, creating an electrifyingambiance.2. The soft strains of background music fill the air, setting the tone for an exhilarating performance in a vibrantarena setting.3. Soft, melodic strains fill the air as the theme music subtly builds, establishing a harmonious ambiance in thearena.4. The arena comes alive with a gentle, orchestral tune that gradually gains momentum, creating an uplifting andenergetic atmosphere.",
  "Instruction-Response PairsAudioSet IDCaptionTimestamp Events": "Instruction:Analyze the audio tounderstand the potential emotionalstate or mood of the man. Howdoes the progression from typing tospeech to chewing reflect his tran-sition through different phases ofwork or activity? Output:The maninitially seems engaged and fo-cused during the typing and speak-ing portion, which might then tran-sition into relaxation during thebreak, suggested by the chewingsound.",
  "YCecEf0abd4YA man speakswhile typing ona keyboard ina small room,followed by thesound of chew-ing": "(Generic impact sounds-0.0-1.037),(Backgroundnoise-0.0-10.0),(Genericimpactsounds-1.191-1.421), (Generic impact sounds-2.01-2.202),(Genericimpactsounds-2.343-2.574),(Malespeech, man speaking-2.727-3.393),(Genericimpactsounds-3.163-3.406), (Generic impact sounds-3.585-3.905),(Genericimpactsounds-4.136-4.379), (Breathing-4.405-4.917),(Genericimpactsounds-4.93-5.288), (Generic im-pact sounds-5.442-5.608), (Genericimpactsounds-5.736-6.12),(Genericimpactsounds-6.274-6.569),(Breathing-6.825-7.26),(Genericimpactsounds-6.863-7.042),(Malespeech,manspeaking-7.81-8.873),(Genericimpactsounds-8.041-8.348),(Breathing-9.001-9.36), (Humansounds-9.014-9.181),(Genericimpactsounds-9.309-9.565),(Scrape-9.449-10.0) Instruction:Considering the di-verse array of sounds present in theaudio, from insects to birds to adog, infer the type of ecosystemthis outdoor setting might repre-sent. What does the combinationof these sounds tell us about thebiodiversity and potential humanimpact in this area? Output:Theecosystem is likely a suburban orrural area with a mix of wildlifeand human habitation, indicated bythe variety of animal sounds andintermittent female speech.",
  "YcQiEI7HLGJgBirds chirp inthe distance asa dog barks, cre-ating a livelyatmosphere ina peaceful out-door setting": "(Insect-0.0-0.724), (Mechanisms-0.0-9.777),(Femalespeech,womanspeaking-0.737-1.434),(Bird vocalization, bird call, birdsong-1.243-1.775), (Insect-2.376-3.182), (Female speech, womanspeaking-3.386-3.509),(Insect-4.397-5.23),(Dog-7.906-8.78),(Surface contact-8.603-9.654) Instruction:Given the presence ofmechanisms throughout the audioand the interspersed generic impactsounds, infer the type of machinerythat is likely operating in the back-ground and its commonality in anoffice environment. Output:Thecontinuous mechanism sound sug-gests a printer or copier, which arecommon in office settings.",
  "(Male singing-0.0-6.594), (Music-0.0-10.0),(Mechanisms-0.0-10.0),(Breathing-7.064-8.314),(Breathing-8.911-10.0),(Malesinging-9.713-10.0)": "Instruction:Based on the audio,ascertain the possible relationshipbetween the gunfire sounds, ar-tillery fire, and music. How doesthe sequencing and manner ofthese sounds contribute to the at-mosphere of the scene? Output:The gunfire and artillery soundslikely serve as a ceremonial dis-play, with the music adding to thegrandeur and solemnity of a mili-tary parade."
}