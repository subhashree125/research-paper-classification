{
  "Abstract": "Fine-tuning-based unlearning methods prevailfor preventing targeted harmful, sensitive, orcopyrighted information within large languagemodels while preserving overall capabilities.However, the true effectiveness of these meth-ods is unclear. In this work, we delve intothe limitations of fine-tuning-based unlearn-ing through activation patching and parameterrestoration experiments. Our findings revealthat these methods alter the models knowledgeretrieval process, providing further evidencethat they do not genuinely erase the problem-atic knowledge embedded in the model param-eters. Instead, the coefficients generated bythe MLP components in the models final layerare the primary contributors to these seeminglypositive unlearning effects, playing a crucialrole in controlling the models behaviors. Fur-thermore, behavioral tests demonstrate that thisunlearning mechanism inevitably impacts theglobal behavior of the models, affecting unre-lated knowledge or capabilities. The code is re-leased at",
  "Introduction": "Large language models (LLMs), due to their exten-sive pre-training corpora, often inadvertently learnharmful, sensitive, or copyright-protected knowl-edge (Chang et al., 2023a; Mozes et al., 2023; El-dan and Russinovich, 2023; Ye et al., 2022). Con-sequently, recent research has focused on develop-ing efficient unlearning methods as a post-trainingtechnique to selectively unlearn the specific knowl-edge (Blanco-Justicia et al., 2024; Liu et al., 2024).Currently, the core mechanism of these unlearn-ing methods involves finetuning (Eldan and Russi-novich, 2023; Jang et al., 2023; Yao et al., 2024;Rafailov et al., 2023), with corresponding adjust-ments and designs in the loss function to facilitate",
  "*Work done during an internship at IDEA.Corresponding authors": "the unlearning process. Although earlier investi-gations (Hong et al., 2024; Lee et al., 2024a) haveproven that these methods are ineffective at com-pletely erasing model-embedded knowledge, thefactors contributing to the misleading success ofthese techniques remain unclear.Therefore, in this paper, we try to unveil whyexisting finetuning-based unlearning methods per-form well in behavioral tests by analyzing the mech-anisms of internal knowledge recall and flow withinmodels (Meng et al., 2022; Pochinkov and Schoots,2024; Geva et al., 2021a). Specifically, we investi-gate which components or parameters carry theseunlearning effects. We design activations patchingand parameters restoration experiments in three set-tings, aiming to independently study the impact ofunlearning methods on the coefficients and valuevectors in the MLPs, as well as on the attentioncomponents states. Our findings further confirmthat the methods do not truly alter the knowledgeembedded in the value vectors of MLPs, and re-veal that they will change how they extract andtransfer this knowledge through modifications inthe coefficients of MLPs and attention componentsduring unlearning. Notably, the coefficients pro-duced by the MLP in the final layers are primarilyresponsible for achieving the unlearning effects offinetuning-based methods.We further test the global behavior impactof these fine-tuning-based unlearning methodson LLaMA2-7B-chat (Touvron et al., 2023) andOLMo-7B (Groeneveld et al., 2024) by implement-ing them on the respective pretraining datasets ofboth models, aiming to more closely simulate theerasure of knowledge acquired during the pretrain-ing process. We discover that while these methodsappear to effectively unlearn target knowledge, theyalso inevitably affect the output and behavior re-lated to unrelated knowledge. This unintended con-sequence stems from the fact that these approachesare based on altering the models internal knowl- edge retrieval mechanisms, thereby impacting itsglobal behavior and overall performance.Ultimately, we conclude once again that cur-rent fine-tuning-based unlearning methods cannotcompletely erase sensitive knowledge embeddedin models, particularly within the MLPs, insteadadjusting the mechanisms by which the model re-trieves knowledge. These methods are vulnera-ble to recovery attacks in components activationsand unsuitable for true unlearning. We advocatefor future unlearning evaluations to concentrate onprecise measurement of both the actual storage oftargeted knowledge within the models entire pa-rameter set and the specific dynamics of how thisknowledge is retrieved and utilized.",
  "Background and Related Work": "Unlearning in Large Language ModelsSincelarge language models learn knowledge from dif-ferent domains and corpora during the pre-trainingprocess, it is often found that they contain harm-ful, sensitive or private knowledge, leading to thepossibility that language models produce outputbehaviors containing corresponding sensitive orharmful information (Liu et al., 2024; Chang et al.,2023a; Mozes et al., 2023). Therefore, unlearningemerges as a timely and important post-pretrainingprocessing method for LLM safety. Currently, thevast majority of LLM unlearning methods use fine-tuning as the primary operational approach. Interms of classifying them by different training ob-jectives, they include gradient direction control(Jang et al., 2023; Yao et al., 2024, 2023) and pref-erence optimization methods (Rafailov et al., 2023;Zhao et al., 2024; Lee et al., 2024b). In terms ofclassifying them by the parameters covered duringtraining, they include full parameters fine-tuning(Eldan and Russinovich, 2023; Jang et al., 2023;Yao et al., 2024; Rafailov et al., 2023), sparse fine-tuning (Chang et al., 2023b; Stoehr et al., 2024),and parameter-efficient fine-tuning (Lu et al., 2024;Chen and Yang, 2023). Additionally, there arealso a few knowledge editing methods (Patil et al.,2024). We present the specific logic details of eachmethod in A. Knowledge Storation in Large Language Mod-elsStudying how knowledge is stored, trans-ferred, and extracted in LLMs has always beenan important direction in the research of LLMs in-terpretability (Meng et al., 2022; Geva et al., 2021b;Sukhbaatar et al., 2015; Geva et al., 2023). It is known that in transformer-based language mod-els, the MLP is a crucial component for storingthe models factual knowledge, and its sub-layerscan be viewed as key-value memories (Geva et al.,2021b). To be specific, the first layer* of MLP sub-layers can be viewed as a matrix WK formed bykey vectors {k1, k2, . . . , kn}, used to capture a setof patterns in the input sequence, and ultimatelyoutputting the coefficient scores. The second layercan be viewed as a matrix WV formed by valuevectors {v1, v2, . . . , vn}, with each value vectorcontaining the corresponding factual knowledge(represented through token distributions). Finally,the MLPs output can be defined as the sum of valuevectors weighted by their memory coefficients:",
  "M = fW KxW V = mW V ,(1)": "where M represents the output of the MLP inthe transformers -th layer for an input hiddenstate x at that layer with the parameters, W Kand W V Rnd. f is a non-linearity function.m Rn represents the coefficient scores. Thedimension size of hidden states is d and it is n forthe intermediate MLP.In addition to the MLP, primarily responsible forknowledge storage, the attention component is cur-rently considered the main component responsiblefor knowledge transfer and extraction in languagemodels (Geva et al., 2023). Here, we will not gointo detail about its specific structure but only studythe impact it has on knowledge extraction. The fi-nal computation formula for the hidden states inthe language model is defined as:",
  "Patching Investigation": "Hypothesis and Experimental DesignBasedon Eq. (1) and Eq. (2), we hypothesize that thereare three main reasons why the current fine-tuning-based unlearning methods appear successful in be-havioral tests and seem to suggest that true unlearn-ing has been achieved: *Currently, in most decoder-only models such as GPT-2 (Radford et al., 2019) and GPT-J(Chen et al., 2021), theMLP component has two layers, while in LLaMA (Touvronet al., 2023) it has three layers. However, we can still considerLLaMAs first two layers together as the key matrices, withtheir output serving as the coefficient scores.Here, the bias term is omitted for brevity.",
  ". The value vectors W V in MLPs are changed,causing a change in the knowledge they con-tain;": "3. The change that happens in attention compo-nents caused the models focus and the corre-sponding information extracted by these atten-tion components A to change, thus reducingthe target knowledge-related information inthe output.Here, for the sake of simplicity and better un-derstanding, we continue to use the definitions ofm, W V , and A as given in Eq. (1) and Eq. (2) inthe following. We ignore the minor effects causedby other components or parameters, such as thelanguage models unembedding matrix and the nor-malization layers. Based on the possible reasonsdescribed above, on the unlearned model, we con-duct three different sets of activation patching orcomponents parameter restoration experiments,trying to recover the output of the target knowledgein the unlearned model. The specific operationprocess is as follows: 1. In the first set of experiments, we restore thecoefficient scores m corresponding to eachMLP component, layer by layer, in the lan-guage model, without making any intentionalchanges to the value vector parameters W V ofthe MLPs or the attention components statesA in any layer. 2. In the second set of experiments, we restorethe parameters of value vectors W V in MLPslayer by layer, recovering the knowledge theyoriginally contained. In this process, we avoidmaking intentional changes to the unlearnedmodels original coefficients m and the atten-tion components states A. 3. In the third set of experiments, we restore theoriginal attention components states A, butwithout intentionally altering the MLPs coef-ficient scores m or the value vectors parame-ters W V , only studying the impact brought bythe attention components which are responsi-ble for extracting and transferring knowledge.To evaluate the extent of knowledge restoration,we propose the metric of Knowledge RecoveryScore (KRS):",
  "where the losses are the average of MSE() on": "Li,n and Li,n and on Loi,n and Li,n, respectively.MSE() represents the mean squared error (MSE)loss function. L, L, and Lo are the logit distri-bution of the subsequent token produced by thevanilla model, unlearned model, and unlearned-then-recover model, respectively. The average lossis computed on the next I generated tokens on Nknowledge-related questions.Finally, if KRS approaches 1, it indicates Loi,nand Li,n that are nearly consistent, representing ahigher degree of knowledge recovery. Conversely,a lower KRS suggests a lower degree of that. Activation Patching and Parameters Restora-tion ExperimentsWe conduct the experimentson two recent LLMs, LLaMA2-7B-chat (Touvronet al., 2023) and OLMo-7B (Groeneveld et al.,2024). We apply two example finetuning-based un-learning methods, DPO (Rafailov et al., 2023) andGradient Difference (Yao et al., 2024), to performunlearning on the large language models and cal-culate the average KRS scores. Inspired by (Eldanand Russinovich, 2023), which tries to unlearn theconcept knowledge of Harry Potter in languagemodels, we extend this experiment by selecting10 well-known concepts per model from the Con-ceptVectors Benchmark (Hong et al., 2024), whichis a collection of concepts that language models arewell-acquainted with and have substantial knowl-edge about. Examples of them are provided in Ta-ble 2 of B. For the unlearning training, we use thetexts containing the corresponding concepts fromRedpjama and Dolma (Soldaini et al., 2024). Red-pjama is a replication of the pretraining corpus forthe LLaMA model, while Dolma is the open-sourcepre-training dataset for the OLMo model. Detailedinformation is provided in B. So here we can en-sure that the knowledge to be unlearned was atleast seen by the model during the pre-training pro-cess, and that the training data used more broadlycovers the textual sources from which the model ac-quired the corresponding knowledge about certainconcepts.After obtaining the unlearned model, we followthe steps mentioned in the hypothesis to performactivation patching and parameter restoration ex-periments on the unlearned models. To calculatethe Knowledge Recover Scores, we set I to 30 andN to 10, indicating the generation of the next 30tokens and the selection of 10 questions related toeach concept. To make the recovery effects more pronounced and the whole process easier to ob-serve, we adopt techniques from (Meng et al., 2022,2023) which implemented causal mediation, set-ting the size of the recovery window to five. Thisallows us to observe the average effects of recov-ering five consecutive layers at a time. Details canbe found in B.The specific results are shown in . Fromour analysis, surprisingly, we observe that whenwe solely recover the parameters contained in thevalue vectors of each layer in the unlearned modelwithout interfering with the coefficients or atten-tion components states, the recovery of the targetknowledge is negligible (The KRS scores are allbelow 0.001). This holds regardless of which layeris recovered, and regardless of the specific modelbeing considered.However, when recovering the attention compo-nents states in the intermediate layers (from the15th layer onward) or deeper layers (from the 27thlayer onward), we can observe that the averageKRS for both models has increased to exceed 0.3and 0.4, respectively, indicating that a significantportion of the corresponding knowledge has beenrecovered. Whats more, restoring the coefficientsof the MLPs in the intermediate layers (from the20th layer onward) and deeper layers (from the29th layer) also yields impressive knowledge re-covery effects.The layers at which the scores start to increaseunder the two settings generally align closely withthe observation by Geva et al. (2023) that the MLPmodules recall knowledge in intermediate layers,and the attention components mostly start to extractand transfer information in the deeper layers. or af-ter the model has completed the relevant knowledgerecall. We also tried simultaneously recovering thecoefficients and attention states and found that themodel can achieve much greater knowledge recov-ery, with the peak KRS score exceeding 0.9 on bothmodels.Additionally, it is noteworthy that, simply restor-ing the coefficient scores of the MLP outputs fromthe last two or three layers can significantly ele-vate the KRS of the unlearned LLaMA and OLMomodels to 0.8 or above. This suggests that the coef-ficient scores of the MLPs in the last layers mightplay a crucial role in the final behavior results of theLLM. To better isolate the effects of restoring m,W V , and A individually and support the aboveargument, we present a more rigorous patching andrestoration experiment in C, with the correspond- 0.0 0.2 0.4 0.6 0.8 1.0 Knowledge Recover Score(KRS) Patching & Restoration on LLaMA",
  "Layer": "0.0 0.2 0.4 0.6 0.8 1.0 Knowledge Recover Score(KRS) Patching & Restoration on OLMo Restoring Value VectorsRestoring CoefficientsRestoring Attention StatesRestoring Coefficients and Attention States : Results of KRS on LLaMA and OLMo un-der three activations patching or parameters restorationsettings. We also included another setting that restoresboth attention and coefficients to compare the final out-comes. ing results shown in . Ultimately, we foundthat the restoration of the attention states also con-tributed to the coefficients of the MLP in the finallayers, further confirming that these coefficientscarry the primary role of achieving the effects offinetuning-based unlearning. It also indicates thatfine-tuning largely adjusts the models behavior bymodifying the coefficients of the deep MLP layers,likely because this enables faster adaptation com-pared to other knowledge adjustment mechanisms,such as altering knowledge encoded in the MLP it-self. This phenomenon and the potential defensivestrategy have not been discussed in the previousliterature, warranting further investigation in futurestudies. Overall, these results all further confirm that thefinetuning-based unlearning methods essentiallydo not modify the model knowledge contained inthe value vectors, but adjust the way knowledgeis called during the fine-tuning process, either byadjusting the coefficients to modulate the MLP ac-tivation or by adjusting the attention to extract andtransfer knowledge.",
  "Global Negative Effect of Fine-TuningUnlearning": "In the previous section, we demonstrated that thesefinetuning-based methods alter the models final be-havior by adjusting the MLP output coefficients inthe final layers. Therefore, we hypothesize that thisbehavioral change will has a global effect, poten-tially impacting the output of unrelated knowledgeas well. In this section, we verify this hypothesisthrough the following experiments.We apply four fine-tuning-based unlearningmethods to the concepts used in 3 on their pretrain-ing text sources (from RedPajama and Dolma) withthe goal of erasing the learned knowledge duringpretraining through a reverse process. These meth-ods are as follows: DPO (Rafailov et al., 2023),NPO (Zhao et al., 2024), NPO+KL (Zhao et al.,2024) and Gradient Difference (Yao et al., 2024).The details of these baselines and data statisticsare shown in A and B. We evaluate the unlearn-ing effectiveness of these methods on the conceptsrelated QA pairs and the unlearning impact on un-related QA pairs, reporting the average scores ofBLEU (Papineni et al., 2002) by comparing themodels response before and after unlearning. In, we report their performance at the end ofeach training epoch respectively.We can observe that for finetuning-based meth-ods, as the number of training epochs increases,aiming to achieve a lower target QA BLEU score,the corresponding unrelated QA BLEU score alsodecreases accordingly, exhibiting a positive corre-lation. This suggests that the impact of finetuning-based methods on the models output behavior isglobal. While unlearning the target knowledge,they inadvertently alter the output behavior or man-ner for unrelated knowledge to a certain degree.",
  "Discussion and Conclusion": "We have deeply investigated the reasons why fine-tuning-based unlearning methods seemingly suc-ceeded in behavior-based testing for large languagemodel unlearning: Through activation patching andparameter restoration experiments, we find thatthese methods alter the way knowledge is extractedby changing MLP activations or models attention,ultimately affecting the output. This is evidencedby the fact that the models output regarding thetarget knowledge is largely restored after patch-ing the activations and the attention componentsstates. Furthermore, we conduct experiments on 0.00 0.25 0.50 0.75 1.00 Target QA BLEU Unlearning on LLaMA NPODPONPO_KLGrad Diff 0.00 0.25 0.50 0.75 1.00 Target QA BLEU Unlearning on OLMo",
  "Limitations": "In the experiments detailed in 3, we have disre-garded the potential unlearning impact caused byparameter changes in other model components dur-ing the fine-tuning process. This decision is basedon the observation that the impact of such changesappears to be minimal. For instance, during ourparameter comparison analysis, we found that thechanges in the unembedding matrix and normaliza-tion layer parameters resulted in cosine similarityvalues above 0.999. This suggests that the mod-ifications to these components are quite small inmagnitude. However, it remains unclear whether even suchminimal parameter changes can still have any mean-ingful effect on the models overall behavior andknowledge. Further verification and analysis wouldbe needed to conclusively determine the extent towhich these ancillary parameter updates might in-fluence the unlearning outcome.",
  "Acknowledgements": "The work was fully supported by the IDEA In-formation and Super Computing Centre (ISCC),National Natural Science Foundation of China(Grant No.62406114), the Guangzhou Basicand Applied Basic Research Foundation (GrantNo. 2023A04J1687), and the Fundamental Re-search Funds for the Central Universities (GrantNo. 2024ZYGXZR074). Di Wang and Lijie Huare supported in part by the funding BAS/1/1689-01-01,URF/1/4663-01-01,REI/1/5232-01-01,REI/1/5332-01-01, and URF/1/5508-01-01 fromKAUST, and funding from KAUST - Center of Ex-cellence for Generative AI, under award number5940. Alberto Blanco-Justicia, Najeeb Jebreel, Benet Man-zanares, David Snchez, Josep Domingo-Ferrer,Guillem Collell, and Kuan Eeik Tan. 2024. Digi-tal forgetting in large language models: A survey ofunlearning methods. Preprint, arXiv:2404.02062. Kent K. Chang, Mackenzie Hanh Cramer, Sandeep Soni,and David Bamman. 2023a. Speak, memory: An ar-chaeology of books known to chatGPT/GPT-4. InThe 2023 Conference on Empirical Methods in Natu-ral Language Processing.",
  "Ting-Yun Chang, Jesse Thomason, and Robin Jia.2023b.Do localization methods actually local-ize memorized data in llms?arXiv preprintarXiv:2311.09060": "Jiaao Chen and Diyi Yang. 2023. Unlearn what youwant to forget: Efficient unlearning for llms. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 1204112052. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374.",
  "Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021a. Transformer feed-forward layers are": "key-value memories. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 54845495, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021b. Transformer feed-forward layers arekey-value memories. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 54845495. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-gia, Rodney Kinney, Oyvind Tafjord, Ananya HarshJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,Shane Arora, David Atkinson, Russell Authur, Khy-athi Raghavi Chandu, Arman Cohan, Jennifer Du-mas, Yanai Elazar, Yuling Gu, Jack Hessel, TusharKhot, William Merrill, Jacob Morrison, Niklas Muen-nighoff, Aakanksha Naik, Crystal Nam, Matthew E.Peters, Valentina Pyatkin, Abhilasha Ravichander,Dustin Schwenk, Saurabh Shah, Will Smith, EmmaStrubell, Nishant Subramani, Mitchell Wortsman,Pradeep Dasigi, Nathan Lambert, Kyle Richardson,Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Sol-daini, Noah A. Smith, and Hannaneh Hajishirzi. 2024.Olmo: Accelerating the science of language models.arXiv preprint arXiv:2402.00838.",
  "Yihuai Hong, Lei Yu, Haiqin Yang, Shauli Ravfogel,and Mor Geva. 2024. Intrinsic evaluation of unlearn-ing using parametric knowledge traces. Preprint,arXiv:2406.11614": "Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,Moontae Lee, Lajanugen Logeswaran, and MinjoonSeo. 2023. Knowledge unlearning for mitigatingprivacy risks in language models. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1438914408, Toronto, Canada. Associationfor Computational Linguistics. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wat-tenberg, Jonathan K. Kummerfeld, and Rada Mihal-cea. 2024a. A mechanistic understanding of align-ment algorithms: A case study on DPO and toxicity.In Forty-first International Conference on MachineLearning. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Watten-berg, Jonathan K Kummerfeld, and Rada Mihalcea.2024b. A mechanistic understanding of alignmentalgorithms: A case study on dpo and toxicity. arXivpreprint arXiv:2401.01967. Sijia Liu,Yuanshun Yao,Jinghan Jia,StephenCasper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu,Yuguang Yao, Hang Li, Kush R. Varshney, MohitBansal, Sanmi Koyejo, and Yang Liu. 2024. Rethink-ing machine unlearning for large language models.Preprint, arXiv:2402.08787.",
  "Maximilian Mozes, Xuanli He, Bennett Kleinberg, andLewis D. Griffin. 2023. Use of llms for illicit pur-poses: Threats, prevention measures, and vulnerabili-ties. Preprint, arXiv:2308.12833": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Vaidehi Patil, Peter Hase, and Mohit Bansal. 2024. Cansensitive information be deleted from LLMs? ob-jectives for defending against extraction attacks. InThe Twelfth International Conference on LearningRepresentations.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Thirty-seventhConference on Neural Information Processing Sys-tems. Luca Soldaini, Rodney Kinney, Akshita Bhagia, DustinSchwenk, David Atkinson, Russell Authur, Ben Bo-gin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,et al. 2024. Dolma: An open corpus of three tril-lion tokens for language model pretraining research.arXiv preprint arXiv:2402.00159.",
  "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.2015. End-to-end memory networks. Advances inneural information processing systems, 28": "Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, D. Bikel, Lukas Blecher, Cristian CantnFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal,A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-abel M. Kloumann, A. Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, R. Subramanian,Xia Tan, Binh Tang, Ross Taylor, Adina Williams,Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, IliyanZarov, Yuchen Zhang, Angela Fan, Melanie Kam-badur, Sharan Narang, Aurelien Rodriguez, RobertStojnic, Sergey Edunov, and Thomas Scialom. 2023.Llama 2: Open foundation and fine-tuned chat mod-els. arXiv preprint arXiv:2307.09288.",
  "Large language model unlearning.Preprint,arXiv:2310.10683": "Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, SonghuaLiu, Xin Jin, Mingli Song, and Xinchao Wang.2022. Learning with recoverable forgetting. In Euro-pean Conference on Computer Vision, pages 87103.Springer. Weixiang Zhao, Yulin Hu, Zhuojun Li, Yang Deng,Yanyan Zhao, Bing Qin, and Tat-Seng Chua. 2024.Towards comprehensive and efficient post safetyalignment of large language models via safety patch-ing. arXiv preprint arXiv:2405.13820.",
  "BUnlearning Experiments Corpus": "Here, we present detailed information about thedata used for activation patching experiments andthe unlearning experiments conducted in 3 and4. We select 10 well-known concepts from Con-ceptVectors Benchmark (Hong et al., 2024) andextract 6,000 corresponding training data segmentscontaining knowledge about the respective con-cepts per model from the pre-training datasets ofRedpjama and Dolma. These extracted data seg-ments are used for unlearn training of the twomodels respectively. For each concept, we alsoinclude ten related questions from the ConceptVec-tors Benchmark, along with 50 unrelated questionssampled from other unrelated concepts. These areused in 4 to evaluate the unlearning effectivenessfrom the behavior perspective on the specific con-cepts, as well as to assess whether the modelsunrelated capabilities were affected. We have man-ually checked and verified that the vanilla LLaMAand OLMo models can accurately answer these se-lected questions, indicating that the models possessthe knowledge. All the statistics and examples areshown in and , respectively.",
  "CMore Rigorous Patching Investigation": "In 3, during our activation patching and parame-ters restoration experiments, we restore m, W V ,or A layer by layer respectively, while avoidingintentional changes to the other two states in the un-learned model. However, for instance, restoring A in -th layer may aid in the recovery of m in subse-quent layers, ultimately leading to an improvementin KRS. Therefore, in this part of the experiment,when restoring each element layer by layer, we pur-posefully keep the other two elements unchanged(e.g., when restoring A, we maintain the originalstates of m and W V for both the current and sub- 0.0 0.2 0.4 0.6 0.8 1.0 Knowledge Recover Score(KRS) Patching & Restoration on LLaMA",
  ": Results of KRS on LLaMA and OLMo un-der three activations patching or parameters restorationsettings, isolating the effects of the two others wheninvestigating each factor individually": "sequent layers). This approach thoroughly isolatesthe effects of these three different elements. presents the results in this setting. Wecan observe the following: (1) When W V is re-stored layer by layer, its effect on improving KRSremains very small, which is consistent with priorexperiments. (2) When restoring A layer by layerand isolating its effects from the other two factors,its contribution to KRS remains insignificant, stay-ing at a low level and only increasing to around 0.08on LLaMA and 0.2 on OLMO in the final layers.(3) When m is restored layer by layer, isolating itsinfluence from the other elements, we observe a no-table rise in KRS in the last three layers, reachingvalues as high as 0.8 or above. This supports theidea that neurons responsible for m in the MLPcomponents of the final layers primarily carry theunlearning effects of these finetuning-based meth-ods."
}