{
  "Abstract": "Personalized large language models (LLMs)aim to tailor interactions, content, and rec-ommendations to individual user preferences.While parameter-efficient fine-tuning (PEFT)methods excel in performance and generaliza-tion, they are costly and limit communal bene-fits when used individually. To this end, we in-troduce PERSONALIZED PIECES (PER-PCS)1,a framework that allows users to safely shareand assemble personalized PEFT efficientlywith collaborative efforts. PER-PCS involvesselecting sharers, breaking their PEFT intopieces, and training gates for each piece. Thesepieces are added to a pool, from which tar-get users can select and assemble personalizedPEFT using their history data. This approachpreserves privacy and enables fine-grained usermodeling without excessive storage and com-putation demands. Experimental results showPER-PCS outperforms non-personalized andPEFT retrieval baselines, offering performancecomparable to OPPU with significantly lowerresource use across six tasks. Further analy-sis highlights PER-PCSs robustness concern-ing sharer count and selection strategy, piecessharing ratio, and scalability in computationtime and storage space. PER-PCSs modularitypromotes safe sharing, making LLM person-alization more efficient, effective, and widelyaccessible through collaborative efforts.",
  "Introduction": "Personalization involves mining users history datato tailor and customize a systems interaction, con-tent, or recommendations to meet the specificneeds, preferences, and characteristics, of individ-ual users (Tan and Jiang, 2023; Chen et al., 2023;Kirk et al., 2024).By adapting to each usersunique preferences, personalization enhances theuser experience and has become increasingly im-portant in content recommendation (Li et al.,",
  "Target Personal PEFTPEFT Pieces": ": User personal PEFT parameter sharing frame-work: Sharers provide parts of their PEFT parameters(PEFT pieces). Using the target users history data, werecycle the PEFT pieces shared by anchor users andassemble the target users personal PEFT. 2023b; Wu et al., 2023; Baek et al., 2023), usersimulation (Dejescu et al., 2023; Zhang and Balog,2020), personalized chatbot (Srivastava et al., 2020;Ma et al., 2021), user profiling (Gu et al., 2020; Gaoet al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al.,2021; Pratama et al., 2023).Large language models (LLMs) are revolution-izing the research landscape with emergent abil-ities not observed in smaller models (Wei et al.,2022a; Lu et al., 2023), due to their training onmassive textual corpora and billions of parame-ters. These abilities include step-by-step reasoning(Wei et al., 2022b), in-context learning (Min et al.,2022), and instruction following (Wei et al., 2021).Despite these capabilities, current LLMs adhereto a \"one-size-fits-all\" paradigm, being trained onbroad, domain-agnostic data, which limits theireffectiveness in adapting to individual user prefer-ences (Chen et al., 2023). Consequently, personal-izing LLMs to align with users unique needs hasbecome a crucial research focus (Li et al., 2023a).PreviousendeavorstopersonalizeLLMscanbecategorizedintoprompt-basedandparameter-efficient fine-tuning (PEFT)-based meth-ods. Prompt-based personalization involves design-ing prompt templates to help LLMs understand user preferences, using methods such as vanillapersonalized prompting (Dai et al., 2023), retrieval-augmented prompting (Mysore et al., 2023), andprofile-augmented prompting (Richardson et al.,2023). However, prompt-based methods exposeuser data to centralized LLM and can be easilydistracted by irrelevant user history data, whichretrieval can hardly avoid (Shi et al., 2023). PEFT-based personalization methods focus on storingusers preferences and behavior patterns in per-sonal lightweight parameters. OPPU (Tan et al.,2024) is the pioneering work that stores users pref-erences and behavior patterns in personal PEFT pa-rameters, showing the superiority of model owner-ship and better user behavior pattern generalizationcompared to prompt-based methods. Despite theirsuccess, the one-PEFT-per-user\" paradigm is com-putationally and storage-intensive, especially forlarge user bases. For instance, using OPPU for per-sonalized product rating prediction requires about20 minutes of training on a single RTX A6000GPU and 17 MB of storage per user, scaling lin-early with the number of users. Additionally, indi-vidually owned PEFTs limit community value, aspersonal models cannot easily share knowledge orbenefit from collaborative improvements.Inspired by the exhaustiveness of human pref-erences (Lee et al., 2024), we propose the PER- SONALIZED PIECES (PER-PCS) framework, whichallows users to safely share a small fraction of theirPEFT parameters and build personalized LLMs ef-ficiently through collaborative efforts ().Specifically, we first select representative users assharers and train their PEFTs with their personalhistory data. We then break down the PEFT pa-rameters into pieces, inject a routing gate for eachpiece, and update the gate parameters while keep-ing the other parameters frozen with a few steps.These pieces are added to a pieces pool along withtheir corresponding gates for selection. In the as-sembly stage, PER-PCS feeds the target users his-tory data and selects PEFT pieces from the piecespool in an auto-regressive way, recycling the PEFTmodules in the pieces pool. By processing all thehistory data through this pipeline, we determine thePEFT piece choices for all layers and obtain thetarget users personal PEFT. PER-PCS is training-free and only requires the storage of sharers indexand corresponding composition weights, making itcomputation and storage efficient.Experimental results show that PER-PCS outper-forms non-personalized and PEFT retrieval base- lines, delivering performance comparable to OPPUbut with significantly reduced resource require-ments across six personalization tasks in the LaMPbenchmark (Salemi et al., 2023). Further stud-ies highlight PER-PCSs robustness against sharercount and selection strategy. Even when sharersconsent to share only a small portion of their pieces,PER-PCS maintains strong performance, compara-ble to scenarios where all pieces are shared. Timeanalysis reveals that PER-PCS is 38 times more ef-ficient in storage and 7 times more efficient in com-putation costs compared to OPPU. These findingsunderscore the potential of personalizing general-purpose LLMs by integrating modular and collabo-rative parametric knowledge from personal PEFTpieces shared by users.In summary, the contribution of PER-PCS is thepioneering framework that enables users to safelyshare personal PEFTs, facilitating efficient and fine-grained LLM personalization through collaborativeefforts. Unlike OPPU, where personal PEFTs ben-efit only the individual user, PER-PCS allows usersto share a limited portion of their PEFT parame-ters with others, ensuring user privacy. For targetusers, PER-PCS maintains model ownership andsupports fine-grained user modeling comparableto OPPU, but with significantly reduced storageand computation resources. We envision PER-PCSas an initiative to encourage users to share theirpersonal PEFT pieces, fostering collaboration inpersonalizing LLMs to create value for others. Thisapproach preserves sharer privacy and reduces thecarbon footprint of PEFT-based personalized LLM.",
  "PERSONALIZED PIECES (PER-PCS)": "We introduce PER-PCS, a novel framework to em-power LLM personalization with modular and col-laborative PEFT pieces within the community (). We first adapt non-personalized base LLMs tothe task without incorporating personal preferences(2.2). We then train personal PEFT and post-hocgates for sharers and add them to the pool (2.3).Finally, we assemble the target users PEFT usingtheir history and pieces from the pool (2.5).",
  "Pieces Pool": ": Overview of PER-PCS. First, we train PEFT and gate each piece for sharing. Next, we feed the targetusers history, utilizing history activation and piece gates to score and select PEFT pieces from the pool. Theseselected pieces are then assembled to create a personalized PEFT for the target user. users in set U have personal PEFT, while the targetuser u / U does not, our goal is to assemble thetarget users PEFT u from {u, u U}.PEFT Pieces. We assume a PEFT method intro-duces modules throughout the whole model. Forexample, LoRA (Hu et al., 2021) introduces a low-rank update at every linear layer in the model. Werefer to each of these updates as a piece.",
  "Base LLM Task Adaption": "Since off-the-shelf LLMs do not inherently un-derstand personalization tasks, we follow LaMP(Salemi et al., 2023) and Richardson et al. (2023)to fine-tune LLMs for fair comparison and taskcomprehension. In adapting the base LLM, we usedata that excludes target users and sharers datato build an LLM that understands task-related ca-pabilities rather than personal preferences. Specif-ically, the base LLM parameter o is optimizedw.r.t. loss L = CE[o((qu, R(qu, Hu, m))), ru],where CE denotes the cross entropy loss function,R is the retriever, is the prompt construction func-tion, m is the number of retrieval items, and Hu isthe entire user behavior history. For computationalefficiency, we adopted LoRA (Hu et al., 2021) forparameter-efficient fine-tuning and merged it intopretrained weights to obtain the base LLM.",
  "Sharer Selection and PEFT Training": "After adapting the base LLMs to the task with-out incorporating the target users personal prefer-ences, we select sharers who consent to share theirPEFTs with the community and train their personalPEFTs. To select representative users, we first getembeddings for all candidate users by encoding their history with an encoder-only language model,DeBERTa-v3-Large (He et al., 2022). The user em-bedding Eu = huHu Enc(hu)/|Hu| by averag-ing all history items hu from user u. We then clus-ter user embeddings with the k-means algorithm(K=50 by default) and select the most active userswithin the i-th cluster as sharer si (i = 1, ..., K),2",
  "si = {arg maxuCi |Hu|, Ci k-means(E, K)},": "where Ci denotes the i-th user cluster, E={Eu, u U} denotes the embedding set of allsharer candidates. Following OPPU (Tan et al.,2024), we then train personal PEFT parameterssi for sharer si using sharers history data Hsi.We then break the sharers PEFT parameterssi into pieces. For clarity, we consider the casewhere users perform personal PEFT using LoRA(Hu et al., 2021). Its worth noting that PER-PCS iscompatible with all PEFT methods that introducetrainable modules throughout the model, such asAdapter (Houlsby et al., 2019), (IA)3 (Liu et al.,2022), and prefix tuning (Li and Liang, 2021). Weprimarily focus on LoRA due to its popularity,widespread use, and superior performance demon-strated by OPPU. LoRA modifies the output ofl-th linear layer from zlt = W lovlt using a low-rank decomposition to zlt = W lovlt + W lvlt =W lovlt + BlAlvlt, where vlt Rn denotes the t-thinput activation at layer l, W lo Rdn denotes thebase model parameters which remain frozen dur-ing fine-tuning, Al Rrn and Bl Rdr aretrainable parameters. Therefore, a pair of (Bl, Al)is defined as a \"piece\" and personal PEFT param-eters si for sharer si can break down to pieces",
  "zlt = W lovlt + BlsiAlsivlt(glsi vlt),": "where glsi Rn is a trainable gate vector for sharersi at layer l and initialized to all zeros, is thesigmoid activation function, W lo, Blsi, and Alsi arefrozen. For sharer si, we optimize {glsi}Ll=1 usingsharer history Hsi. We then add the sharers PEFTpieces and corresponding gates to the pieces poolfor the upcoming selection and assembly. Gatetraining is limited to around 50 steps, making itcomputationally efficient. The post-hoc nature ofgate learning also adds flexibility, facilitating easierdeployment in real-world scenarios.",
  "Assemble Target Personal PEFT": "Motivated by the exhaustiveness of human prefer-ences (Lee et al., 2024), we assemble PEFT mod-ules for target users using the PEFT modules andgate vectors from sharers. Using the target usershistory Hu as input, we perform auto-regressivePEFT piece selection to assemble the target usersPEFT from input to output. For each layer l inLoRA, we feed the user history in LLM and com-pute the score for the input activation vlt and can-didate pieces using cosine similarity, then aggre-gate these scores from the token level to obtain thepiece-level score lsi for the piece from sharer si:",
  "t=b(glsi vlt),": "where glsi and vlt are the normalized gate vectorand activation. For user history (xu, yu) Hualigned with the task format, we set begin positionb = |xu| + 1 and end position e = |xu| + |yu| + 1,where | | denotes sequence length. Otherwise, forhistory xu Hu, we set b = 1 and e = |xu| + 1.We then select the top-k PEFT pieces at l-thlayer to select sharer set Sl for target user PEFTassemble Sl= {si, keep top-k ranked by lsi}.Next, we normalize the selected weights with the",
  "zlt = W lovlt + W luvlt": "After detailing the assembly process for a singlepiece in the target users PEFT, we extend it to theentire model that contains L layers. Once the pieceat l-th layer parameter assembly is complete, theoutput zlt is used as the input activation for the l+1layer selection. After computing parameter selec-tion for all history items and layers, we averagethe composed parameters to obtain the final PEFTparameters for the target user u = {W lu}Ll=1,which is a set of assembled parameters across alllayers sourced from sharers piece parameters.Overall, the assembly process does not involvemodel training or optimization, making it compu-tationally efficient compared to training personalPEFT for each target user from scratch. For stor-age, instead of storing the entire set of matrices inLoRA for each target user, PER-PCS only needsto store the selected PEFT piece index Sl and cor-responding weights wls across all layer positions,ensuring PER-PCS storage efficient.",
  "Experiment Settings": "DatasetsWe adopt the Large Language ModelPersonalization (LaMP) benchmark (Salemi et al.,2023) for our experiments, which consists of sixpublic language model personalization tasks, in-cluding three text classification tasks (personalizedcitation identification, movie tagging, and produc-ing rating) and three text generation tasks (person-alized news headline generation, scholarly title gen-eration, and tweet paraphrasing).3 We randomly",
  "Task details can be found in Appendix D. We excludethe LaMP-6: Email subject generation task since it involvesprivate data that we cannot access": ": Main experiment results on the LaMP benchmark. R-1 and R-L denote ROUGE-1 and ROUGE-L. k refersto the number of retrieved items, with k=0 indicating no retrieval; k=1 is the default. means higher values arebetter, and means lower values are better. The best score for each task is in bold, and the second best is underlined. indicates significant improvement against counterparts without PER-PCS.",
  "LAMP-7: PERSONALIZEDTWEET PARAPHRASINGR-1 .516.514.552.565.522.552.559.528.563.565.529.559.561R-L .463.457.511.517.475.512.517.482.521.519.480.515.519": "select 25% of users to train the base model fortask adaptation. From the remaining users, we ran-domly sample 100 to serve as test users for efficientand fair comparison with OPPU (Tan et al., 2024).The rest of the users are used as sharer candidateswho consent to share their PEFT parameters.4 BaselinesWe compare our proposed PER-PCSwith the non-personalized baseline, prompt-basedmethods (retrieval-augmented (Salemi et al., 2023)and profile-augmented personalization (Richard-son et al., 2023)), and PEFT-based personaliza-tion methods (PEFT retrieval (Zhao et al., 2024)and OPPU (Tan et al., 2024)). Although PEFTretrieval has not been applied to personalizationbefore, we employ it as a PEFT-level composi-tion baseline. compared with PER-PCS, OPPUrequires significantly more resources, which can beseen as the upper bound for sharer personal PEFTcomposition. We provide more baseline details inAppendix E. For all baselines and PER-PCS, weuse Llama-2-7B (Touvron et al., 2023) as the baseLLM and BM25 (Trotman et al., 2014) for retrievaloperations to ensure efficient and fair comparisons. Evaluation MetricsFollowing LaMP (Salemiet al., 2023), we use accuracy and F1-score forpersonalized text classification tasks (LaMP-1 andLaMP-2), and MAE and RMSE for LaMP-3: per-sonalized product rating. For personalized textgeneration tasks (LaMP-4, LaMP-5, and LaMP-7),we adopt ROUGE-1 and ROUGE-L (Lin, 2004).Higher scores indicate better performance for allmetrics except RMSE and MAE used in LaMP-3.",
  "shows the performance on the curated testset of six public tasks in the LaMP benchmark. Wehave observations as follows": "Performance with PER-PCS.Models equippedwith PER-PCS outperform non-personalized, RAG,and PAG counterparts across all six tasks. In per-sonalized text classification, PER-PCS achieves11.79% and 6.02% relative gains in accuracy andF1-score for movie tagging, and 27.32% and24.92% improvements in MAE and RMSE forproduct ratings. For personalized text generation,PER-PCS shows 4.25% and 4.28% relative im-provements in ROUGE-1 and ROUGE-L scoresfor news headline generation. These results demon-strate PER-PCSs effectiveness in enhancing LLMpersonalization. PER-PCS vs. PEFT Retrieval.Compared to thePEFT retrieval method, PER-PCS shows clear su-periority. For instance, PER-PCS achieves 8.76%and 22.09% performance gains in accuracy andF1-score for citation identification. Significant im-provements are also seen in movie tagging, prod-uct rating prediction, and news headline genera-tion tasks, highlighting the benefits of fine-grainedPEFT piece composition over PEFT-level composi-tion, which may risk user data leakage. PER-PCS vs. OPPU.Compared to OPPU, whichtrains personal PEFT from scratch and requiresmore computational and storage resources, PER-PCS achieves comparable or slightly better results.Specifically, PER-PCS achieves 99.28% of OPPUsperformance on average with 7 times less computa-tion and 38 times less storage in personalized text # Sharers 0.30 0.35 0.40 MAE Personalized Product Rating () PEFT RetrievalPer-Pcs (Ours) # Sharers 0.6 0.8 1.0 RMSE Personalized Product Rating () PEFT RetrievalPer-Pcs (Ours) # Sharers 0.180 0.190 0.200 ROUGE-1 Personalized News Headline () PEFT RetrievalPer-Pcs (Ours) # Sharers 0.165 0.170 0.175 0.180 ROUGE-L Personalized News Headline () PEFT RetrievalPer-Pcs (Ours) : Model performance with different numbers ofsharers in the product rating task (lower values indicatebetter performance). Our piece-level composition PER-PCS is stable and consistently outperforms the PEFT-level composition baseline.",
  "classification. In personalized text generation, PER-PCS shows comparable or better results in scholarlytitle generation and tweet paraphrasing": "PER-PCS with Non-Parametric Knowledge.Integrating both parametric user knowledge in per-sonal PEFT and non-parametric in retrieval anduser profile leads to notable performance gain. Av-eraging all tasks, RAG and PAG bring 1.67% and12.4% performance gain in text classification tasks,as well as 6.11% and 6.35% enhancement in textgeneration tasks.Note that introducing RAG and PAG meansusers would expose their historical data or pro-files to a centralized LLM, raising concerns abouthow user data are stored, used, and protected, andpotentially affecting model ownership. For usersprioritizing privacy and ownership, pure PER-PCSwithout retrieval avoids revealing user data to cen-tralized LLM, and our experiments show it sig-nificantly outperforms non-personalized baselines.Conversely, those seeking optimal performance andconsent to reveal data to centralized LLMs shouldopt for PER-PCS+RAG/PAG.",
  "Analysis": "Robustness against Sharer CountIn real-worlddeployment, the number of users who consent toshare their personal PEFT can vary, and compu-tational resources may constrain the number ofsharers, making the sharer count a crucial factor inPER-PCS. In this experiment, we alter the numberof sharers in two representative tasks from the textclassification and generation categories to test themodels robustness. As shown in , PER-PCS exhibits relatively stable performance despitechanges in the number of sharers and achieves the AccuracyF1-Score 0.2 0.3 0.340 0.268 0.394 0.294 0.399 0.295 0.410 0.300",
  "Share Ratio": "0.16 0.18 0.20 ROUGE-L News Headline Generation () Per-PcsRAG + Per-Pcs : The PER-PCS performance with differentPEFT parameter sharing ratios. PER-PCS maintainsstable performance with a small sharing ratio, while non-parametric user knowledge via RAG enhances stabilityand performance. best performance with just 30 sharers in the per-sonalized product rating prediction task, demon-strating its strong efficiency. Compared with thePEFT-level composition baseline (PEFT Retrieval),PER-PCS consistently shows better performance,highlighting the effectiveness of fine-grained piece-level composition in user modeling. On Sharer Selection StrategyIn the main re-sults, we present findings based on selecting sharersby clustering user history embeddings. However,users can have diverse distributions, and those whoconsent to share PEFT parameters may be biasedin their distribution. Therefore, we tested PER-PCSwith different sharer selection strategies to demon-strate its robustness against sharer selection. Specif-ically, we tested three strategies by restricting thesharer number to 50 users: \"Most Active,\" whichselects the 50 most active users; \"Profile Cluster,\"which uses a DeBERTa-v3-Large encoder to obtainuser embeddings for k-means clustering; and \"His-tory Cluster,\" the default setting, which averagesuser history embeddings to obtain user embeddingsfor clustering. As shown in , all sharerselection strategies lead to better performance thanthe non-personalized baseline. Furthermore, PER-",
  "Retrieval": ": Case study on a specific users PEFT assembled from sharers and corresponding piece weights in PER-PCS,compared with the PEFT retrieval choice. Unlike PEFT-level retrieval, PER-PCS models user history data in a morefine-grained manner while ensuring the privacy of sharers. PCSs performance remains stable across differentsharer selection strategies, demonstrating its robust-ness. We hypothesize that fine-grained piece-levelparameter composition can decompose complexuser preferences from diverse dimensions, facilitat-ing robustness against different sharer distributions. Shared Pieces Ratio StudyWe designed PER-PCS to enable sharers to share a small portion oftheir PEFT parameters, preserving user privacywhile maintaining strong performance. In this ex-periment, we varied the PEFT parameter sharingratio and assessed its impact on model performance.Shown in , using two representative tasksfrom text classification and generation, we foundthat PER-PCS is highly robust to the sharing ra-tio, achieving comparable performance with just20% of the sharers PEFT parameters comparedto full parameter sharing. Additionally, with non-parametric user knowledge from RAG, PER-PCSdemonstrates greater stability and performance.These results show that PER-PCS effectively bal-ances privacy preservation and model performance. Case StudyTo better understand the mechanismof piece-level composition in PER-PCS, we con-ducted a case study on piece selection and corre-sponding composition weights in product ratingprediction and news headline generation, represent-ing text classification and generation categories,respectively. As illustrated in , we observethat in both text classification and generation tasks,the selected pieces are diverse. Additionally, theweight distribution in generation tasks is more uni-form, likely due to the intrinsic complexity of per-sonality in text generation tasks. Compared withPEFT-level retrieval, we find that PER-PCS almostnever selects the same PEFT chosen by retrieval,yet it outperforms PEFT retrieval by 19.11% and4.15% in product rating prediction and news head- # User Disk Space (MB)",
  ": Comparison of storage and time complexitybetween our PER-PCS and OPPU, demonstrating thatPER-PCS requires significantly less time to assemblepersonal PEFTs and less storage space to save them": "line generation tasks, respectively. We speculatethat this is due to PER-PCSs ability to effectivelydecompose and combine sharer PEFT pieces in afine-grained manner, leveraging multiple sharersparameters to enhance generalization. Time and Space Complexity AnalysisScalabil-ity and efficiency are crucial for large-scale deploy-ment of personalization methods. We comparedPER-PCS and OPPU in terms of storage and as-sembly time. For storage efficiency, we used theproduct rating prediction task, observing require-ments as the user count increased. For time effi-ciency, we examined a single user in the movietagging task by varying the number of user historyitems. As shown in , PER-PCS is signif-icantly more efficient than OPPU in both storageand time. With increasing numbers of users andhistory items, PER-PCSs efficiency advantage be-comes even more pronounced, being approximately38 times more efficient in storage and 7 times moreefficient in time. Moreover, as the number of usersand history items grows, the efficiency advantageof PER-PCS becomes even more pronounced, beingapproximately 38 times more efficient in storageand 7 times more efficient in time. 1020304040+ # History Item 0.3 0.4 0.5 0.6 MAE Movie Tagging Prediction () Non-PersonalizedOPPUPer-Pcs (Ours) 1020304040+ # History Items 0.20 0.25 0.30 0.35 0.40 RMSE Movie Tagging Prediction () Non-PersonalizedOPPUPer-Pcs (Ours) 20406080100100+ # History Items 0.15 0.17 0.20 0.23 0.25 ROUGE-1 News Headline Generation () Non-PersonalizedOPPUPer-Pcs (Ours) 20406080100100+ # History Items 0.12 0.15 0.18 0.20 0.23 ROUGE-L News Headline Generation () Non-PersonalizedOPPUPer-Pcs (Ours)",
  "Modeling Users with Different ActiveLevels": "Users can exhibit different levels of activity, re-sulting in varying lengths of user history items foruser modeling and personalization. To investigatethe impact of user activity levels, quantified bythe number of historical behavior items, on modelperformance, we randomly sampled 10 users fromeach range of activity levels. As shown in , we observe that (i) PER-PCS generally showsstronger relative performance when user behavioritems are fewer than 20, likely due to the collabo-rative signals captured during the assembling pro-cess that help the model understand user prefer-ences. (ii) PER-PCS generally performs similarlyto OPPU, which requires training and maintainingpersonal PEFT from scratch and significantly moreresources, and (iii) both OPPU and PER-PCS out-perform the non-personalized baseline at almost allactivity levels. Overall, these results demonstratethe strong performance and robustness of PER-PCSacross all user activity levels.",
  "Ablation Study": "As PER-PCS outperforms various baselines in per-sonalization tasks, we investigate the impact ofeach design choice in PER-PCS to verify their effec-tiveness. More specifically, we perform ablation onthe assembling process in both attention aggrega-tion and piece selection steps. As is shown in , the full PER-PCS outperforms all ablated models,proving our design choices effectiveness. More-over, the weighted aggregation of PEFT pieces hasa significant impact on performance and is essen-tial for model generalization for target users. Wealso find that TopP and TopK sampling strategiesfor pieces strategy would involve randomness and : Performance of PER-PCS across different ab-lated versions: Top-p refers to setting a cumulative prob-ability threshold p and aggregating all pieces that firstreach this threshold. Topk-Sampling denotes samplingone piece from the top k pieces with normalized scoresas probabilities.",
  "Personalization of LLMs": "Existing LLM personalization methods can be cate-gorized into prompt-based and Parameter EfficientFine-tuning (PEFT)-based methods.Prompt-based personalization method focuseson designing prompts that incorporate user-generated content and behavior to help LLMsunderstand user preferences, which can be fur-ther categorized into vanilla personalized prompt-ing, retrieval-augmented personalized prompting,and profile-augmented personalized prompting.Vanilla personalized prompting leverages LLMsin-context learning and few-shot learning abilitiesby encoding either complete or randomly sampleduser history behaviors as contextual examples (Daiet al., 2023; Wang et al., 2023; Kang et al., 2023).To manage the rapidly growing user behavior andLLMs limited context window, researchers haveproposed retrieval-augmented methods for person-alized LLMs (Salemi et al., 2023), and enhance thecalibration (Mysore et al., 2023) and optimize re-trieval (Salemi et al., 2024). Moving beyond simpleretrieval, some researchers have proposed profile-augmented personalization prompting, summariz-ing natural language user preferences and behaviorpatterns to augment user queries (Richardson et al.,2023), and constructing hierarchy personalized re-trieval databases (Sun et al., 2024).PEFT-based personalization methods store userpreferences and behavior patterns in parameters.OPPU (Tan et al., 2024) equips each user with a per-sonal PEFT module, storing preferences in PEFTparameters and offering better generalization ofuser behavior patterns compared to prompt-basedmethods. Another line of work focuses on design-ing personalized alignment methods via parameter",
  "Model Parameter Composition": "Existing work has shown that performing weightedlinear interpolation of model parameters leads tothe composition of each model ability (Li et al.,2022; Tam et al., 2023; Dou et al., 2024). Thisapproach recycles efforts and computational re-sources used to create specialized models. Thesemethods can be divided into model-, PEFT-, andpiece-level compositions.Model-level composition methods treat the en-tire model parameter as the minimum compositionunit (Wortsman et al., 2022; Zhang et al., 2024;Choshen et al., 2022; Ram et al., 2023; Jin et al.,2022; Liu et al., 2024; Jian et al., 2024). Ilharcoet al. (2022) propose the task vector, which sub-tracts the weights of a fine-tuned model from thepre-trained weights and conducts task vector arith-metic to enable generalization across tasks and do-mains. PEFT offers lightweight alternatives forfine-tuning LLMs by updating small, plug-in pa-rameters while keeping the pre-trained weightsfrozen to save computational resources (He et al.,2021). In PEFT-level composition, the entire PEFTmodule is treated as the minimum unit. By com-posing PEFT parameters, models can achieve taskand domain generalization (Shah et al., 2023; Gouet al., 2023; Zhang et al., 2023). LoRAHub (Huanget al., 2023) uses a black-box optimizer to integratespecialized LoRAs, facilitating generalization tounseen tasks. Another line of work focuses on re-trieving PEFT (Jang et al., 2023b). LoRARetriever(Zhao et al., 2024) retrieves and composes multipleLoRAs based on the given input. In piece-levelcomposition, the minimum composition unit is aplug-in sub-component of PEFT within a specificlayer. For instance, in LoRA, each low-rank updateat a linear layer constitutes a \"piece.\" Muqeeth et al.(2024) focuses on task generalization and proposesrecycling PEFT pieces by employing per-token andper-piece composition under zero-shot settings.In this work, we propose PER-PCS, a personalPEFT sharing framework that takes advantage ofpiece-wise parameter composition, enabling usersto share partial parameters. This approach ensuresthe sharers privacy while maintaining model own-ership, fine-grained user modeling, and strong effi-ciency for personalized LLM democratization.",
  "Conclusion": "We proposed PER-PCS, a novel framework that en-ables users to share their personal PEFTs, creatingcommunity value while preserving privacy. For tar-get users, PER-PCS maintained model ownership,efficiency, and fine-grained personalization by em-ploying piece-level composition based on user his-tory data. Extensive experiments showed that PER-PCS outperforms non-personalized and PEFT re-trieval methods, and performs close to OPPU withsignificantly lower computational and storage re-sources. We envisioned PER-PCS as a community-driven effort to advance personalized LLM, makingit more modular, effective, and widely accessible.",
  "Limitations": "We identify two key limitations in PER-PCS. First,constrained by the dataset, our focus is primarilyon one specific task per user rather than examininguser behaviors across multiple tasks and domains.For instance, in the movie tagging task, users aresolely engaged in that specific activity, withoutthe inclusion of behaviors from other domains orplatforms. Despite this, the PER-PCS frameworkis inherently adaptable to any text sequence gen-eration task and is compatible with diverse userinstructions across various tasks and domains. Per-sonalizing LLM across a broader range of tasksand domains is left as future work. Second, de-spite our proposed PER-PCS is compatible withall PEFT methods that introduce trainable modulesthroughout the model, such as Adapter (Houlsbyet al., 2019), (IA)3 (Liu et al., 2022), and prefixtuning (Li and Liang, 2021), we primarily focus onLoRA in this work. This is due to LoRAs popu-larity, widespread use, and superior performancedemonstrated by OPPU (Tan et al., 2024), whilewe expect to expand our experiment and analysisto more PEFT methods in future work.",
  "Ethical Considerations": "Data BiasPersonalizing LLMs relies heavily onpersonal data input into the system. If this datais biased or unrepresentative, the models outputscould perpetuate these biases, leading to unfair orprejudiced responses. It is crucial to monitor andmitigate such biases in personal data and person-alized models to ensure fair, unbiased, and saferesponses from personalized LLMs. In PER-PCS,where users build personal PEFTs through collabo-rative efforts, bias in user data could spread within",
  "the community, amplifying negative effects. Fu-ture work could focus on preventing harmful biasesin user data at both the personal and communitylevels": "AccessibilityWhileadvancingpersonalizedLLMs aims to enhance user interactions with AIsystems, their complexity and resource-intensivenature can pose accessibility challenges. Smallerentities or individual researchers with limited com-putational power and budgetary constraints maystruggle to engage with advanced personalizedLLMs, potentially widening the gap in AI researchand application. Efforts should be made to makethese technologies more accessible to a broaderaudience to ensure equitable advancement in AIresearch. PrivacyThe personalization of LLMs necessi-tates tailoring responses based on user-specific data,which may include sensitive or private informa-tion. The ability of an LLM to adapt its outputsto individual users raises privacy concerns, as itmight inadvertently reveal personal details. Thishighlights the importance of implementing robustprivacy safeguards in LLM personalization to en-sure that personal data is handled respectfully andsecurely, preventing any unintended disclosures.",
  "Leshem Choshen, Elad Venezian, Noam Slonim, andYoav Katz. 2022. Fusing finetuned models for betterpretraining. arXiv preprint arXiv:2204.03044": "Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu,Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang,and Jun Xu. 2023.Uncovering chatgpts capa-bilities in recommender systems.arXiv preprintarXiv:2305.02182. Cosmina Andreea Dejescu, Lucia V Bel, Iulia Melega,Stefana Maria Cristina Muresan, and Liviu Ioan Oana.2023. Approaches to laparoscopic training in veteri-nary medicine: A review of personalized simulators.Animals, 13(24):3781.",
  "Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong,Haofen Wang, and Jiawei Zhang. 2023.Chat-rec:Towards interactive and explainable llms-augmented recommender system.arXiv preprintarXiv:2303.14524": "Dmitri Goldenberg, Kostia Kofman, Javier Albert, SaraiMizrachi, Adam Horowitz, and Irene Teinemaa. 2021.Personalization in practice: Methods and applica-tions. In Proceedings of the 14th ACM internationalconference on web search and data mining, pages11231126. Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, HangXu, Aoxue Li, Dit-Yan Yeung, James T Kwok, andYu Zhang. 2023. Mixture of cluster-conditional loraexperts for vision-language instruction tuning. arXivpreprint arXiv:2312.12379. Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, andDawei Yin. 2020. Hierarchical user profiling fore-commerce recommender systems. In Proceedingsof the 13th International Conference on Web Searchand Data Mining, pages 223231. Charles R Harris, K Jarrod Millman, Stfan J VanDer Walt, Ralf Gommers, Pauli Virtanen, David Cour-napeau, Eric Wieser, Julian Taylor, Sebastian Berg,Nathaniel J Smith, et al. 2020. Array programmingwith numpy. Nature, 585(7825):357362. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards aunified view of parameter-efficient transfer learning.In International Conference on Learning Representa-tions. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022.Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. In The Eleventh International Conference onLearning Representations. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational Conference on Machine Learning, pages27902799. PMLR. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations.",
  "Chengsong Huang, Qian Liu, Bill Yuchen Lin, TianyuPang, Chao Du, and Min Lin. 2023. Lorahub: Effi-cient cross-task generalization via dynamic lora com-position. arXiv preprint arXiv:2307.13269": "Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Ludwig Schmidt, Hannaneh Hajishirzi, and AliFarhadi. 2022. Editing models with task arithmetic.In The Eleventh International Conference on Learn-ing Representations. Joel Jang, Seungone Kim, Bill Yuchen Lin, YizhongWang, Jack Hessel, Luke Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.2023a. Personalized soups: Personalized large lan-guage model alignment via post-hoc parameter merg-ing. arXiv preprint arXiv:2310.11564. Joel Jang, Seungone Kim, Seonghyeon Ye, DoyoungKim, Lajanugen Logeswaran, Moontae Lee, Kyung-jae Lee, and Minjoon Seo. 2023b. Exploring thebenefits of training expert language models over in-struction tuning. In International Conference on Ma-chine Learning, pages 1470214729. PMLR. Yiren Jian, Tingkai Liu, Yunzhe Tao, Chunhui Zhang,Soroush Vosoughi, and Hongxia Yang. 2024. Ex-pedited training of visual conditioned language gen-eration via redundancy reduction. In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers,Oral Presentation). Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, andPengxiang Cheng. 2022. Dataless knowledge fu-sion by merging weights of language models. InThe Eleventh International Conference on LearningRepresentations. Kevin B Johnson, Wei-Qi Wei, Dilhan Weeraratne,Mark E Frisse, Karl Misulis, Kyu Rhee, Juan Zhao,and Jane L Snowdon. 2021. Precision medicine, ai,and the future of personalized health care. Clinicaland translational science, 14(1):8693.",
  "Seongyun Lee, Sue Hyun Park, Seungone Kim, andMinjoon Seo. 2024. Aligning to thousands of pref-erences via system message generalization. arXivpreprint arXiv:2405.17977": "Cheng Li, Mingyang Zhang, Qiaozhu Mei, YaqingWang, Spurthi Amba Hombaiah, Yi Liang, andMichael Bendersky. 2023a.Teach llms topersonalizean approach inspired by writing educa-tion. arXiv preprint arXiv:2308.07968. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, XinShen, Jingbo Shang, and Julian McAuley. 2023b.Text is all you need: Learning language representa-tions for sequential recommendation. arXiv preprintarXiv:2305.13731. Margaret Li, Suchin Gururangan, Tim Dettmers, MikeLewis, Tim Althoff, Noah A Smith, and Luke Zettle-moyer. 2022. Branch-train-merge: Embarrassinglyparallel training of expert language models. arXivpreprint arXiv:2208.03306. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Lin-guistics.",
  "models just in-context learning?arXiv preprintarXiv:2309.01809": "Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong,and Ji-Rong Wen. 2021. One chatbot per person:Creating personalized chatbots based on implicit userprofiles. In Proceedings of the 44th internationalACM SIGIR conference on research and developmentin information retrieval, pages 555564. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer. 2022. Rethinking the role of demonstrations:What makes in-context learning work? In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1104811064.",
  "Chanwoo Park, Mingyang Liu, Kaiqing Zhang, andAsuman Ozdaglar. 2024. Principled rlhf from hetero-geneous feedback via personalization and preferenceaggregation. arXiv preprint arXiv:2405.00254": "Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, et al. 2019. Pytorch: An imperative style,high-performance deep learning library. Advances inneural information processing systems, 32. Muh Putra Pratama, Rigel Sampelolo, and Hans Lura.2023. Revolutionizing education: harnessing thepower of artificial intelligence for personalized learn-ing.Klasikal: Journal of Education, LanguageTeaching and Science, 5(2):350357. Alexandre Ram, Kartik Ahuja, Jianyu Zhang, MatthieuCord, Lon Bottou, and David Lopez-Paz. 2023.Model ratatouille: Recycling diverse models for out-of-distribution generalization. In International Con-ference on Machine Learning, pages 2865628679.PMLR. Chris Richardson, Yao Zhang, Kellen Gillespie, SudiptaKar, Arshdeep Singh, Zeynab Raeesy, Omar ZiaKhan, and Abhinav Sethy. 2023. Integrating sum-marization and retrieval for enhanced personaliza-tion via large language models.arXiv preprintarXiv:2310.20081.",
  "Alireza Salemi, Sheshera Mysore, Michael Bendersky,and Hamed Zamani. 2023. Lamp: When large lan-guage models meet personalization. arXiv preprintarXiv:2304.11406": "Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu,Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.2023. Ziplora: Any subject in any style by effectivelymerging loras. arXiv preprint arXiv:2311.13600. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Inter-national Conference on Machine Learning, pages3121031227. PMLR.",
  "Zhaoxuan Tan and Meng Jiang. 2023.User mod-eling in the era of large language models: Cur-rent research and future directions. arXiv preprintarXiv:2312.11518": "Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, ZheyuanLiu, Bing Yin, and Meng Jiang. 2024.De-mocratizing large language models via personal-ized parameter-efficient fine-tuning. arXiv preprintarXiv:2402.04401. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Danqing Wang, Kevin Yang, Hanlin Zhu, XiaomengYang, Andrew Cohen, Lei Li, and Yuandong Tian.2023. Learning personalized story evaluation. arXivpreprint arXiv:2310.03304": "Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022a. Emergent abilities of large language models.Transactions on Machine Learning Research. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022b. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2020. Transformers: State-of-the-art naturallanguage processing. In Proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: system demonstrations, pages 3845. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-cos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,Simon Kornblith, et al. 2022. Model soups: averag-ing weights of multiple fine-tuned models improvesaccuracy without increasing inference time. In In-ternational conference on machine learning, pages2396523998. PMLR.",
  "Jinghan Zhang, Junteng Liu, Junxian He, et al. 2023.Composing parameter-efficient modules with arith-metic operation. Advances in Neural InformationProcessing Systems, 36:1258912610": "Shuo Zhang and Krisztian Balog. 2020. Evaluating con-versational recommender systems via user simulation.In Proceedings of the 26th acm sigkdd internationalconference on knowledge discovery & data mining,pages 15121520. Ziyu Zhao, Leilei Gan, Guoyin Wang, WangchunshuZhou, Hongxia Yang, Kun Kuang, and Fei Wu. 2024.Loraretriever: Input-aware lora retrieval and compo-sition for mixed tasks in the wild. arXiv preprintarXiv:2402.09997.",
  "We present the task details as follows to help read-ers gain a better understanding of the task format": "Personalized Citation Identification is a binarytext classification task. Specifically, given user uwrites a paper x, the task aims to make the modeldetermine which of the two candidate papers uwill cite in paper x based on the users historydata, which contains the publications of user u. Personalized News Categorization is a 15-waytext classification task to classify news articleswritten by a user u. Formally, given a news ar-ticle x written by user u, the language model isrequired to predict its category from the set ofcategories based on the users history data, whichcontains the users past article and correspondingcategory. Personalized Movie Tagging is a 15-way textclassification task to make tag assignmentsaligned with the users history tagging prefer-ence. Specifically, given a movie description x,the model needs to predict one of the tags for themovie x based on the users historical movie-tagpairs. Personalized Product Rating is a 5-way textclassification task and can also be understood asa regression task. Given the user us historical re-view and rating pairs and the input review x, themodel needs to predict the rating correspondingto x selected from 1 to 5 in integer. Personalized News Headline Generation is atext generation task to test the models abilityto capture the stylistic patterns in personal data.Given a query x that requests to generate a newsheadline for an article, as well as the user profilethat contains the authors historical article-titlepairs, the model is required to generate a newsheadline specifically for the given user. Personalized Scholarly Title Generation is atext generation task to test personalized text gen-eration tasks in different domains. In this task,we require language models to generate titles foran input article x, given a user profile of historicalarticle-title pairs for an author. Personalized Tweet Paraphrasing is also a textgeneration task that tests the models capabili-ties in capturing the stylistic patterns of authors.Given a user input text x and the user profile ofhistorical tweets, the model is required to para-phrase x into y that follows the given users tweetpattern.",
  "EBaseline Details": "Non-Personalized baseline: We present twoapproaches under the non-personalized setting:non-retrieval and random history. Non-retrievalmethod (k=0) refers to only feeding the usersquery without revealing the users behavior his-tory to the LLMs.Random history baselinemeans augmenting the users query with randomhistory behavior from all user history corpus. Retreival-Augmented Personalization (RAG):We follow the retrieval-augmented personaliza-tion method presented in LaMP (Salemi et al.,2023), where the users query is augmented withtop k retrieved items from the correspondingusers history corpus. We take k=1 by defaultin this work. Profile-Augmented Personalization (PAG):This method is taken from Richardson et al.(2023), in which the users input sequence wouldconcatenate the users profile summarizing theusers preference and behavior patterns. In ourexperiments, we generate user profiles using theMistral-7B (Jiang et al., 2023) model. More-over, the profile-augmented method could becombined with the retrieval augmentation. Inthis case, we take the number of retrieval items",
  "PEFT Retrieval: Similar to Jang et al. (2023b);": "Zhao et al. (2024), when a target user comes,we compute the cosine similarity between em-beddings of target users and sharers and find thetop-k similar users and conduct weighted aggre-gation to obtain target users PEFT. The PEFTretrieval method has not been applied to LLMpersonalization before and we select it as a PEFT-level composition baseline to compare with ourproposed fine-grained piece-level compositionmethod.",
  "LAMP-1: PERSONALIZED CITATION IDENTIFI-": "CATION### User Profile:{USER PROFILE}### User History:{USER HISTORY}### User Instruction:Identify the most relevant reference for the listed publication by the researcher.Select the reference paper that is most closely related to the researchers work. Pleaserespond with only the number that corresponds to the reference.Paper Title: {QUERY PAPER TITLE} Reference: - {OPTION1} - {OPTION2}Answer: LAMP-2: PERSONALIZED MOVIE TAGGING### User Profile:{USER PROFILE}### User History:{USER HISTORY}### User Instruction:Which tag does this movie relate to among the following tags? Just answer withthe tag name without further explanation. tags: [sci-fi, based on a book, comedy,action, twist ending, dystopia, dark comedy, classic, psychology, fantasy, romance,thought-provoking, social commentary, violence, true story]Description: {QUERY MOVIE DESCRIPTION} Tag: LAMP-3: PERSONALIZED PRODUCT RATING### User Profile:{USER PROFILE}### User History:{USER HISTORY}### User Instruction:What is the score of the following review on a scale of 1 to 5? just answer with 1, 2,3, 4, or 5 without further explanation.Review: {QUERY REVIEW} Score: LAMP-4: PERSONALIZED NEWS HEADLINEGENERATION### User Profile:{USER PROFILE}### User History:{USER HISTORY}### User Instruction:Generate a headline for the following article.Article: {QUERY ARTICLE} Headline: LAMP-5: PERSONALIZED SCHOLARLY TITLEGENERATION### User Profile:{USER PROFILE}### User History:{USER HISTORY}### User Instruction:Generate a title for the following abstract of a paper.Abstract: {QUERY ABSTRACT} Title:"
}