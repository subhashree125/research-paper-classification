{
  "Abstract": "Training large language models (LLMs) for ex-ternal tool usage is a rapidly expanding field,with recent research focusing on generatingsynthetic data to address the shortage of avail-able data. However, the absence of system-atic data quality checks poses complications forproperly training and testing models. To thatend, we propose two approaches for assessingthe reliability of data for training LLMs to useexternal tools. The first approach uses intuitive,human-defined correctness criteria. The sec-ond approach uses a model-driven assessmentwith in-context evaluation. We conduct a thor-ough evaluation of data quality on two popularbenchmarks, followed by an extrinsic evalua-tion that showcases the impact of data qualityon model performance. Our results demon-strate that models trained on high-quality dataoutperform those trained on unvalidated data,even when trained with a smaller quantity ofdata. These findings empirically support thesignificance of assessing and ensuring the relia-bility of training data for tool-using LLMs.",
  "Introduction": "Enabling LLMs to make use of external tools is apromising frontier that allows tapping into informa-tion that is not readily available to the model itself(Huang et al., 2024; Li et al., 2023a; Qin et al.,2024; Tang et al., 2023; Yang et al., 2023; Patilet al., 2023; Schick et al., 2023). Given a requestand a list of available external API functions, thebasic task of a model is to collect information byinvoking functions, and then to generate a responsefor the request. Due to the lack of data for thetask and the high cost of creating such data, re-searchers have devised synthetic datasets, predomi-nantly with the assistance of LLMs (Huang et al.,2024; Li et al., 2023a; Tang et al., 2023). These",
  "*This work was done during an internship in Amazon andas part of graduate studies at the Technion - Israel Institute ofTechnology": ": Data quality assessment methods for improv-ing the training process of tool-using LLMs (a), em-ploying two different approaches: (b) intrinsic qualityevaluation, using an external LLM to measure varioushuman-defined criteria; (c) in-context evaluation, usingthe target LLM to measure the educational value of datainstances. A smaller high-quality training dataset ismore effective than a larger unvalidated set. datasets have facilitated a great leap in promotingthe appealing applications of tool-using LLMs.Recently, Zhou et al. (2023) showed that higherquality training data yields better performance byLLMs in text generation tasks. However, leadingworks on tool-using LLMs have not made an ef-fort to measure the quality of training data. Rather,only model outputs are extrinsically evaluated, dis-regarding the effect of the data on the tested models.Most research on tool-using LLMs focuses on im-proving training and evaluation processes (Huanget al., 2024; Qin et al., 2024; Tang et al., 2023). Thelack of attention to data quality makes it difficultto interpret potential pitfalls for models. In turn,this wastes valuable resources for configuring andtuning models over possibly erroneous data.Datasets for tool-using LLMs comprise instruc-tions and ground truth API call sequences, andare created mainly with LLMs. Two such promi-nent datasets (Qin et al., 2024; Tang et al., 2023)were produced with the help of ChatGPT (OpenAI,2024), and were not explicitly assessed for theirquality. A closer inspection, conducted in this work, reveals numerous errors within the data, both in theinstructions and in the ground-truth API calls (4).To conduct our inspections, we define intrinsicmeasures for data quality assessment, focusing ondifferent aspects of quality. For each aspect weoutline human evaluation guidelines, as well as im-plement automated methods for evaluation. Theautomatic methods employ ChatGPT, either by di-rectly asking for its evaluation or by having it per-form a proxy task and deriving the evaluation fromits output. We show high agreement for our au-tomated methods with expert human annotations.In addition to the intrinsic measures, we proposea metric we call In-Context Evaluation (ICE; 5).ICE evaluates a data instance by how helpful itis for in-context learning, thus predicting its help-fulness for training a model (5). This metric isfully automated and does not rely on task-specificmeasurement definitions.Other than being appraisal instruments, the in-trinsic evaluation and ICE metrics can be used toautomatically filter out low quality data from anexisting dataset. In we carry out thisprocedure, and display the effect of training tool-using LLMs with higher quality data. Our find-ings, demonstrated on the ToolBench (Qin et al.,2024) and ToolAlpaca (Tang et al., 2023) bench-marks, show either better or comparable perfor-mance when using a small high-quality trainingdataset, compared to the original models trained onlarger unverified datasets. The two benchmarks arebased on different API function sets, and differentdata generation and training methods, indicatingthe generalized applicability of our methods.",
  "External Tool Usage by LLMs": "Tool learning is a recent area of research, aimingto enable LLMs to overcome limitations byaccessing tools for, e.g., retrieving up-to-dateinformation (Kasai et al., 2023; Cheng et al., 2024),or performing mathematical calculations (Schicket al., 2023), thereby enhancing their usability forreal-world needs.Research on tool learning focuses on variousaspects of training LLMs to use external tools.These mainly include tool selection, tool usage,and planning (Zhuang et al., 2023; Qin et al., 2024;Patil et al., 2023; Yao et al., 2023). Such modelsare mainly evaluated extrinsically, only measuringthe final results. T-Eval (Chen et al., 2024) is the first evaluation framework that analyzes tool-usingLLMs intrinsically. That is, it decomposes the eval-uation into all sub-tasks (such as selection, usageand planning), measuring the fine-grained abilitiesof models as tool agents. We intrinsically evaluatethe data for tool-usage instead of a model.",
  "Data Generation": "Recent notable works generated synthetic data fortool learning. ToolBench (Qin et al., 2024) lever-ages a large pool of real API functions.1 ChatGPT(OpenAI, 2024) was used to generate an instruc-tion that would require invoking a given small setof these tools, as well as to produce a solutionpath for the respective instruction. The data wasconstructed with a varying number of tools per in-stance and varying relatedness between the tools.API Bank (Li et al., 2023a) created synthetic APIdocumentation, instruction queries, and responsesusing strong LLMs (GPT-4 and ChatGPT; Ope-nAI, 2024). A smaller test set was created andvalidated manually by humans. Tang et al. (2023)constructed the ToolAlpaca dataset using ChatGPTto generate cleaner documentation upon existingAPIs, and respective instructions and responses. InToolAlpaca, most synthesized instructions only re-quire a single tool to fulfill the request. The test setwas validated by humans to ensure quality.To strengthen the credibility of our findings inthis work, we conduct our experiments over bothToolBench and ToolAlpaca, which differ in APIquality and instruction requirements.",
  "Data Quality": "The ever-increasing dependence on data for train-ing large models has paved a line of work thatanalyzes the effect of data quality on fine-tuningmodels. Findings show that a small but high-qualitydataset can be highly effective for fine-tuning a rel-atively small model, surpassing the performance ofa larger model. For example, Phi (Gunasekar et al.,2023; Li et al., 2023b) explored code generationtasks and prompted GPT-4 to assess the educationalvalue of coding examples. They demonstrated thata small number of high-quality and diverse exam-ples are sufficient to reach good quality of codegeneration. In the realm of instruction tuning, Liet al. (2024) suggest employing self-augmentationand self-curation to iteratively improve the set of in-structions used for instruct-tuning an LLM. LIMA",
  "Based on RapidAPI:": "(Zhou et al., 2023) considers the broader pictureof data quality, and show that as few as 1000 high-quality examples can be sufficient for training aninstruction-following model.Our work differs from these studies in that itapplies to the regime of tool usage. It can be seenas additional evidence reinforcing the prevailingless is more trend, proving the importance of dataquality in this regime.",
  "Task Setup": "Tool-using LLMs are expected to behave as follows.Given a set of tools T = {t1, ..., tn}, represented asAPI functions, and an instruction query q, a modelis required to plan a call sequence S = (t1, ..., tk),based on T, that would obtain information, or per-form actions, needed to address q. Based on theresponses obtained after performing the call se-quence (using an external API invoker), the modelthen generates a final response r that responds to q.The primary method for model evaluation is basedon calculating the pass rate, which measures theproportion of instances that successfully addressedtheir instructions, i.e., a predicted r responded to qadequately (explained further in 6).As mentioned in , the prominentdatasets created for training and testing tool-usingmodels were created synthetically with the assis-tance of LLMs. Specifically, we utilize the Tool-Bench (Qin et al., 2024) and ToolAlpaca (Tanget al., 2023) datasets. summarizes theircharacteristics. The main practical differences arethe quality of the APIs (i.e., the documentation clar-ity and uniformity of ToolBench is inferior to thatof ToolAlpaca), and the number of tools required torespond to a query instruction (ToolBench might re-quire several calls to unrelated tools, while ToolAl-paca requires calling a maximum of two relatedtools). As presented later in this work, these twodifferences strongly reflect on the overall qualityof the respective datasets.",
  "Problem statement.Our primary focus ison evaluating and improving data quality, and": "to show its effect on model performance intool-using LLMs.Following a similar line ofresearch, we hypothesize that a small quantity ofhigh-quality training data is preferred over a largequantity of lower-quality data. To demonstratethis, we first define intrinsic quality criteria forthe data (4.1) and implement automated metricsaccordingly (4.3). We additionally propose analternative data quality appraisal method usingin-context evaluation (5). Finally, we filter outthe lower-quality data from datasets using ourautomated metrics, and analyze the effect of theimproved data quality on model performance (6).",
  "Quality Criteria": "We set out to understand what makes an instanceof data high quality, specifically for training tool-using LLMs. The criteria we discuss pertain to boththe query instruction and the API call sequence ofa data instance.2 4.1.1Instruction PropertiesIn our setting, an instruction is a free-form textof one-to-a-few sentences that describes a user re-quirement. An instruction can contain more thanone request, likely implying the need for severaltool invocations. The following properties in the in-struction demand validation (examples in ):",
  "Solvability.The requests within the instructioncan be addressed by the given API tools": "4.1.2API-Call Sequence PropertiesApart from the instruction, given as input to amodel, the other vital component of a training in-stance is the ground-truth output used for training(or evaluating) a model. In our setting, this is the se-quence of API calls that the model is expected to in-fer. We define the following properties for API-callsequence correctness (see for examples): 2We considered other properties that were eventually ex-cluded from our framework, such as diversity and syntaxvalidity. See Appendix A.1 for more details.3Note that we deal with a setting where the agent is ex-pected to complete the instruction without asking clarificationquestions.",
  "Manual Annotations": "The six intrinsic properties defined above specifythe desired qualities for data instances of tool-usingLLMs. Existing datasets do not always abide bythese quality criteria, especially when they are col-lected synthetically and do not go through a clean-ing phase. We inspect such noisy data by prepar-ing annotation guidelines with respect to the crite-ria, and annotating accordingly. Specifically, wemethodically4 annotated 50 (instruction, API se-quence) pairs from each of the training sets of Tool-Bench (Qin et al., 2024) and ToolAlpaca (Tanget al., 2023), as well as a large portion of the Tool-Bench test set (700 instances).5 Each of the crite-ria is marked either as valid or invalid for each ofthe annotated instances. The annotated data is used",
  "Automated Metrics": "Although manual assessment of data is preferredfor its reliability, it is labor-intensive and thereforenot scalable or practical. We propose automaticmetrics for the intrinsic quality criteria definedabove. The metrics are based on ChatGPT,6 whichis tasked to determine the validity of each criterionas a binary decision.For the dimensions of Specificity, Coherenceand Parameter alignment, direct annotation withChatGPT proved to be challenging. That is, sim-ply asking the model to validate the property ina natural language instruction did not yield suffi-cient decisions (see Appendix A.3). Thus, we trans-formed the direct annotation tasks into traditionalNLP tasks, on which ChatGPT performed better. Specificity.Validating the specificity of requestsis modeled as an extraction task. ChatGPT is taskedto infer the details required for a given request,and then extract the available values from the in-struction, or mark a parameter as #missing. Wethen compute a proxy score for specificity: 1 if allparameters were successfully extracted from theinstruction, and 0 otherwise7. 6Throughout the paper, we use gpt-3.5-turbo-0613.7A continuous score can be computed as the percentage ofextracted parameters from the total number of parameters, butwe opted for a binary score for simplicity.",
  "Overall Correctness0.860.890.950.920.760.740.900.81": ": Validation results of the automated metrics for each criterion compared against human annotations. Coarse-grained correctness considers combined correctness over specific criteria. Note that precision, recall and F1 aremeasured w.r.t. a label that is positive when an error occurs, so e.g., recall means the amount of errors caught. Coherence.We adopt the concept of nextsentence prediction to assess coherence.Theinstruction is split into sentences, and ChatGPTdetermines if each subsequent sentence logicallyfollows the previous one.We set a coherencescore as 1 if all sentence pairs are judged logicallyconnected, and 0 otherwise.",
  "Evaluation of Automated Metrics": "Using the manually annotated data (described in4.2), we conduct an assessment of the automaticmetrics proposed. For each of the ToolBench andToolAlpaca datasets, the 50 annotated instances arecompared against the automatically produced val-ues, producing measures of accuracy (agreement),precision, recall and F1 score. We treat instancesmarked as incorrect instances as positive labels,since we aim to identify and filter erroneous in-stances.We conduct a coarser-grained evaluation of thecriteria, assessing Instruction Correctness as in-correct if any instruction criterion is wrong, andSequence Correctness as incorrect if any API-callsequence criterion is wrong. Overall Correctnessaggregates all six criteria similarly.Results are presented in . Given that ourmain objective is to identify and filter out incorrectdata samples, our emphasis is on achieving high recall. This objective is largely met across mostcriteria in both datasets. In the Overall Correctnessassessment, which aggregates all criteria, weobserve high recall and precision, demonstrating astrong alignment of the automated metrics with hu-man judgment. This approach thus offers a reliablemechanism to identify problematic data instances.",
  "Quality of Datasets": "presents the percentage of instances con-taining errors in the train sets of both ToolBenchand ToolAlpaca, as determined by the automatedmetrics. These statistics provide insights into thequality of the data in each dataset. In the ToolBenchdataset we observe a much higher percentage oferrors across most quality criteria, when comparedto ToolAlpaca. This difference may be attributedto (1) the complexity of instructions in ToolBench,which can require several (up to 5) API calls; (2)real-world APIs used in ToolBench, where the APIdocumentation is not always clear, resulting in in-correctly generated instructions and API-call se-quences. Notice that in both datasets, over 33% ofinstances have parameter alignment errors. Suchan error means that one of the core requirementsof a tool-using model identifying parameters cor-rectly is misleadingly learned in more than a thirdof the cases, due to wrong training examples. Someanecdotal examples of incorrect instructions foundby our metrics can be seen in Appendix A.5.",
  "In-Context Evaluation (ICE) as anAlternative Data Measurement": "Using intrinsic evaluation, we have defined an intu-itive and straightforward approach to identify low-quality data instances based on human understand-ing of data correctness. However, assessing theeducational value of an instance, i.e., its contribu-tion to the learning process of a model, is a com-plex task. In addition, the intrinsic evaluation met-rics proposed rely on prompting a powerful LLM,which can become costly on large datasets. Toaddress these challenges, we propose In-ContextEvaluation (ICE) as an alternative automatic ap-proach for assessing data quality.Recent studies found a connection between in-context learning and fine-tuning, demonstratingthat language models implicitly perform gradi-ent descent when dealing with in-context tasks(Von Oswald et al., 2023; Dai et al., 2023). Moti-vated by this insight, we seek to evaluate the edu-cational value of each data instance by measuringthe performance of in-context learning using thespecific instance as a one-shot example.",
  "Setup": "To construct the in-context task for external tooluse, we prepare a set of 10 human-written APIs,denoted by A, with simple accompanying docu-mentation. In addition, we hand-craft a set of 7 testquery instructions, TEST, where each such examplecontains a natural language instruction and an ex-pected API-call sequence, from the APIs in A, thatwould address the instruction. For each evaluationinstance, we insert an in-context example, x, whichconsists of an instruction and API-call sequencefrom the training dataset (i.e., ToolBench or ToolAl-paca). x follows the structure of the test examples.We then formulate a prompt for the LLM that weaim to train, that asks to generate responses forthe 7 test cases. In particular, the prompt includes(1) task instructions, (2) the API documentation ofA, (3) the training instance, x, given as a one-shotexample, (4) the 7 testing instructions of TEST. The prompt is given to an LLM we aim totrain: LLaMA-7B for ToolBench or Vicuna-7B forToolAlpaca. We analyze its response, that shouldinclude the 7 API-call sequences of TEST. Theresponses for the test instructions are evaluatedagainst the ground truth (using Levenshtein simi-larity (Levenshtein et al., 1966), expecting an exactmatch for API-call sequences). The final ICE scorefor x is the average over the 7 test examples, inter-preted as a measure of the educational value of x.We provide the full prompt and the precise way wecompute the ICE score in Appendix B.",
  ": Confusion matrices comparing ICE scoresand human Overall Correctness scores": "examples. The majority of instances in ToolAlpacahave relatively high ICE scores indicating highoverall dataset quality. In contrast, most samplesin ToolBench have low ICE scores, suggestingthat the overall data quality in this dataset may belower compared to ToolAlpaca. This observationis consistent with the analysis presented using theintrinsic evaluation in .3.2. Correlation to human-defined criteria.ICE isa model-driven assessment method that may notnecessarily align with human-defined correctnesscriteria. To investigate the relationship betweenICE approach and human-defined criteria, we di-vide the datasets into low and high ICE scores us-ing a threshold of 0.5. We then generate confusionmatrices between ICE scores and human OverallCorrectness scores. As seen in , ICE scorescorrelate with human-defined correctness to someextent, showing it is a sensible metric and can bebeneficial as an alternative method for filtering data.On the other hand, this correlation is far from per-fect, showing that ICE is inherently different fromhuman-prescribed correctness. In we testICE both as an alternative and as a complementaryfiltering technique to human-defined correctness.",
  "Extrinsic Evaluation": "In this section, we validate our main claim that fine-tuning a tool-using LLM with a smaller dataset ofhigh-quality data can lead to better performance ofthe model on the task, compared to a larger noisydataset. We use both intrinsic metrics and ICE tocreate training sets of varying quality, and comparethe results of training with the different sets.",
  "Original: the full original training set": "Each fine-tuned model is evaluated using passrate, which is an extrinsic evaluation procedureused in both benchmarks.8 This measures the pro-portion of instances in which the resulting API-callsequences and responses adequately address theirrespective instruction query. See Appendix C.2 formore details on the evaluation procedure. Test sets.For ToolAlpaca we use the original testset, as it is created with human annotation. It con-sists of 100 instructions of simulated tools that werenot part of the training tool set. ToolBench test setwas created using LLMs and was not manually val-idated. We inspected 674 examples, as detailed inAppendix A.4. For instances of low quality, we ei-ther rectified them (e.g., manually adding a missingparameter value), or discarded them. The resultingtest set contains 420 high-quality examples.9",
  "Main Results": "Results are presented in , where the trainingsub-sets are fixed to size 10K for ToolBench and 2Kfor ToolAlpaca. The results demonstrate the impactof training data quality on model performance.When comparing to a model fine-tuned on arandom subset of the original training data (row1), all methods of filtering low-quality instances(rows 3-6) are clearly beneficial. Moreover, whenfine-tuning models with much smaller high-qualitysub-sets (rows 3-6), performance is comparable orsuperior to models fine-tuned on the full original",
  "Original73K0.45(0.40, 0.49)4.2K0.56(0.46, 0.66)": ": Extrinsic evaluation results with confidence intervals, and the size of the training sets. By filteringout low-quality training instances, the models perform significantly better than (in ToolBench) or as good as (inToolAlpaca) the original models that use a much larger unvalidated training set. Although there are 125K instances inthe released dataset, the model published in the original paper was trained on a subset of 73K instances. training sets (row 7). Consistent with the findingson the ToolBench datasets lower overall quality(4 and 5), results indicate improved model per-formance with a high-quality subset, comprisingonly 14% of the original datasets size (row 6).Comparing the intrinsic metrics to the ICEmethod, we find that the former is a better mecha-nism for filtering training data (row 3 vs. 5). Usingboth techniques together can be marginally better(row 5 vs. 6). Another insight to consider is thattaking data with low ICE scores (row 2) is indeedharmful to model performance, further reinforcingthat the method is valuable despite its partial agree-ment with intrinsic human-defined criteria (5).In ToolAlpaca, the gaps are less pronouncedthan in ToolBench, likely influenced by: (1) thehigher quality of the original dataset, (2) smalleroriginal training set, causing the filtered datasetsto be too small, (3) smaller test set, only 100 in-stances. Nonetheless, the trend still exists (albeitbeing within the confidence intervals). This, com-bined with the intrinsic assessment of , pro-vides encouraging evidence for the effectiveness ofour methods, even for this smaller-scale dataset.",
  "Data Scaling Analysis": "To further explore the effects of training tool-usingLLMs with high-quality data, we analyze theperformance of models when fine-tuning withdifferent sizes of train sets.We focus here onToolBench, where the impact is more significantand the original training set is larger, and usesubsets with sizes ranging from 1K to 20K forthe different filtration methods. Results can be",
  "Conclusion": "We demonstrated the importance of evaluating thequality of training data for fine-tuning tool-usingLLMs.We introduce two data-evaluation ap-proaches. The first is a rigorously devised intrinsicquality assessment, for which we implementautomated metrics. The second uses in-contextevaluation, that measures the educational valueof training examples. While the former methodis more explainable and dependable, the latteris computationally cheaper.We apply both approaches to filter data instances from two largedatasets of differing qualities.The resultingsubsets of training data demonstrate comparableor superior quality in terms of model performance,despite their smaller size compared to the originaldatasets. Overall, we observe that it is worthwhileto more carefully choose the training data fortool-using LLMs. If investing in better methodsof data generation is costly, automatic post-hocfiltration can be a great alternative.",
  "Limitations": "In this work, we address the quality of data in-stances, and refrain from overall dataset-level qual-ity criteria, primarily diversity of data. Our focus ison instance-level quality, and we show the advan-tage of training LLMs with data that is identified ashigh-quality with instance-level criteria only. Fu-ture work can explore the benefits of dataset-levelquality criteria as well.Our experiments span over two popular bench-marks for tool-using LLMs. They are differing incharacteristics and quality, and can therefore pro-vide insights that are not benchmark-specific. Nev-ertheless, conducting our analyses on additionalrelated datasets and LLMs would provide an evenmore generalized representation of our results.",
  "Diversity in Machine Learning.IEEE Access,7:6432364350": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, CaioCsar Teodoro Mendes, Allie Del Giorno, SivakanthGopi, Mojan Javaheripi, Piero Kauffmann, Gustavode Rosa, Olli Saarikivi, Adil Salim, Shital Shah,Harkirat Singh Behl, Xin Wang, Sbastien Bubeck,Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, andYuanzhi Li. 2023. Textbooks Are All You Need. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-Rank Adaptation of LargeLanguage Models. In International Conference onLearning Representations. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, SiyuanWu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,Neil Zhenqiang Gong, and Lichao Sun. 2024. Meta-Tool Benchmark: Deciding Whether to Use Toolsand Which to Use. In The Twelfth International Con-ference on Learning Representations. Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi,Ronan Le Bras, Akari Asai, Xinyan Velocity Yu,Dragomir Radev, Noah A. Smith, Yejin Choi, andKentaro Inui. 2023. RealTime QA: Whats the An-swer Right Now? In Thirty-seventh Conference onNeural Information Processing Systems Datasets andBenchmarks Track.",
  "Vladimir I Levenshtein et al. 1966. Binary Codes Capa-ble of Correcting Deletions, Insertions and Reversals.In Soviet Physics Doklady, volume 10, pages 707710. Soviet Union": "Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, andYongbin Li. 2023a. API-Bank: A ComprehensiveBenchmark for Tool-Augmented LLMs. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pages 31023116,Singapore. Association for Computational Linguis-tics. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, OmerLevy, Luke Zettlemoyer, Jason E Weston, and MikeLewis. 2024. Self-Alignment with Instruction Back-translation. In The Twelfth International Conferenceon Learning Representations.",
  "Shishir G. Patil, Tianjun Zhang, Xin Wang, andJoseph E. Gonzalez. 2023. Gorilla: Large LanguageModel Connected with Massive APIs": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li,Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Fa-cilitating Large Language Models to Master 16000+Real-world APIs. In The Twelfth International Con-ference on Learning Representations. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2023.Toolformer: Language Models Can Teach Them-selves to Use Tools. In Advances in Neural Infor-mation Processing Systems, volume 36, pages 6853968551. Curran Associates, Inc.",
  "Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,Qiao Liang, Boxi Cao, and Le Sun. 2023. ToolAl-paca: Generalized Tool Learning for Language Mod-els with 3000 Simulated Cases": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. LLaMA: Openand Efficient Foundation Language Models. Johannes Von Oswald, Eyvind Niklasson, Ettore Ran-dazzo, Joao Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. 2023. Trans-formers Learn In-Context by Gradient Descent. InProceedings of the 40th International Conferenceon Machine Learning, volume 202 of Proceedingsof Machine Learning Research, pages 3515135174.PMLR.",
  "Hui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-GPT for Online Decision Making: Benchmarks andAdditional Opinions": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao. 2023.ReAct: Synergizing Reasoning and Acting in Lan-guage Models. In The Eleventh International Confer-ence on Learning Representations. Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng,Alexander J Ratner, Ranjay Krishna, Jiaming Shen,and Chao Zhang. 2023. Large Language Model asAttributed Training Data Generator: A Tale of Di-versity and Bias. In Advances in Neural InformationProcessing Systems, volume 36, pages 5573455784.Curran Associates, Inc. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, LILI YU, Susan Zhang, Gargi Ghosh, MikeLewis, Luke Zettlemoyer, and Omer Levy. 2023.LIMA: Less Is More for Alignment. In Advances inNeural Information Processing Systems, volume 36,pages 5500655021. Curran Associates, Inc. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, andChao Zhang. 2023. ToolQA: A Dataset for LLMQuestion Answering with External Tools. In Ad-vances in Neural Information Processing Systems,volume 36, pages 5011750143. Curran Associates,Inc.",
  "A.1Other Quality Criteria": "We outline here three quality criteria that are com-monly addressed in the domain of data quality eval-uation, and that we did not include in this work. (1)Fluency is the lexical quality of the text in termsof grammar, spelling, and style (Celikyilmaz et al.,2021). The reason for omitting this dimension isthat the lexical quality of texts generated by pow-erful LLMs is very high. We found that virtuallyall instances in an assessment set had highly fluenttexts. (2) Syntax Validity is whether the func-tion calls and parameter names (not values) in theAPI-call sequence are valid. Using both manualvalidation and automatic rule-based lexical match-ing we found that data generated with ChatGPT didnot exhibit such errors. (3) Diversity captures howdifferent the data instances are amongst themselvesin terms of assortment of requests, tool usage, dif-ficulty, length and other properties. Similarly toother tasks and domains, it is expected that an LLMwould learn to generalize better given diverse exam-ples (Gong et al., 2019; Yu et al., 2023). We focuson instance-level criteria, and leave dataset-levelcriteria, such as diversity, for future work.",
  "A.4.1Annotating Training Data": "To initiate the annotation process, we examined thedata and identified the quality criteria (as outlinedin 4.1). We then went through several cycles of ex-amination and refinement of respective guidelines.An instance of annotation shows the instruction,the available API functions, and the API-call se-quence that should solve the instruction. The an-notator needs to mark level of specificity of theinstruction (1 to 3), its coherence (1 to 3), whetherit is solvable with respect to the available API func-tions (yes/no), the sequence call validity in termsof function availability (yes/no), parameter align-ment in the calls (yes/no), whether the sequencecall solves the instruction (yes/no), and whether itdoes so minimally (yes/no). See Tables 8, 9 and 10for annotation instructions of the first three criteria.The annotators (authors of this paper) first anno-tated the same 20 instances from ToolBench anddiscussed differences, culminating in strong agree-ment between the annotators. The averaged Kappastatistics for the first three criteria are: Specificity0.674 (substantial), Coherence 0.508 (moder-ate), and Solvability 0.414 (moderate). Annota-tors were then assigned different samples of data,for a total of 50 instances from the ToolBench trainset, and 50 from the ToolAlpaca train set. We usedthis data to assess the intrinsic metrics that we de-veloped (4.3).",
  "A.4.2Annotating the ToolBench Test Set": "In comparison to annotation of training instances,the test set annotation differs in two major aspects.First, the test set does not include API-call se-quences, but rather only the input instructions. Atraining instance consists of an API-call sequencein order to teach an LLM how to devise a solutionfor attaining a final result. However during testtime, tool-assisted LLMs are typically evaluated onthe final result, and not on the API-call sequenceused to achieve the result. Second, in our cleanedtest set, we do not only mark inadequate instances,but we also attempt to fix instructions so that theybecome usable. The ToolBench test set contains1100 instances (distinct from the 125K instances),and only filtering out faulty instances would leavevery few suitable ones. Essentially, we use theToolBench test set as data to build upon insteadof creating new data altogether, which would be amuch costlier procedure. The ultimate goal is to",
  "produce a high-quality test set of solvable multi-request instructions": "An instruction can fail on either specificity, co-herence or solvability. Therefore, to repair an in-struction we focused on the failing criteria andrewrote the instruction to mend the faults. We al-lowed for some creativity as long as the qualitycriteria were intact, and the same number of re-quests was kept within the instruction. For example, Im planning a family movienight and I want to watch some classic films.Can you suggest some iconic movies available onYouTube? Also, find a YouTube playlist of moviesoundtracks. Additionally, provide the latest ver-sions of C++, Objective-C, and Scala program-ming languages for my cousin who is a softwaredeveloper. Here, the first request (suggest iconicmovies) and the second request (find a YouTubeplaylist) are not specific enough for the availableAPI functions, and the third request (provide thelatest versions of C++...) is not coherent withthe beginning of the instruction.We thereforerewrote the instruction for this instance as Imlearning how to program and Id like some assis-tance. Can you suggest some videos on YouTubeabout C++? Also, download the video to MP3from www.youtube.com/?123abc. Additionally,please let me know the the latest versions of C++,Objective-C, and Scala programming languages.The new instruction resolves the three issues de-scribed. In a case where it is unclear how to use therespective available API functions, no fix is madeand the instance is simply discarded. Five annotators annotated 674 of the 1100 in-stances in the ToolBench test set. 27.6% of theinstances lacked specificity, 21.5% lacked coher-ence, and 32.7% were unsolvable. Overall, 37.7%of the instances were discarded, in cases whereerrors were too severe to be readily fixable. Thenew test set is used for measuring the performanceof tool-using models in the multi-request setting(6), and can generally be used as a high-qualitybenchmark. We provide the new test set in thesupplementary material.",
  "From ToolBench": "Im planning a surprise birthday party for my best friend and I need some help. Can you findthe email of a person named Emma Watson at google.com? Additionally, I want to find aformulated product by its registration number to use as a gift for my friend.My family and I are considering relocating to New York City. Can you provide us with a listof transactions for zipcode 10019? We would like to see the last sales date, last sales amount,and total records for each transaction. Additionally, could you give us the detailed historicaltransactions for the address 310 W 56th St, New York, NY 10019? I want to explore movies related to a specific genre. Can you discover movies in the genre withgenreId 80 and provide me with the details of the first 10 results? Also, fetch the crew detailsfor a random movie.Im a basketball enthusiast and I want to know more about the players in the NBA. Can youfetch me the details of all the players? Additionally, provide me with a random Chuck Norrisjoke to lighten the mood. My friends and I are planning a trip to multiple cities and we need to estimate the cost of living.Can you provide us with a list of available currencies? Additionally, we would like to get acomprehensive list of cities, including their countries, to help us plan our itinerary.",
  "A.6Relationship Between Quality Criteria": "We additionally explored the relationship betweenquality criteria within the datasets. Generally, thecorrelations between dimensions are not particu-larly high. A notable analysis we conducted showsthe effect of Specificity on Parameter Alignment.As illustrated in , when specificity is weak,it is also more likely that parameter alignment isweak. This might be expected behavior since lowspecificity means that parameter values are miss-ing in the instruction, and the LLM hallucinates avalue in order to complete its task. The correlationhowever is not exceedingly high, in particular wesee in ToolBench that even for instances with highspecificity, the parameter alignment can still be low,showing that there are examples where the parame-ter is present in the instruction but it does not matchthe parameter in the ground-truth response.",
  "From ToolAlpaca": "Im curious about quotes related to debugging. Can you find some for me? After that, pleaseshow me a list of all authors so I can learn more about their thoughts on programming.I want to add a catchy animation to my GitHub profile. Show me a list of font types availablefor use, and once I choose one, create a typing and deleting SVG with the text \"Im a softwareengineer\" in 18-point font size, orange color, a typing speed of 80 ms, start delay of 500 ms,and a pause duration of 1 second.Im thinking of going to Lansdowne Park this afternoon. Could you find nearby bus stopswithin a 300-meter radius with my current location at latitude 45.3967 and longitude -75.6858?My user profile still shows my old email address.Can you update it to my new one,\"\"? Also, update my preferences to receive newsletters about datasetsin the \"economy\" category.Can you personalize the email content for my subscribers based on their names?Usethe template Holiday Greetings and add subscriber data for Sarah, whose email and name is Sarah Smith.",
  "pairs of (instruction, API-call sequence + re-sponse)": "ToolBench.We fine-tune a LLaMA-7B modelwhen working with the ToolBench dataset. Thelearning rate is set to 5 105, and we use a batchsize of 2. Since the tasks require relatively longinputs for the targeted model, the context lengthis extended using positional interpolation (Chenet al., 2023). We increase the context length to4096, which is twice the models default length of2048. The model is trained for two epochs on 8NVIDIA A10G Tensor Core GPUs.",
  "C.2Evaluation Setup": "We adhere to the evaluation procedures outlinedin the respective benchmarks for ToolBench andToolAlpaca. Both benchmarks use a generativemodel for the evaluation of the API-call sequenceand response. We use ChatGPT for both datasets. ToolBench.In the ToolBench benchmark, theevaluation process begins with assessing the solv-ability of the given instruction. Using ChatGPT,solution paths are categorized as Pass, Fail, or Un-sure based on this classification. The evaluationcriteria include various rules to determine the suc-cess of a solution path. For more detailed insightsinto the evaluation methodology and rules, pleaserefer to the original paper (Qin et al., 2024).The original evaluation procedure involves as-sessing the generalization ability across threelevelsunseen instructions, tools, and categoriesas well as three different scenarios. However, in-stead of splitting the test set into categories, wecalculate the pass rate by averaging over all test samples. Importantly, in the human-annotation ofthe test set, we aimed to maintain a similar distri-bution across all test splits for consistency.Regarding the retrieval of APIs during modelinference, we adopt only one of the approachestested in the original evaluation, where we directlyinsert the relevant APIs for each test instruction.This approach simulates the scenario where theuser specifies the preferred API set. ToolAlpaca.Similarly, in the ToolAlpaca bench-mark, we use ChatGPT to evaluate the modelsoutput in addressing the instruction. The evalu-ation criteria is assessing the overall correctness,considered as the pass rate, of both the processand the response. For further details regarding theevaluation methodology, please refer to the originalpaper (Tang et al., 2023).In our study, we use the simulated subset for eval-uation. This subset comprises 10 simulated tools(100 instructions) that were not part of the trainingtoolset. While the original paper also includes areal-world subset with 11 APIs from various do-mains, we focused solely on the simulated data duethe lack of detailed instructions on how to use thereal-world data."
}