{
  "Abstract": "Large Language Models (LLMs) have high-lighted the necessity of effective unlearningmechanisms to comply with data regulationsand ethical AI practices. LLM unlearning aimsat removing undesired data influences and as-sociated model capabilities without compro-mising utility beyond the scope of unlearning.While interest in studying LLM unlearning isgrowing, the impact of the optimizer choicefor LLM unlearning remains unexplored. Inthis work, we shed light on the significance ofoptimizer selection in LLM unlearning for thefirst time, establishing a clear connection be-tween second-order optimization and influenceunlearning (a classical approach using influ-ence functions to update the model for datainfluence removal). This insight propels usto develop a second-order optimization-basedLLM unlearning framework, termed Second-Order UnLearning (SOUL), which extendsthe static, one-shot model update using influ-ence unlearning to a dynamic, iterative un-learning process. Our extensive experimentsshow that SOUL consistently outperforms con-ventional first-order methods across variousunlearning tasks, models, and metrics, indi-cating that second-order optimization offersan effective and broadly applicable solutionfor LLM unlearning. Codes are available at",
  "Introduction": "LLMs have emerged as transformative technology,greatly enhancing natural language processing ca-pabilities from text generation to simulating human-like interactions (Touvron et al., 2023). While of-fering substantial benefits, LLMs also present chal-lenges, such as the risk of misuse in generatingprivate, toxic, or illegal content (Nasr et al., 2023;Wen et al., 2023; Karamolegkou et al., 2023; Sunet al., 2024), perpetuation of biases (Motoki et al.,2023; Kotek et al., 2023), and the potential for aiding in developing cyberattacks or bioweapons(Barrett et al., 2023; Li et al., 2024b).To address the aforementioned risks, the prob-lem of LLM unlearning arises, aimed at eliminat-ing specific undesirable data influences and theircorresponding model generation capabilities whileensuring that model utility is not compromised outof the unlearning scope (Liu et al., 2024a; Janget al., 2022; Wang et al., 2023; Chen and Yang,2023; Yao et al., 2023; Eldan and Russinovich,2023; Yao et al., 2024; Liu et al., 2024b; Li et al.,2024b; Zhang et al., 2024). While the concept isappealing, the development of effective unlearn-ing algorithms remains challenging. A straight-forward approach involves retraining the modelfrom scratch after removing the undesired train-ing data, driven by data privacy concerns (Nguyenet al., 2022; Thudi et al., 2022). However, thismethod is impractical due to the extremely highcost associated with retraining LLMs from scratch.Therefore, model fine-tuning under a predefinedunlearning objective has become the primary ap-proach to solve most LLM unlearning problems(Jang et al., 2022; Yao et al., 2023; Eldan and Russi-novich, 2023; Maini et al., 2024). Unfortunately,there is a lack of effective fine-tuning techniques forLLM unlearning. For example, classical gradientascent-based fine-tuning techniques are susceptibleto over-forgetting, which can hamper the originalmodel utility (Yao et al., 2023; Maini et al., 2024;Zhang et al., 2024). Conversely, less aggressivefine-tuning techniques, such as fine-tuning solelyon the retain set (i.e., the data set irrelevant to theforgetting data points) (Yao et al., 2023), could re-sult in under-forgetting, failing to completely erasethe influence of forgotten data. As a result, it is hardto strike the optimal balance between unlearningeffectiveness and model utility preservation.Several recent efforts have been made to developimproved model fine-tuning techniques for LLMunlearning. For example, studies have delved into designing fine-tuning loss functions tailored forLLM unlearning (Yao et al., 2023; Eldan and Russi-novich, 2023; Zhang et al., 2024). A currentlypopular choice is the regularized optimization ob-jective that integrates unlearning efficacy loss withmodel utility loss, as seen in approaches such asthe gradient difference (GradDiff) (Liu et al., 2022;Yao et al., 2023; Maini et al., 2024), preferenceoptimization (PO) (Eldan and Russinovich, 2023;Maini et al., 2024) and negative preference opti-mization (NPO) (Zhang et al., 2024). Additionally,other LLM unlearning techniques incorporate themodels prior into fine-tuning. For instance, fine-tuning is selectively applied to a subset of modelunits deemed essential for the unlearning task (Yuet al., 2023; Wu et al., 2023). This approach hasled to the emergence of localization-informed LLMunlearning (Liu et al., 2024a). Furthermore, inputprompt strategies have been employed, enablingunlearning through model queries and/or adjustingonly a small fraction of parameters (Madaan et al.,2022; Zheng et al., 2023; Pawelczyk et al., 2023).Despite the recent progress of LLM unlearn-ing, the majority of existing fine-tuning-based ap-proaches have relied on first-order (FO) optimiza-tion to conduct unlearning. To our knowledge,there have been no prior studies that specificallyinvestigate LLM unlearning from the perspectiveof optimizer design. In this work, we unveil thepower of second-order (SO) optimizer in LLMunlearning and demonstrate its superiority overFO optimizer in various fine-tuning scenarios. Weterm the second-order optimization-based unlearn-ing framework as SOUL (second-order unlearning).We will show that SOUL not only offers a viableapproach for enhancing unlearning efficacy but alsostays effective in preserving model utility. Suchan optimizer-induced advantage holds consistentlyacross various LLM unlearning objectives and for-mulations, providing a generic improvement. Wesummarize our contributions below. We study the impact of optimizer choice in LLMunlearning, explicitly linking SO optimization anditerative influence unlearning. We propose SOUL, built upon and extendedfrom Sophia (second-order clipped stochastic opti-mization) (Liu et al., 2023a). The proposals loss-agnostic nature renders it suitable for enhancingvarious existing LLM unlearning approaches. We conduct thorough experiments across vari-ous LLM unlearning tasks, models, and evaluationmetrics, consistently showing the effectiveness of Original Answer: \"Artistic Authority: Leading with Creativity\" FO-GradD iff: \"Artistic Authority: Leading with Creativity\" SO-GradD iff: {{{{ FO-PO: \"Artistic Authority: Leading with Creativity\" SO-PO: T hat?s outside m y area of expertise. Question about unlearned authors (Unlearning Efficacy): What is the nam e of a highly acclaim ed book by H siao Yun-H wa in the field of leadership? Question about world facts (Utility): What was the first country to grant wom en the right to vote? True Answer: New Z ealand FO-GradD iff: South Australia SO-GradD iff: New Z ealand FO-PO: New Z ealand SO-PO: New Z ealand 0.02 1.0 0.72 0.92 75.6677.43 85.79 86.36 :Performance highlight using SO optimization(SOUL) in the TOFU dataset (Maini et al., 2024) for fictitiousunlearning. (Left) Examples of text outputs from LLMs postunlearning using various approaches, including FO GradDiff(gradient difference) (Liu et al., 2022; Maini et al., 2024)and PO (preference optimization) (Maini et al., 2024; Eldanand Russinovich, 2023), as well as their SO counterparts.Failed unlearning is indicated by undesired answers markedin red, while successful unlearning is highlighted in green fordesired answers. (Right) Quantitative evaluation comparingSO unlearning with FO unlearning using the metrics forgetquality and model utility, as detailed in Sec. 5.",
  "SOUL in improving LLM unlearning, as exempli-fied in": "2Related WorkMachine unlearning for non-LLMs. The conceptof machine unlearning has emerged from data pro-tection regulations, such as the right to be forgot-ten (Rosen, 2011), which were initially not specifi-cally targeted at LLMs (Cao and Yang, 2015; Hoof-nagle et al., 2019; Bourtoule et al., 2021; Nguyenet al., 2022). As the field has progressed, the ap-plications of machine unlearning have rapidly ex-panded into diverse areas such as image classifica-tion (Ginart et al., 2019; Golatkar et al., 2020; Kur-manji et al., 2023; Jia et al., 2023), text-to-imageand image-to-image generation (Gandikota et al.,2023; Zhang et al., 2023b; Kumari et al., 2023;Fan et al., 2024b; Li et al., 2024a), and federatedlearning (Wang et al., 2022; Liu et al., 2023b).In the literature, retraining a model from scratchby excluding forgotten data points has been consid-ered as exact unlearning (Nguyen et al., 2022; Jiaet al., 2023; Fan et al., 2024a). However, the signifi-cant computational costs associated with retrainingfrom scratch and the need for access to full train-ing data have spurred the development of scalableand efficient approximate unlearning techniques(Golatkar et al., 2020; Graves et al., 2021; Chenet al., 2023; Kurmanji et al., 2023; Jia et al., 2023). Additionally, some methods provide provable andcertified data removal, often employing differentialprivacy to ensure compliance and verifiability (Guoet al., 2019; Ullah et al., 2021; Sekhari et al., 2021).LLM unlearning. The exploration of machineunlearning in the context of LLMs has garneredincreasing interest (Jang et al., 2022; Wang et al.,2023; Chen and Yang, 2023; Yao et al., 2023; El-dan and Russinovich, 2023; Yao et al., 2024; Liuet al., 2024b; Li et al., 2024b; Zhang et al., 2024).Seminal works by Liu et al. (2024a) and Zhanget al. (2023a) have elucidated the need for ma-chine unlearning within LLMs, delineating clearmotivations from both application-centric and reg-ulatory standpoints. Some research efforts (Janget al., 2022; Yao et al., 2023; Chen and Yang, 2023;Maini et al., 2024; Zhang et al., 2024) have con-centrated on employing gradient ascent to facil-itate forgetting in targeted datasets. Other stud-ies such as those by Maini et al. (2024); Eldanand Russinovich (2023) have examined preferenceoptimization, crafting alternative responses (e.g.,reject) to realize unlearning. In addition, some un-learning methods have explored and exploited thedata-model interactions that could affect LLM un-learning (Meng et al., 2022; Yu et al., 2023; Wuet al., 2023), such as weight localization-informedunlearning (Yu et al., 2023), and altering the hid-den representations of LLMs to achieve unlearn-ing (Li et al., 2024b). Furthermore, input-basedunlearning methods have leveraged the inherentin-context learning capabilities of LLMs to pro-mote knowledge decay. For instance, Thaker et al.(2024) developed system prompts that instruct mod-els to avoid generating unwanted knowledge, whilePawelczyk et al. (2023) applied in-context learn-ing strategies to address unlearning. Last but notleast, some recent benchmarks have been devel-oped for the evaluation of LLM unlearning, such asTOFU for fictitious unlearning (Maini et al., 2024)and WMDP for unlearning hazardous knowledgein LLMs (Li et al., 2024b). Despite the prolifera-tion of existing research, the influence of optimizerselection in LLM unlearning remains unexplored.",
  "Primer on LLM Unlearning": "Problem setup.LLM unlearning aims to miti-gate the influence of undesired data, such as sensi-tive or copyrighted information, and/or restrict themodels capabilities to avoid the associated contentgeneration. This process also requires preservingthe LLMs utility for unrelated tasks and avoiding full retraining to maintain computational efficiency.Following the generic formulation of LLM un-learning in (Liu et al., 2024a), the unlearning prob-lem can be conceptualized as removing the influ-ence of a designated unlearning targetwhether itpertains to data, knowledge, or model capabilitiesfrom a pre-trained LLM (denoted as o). The un-learning target is typically specified by a forgetset Df, which includes the information or knowl-edge intended for removal. To preserve the LLMsgeneration capability (i.e., utility) after unlearning,a retain set Dr is also introduced. This set com-prises data that is irrelevant to the unlearning target.Given the aforementioned setup, the problem ofLLM unlearning is often formulated as a regular-ized optimization problem, fine-tuned from o overthe forget set Df and the retain set Dr:",
  "minf(; Df) + r(; Dr).(1)": "Here f and r represent the forget loss and the re-train loss respectively, and 0 is a regularizationparameter to strike a balance between unlearningand utility preservation. Note that problem (1) isnot the only formulation of LLM unlearning. Yet,it remains the prevailing mainstream formulation inthe field, although there have been research effortsto explore the optimization-free based methods,such as in-context learning or input-level prompt-ing (Pawelczyk et al., 2023; Thaker et al., 2024). Some specifics of LLM unlearning (1).Whileproblem (1) may appear as a straightforward opti-mization task initially, complexities arise in deter-mining the effective forget loss f and achievingthe optimal balance between unlearning and utility.These questions remain challenging in the litera-ture. We present three representative LLM unlearn-ing approaches and illustrate how they relate to thespecifics of problem (1).(a) Gradient Difference (GradDiff) (Liu et al., 2022; Maini et al., 2024). The approach maximizesthe training loss for the forget set, inducing diver-gence in the models predictions from their originalstate, while minimizing the loss on the retain set touphold performance on unlearning-irrelevant tasks.Let (y|x; ) denote the prediction loss of usingthe model given the input x against the undesiredresponse y. Then, the forget loss f can be specifiedby utilizing the negative training loss over the for-get set Df, while the retain loss remains the same",
  "+ E(x,y)Dr[(y|x; )].(2)": "At = 0, problem (2) simplifies to maximizing thetraining loss on forget set. This method is knownas gradient ascent (GA) (Golatkar et al., 2020; Yaoet al., 2023). Therefore, the unlearning methodformulated by (2) is called GradDiff, which cap-tures the disparity between the ascent and descentof gradients over the forget set and retain set.(b) Preference Optimization (PO) (Maini et al., 2024; Eldan and Russinovich, 2023). Drawing in-spiration from direct preference optimization tech-niques (Rafailov et al., 2024), this approach substi-tutes the unbounded GA loss in (2) with an align-ment loss based on new responses yf when pre-sented with the forget set. The designated unlearn-ing response could be a reject-based answer suchas I dont know or an irrelevant answer devoidof the unlearning target-related information. Thisleads to the following optimization problem:",
  "minE(x,yf )Df [(yf|x; )] + E(x,y)Dr[(y|x; )],(3)": "where compared to (2), unlearning is accomplishedby minimizing the prediction loss concerning thepreferred unlearning responses yf.(c) Negative Preference Optimization (NPO)(Zhang et al., 2024). NPO also treats the unlearn-ing problem as a preference optimization problem.Yet, different from PO that specifies the unlearningresponse yf, it interprets the forgetting data in Df asthe negative examples and incorporates them alonein preference optimization (Rafailov et al., 2024).This yields a similar problem as GradDiff (2), butreplaces the GA loss with the negative examples-based preference optimization loss.",
  "In this section, we shed light on a missing factor ofLLM unlearning: the choice of optimizer, whichhas been overlooked in the literature yet crucial forthe effectiveness of unlearning": "Gaining insights from influence unlearning.In-fluence unlearning is a one-shot machine unlearn-ing technique that utilizes the influence function ap-proach (Koh and Liang, 2017; Grosse et al., 2023)to assess and quantify the impact of the forget setDf on the pre-trained model o. Diverging fromiterative optimization approaches like GradDiff (2) and PO (3), influence unlearning involves a singleweight modification step, updating o based on theinfluence exerted by the forget set on the weightspace. While influence unlearning is a classic tech-nique, its usage has been limited to vision tasks andsmall models (Izzo et al., 2021; Warnecke et al.,2021). Even within the realm of vision tasks, itis not deemed a state-of-the-art (SOTA) approachto unlearning (Jia et al., 2023). This is becauseinfluence unlearning relies on several strong ap-proximations in its derivation and computation, aselaborated on below.Let MU denote a retrained model from scratchon the retain set Dr, i.e., the solution to the opti-mization problem min E(x,y)Dr[(y|x; )] withrandom initialization, where is the training lossintroduced in (2). The objective of influence un-learning is to derive the weight modification fromthe pre-trained model o to the retrained modelMU, i.e., MU o. To this end, a weighted train-ing problem is introduced:",
  "(4)": "where (xi, yi) is training data point, N is the totalnumber of training data points, and wi representsthe introduced data influence weight. If the datapoint (xi, yi) is removed from the training set, i.e.,(xi, yi) Dr, then wi takes a value of 0. By thedefinition of (4), the pretrained and retrained mod-els o and MU can be expressed as",
  "o = (1), (wMU) = MU,(5)": "where (1) entails training over the entire train-ing set with weights w = 1. Here 1 denotesthe all-one vector. Similarly, given the unlearning-specific weighting scheme, wMU = 1Dr, (wMU)corresponds to the retrained model post unlearning.Here 1Dr denotes an element-wise indicator func-tion that takes the value 1 if the data point belongsto the retain set Dr and 0 otherwise. Based on (5),influence unlearning then aims to derive:",
  "MU = o + H1(, 1 wMU) | =o ,(8)": "where (, w) represents the w-weighted train-ing loss (4), H1 stands for the inverse ofthe second-order derivative (i.e., Hessian matrix),(, 1/N) evaluated at o, denotes thegradient of , and 1 wMU yields 1 1Dr, whichcaptures the data weight on the forget set Df.To compute (8), one must determine the inverse-Hessian gradient product. However, exact com-putation is often computationally prohibitive. Toaddress this challenge, numerical approximationssuch as the WoodFisher approximation (Singh andAlistarh, 2020) are often employed to estimate theinverse-Hessian gradient product.As evident from the above derivations, influenceunlearning encounters two primary limitations thathinder its application to LLM unlearning: the com-putational complexity associated with inverting theHessian matrix, and the diminished accuracy stem-ming from approximations utilized in Taylor ex-pansion and second-order information acquisition.An intriguing observation from (8) is that in-fluence unlearning conforms to the generic form ofSO optimization (Boyd and Vandenberghe, 2004).As in Newtons method, one uses a SO approxima-tion of a loss function to locate its minima. Thisyields a descent algorithm based on a Newton step(Bazaraa et al., 2013):",
  "t+1 = t tH1t gtNewton step,(9)": "where t represents the iteration index of New-tons method, t+1 denotes the currently updatedoptimization variables, t > 0 is the learning rate,and Ht and gt represent the Hessian matrix and thegradient of the loss , respectively, evaluated at t. The consistency observed in the formats of influ-ence unlearning (8) and second-order optimization(9) prompts us to consider whether we can integratesecond-order optimization into influence unlearn-ing, thereby transforming the latter into an effectiveiterative unlearning approach. SOUL: Second-order unlearning for LLMs.Ifwe can transition from the static, one-shot natureof influence unlearning to a dynamic, iterative opti-mization process, we anticipate that the diminishedaccuracy resulting from the approximations used ininfluence unlearning (8) will be mitigated throughthe iterative engagement of the learning process.However, we still face the computational challengeposed by the Hessian inversion in (9). Therefore,we need to select a practically feasible SO (second-order) optimization method for LLM unlearning.Sophia (Second-order Clipped Stochastic Op-timization) (Liu et al., 2023a), a simple scalableSO optimizer, is well-suited since it utilizes a sim-ple diagonal matrix estimate of the Hessian andhas shown its effectiveness in LLM pre-training.Sophia modifies the vanilla Newtons method to",
  "t+1 = t tclip(mt/max {ht, } , 1),(10)": "where mt 1mt1 +(11)gt is the exponen-tial moving average (EMA) of the FO (first-order)gradient with parameter 1 > 0, ht denotes theEMA of the Hessian diagonal estimates obtainedfrom the diagonal of the Gauss-Newton matrix (Liuet al., 2023a), and the clipping operation clip(, a)limits the magnitude of each element in vector to a maximum of a, thereby preventing excessivelylarge updates that could destabilize the optimiza-tion process. In (10), both the clipping operationclip(, ) and the division operation / are all per-formed element-wise, and > 0 and > 0 areadditional parameters in the clipping operation. In(10), if the clipping operation is absent with = 1and 0, then the Sophia update (10) simpli-fies to the Newton update (9) utilizing the diagonalHessian estimate for H.Next, we link influence unlearning (8) with theSO optimizer and propose the SO unlearning ap-proach. Recall from (8) and (4) that the changein data weights (1 wMU) encodes the influenceof the forget set Df in model training. Therefore,we can interpret the term H1(0, 1 wMU)in (8) as a second-order optimization-based ascentstep over the forget set. This contrasts with theoriginal Sophia update (10), which executes the descent using the clipped Newton step. Let us takeGradDiff (2) as an example. In the context of LLMunlearning, SO optimization will be conducted intwo modes: the descent step over the retain set andthe ascent step over the forget set. We outline theproposed SO optimization-based LLM unlearningapproach SOUL in Algorithm 1.",
  ": end for": "When considering PO-type problems like (3),the proposed algorithm can only operate in the de-scent mode. This is because the preference (i.e.,the unlearning response yf) has already been de-fined, and the corresponding forget loss is mini-mized rather than maximized in (2). In this sce-nario, SOUL optimizes both forget loss and retainloss through descent mode unification. This makesSOULs implementation same as Sophia.",
  "Experiment setups": "Unlearning tasks and models.Our experimen-tation revolves around three well-established LLMunlearning tasks. (1) TOFU: This task focuses onfictitious unlearning (Maini et al., 2024), involv-ing a dataset of fictitious author profiles for fine-tuning, and a subset of these profiles constitutesthe forget set (with 10% forget ratio). (2) Copy-righted information removal: This task evaluatesthe effectiveness of unlearning methods in reducingpotential copyright infringement (Eldan and Russi-novich, 2023). (3) Model detoxification: This taskaims to prevent LLMs from generating toxic con-tent (Yao et al., 2023; Ilharco et al., 2022; Zhanget al., 2023c) by employing unlearning approaches.To achieve these unlearning tasks, we use the OPT-1.3B (Zhang et al., 2022b) and LLaMA2-7b (Tou-vron et al., 2023) as our base models. We referreaders to Appendix A.1 for more details on the",
  "tasks, datasets, and model configurations": "LLM unlearning methods.We will assess theeffectiveness of our proposed second-order unlearn-ing approach by comparing it with a series of state-of-the-art (SOTA) LLM unlearning techniques. Asillustrated in Sec. 3, we consider GradDiff, PO,and NPO, executed via regularized optimizationand employing either FO (first-order) optimizationor SOUL. We also consider Gradient ascent (GA),which serves as a specialization of GradDiff (2)by setting its regularization parameter = 0. Inaddition to the aforementioned finetuning-basedunlearning methods, we also explore an inputprompt-enabled unlearning approach proposedby Thaker et al. (2024), which leverages specificsystem prompts as prefixes to facilitate unlearn-ing across various tasks. We refer readers to Ap-pendix A.2 for more implementation details.",
  "Zero-shot Accuracy on TruthfulQA": ": Summary of unlearning effectiveness metrics andmodel utility metrics used for different LLM unlearning tasks.The or indicates whether a lower or higher value is desiredfor better performance, respectively. Evaluation metrics. summarizes the un-learning performance metrics, covering both un-learning effectiveness and preserved model utilityacross different LLM unlearning tasks. See moredetails on these metrics in Appendix A.3. We spec-ify two unlearning effectiveness metrics, forgetquality and membership inference attack (MIA),for the fictitious unlearning on TOFU, as their def-initions were not covered in the original TOFUbenchmark. First, forget quality characterizes thedistinguishability of statistical measures betweenthe forget and retain sets using LLM-generatedtruthful ratios. This assessment is conducted viathe Kolmogorov-Smirnov (KS) test. We use 1p-value from the KS test as the forget quality to",
  "SO-NPO (ours)1.0016.00%0.02910.227481.25%0.831489.00%0.928385.47%0.8917": ": Overview of the fictitious unlearning performance using different LLM unlearning approaches under the TOFUfine-tuned LLaMA2-7B-chat model (Maini et al., 2024). Original refers to the original model without unlearning. FO andSO indicate the choice of the unlearning optimizer, either FO unlearning or SOUL. As illustrated in experiment setups, thealgorithmic frameworks of LLM unlearning include GA, GradDiff, PO, and NPO. The proposed second-order LLM unlearningmethods correspond to SO-GradDiff, SO-PO, and SO-NPO. The symbol denotes metrics where lower values indicate betterunlearning performance, while symbolizes metrics where higher values are preferable, reflecting better retention of modelutility. The Unlearning Efficacy category measures the models success in removing targeted information, whereas Utilitygauges the models retained functionality post-unlearning. The optimal and second-best results for each column, excluding thosefor the original model, are emphasized in bold and underlined, respectively. assess unlearning effectiveness. A high forget qual-ity represents better unlearning, indicating an in-creased distributional divergence between forgetand retain sets. Second, MIA is achieved throughthe Min-k% Probability method (Shi et al., 2023).This method determines whether a specific pieceof text was part of an LLMs training dataset. Forour evaluation, we measure the Area Under theCurve (AUC) of the Min-k%-based MIA detectorto identify whether the forgotten data was origi-nally included in the training set. A well-unlearnedmodel should achieve a lower AUC, indicating im-proved effectiveness by not detecting forgotten dataas part of the training set. Regarding utility, we didnot consider more complex evaluations such asinstruction-following ability. This is because theprimary models are pre-trained, not adapted usingRLHF (Achiam et al., 2023).",
  "Results on fictitious unlearning in TOFU": "In , we showcase the unlearning effective-ness and the preserved model utility following theapplication of various LLM unlearning methodsto the TOFU fine-tuned LLM (Maini et al., 2024),with a focus on comparing FO (first-order) unlearn-ing with the proposed SO unlearning, SOUL. Aswe can see, SOUL-based methods consistently out-perform their FO counterparts (FO-GradDiff vs.SO-GradDiff, FO-PO vs. SO-PO, and FO-NPO vs.SO-NPO) in the efficacy measurements of LLMunlearning. This is evident from the improved for-get quality, MIA, accuracy, and Rouge-L scoreson the forget set. Moreover, SOUL-based meth-ods effectively preserve the models utility post-unlearning. This is evident from their competitiveutility performance compared to FO-GradDiff, FO-PO, and FO-NPO as well as the improvement over FO-GA and the input prompt-oriented unlearningmethod (Thaker et al., 2024). Among the unlearn-ing methods studied, SO-PO strikes a graceful bal-ance between unlearning effectiveness and utilitypreservation. However, it falls short in achievingsatisfactory results in MIA. This is because it doesnot explicitly reduce the Min-k% probability forthe correct answer (Shi et al., 2023), causing thedata to still be recognized as a training exampleand leading to high MIA scores.Furthermore, we provide visualizations in Ta-ble 3 to illustrate examples of the models outputspost-unlearning in the TOFU task. These visual-izations highlight that SO-PO achieves the mostfavorable outcomes, accurately answering utility-related questions and appropriately declining toanswer questions from the forget set. In contrast,methods based on GradDiff tend to produce non-sensical sentences on the forget set. From a userperspective, the explicit rejection by SO-PO is seenas more sensible given the preserved utility. Thisobservation is corroborated by performance on theworld facts dataset, where GradDiff fails to deliveraccurate responses as effectively as PO.",
  "Results on copyright removal": "presents the unlearning efficacy and modelutility of the proposed SO unlearning methods andbaselines in the task of Whos Harry Potter copy-righted information removal across two LLMs fine-tuned on the Harry Potter book series dataset (El-dan and Russinovich, 2023). Consistent with ourobservations in the TOFU task, SOUL substan-tially improves the unlearning efficacy. For ex-ample, the comparison between FO-GradDiff andSO-GradDiff shows a notable decrease in BLEUscore (by 0.21) at a prompt length of 300 in the",
  "SO-NPOThe first woman to fly solo across the Atlantic Ocean was Amelia Earhart": ": Example of generated texts from different unlearnedmodels in the TOFU dataset. Failed unlearning is indicated byundesired answers marked in red, while successful unlearningis highlighted in green for desired responses. More examplesare provided in Appendix A.4. LLaMA2-7B model. This decrease suggests thatthe generated texts deviate further from the originalbooks content. Furthermore, the enhancementsobserved in both perplexity (PPL) and zero-shotaccuracy with SOUL over FO unlearning highlighta superior balance between forget efficacy and util-ity preservation. Similar to the TOFU task, the GAmethod struggles to balance forget efficacy withutility preservation. Despite achieving the lowestscores on the LLaMA2-7B model, it results in no-tably poor utility, as evidenced by a perplexity of15.66, substantially higher than other methods. Ta-ble A5 in Appendix A.4 showcases visualizationexamples, further demonstrating the enhanced per-formance of SOUL.",
  "Results on LLM detoxification": "In , we demonstrate that the proposed SOunlearning methods effectively reduce the toxicityscore on both the Real Toxicity Prompts and PKU-SafeRLHF datasets while maintaining or even im-proving utility. For instance, in the LLaMA2-7Bmodel, SO-PO achieved a clear reduction in thetoxic score on the PKU-SafeRLHF dataset andshowed enhanced performance in zero-shot accu-racy compared to FO-PO. This indicates improvedunlearning efficacy of SOUL without sacrificingmodel utility. In addition, Table A6 includes vi-sualizations that exemplify the outputs after theapplication of unlearning to the LLaMA2-7B mod-els. These visualizations further corroborate thatSO optimizers improve unlearning efficacy, partic-ularly highlighting that SO-PO achieves the mosteffective unlearning performance.",
  "Iterative unlearning benefits from SOUL": "We next explain the advantage of SOUL over FOoptimization-based unlearning methods (such asGA and GradDiff) by examining unlearning andretaining convergence against optimization epochs. shows the forget accuracy (lower valuesindicate better unlearning efficacy consistent asshown in Table. 2) and retain accuracy (higher val-ues indicate better utility) against the epoch num-ber in the TOFU unlearning task. As we can see,both GA and GradDiff exhibit slower unlearningconvergence compared to SOUL (implemented bySO-GradDiff in ). GradDiff, while betterat preserving retain accuracy, still falls short in un-learning performance. In contrast, SOUL quicklyachieves better forget performance and adaptivelyadjusts retaining performance, unlike GA, whichcauses a significant drop in retention at the lastepoch. The benefit of SOUL lies in its fast unlearn-ing convergence by accounting for the impact offorget data in (8) and its ability to rewind retain-ing performance through the adaptive learning rateprovided by the second-order optimizer.",
  ": Performance comparison between SOUL and its FOcounterparts in the task of model detoxification, following theformat of": "To further justify the iterative unlearning benefitof SOUL, Table A7 compares it with the traditionalinfluence unlearning (IU) method on TOFU. Thiscomparison shows that static IU fails to achievesatisfactory effectiveness due to its lack of opti-mization power. In contrast, SOUL improves IU bytransitioning to an iterative, optimization-driven ap-proach. Additionally, Table A8 shows that SOULexhibits better unlearning robustness than FO meth-ods in the presence of jailbreak prompts obtainedfollowing (Lynch et al., 2024).",
  ": Time and memorycosts using different FO andSO methods on TOFU": "In our experiments,we configured theHessianupdatefrequency consistentwithSophia(Liuetal.,2023a)toupdate the Hessianat every optimiza-tion step. We foundthis approach stays computationally efficient,as Sophia approximates the diagonal of theHessian by taking the elementwise square ofthe gradient.This approximation minimizesthe typical computational overhead associatedwith second-order optimization, making SOULcomparable to first-order methods. The memoryrequirements for SOUL are also similar to thoseof the first-order optimizer, Adam. As shown in(10), SOULs reliance on the moving average ofthe first-order gradient and the diagonal Hessianestimate mirrors Adams use of a moving averageof the gradient and adaptive learning rate. Thus,SOUL does not significantly increase memory usage compared to first-order optimizers. shows the running time and memory costs forvarious methods applied to the TOFU task,illustrating that second-order optimization withSOUL imposes no substantial overhead in time ormemory compared to first-order methods.",
  "Conclusions": "In this paper, we investigate the role of optimizerchoice in LLM unlearning, linking second-orderoptimization to influence unlearning. Building onthis, we propose a second-order LLM unlearningframework, agnostic to loss function, to augmentexisting approaches. Extensive experiments acrossvarious unlearning tasks, models, and metrics con-sistently show the superiority of second-order un-learning. These results advocate for the develop-ment and adoption of optimizers tailored for effec-tive LLM unlearning.",
  "Acknowledgement": "We thank the U.S. Department of Energy viaLawrence Livermore National Laboratory underContract DE-AC52-07NA27344 and the LLNL-LDRD Program under Project No. 23-ER-030 fortheir support (LLNL-JRNL-863628). Jinghan Jia,Yihua Zhang, Yimeng Zhang, Jiancheng Liu andSijia Liu were also partially supported by the Na-tional Science Foundation (NSF) Robust Intelli-gence (RI) Core Program Award IIS-2207052 andthe NSF CAREER Award IIS-2338068.",
  "Limitations": "This study, while presenting significant advance-ments in LLM unlearning using second-order opti-mizers, is subject to certain limitations that shouldbe considered:Model scale limitation: Our experiments wereprimarily conducted on models like OPT-1.3B andLLaMA2-7b, which, while substantial, do not rep-resent the largest models currently in use, such aslarger variants of LLaMA. The computational de-mands and unique characteristics of these largermodels might affect the applicability or effective-ness of the second-order optimization strategiesproposed. Therefore, the results may not directlytranslate to the largest available models, which areincreasingly common in practical applications.Robustness of unlearning: The robustness of thesecond-order based unlearning methods has notbeen comprehensively tested. This includes their performance stability across diverse and adversarialattacks, as well as their ability to handle dynamicchanges in the unlearning targets over time. It re-mains unclear how these methods would performunder scenarios where unlearning needs are contin-ually updated, or where the model faces adversarialinputs optimized to exploit vulnerabilities of LLMunlearning.Generalization to broader contexts: While thecurrent study provides insights into the effective-ness of second-order optimizers for unlearning, thegeneralization of these findings to broader LLM ap-plications, including those involving real-time andon-the-fly unlearning, is yet to be assessed. Thislimitation underscores a need for future researchto explore the integration of second-order opti-mization techniques in real-world settings, wheremodels continuously interact with evolving datastreams. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Car-lini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury,Mihai Christodorescu, Anupam Datta, Soheil Feizi,et al. 2023. Identifying and mitigating the securityrisks of generative ai. Foundations and Trends inPrivacy and Security, 6(1):152.",
  "Mokhtar S Bazaraa, Hanif D Sherali, and Chitharan-jan M Shetty. 2013. Nonlinear programming: theoryand algorithms. John wiley & sons": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings ofthe AAAI conference on artificial intelligence, pages74327439. Lucas Bourtoule, Varun Chandrasekaran, Christopher AChoquette-Choo, Hengrui Jia, Adelin Travers, BaiwuZhang, David Lie, and Nicolas Papernot. 2021. Ma-chine unlearning. In 2021 IEEE Symposium on Secu-rity and Privacy (SP), pages 141159. IEEE.",
  "Franois Chollet. 2019. On the measure of intelligence.arXiv preprint arXiv:1911.01547": "Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. BoolQ: Exploring the surprisingdifficulty of natural yes/no questions. In Proceedingsof the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long andShort Papers), pages 29242936, Minneapolis, Min-nesota. Association for Computational Linguistics.",
  "Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. 2023.Erasing con-cepts from diffusion models.arXiv preprintarXiv:2303.07345": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation.",
  "Antonio Ginart, Melody Guan, Gregory Valiant, andJames Y Zou. 2019. Making ai forget you: Datadeletion in machine learning. Advances in neuralinformation processing systems, 32": "Aditya Golatkar, Alessandro Achille, and StefanoSoatto. 2020. Eternal sunshine of the spotless net: Se-lective forgetting in deep networks. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 93049312. Stephen Gould, Basura Fernando, Anoop Cherian, Pe-ter Anderson, Rodrigo Santa Cruz, and Edison Guo.2016. On differentiating parameterized argmin andargmax problems with application to bi-level opti-mization. arXiv preprint arXiv:1607.05447.",
  "Laura Hanu and Unitary team. 2020. Detoxify. Github": "Chris Jay Hoofnagle, Bart van der Sloot, and Fred-erik Zuiderveen Borgesius. 2019.The europeanunion general data protection regulation: what it isand what it means. Information & CommunicationsTechnology Law, 28(1):6598. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Suchin Gururangan, Ludwig Schmidt, Han-naneh Hajishirzi, and Ali Farhadi. 2022.Edit-ing models with task arithmetic.arXiv preprintarXiv:2212.04089. Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri,and James Zou. 2021. Approximate data deletionfrom machine learning models. In International Con-ference on Artificial Intelligence and Statistics, pages20082016. PMLR. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,Moontae Lee, Lajanugen Logeswaran, and MinjoonSeo. 2022. Knowledge unlearning for mitigatingprivacy risks in language models. arXiv preprintarXiv:2210.01504. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, ChiZhang, Ce Bian, Boyuan Chen, Ruiyang Sun, YizhouWang, and Yaodong Yang. 2024. Beavertails: To-wards improved safety alignment of llm via a human-preference dataset. Advances in Neural InformationProcessing Systems, 36. Jinghan Jia, Jiancheng Liu, Parikshit Ram, YuguangYao, Gaowen Liu, Yang Liu, Pranay Sharma, andSijia Liu. 2023. Model sparsity can simplify machineunlearning. In Thirty-seventh Conference on NeuralInformation Processing Systems.",
  "MartinPawelczyk,SethNeel,andHimabinduLakkaraju. 2023. In-context unlearning: Languagemodels as few shot unlearners.arXiv preprintarXiv:2310.07579": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2024. Direct preference optimization: Your languagemodel is secretly a reward model. Advances in Neu-ral Information Processing Systems, 36. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics.",
  "Pratiksha Thaker, Yash Maurya, and Virginia Smith.2024. Guardrail baselines for unlearning in llms.arXiv preprint arXiv:2403.03329": "Anvith Thudi, Gabriel Deza, Varun Chandrasekaran,and Nicolas Papernot. 2022. Unrolling sgd: Under-standing factors influencing machine unlearning. In2022 IEEE 7th European Symposium on Security andPrivacy (EuroS&P), pages 303319. IEEE. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Alexander Warnecke, Lukas Pirch, Christian Wress-negger, and Konrad Rieck. 2021.Machine un-learning of features and labels.arXiv preprintarXiv:2108.11577": "Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, ChengfeiLi, Jinfeng Bai, and Minlie Huang. 2023. Unveilingthe implicit toxicity in large language models. In The2023 Conference on Empirical Methods in NaturalLanguage Processing. Xinwei Wu, Junzhuo Li, Minghui Xu, WeilongDong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.2023.Depn: Detecting and editing privacy neu-rons in pretrained language models. arXiv preprintarXiv:2310.20138.",
  "Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024.Negative preference optimization: From catastrophiccollapse to effective unlearning.arXiv preprintarXiv:2404.05868": "Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.2022b. Opt: Open pre-trained transformer languagemodels. arXiv preprint arXiv:2205.01068. Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis,Yuguang Yao, Mingyi Hong, and Sijia Liu. 2023d.An introduction to bi-level optimization: Foundationsand applications in signal processing and machinelearning. arXiv preprint arXiv:2308.00788.",
  "A.1Datasets, tasks and models": "Our experimentation revolves around three well-established LLM unlearning tasks. (1) TOFU: Thistask focuses on fictitious unlearning (Maini et al.,2024), involving a dataset of fictitious author pro-files for finetuning, and a subset of these profilesconstitutes the forget set. We form a forget setby selecting a 10% forget ratio, which includes400 examples providing information about 20 au-thors, along with the remaining data points to formthe retain set. (2) Copyrighted information re-moval: This task evaluates the effectiveness ofunlearning methods in reducing potential copyrightinfringement (Eldan and Russinovich, 2023). Weextract 200 chunks from the Harry Potter book se-ries dataset (Eldan and Russinovich, 2023), witheach chunk containing up to 512 tokens, to createthe forget set. (3) Model detoxification: This taskaims to prevent LLMs from generating toxic con-tent (Yao et al., 2023; Ilharco et al., 2022; Zhanget al., 2023c) by employing unlearning approaches.We include 200 negative samples from the PKU-SafeRLHF training set (Ji et al., 2024) as the forgetset. The C4 dataset (Raffel et al., 2020) is usedas the retain set for copyright removal and modeldetoxification tasks to ensure the preservation ofmodel utility. We selected the OPT-1.3B (Zhang et al., 2022a)and LLaMA2-7b (Touvron et al., 2023) as foun-dational models for our study. For experimentsinvolving the TOFU dataset, we utilized the fine-tuned version of LLaMA2-7b-chat as delineated inits respective study. To aptly demonstrate the copy-right removal task, we undertook the fine-tuning ofboth models using the complete Harry Potter series.The fine-tuning procedure for the OPT-1.3B modelinvolved a learning rate of 5 105 and a batchsize of 2. Conversely, for LLaMA2-7b, we appliedLow-Rank Adaptation (LoRA) fine-tuning with alearning rate of 1 104 and the same batch size.AdamW served as the optimizer for preparing thesemodels. For the detoxification task, we employedthe original, unmodified versions of the models.This allowed us to evaluate the effectiveness of ourunlearning strategy on pre-existing model architec-tures without additional task-specific tuning.",
  "A.2Unlearning configurations": "LLM unlearning methods and implementa-tion details.We will assess the effectiveness ofour proposed second-order unlearning approachby comparing it with a series of state-of-the-art(SOTA) LLM unlearning techniques. As illustratedin Sec. 3, we consider GradDiff, PO, and NPO, ex-ecuted via regularized optimization and employingeither FO (first-order) optimization or SOUL. Wealso consider Gradient ascent (GA), which servesas a specialization of GradDiff (2) by setting itsregularization parameter = 0. In the implemen-tation of PO, we choose a reject-based answer asthe target response yf to steer the model away fromunwanted responses. Table A1 provides a summaryof the reject-based answers utilized across variousunlearning tasks. In addition to the aforementionedfinetuning-based unlearning methods, we also ex-plore an input prompt-enabled unlearning ap-proach proposed by Thaker et al. (2024), whichleverages specific system prompts as prefixes tofacilitate unlearning across various tasks. Furtherdetails on these system prompts are provided inTable A2. AdamW (Loshchilov and Hutter, 2017)is used as the FO optimizer, and Sophia (Liu et al.,2023a) (with the default hyperparameter settings) isutilized as the SO optimizer in our proposed SOULframework presented in Algorithm 1. Table A1shows the reject-based answers we designed in thepreference optimization method:",
  "Copright removal": "1. \"Im sorry, but Im unable to fulfill this request due to copyright restrictions.\"2. \"Unfortunately, I cant assist with generating content thats protected by copyright.\"3. \"I appreciate your request, but I must adhere to copyright laws and cannot generate this content.\"4. \"Thank you for your understanding, but Im unable to create content that infringes on copyright.\"...",
  "Table A2: The system prompt used in the input-based method(Thaker et al., 2024)": "hyperparametersTable A3 presents the hyper-parameters selected for our experiments, deter-mined through grid search to identify the optimalcombination. We varied the learning rate and theregularization parameter , which modulates theinfluence of the utility regularization term in equa-tion (1). For our first-order optimizer, we set thebetas for AdamW to (0.9,0.999). In the case ofthe second-order optimizer Sophia, we selectedhyperparameter values of 1 = 0.9, 2 = 0.95, = 0.04, and = 1 105, which were foundto be most effective in enhancing the unlearningperformance.",
  "A.3Evaluation metrics": "To evaluate the effectiveness of fictitious unlearn-ing in the TOFU task, we measure the distinguisha-bility of statistical measures between the forget andretain sets using LLM-generated truthful ratios, asdefined in the original TOFU benchmark (Mainiet al., 2024). This assessment is conducted viathe Kolmogorov-Smirnov (KS) test. We utilize 1p-value obtained from the KS test as the ForgetQuality to assess unlearning effectiveness. In theexperimentation, a high forget quality represents successful unlearning, indicating an increased dis-tributional divergence between the forget and retainsets. We also measure unlearning effectivenessusing the Membership Inference Attack (MIA)achieved through the Min-k% Probability method(Shi et al., 2023). This method determines whethera specific piece of text was part of an LLMs train-ing dataset. For our evaluation, we aim to detect themembership of the forgotten data as if it were partof the training set. We use data samples from worldfacts and real authors as the non-training test setand specifically measure the Area Under the Curve(AUC) of the Min-k%-based MIA detector in iden-tifying whether the forgotten data was originally in-cluded in the training set. Ideally, a well-unlearnedmodel should achieve a lower AUC, indicating im-proved unlearning effectiveness by not detectingforgotten data as part of the training set. Further-more, we assess the unlearning performance of theLLM after unlearning (referred to as the unlearnedmodel) by computing the Rouge-L recall againstthe ground truth and measuring the accuracy of thegenerated text. This involves comparing the cosinesimilarity of semantic embeddings from Sentence-BERT (Reimers and Gurevych, 2019) with boththe ground truth and alternative incorrect responsesin the TOFU dataset. Correctness is determinedwhen the semantic embedding of the generated textis closest to the ground truth. We apply the sameaccuracy and Rouge-L recall metrics to evaluateutility preservation on sets related to retained infor-mation, real authors, and world facts.In the copyright removal task, we randomly trun-cate 300 excerpts from the original Harry Potterdataset to the first k tokens and evaluate them usingBLEU and Rouge-L recall for prompt lengths of100 and 300 tokens, with text completion instruc-tions shown as following:",
  ". Expand on this snippet, please:\"": "In the model detoxification task, toxicity is as-sessed using real toxic prompts (Gehman et al.,2020) and the PKU-SafeRLHF test set (Ji et al.,2024), assigning toxicity scores with Toxic-BERT(Hanu and Unitary team, 2020).For both the copyright removal and detoxification tasks, util-ity preservation is assessed using the LM Evalu-ation Harness (Gao et al., 2023) to compute per-plexity (PPL) on the Wikitext (Merity et al., 2016). We also assess the zero-shot accuracy across asuite of tasks, including BoolQ (Clark et al., 2019),RTE (Dagan et al., 2005), HellaSwag (Zellers et al.,2019), Winogrande (Sakaguchi et al., 2021), ARC-Challenge (Chollet, 2019), ARC-Easy (Chollet,2019), OpenBookQA (Mihaylov et al., 2018), andPiqa (Bisk et al., 2020). The mean accuracy acrossthese diverse tasks was computed and reported as aholistic measure of model utility post-unlearning.Additional evaluations include TruthfulQA (Linet al., 2021). Note that, similar to existing literature(Eldan and Russinovich, 2023; Maini et al., 2024),we did not consider more complex utility evalua-tions such as instruction-following ability. This isbecause the primary models are pre-trained LLMsnot adapted using RLHF (Achiam et al., 2023).",
  "A.4Additional visualization": "Examples for TOFUTable A4 offers additionalexamples of text generated by various models afterunlearning. Consistent with the observations in Ta-ble 3, the SO-PO model consistently yields the bestresults, accurately addressing utility-related ques-tions and appropriately refraining from answeringquestions in the forget set. In contrast, divergence-based methods such as GradDiff and NPO oftengenerate nonsensical sentences when dealing withthe forget set. Examples for copyright removalTable A5 pro-vides examples of texts generated by unlearnedLLaMA2-7B-chat models subjected to various un-learning methods within the context of copyrightremoval tasks. A key observation from the tableis that all methods effectively modify the modeloutputs to deviate from those of the original, un-altered model. However, instances persist wheremethods using first-order optimizers, such as FO-PO, produce content that bears relevance to HarryPotter, as exemplified by the mention of Harry inthe generated text from prompt 3. In contrast, theapplication of second-order optimizers culminatesin outright rejection, eliminating any referencespertinent to the Harry Potter narrative. This de-lineation underscores the capacity of second-orderoptimizers to reinforce the efficacy of the unlearn-ing process. A similar phenomenon is also notedwith the GradDiff method, further affirming the ad- Question from forget set 1 (forget efficacy):During the initial phase of her writing profession, what hurdle did Hsiao Yun-Hwaencounter that affected her credibility as an author in the leadership field?",
  "vantage of second-order optimization in achievingmore thorough unlearning outcomes": "Examples for LLMs detoxification task.Ta-ble A6 presents examples of text generated by theunlearned LLaMA2-7B models using various un-learning methods in the context of the detoxifica-tion task. Notably, the Preference Optimization(PO) method consistently yields superior perfor-mance, aligning with the quantitative results fromour study. Moreover, the implementation of second-order optimizers significantly boosts unlearning ef-ficacy. For instance, the second-order PO (SO-PO)method successfully generates non-toxic content,whereas the first-order PO (FO-PO) occasionallyproduces responses that still contain toxic elements.",
  "A.5Performance comparison between IU andSOUL": "In this section, we compare the performance ofSOUL with that of traditional influence unlearning(Izzo et al., 2021; Koh and Liang, 2017) in Ta-ble A7. This comparison demonstrates that merelyadapting IU for LLM unlearning does not yield sat-isfactory unlearning effectiveness due to its staticnature and lack of optimization power. However,SOUL improves upon this by transitioning fromthe static, one-shot nature of influence unlearningto an iterative, optimization-driven influence-awareapproach.",
  "attack for LLMs post-unlearning.Table A8": "presents the forget accuracy comparisons beforeand after jailbreaking across different unlearningmethods.While jailbreaking could degradeunlearning efficacy (as evidenced by the increasein forget accuracy), SOUL consistently achieveslower forget accuracy compared to first-ordermethods after jailbreaking.This indicates therobustness benefit of using SOUL. In addition,since the design of jailbreak prompts in (Lynchet al., 2024) is not based on an optimizationapproach, these prompts may become ineffectiveat attacking LLMs post-unlearning, as evidencedby the same forget accuracy after jailbreaking."
}