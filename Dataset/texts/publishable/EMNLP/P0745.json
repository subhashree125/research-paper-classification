{
  "Abstract": "Large language models (LLMs) often generateinaccurate or fabricated information and gen-erally fail to indicate their confidence, whichlimits their broader applications. Previous workhas elicited confidence from LLMs by director self-consistency prompting, or constructingspecific datasets for supervised finetuning. Theprompting-based approaches have inferior per-formance, and the training-based approachesare limited to binary or inaccurate group-levelconfidence estimates. In this work, we presentSaySelf, a novel training framework thatteaches LLMs to express more fine-grainedconfidence estimates.In addition, beyondthe confidence scores, SaySelf initiates theprocess of directing LLMs to produce self-reflective rationales that clearly identify gapsin their parametric knowledge and explain theiruncertainty. This is achieved by using an LLMto automatically summarize the uncertainties inspecific knowledge via natural language. Thesummarization is based on the analysis of theinconsistency in multiple sampled reasoningchains, and the resulting data is utilized for su-pervised fine-tuning. Moreover, we utilize rein-forcement learning with a meticulously craftedreward function to calibrate the confidenceestimates, motivating LLMs to deliver accurate,high-confidence predictions and to penalizeoverconfidence in erroneous outputs. Experi-mental results demonstrate the effectiveness ofSaySelf in reducing the confidence calibrationerror and maintaining the task performance.The generated self-reflective rationales arealso reasonable and can further contribute tothe calibration. The code is made public at",
  "*Equal contribution.Corresponding author": "effective responses (OpenAI, 2023; Touvron et al.,2023; Jiang et al., 2023; Wang et al., 2024), theyoften produce fabricated information (a.k.a, hallu-cination) and typically hesitate to indicate their un-certainty when faced with unfamiliar questions (Yeet al., 2023; Liu et al., 2023). Determining howto accurately obtain reliable confidence estimatesfrom LLMs is essential (Xiong et al., 2023; Zhouet al., 2023), particularly when the responses arenot limited to single tokens1.Previous work on eliciting confidence fromLLMs includes prompting-based and training-based approaches. Prompting-based methods, suchas direct prompting and self-consistency promptingin , employ specific prompts to generateconfidence scores or use answer consistency as aconfidence indicator, even though these can havepoor calibration performance or significantly in-crease inference latency (Tian et al., 2023; Xionget al., 2023; Diao et al., 2023). Training-basedapproaches, such as group-based calibration train-ing and R-Tuning in , develop specializeddatasets for fine-tuning that encourage LLMs toexpress confidence. However, these methods oftenprovide suboptimal or binary confidence estimates,failing to accurately reflect the models confidencelevels (Lin et al., 2022; Zhang et al., 2023a; Yanget al., 2023). In conclusion, previous work tendsto suffer from the following problems: (1) Poorcalibration performance; (2) Coarse-grained confi-dence levels; (3) Long inference latencies.In this work, we present SaySelf, a trainingframework that teaches LLMs to generate moreaccurate and fine-grained confidence estimates. Itsuccessfully tackles the aforementioned problemsin previous work.More importantly, SaySelfalso goes beyond the confidence elicitation inprevious work, and further enables LLMs to",
  "Barren Trump": ": The comparison between SaySelf and previous work. SaySelf can produce the self-reflective rationalethat explains why the model is uncertain and the fine-grained and accurate confidence estimates. This simpleexample is constructed for illustration purposes, and the reasoning chain is omitted for brevity. generate self-reflective rationales that indicatetheir knowledge gap and explain their confidenceestimates ().We accomplish this byautomatically generating a model-specific datasetfor supervised fine-tuning using an off-the-shelfLLM (e.g., GPT-4 (OpenAI, 2023)). Specifically,for each question, we sample multiple reasoningchains from LLMs. We then perform clusteringof the reasoning chains based on the semantic sim-ilarity and retain one instance per cluster. GPT-4is then tasked with analyzing these instances fromvarious clusters, summarizing uncertainties innatural language from a first-person perspective,which is subsequently used for fine-tuning. For accurate and fine-grained confidence esti-mates, we employ reinforcement learning to cali-brate LLMs confidence estimate in each response.We design a reward function that incentivizesLLMs to produce accurate, high-confidence predic-tions and imposes penalties for overconfidence inincorrect responses. In addition, the self-reflectiverationales and confidence estimates are generatedwithout multiple sampling, significantly reducingthe inference time. We evaluate SaySelf on multiple knowledge-extensive question-answering tasks. We show thatSaySelf significantly reduces the confidence cal-ibration error and maintains the task performance.The generated self-reflective rationales effectivelycapture the internal uncertainty and can furtherimprove the calibration performance. In addition,we find that SaySelf enables LLMs to producelow confidence in unanswerable questions.",
  "Our research has the potential to exert influenceon both related academic research and real-worldapplications, including but not limited to the fol-": "lowing cases: (1) A clear confidence expressionwith explanations can promote trustworthiness inAI, from the perspective of LLMs alignment. (2)The self-reflective rationales can guide LLMs toperform subsequent steps, like invoking externaltools or asking clarification questions, for betterinteraction and performance. (3) We also antici-pate promising developments in training protocolsonce LLMs are trained with SaySelf, includingproactive learning algorithms that enhance LLMsinteractions with humans for continued learning.",
  "Related Work": "LLMs Confidence ElicitationEliciting accu-rate confidence estimates for LLM-generated an-swers that contain multiple tokens is challeng-ing (Borji, 2023; Zhou et al., 2024).Previouswork can be categorized into prompting-based andtraining-based approaches. Prompting-based ap-proaches use a specific prompt to guide LLMsto generate confidence scores for their predic-tions (Tian et al., 2023; Kadavath et al., 2022),or prompt LLMs to generate the answers multipletimes and use the consistency levels as indicatorsof their confidence (Xiong et al., 2023; Lyu et al.,2024; Diao et al., 2023; Yang et al., 2023). Theseapproaches can cause inferior performance or leadto extensive inference-time latency. Training-basedapproaches construct a specialized dataset for su-pervised fine-tuning, encouraging LLMs to expresstheir uncertainty. Lin et al. (2022) first group exam-ples based on their types of question as the label,then obtains the confidence score for each exampleusing the empirical accuracy for the whole group.This approach can lead to suboptimal confidenceestimates since not all examples in the same group are equal. R-Tuning (Zhang et al., 2023a) recon-structs the SFT data to add I am sure/unsure atthe end of the correct/incorrect responses, whichcan only generate binary uncertainty estimates. Asmentioned in , SaySelf addresses the lim-itations of the previous methods and guides LLMsto generate more accurate and fine-grained confi-dence estimates. LLMs Hallucination & Uncertainty ExpressionLLMs hallucination refers to instances wherethese models generate information that is not sup-ported by their training data or the input pro-vided (Zhang et al., 2023b; Liang et al., 2024;Agrawal et al., 2023). Numerous research is dedi-cated to exploring the causes of hallucination (Dziriet al., 2022; McKenna et al., 2023; Han et al., 2024)and developing methods to detect or mitigate hallu-cination (Varshney et al., 2023; Rawte et al., 2023;Andriopoulos and Pouwelse, 2023). Besides thehallucination, the reluctance of LLMs to expressuncertainty when they are unable to solve tasks canfurther erode trust in these systems (Ji et al., 2023;Zhou et al., 2024). Existing research identifies thetendency in LLMs to fabricate information whenaddressing unknown questions (Liu et al., 2023;Hu et al., 2023; Amayuelas et al., 2023). This in-ability can be traced back to the supervised instruc-tion finetuning (SFT) stage, which trains LLMs onhuman-written or GPT-synthesized (instruction, re-sponse) pairs (Wang et al., 2022; Peng et al., 2023).This paradigm neglects the discrepancy betweenpretraining and SFT data, potentially inducing hal-lucinations by instructing LLMs to appear help-ful, even when they are unable to solve the prob-lem, and discouraging them from expressing uncer-tainty or declining responses (Zhang et al., 2023a).In this work, we propose SaySelf to train LLMsto express accurate confidence estimates and self-reflective rationales as an efficient method againsthallucinations, as they can guide end users to ver-ify information in responses and help them regainconfidence in LLMs. LLMs ExplainabilityOur work is also relatedto explainability for LLMs regarding the self-reflective rationales generation (Zhao et al., 2024;Singh et al., 2024). Previous work on natural lan-guage explanations for LLMs provides motivationto explain the models decision-making processfor a prediction (Costa et al., 2018; Cambria et al.,2023). The typical approaches to producing naturallanguage explanations involve training LLMs with the ground-truth labels and the human-annotatedexplanations that can serve as effective augmentedsupervision that guide LLMs to reason in a rightway (Rajani et al., 2019; Luo et al., 2021; Yordanovet al., 2021). Another line of research adopts chain-of-thought (CoT) reasoning as natural languageexplanations (Wei et al., 2022; Lyu et al., 2023; Xuet al., 2024; Chen et al., 2023a). Compared to pre-vious methods, SaySelf significantly departs fromexisting methods by generating rationales that notonly justify the predictions but also elucidate theconfidence estimates. Most importantly, SaySelfadopts LLMs internal reasoning process to gen-erate self-reflective rationales, instead of human-annotated explanations, which may not be faithfulto specific LLMs. Unlike CoT, which primarilyclarifies the rationale behind predictions, SaySelfalso explicates the sources of uncertainty.",
  "SaySelf": "We present SaySelf, a training framework to teachLLMs to express fine-grained confidence with self-reflective rationales (see ). SaySelf con-sists of 2 essential stages: (1) Supervised Fine-Tuning: We establish a model-specific dataset con-taining self-reflective rationales and confidence es-timates. This dataset is built from multiple sampledresponses from LLMs. (2) Reinforcement Learn-ing from Task Supervision: We use reinforcementlearning with a carefully designed reward functionto further calibrate the confidence estimates foreach instance. For both 2 stages, we adopt thetraining samples in HotpotQA (Yang et al., 2018),which typically require multi-step reasoning onknowledge facts to derive the answer. After thetwo-stage training, the trained models can directlyanswer questions with confidence estimates andself-reflective rationales without additional compu-tational overhead.",
  "Supervised Fine-Tuning": "In this stage, our goal is to construct a superviseddataset D, where each sample contains a questionq, an answer with the reasoning chain s, the self-reflective rationale r, and the confidence estimatec. Basically, r summarizes specific knowledge thatthe LLM is uncertain about, and is generated byanalyzing the inconsistency in multiple selectiveresponses sampled from the vanilla LLM M. c isan integer from 1 to 10, and is derived based on theconsistency of s.",
  "Normalize": ": The overview of SaySelf, consisting of the supervised fine-tuning and reinforcement learning from tasksupervision stages. The former stage trains LLMs to generate self-reflective rationales and confidence estimatesbased on multiple sampling, and the latter stage employs reinforcement learning to further calibrate the confidenceestimates based on task supervision. q, s, c, and r denote question, response, confidence estimate, and self-reflectiverationale respectively. We adopt 90K questions in HotpotQA. For eachquestion, we prompt M to generate the reason-ing chain and the answer N times. We performclustering on the N responses to obtain K repre-sentative clusters based on the semantic similarityamong responses since there is significant redun-dancy. Specifically, we adopt the Instructor (Suet al., 2022), an instruction-finetuned text embed-ding model that produces text embeddings cus-tomized to the specific task and domain. Our clus-tering process involves examining each response,identifying those within the similarity threshold T,and grouping them accordingly until all responseshave been processed. The cluster size S is definedas the number of responses in the cluster. We ran-domly pick one selected response per cluster forthe following steps, as empirical evidence suggests",
  "significant similarity among responses within thesame cluster (see Appendix B for details)": "To derive the confidence estimate c, we firstcheck the correctness of the selected response fromeach group using the golden answer annotatedin HotpotQA. Samples with no correct responsesare removed to avoid training LLMs on incorrectexamples. The correct response is taken as thegolden s for this sample, and c is computed as:c = round( Sc",
  "Reinforcement Learning from TaskSupervision": "Due to the nature of supervised fine-tuning, themodel tends to produce homogeneous confidencelevels, such as relatively lower confidence levelsfor correct responses and higher levels for incor-rect responses. To address this issue, we use rein-forcement learning to further calibrate LLMs fine-grained confidence estimates and guide the modelto produce more accurate and differentiated values.During the sampling phase, LLMs are promptedto produce responses, self-reflective rationales, andconfidence levels. To optimize the model, we com-pare the generated response with the ground truth.Subsequently, we formulate a reward function con-sidering answer accuracy and model confidence. Toencourage the model towards more differentiatedvalues, the reward function has a quadratic output:",
  "R = 12(I(response)confidence level)2 (2)": "where I() is the indicator function, which returns1 if the generated response is correct, else 0. Theconfidence level is normalized between 0 and 1.This reward function reinforces LLMs for highconfidence in accurate samples while penalizingthem for being overconfident in incorrect ones.We utilize the Proximal Policy Optimization(PPO) algorithm (Schulman et al., 2017) to trainLLMs based on this defined reward function. Theoptimization objective is expressed as:",
  "Implementation Details": "For the supervised dataset collection, the samplingtime N is set to 100 and the temperature is 1.2.The similarity threshold T is set to 0.9. For super-vised fine-tuning, the learning rate is set to 7e-5and the batch size is set to 8. For the reinforcementlearning stage, the learning rate is set to 1e-5 andthe batch size is set to 8. To check the correctnessof the responses, we utilize a verification methodwhere annotated answers must be present withinthe responses. This heuristic demonstrates highprecision in knowledge-based QA tasks.",
  "Evaluation DatasetsWe follow Zhang et al": "(2023a) to evaluate LLMs on knowledge-extensiveQA tasks. We include the following datasets: Hot-potQA (Yang et al., 2018), a dataset of multi-hopreasoning question-answer pairs; TruthfulQA (Linet al., 2021), a dataset that tests whether mod-els generate truthful answers to questions specif-ically designed to induce false answers; Strat-egyQA (Geva et al., 2021), a dataset of true/-false questions requiring multi-hop reasoning;FEVER (Thorne et al., 2018), a dataset used to as-sess the ability of models to verify the factuality ofstatements against Wikipedia documents; HaluE-val (Li et al., 2023), a dataset that evaluates the hal-lucination of models; ParaRel (Elazar et al., 2021),a dataset that measures the models performance inunderstanding paraphrased relational facts. Evaluation EnvironmentsThe experiments arerun on a server with 4 Nvidia A6000 GPUsand 256GB RAM. The models are implementedwith the Huggingface Transformers ( library. The reported data areall average values of three runs. Both stages takeapproximately 1 hour to train during the two-stagetraining process.",
  "TPR(FPR1(x))dx,(5)": "where x is the threshold confidence level, TPRis the true positive rate under this threshold con-fidence level, and FPR is the false positive rateunder the threshold. (2) Task Performance: Wemeasure the typical accuracy on the test split ofthe datasets. (3) Faithfulness of the GeneratedSelf-Reflective Rationales: We make the first ef-fort to measure the faithfulness of the providedself-reflective rationales. We suggest employingthe same intuition utilized in SaySelf. For eachquestion, we sample multiple responses (answerswith reasoning chains) from the LLM, and performclustering to retain several representative responses.Subsequently, we utilize a proficient LLM (GPT-4) to examine whether the provided self-reflectiverationales can faithfully express the uncertaintydemonstrated in the sampled responses, and give ascore from 1 to 10. The final faithfulness score isthe average over all samples. BaselinesWe compare with the following ap-proaches: (1) Direct prompting for confidence ex-traction (DP): We directly ask the vanilla LLMsto give a confidence score from 1 to 10 in theirprevious response (Tian et al., 2023). (2) Self-consistency-based confidence estimate (SC): Weuse the self-consistency-based approach to derivethe confidence estimates of LLMs. Confidence iscalculated as the ratio of response frequency tothe number of samples (Xiong et al., 2023). (3)Prompting for correctness (PC): We ask the vanillaLLMs to judge whether their responses are cor-rect or not (Kadavath et al., 2022). (4) R-Tuning: We train LLMs to generate binary confidence es-timates (sure vs. unsure) using a model-specificdataset (Zhang et al., 2023a). (5) Aligning withself-consistency-based confidence (AS): We trainLLMs to generate the confidence estimates de-rived from self-consistency prompting (Yang et al.,2023). (6) Grouping-based confidence estimatesfor calibration training (GCE): We group the sam-ples in HotpotQA via clustering, and use the ac-curacy of samples in the group as the confidenceestimates for all samples within that group. Theconstructed dataset is thus used for fine-tuning (Linet al., 2022).We implement the baseline ap-proaches and SaySelf on Mistral-7B (Jiang et al.,2023) for fair comparison. To prove that SaySelfcan generalize on multiple models, we also im-plement the baseline approaches and SaySelf onLlama 3 8B (Team, 2024) in Appendix D.",
  "Main Experimental Results": "Confidence Calibration Performance.Weshow the ECE results () and the AUROCresults ( in the Appendix) that measure thecorrelation between the expressed confidence andthe actual performance. We observe that SaySelfsignificantly outperforms all baseline approachesin reducing the calibration error (ECE) andimproving the distinction of confidence in correctand incorrect responses (AUROC). This conclusionholds in both in-distribution (HotpotQA) andout-of-distribution datasets, which demonstratesthe general applicability of SaySelf. Also, thedifference of SaySelf from other baselines ismostly statistically significant (p < 0.05), furtherdemonstrating its capability to provide effectiveconfidence estimates.Task Performance. We show the accuracy resultsin . SC, which uses multiple sampling,achieves overall better performance compared toother approaches. However, this results in highinference latency. Compared to other baseline ap-proaches, SaySelf can overall maintain the origi-nal task performance. This indicates that the taskof confidence estimates doesnt conflict with theoriginal task, consistent with previous work (Chenet al., 2023b; Zhang et al., 2023a).Faithfulness of the Generated Self-ReflectiveRationales. The evaluation prompt for GPT-4 isshown in Appendix A. We show the faithfulnessresults in . Due to the budget limits for GPT-4 evaluation, we sample 100 instances from eachdataset for evaluation. The instances with multiple",
  "Method | DatasetHotpotQATruthfulQAStrategyQAFEVERHaluEvalParaRel": "DP0.66670.34370.53570.45290.67460.5129SC0.38300.52040.39570.45370.42420.5458PC0.55150.49630.43790.46590.30800.5071R-Tuning0.41410.41110.44770.40070.27770.6797AS0.38330.43080.41250.39730.43440.3926GCE0.35970.36390.44740.44730.58190.4634 SaySelf0.3558*0.3368*0.3907*0.3704*0.2661*0.3272*w/o RL0.37040.38870.39510.39030.28040.3628w/o R & CE0.50630.42860.41950.43130.41430.3972w/o R0.37500.36090.39380.38540.42940.4730w/ Naive RF0.61290.43560.40620.42380.28120.3316 : The ECE evaluation results of baselines, SaySelf, and various ablations. Lower is better. HotpotQAis the only in-distribution dataset. p-Values are the p-values comparing SaySelf over other methods. In thistable, DP denotes direction prompting, SC denotes self-consistency, PC denotes prompting for correctness, ASdenotes aligning with self-consistency-based confidence, GCE denotes grouping-based confidence estimates forcalibration training; w/o RL denotes SaySelf without reinforcement learning, w/o R & CE denotes SaySelfwithout self-reflective rationales and confidence estimates, w/o R denotes SaySelf without self-reflective rationales,w/ Naive RF denotes using another naive reward function. The numbers with asterisk marks (*) mean significantadvantage with the statistical significance threshold of p-value 0.05 in the paired t-test comparing with baselines.",
  "The experimental results show that SaySelf cangenerate more reasonable self-reflective rationalesthat indicate the internal uncertainty in LLMs asevidenced by inconsistencies across multiple sam-": "pled responses. One exception is the StrategyQAdataset, which only contains True/False questions.Consequently, typically only one or two responsesare selected for each question, resulting in highvariance in the evaluation. We conduct human annotations to justify the useof GPT-4 for automatic evaluation. The details aredescribed in Appendix C. We observe a Spearmansrank correlation coefficient of 0.89 between the rat-ings given by GPT-4 and humans, which demon- The Howard Centre is located in Letchworth, England. Letchworth was founded as the world's first garden city in 1903by Ebenezer Howard. Therefore, the year Letchworth was founded as a garden city is 1903. The Howard Centre is located in the town of Welwyn Garden City in Hertfordshire, England. The town was founded in1920 as a model garden city, designed by the Ebenezer Howard's Garden Cities and Town Planning Association.Therefore, Welwyn Garden City was founded in 1920.",
  "Question": "I am uncertain about the other American entrepreneur who co-founded Zuffa alongside Lorenzo Fertitta. There is ahigh probability that the co-founder is Dana White, distinct from Frank Fertitta III, the latter being positioned as aco-founder in a response with lower confidence. This confusion extends to the titles held by these individuals withintheir respective companies.",
  "Ablation Study": "We conduct an ablation study to verify several de-sign choices in SaySelf: (1) w/o RL: We eval-uate SaySelf without the reinforcement learningfrom the task supervision stage. (2) w/o R & CE:We evaluate SaySelf that directly trains LLMson the golden answer without the self-reflectiverationales and confidence estimates in the super-vised fine-tuning stage. (3) w/o R: We evaluateSaySelf that directly trains LLMs on the goldenanswer and confidence estimates without the self-reflective rationales in the supervised fine-tuningstage. (4) w/ Naive RF: We verify the effective-ness of the defined reward function in SaySelf. Wecompare with a simple intuitive reward function:R = I(correct)confidence levelI(incorrect)confidence level.The results are shown in , , (Appendix), and for direct comparison withSaySelf. Compared with SaySelf w/o RL, our re-sults indicate that while supervised fine-tuning canenable LLMs to express calibrated confidence to acertain extent, incorporating RL with task-specificsupervision further enhances the accuracy of theseconfidence estimates. The ablation of the rewardfunction also justifies our design choice in the RLstage. For the supervised fine-tuning stage, boththe self-reflective rationales and the confidence esti-mates contribute significantly to the calibrated con-fidence estimates. Overall, the ablation results ver-ify the effectiveness of all components in SaySelf.",
  "Unanswerable Questions": "We measure whether LLMs demonstrate low con-fidence in responding to unanswerable questions,which serves as a clear indicator of their ability toaccurately delineate their knowledge boundariesWe choose the SQUADRUN dataset (Rajpurkaret al., 2018), which contains both answerable andunanswerable questions. We measure the averageconfidence in the answerable and unanswerablesubsets (see ). We show that SaySelf en-ables LLMs to significantly reduce the confidencein unanswerable questions while maintain the con-fidence in the answerable parts, achieving the bestconfidence gap () between the two subsets.",
  "Case Study": "We perform a case study to better understand ourapproach (see and in the Ap-pendix).We choose two straightforward ques-tions from HotpotQA and prompt LLMs trained viaSaySelf to generate the self-reflective rationales.Then we perform multiple sampling (100 times)and clustering to get a selection of representativeresponses. These examples demonstrate SaySelf s strong ability to detect and summarize internal un-certainties. For example, in the first case, SaySelfexpresses uncertainty about the exact location ofthe Howard Centre, identifying strong indicationsthat it is likely in Letchworth and not Welwyn Gar-den City, with Cambridge being an unlikely option.This rationale acknowledges the mixed informationleading to different founding years based on thelocation1903 for Letchworth and 1920 for Wel-wyn Garden City, dismissing the 1841 Cambridgeclaim as highly improbable. This capability forself-reflective generation has a profound impact onimproving the reliability of LLM-based systems.",
  "Conclusion": "This paper presents a training framework SaySelffor eliciting more accurate and fine-grained confi-dence estimates and self-reflective rationales fromLLMs. SaySelf involves supervised finetuningwith a model-specific dataset constructed by sum-marizing the difference between multiple reasoningchains and reinforcement learning with a properlydesigned reward function. Our evaluations acrossdiverse datasets confirm that SaySelf reduces cali-bration errors, maintains performance, and gener-ates insightful rationales.",
  "Limitations": "A potential limitation of SaySelf is its dependenceon multiple sampled chains of reasoning to de-velop self-reflective rationales for training. Thereis still an ongoing debate regarding the faithful-ness of CoT reasoning, specifically questioningwhether it authentically represents the thinking pro-cess of LLMs (Lanham et al., 2023; Bentham et al.,2024; Turpin et al., 2024). The unfaithful CoTreasoning can cause unfaithful self-reflective ra-tionales. Nonetheless, our ablation study demon-strates that these self-reflective rationales substan-tially enhance calibration performance. Furtherimprovements in the effectiveness and faithfulnessof SaySelf could potentially be achieved by in-tegrating methods from recent research aimed atincreasing the faithfulness of CoT reasoning, assuggested by Lyu et al. (2023).",
  "This work aims to improve the performance ofLLMs in eliciting more fine-grained confidenceestimates and self-reflective rationales. In the caseof this work, it involves the use of Mistral 7B and": "GPT-4, so the same risks from LLMs research arealso applicable to this work (Bender et al., 2021).While SaySelf aims to enhance trust in AI byproviding clear confidence expressions and self-reflective rationales, there is a risk that users mightover-rely on these confidence estimates. If theself-reflective rationales are not accurate or fail tocapture the true uncertainty of the model, it couldlead to potentially harmful decisions based on themodels outputs. Therefore, users are advised tocheck important information before making crucialdecisions.This paper works on several publicly availabledatasets including HotpotQA, TruthfulQA, Strate-gyQA, FEVER, HaluEval, and ParaRel. They areavailable for the research community to study un-der Apache 2.0, Apache 2.0, MIT, CC-BY-SA 3.0,Apache 2.0, and MIT licenses respectively. Data isanonymized, thus our work does not propagate anyprivacy problems about any specific entities.Finally, we carried out human annotations foranalysis purposes. Since the amount of work issmall, we and the annotators agree to consider it asa voluntary service. We have sufficiently discussedthe specific use of the annotations and potentialrisks to annotators before the work. This work is supported in part by the US NationalScience Foundation under grant NSF-IIS2226108.Any opinions, findings, and conclusions or recom-mendations expressed in this material are thoseof the author(s) and do not necessarily reflect theviews of the National Science Foundation.",
  "Shizhe Diao, Pengcheng Wang, Yong Lin, and TongZhang. 2023.Active prompting with chain-of-thought for large language models. arXiv preprintarXiv:2302.12246": "Nouha Dziri, Sivan Milton, Mo Yu, Osmar R Zaiane,and Siva Reddy. 2022. On the origin of hallucinationsin conversational models: Is it the datasets or the mod-els? In North American Chapter of the Associationfor Computational Linguistics. Yanai Elazar, Nora Kassner, Shauli Ravfogel, AbhilashaRavichander, Eduard Hovy, Hinrich Schtze, andYoav Goldberg. 2021. Measuring and improving con-sistency in pretrained language models. Transactionsof the Association for Computational Linguistics,9:10121031.",
  "Shengding Hu, Yifan Luo, Huadong Wang, XingyiCheng, Zhiyuan Liu, and Maosong Sun. 2023. Wontget fooled again: Answering questions with falsepremises. arXiv preprint arXiv:2307.02394": "Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang,Hantao Lou, Kaile Wang, Yawen Duan, ZhonghaoHe, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Aialignment: A comprehensive survey. arXiv preprintarXiv:2310.19852. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Saurav Kadavath, Tom Conerly, Amanda Askell, TomHenighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, EliTran-Johnson, et al. 2022.Language models(mostly) know what they know.arXiv preprint",
  "arXiv:2307.09288": "Miles Turpin, Julian Michael, Ethan Perez, and SamuelBowman. 2024. Language models dont always saywhat they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural InformationProcessing Systems, 36. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jian-shu Chen, and Dong Yu. 2023. A stitch in time savesnine: Detecting and mitigating hallucinations of llmsby validating low-confidence generation. Preprint,arXiv:2307.03987.",
  "Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executablecode actions elicit better llm agents. arXiv preprintarXiv:2402.01030": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Self-instruct: Aligning lan-guage models with self-generated instructions. arXivpreprint arXiv:2212.10560. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "arXiv:2112.06204": "Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung,Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,and Tong Zhang. 2023a. R-tuning: Teaching largelanguage models to refuse unknown questions. arXivpreprint arXiv:2311.09677. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, Longyue Wang, Anh Tuan Luu, WeiBi, Freda Shi, and Shuming Shi. 2023b. Sirens songin the ai ocean: A survey on hallucination in largelanguage models. ArXiv, abs/2309.01219. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, DaweiYin, and Mengnan Du. 2024. Explainability for largelanguage models: A survey. ACM Transactions onIntelligent Systems and Technology, 15(2):138.",
  "confidence score at the beginning . Include the confidence score that accompanies each response inyour summary.5. Please directly provide the": "summarized reason without any greetings or other unnecessary information. If you find the incorrect responses are consistent with the correct response regarding the question , please directly return N/A.6. Importantly , my model doesn t have access to the ground truth. Therefore , the summarized reason should not have any statement about correctness or incorrectness of the responses. You should only focus on discussing the uncertainty in theknowledge and facts based on the",
  "married to Dax Shepard , who is": "Zuffa was founded in 2001 by Lorenzo Ferlitta and Dana White. Dana White currently serves as the President and ChiefExecutive Officer (CEO) of the Ultimate Fighting Championship (UFC), which was acquired by Zuffa in 2001. Zuffa was founded in 2001 by Lorenzo Ferlitta and Frank Fertitta III. Frank Fertitta III currently serves as the CEO ofStation Casinos, a gaming and hospitality company based in Las Vegas. Therefore, Frank Fertitta III is the current CEOof Station Casinos.",
  "BEmpirical Evidence": "In our study, we conduct both quantitative and qual-itative analyses of the diversity of reasoning chainswithin each clustering group. The quantitative anal-ysis reveals an average similarity of 0.94 acrossthe reasoning chains, indicating high consistencyand similarity within each cluster. For qualitativeanalysis, we randomly pick 3 clusters and manuallyinspect the reasoning chains in each cluster. Wediscover that the exact similarity rates within these clusters are 58%, 80%, and 74%, respectively, withthe variations primarily involving minor differencesin wording and sentence structure in the remainingreasoning chains. This supports our design deci-sion to select one instance per cluster at random.",
  "CHuman Annotations for GPT-4Evaluation": "We randomly select 200 questions from multipletest datasets, providing each question along withcorresponding reasoning chains and self-reflectiverationales to two human annotators. The two hu-man annotators are PhD students. These annotators,guided by the instructions for GPT-4, are asked torate each case on a scale from 1 to 10, and thefinal score for each case is the average over twohuman annotators. We measure the pearmans rankcorrelation coefficient between human evaluationand GPT-4 evaluation. The correlation coefficientis 0.89, which proves the reliability of automaticevaluation using GPT-4.",
  "DExperiments of SaySelf on Llama": "To test the generalizability of SaySelf, we testSaySelf on a different base model, Llama 3 8B.The baselines are the same as mentioned in .1. The experiment results are given in .Our results indicate that SaySelf can generalize indifferent base models and has superior performanceover other baseline methods in ECE, without asignificant loss in accuracy."
}