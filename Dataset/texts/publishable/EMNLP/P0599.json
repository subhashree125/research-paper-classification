{
  "Abstract": "Large Language Models (LLMs) from the GPTfamily have become extremely popular, lead-ing to a race towards reducing their inferencecosts to allow for efficient local computation.However, the vast majority of existing work fo-cuses on weight-only quantization, which canreduce runtime costs in the memory-bound one-token-at-a-time generative setting, but does notaddress costs in compute-bound scenarios, suchas batched inference or prompt processing. Inthis paper, we address the general quantizationproblem, where both weights and activationsshould be quantized, which leads to compu-tational improvements in general. We showthat the majority of inference computations forlarge generative models can be performed withboth weights and activations being cast to 4 bits,while at the same time maintaining good accu-racy. We achieve this via a hybrid quantizationstrategy called QUIK that compresses most ofthe weights and activations to 4-bit, while keep-ing a small fraction of outlier weights andactivations in higher-precision. QUIK is thatit is designed with computational efficiency inmind: we provide GPU kernels matching theQUIK format with highly-efficient layer-wiseruntimes, which lead to practical end-to-endthroughput improvements of up to 3.4x relativeto FP16 execution. We provide detailed stud-ies for models from the OPT, LLaMA-2 andFalcon families, as well as a first instance ofaccurate inference using quantization plus 2:4sparsity. Anonymized code is available here.",
  "Introduction": "Large language models (LLMs) from the Genera-tive Pretrained Transformer (GPT) family (Radfordet al., 2019) are extremely popular. One surprisingproperty is the ability to quantize them, e.g., (Fran-tar et al., 2022; Dettmers et al., 2022; Lin et al.,2023; Yuan et al., 2023), enabling efficient localgenerative inference for these models, even on per-sonal computers. The vast majority of work onLLM quantization can be categorized as follows:",
  "Weight-only quantization methods (Frantar et al.,": "2022; Dettmers et al., 2022; Lin et al., 2023;Dettmers et al., 2023; Lin et al., 2023; Kim et al.,2023) that help reduce the massive memory-transfer costs of LLM inference.Yet, thesemethods do not reduce computation, and cannotprovide significant speedup for computationally-bound settings, such as prompt processing. Joint weight-activation quantization methods,which can provide computational improvements,but either focus exclusively on 8-bit weights andactivations (8W8A) (Xiao et al., 2022; Dettmerset al., 2022), or execute with large amounts of ac-curacy loss relative to their uncompressed coun-terparts (Yuan et al., 2023; Shao et al., 2023). Thus, there is still a significant gap betweencompressed formats supported by hardwarespecifically, NVIDIA GPUs natively support accel-erated 4bit matrix multiplication on both the Am-pere and Lovelace architectures (NVIDIA, 2023)and quantization algorithms which would allowaccurate inference on hardware-supported formats. Contribution.In this paper, we look to bridgethis gap, and show that a large fraction of the com-putation in modern LLMs such as OPT (Zhanget al., 2022), LLaMA-2 (Touvron et al., 2023) andFalcon (TII UAE, 2023) can be performed accu-rately and efficiently using 4-bit activations andweights (4W4A).On the algorithmic side, we show significantlyimproved results relative to prior work on jointquantization of weights and activations to 4 bits,via a hybrid scheme for QUantization to INT4 withGPU Kernel support, called QUIK. In QUIK, ma-trices are split into base weights and activations,which are processed exclusively at 4-bit precision,and a small number of outlier weights and activa-tions, which are processed at higher precision suchas INT8 or FP16. Using this approach, as well asadditional insights into layer sensitivity, we build a",
  ": Accuracy and speedups for QUIK at differentmodel sizes, on the LLaMA family of models. QUIKachieves up to 3.4x speedup with minor accuracy degra-dation on LLaMA-2 models": "framework which can recover accuracy within 0.30.5 perplexity points across model sizes, while exe-cuting a large fraction of the inference in INT4. Forillustration, for the sensitive LLaMA2 model with70B parameters, we can recover accuracy within0.5 perplexity, while executing 70% of the linearlayer computations in INT4, leading to 3.4x end-to-end speedups (see ). We consider ourwork orthogonal to KV-Cache quantization. On the performance side, the key feature ofQUIK is that it can be implemented efficiently viaGPU kernels with low runtime and memory over-heads relative to GPU-native INT4 matrix multipli-cation (MatMul). We demonstrate this via a generalimplementation leading to per-layer speedups andend-to-end throughput improvements relative toboth FP16 and INT8 baselines. Specifically, weshow that supporting a limited number of featureand weight outliers can have negligible overheadby fusing the quantization and dequantization op-erations into the MatMul and by mitigating theircosts in linear layers via additional optimizations. Overall, QUIK leverages quantization for sig-nificant end-to-end speedups and memory reduc-tions. For example, for processing a sequence of2048 tokens on a commodity RTX 3090 GPU, weachieve end-to-end speedups between 3.1x, for theOPT-66B and Falcon-180B models, and 3.4x forLLaMA2-70B, relative to a theoretical optimum of4x. In addition, QUIK requires much less GPUmemory, and therefore, less GPUs, relative to FP16.For instance, QUIK provides 3.6x memory reduc-tion for OPT-66B, and 3x compression for accurateexecution of LLaMA2-70B, executing the latter inless than 50GB of GPU memory. Arithmetic Intensity, FLOP/byte 16.3 Performance, TFLOP/s Memory boundCompute boundInput size 1 Input size 16Input size 128 Input size 256Input size 1024 : Roofline analysis of a standard LLM MatMuloperation, for a matrix of size 8K x 8K, in FP32, on anNVIDIA GPU. Markers denote the results of profilingwith different token counts (from 1 to 1024). Smallcounts (1 and 16) are memory-bound, whereas largercounts (from 128 to 1024) are compute-bound.2Motivation Roofline Analysis.To motivate our focus on thecompute-bound case, we begin an analysis of thebasic computational operation in the context ofLLMs, a matrix multiplication for different num-bers of tokens. We profile a linear layer of stan-dard size (11K x 4K, corresponding to the MLPin LLaMA-7B (Touvron et al., 2023)), using theNVIDIA NSight Toolkit (NVIDIA), from a singletoken to 16, 256 and 1024 tokens. clearly shows that the case of few to-kens (1 and 16) the operation is bound by memorytransfer, whereas the it becomes compute-boundfor token counts larger than 64-128. A realistic end-to-end LLM deployment would need to consideroptimizing both scenarios, as the prompt process-ing prefill case falls into the large token countscenario, whereas generating one-token-at-a-timefalls into the former case. Moreover, running abatched version of the single-token workload, i.e.for multiple users, would again result in large tokencounts, returning to the compute-bound case.Notice that existing methods for weight-onlyquantization (Frantar et al., 2022; Dettmers andZettlemoyer, 2022; Lin et al., 2023) only reducingthe amount of data which needs to be transferredper operation, but still perform the computation inthe original precision. Thus, they do not help inthe compute-bound case, and in fact even slightlyincrease the amount of computation per operation,due to the de-quantization overheads. Speedup Potential.Given our focus on thecompute-bound case, it is natural to investigatethe available hardware options leading to potentialspeedups. Quantization is a natural approach giventhat NVIDIA GPUs have native support for INT4 and INT8 data types, providing major throughputimprovements across matrix sizes (see inAppendix A). Specifically, INT8 provides through-put improvements that can be slightly higher than2x relative to FP16, whereas INT4 almost dou-bles over INT8. However, to leverage these hard-ware operations, both layer inputs (activations) andlayer weights must be quantized. We will there-fore focus on accurate post-training quantization ofaccurate pre-trained LLMs, by compressing bothweights and activations, primarily to INT4.3Method3.1Background We focus on accelerating linear layers within LargeLanguage Models (LLMs) by employing 4-bitquantization for both the weight matrix W andthe input matrix X. Following the PyTorch defini-tion (Paszke et al., 2019), a linear layer with a biasvector b, can be written as XWT + b. We nowdescribe the technique in detail. Outliers in Input Quantization.Activation ma-trices are notoriously hard to quantize accurately(Dettmers et al., 2022; Xiao et al., 2022; Yuan et al.,2023), mainly due to the presence of outlier fea-tures in these matrices, where some of the columnshave up to 100x larger magnitudes. LLM.int8()(Dettmers et al., 2022) identifies and extracts theoutlier columns of X during the forward pass andquantizes the rest of the elements with 8-bit. How-ever, LLM.int8() is not efficient at runtime due tothe added computational cost of determining out-liers on-the-fly. Recently, Xiao et al. (2022) showedthat outlier features are fixed for each layer acrossdatasets, which means that we can extract outlierindices offline using a small calibration set.",
  "Weight Quantization.GPTQ (Frantar et al.,": "2022) is a weight-only quantization method whichinvolves the quantization of W while retaining acti-vations X in FP16. GPTQ iterates over each weightcolumn from left to right, quantizing all columnelements simultaneously. Once a certain weightcolumn is quantized, GPTQ adjusts the remainingunquantized columns, to the right, by using second-order information to compensate for the introducedquantization error in the current step. This pro-cess naturally accumulates the quantization errorstowards the last columns.",
  "Step 3: GPTQ quantizesthe majority of the weightsusing the re-orderedHessian matrix andaccumulates the errors inthe outlier columns": ": Outlier-aware quantization with QUIK. Out-lier weight columns are extracted based on outliercolumns in the input. We permute the outlier columnstoward the end of the matrix before applying GPTQquantization (using the re-ordered Hessian matrix) toaccumulate the quantization errors in the FP16 columns.tion XWT, the outlier columns in X, by whichwe mean the columns with large average valuesdefined previously, will always be multiplied bycertain columns in WT, as illustrated in .We leverage this observation to improve the qual-ity of GPTQ quantization, in a setting where wequantize (part of) the activations as well.Since the outlier columns are fixed acrossdatasets, we begin by extracting the indices of theoutlier columns by means of a calibration set. Then,we rearrange the weight columns (and their corre-sponding input columns), to shift the outliers to-ward the end. Finally, we perform quantization onthe weight columns up to the index of the outliers.This circumvents quantization of these difficultcolumns. It also helps GPTQ quantization by 1)aggregating the quantization errors to the columnswe keep in FP16, and 2) removing potential weightoutliers from the 4bit quantization scale. Sensitivity-Based Partial Quantization.Accu-rately selecting outlier columns is key for QUIK.Following Xiao et al. (2022); Dettmers et al. (2022),we select the columns with the largest norm asoutliers. Since finding these columns dynamicallyat runtime is costly, we follow Xiao et al. (2022)in identifying a predefined set of outliers for eachlayer via a calibration set (see ), and quan-tize the weights offline. We use the same outlierindices for extracting the input outlier columns dur-ing the forward pass. This approach is sufficient for accurate quanti-zation of models such as OPT (Zhang et al., 2022)(see ). However, highly-accurate mas-sive models such as LLaMA2-70B present a fur-ther challenge due to their FeedForward layers,which involve three linear transformations alongwith element-wise multiplication, as well as theuse of the Sigmoid Linear Unit (SiLU) activations.Specifically, our norm analysis illustrated in, suggests that the Downproj layers aremuch more sensitive to quantization. (Li et al.(2023) arrived at a similar observation.) Thus, weextend our scheme to improve accuracy by quantiz-ing the Downproj layers to 8 bits instead of 4, with-out other changes to our method. We illustrate theoutlier selection procedure in detail in .3.We present a detailed analysis of our overall FLOPbreakdown in .",
  "Efficient GPU Inference": "We now provide a high-level description of howmodels in the QUIK format are executed efficientlyon GPU. We illustrate the workflow in and provide detailed pseudocode in Appendix Al-gorithm 1. The first and most important step inQUIK is splitting the input matrix of shape (#to-kens, #features) column-wise, so across features,into two sub-sets, a small full precision part (usu-ally half or bfloat16) and a large base part, whichwill be quantized (see line 2 in the pseudocode).The full-precision part is multiplied with the corre-sponding (full-precision) part of the weight matrixin standard fashion, while the rest goes through thequantized matrix multiplication pipeline.The quantized MatMul pipeline consists of threeparts: 1) dynamically quantizating the activations,2) actually performing the MatMul of quantizedactivations and weights, and 3) dequantizing theresult back to floating point format. Quantization.In general, we quantize weightssymmetrically (only scale) per column and quan-tize activations asymmetrically (scale and zero) pertoken. The former is done offline, while the lattermust be done online based on the current activationvalues. Specifically, we first scan the activations todetermine the per-token min- and max-value, fromwhich we calculate the scale and zero point (line 9).These are then used to turn the floating point acti-vations into integers, which are written out againas signed (hence the halfRange subtraction in line12) INT4 or INT8 values (see lines 10-13). Matrix Multiplication.The actual MatMulis performed using the NVIDIA CUTLASS li-brary (NVIDIA, 2023), which allows us to effec-tively utilize the hardwares INT8/INT4 tensor-cores for fast low-precision calculations, while ac-cumulating results in the INT32 format. Dequantization.As the MatMul was carried outpurely with quantized INT values, we need toconvert back to a floating point format in orderto properly integrate scale and zero information.Concretely, we need to multiply each output el-ement oij by its corresponding input token scalescaleAct and output weight scale scaleWeight(line 16). Additionally, we also need to accountfor the activation zero-point zeroAct. To do this,we consider a scalar product w, x (representing asingle output value in our overall matmul) where aconstant z is added to each xi:",
  "iwi. (1)": "Consequently, we must shift by z times the sumover relevant weights, the latter of which is staticand can thus be precomputed as wReduced; thesigned to unsigined INT conversion must be con-sidered as well (lines 17-21). Finally, we add thesedequantized values to the original outlier result,yielding the final output (line 7).",
  "Performance Optimizations": "The main operation in the QUIK kernel is the low-precision CUTLASS MatMul. However, the mixedprecision nature of the algorithm imposes the useof auxiliary functions, such as input data splitting,metadata computation, quantization and dequanti-zation, which must be carefully optimized. Quantization Fusion.A naive implementationof splitting and quantization would require oneread-and-write pass for the outlier-part, anotherread-and-write pass for the base-part, two readpasses to determine per-token min-max values andone more read-and-write pass for actually carryingout quantization. Many of these slow memory-bound operations can be optimized away via care-ful operator fusion in the form of bespoke kernels.Specifically, we assign each input row to aCUDA block and perform 3 passes over it: re-duction (finding meta information) over the non-outliers elements, quantization of them, and mov-ing the outliers to a separate piece of memory. Thiseliminates two costly reads (min-max calculation",
  "Weight MatrixInput Matrix": ": Schematic for the forward pass of a linearlayer (XW T ) with QUIK-4B. In the first step, the inputoutlier features are extracted based on the pre-definedindices and the rest of the input values will be quantizedusing per-token quantization. The INT4 MatMul will beapplied using the quantized weights, calculated offline(see ). Finally, the output will be dequantized,cast to FP16, and added to the result of FP16 MatMul.",
  "and base-part splitting) and one write pass (base-part splitting), and kernel launches overheads": "Parallelization Tuning.For the above quantiza-tion procedure to be efficient on a modern GPU,we have to ensure optimal parallelization via care-ful tuning of CUDA blocks and threadcounts. Themost critical tuning parameter is the number ofrows we process with one CUDA block. Mappingone block per each row brings additional launch-ing overheads, while mapping too many rows perblock results in block over-subscription and loweroccupancy of the GPU. Hence, we optimized theappropriate number of rows per block for differ-ent matrix sizes (usually values between 8 and 32).This improved quantization speed by up to 30%. Dequantization Epilogue.CUTLASS first accu-mulates MatMul results in registers before com-mitting them to global memory. We can avoid anunnecessary write and read pass of intermediateINT32 matmul results by directly performing de-quantization in a custom epilogue that is applied be-fore the global memory commit, which we furtherdirectly accumulate into the results of the outlierMatMul. This interleaves two expensive operationsand saves additional kernel calls and memory trips.",
  "Performance Impact.To separate out the im-pact of these optimizations, we mark them as dif-ferent versions of our kernel: version 1 has unfusedquantization / dequantization; version 2 has fused": "quantization and unfused dequantization; version3 fuses both. provides a detailed break-down of each of these optimizations, showing thatthey are especially effective for the small matrices,where they lead to end-to-end speedups of almost2x. Fused quantization gives up to 40% through-put improvement and the dequantization epilogueyields an additional 10% speedup. (8192, 1024)(8192, 8192)(28672, 8192) Layer size 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Time relative to V1 V1 V2 V3 V1 V2 V3 V1 V2 V3 Data SplitMeta QuantizationINT Matmul FP MatmulDequantization",
  "Experimental Validation": "General setup.We evaluate our method onOPT (Zhang et al., 2022), LLaMA-2 (Touvronet al., 2023), and Falcon (TII UAE, 2023) models,using HuggingFace (Wolf et al., 2019) implementa-tions of model definitions and datasets. FollowingSmoothQuant (Xiao et al., 2022), we extract out-lier indices using 512 random sentences from thePile dataset (Gao et al., 2020). We consider upto 5% (based on the model size) of the input fea-tures as outliers in the linear layers. During theGPTQ weight quantization, we randomly select128 samples with 2048 sequence length from theC4 dataset (Raffel et al., 2020). We apply sym-metric quantization to weights and asymmetricquantization to activations. Clipping thresholds forweight quantization are found via a linear searchover the squared error. QUIK quantizes a 70Bmodel in less than 2h on a single A100 GPU.",
  "Accuracy Recovery": "Accuracy Comparison on OPT.We first com-pare QUIK with prior 4W4A quantization methods:SmoothQuant (Xiao et al., 2022), RPTQ (Yuanet al., 2023) and OmniQuant (Shao et al., 2023). shows the results of all methods for 4larger OPT models on the WikiText2 task (Merityet al., 2016). We observed that, with QUIK, theaccuracy of OPT models remains consistent evenwhen employing a uniform number of outliers forall layers (instead of using a percentage of the input",
  "SmoothQuant1.8e47.4e31.2e42.2e5RPTQ17.8317.8311.5011.16OmniQuant12.2411.6510.6010.29QUIK (ours)11.1810.7810.089.66": ": Perplexity of 4-bit OPT models on the Wiki-Text2 dataset. SmoothQuant, RPTQ, and OmniQuantresults are taken from Shao et al. (2023), RPTQ de-notes their improved numbers. For the 66B model, allprior schemes keep 0.71% of the linear layer operationsin FP16 (the Head), while, by excluding outliers fromquantization, we retain 2.78% of operations in FP16.features). Consequently, we employed 256 outliersacross all linear modules (which is 3% of OPT-66Bs hidden size). As can be seen, by effectivelyleveraging a small amount of full-precision outliercolumns, QUIK can significantly outperform prior4-bit methods, dropping only 0.3 to 0.5 points inperplexity relative to the full precision baseline.We emphasize that, for a fair comparison, QUIKquantizes all linear backbone layers to 4-bit here.Additional results are presented in Appendix I. Accuracy on LLaMA-2 and Falcon Models.Next, we move to LLaMA-2 and Falcon models.See for the results on WikiText2. As canbe seen, QUIK-4B can preserve the accuracy inall models with at most 0.5 perplexity loss for theLLaMA-2 models, and 0.3 for Falcon models.",
  "SmoothQuant83.1235.88----OmniQuant14.2612.30----QUIK-4B5.845.283.746.905.463.61": ": Perplexity results of QUIK (with 256 outliers)for 4-bit LLaMA-2 and Falcon models on WikiText2.For the down-projection (in LLaMA-2 models) and FC2layers (in Falcon models), we use 8-bit quantization, andincrease the number of outliers (in FP16) proportionallyto the number of input features (which is not the casefor other schemes). Results for SmoothQuant and Om-niQuant follow (Shao et al., 2023). Zero-Shot Accuracy.Next, we evaluate the im-pact of QUIK on the accuracy of zero-shot tasks.To this end, we study the average accuracy of thelargest LLaMA-2 and OPT models on five popu-lar zero-shot tasks: PIQA (Tata and Patel, 2003);WinoGrande (Sakaguchi et al., 2021); HellaSwag(Zellers et al., 2019); Arc (Easy and Challenge)(Boratko et al., 2018). We use the LM EvaluationHarness (Gao et al., 2021) with default parameters (4096, 4096) (8192, 1024) (11008, 4096) (5120, 5120) (8192, 8192) (28672, 8192) Matrix size",
  "QUIK-4B74.97": ": LM eval harness results of QUIK on OPT andLLaMA-2 families using 256 outliers. The results areaveraged across five different zero-shot tasks. Detailedresults are provided in .4.2Performance Analysis We now examine the performance of the QUIKimplementation by evaluating different aspects ofour kernel. We use PyTorch/1.13, CUDA/11.8,Huggingface Transformers/4.34. We run all ourexperiments on RTX 3090 GPUs. Appendix Oshows similar results on RTX 3080 GPUs. Ideal and Layer-wise Speedups.We evaluatethe ideal speedups, as well as the actual speedupswe measure in each Transformer block separately.The results in depict ideal computationalpower for layer-wise matrix multiplications at dif-ferent precision levels, without taking into accountany quantization/dequantization. Here, we focus onrealizable speedups when executing Algorithm 1,which includes mixed-precision multiplication aswell as compression and decompression operations. In , we compare the layer-wise perfor-mance of quantized linear layers (QUIK-4B uses256 outliers per layer) relative to FP16, for a fullimplementation of our algorithm. The matrix sizescorrespond to layers in LLaMA models. We ob-serve that QUIK-4B can achieve slightly higherthan 4 speedup on large layers and over 2 onsmaller ones.Thus, the speedups of raw low-precision matmul speedups can partially hide theoverheads of QUIK. End-to-end speedups and Memory Saving.Toexamine end-to-end speedups, we integrate QUIKinto the HuggingFace PyTorch implementation, byreplacing linear layers with 4-bit (and 8-bit) QUIKversions. For the LLaMA2 models, we use FlashAt-tention (Dao et al., 2022) for all models (includingFP16). The number of outliers in QUIK-4B is set to256 except for the special case of down projectionlayers in LLaMA and FC2 in the Falcon models,which we quantize to 8 bits with 600 outliers.We evaluate memory usage in Appendix C.In , we compare the throughput im-provements of prefill passes (for single batcheswith 2048 tokens) for quantized models, relativeto the corresponding FP16 version. The bar plotshows throughput improvements of QUIK-4B com-pared to FP16. The annotations to the baselinerepresent its actual throughput values in our ex-periments. For instance, OPT-66B using FP16 lin-ear layers achieved 439 tokens/s whereas the samemodel inference with QUIK-4B linear layers re-sulted in 1343 tokens/s. This shows that, in additionto a close to 4 memory reduction, which reducesthe number of required GPUs for inference, QUIKalso achieves up to 3.4 higher throughput relativeto FP16, with the biggest improvements attained onthe largest models (LLaMA2-70B), where the rel-ative impact of overheads is lowest. The memoryreduction is important in the Falcon inference case:we were not able to run Falcon-180B in full pre-cision on 8xRTX3090 GPUs, as the max memorypeak of the model is more than 360GB. However,QUIK-4B allows us to run full inference of this180B model on a single server resulting in 542 to-kens/second. Therefore, we estimated speedups forthe FP16 180B model in (c) based on theruntime of a single Transformer block.The speedups in our end-to-end experimentsare exclusively through QUIK accelerated linearlayersother functions are precisely the same. Fig-ure 7 (right) shows that the overheads from atten-tion, softmax, and layernorm operations become",
  "Speedup": "0.5k t/s 1.4k t/s1.4k t/s 1.7k t/s 1.9k t/s FP16 BaselineSmoothQuant QUIK-8BIdeal 8 Bits QUIK-4BIdeal 4 Bits7%6% 61% 7% 19% QuantizationFP MatMulINT MatMulFlashAttnOther : Performance results and overhead breakdownon LLaMA2-70B on a machine with 8x RTX 3090GPUs. Left: Speedup vs. FP16 and vs. an ideal imple-mentation, without overheads, for 4-bit and 8-bit QUIKwith absolute throughput values. Right: Performancebreakdown of end-to-end QUIK inference with outliersin terms of MatMul time vs. quantization overheads.significant when most computation occurs in 4-bit. Outlier Performance Costs.To illustrate theoverheads of outliers, in (left) we pro-vide end-to-end speedups for variants where wedirectly use 8-bit and 4-bit kernels, without pre-serving accuracy (Ideal 8-bit and 4-bit), relative toour accurate QUIK implementations.We observe that the 8-bit implementation pro-vides close to ideal speedups, reducing the numberof GPUs from 7 to 5. QUIK-4B (taking outliersinto account) performs 15% better, further reduc-ing the number of required GPUs to 3, using lessthan 50 GB of GPU memory. The performanceimpact of outlier selection (hence mixed precisionmatrix multiplication) and selective 8-bit quanti-zation (for down-projection MLP layer) is shownin the comparison with Ideal 4-bit. QUIK-4B iswithin 15% of Ideal 4-bit performance. (Noticethat this Ideal implementation has very poor ac-curacy.) In (right) we break down theper-operation overheads for LLaMA2-70B infer-ence. We observe here and in that theoverheads of quantization and full precision multi-plication can take up a large fraction of the overalloperation time, especially for smaller matrices.",
  "Ablation Studies": "We now provide in-depth examples of QUIK on thelarge LLaMA2-70B and Falcon-180B models. Theformer model is important as it is highly accurateand sensitive, while the latter is the largest openly-available GPT3-type model. Case Study 1: LLaMA2-70B.First, we studythe FLOP breakdown across precisions usingQUIK-4B on LLaMA2-70B. Within the MLP mod-ule of the LLaMA2-70B model, three linear layersare present, referred to as \"Up-Proj\", \"Gate-Proj\",and \"Down-Proj\". \"Up-Proj\" and \"Gate-Proj\" share 6.7B13B30B66B",
  ": Ablation for keeping the Downproj in 4-bits": "We use input variance across layers to chooseboth the number of outliers and the set of layers tobe executed in 8bit. (This is illustrated in for LLaMA2-70B.) Specifically, the \"Down-Proj\"layers have large input variance, mainly due to theHadamard product of the previous two outputs. Toaddress this, we employ 8-bit quantization for boththe weights and activations within the \"Down-Proj\"layers of LLaMA2 models. shows thatkeeping the down-projection layers in 8-bit is criti-cal for high accuracy on LLaMA2, as it improvesperplexity by > 2 points, across all models. Case Study 2: Falcon-180B.Finally, we applyQUIK to Falcon-180B, one of the largest GPT-style openly-available models. The model requires 365GB of GPU memory for the inference, whichmakes it impossible to run inference on a GPUserver with 8x RTX3090 nodes (192 GB memory),illustrating the importance of reducing the memoryfootprint of this model. The results in Tables 2and 8, and already presented quantiza-tion results; in addition we exlore the hardware-supported 2:4 sparse + INT4 format by combiningQUIK with 2:4 sparsity.Instead of just sparsifying the already-quantizedmodel, which results in high accuracy drops, we",
  ":4MLP Blocks3.9336%": ": Accuracy results for quantized + 2:4 sparsifiedon Falcon-180B. For the quantized experiments, weapply quantization on all layers with 256 outliers butkeep some of the layers in dense (mentioned in theTable) for a single Transformer block. extend the SparseGPT algorithm (Frantar and Alis-tarh, 2023) to support our outlier scheme to jointlyquantize and sparsify the model, while keepingthe outlier features in dense FP16. In , wepresent the results of quantizing all layers, but se-lectively keep some layer types dense. Specifically,we found that one-shot pruning of the weights inthe attention blocks to the 2:4 pattern throughout alllayers largely preserves accuracy, leading to smallmemory gains. We present 8-bit results in the samesetting in Appendix M. Discussion.In summary, QUIK shows that onecan execute a large majority of inference computa-tion in 4-bit precision, with efficient GPU support.Specifically, one can obtain speedups of over 3x inusing QUIK across several LLM types.",
  "Limitations": "Our current experiments are limited to compressingthe linear layers of LLMs. However, our scheme iscompatible with virtually any scheme for compress-ing attention layers or the KV-cache (Sheng et al.,2023), which can be applied orthogonally. Anotherlimitation, which we plan to address in future work,is experimenting with recent Mixture-of-Experts(MoE) architectures, and integration with specula-tive decoding (Leviathan et al., 2023). Michael Boratko, Harshit Padigela, Divyendra Mikki-lineni, Pritish Yuvraj, Rajarshi Das, Andrew McCal-lum, Maria Chang, Achille Fokoue-Nkoutche, Pa-van Kapanipathi, Nicholas Mattei, et al. 2018. Asystematic classification of knowledge, reasoning,and context within the ARC dataset. arXiv preprintarXiv:1806.00358.",
  "Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, andChristopher R. 2022.FlashAttention: Fast andmemory-efficient exact attention with io-awareness.arXiv preprint arXiv:2205.14135": "Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. 2022. LLM.int8(): 8-bit matrix mul-tiplication for transformers at scale. Advances inNeural Information Processing Systems 35: AnnualConference on Neural Information Processing Sys-tems 2022, NeurIPS 2022. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,Alexander Borzunov, Torsten Hoefler, and Dan Al-istarh. 2023. Spqr: A sparse-quantized representa-tion for near-lossless llm weight compression. arXivpreprint arXiv:2306.03078.",
  "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, andDan Alistarh. 2022. Gptq: Accurate post-trainingquantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323": "Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,Anthony DiPofi, Charles Foster, Laurence Golding,Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,et al. 2021.A framework for few-shot languagemodel evaluation. Version v0. 0.1. Sept.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99106": "Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, PengXu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, PengGao, Yu Qiao, and Ping Luo. 2023.Omniquant:Omnidirectionally calibrated quantization for largelanguage models. Preprint, arXiv:2308.13137. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuo-han Li, Max Ryabinin, Beidi Chen, Percy Liang,Christopher R, Ion Stoica, and Ce Zhang. 2023.Flexgen: High-throughput generative inference oflarge language models with a single gpu. In Inter-national Conference on Machine Learning, pages3109431116. PMLR.",
  "TII UAE. 2023. The Falcon family of large languagemodels": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771.",
  "CQUIK Peak Memory Usage": "In this section, we assess the memory usage of our quantized models. In , we evaluate the peakmemory usage across different configurations for the OPT and LLaMA-2 families. For OPT-66B, theQUIK-8B and QUIK-4B models demonstrate peak memory reductions of approximately 47% (comparedto the ideal 50% reduction) and 74% (compared to the ideal 75% reduction), respectively. For theLLaMA2-70B model, the reductions are 32% for QUIK-8B and 67% for QUIK-4B. This is because wekeep the down-projection in 8-bits and use additional outliers. Additional overheads come from auxiliarybuffers, which differ for various layer sizes.",
  "FOutlier Analysis": "In this section, we look at how different outlier counts affect the WikiText2 score for the LLaMA2-70Bmodel. In , we observe that increasing the outliers from 128 to 1024 results in a 0.2 perplexityimprovement. We also adjusted the outliers for down-projection layers, ensuring there are 3.5x timesmore than the other linear layers, to match input size. Our results show that using 256 outliers is already agood choice for our experiments. Using additional outliers does not significantly improve accuracy.",
  "GOutlier-Free Layers": "We study the effect of keeping multiple linear layers without any outliers. This might help boost end-to-endperformance by removing all the outlier-related overheads during the forward pass. (Although, as weshow later, these overheads are minor.) shows how the accuracy of different models changes whenwe use different absolute threshold values (shown by T), extracted using a linear search, for the outliers.We conclude that there is no universal threshold across all models, which would preserve accuracy acrossall models. For example, Falcon-180B can achieve reasonable accuracy even if 24% of the linear layers(115 out of 480) contain zero outliers. However, this is not the case for smaller models: LLaMA2-70Bcan recover accuracy with up to 5% of the linear layers (30 out of 560) having zero QUIK outliers. Weprovide additional experiments in Appendix L.",
  "HDetailed Zero-Shot Results": "shows the detailed results of QUIK-4B on OPT and LLaMa-2 families on five popular zero-shottasks: PIQA (Tata and Patel, 2003); WinoGrande (Sakaguchi et al., 2021); HellaSwag (Zellers et al.,2019); Arc (Easy and Challenge) (Boratko et al., 2018). We use the LM Evaluation Harness (Gao et al.,2021) with default parameters in our experiments.",
  "IFull OPT Accuracy Results": "shows the perplexity results of OPT models. We use symmetric quantization for the weights inall our experiments. The results suggest that in a 4-bit setting, considering outlier features is crucial topreserve the accuracy even in small models (like OPT-1.3b). We note that 256 outliers is equivalent to12.5% of the 1.3B models hidden size (and 2.77% of the 66B models hidden size).",
  "JFull LLaMA-2 Accuracy Results": "shows the perplexity of QUIK on LLaMA-2 models. We provide a list of tricks to improve thequality of the model without too much overhead. We found that keeping the down-proj layer in 8 bits canimprove the perplexity by about 3 points. Also, we found weight clipping as a cheap and efficient trick forimproving the accuracy of QUIK-4B.",
  "KFull INT-8 Accuracy Results": "shows QUIK-8B comparison against SmoothQuant on the WikiText2 dataset. We use per-token(per-column) quantization for the activations (weights) in SmoothQuant and only apply the quantizationon the linear layers (which is the case for QUIK also). We exclude the Falcon-7B model as this model hasa single layer-norm for both MLP and Attention blocks and it is not clear how the weights of the FC1 andKQV will be updated in the SmoothQuant algorithm.",
  "PPerformance at different sequence sizes": "We mainly focus our work on the prefill cases with large sequence sizes (in all our experiments sequencesize is equal to 2048). In this section we explore the performance of the QUIK-4B with other inputsequence sizes. In Figures 13(a) and 13(b) we vary input size from 1 to 8k. In the first expeeriment(Figure. 13(a)) we ran layer-wise benchmark, in the second ((b)) we ran inference of a singleTransformer block (on a single GPU). We see that at small input sequence sizes QUIK is noticably slowerfor smaller layer size and models. It can be explained by the fact that the gains of low precision matrixmultiplication at this scale can not compensate the quantization overheads. However, at large layer andmodel sizes QUIK has up to 2x speedup even with single token input. In case of the large input sequenceswe see that performance decreases meaning that low precision matrix multiplication saturates at this scale.",
  "QPerformance with various outlier number": "In this section we explore the effect of outliers numbers on the QUIK performances. suggeststhat the timing of QUIK matmul stays the same across all layer sizes for all non-zero outlier numbers. Thezero outliers case superiority can be explained by the fact that it does not have additional full precisionmatrix multiplication and input data movements. However, these results show that QUIK allow increasethe outlier number without performance sacrifices which is crucial for the accuracy recovery, as wediscussed in the Section ??. Input size Time relative to fp16 (4096, 4096)(8192, 1024)(8192, 8192)(28672, 8192)"
}