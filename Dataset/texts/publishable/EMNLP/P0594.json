{
  "Abstract": "We investigate the mechanism of in-contextlearning (ICL) on sentence classificationtaskswithsemantically-unrelatedlabels(\"foo\"/\"bar\"). We find intervening in only 1%heads (named \"in-context heads\") significantlyaffects ICL accuracy from 87.6% to 24.4%.To understand this phenomenon, we analyzethe value-output vectors in these heads anddiscover that the vectors at each label positioncontain substantial information about thecorresponding labels. Furthermore, we observethat the prediction shift from \"foo\" to \"bar\" isdue to the respective reduction and increase inthese heads attention scores at \"foo\" and \"bar\"positions. Therefore, we propose a hypothesisfor ICL: in in-context heads, the value-outputmatrices extract label features, while thequery-key matrices compute the similaritybetween the features at the last position andthose at each label position. The query andkey matrices can be considered as two towersthat learn the similarity metric between thelast positions features and each demonstrationat label positions. Using this hypothesis, weexplain the majority label bias and recencybias in ICL and propose two methods to reducethese biases by 22% and 17%, respectively.",
  "Introduction": "In-context learning (ICL) is an emergent abil-ity (Wei et al., 2022a) of large language models(Brown et al., 2020; Ouyang et al., 2022; Touvronet al., 2023). By using some demonstration-labelpairs as prompts, ICL performs well without up-dating parameters on many tasks, such as machinetranslation (Sia and Duh, 2023), complexity reason-ing (Li et al., 2023a), compositional generalization(Zhou et al., 2022) and information extraction (Heet al., 2023).Because the mechanism of ICL remains unclear,many studies focus on understanding how ICLworks. Pan et al. (2023) find that ICL can be disen-tangled into task recognition (TR) and task learning (TL). TR does not rely on the demonstration-labelmappings because the roles of demonstrations andlabels are helping the model know \"what is thetask\". In this situation, the model have similarpredictions when the mappings are wrong (Minet al., 2022), because the predictions are based onpre-trained priors. On the other hand, TL relieson the demonstration-label mappings because thesemantic priors are removed. For example, in anICL sentiment classification task, if the labels are\"positive/negative\", the task is TR. If the labelsare \"foo/bar\", the task is TL because the labels aresemantically-unrelated (Wei et al., 2023). Wanget al. (2023) analyze the information flow by aver-aging all attention heads and find the label wordsare anchors to merge the semantic information ofcorresponding demonstrations in shallow layers,and information is extracted from label words tothe final prediction in deep layers.",
  "(a)(a)(a)": ": Hypothesis of ICL mechanism. (a) Shallowlayers merge features into label positions and last posi-tion. In in-context heads, (b) value-output matrix VO ex-tracts label information. (c) Query matrix Q and (d) keymatrix K compute the (e) similarity scores between lastposition and each demonstration, deciding how muchlabel information is transferred into the last token. Although these studies are important for under-standing ICL, the exact mechanism of ICL remainsa mystery for several reasons. Firstly, the informa-tion flow is typically observed as an average acrosseach head, but understanding ICL requires explor-ing the precise importance of each head. Secondly,each head has a query matrix, key matrix, valuematrix, and output matrix; it is essential to studythe role of each matrix in detail. Lastly, ICL isplagued by issues such as majority label bias andrecency bias, and how to explain and mitigate thesebiases has not yet been thoroughly investigated. In this paper, we address these issues by iden-tifying important heads for ICL and studying theroles of each matrix within these heads. Using twomethods, we identify 12 important heads (namedin-context heads) that significantly affect ICL ac-curacy across five datasets, reducing it from 87.6%to 24.4% on average.Intervening in 6 heads(fooheads) decreases the probabilities of \"foo\",while intervening in the other 6 heads (barheads)reduces the probabilities of \"bar\". To explore thereason of this phenomenon, we study these headsvalue-output vectors computing by value-outputmatrices, and find that the vectors on label positionscontain much information about the correspondinglabels. Moreover, we observe the attention scoresin the in-context heads when predictions shift from\"foo\" to \"bar\", and find that the attention scores at\"foo\" positions decrease, while the attention scoresat \"bar\" positions increase. Based on these observa-tions, we propose a hypothesis for ICL, as shownin : in in-context heads, value-output ma-trices extract label information (\"foo\"/\"bar\") fromcorresponding labels, and query-key matrices com-pute the similarity between the last position andeach label position. The query and key matricescan be regarded as two towers for learning the simi-larity between the features at last position and eachdemonstration at label positions. The greater thesimilarity, the higher the probability of the corre-sponding label. Based on this hypothesis, we explore the reasonwhy ICL has majority label bias (Zhao et al., 2021)and recency bias (Lu et al., 2021). The existing ofmajority label bias matches our hypothesis: queryand key matrices compute the attention weightsbetween the last position and each demonstration,so the sum of one labels attention weights is largerwhen this label is related to more demonstrations.About recency bias, we hypothesize that it is caused by the influence of positional embedding duringattention score computation in both shallow anddeep layers. Based on our analysis, we proposetwo methods for reducing these biases. For ma-jority label bias, we increase the attention weightof the imbalanced labels position in in-contextheads, and the majority label bias reduces 22%.For recency bias, we remove the affect of positionembedding in in-context heads, and the recencybias reduces 17%. Our code and data will be re-leased on",
  "Understanding ICL": "Many studies have explored the mystery of ICL.Min et al. (2022) find that randomly replacingthe ground truth labels does not hurt performancemuch. Wei et al. (2023) argue the reason of thisphenomenon is the model can rely on semantic pri-ors. Therefore, they study semantically-unrelatedlabel ICL by transferring the labels into \"foo\" and\"bar\" and find that the performance is related tothe demonstration-label mapping. Pan et al. (2023),disentangle ICL into task recognition (TR) and tasklearning (TL) to explain this phenomenon. Chanet al. (2022) demonstrate that the ICL ability is ob-tained when training data have enough rare classes.Liu et al. (2021) argue that selecting the closestneighbors as demonstrations can enhance ICL abil-ity. Gonen et al. (2022) propose choose low per-plexity demonstrations to increase the performanceof ICL. Dong et al. (2022) conclude these methodsin a survey for ICL. Wang et al. (2023) find thelabel words are anchors to extract demonstrationsin shallow layers, and the last position extracts in-formation from label words in deep layers.Some studies try to explain ICL theoretically. Xie et al. (2021) argue that ICL ability is gainedwhen the pretraining distribution is a mixture ofHMMs, and they explain ICL as implicit Bayesianinference. Garg et al. (2022) prove that transform-ers can learn linear functions by ICL. Akyrek et al.(2022) find transformers can learn linear regressionfunctions and hypothesize that ICL can implementstandard learning algorithms implicitly. Li et al.(2023b) explore the softmax regression and findthat attention-only transformers are similar withgradient descent models. Von Oswald et al. (2023)and Dai et al. (2022) regard ICL as meta-learningand argue that ICL does gradient descent implicitly.",
  "Mechanistic Interpretability": "The goal of mechanistic interpretility (Olah, 2022;Nanda et al., 2023) is to reverse engineer the cir-cuits from inputs to outputs. One common methodis to apply gradient-based methods (Sundararajanet al., 2017; Kindermans et al., 2019) or causal trac-ing methods (Pearl, 2001; Vig et al., 2020; Menget al., 2022) to analyze the importance of differ-ent attention heads and hidden states. Olsson et al.(2022) find that induction heads in attention lay-ers are helpful for copying words from the inputsequence (e.g. [X][Y]...[X] -> [Y]). Wang et al.(2022) interpret the circuits on indirect object iden-tification task in GPT2. Hanna et al. (2023) studieshow GPT2 computes greater-than by constructing acomputational graph of head node and MLP node.Another common method for mechanistic inter-pretability is the logit lens (Nostalgebraist, 2020),whose idea is to analyze the hidden vectors in un-embedding space (also named vocabulary space).Many studies have found that the parameters intransformers are interpretable when projecting intovocabulary space (Elhage et al., 2021; Geva et al.,2022; Dar et al., 2022).",
  "Hypothesis for ICL Mechanism": "Our hypothesis is motivated by a case study in.1. We find that ICL performance canbe affected much by only 1% heads, where somecan enhance the probabilities for \"foo\" and othersfor \"bar\" (.2). To understand why thishappens, we analyze the value-output vectors andattention scores in .3 and find that value-output matrices extract the label information andattention scores computed by query-key matricescontrol the label information flow. At last, we dis-cuss our hypothesis for ICL in .4.",
  "Hypothesis Motivated by Case Study": "Our hypothesis and analysis is motivated by a casestudy in GPT2-large (Radford et al., 2019). Wedesign a simple ICL case for word classification:\"love : bar like : bar eight : foo two : foo one :\",where the models prediction is \"foo\". In this case,\"foo\" is the semantic-unrelated label for \"number\"and \"bar\" is for \"sentiment\". We propose a locate-and-project method for case study: we first locatethe most important heads using the method dis-cussed in .2, then project the vectors onlabel and last positions into vocabulary space bymultiplying each vector v and the unembedding",
  "positiontop words in vocabulary space": "2-valueBAR, Barron, Barrett, Band, Bray, Bars,Baron, Bar, Bay, Boyd5-valueBAR, Barron, Barrett, Baron, Bar, Band,Barbie, Barbar, Bard8-valuefoo, Foo, FO, fo, Foley, Fresno, FDR, fas-cists11-valuefoo, Foo, fo, FO, fascists, FDR, Foley, Goo,fascists2-keykisses, goddess, love, charms, idol, stress,nobles, happiness5-keystyle, oriented, +++, like, indo, height,Lover, xual, dont, foo8-keyfoo, mc, blah, happ, avg, french, omega,prod, english, google, height, neigh11-keyfoo, mc, infinity, omega, three, two, repeat,twelve, 666, Three, thirds, five, sixteen13-queryfirst, end, only, no, all, given, person, cer-tain, call, same, short, long, 1, one, value",
  ": Top tokens at label positions and last position": "Label positions value-output vectors containconcepts about the labels, and their key vectorscontain the corresponding demonstrations. For ex-ample, the label at position 2 is \"bar\" and the value-output vector contains \"BAR, Bars, Bar\". Its keyvectors top tokens are related to the correspond-ing demonstration \"love\". The last position haveconcepts about the input text \"one\". Hence, wehypothesize that value-output matrices extract thelabel information and query-key matrices computethe similarity between the last position (encodes theinput text) and each label position (encodes demon-stration). We also note interpretable results in sen-tence classification cases, detailed in Appendix A.",
  "Identifying Important Heads for ICL": "Datasets and models.We conduct the experi-ments on five sentence classification datasets, in-cluding financial phrasebank (Financ) (Malo et al.,2014), AGs news topic classification (AGnews)(Zhang et al., 2015), Amazon reviews (Amazon)(McAuley and Leskovec, 2013), Hate Speech De-tection (ETHOS) (Mollas et al., 2020), and Stan-ford Sentiment Treebank binary (SST2) (Socheret al., 2013). We conduct experiments on Llama-7B (Touvron et al., 2023) with 32 layers (32 headsper layer), and GPT-J (Wang and Komatsuzaki,2021) with 28 layers (16 heads per layer).",
  ": ICL accuracy (%) with correct label \"foo\"/\"bar\"in Llama (first block) and GPT-J (second block)": "Inspired by Pan et al. (2023) and Wei et al. (2023)that task learning is the emergent ability of largelanguage models (LLMs), we replace the labelswith semantic-unrelated labels \"foo\" and \"bar\" tostudy the mechanism of ICL task learning ability.In each dataset, we randomly sample two sentenceswith each label, and propose the ICL sentence: \"S0: bar S1 : bar S2 : foo S3 : foo S4 :\" with correctlabel \"foo\" and \"S0 : foo S1 : foo S2 : bar S3 :bar S4 :\" with correct label \"bar\", where S0 andS1 have the same label, and S2, S3, S4 have theother label. We randomly sample 1,000 sentencesin each dataset. The accuracy when correct labelsare \"foo\" and \"bar\" are shown in , whichindicate that the ICL ability exists in most datasets. Methods.We apply two methods to identify theimportant heads for ICL. Firstly, we use causaltracing methods (Pearl, 2001; Vig et al., 2020) andintervene each head in deep layers by setting theheads parameters to zero, and re-calculate the de-crease in each dataset. Secondly, following Yu andAnaniadou (2024), we compute the log probabil-ity increase Shl of each head to find which headsdirectly contribute to the final predictions:",
  "Shl = log(p(b|ohl + Linl)) log(p(b|Linl)) (2)": "where b is the predicted label (\"foo\"/\"bar\"), Linl islth layers input, and ohl is the head output vectoron layer l, head h. The probability is calculatedby multiplying the vector with the unembeddingmatrix Eu (Eq.1). If the score is large, the head isuseful for increasing the probability of label b. Weidentify the heads rank top10 in both methods, andthere are 6 important \"fooheads\" affecting \"foo\"and 6 important \"barheads\" affecting \"bar\" in bothmodel. The average accuracy change when inter-vening the fooheads and barheads is shown in . When intervening the fooheads, datasets withcorrect label \"foo\" show a significant decrease inaccuracy, while those with correct label \"bar\" ex-perience a substantial increase in accuracy. Whenmasking in the barheads, datasets with correct la-bel \"bar\" show a significant decrease in accuracy, while those with correct label \"foo\" experience asubstantial increase in accuracy. Therefore, ouridentified fooheads and barheads are important forpredicting \"foo\" and \"bar\", respectively. We namethese heads \"in-context heads\".",
  "p=0p vop(3)": "where T is the length of the input text. is theattention score computed by the softmax functionon the inner product of last positions query vectorand each positions key vector. vo is computed bythe linear transform of value-output matrices oneach positions layer input. To explore the impor-tance of label positions in each in-context head,we investigate sentences with correct label \"foo\",and compute the logit minus score M at \"foo\" and\"bar\" positions weighted value-output vectors: M = log(p(foo|p vop)) log(p(bar|p vop))(4)If M is larger than zero, the vectors are importantfor enhancing \"foo\" probability. On the contrary,they are important for enhancing \"bar\" probability.The average logit minus scores at \"foo\" positions(fp) and \"bar\" positions (bp) in fooheads (fh) andbarheads (bh) are shown in . In both mod-els, foo positions contain much information about\"foo\" in fooheads, and bar positions contain muchinformation about \"bar\" in barheads. Furthermore,the proportion between label positions logit minusscores and the in-context heads logit minus scoresis 99.1%. Therefore, the reason fooheads/barheadsaffect probabilities of \"foo\"/\"bar\" is due to the in-formation saved at \"foo\"/\"bar\" positions weightedvalue-output vectors vo.To explore the roles of query-key matrices andvalue-output matrices, we compute the attentionscores and the value-output vectors logit minus",
  ": Attention score and logit minus at \"foo\"/\"bar\"positions in fooheads/barheads in Llama (first block)and GPT-J (second block), averaged on all datasets": "Both query-key matrices and value-output matri-ces can affect the probabilities. In Llama fooheads,the query-key matrices play large roles for predict-ing \"foo\". The value-output matrices can extractboth \"foo->foo\" and \"bar->bar\", since the absolutevalues of logit minus scores at \"foo\" and \"bar\" po-sitions are similar. In GPT-J fooheads, both query-key matrices and value-output matrices play largeroles for enhancing \"foo\". In Llama barheads andGPT-J barheads, value-output matrices play largerrole than query-key matrices for predicting \"bar\".To explore how the predictions change from\"foo\" to \"bar\", we compare the sentences \"S0 : barS1 : bar S2 : foo S3 : foo S4 :\" and \"S0 : foo S1: foo S2 : bar S3 : bar S4 :\" in each dataset. Wecompute the change of absolute value on weightedvalue-output vectors logit minus scores (minus-w),value-output vectors logit minus scores (minus),and attention scores, shown in .The prediction shift is caused by the change ofweighted value-output vectors logit minus scores.When changing the labels, fooheads foo positionscontain less information about \"foo\", and barheadsbar positions contain more information about \"bar\".The \"foo\" decrease at fooheads \"foo\" positionsand the \"bar\" increase at barheads \"bar\" positionscause the probability change from \"foo\" to \"bar\".",
  ": Change of attention score and logit minus at\"foo\"/\"bar\" positions in fooheads/barheads in Llama(first block) and GPT-J (second block) on all datasets": "The attention scores change significantly whenthe predictions shift from \"foo\" to \"bar\". Attentionscores at fooheads \"foo\" positions decrease sub-stantially, while those at barheads \"bar\" positionsincrease markedly. Comparatively, the change di-rection of the value-output vectors logit minusscores does not show a relevant trend with the logitminus scores of the weighted value-output vectors.Therefore, we hypothesize that the change of atten-tion scores within in-context heads is the primarycause for the prediction shift from \"foo\" to \"bar\".",
  "Proposed Hypothesis and Discussion": "For better understanding, we list the evidence of ex-isting studies and previous sections: a) Wang et al.(2023) demonstrate that the label positions (\"foo\",\"bar\") extract corresponding demonstrations fea-tures in shallow layers. b) In .2, we findthat in deep layers there are a few fooheads impor-tant for predicting \"foo\" and barheads for \"bar\". c) proves that the \"foo\" positions in fooheadsand the \"bar\" positions in barheads contain muchinformation for predicting \"foo\" and \"bar\", respec-tively. d) The experiments in demonstratethat both query-key matrices and value-output ma-trices can affect the information storage. e) s results prove that the change of attention scoreswithin fooheads and barheads is the primary causefor the prediction shift from \"foo\" to \"bar\" whenreversing the demonstrations labels.Based on these findings, we conclude our hy-pothesis: In shallow layers, the label positionsextract features from the corresponding demon-strations (hypothesized from evidence a), whilethe last position encodes information of the inputtext and previous demonstrations/labels (X% inputtext + Y% near demonstrations + Z% far demon-strations). In deep layers in-context heads, thevalue-output matrices extract the label features intovalue-output vectors (hypothesized from evidence b and c). For example, fooheads extract \"foo->foo\"and barheads learn \"bar->bar\". The query-key ma-trices compute the similarity between the last po-sitions features and each label positions features.When the labels change from \"foo\" to \"bar\", thechange of last position features causes the similar-ity scores change and the prediction shift (hypoth-esized from evidence d and e). For instance, thefooheads similarity scores at foo positions changefrom SIM((X+Y)%foo, foo) to SIM(Z%foo, foo),and the barheads similarity scores at bar positionschange from SIM(Z%bar, bar) into ((X+Y)%bar,bar). Hence, the foo positions attention scores de-crease in fooheads and the bar positions attentionscores increase in barheads, causing the probabilitychange from \"foo\" to \"bar\". If considering all the in-context heads together,the overall value-output matrices can learn both\"foo->foo\" and \"bar->bar\". Under our hypothesis,the query and key matrices can be regarded as twotowers computing the semantic similarity betweenthe last positions features and each label positionsdemonstration features. If the similarity score islarge, more corresponding label information is in-corporated, enhancing the probability of that label.There are four modules related to the ICL ability. a) Information extraction ability of shallowlayers. Shallow layers can be regarded as featureextraction modules. The ability of extracting corre-sponding demonstrations and the input text decidesthe quality of features.",
  "b) Value projection ability of in-context headsvalue-output matrices. If the value projectionability is good enough, the in-context heads shouldproject \"foo\" and \"bar\" together and fairly": "c) Metric learning ability of in-context headsquery and key matrices. The query and key matri-ces might be the most important module, becausethey should learn computing different metrics usingthe same matrices. If different ICL tasks share thesame in-context heads, the query and key matricesshould learn these metrics jointly. d) Numbers and parameters of in-contextheads. If we regard one in-context head as a two-tower model for metric learning, the parameters ofthe head are directly related to the learning ability.At the same time, different in-context heads can beregarded as voting or ensemble models, so the headnumber also controls the learning ability.",
  "Understanding Majority Label Bias andRecency Bias in ICL": "There are several phenomena of ICL that haventbeen explained. Zhao et al. (2021) demonstratethat models tend to predict majority labels and thelabels near the input text. Lu et al. (2021) also findthat changing the demonstration order can affectpredictions a lot. Based on our hypothesis, weexplore why ICL has majority label bias (in .1) and recency bias (in .2).",
  "Understanding Majority Label Bias": "According to our hypothesis, it is reasonable thatthe model tends to predict majority labels, becausethe label information flow is controlled by the simi-larity between last position and each label position.When a label has high frequency, the sum of simi-larity scores will be larger, thus the probability ofthis label is larger in final prediction. We design animbalanced dataset to verify this. For each sentencewith correct label \"foo\", we remove the last demon-stration and label. For example, \"S0 : bar S1 :bar S2 : foo S3 : foo S4 :\" is changed into \"S0 :bar S1 : bar S2 : foo S4 :\". We compute the sumof attention weights on \"foo\" positions in fooheadsand \"bar\" positions in barheads on the imbalanceddatasets and the original datasets, averaged on allfive datasets. The changing of attention scores at\"foo\" positions and \"bar\" positions in both modelsare shown in .",
  ": Attention scores on foo positions in fooheadsand bar positions in barheads, on original dataset andimbalanced dataset in Llama (left) and GPT-J (right)": "In both models, the sum of attention weights on\"foo\" positions decrease on the imbalanced dataset.On the contrary, the attention weights on \"bar\"positions increase. The results meet our analysis.The attention weights are computed by a softmaxfunction, so when a \"foo\" demonstration and itslabel are removed, the sum of attention weightson \"foo\" positions will decrease, and that on \"bar\"positions will increase.",
  "Understanding Recency Bias": "The ICL performance is extremely sensitive to thedemonstration order. We hypothesize that the re-cency bias is caused by the influence of positionalembeddings on the attention score computation inboth shallow layers and deep layers. The attentionscore is calculated by applying a softmax functionto the product of the last positions query vectorand each label positions key vector. These queryand key vectors are derived from the layer input,which is a combination of the positional embed-ding, the word embedding, and the output vectorsfrom previous attention layers and feed-forwardnetwork (FFN) layers. Therefore, a \"position term\"consistently influences the attention scores.The feature extraction of last position is relatedto the attention scores in shallow layers heads.Due to the influence of positional embedding, themodel tends to extract varying amounts of featuresat different positions. Let us consider the case \"S0: bar S1 : bar S2 : foo S3 : foo S4 :\". Thelast position contains X% S4 + Y% (S2+S3) andZ% (S0+S1), simplified into (X+Y)% foo + Z%bar. If the demonstration order is changed into\"S2 : foo S3 : foo S0 : bar S1 : bar S4 :\", thelast position will contain X% S4 + Z% (S0+S1) +Y% (S2+S3), simplified into (X+Z)% foo + Y%bar. Hence, the final prediction probability will bedifferent between these two sentences if Y and Zare different. If Y is larger than Z, the last positionwill contain less \"foo\". Similarly, the influence ofpositional embeddings also exists in deep layersheads, which tends to enlarge the attention scoreson later positions in these heads.We design a reverse dataset to evaluate the differ-ence among different positions. For each sentenceS0 : bar S1 : bar S2 : foo S3 : foo S4 :, wetransfer it into a reverse sentence S2 : foo S3 : fooS0 : bar S1 : bar S4 :. We compute the average at-tention score change at \"foo\" positions in fooheadsand \"bar\" positions in barheads, between the orig-inal and the reverse dataset, shown in .Moreover, we remove the impact of positional em-bedding in each in-context head and re-computethe attention scores (original modify and reversemodify in ).Compared with the original dataset, \"foo\" po-sitions attention weights decrease and \"bar\" po-sitions attention weights increase in the reversedataset in both models. This result aligns with theobservations in previous studies (Zhao et al., 2021)",
  ": Attention scores on foo positions in fooheadsand bar positions in barheads, on original dataset andreverse dataset in Llama (left) and GPT-J (right)": "that the probability is affected much when revers-ing the demonstration order. When removing theimpact of positional embedding in each head, thenear positions attention scores decrease and the farpositions scores increase. Hence, our hypothesis isverified: the positional term in each head enlargesthe attention scores on later positions. After re-moving the positional term in in-context heads, theattention score is still different between the originaldataset and the reverse dataset. This difference iscaused by the difference in shallow layers featureextraction stage. To provide a clearer perspective, we illustrate theattention score change on \"foo\" positions in eachfoohead and \"bar\" positions in each barhead. Thechange of imbalanced dataset and reverse dataset inLlama and GPT-J is shown in and 6, wherethe first 6 columns are \"foo\" positions attentionscores in fooheads and the last 6 columns are \"bar\"positions scores in barheads. Compared with theoriginal dataset, the attention scores decrease on\"foo\" positions and increase on \"bar\" positions inimbalanced dataset and reverse dataset.",
  "Reducing Majority Label Bias byEnlarging Imbalanced Label Attention": "According to our analysis in .1, the ma-jority label bias can be attributed to the lack ofattention weights on imbalanced label positions.So we propose a method to reduce the majoritylabel bias by enlarging the imbalanced label posi-tions attention scores. Specifically, we multiplyan amplified score a on the imbalanced label po-sitions weighted value-output vectors (ap vop in Eq.3) and add this vector into the final embed-ding. a is the product of a constant hyperparameterac and a varying score av, where av is the ratioof the larger demonstration number to the smallerdemonstration number.We first make a balanced dataset by randomlysampling 2-4 demonstrations in each label, and ran-domly set the demonstration order. The correctlabels of the balanced sentences are \"foo\". Thenwe get a \"lackfoo\" sentence by randomly removinga \"foo\" demonstration, and a \"lackbar\" sentence byrandomly removing a \"bar\" demonstration. Exceptthe results in Financ GPT-J, the accuracy of \"lack-foo\" dataset is smaller than the balanced datasetdue to the lack of \"foo\" demonstrations, and \"lack-bar\" accuracy is larger than the balanced dataset.Compared to the balanced dataset, we calculatethe sum of accuracy change on \"lackfoo\" and \"lack-bar\" datasets before and after applying our methodwith amplified constant score ac 0.03. The accuracychange is shown in . On average, the accu-",
  "Reducing Recency Bias by RemovingPositional Embedding Affect": "As discussed in .2, we find the recencybias is due to the effect of positional embeddingon the calculation of attention scores. Hence, inorder to reduce the recency bias, we reduce theposition term in in-context heads, and re-calculatethe output vectors in all in-context heads. Thismethod is similar with adding a shortcut adapterfrom each in-context head to the final embedding.",
  ": Standard deviation of accuracy and attentionscores before/after applying our method in Llama (firstblock) and GPT-J (second block)": "We apply this method to the original dataset andthree recency datasets with different demonstra-tion orders, detailed in Appendix B. We calculatethe standard deviation in accuracy and in-contextheads attention scores before (acc-be, attn-be) andafter (acc-af, attn-af) applying our method. Theresults are shown in . On average, the ac-curacy standard deviation reduces 23.4% in Llamaand 10.6% in GPT-J, and the attention score stan-dard deviation reduces 40.1% in Llama and 37.7%in GPT-J. Therefore, removing the positional termin in-context heads is helpful for reducing the re-cency bias. It is also important to reduce the re-cency bias during feature extraction in shallow lay-ers, and we leave this exploration in future work.",
  "Conclusion": "We identify the important heads for ICL and ana-lyze the value-output vectors and attention scoresin these heads. We propose a hypothesis for themechanism of ICL. In shallow layers, the demon-strations and input text is captured by the label posi-tions and the last position. In in-context heads, thevalue-output matrices project the label features intovalue-output vectors. The query and key matricescan be regarded as two towers learning the simi-larity between the last positions features and eachlabel positions features. If the similarity scoreis high, the corresponding labels probability isenlarged. Based on this hypothesis, we interpretwhy ICL has majority label bias and recency bias.Furthermore, we propose two methods to reducethese biases by 22% and 17%. Overall, our studyprovides a new method and a reasonable hypothe-sis for understanding the mechanism of in-contextlearning.",
  "Limitation": "In this paper, we focus on understanding the mech-anism in in-context heads in deep layers. It is alsoimportant to study how shallow layers transfer fea-tures into label positions and the last position. Ourhypothesis explains the ICL mechanism for classifi-cation tasks. More studies should be done on otherICL tasks, such as chain-of-thought reasoning (Weiet al., 2022b).Another limitation of our work comes from theattribution method for identifying important heads.Gradient-based methods and causal tracing meth-ods, which calculate a modules impact on the fi-nal prediction, are commonly employed for impor-tance attribution. Additionally, many studies utilizesaliency score-based methods. In this paper, weapply both causal tracing and saliency score-basedmethods to identify important heads, and we be-lieve the results in support our findings.However, it is important to note that there is nounified method for attributing important modules,and further exploration is needed to design betterattribution methods.",
  "Ekin Akyrek, Dale Schuurmans, Jacob Andreas,Tengyu Ma, and Denny Zhou. 2022. What learningalgorithm is in-context learning? investigations withlinear models. arXiv preprint arXiv:2211.15661": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Stephanie Chan, Adam Santoro, Andrew Lampinen,Jane Wang, Aaditya Singh, Pierre Richemond, JamesMcClelland, and Felix Hill. 2022. Data distributionalproperties drive emergent in-context learning in trans-formers. Advances in Neural Information ProcessingSystems, 35:1887818891. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,and Furu Wei. 2022. Why can gpt learn in-context?language models secretly perform gradient descent asmeta optimizers. arXiv preprint arXiv:2212.10559.",
  "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. 2022. A survey for in-context learning.arXiv preprint arXiv:2301.00234": "Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al.2021. A mathematical framework for transformercircuits. Transformer Circuits Thread, 1. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gre-gory Valiant. 2022. What can transformers learnin-context? a case study of simple function classes.Advances in Neural Information Processing Systems,35:3058330598.",
  "Michael Hanna, Ollie Liu, and Alexandre Variengien.2023. How does gpt-2 compute greater-than?: In-terpreting mathematical abilities in a pre-trained lan-guage model. arXiv preprint arXiv:2305.00586": "Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, XingXu, and Heng Tao Shen. 2023. Icl-d3ie: In-contextlearning with diverse demonstrations updating fordocument information extraction.arXiv preprintarXiv:2303.05063. Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo,Maximilian Alber, Kristof T Schtt, Sven Dhne,Dumitru Erhan, and Been Kim. 2019. The (un) relia-bility of saliency methods. Explainable AI: Interpret-ing, explaining and visualizing deep learning, pages267280.",
  "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2021.Whatmakes good in-context examples for gpt-3? arXivpreprint arXiv:2101.06804": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2021. Fantastically orderedprompts and where to find them:Overcomingfew-shot prompt order sensitivity. arXiv preprintarXiv:2104.08786. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, andP. Takala. 2014. Good debt or bad debt: Detecting se-mantic orientations in economic texts. Journal of theAssociation for Information Science and Technology,65. Julian McAuley and Jure Leskovec. 2013. Hidden fac-tors and hidden topics: understanding rating dimen-sions with review text. In Proceedings of the 7thACM conference on Recommender systems, pages165172.",
  "Chris Olah. 2022. Mechanistic interpretability, vari-ables, and the importance of interpretable bases. InTransformer Circuits Thread": "Catherine Olsson, Nelson Elhage, Neel Nanda, NicholasJoseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.In-context learning and induction heads.arXivpreprint arXiv:2209.11895. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks. In Interna-tional conference on machine learning, pages 33193328. PMLR": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. 2020. Investigating gender bias in languagemodels using causal mediation analysis. Advancesin neural information processing systems, 33:1238812401. Johannes Von Oswald, Eyvind Niklasson, Ettore Ran-dazzo, Joo Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. 2023. Trans-formers learn in-context by gradient descent. In In-ternational Conference on Machine Learning, pages3515135174. PMLR.",
  "Ben Wang and Aran Komatsuzaki. 2021.GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-guage Model": "Kevin Wang, Alexandre Variengien, Arthur Conmy,Buck Shlegeris, and Jacob Steinhardt. 2022.In-terpretability in the wild: a circuit for indirect ob-ject identification in gpt-2 small.arXiv preprintarXiv:2211.00593. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. arXiv preprintarXiv:2305.14160. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022a. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022b. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, AlbertWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,Da Huang, Denny Zhou, et al. 2023. Larger languagemodels do in-context learning differently.arXivpreprint arXiv:2303.03846.",
  "We analyze a sentence classification case sampledin AGNews dataset. The top tokens in head 23-13 in GPT2 large are shown in . With theprediction \"foo\", the case is:": "Wall St.Bears Claw Back Into the Black(Reuters) Reuters - Short-sellers, Wall Streetsdwindling band of ultra-cynics, are seeing greenagain. : bar Stoking the Steamroller No otherrecording artist can channel American middle-class tastes quite like Chip Davis and his best-selling band. : bar Liverpool completes signingsof Alonso, Garcia LIVERPOOL, England (AP) Spanish pair Xabi Alonso from Real Sociedad andLuis Garcia from Barcelona signed five-year con-tracts with Liverpool on Friday. : foo U.S. DopingWatchdog to Question BALCOs Conte - IAAFHELSINKI (Reuters) - U.S . anti-doping officialsplan to question Victor Conte after the BALCOhead claimed he saw sprinter Marion Jones takingbanned drugs, world athletics body the IAAF saidSaturday. : foo Liverpool Progresses to ChampionsLeague; Monaco, Inter Advance Four-time cham-pion Liverpool progressed to soccer ChampionsLeague 2-1 on aggregate, overcoming a 1-0 homedefeat to AK Graz in the second leg of qualifying.:",
  "In this case, the false demonstrations with label": "\"bar\" are sampled from the \"Business\" class. Thetrue demonstrations with label \"foo\" and the inputtext are sampled from the \"Sports\" class. On la-bel positions value-output vectors, \"bar\" and \"foo\"have top rankings. As for the key vectors at labelpositions, the labels correspond to business demon-strations extract the concepts about business, suchas \"investor\" and \"profit\". The top tokens of truelabels are related to places such as \"Liverpool\"and \"Spanish\", which exist in the correspondingdemonstrations. These observations indicate thatthe value-output matrices extract label features, andthe key matrix extract corresponding demonstrationfeatures. Analyzing the last positions query vector,we also observe concepts related to \"Liverpool\"."
}