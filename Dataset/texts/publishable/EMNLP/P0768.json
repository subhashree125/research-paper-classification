{
  "Abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling),a novel self-supervised learning approach forspeech representation learning.In contrastto the prior methods that use random mask-ing schemes for Masked Acoustic Modeling(MAM), we introduce a novel selective andadaptive masking strategy. Specifically, dur-ing SSL training, we progressively introduceharder regions to the model for reconstruc-tion. Our approach automatically selects hardregions and is built on the observation thatthe reconstruction loss of individual frames inMAM can provide natural signals to judge thedifficulty of solving the MAM pre-text taskfor that frame. To identify these hard regions,we employ a teacher model that first predictsthe frame-wise losses and then decides whichframes to mask. By learning to create chal-lenging problems, such as identifying harderframes and solving them simultaneously, themodel is able to learn more effective represen-tations and thereby acquire a more comprehen-sive understanding of the speech. Quantita-tively, EH-MAM outperforms several state-of-the-art baselines across various low-resourcespeech recognition and SUPERB benchmarksby 5%-10%. Additionally, we conduct a thor-ough analysis to show that the regions maskedby EH-MAM effectively capture useful contextacross speech frames 1.",
  "Model": "(Student) Selective Masking with Teacher (b) EH-MAM Pre-training (a) Traditional MAM Pre-training : EH-MAM compared to random masking schemesemployed widely in the literature. EH-MAM first identifieswhich frames to mask using a Teacher model and then solvesthe MAM task by reconstructing the selected masked regionsusing a Student model. like Automatic Speech Recognition (ASR), SpeechEmotion Recognition (SER), etc (Huang et al.,2001). Progress in SSL for speech has led to sig-nificant performance improvements in a range oflow-resource SLP tasks including Phoneme Recog-nition (PR), Keyword Spotting (KS), etc (Mo-hamed et al., 2022). Masked Acoustic Modeling(MAM) has been one the most prevalent pretexttasks for SSL-based speech representation learningwherein the model tries to reconstruct frames thatare masked at the input, utilizing the context of thesurrounding frames (Baevski et al., 2022, 2023). Although a considerable amount of research inMAM has been performed, most has focused on im-proving model architectures (Baevski et al., 2022;Chang et al., 2022; Baevski et al., 2023) and pre-text tasks (Hsu et al., 2021; Lodagala et al., 2023;Liu et al., 2024), with very limited progress in im-proving the masking algorithm (Yue et al., 2022;Baevski et al., 2023). Most MAM algorithms stillperform random masking of input frames. On theother hand, selective masking strategies for otherdomains, like computer vision (CV) (Bao et al.,2021; He et al., 2022; Kakogeorgiou et al., 2022)",
  "Random MaskingSelective Masking": ": Increase in relative WER using selective and ran-dom masking schemes. During inference, under similar ex-perimental settings, we selectively mask the frames with highreconstruction values and compare it against random masking.The former consistently shows a significant increase in rela-tive WER than the later, thereby indicating that these framescapture more useful context for speech reconstruction as aresult of capturing more information, Thus building on thisresult we hypothesize that asking a model to reconstruct theseframes will result in stronger learning signals. and natural language processing (NLP) (Sadeqet al., 2022a, 2023; Xiao et al., 2022) that focuseson masking useful context, have shown significantimprovements over random masking. This can beattributed to multiple factors, including: (1) Vari-able Information Content: Variable informationcontent in data translates to variable learning sig-nals for the reconstruction task. For instance, inMasked Language Modeling (MLM) (Devlin et al.,2019), the reconstruction of high-frequency stopwords such as the or is offers minimal discrim-inative power due to the ubiquity and low semanticload of these words (Sadeq et al., 2022a, 2023). Inspeech, for example, this can be translated to re-constructing frames corresponding to random noiseor partial phonemes, where much of the frames isalready available as context. (2) Progressive Learn-ing: Random masking fails to imitate the progres-sive human learning process (Madan et al., 2024).Humans do not receive knowledge uniformly; in-stead, they are exposed to progressively more com-plex information as they advance in the learningprocess. Mimicking this progression in the mask-ing algorithm by initially exposing the model tosimpler, more predictable speech patterns and grad-ually introducing more complex, less predictableones can significantly enhance the learning trajec-tory. This approach aligns better with how humanslearn, moving from simpler to more complex in-formation, and helps the model develop a deeper understanding of language over time.Main Contributions. To overcome the aforemen-tioned problems, in this paper, we propose EH-MAM (Easy-To-Hard adaptive Masked AcousticModelling), a novel selective and adaptive mask-ing scheme for MAM. We build EH-MAM on thecore hypothesis that hard regions, characterizedby collections of speech frames that are more dif-ficult to reconstruct, serve as stronger signals forthe learning process. shows the results ofa simple experiment we performed to validate ourhypothesis. By selectively masking hard regions,we notice a greater and consistent drop in WERperformance for ASR. This suggests that maskinghard regions captures useful context in the speechinput. Our main contributions are as follows: WeproposeEH-MAM,anovelself-supervised speech representation learning al-gorithm. In contrast to solving a predefinedMAM pre-text task, such as reconstructingrandomly masked frames, EH-MAM aimsto generate and align itself towards a moreformidable MAM pre-text task. For generat-ing a challenging MAM pre-text task, we firstidentify a collection of difficult frames to re-construct, also called hard regions, followedby selectively masking them. We proposea lightweight loss predictor (introduced in.2.2) that predicts the frame-level re-construction loss values and determines hardregions based on the output.To train theloss predictor jointly with MAM, we designa novel auxiliary loss (introduced in Sec-tion 3.2.2) that forces the predictor to learn therelative correlations between speech frames.Finally, to align the model towards recon-structing hard regions, we propose an easy-to-hard masking strategy (introduced in Sec-tion 3.2.3) that guides the EH-MAM learning. We show the effectiveness of the speech rep-resentation learned by EH-MAM through ex-tensive evaluations on low-resource speechrecognition benchmarks (Kahn et al., 2020)and downstream evaluation on SUPERB (wenYang et al., 2021). EH-MAM beats prior artswith a relative improvement of 5%-10%",
  "Reconstruction Loss": "EMA : Illustration of EH-MAM SSL algorithm. EH-MAM employs the self-distillation SSL framework that consists ofidentical student and teacher networks. At each training iteration, the teacher is updated by the exponential moving average(EMA) of the student. 1 For a speech input Z, we first use the teacher network to identify the speech frames that are hard toreconstruct, also called as hard regions. To achieve this, we predict the frame-level reconstruction loss values Ltp using a losspredictor dt by feeding Z to the teacher network. 2 Next, we utilize our easy-to-hard masking strategy to identify the maskindices M S associated with hard regions, followed by progressively introducing them with random mask indices M R over eachepoch. 3 Finally, a masked variant Z is fed to the student network, where it is tasked to 4 reconstruct masked regions byoptimizing a reconstruction loss (as shown in Eqtn. 3) and 5 train a loss predictor ds by computing an auxiliary loss betweenpredicted and original reconstruction loss values, Lsp and Lrec respectively (as shown in Eqtn. 6).",
  "Related Work": "Self-Supervised Learning.SSL has emergedas a prevalent speech representation learningparadigm, demonstrating impressive downstreamperformance under low-resource settings (Lee et al.,2022; Mohamed et al., 2022). At its core, SSLrelies on the quality of pretext tasks for capturingvaried learning signals from unlabeled data sources.Based on the nature of the pretext tasks, the SSLframeworks are further categorized into the fol-lowing sub-categories: 1) Contrastive Approaches:The pretext task is designed to maximize latentspace similarity between the anchor and positivesamples while minimizing the similarity betweenthe anchor and negative samples. 2) GenerativeApproaches: These methods primarily focus onfirst building a target by randomly masking mul-tiple speech frames and then reconstructing themby optimizing a similarity measure (MSE or CrossEntropy) between the predicted frames and the tar-gets. The pretext task includes predicting futureinput from past inputs (Oord et al., 2018; Yanget al., 2022), masked from unmasked (Baevski",
  "et al., 2022, 2023) or the original from some othercorrupted view (Lodagala et al., 2023). MaskedAcoustic Modeling has undoubtedly seen the mostsuccess for speech representation learning": "Masked Acoustic Modeling (MAM) Conven-tional MAM architectures first perform frame-levelmasking, where randomly selected speech framesare masked using various existing masking strate-gies, including block or random masking (Baoet al., 2021; He et al., 2022). Next, they eitheremploy a single encoder network like BERT (De-vlin et al., 2019) to predict masked regions in aspeech input (Liu et al., 2020; Chang et al., 2022;Chen et al., 2022; Hsu et al., 2021) or utilize self-distillation methods, where the student learns toreconstruct masked information under the guid-ance of an identical teacher network (Baevski et al.,2022, 2023; Liu et al., 2024). Although a consid-erable amount of research in MAM has undergonetowards improving model architecture (Baevskiet al., 2022, 2023) and introducing novel pretexttasks (Liu et al., 2024), developing better maskingstrategies is still under-explored.",
  "Overview of EH-MAM": "We illustrate EH-MAM in . At its core, EH-MAM incorporates the self-distillation based SSLtraining paradigm for solving MAM pretext task,similar to Baevski et al. (2022, 2023). Specifically,EH-MAM consists of two identical networks, ateacher {ft, dt} and a student {fs, ds}. A sep-arate decoder dR is employed for reconstructingmasked frames from the student representations.The context encoders f are built using K-layeredtransformers (Vaswani et al., 2017), whereas the de-coder dR and the loss predictor d are constructedwith light-weight D-layered 1D-convolution lay-ers (Kiranyaz et al., 2019). During pre-training,the teacher parameters t, t are updated by per-forming exponential moving average (EMA) of thestudent parameters s, s (Tarvainen and Valpola,2017). Formally, we define the update as follows:",
  "t = t + (1 )s(1)": "where t = {t, t}, s = {s, s}, and is thedecay rate. The student and decoder parameters areupdated using gradient descent.At each training iteration, we first extract low-frequency feature representations Z RNd fromraw speech signals x X (Baevski et al., 2020)and feed it to the teacher network to get frame-level predicted reconstruction loss values Ltp =dt(ft(Z)). With the help of Ltp, we generatebinary mask indexes MA = {0, 1}N using theeasy-to-hard masking strategy (introduced in Sec-tion 3.2.3), followed by creating a masked versionof the original speech input Z Z MA. Finally,the student is trained with gradient descent to mini-mize a weighted combination of reconstruction lossLrec (introduced in .2.1) and an auxiliaryloss Laux (introduced in .2.2). Formally,we define the objective function below:",
  "Selective Masking with EH-MAM": "Motivation: EH-MAM distinguishes itself fromthe conventional self-distillation-based SSL train-ing methods that are fixated on solving a prede-fined MAM task generated using random mask-ing (Baevski et al., 2022; Chen et al., 2022; Changet al., 2022), by enforcing the teacher to generatemore challenging pretext tasks. To achieve this,EH-MAM first uses the teacher to identify hardregions, a collection of speech frames that are diffi-cult to reconstruct, and then selectively mask thesehard regions to create challenging MAM pretexttasks for the student to solve. Being constantly chal-lenged by the teacher further directs the student todevelop a much more nuanced understanding ofspeech. Additionally, we take inspiration from therecent studies in NLP and CV that have highlightedthe significance of generating formidable pretexttasks for MLM (Masked Language Modeling) andMIM (Masked Image Modeling) using selectivemasking (Bao et al., 2021; Sadeq et al., 2022b).To reweigh the model attention towards recon-structing such hard regions, we introduce the losspredictors ds, dt for the student and teacher net-works, respectively. Further to train the loss predic-tor, we also propose an auxiliary objective functionLaux, that the model optimizes alongside the re-construction loss Lrec.",
  "where MA ft(Z) represents teacher representa-tions associated with the masked speech input": "3.2.2Loss Predictor and Auxiliary LossMotivation: Given the sequence of frame-levelreconstruction loss values Lrec RN, our goal isto create a challenging MAM pretext task for thestudent by selectively masking frames with highreconstruction values. As original reconstructionloss values Lrec are computed only for the maskedregions (see .2.1), it provides limited in-formation for deciding which frames to mask. Tomitigate this problem, we introduce lightweightloss predictors ds, dt, which can be easily inte-grated with the student-teacher network, and addreconstruction loss predicting capabilities acrossboth networks. To train these loss predictors, wepropose a novel auxiliary loss Laux that guides ittowards capturing relative correlations between in-dividual frames rather than forcing the predictor togenerate exact frame-level reconstruction values.Specifically, for each masked frame (i, j) wherei = j and (i, j) {1, 2, ..., N}, if Lreci> Lrecjthan the predicted counterpart Lsp = ds(fs( Z))must also have Lspi > Lspj. To formulate this con-straint as a differentiable objective function, wefirst define a target distribution as an indicator vari-able I that captures the relative correlations be-tween original reconstruction loss values, such asLreci> Lrecj. Formally we define this as follows:",
  "epochs": ": For a random speech utterance, we show the varia-tion in frame-level reconstruction loss values across trainingepochs. During the initial stages of EH-MAM pre-training,we find that the model exhibits high frame-level reconstruc-tion loss values, which results in low distinctiveness amongstindividual values. This leads to increased stochasticity in theselective masking. where Si,j > 0.5 if Lspi > Lspj. Finally, we formu-late our auxiliary objective function Laux by firstcomputing a vanilla cross entropy H() betweenthe target distribution I and the predicted distribu-tion S: Laux H(I, S) and then minimizing itjointly with the reconstruction loss. We define theformulation of Laux below:",
  "Selecting Hard Regions forReconstruction": "Motivation: shows that during the initialstage of a EH-MAM pre-training, the reconstruc-tion loss values are significantly high and exhibitlow discriminative power (Lreci Lrecj ). Thisleads to increased stochasticity in the overall selec-tive masking process. Thus, inspired by the generalhuman learning approach, where humans do notperceive knowledge uniformly but are subjected toa learning environment where they progressivelycomprehend more complex information, we pro-pose an easy-to-hard masking strategy that guidesthe model to progressively mask harder regions forreconstruction. Specifically, we linearly increasethe proportion of mask indices associated with hardregions at each training epoch. We define hardregions as a collection of speech frames that the model finds difficult to reconstruct.We illustrate the masking strategy in . Ateach training iteration t and with a masking per-centage P, we first compute P S and P R, the in-dividual masking percentages for selective andrandom masking respectively. Precisely, we up-date P S and P R linearly as P S = P t T andP R = 1P S, where T is the total number of train-ing iterations. In selective masking, for each sam-pled batch z Z, we build mask MS by selectingframe indices associated with the top k predictedreconstruction values Ltp. We use k = P SF(z),where F(z) denotes the number of speech framesfor an input batch z. To build a random mask MR,we randomly sample P RF(z) frame indices. Fi-nally, we compute the adaptive mask MA by takinga union of MS, MR. We summarize the completeprocess of easy-to-hard masking in Algorithm 1",
  "Experimental Setup": "Pre-training Following Baevski et al. (2022,2023); Liu et al. (2024), we pre-trained our modelwith 960 hours of unlabelled speech from Lib-riSpeech corpus (Panayotov et al., 2015). Due toresource constraints, we use a base variant of thecontext encoder (Baevski et al., 2020), with thenumber of transformer layers K = 12 and maskingpercentage = 50%. For the loss predictor and thereconstruction decoder, we utilize 1D-convolutionlayers, with the number of convolution layers D= 4. Moreover, a balancing parameter is intro-duced and set to 0.05 during the joint optimizationof reconstruction and auxiliary loss. All the pre-training experiments are performed on 4 A10040GB GPUs, for 400k updates and using a batchsize of 63 minutes of speech (Additional detailson the hyper-parameters used in EH-MAM can befound in ).Fine-tuning Similar to Liu et al. (2024), to showthe effectiveness of the learned speech represen-tation, we fine-tune only the student counterpartwith an additional CTC layer (Graves et al., 2006).We conduct a comprehensive evaluation under alow-resource labeled data setting using a 10 mins/ 1 hour / 10 hours split from LibriLight bench-mark (Kahn et al., 2020) and 100 hours split fromLibrispeech (Panayotov et al., 2015). For all thesplits, we follow a similar fine-tuning setup aswav2vec2 (Baevski et al., 2020) (We provide addi-tional fine-tuning details for all the splits in the Ap-pendix B.1). We also perform a SUPERB (Speech",
  "EH-MAM3.864.8997.0198.0189.4722.04": ": Results on Speech Processing Universal PERfor-mance Benchmark (SUPERB). The downstream tasks in-clude phoneme recognition (PR), automatic speech recogni-tion (ASR), keyword spotting (KS), intent classification (IC),and slot filling (SF). The evaluation metrics used are accuracy(Acc), phoneme error rate (PER), word error rate (WER), f1score (F1), and concept error rate (CER). The best and thesecond best results are bolded and underlined respectively. Processing Universal PERformance Benchmark)evaluation (wen Yang et al., 2021), where a sepa-rate prediction head is trained on top of a frozenpre-trained model for various downstream tasks(Additional details on the downstream tasks presentin SUPERB can be found in Appendix B.2) Baselines We compare the performance of EH-MAM across various SSL-based speech represen-tation learning baselines that employ 1) single en-coder: wav2vec 2.0 (Baevski et al., 2020), Hu-BERT (Hsu et al., 2021) and 2) self-distillationnetwork: data2vec (Baevski et al., 2022), data2vec2.0 (Baevski et al., 2023) and DinoSR (Liu et al.,2024) to reconstruct masked frames (Additionaldetails on all the baselines can be found in Ap-pendix A). Due to compute constraints, we avoidretraining the baselines from scratch and use thecheckpoints open-sourced by the authors. Dataset and Evaluation Metric We pre-train EH-MAM on 960 hours of unlabelled speech data fromthe LibriSpeech corpus (Panayotov et al., 2015).Further, we evaluate EH-MAM on a wide rangeof speech-related downstream tasks, including 1)Low resource ASR benchmarks: Libri-Light (Kahnet al., 2020), 100 hours LibriSpeech corpus (Panay-otov et al., 2015), Wall-Street Journal (WSJ) (Pauland Baker, 1992), SwitchBoard (Godfrey et al.,1992) and 2) SUPERB evaluation: a collection of adiverse set of downstream tasks including PhonemeRecognition (PR), Automatic Speech Recognition(ASR), Keyword Spotting (KS), Intent Classifica-tion (IC) and Slot Filling (SF). Additional detailson duration, train/test splits, and evaluation metricscan be found in Appendix B.",
  "EH-MAM400k632.26.12.86.3": ": Results on LibriLight benchmark and LibriSpeech for ASR. All the models share a similar BASE size encoder and arefirst fine-tuned with a 10 min / 1hr / 10hr / 100hr labeled dataset and then evaluated on common dev/test splits. The evaluationmetric used is word error rate (WER). The best and the second best results are bolded and underlined respectively",
  "Results and Analysis": "In this section, we present the quantitative and qual-itative results. For quantitative evaluation, we firstfine-tune EH-MAM on LibriLight (Kahn et al.,2020) and evaluate across all the test splits. Next,to show the scalability of the speech representa-tions learned by EH-MAM, we conduct a down-stream evaluation on SUPERB benchmark (wenYang et al., 2021). Additionally, we also performa qualitative analysis on the masked regions pre-dicted by the EH-MAM. All the results reportedfor EH-MAM are averaged across five runs.",
  "Evaluation on Low-Resource ASR": "For low-resource ASR evaluation, we follow a sim-ilar procedure as Baevski et al. (2020) whereinwe fine-tune only the student counterpart of EH-MAM with an additional CTC layer (Graves et al.,2006) on top. We perform fine-tuning using low-resource labeled datasets under four different se-tups, 10min / 1hour / 10hour from LibriLight (Kahn et al., 2020) and 100hour Librispeech (Panayotovet al., 2015). For evaluation, we use the standard de-v/test split of Librispeech and report the word errorrate (WER) by decoding with the official 4-gramlanguage model. Following the prior work Baevskiet al. (2022, 2023), the decoding hyper-parameteris searched with Ax (refer to Section C.1). Asshown in , EH-MAM consistently outper-forms all the prior SSL methods across all the se-tups. We also provide additional results on otherlow-resource ASR benchmarks such as Wall StreetJournal (WSJ) (Paul and Baker, 1992) and Switch-Board (SB) (Godfrey et al., 1992) in Appendix E.",
  "Random MaskingEH-MAM Masking": ": We compare the increase in relative Word ErrorRate (WER) by selectively masking hard regions predicted bythe loss predictor (EH-MAM Masking) Vs randomly maskingframes. The increase in relative WER indicates that the EH-MAM Masking scheme masks useful context in an input. tent, speaker, semantics, and paralinguistics. Toinvestigate the models capabilities to understandspeech content and semantics, we report the resultson phoneme recognition (PR), automatic speechrecognition (ASR), keyword spotting (KS), intentclassification (IC), and slot filling (SF) (Additionaldetails on all the downstream tasks can be found inAppendix B.2). For downstream evaluation on SU-PERB, we follow a similar setup as wen Yang et al.(2021), where we train a prediction head on top ofthe frozen pre-trained models instead of completefine-tuning. As shown in , for semantictasks like IC and SF, the EH-MAM outperformsprior art, showing its capabilities to capture bettersemantic information from speech input. On con-text tasks, EH-MAM surpasses prior art in KS andachieves comparable performance on PR and ASR.",
  "Qualitative Analysis": "EH-MAM mask useful context: To show EH-MAM does mask useful context, we conduct asimple experiment wherein during ASR inference,we selectively mask the frames with high predictedreconstruction value using the loss predictor andcompare the increase in relative WER with ran-dom masking. As shown in , under SUPERBevaluation setting for ASR (refer ), wefind selectively masking frames with EH-MAMconstantly shows a higher relative WER when com-pared to random masking across various maskingpercentages. Higher relative WER indicates thata selective masking scheme with the EH-MAMmasks useful context in a speech input.",
  "easy-to-hard Masking": ": We compare the effectiveness of EH-MAM in op-timizing a MAM pretext task, such as the reduction in recon-struction loss, with hard and easy-to-hard masking schemes.The easy-to-hard masking scheme shows better convergencein reconstruction loss compared to hard masking strategies. EH-MAM adapts well towards reconstructinghard regions: To show how well EH-MAM adaptstowards reconstructing hard regions, we conductan experiment wherein we compare the EH-MAMability to reconstruct hard regions (collection offrames with high reconstruction values) using 1)hard masking, masking only hard regions at eachepoch and 2) easy-to-hard masking, where we pro-gressively introduce hard regions with randomlymasked regions at each epoch. As shown in ,while pre-training EH-MAM, easy-to-hard mask-ing scheme shows better convergence in reconstruc-tion loss when compared with hard masking strate-gies. This indicates that progressively introduc-ing hard regions in an easy-to-hard manner, im-proves EH-MAM adaptability toward reconstruct-ing masked regions during pre-training.",
  "Conclusion": "In this paper, we propose EH-MAM, a novel SSLframework for learning robust speech represen-tations. In contrast to prior work that relies onrandom masking schemes for creating MAM pre-text tasks, EH-MAM first identifies hard regionsto reconstruct using a teacher network and thenchallenges the student to reconstruct them by pro-gressively introducing hard regions throughout thelearning process. Next, we introduce an easy-to-hard masking scheme that guides the EH-MAMto mask harder regions to reconstruct step-by-step.EH-MAM outperforms all the other models onpopular low-resource ASR benchmarks and down-stream evaluation on SUPERB.",
  "We do not employ a LARGE size encoder inEH-MAM, for example, a 24-layer variantused by Baevski et al. (2023) due to computeconstraints": "The loss-predictors used in EH-MAM in-crease the trainable parameter count com-pared to other baselines such as data2vec2.0 (Baevski et al., 2023) during pre-training.However, we acknowledge that this accountsonly for a slight increase in the total parametercount (roughly 5%). Due to recourse constraints, we conduct thedownstream evaluation on SUPERB for con-text and semantic-related tasks. We plan toextend the evaluation across speaker and par-alinguistic tasks in the future.",
  "This project is supported in part by NSF#1910940": "Alexei Baevski, Arun Babu, Wei-Ning Hsu, andMichael Auli. 2023. Efficient self-supervised learn-ing with contextualized target representations for vi-sion, speech and language. In International Con-ference on Machine Learning, pages 14161429.PMLR. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, ArunBabu, Jiatao Gu, and Michael Auli. 2022. Data2vec:A general framework for self-supervised learningin speech, vision and language.In InternationalConference on Machine Learning, pages 12981312.PMLR. Alexei Baevski and Abdelrahman Mohamed. 2020. Ef-fectiveness of self-supervised pre-training for asr.In ICASSP 2020-2020 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 76947698. IEEE. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,and Michael Auli. 2020. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations.Advances in neural information processing systems,33:1244912460.",
  "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.2021. Beit: Bert pre-training of image transformers.arXiv preprint arXiv:2106.08254": "Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee.2022.Distilhubert: Speech representation learn-ing by layer-wise distillation of hidden-unit bert.In ICASSP 2022-2022 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 70877091. IEEE. Sanyuan Chen, Chengyi Wang, Zhengyang Chen,Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.Wavlm: Large-scale self-supervised pre-training forfull stack speech processing. IEEE Journal of Se-lected Topics in Signal Processing, 16(6):15051518. Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma,Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song,Xian-Ling Mao, Heyan Huang, et al. 2021. Xlm-e:Cross-lingual language model pre-training via electra.arXiv preprint arXiv:2106.16138. Alice Coucke, Alaa Saade, Adrien Ball, ThodoreBluche, Alexandre Caulier, David Leroy, ClmentDoumouro, Thibault Gisselbrecht, Francesco Calt-agirone, Thibaut Lavril, Mal Primet, and JosephDureau. 2018. Snips voice platform: an embeddedspoken language understanding system for private-by-design voice interfaces. Preprint, arXiv:1805.10190. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. John J Godfrey, Edward C Holliman, and Jane Mc-Daniel. 1992. Switchboard: Telephone speech cor-pus for research and development.In Acoustics,speech, and signal processing, ieee international con-ference on, volume 1, pages 517520. IEEE Com-puter Society. Alex Graves, Santiago Fernndez, Faustino Gomez, andJrgen Schmidhuber. 2006. Connectionist temporalclassification: labelling unsegmented sequence datawith recurrent neural networks. In Proceedings of the23rd international conference on Machine learning,pages 369376. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-otr Dollr, and Ross Girshick. 2022. Masked autoen-coders are scalable vision learners. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1600016009. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460.",
  "Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, andRaj Reddy. 2001. Spoken language processing: Aguide to theory, algorithm, and system development.Prentice hall PTR": "Jacob Kahn, Morgane Riviere, Weiyi Zheng, EvgenyKharitonov, Qiantong Xu, Pierre-Emmanuel Mazar,Julien Karadayi, Vitaliy Liptchinsky, Ronan Col-lobert, Christian Fuegen, et al. 2020. Libri-light:A benchmark for asr with limited or no supervision.In ICASSP 2020-2020 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 76697673. IEEE. Ioannis Kakogeorgiou, Spyros Gidaris, Bill Pso-mas, Yannis Avrithis, Andrei Bursuc, KonstantinosKarantzalos, and Nikos Komodakis. 2022. What tohide from your students: Attention-guided maskedimage modeling. In European Conference on Com-puter Vision, pages 300318. Springer. Serkan Kiranyaz, Turker Ince, Osama Abdeljaber, OnurAvci, and Moncef Gabbouj. 2019. 1-d convolutionalneural networks for signal processing applications.In ICASSP 2019-2019 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 83608364. IEEE. Hung-yi Lee, Abdelrahman Mohamed, Shinji Watan-abe, Tara Sainath, Karen Livescu, Shang-Wen Li,Shu-wen Yang, and Katrin Kirchhoff. 2022. Self-supervised representation learning for speech pro-cessing. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies: Tutorial Abstracts, pages 813. Associa-tion for Computational Linguistics. Alexander H Liu, Heng-Jui Chang, Michael Auli, Wei-Ning Hsu, and Jim Glass. 2024.Dinosr: Self-distillation and online clustering for self-supervisedspeech representation learning. Advances in NeuralInformation Processing Systems, 36. Andy T. Liu, Shu-wen Yang, Po-Han Chi, Po-chunHsu, and Hung-yi Lee. 2020. Mockingjay: Unsu-pervised speech representation learning with deepbidirectional transformer encoders. In ICASSP 2020- 2020 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 64196423. Vasista Sai Lodagala, Sreyan Ghosh, and SrinivasanUmesh. 2023. data2vec-aqc: Search for the rightteaching assistant in the teacher-student trainingsetup. In ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 15. IEEE.",
  "Nafis Sadeq, Canwen Xu, and Julian McAuley. 2022b": "Informask: Unsupervised informative masking forlanguage model pretraining. In Proceedings of the2022 Conference on Empirical Methods in Natu-ral Language Processing, pages 58665878, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Antti Tarvainen and Harri Valpola. 2017. Mean teachersare better role models: Weight-averaged consistencytargets improve semi-supervised deep learning re-sults. In Advances in neural information processingsystems, pages 11951204. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30.",
  "Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed,and Hung yi Lee. 2021. Superb: Speech process-ing universal performance benchmark.Preprint,arXiv:2105.01051": "Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.2022. Retromae: Pre-training retrieval-oriented lan-guage models via masked auto-encoder. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 538548. As-sociation for Computational Linguistics. Gene-Ping Yang, Sung-Lin Yeh, Yu-An Chung, JamesGlass, and Hao Tang. 2022. Autoregressive predic-tive coding: A comprehensive study. IEEE Journalof Selected Topics in Signal Processing, 16(6):13801390. Xianghu Yue, Jingru Lin, Fabian Ritter Gutierrez, andHaizhou Li. 2022. Self-supervised learning with seg-mental masking for speech representation.IEEEJournal of Selected Topics in Signal Processing,16(6):13671379.",
  "ABaseline Details": "wav2vec 2.0. 2 (Baevski et al., 2020) The wav2vec2.0 model integrates contrastive learning withmasking. Similar to the CPC model (Oord et al.,2018), it employs the InfoNCE loss (Baevski et al.,2020) to maximize the similarity between a contex-tualized representation (anchors) and a localizedrepresentation (positives) simultaneously minimiz-ing the similarity with other masked regions (nega-tives). Instead of directly using the contextualizedrepresentations, wav2vec 2.0 employs a separatequantization module to generate positives and neg-atives.HuBERT. 3 (Hsu et al., 2021) Like BERT (De-vlin et al., 2019), HuBERT follows a generativeapproach by discretizing the continuous MFCCfeatures using the K-means algorithm and creatingtargets by randomly masking the quantized units.Unlike BERT, HuBERT employs a two-iterationtraining process wherein, in the first iteration, themodel is trained to predict targets generated fromthe MFCC features, followed by quantizing thelearned representations obtained from the first it-eration training using K-means to generate newtargets, which the model utilize in the second itera-tion training. WavLM. 4 (Chen et al., 2022) WavLM extendsthe HuBERTs learning paradigm by introducing agated relative position bias (Chi et al., 2021) at eachtransformer layer. Further, WavLM proposes anutterance-mixing strategy wherein training samplesare augmented by mixing utterances from differ-ent speakers, and the targets are created from theoriginal sample. data2vec. 5 (Baevski et al., 2022) data2vec in-troduces a self-distillation-based student-teachernetworks for speech representation learning. Thecore idea is to predict the latent representations ofthe whole speech unlabeled data from the maskedview. data2vec trains a student network by feed-ing a masked version of input to predict the latentrepresentation obtained by feeding the whole inputto a teacher network. The teachers parameters areupdated by the exponential moving average (ema)of the students parameters. data2vec 2.0. 6(Baevski et al., 2023) data2vec2.0 uses an identical learning objective as data2vecbut with two key changes. Firstly, data2vec 2.0introduces a lightweight decoder module that re-constructs the masked frames in student represen-tation before maximizing the similarity with theteacher representations. Next, data2vec 2.0 em-ploys a multi-mask strategy where multiple maskvariants of the same input are fed to the student net-work, followed by calculating reconstruction lossfor all the variants with a common teacher repre-sentation obtained from the original speech input. DinoSR. 7 (Liu et al., 2024) DinoSR uses similararchitecture as (Baevski et al., 2022), but introducesa novel gradient-free online clustering method forlearning discrete acoustic units. DinoSR initiallyemploys a teacher network to extract contextual-ized embeddings from the input audio. It thenapplies an online clustering scheme to these em-beddings to create a machine-discovered phoneinventory. Finally, it uses the discretized tokens toguide a student network.",
  "B.1ASR Evaluation": "LibriSpeech. 11 (Panayotov et al., 2015) The Lib-riSpeech dataset is a widely-used corpus of Englishread speech with approximately 1000 hours of au-diobooks available in the public domain, whichincludes a broad range of speakers, both male andfemale, with diverse accents and ages, providing arich source for speech and language research. Wepre-train our model on 960 hours LibriSpeech un-labeled data and fine-tune for ASR evaluation on100 hours of labeled data.Libri-Light. 12 (Kahn et al., 2020) The Libri-lightis a dataset derived from the LibriVox project, con-sisting of audiobooks in the public domain, muchlike the LibriSpeech dataset, but aims to address thelimitations of traditional ASR datasets by provid-ing 60 hours of unlabelled speech complementedwith a smaller amount of labeled data. We conductevaluation on ASR with labeled data of 10 mins / 1hour / 10 hours split from LibriLight.WSJ. 13 (Paul and Baker, 1992) The WSJ datasetconsists of approximately 80 hours of read speechderived from articles in the Wall Street Journal, of-fering high-quality audio and transcriptions idealfor training and evaluating ASR systems. The WSJdataset includes recordings from 84 speakers, pro-viding diverse voice samples, including accurateword-level transcriptions for all audio files and",
  "metadata for speaker identities and recording con-ditions. We use the WSJ dataset for our ASR taskevaluation on 80 hours of unlabeled data for train-ing and 1.5 hours for of labeled data for testing": "Switchboard. 14 (Godfrey et al., 1992) The Switch-board is a telephone speech corpus consisting ofapproximately 260 hours of speech, which includes2,400 two-sided telephone conversations among543 speakers. The dataset conversations cover 70different topics, from current events to personalinterests, providing varied and natural discourse,making it an invaluable resource in the field ofspeech recognition, dialogue systems, and conver-sational analysis. We report the our ASR evaluationon 30 hours of unlabeled data for training and 5hours of data for testing.",
  "B.2SUPERB (Speech processing UniversalPERformance Benchmark)": "SUPERB (wen Yang et al., 2021) is a leaderboardto benchmark the performance of a shared modelacross a wide range of speech processing taskswith minimal architecture changes and labeled data.The key focus here is to extract the representa-tion learned from SSL and to learn task-specializedlightweight prediction heads on top of the frozenshared models. Below we detail the tasks in SU-PERB that we use for evaluation.",
  ": (Left) EH-MAM pre-training hyper-parameters. IN is instance normalization; AVG is mean pooling. (Right)EH-MAM fine-tuning hyper-parameters for LibriLight (Kahn et al., 2020)": "Phoneme Recognition (PR) converts spoken lan-guage into its smallest units of sound, known asphonemes. This task incorporates alignment mod-eling to circumvent issues with incorrect forcedalignments. The LibriSpeech (Panayotov et al.,2015) subsets train-clean-100/dev-clean/test-cleanare utilized for training, validation, and testing inthe SUPERB framework. The primary metric forevaluation is the phone error rate (PER). Automatic Speech Recognition (ASR) transcribesspoken words into text. While PR focuses on theprecision of phoneme modeling, ASR assesses im-provements in terms of their practical relevance.The training, validation, and testing phases usethe LibriSpeech (Kahn et al., 2020) subsets train-clean-100/dev-clean/test-clean. The word error rate(WER) serves as the evaluation metric. Keyword Spotting (KS) involves the detectionof specified keywords within speech, categorizingutterances into a set list of terms. The SpeechCommands dataset v1.0 (Warden, 2018), whichincludes ten keyword categories, a silence category,and an \"unknown\" category for erroneous detec-tions, is used in this task. Accuracy (ACC) is themetric for assessing performance.",
  "Intent Classification (IC) assigns categories tospoken utterances to ascertain the speakers intent": "It employs the Fluent Speech Commands (Lugoschet al., 2019) dataset, where utterances are labeledaccording to three intent categories: action, object,and location. The evaluation metric here is alsoaccuracy (ACC).Slot Filling (SF) entails predicting a series ofsemantic slots from speech. The Audio SNIPS(Coucke et al., 2018) dataset, which features synthe-sized multi-speaker utterances for the SNIPS NLUbenchmark, is used for this purpose. Evaluationis based on the slot-type F1 score and slot-valuecharacter error rate (CER).",
  "DAdditional Details: General": "Compute details. For all our pre-training and fine-tuning experiments, we used four NVIDIA A100-40GB GPUs. The pre-training EH-MAM requiresfive days of training and consists of 94.40M pa-rameters. All the fine-tuning experiments on Lib-riLight (Kahn et al., 2020) require two days each.Additionally, individual downstream evaluation onSUPERB requires one day.Potential Risk. As the EH-MAM follows a self-supervised training regime, it may learn spuriouscorrelations which can affect downstream perfor-mance on ASR, PR, etc. Moreover, EH-MAM might get biased towards a particular type of ac-cent, dialect, or domain, such as telephonic or readspeech, due to a huge amount of unlabeled data,which may not be diverse.Software and Packages details. We implementall our models in PyTorch16 and use Fairseq 17",
  "EAdditional Results": "We present additional results for low-resource ASRevaluation on WSJ (Paul and Baker, 1992) andSwitchboard (Godfrey et al., 1992). The evalua-tion settings for both datasets are similar to Lib-riLight (Kahn et al., 2020). Train/Test splits forboth datasets can be found in . As shown in, EH-MAM outperforms the state-of-the-artmodel, data2vec 2.0, across all the dev/test splits.",
  "Algorithm 2 Pseudo-Code of Easy-to-Hard Masking": "defcompute_mask_indices_ema_loss (shape :Tuple [ int ,i n t ] ,padding_mask :Optional [ t o r c h . Tensor ] ,l o s s _ p r e d :Optional [ t o r c h . Tensor ] ,mask_prob :f l o at ,mask_length :int ,current_epoch :int ,t o t a l _ e p o c h :int ,mask_type :s t r = \" s t a t i c \" ,min_masks :i n t = 0 ,require_same_masks :bool = True ,mask_dropout :f l o a t =0 . 0 ) : bsz ,a l l _ s z = shapemask = np . f u l l ( ( bsz ,a l l _ s z ) ,False )# add a random numberf o rp r o b a b i l i s t i croundingall_num_mask = i n t (mask_prob *a l l _ s z/f l o a t ( mask_length ) + np . random . rand ( ) )# Getthel o s sl a t t i c efromdecoderi d s _ s h u f f l e _ l o s s= t o r c h . a r g s o r t ( loss_pred ,dim = 1 ) . cpu ( ) . detach ( ) . numpy ( ) all_num_mask = max( min_masks ,all_num_mask )# guidethemakingwrttot r a i n i n gepoch# k e e p _ r a t i o = 1.0k e e p _ r a t i o = f l o a t ( ( current_epoch + 1)/t o t a l _ e p o c h )mask_idcs = foriinrange ( bsz ) :i fpadding_maski snot None :sz = a l l _ s z padding_mask [ i ] . long ( ) . sum ( ) . item ( )num_mask = i n t (# add a random numberf o rp r o b a b i l i s t i croundingmask_prob * sz/f l o a t ( mask_length )+ np . random . rand ( ))num_mask = max( min_masks ,num_mask )e l s e :sz = a l l _ s znum_mask = all_num_mask",
  "i fsum( l e n g t h s ) == 0:l e n g t h s = min ( mask_length ,sz 1)": "min_len = min ( l e n g t h s )i fsz min_len <= num_mask :min_len = sz num_mask 1# r e v e r s etheindexl i s ttogetthein dexesa s s o c i a t e dwith maxl o s s e ssample_loss_index =i d s _ s h u f f l e _ l o s s [ i ] [ : : 1 ]",
  "i flen ( mask_idc ) == 0:combine_idc = loss_mask_idce l s e :combine_idc = np . c o n c a t e n a t e ( ( loss_mask_idc ,mask_idc ) )": "mask_idcs . append ( np . unique ( combine_idc [ combine_idc < sz ] ) )min_len = min ( [ len (m)for m inmask_idcs ] )fori ,mask_idcinenumerate ( mask_idcs ) :i flen ( mask_idc ) > min_len and require_same_masks :mask_idc = np . random . choice ( mask_idc ,min_len ,r e p l a c e = False )i fmask_dropout > 0:num_holes = np . r i n t ( len ( mask_idc ) * mask_dropout ) . ast ype ( i n t )mask_idc = np . random . choice (mask_idc ,len ( mask_idc ) num_holes ,r e p l a c e = False)mask [ i ,mask_idc ] = Truereturn mask"
}