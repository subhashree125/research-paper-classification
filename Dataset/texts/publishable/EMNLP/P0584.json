{
  "Abstract": "The integration of visual and textual informa-tion represents a promising direction in theadvancement of language models.In thispaper, we explore the dual modality of lan-guageboth visual and textualwithin an au-toregressive framework, pre-trained on bothdocument images and texts. Our method em-ploys a multimodal training strategy, utilizingvisual data through next patch prediction witha regression head and/or textual data throughnext token prediction with a classification head.We focus on understanding the interaction be-tween these two modalities and their combinedimpact on model performance.Our exten-sive evaluation across a wide range of bench-marks shows that incorporating both visual andtextual data significantly improves the perfor-mance of pixel-based language models. Re-markably, we find that a unidirectional pixel-based model trained solely on visual data canachieve comparable results to state-of-the-artbidirectional models on several language un-derstanding tasks.This work uncovers theuntapped potential of integrating visual andtextual modalities for more effective languagemodeling.We release our code, data, andmodel checkpoints at",
  "Introduction": "Recent advancements in large language models(LLMs) have pushed the boundaries of their ca-pabilities in diverse applications, including lan-guage assistant (Touvron et al., 2023a), code gen-eration (Lozhkov et al., 2024; Chai et al., 2023),and multimodal comprehension (OpenAI, 2023;Anil et al., 2023). LLMs typically tokenize inputtext into sequences of discrete subword units, al-lowing for a wide array of applications. However,tokenization-based approaches struggle with visu-ally complex textual content, such as PDFs, where",
  "*Work done during QL and JXs internship at Baidu": "converting visual data into plain text often results insignificant information loss. Traditional solutionsrely on optical character recognition (OCR) modelsfor extracting text from images, but these methodsare inherently limited by the accuracy of text ex-traction and the fidelity of the original documentstructure.To address these challenges, recent work has in-troduced a new paradigm: pixel-based languagemodeling. This approach learns directly from thevisual representation of text (as images) rather thanrelying solely on tokenized text. Models such asPIXEL (Rust et al., 2023) exemplify this shift, offer-ing solutions that circumvent the limitations of tra-ditional tokenization by treating text as image data.Pixel-based modeling also addresses the vocabu-lary bottlenecka trade-off between input encod-ing granularity and the computational costs asso-ciated with vocabulary estimation in conventionallanguage models (Rust et al., 2023).In the previous literature, the development ofpixel-based language models has been bifurcatedinto encoder-based (Rust et al., 2023; Tschan-nen et al., 2023) or encoder-decoder architec-tures (Salesky et al., 2023), encompassing modelsthat either employ bidirectional mechanisms akinto MAE (He et al., 2022) or utilize an encoder-decoder framework, where a pixel-based modelserves as the encoder, paired with a unidirectionallanguage decoder. Despite these advancements,the exploration of pixel-based models employing adecoder-centric approach remains in its infancy.Moreover, current research often processes vi-sual text as 8-bit grayscale (Rust et al., 2023) or 2-bit binary images (Tai et al., 2024). This approachconstrain the richness of the visual input, especiallywhen processing content with color information,such as emojis or highlighted text. This limitationsuggests that processing real-valued RGB imagescould offer a more detailed representation of vi-sual text. However, the potential of pre-training autoregressive language models on raw RGB im-ages, which more closely mirror the natural visualcharacteristics of documents, has not been fullyexplored.This research addresses two distinct challengesin language modeling:(1) the feasibility oftokenization-free autoregressive pre-training usingPixelGPT, and (2) the synergistic benefits of multi-modal pre-training with DualGPT.First, we focus on the performance of PixelGPT,a tokenization-free model that processes raw vi-sual text images. We investigate whether trainingan autoregressive model directly on real-valuedpixels can achieve competitive results without to-kenization, particularly in multilingual contexts.This exploration assesses whether PixelGPT canovercome the vocabulary bottleneck in multilingualtasks by generalizing linguistic features across di-verse languages, thus bypassing the constraints ofpredefined vocabularies typically encountered intraditional text-based models.Second, we evaluate DualGPT, which integratesboth visual and textual modalities during pre-training. By leveraging pixel-based and text-basedpre-training together, DualGPT is designed to har-ness the interaction between these two modalities.We explore how this multimodal strategy improvesmodel performance on language understandingtasks and cross-lingual generalization, offering ad-vantages over models that rely on a single modality. ContributionTo conclude, our main contribu-tions are as follows: We empirically demonstrate the substantial po-tential of integrating visual text images for en-hanced language model training. We show thatpre-training decoder-only transformers on vi-sual images can match or slightly underperformcompared to text-based inputs but achieve com-petitive results with bidirectional PIXEL mod-els (Rust et al., 2023). This illustrates the po-tential for scaling trends to eventually surpasstext-based pre-trained models. We systematically explore autoregressive pre-training on both visual text images and plaintext modalities, demonstrating the potential ofcausal language models to effectively learn fromvisual text images and highlighting the interplaybetween different modalities.",
  "Pixel Representations for Text": "Advances in pixel-based language modeling haveincreasingly focused on exploiting the orthographicand typographic properties of text through visualrepresentations. PIXEL (Rust et al., 2023) utilizesmasked auto-encoders to address the vocabularybottleneck by reconstructing pixels in masked textimages.Moreover, CLIPPO (Tschannen et al., 2023) demonstrates enhanced language comprehen-sion using a unified encoder for both image and textmodalities. Further research by Lotz et al. (2023)evaluates the impact of rendering techniques onthe efficacy of pixel-based encoders. These studiesprimarily utilize bidirectional encoders and processtext as grayscale images.In contrast, our approach leverages RGB imag-ing to render text, employing a 24-bit color depth toenrich the visual data interpretation. This enhance-ment allows for handling of elements like emojisand colored text, prevalent in digital communica-tions. Concurrent work by Tai et al. (2024) exploresbinary image rendering and binary cross-entropyloss in discrete space, whereas we implement amean square error loss in continuous pixel spacefor finer reconstruction granularity. Moreover, re-search such as OCR-free visually-rich documentunderstanding (Kim et al., 2022), which focuseson direct learning from visual document images,shares similarities with our approach. However, ourwork distinctively explores rendered text, expand-ing the potential for comprehensive multimodaltext pre-training.",
  ": Detailed comparison of pixel-based baselines": "For fair comparison, we summarize the compar-ison of our PixelGPT with pixel-based baselines,including PIXEL (Rust et al., 2023), PIXAR (Taiet al., 2024), in . It is worth noting that ourwork is different from PIXAR, which uses differenttraining objectives and data rendering approachesfrom PIXEL and ours. Instead, our model can beseen as an autoregressive version of PIXEL.",
  "Autoregressive Pre-training on Pixels": "Existing methods in pixel-based autoregressivepre-training divide into vector quantization tech-niquestransforming continuous images into dis-crete tokensand direct pixel prediction. Theseapproaches include VQ-VAE (Van Den Oord et al.,2017) and VQGAN (Esser et al., 2021) followed bynext token prediction (Chen et al., 2020; Rameshet al., 2021), and prefix language modeling thatpredicts future visual patches from bidirectionalpixel contexts (El-Nouby et al., 2024).These models are trained on regular images. Ourresearch diverges by focusing exclusively on visualand rendered texts, thereby extending the capabilityof autoregressive models to understand and gener-ate language from its visual form.",
  "Rendering Text as Images": "Following Rust et al. (2023), we utilize textrenderer adept at converting textual data into avisually-rich RGB format. This pivotal componenttakes input text and transforms it into a detailedRGB image, x RHWC. We define the height(H) at 16 pixels and the width (W) at 16,384 pix-els, encapsulating the text within a 24-bit colordepth across three channels (C = 3), thus forminga visual text image that represents a grid of 1024patches, each 16x16 pixels in size.The text renderer supports rendering requiredfor a diverse set of textual representations, includ-ing multicolored emojis, bidirectional text systems, and scripts necessitating the use of ligatures. Inalignment with models like PIXEL, our text se-quences may be single paragraphs or pairs of re-lated segments. We use 16x16 black patches as vi-sual cues for end-of-sequence (EOS) marker. Thesepatches are treated as non-interactive elements byour model, where no attention mechanism is en-gaged or loss calculated.When confronted with sequences that surpassthe maximum length threshold, our model employsstrategies of truncation or segmentation into multi-ple sequences, ensuring efficient processing whilepreserving contextual integrity. We refer to Ap-pendix A for the rendering details.",
  "Input TextRendered Image": ": Illustration of dual-modality pre-training on paired text-image (DualGPT). Autoregressive pre-training onpure text and visual text images, apply next patch prediction and next token prediction, respectively. denotes the total number of patches. The patchesare then flattened, mapped to a D-dimensionalspace through a learnable linear projection, andfinally fed into the transformers sequential pro-cessing stream. Unlike ViT, which caters to two-dimensional inputs, our model processes thesepatches in the sequence order in which the textappears, emulating the linear progression of read-ing. This patch-based segmentation aligns with thesequential nature of language, enabling our modelto predictively learn from the visual data. Text InputWe leverage the same tokenizer asLlama 2, segmenting input text into discrete tokenswith a total vocabulary size of 32k. These tokensare then transformed into dense vector representa-tions through an embedding lookup table.",
  "t=1p(xtp|x1p, x2p, , xt1p)": "For visual inputs, we employ a next patch predic-tion strategy, where a normalized mean squarederror (MSE) loss quantifies the pixel reconstructionaccuracy by comparing the normalized target imagepatches with the reconstructed outputs, excludingthe EOS patches. Next Token PredictionFor text inputs, we uti-lize a conventional next token prediction objective,optimizing a cross-entropy loss that evaluates thefidelity of predicted token sequences generated viateacher-forcing against the ground truth tokens.",
  "Model Configuration": "To explore previous research questions, our pre-training regimen explores various configurationsfor ablation analysis: (1) TextGPT: Pre-trainingsolely on text data. (2) PixelGPT: This involvestraining solely on rendered image data, employinga mean squared error (MSE) loss, as visualizedin (a). (3) MonoGPT: Trained on separatestreams of rendered image and text data withoutany intermodal pairing. (4) DualGPT: Trained onunpaired image and text input, and on paired image-text data (dual-modality). When handling paireddata, we concatenate the image data sequence be-fore the text sequence and feed them simultane-ously to the model, as delineated in . Werefer to Appendix D for details.",
  "Pre-training Details": "Model ArchitectureOur architecture, illustratedin (b), is built upon a stack of N = 24 stan-dard transformer decoder (Vaswani et al., 2017),following Llama 2 (Touvron et al., 2023b). We in-corporate RMSNorm for pre-normalization (Zhangand Sennrich, 2019), SwiGLU activation func-tions (Shazeer, 2020; Chai et al., 2020), rotary po-sition embeddings (Su et al., 2024), and groupedquery attention (Ainslie et al., 2023). Comprehen-sive specifications and additional implementationdetails of our architecture are in Appendix B. DataFor visual image data, we use renderedthe corpus of peS2o, English Wikipedia and C4datasets for pre-training; while for text data, weadopt peS2o, English Wikipedia, C4, CommonCrawl, and The Stack v1. We refer the readersto Appendix C for details.",
  "Experimental Setup": "Fine-tuning ProtocolsOur evaluation entailedfine-tuning an autoregressive pixel-based pre-trained model for downstream tasks to thoroughlyassess its performance. We adapted our pixel-basedmodel to various downstream tasks by substitutingthe language modeling head with a linear MLP fordownstream tasks. Specifically, PixelGPT, initiallypre-trained on pixel data, undergoes fine-tuning onsimilarly rendered pixel data. Conversely, MonoGPTand DualGPT, which benefitted from a joint pre-training regime incorporating both text and pixeldata, were fine-tuned across different input modali-ties: pixel, text, and a combination of both.Evaluation TasksOur assessment of the genera-tive pixel pre-training models encompasses tasks innatural language understanding (NLU) and cross-lingual language understanding. For NLU, we uti-lize the GLUE benchmark, aligning the fine-tuningdata rendering approach with the pre-training pro-cess outlined in Appendix A. Sentence pairs fromGLUEs natural language inference tasks are indi-vidually rendered and subsequently concatenated,with a black block serving as the end-of-sentencetoken. The cross-lingual understanding capabilityis evaluated on the XNLI dataset over fifteen dif-ferent languages. Following Conneau et al. (2020),our evaluation is performed in two distinct sce-narios: (1) Translate-Train-All, where the modelis fine-tuned on a blend of original English andmachine-translated data from other 14 languages,aiming to appraise the models multilingual un-derstanding; (2) Cross-lingual Transfer settings,wherein fine-tuning is conducted solely on En-glish data, with multi-language test sets employedto evaluate the models transferability across lan-guages. Comprehensive experimental details areprovided in the Appendix E.",
  "Results": "Autoregressive Pixel-based Pre-training RivalsPIXEL.Our empirical investigation, detailed in, scrutinizes the feasibility of pure pixel-based autoregressive pre-training on RGB imagesof visual texts. The proposed PixelGPT model,training solely on rich raw visual inputs (24-bitRGB images), demonstrates not merely a competi-tive edge but, in several tasks, surpasses the perfor-mance of models pre-trained on text alone. Specifi-cally, PixelGPT exhibits remarkable superiority onGLUE benchmarks evidenced by its marked per-formance increases on the STS-B (+5.4), MRPC(+13.1), RTE (+11.9), and WNLI (+4.3) assess-ments compared to GPT-2. This demonstrates theviability of pixel-based pre-training in capturingcomplex linguistic constructs.When compared to PIXEL, which leverages abidirectional encoder architecture, PixelGPT ex-hibits enhanced performance in QQP (+1.5), RTE(+3.4), and WNLI (+5.4). These results collec-tively affirm the hypothesis that autoregressivepre-training on raw visual images is feasible forlanguage modeling. PixelGPT achieves the opti-mal performance among pixel-based approaches onGLUE, underscoring the transformative impact ofintegrating rich visual information into pre-training.Refer to G.5 for detailed discussion.As shown in Figures 3 and 4, PixelGPT demon-strates a scaling trend with increased training datacompute, indicating a promising direction for datascaling. This suggests that with more extensivetraining, PixelGPT has the potential to outperformtext-based models, such as GPT-2 and BERT. Dueto computational constraints, we will explore thisin future work. Impact of Autoregressive Pixel Pre-training onMultilingual Tasks.Traditional language mod-els, exemplified by BERT, typically utilize a sub-word tokenization process such as WordPiece (De-vlin et al., 2019) or BPE (Sennrich et al., 2015)that decomposes sentences into a predefined setof text tokens. While effective within the scopeof a single language or similar language families,this approach is constrained by a vocabulary bottle-",
  "PixelGPT1317M77.755.466.769.067.471.259.165.671.461.747.065.254.466.150.563.2": ": Cross-lingual performance evaluation on the XNLI dataset in translate-train-all settings. We report theaccuracy achieved by each model across the multiple languages featured in the XNLI dataset, along with theiraverage accuracy scores. The number of languages (#lg) incorporated during pre-training and the model size(#Param) are provided for reference. PixelGPT demonstrates superior performance over PIXEL, showcasing theefficacy of exclusive pixel-based input modality in cross-lingual contexts. neck (Rust et al., 2023) in multilingual scenarios,limiting its efficacy. Pixel-based representations,however, transcend this limitation by representingtext in a modality that inherently supports unifiedprocessingthe visual domain of images.In our cross-lingual evaluation, conducted onthe XNLI dataset in the translate-train-all config-uration and detailed in , PixelGPT demon-strates a robust capability for multilingual compre-hension. It not only matches the performance ofBERT, but also consistently surpasses the PIXELmodel in average accuracy across evaluated lan-guages.Remarkably, PixelGPT exhibits pro-nounced gains over BERT in languages that di-verge significantly from English, such as Thai andChinese, with improvements of +11.3 and +4.3,respectively. This enhanced performance may beattributed to two primary factors: the absence ofPixelGPTs reliance on language-specific tokeniza-tion, enabling more effective learning from the vi-sual forms of text, and the limitations of BERTsEnglish-centric pre-training, which exhibits short-comings when faced with linguistically distant fam-ilies.Thus, PixelGPTs proficiency in leverag-ing the visual features of text contributes to itsadvanced multilingual understanding, signaling asignificant stride in overcoming the challenges as-",
  "sociated with the vocabulary bottleneck": "Synergistic Effects of Multimodal Pre-training.In our investigation into the interplay between dis-tinct pre-training data modalities, we contrasted theperformances of MonoGPT and DualGPTmodelsthat integrate different input modalitieswith thatof TextGPT under equivalent conditions of alignedtext token pre-training. TextGPT and MonoGPT un-derwent pre-training on 40 billion text tokens, withMonoGPT additionally exposed to 40 billion imagepatches. DualGPT, on the other hand, was pre-trained on 38.4 billion text tokens complementedby 48 billion image patches and 9.6 billion tokensof image-text paired data.This comparative analysis, spanning both GLUEand XNLI datasets (the latter within the translate-train-all settings), is shown in Tables 4 and 5. Apivotal finding is that the incorporation of dual-modality data during pre-training markedly en-hances average performance across language un-derstanding tasks: DualGPT (76.9) surpasses bothTextGPT (76.3) and MonoGPT (75.4). This sug-gests that potential conflicts arising from unimodaltraining can be significantly alleviated through amultimodal pre-training approach. This inferenceis corroborated by XNLI outcomes, wherein the",
  "Analysis": "Scaling Training Tokens vs. GLUE PerformanceIn , we delineate the correlation betweenthe scale of training data and the ensuing per-formance on the GLUE benchmark. Our analy-sis encompasses a spectrum of total training to-kens/patches from 10 billion (B) to 240B, jux-taposing the trajectories of TextGPT, PixelGPT,MonoGPT, and DualGPT, with BERT and PIXELserving as benchmarks. The MonoGPT and DualGPTmodels are evaluated under two different inputmodalities: text and pixel. From our findings, twoprimary insights emerge: (1) Pixel-based autore-gressive pretraining models exhibit an increaseddata demand. With minimal training (e.g., at 10B),pixel-based models initiate at a lower performancethreshold in pixel modality (all under 55%), com-pared to their text modality counterparts, whichapproximate a performance level of 70%. Never-theless, with the increase of training data, a criticalvolume threshold catalyzes a substantial rise in per-formance for PixelGPT, MonoGPT, and DualGPT inpixel modality. This trajectory reveals a progres-sive convergence of PixelGPT towards the text-based baseline, culminating in its overtaking of PIXEL at around 200B tokens/patches and near-ing TextGPT with a less than 5-point performancedifferential, while still on an upward trend. (2)The integration of paired dual-modality dataduring pretraining appears to confer significantbenefits on multimodal learning, particularly forpixel-based input. When matched for training datavolume, DualGPT consistently eclipses MonoGPTacross comparable benchmarks, with the formermaintaining a pronounced lead in pixel modality.This trend underscores the value of incorporatingpaired text-image data in pretraining to enhance theefficacy of multimodal learning. #tokens/patches(B) GLUE (Avg.) Performance",
  ": Training tokens/patches versus overall perfor-mance on XNLI benchmark": "tokens/patches. This comparison, delineated in, focused on the Translate-Train-All settingof the XNLI benchmark.(1) Pixel-based autoregressive models displaya heightened requirement for training data inmultilingual tasks, corroborating the trend ob-served on the GLUE benchmark. Initially, thereis a notable performance disparity between pixeland text modalities, with pixel-based models lag-ging behind when training on a lesser volume oftokens/patches. However, this gap diminishes sub-stantially with the increase in training volume. Re-markably, upon reaching the 200B, PixelGPT notonly surpasses PIXEL but also matches the perfor-mance of BERT, indicating a continued potentialfor further enhancement in its multilingual profi-ciency with additional training data.(2) The injection of dual-modality data at theearly stages of training appears to be particu-larly beneficial for models learning from pixeldata. When comparing DualGPT and MonoGPT un-der the pixel modality, DualGPT demonstrates anotable performance advantage at the outset oftraining (55% vs. 45.8% at the 10B token/patchmark). Although this edge tapers as the trainingvolume expands, it suggests that early-stage mul-timodal alignment aids the pixel-based models inleveraging the textual data for enhanced multilin-gual understanding.(3) Our text-based pre-training approach,TextGPT, demonstrates superior results overBERT. This is evident when training reaches ap-proximately 100B tokens, where TextGPT outper-forms BERT. This improvement may be attributed,in part, to our byte-level BPE tokenization as uti- F1 QQP MCC CoLA Spear. STS-B",
  ": Analysis of escalating the global batch size": "lized in Llama 2, which effectively deconstructs un-seen languages into their constituent raw bytesacapability not afforded by BERT. Additionally, theenrichment of our text pre-training corpus fromdiverse sources contributes to this. For a detailedbreakdown of the text pre-training data, we referreaders to Appendix C.2. A Large Batch Size Improves Stable Train-ingWe observe a distinct preference for largerbatch sizes when fine-tuning pixel-based modal-ities across certain datasets. As in , weevaluate how different batch sizes64, 128, 256,and 512affect model performance on selectedGLUE benchmark tasks, namely QQP, CoLA, andSTS-B. A clear trend emerges from the data: in-creasing the batch size correlates with improvedmodel performance. Our analysis suggests thatpixel modality fine-tuning exhibits greater variancethan text modality and benefits from the use oflarger batch sizes. This appears to mitigate the vari-ability inherent in different training batches, thusenhancing training stability. It prevents prematureconvergence to suboptimal local minima and fos-ters higher model accuracy. Font Transfer AnalysisWe extend to ex-amining the adaptability of PixelGPT to di-verse font styles during fine-tuning.We em-ployed three distinct fonts for rendering the data:GoNotoCurrent, which was utilized during pre-training; NotoSerif-Regular, a font stylisticallyakin to GoNotoCurrent; and JournalDingbats1,a font that renders text as distinct image-basedsymbols, markedly divergent from the others. Theadaptability was tested across five datasets from theGLUE benchmarkCoLA, STS-B, MRPC, RTE,and WNLI. As depicted in , the perfor-mance of PixelGPT remained stable across differ-ent fonts for all selected datasets barring CoLA.Notably, even when fine-tuned with data renderedin JournalDingbats1, which bears little resem-blance to the pre-training font, the results demon-strated a commendable degree of resilience, indicat- CoLASTS-BMRPCRTEWNLI",
  "ing that the pixel pre-training is robust to generalizeacross significantly varied visual representations": "Impact Analysis of Color RetentionUnlike pre-vious that renders text as grayscale or binary im-ages, PixelGPT employs RGB-rendered data, re-taining richer informational content. We evaluatedthe performance of these rendering approaches onHatemojiBuild dataset (Kirk et al., 2022), designedfor detecting online hate speech conveyed throughemojis. presents our findings, where theRGB-rendered data fine-tuning significantly outper-forms its grayscale counterpart. This performanceenhancement can be attributed to the models ca-pacity to utilize color cues within emojis, whichare critical for inferring the emotional context ofsentences. For a more detailed illustration, provides specific examples where color retentionhas improved model interpretability.",
  "Conclusion and Future Work": "In this paper, we have investigated the potentialof pixel-based autoregressive pre-training usingvisual text images. Our results demonstrate thatincorporating visual orthographic features signifi-cantly enhances language understanding and mul- tilingual capabilities. Additionally, our empiricalfindings suggest that using pixel-text paired dataeffectively reduces modality competition duringtraining, thereby improving model performance.Looking forward, scaling this approach to largermodel sizes holds considerable promise for advanc-ing the field of multimodal language processing.",
  "Limitations": "Model ScaleThe current implementation of ourmodel utilizes 24 layers of transformer decoders,which has been effective for the scope of our ex-perimental framework. However, the explorationof scaling our model to much larger configurations,such as 7B, 13B, 70B, or over 100B parameters,remains untested. Expanding the language modelscapacity could significantly improve its ability ofscaling, potentially enhancing both performanceand generalizability. Training ComputeOur training was restrictedby computational resources, limiting us to pre-training on only 100 to 200 billion tokens orpatches. This constraint curtails our capacity toexploit the full benefits of extensive data scale train-ing. Future work can extend the pre-training tomore than 1,000 billion tokens or patches couldyield promising insights into the scalability. Extended Evaluation on Text GenerationOnelimitation of our approach is related to generationtasks. Since the models input and output are imagepatches, directly obtaining text outputs requires anadditional OCR postprocessing step. This intro-duces an additional layer of complexity and poten-tial error. We plan to address this in future work,exploring more integrated solutions for text genera-tion tasks. Preliminary Nature of StudyIt is crucial to ac-knowledge that this research constitutes a prelim-inary foray into the realm of pixel-based autore-gressive models for multilingual and multimodallanguage processing. As such, while the results areencouraging, they should be viewed as exploratory.We invite further research to build upon our ini-tial findings, addressing these limitations and fur-ther testing the robustness and applicability of themodel in a wider array of settings.",
  "This research into pixel-based autoregressive pre-training for visual text images raises several ethicalconsiderations that warrant careful attention:": "Data Privacy and SecurityThe utilization ofvisual text images, especially from diverse sourcessuch as multilingual datasets, necessitates stringentadherence to data privacy and security guidelines.It is vital to ensure that all data used for trainingand testing respects the privacy rights of individualsand complies with applicable legal frameworks. Bias and FairnessMachine learning models, par-ticularly those involved in language processing, aresusceptible to biases that may be present in thetraining data. It is imperative to conduct thoroughbias audits and fairness assessments to identify andmitigate any discriminatory patterns in model pre-dictions, ensuring that the technology is equitableacross different languages and cultural contexts. Misuse PotentialWhile our study focuses on thepositive applications of enhancing multilingual ca-pabilities and understanding, there is a potentialfor misuse in various contexts. We advocate for re-sponsible use guidelines and transparency in modeldeployment to prevent malicious applications ofthe technology. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, YuryZemlyanskiy, Federico Lebrn, and Sumit Sanghai.2023. Gqa: Training generalized multi-query trans-former models from multi-head checkpoints. arXivpreprint arXiv:2305.13245. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Gemini: A fam-ily of highly capable multimodal models. CoRR,abs/2312.11805. Yekun Chai, Shuo Jin, and Xinwen Hou. 2020. High-way transformer: Self-gating enhanced self-attentivenetworks. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 68876900, Online. Association for Computa-tional Linguistics. Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, HaoTian, and Hua Wu. 2023. ERNIE-code: BeyondEnglish-centric cross-lingual pretraining for program-ming languages. In Findings of the Association forComputational Linguistics: ACL 2023, pages 1062810650, Toronto, Canada. Association for Computa-tional Linguistics. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu,Heewoo Jun, David Luan, and Ilya Sutskever. 2020.Generative pretraining from pixels. In Proceedings ofthe 37th International Conference on Machine Learn-ing, ICML 2020, 13-18 July 2020, Virtual Event,volume 119 of Proceedings of Machine LearningResearch, pages 16911703. PMLR. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Associa-tion for Computational Linguistics, ACL 2020, On-line, July 5-10, 2020, pages 84408451. Associationfor Computational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-ina Williams, Samuel R. Bowman, Holger Schwenk,and Veselin Stoyanov. 2018. XNLI: evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Natu-ral Language Processing, Brussels, Belgium, Octo-ber 31 - November 4, 2018, pages 24752485. Asso-ciation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.arXiv preprintarXiv:2010.11929. Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai,Miguel Angel Bautista, Alexander Toshev, VaishaalShankar, Joshua M Susskind, and Armand Joulin.2024. Scalable pre-training of large autoregressiveimage models. arXiv preprint arXiv:2401.08541. Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021.Taming transformers for high-resolution image syn-thesis. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages1287312883. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,Piotr Dollr, and Ross B. Girshick. 2022. Masked au-toencoders are scalable vision learners. In IEEE/CVFConference on Computer Vision and Pattern Recog-nition, CVPR 2022, New Orleans, LA, USA, June18-24, 2022, pages 1597915988. IEEE. Geewook Kim,Teakgyu Hong,Moonbin Yim,JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Won-seok Hwang, Sangdoo Yun, Dongyoon Han, andSeunghyun Park. 2022. Ocr-free document under-standing transformer. In European Conference onComputer Vision, pages 498517. Springer. Hannah Kirk, Bertie Vidgen, Paul Rottger, TristanThrush, and Scott Hale. 2022. Hatemoji: A test suiteand adversarially-generated dataset for benchmark-ing and detecting emoji-based hate. In Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 13521368,Seattle, United States. Association for ComputationalLinguistics. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,Chenghao Mou, Carlos Muoz Ferrandis, Yacine Jer-nite, Margaret Mitchell, Sean Hughes, Thomas Wolf,et al. 2022. The stack: 3 tb of permissively licensedsource code. arXiv preprint arXiv:2211.15533. Jonas F. Lotz, Elizabeth Salesky, Phillip Rust, andDesmond Elliott. 2023. Text rendering strategiesfor pixel language models. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2023, Singapore, De-cember 6-10, 2023, pages 1015510172. Associationfor Computational Linguistics. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-erico Cassano, Joel Lamy-Poirier, Nouamane Tazi,Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,Tianyang Liu, Max Tian, Denis Kocetkov, ArthurZucker, Younes Belkada, Zijian Wang, Qian Liu,Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry YueZhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade,Wenhao Yu, Lucas Krau, Naman Jain, Yixuan Su,Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai,Niklas Muennighoff, Xiangru Tang, MuhtashamOblokulov, Christopher Akiki, Marc Marone, Cheng-hao Mou, Mayank Mishra, Alex Gu, Binyuan Hui,Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Pa-try, Canwen Xu, Julian J. McAuley, Han Hu, TorstenScholak, Sbastien Paquet, Jennifer Robinson, Car-olyn Jane Anderson, Nicolas Chapados, and et al.2024. Starcoder 2 and the stack v2: The next genera-tion. CoRR, abs/2402.19173.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, ScottGray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. 2021. Zero-shot text-to-image gener-ation. In International conference on machine learn-ing, pages 88218831. Pmlr. Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliz-abeth Salesky, Miryam de Lhoneux, and DesmondElliott. 2023. Language modelling with pixels. InThe Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. OpenReview.net. Elizabeth Salesky, Neha Verma, Philipp Koehn, andMatt Post. 2023. Multilingual pixel representationsfor translation and effective cross-lingual transfer. InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 1384513861. Association for Computational Linguistics.",
  "Noam Shazeer. 2020. Glu variants improve transformer.arXiv preprint arXiv:2002.05202": "Luca Soldaini, Rodney Kinney, Akshita Bhagia, DustinSchwenk, David Atkinson, Russell Authur, Ben Bo-gin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar,Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,Jacob Morrison, Niklas Muennighoff, AakankshaNaik, Crystal Nam, Matthew E. Peters, AbhilashaRavichander, Kyle Richardson, Zejiang Shen, EmmaStrubell, Nishant Subramani, Oyvind Tafjord, PeteWalsh, Luke Zettlemoyer, Noah A. Smith, HannanehHajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,and Kyle Lo. 2024. Dolma: An Open Corpus ofThree Trillion Tokens for Language Model Pretrain-ing Research. arXiv preprint.",
  "Yintao Tai, Xiyang Liao, Alessandro Suglia, and An-tonio Vergari. 2024.Pixar: Auto-regressive lan-guage modeling in pixel space.arXiv preprintarXiv:2401.03321": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023a. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Michael Tschannen, Basil Mustafa, and Neil Houlsby.2023. CLIPPO: image-and-language understandingfrom pixels only. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, CVPR 2023,Vancouver, BC, Canada, June 17-24, 2023, pages1100611017. IEEE.",
  "Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neuraldiscrete representation learning. Advances in neuralinformation processing systems, 30": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2018.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In Proceed-ings of the Workshop: Analyzing and InterpretingNeural Networks for NLP, BlackboxNLP@EMNLP2018, Brussels, Belgium, November 1, 2018, pages353355. Association for Computational Linguistics.",
  "AText Renderer Details": "The renderer transposes one or more segments oftext onto a virgin RGB canvas structured into 1024distinct patches, each delineated into a 16x16 pixelmatrix. This configuration is shown in .A visual syntax is adopted to distinguish textboundaries: a solitary black patch of 16x16 pixelsoperates as both a delimiter and an indicator of thesequences conclusion (End of Sequence, EOS).Subsequent white patches post-EOS are deemedpaddingthey remain inert in the attention mech-anism, thus excluding them from the computationof attention scores.For the rendition of text documents, the renderertackles content on a line-by-line basis. It incor-porates a binary search algorithm to intelligentlygauge the maximum quota of words renderable ina single pass, ensuring the texts width remainswithin the permissible pixel threshold. This dy-namic segmentation capability circumvents poten-tial truncation issues inherent in rendering exten-sive lines of text, allowing for a seamless integra-tion of longer passages without compromise to vi-sual fidelity or contextual integrity.",
  "BModel Architecture": "specifies the comprehensive configurationof our models architecture, based on similar trans-former decoder architecture to Llama 2 (Touvronet al., 2023b) with specific adaptations. We employSwiGLU as the hidden activation function (Shazeer,2020; Chai et al., 2020), noted for its effective non-linear processing capabilities. The initializer rangeis set to 0.02 to promote optimal weight initial-ization. An intermediate size of 2816 is specified,offering a balance between the models representa-tional capacity and computational demands. Thehidden size and the maximum number of position embeddings are both set at 1024, facilitating de-tailed representation of inputs and accommodatingsequences up to 1024 tokens.The models attention architecture utilizesgrouped query attention (Ainslie et al., 2023) with16 attention heads and 8 key-value heads. We use astack of 24 transformer layers, endowing the modelwith substantial depth for complex pattern recog-nition. Also, we use RMSNorm (Zhang and Sen-nrich, 2019) with epsilon of 1e-05 and rotary em-beddings (Su et al., 2024).",
  "CPre-training Data": "For the text-based pre-training, we utilized theexpansive Dolma dataset (Soldaini et al., 2024),which comprises an extensive collection of 3 tril-lion tokens. This dataset is sourced from a het-erogenous compilation of materials, including anarray of web-based content, scholarly articles, pro-gramming code, literary works, and comprehen-sive encyclopedic entries. For the image-basedpre-training, we transformed the textual contentfrom the peS2o corpus, English Wikipedia, and theC4 dataset into visual representations, amountingto a total of over 400 million document images.",
  "C.1Pre-training Data for Visual Images": "We pretrained on a rendered version of the peS2o,English Wikipedia and C4.The peS2o dataset, acurated collection of approximately 40 million cre-ative open-access academic papers, has been metic-ulously cleaned, filtered, and formatted to facilitatethe pretraining of language models. Meanwhile,The C4 dataset represents a substantial refinementof the Common Crawl corpus. This dataset, derivedfrom the extensive Common Crawl web scrape,undergoes rigorous cleaning and preprocessing toensure the quality and relevance of the text data.The C4 dataset is exclusively composed of Englishlanguage texts, with a stringent criterion that eachpage must have at least a 99% probability of beingin English, as determined by the langdetect tool,to be included. This selection process ensures thatthe dataset primarily contains natural language text,free from boilerplate or nonsensical content, and isextensively deduplicated to avoid redundancy.",
  ": Model configuration parameters": "of each web page as its identifier, facilitating theexploration of relationships between different doc-uments. Covering data from May 2020 to June2023 across 24 shards, Common Crawl includesabout 4,600 million documents and 2,415 billiontokens. It is hosted on Amazon S3 as part of theAmazon Web Services Open Data Sponsorshipprogram and can be accessed freely, adhering tothe Common Crawl terms of use.",
  "C4 (Raffel et al., 2020)The C4 dataset is acleaned and annotated subset of Common Crawl,": "specifically extracted from a shard dated April2019. It includes URLs as metadata, which canbe used to restore the original HTML files and un-derstand document linkages. The dataset contains364 million documents, totaling 175 billion tokens,and is available on the HuggingFace Hub under theODC-By 1.0 license, allowing for broad academicand research usage. peS2o (Soldaini and Lo, 2023)Derived from theSemantic Scholar Open Research Corpus (S2ORC),peS2o uses the Semantic Scholar Corpus ID tolink documents to their corresponding manuscripts,enabling the recovery of original PDFs throughassociated metadata. The dataset encompasses 38.8million documents and 57 billion tokens, and isaccessible through the Semantic Scholar PublicAPI under the ODC-By 1.0 license. The Stack (Kocetkov et al., 2022)This datasetcomprises a variety of computer code sourced fromdifferent GitHub repositories, with metadata thatincludes filenames and repository names to facil-itate the retrieval of original content. The Stackcontains 236 million documents and 430 billiontokens and is hosted on the HuggingFace Hub. Itfeatures code released under various permissive li-censes, supporting diverse software developmentand research projects. Project GutenbergProject Gutenberg offers acollection of public domain books in the U.S., witheach document beginning with the books title toease identification. This dataset provides accessto about 52,000 documents and 4.8 billion tokens,and is freely available at gutenberg.org withoutany copyright restrictions, making it a valuableresource for literary and historical research. Wikipedia and WikibooksThese datasets con-sist of encyclopedic content from Wikipedia andeducational materials from Wikibooks, featuringmetadata that includes URLs from which content isextracted. This allows users to reconstruct the struc-ture and connections between documents. Together,they contain 6.1 million documents and 3.6 billiontokens. The data is freely available via Wikimediadata dumps and is released under the CC BY-SA4.0 license, promoting widespread educational andinformational use.",
  "DPre-training Details": "We list the pre-training hyperparameters in Ta-ble 10. Pre-training was executed across a suiteof 32 NVIDIA A100 GPUs. For TextGPT andPixelGPT, we adopted a global batch size of 4million tokens or patches, respectively. In the caseof MonoGPT, the global batch size was set at 8 mil-lion, maintaining an equal distribution between textand image data. For DualGPT, the global batchsize was increased to 10 million, with a ratio oftext/image/pair data with 4:4:2.",
  ": Hyperparameters of pre-training settings": "For clarification, we summarize the trainingtasks in for various training configura-tions. TextGPT was trained exclusively on textdata. In contrast, PixelGPT was pre-trained solelywith image data. MonoGPT represents a hybrid ap-proach, utilizing both text and image data indepen-dently but not in paired form. DualGPT stands asthe most integrative model, incorporating text data,",
  "E.1Fine-tuning Dataset": "The main experiments of our fine-tuning phasewere conducted on GLUE and XNLI to evaluatethe models language and multilingual understand-ing ability, respectively. HatemojiBuild was usedto analyze the effect of color retention. The detailsof the dataset are described below: GLUE (Wang et al., 2018)A benchmark of ninesentence- or sentence-pair language understand-ing tasks, including MNLI(392k), QQP(363k),QNLI(108k), SST-2(67k), CoLA(8.5k), STS-B(5.7k), MRPC(3.5k), RTE(2.5k), WNLI(635),built on established existing datasets and selected tocover a set of three tasks. In this paper, for MNLI,QNLI, SST-2, RTE, and WNLI tasks, we report theAccuracy (Acc); for QQP and MRPC, we reportthe F1 score; for CoLA, we report the Matthewscorrelation coefficient (MCC); for STS-B we reportSpearman correlation (Spear.). The MNLI datasethas matched development/test sets with the samesources as those in the training set, and unmatchedsets that do not closely resemble any of the sets wesaw during training are denoted as MNLI-m/mm.We conduct experiments on both settings. In addi-tion, some previous works ignored WNLI becauseof its different training and validation/testing setdistribution. We still performed on it and foundthat Pixel pre-training leads to a boost at WNLI. XNLI (Conneau et al., 2018)The Cross-lingual Natural Language Inference (XNLI) cor-pus is an extension of the Multi-Genre NLI(MultiNLI) (Williams et al., 2018) corpus, designedfor cross-lingual natural language inference, con-taining data in 15 languages. The dataset was cre-ated by manually translating the validation andtest sets of MultiNLI into each of these 15 lan-guages. For all languages, the English trainingset was machine-translated. The task is to predicttextual entailment, a classification task determin-ing whether sentence A implies, contradicts, or isneutral to sentence B, given two sentences. HatemojiBuild (Kirk et al., 2022)Hatemo-jiBuild is a benchmark for online hate detectioninvolving emojis. The dataset includes 5,912 chal-lenging examples of adversarial perturbations gen-erated through a human-and-model-in-the-loop ap-proach on Dynabench. This allows us to predicthateful emotions expressed with emojis.",
  "E.2Fine-tuning Setting": "We fine-tune PixelGPT, MonoGPT, DualGPT andTextGPT on downstream tasks. we use NVIDIATesla V100 GPUs to fine-tune TextGPT and theNVIDIA A100 GPUs to fine-tune pixel-based pre-training models. The same rendering settings asin pre-training are used to render pixel data forfine-tuning PixelGPT, MonoGPT, and DualGPT, un-less specified. We use the last patch to predict thelabel when fine-tuning the generative pixel-basedpre-training models. In our analysis experiments,MonoGPT and DualGPT are also fine-tuned on dual-modality data obtained by concatenating renderedimages with the original text. Specifically, weright-fill the image with white padding blocks foralignment. To avoid the impact of padding patchesbetween the image and the text, we then set theattention mask to mask the padding blocks duringfine-tuning.We searched fine-tuning hyperparameters foreach dataset in GLUE and two XNLI settingsfor PixelGPT, MonoGPT, DualGPT and TextGPT, re-spectively. shows the searched hyperpa-rameters and values. We present the best searchedresults for GLUE in and and fortranslate-train-all and cross-lingual transfer settingson XNLI in . During the hyperparametersearching, we found that using a larger batch sizeto fine-tune the generative pixel-based pre-trainingmodel improves training stability and achieves bet-",
  "E.3Implementation for Different RenderModes": "We use RGB render mode for fine-tuning data ren-dering by default, as described in Appendix A. Toobtain and adapt to grayscale and binary rendereddata, we modify (1) the data preprocessing pro-cess and (2) the models linear projection in thepatch embedding layer. Specifically, we first ren-der the data uniformly using RGB mode and getthree-channel RGB images. After that, in the pre-processing stage, to get the grayscale version ofthe rendered image, we converted the RGB im-age to grayscale (with pixel values ranging from0 to 255) using the convert function of the Imageclass in the PIL library and setting the functionparameter model to L to get the rendered binaryimage, we set the pixel threshold (set to 128 inour experiments) based on the converted grayscaleimage and set the pixels below the threshold inthe grayscale image to 0 and the pixels above thethreshold to 255. This way, we transformed thethree-channel RGB-rendered image into a single-channel grayscale and binary image. Next, sincethe patch embeeding layer of the pre-trained modeltakes the three-channel image as input by default,we need to modify the linear projection layer in itto adapt to the single-channel image. Therefore,we average the linear layer weights by channel anduse them as initial weights before fine-tuning sothat the model supports the processing of single-channel images.",
  ": Fine-tuning settings for XNLI. We report the best hyperparameters for all models on Translate-Train-Alland Cross-lingual Transfer, respectively": "increases the parameter count to 1.5 billion, whichenhances its ability to generate more coherent andcontextually relevant text across a wide array ofdomains without task-specific training. With atransformer-based architecture, GPT-2 operates onunsupervised learning, using only a large corpusof text data scraped from the internet (WebText)to learn various language patterns and tasks. Thismodel exemplifies a significant shift towards morerobust and generalized language models, therebysupporting the development of AI systems capableof understanding and generating human-like textwith minimal task-specific data. BERTBERT (Bidirectional Encoder Represen-tations from Transformers) is a groundbreakingmodel in natural language processing introducedby Devlin et al. (2019) at Google AI Language.It utilizes the bidirectional Transformer, an atten-tion mechanism that learns contextual relations be-tween words in a text. Unlike previous models thatonly consider text in a single direction (left-to-right or right-to-left), BERT processes words simulta-neously in both directions. This bi-directionalityallows the model to capture a richer understand-ing of context. Pre-trained on a large corpus ofunlabeled text, BERT is fine-tuned with additionaloutput layers to perform a wide array of languageprocessing tasks.",
  "F.2Image-based Baselines": "DONUTThis OCR-free visual document under-standing model (Kim et al., 2022) is fundamentallydesigned to interpret and extract structured infor-mation directly from document images, bypass-ing traditional optical character recognition (OCR)techniques. DONUT leverages a transformer ar-chitecture to encode document images into embed-dings and decode these embeddings into structuredoutputs like JSON formats without preliminary textdetection and recognition stages. Pre-trained us-ing a combination of real and synthetically gener-ated document images, DONUT achieves impres- sive benchmarks on several visual document under-standing tasks, outperforming state-of-the-art OCR-dependent models in terms of both accuracy andprocessing speed. A synthetic data generator fur-ther enhances The models pre-training, enablingit to readily adapt to different languages and doc-ument formats, thereby extending its applicabilityto global and diverse application scenarios. CLIPPOCLIPPO (Tschannen et al., 2023) inte-grates a single vision transformer that processes allinput typesimages and textequally, using thesame model parameters. By adopting a contrastivelearning framework, this unified model learns toalign the representations of text and images intoa cohesive latent space. This approach simplifiesthe architecture by removing the necessity for sepa-rate text and image towers and enhances efficiencyby halving the parameter count compared to dual-tower systems. The key innovation of CLIPPOlies in its ability to perform complex multimodaltasks, including zero-shot classification and naturallanguage understanding, with competitive perfor-mance while relying solely on pixel data. PIXELThe PIXEL (Rust et al., 2023) (Pixel-based Encoder of Language) model reimagineslanguage modeling by rendering text as images,effectively bypassing the vocabulary bottleneck oflanguage models. This pre-trained model convertstext into fixed-sized image patches, which are thenprocessed by a Vision Transformer (ViT) encoder.Unlike conventional models that predict a distribu-tion over a vocabulary of tokens, PIXEL focuses onreconstructing the pixels of masked image patches.This approach allows PIXEL to support many lan-guages and scripts, leveraging orthographic similar-ities. The model performs better in handling scriptsnot present in its training data and is robust againstorthographic attacks and linguistic code-switching.",
  "G.1Performance on Cross-lingual Transfer": "In this section, We analyze the cross-lingual trans-fer ability of pixel-based autoregressive models onXNLI under the Cross-lingual Transfer setting. Asshown in , we compared three differentmodels: PixelGPT, MonoGPT, and DualGPT. Ourfindings indicate that incorporating additional textmodality data in the pre-training phase enhancesthe cross-lingual transfer capabilities of these mod-els. Nevertheless, a notable performance disparity",
  "G.2Probing Dual-Modality Fine-Tuning": "We delved into the synergistic potential betweentext and pixel modalities during the fine-tuningphase. A comparative experimental design was im-plemented to fine-tune pixel pre-trained models intwo distinct manners: (1) exclusively on text data,and (2) on an amalgamation of rendered image dataand original text. We assessed the performance im-pact of these fine-tuning approaches with MonoGPTand DualGPT models on XNLI. As delineated in, the models fine-tuned with dual-modalitydata consistently outperformed those fine-tuned ontext data alone, with clear gains in multilingual un-derstanding tasks. This evidence suggests that theinherent strengths of pixel-based representationsin capturing multilingual nuances are amplifiedwhen combined with textual information duringfine-tuning.",
  "G.3RGB vs. Grayscale vs. Binary Rendering": "Rendering modes offer trade-offs between the rich-ness of information and processing efficiency, withRGB providing a three-channel image dense withinformation, whereas grayscale and binary modesare optimized for speed. To assess the impact ofthese rendering choices, we explored the robustnessof our model, pre-trained using RGB visual text,across different rendering modes within the down-stream context of the XNLI task. As shown in Fig-ure 9, our experiments reveal that the performancewhen fine-tuning in grayscale and binary modesclosely parallels that of RGB. This equivalenceunderscores the robustness of the pixel-based pre-training, indicating that its cross-linguistic transfercapability transcends the specific rendering modeemployed in downstream tasks. Detailed experi-mental results are in the .",
  ": Performance of using three render modes tofine-tune PixelGPT on XNLI. PixelGPT shows strongrobustness to fine-tuning render mode": "English and Arabic. Particularly, PixelGPT reg-isters marked improvements over BERT in Thaiand Chinese languages. These results suggest thatthe tokenizer-independent, pixel-based autoregres-sive design of PixelGPT offers a potent solutionto the vocabulary bottleneck issue commonly en-countered in language models, thus enhancing itsapplicability to multilingual tasks. XNLI(avg) ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO XNLI(avg) BERTPIXELPixelGPT (pixel only)"
}