{
  "Abstract": "Political discourse on social media often con-tains similar language with opposing intendedmeanings. For example, the phrase thoughtsand prayers, is used to express sympathy formass shooting victims, as well as satiricallycriticize the lack of legislative action on guncontrol. Understanding such discourse fullyby reading only the text is difficult. However,knowledge of the social context informationmakes it easier. We characterize the social context required tofully understand such ambiguous discourse, bygrounding the text in real-world entities, ac-tions, and attitudes. We propose two datasetsthat require an understanding of social contextand benchmark them using large pre-trainedlanguage models and several novel structuredmodels. We show that structured models, ex-plicitly modeling social context, outperformlarger models on both tasks, but still lag sig-nificantly behind human performance. Finally,we perform an extensive analysis, to obtain fur-ther insights into the language understandingchallenges posed by our social grounding tasks.",
  "Introduction": "Over the past decade, micro-blogging websiteshave become the primary medium for US politi-cians to interact with general citizens and influ-ence their stances for gaining support. As a result,politicians from the same party often coordinatethe phrasing of their social messaging, to amplifytheir impact (Vaes et al., 2011; Weber and Neu-mann, 2021). Hence, repetitive, succinct phrases,such as Thoughts and Prayers, are extensivelyused, although they signal more nuanced stances.Moreover, the interaction among politicians fromopposing parties often leads to messaging phrasedsimilarly, but signaling opposing real-world actions.For example, Thoughts and Prayers, when usedby Republicans, expresses condolences in mass",
  ": An example of varied intended meaningsbehind the same political message depending on theAuthor and Event in context": "shooting events, but when used by Democrats con-veys an angry or sarcastic tone as a call for actiondemanding tighter gun control measures. Simi-larly, fig. 1 shows contrasting interpretations of thephrase We need to keep our teachers safe! de-pending on different speakers and in the context ofdifferent events.Humans familiar with the stances of a politicianand, possessing knowledge about the event fromthe news, can easily understand the intended mean-ing of political phrases. However, automaticallyunderstanding such language is challenging. Ourmain question in this paper is - Can an NLP modelfind the right meaning? From a linguistic per-spective, we follow the distinction (Bach, 2008)between semantic interpretation (i.e., meaning en-coded directly in the utterance and does not changebased on its external context), and pragmatic in-terpretation (that depends on extra-linguistic infor-mation). The latter has gathered significant inter-est in the NLP community recently (Bender andKoller, 2020; Bisk et al., 2020), focusing on lan-guage understanding, when grounded in an exter-nal context (Fried et al., 2023). To a large extent,",
  "Targets: Brett Kavanaugh (negative), Julie Swetnick (positive)Christine Ford (positive), Deborah Ramirez (positive)": "Disambiguation: The withdrawal from the Paris climate agreementis the first step of many to come for the Trump administration. It willnot be the last, as more positive changes are sure to follow.Incorrect Disambiguations:1) Joe Bidens inauguration marks the first day of a new era of progressand prosperity, lasting positive changes are coming. (Incorrect Event)2) The Paris Climate Agreement withdrawal is the first of manybackward steps this Trump administration is sure to take in destroyingour environment. (Incorrect Stance)3) This is the time for America to move forward and make progresswithout being held back by a global agreement that doesnt serveour interests. (Doesnt match the vague text)",
  ": Examples of Annotated Datasets and their statistics": "the focus of such studies has been on groundinglanguage in a perceptual environment (e.g., imagecaptioning (Andreas and Klein, 2016; Sharma et al.,2018; Alikhani et al., 2020), instruction following(Wang et al., 2016; Suhr et al., 2019; Lachmy et al.,2022), and game playing (Potts, 2012; Udagawaand Aizawa, 2019) tasks). Unlike these works,in this paper, we focus on grounding languagein a social context, i.e., modeling the commonground (Clark and Brennan, 1991; Traum, 1994;Stalnaker, 2002) between the author and their so-cial media followers, that enables understandingan otherwise highly ambiguous utterances. TheSocial Context Understanding, needed for buildingsuccessful models for such tasks, can come froma wide variety of sources. The politicians affilia-tion and historical stances on the issue provide cancapture crucial social context. Social relationships,knowledge about the involved entities, and relatedprior and upcoming events form important part ofthe puzzle as well. In fig. 1 event #1, combin-ing the event information (school shooting) withthe speakers gun control stances, would facilitateunderstanding the intended meaning of the text. The main motivation of this paper work isto operationalize the Social Context Ground-ing problem as a pragmatic understanding task.From a practical perspective, this would enablethe creation of better NLP-CSS models that canprocess social media text in settings that requirecontextualized understanding. We suggest severaldatasets, designed to evaluate this ability in com-putational models. These task capture the intendedmeaning at different level of granularity. At themost basic level, providing the social context canhelp identify the entities targeted, and the sentimenttowards them. In fig. 1, the social context event#1, Harrisand the text we need to keep our teacherssafe negative attitude towards guns. A morenuanced account of meaning, which we formulateas a separate task, captures the specific means inwhich the negative attitude is expressed (the Inter-pretation in fig. 1). We additionally present twodatasets corresponding to these tasks, namely, Tar-get Entity and Sentiment Detection and VagueText Disambiguation. In the first, the goal is topredict: 1) whether a given entity is the intendedtarget of a politicians tweet and 2) the sentimenttowards the intended targets. We explicitly focuson tweets that do not always mention the targetsin their text to incentivize modeling the pragmaticcommunicative intent of the text. In the secondtask, given an ambiguous political message suchas We demand justice and its social context (as-sociated event, & the authors party affiliation), thetask is to identify a plausible unambiguous expla-nation of the message. Note that the ground truthfor all these tasks is based on human pragmaticinterpretation, i.e., guns is a negative target ofwe need to keep our teachers safe, despite notbeing mentioned in the text, since it was perceivedin this way by a team of human annotators readingthe tweet and knowing social context. We showexamples of each task in table 1. We describe thedatasets in detail in section 3. We evaluate the performance of various models,as a way to test the need for social context and com-pare different approaches for modeling it. Theseinclude pre-trained LM-based classifiers, and LLMin-context learning (Brown et al., 2020a; Blacket al., 2022), which use a textual representation ofthe social context. We also adopt an existing graph-based discourse contextualization framework (Pu-jari and Goldwasser, 2021; Feng et al., 2022), to explicitly model the social context needed to solvethe proposed tasks. Our results demonstrate thatthe discourse contextualization models outperformother models on both tasks. We present an erroranalysis to gain further insights. We describe themodels in section 4 and the results in section 5.We also present a qualitative visualizationof a political event, Brett Kavanaugh SupremeCourt Nomination (section 6.4), from target entity-sentiment perspective. It showcases a unique sum-mary of the event discourse. We perform humanevaluation on our Vague Text Disambiguationdataset, and observe that humans find this taskmuch easier than the evaluated models. We alsopresent observations of human vs. LLM errors indisambiguation. In summary, our contributions are:",
  "Related Work": "Pragmatic Language Grounding gained signifi-cant focus recently (Bender and Koller, 2020; Bisket al., 2020) following the rise of Pretrained Lan-guage Models (Devlin et al., 2019; Liu et al., 2019;Brown et al., 2020a) as unified NLP models. Mostgrounding tasks address multi-modal or physicalenvironment descriptions (Barnard et al., 2003; Vo-gel and Jurafsky, 2010; Chen and Mooney, 2011;Tellex et al., 2011; Mitchell et al., 2012; Andersonet al., 2018). We refer the reader to (Fried et al.,2023) for a thorough overview. In contrast, wefocus on grounding language in a social context.Social Context Modeling Hovy and Yang (2021)show that modeling social context is necessary forhuman-level NLU. As political messages are of-ten targeted at the voter base aware of the politicalcontext (Weber and Neumann, 2021; Vaes et al.,2011), they are vague by design. Several previousworks model social context for entity linking (Yanget al., 2016), social media connections relationshipfor fake news detection (Baly et al., 2018; Mehta",
  "Ourdataandcodeisat": "et al., 2022) and, political bias detection (Li andGoldwasser, 2019; Baly et al., 2020). These worksmodel partial aspects of social context, relevant totheir tasks. Two recent frameworks aim to capturesocial context holistically (Pujari and Goldwasser,2021; Feng et al., 2022). Evaluation tasks presentedin both works show interesting social context un-derstanding but are not fully representative of thechallenges of Social Context Grounding. Zhan et al.(2023) propose a dataset for dialogue understand-ing addressing general social commonsense.Related Semantic and Pragmatic tasks closestto our Target Entity Sentiment Identification taskis Stance Detection in social media (Mohammadet al., 2016; AlDayel and Magdy, 2020). To clarifyour contribution, Mohammad et al. (2016), a pop-ular SemEval task, looks at sentiment towards 5targets, while our data has 362 unique targets. All-away and McKeown (2020) and Zhang et al. (2022)also propose stance datasets on tweets. But, theyfocus mainly on semantic understanding of textthat allows them to predict agreement or disagree-ment with well-defined statements. Our Vague TextDisambiguation task is related to recent works thatstudy implicit inferences (Hoyle et al., 2023), andpragmatic understanding (Hu et al., 2023). How-ever, our tasks evaluate pragmatic understandingusing an explicit context, absent in those tasks.",
  "Social Context Grounding Tasks": "We design and collect two datasets for Social Con-text Grounding evaluation, and define three prag-matic interpretation tasks. In the Tweet Target En-tity and Sentiment dataset, we collect annotationsof opinionated tweets from known politicians fortheir intended targets and sentiments towards them.We focus on three political events for this task.The dataset and its collection are described belowin section 3.1. In the Vague Text DisambiguationTask, we collect plausible explanations of vaguetexts, given the social context, consisting of authoraffiliation and specific event. We focus on eight po-litical events. This dataset is detailed in section 3.2.Examples and data statistics are shown in table 1.",
  "Tweet Target Entity and Sentiment Task": "In this task, given a tweet T, its context, and anentity E, the objective is to predict whether or not Eis a target of T and the sentiment towards E. Politi-cal discourse often contains opinionated discourseabout world events and social issues. We collect tweets that dont directly mention the target entities.Thus, connecting the text with the event details andthe authors general perspectives is necessary tosolve this task effectively. We pick the focal enti-ties for the given event and let human annotatorsexpand on that initial set, based on their interpre-tation of the contextualized text. A target entityis conceptualized as an entity present in the fullintended interpretation of the tweet.We focus our tweet collection on three recentdivisive events: George Floyd Protests, 2021 USCapitol Attacks, and Brett Kavanaughs SupremeCourt Nomination. We identify relevant participat-ing entities for each of the three events. Examplesof the involved entities for the event George FloydProtests were George Floyd, United States Police,Derek Chauvin, Donald Trump, Joe Biden, UnitedStates Congress, Black people, Democratic Party,Republican Party, BLM, Antifa. 3.1.1Target-Sentiment Data CollectionWe filter 3, 454 tweets for the three events usinghashtags, keyword-based querying, and the dates ofthe event-based filtering from the Congress Tweetsrepository corpus2. We collect a subset of 1, 779tweets that contain media (images/video) to in-crease the chances of the tweet text not containingthe target entity mentions. Then, we use 6 in-househuman annotators and Amazon Mechanical Turk(AMT) workers who are familiar with the eventcontext for annotation. We ask them to annotatethe targeted entities and sentiments towards the tar-gets. The authors of this paper also participatedin the annotation process. We provide them withentity options based on the event in the focus ofthe tweet. Annotators are allowed to add additionaloptions if needed. We also ask the annotators tomark non-targets for each tweet. We instruct themto keep the non-targets as relevant to the event aspossible to create harder negative examples. Eachtweet is annotated by three annotators. We filter865 unique tweets with 5, 891 annotations, withmajority agreement on each tweet. All the AMTannotations were additionally verified by in-houseannotators for correctness. AMT workers werepaid USD 1 per tweet. It took 3 minutes on av-erage for each assignment, resulting in an hourlypay of USD 20. We include screenshots of thecollection task GUIs in the appendix. We split thetrain, and test sets by events, authors, and targetsto incentivize testing the general social grounding capabilities of the models. The test set also con-sists of authors, targets, and events not seen in thetraining set. We use Capitol Riots event for thetest set of Target Entity and Sentiment Task. Wesplit the examples into 4, 370 train, 511 develop-ment, and 1, 009 test examples. We compute themean Cohens kappa score for annotations and re-port inter-annotator agreement for annotated targets(0.47) and sentiment (0.73)",
  "Vague Text Disambiguation Task": "The task of Vague Text Disambiguation is de-signed to capture pragmatic interpretation at a finer-grained level. It can be viewed as a variant of thewell known paraphrase task, adapted for the so-cial context settings. The model is evaluated on itsability to identify plausible interpretations (i.e., asentence explicitly describing the authors intent)of an ambiguous quote given the event context andauthors affiliation. E.g., protect our children frommass shootings could easily be disambiguated aseither ban guns or arm teachers when the au-thors stance on the issue of gun rights is known.Our data collection effort is designed to capturedifferent aspects of social context grounding and fa-cilitate detailed error analysis. Defined as a binaryclassification task over tuples Party, Event, Vague text, Explicit text, we create negative examplesby flipping tuple elements values of positive exam-ples. This allows us to evaluate whether modelscan capture event relevance, political stance, orconstrain the interpretation based on the vague text.For example, in the context of Event #1 in fig. 1,we can test if models simply capture the correlationbetween Democrats and negative stance towardsguns access by replacing the vague text to let yourvoice be heard, which would make the interpreta-tion in fig. 1 implausible despite being consistentwith that stance, while other consistent interpreta-tions would be plausible (e.g., go outside and jointhe march for our lives).",
  "Vague Text Data Collection": "Data collection was done in several steps. (1)VagueTexts Collection. We collected vague text can-didates from tweets by US politicians (i.e. sena-tors and representatives) between the years 2019to 2021 from Congress Tweets corpus. We identi-fied a list of 9 well-known events from that periodand identified event-related tweets using their timeframe and relevant hashtags. We used a pre-trainedBERT-based (Devlin et al., 2019) NER model to collect tweets that contain few or no entity men-tions to identify potential candidates for vague texts.We manually identified examples that could havecontrasting senses by flipping their social context.We obtain 93 vague text candidates via this process.(2) In-Context Plausible Meaning Annotation.We match the 93 ambiguous tweets with differ-ent events that fit them.We use both Demo-crat/Republican as the author party affiliation. Weobtain 600 context-tweet pairs for AMT annotation.For each tweet, we ask AMT workers to annotatethe following two aspects: 1) sentiment towardsthe three most relevant entities in the event (sanitycheck) and 2) a detailed explanation of the intendedmeaning given the event and authors party affilia-tion. We obtain 469 reasonable annotations. Afterthis step, each annotation was screened by in-houseannotators. We ask three in-house annotators tovote on the correctness, appropriateness, and plau-sibility of the annotation given the context. Thus,we create a total of 374 examples.(3)LLM-based Data Expansion. Using these ex-amples, we further generate candidates for the taskusing LLM few-shot prompting. We use the exam-ples from the previous step as in-context few-shotexamples in the prompt. We use GPT-NeoX (Blacket al., 2022) and GPT-3 (Brown et al., 2020a) forcandidate generation. Manual inspection by threein-house annotators is performed for each gener-ated answer to ensure data quality. We generate928 candidates using GPT-NeoX and GPT-3. Man-ual filtering results in 650 generations that pass thequality check. After removing redundant samples,we obtain 365 additional examples. Thus, we ob-tain a total of 739 annotations for this task. Then,for each of the 739 examples, we ask in-house an-notators to select 3 relevant negative options fromthe pool of explanations. We instruct them to pickhard examples that potentially contain overlappingentities with the gold answer. This results in 2, 956binary classification data samples. We analyze anddiscuss the results of human validation of large LMgenerations in section 6).This process allows us to create three variantsof the task: binary-classification, multiple-choiceand generation variants. We evaluate several classi-fication models on the binary classification variant(Tab.3). We evaluate LLMs on the generation vari-ant (6.2). We benchmark humans and the bestmodels on the multiple-choice variant (6.3).Similar to the previous task, we split the train,test sets by events, and vague text to test the gen- eral social understanding capabilities of the model.We reserve Donald Trumps second impeachmentverdict event for the test set. We also reserve Demo-cratic examples of 2 events and Republican exam-ples of 2 events exclusively for the test set. Wesplit the dataset into 1, 916 train, 460 development,and 580 test examples. 180 of the test examplesare from events/party contexts unseen in train data.",
  "Modeling Social Context": "The key technical question this paper puts for-ward is how to model the social context, suchthat the above tasks can be solved with high ac-curacy. We observe that humans can perform thistask well (section 6.3), and evaluate different con-text modeling approaches in terms of their abilityto replicate human judgments. These correspond toNo Context, Text-based context representation(e.g., Twitter Bio, relevant Wikipedia articles), andGraph-based context representation, simulatingthe social media information that human users areexposed to when reading the vague texts.We report the results of all our baseline experi-ments in table 2 and table 3. The first set of resultsevaluate fine-tuned pre-trained language models(PLM), namely BERT (Devlin et al., 2019) andRoBERTa (Liu et al., 2019), with three stages ofmodeling context. Firstly, we evaluate no con-textual information setting. Second, we includethe authors Twitter bios as context. Finally, weevaluate the information from the author, event,and target entity Wikipedia pages as context (mod-els denoted PLM Baselines {No, Twitter Bio,Wikipedia} Context, respectively).We evaluate GPT-33 in zero-shot and four-shotin-context learning paradigm on both tasks. Weprovide contextual information in the prompt asshort event descriptions and authors affiliation de-scriptions. Note that GPT-3 is trained on news datauntil Sep. 2021 which includes the events in ourdata (models denoted LLM Baseline).We evaluate the performance of politician em-beddings from Political Actor Representation(PAR) (Feng et al., 2022) and Discourse Contex-tualization Framework (DCF) (Pujari and Gold-wasser, 2021) models. (models denoted StaticContextutalized Embeddings). We use PARembeddings available on their GitHub repository4.For DCF model, we use released pre-trained mod-",
  "Discourse Contextualization ModelsBERT-large + DCF52.7659.3169.9476.55BERT-base + DCF52.7360.0070.0676.55": ": Results of baseline experiments on Vague TextDisambiguation dataset test split, a binary classifica-tion task. We report macro-averaged Precision, macro-averaged Recall, macro-averaged F1, and Acc. metrics els from GitHub repository5 to generate author,event, text, and target entity embeddings. We eval-uate the embeddings on both tasks. We brieflyreview these models in section 4.1 & section 4.2.Finally, we use tweets of politicians from relatedprevious events and build context graphs for eachdata example as proposed in Pujari and Goldwasser(2021). We use Wikipedia pages of authors, events,and target entities to add social context informa-tion to the graph. Then, we train the DiscourseContextualization Framework (DCF) for each taskand evaluate its performance on both tasks (modelsdenoted Discourse Contextualization Model).Further details of our baseline experiments are pre-sented in subsection section 4.3. Results of ourbaseline experiments are discussed in section 5.",
  "Discourse Contextualization Framework": "Discourse Contextualization Framework (DCF)(Pujari and Goldwasser, 2021) leverages relationsamong social context components to learn contex-tualized representations for text, politicians, events,and issues. It consists of encoder and composermodules that compute holistic representations ofthe context graph. The encoder creates an initialrepresentation of nodes. Composer propagates theinformation within the graph to update node rep-resentations. They define link prediction learningtasks over context graphs to train the model. Theyshow that their representations significantly outper-form several PLM-based baselines trained usingthe same learning tasks.",
  "Political Actor Representation": "Feng et al. (2022) propose the Political Actor Rep-resentation (PAR) framework, a graph-based ap-proach to learn more effective politician embed-dings. They propose three learning tasks, namely,1) Expert Knowledge Alignment 2) Stance Con-sistency training & 3) Echo chamber simulation,to infuse social context into the politician repre-sentations. They show that PAR representationsoutperform SOTA models on Roll Call Vote Predic-tion and Political Perspective Detection.",
  "Experimental Setup": "Target Entity Detection is binary classification withauthor, event, tweet, target-entityas input andtarget/non-target label as output. Sentiment De-tection is set up as 4-way classification. Input is thesame as the target task and output is one of: {pos-itive, neutral, negative, non-target}. Vague TextDisambiguation is a binary classification task with party-affiliation, event, vague-text, explanation-textand a match/no-match label as output.In phase 1 no-context baselines, we use the au-thor, event, tweet, and target embeddings gener-ated by PLMs. We concatenate them for input. InTwitter-bio models, we use the authors Twitter bioembeddings to represent them. Wiki context mod-els receive Wikipedia page embeddings of author,event, and target embeddings. It is interesting tonote that the Wikipedia context models get all theinformation needed to solve the tasks.. In phase2 LLM experiments, we use train samples as in-context demonstrations. We provide task and eventdescriptions in the prompt. In phase 3 PAR mod-els, we use politician embeddings released on thePAR GitHub repository to represent authors. We re-place missing authors with their wiki embeddings.For the Vague Text task, we average PAR embed-dings for all politicians of the party to obtain partyembeddings. For DCF embedding models, we gen-erate representations for all the inputs using contextgraphs. We also use authors tweets from relevantpast events. We build graphs using author, event,tweet, relevant tweets, and target entity as nodesand edges as defined in the original DCF paper. Inphase 4, we use the same setup as the DCF em-bedding model and additionally back-propagate toDCF parameters. This allows us to fine-tune theDCF context graph representation for our tasks.",
  "Results": "The results of our baseline experiments are de-scribed in Tab. 2 and 3. We evaluate our modelsusing macro-averaged precision, recall, F1, and ac-curacy metrics (due to class imbalance, we focuson macro-F1). Several patterns, consistent acrossall tasks, emerge. First, modeling social context isstill an open problem. None of our models wereable to perform close to human level. Second,adding context can help performance, comparedto the No-Context baselines, models incorporatingcontext performed better, with very few exceptions.Third, LLMs are not the panacea for social-contextpragmatic tasks. Despite having access to a textualcontext representation as part of the prompt, andhaving access to relevant event-related documentsduring their training phase, these models under-perform compared to much simpler models thatwere fine-tuned for this task. Finally, explicit con-text modeling using the DCF model consistentlyleads to the best performance. The DCF model mainly represents the social context in the formof text documents for all nodes. Further symbolicaddition of other types of context such as socialrelationships among politicians and relationshipsbetween various nodes could further help in achiev-ing better performance on these tasks. In the TargetEntity task, RoBERTa-base + DCF embeddings ob-tain 73.56 F1 vs. 68.83 for the best no-contextbaseline. Twitter bio and wiki-context hardly im-prove, demonstrating the effectiveness of modelingcontextual information explicitly vs. concatenat-ing context as text documents. No context per-formance well above the random performance of50 F1 indicates the bias in the target entity dis-tribution among classes. We discuss this in sec-tion 6.4. In Sentiment Identification task, we seethat BERT-large + DCF back-propagation outper-forms all other models. Vague Text Disambigua-tion task results in table 3 show that DCF modelsoutperform other models significantly. 71.71 F1 isobtained by BERT-base + DCF embeddings. BERT-base performing better than bigger PLMs might bedue to DCF models learning tasks being trainedusing BERT-base embeddings.",
  "Ablation Analysis on Vague Text Task": "We report ablation studies in table 5 on the VagueText task test set.We consider 5 splits:(1)Unseen Party: party, eventnot in the train setbut opposing-party, eventis present, (2) UnseenEvent: party not in train set, (3) Flip Event: neg-ative samples with corresponding event flipped-party/vague tweet matched positive samples intrain set and analogous (4) Flip Party and (5) FlipTweet splits. We observe the best model in eachcategory. They obtain weaker performance on un-seen splits, as expected, unseen events being thehardest. Contextualized models achieve higher mar-gins. DCF gains 7.6(13.2%) and DCF embeddingsattain 8.12(20.42%) macro-F1 improvement overBERT-base+wiki compared to respective marginsof 8.86% and 11.42% on the full test set. In theflip splits with only negative examples, accuracygain over random baseline for all splits is seen.This indicates that models learn to jointly conditionon context information rather than learn spuriouscorrelations over particular aspects of the context.Specifically, flip-tweet split results indicate thatmodels dont just learn party-explanation mapping.",
  "Vague Text LLM Generation Quality": "We look into the quality of our LLM-generated dis-ambiguation texts. While GPT-NeoX (Black et al.,2022) produced only 98 good examples out of the498 generated instances with the rest being redun-dant, GPT-3 (Brown et al., 2020a) performed muchbetter. Among the 430 generated instances, 315were annotated as good which converts to an accep-tance rate of 20.04% for GPT-NeoX and 73.26%for GPT-3 respectively. In-house annotators evalu-ated the quality of the generated responses for howwell they aligned with the contextual information.They rejected examples that were either too vague,align with the wrong ideology, or were irrelevant.In the prompt, we condition the input examples inall the few shots to the same event and affiliationas the input vague text. In comparison, the valida-tion of AMT annotations for the same task yielded79.8% good examples even after extensive trainingand qualification tests. Most of the rejections fromAMT were attributed to careless annotations.",
  "Vague Text Human Performance": "We look into how humans perform on the VagueText Disambiguation task. We randomly sample97 questions and ask annotators to answer them asmultiple-choice questions. Each vague text-contextpair was given 4 choices out of which only onewas correct. We provide a brief event descriptionalong with all the metadata available to the annota-tor. Each question was answered by 3 annotators.Among the 97 answered questions, the accuracywas 94.85%, which shows this task is easy for hu-mans who understand the context. Respective per-formance of best models on this subset of data",
  "Target Entity Visualization": "The main goal of this analysis is to demonstratethe usefulness and inspire modeling research inthe direction of entity-sentiment-centric view ofpolitical events. table 4 visualizes one componentof how partisan discourse is structured in theseevents. We study Kavanaugh Supreme Court Nom-ination. We identify discussed entities and separatethem into divisive and agreed-upon entities. Thisanalysis paints an accurate picture of the discussedevent. We observe that the main entities of Trump,Dr. Ford, Kavanaugh, Sen. McConnell, and otheraccusers/survivors emerge as divisive entities. Enti-ties such as Susan Collins and Anita Hill who werevocal mouthpieces of the respective party stancesbut didnt directly participate in the event emergeas partisan entities. Supreme Court, FBI, and otherentities occur but only as neutral entities.",
  "DCF Context Understanding": "We look into examples that are incorrectly pre-dicted using Wikipedia pages but correctly pre-dicted by the DCF model in the appendix (table 6).In examples 1 & 2 of Target Entity-Sentimenttask, when the entity is not explicitly mentionedin the tweet, the Wiki-Context model fails to iden-tify them as the targets. We posit that while theWikipedia page of each relevant event will containthese names, explicit modeling of entities in theDCF model allows correct classification. Exam-ples 1 3 of Vague Text Disambiguation task showthat when no clear terms indicate the sentimenttowards a view, the Wiki-Context model fails todisambiguate the tweet text. Explicit modeling ofpolitician nodes seems to help the DCF model.",
  "Acknowledgements": "We thank Shamik Roy, Nikhil Mehta, and theanonymous reviewers for their vital feedback. Theproject was funded by NSF CAREER award IIS-2048001 and the DARPA CCU Program. The con-tents are those of the author(s) and do not necessar-ily represent the official views of, nor an endorse-ment by, DARPA, or the US Government.",
  "Limitations": "Our work only addresses English language text inUS political domain. We also build upon large lan-guage models and large PLMs which are trainedupon huge amounts of uncurated data. Althoughwe employed human validation at each stage, bi-ases could creep into the datasets. We also dontaccount for the completeness of our datasets as itis a pioneering work on a new problem. Socialcontext is vast and could have a myriad of com-ponents. We only take a step in the direction ofsocial context grounding in this work. The perfor-mance on these datasets might not indicate full so-cial context understanding but they should help insparking research in the direction of models that ex-plicitly model such context. Although we tuned ourprompts a lot, better prompts and evolving modelsmight produce better results on the LLM baselines.Our qualitative analysis is predicated on a handfulof examples. They are attempts to interpret the re-sults of large neural models and hence dont carryas much confidence as our empirical observations.We believe the insights from our findings will en-courage more research in this area. For example,the development of discourse contextualized mod-els that aim to model human-style understandingof background knowledge, emotional intelligence,and societal context understanding is a natural nextstep of our research.",
  "In this work, our data collection process consists ofusing both AMT and GPT-3. For the Target Entityand Sentiment task, we pay AMT workers $1 per": "HIT and expect an average work time of 3 minutes.This translates to an hourly rate of $20 which isabove the federal minimum wage. For the VagueText Disambiguation task, we pay AMT workers$1.10 per HIT and expect an average work time of3 minutes. This translated to an hourly rate of $22.We recognize collecting political views fromAMT and GPT-3 may come with bias or explicitresults and employ expert gatekeepers to filter outunqualified workers and remove explicit resultsfrom the dataset. Domain experts used for anno-tation are chosen to ensure that they are fully fa-miliar with the events in focus. Domain expertswere provided with the context related to the eventsvia their Wikipedia pages, background on the gen-eral issue in focus, fully contextualized quotes, andauthors historical discourse obtained from ontheis-sues.org. We have an annotation quid-pro-quo sys-tem in our lab which allows us to have a network ofin-house annotators. In-house domain experts areresearchers in the CSS area with familiarity witha range of issues and stances in the US politicalscene. They are given the information necessaryto understand the events in focus in the form ofWikipedia articles, quotes from the politicians infocus obtained from ontheissues.org, and news ar-ticles related to the event. We make the annotationprocess as unambiguous as possible. In our annota-tion exercise, we ask the annotators to mark onlyhigh-confidence annotations that can be clearly ex-plained. We use a majority vote from 3 annotatorsto validate the annotations for the target entity task.Our task is aimed at understanding and ground-ing polarized text in its intended meaning. We takeexamples where the intended meaning is clearlybacked by several existing real-world quotes. Wedo not manufacture the meaning to the vague state-ments, we only write down unambiguous explana-tions where context clearly dictates the providedmeaning. Applications of our research as we en-vision would be adding necessary context to shorttexts by being able to identify past discourse fromthe authors that are relevant to the particular text inits context. It would also be able to ground the textin news articles that expand upon the short texts toprovide full context.",
  "Abeer AlDayel and Walid Magdy. 2020. Stance de-tection on social media: State of the art and trends.CoRR, abs/2006.03644": "Malihe Alikhani, Piyush Sharma, Shengjie Li, RaduSoricut, and Matthew Stone. 2020. Cross-modal co-herence modeling for caption generation. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 65256535, On-line. Association for Computational Linguistics. Emily Allaway and Kathleen McKeown. 2020. Zero-Shot Stance Detection: A Dataset and Model usingGeneralized Topic Representations. In Proceedingsof the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 89138931, Online. Association for Computational Lin-guistics. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,Mark Johnson, Niko Snderhauf, Ian Reid, StephenGould, and Anton Van Den Hengel. 2018. Vision-and-language navigation:Interpreting visually-grounded navigation instructions in real environ-ments. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 36743683. Jacob Andreas and Dan Klein. 2016. Reasoning aboutpragmatics with neural listeners and speakers. In Pro-ceedings of the 2016 Conference on Empirical Meth-ods in Natural Language Processing, pages 11731182, Austin, Texas. Association for ComputationalLinguistics.",
  "Kent Bach. 2008. Pragmatics and the Philosophy ofLanguage, pages 463 487. Wiley Online Library": "Ramy Baly, Giovanni Da San Martino, James Glass,and Preslav Nakov. 2020. We can detect your bias:Predicting the political ideology of news articles. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 49824991. Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov,James Glass, and Preslav Nakov. 2018.Predict-ing factuality of reporting and bias of news mediasources. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 35283539, Brussels, Belgium. Associationfor Computational Linguistics.",
  "Kobus Barnard, Pinar Duygulu, David Forsyth, NandoDe Freitas, David M Blei, and Michael I Jordan. 2003.Matching words and pictures. The Journal of Ma-chine Learning Research, 3:11071135": "Emily M. Bender and Alexander Koller. 2020. Climbingtowards NLU: On meaning, form, and understandingin the age of data. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 51855198, Online. Association forComputational Linguistics. Yonatan Bisk, Ari Holtzman, Jesse Thomason, JacobAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-ata, Angeliki Lazaridou, Jonathan May, AleksandrNisnevich, Nicolas Pinto, and Joseph Turian. 2020.Experience grounds language. In Proceedings of the",
  "Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 87188735,Online. Association for Computational Linguistics": "Sid Black, Stella Biderman, Eric Hallahan, Quentin An-thony, Leo Gao, Laurence Golding, Horace He, Con-nor Leahy, Kyle McDonell, Jason Phang, MichaelPieler, USVSN Sai Prashanth, Shivanshu Purohit,Laria Reynolds, Jonathan Tow, Ben Wang, andSamuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model. In Proceed-ings of the ACL Workshop on Challenges & Perspec-tives in Creating Large Language Models. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020a. Language models are few-shot learners. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020b. Language models are few-shot learn-ers. CoRR, abs/2005.14165. David Chen and Raymond Mooney. 2011. Learningto interpret natural language navigation instructionsfrom observations. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 25, pages859865.",
  "Herbert H Clark and Susan E Brennan. 1991. Ground-ing in communication.Perspectives on SociallyShared Cognition, pages 127 149": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Shangbin Feng, Zhaoxuan Tan, Zilong Chen, NingnanWang, Peisheng Yu, Qinghua Zheng, Xiaojun Chang,and Minnan Luo. 2022.Par: Political actor rep-resentation learning with social context and expertknowledge. arXiv preprint arXiv:2210.08362. Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Pa-tel, and Aida Nematzadeh. 2023. Pragmatics in lan-guage grounding: Phenomena, tasks, and modelingapproaches. In Findings of the Association for Com-putational Linguistics: EMNLP 2023, pages 1261912640, Singapore. Association for ComputationalLinguistics. Dirk Hovy and Diyi Yang. 2021. The importance ofmodeling social factors of language: Theory andpractice. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 588602, Online. Associationfor Computational Linguistics. Alexander Hoyle, Rupak Sarkar, Pranav Goel, andPhilip Resnik. 2023. Natural language decompo-sitions of implicit content enable better text repre-sentations. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1318813214, Singapore. Association forComputational Linguistics. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, EvelinaFedorenko, and Edward Gibson. 2023.A fine-grained comparison of pragmatic language under-standing in humans and language models. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 41944213, Toronto, Canada. Associ-ation for Computational Linguistics. Royi Lachmy, Valentina Pyatkin, Avshalom Manevich,and Reut Tsarfaty. 2022. Draw me a flower: Process-ing and grounding abstraction in natural language.Transactions of the Association for ComputationalLinguistics, 10:13411356. Chang Li and Dan Goldwasser. 2019. Encoding so-cial information with graph convolutional networksforPolitical perspective detection in news media. InProceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 25942604, Florence, Italy. Association for ComputationalLinguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. ArXiv, abs/1907.11692. Nikhil Mehta, Mara Leonor Pacheco, and Dan Gold-wasser. 2022. Tackling fake news detection by con-tinually improving social context representations us-ing graph neural networks. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages13631380.",
  "Saif M. Mohammad, Parinaz Sobhani, and SvetlanaKiritchenko. 2016. Stance and sentiment in tweets.CoRR, abs/1605.01655": "Christopher Potts. 2012. Goal-driven answers in thecards dialogue corpus. In Proceedings of the 30thwest coast conference on formal linguistics, pages 120. Cascadilla Proceedings Project Somerville, MA. Rajkumar Pujari and Dan Goldwasser. 2021. Under-standing politics via contextualized discourse pro-cessing. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 13531367, Online and Punta Cana, Domini-can Republic. Association for Computational Lin-guistics. Piyush Sharma, Nan Ding, Sebastian Goodman, andRadu Soricut. 2018. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic im-age captioning. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 25562565,Melbourne, Australia. Association for ComputationalLinguistics.",
  "Robert Stalnaker. 2002. Common ground. Linguisticsand philosophy, 25(5/6):701721": "Alane Suhr, Claudia Yan, Jack Schluger, Stanley Yu,Hadi Khader, Marwa Mouallem, Iris Zhang, andYoav Artzi. 2019.Executing instructions in situ-ated collaborative interactions. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 21192130, Hong Kong,China. Association for Computational Linguistics. Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew Walter, Ashis Banerjee, Seth Teller, andNicholas Roy. 2011. Understanding natural languagecommands for robotic navigation and mobile manip-ulation. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 25, pages 15071514.",
  "David Traum. 1994. A computational theory of ground-ing in natural language conversation. Ph.D. thesis,University of Rochester Rochester, New York": "Takuma Udagawa and Akiko Aizawa. 2019. A naturallanguage corpus of common grounding under contin-uous and partially-observable context. In Proceed-ings of the AAAI Conference on Artificial Intelligence,volume 33, pages 71207127. Jeroen Vaes, Maria Paola Paladino, and Chiara Maga-gnotti. 2011. The human message in politics: Theimpact of emotional slogans on subtle conformity.The Journal of Social Psychology, 151(2):162179.PMID: 21476460.",
  "Derek Weber and Frank Neumann. 2021. Amplifyinginfluence through coordinated behaviour in socialnetworks. Social Network Analysis and Mining, 11": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "Yi Yang, Ming-Wei Chang, and Jacob Eisenstein. 2016": "Toward socially-infused information extraction: Em-bedding authors, mentions, and entities. In Proceed-ings of the 2016 Conference on Empirical Methodsin Natural Language Processing, pages 14521461,Austin, Texas. Association for Computational Lin-guistics. Haolan Zhan, Zhuang Li, Yufei Wang, Linhao Luo, TaoFeng, Xiaoxi Kang, Yuncheng Hua, Lizhen Qu, Lay-Ki Soon, Suraj Sharma, Ingrid Zukerman, ZhalehSemnani-Azad, and Gholamreza Haffari. 2023. So-cialdial: A benchmark for socially-aware dialoguesystems. Xinliang Frederick Zhang, Nick Beauchamp, andLu Wang. 2022. Generative entity-to-entity stance de-tection with knowledge graph augmentation. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 99509969, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics.",
  "BReproducibility": "We use the HuggingFace Transformers (Wolf et al.,2020) library for PLMs. We use GPT-NeoX im-plementation by ElutherAI (Black et al., 2022) andGPT-3 (Brown et al., 2020b) via OpenAI API forour LLM baselines. We run 100 epochs for allexperiments. We use 10 NVIDIA GeForce 1080iGPUs for our experiments. We use the train, de-velopment, and test splits detailed in section 3 forour experiments. We use the development macro-F1 for early stopping. We run all our experimentsusing random seeds to ensure reproducibility. Weexperiment with a random seed value set to {13}.",
  "Entity: Christine Blasey Ford": "Paraphrase: Theres nothing surprising in withdrawing fromthe Paris agreement. Thanks for not caring our environment andfuture generations.Wiki-Context Prediction: Not Target | DCF Prediction: Target (correct)Wiki-Context Prediction: No | DCF Prediction: Yes (correct) Tweet: We will not be intimidated. Democracy will not beintimidated. We must hold the individuals responsible for theJan. 6th attack on the U.S. Capitol responsible. Thank you@RepAOC for tonights Special Order Hour and we willcontinue our efforts to #HoldThemAllAccountable.",
  "Entity: Donald Trump": "Paraphrase: The failure of the Democrats to impeach DonaldTrump is a strong moment for our legislature which can getback to its work helping the American people. Today weve beenable to tell the American people what we have known all along,that Donald Trump was not guilty of these charges.Wiki-Context Predicted: Not Target | DCF Prediction: Target (correct)Wiki-Context Predicted: Yes | DCF Prediction: No (correct) Tweet: #GeorgeFloyd #BlackLivesMatter #justiceinpolicingQT @OmarJimenez Former Minneapolis police officer DerekChauvin is in the process of being released from the HennepinCounty correctional facility his attorney tells us. He is one ofthe four officers charged in the death of George Floyd. Hefaces murder and manslaughter charges."
}