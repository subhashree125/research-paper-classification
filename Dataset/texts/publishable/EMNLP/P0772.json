{
  "Abstract": "In this work, we optimize speculative samplingfor parallel hardware accelerators to improvesampling speed. We notice that substantial por-tions of the intermediate matrices necessary forspeculative sampling can be computed concur-rently. This allows us to distribute the workloadacross multiple GPU threads, enabling simul-taneous operations on matrix segments withinthread blocks. This results in profiling timeimprovements ranging from 6% to 13% rela-tive to the baseline implementation, withoutcompromising accuracy. To further acceleratespeculative sampling, probability distributionsparameterized by softmax are approximatedby sigmoid. This approximation approach re-sults in significantly greater relative improve-ments in profiling time, ranging from 37% to94%, with a minor decline in accuracy. We con-duct extensive experiments on both automaticspeech recognition and summarization tasks tovalidate the effectiveness of our optimizationmethods.",
  "Introduction": "Large foundational speech and language modelsbased on autoregressive Transformer (Vaswaniet al., 2017) architectures have demonstrated re-markable proficiency across a variety of down-stream tasks (Hsu et al., 2021; Radford et al.,2022; Touvron et al., 2023b; Achiam et al., 2024).These models frequently increase in size, con-sequently requiring more memory and computa-tional resources. However, downstream applica-tions, such as dialogue systems, have strict wall-clock constraints and are often required to generatelong sequences (Pope et al., 2023; Chi et al., 2023;Fischer et al., 2024). Due to the sequential tokengeneration in autoregressive decoding, latency in-creases with both the length of the sequence andthe size of the model, resulting in a significant bar-rier to widespread deployment. On many general-purpose GPU hardware accelerator architectures, the increasing size of models leads to more read andwrite operations between high-bandwidth memory(HBM) and on-chip shared memory (SRAM) ateach decoding step, necessitating more memorybandwidth to meet latency constraints (Pope et al.,2023; Dao et al., 2022; Dao, 2024). Consequently,the speed of autoregressive decoding becomes pri-marily limited by the available memory bandwidthand not by the number of computations that needto be executed on the dedicated hardware (Shazeer,2019).In many cases, however, tokens may be accu-rately generated by much smaller models that re-quire fewer resources. Motivated by this hypothe-sis, speculative sampling has been developed toaccelerate autoregressive sampling (Stern et al.,2018; Xia et al., 2023; Leviathan et al., 2023; Chenet al., 2023a). Speculative sampling employs asmall draft model to generate tokens, which arepotential future outputs of a larger target model.These drafted tokens are then verified in parallelby the target model, and only tokens that meetthe validation criteria are retained as final outputsto ensure generation accuracy. This approach hasbeen shown to significantly reduce the frequency oftime-consuming operations, thereby improving in-ference latency (Leviathan et al., 2023; Chen et al.,2023a).In this paper, we focus on optimizing the valida-tion part of speculative sampling to further increasethe inference speed. Inspired by recent advancesin accelerating computations in the attention mech-anism (Dao et al., 2022; Dao, 2024), we exploretwo faster methods for assessing drafted tokens byleveraging the parallelism capabilities of modernGPUs. We identify that a significant portion ofthe intermediate matrices required for the samplingprocess can be computed independently. Exploit-ing this observation, we distribute the workloadacross multiple GPU threads and simultaneouslycompute portions of the intermediate output matri- ces within thread blocks. This optimization methodis faster than the non-optimized baseline imple-mentation and exact with regard to the decodingoutputs, i.e., it generates the same outputs as thenon-optimized method.To further accelerate speculative sampling, wepropose using sigmoid as an element-wise approxi-mation to softmax (Bridle, 1989), which is used toparameterize distributions of target and draft mod-els. Since sigmoid is applied to logits in element-wise fashion, it can be computed in parallel andfused with other sampling-related computations.This enables significant acceleration of the overallprocess, but results in a small accuracy decline dueto the non-exact nature of the method.We evaluate our two optimized algorithms onautomatic speech recognition (ASR) and summa-rization tasks, covering draft model sizes between166M and 2B parameters and target model sizesbetween 244M and 13B parameters. The exact op-timization method reduces profiling time between6% and 13% relative to the baseline implementa-tion without compromising accuracy. Moreover,the non-exact optimization method improves pro-filing time by 37% to 94%, albeit with a smallreduction in accuracy. We summarize our maincontributions as follows:1",
  "Related work": "Techniques such as quantization (Dettmers et al.,2022; Bondarenko et al., 2023; Stock et al., 2021;Nagel et al., 2021), pruning (Voita et al., 2019; La-gunas et al., 2021; Gromov et al., 2024) and knowl-edge distillation (Sun et al., 2019; Sanh et al., 2019;Jiao et al., 2020; Hsieh et al., 2023) have proven ef-fective in reducing inference latency with minimalperformance impact. However, these approachesoften require architectural changes or custom train-ing procedures. Efforts specifically targeting the",
  "The source code of our optimized sampling al-gorithmisavailableat": "reduction of memory bandwidth bottlenecks dur-ing decoding include methods like multi-query at-tention, which aims to optimize memory usageper attention layer (Shazeer, 2019), or FlashAtten-tion (Dao et al., 2022; Dao, 2024), which aims to re-duce the number of read/write operations betweenHBM and SRAM on GPUs. Pope et al. (2023),achieve improvements in large-scale inference la-tency by partitioning models and workload acrossmultiple accelerators combined with various low-level optimizations to improve communication effi-ciency between devices.Speculative sampling approaches can be broadlycategorized based on how drafting and verifica-tion are conducted (Xia et al., 2024a). Draftingrefers to the efficient prediction of multiple futuretokens with a draft model and verification refersto the methods used to verify the token sequencewith the target model. Some works use specializeddraft models (Xia et al., 2023; Zhou et al., 2024).Others employ an existing smaller model from thesame series (Chen et al., 2023a; Spector and Re,2023; Leviathan et al., 2023; Yang et al., 2024)or leverage the target model directly for drafting,e.g., by skipping intermediate layers (Zhang et al.,2024b), using special look-ahead tokens (Moneaet al., 2023), or additional modeling heads (Sternet al., 2018; Cai et al., 2024; Zhang et al., 2024a).Verification approaches first supported greedy de-coding (Stern et al., 2018; Xia et al., 2023; Zhanget al., 2024b) and were subsequently extended tosupport other methods such as nucleus sampling(Leviathan et al., 2023; Chen et al., 2023a). Re-cently, methods to verify multiple draft sequencesin parallel have also been explored (Miao et al.,2024; Cai et al., 2024; Spector and Re, 2023).Several studies have experimented with ReLUand sigmoid as alternatives to softmax in the at-tention mechanism (Bai et al., 2023b; Shen et al.,2023; Hron et al., 2020; Hua et al., 2022; Li et al.,2022; Wortsman et al., 2023; Ramapuram et al.,2024). They either require training new modelsfrom scratch or maintain the computational over-head of gathering information along the full se-quence length. Other softmax-related optimiza-tions are tailored to reduce the memory require-ment during training by computing only smallerfractions of the full softmax output in the backwardpass (Lee and Lee, 2023) or leverage word frequen-cies in the training data to speed up computation(Grave et al., 2017). Shim et al. (2017) approxi-mate softmax by computing only a fraction of the full input with singular value decomposition. Otherapproximation methods are specifically designedfor custom hardware such as field-programmablegate arrays (Chen et al., 2023b) and applicationspecific integrated circuits (Geng et al., 2018).",
  "Method": "3.1PreliminariesSpeculative sampling.Let Mtargetpbe an autore-gressive target model, which induces a categoricaldistribution distribution p(x|x<i+1) over a vocab-ulary V = {x N : 1 x vocab_size},given the prefix x<i+1 = (x1, . . . , xi). Our goal isto use speculative sampling to accelerate the sam-pling process of discrete tokens x V. This isachieved by approximating the target model with adraft model Mdraftq, resulting in another categori-cal distribution q(x|x<i+1).First, given the prefix (x1, . . . , xi+c1), N+draft tokens are sequentially sampled with Mdraftq:xi+c q(x|x<i+c) for c = 1, . . . , . The drafttokens are then evaluated using the target modelMtargetp, a process that can be performed in par-allel. Each draft token xi+c V is accepted ifrc c(xi+c) for c = 1, . . . , . The terms rc andc(xi+c) are computed as follows:",
  "(3)": "We denote the numerator of Eq. 3 by a(x) andthe denominator by b, as these terms are treatedseparately in subsequent sections.The underlying concept of speculative samplingis similar to rejection sampling (Neal, 2003). Intu-itively, a new token for the target model Mtargetp is generated by first sampling from a smaller draftmodel Mdraftq, which shares the same support (V)as Mtargetp. The token sampled from Mdraftqisthen evaluated in parallel with Mtargetp, and its ac-ceptance is determined based on the probabilityratio defined in Eq. 1. If the token is rejected, anew one is drawn using the modified distributionin Eq. 2. GPU memory and execution model.We brieflydescribe the memory components and parts of theexecution model of GPU hardware acceleratorsrelevant to this work. GPU memory has a hierarchi-cal layout, consisting of various types of memorythat differ in size and read/write bandwidth (Jiaet al., 2018). Recent GPUs (e.g., NVIDIAs A100series) typically feature several gigabytes of high-bandwidth memory (HBM) and only a few hundredkilobytes of on-chip shared memory (SRAM) perstreaming multiprocessor (SM) (NVIDIA Corpo-ration, 2020). While HBM provides substantialcapacity, its memory bandwidth is lower comparedto SRAM. The execution model of GPU hardwareaccelerators involves a large number of threads ex-ecuting operations known as kernels (Cheng et al.,2014). These threads are organized into threadblocks and assigned to SMs. Each SM partitionsits assigned thread blocks into warps of 32 threads,which are then queued for execution on availablehardware resources. Each kernel typically followsa pattern: loading inputs from HBM into registersand SRAM, performing computations, and writingthe outputs back to HBM. 3.2Acceleration of speculative sampling3.2.1Exact optimizationOur optimization of speculative sampling is de-signed for parallel heterogeneous hardware accel-erators, such as NVIDIA GPUs, which are widelyemployed to perform inference on large scale mod-els. Similar to the approaches described in Ryooet al. (2008) and Dao et al. (2022), we aim to redis-tribute the speculative sampling workload acrossthreads and thread blocks. We load chunks of in-puts from HBM to SRAM and make the necessarycomputations with respect to this input chunk be-fore writing the final result back to HBM.We notice that the intermediate elements neededfor speculative sampling can be computed concur-rently within thread blocks and are largely indepen-dent of other thread blocks. Specifically, we cancompute (c(x))xV and parts of Eq. 3 in parallel.The kernel is tiled (Lam et al., 1991), such that",
  "HBM": ": Overview of the computations within each thread block for sigmoid approximation. Each set of logits is scaled bya minimum constant and a maximum constant . Sigmoid activations are then computed and stored in SRAM for eachsegment of draft and target logits. Subsequently, the intermediate values fk(x), ak(x), bk, and ck(x) are computed analogousto . The resulting outputs are then used to update c(x), a(x), and b in HBM.",
  "denominator with the sub-vocabulary Vk in SRAM,and perform the final aggregation across blocks inthe subsequent procedure on HBM": "3 The partial results, ck, ak(x), and bk, arewritten back to HBM. The partial sum bk is nowcombined with the partial sums from other threadblocks to compute the full sum b. The final divisionoperation to compute max_norm(f(x)) in Eq. 3 andthe resampling procedure in Eq. 2 are done onceall the partial results are aggregated.By reorganizing the computations as illustratedin , batches of p(x|x<i+c) and q(x|x<i+c)are loaded only once from HBM into SRAM. More-over, most operations are coupled within the kerneland performed in parallel while using fast SRAMto store intermediate values. Only the results nec-essary to produce token acceptance decisions arewritten to HBM. 3.2.2Approximated optimizationTo further accelerate speculative sampling, we usesigmoid to approximate p(x|x<i+c) and q(x|xi+c),which are parameterized by softmax in the baselineimplementation and the exact method describedin .2.1. Instead of treating p(x|x<i+c)and q(x|xi+c) as precomputed inputs to the kernel,the sigmoid approximation is tightly coupled withthe other operations in the speculative samplingprocess. This integration within the kernel substan-tially accelerates the overall sampling procedure. Bottleneck of softmax.For any given input vec-tor w = (w1, . . . , w|V|), the outputs must be posi-tive and they must sum to unity to be interpretableas a probability distribution (Bridle, 1989). In soft-max, both conditions are satisfied via a normalizedexponential transformation. With limited valueranges that can be represented in hardware, soft-max is prone to overflow or underflow due to theexponentiation. Therefore, a numerically stableversion is often used (Milakov and Gimelshein,",
  "|V|l=1 exp(wl wmax)(4)": "for j = 1, . . . , |V|, where wmax = max{wl : 1 l |V|}. Eq. 4 requires summing over the sizeof the vocabulary and finding wmax, which makesparallelization on GPUs challenging, since boththe summation and wmax require keeping track ofintermediate values across blocks (Dao et al., 2022;Rabe and Staats, 2021; Wortsman et al., 2023).The attention mechanism including its soft-max computation has been optimized in FlashAt-tention (Dao et al., 2022) by fusing its opera-tions and using an online algorithm (Milakov andGimelshein, 2018; Rabe and Staats, 2021) thatsplits the workload into blocks and rescales theoutput of each block. Unlike FlashAttention, weexplore a fully local operation that can run with-out expensive tracking of intermediate variablesacross blocks, thus allowing for non-blocking par-allel computation. Sigmoid approximation.Let zp(x|x<i+c) bethe logits of the target model Mtargetpgiven theprefix (x1, . . . , xi+c1). Similarly, let zq(x|x<i+c)be the logits of the draft model Mdraftq. First, werescale the logits using predefined constant values < 0 and > 0, and then apply sigmoid tothese scaled logits to approximate p(x|xi+c) andq(x|x<i+c) as follows:",
  "xi+c max_norm (p(x|x<i+c) q(x|x<i+c))": "illustrates the computations with sig-moid approximation executed in parallel withineach thread block. The main changes are high-lighted in red rectangles.1 Let zpk(x|x<i+c)and zqk(x|x<i+c) be restrictions of the log-its zp(x|x<i+c) and zq(x|x<i+c) to the sub-vocabulary Vk of the corresponding currenttile.The function values of zpk(x|x<i+c) andzqk(x|x<i+c) evaluated on Vk are loaded fromHBM into SRAM. 2 We apply sigmoid to therescaled logits zpk(x|x<i+c) and zqk(x|x<i+c).Since the computation of sigmoid is an element-wise operation and does not depend on values fromother threads and blocks, we can execute it in par-allel, thereby further accelerating speculative sam-pling. Similar to step 2of the exact optimiza-tion in , the partial results, fk(x), ak(x), bk,and ck(x), which are approximations of fk(x),ak(x), bk, and ck(x), respectively, are computedand stored in SRAM. 3The partial results arewritten back to HBM, and for bk, they are aggre-gated across blocks to compute the final result b,which is approximation of b.",
  "Experimental setup": "Datasets and metrics.We evaluate accuracy andinference speed of our optimized speculative sam-pling on ASR and single-document summarization.For ASR, we measure word error rates (WERs)on the test portions of three English datasets:CommonVoice 16 (CV16) (Ardila et al., 2020),LibriSpeech (Panayotov et al., 2015), and TED-LIUM (Rousseau et al., 2012). For summarization,we use the test portions of Extreme Summarization(Xsum) (Narayan et al., 2018) and CNN/Daily Mail(CNN/DM) (Nallapati et al., 2016) to evaluate thequality of summaries generated by language mod-els with ROUGE-1 (Lin, 2004). Additional datasetdetails are provided in Appendix A.3.For all tasks, we use the PyTorch (Paszke et al., 2019) profiling tool to obtain execution times forperforming speculative sampling.We measurethe execution time within the entire call stack ofthe speculative sampling function, including anynested function call (e.g. softmax). The profilingtimes are summed over all decoding steps and ex-amples in a dataset, before the relative improve-ment is calculated. Hyperparameters.We set the batch size B to 1and employ the same heuristic used in the baselinespeculative sampling implementation in the Trans-formers library (Wolf et al., 2020), to determinethe number of draft tokens . Initially, is set to 5and increases by 2 if all speculative tokens sampledfrom the draft model are accepted; otherwise, it de-creases by 1. We set the maximum sequence lengthto 256 tokens for ASR and 100 tokens for summa-rization. For ASR, using sigmoid approximation, and are set to 103 and 103, respectively. Insummarization experiments, we use = 104 and = 104. We set n = 1024, i.e., the maximumavailable threads per block on the NVIDIA A100GPU. Target models.We employ Whisper (Radfordet al., 2022) as the target model series for the ASRtask. We use both the multilingual 1.55B param-eter whisper-large-v2 version and the English-only 244M parameter whisper-small.en versionof the model. For the summarization task, we useLlama2 7B/13B (Touvron et al., 2023b), Qwen1.8B/7B (Bai et al., 2023a), and Gemma 7B (Mes-nard et al., 2024). More details on the target modelsare provided in Appendix A.1.",
  "Draft models.Following Leviathan et al. (2023),": "Chen et al. (2023a), and Zhou et al. (2024) weeither use smaller models of the same series ordistilled versions of the target model for drafting.The draft model family for the ASR task is Distil-Whisper (Gandhi et al., 2023). In particular, weuse the 166M parameter small.en and the 756Mparameter distil-large-v2 versions. The draftmodel for Llama2 is Sheared-LLaMA (Xia et al.,2024b), a version of Llama2 pruned to 1.3B param-eters. The draft models corresponding to Qwen andGemma are the 500M and 2B parameter versionsof the same series. More details on the draft modelsare provided in Appendix A.2. Implementation details.We use the implementa-tion of speculative sampling provided by the Trans-formers library (Wolf et al., 2020) (v4.38.2) in con-junction with PyTorch (v2.2.2) as our baseline. Un-less stated otherwise, all models are loaded in FP16and executed on A100 GPUs with 80GB HBM us-ing the same compute node running CUDA 12.3and NVIDIA device driver version 545.",
  "Main results": "summarizes accuracy metrics and profil-ing results for the ASR and text summarizationtasks. The table details the datasets, target anddraft models used, performance metrics, and therelative reduction in overall GPU profiling timeachieved by our optimized approaches (exact andsigmoid approximation) compared to the baseline.In the ASR task, our exact optimization methodmaintains the same WER compared to the baselineand achieves reduction in profiling time rangingfrom 8.7% to 12.5%. The sigmoid approximationapproach results in moderately increased WERs,but yields more significant profiling time improve-ments, ranging from 71.9% to 84.0%.In the text summarization task, our exact opti-mization method demonstrates a similar trend asobserved in the previous ASR experiments, reduc-ing profiling time by 5.7% to 11.1% without af-fecting ROUGE-1 scores. The non-exact sigmoidapproximation further achieves significant profilingtime reductions, reaching up to 93.6%. However,we also observe an absolute difference in ROUGE-1 of 0.02 to 0.06 points.Additionally, we provide relative wall-clock timeimprovements for the overall text generation pro-cess in of Appendix A.5, showing that theresults obtained via profiling translate into improve-ments in wall-clock time for both the exact and the",
  "Analysis and discussion": "Execution times remain stable over varying .To assess the robustness of our exact and sigmoidoptimization methods, we measure execution timesacross different models and varying numbers ofinitial draft tokens . For text summarization, werandomly sample 10% of the Xsum test set and useGemma, Qwen, and Llama2 model combinationsto generate summaries. For ASR, we use 10% ofrandomly sampled examples from the CV16 testset.a and b illustrate the averageexecution times of the different implementationsprofiled per decoding step. Both figures show aver-age execution times measured in milliseconds (ms)for the number of draft tokens ranging from 1 to20. The average execution times for the optimizedapproaches (exact and sigmoid) are consistently be-low the baseline across all models and values of .Furthermore, the execution times of the optimizedapproaches are stable across different choices of for the Gemma and Qwen models, whereas theLlama2 7B/Sheared LLaMA 1.3B model combi-nation exhibits small sensitivity to the number ofdraft tokens.As depicted in b, the ASR models alsoexhibit stable execution times across different ,further validating the robustness of our optimiza-tion methods with varying numbers of draft tokens.",
  "Optimized sampling does not introduce addi-tional memory overhead.We assess the mem-": "ory usage of our optimized methods relative to thebaseline implementation. shows the peakmemory usage (HBM) on randomly sampled in-stances (10%) of the Xsum test set with variousinitial values of and different language models.The graph indicates that our optimized approachesdo not introduce additional memory overhead com-pared to the baseline. For all three model combina-tions (Llama2, Qwen, and Gemma), the memoryusage of the optimized methods fluctuates slightly(within a range of approximately 200MB) aroundthe memory usage of the baseline across all drafttokens. As illustrated in , the memory usageresults for the Whisper models display a patternconsistent with the text summarization experiments,showing fluctuation within a range of under 10 MB. Effect of scaling logits.To study the effect oflogit scaling in the sigmoid approximation method,we compare profiling time and performance met-rics under varying values of and (cf. Eq. 5). provides a comparison of different scalingfactors for subsets (10%) of CV16 and Xsum usingWhisper Small.EN and Llama2 7B, respectively.For each task, the table shows the values of and applied, the resulting WER or ROUGE-1 score,and the relative improvement in profiling time overthe non-optimized baseline implementation. Notethat scaling is necessary due to the numerical insta-bility induced by the exponentiation in the sigmoidfunction.The Llama2 model combination exhibits relativestability across different scaling factors, showing # of Draft Tokens ( ) 2.4 3.0 3.6 4.2 4.8 5.4 msXsum Gemma 7B/2B # of Draft Tokens ( ) Qwen 7B/0.5B # of Draft Tokens ( ) Llama2 7B/Sheared 1.3B BaselineExact",
  ": Peak memory usage (HBM) on randomly sampled10% of the Xsum test set for varying initial values of": "minor fluctuations in ROUGE-1 scores and pro-filing time improvements, whereas the Whispermodel combination is more sensitive, with scalingfactors of 105 leading to substantial deteriora-tion of both WER and profiling time. This can beattributed to the logits of Whisper models beinggenerated in half precision, whereas the logits gen-erated by the Llama models are available in fullprecision. However, we also find that scaling fac-tors of 103 and 104 generally yield comparableresults in both accuracy and profiling time improve-ment across the model combinations investigatedin this work. The results of the same analysis forthe other draft and target model combinations areprovided in from Appendix A.6.",
  ": Peak memory usage (HBM) on randomly sampled10% of the CV16 test set for varying initial values of": "Furthermore, we see a general trend wherehigher accuracy (lower WER and higher ROUGE-1) coincides with higher profiling time improve-ments. This relationship is due to the token ac-ceptance process, where better calibrated modelsaccept more tokens, requiring fewer executions ofresampling and fewer overall calls of the specula-tive sampling kernel. Data transfer between HBM and SRAM.Toassess efficiency in terms of data movement be-tween HBM and on-chip SRAM, we compare therealized bandwidths of each implementation. Thebandwidth is calculated by dividing the total bytestransferred by the execution time. The number of",
  ": Impact of varying and on accuracy and profilingtime of sigmoid approximation on CV16 and Xsum": "bytes transferred is derived from the total numberof sectors moved between HBM and SRAM, witheach sector consisting of 32 bytes. Execution timerefers to the duration during which the GPU is ac-tively running a kernel, i.e., when at least one GPUunit is engaged in computation rather than idling,waiting, or stalled. A lower realized bandwidthindicates a reduced communication overhead be-tween HBM and SRAM.The results in show lower memory band-width usage for the Qwen and Gemma models withthe exact optimization approach compared to thebaseline implementation. However, in the case ofthe Llama2 and Whisper models, despite overalllower realized bandwidths relative to the Qwen andGemma models, higher bandwidths are observedwith the exact optimization compared to their cor-responding baseline.The sigmoid approximation has consistentlyhigher realized bandwidths across all model com-binations compared to the baseline. Although thesigmoid optimization approach reduces the overallamount of data transferred, i.e., the total number ofbytes moved between HBM and SRAM, due to theelement-wise approximation of the softmax func-tion within the sampling kernel, the significantlyfaster execution times result in higher overall real-ized bandwidths. However, even the highest real-ized bandwidths are far below the theoretical HBMbandwidth limit of 2 TB/s (NVIDIA Corporation,2020), indicating that memory transfer is not thelimiting factor for performance. Results on RTX 2080 TI GPU. showsan accuracy and profiling time comparison usingRTX 2080 TI GPUs. We observe that our opti-mization methods achieve performance similar toA100 GPUs with relative improvements in profil-ing time ranging between 5% and 13% withthe exact method and between 62% and 82%",
  "WhisperLarge V2Distil-WhisperLarge V29.32 GB/s 11.18 GB/s 16.06 GB/s": "Qwen 7BQwen 0.5B44.99 GB/s 31.65 GB/s 52.14 GB/sGemma 7BGemma 2B53.69 GB/s 38.51 GB/s 62.99 GB/sLlama2 7B Sheared Llama 1.3B 24.56 GB/s 27.70 GB/s 30.52 GB/sLlama2 13B Sheared Llama 1.3B 20.18 GB/s 24.08 GB/s 31.39 GB/s : Comparison of realized bandwidths across modelsand optimization techniques using 100 examples of the XSumtest set for Qwen, Gemma, and Llama2 models and 100 exam-ples of the CV16 test set for Whisper models. with sigmoid approximation. The summarizationexperiment with Qwen uses a 1.8B/0.5B parametermodel combination instead of the 7B/0.5B modelcombination from the main experiments in ,due to the limited HBM size of 11GB available onthe RTX 2080 TI series.",
  "Conclusions": "We introduced two optimization methods to ac-celerate speculative sampling for autoregressivemodels on hardware accelerators. By computingsignificant portions of intermediate matrices acrossmultiple GPU threads within thread blocks, our ex-act optimization method led to improved samplingspeed without compromising accuracy. Addition-ally, we employed an approximation technique us-ing element-wise sigmoid instead of softmax, toenable parallel computation of probabilities. Thisapproximation further accelerated the decoding pro-cess but resulted in a small degradation of samplingquality.",
  "In this work, we study the inference efficiency ofspeech and language models in the context of spec-": "ulative decoding using GPU hardware accelerators.Our investigation includes two optimized specula-tive sampling algorithms, tested on these modelsto enhance inference speed. While GPUs are themost common general-purpose hardware accelera-tors, there exist purpose-built architectures such asCerebrass Wafer Scale Engines, Googles TPUs,and GraphCores IPUs, where the differences insystem design may negate or significantly reducethe latency gains. Our experiments were conductedexclusively on A100 and RTX 2080 TI GPUs on asingle compute node. Therefore, the generalizabil-ity of the results to other hardware configurationsremains uncertain. The performance outcomes maybe influenced by other hardware and network con-figurations, such as multi-GPU and multi-node se-tups, as well as the availability fast interconnects(e.g. Infiniband), and other network conditions. Ad-ditionally, our study evaluates the effectiveness ofthe optimized algorithm based on decoding time,and our claims may not translate to other metrics,such as energy usage or heat generation, althoughthey play an important role in real-world produc-tion settings. We gratefully acknowledge the scientific sup-port and HPC resources provided by the Er-langen National High Performance ComputingCenter (NHR@FAU) of the Friedrich-Alexander-Universitt Erlangen-Nrnberg (FAU) under theNHR project b196ac14.NHR funding is pro-vided by federal and Bavarian state authorities.NHR@FAU hardware is partially funded by theGerman Research Foundation (DFG) 440719683.This work was supported by the Bavarian StateMinistry of Science and the Arts under grant H.2-F1116.N/61/2.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023a. Qwen technical report. arXivpreprint arXiv:2309.16609": "Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, andSong Mei. 2023b.Transformers as statisticians:Provable in-context learning with in-context algo-rithm selection. Advances in Neural InformationProcessing Systems (Neurips). Yelysei Bondarenko, Markus Nagel, and TijmenBlankevoort. 2023. Quantizable transformers: Re-moving outliers by helping attention heads do noth-ing.Advances in Neural Information ProcessingSystems (NeurIPS). John Bridle. 1989. Training stochastic model recogni-tion algorithms as networks can lead to maximummutual information estimation of parameters. Ad-vances in Neural Information Processing Systems(NeurIPS). Tianle Cai, Yuhong Li, Zhengyang Geng, HongwuPeng, Jason D Lee, Deming Chen, and Tri Dao. 2024.Medusa: Simple LLM inference acceleration frame-work with multiple decoding heads. arXiv preprintarXiv:2401.10774. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. 2023a. Accelerating large language modeldecoding with speculative sampling. arXiv preprintarXiv:2302.01318. Ke Chen, Yue Gao, Haroon Waris, Weiqiang Liu, andFabrizio Lombardi. 2023b. Approximate softmaxfunctions for energy-efficient deep neural networks.IEEE Transactions on Very Large Scale Integration(VLSI) Systems.",
  "Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,and Christopher Re. 2022. Flashattention: Fast andmemory-efficient exact attention with IO-awareness.Neural Information Processing Systems (NeurIPS)": "Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. 2022. GPT3.int8(): 8-bit matrix mul-tiplication for transformers at scale. In Advances inNeural Information Processing Systems (NeurIPS). Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk,and Emma Strubell. 2023. The framework tax: Dis-parities between inference efficiency in NLP researchand deployment. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 15881600, Singapore. Associa-tion for Computational Linguistics. Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg,Iain Mackie, Federico Rossetto, and Jeffrey Dal-ton. 2024.GRILLBot in practice: Lessons andtradeoffs deploying large language models for adapt-able conversational task assistants. arXiv preprintarXiv:2402.07647.",
  "Jiri Hron, Yasaman Bahri, Jascha Narain Sohl-Dickstein,and Roman Novak. 2020. Infinite attention: Nngpand ntk for deep attention networks. InternationalConference on Machine Learning (ICML)": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, RanjayKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-tilling step-by-step! outperforming larger languagemodels with less training data and smaller modelsizes. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 80038017,Toronto, Canada. Association for Computational Lin-guistics. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Trans. Audio, Speechand Lang. Proc., page 34513460.",
  "Zhe Jia, Marco Maggioni, Benjamin Staiger, andDaniele P. Scarpazza. 2018. Dissecting the NVIDIAvolta GPU architecture via microbenchmarking.arxiv preprint arXiv:1804.06826": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, XiaoChen, Linlin Li, Fang Wang, and Qun Liu. 2020.TinyBERT: Distilling BERT for natural language un-derstanding. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 41634174, Online. Association for Computational Lin-guistics. Franois Lagunas, Ella Charlaix, Victor Sanh, andAlexander Rush. 2021. Block pruning for faster trans-formers. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Process-ing, pages 1061910629, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Monica D. Lam, Edward E. Rothberg, and Michael E.Wolf. 1991. The cache performance and optimiza-tions of blocked algorithms. Architectural Supportfor Programming Languages and Operating Systems(ASPLOS).",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Thomas Mesnard, Cassidy Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,Morgane Rivire, Mihir Sanjay Kale, Juliette Love,et al. 2024.Gemma:Open models based ongemini research and technology.arXiv preprintarXiv:2403.08295. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, XinhaoCheng, Zeyu Wang, Zhengxin Zhang, Rae Ying YeeWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chu-nan Shi, Zhuoming Chen, Daiyaan Arfeen, ReynaAbhyankar, and Zhihao Jia. 2024. Specinfer: Accel-erating large language model serving with tree-basedspeculative inference and verification. In Proceed-ings of the 29th ACM International Conference onArchitectural Support for Programming Languagesand Operating Systems, Volume 3.",
  "Markus Nagel, Marios Fournarakis, Rana Ali Amjad,Yelysei Bondarenko, Mart Van Baalen, and TijmenBlankevoort. 2021. A white paper on neural networkquantization. arXiv preprint arXiv:2106.08295": "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,aglar Gulehre, and Bing Xiang. 2016. Abstrac-tive text summarization using sequence-to-sequenceRNNs and beyond.In Proceedings of the 20thSIGNLL Conference on Computational Natural Lan-guage Learning, pages 280290, Berlin, Germany.Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata.2018. Dont give me the details, just the summary!topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 17971807, Brussels, Bel-gium. Association for Computational Linguistics.",
  "Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-jeev Khudanpur. 2015. Librispeech: An asr corpusbased on public domain audio books. In ICASSP": "Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Pytorch:An imperative style, high-performance deep learn-ing library. Neural Information Processing Systems(NeurIPS). Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,Jacob Devlin, James Bradbury, Jonathan Heek, Ke-fan Xiao, Shivani Agrawal, and Jeff Dean. 2023.Efficiently scaling transformer inference. MachineLearning and Systems.",
  "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019": "Patient knowledge distillation for BERT model com-pression. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages43234332, Hong Kong, China. Association for Com-putational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "Hugo Touvron et al. 2023b.Llama 2: Open foun-dation and fine-tuned chat models. arXiv preprintarXiv:2307.09288": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems (NeurIPS). Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-nrich, and Ivan Titov. 2019. Analyzing multi-headself-attention: Specialized heads do the heavy lift-ing, the rest can be pruned. In Proceedings of the57th Annual Meeting of the Association for Computa-tional Linguistics, pages 57975808, Florence, Italy.Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "Mitchell Wortsman, Jaehoon Lee, Justin Gilmer,and Simon Kornblith. 2023.Replacing softmaxwith relu in vision transformers.arXiv preprintarXiv:2309.08586": "Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, FuruWei, and Zhifang Sui. 2023.Speculative decod-ing: Exploiting speculative execution for accelerat-ing seq2seq generation. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 39093925, Singapore. Association for Com-putational Linguistics. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, andZhifang Sui. 2024a.Unlocking efficiency inlarge language model inference: A comprehensivesurvey of speculative decoding.arXiv preprintarXiv:2401.07851. Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and DanqiChen. 2024b. Sheared LLaMA: Accelerating lan-guage model pre-training via structured pruning. In-ternational Conference on Learning Representations(ICLR).",
  "Biao Zhang and Rico Sennrich. 2019. Root mean squarelayer normalization. Neural Information ProcessingSystems (NeurIPS)": "Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,Gang Chen, and Sharad Mehrotra. 2024b. Draft&verify: Lossless large language model accelerationvia self-speculative decoding. In Proceedings of the62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1126311282, Bangkok, Thailand. Association forComputational Linguistics. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,Aditya Krishna Menon, Afshin Rostamizadeh, SanjivKumar, Jean-Franois Kagy, and Rishabh Agarwal.2024. Distillspec: Improving speculative decodingvia knowledge distillation. In International Confer-ence on Learning Representations (ICLR).",
  "AAppendix": "A.1Target model detailsWhisper.Whisper (Radford et al., 2022) is a fam-ily of models trained to perform multiple taskssuch as multilingual ASR, language identification,and speech translation. The models are trained on680k hours of labeled audio data retrieved fromthe world wide web and are available in five sizesranging from 39M parameters to 1.55B parame-ters. Utilizing an encoder-decoder Transformerarchitecture, Whisper receives 80-channel log-Melspectrogram representations with a 25ms windowand a stride of 10ms as inputs. We conduct exper-iments on both the multilingual 1.55B parameterwhisper-large-v2 version and the English-only244M parameter whisper-small.en version. Llama2.Llama2 (Touvron et al., 2023b) is acollection of LLMs ranging from 7B to 70B pa-rameters.The models are pretrained on 2 tril-lion tokens of text data from publicly availablesources. The architecture is based on Llama1 (Tou-vron et al., 2023a), utilizing pre-normalization withRMSNorm (Zhang and Sennrich, 2019), SwiGLU(Shazeer, 2020) activation functions, and rotarypositional embeddings (RoPE) (Su et al., 2024).Notable architectural changes include an expandedcontext length of 4K tokens and the adoption ofgrouped-query attention (GQA) for the 34B and70B models. We employ the 7B and 13B versionsof Llama2 as target models. Qwen.The Qwen (Bai et al., 2023a) model seriesoffers a range of decoder-only language modelswith parameter counts between 500M and 110B.The models are pretrained on up to 3 trillion tokensof various multilingual text, code, and mathematicsresources. The architecture is similar to Llama2with small modifications, such as no weight tyingbetween input embeddings and output projection.We employ Qwen v1.5 in our experiments and usethe 7B parameter variant as the target model. Gemma.Gemma (Mesnard et al., 2024) com-prises two model variants, featuring 2B and 7Bparameters, pretrained on 3 trillion and 6 trilliontokens respectively. Gemma is based on the Gem-ini (Anil et al., 2023) model family. The focusis primarily on English text from web documents,mathematics, and code, omitting multimodal ca-pabilities and optimization for multilingual tasks.Similar to Llama2, the Gemma models leverageRoPE and RMSNorm, and embeddings are shared",
  "across inputs and outputs to reduce model size. Weuse the 7B parameter variant of Gemma v1.0 as thetarget model": "A.2Draft model detailsDistil-Whisper.The draft model series for theASR task is Distil-Whisper (Gandhi et al., 2023),a collection of smaller versions of the Whispermodel. Distil-Whisper applies knowledge distil-lation (Hinton et al., 2015) to emulate the perfor-mance of the original Whisper model using a large(21k hours) pseudo-labeled training corpus. Thedistilled models aim to maintain the robustness ofWhisper towards varying audio domains and noisyacoustic conditions and are designed to be pairedwith Whisper in a speculative decoding setting. Weuse the 166M parameter small.en version as thedraft model for the small 244M parameter targetmodel and the 756M parameter distil-large-v2version as the draft model for the large 1.55B pa-rameter target model. Sheared-LLaMA.The draft model series for ourexperiments with Llama2 is Sheared-LLaMA (Xiaet al., 2024b). Sheared-LLaMA utilizes a struc-tured pruning approach to reduce the size of the 7Bparameter Llama 2 model to 1.3B and 2.7B param-eters. The structured pruning approach removes pa-rameters from the source model until a given targetconfiguration is satisfied. Learned pruning masksrepresenting discrete prune or retain decisions areused to create a smaller sub-network matching thespecified target configuration. We employ the 1.3Bversion of Sheared-LLaMA in our experiments. A.3Dataset detailsASR.We used the test sets of three English ASRbenchmark datasets: CommonVoice v16 (Ardilaet al., 2020), LibriSpeech (Panayotov et al., 2015),and TED-LIUM (Rousseau et al., 2012) for theASR task. The data comprises multiple domainssuch as audiobooks, political speeches, interviews,and narrated Wikipedia articles. The utterancelengths vary between 0.2 and 330 seconds withan average duration of 7.66.6 seconds. Text summarization.We used two datasetsfor text summarization: Extreme Summarization(Xsum) (Narayan et al., 2018) and CNN/Daily Mail(CNN/DM) (Nallapati et al., 2016). The Xsum testset contains 11,334 online articles from the BritishBroadcasting Corporation (BBC) and the CNN/DMtest set contains 11,490 news articles published byCNN and the Daily Mail. We performed 0-shot",
  "Gemma 7BGemma 2B1.2%18.5%Qwen 7BQwen 0.5B4.3%59.1%Llama2 7BSheared Llama 1.3B6.5%53.1%Llama2 13BSheared Llama 1.3B10.9%23.6%": ": Relative wall-clock time improvements for both exact and sigmoid sampling on all tasks and model combinations.Wall-clock time measures the total time spent in the speculative decoding loop, including all forward passes through the draftand target models. The relative improvements are computed based on the total time required to perform speculative decoding forthe full dataset. evaluation for CNN/DM and Xsum, and used theROUGE-1 metric for comparison. To prompt themodel for a summary, we placed Summary: aftereach input article. Summaries were generated witha maximum token length of 100 for both Xsum andCNN/DM.",
  "A.4Wall-clock time improvement": "summarizes the relative wall-clock timeimprovements for the overall text generation pro-cess. Both the exact and the sigmoid approximationmethod translate into relative improvements com-pared to the baseline implementation. Wall-clocktimes are less precise, since they also include theforward passes through the draft and target models,which may lead to additional overhead introducedby the deep learning framework (Fernandez et al.,2023), and the time spent on CPU, which does nottake varying rates of context switches and stallingdue to execution of higher-priority processes intoaccount.",
  "A.5Average times per decoding step": "The average times spent in the speculative samplingprocedure per decoding step are summarized in Ta-ble 6. Our implementation achieved consistentlylower average sampling times than the referenceimplementation. While the the average samplingtime was generally longer for the text generationtasks, the average times with our implementationwere still consistently lower than the reference im-plementation.",
  "shows the impact of various logit scalingfactors on performance and profiling time of sig-moid approximation on CV16 and Xsum. Thevalues are computed on a random sample of 10%of each dataset": "A.7Relation to other optimization methodsOur method is orthogonal to other optimizations ofspeculative decoding. Whenever speculative sam-pling is used, our kernel can serve as a drop-inreplacement for the standard implementation. Forexample, our proposed method can be integratedwith the recently proposed self-speculative decod-ing approach (Zhang et al., 2024b). Instead of usinga separate draft model, self-speculative decodingsamples draft tokens by skipping some layers ofthe target model. Afterwards, it follows the samedraft verification and resampling procedure as theoriginal speculative decoding, which can be furtheraccelerated with our optimization method.Our method is also orthogonal to other ap-proaches for accelerating decoding. For instance,FlashAttention (Dao et al., 2022; Dao, 2024),which focuses on optimizing the attention com-putation, can be easily combined with our methodto further improve efficiency. A.8Overhead caused by resamplingTo assess the overhead caused by resampling, wefollowed Chen et al. (2023a) and computed aver-age acceptance rates of draft tokens for variousmodels on 10% of Xsum. includes theacceptance rates using a varying number of draft",
  "Xsum": "Gemma 7BGemma 2B6.390.505.760.42 4.610.55 9.9%27.9%Qwen 7BQwen 0.5B11.551.39 10.651.51 3.200.42 7.8%72.3%Llama2 7B Sheared Llama 1.3B 4.660.424.142.88 3.640.56 11.1%21.8%Llama2 13B Sheared Llama 1.3B 4.670.414.171.70 3.700.44 10.5%20.7% : Average time and standard deviation spent within the speculative sampling algorithm per decoding step. The column% Prof. Time measures the relative reduction in average time per decoding step (Baseline vs. Exact andSigmoid).Scaling constants for sigmoid approximation: = 103 and = 103 for ASR, = 104 and = 104 for summarization. tokens ( {3, 5, 10, 15}), as well as the averageexecution time per decoding step of the speculativesampling algorithm. Since our exact optimizationmethod aims to generate the same tokens as thebaseline implementation, we expect the acceptancerates to be the same. shows that this isindeed the case for all choices of and model com-binations. also shows that the acceptancerates with the sigmoid optimization method areoften higher than the acceptance rates of the base-line and the exact method. However, these higheracceptance rates, do not have a significant effecton the average execution time. In particular, theacceptance rates of the three methods (sigmoid, ex-act, and baseline) are similar for the Qwen modelcombination (48% with = 10), but the sigmoidapproximation achieves better execution times thanthe exact optimization and the baseline. The aver-age execution times provided are consistentwith the ones presented in a."
}