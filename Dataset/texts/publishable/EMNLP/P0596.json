{
  "Abstract": "Pixel-based language models have emergedas a compelling alternative to subword-basedlanguage modelling, particularly because theycan represent virtually any script.PIXEL, acanonical example of such a model, is a vi-sion transformer that has been pre-trained onrendered text. While PIXEL has shown promis-ing cross-script transfer abilities and robustnessto orthographic perturbations, it falls short ofoutperforming monolingual subword counter-parts like BERT in most other contexts. Thisdiscrepancy raises questions about the amountof linguistic knowledge learnt by these mod-els and whether their performance in languagetasks stems more from their visual capabili-ties than their linguistic ones. To explore this,we probe PIXEL using a variety of linguisticand visual tasks to assess its position on thevision-to-language spectrum. Our findings re-veal a substantial gap between the models vi-sual and linguistic understanding. The lowerlayers of PIXEL predominantly capture superfi-cial visual features, whereas the higher layersgradually learn more syntactic and semantic ab-stractions. Additionally, we examine variantsof PIXEL trained with different text renderingstrategies, discovering that introducing certainorthographic constraints at the input level canfacilitate earlier learning of surface-level fea-tures. With this study, we hope to provide in-sights that aid the further development of pixel-based language models.1",
  "Introduction": "Subwords are currently the standard units of pro-cessing in language modelling (Sennrich et al.,2016). While they have been shown to work wellin monolingual models (Devlin et al., 2019; Liuet al., 2019a), in a multilingual context they canlead to an inevitable vocabulary bottleneck witheach language competing for space in a finite vo-cabulary (Rust et al., 2023; Liang et al., 2023). Characters and byte-based models have been pro-posed as alternatives to subwords, but they lead tolonger input sequences (Raffel et al., 2020; Xueet al., 2022; Tay et al., 2022; Clark et al., 2022a).Another proposed solution is pixel-based modelswhere patches of pixels are the main unit of repre-sentation. A canonical example of this is the PIXEL(Pixel-based Encoder of Language) model (Rustet al., 2023), where text is rendered as a sequence offixed-sized patches and passed as input to a visiontransformer (ViT) (Dosovitskiy et al., 2021). Thisapproach allows the model to represent virtuallyany script.Although current versions of the pixel-based lan-guage models do not outperform their monolingualsubword-based counterparts on most downstreamtasks (Rust et al., 2023; Lotz et al., 2023), they are apromising approach to multilingual modelling andoffer a unique opportunity to explore modellinglanguage through images.PIXEL is a juxtaposi-tion of a vision and language model: even thoughit receives image patches as input, the content ofthose patches is rendered text, making it a visualmodel of language. With this study, we aim tounderstand where PIXEL stands on the vision-to-language spectrum. To this end, we probe PIXELon various visual and language tasks and compareperformance with BERT (Devlin et al., 2019) thelanguage model it is most comparable to andVIT-MAE (He et al., 2022) the vision model it ismost comparable to. We conduct a comprehensiveanalysis of the linguistic and visual capabilities of",
  "We find that PIXEL learns surface-level linguisticinformation in the lower layers, resulting in higher-": "level syntactic and semantic abstractions appearingin higher layers than BERT (5.1). When compar-ing to VIT-MAE, PIXEL underperforms on imagetasks, with visual probing accuracy decreasing inthe higher layers (5.2). Thus, the surface-level in-formation is diluted as it acquires linguistic knowl-edge in the higher layers. Lotz et al. (2023) trained newer pixel-based lan-guage models that add some orthographic con-straints to the input that can potentially augmentlinguistic learning in the lower layers. In this con-text, we ask the following question:",
  "PIXEL": "The pixel-based language models examined in thisstudy are ViTs (Dosovitskiy et al., 2021) that lieat the confluence of NLP and computer vision. AViT is an application of the transformer architec-ture (Vaswani et al., 2017; Devlin et al., 2019) toprocess images. An image is split into patches thatare each flattened into a vector and then projectedinto a lower-dimensional space through a lineartransformation. Positional embeddings are addedto retain spatial information before feeding thesepatch vectors into a transformer encoder.Inspired by the self-supervised masked languagemodelling paradigm, a variant of ViT is the maskedauto-encoder (He et al., 2022), or VIT-MAE, thatlearns image representations by masking randomimage patches. A decoder reconstructs the imagefrom the latent representation of the mask tokens.The PIXEL model by Rust et al. (2023) is trainedon the VIT-MAE architecture. It takes a renderedimage of text sized 16 8464 as input, which issplit into patches of 16 16 pixels. Instead of",
  "By visual capability, we refer to a surface level under-standing of characters in a text, analogous to the kind de-scribed in (Conneau et al., 2018)": "randomly masking individual patches, PIXEL ran-domly masks spans of patches to force the model tolearn higher levels of language abstraction. PIXELis pre-trained on a rendered version of the EnglishWikipedia and the BookCorpus (Zhu et al., 2015).Thus, it is comparable to BERT in terms of pre-training data and VIT-MAE in terms of architectureand parameters. PIXEL follows the idea of visual text representa-tions by Salesky et al. (2021), who embed renderedtext using 2D convolutions for continuous open-vocabulary machine translation. They demonstratethat visual text representations are more robust tonoise and provide the benefits of a tokenization-free text processing pipeline. In other applications,Borenstein et al. (2023) examined the benefits of us-ing pixel-based encoders for an OCR-free approachto language modelling of historical documents. Lotz et al. (2023) further improved PIXEL by ex-perimenting with different text rendering strategies.Their work provides insights into the semantic mod-elling capabilities of PIXEL models and correlatesthat to frequency bias. We include some of thesemodels in our study.",
  "Model Interpretability": "The survey by Zhao et al. (2024) categorises modelinterpretability into local explanations of predic-tions and global explanations of model behaviour.Global explanations aim to understand the generalconcepts encoded in the individual components ofa language model. The most prominent method forglobal explanations of linguistic understanding inlanguage models is probing, specifically classifier-based probing (Belinkov, 2022).In this approach, model weights are frozen andfor each of its layers, a small classifier is trainedto solve a task given a pooled representation ofthe intermediate embeddings at that layer. Thetask is designed to isolate an aspect of linguisticunderstanding that may or may not be present inthe embedding (Adi et al., 2016; Hewitt and Man-ning, 2019; Sahin et al., 2020; Zhu et al., 2022).The same idea has been used for investigating com-puter vision models (Alain and Bengio, 2018; Basajet al., 2021) and, more recently, multi-modal mod-els (Dahlgren Lindstrm et al., 2020).A standard framework for linguistic probing isSentEval (Conneau and Kiela, 2018), which in-cludes various probing tasks that uncover differentlevels of linguistic information in sentence embed-dings. SentEval has been extensively employed to",
  ": Description of probing tasks used in this study": "analyse models for sentence-level semantics (Maet al., 2019; Krasnowska-Kieras and Wrblewska,2019; Ravichander et al., 2021), and it is the datasetwe adopt in this study.Linguistic probing has been used prominentlyin BERTology (Rogers et al., 2020) to understandthe levels of linguistic information stored in BERTembeddings (Tenney et al., 2019b; Jawahar et al.,2019; Mehrafarin et al., 2022). It has been estab-lished that BERT tends to encapsulate more syntac-tic knowledge in its middle layers, while semanticcomprehension is more pronounced in the higherlayers (Tenney et al., 2019a). In this context, weaim to gain analogous insights into pixel-based lan-guage models.",
  "Probing Tasks": "We now introduce the probing tasks used in ourexperiments. We probe PIXEL on two levels: lin-guistic and visual. For linguistic probing, we relyon the SentEval framework. ViTs have more directaccess to surface-level information than subword-based models since their input is segmented intounits of fixed visual size (as opposed to variable-sized tokens) and shown to the model after a con-tinuous linear projection (as opposed to a lookup).Thus, we also employ tasks that are designed toverify whether orthographic information is moreeasily identifiable throughout PIXEL.",
  "The SentEval framework contains probes that quan-tify three levels of linguistic knowledge present in": "sentence embeddings: surface, syntactic, and se-mantic (Conneau et al., 2018). presents allthe tasks, with their type and description. We eval-uate the performance of the models at each layeron these tasks to explain the hierarchy of linguisticunderstanding contained within the model. We note, however, that all tasks falling under thesemantic category do not all probe for the samekind of information. Tense, SubjNum and ObjNumcan be solved by trivial surface cues like the pres-ence of certain morphemes like the suffixes -ed and-es. However, unlike surface tasks, performance onthese tasks does not drastically degrade in the upperlayers as the model gains semantic understanding(Jawahar et al., 2019), and they can be predictorsof downstream semantic performance (Zhu et al.,2022). Thus, we dub these tasks surface semantic. SOMO and CoordInv, on the other hand, needmore complex semantic learning to be solved. Wetherefore term these tasks as complex semantic.The distinction between surface semantic and com-plex semantic can also be justified by the differ-ences in accuracy between human evaluation andmodel performance for these tasks as reported byConneau et al. (2018). Most neural models are ableto either match or surpass human evaluation forthe surface semantic tasks, but not for the complexsemantic tasks. This re-categorization also helps toidentify consistencies in linguistic understanding,particularly when explaining trends with BERT.",
  "Visual Probing Tasks": "We introduce two new tasks to probe for purelyvisual information MaxCount and ArgmaxCount(see ). Every word in every sentence ofthe SentLen task is replaced by a random Englishword generated with the wonderwords3 library tocreate synthetic datasets. By using random wordsinstead of a sentence, we ensure that the task ispurely visual, but does not disadvantage the BERTtokenizer (as opposed to using random characterswhich could result in single-character tokens). Thisalso distinguishes them from the surface tasks inSentEval since there is no underlying linguisticpattern to this data. The labels are binned to ensurea uniform distribution and we down-sample thelabels that occur with a very high frequency (forexample, e is the most frequent letter in 50% ofthe dataset). More task details are in Appendix A. MNISTAs a final task to probe for purely vi-sual information, we rely on MNIST (Deng, 2012),which consists of white-on-black images of hand-written digits (0 to 9). It is an image classificationbenchmark dataset and its resemblance to renderedtext as well as the simplicity of the task make itsuitable for probing.4 We do not evaluate BERT onthis task since it cannot represent images.",
  "Models": "Our analysis will primarily focus on the PIXEL-base model trained by Rust et al. (2023), furthertermed PIXEL. We also make a comparison withits variants introduced by Lotz et al. (2023) forRQ3. Specifically, we look at PIXEL-bigrams, pre-trained using the bigrams rendering strategy whichensures that every patch contains at most 2 charac-ters, and that no patch overlaps a word boundary,adding extra space where needed. We also look at PIXEL-small-words, trained on the words renderingstrategy that merely enforces the second constraint.Since it has no base version released, we addition-ally probe PIXEL-small-bigrams and PIXEL-smallfor a fair comparison.5 All these are comparedagainst BERT and VIT-MAE. An overview of themodel parameters is in Appendix B.",
  "Probing": "We follow the same probing setup as defined byConneau and Kiela (2018). Sentence representa-tions for each example in the datasets are obtainedby mean-pooling the token or patch embeddingsgenerated at every hidden layer for each model.These embeddings are passed to a classifier thatlearns to predict the corresponding class label usinga cross-entropy loss. For our experiments, we usethe implementation and default hyper-parametersproposed by Araujo et al. (2022) for both linguisticand visual tasks.",
  "Fine-tuning": "For a better understanding of the general linguis-tic abilities of vision models (RQ1), we fine-tuneVIT-MAE on universal dependencies (UD) (Nivreet al., 2016) POS-tagging, dependency parsing andGLUE (Wang et al., 2018) using the same hyperpa-rameters as Rust et al. (2023). We re-use PIXELstext rendering configuration, and render text into asquare image of 224 224 to match the input sizeof VIT-MAE. To gauge the general visual abilitiesof PIXEL (RQ2), we fine-tune PIXEL and VIT-MAEon the CIFAR100 (Krizhevsky and Hinton, 2009)image classification dataset.",
  ": Example of \"cool\" being rendered differently indifferent contexts for PIXEL. The red lines represent patchboundaries": "To investigate which of the two factors explainsthe advantage, we run the linguistic probing taskson PIXEL, BERT, and VIT-MAE, illustrated in Fig-ure 1. Each plot also includes the majority base-line6 for that task as a lower bound for each model.If the embeddings do not contain any useful infor-mation for the task, we would expect the perfor-mance to be equivalent to the majority baseline.The performance of BERT is consistent with whatis documented in literature. Surface features areencoded in the lower layers, syntactic features arerepresented in the middle layers, and semantic fea-tures are found in the upper layers (Jawahar et al.,2019). The performance for VIT-MAE for all lay-ers, for most tasks, is very close to the majoritybaseline. For tasks where some visual informationcan be useful, for example in SentLen, and Tense(the visual presence of morpheme -ed can be asso-ciated with label PAST), VIT-MAE performs betterthan the majority baseline but does not improveor decline through the layers. The performance of PIXEL, when higher than VIT-MAE, can thus beattributed to its linguistic knowledge and not due tohaving input that is closer to the downstream task.Across all tasks, PIXEL consistently has an initialmonotonic rise in accuracy, starting with a similarperformance as VIT-MAE in the lower layers. Thisindicates that it is using purely visual informationin the lower layers, and learns linguistic informa-tion in the higher layers. In other words, PIXEL",
  "The accuracy if always predicting the most frequent label": "starts as a visual model, and becomes more of alanguage model through the layers.However, PIXEL never matches the peak perfor-mance of BERT in any layer. This is consistent withthe results from Rust et al. (2023), where PIXELunderperforms BERT on both syntactic and seman-tic downstream English tasks. We can, therefore,hypothesize that much of PIXELs capacity is usedin recovering from the performance gap between avision and language model. Does PIXEL learn syntax and semantics?Un-like BERT, PIXEL does not have a consistent curveacross the surface, syntactic and semantic tasks.This is most striking in the surface tasks. For BERT,there is an inverse relation between model depthand accuracy. For SentLen, the accuracy curve of PIXEL rises until layer 5 and then stagnates. For WC,on the other hand, it has a steep rise in the initial lay-ers until layer 7, where it drops. The task requiresa good understanding of word-level features andboundaries something that is encoded in BERTalready at the input level, but PIXEL has to learnduring training. We illustrate this further in Fig-ure 2. The patches encoding the word \"cool\" differwhen used in the context of a sentence comparedto when it is rendered alone Thus, it may take morelayers for PIXEL to reconcile the two different em-beddings as the same word. Lotz et al. (2023) havealso commented on this phenomenon and linked itto poor downstream semantic performance. Theyalso found that PIXEL-based language models formbetter contextualised word representations in theupper layers of the model.We can extrapolate this phenomenon to explainthe initial monotonic rise in other tasks. For syntac-tic tasks, PIXEL peaks at layer 9, later than BERT,",
  ": Visual probing results for layers 1-12 of PIXEL,VIT-MAE and BERT": "then stagnates or declines. This leads to a delayedlearning of higher level abstractions, suggestingthat PIXEL needs more layers to match BERTs per-formance. We leave this exploration to future work.The performance across the surface semantictasks for PIXEL shows some consistency. Thereis a steep rise until layer 3, after which the curvehas a more gradual rise, crossing BERT accuracyin the higher layers. For complex semantic tasks,both PIXEL and BERT achieve peak performance be-tween layers 9 and 12. However, the performancegap between the two is substantial, indicating that PIXEL does not learn semantic abstractions at thesame level as BERT. This is also substantiated bythe difference in the downstream performance gapbetween BERT and PIXEL for syntactic and seman-tic tasks, mentioned in . PIXELs perfor-mance on dependency parsing and POS-taggingis very close to BERT, while its performance onGLUE, which contains tasks requiring more se-mantic understanding, is about 6% lower.The drop in performance for surface tasks inthe higher layers also indicates that PIXEL forgetssome surface level information as it learns morelinguistic abstractions. We substantiate this furtherwith the results on the visual probing tasks below.",
  "PIXEL have?": "We investigate this question by first probing PIXELon the visual tasks introduced in 3.2 to understandwhether it is indeed forgetting the surface levelinformation in the higher layers. Results are shownin .For both MaxCount and ArgmaxCount, we seethat VIT-MAE has the highest performance in thelower layers, followed by PIXEL and then BERT.",
  "much decline through the layers, leading to a higherperformance than VIT-MAE in the higher layers": "PIXEL has slight increases in performance in themiddle layers, analogous to the performance peaksin surface tasks. The substantially higher perfor-mance than BERT, combined with the similarity toVIT-MAE performance, indicates that PIXEL stillretains much surface level information in the higherlayers. PIXELs high performance on surface se-mantic tasks in the higher layers also substantiatethis since PIXEL has access to both surface andsemantic information. Can PIXEL be a vision model?If PIXEL stillretains much surface information in the higher lay-ers, is it able to perform well on vision tasks? Toinvestigate this question, we present fine-tuning re-sults for PIXEL and VIT-MAE in . If PIXELperforms competitively, it implies that PIXEL is fun-damentally a vision model that has acquired somelanguage understanding. We also fine-tune a trans-former of the same size with randomized weightsas a lower bound baseline.",
  "The performance gap between PIXEL and VIT-": "MAE on image classification is analogous to the per-formance gap between the two on the GLUE tasksin . Thus, even though PIXEL is a visiontransformer and it retains much surface level in-formation, its pre-training regime on language haslead to a substantially worse performance on imageclassification, much closer to the random baselinethan to VIT-MAE. It can be argued that PIXELspoorer performance on CIFAR-100 is due to a do-main mismatch, stemming from its pre-training onblack-and-white text, which offers limited exposureto the color and complexity of the input.To disentangle this, we probe PIXEL on MNISTat every layer. The results are in . Thecurves for PIXEL are consistent with the curves in and surface tasks in , in that thereis a performance decline through the layers. Thedifference is that PIXELs performance declinesimmediately after layer 1, and unlike , itis at a lower accuracy than VIT-MAE in the lower",
  "RQ3: Does adding orthographicconstraints to the input enhance thelinguistic capabilities in PIXEL?": "Results from 5.1 and 5.2 establish that PIXELlearns surface level information in the lower layers,which leads to delayed learning of higher level se-mantics. This raises the question of how the gap be-tween visual and linguistic understanding in layers1 - 6 (the layer with peak performance on surfacetasks) can be bridged earlier in the model. Encod-ing words with differing visual patch representa-tions, as shown in , can be made easier byensuring consistent rendering of words across con-texts. The added constraints to the rendering in the",
  "PIXEL-small-words have better downstream perfor-mance than PIXEL. Probing results on selectedtasks for PIXEL-small, PIXEL-small-words, and": "PIXEL-small-bigrams are in . We also in-clude BERT-base and VIT-MAE-base in the graphsfor reference.At the small scale, PIXEL suffers an almost catas-trophic decline in performance, showing no morelinguistic understanding than VIT-MAE. Similarly, PIXEL-small-bigrams also does not demonstrateany meaningful linguistic understanding. PIXEL-small-words, on the other hand, displays probingperformance comparable to PIXEL-base, even atthe small scale. It starts with much higher accuracythan VIT-MAE in layer 1 indicating that thereis already linguistic information present in the ini-",
  ": Selected linguistic probing results for layers 1-12 ofsmall PIXEL variants. Base models are indicated with dottedlines": "tial layers due to the imposed structure at the inputlevel. It also achieves peak performance in mosttasks earlier than PIXEL-base7.Specifically, for WC, the accuracy rises only untillayer 4 before it declines. The curves for syntactictasks are more similar to BERT, with the lower lay-ers achieving scores higher than PIXEL. A combi-nation of visual and some semantic understandingleads to scores for surface semantic tasks beingeven higher than BERT in the upper layers. Forcomplex semantic tasks, however, the curve risesuntil layers 7-8 and then plateaus, indicating highersemantic abstractions are still not being learnt bythe model.Since PIXEL-small-bigrams and PIXEL-small donot have any meaningful linguistic representationsat the small scale, we also compare the base versionof the two models on the linguistic probes to findsimilar trends. PIXEL-bigrams at both the base andsmall scale performs worse than PIXEL. Specific 7We speculate that PIXEL-small-words outperforms PIXEL-small-bigrams, even though both prevent patch overlap acrossword boundaries, because the extra space added by bigramsrendering to maintain two characters per patch leads to a lossof word boundary information and longer sequences. A deeperexploration is left for future work.",
  "results and analysis can be found in Appendix C": "Why is fine-tuned PIXEL-bigrams better thanfine-tuned PIXEL?The observation above is atodds with the downstream performance of PIXEL-bigrams, which Lotz et al. (2023) found to be betterthan PIXEL. To understand this discrepancy, we runthe linguistic probes on fine-tuned versions of themodels. We fine-tuned the PIXEL-base-bigramsmodel on UD parsing (syntactic) and MNLI (se-mantic) with the same hyper-parameter setup as",
  "PIXEL, and compare them to the fine-tuned PIXELmodels made available by Rust et al. (2023) on thesame tasks. Results are in and .We see that across all probing tasks, fine-tuned": "PIXEL-bigrams demonstrates better performancethan fine-tuned PIXEL.Merchant et al. (2020)found that finetuning BERT on dependency pars-ing shows effects throughout the model, but MNLIonly affects the top layers. Moreover, fine-tuningcan cause the model to potentially forget some lin-guistic knowledge. Mehrafarin et al. (2022) alsoechoed that fine-tuning on tasks with larger datasizes (like MNLI) can lead to loss of linguisticinformation in the pre-trained encodings.We see this trend in PIXEL, where both UD andMNLI fine-tuning decrease probing performance",
  ": Selected probing results for layers 1-12 of PIXELand PIXEL-bigrams finetuned on MNLI": "on BShift and SOMO. There is a slight decline inperformance on all other probing tasks with UDfine-tuning, but with MNLI fine-tuning, the perfor-mance remains similar to pre-trained PIXEL.We observe the contrary with PIXEL-bigrams.Both UD and MNLI fine-tuning have enhanced thelinguistic knowledge encoded in all the layers, withprobing performance compared to PIXEL-bigramspre-trained being much higher. Additionally, UDfine-tuning particularly increases probing perfor-mance on syntactic tasks in the top layers, andMNLI fine-tuning increases probing performanceon the complex semantic tasks in the top layers.Thus, we can speculate that the inductive biaslearnt during fine-tuning creates better linguisticrepresentations in PIXEL-bigrams.",
  "On the spectrum of vision and language, it can beconcluded from the results of RQ1 and RQ2 that": "PIXEL is more of a language model than a visionmodel. The difference in downstream performancebetween PIXEL and VIT-MAE is much larger thanbetween PIXEL and BERT. Although with a loweraccuracy, PIXELs behaviour revealed by probingis more similar to BERT than VIT-MAE. However, much of PIXELs linguistic knowledgeis surface level. The lower layers in PIXEL learnsurface level information, as demonstrated by thevisual and linguistic probes. The linguistic knowl-edge acquired in the upper layers demonstratessome syntactic capabilities, but does not capturevery strong semantic information. This indicatesthat adding more layers to the model could allow itto have better semantic representations. Other ar-chitectural solutions such as a RoBERTa-like (Liuet al., 2019b) approach to PIXEL pretraining withlonger training and a revisiting of the masking andreconstruction method could also be explored.While these are solutions on the architecture side,on the input side from the results of RQ3 we canconclude that experimenting with the renderingstrategies can be very promising.PIXEL-wordsemerges as the current best solution to bridging thegap between visual and language understandingin the lower layers in the model. Nevertheless, itstill lacks in semantic understanding, and as Lotzet al. (2023) have noted it is not very efficient totrain. PIXEL-bigrams has worse linguistic probingperformance than PIXEL, but fine-tuning dramati-cally improves the linguistic knowledge encodedin its layers which could be due to the inductivebias it learns in the process. PIXEL, on the con-trary, forgets some linguistic information duringfine-tuning.One can argue that the input patches for PIXEL-words and PIXEL-bigrams most closely resemblethe input of a subword-based model, with tok-enized units where a patch/token never crossesword boundaries. Even though the model doesnot have a fixed vocabulary, structured renderingrequires some level of pre-tokenization and aware-ness of linguistic granularity. This raises ques-tions about whether these structured rendering ap-proaches might lead to the same issues and de-bates that surround traditional subword tokeniza-tion when it comes to heuristics used for segmenta-tion (Clark et al., 2022b), particularly when appliedto languages without conventional word bound-aries. Lotz et al. (2023) have also noted that whilestructured rendering strategies can give PIXEL anadvantage, they can also make it difficult to gen-eralise to other languages. Thus, development of PIXEL along these lines must be informed by care-ful consideration of linguistic diversity and the po-tential limitations posed by structured rendering,ensuring that solutions are adaptable to languageswith varied morphosyntactic structures.",
  "Conclusion": "This study is a first step towards understandingthe language modelling capabilities of pixel-basedmodels. Although these models exhibit substantiallinguistic understanding, the nature of image-textrepresentations leads to a gap in visual and lin-guistic understanding. Pixel-based models needto learn the discrete representations that subword-based models already have access to at the inputlevel. Adding orthographic constraints to the inputcan help bridge this gap, but further architecturalmodifications could improve these models more,which is a promising direction for future work.",
  "Limitations": "Our main approach to understanding the linguisticinformation encoded in pixel-based language mod-els is probing. We acknowledge that although thisis our primary method of inquiry, it comes with itsflaws. Belinkov and Glass (2019) have noted thateven though certain information is detected by aprobe as being present in the embeddings, it doesnot necessarily imply that the information is usedby the model. They also remark that using a deeperauxiliary classifier for the probe may lead to betterresults. There are other criticisms of the approachlike Hewitt and Liang (2019) that question whetherthe probe uncovers information encoded in the em-bedding, or just learns the linguistic task itself thatit is trained on. Pimentel et al. (2020) challengethis and present evidence of the former. Zhu andRudzicz (2020) recommend using a control mecha-nism to select probes, based on discussions aboutthe dichotomy raised above. Thus, although thisdoes not dismiss the validity of our findings, wenote that our results and conclusions should be readwith these caveats in mind.",
  "Acknowledgements": "The computational resources and services used inthis work were provided by the VSC (Flemish Su-percomputer Center), funded by the Research Foun-dation - Flanders (FWO) and the Flemish Govern-ment - department EWI (for KT, TB and ML). TBis funded by a Bijzonder Onderzoeksfonds (BOF)internal fund at KU Leuven, namely the C1 projectfund with reference C14/23/096.",
  "Guillaume Alain and Yoshua Bengio. 2018. Under-standing intermediate layers using linear classifierprobes. Preprint, arXiv:1610.01644": "Vladimir Araujo, Andrs Carvallo, Souvik Kundu, JosCaete, Marcelo Mendoza, Robert E. Mercer, FelipeBravo-Marquez, Marie-Francine Moens, and AlvaroSoto. 2022. Evaluation benchmarks for Spanish sen-tence representations. In Proceedings of the Thir-teenth Language Resources and Evaluation Confer-ence, pages 60246034, Marseille, France. EuropeanLanguage Resources Association. Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki,Micha Grszczak, Barbara Rychalska, TomaszTrzcinski, and Bartosz Zielinski. 2021.Explain-ing self-supervised image representations with vi-sual probing. In Proceedings of the Thirtieth Inter-national Joint Conference on Artificial Intelligence,IJCAI-21, pages 592598. International Joint Confer-ences on Artificial Intelligence Organization. MainTrack.",
  "Yonatan Belinkov and James Glass. 2019. Analysismethods in neural language processing: A survey.Transactions of the Association for ComputationalLinguistics, 7:4972": "Nadav Borenstein, Phillip Rust, Desmond Elliott, and Is-abelle Augenstein. 2023. PHD: Pixel-based languagemodeling of historical documents. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 87107, Singapore.Association for Computational Linguistics. Jonathan H. Clark, Dan Garrette, Iulia Turc, and JohnWieting. 2022a. Canine: Pre-training an EfficientTokenization-Free Encoder for Language Represen-tation. Transactions of the Association for Computa-tional Linguistics, 10:7391. Jonathan H. Clark, Dan Garrette, Iulia Turc, and JohnWieting. 2022b. Canine: Pre-training an EfficientTokenization-Free Encoder for Language Represen-tation. Transactions of the Association for Computa-tional Linguistics, 10:7391. Place: Cambridge, MAPublisher: MIT Press. Alexis Conneau and Douwe Kiela. 2018. SentEval: Anevaluation toolkit for universal sentence representa-tions. In Proceedings of the Eleventh InternationalConference on Language Resources and Evaluation(LREC 2018), Miyazaki, Japan. European LanguageResources Association (ELRA). Alexis Conneau, German Kruszewski, Guillaume Lam-ple, Loc Barrault, and Marco Baroni. 2018. Whatyou can cram into a single $&!#* vector: Probingsentence embeddings for linguistic properties. InProceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 21262136, Melbourne, Aus-tralia. Association for Computational Linguistics. Adam Dahlgren Lindstrm, Johanna Bjrklund, SunaBensch, and Frank Drewes. 2020.Probing mul-timodal embeddings for linguistic properties: thevisual-semantic case. In Proceedings of the 28thInternational Conference on Computational Linguis-tics, pages 730744, Barcelona, Spain (Online). In-ternational Committee on Computational Linguistics.",
  "Li Deng. 2012. The mnist database of handwritten digitimages for machine learning research [best of theweb]. IEEE Signal Processing Magazine, 29(6):141142": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. In International Conference onLearning Representations. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,Piotr Dollr, and Ross Girshick. 2022. Masked au-toencoders are scalable vision learners.In 2022IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 1597915988. John Hewitt and Percy Liang. 2019. Designing and in-terpreting probes with control tasks. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 27332743, Hong Kong,China. Association for Computational Linguistics. John Hewitt and Christopher D. Manning. 2019. Astructural probe for finding syntax in word represen-tations. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41294138, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "A. Krizhevsky and G. Hinton. 2009. Learning multiplelayers of features from tiny images. Masters the-sis, Department of Computer Science, University ofToronto": "Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Na-man Goyal, Marjan Ghazvininejad, Luke Zettle-moyer, and Madian Khabsa. 2023. XLM-V: Over-coming the vocabulary bottleneck in multilingualmasked language models. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1314213152, Singapore.Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019a.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019b.RoBERTa: A Robustly Optimized BERT Pretrain-ing Approach. arXiv preprint. ArXiv:1907.11692[cs]. Jonas Lotz, Elizabeth Salesky, Phillip Rust, andDesmond Elliott. 2023. Text rendering strategies forpixel language models. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1015510172, Singapore.Association for Computational Linguistics.",
  "Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nalla-pati, and Bing Xiang. 2019. Universal Text Repre-sentation from BERT: An Empirical Study. arXivpreprint. ArXiv:1910.07973 [cs]": "Houman Mehrafarin,Sara Rajaee,and Moham-mad Taher Pilehvar. 2022. On the importance of datasize in probing fine-tuned models. In Findings ofthe Association for Computational Linguistics: ACL2022, pages 228238, Dublin, Ireland. Associationfor Computational Linguistics. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, andIan Tenney. 2020. What happens to BERT embed-dings during fine-tuning?In Proceedings of theThird BlackboxNLP Workshop on Analyzing and In-terpreting Neural Networks for NLP, pages 3344,Online. Association for Computational Linguistics. Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.2016. Universal Dependencies v1: A multilingualtreebank collection. In Proceedings of the Tenth In-ternational Conference on Language Resources andEvaluation (LREC16), pages 16591666, Portoro,Slovenia. European Language Resources Association(ELRA). Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,Ran Zmigrod, Adina Williams, and Ryan Cotterell.2020. Information-theoretic probing for linguisticstructure. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 46094622, Online. Association for Computa-tional Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Abhilasha Ravichander, Yonatan Belinkov, and EduardHovy. 2021. Probing the Probing Paradigm: DoesProbing Accuracy Entail Task Relevance? In Pro-ceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 33633377, Online.Association for Computational Linguistics.",
  "Elizabeth Salesky, David Etter, and Matt Post. 2021": "Robust open-vocabulary translation from visual textrepresentations. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 72357252, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural Machine Translation of Rare Wordswith Subword Units. In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computa-tional Linguistics. Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,Hyung Won Chung, Dara Bahri, Zhen Qin, SimonBaumgartner, Cong Yu, and Donald Metzler. 2022.Charformer: Fast character transformers via gradient-based subword tokenization. In International Con-ference on Learning Representations.",
  "Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019a": "BERT rediscovers the classical NLP pipeline. InProceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 45934601, Florence, Italy. Association for ComputationalLinguistics. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-jamin Van Durme, Sam Bowman, Dipanjan Das, andEllie Pavlick. 2019b. What do you learn from con-text? probing for sentence structure in contextualizedword representations. In International Conferenceon Learning Representations. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel Bowman. 2018. GLUE:A multi-task benchmark and analysis platform for nat-ural language understanding. In Proceedings of the2018 EMNLP Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP, pages353355, Brussels, Belgium. Association for Com-putational Linguistics. Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts,and Colin Raffel. 2022. ByT5: Towards a token-freefuture with pre-trained byte-to-byte models. Transac-tions of the Association for Computational Linguis-tics, 10:291306. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, DaweiYin, and Mengnan Du. 2024. Explainability for largelanguage models: A survey. ACM Trans. Intell. Syst.Technol., 15(2). Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-dinov, Raquel Urtasun, Antonio Torralba, and SanjaFidler. 2015. Aligning books and movies: Towardsstory-like visual explanations by watching moviesand reading books. In Proceedings of the IEEE Inter-national Conference on Computer Vision (ICCV). Zining Zhu and Frank Rudzicz. 2020.An informa-tion theoretic view on selecting linguistic probes. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 92519262, Online. Association for Computa-tional Linguistics. Zining Zhu, Soroosh Shahtalebi, and Frank Rudzicz.2022. Predicting fine-tuning performance with prob-ing. In Proceedings of the 2022 Conference on Em-pirical Methods in Natural Language Processing,pages 1153411547, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.",
  "AVisual Tasks": "Max Count Character (MaxCount)Every let-ter from the random words in an example iscounted. Per example, for the raw counts f foreach letter, we compute max f and split the re-sults into 4 uniformly occurring contiguous bins.The task is to predict this bin given the sentence.Examples where multiple letters have the samemaximal count are excluded to ensure that the prob-ing task can only be solved by noticing one partic-ular character. We exclude examples with less than3 unique characters. Details about labels and fre-quency of each bin are in",
  ": Bin sizes, labels in bin and total data size forMaxCount. The labels correspond to the count of the char-acter with the maximum frequency in an example": "Argmax count character (ArgmaxCount)Wecount the letters in each example again, but now thetask is to predict = arg max f given the ex-ample. The same examples are excluded as above(meaning the argmax is unique), and we skip exam-ples where the argmax is not one of the 26 lower-case Latin letters {a, b, . . . , z}. To mitigate againstthe strong skew towards higher-frequency letters (e,t, a, ...), letters are grouped into an approximationof a uniform distribution of 5 bins (without contigu-ity constraint) after which the bins are subsampledto have the same amount of sentences as the small-est bin. Details about labels and frequency of eachbin are in .",
  "Select probing results for PIXEL and PIXEL-bigrams base models are in . As observed,": "PIXEL-bigrams performs worse than PIXEL acrossall probing tasks. We theorize that even thoughbigrams rendering imposes some structure on theinput text, it results in a loss of word boundaryinformation and longer sequences. The renderingstrategy adds extra space even within words to en-sure that one patch has only two characters, andcreates more ambiguity about the structure of theword. This is most prominently seen in the tasksthat test for word level information within a sen-tence - namely, WC, BShift and SOMO. For the later2, PIXEL-bigrams barely outperforms the majoritybaseline."
}