{
  "Abstract": "Numerous recent works target to extend effec-tive context length for language models andvarious methods, tasks and benchmarks exist tomeasure models effective memorization length.However, through thorough investigations, wefind limitations for currently existing evalua-tions on models memorization capability. Weprovide an extensive survey for limitations inthis work and propose a new method calledforgetting curve to measure the memorizationcapability of long-context models. We showthat forgetting curve has the advantage of beingrobust to the tested corpus and the experimentalsettings, of not relying on prompts and can beapplied to any model size. We apply our forgetting curve to a large va-riety of models involving both transformerand RNN/SSM based architectures.Ourmeasurement provides empirical evidence forthe effectiveness of transformer extensiontechniques while raises questions for theeffective length of RNN/SSM based mod-els. We also examine the difference betweenour measurement and existing benchmarksas well as popular metrics for various mod-els.Our code and results can be found at",
  "Introduction": "Arguably driven by large language model (LLM)practical needs, numerous works today aim to ex-tend LLM contexts. The approaches range frommodifying existing transformer architectures (Daiet al., 2019; Wu et al., 2022; Munkhdalai et al.,2024), training existing LLMs to extend their con-texts (Chen et al., 2023b; Xiong et al., 2023; Chenet al., 2023a), to more recent modelling basedon fundamental architectural changes using RNNsor hybrid architectures (Gu and Dao, 2023; Penget al., 2023; Lieber et al., 2024), etc. All of the",
  "*These authors contributed equally to this work.Corresponding author": ": The forgetting curve of Llama-2-base-32k (To-gether.AI, 2024). The x-axis denotes the prefix length.Green, blue, and red areas respectively indicate fine-grained memory where the model achieves 99% tokenreplication accuracy (except for very short sequences),coarse-grained memory where copy accuracy surpassesLM accuracy, and the amnesia area where the modelcompletely ignores the prefix. approaches claim to significantly extend the con-text window of corresponding baseline models withempirical validations. However, to authors bestknowledge, the community has not established (andthus has not agreed on) a standard metric that canreflect models inherent memory with respect tothe long context, in other words, models effectivememory length.The most commonly used metric for evaluat-ing a models natural language generation abilityis Perplexity (PPL). It is generally assumed thata superior long-range perplexity suggests a moreeffective utilization of long context. However, re-cent findings indicate that a lower perplexity forlong context does not necessarily lead to improvedperformance in downstream long context applica-tions (Hu et al., 2024b). This raises questions aboutthe validity of using perplexity as an indicator of amodels long-context memorization capability.1 Al-ternative metrics have been proposed to evaluate amodels long-term memory, including Needle in a",
  "The same paper suggests that perplexity effectively indi-cates model performance on short rather than long sequences": "Haystack (gkamradt, 2023) and its derived metrics(Song et al., 2024). However, these metrics alsopresent their own limitations. For instance, Nee-dle in a Haystack is highly sensitive to the promptused,2 and such sensitivity is attributed more to thevarying instruction-following capabilities acrossdifferent models, rather than differences in theirmemory capabilities. Other metrics (Tay et al.,2020; Bulatov et al., 2022, 2023), which involve toytasks, face challenges in generalizing to real-worldsettings and are susceptible to severe overfitting.We survey a significant number of popular metricsand benchmarks which measure model long-termmemory in this work.To address these limitations and provide a reli-able measure of a models long-term memory, weintroduce a new method called forgetting curve inthis paper. Our method also begins with a languagemodel trained on a real-world corpus. We thenexploit the models emerging copy capability, as-sessing whether the trained model can replicate itsprefix as part of the completion process. In orderto account for instances where the token predic-tion relies solely on the models natural languagemodelling (LM) capabilities instead of long-termmemorization, we additionally plot the LM accu-racy curve. illustrates our method, demon-strating the progression of a memory pattern fromfine-grained to coarse-grained memory, and ulti-mately to complete memory absence.The paper is organized as the following. We startwith a critical evaluation of the limitations inher-ent in the metrics and tasks currently employed tomeasure a models long-term memorization capa-bility (). Following this, we detail theconstruction of the forgetting curve, highlightingits dual function in visualizing memory and en-abling statistical calculation of memory length. Wealso establish the reliability of our method in assess-ing a models memorization capability ().Using our forgetting curve, we comprehensivelyexamine the long-range memorization capabilitiesof various current models and architectures (Sec-tion 4). We demonstrate that the forgetting curveseffectively illustrate a models memorization capa-bility through visualizations, revealing varying pat-terns across different architectures and models. Wealso validate certain context extension techniques,and raise questions about some architectural claims.",
  "Limitations of Long-Range MemoryMeasures": "Limited Memory Usage (LMU)The first limita-tion that we identify in the long-range measurementliterature is limited memory usage (LMU), wherethe measurement or metric does not have a strongrelationship with models capability to leveragelong context. Perplexity is a surprising exampleof LMU. Recent works (Chen et al., 2023b; Yang,2023; Xiao et al., 2023) typically demonstrate theirmodels long-range capability by showing a sta-bly low perplexity given long contexts. However,Hu et al. (2024b) recently found that lower per-plexity does not imply an improved downstreamperformance on long-context tasks. In section 5,we further demonstrate, using forgetting curves,that some models can improve long-range perplex-ity without enhancing long-range memory. LMUalso manifests in popular long-range measurementtasks such as Needle in a Haystack(gkamradt,2023) which assesses the capability to retrieve aspecific piece of information from lengthy distract-ing texts. The memory usage is clearly limitedsince the model can theoretically achieve great per-formance by focusing only on the needle and ig-noring all the long context. Methods derived fromNeedle in a Haystack like Counting Stars (Songet al., 2024) alleviate this drawback but still inheritsuch limitations. Prompt Required (PR)The second limitationin existing evaluation methods is their strong de-pendency on the exact prompts during evaluation.For instance, in the Claude 2.1 experiments (An-thropic, 2023), a single prompt change dramaticallyimproved the models ability to find the needle,boosting Claude 2.1s task success rate from 27%to 98% in the original evaluation. Such signifi-cant dependency on prompts raises the followingtwo issues. Firstly, it makes the comparisons be-tween models extremely difficult due to the largeperformance variance across different prompts.3",
  "Treating all prompts as i.i.d experiments allows for theo-retical comparison of two models using statistical tests. How-ever, large variance makes achieving statistical significance": "Secondly, it is challenging to apply these metricsto unaligned models and smaller ones due to theirlimited capability to follow instructions to find theneedle. This implies that new architectures needto be scaled up and aligned before these metricscan be applied, preventing rapid and low-cost ex-perimental validation of a models long-contextcapability. Although latest benchmarks such asRuler (Hsieh et al., 2024) and StreamEval (Xiaoet al., 2023) can force a model to produce answersthrough carefully designed prompts, this approachis still cumbersome and model-specific. Inference Factors (IF)The third limitation weidentify is that current long-context evaluations of-ten fail to distinguish the influence of inherent infer-ence factors, such as natural language understand-ing ability, etc. For instance, in the Needle in aHaystack task where the model is prompted tolocate the needle, a failure can be difficult to diag-nose, as it could be due to a deficiency in either themodels long-context memorization capability orits natural language understanding capability. Sim-ilarly, failure in tasks such as long context questionanswering or long mathematical expression calcu-lation may not reflect shortcomings in the modelslong-term memory, but rather, deficiencies in itsspecific abilities. Limited Context (LC)The fourth limitationarises from the limited context inherent in bench-marks. Some benchmarks (Li et al., 2024; Zhanget al., 2024; Li et al., 2023b) conduct their testswithin a confined context, which restricts theirability to evaluate model performance in longercontexts. Expanding these benchmarks to includeextended contexts is typically laborious and time-intensive, posing a challenge to keep pace with therapid increase in context length that large languagemodels can handle. Training on Toy Tasks (ToTT)Finally, we no-tice that many recent studies demonstrate a modelslong-range capability by training on toy tasks (Tayet al., 2020; Gu and Dao, 2023; Bulatov et al.,2023). While we recognize its contributions, wealso argue that because long-context memory is acapability that matters in real applications such aslong document summarization (Fan et al., 2019),role playing (Li et al., 2023a; Wang et al., 2023)etc., the tests should be performed on real data. Inother words, we argue that long-context capability",
  "Passkeys (Munkhdalai et al., 2024)ToTT, LMU, PR, IFSelective Copy (Gu and Dao, 2023)ToTT, LMUCopy (Bulatov et al., 2022)ToTTLRA (Tay et al., 2020)ToTT, LC": "BABILong (Kuratov et al., 2024)LMU, PR, IFLongICLBench (Li et al., 2024)PR, IFRuler (Hsieh et al., 2024)LMU, PR, IFLongBench (Bai et al., 2023)LC, PR, IFInfiniteBench (Zhang et al., 2024)LC, LMU, PR, IFLooGLE (Li et al., 2023b)LC, PR, IFZeroSCROLLS (Shaham et al., 2023)LC, PR, IFL-Eval (An et al., 2023)LC, PR, IFBAMBOO (Dong et al., 2023)LC, PR, IFLV-Eval (Yuan et al., 2024)LC, LMU, PR, IFCLongEval (Qiu et al., 2024)LC, LMU, PR, IF",
  ": Overview of long-context measurements andlimitations according to the categorization in": "should be an emerging capability to be measured,which does not involve any specific training, espe-cially not any training on toy datasets. Ideally, themeasurement should also be on a real dataset to mit-igate issues like overfitting and data leakage (Wuet al., 2023; Balloccu et al., 2024). lists popular methods for assessing amodels long-context performance along with theirlimitations. It is important to note that these limi-tations are often interrelated. Ideally, assessmentmethods should overcome all the listed limitationsto provide an accurate measure of a models perfor-mance.",
  "Evaluation Methodology": "The forgetting curve is derived from LLM copytasks and consists of two curves (i.e copy accuracycurve and LM accuracy curve) as shown in Fig-ure 1. For both curves, teacher forcing is appliedto measure the next token prediction accuracy. Werefer to teacher forcing the setting widely used tomeasure language model perplexity where the nexttoken prediction probability is taken into account : The forgetting curve task measures the LLM prediction accuracy for the target sequence This is a toy taskfor testing memory under two settings. The above figure illustrates the copy setting, while the below one shows thelanguage modelling setting. We calculate the difference between these two settings to obtain the forgetting curvereflecting models memory behaviour. As shown in the figure, only the later half of the tokens are taken into accountto construct the forgetting curve. by always assuming a correct prefix derived froma corpus. We only differ from perplexity measure-ment in that we measure token prediction accuracy(i.e. 0 or 1 for each token) instead of log probabil-ity scores for both copy accuracy curve and LMaccuracy curve. We further detail the measurementof the two curves below.",
  "Evaluation Tasks": "Copy Accuracy Curve.The first curve, depictedas the yellow curve in , is the copy curve.This curve assesses an LLM based on its copyaccuracy. We calculate this accuracy by taskingthe model to predict the second half of a copiedstring using its first half, as illustrated at the topof . One key difference from the standardcopy task is our requirement for the model to benot trained on such tasks, thus treating copy as anemergent capability of the LLM. Such constraintsavoid trivial solutions which overfit to the copytasks, as pointed out by Gu and Dao (2023). Sec-ondly, instead of using natural language promptsto trigger the LLMs copy behavior, we employteacher forcing and measure the resulting predictedcopy accuracy. The initial tokens are disregardedfrom the measurement as they are mainly used asfew-shot learning examples to inform the modelof the copy task. 4 In our experiments, all curvemeasurements begin with the copy performance forthe second half of the target sequence. 4Remark that such way of indicating copy task (by show-ing the model the copied tokens) effectively avoids relying onmodels understanding capability, making our method applica-ble for unaligned/smaller models as we show in . LM Accuracy Curve.In the forgetting curve, wechoose real-life natural language texts to avoid triv-ial or overfitting solutions to the copy task. How-ever, this choice also implies a non-negligible prob-ability of correctly predicting the token even with-out access to any long-range contexts.5 To takethis probability into account, the forgetting curvealso plots the language modelling accuracy, repre-sented by the blue curve in . Specifically,an irrelevant but real text of the same length as thecopy target is chosen.6 We measure the languagemodels accuracy in predicting the later half of thetarget sequence, using the irrelevant prefix and thefirst half of the sequence, as depicted in .",
  "An Illustrative Example": "We run Llama-2-base-32k model (Together.AI,2024) using test text from PG-19 (Rae et al., 2019)to produce a forgetting curve (). In theseexperiments, we compute an accuracy score every1K tokens for both the copy curve and the languagemodel curve. Each accuracy is calculated 10 times,allowing us to plot the mean accuracy and the vari-ance in the curve.We visualize the corresponding forgetting curvein . From the first accuracy data point(1K), we observe that Llama-2-base-32k performsthe copy task perfectly without explicit instruc- 5One might notice that this is the same issue we highlightedin regarding perplexity.6In .4, we demonstrate that the distribution usedto sample the random string does not impact the languagemodel accuracy.",
  "en_trainPG19-trainID(En)en_testPG19-testID(En)zhLooksJuicy/ruozhibaID(Zh)randomRandomOOD": ": We extract both irrelevant and target copyingtext from various sources. All texts from PG-19 andRuozhiba, which includes English and generated Chi-nese responses by GPT-4 respectively, are natural lan-guage and hence considered in-distribution. In contrast,a random token sequence, not adhering to any naturallanguage distribution, is deemed out-of-distribution. tion, a phase lasting until 4K.7 We refer to thisphase as fine-grained memory, as the model canperfectly memorize past tokens. After this phase,the model can no longer precisely remember allthe previous context, as shown by the copy accu-racy curve. However, the model still exhibits sig-nificantly higher copy accuracy. We refer to thisphase as coarse-grained memory.8 For Llama-2-base-32k, we notice that during the coarse-grainedmemory phase, the copy performance graduallydegrades to the level of language modeling (LM)accuracy.9 Finally, we observe no difference in themodels performance based on whether the targetsequence has been previously seen by the model.We refer to this as the model entering the amnesiaphase.",
  "In-depth Analyses on Forgetting Curves": "In this subsection, we critically assess the relia-bility of the forgetting curve as an indicator of amodels memorization capacity. Our evaluationfocuses on two primary aspects: the effect ofusing various irrelevant prefixes when measuringlanguage modeling accuracy, and the impact oftesting on different text corpora. Moreover, we in-vestigate whether the forgetting curve mitigates thelimitations highlighted in . 7We remark that the copy accuracy is not 100% for the firstmeasurement. The phenomenon is universal (see Appendix Cfor the forgetting curves of other models we measure) and webelieve it is because at this stage, with relatively few examplesdemonstrating the copy behaviour, the model does not learnto copy perfectly yet.8In our setting, fine-grained memory length is where themodel can accurately copy more than 99% tokens, whilecoarse-grained memory length is where copy accuracy ex-ceeds language model accuracy by more than 1%.9Note that in ideal case, one would expect coarse-grainedmemory to last much longer than models claimed contextlength. LM Accuracy with Various Irrelevant PrefixesTo examine the effect of various irrelevant textson the language model accuracy, we experimentwith varying irrelevant text prefixes, ranging fromin-distribution (ID) to out-of-distribution (OOD)relative to the Llama-2-base model. We sourcethese irrelevant texts from four distinct text corpora,as detailed in , with the PG-19 test textsserving as the copy target in all experiments.The results are illustrated in a. Exceptfor the random prefix setting, all other settings yieldsimilar language modeling accuracy, showing a sig-nificant downturn around half the claimed contextlength. These trends remain statistically indistin-guishable up to the memorization threshold, as con-firmed by our statistical analysis in Appendix A.Importantly, these observations suggest that themodels coarse-grained memory length is consis-tent across different prefix settings, demonstratingthe robustness of the forgetting curve as a measureof memorization. Forgetting Curves with Various Copy TargetsWe then assess the impact of copy text on the for-getting curve, using different copy targets sourcedaccording to . The forgetting curves, de-picted in b, show uniform trends acrossvarious settings, except when random targets areused. This uniformity in forgetting curves under-scores a consistent coarse-grained memory length,highlighting the measures reliability across differ-ent textual contexts.Our results confirm that the forgetting curve isa reliable measure of memorization, even whenwe change the irrelevant text at the beginning orthe texts used in the copying task. This consistencyholds as long as the texts come from well-organizedsources and not just random samples. Essentially,the forgetting curve proves to be more dependablethan expected, consistently showing similar pat-terns and lengths of memorization across differenttests. Analysis of ReliabilityOur approach effectivelyaddresses the limitations aforementioned in . By requiring the language model to replicate allseen content in the copy task, we overcome thelimitation of LMU. The inherent copy capabilitiesof language models, leveraged in our methodology,allow for the completion of the copy task usingthe copy text itself as a contextual cue, therebycircumventing the need for task-specific trainingToTT or specialized prompts PR. Furthermore, the",
  "(b)": ": Llama-2-7b forgetting curve with various textsources. (a) Various irrelevant text sources, and copytext is sourced from pg19 test set. (b) Various copy textsources, and irrelevant text is sourced from pg19 testset. ability to construct a forgetting curve of arbitrarylength eliminates the LC limitation. Finally, bycomparing the language modeling accuracy andexcluding prompt words from our evaluations, weminimizes the impact of inherent inference factorsIF.",
  "Experiments": "Experimental SettingsWe experiment with intotal 14 open-source long-context LLMs and tracetheir forgetting curves to examine their memoriza-tion capability; the detailed information about theexperimented models is included in Appendix B.The forgetting curve does not require natural lan-guage understanding capability and can be appliedto any language models, allowing us to cover di-verse model type (aligned or not aligned), modelarchitecture (transformers or RNNs), and claimedcontext lengths (32k to 1M). For all models, wepull the model from their huggingface releases andrun the inference in BFloat16 on 8 NVIDIA A100GPUs. Both the copy accuracy curve and the LMaccuracy curve are based on token prediction ac-curacy which is derived from the teacher forcingsetting explained in subsection 3.1.We utilize the PG19 test set (Rae et al., 2019) containing a collection of 100 books. Each bookis individually tokenized based on the tokenizersettings of the model under test. To measure con-text of arbitrary length, we concatenate all booksinto an extremely long sequence. Subsequently,we divide the maximum supported length l, asclaimed by the model, into n test lengths wheren is a fixed granularity (i.e we test model on length",
  "( ln, 2l": "n , ..., l) respectively).10 For each test length,we randomly select a continuous sequence fromthe concatenated text as a copy target while theremaining part is used to sample the irrelevant textto measure LM accuracy curve. As shown in Fig-ure 2, the copy task is presented to LM modelin the form of [bos]S[bos]S[eos] where S is thetarget sequence to copy. For the language mod-eling task, LM accuracy is computed based on[bos]I[bos]S[eos] where I represents the irrelevanttext. For each accuracy datapoint of certain contextlength, we sample 10 different copy target so thatthe curve contains mean and variance. Remarks on Closed Source ModelsThe forget-ting curve can be applied to closed source modelsto measure their long context memorization capa-bility. However, as the parameters of commercialmodels are not accessible, simulating the teacherforcing process requires feeding the prefix to themodel and calculating the prediction accuracy foreach token, which is too costly and inefficient toperform. Results shows our main results, listingthe key statistics derived from forgetting curves ofeach model. The exact forgetting curves can befound in figures from Appendix C. The models aregrouped into three families: Llama models withoutcontext extensions, Transformer based models withcontext extensions and SSM/RNN models.We observe a clear memorization capability im-provement from the Llama2 series to the Llama3series. While our results on coarse length confirmsthe claimed length for all four tested models,11 weobserve a significant enhancement in fine lengthwhere the model perfectly remembers the copytarget, notably between Llama-3-base and Llama-2-base (4k vs 0). Considering the similar modelstructures of Llama3 and Llama2, and the fact that",
  "Mamba (2.8B)2k/infinite02.3k<1kRWKV (7B)4k/infinite2653.6k1k": ": Performance of open-source models in Forgetting Curve and Ruler (Hsieh et al., 2024). The Fine-grainedMemory Length (Fine Length) is the maximum length at which copy accuracy surpasses 99%. The Coarse-grainedMemory Length (Coarse Length) is the maximum length at which the copy accuracy is greater than the languagemodelling accuracy by at least 1%. In Ruler, the Effective Context Length (Effective Length) is the maximum lengthwhere the performance reaches a certain threshold, which is defined by the Llama-2-7b-base/chat performance at4K. In the Fine length/Coarse Length columns, if there is a > symbol, it signifies that the length exceeds theindicated measure. Llama3s additional GQA (Ainslie et al., 2023)doesnt extend the context window, we attributethis improvement to Llama3s increased trainingdata.Our experiments validate the effectiveness ofcontext-extending models based on transformer ar-chitectures (Peng et al., 2024; Chen et al., 2023a;bloc97, 2023; emozilla, 2023). Their measuredcoarse lengths meet or exceed the claimed lengths,indicating superior long context modelling capa-bilities compared to original models. Most of thelong-sequence Transformers we examined wereproduced by increasing the theta in RoPE (Su et al.,2024) and then fine-tuning on long sequences; ourempirical results suggest such strategies are gen-erally effective approaches to extend transformerbased LLM context. We argue that the main con-cern for transformer models to extend to long con-text remain the quadratic time and space complex-ity.12",
  "Our experiments on LWM (A model with 1M claimedcontext length and we show the forgetting curve partially atAppendix ) are very slow and suffers from out-of-memory (OOM) issues in our settings": "grained Memory Length, indicating an inability toperfectly memorize or to retain memory at any sig-nificant length. This highlights current limitationsin RNN development for accurate token recall. Fur-thermore, in our measurement, RNNs like Mambaand RWKV exhibit higher language modeling ac-curacy than copy accuracy outside training contextlength (contrary to transformer models where twocurves converge after certain length), suggestingan RNN model memory issue which seems to neg-atively affect correct token prediction at long con-text, see Appendix C for the Mamba and RWKVforgetting curves for more details.",
  "Analysis & Discussion": "Forgetting curve can be applied to a small modeland is robust to data leakage.We want to seewhether the forgetting curve can be used to mea-sure model memory for small models. To this end,we train an 83M parameter model using the Llamaarchitecture on the PG-19 training dataset with atraining context size of 1024. Further details areprovided in Appendix D.1. We then plot its forget-ting curves, measured using both the PG-19 testand training datasets. shows the corresponding forgettingcurves for the Llama-83M model. According toour fine memory and coarse memory definition pre- : The forgetting curves for the Llama modelwith 83M parameters. The solid and dashed lines rep-resent copy accuracy and language modeling accuracyrespectively. The model is trained on the PG-19 trainingdataset, and the forgetting curves are plotted using boththe PG-19 training and test datasets. sented in the previous section, the model does nothave fine memory but clearly exhibit coarse mem-ory slightly beyond half of the training contextsize. We remark that the measurement is achievedon such small and arguably under-trained modelsbecause our measurement could separate betweenthe memory capability and natural language un-derstanding capability. In the contrary, it is notpossible to measure memory using popular metricssuch as Needle in a Haystack as the model hasnot yet acquired understanding at this stage.We finally remark the closeness between for-getting curves measured on PG-19 test set andthose measured on the training set. Obviously, bothmeasurements point to very close coarse memorylength. This shows that the forgetting curve is ro-bust to data leakage as the forgetting curve greatlyfocuses on the difference between copy accuracyand LM accuracy. Revisiting Perplexity using forgetting curve.Using the forgetting curve, we now revisit the hy-pothesis that perplexity is not directly related tobetter memorization or better usage of long context,which is first raised by (Hu et al., 2024b). We testthis hypothesis using Transformer-XL (Dai et al.,2019), which has shown perplexity improvement,often attributed to its superior utilization of longcontext. We train a Transformer-XL style Llama(Llama-XL) on the PG-19 training set (details inAppendix D.2).We plot the forgetting curve as well as model per-plexity curve in measured by PG-19 testset. For the perplexity curve, we confirm the find-ings of the original Transformer-XL paper wherethe architecture enables low perplexity for long con- text. However, the forgetting curve clearly showsthat the perplexity performance is not related tomodels memorization (or leverage) of long context,as copy curve becomes indistinguishable from theLM curve from 1k tokens while perplexity contin-ues to decrease until several order of magnitude.13 Our empirical results support the hypothesis (Huet al., 2024b) that perplexity tests models shortcontext modelling capability and is not an appropri-ate measure to test models long-context capabilityincluding memorization.",
  ": The forgetting curve (top) and perplexity (bot-tom) for the Llama-XL, which is trained and tested onthe PG-19 dataset": "Comparison of forgetting curve and other con-text length measurement.We show contextlength measured by forgetting curves as well asby Ruler (Hsieh et al., 2024) (i.e. the EffectiveLength column) in . One can see that formodels with low context length measured by for-getting curves (e.g. Mamba, RWKV), the effectivelength measured by Ruler is also low; however,the inverse does not hold, as shown by the exper-iments on ChatGLM3, Yarn-Llama-2 and LWMwhere we observe high context length while Rulereffective length remains low (<4k). We think thisdiscrepancy are due to two interleaved factors: Ruler uses mainly synthetic tasks fulfilling whichrequires models understanding capability, and",
  "Anecdotally, Mistral v0.1 that uses sliding window at-tention (a technique that improves perplexity at long range)does not perform well on Needle in a Haystack at longrange (v_JULY_v, 2024)": "to be counted as effective length in Ruler, the mea-sured capability needs to exceed Llama-2-base/chatbaseline. Contrary to Rulers measurement, ourmeasurement does not require understanding ca-pability allowing us to measure model of any size(e.g. see for our measurement for a 83Mmodel) and our method does not require a baselinemodel to define memory length. Fundamentally,we devise our approach trying to disentangle the un-derstanding capability and the memory capability,and uniquely measure the later one.",
  "Conclusion": "In this work, we identify the limitations present inexisting long-sequence language modeling metrics,tasks and benchmarks. To address these limitations,we introduce a new method called forgetting curve,which provides a flexible and robust measure tovisualize LLMs long-range memory capabilities.Using forgetting curve, we study the memorycapabilities of fourteen open-source models withclaimed context sizes ranging from 4K to 1M. Ourfindings empirically validate the effectiveness ofexisting transformer context extension methods,while raising questions for RNN/SSM architectures.Forgetting curve further reveals no direct correla-tion between memory capacity and perplexity.Compared to existing ways to measure long-context memory, forgetting curves mainly differsin trying to effectively decouple the models long-term memory and language understanding ability.This provides a novel perspective for evaluatinglanguage models at all sizes, potentially guidingfuture research in models long-term memory.",
  "Limitations": "There are primarily two limitations in our work.Firstly, as the copy task requires two identical textsegments, the maximum token dependency lengthis only half of the total sequence length, thus notachieving maximum efficiency for measurement.Notably, when the model imposes a strict limit onits maximum input length (e.g., ChatGLM3), ourmethod allows only measuring up to half the max-imum context length. Secondly, although we ex-tensively tested the robustness under various sizes(83M, 7B), using different alignment settings (baseand chat) and under various corpus settings, weremark that all the tests are performed within theLlama model family, leaving transferability of for-getting curves to other models unexplored. This work was supported in part by the NationalScience Foundation of China (No.62276056), theNatural Science Foundation of Liaoning Provinceof China (2022-KF-16-01), the Fundamental Re-search Funds for the Central Universities (Nos.N2216016 and N2316002), the Yunnan Fundamen-tal Research Projects (No. 202401BC070021), andthe Program of Introducing Talents of Disciplineto Universities, Plan 111 (No.B16009). Joshua Ainslie, James Lee-Thorp, Michiel de Jong, YuryZemlyanskiy, Federico Lebrn, and Sumit Sanghai.2023. Gqa: Training generalized multi-query trans-former models from multi-head checkpoints. arXivpreprint arXiv:2305.13245. Chenxin An, Shansan Gong, Ming Zhong, MukaiLi, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.2023. L-eval: Instituting standardized evaluationfor long context language models. arXiv preprintarXiv:2307.11088.",
  "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-glora: Efficient fine-tuning of long-context large lan-guage models. arXiv preprint arXiv:2309.12307": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-bonell, Quoc V Le, and Ruslan Salakhutdinov.2019.Transformer-xl: Attentive language mod-els beyond a fixed-length context. arXiv preprintarXiv:1901.02860. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,and Ji-Rong Wen. 2023. Bamboo: A comprehen-sive benchmark for evaluating long text modelingcapacities of large language models. arXiv preprintarXiv:2309.13345.",
  "Albert Gu and Tri Dao. 2023. Mamba: Linear-timesequence modeling with selective state spaces. arXivpreprint arXiv:2312.00752": "Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-tanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-burg. 2024. Ruler: Whats the real context size ofyour long-context language models? arXiv preprintarXiv:2404.06654. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, GanquCui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxi-ang Huang, Weilin Zhao, et al. 2024a. Minicpm:Unveiling the potential of small language modelswith scalable training strategies.arXiv preprintarXiv:2404.06395.",
  "Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang,and Yansong Feng. 2024b. Can perplexity reflectlarge language models ability in long text under-standing? arXiv preprint arXiv:2405.06105": "Yuri Kuratov, Aydar Bulatov, Petr Anokhin, DmitrySorokin, Artyom Sorokin, and Mikhail Burtsev. 2024.In search of needles in a 10m haystack: Recur-rent memory finds what llms miss. arXiv preprintarXiv:2402.10790. Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, HaoWang, Weishi Mi, Yaying Fei, Xiaoyang Feng, SongYan, HaoSheng Wang, et al. 2023a. Chatharuhi: Re-viving anime character in reality via large languagemodel. arXiv preprint arXiv:2308.09597.",
  "Tsendsuren Munkhdalai, Manaal Faruqui, and Sid-dharth Gopal. 2024.Leave no context behind:Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,Samuel Arcadinho, Stella Biderman, Huanqi Cao,Xin Cheng, Michael Chung, Matteo Grella, et al.2023. Rwkv: Reinventing rnns for the transformerera. arXiv preprint arXiv:2305.13048. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and EnricoShippole. 2024. YaRN: Efficient context window ex-tension of large language models. In The TwelfthInternational Conference on Learning Representa-tions.",
  "v_JULY_v. 2024.The paper-review dataset wasusedtofine-tuneseparatelymistral,gemma": "Zekun Moore Wang, Zhongyuan Peng, Haoran Que,Jiaheng Liu,Wangchunshu Zhou,Yuhan Wu,Hongcheng Guo, Ruitong Gan, Zehao Ni, ManZhang, et al. 2023. Rolellm: Benchmarking, elic-iting, and enhancing role-playing abilities of largelanguage models. arXiv preprint arXiv:2310.00746. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771.",
  "Jianxin Yang. 2023. Longqlora: Efficient and effectivemethod to extend context length of large languagemodels. arXiv preprint arXiv:2311.04879": "Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang,Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao,Dahua Lin, Boxun Li, et al. 2024. Lv-eval: A bal-anced long-context benchmark with 5 length levelsup to 256k. arXiv preprint arXiv:2402.05136. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. 2022.Glm-130b:An open bilingual pre-trained model. arXiv preprintarXiv:2210.02414. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zi-hang Xu, Junhao Chen, Moo Khai Hao, Xu Han,Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al.2024. bench:Extending long context eval-uation beyond 100k tokens.arXiv preprintarXiv:2402.13718.",
  "AStatistical Analysis": "In , the one-way ANOVA tests and Kruskal-Wallis H-test were conducted with the copy targetsetting fixed at en_test, and the irrelevant prefix set across three different settings: en_test, en_train, and zh.It can be seen that within half of the claimed length (2k), the choice of irrelevant prefix has no significantimpact on the accuracy of language modeling.",
  "D.1Llama-83M": "We train the Llama-83M model for one epoch on the PG-19 training set, with the objective of maximizingthe probability of the next token, which is the standard pre-training objective for language models. Themodel and training hyperparameters are listed in . We use the WSD scheduler (Hu et al., 2024a)shown in . The optimization is performed using the AdamW optimizer (Loshchilov and Hutter,2017) with parameters (beta1, beta2) = (0.9, 0.95), epsilon set to 1e-8, and a weight decay of 0.1. We donot employ dropout in our model.",
  "hn=on W no(8)": "n denotes the layer index of the attention module. denotes the segment index of the data. [; ] denotesconcatenation along the sequence dimension, while [:] denotes slicing along the sequence dimension. hn1is the hidden state from the previous layer. W nq , W nk , W nv , W no are the weight matrices for query, key, value,and output, respectively. qn , kn , vn are the query, key, and value matrices. kn is the key matrices after L2normalization along the embedding dimension. kn , vn are the concatenated key and value matrices withthe cached ones from the previous segment along the sequence dimension. k_cachen, v_cachen are thecached key and value matrices, updated with the current segment. pids is the sequence of position indicesfrom 0 to len(kn ) 1. apply_rope is the function that applies the rotary position embedding (Su et al., 2024). Mask is the attention mask, which is a LowerTriangularFromBottomRightMask as illustrated in. on is the attention output. hn is the output hidden state.The main modification is in Equation (3), where we concatenate the KV cache from the previous",
  ": Ordered DataLoader": "segment to the current one, unlike Transformer-XL, which concatenates the hidden state. It is worth notingthat our position encoding spans from 0 to len(kn ) 1, thereby guaranteeing that the model employsposition encodings encountered during training for inference. Normalizing the kn with L2Norm helpswith the extrapolation of the position encoding (Su, 2023), although we do not use position encodingoutside of the training range. We also use an ordered DataLoader to ensure that segments are consecutive,as shown in . Other experimental settings are consistent with Llama-83M."
}