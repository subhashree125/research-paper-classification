{
  "Abstract": "Extensive efforts in automated approaches forcontent moderation have been focused on devel-oping models to identify toxic, offensive, andhateful content with the aim of lightening theload for moderators. Yet, it remains uncertainwhether improvements on those tasks have trulyaddressed moderators needs in accomplishingtheir work. In this paper, we surface gaps be-tween past research efforts that have aimed toprovide automation for aspects of content mod-eration and the needs of volunteer content mod-erators, regarding identifying violations of var-ious moderation rules. To do so, we conducta model review on Hugging Face to reveal theavailability of models to cover various modera-tion rules and guidelines from three exemplarforums. We further put state-of-the-art LLMsto the test, evaluating how well these modelsperform in flagging violations of platform rulesfrom one particular forum. Finally, we conducta user survey study with volunteer moderatorsto gain insight into their perspectives on usefulmoderation models. Overall, we observe a non-trivial gap, as missing developed models andLLMs exhibit moderate to low performanceon a significant portion of the rules. Modera-tors reports provide guides for future work ondeveloping moderation assistant models.",
  "Introduction": "Content moderation guards online forums againsthostility and extremism while maintaining commu-nity norms, ensuring the forums remain healthyand open to all participants. While many platformspay for this service, others, such as Reddit, Discord,Facebook, and Twitch, use a hybrid model, relyingon the labor of volunteers. Yet, behind the screen,being a volunteer content moderator is time- andemotionally-draining work. Moderators frequentlydeal with abusive language, sensitive posts, andunpleasant interactions with users (Seering et al.,2019; Gilbert, 2020; Dosono and Semaan, 2019;Wohn, 2019; Jiang et al., 2019), often doing this work in addition to full-time jobs. To support thesevolunteers, efforts have been made to develop mod-els, such as Google Perspective API1 and OpenAIundesired content detection (Markov et al., 2023),that can automatically identify content for removalin order to alleviate moderators workload.Although these systems have shown great suc-cess in detecting undesired content, they primar-ily focus on toxic content. Yet, content moderationencompasses more than toxicity detection,2 partic-ularly in platforms that leverage volunteer modera-tion within smaller communities hosted by the site.For example, Reddit is a platform consisting of var-ious communities, known as subreddits, focusedon a diverse set of topics, and each subreddit hasits own moderation rules. Fiesler et al. (2018) con-ducted a study to explore various subreddit rules,consolidating similar ones, and arrived at 25 dis-tinct rule types. Hence, in order to support moder-ators in detecting potential rule-violating content,content moderation tools need to support muchmore than just toxicity detection.In this paper, we aim to assess to what extentcurrent natural language processing (NLP) modelscan serve the wide spectrum of moderation rulesso that they can be helpful in assisting moderators.First, to understand the functions previous auto-mated content moderation models have focused on,we conduct a model review on Hugging Face (HF)with rules from three subreddits as exemplars. Thisallows us to gauge the progress of past model devel-opments in covering various moderation rules. Weuse model review as opposed to the more commonliterature review to gain a technical understandingof the existing models functions. In addition toexamining models that are built to handle specifictasks, we also assess so-called general-purposedlarge language models (LLMs) capability in cov-",
  "use toxicity detection as an umbrella term for hatespeech detection, incivility detection, etc": "ering various moderation rules. We evaluate GPT-4 and Llama-2 on a new evaluation dataset thatwe collected, consisting of moderation decisionsfrom r/AskHistorians and covering a wide rangeof rules. Finally, using the models performanceon this new dataset as an empirical grounding, weconducted a survey study with active moderatorsfrom r/AskHistorians (N = 11). Through this sur-vey study, we aim to gain insights into model usersperspectives on the performance requirements foruseful moderation models on different rules.We find a substantial gap in both existing NLPmodels and LLMs performance when to coverthe diverse set of moderation rules that subredditsemploy. Our analysis shows the majority of mod-eration rules from the three subreddits ( 80%)are unrelated to toxicity detection, with nearly 70%lacking an huggingface model designed for theirresolution. While one might hope that general-purpose LLMs could fill this gap, our experimentswith GPT-4 and Llama-2 show that LLMs fall shortin their ability to detect violations of many rules.Specifically, both GPT-4 and Llama-2 exhibitedmoderate to low precision and/or recall (< 70%)for half of the rules from r/AskHistorians. Findingsfrom our survey study also indicate that neitherLLM has good enough performance to be usefulfor 6 of 23 r/AskHistorians rules (26%), includingrules such as Scope, Digression, and Sources3.Meanwhile, our survey study also shows that mod-erators are excited about an assistant modeltheyare okay with even imperfect tools if they are well-informed about their limitations.For differentkinds of rules, moderators have complex needsin terms of model precision and recall. For in-stance, they need high recall for complex rules (e.g.",
  "For explanation of the rules, please refer to inappendix": "2020), trolling (Binns, 2012), and abuse (Dibbell,1994). Currently, volunteers contribute moderationefforts on nearly all major platforms. For example,volunteers are responsible for moderation withinFacebooks Groups, Discords servers, Twitchsstreams, Reddits subreddits, and through Xs (for-mally known as Twitter) Community Notes. Theseefforts bring significant value to platforms, withone study estimating that, at the baseline, volunteermoderators on Reddit collectively work 466 hoursa day, amounting to a value of about 3.4 millionUSD per year (Li et al., 2022). With the work-load and continuous exposure to online harassment(Gilbert, 2020; Wohn, 2019; Dosono and Semaan,2019) and toxic content, such as hateful languageand violent or upsetting imagery, this moderationwork can easily result in burnout. A common way to mitigate burnout is throughthe development of tools that support moderationlabor, particularly through automation. For ex-ample, prior study has found that automation canhelp at scale, supporting moderators of large com-munities with high levels of activity (Kiene andHill, 2020; Seering et al., 2018). Automation isalso helpful when communities experience unprece-dented or unexpected growth, such in cases wherecommunities receive an influx of new users (Kieneet al., 2016) and in cases where communities aresubject to sustained brigades of bad actors spam-ming hateful content (Han et al., 2023). Mostly,automated tools are developed and maintained bymoderators themselves. For example, the mostcommonly used tool on Reddit, Automod, wasoriginally designed by a moderator before the com-pany took over responsibility for its developmentand maintenance (Jhaver et al., 2019) and moder-ators on Twitch have developed bots on the fly inresponse to hate raids (Han et al., 2023). Whileautomation can help support moderation work, itmay also add to it in different wayslike needingto build and maintain a botand requires skill setsnot all moderators have (Jhaver et al., 2019). Meanwhile, the NLP field has a long line ofwork on developing automated models to detecttoxic content, which can be used to support moder-ation work (e.g. Jigsaw, 2019; Pavlopoulos et al.,2020; Park and Fung, 2017; Zampieri et al., 2019).Beyond that, Lees et al. (2022) extended beyondEnglish content and proposed the multilingual Per-spective API to detect offensive and hateful contentfrom a diverse range of languages. Moreover, some study delves into various types of toxic content,trying to improve the nuance of detection models(Markov et al., 2023; Price et al., 2020). However,these studies primarily focus on detecting toxiccontent, which is merely a subset of moderationwork. Therefore, our study uses Reddit rules asan example to identify gaps in NLP moderationmodels within a wide spectrum of rules.Recently, Kumar et al. (2023) tested LLMs per-formance on rule-based moderation with rules from95 subreddits. They evaluated GPT models (3, 3.5,4) on a dataset consisting of removed Reddit postsas violating and not violating content. They con-clude that the GPT models are effective in con-ducting moderation on subreddits like r/Movies butstruggle with other subreddits such as r/askscienceand r/AskHistorians. Our study builds on previousresults by studying gaps in both purpose-built mod-eration models and LLMs. Rather than focusing onthe broad binary questions of removing or keepingposts, we focus on coverage for specific modera-tion rules. In addition, we include the perspectivesof human moderators to understand how existingmodels address the needs of real-world moderation.",
  "Hugging Face Model Review": "One traditional approach to understanding a scien-tific context is to perform a literature review. Be-cause we are particularly interested in deployablemodels, we opt for a model review, where insteadof reviewing papers, we review publicly availablemodels that can be adapted to assist content moder-ators in making moderation decisions, with the fo-cus on Reddit rules. We conduct the model reviewvia Hugging Face (HF)4, an open-source platformthat provides pre-trained models for various NLPtasks, to understand the progress of model devel-opment from existing research in covering variousmoderation rules. To conduct the model review,we gather rules from three subreddits and manuallyinvestigate for each rule whether there exists an HFmodel suitable for detecting such rule violations.",
  "Method": "Evaluation DatasetThe evaluation dataset wascreated and annotated by a volunteer moderatorfor r/AskHistorians who has expertise in bothmoderation and qualitative analysis.For eachrule, 1113 violating posts or comments wereselected, save for posts violating the Illegal artifacts rule, which, due to the rareness of vio-lations, was only represented 3 times in the dataset.Most content was selected from r/AskHistory, aReddit community that is thematically similar tor/AskHistorians but with different moderation rules.Some additional content was taken directly fromr/AskHistorians. We exclude all borderline casesand only select posts and comments that clearlyviolate the rules. In total, the dataset includes 101questions and 134 comments.5 Because content of-ten violates more than one rule, questions and com-ments included in the dataset often reflect multipleviolations; however, annotations represent the mostrelevant or severe violation. For example, a com-ment containing only a joke would be annotated asviolating the rule prohibiting joke responses, even 5In r/AskHistorians, posts are questions users have askedand comments are responses to questions. Comments includedin the dataset are not necessarily responses to the questionsincluded in the dataset. though it would also violate rules on comprehen-siveness or depth. The dataset also included 11questions and 10 comments that do not violate anyr/AskHistorians rules. We use these data as ourevaluation dataset.6 Model Testing and PromptsTo assess the per-formance of LLMs, we test GPT-4 and Llama-2-13b models. We conduct a pilot experiment witha small number of randomly selected examplesfrom our dataset to determine the prompt for modelassessment. Since moderators are mostly unfamil-iar with prompt tuning and have limited time tolearn this skill, we aim to test the models withoutextensively experimenting different wordings orexamples of the prompt, roughly as they might beused by moderators.First, as suggested in the prompting guidelines(Shieh, 2023), we find that using rule descriptionsin the prompt is helpful for the model to detectviolations of the rule. Hence, we crawl rule de-scriptions from r/askHistorians rule explanationweb page. As the descriptions from the subredditweb page are long and include many constructivesuggestions for post-writers, we manually selectuseful information for moderation to include in theprompt. See appendix for the list of therules and their descriptions.Moreover, we experiment with both zero-shotand few-shot settings. In the few-shot setting, weprovide the model with three examples that vio-lated the rule and three examples that did not forquestion contents. For comment contents, we pro-vide one example for each due to the models inputlength limit. We realize that few-shot examples arehelpful for the Llama-2 model to better understandthe task, whereas the GPT-4 model does not havesignificant improvement from incorporating the ex-amples. Therefore, for the main experiment, wetest GPT-4 with the zero-shot setting and Llama-2with the few-shot setting, but we also report the fullset of experiments in the appendix.Finally, following the suggestions from Shieh (2023), we set the models role as a moderatorassistant and put the main question at the beginningof the prompt. In the few-shot setting, we alsorepeat the main question at the end of the prompt.See Appendix B for the exact prompts we used.",
  "Model Review Results": "As shown in , among the 41 moderationrules, 37 (90%) of the rules cannot be handled byrule-based approaches. Among these, only 7 (19%)are toxicity-related. This reemphasizes that modelssolely designed for detecting toxicity fall short inmeeting the practical needs of content moderators.Among the 7 toxicity-related rules, we see that 4(57%) require model modifications. Most of thesemodifications are customization for the rule, whichwe will describe later. Even toxicity-related rules,despite having many developed models, do nothave perfectly matching models.Among the 37 non-toxicity-related rules, only6 (20%) rules have matching HF models that canbe applied directly. To see some evaluation re-sults of these matching models, please refer toAppendix A in appendix. 13 (43%) of the rulesrequire some model modifications in order forthe matching model to be adapted for the rule,whereas 11 (37%) rules have no matching model atall. These rules include, for example, low-effort",
  "Information rule in r/Movies can be handledwith a misinformation detection model, butthe model needs to be adapted to title-liketexts and information about movie releases": "2. Some models are developed with a limitedscope of training data and thus may require ex-tended training to be used for moderation. Forinstance, we found a plagiarism model thatwas trained on the Machine Paraphrase Cor-pus (Wahle et al., 2022) consisting of 200kexamples of original and paraphrases usingtwo online paraphrasing tools. The plagiarismcases on Reddit may be more complicatedthan paraphrasing, and the potential plagia-rism source is larger. Hence, the model needsan extension of scope in order to be useful.",
  ". Some models can be applied to certain rules,": "but with a degree of stretching. For instance,violations of the rule Off Topic can poten-tially be caught by a topic classifier model,but the model requires some major changes toadapt to the subreddit and its related topics.To see detailed annotations for each question,please refer to Tables 3, 4, and 5 in appendix.",
  "LLM Performance on Subreddit Rules": "Finding a suitable NLP model for handling a mod-eration rule, let alone executing the necessary mod-ifications to the model, is not trivial, and somerules simply lack a matching model to address theirspecific issues. It requires familiarity with NLPtechniques and tasks, which may not be within thescope of a content moderators abilities. Question-answering-based LLM applications, on the otherhand, may be more practical for content modera-tors to employ in assisting their jobs. Therefore,we also evaluate LLMs performance in catchingrule violations. For this evaluation, we focus on ther/AskHistorians subreddit, which has 23 modera-tion rules, and two LLMs Llama-2 and GPT-4.",
  "inclusivecoref": ": Model performance on detecting question posts that violate moderation rules with GPT-4 (top) andLlama-2 (bottom) models. The x-axis is the moderation rules for question posts. Each bar pair is the precision (left)and recall (right) scores on the specific rule. The ones marked grey are the model scores that moderators would notconsider useful as a moderation assistant tool. The left-most rules have good performance from both of the LLMs;the rules in the middle have good performance from at least one of the LLMs; the right-most rules do not have goodenough performance from either of the LLMs.",
  "Moderator Evaluation": "To understand volunteer moderators perspectiveson the relative performance of LLMs, we also con-ducted an evaluation survey with active moderatorsfrom r/AskHistorians7. The survey aimed to cap-ture 1) whether moderators would use the LLMto help with their moderation work on each rule,given the LLMs performance (precision and recall)identifying violations for this rule, and 2) for eachrule, what kind of model performance is needed toeffectively assist moderators.To answer the first question, we asked partici-pants to evaluate each models performance. Weshowed participants the LLMs precision and recallscores for each rule from our experiment and askedthem to choose among: 1. performance for thisrule is good enough that I would use the tool, 2.I would use this tool for this rule if it could catchmore violations (need higher recall), 3. I would usethis tool for this rule if it could be correct moreoften (need higher precision), or 4. both measureswould have to improve for me to use this tool.For the second question, we asked participantsto cluster moderation rules based on how impor-tant it is for the model to have high precision (orhigh recall) for each rule, compared to other rules.Options included: most important, less important,and would not use a model for this rule. We alsoasked for participants comments on using LLMs",
  "Approved by our institutional IRB, #1704882-9": "to identify violating posts in general. See appendixFigures 7 and 8 for the exact questions we asked.Overall, by invitation, we recruited 11 out of36 total active moderators from r/AskHistorians.We obtained their consent at the beginning of thesurvey. Our participants spent an average of 25minutes completing the survey. In return, partici-pants received compensation in the amount of $25USD upon completion of the survey in full.",
  "Model Performance Results": "We tested GPT-4 and Llama-2 on the evaluationdataset as we described above. We use the firstoccurrence of Yes or No in the generated textas the models output. The results are shown in and (See appendix and for the results of the models in both few-shot and zero-shot settings). The results are mixed some rules have both high precision and high recallfrom at least one model, while many of the ruleshave moderate to low precision, recall, or both.Neither of the two models is clearly better than theother in identifying violating contents for all therules. The results indicate that certain rules remainunresolved, even for state-of-the-art LLMs.Our survey results indicate that different moder-ators have different perceptions of what constitutesan adequate model: for the evaluation questions,participants exhibited only fair agreement (FleissKappa 0.34). Mirroring policy decision-makingstandards in the subreddit, we use majority vote : Model performance on detecting comment posts that violate moderation rules with GPT-4 (top) andLlama-2 (bottom) models. The x-axis is the moderation rules for comment posts. Each bar pair is the precision (left)and recall (right) scores on the specific rule. The ones marked grey are the model scores that moderators would notconsider useful as a moderation assistant tool. The left-most rules have good performance from both of the LLMs;the rules in the middle have good performance from at least one of the LLMs; the right-most rules do not have goodenough performance from either of the LLMs. : Clustering results from the survey study. Eachpoint is a moderation rule from r/AskHistorians. Thex-axis shows the precision importance score, or howimportant it is for a model to have high precision for thisrule. Similarly, the y-axis shows the recall importancescore. Note that we removed rules for which at leastthree participants stated they would not use a model. to summarize participants evaluation of the mod-els performance. The results are shown in Fig-ure 2 and . In most cases, moderatorswould consider models with > 70% precision andrecall to be useful. Among the 23 rules, both mod-els are considered useful for three question rulesand three comment rules. For some rules, suchas NSFW/Sensitive content, No plagiarism, No",
  "Scope question rules and No partial answer or": "placeholders, Digression, and Sources com-ment rules. This emphasizes that, while currentLLMs provide possibilities for supporting contentmoderation work, further improvement is neededto support useful moderation tools.Most of the moderators were excited about theidea of having even imperfect assistant models tosupport their work. Two participants expressedwillingness to accept a model with moderate levelsof precision and recall, because any help is bet-ter than no help. One stated A tool flags a lotof things but isnt right too often? Well, its stillflagging things for my attention. A tool that doesntflag a lot but is very frequently right? Great, I donthave to think about whether or not to remove whatits found. This response emphasizes the impor-tance of model transparency. Model users often canadapt to a flawed model and make it useful as longas they are informed of its performance limitations.Some moderators value precision more than re-call because a model that mis-flagged many postswould make moderators workload unnecessarilylarge. On the other hand, some moderators considerrecall to be more important to catch all violating posts missed by moderators, especially for urgentor sensitive content, such as for the rule Civility.Moreover, some participants state that the need forthe model varies according to the rules. One partic-ipant specified, A tool would be useful for assess-ing and filtering breaches of more complex ruleslike plagiarism (especially Chat GPT use) or poorsourcing, but given the time needed to assess theseclaims it would need to be correct the overwhelm-ing majority of the times it flagged something. Forsimpler rules like clutter, depth, or no jokes thenI would like to see a tool which is able to flag themajority of comments which break the rules. Thesame participant also pointed out that they wouldnot use a model for rules that are inherently sub-jective between moderators, such as Basic facts.Another participant further indicated the need formodel explanations for complex rules.To inform future improvement on moderationassistant models, we asked moderators to clusterrules according to their needs. The results, as in, provide quantified insight into moder-ators needs for model precision and recall. Weaggregate participants answers by averaging theirchoice of cluster for each rule. Each time a partici-pant assigned a rule to most important, it is scoredas two points; assignments to less important arescored as one point. Rules for which a participantselected would not use are not included in the scorecalculation but reported separately. Among the23 rules, ten require both high precision and highrecall (> 1.5). For three rules, at least three partici-pants claimed they would not use a model Basic",
  "Discussion": "Overall, our findings suggest potential directionsfor future research on moderation assistants. First,there remains ample opportunity for model im-provements. There are ten rules that moderators re-quire both high precision and high recall, as in Fig-ure 4. For three of them, neither GPT-4 nor Llama-2 are considered useful by moderators: Privacyand Homework with low recall (27%), and No partial answer or placeholders with lowprecision (63%).For four of the ten rules, al-though moderators mark one model as acceptable,there is still room for improvement in precisionor recall: Plagiarism, NSFW/Sensitive content,",
  "and Depth with recall 40 62%, and Do not just": "post links or quotations with precision 69%.Besides, the rule Digression, which requires highprecision, has low precision from both models(58% and 64%). Though some rules are distinctfor r/AskHistorians (e.g., Homework), many rulesare also enforced in other subreddit communities.For example, among the 523 subreddits studiedby Fiesler et al. (2018), 39% have rules on off-topic content (similar to the Digression rule inour study) and 27% have rules on NSFW content.Hence, our findings underscore the existence ofplenty of rules from online communities beyondtoxicity detection that require exploration. Despite the models not performing as well asone might hope, volunteer moderators express sig-nificant enthusiasm and flexibility about havingmoderation assistants. People are used to workingwith tools that may not be perfect, and they areskilled at finding ways to make the most of them.In our findings, moderators explained several per-sonal approaches they take in using a model withmoderately low precision or with moderately lowrecall. However, in order to exercise these strate-gies, models abilities must be transparent to enableusers to adapt accordingly. Furthermore, we observed varying requirementsamong users and rules. Even for the same rule,different moderators expressed distinct preferencesfor high precision or recall. This suggests thatin situations where a trade-off between precisionand recall is necessary, moderators may lean to-wards a model offering more user control, thusallowing for an adjustable trade-off. This way,they can customize the model to align with theirpreferences. In addition, our participants also ex-pressed some common preferences. For complexrules (e.g. Plagiarism and Digression), moder-ators prefer higher recall and model explanationsover mere flagging of violations. Conversely, forsimpler rules (e.g. Current Event and Jokes and",
  "Conclusions": "We identified gaps between the functionalities ofprevious models on automated moderation and thefunctions needed to address Reddit moderationrules. We conducted a model review with Hug-ging Face models to to see for how many rules anexisting model can be used to detect violations. Thefindings reveal that a considerable number of rulesare not covered by existing Hugging Face models.Even when a matching model exists, more than halfof the rules require non-trivial adjustments of themodel in order to be useful. We additionally iden-tified four major types of necessary adjustments:domain adaptation, model scope extension, cus-tomization, and function shift.Moreover, we evaluated two LLMs, GPT-4 andLlama-2, on an evaluation dataset gathered fromr/AskHistorians, and conducted a survey study toreveal whether the state-of-the-art general-purposeLLMs can handle various moderation rules. The re-sults indicate there is still a significant gap thesemodels exhibit either low recall or moderate to lowprecision for many of these rules. Also, moderatorsare not satisfied with either of the models perfor-mance for six of 23 rules. This limits the usabilityof these models in aiding moderators to identifyviolations in real-world moderation.Finally, our moderator survey study provides in-sight into model-performance requirements for amoderation assistant model. We observed modera-tors excitement about the model and willingness tobe flexible to accommodate a flawed model. Mod-erators also offered constructive feedback on howthey wish to use an assistant model differently fordifferent rules. Such user-centered guidance shouldbe useful in building improved, customized moder-ation tools in the future.",
  "Limitations": "Our study has several limitations. We are primarilystudying Reddit; other platforms may have differ-ent sets of rules. We are also focusing on Englishand text-only posts; violations in other languagesor in other forms, such as images and videos, mayface different challenges. Furthermore, our surveystudy is limited to volunteer content moderatorsfrom the r/AskHistorians community. While weanticipate that the insights gained from the moder-ation experience might have broader applicabilityacross communities and for other moderators, theextent of generalization remains uncertain without further study.Moreover, our study and findings are groundedin the set of Reddit moderation rules rather thanthe frequency of their violation. Consequently, welack information regarding which rules moderatorsencounter most frequently and for which rules theywant additional assistance.In addition, our model testing is limited to one-shot and few-shot contexts, as opposed to otherstrategies, such as interactive teaching and Chain-of-Thoughts.",
  "Julian Dibbell. 1994. A rape in cyberspace or how anevil clown, a haitian trickster spirit, two wizards, anda cast of dozens turned a database into a society. Ann.Surv. Am. L., page 471": "Bryan Dosono and Bryan Semaan. 2019. Moderationpractices as emotional labor in sustaining online com-munities: The case of AAPI identity work on Reddit.In Proceedings of the 2019 CHI conference on humanfactors in computing systems, pages 113. Casey Fiesler, Jialun Jiang, Joshua McCann, Kyle Frye,and Jed Brubaker. 2018. Reddit rules! Characterizingan ecosystem of governance. Proceedings of theInternational AAAI Conference on Web and SocialMedia, 12(11). Sarah A Gilbert. 2020. \"I run the worlds largest his-torical outreach project and its on a cesspool of awebsite.\" Moderating a public scholarship site onReddit: A case study of r/AskHistorians. Proceed-ings of the ACM on Human-Computer Interaction,4(CSCW1):127. Catherine Han, Joseph Seering, Deepak Kumar, Jef-frey T Hancock, and Zakir Durumeric. 2023. Hateraids on twitch: Echoes of the past, new modalities,and implications for platform governance. Proceed-ings of the ACM on Human-Computer Interaction,7(CSCW1):128. Shagun Jhaver, Iris Birman, Eric Gilbert, and AmyBruckman. 2019. Human-machine collaboration forcontent regulation: The case of reddit automoderator.ACM Transactions on Computer-Human Interaction(TOCHI), 26(5):135. Jialun Aaron Jiang, Charles Kiene, Skyler Middler,Jed R Brubaker, and Casey Fiesler. 2019. Moderationchallenges in voice-based online communities on dis-cord. Proceedings of the ACM on Human-ComputerInteraction, 3(CSCW):123.",
  "Conversational AI Jigsaw. 2019. Jigsaw unintendedbias in toxicity classification. Kaggle": "Charles Kiene and Benjamin Mako Hill. 2020. Whouses bots? A statistical analysis of bot usage in mod-eration teams. In Extended abstracts of the 2020 CHIconference on human factors in computing systems,pages 18. Charles Kiene, Andrs Monroy-Hernndez, and Ben-jamin Mako Hill. 2016. Surviving an \"eternal septem-ber\" how an online community managed a surge ofnewcomers. In Proceedings of the 2016 CHI Con-ference on Human Factors in Computing Systems,pages 11521156.",
  "Deepak Kumar, Yousef AbuHashem, and Zakir Du-rumeric. 2023. Watch your language: Large languagemodels and content moderation. (arXiv:2309.14517).ArXiv:2309.14517 [cs]": "Alyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, JaiGupta, Donald Metzler, and Lucy Vasserman. 2022.A new generation of perspective api: Efficient multi-lingual character-level transformers. In Proceedingsof the 28th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining, KDD 22, page31973207, New York, NY, USA. Association forComputing Machinery. Hanlin Li, Brent Hecht, and Stevie Chancellor. 2022.Measuring the monetary value of online volunteerwork. In Proceedings of the International AAAI Con-ference on Web and Social Media, volume 16, pages596606. Todor Markov, Chong Zhang, Sandhini Agarwal, Flo-rentine Eloundou Nekoul, Theodore Lee, StevenAdler, Angela Jiang, and Lilian Weng. 2023. A holis-tic approach to undesired content detection in thereal world. Proceedings of the AAAI Conference onArtificial Intelligence, 37(12):1500915018. Ji Ho Park and Pascale Fung. 2017. One-step and two-step classification for abusive language detection onTwitter. In Proceedings of the First Workshop onAbusive Language Online, pages 4145, Vancouver,BC, Canada. Association for Computational Linguis-tics. John Pavlopoulos, Jeffrey Scott Sorensen, Lucas Dixon,Nithum Thain, and Ion Androutsopoulos. 2020. Toxi-city detection: Does context really matter? In AnnualMeeting of the Association for Computational Lin-guistics. Ilan Price, Jordan Gifford-Moore, Jory Fleming, SaulMusker, Maayan Roichman, Guillaume Sylvain,Nithum Thain, Lucas Dixon, and Jeffrey Sorensen.2020.Six attributes of unhealthy conversation.arXiv:2010.07410 [cs]. ArXiv: 2010.07410. Joseph Seering. 2020. Reconsidering community self-moderation:the role of research in supportingcommunity-based models for online content modera-tion. Proceedings of the ACM on Human-ComputerInteraction, 4:107. Joseph Seering, Juan Pablo Flores, Saiph Savage, andJessica Hammer. 2018.The social roles of bots:evaluating impact of bots on discussions in onlinecommunities. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):129.",
  "Jessica Shieh. 2023. Best practices for prompt engineer-ing with openai api. [Online; accessed Nov 2023]": "Jan Philip Wahle, Terry Ruas, Tom Foltnek, Nor-man Meuschke, and Bela Gipp. 2022. Identifyingmachine-paraphrased plagiarism. In Information fora Better World: Shaping the Global Future, pages393413, Cham. Springer International Publishing. Donghee Yvette Wohn. 2019. Volunteer moderators intwitch micro communities: How they get involved,the roles they play, and the emotional labor they ex-perience. In Proceedings of the 2019 CHI conferenceon human factors in computing systems, pages 113. Marcos Zampieri, Shervin Malmasi, Preslav Nakov,Sara Rosenthal, Noura Farra, and Ritesh Kumar.2019. Predicting the type and target of offensiveposts in social media. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), pages 14151420, Minneapolis, Minnesota.Association for Computational Linguistics.",
  "We evaluated the HuggingFace matching models with our r/AskHistorians evaluation dataset. As indicatedin , there are four non-toxicity-related rules that have matching HF models: NSFW/Sensitive": "content, Illegal artifacts, Basic Facts, and Jokes and humor. Among these four matching models,the model for Illegal artifacts is not compatible with the HuggingFace model pipeline and lacks theappropriate code for inference. Similarly, the model for Basic Facts uses a model structure that is notdefined within Huggingface module.Consequently, we evaluated only the models for NSFW/Sensitive content and the model for Jokes",
  "CLLMs Performances under Zero-shot and Few-shot Settings": ": Model performance on detecting violations of moderation rules with GPT-4 model under few-shot setting(top) and zero-shot setting (bottom). The x-axis is the moderation rules. Each bar pair is the precision (left) andrecall (right) scores on the specific rule. : Model performance on detecting violations of moderation rules with Llama-2 model under few-shotsetting (top) and zero-shot setting (bottom). The x-axis is the moderation rules. Each bar pair is the precision (left)and recall (right) scores on the specific rule.",
  "No\"Poll\"-typequestions": "\"Poll\"-type questions arent appropriate here: \"Who was the most influential person inhistory?\" or \"Who was the worst general in your period?\" or \"Who are your Top 10 favoritepeople in history?\" If your question includes the words \"most\" or \"least\", or \"best\" or \"worst\"(or can be reworded to include these words), its probably a \"poll\"-type question.",
  "No \"Soapboxing\" orLoaded Questions": "All questions must allow a back-and-forth dialogue based on the desire to gain furtherinformation and not be predicated on a false and loaded premise in order to push an agenda.Example: Good Question: \"People say that Nixon is the worst President of all time. Whyis this so?\" Bad Question: \"Nixon was the worst President of all time. Why isnt Obamaconsidered the worst?\" The bad question is a fishing expedition to try to start a debate aboutObamas presidency. Most of these questions will break our 20-year rule, or try to set up adebate about an issue using a long wall of text in the main post.",
  "PrivacyWe do not allow questions that pose possible privacy issues for living or recently deceasedpersons who are not in the public eye. The cut-off for \"recent\" is 100 years": "Illegal artifactsIt is our policy to disallow posts asking for further information on artifacts where there is alikelihood that the acquisition or possession of the item might be illegal, unethical, and/orrun contrary to sound, historical practices. Basic FactsQuestions looking for specific, basic facts - for the purpose of this rule, seeking a name, a dateor time, a number, a location, the origin of a word, the first or last known instance/exampleof an object/phenomenon/etc., or a simple list of examples or facts - are not allowed asstandalone threads. DepthAn in-depth answer provides the necessary context and complexity that the given topic callsfor, going beyond a simple cursory overview. Your answer should be giving context to theevents being discussed, not simply listing some related facts. SourcesWe do not require sources to be preemptively listed in an answer here, but do expect thatrespondents be familiar with relevant and reliable literature on the topic, and that answersreflect current academic understanding or debates on the subject at hand. But sole relianceon tertiary sources for context and analysis is not allowed, and will result in the removal of aresponse.",
  "No political agen-das or moralizing": "This subreddit is a place for learning and open-minded discussion. As such, answers shouldnot be written in the interests of advancing a personal agenda, but should represent a sincereeffort to make an argument from the historical record. They should be constructed in keepingwith the principles of the historical method - that is to say, your evidence should not bechosen selectively to support an argument that you feel is right; your argument should insteaddemonstrably flow from your critical engagement with an appropriate range of evidence.",
  "Do not just postlinks or quotations": "Do not just post links to other sites as an answer. This is not helpful. The expectation is thata user is posting to this subreddit because they are looking for the type of answer dictated bythe rules in place here. Please take some time to put the links in context for the person askingthe question. Avoid only recommending a source whether thats another site, a book, orlarge slabs of copy-pasted text. If you want to recommend a source, please provide at least asmall summary of what the source says."
}