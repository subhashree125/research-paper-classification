{
  "Abstract": "With the rise of large language models (LLMs),many studies are interested in transferringthe reasoning capabilities of LLMs to smalllanguage models (SLMs). Previous distilla-tion methods usually utilize the capabilities ofLLMs to generate chain-of-thought (CoT) sam-ples and teach SLMs via fine-tuning. However,such a standard distillation approach performspoorly when applied to out-of-distribution(OOD) examples, and the diversity of the gener-ated CoT samples is insufficient. In this work,we propose a novel counterfactual distillationframework. Firstly, we leverage LLMs to auto-matically generate high-quality counterfactualdata. Given an input text example, our methodgenerates a counterfactual example that is verysimilar to the original input, but its task labelhas been changed to the desired one. Then,we utilize multi-view CoT to enhance the di-versity of reasoning samples. Experiments onfour NLP benchmarks show that our approachenhances the reasoning capabilities of SLMsand is more robust to OOD data. We also con-duct extensive ablations and sample studies tounderstand the reasoning capabilities of SLMs.",
  "Introduction": "Large language models (LLMs) have demonstratedremarkable performance in a wide range of down-stream tasks (Brown et al., 2020; Wei et al., 2021;Hoffmann et al., 2022). Recent works have shownthat chain-of-thought (CoT) can elicit reasoningcapabilities in LLMs by asking the model to in-corporate intermediate reasoning steps while solv-ing a problem (Kojima et al., 2022; Wang et al.,2022). However, the efficacy of prompt-based CoTmethods is restricted to very large models (beyond10B parameters) (Wei et al., 2022). Due to thesubstantial computational resources or expensiveAPI calls required for accessing LLMs that supportCoT, various studies have delved into distilling the",
  "A:forest B:barn C:public office D:freezer": "Weasels are known for their small size and agility, making them adept at entering buildings or enclosures where they can find food. In this context, the mention of a weasel becoming a problem by getting into where the eggs were kept strongly suggests a scenario where the eggs are stored in a barn.",
  ": Rationales generated respectively by the LLMand the SLM with CoT distillation on common sensereasoning data": "reasoning ability of LLMs into SLMs (Li et al.,2023b; Ho et al., 2022). Existing works focus onCoT distillation, a method that utilizes the CoT ra-tionales of LLMs as supervision for training SLMs,rather than just labels (Shridhar et al., 2023; Wanget al., 2023b; Chen et al., 2023a; Zhao et al., 2023).The SLMs improve their reasoning capabilities byimitating the reasoning process of the LLMs. Al-though the CoT distillation method has been provento be effective, it has the following weaknesses: (1) SLM is constrained by its own capacity andthe scale of the annotated data. In the processof imitating LLM reasoning, it may learn spu-rious correlations, which leads SLM to merelyremember the keywords and patterns in thetraining data, without understanding causalfeatures. Therefore, this approach is typicallyeffective for data from the same distribution,but it struggles when dealing with OOD data.In , the rationales generated by theSLM indicate that it has learned the correla-tion between \"egg kept\" and \"freezer\" throughCoT distillation. However, during inference,it completely ignores the context and directlylinks these two phrases together, lacking adeep understanding of the text semantics.",
  "(2) Large language models typically only gener-ate rationales for options they consider worthattention, rather than from the perspective of": "small language models. As shown in ,the LLM explains why \"barn\" is the correctanswer but does not mention the other op-tions. Given the rich knowledge and reason-ing capabilities of LLMs, they may perceivethat the other three options are not substan-tial enough to provoke controversy. However,SLMs with limited capacity do not possessthis ability. Furthermore, this knowledge iscrucial for SLMs because they can utilize itthrough the process of elimination to deter-mine the answer. In the process of answeringquestions, humans also adopt a similar ap-proach. When individuals are confident that aparticular option is highly accurate, they typ-ically tend to choose directly. However, insituations where there is uncertainty about theanswer, employing the method of exclusionbecomes an effective strategy. To address these issues, we propose to enhancethe standard CoT distillation process from twoaspects respectively. In order to mitigate the re-liance of SLMs on spurious correlations (Calderonet al., 2022; Li et al., 2023a; Deng et al., 2023),we propose counterfactual data augmentation toencourage SLMs to better learn the causal structureof the text. Specifically, we utilize LLMs to edittask instances by adjusting causal relationships inthe instances or modifying text segments that arecrucial for label assignments. First, we use off-the-shelf language processing tools and LLMs toobtain the parts of the text that need to be edited.Then, prompt engineering and in-context learningare applied with LLMs to generate and filter coun-terfactual data, which closely resembles the orig-inal input but with altered answers. Additionally,we employ the multi-view CoT to introduce morediversified knowledge for SLMs, including bothpositive view CoT (PVC) and negative view CoT(NVC). PVC denotes the standard CoT generatedby LLMs when answering questions, while NVCrequires LLMs to generate refutational rationalesfor each incorrect option, explaining why the cur-rent option is considered erroneous.We conduct experiments on four question an-swering tasks that require knowledge-intensive rea-soning. Experiments demonstrate that: (1) Bothcounterfactual data augmentation and multi-viewCoT are beneficial to improving model perfor-mance. (2) On the same distribution dataset, theproposed approach significantly outperforms stan- dard CoT distillation, with an average improvementof 11.43%. (3) Across various parameter scales(ranging from 120M to 770M) and model struc-tures (from decoder-only to encoder-decoder) forsmall models, our method consistently shows im-proved performance. (4) Compared to the baselinemodel, our approach demonstrates robust gener-alization on OOD data. Furthermore, extensiveexperiments indicate that our method enhances thereasoning capabilities of SLMs.",
  "Counterfactual Data Augmentation": "Augmenting models with counterfactual data is apopular recent approach for improving model ro-bustness (Kaushik et al., 2019; Bitton et al., 2021;Khashabi et al., 2020; Wu et al., 2021; Ross et al.,2021). With the rise of LLMs, some research workshave utilized them to generate counterfactual datato improve the performance of text classification orreasoning (Dixit et al., 2022; Sachdeva et al., 2023;Li et al., 2023e). The work most similar to oursis DISCO (Chen et al., 2023b), which constructscounterfactual data by prompting LLMs to gener-ate phrase perturbations. However, DISCO is nota general counterfactual augmentation strategy, asit is only applicable to natural language inferencetasks. Moreover, DISCO modifies only a singlespan of text at a time, resulting in counterfactualdata with limited semantic diversity, which maylead to the risk of model overfitting. Differently,our method attempts to break these causal relation-ships and reconstruct them using LLMs, therebyincreasing the semantic diversity of the text. Thisis beneficial to reduce the risk of overfitting of theaugmented model and enhance its performance onOOD data.",
  "Chain-of-Thought Distillation": "LLMs have demonstrated outstanding performanceacross various downstream tasks, using in-contextexemplars or human instructions (Wang et al.,2023a; Si et al., 2023; Gu et al., 2023; Li et al.,2023c).Recent research indicates that CoTprompts can enhance the reasoning capabilities ofLLMs for complex problems (Wei et al., 2022;Wang et al., 2022). However, such benefits are onlyobservable in language models of substantial scale.Therefore, migrating CoT capabilities into SLMsthrough distillation has attracted much attention.The approach typically employs CoT prompts to If an innocent person is convicted of a crime, one of the main concerns for society would be the issue of injustice. This can erode public trust in the legal system and have far-reaching consequences for both the individual wrongfully convicted and society as a whole. Therefore, the answer is A.",
  "mask operationinput": ": Overview of the counterfactual distillation with multi-view CoT. Our approach consists of three steps:mask operation, counterfactual generation, and multi-view CoT. Initially, through the topic word and syntacticanalysis, we identify phrases in the text that may involve causal features, and replace them with the special character[MASK]. Then, given the expected answer, LLM is utilized to complete the masked text and evaluate the generatedtext to obtain a high-quality counterfactual dataset. In the end, LLM generates rationales for each option, providingsupporting evidence for the correct answer and refuting evidence for the incorrect ones. generate rationales from very large teacher models,and uses them to fine-tune small student models(Huang et al., 2022; Yang et al., 2023; Pezeshkpouret al., 2023; Li et al., 2023d; Chae et al., 2023). Byestablishing a feedback mechanism between LLMsand SLMs, small models can learn and improvetheir own shortcomings in a targeted manner (Jianget al., 2023). Furthermore, the rationales gener-ated by LLMs are closely related to the answers bycontrastive decoding, thereby effectively reducingthe reasoning errors and hallucinations that SLMsinherit from LLMs (Wang et al., 2023b). For com-plex multi-step reasoning data, semantic decompo-sition is beneficial for SLMs reasoning(Shridharet al., 2023). However, in these approaches, LLMstypically only generate rationales for options theydeem worthy of attention, rather than from the per-spective of SLMs. Our method uses multi-viewCoT to increase the diversity of rationales, allow-ing small models to learn different types of knowl-edge, which improves their performance in solvingcomplex reasoning problems. The experimental re-sults show that our method improves performanceacross multiple QA reasoning datasets, effectivelyenhancing the reasoning capabilities of SLMs.",
  "Method": "The core idea of our method is to disrupt the causalrelationship in the original text and reconstruct itusing LLM. Since the answers to the newly gen-erated questions change, SLM is able to learn thecausal differences between similar texts throughmulti-view CoT distillation, thereby enhancing itsreasoning capabilities. The overview of our ap-proach is illustrated in . In this section, weelaborate on our method and discuss the motivationbehind it.",
  "Mask Operation": "In order to disrupt the causal relationships in theoriginal text, we first need to identify phrases asso-ciated with causal features. To address this issue,we propose a method involving the topic word andsyntactic analysis to extract phrases in the text thatare relevant to causal features. Topic WordIn general, humans tend to havea clear topic when making statements to ensurethat the discussion has a clear focus and accu-rately conveys information. Similarly, we believethat there are \"topic words\" in the text, and the majority of causal features are associated withthem. As shown in , the topic word of\"What might be the result if one is convicted ofobstructing justice?\" is \"obstructing justice\". Inorder to get the topic word in the text, a promptgoes like this:",
  "In , the underlined texts are completedby LLMs with in-context learning examples": "Syntactic AnalysisIn QA, the set of possi-ble causal features is large, and relying solelyon the topic word cannot fully capture them. Asan important part of language structure, nounphrases play a key role in language expressionand communication. They are employed to in-troduce, describe, and connect concepts, therebyenhancing the expressiveness and accuracy of thetext. Therefore, noun phrases are highly likelyto constitute elements of causal features. In thispaper, we use the Stanford syntactic analysis tool1 to obtain noun phrases in the text, but in orderto preserve the elements of the original text asmuch as possible, personal pronouns and posses-sive pronouns will be retained. While maintaining the original linguistic structureof the text, we replace the topic word and nounphrases with the special character [MASK] to dis-rupt the causal relationships in the original text.Compared with the method of using LLM to per-turb a single span in the text (Chen et al., 2023b),our method aims to reduce the difficulty of counter-factual data generation and improve the semanticdiversity of the generated text.",
  "Counterfactual Generation": "As illustrated in , to prompt LLMs forcounterfactual text generation, we utilize demon-strations and instructions to construct the prompt.Each demonstration consists of four parts: ques-tions, choices, expected answers, and question com-pletion. In the context of given options, LLMs needto complete the masked question to align it with Question: Aside from [MASK] what does your [MASK] need ?Choices: A: bone B: charm C: lots of attention D: walkedExpected answer: lots of attentionCompletion Question: Aside from water and nourishment what does your dog need? Instruction: Multiple choice questions consist of questions, options and answers. Based on the above example, please complete the [MASK] part of the question to make it a multiple-choice question with smooth semantics and clear logic. Question: What might be [MASK] if [MASK] is convicted of [MASK] ?Choices: A: injustice B: criminal charges C: fear D: going to jailExpected answer: injusticeCompletion Question: What might be a concern for society if an innocent person is convicted of a crime?",
  ": The prompt of counterfactual generation": "the expected answer. Since QA data are usuallymultiple-choice questions, a request needs to bemade to LLM for each option except the answer, soeach original QA generates multiple counterfactualexamples. To obtain high-quality counterfactualdata, we design an evaluation strategy aimed atselecting the most promising counterfactual exam-ples, thereby eliminating potential errors. The strat-egy is used to verify whether the generated com-pletion questions match the new answers. Basedon the self-consistency principle of LLMs (Wanget al., 2022), we sample five reasoning paths foreach counterfactual example. If more than threepaths are consistent with the new answer, the datais retained; otherwise, it is discarded.In order to preserve the elements of the origi-nal text, our method only modifies the [MASK]part of the text while leaving the rest of the textunchanged. This enables SLMs to focus on the dif-ferences between factual and counterfactual texts,thereby allowing it to gain a profound understand-ing of how different causal features lead to diverseanswers in similar texts.",
  "Multi-view CoT": "Based on the powerful knowledge base and reason-ing capability of LLMs, they often ignore someincorrect options that are obvious to them whenanswering questions. However, these options maybe difficult for SLMs to discern. Therefore, wepropose the multi-view CoT, which enables SLMsto learn different types of knowledge by allowingLLMs to generate diverse reasoning paths fromdifferent perspectives.As shown in ,the multi-view CoT consists of two parts: thepositive view CoT (PVC) and the negative viewCoT (NVC), both generated by LLMs through in-context learning. The goal of PVC is to explorerelevant information for the correct answer, whileNVC focuses more on generating negative reason-ing paths for each option except the answer. Our",
  "Training": "The original data and counterfactual data aremixed together to form the training set. Givenaninputinstancex = (q, o, a)inthissetconsisting of a question,a set of optionsand the corresponding answer, the generatedPVC and NVC are xpvc = (q, o, rpvc, a) andxnvc = (q, o, {(ok, rknvc)}Nk=1) respectively.Inxnvc, N represents the number of options apartfrom the answer. Since the knowledge learnedby SLMs from xpvc and xnvc is different and toavoid model confusion, we use special strings toconstruct two types of data formats during modeltraining.",
  "inputnvc = q o [Elimination method] rknvc Therefore the answer would not be ok": "The represents text concatenation. We use thespecial strings \"[Direct election]\" and \"[Elimina-tion method]\" to guide the small model to generatereasoning paths that either support or refute a cer-tain option.Given a question, options, and the specialstring(st), the small model is trained to output asequence of rationale tokens concatenated withthe label tokens. In this paper, our approach in-volves fine-tuning a text-to-text language modelusing standard language modeling loss on the train-ing data.",
  "Experiments": "We study how small models can learn to reason bet-ter on four multi-step reasoning datasets: Common-senseQA (CSQA) (Talmor et al., 2018), QuaRel(Tafjord et al., 2019), ARC (Clark et al., 2018)andQASC (Khot et al., 2020). The ARC dataset isdivided into two parts: the challenge set and theeasy set. In our experiments, we combine thesetwo subsets for model training. Since the test la-bels of CSQA and QASC datasets are not public,we use the official development set as our test set. In this paper, we utilize the gpt-3.5-turbo API 2 togenerate reasoning paths. In the experiment, GPT-2(Radford et al., 2019), OPT(Zhang et al., 2022),and GPT-Neo(Black et al., 2021) are used as smallmodels, and the batch size is set to 64 during train-ing, with a total of 20 epochs. We use HuggingFacetransformers and Pytorch for the implementation.",
  "Main Results": "summarizes the accuracy of small modelsusing the proposed method compared to existingFT and FT-CoT. In different scales of models withparameter counts ranging from 120 million to 770million, our method outperforms FT-CoT. Specifi-cally, our method achieves performance improve-ments ranging from 4.17% to 23.22% across thesescales, with an average improvement of 11.43%.In the QASC and ARC, the distillation perfor-mance of our method on models with fewer pa-rameters exceeds that of models trained with more",
  "FT40.95%56.52%24.51%28.10%FT-CoT52.33%58.51%32.83%38.61%FT-MVC54.22%60.87%39.96%39.60%FT-CD-CoT57.33%60.51%45.68%44.25%Our method60.69%67.93%52.48%50.42%": ": In experiments, we test the accuracy of different methods on four reasoning datasets. And in order tofully verify the impact of small model size on distillation performance, we conduct experiments on GPT2-base,OPT-base, GPT-neo, GPT2-medium, and GPT2-large models respectively. parameters using FT-CoT. For instance, in the ARCdataset, our method achieves a performance of40.42% on GPT2-base, which is 3.27% higher thanthat of GPT2-medium trained with FT-CoT. Ad-ditionally, our method outperforms GPT2-largetrained with FT-CoT by 8.57% on GPT2-medium. FT-MVC vs FT-CoTAs shown in , thereasoning performance of FT-MVC on models ofvarious sizes exceeds that of FT-CoT. The max-imum and minimum improvements are 13.07%and 0.36% respectively, and the average improve-ment reaches 4.33%. The experimental resultsindicate that the diversified reasoning paths gen-erated by multi-view CoT enable small modelsto not only learn reasoning knowledge that sup-ports correct answers but also acquire refutationalknowledge regarding incorrect options, therebyeffectively enhancing their own reasoning capa-bilities.",
  "FT-CD-CoT vs FT-CoTCompared to FT-CoT, FT-CD-CoT shows an average improve-ment of 7.20% across four datasets. This phe-": "nomenon shows that since models with fewer pa-rameters have insufficient capacity, they may notreally learn how to reason during the fine-tuningprocess of FT-CoT, but only remember some pat-terns and keywords in the training set. In contrast,our approach forces small models to focus on thecausal relationships of text, thereby enhancingthe accuracy and generalization capability of rea-soning. The case study in the appendix providesa more intuitive explanation.",
  "OOD data": "To compare the generalization abilities of differentmethods on OOD data, we select one of the fourdatasets as the training set and the remaining threeas test sets in the experiment. From , it canbe observed that when CSQA, QASC, and ARC areused as the training sets, our method demonstratesa significant improvement compared to FT-CoT,with enhancements ranging from 3.6% to 26.6%,and an average improvement of 14.9%.However, when QuaRel is used as the trainingset, the improvement of our method is not signifi-",
  "ARC": ": The performance of different methods in out-of-distribution (OOD) scenarios. Specifically, for thesefour datasets, we select one as the training set and the other three as the test sets to validate the generalizationperformance of the proposed method. cant. There are mainly two reasons for this. Firstly,the training samples in the QuaRel dataset are rel-atively limited, consisting of only 1941 samples.Secondly, more importantly, there is a mismatchin task formats. QuaRel is a QA task with onlytwo options, while the other three datasets have nofewer than four options. Therefore, we find thatin this scenario, both the baseline model and ourmethod exhibit very low performance.",
  "Small model architectures": "The small models used in previous experimentsare all decoder-only language models. To validatethe performance of the proposed method on smallmodels with different architectures, we conduct dis-tillation experiments on encoder-decoder languagemodels. illustrates the results of differentmethods when the small model adopts Bart (Lewiset al., 2019) and T5 (Raffel et al., 2020). Comparedto the standard supervised fine-tuning baseline (FT-CoT), our method improves by 13.2% and 11.6%on BART and T5 respectively. This result fullydemonstrates that our method can consistently im-prove performance across small models of differentarchitectures, and highlights its universality and ef-fectiveness in distillation tasks.",
  ": Classification accuracy of GPT-4 on fourdatasets": "pendix includes specific prompt examples. We ran-domly select 100 data samples from each datasetfor data quality evaluation. As demonstrated in Ta-ble 3, the average classification accuracy of GPT-4across four datasets is 48%. Given that this is abinary classification task, this accuracy rate is com-parable to that of random selection, suggesting thatGPT-4 is unable to effectively distinguish betweenoriginal data and their corresponding counterfac-tual data. This result further substantiates the highquality of the counterfactual data generated by ourmethod.",
  "Causal Features": "We aim to demonstrate that the enhancement inmodel performance stems from the learning ofcausal features, not merely from the growth in datavolume. To validate this hypothesis, we have de-signed the following experiment.Specifically, we randomly select N samples fromthe CSQA dataset, which we name CSample. Sub-sequently, we randomly extract N/2 samples fromthe CSample, which we call CHalf.Based onCHalf, we construct N/2 pieces of counterfactualdata, which we label as CCD. Finally, we mergethe CHalf and CCD data to form a new dataset,which we name CMix. Therefore, the CSample andCMix datasets each contain N samples, and we thenvalidate the performance of these two datasets ondifferent models.As shown in , CMix outperforms CSam-ple across various models. This phenomenon re-veals that by contrasting the differences betweenfactual and counterfactual data, the model can ef-fectively suppress the interference of spurious cor-relations, thereby enhancing the learning of causalfeatures. It is worth noting that the improvement",
  "Non-multiple choice tasks": "In the aforementioned experiments, we mainlyfocus on multiple choice questions. To validatethe performance of the proposed method on non-multiple choice tasks, we select the SVAMP(Patelet al., 2021) and bAbI(Weston et al., 2015) datasetsfor in-depth analysis. SVAMP is a mathematicalreasoning dataset, while bAbI is a reading com-prehension dataset consisting of 20 subtasks. Toconstruct the training set, we extract 50 samplesfrom each subtask of bAbI, resulting in a total of1000 samples. Notably, since both the SVAMP andbAbI datasets do not include options, we utilizeLLM to generate possible options for these twodatasets during the training phase, thereby trans-forming them into multiple-choice tasks. As shownin , our method outperforms the baseline onboth datasets. This result not only verifies the effec-tiveness of our method on multiple-choice tasks butalso proves that it is also applicable to open-endedtasks.",
  "Limitations": "In our research, we find that the text generated byLLMs does not always align with the expected an-swers, leading us to discard some data during theevaluation phase. This indicates that further opti-mization is needed in terms of data alignment andanswer consistency. Additionally, our experimentsare limited to English datasets and single-task set-tings. To better compare with the few-shot settingsof large language models, future research can ex-plore other languages and multi-task settings.The use of closed-source large language mod-els in this study incurs additional costs. Futureresearch should explore the distillation effects onmore open-source large models to reduce costs andimprove the generalizability of the method. Ad-ditionally, the current experiments only involvequestion-answering reasoning tasks.Exploringhow to validate the methods effectiveness on moretask types is an important direction for future re-search. This work was supported by the NSFC project(No. 62072399), the Zhejiang Provincial Natu-ral Science Foundation of China under Grant No.LZ23F020009, MoE Engineering Research Centerof Digital Library, China Research Centre on Dataand Knowledge for Engineering Sciences and Tech-nology, the Fundamental Research Funds for theCentral Universities (No. 226-2024-00170), andAnt Group. We also express our sincere gratitudeto anonymous reviewers for their invaluable feed-back and constructive comments. Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz,and Michael Elhadad. 2021. Automatic generationof contrast sets from scene graphs: Probing thecompositional consistency of gqa. arXiv preprintarXiv:2103.09591. Sid Black, Leo Gao, Phil Wang, Connor Leahy, andStella Biderman. 2021. Gpt-neo: Large scale autore-gressive language modeling with mesh-tensorflow.If you use this software, please cite it using thesemetadata, 58:2. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Hailin Chen, Amrita Saha, Steven Hoi, and Shafiq Joty.2023a. Personalised distillation: Empowering open-sourced llms with adaptive learning for code genera-tion. arXiv preprint arXiv:2310.18628": "Zeming Chen, Qiyue Gao, Antoine Bosselut, AshishSabharwal, and Kyle Richardson. 2023b. Disco: dis-tilling counterfactuals with large language models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 55145528. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Xun Deng, Wenjie Wang, Fuli Feng, Hanwang Zhang,Xiangnan He, and Yong Liao. 2023. Counterfactualactive learning for out-of-distribution generalization.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 1136211377.",
  "Daniel Khashabi, Tushar Khot, and Ashish Sabharwal.2020. More bang for your buck: Natural perturba-tion for robust question answering. arXiv preprintarXiv:2004.04849": "Tushar Khot, Peter Clark, Michal Guerquin, PeterJansen, and Ashish Sabharwal. 2020.Qasc: Adataset for question answering via sentence compo-sition. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 34, pages 80828090. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-noising sequence-to-sequence pre-training for naturallanguage generation, translation, and comprehension.arXiv preprint arXiv:1910.13461.",
  "Jiaxuan Li, Lang Yu, and Allyson Ettinger. 2023a.Counterfactual reasoning: Testing language mod-els understanding of hypothetical scenarios. arXivpreprint arXiv:2305.16572": "Liunian Harold Li, Jack Hessel, Youngjae Yu, Xi-ang Ren, Kai-Wei Chang, and Yejin Choi. 2023b.Symbolic chain-of-thought distillation: Small mod-els can also\" think\" step-by-step.arXiv preprintarXiv:2306.14050. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,Yuan Ni, Guotong Xie, Xiaoling Wang, and XipengQiu. 2023c. Unified demonstration retriever for in-context learning. arXiv preprint arXiv:2305.04320.",
  "Peifeng Wang, Zhengyang Wang, Zheng Li, YifanGao, Bing Yin, and Xiang Ren. 2023b.Scott:Self-consistent chain-of-thought distillation. arXivpreprint arXiv:2305.01879": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2022. Self-consistency improves chainof thought reasoning in language models.arXivpreprint arXiv:2203.11171. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-der M Rush, Bart Van Merrinboer, Armand Joulin,and Tomas Mikolov. 2015.Towards ai-completequestion answering: A set of prerequisite toy tasks.arXiv preprint arXiv:1502.05698.",
  "Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao,and Chenghua Lin. 2023.Effective distillationof table-based reasoning ability from llms. arXivpreprint arXiv:2309.13182": "Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068. Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Ben-jamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee,Mohit Iyyer, and Andrew McCallum. 2023. Multi-stage collaborative knowledge distillation from largelanguage models. arXiv preprint arXiv:2311.08640.",
  "Unlike a spider and his many sight seers, people only have what?A: tonguesB: namesC: brainsD: feelingsE: two eyes": "Unlike a spider and his many sightseers, people only have names. Sightseers are individuals who can perceive and identify objects through their sight, whereas names are a unique characteristic that is unique to each individual. Therefore the answer would be B. Unlike a spider and his many sightseers, people only have two eyes. The combination of a spiders sensory organs, including its eyes, allows it to perceive the world in a highly visual and keenly focused manner. While other options like tongues, names, brains, and feelings are all relevant to human cognition, the combination of two eyes is a unique characteristic that sets humans apart from many other species. Therefore the answer would be E.",
  "What could go on top of wood?A: lumberyardB: synagogueC: floorD: carpetE: hardware store": "While wood can be used in various settings, such as a synagogue or a floor, it is most commonly associated with being a flooring option. Wood is strong, durable, and can provide a comfortable and durable surface for various purposes, such as carpentry or flooring. Therefore the answer would be C. If we're talking about what could physically go on top of wood, a carpet would be a suitable choice. Carpets are can be placed on top of wood to provide a soft and comfortable surface. Therefore the answer would be D.",
  ": Comparison of the results of our method andFT-CoT on commonsense reasoning data": "To better illustrate the differences between our ap-proach and FT-CoT, we present some examples incommonsense reasoning. As shown in ,FT-CoT only captures the surface correlation ofsemantics, such as \"people\" and \"names,\" \"wood\"and \"floor\". Small models typically focus on word-level associations during reasoning, rather than thecomplete semantic of the context. Differently, ourmethod focuses on deeper semantic representations,which helps small models better understand thecausal relationships within the text.",
  "BPrompt": "You are an experienced AI content detector. You will be given two multiple-choice questions. One of them is written by human and the other is written by AI. Human-written questions are usually more cohesive in logics and topics and close to real life in its scenario construction, while AI generated variations may be inconsistent in phrasing and impractical in the set-up, as well as being unusually detailed sometimes. Here are these two questions. [Question A]What is a common sign that someone is lying?a. ordering dog food onlineb. avoid eye contactc. feel guiltyd. fall asleepe. blush[Question B]What is a common sign when someone is embarrassed?a. ordering dog food onlineb. avoid eye contactc. feel guiltyd. fall asleepe. blush",
  "CMore Results": "To further verify the performance of FT-MVC andFT-CD-CoT on OOD data, we design an experi-ment similar to that in , where one of thefour datasets is selected as the training set, andthe remaining datasets are used as the test set. Asshown in , FT-MVC and FT-CD-CoT out-perform FT-CoT in most cases. Additionally, in"
}