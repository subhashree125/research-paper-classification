{
  "Abstract": "Southeast Asia (SEA) is a region characterizedby rich linguistic diversity and cultural variety,with over 1,300 indigenous languages and apopulation of 671 million people. However,the performance of contemporary AI modelsfor SEA languages is compromised by a signifi-cant lack of representation of texts, images, andauditory datasets from SEA. Evaluating mod-els for SEA languages is challenging due to thescarcity of high-quality datasets, compoundedby the predominance of English training data,which raises concerns regarding potential cul-tural misrepresentation. To address these chal-lenges, we introduce SEACrowd, a collabora-tive initiative that consolidates a comprehensiveresource hub1 to bridge the resource gap by pro-viding standardized corpora and benchmarks2 in nearly 1,000 SEA languages across threemodalities. We assess the performance of AImodels on 36 indigenous languages across 13tasks included in SEACrowd, offering valuableinsights into the current AI landscape in SEA.Furthermore, we propose strategies to facilitate",
  "Introduction": "Despite Southeast Asia (SEA) being home to 1,300indigenous languages (18% of the worlds lan-guages) and 671 million people (8.75% of theworlds population), the representation of texts,images, and audio datasets from this region issignificantly lacking in machine learning models.This deficiency adversely affects the model qual-ity for SEA languages. The language coverageof SEA languages in two common pre-trainingresources, Common Crawl3 and C4 (Xue et al.,2021), is extremely limited, with only 2.36% (in 11languages) and 10.62% (in 11 languages), respec-tively. In modalities beyond text, the representa-tion is even more limited. For instance, CommonVoice, one of the largest multilingual speech cor-pora, includes six SEA indigenous languages (Con-neau et al., 2021; Ardila et al., 2020), and LAION-5B, one of the largest multilingual vision-language (VL) corpora, includes 12 SEA indigenous lan-guages (Schuhmann et al., 2022). Datasets forother SEA indigenous languages exist, but are of-ten scattered, insufficiently documented, or variedin quality and formatting, thereby making accessand usage challenging (Cahyawijaya et al., 2023a;Joshi et al., 2020; Aji et al., 2023).In terms of evaluation, the sparse availabilityof high-quality test sets for these languages alsocomplicates evaluating models for SEA languages.Despite there being 1,300+ languages in the SEAregion, prior works (Winata et al., 2023; Cahyaw-ijaya et al., 2021; Koto and Koto, 2020; Zhanget al., 2024; Wang et al., 2024; Nguyen et al., 2023;Leong et al., 2023; Yong et al., 2023) have onlyevaluated fewer than 10 SEA languages collec-tively. The actual performance of current modelson most SEA languages remains largely unknown.Moreover, the dominance of Anglocentric train-ing data may result in cultural bias when gener-ating texts, images, or audio in underrepresentedSEA languages (Sgaard, 2022; Talat et al., 2022).Further, Durmus et al. (2023); AlKhamissi et al.(2024); Cahyawijaya et al. (2024a) have shownthat the learned representations in large languagemodels (LLMs) often fail to reflect local culturalvalues in SEA (Koto et al., 2024; Liu et al., 2024;Adilazuarda et al., 2024). This raises concernsabout the ability of current LLMs to generate natu-ral, high-quality texts for this region. In addition,the discrepancy in language support creates lan-guage barriers in technological access and risksmarginalizing minority groups who do not speakthe dominant language.In this work, we investigate the current AIprogress for SEA languages by addressing the chal-lenges of resources, evaluation, and generationquality. Our contributions are three-fold: We bridge the resource gap by centralizingand standardizing 500 corpora in nearly1,000 SEA languages in SEACrowd, a com-prehensive and standardized resource center,across three modalities: text, image, and au-dio. We close the evaluation gap in SEA languageswith the SEACrowd Benchmarks, which cover38 SEA indigenous languages on 13 tasksacross 3 modalities, providing insights intothe performance of a diverse spectrum of AImodels. Further, our study reveals that thegenerative outputs of existing LLMs exhibit acloser resemblance to translationese rather",
  "SEACrowd": "SEACrowd represents the first comprehensive AIdataset collection initiative for SEA, developedthrough a collaborative effort among researchersand engineers primarily based in the SEA re-gion. As addressed in 1, resource scarcity andthe scattered nature of the data are crucial chal-lenges in SEA. SEACrowd addresses these issuesthrough two primary contributions: 1) consolidat-ing datasheets to enhance data discoverability; and2) standardizing dataloaders for easier use, espe-cially in multiple dataset loading. We also followdata provenance practices (Longpre et al., 2023) topreserve the proprietary rights of dataset owners. Consolidating datasheetsWe invited contribu-tors to submit datasheet forms (Gebru et al., 2021)for publicly available datasets across all modalitiesincluding text, audio, and image in SEA languagesand/or cultures. These datasheets include detailedinformation about each dataset, such as data sub-set(s), description, task, language, license, URLaccess, annotation method(s), annotation valida-tion, relevant publications, publication venue, anddata splits. For each submission, we manually ver-ify and correct it as necessary to ensure datasheetaccuracy. Standardizing dataloadersFor each approveddatasheet, we created a standardized dataloaderwrapper to facilitate ready-to-use data access sinceonly 38.4% of the consolidated data sources wereoriginally hosted on Hugging Face4. To supportdiverse task types, we carefully designed the stan-dardized seacrowd schema to support differentdata structures and modalities (see Appendix F).We also adhere to data provenance practices (Long-pre et al., 2023) and document the relevant meta-data (e.g., license) in the dataloaders. Furthermore,we engaged with data owners and successfully con-verted three private datasets into public ones.These efforts have culminated in 498 datasheetsin SEACrowd Catalogue and 399 dataloaders inSEACrowd Data Hub (2.1). Notably, our cen-tralized data repository covers 1,000 SEA lan-guages, underscoring the extensive linguistic diver-sity captured by SEACrowd. We elaborate on the",
  "SEACrowd Catalogue & Data Hub": "SEACrowd comprises two interconnected plat-forms: SEACrowd Catalogue5 and SEACrowdData Hub. These platforms work in tandem to con-solidate the datasheet submissions and provide astandardized pipeline for SEACrowd. Specifically,Catalogue houses the datasheets (metadata), whileData Hub stores the standardized dataloaders andthe seacrowd library6 for the schemas and config-urations (Appendix F). These systems share infor-mation on the datasheets and dataloaders, allowingusers to seamlessly explore and utilize them.",
  "Datasets in SEACrowd": "SEACrowd consolidates 498 datasheets with di-verse tasks in SEA languages and provides stan-dardized access through dataloaders to 399 of them.As shown in , approximately 81% of thedatasets in SEACrowd are textual data, with theremaining 8% and 11% being VL and speech,respectively. The complete list of SEA indigenouslanguages covered by SEACrowd and their map-ping to the relevant SEA regions are provided in",
  "K. Around 53% of the datasets have acommercially permissive license": "A total of 83 tasks are provided in SEACrowdwith a breakdown of 66 in NLP (e.g., abusive lan-guage detection, intent classification, instructiontuning, named entity recognition, etc.), 10 in VL(image-to-text generation, sign language recog-nition, video captioning, etc.), and 7 in speech(e.g., automatic speech recognition, text-to-speech,speech emotion recognition, and others). Thesetasks are then standardized into 20 dataloaderschemas described in Appendix F. Further discus-sion regarding resources in SEACrowd is in 5.1.",
  "SEACrowd Benchmarks": "To understand the capability of state-of-the-artmodels, we conduct comprehensive evaluations ofexisting LLMs, VLMs, and speech models fromvarious architectures and training approaches. Toconstruct a benchmark suite7, we select a subsetof the dataset that has been manually annotatedand/or validated from the data presented in 2.2.More details regarding the data subsets, baselines,and prompts used for the evaluations are given inAppendix G.1, G.2, and G.3. GPT-4 Command R Mistral 7B Llama3 8B Falcon 7B mT0 XL BLOOMZ 7B BactrianX- Llama 7B AYA-23 8B AYA-101 13B SEA-LION 7B SeaLLM v2.5 7B Sailor 7B Cendol- mT5 XL Cendol-Llama2 7B Merak v4 7B WangchanX- Llama3 8B",
  "Datasets": "NLPOur natural language understanding (NLU)benchmark consists of 131 data subsets and 7tasks: sentiment analysis, topic classification, nat-ural language inference (NLI), commonsense rea-soning, exam-style multiple-choice question an-swering (QA), culture understanding, and readingcomprehension. It covers English (ENG) and 33SEA indigenous languages.We utilize 100 data subsets for the natural lan-guage generation (NLG) benchmark, which cov-ers machine translation (MT) between English andSEA languages from both directions, summariza-tion, as well as extractive or abstractive questionanswering, covering 27 SEA indigenous languages.",
  "Complete details regarding the model architectures,model sizes, seen languages, corresponding publi-cations, and other aspects are in Appendix G.2": "NLPTo evaluate the zero-shot performance ofinstruction-tuned LLMs on SEA languages, webenchmark two commercial, i.e., GPT-4 (Ope-nAI et al., 2024) and Command-R8, and 17 open-source baselines, the majority of which are 7B-13B parameters. We categorize the open-sourcebaselines according to the language(s) coverage inpre-training and/or instruction tuning, i.e., 1) En-glish: Llama3 (Touvron et al., 2023), Mistral (Jianget al., 2023), and Falcon (Almazrouei et al.,2023); 2) Multilingual: AYA-101, AYA-23 (stnet al., 2024), mT0, BLOOMZ (Muennighoff et al.,2022), and BactrianX-Llama (Li et al., 2023a);3) SEA regional: SEA-LION (Singapore, 2023),Sailor (Dou et al., 2024), and SeaLLM (Nguyenet al., 2023);and 4) SEA country-specific:",
  "from Malaysia": "SpeechWe evaluate the zero-shot performanceof state-of-the-art multilingual pre-trained speechmodels in transcribing speech in SEA languages.Specifically, we consider Whisper v3 (Radfordet al., 2023), MMS 1B (Pratap et al., 2024), andSeamless M4T v2 (Communication et al., 2023),which have shown proficiency in accurately tran-scribing multiple languages without fine-tuning.Additionally, we include models that are fine-tuned on specific language(s), SEA or English,based on 1) Wav2Vec2 XLSR (Conneau et al.,2021) and 2) XLS-R (Babu et al., 2021), known fortheir cross-lingual speech representation learningby pre-training on raw speech waveforms acrossdiverse languages, with XLS-R offering broaderlanguage coverage, and 3) Whisper, which lever-ages weakly supervised pre-training on spectro-grams of speech in diverse languages. The spe-cific fine-tuned models are evaluated: XLSR on IND, JAV, SUN; XLSR and Whisper on Indonesian(IND); XLSR and Whisper on Thai (THA); XLS-R on Tagalog (TGL); XLS-R on Burmese (MYA);XLS-R and Whisper on Khmer (KHM); and XLSRon English (ENG). See Appendix G.2 for details. VLWe consider state-of-the-art VLMs primar-ily trained on English pre-training and instruction-following data: LLaVA (Liu et al., 2023b,a), In-structBLIP (Dai et al., 2024), and Idefics2 (Lau-renon et al., 2024), and VLMs trained in a multi-lingual manner: mBLIP (Geigle et al., 2023) andPaliGemma (Gemma Team et al., 2024), to assesstheir image captioning ability in SEA languages.",
  "Experimental Settings": "We conduct all evaluations in a zero-shot fashion.We employ 3 prompt templates in English for eachNLU task and 1 for each NLG task. We utilizethe weighted F1 score to measure the model perfor-mance on NLU tasks and n-gram reference-basedmetrics, i.e., chrF++ (Popovic, 2015, 2017) andROUGE-L (Lin, 2004), on NLG tasks. As forVL, aside from a prompt template in English, wealso use a prompt template in the respective SEAindigenous language per data subset. We report",
  "English": "19.2 68.2 59.4 61.1 70.2 12.19996.4 65.1 84.2 23.5 26.1 86.5 26.9 54.8 31.1 25.7 24.82746.3 99.5 97.1 99.5 13.9 97.5 92.4 17.5 62.6 13.3 15.6 34.5 61.5 69.9 67.7 77.4 39.4 44.6 32.810042.3 35.2 27.7 84.1 31.261 36.8 38.5 22.4 26.8 48.210099.2 94.8 50.110099.9 42.8 80.49394.7 45.36236.6 34.7 46.91001009545.610099.9 50.7 79.8 92.4 93.6 19.5 68.7 56.4 53.4 64.7 27.210097.8 59.610035.9 31.5 81.4 33.5 64.2 10010010010010024.71009990.7100100100100100100 29.7 72.3 72.4 63.1 67.61010096.8 79.710089.3 30.1 95.5 28.2 65.5 10010098.19794.310010095.7 50.610099.71009761.2 74.8 10010010010010010010085.3100100100100100100100 10010010010010010010098.8 95.5 44.1100100100100100 28.810010059.5 67.9 97.49795.1 68.6 74.4 35.3 28.6 91.8 37.7 70.6 10099.91009596.110096.7 94.8 55.1 98.910095.4 95.2 90.5 92.5",
  "State-of-the-Art Models on SEA languages": "LLMsa and 2b illustrate the overallmodel performance of the LLM baselines in SEAlanguages for both NLU tasks and NLG tasks. Inour NLU evaluation, AYA-101, a large multilin-gual instruction-tuned language model covering101 languages, demonstrates the best zero-shot per-formance. It is followed by the commercial base-lines, which achieve a median of 0.6 weightedF1-score. Sailor and SeaLLM, models specificallytrained with SEA languages, also display competi-tive performance. Similarly, mT0 exhibits stronggeneralization abilities due to its exposure to 100languages in pre-training, including those from theSEA region (Muennighoff et al., 2022). In contrast,most English and SEA country-specific baselinesperform less effectively, likely due to their narrowfocus on English or a limited set of SEA languages,such as Indonesian languages for Cendol and Thaifor WangchanX-Llama3. Similar and consistenttrends are observed on MT task, while the base-lines poorer scores on abstractive/extractive QAand summarization indicate their ineffectiveness inproducing acceptable outputs in SEA languages forthese tasks, which is especially pronounced in theopen-source baselines. Appendix G.4 describes theperformance of LLMs per language.To analyze the equality in model performanceacross SEA languages, following Khanuja et al.(2023), we utilize the Gini coefficientoriginally 0.00.10.20.30.40.5 mBLIP-mT0 XL PaliGemma 3B InstructBLIP 7B LLaVA v1.5 7B LLaVA v1.6 7B Idefics2 8B Prompt lang: Eng vietha indfil 0.00.10.20.30.40.5 MultilingualEnglish Prompt lang: Fil/Ind/Tha/Vie CIDEr",
  ": Existing VLMs produce subpar image cap-tions in SEA languages. We report CIDEr (Vedantamet al., 2015)": "used to observe income equality (Dorfman, 1979)weighted by demand and parameterized by . Here, = 1 corresponds to a demographic notion ofdemand, considering language population size,while = 0 does not take population size intoaccount (Blasi et al., 2022). shows thatmodels trained on more SEA languages, such asmultilingual and SEA regional baselines, gener-ally exhibit greater language equity. For instance,although Command-R and GPT-4 are competi-tive performance-wise against AYA-101 and mT0,AYA-101 and mT0 demonstrate higher equalityacross all SEA languages under study. This trendis consistent across different (see Appendix G.5). Speech models presents the off-the-shelf speech model performance on ASR acrosslanguages in SEA, measured by the error rate per-centage. 9 of the 15 SEA languages in our speechevaluation belong to the Austronesian languagefamily.The other 6 are KHM and VIE, whichbelong to Austro-Asiatic, CNH and MYA belongto Sino-Tibetan, and THA and VIE belong to theKra-Dai language family. The multilingual pre-trained baselines have a competitive generalizationcapability across languages, although it varies bylanguage. For instance, Whisper v3 demonstratessignificantly higher effectiveness for national lan-guages such as IND, ZLM, FIL, THA, and VIE, whileperforming less optimally for other indigenous lan-guages. Conversely, Seamless M4T v2 shows amore balanced performance across the languages.Regarding fine-tuned baselines, error rates decreasefor their seen languages. The fine-tuned Whispermodels, however, manage to better optimize forthe target language while retaining their originalcapabilities in other SEA languages compared totheir Wav2Vec2 XLSR and XLS-R counterparts,despite both having been pre-trained in a multi-",
  ": Current LLMs are still incapable of generatingnatural texts in SEA languages.As spoken in SEAregions, not worldwide": "lingual manner. This observation aligns with thefindings of Rouditchenko et al. (2023), who findthat the number of hours seen per language andlanguage family during pre-training is predictiveof how the models compare, in which Whisperspre-training data duration for these four languagefamilies exceeds that of XLSR. VLMs depicts the zero-shot perfor-mance of off-the-shelf VLMs on image caption-ing in SEA indigenous languages. Despite the ca-pability of LLMs for zero-shot cross-lingual gen-eralization (Huang et al., 2021; Tckstrm et al.,2012; Neubig and Hu, 2018; Artetxe et al., 2020),VLMs trained only in English (i.e., InstructBLIP,LLaVA, and Idefics2) fail to exhibit this capabil-ity, struggling to generate adequate image captionsin SEA languages. Multilingual VL pre-trainingis crucial to achieving aligned multilingual rep-resentations (Burns et al., 2020; Li et al., 2023b;Huang et al., 2021). For instance, PaliGemma andmBLIP generate better image captions in THA and FIL when prompted in the relevant SEA languages.However, when prompted in ENG, the perfor-mance of these multilingual baselines varies no-tably. PaliGemmas performance collapses com-pletely, while mBLIPs performance shows bothincreases and decreases across different SEA lan-guages. This raises the question of whether themultilingual VLMs can maintain consistent per-formance across different languages used in theinstructions and the tasks. It highlights the need # SEA Languages <10 <100 <1K <10K <100K <1M <10M <100M <1B 1B N/A # Speakers In SEACrowd? NoYes",
  ": The resource gap in SEA in terms of language coverage, annotation quality, and cultural relevance": "for further research into the mechanisms that drivethese variations and how to achieve robust multilin-gual performance in VLMs across diverse linguisticcontexts. Understanding these dynamics is crucialfor improving VLMs generalization capabilitiesand ensuring equitable performance across all lan-guages, despite most related works focusing onmonolingual visual instruction tuning (Liu et al.,2023b; Gong et al., 2023; Zhu et al., 2024).",
  "Generation Quality in SEA Languages:Translationese vs. Natural Language": "Classifying Translationese in SEA LanguagesTo analyze the generation quality of LLMs in SEAlanguages, we build a text classifier to discrimi-nate between translationese and natural texts (Ri-ley et al., 2020). We construct a translationeseclassification training and testing dataset using 49and 62 data subsets, respectively, covering approxi-mately 39.9k and 51.5k sentences across English(ENG) and 8 SEA languages: Indonesian (IND),Khmer (KHM), Lao (LAO), Burmese (MYA), Fil-ipino (FIL), Thai (THA), Vietnamese (VIE), andMalay (ZLM). The training and test data are de-tailed in Appendix H.1.We fine-tune a classifier from mDeBER-TaV3 (He et al., 2020, 2022)10 using these data andachieve 79.08% accuracy on the test set in predict-ing translationese across these 9 languages. Thedetailed results and ablation studies of our transla-tionese classifier experiments are provided in Ap-pendix H.2. This classifier enables us to assessthe generation quality of LLMs by distinguishingbetween translationese and naturally occurring text,providing insights into the models performance inproducing authentic language output. Generation Quality of LLMsWe evaluate thegeneration quality of LLMs in 9 SEA languages bygenerating answers to natural, general, and safetyquestions from Sea-Bench (Nguyen et al., 2023).As shown in a, LLMs with extensive lan-guage coverage but less focus on SEA languages,e.g., AYA-101 (stn et al., 2024), GPT-4 (OpenAIet al., 2024), mT0 (Muennighoff et al., 2023; Xueet al., 2021), and Llama3 (AI@Meta, 2024), tend toproduce natural sentences less than 20% of the time.In contrast, models with narrower language cover-age but a greater focus on SEA languages, suchas Cendol-Llama2 (Cahyawijaya et al., 2024b),Sailor (Dou et al., 2024), AYA-23 (Aryabumi et al.,2024), and SEA-LION (Singapore, 2023), generatenatural sentences over 35% of the time.However, even the LLM with the least trans-lationese generation, SEA-LION, only producesnatural SEA sentences 57.71% of the time, high-lighting a significant quality gap in generating nat-ural sentences in SEA languages. As displayedin b, the translationese issue varies acrossSEA languages. Languages such as Tagalog (TGL),Burmese (MYA), and Malay (ZLM) have more se-vere translationese problems, with existing LLMsproducing natural sentences only 11.58%, 19.47%,and 22.24% of the time, respectively. This under-scores the need for further improvements in LLMsto more effectively address the linguistic diversityand complexity of SEA languages.",
  "Resource Gaps in SEA": "CoverageSEACrowd covers 980 out of the1,308 languages spoken in SEA (74.9%).De-spite this high coverage, language representationin SEACrowd exhibits a very long-tail distribution,with over 700 languages having only 1 or 2 datasets, and only 23 languages having 20 datasets or more.These less represented languages typically existonly in the form of lexicons (Asgari et al., 2020;List et al., 2022) or unlabeled data (Leong et al.,2022; Kudugunta et al., 2024; Nguyen et al., 2024).Existing tasks in SEACrowd still cover only a smallportion of languages. For instance, sentiment anal-ysis data is available for only 22 languages, andnamed entity recognition (NER) data is availablefor just 17 languages. Furthermore, for modalitiesbeyond text, SEA resources are extremely under-represented. Approximately 90% of SEA indige-nous languages lack both speech and VL datasets. Quality78.7% of the datasets in SEACrowd arepublished in peer-reviewed venues, and most of thedata has undergone external validation. The overallquality of the datasets in SEACrowd is depictedin b. We compile the reported data con-struction methods by the authors, considering boththe data collection method (i.e., data source) andlabel annotation validation (i.e., quality control).Nearly 19% of the datasets in SEACrowd havemachine-generated and machine-translated anno-tations, while more than 80% were obtained fromonline texts (e.g., web crawling) and expert genera-tion. In terms of label annotation validation, 62.4%of the datasets have been fully manually checked,while the remaining portion is partially validatedand automatically checked. Note that these statis-tics only provide an initial indication of datasetcollection quality on the surface and do not neces-sarily reflect the exact quality. Only a few datasets(6%) in SEACrowd report their detailed qualitymetrics (e.g., inter-annotator agreement scores). Adeeper investigation is required for future work. Cultural RelevanceThe resource gap in SEAextends to the cultural aspect, where misrepresen-tation can lead to offensive behaviors, e.g., cul-tural appropriation and stereotyping (Evans et al.,2020; Glotov, 2023). As a proxy of the culturalrelevance of SEA datasets, we manually curated259 data subsets used in SEACrowd evaluationbased on their data source. Specifically, we cate-gorize them whether they are 1) translated fromanother language, 2) crawled from local sources,or 3) hand-crafted to capture cultural relevance. Inc, approximately 70% lack cultural rele-vance, as many are machine-translated from En-glish sources. About 20% are taken from localnews, social media, or other local outlets, whichpotentially contain some culturally relevant data. Only the remaining 10% are designed to considercultural relevance, derived from studies highlight-ing serious deficiencies in cultural understandingby LLMs for underrepresented languages (Kabraet al., 2023; Koto et al., 2023a; Wibowo et al., 2023;Liu et al., 2024; Koto et al., 2024).",
  "Conclusion & Future Work": "Southeast Asia is home to highly diverse languagesand cultures; the majority of its people do not useEnglish as their primary language. The utility ofEnglish-first AI is limited for the majority of South-east Asian users, especially in critical sectors likehealthcare and education. Through SEACrowd, wehave explored the AI landscape in SEA and bridgedthe gaps in resources, evaluation, and naturalnessanalysis of AI models in SEA languages. Further,our initiative has nurtured an open-source researchcommunity, which will actively continue to addand maintain datasheets and dataloaders, as well asdrive AI research and developments in SEA.Nonetheless, AI development in SEA requiresconcentrated efforts by a range of stakeholders,who may prioritize differently when it comes toincorporating the regions 1,300+ languages intoAI models. Moving forward, our work suggests AIdevelopment in SEA should prioritize two key met-rics: 1) potential utility and 2) resource equity.11 Potential utilityPotential utility is defined asthe gap between current utility and ideal utility, inwhich model capability acts as a proxy for utility.Based on potential utility, unsurprisingly the de-velopment of the national languages (except forEnglish and Chinese used in Singapore), i.e., In-donesian (IND), Burmese (MYA), Vietnamese (VIE),Thai (THA), Filipino (FIL), Khmer (KHM), Malay(ZLM), and Lao (LAO) in , will bring thebiggest benefit. Among them, we identify notablegaps in the naturalness of Malay, Burmese, andFilipino AI-generated outputs (4.2). Focused ef-forts in resource building for these languages maymove the needle the most for utility. Beyond thenational languages, growing local languages or di-alects with large speaker bases, e.g., Javanese (JAV),Sundanese (SUN), and Hmong (HMN), is key.",
  ": SEA languages prioritization based on (top) current utility and (bottom) resource availability. Thelanguages are ranked based on the descending order of the area size of their missing potential": "level of resources. These include Northeastern Thai(TTS), Northern Thai (NOD), Hmong Do (HMV),Southern Thai (SOU), Cebuano (CEB), Ilocano(ILO), and others. Efforts to narrow these gapswould not only help preserve these languages butalso ensure the continuation of the cultural heritageof the speakers of these languages. More details onSEA language prioritization for different weight-ings of demand can be found in Appendix I.To improve these metrics, governments, and in-dustry leaders in the region should invest in R&Dactivities to improve regional language capabilityfor both the national languages and local dialects.This could include funding for open data collectionand collaborations with local communities to ad-dress the resource gap in local languages. This alsorequires long-term sustainable strategies, such ascatalyzing profitable use cases based on inclusiveAI models, promoting fair and responsible compen-sation schemes for data workers, and orchestrat-ing win-win exemplar collaborations between dataowners, AI, and application developers. We would like to thank our amazing contributors:Joshua Spergel, Tiezheng Yu, Parinthapat Pengpun,Ishan Jindal, Muhammad Satrio, Jipeng Zhang,Bhavish Pahwa, Haryo Akbarianto Wibowo, Hi-roki Nomoto, Yohanes Sigit Purnomo W.P., AhmadFathan Hidayatullah, Bryan Wilie, Ruhiyah Far-adishi Widiaputri, Rafif Rabbani, Fawwaz Mayda,Manoj Khatri, Supryadi Supryadi, Virach Sorn-lertlamvanich, Pavaris Ruangchutiphophan, ErlandHilman Fuadi, Mega Fransiska, Richardy Sapan, and Camilla Johnine Cosme, for their hard work insubmitting datasheets and implementing dataload-ers for SEACrowd.This work is supported by the National ResearchFoundation, Singapore under its AI Singapore Pro-gramme; PhD Fellowship Award, the Hong KongUniversity of Science and Technology; and PF20-43679 Hong Kong PhD Fellowship Scheme, Re-search Grant Council, Hong Kong. JMI is fundedby National University Philippines and the UKRICentre for Doctoral Training in Accountable, Re-sponsible and Transparent AI [EP/S023437/1] ofthe University of Bath. In addition, we would liketo express our gratitude to Cohere For AI for provid-ing research grants that enabled us to conduct exper-iments using a commercial baseline, Command-R.",
  "Limitations": "While our work covers nearly 1,000 SEA lan-guages, many dialects, which are considered as be-longing to a parent language, are missing from ourevaluation benchmark. For instance, for the Malaylanguage, only Standard Malay (ZSM) is evaluated,but not other dialects such as Sarawak Malay (ZLM- SAR). Furthermore, the majority of our datasetsalso do not contain code-switched texts, which is acommon linguistic phenomenon of SEA languageusage (Aji et al., 2023). Moreover, the languagecoverage of different evaluation tasks varies signifi-cantly. For instance, NLP tasks cover 34 languagesin total, whereas VL tasks only cover 4 languages.Tackling these limitations is essential to achievinga better representation of SEA, and we stronglyencourage future works to prioritise these aspects.",
  "Ethics Statement": "In developing an evaluation benchmark for SEAlanguages, we have taken several steps to ensureethical considerations are addressed comprehen-sively. First, the data used for this benchmark issourced from publicly available resources, ensuringcompliance with legal and ethical standards regard-ing data privacy. Where applicable, explicit consentwas obtained from data contributors. Furthermore,all the datasets and resources utilized in this bench-mark are used in accordance with their respectivelicenses. Second, our benchmark aims to be inclu-sive, representing a wide range of SEA languages,including those that are underrepresented in cur-rent linguistic resources. Lastly, our research pro-cess, including data collection, benchmark devel-opment, and evaluation methodologies, is entirelyopen-sourced and is documented transparently toenable reproducibility and accountability. David Adelani, Jesujoba Alabi, Angela Fan, JuliaKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,Dietrich Klakow, Peter Nabende, Ernie Chang, Tajud-deen Gwadabe, Freshia Sackey, Bonaventure F. P.Dossou, Chris Emezue, Colin Leong, Michael Beuk-man, Shamsuddeen Muhammad, Guyo Jarso, OreenYousuf, Andre Niyongabo Rubungo, Gilles Hacheme,Eric Peter Wairagala, Muhammad Umair Nasir, Ben-jamin Ajibade, Tunde Ajayi, Yvonne Gitau, JadeAbbott, Mohamed Ahmed, Millicent Ochieng, An-uoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,Fatoumata Ouoba Kabore, Godson Kalipe, DergueneMbaye, Allahsera Auguste Tapo, Victoire Memd-jokam Koagne, Edwin Munkoh-Buabeng, Valen-cia Wagner, Idris Abdulmumin, Ayodele Awokoya,Happy Buzaaba, Blessing Sibanda, Andiswa Bukula,and Sam Manthalu. 2022a. A few thousand trans-lations go a long way! leveraging pre-trained mod-els for African news translation. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 30533070,Seattle, United States. Association for ComputationalLinguistics. David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassi-lyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, andEn-Shiun Lee. 2024. SIB-200: A simple, inclusive,and big evaluation dataset for topic classification in200+ languages and dialects. In Proceedings of the18th Conference of the European Chapter of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 226245, St. Julians, Malta.Association for Computational Linguistics.",
  "David Adelani, Graham Neubig, Sebastian Ruder,Shruti Rijhwani, Michael Beukman, Chester Palen-": "Michel, Constantine Lignos, Jesujoba Alabi, Sham-suddeen Muhammad,Peter Nabende,CheikhM. Bamba Dione, Andiswa Bukula, RooweitherMabuya, Bonaventure F. P. Dossou, Blessing Sibanda,Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe,Derguene Mbaye, Amelia Taylor, Fatoumata Kabore,Chris Chinenye Emezue, Anuoluwapo Aremu, PerezOgayo, Catherine Gitau, Edwin Munkoh-Buabeng,Victoire Memdjokam Koagne, Allahsera AugusteTapo, Tebogo Macucwa, Vukosi Marivate, Mbon-ing Tchiaze Elvis, Tajuddeen Gwadabe, TosinAdewumi, Orevaoghene Ahia, and Joyce Nakatumba-Nabende. 2022b. MasakhaNER 2.0: Africa-centrictransfer learning for named entity recognition. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 44884508, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. David Ifeoluwa Adelani, Jade Abbott, Graham Neu-big, Daniel Dsouza, Julia Kreutzer, Constantine Lig-nos, Chester Palen-Michel, Happy Buzaaba, ShrutiRijhwani, Sebastian Ruder, Stephen Mayhew, Is-rael Abebe Azime, Shamsuddeen H. Muhammad,Chris Chinenye Emezue, Joyce Nakatumba-Nabende,Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau,Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-mam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,Rubungo Andre Niyongabo, Jonathan Mukiibi, Ver-rah Otiende, Iroro Orife, Davis David, Samba Ngom,Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,Gerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-wuneke, Nkiruka Odu, Eric Peter Wairagala, SamuelOyerinde, Clemencia Siro, Tobius Saul Bateesa,Temilola Oloyede, Yvonne Wambui, Victor Akin-ode, Deborah Nabagereka, Maurice Katusiime, Ayo-dele Awokoya, Mouhamadane MBOUP, Dibora Ge-breyohannes, Henok Tilaye, Kelechi Nwaike, De-gaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-vaoghene Ahia, Bonaventure F. P. Dossou, KelechiOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,Adewale Akinfaderin, Tendai Marengereke, and Sa-lomey Osei. 2021.MasakhaNER: Named entityrecognition for African languages.Transactionsof the Association for Computational Linguistics,9:11161131. David Ifeoluwa Adelani, Marek Masiak, Israel AbebeAzime, Jesujoba Alabi, Atnafu Lambebo Tonja,Christine Mwase, Odunayo Ogundepo, BonaventureF. P. Dossou, Akintunde Oladipo, Doreen Nixdorf,Chris Chinenye Emezue, Sana Al-azzawi, BlessingSibanda, Davis David, Lolwethu Ndolela, JonathanMukiibi, Tunde Ajayi, Tatiana Moteu, Brian Odhi-ambo, Abraham Owodunni, Nnaemeka Obiefuna,Muhidin Mohamed, Shamsuddeen Hassan Muham-mad, Teshome Mulugeta Ababu, Saheed Abdul-lahi Salahudeen, Mesay Gemeda Yigezu, Tajud-deen Gwadabe, Idris Abdulmumin, Mahlet Taye,Oluwabusayo Awoyomi, Iyanuoluwa Shode, Tolu-lope Adelani, Habiba Abdulganiyu, Abdul-HakeemOmotayo, Adetola Adeeko, Abeeb Afolabi, An-uoluwapo Aremu, Olanrewaju Samuel, ClemenciaSiro, Wangari Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo, Jes-sica Ojo, Oyinkansola Awosan, Tadesse Kebede,Toadoum Sari Sakayo, Pamela Nyatsine, Freed-more Sidume, Oreen Yousuf, Mardiyyah Odu-wole, Kanda Tshinu, Ussen Kimanuka, ThinaDiko, Siyanda Nxakama, Sinodos Nigusse, Ab-dulmejid Johar, Shafie Mohamed, Fuad Mire Has-san, Moges Ahmed Mehamed, Evrard Ngabire,Jules Jules, Ivan Ssenkungu, and Pontus Stenetorp.2023. MasakhaNEWS: News topic classification forAfrican languages. In Proceedings of the 13th In-ternational Joint Conference on Natural LanguageProcessing and the 3rd Conference of the Asia-PacificChapter of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 144159,Nusa Dua, Bali. Association for Computational Lin-guistics.",
  "AI@Meta. 2024. Llama 3 model card": "Alham Fikri Aji, Jessica Zosa Forde, Alyssa Marie Loo,Lintang Sutawika, Skyler Wang, Genta Indra Winata,Zheng-Xin Yong, Ruochen Zhang, A. Seza Dogruz,Yin Lin Tan, and Jan Christian Blaise Cruz. 2023.Current status of NLP in south East Asia with in-sights from multilingualism and language diversity.In Proceedings of the 13th International Joint Con-ference on Natural Language Processing and the3rd Conference of the Asia-Pacific Chapter of theAssociation for Computational Linguistics: TutorialAbstract, pages 813, Nusa Dua, Bali. Associationfor Computational Linguistics. Alham Fikri Aji, Genta Indra Winata, Fajri Koto,Samuel Cahyawijaya, Ade Romadhony, Rahmad Ma-hendra, Kemal Kurniawan, David Moeljadi, Radi-tyo Eko Prasojo, Timothy Baldwin, Jey Han Lau,and Sebastian Ruder. 2022. One country, 700+ lan-guages: NLP challenges for underrepresented lan-guages and dialects in Indonesia. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 72267249, Dublin, Ireland. Association forComputational Linguistics.",
  "Noune, Baptiste Pannier, and Guilherme Penedo.2023. Falcon-40B: an open large language modelwith state-of-the-art performance": "Rosana Ardila, Megan Branson, Kelly Davis, MichaelKohler, Josh Meyer, Michael Henretty, ReubenMorais, Lindsay Saunders, Francis Tyers, and Gre-gor Weber. 2020.Common voice: A massively-multilingual speech corpus. In Proceedings of theTwelfth Language Resources and Evaluation Confer-ence, pages 42184222, Marseille, France. EuropeanLanguage Resources Association. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.2020. On the cross-lingual transferability of mono-lingual representations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46234637, Online. Associationfor Computational Linguistics. Viraat Aryabumi, John Dang, Dwarak Talupuru,Saurabh Dash, David Cairuz, Hangyu Lin, BharatVenkitesh, Madeline Smith, Kelly Marchisio, Se-bastian Ruder, Acyr Locatelli, Julia Kreutzer, NickFrosst, Phil Blunsom, Marzieh Fadaee, Ahmet stn,and Sara Hooker. 2024. Aya 23: Open weight re-leases to further multilingual progress.Preprint,arXiv:2405.15032. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu,Terra Blevins, Hila B Gonen, Machel Reid, YuliaTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.2023. BUFFET: Benchmarking large language mod-els for cross-lingual few-shot transfer.Preprint,arXiv:2305.14857. Ehsaneddin Asgari, Fabienne Braune, Benjamin Roth,Christoph Ringlstetter, and Mohammad Mofrad.2020. UniSent: Universal adaptable sentiment lex-ica for 1000+ languages.In Proceedings of theTwelfth Language Resources and Evaluation Confer-ence, pages 41134120, Marseille, France. EuropeanLanguage Resources Association.",
  "Code-mixed sentiment analysis using transformerfor twitter social media data. International Journalof Advanced Computer Science and Applications,14(10)": "Arun Babu, Changhan Wang, Andros Tjandra, KushalLakhotia, Qiantong Xu, Naman Goyal, Kritika Singh,Patrick von Platen, Yatharth Saraf, Juan Pino, AlexeiBaevski, Alexis Conneau, and Michael Auli. 2021.Xls-r: Self-supervised cross-lingual speech represen-tation learning at scale. Preprint, arXiv:2111.09296. Lucas Bandarkar, Davis Liang, Benjamin Muller, MikelArtetxe, Satya Narayan Shukla, Donald Husa, NamanGoyal, Abhinandan Krishnan, Luke Zettlemoyer, andMadian Khabsa. 2023. The belebele benchmark: aparallel reading comprehension dataset in 122 lan-guage variants. arXiv preprint arXiv:2308.16884. Damian Blasi, Antonios Anastasopoulos, and Gra-ham Neubig. 2022. Systematic inequalities in lan-guage technology performance across the worldslanguages. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 54865505, Dublin,Ireland. Association for Computational Linguistics. Andrea Burns, Donghyun Kim, Derry Wijaya, KateSaenko, and Bryan A Plummer. 2020.Learn-ing to scale multilingual representations for vision-language tasks. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 197213.Springer. Samuel Cahyawijaya, Alham Fikri Aji, Holy Lovenia,Genta Indra Winata, Bryan Wilie, Rahmad Mahendra,Fajri Koto, David Moeljadi, Karissa Vincentio, AdeRomadhony, and Ayu Purwarianti. 2022. Nusacrowd:A call for open and reproducible nlp research in in-donesian languages. Preprint, arXiv:2207.10524. Samuel Cahyawijaya, Delong Chen, Yejin Bang, LeilaKhalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, andPascale Fung. 2024a. High-dimension human valuerepresentation in large language models.arXivpreprint arXiv:2404.07900. Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji,Genta Winata, Bryan Wilie, Fajri Koto, RahmadMahendra, Christian Wibisono, Ade Romadhony,Karissa Vincentio, Jennifer Santoso, David Moel-jadi, Cahya Wirawan, Frederikus Hudi, Muham-mad Satrio Wicaksono, Ivan Parmonangan, Ika Al-fina, Ilham Firdausi Putra, Samsul Rahmadani, Yu-lianti Oenang, Ali Septiandri, James Jaya, KaustubhDhole, Arie Suryani, Rifki Afina Putri, Dan Su, KeithStevens, Made Nindyatama Nityasya, MuhammadAdilazuarda, Ryan Hadiwijaya, Ryandito Diandaru,Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu,Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ich-wanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Gra-ham Neubig, Timothy Baldwin, Sebastian Ruder, Pas-cale Fung, Herry Sujaini, Sakriani Sakti, and Ayu Pur-warianti. 2023a. NusaCrowd: Open source initiativefor Indonesian NLP resources. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 1374513818, Toronto, Canada. Associationfor Computational Linguistics. Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, DeaAdhista, Emmanuel Dave, Sarah Oktavianti, SalsabilAkbar, Jhonson Lee, Nuur Shadieq, Tjeng WawanCenggoro, Hanung Linuwih, Bryan Wilie, GalihMuridan,Genta Winata,David Moeljadi,Al-ham Fikri Aji, Ayu Purwarianti, and Pascale Fung.2023b.NusaWrites:Constructing high-qualitycorpora for underrepresented and extremely low-resource languages. In Proceedings of the 13th In-ternational Joint Conference on Natural LanguageProcessing and the 3rd Conference of the Asia-PacificChapter of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 921945,",
  "Nusa Dua, Bali. Association for Computational Lin-guistics": "Samuel Cahyawijaya, Holy Lovenia, Fajri Koto,Rifki Afina Putri, Emmanuel Dave, Jhonson Lee,Nuur Shadieq, Wawan Cenggoro, Salsabil MaulanaAkbar, Muhammad Ihza Mahendra, Dea AnnisayantiPutri, Bryan Wilie, Genta Indra Winata, Alham FikriAji, Ayu Purwarianti, and Pascale Fung. 2024b. Cen-dol: Open instruction-tuned generative large lan-guage models for indonesian languages. Preprint,arXiv:2404.06138. Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,Karissa Vincentio, Xiaohong Li, Adhiguna Kun-coro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-har, Masayu Khodra, Ayu Purwarianti, and PascaleFung. 2021. IndoNLG: Benchmark and resources forevaluating Indonesian natural language generation.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages88758898, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics.",
  "Jasper Kyle Catapang and Moses Visperas. 2023": "Emotion-based morality in Tagalog and English sce-narios (EMoTES-3K): A parallel corpus for explain-ing (im)morality of actions. In Proceedings of theJoint 3rd International Conference on Natural Lan-guage Processing for Digital Humanities and 8thInternational Workshop on Computational Linguis-tics for Uralic Languages, pages 16, Tokyo, Japan.Association for Computational Linguistics. Seamless Communication, Loc Barrault, Yu-An Chung,Mariano Cora Meglioli, David Dale, Ning Dong,Paul-Ambroise Duquenne, Hady Elsahar, HongyuGong, Kevin Heffernan, John Hoffman, ChristopherKlaiber, Pengwei Li, Daniel Licht, Jean Maillard,Alice Rakotoarison, Kaushik Ram Sadagopan, Guil-laume Wenzek, Ethan Ye, Bapi Akula, Peng-JenChen, Naji El Hachem, Brian Ellis, Gabriel MejiaGonzalez, Justin Haaheim, Prangthip Hansanti, RussHowes, Bernie Huang, Min-Jae Hwang, Hirofumi In-aguma, Somya Jain, Elahe Kalbassi, Amanda Kallet,Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Rus-lan Mavlyutov, Benjamin Peloquin, Mohamed Ra-madan, Abinesh Ramakrishnan, Anna Sun, KevinTran, Tuan Tran, Igor Tufanov, Vish Vogeti, CarleighWood, Yilin Yang, Bokai Yu, Pierre Andrews, CanBalioglu, Marta R. Costa-juss, Onur Celebi, MahaElbayad, Cynthia Gao, Francisco Guzmn, JustineKao, Ann Lee, Alexandre Mourachko, Juan Pino,Sravya Popuri, Christophe Ropers, Safiyyah Saleem,Holger Schwenk, Paden Tomasello, Changhan Wang,Jeff Wang, and Skyler Wang. 2023. Seamlessm4t:Massively multilingual & multimodal machine trans-lation. Preprint, arXiv:2308.11596. Alexis Conneau, Alexei Baevski, Ronan Collobert, Ab-delrahman Mohamed, and Michael Auli. 2021. Un-supervised Cross-Lingual Representation Learningfor Speech Recognition. In Proc. Interspeech 2021,pages 24262430. Alexis Conneau, Ankur Bapna, Yu Zhang, Min Ma,Patrick von Platen, Anton Lozhkov, Colin Cherry,Ye Jia, Clara Rivera, Mihir Kale, Daan van Esch, VeraAxelrod, Simran Khanuja, Jonathan Clark, OrhanFirat, Michael Auli, Sebastian Ruder, Jason Riesa,and Melvin Johnson. 2022. XTREME-S: Evaluat-ing Cross-lingual Speech Representations. In Proc.Interspeech 2022, pages 32483252. Alexis Conneau, Ruty Rinott, Guillaume Lample, AdinaWilliams, Samuel Bowman, Holger Schwenk, andVeselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 24752485, Brus-sels, Belgium. Association for Computational Lin-guistics. Marta R. Costa-juss, James Cross, Onur elebi,Maha Elbayad, Kenneth Heafield, Kevin Heffer-nan, Elahe Kalbassi, Janice Lam, Daniel Licht,Jean Maillard, Anna Sun, Skyler Wang, GuillaumeWenzek, Al Youngblood, Bapi Akula, Loic Bar-rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik RamSadagopan, Dirk Rowe, Shannon Spruit, ChauTran, Pierre Andrews, Necip Fazil Ayan, ShrutiBhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Goswami, Francisco Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, Jeff Wang, andN. L. L. B. Team. 2024. Scaling neural machinetranslation to 200 languages. Nature. Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan,Ratish Puduppully, Mitesh Khapra, and Pratyush Ku-mar. 2022. IndicBART: A pre-trained model for indicnatural language generation. In Findings of the As-sociation for Computational Linguistics: ACL 2022,pages 18491863, Dublin, Ireland. Association forComputational Linguistics. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo,Thuat Nguyen, Franck Dernoncourt, Ryan A Rossi,and Thien Huu Nguyen. 2023. Okapi: Instruction-tuned large language models in multiple languageswith reinforcement learning from human feedback.arXiv e-prints, pages arXiv2307. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36.",
  "David M. Eberhard, Gary F. Simons, and Charles D.Fennig. 2021. Ethnologue: Languages of the World.Twenty-fourth edition. Dallas, Texas: SIL Interna-tional": "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, JohnOrtega, Ricardo Ramos, Annette Rios, Ivan VladimirMeza Ruiz, Gustavo Gimnez-Lugo, ElisabethMager, Graham Neubig, Alexis Palmer, RolandoCoto-Solano, Thang Vu, and Katharina Kann. 2022.AmericasNLI: Evaluating zero-shot natural languageunderstanding of pretrained multilingual models intruly low-resource languages. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages62796299, Dublin, Ireland. Association for Compu-tational Linguistics.",
  "Gregor Geigle, Abhay Jain, Radu Timofte, and GoranGlava. 2023. mblip: Efficient bootstrapping of mul-tilingual vision-llms. arXiv, abs/2307.06930": "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy. Preprint, arXiv:2403.08295.",
  "Po-Yao Huang, Mandela Patrick, Junjie Hu, GrahamNeubig, Florian Metze, and Alexander Hauptmann": "2021. Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models.In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 24432459, Online. Association for Computa-tional Linguistics. Tin Van Huynh, Kiet Van Nguyen, and Ngan Luu-Thuy Nguyen. 2022. ViNLI: A Vietnamese corpusfor studies on open-domain natural language infer-ence. In Proceedings of the 29th International Con-ference on Computational Linguistics, pages 38583872, Gyeongju, Republic of Korea. InternationalCommittee on Computational Linguistics.",
  "Muhammad Ichsan. 2023. Merak-7b: The llm for ba-hasa indonesia. Hugging Face Repository": "Joseph Marvin Imperial, Jeyrome Orosco, Shiela MaeMazo, and Lany Maceda. 2019. Sentiment analysisof typhoon related tweets using standard and bidi-rectional recurrent neural networks. arXiv preprintarXiv:1908.01765. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Shengyi Jiang, Sihui Fu, Nankai Lin, and Yingwen Fu.2022. Pretrained models and evaluation data for thekhmer language. Tsinghua Science and Technology,27(4):709718": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293, Online. Association for ComputationalLinguistics. Sarah Samson Juan, Laurent Besacier, Benjamin Lecou-teux, and Mohamed Dyab. 2015. Using resourcesfrom a closely-related language to develop asr fora very under-resourced language: A case study foriban. In Proceedings of INTERSPEECH, Dresden,Germany. Anubha Kabra, Emmy Liu, Simran Khanuja, Al-ham Fikri Aji, Genta Winata, Samuel Cahyawijaya,Anuoluwapo Aremu, Perez Ogayo, and Graham Neu-big. 2023. Multi-lingual and multi-cultural figurativelanguage understanding. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 82698284, Toronto, Canada. Association forComputational Linguistics. Divyanshu Kakwani, Anoop Kunchukuttan, SatishGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.Khapra, and Pratyush Kumar. 2020. IndicNLPSuite:Monolingual corpora, evaluation benchmarks andpre-trained multilingual language models for Indian",
  "languages. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 49484961, Online. Association for Computational Lin-guistics": "Ichwanul Muslim Karo Karo, Mohd Farhan Md Fudzee,Shahreen Kasim, and Azizul Azhar Ramli. 2022.Sentiment analysis in karonese tweet using machinelearning. Indonesian Journal of Electrical Engineer-ing and Informatics (IJEEI), 10(1):219231. Simran Khanuja, Sebastian Ruder, and Partha Talukdar.2023. Evaluating the diversity, equity, and inclu-sion of NLP technology: A case study for Indianlanguages. In Findings of the Association for Compu-tational Linguistics: EACL 2023, pages 17631777,Dubrovnik, Croatia. Association for ComputationalLinguistics. Fajri Koto, Nurul Aisyah, Haonan Li, and Timothy Bald-win. 2023a. Large language models only pass pri-mary school exams in Indonesia: A comprehensivetest on IndoMMLU. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Singapore. Associationfor Computational Linguistics. Fajri Koto, Nurul Aisyah, Haonan Li, and Timothy Bald-win. 2023b. Large language models only pass pri-mary school exams in Indonesia: A comprehensivetest on IndoMMLU. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1235912374, Singapore.Association for Computational Linguistics.",
  "Fajri Koto, Timothy Baldwin, and Jey Han Lau. 2022": "Cloze evaluation for deeper understanding of com-monsense stories in Indonesian. In Proceedings ofthe First Workshop on Commonsense Representationand Reasoning (CSRR 2022), pages 816, Dublin,Ireland. Association for Computational Linguistics. Fajri Koto and Ikhwan Koto. 2020. Towards computa-tional linguistics in Minangkabau language: Studieson sentiment analysis and machine translation. InProceedings of the 34th Pacific Asia Conference onLanguage, Information and Computation, pages 138148, Hanoi, Vietnam. Association for ComputationalLinguistics. Fajri Koto, Rahmad Mahendra, Nurul Aisyah, andTimothy Baldwin. 2024.Indoculture: Exploringgeographically-influenced cultural commonsense rea-soning across eleven indonesian provinces. Preprint,arXiv:2404.01854. Sneha Kudugunta, Isaac Caswell, Biao Zhang, XavierGarcia, Derrick Xin, Aditya Kusupati, Romi Stella,Ankur Bapna, and Orhan Firat. 2024.Madlad-400: a multilingual and document-level large auditeddataset. In Proceedings of the 37th InternationalConference on Neural Information Processing Sys-tems, NIPS 23, Red Hook, NY, USA. Curran Asso-ciates Inc. Aman Kumar, Himani Shrotriya, Prachi Sahu, AmoghMishra, Raj Dabre, Ratish Puduppully, AnoopKunchukuttan, Mitesh M. Khapra, and Pratyush Ku-mar. 2022.IndicNLG benchmark: Multilingualdatasets for diverse NLG tasks in Indic languages.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages53635394, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "Hugo Laurenon, Lo Tronchon, Matthieu Cord,and Victor Sanh. 2024.What matters whenbuilding vision-language models?Preprint,arXiv:2405.02246": "Thang Le and Anh Luu. 2023. A parallel corpus forVietnamese central-northern dialect text transfer. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 1383913855, Singa-pore. Association for Computational Linguistics. Colin Leong, Joshua Nemecek, Jacob Mansdorfer, AnnaFilighera, Abraham Owodunni, and Daniel White-nack. 2022. Bloom library: Multimodal datasets in300+ languages for a variety of downstream tasks.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages86088621, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Wei Qi Leong, Jian Gang Ngui, Yosephine Su-santo, Hamsawardhini Rengarajan, KengatharaiyerSarveswaran, and William Chandra Tjhi. 2023.Bhasa: A holistic southeast asian linguistic andcultural evaluation suite for large language models.arXiv preprint arXiv:2309.06085. Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,and Timothy Baldwin. 2023a. Bactrian-x: A multilin-gual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011. Zejun Li, Zhihao Fan, Jingjing Chen, Qi Zhang, Xu-anjing Huang, and Zhongyu Wei. 2023b.Unify-ing cross-lingual and cross-modal modeling towardsweakly supervised multilingual vision-language pre-training. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 59395958, Toronto,Canada. Association for Computational Linguistics.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, TianluWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-man Goyal, Shruti Bhosale, Jingfei Du, RamakanthPasunuru, Sam Shleifer, Punit Singh Koura, VishravChaudhary, Brian OHoro, Jeff Wang, Luke Zettle-moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-anov, and Xian Li. 2022. Few-shot learning withmultilingual generative language models. In Proceed-ings of the 2022 Conference on Empirical Methods",
  "in Natural Language Processing, pages 90199052,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics": "Johann-Mattis List, Robert Forkel, Simon J. Greenhill,Christoph Rzymski, Johannes Englisch, and Rus-sell D. Gray. 2022. Lexibank, a public repositoryof standardized wordlists with computed phonologi-cal and lexical features. Scientific Data, 9(1):316. Chen Cecilia Liu, Fajri Koto, Timothy Baldwin,and Iryna Gurevych. 2024.Are multilingualllms culturally-diverse reasoners? an investigationinto multicultural proverbs and sayings. Preprint,arXiv:2309.08591.",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Manuel Mager, Arturo Oncevay, Annette Rios, IvanVladimir Meza Ruiz, Alexis Palmer, Graham Neubig,and Katharina Kann, editors. 2021. Proceedings ofthe First Workshop on Natural Language Processingfor Indigenous Languages of the Americas. Associa-tion for Computational Linguistics, Online. Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan,Fahrurrozi Rahman, and Clara Vania. 2021. IndoNLI:A natural language inference dataset for Indonesian.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages1051110527, Online and Punta Cana, DominicanRepublic. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-ley Schoelkopf, Xiangru Tang, Dragomir Radev,Alham Fikri Aji, Khalid Almubarak, Samuel Al-banie, Zaid Alyafeai, Albert Webson, Edward Raff,and Colin Raffel. 2023.Crosslingual generaliza-tion through multitask finetuning. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1599116111, Toronto, Canada. Associationfor Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng-Xin Yong, HaileySchoelkopf, et al. 2022. Crosslingual generaliza-tion through multitask finetuning. arXiv preprintarXiv:2211.01786.",
  "Aad Muzad and Faisal Rahutomo. 2016. Korpus beritadaring bahasa indonesia dengan depth first focusedcrawling. Prosiding Sentrinov (Seminar NasionalTerapan Riset Inovatif), 2(1):1120": "Graham Neubig and Junjie Hu. 2018. Rapid adapta-tion of neural machine translation to new languages.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages875880, Brussels, Belgium. Association for Com-putational Linguistics. Kiet Nguyen, Vu Nguyen, Anh Nguyen, and NganNguyen. 2020. A Vietnamese dataset for evaluatingmachine reading comprehension. In Proceedings ofthe 28th International Conference on ComputationalLinguistics, pages 25952605, Barcelona, Spain (On-line). International Committee on Computational Lin-guistics. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai,Hieu Man, Nghia Trung Ngo, Franck Dernoncourt,Ryan A. Rossi, and Thien Huu Nguyen. 2024. Cul-turaX: A cleaned, enormous, and multilingual datasetfor large language models in 167 languages. In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 42264237, Torino, Italia. ELRA and ICCL. Xuan-Phi Nguyen, Wenxuan Zhang, Li Xin, MahaniAljunied, Weiwen Xu, Hou Pong Chan, ZhiqiangHu, Chenhui Shen, Yew Ken Chia, Xingxuan Li,Jianyu Wang, Qingyu Tan, Liying Cheng, GuanzhengChen, Yue Deng, Sen Yang, Chaoqun Liu, HangZhang, and Lidong Bing. 2023.Seallms - largelanguage models for southeast asia.Preprint,arXiv:arXiv:2312.00738. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim- ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and Bar-ret Zoph. 2024. Gpt-4 technical report. Preprint,arXiv:2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744.",
  "Chester Palen-Michel and Constantine Lignos. 2023": "LR-sum:Summarization for less-resourced lan-guages. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 68296844,Toronto, Canada. Association for Computational Lin-guistics. Wannaphong Phatthiyaphaibun, Korakot Chaovavanich,Charin Polpanumas, Arthit Suriyawongkul, LalitaLowphansirikul, Pattarawat Chormai, Peerat Limkon-chotiwat, Thanathip Suntorntip, and Can Udom-charoenchaikit. 2023. PyThaiNLP: Thai natural lan-guage processing in python. In Proceedings of the3rd Workshop for Natural Language Processing OpenSource Software (NLP-OSS 2023), pages 2536, Sin-gapore. Association for Computational Linguistics. Wannaphong Phatthiyaphaibun, Surapon Nonesung,Patomporn Payoungkhamdee, Peerat Limkonchoti-wat, Can Udomcharoenchaikit, Jitkapat Sawat-phol,ChompakornChaksangchaichot,EkapolChuangsuwanich, and Sarana Nutanong. 2024.Wangchanlion and wangchanx mrc eval. Preprint,arXiv:2403.16127. Edoardo Maria Ponti, Goran Glava, Olga Majewska,Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020.XCOPA: A multilingual dataset for causal common-sense reasoning. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 23622376, Online. As-sociation for Computational Linguistics. Maja Popovic. 2015. chrF: character n-gram F-scorefor automatic MT evaluation. In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 392395, Lisbon, Portugal. Association forComputational Linguistics. Maja Popovic. 2017. chrF++: words helping charac-ter n-grams. In Proceedings of the Second Confer-ence on Machine Translation, pages 612618, Copen-hagen, Denmark. Association for Computational Lin-guistics. Vineel Pratap, Andros Tjandra, Bowen Shi, PadenTomasello, Arun Babu, Sayani Kundu, Ali Elkahky,Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi,Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-NingHsu, Alexis Conneau, and Michael Auli. 2024. Scal-ing speech technology to 1,000+ languages. Journalof Machine Learning Research, 25(97):152.",
  "Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nak-agawa. 2007. A machine learning approach for In-donesian question answering system. In ArtificialIntelligence and Applications, pages 573578": "I Made Suwija Putra, Daniel Siahaan, and AhmadSaikhu. 2024. Snli indo: A recognizing textual entail-ment dataset in indonesian derived from the stanfordnatural language inference dataset. Data in Brief,52:109998. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the 38th InternationalConference on Machine Learning, volume 139 ofProceedings of Machine Learning Research, pages87488763. PMLR. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine Mcleavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In Proceedings of the 40th InternationalConference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages2849228518. PMLR.",
  "Riccosan and Karen Etania Saputra. 2023. Multilabelmulticlass sentiment and emotion dataset from in-donesian mobile application review. Data in Brief,50:109576": "Parker Riley, Isaac Caswell, Markus Freitag, and DavidGrangier. 2020. Translationese as a language in mul-tilingual NMT. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 77377746, Online. Association forComputational Linguistics. Muhammad Razif Rizqullah, Ayu Purwarianti, and Al-ham Fikri Aji. 2023. Qasina: Religious domain ques-tion answering using sirah nabawiyah. In 2023 10thInternational Conference on Advanced Informatics:Concept, Theory and Application (ICAICTA), pages16. IEEE. Andrew Rouditchenko, Sameer Khurana, SamuelThomas, Rogerio Feris, Leonid Karlinsky, HildeKuehne, David Harwath, Brian Kingsbury, andJames Glass. 2023.Comparison of MultilingualSelf-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages. InProc. INTERSPEECH 2023, pages 22682272. Sebastian Ruder, Jonathan H Clark, Alexander Gutkin,Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijh-wani, Parker Riley, Jean-Michel Sarr, Xinyi Wang,et al. 2023. Xtreme-up: A user-centric scarce-databenchmark for under-represented languages. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 18561884. Victor Sanh, Albert Webson, Colin Raffel, Stephen H.Bach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,Manan Dey, M Saiful Bari, Canwen Xu, UrmishThakker, Shanya Sharma Sharma, Eliza Szczechla,Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,Han Wang, Matteo Manica, Sheng Shen, Zheng XinYong, Harshit Pandey, Rachel Bawden, ThomasWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,Andrea Santilli, Thibault Fevry, Jason Alan Fries,Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers,Thomas Wolf, and Alexander M. Rush. 2021. Multi-task prompted training enables zero-shot task gener-alization. Preprint, arXiv:2110.08207. Auliya Sani, Sakriani Sakti, Graham Neubig, TomokiToda, Adi Mulyanto, and Satoshi Nakamura. 2012.Towards language preservation: Preliminary collec-tion and vowel analysis of indonesian ethnic speechdata. In 2012 International Conference on SpeechDatabase and Assessments, pages 118122. Christoph Schuhmann, Romain Beaumont, RichardVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis,Mitchell Wortsman, et al. 2022. Laion-5b: An openlarge-scale dataset for training next generation image-text models. Advances in Neural Information Pro-cessing Systems, 35:2527825294. Ken Nabila Setya and Rahmad Mahendra. 2018.Semi-supervised textual entailment on indonesianwikipedia data. In International Conference on Com-putational Linguistics and Intelligent Text Processing,pages 416427. Springer.",
  "sealion": "Shivalika Singh, Freddie Vargus, Daniel Dsouza,Brje F. Karlsson, Abinaya Mahendiran, Wei-YinKo, Herumb Shandilya, Jay Patel, Deividas Mataci-unas, Laura OMahony, et al. 2024. Aya dataset: Anopen-access collection for multilingual instructiontuning. arXiv preprint arXiv:2402.06619. Anders Sgaard. 2022. Should we ban English NLP fora year? In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 52545260, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.",
  "Prdect-id: Indonesian product reviews dataset foremotions classification tasks.Data in Brief,44:108554": "Oscar Tckstrm, Ryan McDonald, and Jakob Uszkor-eit. 2012. Cross-lingual word clusters for direct trans-fer of linguistic structure. In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 477487, Montral,Canada. Association for Computational Linguistics. Zeerak Talat, Aurlie Nvol, Stella Biderman, MirunaClinciu, Manan Dey, Shayne Longpre, Sasha Luc-cioni, Maraim Masoud, Margaret Mitchell, DragomirRadev, Shanya Sharma, Arjun Subramonian, JaesungTae, Samson Tan, Deepak Tunuguntla, and Oskar VanDer Wal. 2022. You reap what you sow: On the chal-lenges of bias evaluation under multilingual settings.In Proceedings of BigScience Episode #5 Workshopon Challenges & Perspectives in Creating Large Lan-guage Models, pages 2641, virtual+Dublin. Associ-ation for Computational Linguistics. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, andRadu Soricut. 2022. Crossmodal-3600: A massivelymultilingual multimodal evaluation dataset. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 715729,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Khanh Quoc Tran, Phap Ngoc Trinh, Khoa Nguyen-AnhTran, An Tran-Hoai Le, Luan Van Ha, and Kiet VanNguyen. 2021. An empirical investigation of on-line news classification on an open-domain, large-scale and high-quality dataset in vietnamese. In NewTrends in Intelligent Software Methodologies, Toolsand Techniques, pages 367379. IOS Press. Ahmet stn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, NeelBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,et al. 2024. Aya model: An instruction finetunedopen-access multilingual language model.arXivpreprint arXiv:2402.07827. Kiet Van Nguyen, Tin Van Huynh, Duc-Vu Nguyen,Anh Gia-Tuan Nguyen, and Ngan Luu-Thuy Nguyen.2022. New vietnamese corpus for machine readingcomprehension of health news articles. ACM Trans.Asian Low-Resour. Lang. Inf. Process., 21(5). Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In Proceedings of the IEEEconference on computer vision and pattern recogni-tion, pages 45664575. Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao,Yang Ding, Ai Ti Aw, and Nancy F Chen. 2023.Seaeval for multilingual foundation models: Fromcross-lingual alignment to cultural reasoning. arXivpreprint arXiv:2309.04766.",
  "Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao,Yang Ding, Ai Ti Aw, and Nancy F. Chen. 2024. Seae-val for multilingual foundation models: From cross-lingual alignment to cultural reasoning. NAACL": "Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Haryo Akbarianto Wibowo, Erland Hilman Fuadi,Made Nindyatama Nityasya, Radityo Eko Prasojo,and Alham Fikri Aji. 2023. Copal-id: Indonesianlanguage reasoning with local culture and nuances.arXiv preprint arXiv:2311.01012. Bryan Wilie, Karissa Vincentio, Genta Indra Winata,Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,Sidik Soleman, Rahmad Mahendra, Pascale Fung,Syafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:Benchmark and resources for evaluating Indonesiannatural language understanding. In Proceedings ofthe 1st Conference of the Asia-Pacific Chapter of theAssociation for Computational Linguistics and the10th International Joint Conference on Natural Lan-guage Processing, pages 843857, Suzhou, China.Association for Computational Linguistics. Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-jaya, Rahmad Mahendra, Fajri Koto, Ade Romad-hony, Kemal Kurniawan, David Moeljadi, Radi-tyo Eko Prasojo, Pascale Fung, Timothy Baldwin,Jey Han Lau, Rico Sennrich, and Sebastian Ruder.2023. NusaX: Multilingual parallel sentiment datasetfor 10 Indonesian local languages. In Proceedingsof the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pages815834, Dubrovnik, Croatia. Association for Com-putational Linguistics.",
  "Genta Indra Winata, Ruochen Zhang, and David Ife-oluwa Adelani. 2024.Miners: Multilingual lan-guage models as semantic retrievers. arXiv preprintarXiv:2406.07424": "BigScience Workshop, Teven Le Scao, Angela Fan,Christopher Akiki, Ellie Pavlick, Suzana Ilic, DanielHesslow, Roman Castagn, Alexandra Sasha Luc-cioni, Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:",
  "Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics": "Zheng Xin Yong, Ruochen Zhang, Jessica Forde, SkylerWang, Arjun Subramonian, Holy Lovenia, SamuelCahyawijaya, Genta Winata, Lintang Sutawika, JanChristian Blaise Cruz, Yin Lin Tan, Long Phan, LongPhan, Rowena Garcia, Thamar Solorio, and AlhamAji. 2023. Prompting multilingual large languagemodels to generate code-mixed texts: The case ofsouth East Asian languages. In Proceedings of the6th Workshop on Computational Approaches to Lin-guistic Code-Switching, pages 4363, Singapore. As-sociation for Computational Linguistics. Ruochen Zhang, Samuel Cahyawijaya, Jan Chris-tian Blaise Cruz, Genta Winata, and Alham Aji.2023a. Multilingual large language models are not(yet) code-switchers. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1256712582, Singapore.Association for Computational Linguistics. Wenxuan Zhang,Mahani Aljunied,Chang Gao,Yew Ken Chia, and Lidong Bing. 2023b. M3exam:A multilingual, multimodal, multilevel benchmarkfor examining large language models. In Advances inNeural Information Processing Systems, volume 36,pages 54845505. Curran Associates, Inc. Wenxuan Zhang,Mahani Aljunied,Chang Gao,Yew Ken Chia, and Lidong Bing. 2024. M3exam:A multilingual, multimodal, multilevel benchmarkfor examining large language models. Advances inNeural Information Processing Systems, 36.",
  "Model Performance": "LLMs: SEA-specific models, such as AYA-101 and mT0, show strong performance onzero-shot tasks, outperforming English orcountry-specific models in the region. How-ever, tasks like abstractive QA and summa-rization reveal limitations in existing modelsability to handle SEA languages effectively. Speech: Off-the-shelf models like Whisperv3 show competitive ASR performance formajor SEA languages but struggle with in-digenous languages. In contrast, SeamlessM4T v2 offers more balanced results acrossSEA languages.",
  "VLMs: Current VLMs fail to generate high-quality image captions in SEA languages,highlighting the need for more effective mul-tilingual pre-training": "LLM Generation Quality.SEA language out-puts by LLMs are often plagued by translationese,with models like SEA-LION v1 producing naturalsentences only 57.71% of the time. Languages likeTagalog, Burmese, and Malay suffer from unnatu-ral generation. Resource Gaps.SEACrowd covers 74.9% ofSEA languages but reveals a long-tail distribution,where most languages lack comprehensive datasets.SEA languages also face cultural misrepresenta-tion, with 70% of datasets being translations ratherthan culturally relevant sources. Prioritizing Development.Focus should beplaced on SEA national languages with signifi-cant gaps in naturalness (e.g., Malay, Burmese, Fil-ipino), as well as under-resourced local languageslike Javanese and Cebuano.",
  ": Benchmark comparison. The numbers in SEACrowd and NusaCrowd are the numbers of datasets includedin the evaluation": "such as Indonesian (Mahendra et al., 2021; Wilieet al., 2020; Cahyawijaya et al., 2021, 2023a) andVietnamese (Nguyen et al., 2020; Huynh et al.,2022; Le and Luu, 2023; Van Nguyen et al., 2022).NusaCrowd (Cahyawijaya et al., 2023a) introducethe first multimodal benchmark for Indonesian lan-guages, including text and speech. Ruder et al.(2023) introduce a multimodal benchmark encom-passing 11 indigenous languages from SEA, span-ning a wide array of languages totaling 88.Additionally, Asai et al. (2023) present an LLMbenchmark for cross-lingual few-shot transfer, com-prising 15 distinct tasks and 54 languages sourcedfrom varied multilingual datasets. Furthermore,Dou et al. (2024) find that publicly available pre-training data for SEA languages suffer from qualityissues such as textual duplicates and excessive oc-currences of Unicode escapes. On the other hand,pre-trained LLMs specifically for SEA languagessuffer from limited language coverage; for instance,Cendol (Cahyawijaya et al., 2024b), Sailor (Douet al., 2024), SEA-LION (Singapore, 2023), andSeaLLMs (Nguyen et al., 2023) have only coveredup to 11 different SEA languages, including En-glish and Chinese. Open-source Community Initiatives in NLPOpen-source and open-science communities play acrucial role in engaging native speakers to curatelarge-scale multilingual NLP resources. In the past,collaborative efforts have been organized to collectdata and train multilingual language models eitheron a global scale (Workshop et al., 2022; Singhet al., 2024; stn et al., 2024) or on a regionallevel, e.g., Masakhane for African languages (Ade-lani et al., 2021, 2022b,a, 2023), AI4Bharat for In-dian languages (Kakwani et al., 2020; Kumar et al.,2022; Dabre et al., 2022, inter alia), and Americas-NLP for Latin American languages (Mager et al.,2021; Ebrahimi et al., 2022).In the SEA region, there have been community-based initiatives, e.g., IndoNLP, PyThaiNLP, andRojakNLP, to study NLP on Indonesian languages(Aji et al., 2022; Wilie et al., 2020; Cahyawijaya",
  "C.1 Open Contributions": "We identify four tasks for open contribution inSEACrowd.12 These tasks and the workflow ofSEACrowd are heavily influenced by and extendedupon NusaCrowd (Cahyawijaya et al., 2023a,2022), a collaborative effort to pool data resourcesfor Indonesian NLP. Submitting Metadata for Existing PublicDatasets. Contributors can submit detaileddatasheets for existing datasets through thisform.13 Contributors must provide importantinformation such as data license, size, lan-guage and dialect, annotation method, andso on. The approved datasheets, as well asunder review datasheets, will show up andbe indexed in a monitor spreadsheet and theSEACrowd Catalogue ().",
  ": A glimpse of SEACrowd Catalogue": "in SEACrowd are standardized in terms of for-matting and usage. Contributors can follow adataloader guide and examples available14 inthe SEACrowd Data Hub. Dataloader main-tainers and reviewers also monitor the self-assigned dataloader issues after 2 weeks ofinactivity and ping contributors in case of ablocking impediment. Identifying Private AI Datasets for SEALanguages, Cultures, and/or Regions. Un-fortunately, a number of prior works involvingSEA languages are still not publicly available.These may be due to several different reasons,including (but not limited to): non-release con-tracts related to funding, inclusion of privateand personally identifiable data, and the useof explicitly private data such as those usedby for-profit companies.In this task, contributors can search for worksthat contain private data and fill out a corre-",
  "sponding record form.15 The SEACrowd teamthen attempts to contact the original data own-ers and negotiate the open-sourcing of theirresources": "Opening a Private AI Dataset of SEA. Ifa contributor has previous work with closeddata (or has been contacted by the SEACrowdteam regarding closed-source data), they candecide to release their resources and registerthem in the collection via the public datasheetform. The resource will still be owned bythe original contributor and is still tied to thecontributors previous work, as SEACrowdsimply catalogs it and records its now open-source license.",
  "C.2 Measuring Contributions": "To be considered as a co-author, 20 contributionpoints are required.16 To monitor how many pointsthe contributors have obtained, the contributionpoint tracking is provided and updated regularly.The purpose of the point system is not to barriercollaboration but to reward rare and high-qualitydataset entries. describes the contributionpoints.17 A bonus of 1 point is given if the datasetmodality is speech or vision. We also provide abonus based on the language rarity in terms of avail-able resources as defined by Joshi et al. (2020)18,consisting of 1 point for languages in level 1 and 2,and 2 points for languages in level 0 or absent fromthe list. For other contributions not mentioned in (e.g., maintenance, design, experiment, pa-per writing, etc.), the amount of contribution pointsis adjusted to the bulk and the complexity of therelevant work.",
  "D.1 Timeline": "SEACrowd released the open call for contributionson 1 November 2023. This lasted until 31 March2024, for datasheet submissions, and until 15 May2024 for both dataloaders and private dataset sub-missions. SEACrowd contributors have a biweeklydiscussion regarding the challenges they face whilecontributing, the next steps they should take to pro-ceed, and/or experiment and research ideas for thepaper. The detailed timeline can be seen in Fig-ure 8.",
  "ReviewerSOP:": "avoiding duplicates, and ensuring correctness andrelevance to the SEA region. The SOP includesFAQs addressing common issues such as datasetduplicates and incorrect information, along with anapproval checklist covering aspects like data avail-ability, dataset splits, and licensing. Reviewers areinstructed on how to handle various scenarios, in-cluding correcting errors and determining pointsallocation for multiple contributors. For instance,if the datasheet submitted has incorrect or miss-ing information, the reviewer can either ask thecontributor to fix it (with some guidance) or fixit themself. Upon completion of the review, re-viewers update the status, add notes and points,and await the generation of a GitHub issue for theapproved datasheet.",
  "E.2 Dataloader Reviewing": "The dataloader reviewing SOP governs the reviewprocess for dataloaders in SEACrowd, ensuring ad-herence to the data structure and seacrowd schemaand config standards. It specifies checks for meta-data correctness, subset implementation, test scriptpassing, and adherence to coding conventions. Ad-ditionally, it outlines dataloader config rules basedon dataset types and provides guidelines for mul-tilingual datasets. The SOP emphasizes the im-portance of reviewer collaboration, with each dat-aloader requiring two reviewers per submitted pullrequest, and outlines the approval and reviewerassignment process, either by allocation or by self-assignment based on availability and promptness.",
  "F Schemas in SEACrowd": "Schemas define and format the attributes of thedataset returned by a dataloader. For each dat-aloader, we implement 2 schema types: the sourceschema and the seacrowd schema. The sourceschema presents the dataset in a format similar toits original structure, while the seacrowd schemastandardizes the data structure across similar tasks.The following subsections define the seacrowdschemas in NLP (F.1), speech (F.2), and VL (F.3).",
  ": NLI data subsets used in SEACrowd NLUevaluation": "sis, emotion classification, legal classification,and others. It consists of (id, text, label),where id denotes a unique row identifier ofthe dataset, text denotes an input text, andlabel denotes a deterministic target variable. Multi-label text classification (TEXT MULTI).This schema could be used for hate speechdetection and aspect-based sentiment analysis.It consists of (id, text, labels), where iddenotes a unique row identifier of the dataset,text denotes an input text, and labels de-notes a list of deterministic target variables. Text-to-text (T2T). This schema could be usedfor machine translation, summarization, andparaphrasing. It consists of (id, text_1,text_2,text_1_name,text_2_name),where id denotes a unique row identifierof the dataset, text_1 and text_2 denote",
  ": Weekly status update of the cumulative number of submissions in SEACrowd": "an input text pair, and text_1_name andtext_2_name denote the names of the inputtext pair (e.g., ind and jav for translation in-put text pairs, or document and summary forsummarization input text pairs). Sequence labeling (SEQ LABEL). This schemacould be used for named entity recognition(NER), POS tagging, and others. It consists of(id, tokens, labels), where id denotes aunique row identifier of the dataset, tokensdenotes a list of tokens of an input text, andlabels denotes a list of targets for the tokens. Question answering (QA). This schemacould be used for extractive QA, multiple-choice QA, and others. It consists of (id,question_id,document_id,question,type, choices, context, answer), whereid denotes a unique row identifier of thedataset, question_id denotes a unique iden-tifier of the question, document_id denotesa unique identifier of the context document,question denotes an input question to beanswered, type denotes the type of the QAtask (e.g., extractive, multiple-choice, open-generative, closed-generative, etc.), choicesdenotes a list of answer choices (if required),context denotes a passage that serves as thebackground information of the question (if re-quired), and answer denotes the gold answerto the question (if required).",
  "text_1, text_2, label), where id denotesa unique row identifier of the dataset, text_1and text_2 denote an input text pair, andlabel denotes the target variable": "Single-label text pair classification withcontinuous values or regression (PAIRSSCORE). This could be used for answer grad-ing and semantic textual similarity. It con-sists of (id,text_1,text_2,label),where id denotes a unique row identifier ofthe dataset, text_1 and text_2 denote an in-put text pair, and label denotes a target vari-able as a continuous value. Multi-label text pair classification (PAIRSMULTI). This could be used for morphologi-cal inflection. It consists of (id, text_1,text_2,labels), where id denotes aunique row identifier of the dataset, text_1and text_2 denote an input text pair, andlabels denotes a list of target variables. Knowledge base (KB). This schema could beused for constituency parsing, dependencyparsing, coreference resolution, dialogue sys-tems, and other tasks with complex structures.It consists of (id, passages, entities,events,coreferences,relations).Considering its intricate structure, we encour-age readers to take a look at the implementa-tion of the knowledge base schema.",
  ": Commonsense reasoning data subsets used inSEACrowd NLU evaluation": "nodes), where id denotes a unique row iden-tifier of the dataset, passage denotes thepassage to that particular id; this passageconsist of (id,type,text,offsets),nodes denotes the nodes to that particular id;this nodes consists of (id,type,text,offsets, subnodes). Conversational Chat (CHAT). This schemacould be used for conversational chat and/ormulti-turn conversation. It consists of (id,input, output, meta), where id denotesa unique row identifier of the dataset, inputdenotes a sequence that consists of content",
  ": Multiple-choice QA data subsets used inSEACrowd NLU evaluation": "and role as an input prompt and the role ofthe entity inputting the prompt, output de-notes an answer from that input prompt, andmeta denotes relevant details to allow someflexibility of the schema (if required). End-to-end Task Oriented Dialogue (TOD).This schema could be used for end-to-end task-oriented dialogue.It consistsof (dialogue_idx,dialogue), wheredialogue_idx denotes a unique row identi-fier of the dialogue, dialogue denotes somecore details such as turnlabel, systemutterance, turn idx, belief state (con-sist of slots and act), user utterance, andsystem acts.",
  "F.2 Speech": "Speech-text (SPTEXT). This could be used forspeech recognition, text-to-speech (TTS) orspeech synthesis, and speech-to-text transla-tion. It consists of (id, path, audio, text,speaker_id, metadata), where id denotesa unique row identifier of the dataset, pathdenotes the file path to an input audio source,audio denotes the audio data loaded from thecorresponding path, text denotes an inputtext, speaker_id denotes a unique identifierof the speaker, metadata denotes relevant de-tails such as the age and gender of the speaker(if required). Speech-to-speech (S2S). This could be usedfor speech-to-speech translation.It con-sists of (id, path_1, audio_1, text_1,metadata_1, path_2, audio_2, text_2,metadata_2), where id denotes a unique rowidentifier of the dataset, path_1 and path_2 denote the file path to a respective input audiosource, audio_1 and audio_2 denote the au-dio data loaded from the corresponding path,text_1 and text_2 denote input texts, andmetadata_1 and metadata_2 denote relevantdetails such as the age of the speaker and theirgender (if required). Speech Classification (SPEECH). This schemacould be used for speech classification,speech-language identification, and speech-emotion recognition for single-label useonly.It consists of (id,path,audio,speaker_id, labels, metadata), whereid denotes a unique row identifier of thedataset, path denotes the file path to an in-put audio source, audio denotes the audiodata loaded from the corresponding path,speaker_id denotes a unique identifier ofthe speaker, labels denotes the label of thatparticular speech (only can be single-label),metadata denotes relevant details such as theage and gender of the speaker (if required). SpeechClassificationforMultilabel(SPEECH MULTILABEL). This schema couldbe used for speech classification, speech-language identification, and speech-emotionrecognition for multi-label use only. It con-sists of (id, path, audio, speaker_id,labels,metadata), where id denotes aunique row identifier of the dataset, pathdenotes the file path to an input audiosource, audio denotes the audio data loadedfrom the corresponding path, speaker_iddenotes a unique identifier of the speaker,labels denotes the sequence of labels of thatparticular speech (only can be multi-label),metadata denotes relevant details such as theage and gender of the speaker (if required).",
  "F.3 VL": "Image-text (IMTEXT). This schema couldbe used for image captioning, text-to-imagegeneration, and vision-language pre-training.It consists of (id,text,image_paths,metadata), where id denotes a unique rowidentifier of the dataset, text denotes an in-put text, image_paths denotes a list of pathsto the input image sources, and metadata de-notes relevant details such as visual conceptsand labels (if required).",
  ": ASR data subsets used in SEACrowd speechevaluation": "tion both single-label and multi-label.Itconsists of (id,labels,image_path,metadata), where id denotes a unique rowidentifier of the dataset, labels denotes thelabel of that particular image (can be single-label and multi-label), image_path denotes alist of paths to the input image sources, andmetadata denotes relevant details such as vi-sual concepts and labels (if required). Image Question Answering (IMQA). Thisschema could be used for image/visualquestion answering.It consists of (id,question_id, document_id, questions,type,choices,context,answer,image_paths,meta), where id denotesa unique row identifier of the dataset,question_id denotes a unique identifier ofthe question, document_id denotes a uniqueidentifier of the context document, questiondenotes an input question to be answered,type denotes the type of the QA task (e.g.,extractive, multiple-choice, open-generative,closed-generative, etc.), choices denotes alist of answer choices (if required), contextdenotes a passage that serves as the back-ground information of the question (if re-quired), and answer denotes the gold answerto the question (if required), image_path de-notes a list of paths to the input image sources,and metadata denotes relevant details to al-low some flexibility of the schema (if re-quired).",
  "G.1 Datasets": ", 6, 7, 8, and 9 provide the details of datasubsets used in the NLU evaluation. Sentimentanalysis dataset is originally from NusaX (Winataet al., 2023),NusaTranslation (Cahyawijayaet al., 2023b), SentiTaglish20, SmSA (Purwari-anti and Crisdayanti, 2019), PRDECT-ID (Sutoyoet al., 2022), code-mixed Indonesian-English sen-timent (Astuti et al., 2023), Karonese tweet sen-timent (Karo et al., 2022), Typhoon Yolanda sen-timent (Imperial et al., 2019), GKLMIP Khmersentiment (Jiang et al., 2022), Wisesight sentimentcorpus21, Filipino-Tagalog product reviews Sen-timent22, and multilabel sentiment of Indonesianmobile apps review (Riccosan and Saputra, 2023).Topic classification dataset is originally fromNusaParagraph (Cahyawijaya et al., 2023b), UIT-ViON (Tran et al., 2021), SIB-200 (Adelaniet al., 2024), GKLMIP Khmer news (Jianget al., 2022), and Indonesian news (Muzad andRahutomo, 2016). Natural Language Inferencedataset is originally from IndoNLI (Mahendraet al., 2021), WreTe (Setya and Mahendra, 2018),SNLI Indo (Putra et al., 2024), MyXNLI23, andXNLI (Conneau et al., 2018). Commonsense rea-soning dataset is originally from XStoryCloze (Linet al., 2022), IndoCloze (Koto et al., 2022), andEMoTES-3K (Catapang and Visperas, 2023).Open domain QA dataset is originally from In-doMMLU (Koto et al., 2023b), SeaEval (Wanget al., 2023), M3Exam (Zhang et al., 2023b), andOkapi (Dac Lai et al., 2023). Cultural QA dataset isoriginally from COPAL-ID (Wibowo et al., 2023),XCOPA (Ponti et al., 2020), SeaEval (Wang et al.,2023), and Multilingual Fig-QA (Kabra et al.,",
  ": MT between English and SEA languages data subsets used in SEACrowd NLG evaluation": "2023). The reading comprehension dataset is origi-nally from Belebele (Bandarkar et al., 2023)., 11, and 14 provide the details of datasubsets used in the NLG evaluation. The summa-rization dataset is originally from LR-Sum (Palen-Michel and Lignos, 2023) and XL-Sum (Hasanet al., 2021). The machine translation dataset isoriginally from Lio and the Central Flores cor-pus (Elias, 2018), Flores-200 (Costa-juss et al.,2024) and NTREX-128 (Federmann et al., 2022).Question answering dataset is originally fromFacQA (Purwarianti et al., 2007), QASiNa (Rizqul-lah et al., 2023), MKQA (Longpre et al., 2021),and Open Thai Wikipedia QA dataset24. and 13 provide the details of datasubsets used in the VL and speech evaluation. The image captioning dataset is originally fromXM3600 (Thapliyal et al., 2022). Speech recog-nition dataset is originally from INDspeech NEW-STRA Ethnic collection (Sani et al., 2012), ASRIban (Juan et al., 2015), FLEURS (Conneau et al.,2022), and Common Voice (Ardila et al., 2020).",
  "G.2 Baselines": ", 21, and 22 report the details of baselinemodels used in SEACrowd evaluation (3). Foreach baseline model, we provide information re-garding the model size, origin base model, seenlanguages in the training corpora use, and the URLwhere the models can be downloaded. In princi-ple, this work does not aim to acquire and fit allavailable SEA-trained LLMs over the Internet, asthis is computationally expensive. Rather, we want",
  ": Hyper-parameters of classical models forTranslationese prediction through grid search": "to initiate the exploration of select publicly avail-able models to serve as baselines for the evalua-tion of foundational capabilities on SEA languagesthrough benchmarking on NLU, NLG, speech, andvision tasks aggregated via SEACrowd.Across the various models explored, as listedin the tables, we prioritized the diversity of modelvariation in terms of scale, openness, and coverageof SEA languages. In NLP tasks, we covered 5LLM groups for the main experiments: English-only, multilingual, regional, and country-specificmodels. Instruction-tuned LLMs demonstrate theability to generalize to unseen tasks (Wei et al.,2021; Sanh et al., 2021; Ouyang et al., 2022). Someof these LLMs are based on a multilingual founda-tion, hence their proficiency in generalizing acrosslanguages (Muennighoff et al., 2022; Adilazuardaet al., 2023; Zhang et al., 2023a). For NLU, wecompute the weighted F1-score and obtain the an-swers via log-likelihood for open-source baselinesor string matching for commercial baselines.For the speech benchmark, only two model fam-",
  ": The demographics of the authors based onaffiliation country and origin country": "ilies are available: multilingual models and modelsfine-tuned on specific SEA languages. For visiontasks, we covered English-only and one multilin-gual model. These models utilize a visual back-bone pre-trained on image-text alignment, e.g.,CLIP (Radford et al., 2021), to project image fea-tures into the input space of an existing pre-trainedLM. In summary, we mostly explored open mod-els readily accessible on HuggingFace but alsoincluded commercial models such as GPT-4 andWhisper V3 for performance benchmarking, repro-ducibility, and extension by future works.",
  "G.3 Prompts": "Tables 23, 24, and 25 describe the handwrittenprompt templates used in NLU, NLG, and VL eval-uation (3). For all tasks, we used a zero-shotprompting procedure to serve as the baseline setup.Due to the task complexity and distribution of work-load from volunteer contributors with availablecomputing resources, we limited the experimentprocedure for some setups to ensure the acquisi-tion of results in line with target release dates. For NLU, we explored three prompt styles for eachdataset from core tasks, including commonsensereasoning, question-answering, and NLI. For morechallenging tasks requiring more intensive com-puting power such as NLG and VL, we used onlyone uniform prompt style, but we also exploredprompts translated into SEA languages, i.e., Fil-ipino, Indonesian, Thai, and Vietnamese for VL.",
  "H.1 Training & Evaluation Data": "We manually select and validate the text collectionmethod of each data subset for training and evaluat-ing the translationese classifier, in Tables 28 and 29,respectively. This validation is done by checkingthe relevant publication, domain, and annotationmethod. If the texts in the data subsets are a prod-uct of machine or human translation, we regardthem as translationese. We label data subsets withhuman-generated texts as natural data.",
  "H.2 Experiments": "We aim to assess the capability of ML modelsto differentiate between human-generated/naturalsamples (Nat), human-translated samples (HT), andmachine-translated samples (MT). Our approach in-volves training classifiers using classical ML tech-niques and fine-tuning mDeBERTa models to en-hance learning. Furthermore, we experiment bycombining two label classes into one to evaluatethe predictive difficulty of distinguishing betweenthese labels. This analysis provides valuable in-sights into the relative similarity of the samplesacross these categories. The following section pro-vides a comprehensive overview of our methodol-ogy for this study.",
  "TF-IDF and Bag-of-words (BoW). We run hyper-parameter tuning with grid search to find the besthyper-parameters for each method on validationset, and report the results on test set in": "Encoder LMWe explore fine-tuning encoder-only LM for developing a translationese classi-fier. We utilize mDeBERTa-v3base model25 (Heet al., 2020, 2022)a multilingual encoder-onlyLMas our backbone. We train the model withAdamW (Loshchilov and Hutter, 2019) optimizerusing a learning rate of 1e-5, batch size of 256,and warming up steps of 500 for a maximum of 10epochs. We apply an early stopping of 3 epochsbased on the validation accuracy. We show theresults in .",
  "I Supplementary Details for SEALanguage Prioritization": "Based on the results of the global utility met-ric (Blasi et al., 2022), we provide the top-20 SEAindigenous languages to be prioritized based ontheir demand (i.e., the number of SEA languagespeakers) and current utility () or resourceavailability ().26 We use the performancescores of AYA-101 as one of the best-performingmodels on SEA languages for the current utility.While the current utility, also known as the modelcapability, is relative to the model performance on ENG, the resource availability is relative to 500,which is approximately the number of datasets inKorean language available in HuggingFace. TheKorean language is chosen as the pivot because itis considered a higher-resource language than mostby Joshi et al. (2020).",
  "No.NameC. Points": "1Holy Lovenia5492Samuel Cahyawijaya4803Rahmad Mahendra3174Salsabil Maulana Akbar2435Lester James V. Miranda2346Zheng-Xin Yong1647Jennifer Santoso1648Elyanah Aco1589Akhdan Fadhilah15710Jonibek Mansurov13211Fajri Koto12112Joseph Marvin Imperial11813Ruochen Zhang11414Genta Indra Winata10815Onno P. Kampman10716Joel Ruben Antony Moniz9317Muhammad Ravi Shulthan Habibi9218Frederikus Hudi8319Sedrick Keh8120Alham Fikri Aji8021Railey Montalan7822Peerat Limkonchotiwat72 23Ryan Ignatius5624Joanito Agili Lopo5025William Nixon5026Brje F. Karlsson4927James Jaya4828Ryandito Diandaru4829Yuze Gao4830William Tjhi4631Patrick Amadeus4632Bin Wang4433Jan Christian Blaise Cruz4334Chenxi Whitehouse3635Ivan Halim Parmonangan3636Maria Khelli3637Sebastian Ruder3538Wenyu Zhang3439Lucky Susanto3340Reynard Adha Ryanda3241Sonny Lazuardi Hermawan3042Dan John Velasco2943Muhammad Dehan Al Kautsar2944Willy Fitra Hendria2945Yasmin Moslem2946Noah Flynn2847Muhammad Farid Adilazuarda2748Haochen Li2749Johanes Lee2750R. Damanhuri2751Shuo Sun2752Muhammad Reza Qorib2653Amirbek Djanibekov2554Wei Qi Leong2555Quyet V. Do2456Niklas Muennighoff2457Tanrada Pansuwan2258Ilham Firdausi Putra2159Yan Xu2160Ayu Purwarianti2061Ngee Chia Tai20",
  "EnglishMistral7BMistralN/Amistralai/Mistral-7B-Instruct-v0.3Llama38BLlama3N/Ameta-llama/Meta-Llama-3-8B-InstructFalcon7BFalcon0 SEA langs (mainly English)tiiuae/falcon-7b-instruct": "MultilingualmT03BmT52 SEA langs (VIE, IND), 43 non-SEA langsbigscience/mt0-xlBLOOMZ7BBLOOM2 SEA langs (VIE, IND), 43 non-SEA langsbigscience/bloomz-3bBactrianX-Llama7BLlama6 SEA langs (IND, VIE, KHM, MYA, THA, TGL, VIE), 46 non-SEA langsMBZUAI/bactrian-x-llama-7b-mergedAYA-238BCommand2 SEA langs (IND, VIE), 21 non-SEA langsCohereForAI/aya-23-8BAYA-10113BT59 SEA langs (IND, VIE, THA, ZSM, MYA, CEB, FIL, JAV, SUN), 92 non-SEA langsCohereForAI/aya-101 SEA regionalSEA-LION7BMPT8 SEA langs (IND, VIE, THA, TGL, ZSM, KHM, LAO, MYA), 3 non-SEA langsaisingapore/sea-lion-7b-instructSeaLLM v2.57BSeaLLM8 SEA langs (IND, VIE, THA, TGL, ZSM, KHM, LAO, MYA)SeaLLMs/SeaLLM-7B-v2.5Sailor7BQwen 1.55 SEA langs (IND, VIE, LAO, ZLM, THA), 2 non-SEA langssail/Sailor-7B-Chat SEA countryCendol-mT53BmT51 SEA lang (IND), 18 local Indonesian langsindonlp/cendol-mt5-xlCendol-Llama27BLlama21 SEA lang (IND), 18 local Indonesian langsindonlp/cendol-llama2-7bMerak v47BLlama21 SEA lang (IND)Ichsan2895/Merak-7B-v4WangchanX-Llama38BLlama34 SEA langs (IND, VIE, THA, MYA) and 26 non-SEA langsairesearch/LLaMa3-8b-WangchanX-sft-DemoMalaysian Llama38BLlama31 SEA lang (ZLM)mesolitica/malaysian-llama-3-8b-instruct-16k",
  "No.Prompt template": "Sentiment Analysis1Classify the sentiment of the text below.\\n[INPUT] => Sentiment ([OPTIONS]): [LABEL_CHOICE]2Predict the sentiment of the following text.\\nText: [INPUT]\\nAnswer with [OPTIONS]: [LABEL_CHOICE]3[INPUT]\\nWhat would be the sentiment of the text above? [OPTIONS]? [LABEL_CHOICE] Topic Classification1Classify the topic of the text below.\\n[INPUT] => Topic ([OPTIONS]): [LABEL_CHOICE]2Predict the topic of the following text.\\nText: [INPUT]\\nAnswer with [OPTIONS]: [LABEL_CHOICE]3[INPUT]\\nWhat would be the topic of the text above? [OPTIONS]? [LABEL_CHOICE] Commonsense Reasoning *_seacrowd_text1Classify the morality of the text below.\\n[INPUT] => Morality ([OPTIONS]): [LABEL_CHOICE]2Predict the morality of the following text.\\nText: [INPUT]\\nAnswer with [OPTIONS]: [LABEL_CHOICE]3[INPUT]\\nWhat would be the morality of the text above? [OPTIONS]? [LABEL_CHOICE] Commonsense Reasoning *_seacrowd_qa1Question: [QUESTION]\\nWhat reply makes more sense to answer this question?\\nChoices: [ANSWER_CHOICES]\\nAnswer:[LABEL_CHOICE]2Based on the the following question:\"[QUESTION]\" and choices:[ANSWER_CHOICE the correct answer is:[LABEL_CHOICE]3Question: [QUESTION]\\nChoices: [ANSWER_CHOICES]\\nThe correct answer to the given question is: [LABEL_CHOICE] All QAs1Refertothepassagebelowandanswerthefollowingquestion:\\nPassage:[CONTEXT]\\nQuestion:[QUESTION]\\nChoices: [ANSWER_CHOICES]\\nAnswer: [LABEL_CHOICE]2[CONTEXT]\\nBased on the above text, [QUESTION]\\nChoices: [ANSWER_CHOICES]\\nAnswer: [LABEL_CHOICE]3[CONTEXT]\\nQuestion: [QUESTION]\\nChoices:[ANSWER_ CHOICES]\\nReferring to the passage above, the correct answerto the given question is: [LABEL_CHOICE] NLI1Hypothesis: [INPUT_A]\\nPremise: [INPUT_B]\\nQuestion: What is the relation between the hypothesis and thepremise? [OPTIONS]? [LABEL_CHOICE]2Given the following premise and hypothesis:\\nHypothesis: [INPUT_A]\\nPremise: [INPUT_B]\\nDetermine the logicalrelationship (([OPTIONS])): [LABEL_CHOICE]3Choose the most appropriate relationship ([OPTIONS]) between the premise and hypothesis:\\nRelationship between\"[INPUT_B]\" and \"[INPUT_A]\": [LABEL_CHOICE]",
  "In SEACrowd": "1NUTNungVietnam<1M2KACJingphoMyanmar<1M3TSGTausugPhilippines<1M4NIJNgajuIndonesia<1M5LJPLampung ApiIndonesia<1M6MQYManggaraiIndonesia<1M7MRWMaranaoPhilippines<1M8NIANiasIndonesia<1M9AKBBatak AngkolaIndonesia<1M10SDAToraja-SadanIndonesia<1M11MNWMonMyanmar, Thailand<1M12HNIHaniLaos, Vietnam<1M13KJGKhmuLaos, Thailand, Vietnam<1M14AOZUab MetoIndonesia<1M15BLTTai DamLaos, Vietnam<1M16LUSMizo ChinMyanmar<1M17CPSCapiznonPhilippines<1M18BTXBatak KaroIndonesia<1M19LISLisuMyanmar<1M20MSBMasbatenyoPhilippines<1M21BLKPaoMyanmar, Thailand<1M22TDDTai NaMyanmar<1M23DAYLand DayakIndonesia<1M24XDYMalayic DayakIndonesia<1M25BHPBimaIndonesia<1M26IBGIbanagPhilippines<1M27ZMINegeri Sembilan MalayMalaysia<1M28MDRMandarIndonesia<1M29KGEKomeringIndonesia<1M30BDRWest Coast BajauMalaysia<1M31KDTKuayCambodia, Laos, Thailand<1M32PRKParauk WaMyanmar<1M33SGDSurigaononPhilippines<1M34TETTetunEast Timor, Indonesia<1M35BTORinconada BikolPhilippines<1M36TDTTetun DiliEast Timor<1M37IUMIu MienLaos, Vietnam<1M38KRJKinaray-aPhilippines<1M39KYKKamayoPhilippines<1M40LEWLedo KailiIndonesia<1M41MKNKupang MalayIndonesia<1M42REJRejangIndonesia<1M43MFBBangkaIndonesia<1M44ROBTaeIndonesia<1M45LBWTolakiIndonesia<1M46KNXKendayanIndonesia, Malaysia<1M47GAYGayoIndonesia<1M48MNBMunaIndonesia<1M49RBLMiraya BikolPhilippines<1M50SMWSumbawaIndonesia<1M51KXDBruneiBrunei<1M52KHBLLaos, Myanmar<1M53LHULahuLaos, Myanmar<1M54TWHTai DnLaos, Vietnam<1M55YSMMyanmar Sign LanguageMyanmar<1M56DTPKadazan DusunMalaysia<1M57FBLWest Albay BikolPhilippines<1M58KVRKerinciIndonesia<1M59PCERuching PalaungMyanmar<1M60MRYMandayaPhilippines<1M61NBEKonyak NagaMyanmar<1M62TCZThado ChinMyanmar<1M63JRAJaraiCambodia, Vietnam<1M64XBRKamberaIndonesia<1M65MOGMongondowIndonesia<1M66PWOPwo Western KarenMyanmar<1M67CJAWestern ChamCambodia, Vietnam<1M68AHKAkhaLaos, Myanmar, Thailand<1M69SSBSouthern SamaPhilippines<1M70SXNSangirIndonesia<1M",
  "No.ISO 639-3LanguageRegion(s)Population": "Not in SEACrowd320KRVKavetCambodia<10K321CEYEkai ChinMyanmar<10K322KJTPhrae Pwo KarenThailand<10K323KUKKepoIndonesia<10K324PUTPutohIndonesia<10K325RJGRajongIndonesia<10K326SJBSajau BasapIndonesia<10K327TKZTakuaVietnam<10K328AMVAmbelauIndonesia<10K329WLHWelaunEast Timor, Indonesia<10K330PLZPaluan MurutMalaysia<10K331JKPPaku KarenMyanmar<10K332ADBAtauranEast Timor<10K333NEAEastern NgadaIndonesia<10K334NTDNorthern TidungMalaysia<10K335PHHPhulaVietnam<10K336REBRembongIndonesia<10K337SKXSeko PadangIndonesia<10K338SWUSuwawaIndonesia<10K339TGRTarengLaos<10K340WEURawngtu ChinMyanmar<10K341SAUSalemanIndonesia<10K342THITai LongLaos<10K343LOWTampias LobuMalaysia<10K344NPGPonyo-Gongwang NagaMyanmar<10K345UKKMuak Sa-aakMyanmar<10K346TLQTai LoiLaos, Myanmar<10K347HKNMel-KhaonhCambodia<10K348JKMMobwa KarenMyanmar<10K349LMQLamatukaIndonesia<10K350LVULevukaIndonesia<10K351LWELewoelengIndonesia<10K352RTCRungtu ChinMyanmar<10K353RUULanas LobuMalaysia<10K354TIUAdasenPhilippines<10K355UMNPaungnyuan NagaMyanmar<10K356LHHLahaIndonesia<10K357BJXVanaw KalingaPhilippines<10K358BVTBatiIndonesia<10K359KQVOkolodIndonesia, Malaysia<10K360XKKKachokCambodia<10K361IWKI-wakPhilippines<10K362LKALakaleiEast Timor<10K363BZNBoanoIndonesia<10K364SBRSembakung MurutIndonesia, Malaysia<10K365BFGBusang KayanIndonesia<10K366HAPHuplaIndonesia<10K367KXIKeningau MurutMalaysia<10K368LLQLolakIndonesia<10K369ROCCacgia RoglaiVietnam<10K370SLSSingapore Sign LanguageSingapore<10K371STELiana-SetiIndonesia<10K372ULUUma LungIndonesia<10K373WLIWaioliIndonesia<10K374WRXWae RanaIndonesia<10K375XHVKhuaLaos, Vietnam<10K376TDYTadyawanPhilippines<10K377ZBTBatuiIndonesia<10K378SWSSeluwasanIndonesia<10K379PNIAohengIndonesia<10K380TUJTugutilIndonesia<10K381NPSNipsanIndonesia<10K382UANKuanLaos<10K383VBKSouthwestern BontokPhilippines<10K384DMVDumpasMalaysia<10K385XKOKiorrLaos<10K386KVEKalabakan MurutMalaysia<10K387MCMMalaccan Portuguese CreoleMalaysia<10K388LTULatuIndonesia<10K389GEFGeraiIndonesia<10K390CNCCngVietnam<10K391BPOAnasiIndonesia<10K392HLDHalang DoanLaos, Vietnam<10K393NXKKokak NagaMyanmar<10K394PUJPunan TubuIndonesia<10K395XKNKayan River KayanIndonesia<10K396YCPChepyaLaos<10K397LCSLisabata-NunialiIndonesia<10K398HAFHaiphong Sign LanguageVietnam<10K399SLTSilaLaos, Vietnam<10K"
}