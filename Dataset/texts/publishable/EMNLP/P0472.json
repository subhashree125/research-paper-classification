{
  "Abstract": "Peer review is fundamental to the integrity andadvancement of scientific publication. Tradi-tional methods of peer review analyses oftenrely on exploration and statistics of existingpeer review data, which do not adequately ad-dress the multivariate nature of the process,account for the latent variables, and are fur-ther constrained by privacy concerns due tothe sensitive nature of the data. We introduceAGENTREVIEW, the first large language model(LLM) based peer review simulation frame-work, which effectively disentangles the im-pacts of multiple latent factors and addressesthe privacy issue. Our study reveals signifi-cant insights, including a notable 37.1% vari-ation in paper decisions due to reviewers bi-ases, supported by sociological theories suchas the social influence theory, altruism fatigue,and authority bias. We believe that this studycould offer valuable insights to improve the de-sign of peer review mechanisms. Our code isavailable at",
  "Introduction": "Peer review is a cornerstone for academic publish-ing, ensuring that accepted manuscripts meet thenovelty, accuracy, and significance standards. De-spite its importance, peer reviews often face severalchallenges, such as biases (Stelmakh et al., 2021),variable review quality (Stelmakh et al., 2021), un-clear reviewer motives (Zhang et al., 2022a), andimperfect review mechanism (Fox et al., 2023),exacerbated by the ever-growing number of sub-missions. The rise of open science and preprintplatforms has further complicated these systems,which may disclose author identities under double-blind policies (Sun et al., 2022).Efforts to mitigate these problems have focusedon enhancing fairness (Zhang et al., 2022a), reduc-ing biases among novice reviewers (Stelmakh et al.,",
  "Both authors contributed equally": "2021), calibrating noisy peer review ratings (Luand Kong, 2024), and refining mechanisms forpaper assignment and reviewer expertise match-ing (Xu et al., 2024; Liu et al., 2023b). However,several challenges persist in systematically explor-ing factors influencing peer review outcomes: 1)Multivariate Nature. The peer review process isaffected by a variety of factors, ranging from re-viewer expertise, area chair involvement, to thereview mechanism design. This complexity makesit difficult to isolate specific factors that impact thereview quality and outcomes; 2) Latent Variables.Factors such as reviewer biases and intentions aredifficult to measure but have significant effects onthe review process, often leading to less predictableoutcomes; 3) Privacy Concerns. Peer review dataare inherently sensitive and carry the risk of re-vealing reviewer identities. Investigation of suchdata not only poses ethical concerns but also detersfuture reviewer participation.This Work. We introduce AGENTREVIEW, thefirst framework that integrates large language mod-els (LLMs) (OpenAI, 2023; Touvron et al., 2023)with agent-based modeling (Significant-Gravitas,2023) to simulate the peer review process (Sec. 2).As shown in , AGENTREVIEW is builtupon the capabilities of LLMs to perform realis-tic simulations of societal environments (Wu et al.,2023a; Chen et al., 2024a; Park et al., 2023) andprovide high-quality feedback on academic litera-ture comparable to or exceeds human levels (Chenet al., 2024b,c; Li et al., 2024; DArcy et al., 2024;Zhang et al., 2024; Du et al., 2024).AGENTREVIEW is open and flexible, designedto capture the multivariate nature of the peer re-view process. It features a range of customizablevariables, such as characteristics of reviewers, au-thors, area chairs (ACs), as well as the reviewingmechanisms (Sec. 2.1). This adaptability allowsfor the systematic exploration and disentanglementof the distinct roles and influences of the various",
  "Area ChairMechanism": ": AGENTREVIEW is an open and flexible framework designed to realistically simulate the peer reviewprocess. It enables controlled experiments to disentangle multiple variables in peer review, allowing for an in-depthexamination of their effects on review outcomes. Our findings align with established sociological theories. parties involved in the peer review process. More-over, AGENTREVIEW supports the exploration ofalternative reviewer characteristics and more com-plex review processes. By simulating peer reviewactivities with over 53,800 generated peer reviewdocuments, including over 10,000 reviews, on over500 submissions across four years of ICLR, AGEN- TREVIEW achieves statistically significant insightswithout needing real-world reviewer data, therebymaintaining reviewer privacy.AGENTREVIEWalso supports the extension to alternative reviewercharacteristics and more complicated reviewingprocesses. We conduct both content-level and nu-merical analyses after running large-scale simula-tions of the peer review process.Key findings. Our findings are as follows, whichcould inspire future design of peer review systems: Social Influence (Turner, 1991). Reviewers of-ten adjust their ratings after rebuttals to alignwith their peers, driven by the pressure to con-form to the perceived majority opinion. Thisconformity results in a 27.2% decrease in thestandard deviation of ratings (Sec. 3.1.1);",
  "). Even one under-committed reviewer canlead to a pronounced decline of commitment(18.7%) among all reviewers (Sec. 3.1.2);": "Groupthink and Echo Chamber Effects (Ja-nis, 2008; Cinelli et al., 2021). Biased reviewerstend to amplify each others negative opinionsthrough interactions (Sec. 3.1.3). This can leadto a 0.17 drop in ratings among biased review-ers and cause a spillover effect, influencing thejudgments of unbiased reviewers and leading toa 0.25 decrease in ratings;",
  "Framework Overview": "AGENTREVIEW is designed as an extensibletestbed to study the impact of various stakeholdersand mechanism designs on peer review results. Itfollows procedures of popular Natural LanguageProcessing (NLP) and Machine Learning (ML)conferences, where reviewers provide initial pa-per reviews, update their reviews based on authorsfeedback, and area chairs (ACs) organize discus-sions among reviewers and make final decisions.1",
  ": Our paper review pipeline consists of 5 phases. Solid black arrows represent authorship connections,while blue dashed arrow indicate visibility relations": "AGENTREVIEW integrates three rolesreviewers,authors, and ACsall powered by LLM agents.Reviewers play a pivotal role in peer review. Weidentify three key dimensions that determine thequality of their reviews. 1) Commitment refers tothe reviewers dedication and sense of responsibil-ity in engaging with the manuscript. This involvesa proactive and careful approach to provide thor-ough and constructive feedback on submissions.2) Intention describes the motivation behind thereviews, focusing on whether the reviewer aimsto genuinely help authors improve their papers oris influenced by biases or conflict of interests. 3)Knowledgeability measures the reviewers exper-tise in the manuscripts subject area. Understandingthe effects of each individual aspect is crucial forimproving the peer review process.To explore these dimensionalities, we assign re-viewers into specific categories: knowledgeableversus unknowledgeable reviewers for knowledge-ability, responsible versus irresponsible for com-mitment, and benign versus malicious for intention.These categorizations are set by prompts and fedinto our system as fixed characteristics. For ex-ample, knowledgeable reviewers are described asreviewers that are adept at identifying the signifi-cance of the research and pinpointing any technicalissues that require attention. In contrast, unknowl-edgeable reviewers lack expertise and may over-look critical flaws or misinterpret the contributions.Reviewer descriptions and prompts are detailed inAppendix .Authors submit papers to the conference and pro-vide rebuttals to the initial reviews during theReviewer-AC discussion period (Phase 2 in Fig-ure 1). Although double-blind review policies aretypically in place, authors may still opt to releasepreprints or publicize their works on social media,potentially revealing their identities. We considertwo scenarios: 1) reviewers are aware of the au- thors identities due to the public release of theirworks, and 2) author identities remain unknownto the reviewers. This allows us to explore theimplications of anonymity on the review process.Area Chairs (ACs) have multiple duties, rangingfrom facilitating reviewer discussions, synthesiz-ing feedback into meta-reviews, and making finaldecisions. ACs ensure the integrity of the reviewoutcomes by maintaining constructive dialogues,integrating diverse viewpoints, and assessing pa-pers for quality, originality, and relevance. Ourwork identifies three styles of ACs based on theirinvolvement strategies, each influencing the reviewprocess differently: 1) authoritarian ACs dominatethe decision-making, prioritizing their own eval-uations over the collective input from reviewers;2) conformist ACs rely heavily on other reviewersevaluations, minimizing the influence of their ownexpertise; 3) inclusive ACs consider all availablediscussion and feedback, including reviews, authorrebuttals, and reviewer comments, along with theirexpertise, to make well-rounded final decisions.",
  "Review Process Design": "AGENTREVIEW uses a structured, 5-phase pipeline() to simulate the peer review process.I. Reviewer Assessment. In this phase, three re-viewers critically evaluate the manuscript. To sim-ulate an unbiased review process, each reviewerhas access only to the manuscript and their ownassessment, preventing any cross-influence amongreviewers. Following Liang et al. (2023), we askLLM agents to generate reviews that comprise foursections, including significance and novelty, po-tential reasons for acceptance, potential reasonsfor rejection, and suggestions for improvement.This format is aligned with the conventional reviewstructures of major ML/NLP conferences. Unlessspecified otherwise, each reviewer provides a nu-merical rating from 1 to 10 for each paper. II. Author-Reviewer Discussion. Authors respondto each review with a rebuttal document to ad-dress misunderstandings, justify their methodolo-gies, and acknowledge valid critiques.III. Reviewer-AC Discussion. The AC initiatesa discussion among the reviewers, asking themto reconsider their initial ratings, and provide anupdated review after considering the rebuttals.IV. Meta-Review Compilation. The AC integratesinsights from Phase I-III discussions, their own ob-servations, and numeric ratings into a meta-review.This document provides a synthesized assessmentof the manuscripts strengths and weaknesses thatguides the final decision.V. Paper Decision. In the final phase, the AC re-views all meta-reviews for their assigned papers tomake an informed decision regarding their accep-tance or rejection. We adopt a fixed acceptance rateof 32%, reflecting the actual average acceptancerate for ICLR 2020 2023. Therefore, each ACis tasked with making decisions for a batch of 10papers and accepts 3 4 papers in the batch.",
  "Data Selection": "The paper data for AGENTREVIEW is sourced fromreal conference submissions to ensure that our sim-ulated reviews closely mirror real scenarios. Weadhere to four criteria for data selection: 1) Theconference must have international impact with alarge number of authors and a wide audience, andthe academic achievements discussed should havesignificant real-world impacts; 2) the papers mustbe publicly available; 3) the quality of the papersmust reflect real-world distribution, including bothaccepted and rejected papers; 4) the papers mustspan a broad time range to cover a variety of top-ics and mitigate the effects of evolving reviewerpreferences over time.We select ICLR due to its status as a leadingpublication venue in computer science and its trans-parency in making both accepted and rejected sub-missions available. We retrieve papers spanningfour years (20202023) using OpenReview API2.Papers are categorized into oral (top 5%), spotlight(top 25%), poster, and rejection. We then employ astratified sampling technique to select papers fromeach category, resulting in a diverse dataset with350 rejected papers, 125 posters, 29 spotlights, and19 orals. This approach ensures the inclusion ofpapers with varying quality, closely mirroring real-",
  "Baseline Setting": "Real peer review process inherently entails substan-tial uncertainty due to variations in reviewers ex-pertise, commitment, and intentions, often leadingto seemingly inconsistent numeric ratings. For ex-ample, NeurIPS experiments found significant dif-ferences in reviewers ratings when different sets ofreviewers evaluated the same submissions (Cortesand Lawrence, 2021; Zhang et al., 2022a). Directlycomparing numeric ratings of our experimental out-comes with actual ratings can be inappropriate andfail to disentangle the latent variables.To address this, we establish a baseline settingwith no specific characteristics of LLM agents (re-ferred to as baseline in ). This allows usto measure the impact of changes in one variableagainst a consistent reference. Across all settings,we generate 10,460 reviews and rebuttals, 23,535reviewer-AC discussions, 9,414 meta-reviews, and9,414 paper decisions. Detailed statistics for thedataset are in Appendix , and the experi-mental cost is in Appendix A.2).",
  "The Role of Reviewers": "To study the effect of commitment on the peer re-view outcomes, we start with replacing a normalreviewer with either a responsible or an irrespon-sible reviewer, then gradually increase the numberof reviews. The settings we consider as well as theinitial & final ratings are in , and the ratingdistribution is in . Agent-based reviewersin our environment demonstrate classic phenom-",
  "Overview": "Social Influence Theory (Cialdini and Goldstein,2004) suggests that individuals in a group tend torevise their beliefs towards a common viewpoint.A similar tendency towards convergence is also ob-served among the reviewers. Across all settings,the standard deviation of reviewer ratings ()significant declines after the Reviewer-AC discus-sion, revealing a trend towards conformity. This isparticularly evident when a highly knowledgeableor responsible reviewer dominates the discussion.Overall, responsible, knowledgeable, and benign(well-intentioned) reviewers generally give higherscores than less committed or biased (malicious)reviewers. Although initial review ratings can below, the final ratings in most settings significantlyimprove following discussions, highlighting the im-portance of reviewer-author interactions on address-ing reviewers concerns. In Sec. 3.4, we furtherexplore whether these interactions and subsequentpaper improvements influence the final decisions.",
  "Reviewer Commitment": "Altruism Fatigue & Peer Effect (Angrist, 2014)Paper review is typically unpaid and time-consuming (Zhang et al., 2021), requiring substan-tial time investment beyond reviewers regular pro-fessional duties. This demanding nature, coupledwith altruism fatiguewhere reviewers feel theirvoluntary efforts are unrecognizedoften results inreduced commitment and superficial assessments.The presence of just one irresponsible reviewercan lead to a pronounced decline in overall re-viewer commitment compared with the baseline.Although the initial review length is similar be-",
  "no numeric rating0.2000.05260.40": ": Comparison of final decisions in various set-tings relative to the baseline experiment in terms ofJaccard Index (Jacc.), Cohens Kappa Coefficient (),and Percentage Agreement (%Agree). Jacc. indicate theset of papers accepted by both the investigated settingand the baseline. The highest and second highest valuesare highlighted in bold and underlined, respectively. tween the two settings (baseline and irresponsi-ble), averaging 432.4 and 429.2 words, the averageword count experienced a significant 18.7% drop,from 195.5 to 159.0 words, after reviewers inter-act during the reviewer-AC discussion. This peereffect illustrates how one reviewers subpar perfor-mance can lower the standards and efforts of oth-ers, leading to more cursory review post-rebuttal.The reduction in overall engagement during crit-ical review discussions underscores the negativeimpact of insufficient reviewer commitment, whichcan permit the publication of potentially flawed re-search, misleading subsequent studies and erodingtrust in the academic review process.Groupthink (Janis, 2008) occurs when a groupof reviewers, driven by a desire for harmony orconformity, reaches a consensus without criticalreasoning or evaluation of a manuscript. It can beespecially detrimental when the group includes irre-sponsible or malicious reviewers. To examine sucheffects, we substitute 1 3 normal reviewers withirresponsible reviewers and analyze the changes inratings before & after reviewer-AC discussion. highlights a noticeable decline in reviewratings under the influence of irresponsible review-ers. Replacing 2 normal reviewers with irresponsi-ble ones results in a significant drop of 0.25 from5.256 to 5.005 in the average reviewer rating afterReviewer-AC Discussion (Phase III). In contrast,in the baseline scenario, the final ratings improveby an average 0.06 post-rebuttal, as reviewers moreproactively scrutinize the author feedback and havetheir concerns addressed. Interestingly, the scores",
  "Reviewer Intention": "Conflict Theory (Bartos and Wehr, 2002) statesthat societal interactions are often driven by conflictrather than consensus. In the context of peer review,where the acceptance of papers is competitive, re-viewers may perceive other high-quality submis-sions as threats to their own work due to conflictof interests. This competitive behavior can leadto low ratings for competing papers, particularlyfor concurrent works with highly similar ideas, asreviewers aim to protect their own standing in thefield. Empirically, the reviewer ratings in show a significant shift to a bimodal distribution,primarily centered around [4.0, 4.25], when justone malicious reviewer is involved. This forms astark contrast to the unimodal distribution between[5.0, 5.25] observed in the baseline condition.Echo Chamber Effects (Cinelli et al., 2021) occurwhen a group of reviewers sharing similar biasesamplify their opinions, leaning towards a collec-tive decision without critically evaluating meritsof the work. As illustrated in , increasingthe number of malicious reviewers from 0 to 3 re-sults in a consistent drop in the average rating from5.11 to 3.35, suggesting that the presence of ma-licious reviewers significantly impacts the overallevaluation. Meanwhile, as malicious reviewers pre-dominate, the average rating among these biasedreviewers () experiences a greater drop post-rebuttal, indicating that the inclusion of more bi-ased reviewers not only amplifies the papers issuesbut also solidifies their strong negative opinionsabout the work. This process not only reinforcespre-existing biases and reduces critical scrutiny, butalso has a spillover effect that adversely impactsevaluations from unbiased reviewers. The presenceof 1 and 2 malicious reviewers corresponds to adecline by 0.14 and 0.10, respective, among thenormal reviewers.Content-level Analysis We categorize the reasonsfor acceptance and rejection as shown in with additional details provided in Appendix A.1.While reasons for accepting the papers are consis-tent across all settings, the reasons for rejectiondiffer significantly in distribution. Irresponsiblereviews tend to be shallow, cursory, and notably22.2% shorter, whereas malicious reviews dispro-portionally criticize the lack of novelty in the work (d), a common but vague reason for rejec-tion. Specifically, mentions of lack of novelty bymalicious reviewers account for 10.4% of feedback,marking a 182.9% increase compared to just 3.69%by benign reviewers. They also highlight more pre-sentation issues which, although important for clar-ity, do not pertain to the theoretical soundness ofthe research. On the other hand, benign reviewerstend to focus more on discussions about scalabilityand practicality issues, providing suggestions tohelp enhance papers comprehensiveness.",
  "Reviewer Knowledgeability": "Knowledgeability poses two challenges. Firstly,despite extended efforts at matching expertise, re-view assignments are often imperfect or random(Xu et al., 2024; Saveski et al., 2024). Secondly,the recent surge in submissions to computer sci-ence conferences has necessitated an expansion ofthe reviewer pools, raising concerns about the ad-equacy of reviewers expertise to conduct properand effective evaluations. As shown in ,less knowledgeable reviewers are 24% more likelyto mention insufficient discussion of limitations,whereas expert reviewers not only address these ba-sic aspects but also provide 6.8 % more critiques onexperimental validation, resulting in more concreteand beneficial feedback for improving the paper.",
  "Involvements of Area Chairs": "We quantify the alignment between reviews andmeta-reviews using BERTScore (Zhang et al.,2020) and sentence embedding similarity (Reimersand Gurevych, 2019) in , and measure theagreement of final decisions between baseline andeach setting in . Inclusive ACs align mostclosely with the baseline for final decisions, demon-strating their effectiveness in integrating diverseviewpoints and maintaining the integrity of the re-view process through a balanced consideration ofreviews and their own expertise. In contrast, au-thoritarian ACs manifest significantly lower corre-lation with the baseline, with a Cohens Kappa ofonly 0.266 and an agreement rate of 69.8%. Thissuggests that their decisions may be skewed by per-sonal biases, leading to acceptance of lower qualitypapers or the rejection of high-quality papers thatdo not align with their viewpoints, thereby com-promising the integrity and fairness of the peerreview process. Conformist ACs, while showing ahigh semantic overlap with reviewers evaluationsas evidenced in , might lack independent",
  "Impacts of Author Anonymity": "Recent conferences have increasingly permitted therelease of preprints, potentially impacting paper ac-ceptance (Elazar et al., 2024). Although reviewersare instructed not to proactively seek informationabout author identities, concerns persist that re-views may still be biased by author reputation.Authority bias is the tendency to attribute greateraccuracy and credibility to the opinions of author-ity figures. This bias is closely related to the HaloEffects, a cognitive bias where the positive percep-tion of an individual in one area, such as their previ-ous groundbreaking research, influences judgmentsabout their current work. Reviewers influenced byauthority bias are more likely to give favorable re-views to well-known and respected scientists.To analyze the impact of author identities onreview outcomes, we vary the number of review-ers aware of the authors identities (k), rangingfrom 1 to 3, and adjusted the proportion of paperswith known author identities (r) from 10% to 30%.Specifically, the reviewers were informed that the authors of certain papers were renowned and highlyaccomplished in the field. We categorized papersinto two types: higher quality and lower quality,based on their ground-truth acceptance decisions.For lower-quality papers, awareness of the au-thors renowned identities among 1, 2, or 3 re-viewers resulted in Jaccard indices of 0.364, 0.154,and 0.008, respectively, in terms of paper accep-tance (). The most extreme case has anegative Cohens Kappa (), indicatinga substantial deviation in paper decisions. Whenhigh-quality papers had known author identities,much less significant changes were observed in ac-cepted papers. Notably, changes in paper decisionsare more influenced by the number of reviewersaware of the author identities than by the percent-age of papers with known author identities.",
  ":Similarities between reviews and meta-reviews w/ various intervention strategies from AC. Left:BERTScore, right: sentence embedding similarity": "Effects of Rebuttals.Eliminating the rebuttalphase, which requires substantial time commit-ments from both reviewers and authors, has a sur-prisingly minimal impact on the final paper deci-sions, aligning closely with the baseline scenario.One explanation for this minimal impact isthe anchoring bias, where the initial impressionformed during the first submission (the anchor)predominantly influences reviewers judgments.Even though authors may make substantial im-provements during the rebuttal phase that addressreviewers concerns (Sec. 3.1.1), these changesmay fail to alter their initial judgments. Anotherplausible reason is that all submissions improve inquality during the rebuttal phase. Thus, the relativeposition (ranking of quality) of each paper amongall submissions experiences little change.Effects of Overall Ratings.Numeric ratingsfrom reviewers may serve as a shortcut in thefinal decision-making process for paper accep-tance. When these ratings are omitted, the decision-making landscape changes significantly, leading topotentially divergent decisions. The comparison ofoutcomes with respect to baseline reveals only aminimal overlap, with a Jaccard index of 0.20 interms of accepted papers (). 4Related WorkAnalysis of Peer Review Systems.Peer re-view serves as the backbone of academic research,ensuring the integrity and quality of publishedwork (Zhang et al., 2022b). Several studies havescrutinized various challenges within peer review,such as bias (Stelmakh et al., 2021; Ugarov, 2023;Verharen, 2023; Liu et al., 2023a), conflict of inter-ests (McIntosh and Vitale, 2023), and the broaderissues of review quality and fairness (Stelmakhet al., 2021; McIntosh and Vitale, 2023; Stephen,2024). Research has also delved into the opera-",
  "#Reviewers that Know the Authors (k)": ": Comparison of final decisions with respect to baseline when the author identity is known for varyingratios of papers, relative to the baseline. A smaller Cohens Kappa coefficient suggests a lower correlation with thebaseline. Extension to Broader VenuesAlthough AGENTREVIEW is language-agnostic, our initial focus is onEnglish-centric conferences and journals due to the prevalence of English in international academia andthe availability of data. Current models generally perform better in English than in other languages (Jinet al., 2024; Deng et al., 2024). As more capable multilingual LLMs, such as LLaMA 3 (Dubey et al.,2024) and Mistral Large 2 (Jiang et al., 2023), emerge, our framework can be applied to simulate peerreviews in multiple languages, enabling simulations across a broader range of academic contexts.",
  "Conclusion": "We presented AGENTREVIEW, the first LLM-based framework for simulating the peer reviewprocess. AGENTREVIEW addresses key challengesby disentangling intertwined factors that impact re-view outcomes while preserving reviewer privacy.Our work lays a solid foundation for more equi-table and transparent review mechanism designsin academic publishing. Future works could inves-tigate how intricate interactions between different",
  "Limitation": "Our work has the following limitations.First,AGENTREVIEW is unable to dynamically incor-porate or adjust experimental results in response toreviewer comments during Reviewer-Author Dis-cussion (Phase II in ), as LLMs lack thecapability to generate new empirical data. Sec-ondly, our analysis mainly isolates and examinesindividual variables of the peer review process,such as reviewer commitment or knowledgeability.Real-world peer reviews, however, involve mul-tiple interacting dimensions. Finally, we did notdirectly compare the simulation outcomes with ac-tual peer review results. As described in Sec 2.4,establishing a consistent baseline for such compar-isons is challenging due to the wide variability inhuman reviewer characteristics, such as commit-ment, intention, and knowledgeability, which canvary across papers, topics, and time periods. Theinherent variability and arbitrariness in human peerreviews (Cortes and Lawrence, 2021) add complex-ity to direct comparisons between simulated andreal outcomes.",
  "Ethical Consideration": "Further Investigation into Peer Review data.The sensitivity and scarcity of real-world reviewdata complicate comprehensive studies of peer re-views due to ethical and confidentiality constraints.Our AGENTREVIEW framework generates simu-lated data to study various peer review dynamics,effectively overcoming related challenges.Peer Review Integrity. As discussed, the integrityof the peer review process is underpinned by thecommitment, intention, and knowledgeability ofreviewers. Knowledgeability ensures that review-ers can accurately assess the novelty, significance,and technical soundness of submissions. Goodintention are essential for maintaining the objec-tivity and fairness of reviews, thereby supportingthe credibility and integrity of academic publica-tions. A high level of commitment from reviewersensures comprehensive and considerate evaluationsof submission, which is important for a fair andrigorous evaluation process. However, paper re-view is usually an unpaid and time-consuming task.Such demanding nature can lead the reviewers toconduct cursory or superficial evaluations.Caution about Use of LLMs. Our AGENTRE- VIEW mirrors real-world academic review prac-tices to ensure the authenticity and relevance ofour findings. While AGENTREVIEW uses LLMsto generate paper reviews, there are ethical con-cerns regarding their use in actual peer review pro-cesses (Lee et al., 2023). Recent machine learningconferences have shown an increase in reviewssuspected to be AI-generated (Liang et al., 2024).Although LLM-generated reviews can provide valu-able feedback, we strongly advise against their useas replacements for human reviewers in real-worldpeer review processes. As LLMs are still imperfect,human oversight is crucial for ensuring fair andvaluable assessments of manuscripts and for main-taining the integrity and quality of peer reviews.",
  "Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.2023. Chateval: Towards better llm-based evaluatorsthrough multi-agent debate. In ICLR": "Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen,Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, JindongGu, Huaxiu Yao, Chaowei Xiao, et al. 2024a. Canediting llms inject harm? In ICML 2024 Next Gener-ation of AI Safety Workshop. Yuyan Chen, Yueze Li, Songzhou Yan, Sijia Liu, JiaqingLiang, and Yanghua Xiao. 2024b. Do large languagemodels have problem-solving capability under in-complete information scenarios?In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics. Yuyan Chen, Songzhou Yan, Panjun Liu, and YanghuaXiao. 2024c. Dr.academy: A benchmark for eval-uating questioning capability in education for largelanguage models. In Proceedings of the 62nd An-nual Meeting of the Association for ComputationalLinguistics.",
  "Mike DArcy, Tom Hope, Larry Birnbaum, and DougDowney. 2024. Marg: Multi-agent review generationfor scientific papers. arXiv:2401.04259": "Chengyuan Deng, Yiqun Duan, Xin Jin, Heng Chang,Yijun Tian, Han Liu, Henry Peng Zou, YiqiaoJin, Yijia Xiao, Yichen Wang, et al. 2024.De-constructing the ethics of large language modelsfrom long-standing issues to new-emerging dilem-mas. arXiv:2406.05392. Jiangshu Du, Yibo Wang, Wenting Zhao, ZhongfenDeng, Shuaiqi Liu, Renze Lou, Henry Peng Zou,Pranav Narayanan Venkit, Nan Zhang, Mukund Sri-nath, et al. 2024. Llms assist nlp researchers: Cri-tique paper (meta-) reviewing. arXiv:2406.16253. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Let-man, Akhil Mathur, Alan Schelten, Amy Yang, An-gela Fan, et al. 2024. The llama 3 herd of models.arXiv:2407.21783.",
  "Irving L Janis. 2008. Groupthink. IEEE EngineeringManagement Review, 36(1):36": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv:2310.06825. Bowen Jiang, Zhijun Zhuang, Shreyas S Shivakumar,Dan Roth, and Camillo J Taylor. 2024.Multi-agent vqa: Exploring multi-agent foundation mod-els in zero-shot visual question answering. In TheIEEE/CVF Conference on Computer Vision and Pat-tern Recognition 2024 Workshop on What is Next inMultimodal Foundation Models? Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu,Munmun De Choudhury, and Srijan Kumar. 2024.Better to ask in english: Cross-lingual evaluation oflarge language models for healthcare queries. In WebConference, pages 26272638.",
  "Kayvan Kousha and Mike Thelwall. 2024. Artificialintelligence to support publishing and peer review: Asummary and review. Learned Publishing, 37(1):412": "Ji-Ung Lee, Haritz Puerto, Betty van Aken, YukiArase, Jessica Zosa Forde, Leon Derczynski, An-dreas Rckl, Iryna Gurevych, Roy Schwartz, EmmaStrubell, et al. 2023.Surveying (dis) paritiesand concerns of compute hungry nlp research.arXiv:2306.16900. Manling Li*, Shiyu Zhao*, Qineng Wang*, KangruiWang*, Yu Zhou*, Sanjana Srivastava, Cem Gokmen,Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu,Percy Liang, Li Fei-Fei, Jiayuan Mao, and Jiajun Wu.2024. Embodied agent interface: Benchmarking llmsfor embodied decision making. In NeurIPS.",
  "Ruosen Li, Teerth Patel, and Xinya Du. 2023a. Prd:Peer rank and discussion improve large languagemodel based evaluations. arXiv:2307.02762": "Yuchen Li, Haoyi Xiong, Qingzhong Wang, LingheKong, Hao Liu, Haifang Li, Jiang Bian, ShuaiqiangWang, Guihai Chen, Dejing Dou, et al. 2023b. Coltr:Semi-supervised learning to rank with co-trainingand over-parameterization for web search. TKDE,35(12):1254212555. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp,Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Hao-tian Ye, Sheng Liu, Zhi Huang, et al. 2024. Moni-toring ai-modified content at scale: A case study onthe impact of chatgpt on ai conference peer reviews.arXiv:2403.07183. Weixin Liang, Yuhui Zhang, Hancheng Cao, BingluWang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli,Siyu He, Daniel Smith, Yian Yin, et al. 2023. Canlarge language models provide useful feedback onresearch papers? a large-scale empirical analysis.arXiv:2310.01783.",
  "Dimity Stephen. 2024.Distinguishing articles inquestionable and non-questionable journals us-ing quantitative indicators associated with quality.arXiv:2405.06308": "Mengyi Sun, Jainabou Barry Danfa, and Misha Teplit-skiy. 2022. Does double-blind peer review reducebias? evidence from a top computer science con-ference. Journal of the Association for InformationScience and Technology, 73(6):811819. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M Dai, AnjaHauth, et al. 2023. Gemini: a family of highly capa-ble multimodal models. arXiv:2312.11805. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama 2: Open foundation andfine-tuned chat models. arXiv:2307.09288.",
  "Jeroen PH Verharen. 2023.Chatgpt identifies gen-der disparities in scientific peer review.Elife,12:RP90230": "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,Xiaoyun Zhang, and Chi Wang. 2023a. Autogen:Enabling next-gen llm applications via multi-agentconversation framework. arXiv:2308.08155. Yuxiang Wu, Zhengyao Jiang, Akbir Khan, YaoFu, Laura Ruis, Edward Grefenstette, and TimRocktschel. 2023b. Chatarena: Multi-agent lan-guage game environments for large language models.GitHub repository.",
  "Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, XianjunYang, Xiao Luo, Wenchao Yu, Xujiang Zhao, YanchiLiu, Haifeng Chen, et al. 2024.Large languagemodels can be good privacy protection learners. InEMNLP": "Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye,Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, BernardGhanem, and Guohao Li. 2024. Can large languagemodel agents simulate human trust behaviors? InICLR 2024 Workshop: How Far Are We From AGI. Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Meng-nan Du, Shuaiqiang Wang, Dawei Yin, and SumiHelal. 2024. When search engine services meet largelanguage models: Visions and challenges.IEEETransactions on Services Computing.",
  "A.1Review Categorization": "In our experiment, we utilize GPT-4 to summarize and categorize the reasons for paper acceptance andrejection, as illustrated in . Specifically, we analyze each line from the reasons for acceptance andreasons for rejection fields in the generated reviews. GPT-4 is tasked with automatically classifying eachlisted reason. If an entry does not align with predefined categories, the model establish a new category.Ultimately, we identify five distinct reasons for acceptance and seven reasons for rejection.",
  "A.2Experimental Costs": "To ensure consistent evaluation results, we use the gpt-4-1106-preview version of the GPT-4 modelthroughout our experiments. The model is selected for its superior language understanding and generationcapabilities, essential for simulating an authentic peer review process. To enhance reproducibility andminimize API usage, we establish a baseline settings (Sec. 2.4), where no specific personalities of the roleare detailed (baseline in ). This setting allows us to measure the impact of changes in individualvariables against a consistent standard. For subsequent experiments, we adopt reviews and rebuttals (PhaseI-II) from this baseline when applicable. For example, when we investigate the effects of substitutinga normal reviewer with an irresponsible person, we only generate the reviews for that specific reviewerwhile adopting existing reviews from the baseline setting. This approach minimizes the variability causedby different experimental runs and significantly reduces the API cost compared with rerunning the entirereview pipeline each time. The total cost of API usage across all tests is approximately $2780.",
  "A.3Model Selection": "Additionally, we have also explored the feasibility of alternative models, such as gpt-35-turbo andGemini. These models were initially considered to assess the cost-effectiveness and performance diversity.However, these models either encounter issues related to content filtering limitations, resulting in theomission of critical feedback, or generate superficial evaluations and exhibited a bias towards overlygenerous scoring. Therefore, despite the higher operational costs, we choose despite the higher operationalcosts, due to its more consistent and realistic output in peer review simulations due to its more consistentand realistic output in peer review simulations.",
  "Qualitative Evidence presents the LLM-generated review, rebuttal, and meta-review for thepaper Image as Set of Points (Ma et al., 2022), demonstrating substantial overlap with human reviewsin": "Quantitative EvidenceWe randomly sample 100 papers from our dataset, use LlamaIndex 3 to extractand match major comments in human and LLM-generated reviews in our dataset. To ensure fairness, wefollow Liang et al. and ask the LLM reviewers to generate 4 reasons to accept / reject for each paper. In90% / 77% / 39% of the papers, at least 2 / 3 / 4 out of 4 points align with human reviewers, indicatingthat LLMs provide realistic opinions. Moreover, LLMs highlight unique insights often overlooked byhuman reviewers, such as computational costs, scalability concerns, and experiments on diverse datasets.",
  "shows the prompts used in AGENTREVIEW and the characteristics of each type of roles": "is the distribution of initial and final ratings as 0 3 reviewers become aware of the authorsprestigious identity. It shows that the average reviewer ratings consistently increase with more reviewersknowing the author identities. Meanwhile, reviewer ratings consistently increase after rebuttals. is the Cohens Kappa coefficient () when the author identity is known for varying ratios ofpapers, relative to the baseline. Different lines represent different numbers of reviewers that are awareof the authors identities. is the final rating distribution when we vary one reviewer in the experiment, including theircommitment, intention, or knowledgeability. Reviewers powered by LLMs assign highly consistentnumeric ratings to most submissions, with the majority of the scores in [5, 5.25]. Notable exceptionsoccur under the irresponsible and malicious settings, where the ratings exhibit a bimodal distributionwith peaks at [5, 5.25] and [4.25, 4.5].",
  "A.6Future Works": "Enhancing Realism in Agent BehaviorsSimulating real-world peer review with high fidelity remainschallenging, particularly given the current limitations of large language models (LLMs), such as theirinability to produce novel empirical data or fully capture the nuanced judgment of human reviewers. Futurework could integrate specialized models (Liu et al., 2024; Li et al., 2023b; Yang et al., 2024) or leveragemixture of experts (MoEs) frameworks (Ke et al., 2024) where sub-models, or experts, focus on specifictasks like evaluating technical soundness, assessing novelty, or providing constructive feedback. Thesetask-specific or discipline-specific experts could improve the accuracy of simulations, better reflecting thediversity of expertise seen in real-world peer review.",
  "ReviewOverall rating: 5": "Significance and novelty: The work puts forth a novel image representation paradigm by conceptualizing imagesas unorganized sets of points and using a simplified clustering algorithm for feature extraction. This approach stepsaway from traditional ConvNets and Vision Transformers, which rely on convolution and attention mechanisms,respectively. The proposed method, characterized as Context Clusters (CoCs), has demonstrated comparable oreven slightly better performance on several benchmarks to existing architectures, offering a fresh perspective inthe domain of visual representation.",
  "Novelty and potential impact:": "The work, while novel, may not offer a significantly different or superior approach to existing methodsgiven that the improvements over state-of-the-art are not substantial or broadly demonstrated across multiplebenchmarks. The rationale behind the conceptual shift from organized to unorganized points is not convincingly tied tospecific advantages in the fundamental understanding or performance gain across a wide range of tasks.",
  "RebuttalWe appreciate the constructive feedback and the reviewers recognition of the novelty in our conceptual shiftfrom organized pixels to unorganized point sets and our methods potential efficacy": "Regarding the novelty and potential impact, we agree that the performance improvements over the state-of-the-art are not substantial across all benchmarks. Our primary objective is to offer a new paradigm ratherthan focusing solely on performance metrics. We believe that the shift to unorganized points can lead tomore efficient learning in certain scenarios due to the reduction of inductive biases, which we will investigatefurther. We will expand on the specific advantages of this conceptual shift, exploring applications where thelack of structure in data is inherent (e.g., point clouds, non-uniform datasets). Concerning the technical soundness and completeness, we will extend our ablation studies to better understandthe contribution of each component to the overall performance. We will explore alternative configurations toenhance efficiency without compromising accuracy. We will also conduct a comprehensive comparison withrecent methods in terms of computational efficiency, memory usage, scalability, and adaptability to largedatasets to position our methodology within the current landscape more accurately. In terms of clarity and organization, we acknowledge that the methodology section could be more accessible.We will add further explanations, visual aids, and pseudocode for the clustering operation and region partitionstrategy, aiming to foster better comprehension among readers. Regarding reproducibility and empirical validation, we commit to providing detailed information on hyperpa-rameter tuning, data preprocessing, and setup specifications to ensure reproducibility. We acknowledge theimportance of robustness and behavior on out-of-distribution data and will incorporate experiments in thesesettings to demonstrate the methods generalizability and limitations.",
  "Meta reviewScore: 5": "Summary: This paper introduces a novel approach to visual representation by conceptualizing images asunorganized sets of points. The authors propose Context Clusters (CoCs), an architecture that uses a simplifiedclustering algorithm for feature extraction without relying on traditional convolution or attention mechanisms.The authors have presented empirical evidence suggesting that CoCs can achieve performance comparableto or slightly better than existing ConvNets and Vision Transformers on several benchmarks. The methodaims to provide a new perspective on image representation and is convolution- and attention-free, potentiallyoffering insights into feature extraction for different domains and interpretability through visualization of theclustering process. In the rebuttal phase, the authors responded to concerns regarding the novelty and potential impact, technicalsoundness, and completeness. They addressed the need for a clear rationale behind the shift from organizedpixels to unorganized points and agreed to expand on specific advantages and applications where this couldlead to more efficient learning. The authors acknowledged that in-depth comparison with other recentmethods in terms of computational efficiency, memory usage, scalability, and adaptability to large datasetswas missing and committed to conducting comprehensive comparisons to position CoCs within the currentresearch landscape accurately. The authors also responded to concerns about the methods clarity and organization, reproducibility, empiricalvalidation, and the general lack of a thorough comparison with state-of-the-art methods. They promisedto enhance the clarity of the methodology section by including further explanations, visual aids, and pseu-docode. Moreover, the authors committed to providing detailed information on hyperparameter tuning, datapreprocessing, and setup specifications, as well as incorporating experiments on out-of-distribution data todemonstrate the methods robustness and generalizability. The reviewers have acknowledged the authors response and appreciated the commitment to addressing theconcerns. However, since theres no updated manuscript at this stage that reflects these proposed changes, thereviewers maintained their initial ratings. Should the authors implement the improvements they outlined intheir rebuttal, it is anticipated that the value of their contribution to the community could be notably enhanced.However, the current version of the paper lacks sufficient empirical validation and theoretical analysis towarrant a higher score.",
  "Review": "Summary Of The Paper: This paper proposes a new view of images that considers each image as a set of points (the pixels)and uses a clustering algorithm to extract the features from it. The goal is to investigate the way to utilize this new formof visual representations and evaluate the performance that could be achieved. To this end, the paper introduces a novelbackbone network that includes the proposed Context Clusters and evaluates this model on several vision tasks as well as apoint cloud data application.",
  "Weaknesses:": "By using the region partition mechanism, the set of points is no longer unorganized but becomes structured based on theirlocality. Additional experiments are required to clarify the role of the region partition. Before applying the context clusters operation, the region partition operation, which is similar to the shifting windowsin Swin, is introduced to reduce the computational cost. The authors seem to imply that the region partition trades offperformance for speed. However, the locality introduced by the region partition could also bring useful inductive bias forthe encoder. Therefore, additional experiments are required to answer the following questions:",
  "It would be nice to introduce Swin as one baseline to investigate this problem": "Clarity, Quality, Novelty And Reproducibility: The paper is well-written and easy to follow. The authors also provideadditional explanations of some model designs in the appendix which are much appreciated. Both the topic and the proposedmethod are original. The general architecture is reproducible based on the model description, but additional hyper-parametersare required to reproduce the experimental results. Summary Of The Review: This paper introduces a new form of image representation that considers each image as a setof points and proposes a clustering-based architecture for feature extraction. Both the idea of image as set of points andthe proposed architecture are interesting and novel. The experiment result also shows that the method achieves comparableperformance to ConvNets and ViTs. A small concern is that the role of the region partition mechanism is unclear since goodperformance could actually be attributed to this design.",
  "Irresponsible": "As an irresponsible reviewer, your reviews tend to be superficial and hastily done. You do not like to discuss in the reviewer-AC discussion. Your assessments might overlook critical details, lack depth in analysis, fail to recognize novel contributions, or offer generic feedback that does little to advance the paper's quality."
}