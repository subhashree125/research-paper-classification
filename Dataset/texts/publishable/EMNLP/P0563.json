{
  "Abstract": "The growing demand for larger-scale modelsin the development of Large Language Models(LLMs) poses challenges for efficient trainingwithin limited computational resources. Tradi-tional fine-tuning methods often exhibit insta-bility in multi-task learning and rely heavily onextensive training resources. Here, we proposeMoDULA (Mixture of Domain-Specific andUniversal LoRA), a novel Parameter EfficientFine-Tuning (PEFT) Mixture-of-Expert (MoE)paradigm for improved fine-tuning and param-eter efficiency in multi-task learning.Theparadigm effectively improves the multi-taskcapability of the model by training universalexperts, domain-specific experts, and routersseparately. MoDULA-Res is a new methodwithin the MoDULA paradigm, which main-tains the models general capability by connect-ing universal and task-specific experts throughresidual connections. The experimental resultsdemonstrate that the overall performance ofthe MoDULA-Flan and MoDULA-Res meth-ods surpasses that of existing fine-tuning meth-ods on various LLMs.Notably, MoDULA-Res achieves more significant performance im-provements in multiple tasks while reducingtraining costs by over 80% without losing gen-eral capability. Moreover, MoDULA displaysflexible pluggability, allowing for the efficientaddition of new tasks without retraining ex-isting experts from scratch. This progressivetraining paradigm circumvents data balanc-ing issues, enhancing training efficiency andmodel stability. Overall, MoDULA provides ascalable, cost-effective solution for fine-tuningLLMs with enhanced parameter efficiency andgeneralization capability.",
  "*Equal Contribution.Corresponding Author": "vron et al., 2023a), Qwen (Bai et al., 2023), andYi (Young et al., 2024), have achieved notablesuccesses in natural language processing. How-ever, the increasing complexity and growing sizeof these models make efficient training withinlimited computational resources challenging. Re-searchers tried to address this with Parameter Ef-ficient Fine-Tuning (PEFT), such as LoRA (Huet al., 2021), Prefix Tuning (Liu et al., 2023), and(IA)3 (Liu et al., 2022). LoRA has gained promi-nence for its high performance using low-rankmatrices, but it often encounters instability whentrained on large, mixed datasets. To mitigate thisissue, MoLoRA (Zadouri et al., 2024) has beenintroduced by extending LoRA and integrating theMixture-of-Expert (MoE) architecture as shown in(a). This approach trains multiple LoRA-adapters concurrently, each serving as an expert,to enhance the base LLMs generalization abil-ity across diverse tasks. The integration of MoEinto LoRA aims to improve training efficiency andstability, facilitating more effective fine-tuning oflarge-scale language models for a wide range ofnatural language processing applications.Despite its advantages, MoLoRA has some limi-tations. One limitation is the absence of domain-specific LoRA adapters, as the same experts areemployed universally across all tasks. This unifor-mity may limit the performance ceiling, especiallyfor significantly distinct tasks like math and code,where the inclusion of domain-specific expertscould potentially enhance performance (Zeng et al.,2021). Another challenge is the limited pluggabil-ity of MoLoRA; adding new task capabilities ne-cessitates retraining all parameters from all experts,which can be inefficient and time-consuming.To address the challenges, we propose a three-stage training paradigm called MoDULA, wheredifferent domain-specific experts can be trainedseparately. Moreover, we introduce a more ad-vanced method MoDULA-Res (Mixture of Domain-",
  ": Illustrations of MoLoRA(a), MoDULA-Flan(b), and MoDULA-Res(c) with router omitted": "Specific and Universal LoRA with Residual Con-nection), which incorporates a residual structureto make the training more stable, as seen in Fig-ure 1(c). Unlike MoLoRA, which employs multipleidentical LoRA adapters as experts, our paradigmincorporates a universal expert alongside multi-ple domain-specific experts. The universal expertlearns task-agnostic representations, while eachdomain-specific expert operates as a bias adapter,focusing on domain-specific knowledge.Intu-itively, arranging these adapters in parallel andallocating weights to each adapter via a router con-stitutes the MoDULA-Flan (Mixture of Domain-Specific and Universal LoRA with Flan Rout-ing) method as seen in (b).However,this method may potentially compromise universalcapabilities. To address this, MoDULA-Res in-troduces a refined method that enables domain-specific experts to receive input from the output ofthe universal expert. This design ensures a coher-ent flow of information and facilitates the optimalintegration of both universal and domain-specificexpert functionalities through a residual connec-tion. By dynamically adjusting the contributions ofdomain-specific experts, MoDULA-Res adapts toindividual tasks while preserving broad generaliza-tion capabilities. This flexibility allows the modelto leverage its general competencies for task un-derstanding and summarization when encounteringnew tasks, thereby achieving a more balanced andeffective adaptation in multi-task scenarios. During model training, our MoDULA employsa three-stage optimization process, with detailedillustrations displayed in : 1) Initially,only the universal expert is trained to adapt to gen-eral tasks quickly; 2) Subsequently, each domain-specific expert is trained individually, focusing on its corresponding task; 3) Finally, the parameters ofall experts are frozen, and only router is trained tolearn the optimal combination strategy for differenttasks. This progressive training paradigm allowsour methods to avoid retraining from scratch, dis-tinguishing it from MoLoRA, which trains only anew expert for a new specific task and retraining therouter. This paradigm significantly reduces compu-tational costs, mitigates data balancing challenges,and enhances the models pluggability.To evaluate the effectiveness of our proposedmethods, we conduct extensive experiments on a di-verse set of open-source LLMs, including LLaMA-2 (Touvron et al., 2023b), Qwen (Bai et al., 2023),and Yi (Young et al., 2024), across various tasks.The results consistently demonstrate that MoD-ULA exhibits a significant performance, achiev-ing 4.5% improvements compared to MoLoRA.By introducing residual connections, MoDULA-Res achieves even greater improvements withoutcompromising the general capabilities. Addition-ally, our approach showcases superior adaptabilityto new tasks, outperforming MoLoRA in financeand e-commerce domain with less training data andparameters, highlighting the enhanced task plugga-bility of our approach, making it an efficient andgeneral solution for multi-task learning in LLMs.",
  "Large Language Model": "Recently, the field of natural language processinghas witnessed a paradigm shift with the adventof LLMs (Anil et al., 2023b; Almazrouei et al.,2023; Xu et al., 2023; Scao et al., 2022; Brownet al., 2020; Achiam et al., 2023; Zhang et al.,2023; Du et al., 2022). These state-of-the-art mod-els have departed from traditional approaches that relied on convolutional or recurrent architecturesfor feature extraction, instead embracing noveltechniques such as BERT (Devlin et al., 2019),which leverages the power of Transformers trainedon extensive datasets, yielding bidirectional en-coder representations. Similarly, Generative Pre-trained Transformer (GPT) (Brown et al., 2020)employs decoder layers from Transformer architec-ture (Vaswani et al., 2017) as feature extractors andutilizes autoregressive training on vast texts.Guided by the principles of scaling laws (Kaplanet al., 2020), the development of LLMs has led tothe emergence of colossal models boasting over100 billion parameters, with prominent examplesincluding GPT-4 (Achiam et al., 2023) and Gem-ini (Anil et al., 2023a). Interestingly, open-sourcemodels such as OPT (Zhang et al., 2022), Fal-con (Almazrouei et al., 2023), and Gemma (Mes-nard et al., 2024) have demonstrated competitiveperformance compared to their closed-source coun-terparts, despite possessing a more modest param-eter count. The training process of LLMs typi-cally involves leveraging immense amounts of tex-tual data to enable the prediction of subsequenttokens, empowering these models to generate co-herent and comprehensible responses to a widerange of prompts. This training method has provento be highly effective in capturing the intricacies oflanguage and paved the way for LLMs to achieveSOTA performance across various NLP tasks.",
  "MoE for PEFT": "Our research closely aligns with the work done byMoLoRA (Zadouri et al., 2024), LoraHub (Huanget al., 2023a), MoELoRA (Liu et al., 2024),SiRA (Zhu et al., 2023), and C-Poly (Wang et al.,2023), which explore the intersection of PEFT andMoE. MoLoRA employs a full soft MoE on topof LoRA, utilizing a learned gating mechanismto average all experts, and trains the experts in asingle stage. LoraHub investigates LoRA compos-ability for cross-task generalization and introducesa simple framework for the purposive assemblyof LoRA modules trained on diverse given tasks,aiming to achieve adaptable performance on un-seen tasks. It can fluidly combine multiple LoRAmodules with just a few examples from a new task,without requiring additional model parameters orhuman expertise. MoELoRA devises multiple ex-perts as the trainable parameters and proposes atask-motivated gate function for all MOELoRAlayers to regulate the contributions of each expert and generate distinct parameters for various tasks.SiRA proposes a sparse mixture of low rank adap-tion that enforces the top k experts routing witha capacity limit. It uses expert dropout to reduceover-fitting. C-Poly combines task-common skillsand task-specific skills and jointly learns a skillassignment matrix.While these methods have significantly con-tributed to the field, they face particular chal-lenges and limitations. Training experts on mixeddatasets as in MoLoRA may lead to performancedegradation due to data inconsistency and interfer-ence (Dong et al., 2024). LoraHub relies on few-shot examples in inference stage, and MoELoRArequires task-id to determine which experts shouldbe activated, which weaken the flexibility of bothmethods. Sparse routing, as used by SiRA, requirescareful tuning of the top-k and capacity hyperpa-rameters for each dataset. C-Polys joint learningof task-common and task-specific skills can makebalancing general and specialized abilities difficult.Additionally, incorporating new experts or skillsin these methods may require retraining or modi-fying existing components, potentially impactingsystem stability and training complexity. Train-ing new experts often demands substantial data,resulting in high training costs and sub-optimal per-formance in specific domains. Maintaining optimalperformance on domain-specific benchmarks afteradding new capabilities can be challenging, andnewly added modules may not consistently achievetop performance in their respective benchmarks.These factors can affect the adaptability and effi-ciency of MoLoRA, SiRA, and C-Poly in meetingexpanding task demands.In contrast, MoDULA method trains universaland domain-specific experts separately, mitigat-ing performance degradation from mixed datasets.Designed with \"pluggability\" in mind, the MoD-ULA method allows new experts to be added with-out changing existing ones, ensuring system sta-bility and low training costs. After adding a newexpert, only the router requires retraining to main-tain near-optimal performance. This staged train-ing balances general and domain-specific capabili-ties, making our method adaptable and efficient forgrowing task requirements.",
  ": Illustrations of the three-stage training paradigm for MoDULA-Res": "two methods:MoDULA-Flan and MoDULA-Res. MoDULA-Flan consists of a universal ex-pert and an array of domain-specific experts, whileMoDULA-Res further incorporates residual connec-tions between the universal and domain-specific ex-perts to enhance performance and stability. illustrates the differences between MoLoRA, ourproposed MoDULA-Flan and MoDULA-Res. Inall of these, the base LLMs retain a frozen weightconfiguration, denoted as W0, corresponding to thefixed linear layers within the architecture.MoLoRA. The MoLoRA method serves as thefoundation of our MoDULA. As shown in Fig-ure 1(a), the MoLoRA consists of a router MR anda set of LoRA experts E1, E2, . . . , En. Each ex-pert Ei includes two key components: BMiandAMi . The dynamics of the MoLoRA method canbe summarized by the following equations:",
  "i=1sMi BMi AMi xm(3)": "In these equations, xm represents the hidden vec-tor of the m-th token in the input sequence, sMidenotes the routing coefficient for expert Ei, W MRis the weight matrix of the router, and EM() ex-presses the collective function of the experts in theMoLoRA module.MoDULA. Based on MoLoRA, we propose athree-stage training paradigm called MoDULA, asillustrated in .In the first stage, onlythe universal expert is trained, while the domain-specific experts and router are deactivated.In the second stage, the domain-specific experts aretrained individually for each corresponding task,while the parameters of the universal expert arekept frozen. In the third stage, all the experts pa-rameters are fixed, and only the router is trained.With the MoDULA paradigm, we propose twomethods: MoDULA-Flan and MoDULA-Res.MoDULA-Flan.MoDULA-Flan maintainsthe same architecture as MoLoRA, as illustratedin (b).However, it implements theMoDULA paradigm to separate the experts inMoLoRA into universal expert and domain-specificexperts.The specific training details are asfollows.In the first stage, the universal ex-pert Eflanis trained on universal datasets.Inthe second stage, the domain-specific expertsEflan1, Eflan2, . . . , Eflannare trained on their re-spective domain-specific datasets. The forwardprocess in this stage is formally articulated throughEquations (4) and (8).",
  ": Main experimental results of baseline methods, MoDULA-Flan, and MoDULA-Res on domain-specificbenchmarks": "MoDULA-Res. In order to further improve thegeneral ability of the model, we propose MoDULA-Res, a more advanced method that leverages thestrengths of both universal and domain-specific ex-perts. The architecture of MoDULA-Res is shownin (c). MoDULA-Res integrates both theuniversal expert Eresand the domain-specific ex-perts Eres1, Eres2, . . . , Eresn , tuned in a balancedway to cater to both general and domain-specifictasks. MoDULA-Res introduces a residual connec-tion that allows the model to incorporate the outputof universal expert directly into the final result, en-suring that critical information is preserved andenhancing model robustness.The forward process in MoDULA-Res moduleinvolves two stages. Initially, a hidden vector hmis computed using the universal expert:",
  "Expert Configurations": "A detailed comparison is conducted among the stan-dard LoRA (Hu et al., 2021), MoLoRA (Zadouriet al., 2024), and our newly proposed MoDULA-Flan and MoDULA-Res. The base models selectedfor this study include LLaMA-2 (Touvron et al.,2023b), Qwen (Bai et al., 2023), and Yi (Younget al., 2024). In the training of MoDULA, a batchsize of 128 is utilized, encompassing 1 epoch witha learning rate of 2e-4. The maximum input se-quence length is defined as 4096 tokens for bothLLaMA-2 and Yi. In contrast, Qwen series has8192 tokens due to variations in maximum posi-tional embeddings among different model zoos.The intrinsic rank is configured to 16 for universaland 8 for domain-specific experts. For the multi-task results, the checkpoint selection is based on theaverage metrics across all tasks. To enhance fine-tuning efficiency, we leverage libraries like Hug-gingFaces Transformers (Wolf et al., 2020) andPEFT (Mangrulkar et al., 2022), based on whichwe design MoDULA.",
  "Training Datasets": "To equip our MoDULA-Flan and MoDULA-Res with comprehensive capabilities across uni-versal, mathematical, coding, and medical do-mains, the datasets airoboros-3.2 1, orca-math-word-problems-200k 2, CodeAlpaca-20k 3, andMedQA (Jin et al., 2019) are integrated. In order to evaluate the pluggability of our meth-ods, we fine-tune the baselines, MoDULA-Flan,and MoDULA-Res on three datasets from differ-ent domains: FinGPT-headline 4 from the financedomain, and Title-Optimization and Keyword-Recommendation from the e-commerce do-main.The Title-Optimization and Keyword-Recommendation datasets are sourced from real-world requirements on alibaba.com 5, a leadinge-commerce platform. By fine-tuning on these di-verse datasets, we aim to demonstrate the adaptabil-ity and effectiveness of MoDULA-Res in variousdomain-specific applications, showcasing its mod-ular design and ability to capture both general anddomain-specific knowledge.",
  "Evaluation Benchmarks and Metrics": "To comprehensively assess the performance ofvarious methods, we conduct evaluations acrossa diverse set of benchmarks.Domain-specificperformance is evaluated by testing mathematicalabilities on GSM8K (Cobbe et al., 2021), Arith-metic (Brown et al., 2020), and MathQA (Aminiet al., 2019), coding skills on HumanEval (Chenet al., 2021) and MBPP (Austin et al., 2021), andmedical knowledge on MedQA (Jin et al., 2020)",
  "MoDULA-Flan47.935.9246.596.38MoDULA-Res48.286.9447.887.58": ": Experimental results of methods on T.O. andK.R. (e-commerce) benchmarks. Avg. denotes theaverage performance of different methods on domain-specific benchmarks. T.O. denotes the Title Optimiza-tion task and K.R. the Keyword Recommendation task. and the Medical (Jin et al., 2019) dataset. Generalcapabilities are measured via MMLU (Hendryckset al., 2021) and C-Eval (Huang et al., 2023b)benchmarks, which both cover a wide range oftasks. To evaluate the pluggability and adaptabil-ity of different methods on new domain-specifictasks, we test their performance on the FinGPT-headline (Yang et al., 2023) dataset from the fi-nance domain, as well as the Title-Optimizationand Keyword-Recommendation datasets from thee-commerce domain.Title optimization and keyword recommenda-tion are critical tasks in e-commerce that aim toenhance product visibility and market responsive-ness. These tasks involve integrating high-exposurequeries from specific leaf categories into producttitles to refine original titles and generate new keywords, ultimately achieving a higher Click-Through Rate (CTR). By evaluating the methods ofthese real-world e-commerce tasks, we can assesstheir effectiveness in capturing domain-specificknowledge and potential for practical applicationin industry settings. The specific evaluation met-rics used for each benchmark are summarized in, providing a clear overview of the perfor-mance measures employed in our experiments.",
  "Main Experimental Results": "Our experimental results yield several significantobservations that demonstrate the robustness andeffectiveness of the proposed approach, providingvaluable insights into its performance across vari-ous benchmarks and real-world applications.Superior Advancement over Baselines: Ta-ble 1 highlights the significant performance im-provements achieved by our proposed paradigmacross Qwen, LLaMA-2, and Yi.Models thatare fine-tuned with our paradigm outperform thebase models by an average of 16.6% and surpassthe performance of MoLoRA by 6.3% on average.Notably, Yi demonstrates the most substantial im-provement, with an impressive average increase of10.9% over MoLoRA.Further analysis reveals that performance ad-vancements are more pronounced in smaller-scalemodels than in their larger counterparts, e.g., 4.9%for Qwen-7B while 2.9% for Qwen-14B. This in-dicates that small-scale models with fewer param-eters and inadequate training are more prone tolosing general capability when learning multipletasks, while residual connections can effectivelymitigate this problem. Moreover, MoDULA-Flan does not consistentlyoutperform MoLoRA, suggesting that it has theissue of decreased general capabilities (for exam-ple, the arithmetic benchmark of LLaMA-2-13Bdropped sharply due to the decline in text under-standing ability). In contrast, MoDULA-Res ad-dresses this issue by introducing residual connec-tions for general and expert modules, leading tomore stable performance and significant improve-ments over MoLoRA and MoDULA-Flan.Despite MoDULA-Res demonstrates overallstrong performance, it faces challenges withGSM8K and MedQA tasks, likely due to the mis-match between pre-training data and task-specificrequirements. We recognize these limitations andleave them for further research.Excellent Robustness on ComprehensiveBenchmarks: In order to determine whether thegeneral capability of MoDULA-Res trained onmultiple tasks will decline, we conduct experi-ments using the base, MoLoRA, and the MoDULA-Res model on the comprehensive benchmarksMMLU and C-Eval.The results in indicate that the averageperformance of MoDULA-Res across multiple mod-els is about 1% higher than that of MoLoRA andthe base model, suggesting that the models gen-eral capability is maintained and even partially im-proved through residual connection.Flexible Pluggability over Baselines: To show-case MoDULA-Ress pluggability, we introducethe finance domain (FinGPT-headline) in addi-tion to the initial domains of mathematics, coding,and medical care. Then, we retrained MoLoRA,MoDULA-Flan, and MoDULA-Res, respectively.MoLoRA is trained from scratch on the com-bined dataset, while MoDULA-Flan and MoDULA-Res only require training a new financial expertand the router. This results in MoDULA-Flan andMoDULA-Res using only 19.8% and 37.3% ofthe training parameters and data compared toMoLoRA, respectively.The results in indicate that MoDULA-Res achieves the best average multi-task perfor-mance among the three models, with an averageimprovement of 8.0% in the financial task. No-tably, the overall improvement of Yi-6B is moresignificant, exceeding 11.0%, due to the fewerparameters and relatively balanced pre-trainingdata. MoLoRA encounters issues with data bal-ance, requiring numerous experiments to adjust thedata ratio for each task to achieve the best overall performance when new domain-specific tasks areintroduced, which is time-consuming and labor-intensive.Outstanding Performance in E-Commerce:To assess MoDULAs practical applicability in e-commerce, we introduce title optimization andkeyword recommendation tasks, which involve re-fining titles and generating keywords using high-exposure queries to enhance readability and includemore key points. We employ GPT-4 to evaluate theoptimized titles and keywords across five dimen-sions: helpfulness, relevance, accuracy, readability,and fluency. Each dimension is scored 0, 1, or 2,with a maximum total score of 10. demonstrates that MoDULA-Res signifi-cantly improves performance on title optimizationand keyword recommendation benchmarks, withgains of 44.7% and 24.3% over MoLoRA, respec-tively. Moreover, MoDULA-Res maintains superiorperformance on the original multi-task benchmarks.These results highlight MoDULA-Ress potentialfor e-commerce applications and adaptability tonew tasks under resource constraints.",
  "Analysis on Domain-specific ExpertsAllocation": "To further analyze MoDULA-Res, router distribu-tions for domain-specific experts based on Yi-6Band Qwen-14B are visualized in . Modelsin are reused, and we select layer 0-10-20-30 and 10-20-30-40 for Yi-6B and Qwen-14B,respectively.The results indicate that for both the Yiand Qwen models, the router within the MoD-ULA paradigm allows various experts to concen-trate on their own domain. However, the interpreta-tion of expert assignments varies across differentlayers in different models due to the models train-ing data and method. For instance, Yis deeper lay-ers focus more on separating experts, while Qwenin the shallower layers.",
  "Limitations": "While our proposed MoDULA paradigm shows sig-nificant advancements in parameter efficiency andmulti-task adaptability for LLMs, there are stillsome limitations that need to be addressed. Despitethe overall strong performance of MoDULA-Res, itshows sub-optimal results on certain benchmarkslike GSM8K and MedQA. This may be due to dis-crepancies between the models pre-training dataand the specific task datasets, requiring further in-vestigation to identify the root causes and developtargeted solutions. Our experiments also focus on alimited set of language models (LLaMA-2, Qwen,Yi) and domain-specific tasks (mathematics, cod-ing, medical, finance, e-commerce). To establish stronger generalizability, it would be valuable toextend our evaluations to a broader range of basemodels and diverse task domains. Furthermore,the current study primarily emphasizes the plug-gability and training efficiency of MoDULA whenincorporating new domain experts. However, thescalability and robustness of this approach when in-tegrating a larger number of experts require furtherexploration and stress testing.Future research directions include investigatingtechniques to mitigate performance degradation onspecific benchmarks, conducting comprehensiveevaluations on a wider range of models and tasks,exploring the scalability limits of expert integra-tion, streamlining the multi-stage training process,and enhancing the interpretability of the routersdecision-making. By acknowledging these limi-tations and outlining potential avenues for futurework, we aim to provide a balanced perspectiveon the current state of our research and highlightopportunities for further advancements in PEFT forLLMs. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,and et al. 2023.Gpt-4 technical report.arXivpreprint arXiv: 2303.08774. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Maitha Alhammadi, Mazzotta Daniele, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.",
  ". tiiuae/falcon-180b": "Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.2019. Mathqa: Towards interpretable math wordproblem solving with operation-based formalisms.arXiv preprint arXiv: 1905.13319. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, and et al.2023a. Gemini: A family of highly capable multi-modal models. arXiv preprint arXiv: 2312.11805. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, and et al. 2023b.Palm 2 technical report.arXiv preprint arXiv: 2305.10403. Jacob Austin, Augustus Odena, Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie Cai, Michael Terry, Quoc Le, andCharles Sutton. 2021. Program synthesis with largelanguage models. arXiv preprint arXiv: 2108.07732.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, and et al. 2023. Qwen technical report. arXivpreprint arXiv: 2309.16609": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, and et al. 2020. Language models are few-shot learners. In Proceedings of the 34th Interna-tional Conference on Neural Information ProcessingSystems, volume 159 of NIPS20, pages 18771901,Vancouver, BC, Canada. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,Henrique Ponde de Oliveira Pinto, Jared Kaplan,Harri Edwards, Yuri Burda, Nicholas Joseph, andGreg Brockman et.al. 2021. Evaluating large lan-guage models trained on code. arXiv preprint arXiv:2107.03374. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems. arXiv preprint arXiv: 2110.14168. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota.",
  "Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024.How abilities in large language models are affectedby supervised fine-tuning data composition. arXivpreprint arXiv: 2310.05492": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.Glm: General language model pretraining with au-toregressive blank infilling. arXiv preprint arXiv:2103.10360. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations.",
  "Chengsong Huang, Qian Liu, Bill Yuchen Lin, TianyuPang, Chao Du, and Min Lin. 2023a. Lorahub: Effi-cient cross-task generalization via dynamic lora com-position. arXiv preprint arXiv: 2307.13269": "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, JunleiZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu,Maosong Sun, and Junxian He. 2023b.C-eval:A multi-level multi-discipline chinese evaluationsuite for foundation models. arXiv preprint arXiv:2305.08322. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,Hanyi Fang, and Peter Szolovits. 2020. What dis-ease does this patient have? a large-scale open do-main question answering dataset from medical exams.arXiv preprint arXiv: 2009.13081.",
  "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.Cohen, and Xinghua Lu. 2019. Pubmedqa: A datasetfor biomedical research question answering. arXivpreprint arXiv: 1909.06146": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv: 2001.08361. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-hta, Tenghao Huang, Mohit Bansal, and Colin Raffel.2022. Few-shot parameter-efficient fine-tuning isbetter and cheaper than in-context learning. arXivpreprint arXiv: 2205.05638. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu,Derong Xu, Feng Tian, and Yefeng Zheng. 2024.When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Pro-ceedings of the 47th International ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 11041114.",
  "Sourab Mangrulkar, Sylvain Gugger, Lysandre De-but, Younes Belkada, Sayak Paul, and BenjaminBossan. 2022.Peft: State-of-the-art parameter-efficient fine-tuning methods": "Thomas Mesnard, Cassidy Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,Morgane Rivire, Mihir Sanjay Kale, Juliette Love,and et. al. Pouya Tafti. 2024. Gemma: Open mod-els based on gemini research and technology. arXivpreprint arXiv: 2403.08295. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, and et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv: 2211.05100. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models.arXivpreprint arXiv: 2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, and et al. 2023b. Llama 2: Open foundationand fine-tuned chat models. arXiv preprint arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Proceedings of the 31st InternationalConference on Neural Information Processing Sys-tems, NeurIPS17, pages 60006010, Long Beach,California, USA.",
  "Hongyang Yang, Xiao-Yang Liu, and Christina DanWang. 2023. Fingpt: Open-source financial largelanguage models. arXiv preprint arXiv: 2306.06031": "Alex Young, Bei Chen, Chao Li, Chengen Huang,Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, Kaidong Yu, PengLiu, Qiang Liu, Shawn Yue, Senbin Yang, ShimingYang, Tao Yu, Wen Xie, Wenhao Huang, XiaohuiHu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, YuchiXu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu,Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Openfoundation models by 01.ai. arXiv preprint arXiv:2403.04652. Ted Zadouri, Ahmet stn, Arash Ahmadian, Beyza Er-mis, Acyr Locatelli, and Sara Hooker. 2024. Pushingmixture of experts to the limit: Extremely parameterefficient moe for instruction tuning. In The TwelfthInternational Conference on Learning Representa-tions. Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,Zhiwei Wang, Xin Jiang, ZhenZhang Yang, KaishengWang, and Xiaoda Zhang et al. 2021.Pangu-:Large-scale autoregressive pretrained chinese lan-guage models with auto-parallel computation. arXivpreprint arXiv: 2104.12369. Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, ChaoXu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan,Songyang Zhang, and Shuangrui Ding et al. 2023.Internlm-xcomposer: A vision-language large modelfor advanced text-image comprehension and compo-sition. arXiv preprint arXiv: 2309.15112. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, and Xi Victoria Lin et al.2022. Opt: Open pre-trained transformer languagemodels. arXiv preprint arXiv: 2205.01068. Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang,Tianlong Chen, Lei Shu, Han Lu, Canoee Liu,Liangchen Luo, Jindong Chen, and Lei Meng. 2023.Sira: Sparse mixture of low rank adaptation. arXivpreprint arXiv: 2311.09179.",
  "AAnalysis on the Residual Connection": "The results in validate the importance of theresidual connection in the MoDULA-Res method.Comparing MoDULA-Res with its non-residualcounterpart reveals the residual connections role inenhancing domain-specific tasks while preservinggeneral language understanding.The residual connections impact varies amongmodels. For instance, Qwen-7B and Yi-6B modelsshow significant score improvements of 1.71 and3.01 points, respectively, whereas LLaMA-2-7Bshows a smaller gain of 1.77 points. This suggeststhat the benefits may be model-specific, meritingfurther investigation.In domain-specific tasks, MoDULA-Res excels,particularly in mathematics and medical fields.For example, in Arithmetic and Medical datasets,MoDULA-Res exceeds its non-residual variant byover 5 points, signifying the residual connectionsrole in effective knowledge transfer.However,in some tasks like MBPP andMedQA, the non-residual model slightly outper-forms MoDULA-Res. This nuance suggests a needto further analyze the residual connections mecha-nism across various tasks to improve the modelsrobustness.In conclusion, the findings affirm the MoDULA-Res methods efficacy. Residual connections sig-nificantly enhance overall performance on domain-specific tasks, offering a promising avenue for fu-ture enhancements in the PEFT paradigm. Contin-ued exploration of residual connections in multi-task learning is expected to yield more powerfuland versatile language models."
}