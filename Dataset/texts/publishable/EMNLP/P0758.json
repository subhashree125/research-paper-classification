{
  "Abstract": "Recent advances in instruction-tuned LargeVision-Language Models (LVLMs) have im-bued the models with the ability to generatehigh-level, image-grounded explanations withease. While such capability is largely attributedto the rich world knowledge contained withinthe Large Language Models (LLMs), our workreveals their shortcomings in fine-grained vi-sual categorization (FGVC) across six differentbenchmark settings. Most recent state-of-the-art LVLMs such as LLaVa-1.5, InstructBLIPand GPT-4V not only severely deteriorate interms of classification performance, e.g., av-erage drop of 65.58 in EM for Stanford Dogsfor LLaVA-1.5, but also struggle to generatedescriptive visual attributes based on a conceptthat appears within an input image despite theirprominent zero-shot image captioning ability.In-depth analyses show that instruction-tunedLVLMs suffer from modality gap, showing dis-crepancy when given textual and visual inputsthat correspond to the same concept. In an ef-fort to further the communitys endeavor in thisdirection, we propose a multiple granularityattribute-centric benchmark and training mix-ture, FINER, which aims to establish a groundto evaluate LVLMs fine-grained visual com-prehension ability and provide significantly im-proved explainability.",
  "Introduction": "In recent years, Large Vision-Language Models(LVLMs) that are able to generate image-groundedtext have seen significant progress. Models suchas InstructBLIP (Dai et al., 2023) and LLaVA(Liu et al., 2023b,a) have consistently exhibitedstrong zero-shot capability in generating imagecaptions, visual reasoning and textual descriptions,and even leveraging external knowledge for com-plex question answering tasks (Marino et al., 2019;Schwenk et al., 2022). Such results across diversebenchmarks indicate that these models, most ofthem being built on large language models (LLMs)",
  "Kakapo (Owl Parrot)": ": Current state-of-the-art LVLMs exhibit strongzero-shot downstream task solving abilities (e.g., imagecaptioning, VQA, reasoning). However, when promptedto classify the fine-grained concepts, most of them failto distinguish them into finer categories. Fine-grainedclassification prompt here is omitted for brevity. like Vicuna (Chiang et al., 2023), Flan-T5 (Chunget al., 2022), Llama (Touvron et al., 2023), are al-ready equipped with the ability to simultaneouslyleverage the interplay between the textual para-metric knowledge acquired during pre-training andthe image understanding ability acquired duringinstruction-tuning. Notably, all of these modelsexhibit strong zero-shot task transferability to mul-tiple downstream tasks.Conventionally, in the computer vision domain,many previous works on fine-grained visual clas-sification (FGVC) (Wei et al., 2022b; Diao et al.,2022; Zhu et al., 2022; Yang et al., 2022; Wanget al., 2022) sought to accurately classify diverseimages ranging from different types of birds, plants,animals (Van Horn et al., 2015, 2018) and artifi-cial objects such as cars (Krause et al., 2013) andaircrafts (Maji et al., 2013). In this work, we in-vestigate whether state-of-the-art LVLMs can com-bine their image understanding ability and rich tex-tual knowledge acquired during pre-training to han-dle zero-shot FGVC. To our surprise, while the LVLMs perform almost perfectly, e.g., 98.43 forLLaVA-1.5 (13B) on iNaturalist, at superordinate-level granularity (e.g., birds, jets), their classifi-cation abilities do not extend to the coarse andfiner-grained concepts (e.g., bald eagle, F-22 Rap-tor), exhibiting substantially deteriorated classifi-cation performance (3.2); 46.91 for coarse-leveland 1.56 for fine-level categories on iNaturalist.Our empirical analyses of these models reveal thatthese models suffer from modality gap. We em-pirically demonstrate that such discrepancy stemsfrom LVLMs limited ability to exploit the richparametric knowledge given image input, to inferfine-grained concepts. We also show that such con-straints lead to diminished fine-grained understand-ing of the image, preventing these models fromgenerating accurate and detailed visual attributesof the concepts that appear within an image.We also present an attribute-centric and multi-ple granularity classification benchmark and train-ing mixture, FINER. Our benchmark constructsconcept-indicative attributes for six conventionalFGVC benchmarks like iNaturalist (Van Horn et al.,2018) and FGVC-Aircrafts (Maji et al., 2013) by(i) generating multiple granular concept labels forvisual concept recognition, and (ii) constructing aset of visual attributes per fine-grained concept tomeasure the ability of LVLMs to accurately gen-erate fine-grained concept descriptions given animage. To summarize, our contributions include: We highlight the lack of fine-grained imagecomprehension ability of instruction-tunedLVLMs across various real-life objects. Tothe best of our knowledge, we are the first toexplore FGVC as an evaluation criteria forthese models and their lack of ability thereof. We underscore the persistence of modality gapin state-of-the-art LVLMs by conducting anextensive per-modality-based probing, reveal-ing the discrepancy in how the two modalitiesare processed by these models (4). We construct a novel attribute-centric bench-mark for FGVC to open up a new directionfor future works to measure LVLMs modal-ity gap and their granular image understand-ing capability. Our FINER training mixtureand newly proposed prompting technique AT-",
  "Instruction-tuned Large Vision-LanguageModels": "State-of-the-art LVLMs such as LLaVA (Liu et al.,2023a,b), BLIP-2 (Li et al., 2023), InstructBLIP(Dai et al., 2023), and closed-source models likeGPT-4V (OpenAI, 2023; Yang et al., 2023) havebrought to our attention their zero-shot task solv-ing abilities, especially in downstream tasks suchas Visual Question Answering (VQA), reasoningand image captioning, all of which require outputgeneration conditioned on extensive knowledge ofthe real-world. Based on an intricate interplay be-tween their parametric knowledge and image under-standing ability, they are able to generate sensibleoutputs. However, many of them focus almost ex-clusively on image captioning and reasoning, mostoften disregarding the concept recognition taskstraditional computer vision tasks evaluate on.",
  "Fine-grained Visual Categorization": "Previous works approach FGVC with masked im-age modeling (He et al., 2022; Ryali et al., 2023),concept meta-information injection (Diao et al.,2022), or LLM-generated concept descriptions(Menon and Vondrick, 2023). However, learningof fine-grained visual categories in LVLMs andtheir ability to elaborate on the fine-grained de-tails of the input image via text generation is yet tobe explored. Furthermore, recent works in FGVChave also shown to disregard fine-grained detailsof images (Krojer et al., 2022) and work poorlyon downstream tasks involving localization (Ranas-inghe et al., 2023; Zhong et al., 2022) and objectattributes (Yuksekgonul et al., 2022).",
  "Modality Gap in Vision-Language Models": "Different from instruction-tuned VLMs likeLLaVA and InstructBLIP, contrastively trainedVLMs like CLIP (Radford et al., 2021) and GLIP(Li et al., 2022) rely on directly minimizing the con-trastive objective between visual and textual repre-sentations. A recent analytical work (Liang et al.,2022) on CLIP-like models reveals that there is amodality gap between the text and visual modal-ities, which imposes substantial implications ondownstream task performances. Our work showsthat such modality gap also exists in LVLMs likeLLaVA and InstructBLIP, despite the noticeablearchitectural difference between models like CLIPand LVLMs discussed in this work. : State-of-the-art instruction-tuned LVLM zero-shot performance on fine-grained classification. Allthe models exhibit strong classification capabilities when prompted to classify superordinate-level (e.g., birds,cars) and coarse-grained categories(e.g., owls, SUVs), but exhibit significant deterioration in performance whenprompted to categorize more fine-grained categories on the same images. The gold tags for coarse- and fine-grainedclassifications denote the use of gold labels from the parent category in the prompt.",
  "Evaluation Settings": "DatasetsCovering a wide range of real-worldobjects over various categories, existing FGVCbenchmarks provide richly annotated set of image-concept pairs.As shown in , we useiNaturalist-2021 (Van Horn et al., 2018), FGVC-Aircrafts (Maji et al., 2013), Stanford Dogs (Khoslaet al., 2011), Stanford Cars (Krause et al., 2013),NABirds (Van Horn et al., 2015), and CUB-200-2011 (Wah et al., 2023). For each dataset, we di-vide the ground-truth concept label into three levelsof granularity: superordinate, coarse and fine, asdefined in a previous work (Hajibayova, 2013). Su-perordinate level refers to the highest taxonomicconcepts (e.g., bird, car), coarse level refers to the lower-level granularity concepts (e.g., parrot,SUV), and fine level refers to the lowest, finer-levelgranularity (e.g., owl parrot (Strigops habroptila),Hyundai Santa Fe 2018). We discuss each bench-mark and the construction of superordinate andcoarse-grained labels in detail in . MetricsWe assess the accuracy of the generatedconcept labels given an image and a granularity-specific prompt asking the model to figure out thecorrect category the concept in the image belongsto. Following previous works on concept classifi-cation using auto-regressive models, we employ F1and Exact Match (EM) scores; note that the EMscore used in this work is a modified EM scorethat parses a sequence of generated text and con-siders the output label correct if the ground-truthlabel string exists within a pre-defined maximumnumber of tokens, m (we set m = 20).",
  "attributes (3b)": ": Fine-grained classification pipeline. Ateach level, an output from LVLM is injected into thenext level prompt. (1) Superordinate-level prompt isused to predict the highest-level category (e.g., bird).(2) Coarse-level prompt is subsequently fed with thepredicted output and fed back to the LVLM to generatethe next output (e.g., parrot), and (3a) and (4) followthe same steps. (3b) illustrates ATTRSEEK, a newlyproposed prompting scheme in this work, wherein themodel is prompted to generate the visual attributes. A.1. The open-sourced models like LLaVA-1.5and InstructBLIP follow a generic pipeline of trans-forming an input image Xv with a frozen vision en-coder such as CLIP ViT-L/14 (Radford et al., 2021)into an encoded image representation Zv. Then,these models either project Zv into the languagerepresentation space through a learned projectionlayer W, which becomes Hv = W Zv as in Liuet al. (2023b), or attend over Zv with learnablequeries Q as in Li et al. (2023); Dai et al. (2023).Such transformed visual representations interactwith Xinstruct, a language instruction, which at-tends over the image representations (or queries)within the self-attention layers of the LLM compo-nent to generate the final output sequence.",
  "Brittleness of Vision-Language Models": "Zero-shot Model Performance on FGVCToevaluate the fine-grained image recognition abilityof LVLMs, we measure their classification perfor-mance per granularity by prompting the models togenerate the correct label for a given concept imageas shown in . As illustrated in ,we assess the models classification ability acrossthree different granularity levels. In , weevidence significant deterioration in terms of clas-sification performance across all the five baselines,with some, e.g., iNaturalist-2021, even reachingnear 0% in EM score. While models do perform",
  ": Elicitive prompting results (EM;%) on iNat-uralist. Prompting techniques like Chain-of-Thought(CoT) on the fine-grained classification (Fine) is unableto improve performance in open-source models": "very well for superordinate-level categories, of-ten achieving 100% in EM, the finer granularityleads to substantially worsened classification per-formance. In terms of model size, larger modelslike LLaVA-1.5 (13B) and GPT-4V tend to performbetter than smaller models like the 7B variants. ForInstructBLIP, the 7B version performs better thanthe 13B version by a large margin, with the 13B ver-sion exhibiting less capable instruction-followingability than the 7B one, potentially due to 7B vari-ant being less prone to overfitting and exhibitingefficiency in simple tasks like classification. Elicitive Prompting for FGVCLLMs like Vi-cuna (Chiang et al., 2023) and LLama (Touvronet al., 2023) used as textual reasoning compo-nents of LVLMs are known to perform better whenpresented with elicitive prompts like Chain-of-Thought (CoT) (Wei et al., 2022a) that improvemodels reasoning ability.A unifying thoughtalong this line of prompting techniques is to breakdown a complex problem into a sequence of sub-problems, i.e., divide-and-conquer. Inspired bythese prompting techniques, we propose and eval-uate our prompting technique for FGVC, AT- TRSEEK. In this simple prompting strategy, wefirst (i) prompt the models to generate the most dis-tinctive physical attributes visible in the conceptsin an image, and (ii) feed the generated set of at-tributes along with the concept-asking prompt forfinal prediction as shown in . We evalu-ate the 13B model variants and GPT-4V only oniNaturalist-2021 dataset due to budget constraintof evaluating on the full evaluation set. In ,we do not see much improvement in terms of fine-grained concept classification in the open-source models, with neither CoT nor ATTRSEEK enhanc-ing the classification performance. However, thereis a substantial increase in fine-level performancefor GPT-4V when prompted with our simple yeteffective ATTRSEEK scheme. This result suggeststhat LLaVA-1.5 and InstructBLIP, while they ex-hibit strong image captioning and reasoning ability,are limited in terms of image-grounded attributeunderstanding even when provided with elicitiveprompts; we further elaborate on the need to fine-tune the model according to the newly proposedprompting scheme in .3. This result alsosuggests that open-source LVLMs may lag behindin instruction-following abilities in comparison toGPT-4V. For additional details on few-shot prompt-ing, see Appendix A.3.",
  "Modality Gap: Discrepancy BetweenTextual and Visual Modalities": "We hypothesize that the lack of zero-shot conceptclassification ability of LVLMs arises mainly due tothe modality gap between textual and visual inputs,preventing the models from leveraging the existingconcept-related parametric knowledge when an im-age of a concept is given. Note that modality gapstudied in this work is different from the one previ-ously identified in CLIP-like VLMs (Liang et al.,2022). In this section, we aim to delve into thedetails of how these models process visual and tex-tual modalities by exploring how well they performwhen given only textual descriptions of a concept(4.1) and how they accurately elaborate what theysee in a given image (4.2). We also perform lin-ear probing (4.3) against projected and originalvision encoder output embeddings to gauge the in-fluence of projection and the subsequent loss ofvisual information on the modality gap.",
  "Probing for Concept-Related ParametricKnowledge": "With the drastic performance drop in .2,we first need to verify whether concept-attributeknowledge already exists within the LVLM param-eters to make sure that the models have alreadyacquired the knowledge necessary for zero-shotclassification. The concept-attribute knowledgerefers to the textual parametric knowledge relatedto specific concepts, e.g., a concept Bengal Tigerhas visual attributes dark brown or black stripes.In this experiment, we measure the classifica-tion performance of the models with two differ- : Model Performance on Text-only vs. Image-only Inputs. LLaVA-1.5 (7B and 13B), when providedonly the textual information (7B-, 13B-Text) relatedto the ground-truth concept, outperforms the image-onlyinput (7B-, 13B-Image) counterpart. ent input types: (i) Text-only input that consistsof a concepts external visual attributes (detailsabout the attribute extraction in 5), and (ii) Image-only input that consists of the image of the concept.The text-only input, Xtxt = [I; Attr; C], is com-posed of Instruction (I), visual attributes (Attr),and coarse-grained labels (C); the image-only in-put is Xv = [I; Ximg; C]. The final output is theconcept name. Note, for fair comparison, we alsoinclude C as input for image-only probing. Re-fer to Appendix B.2 for prompts. In , theresults show that even with text-only input that con-tains the detailed physical attributes of a concept,LVLMs are capable of solving fine-grained visualclassification, outperforming the image-only input.The results imply two things: (i) concept-attributeknowledge exists in model parameters, and (ii)while the visual attributes are strongly correlatedwith the concept, the image modalities are inca-pable of leveraging the concept-attribute knowl-edge. We also see that the larger the model size,the better the performance, relating to the amountof parametric knowledge that resides in the LLMs.",
  "Text24.32010.17922.74887.45710.524Image22.6758.81220.47785.4955.067 Avg.1.6441.3672.2711.9615.456": ": Measuring the modality gap via textual similarity against the Web-extracted concept attributes against theLVLM-generated attributes for the iNaturalist subset in FINER. The discrepancy between the attributes generatedfrom Text-only input and Image-only input indicate that VLMs treat the two modalities of the same conceptdifferently. Avg. indicates the average difference between the Text and Image outputs against the reference. reference texts. Then, we prompt the models togenerate a set of external, discriminative physi-cal attributes of a concept when given either animage or text input (see the detailed prompts inAppendix 11); the text input refers to the con-cept label along with a prompt that asks for theconcepts visual attributes. The textual similari-ties between the LVLM-generated attributes andthe web-extracted attributes are compared usingfive different scoring metrics that span both thetoken-level overlap and NLI model-based semanticsimilarity measure: ROUGE-1, 2, L, BertScore(Zhang et al., 2019), and AlignScore (Zha et al.,2023). Both the model-generated attributes andweb-extracted attributes are linearized for textualsimilarity comparison (Appendix A.2).As shown in , text-only inputs showgreater textual similarity to reference attributes, in-dicating that while the concept-attribute knowledgeis being used by the textual modality, the visualmodality does not leverage such knowledge to adegree that matches the textual modality. Asidefrom the modality discrepancy, these models poten-tially fall short on accurately focusing on specificparts of the image, diluting away the fine-graineddetails such as stripes or patterns () for acoarse-grained understanding of the image.",
  ": Linear Probing on Projected Image Em-beddings. Classification accuracy (%) for before andafter image embedding projection to textual space": "the image encoder and use LLaVA-1.5s projector.We freeze both the image encoder and the projectorand finetune a multi-layer perceptron (MLP) layeron top for classification for 10 epochs (for experi-ment details refer to Appendix B.5). As shown in, the loss of visual information encoded bythe vision encoder leads to substantial drop in clas-sification performance across the six FGVC tasks.The results strongly suggest that such loss of visualinformation further contributes to the modality gapbetween the two modalities, especially when per-forming tasks such as FGVC that rely heavily onfine-grained visual attributes.",
  "Categorization Datasets": ": Depiction of the FINER benchmark construction pipeline. Following the aggregation of the six benchmarksin the FGVC domain, concept attributes and concept images are retrieved and extracted from Wikipedia documents. and training mixture, FINER. FINER intends toevaluate the interplay between the concept imageunderstanding and attribute knowledge, in additionto the training mixture that mitigates the modalitygap and enhances fine-grained concept recognition.",
  "Dataset Construction": "We construct FINER based on six different FGVCdatasets: iNaturalist-2021 (Van Horn et al., 2018),FGVC-Aircrafts (Maji et al., 2013), Stanford Dogs(Khosla et al., 2011), Stanford Cars (Krause et al.,2013), NABirds (Van Horn et al., 2015), and CUB-200-2011 (Wah et al., 2023). These datasets spana wide range of objects such as airplanes, insects,plants, birds, mammals and cars, challenging themodels to cover a variety of fine-grained concepts.We first crawl all Wikipedia documents and theirmain images via a search API1. We then extract ex-ternal, visual attributes of a concept (i.e., concept-indicative attributes) with GPT-4V as our attributeextractor (OpenAI, 2023) for its strong zero-shottext span extraction ability (Huang et al., 2024).To briefly elaborate, we divide the extracted at-tributes into two different types: (i) required and(ii) likely attributes. The \"required\" attributes arethe external, concept-indicative attributes that canbe used for concept identification, e.g., blue-tailedhawks with black thorax with a broad apple greenstripe, while the \"likely\" attributes are attributesthat may co-occur with the concept but is notdirectly correlated with the concept, e.g., blue-tailed hawks inhabit trees; we provide the likelyattributes as meta-information since existing mod-els such as MetaFormer (Diao et al., 2022) hasproven that meta-information associated with these fine-grained concepts are beneficial for more accu-rate FGVC performance; however, since the use ofmeta-information for better FGVC is not the mainfocus of this work, we leave it to future works toleverage this information. We also populate thedataset with superordinate and coarse-level con-cept labels for multi-granular concept recognitionperformance evaluation. For example, as shown in, we assign Airplane as the superordinate-level label and Lockheed Martin as the coarse-levellabel since FGVC-Aircrafts dataset provides a con-cept granularity hierarchy, e.g., Boeing Boeing707 Boeing 707 MAX. However, datasets likeStanford Dogs do not provide such granular hierar-chy for their fine-grained concepts. We thereforefew-shot prompt GPT-4V to generate the coarse-level labels and manually inspect their validity. Weprovide the dataset statistics in and extrac-tion prompts in Appendix B.3 and B.4.",
  "Qualitative Analysis": "In , we provide model-generated attributesas a case study on lack of visually-grounded gener-ation. The attributes are from GPT-4V, but note thatthe generated attributes from other models, such asLLaVA-1.5, all exhibit a similar trend. When pro-vided with image-only input, the model generatesa set of attributes that pertain to the image, e.g.,elongated body and two pairs of wings for drag-onfly. However, the attributes from image-only in-put are non-discriminative compared to those fromtext-only input; furthermore, changing the prompt-ing technique to elicit more fine-grained, detailedphysical attributes from the LLMs either lead tohallucination or needlessly verbose outputs thatdescribe non-concept related aspects of the inputimage. In other words, these attributes do not serve",
  "T-tail configuration; Tworear-mounted Rolls-Royce AE3007 series turbofan engines;Straight wing with no winglets;Narrow, tube-like fuselage; Short,nearly oval-shaped passengerwindows": ": Qualitative Analysis of the Text-only and Image-only generated attributes from GPT-4V againstFINER. The generated attributes based on the Text-only and Image-only dataset exhibits notable discrepancy.Juxtaposed with our FINER Attributes, Image-only attributes are not concept-indicative and generic compared toText-only which are discriminative of the concept. We provide FINER attributes as a reference for comparison.We provide the coarse-level and fine-level labels along with the images, and the bounding boxes are only drawn tomatch the highlighted text and are not part of the dataset. as useful knowledge to identify the input image asa specific concept. This again suggests that thesemodels not only fail to properly observe the fine-grained details of a concept, but fail to leverage theknowledge contained within its own parameters ascan be seen from the outputs of text-only inputs.",
  "Enhanced Zero-Shot Transferability fromLearning to Generate Attributes": "To substantiate the effectiveness of our visual at-tributes in FINER, we construct an instruction-tuning mixture based on the ATTRSEEK promptingpipeline to improve the zero-shot attribute genera-tion and FGVC performance (see Appendix A.4).The FINER mixture consists of six subsets, whereeach split is a 5 held-in and 1 held-out FGVCdatasets for training and evaluation, respectively.For instance, for the iNaturalist subset, the iNatural-ist set is not included in training for zero-shot eval-uation. Each instance of the training mixture fol-lows the ATTRSEEK pipeline (3.2). In , wefinetune LLaVA-1.5 (7B) on the training mixtureand see that the FINER-tuned model outperformsthe direct finetuned counterpart that was simplytrained to directly predict the concept label. This implies that in FGVC, instruction-tuning LVLMsto attend to visible attributes in images by explic-itly generating the attributes and then subsequentlyperforming classification improves the models per-formance. Our interpretation is that the generationof the attributes of concepts in the images allow themodel to leverage its concept-attribute parametricknowledge (identified in ), and performbetter zero-shot FGVC classification. It also under-scores the effectiveness of the ATTRSEEK pipelinein model training to improve the inherent, zero-shot capability of LVLMs for fine-grained conceptrecognition. We demonstrate case studies in Ta-ble 5, where we present the zero-shot generatedoutputs of the FINER mixture-trained LLaVA-1.5(7B) and the one that was only finetuned to directlypredict the final concept label. Refer to AppendixA for additional details on training.",
  "LLaVA-1.5 (7B)5.26216.7183.1456.78825.31729.314Direct Prediction21.07122.94212.6107.10924.62428.622FINER20.67336.29713.6927.53029.97432.293": ": Zero-shot Performance on FGVC. Finetuning on the FINER mixture significantly enhances the zero-shotperformance on all six FGVC tasks. We choose LLaVA-1.5 (7B) for this experiment with 1 dataset held-out and 5other datasets held-in for finetuning. Direct Prediction refers to the setting without the ATTRSEEK pipeline and themodel directly predicting the final concept label without the intermediate attribute generation",
  "brown or gray": ": Qualitative analysis of the zero-shot outputs generated by LLaVA-1.5 (7B) instruction-tuned onFINER and Direct Prediction dataset. The generated attributes from the FINER trained LLaVA-1.5 (7B) generatesaccurate, image-grounded outputs when compared to the Direct Prediction counterpart. The attributes are indicatedby their discriminative characteristics in contrast to more generic/hallucinated ones. We provide the coarse-leveland fine-level labels along with the images, and the bounding boxes are only drawn to match the highlighted textand are not part of the dataset. ity gap in LVLMs and propose a prompting scheme,ATTRSEEK, and training mixture, FINER to miti-gate the gap and improve the zero-shot FGVC abil-ity of LVLMs. We also reveal that the loss of visualinformation after projection hinder the effectivecross-modal interplay from manifesting. Such dis-crepancy leads to LVLMs being unable to exploitthe rich parametric knowledge and deteriorates per-formance in visual concept recognition. While thisis the first study of FGVC among instruction-tunedLVLMs, we hope our work would further the re-search endeavors in this direction.",
  "Intra-Concept Variance in ImagesImages ofa single concept can appear in various differentforms. Some images may have the whole view of": "the concept, while other images may have certainparts of the concept (e.g., legs, wings) partiallyoccluded. The attributes collected per concept inFINER are constructed to be visually-groundedbased on the textual attributes extracted from Webdocuments that pertain to these concepts; nonethe-less, such intra-concept variance among imagesmay render an attribute obsolete for certain images.Other problems such as low-quality images mayalso lead to this issue. In future work, exploring thevisual \"ground-ability\" of each attribute throughimage-text retrieval may be a plausible approach toidentifying both the most discriminative attributesthat pertains to an image and the fine-grained con-cept label.",
  "Selection of Baseline ModelsWhile our workcovers LVLMs that receive image and text as in-": "put, there are other VLMs such as Kosmos-2 (Penget al., 2023), Shikra (Chen et al., 2023) and Ferret(You et al., 2023) out there, that receive bounding-box annotated images as input. However, our workonly deals with un-marked image and prompt in-puts, since the objective of this research is to seewhether LVLMs without any referring markings(e.g., bounding boxes), can ground their generativecapabilities on the input image.",
  "Acknowledgement": "We thank the anonymous reviewers for their sugges-tions and comments. We also would like to thankAnsel Blume, Derek Hoiem and Carl Vondrick fortheir ideas, feedback and support for this work.This research is based upon work supported by U.S.DARPA ECOLE Program No. #HR00112390060.The views and conclusions contained herein arethose of the authors and should not be interpreted asnecessarily representing the official policies, eitherexpressed or implied, of DARPA, or the U.S. Gov-ernment. The U.S. Government is authorized toreproduce and distribute reprints for governmentalpurposes notwithstanding any copyright annotationtherein.",
  "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashingmultimodal llms referential dialogue magic. arXivpreprint arXiv:2306.15195": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang,B Li, P Fung, and S Hoi. 2023.Instructblip:Towards general-purpose vision-language modelswith instruction tuning. arxiv 2023. arXiv preprintarXiv:2305.06500.",
  "Lala Hajibayova. 2013. Basic-level categories: A re-view. Journal of Information Science, 39(5):676687": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-otr Dollr, and Ross Girshick. 2022. Masked autoen-coders are scalable vision learners. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1600016009. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Jingwei Huang, Donghan M Yang, Ruichen Rong,Kuroush Nezafati, Colin Treager, Zhikai Chi, Shi-dan Wang, Xian Cheng, Yujia Guo, Laura J Klesse,et al. 2024. A critical assessment of using chatgptfor extracting structured data from clinical notes. npjDigital Medicine, 7(1):106. Aditya Khosla, Nityananda Jayadevaprakash, BangpengYao, and Fei-Fei Li. 2011. Novel dataset for fine-grained image categorization: Stanford dogs. In Proc.CVPR workshop on fine-grained visual categoriza-tion (FGVC), volume 2. Citeseer.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei.2023.Kosmos-2: Grounding multimodal largelanguage models to the world.arXiv preprintarXiv:2306.14824": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Kanchana Ranasinghe, Brandon McKinzie, SachinRavi, Yinfei Yang, Alexander Toshev, and JonathonShlens. 2023. Perceptual grouping in contrastivevision-language models.In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 55715584. Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, ChenWei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggar-wal, Arkabandhu Chowdhury, Omid Poursaeed, JudyHoffman, Jitendra Malik, Yanghao Li, and ChristophFeichtenhofer. 2023. Hiera: A hierarchical visiontransformer without the bells-and-whistles. ICML. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision, pages 146162. Springer. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Grant Van Horn, Steve Branson, Ryan Farrell, ScottHaber, Jessie Barry, Panos Ipeirotis, Pietro Perona,and Serge Belongie. 2015. Building a bird recogni-tion app and large scale dataset with citizen scientists:The fine print in fine-grained dataset collection. InProceedings of the IEEE conference on computervision and pattern recognition, pages 595604. Grant Van Horn, Oisin Mac Aodha, Yang Song, YinCui, Chen Sun, Alex Shepard, Hartwig Adam, PietroPerona, and Serge Belongie. 2018. The inaturalistspecies classification and detection dataset. In Pro-ceedings of the IEEE conference on computer visionand pattern recognition, pages 87698778.",
  "Catherine Wah, Steve Branson, Peter Welinder, PietroPerona, and Serge Belongie. 2023. The caltech-ucsdbirds-200-2011 dataset": "Hao Wang, Junchao Liao, Tianheng Cheng, Zewen Gao,Hao Liu, Bo Ren, Xiang Bai, and Wenyu Liu. 2022.Knowledge mining with scene text for fine-grainedrecognition. 2022 IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages46144623. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022a. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, JianxinWu, Yuxin Peng, Jinhui Tang, Jian Yang, and SergeBelongie. 2022b. Fine-grained image analysis withdeep learning: A survey. IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 44(12):89278948. Xuhui Yang, Yaowei Wang, Ke Chen, Yong Xu, andYonghong Tian. 2022. Fine-grained object classifi-cation via self-supervised pose alignment. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 73997408. Zhengyuan Yang, Linjie Li, Kevin Lin, JianfengWang, Chung-Ching Lin, Zicheng Liu, and LijuanWang. 2023.The dawn of lmms:Preliminaryexplorations with gpt-4v (ision).arXiv preprintarXiv:2309.17421, 9(1):1. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-FuChang, and Yinfei Yang. 2023. Ferret: Refer andground anything anywhere at any granularity. arXivpreprint arXiv:2310.07704. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou. 2022. When and whyvision-language models behave like bags-of-words,and what to do about it? In The Eleventh Interna-tional Conference on Learning Representations. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.2023. AlignScore: Evaluating factual consistencywith a unified alignment function. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1132811348, Toronto, Canada. Associationfor Computational Linguistics.",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-berger, and Yoav Artzi. 2019. Bertscore: Evaluatingtext generation with bert. In International Confer-ence on Learning Representations": "Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-yuan Li, Noel Codella, Liunian Harold Li, LuoweiZhou, Xiyang Dai, Lu Yuan, Yin Li, et al. 2022.Regionclip: Region-based language-image pretrain-ing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages1679316803. Haowei Zhu, Wenjing Ke, Dong Li, Ji Liu, Lu Tian,and Yi Shan. 2022. Dual cross-attention learningfor fine-grained visual categorization and object re-identification. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 46924702.",
  "AExperiment Details": "In this section, we elaborate in detail the experimen-tal settings of our work, including the hyperparam-eter settings of the large vision-language models(LVLMs), and the large language models (LLMs),which are used as a major driving block of theseLVLMs. In addition to the experiment settings, wealso provide details on the training mixture usedin 5.3 and the fine-tuning settings of LLaVA-1.5(7B).",
  ":Hyperparameters of LLaVA-1.5 (7B) forInstruction-tuning on the FINER training mixture": "LLaVA-1.5 or GPT-4V. We therefore opt to usetheir LLM components, Vicuna-7B and -13B, Flan-T5-XL and -XXL to generate the attributes for thetext-only input. The validity of this setting holdssince our goal is to compare the modality gap viahow much the model stores the concept-relatedknowledge within its parameters. MetricsFor ROUGE, we used the PythonsROUGE API2 for calculation, and we only use theF1 score in this work. As for the model-based met-rics, for BertScore (Zhang et al., 2019) we use thebert-base-uncased and for AlignScore (Zhaet al., 2023) we use roberta-large. Thesemodel-based metrics are state-of-the-art models fortextual faithfulness evaluation, making them fit toevaluate both the faithfulness and textual consis-tency of the generated text. Linearization of the Web-Extracted AttributesIn this section, we elaborate on the linearizationprocess of the Web-extracted attributes, which areused as ground-truth reference texts in . The linearization of the attributes is a simpleprocess of concatenating the fine-grained conceptname and the generated attributes of the concept.For example, for a dragonfly named OrthetrumTriangulare, we construct a linearized string thatsays, Orthetrum Triangulare exhibits blue eyes,black thorax with a broad apple green stripe.The format is as follows: [<concept-name>;exhibits; attribute-1; attribute-2;...; attribute-j]; the list is converted into asingle sentence for evaluation;. The same processapplies to both the LVLM-generated attributes and",
  "A.3Enabling Multiple Image Inputs inLVLMs for Few-Shot Prompting": "The three open-sourced LVLMs investigated in ourwork, LLaVA-1.5 (Liu et al., 2023b,a), Instruct-BLIP (Dai et al., 2023) and BLIP-2 (Li et al.,2023) were all pre-trained and instruction-tunedbased on a single image input and a consecutivesequence of pertinent textual instructions. Nonethe-less, these models are capable of receiving multipleimages given its input format. For LLaVA-1.5,we simply provide the few-shot (k) examples sam-pled from iNaturalist dataset in by interleaving k<image> tokens along with the input prompt andtheir ground-truth concept labels. For InstructBLIPand BLIP-2, since they require attending over theinput image via cross-attention with a pre-definedset of query embeddings, we first embed each of thek few-shot sample images with the vision encoder.Then, the image embeddings are passed throughthe Q-Former (Dai et al., 2023; Li et al., 2023)to generate an instruction-attended query embed-dings that contains the image information, and weconcatenate them together to use them as few-shotsamples.",
  "A.4Construction of the Instruction-TuningMixture": "To evaluate the validity of our proposed benchmark,FINER, we construct six different instruction-tuning mixtures on top of LLaVA-1.5s instruction-tuning mixture. For example, to build a trainingmixture to train the model for iNaturalist evalua-tion, i.e., the FINERs iNaturalist subset, we holdout the iNaturalist dataset for evaluation and in-clude the rest of the five other FGVC datasetesinto the instruction-tuning mixture. We use all thesix FGVC datasets to construct the mixture, sam-pling 2.5K instances from each of the datasets andtheir attributes into the training data; note, the 2.5kinstances are sampled to follow a uniform distribu-tion for the number of classes for each dataset. Westructure each instruction-tuning instance into theATTRSEEK pipeline format, with each instructionconsisting of three turns: (i) Asking the model forthe coarse-level concept category given the super-ordinate concept, Can you identify the bird shownin this image?\"; (ii) asking the model to generatea set of external, descriptive visual attributes ofthe coarse-level concept, What kind of external descriptive attributes do you see from the penguin\",and finally (iii) predicting the fine-grained conceptcategory given the coarse-level concept and theself-generated attributes set. For each of the threesteps, we use GPT-4V to generate 15 possible para-phrases of the instruction in order to avoid biasingthe model to specific textual instructions and toretain the models instruction-following ability. Wetrained the models for 1 epoch each; the check-point for 1 epoch is the one we used to evaluate theFGVC performance in .",
  "B.1Fine-Grained Visual ClassificationPrompts": "We structure our prompts as shown in . Fordatasets with less than 100 class categories, weprovide them along with the instruction, allow-ing the models to choose from the provided listof classes. Therefore, for superordinate-levels andcertain coarse-levels, we provide the categories aslists so that the models solve the class generationproblem by choosing from the input prompt; thisis analogous to a multiple choice setting. How-ever, for fine-grained classes, it is difficult to feedin all the concept categories in the input prompt,since some of the datasets like the iNaturalist-2021has 10,000 categories to choose from. In order toconfine the generation scope for the fine-grainedlabels, we decided to input the coarse-level label asdenoted in , to condition the generation ofthe fine-grained output within a specified categoryspace.",
  "B.2Knowledge Probing Prompts": "The knowledge probing prompts are shown in Ta-ble 10. We structure the prompt as explained in.1, where we input the Web-extracted tex-tual attributes along with the coarse-level label forthe text-only setting. Since LVLMs are good atidentifying the superordinate and coarse-level con-cepts, we also provide the coarse-level labels as aprior for the text-only setting for a fair comparisonin the analysis for fine-grained concept knowledge",
  "Dataset:iNaturalist-2021": "Superordinate-levelWhat is the name of the organism that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Arachnids, Mammals, Reptiles,Animalia, Mollusks, Plants, Amphibians, Ray-finned Fishes, Birds, Insects,Fungi]. Coarse-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a bengal tiger, give a coarse-grained label for the image Tiger.Provideyour answer after \"Answer:\". Fine-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a bengal tiger, give a fine-grained label for the image Bengal Tiger oruse its binomial nomenclature Panthera tigris tigris.Provide your answer after \"Answer:\".",
  "Dataset:FGVC-Aircraft": "Superordinate-levelWhat is the name of the object that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Airplane, Car, Train, Bicycle,Cell Phone, Plants, Dogs, Birds, Trucks]. Coarse-levelWhat is the manufacturer of the {concept_placeholder} that appears in this image?Provideyour answer after \"Answer:\" from one of the following categories:[Embraer, LockheedCorporation, Douglas Aircraft Company, Cirrus Aircraft, Airbus, Antonov, deHavilland, Eurofighter, Cessna, Tupolev, Dornier, Yakovlev, Panavia, Robin,ATR, Beechcraft, Dassault Aviation, Fairchild, McDonnell Douglas, Fokker,Gulfstream Aerospace, Boeing, Saab, Canadair, Lockheed Martin, Supermarine,Ilyushin, British Aerospace, Piper, Bombardier Aerospace]. Fine-levelWhat is the name of the airplane model made by {concept_placeholder} that appears in thisimage?For example, if its a picture of a Boeing 787 Dreamliner, give a fine-grained labelfor the image Boeing 787 Dreamliner.Provide your answer after \"Answer:\".",
  "Dataset:Stanford Dogs": "Superordinate-levelWhat is the name of the organism that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Arachnids, Dogs, Reptiles, Mollusks,Plants, Amphibians, Ray-finned Fishes, Birds, Insects, Fungi]. Coarse-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a Golden Retriever, give a coarse-grained label for the image Retriever.Provide your answer after \"Answer:\". Fine-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a Golden Retriever, give a coarse-grained label for the image GoldenRetriever.Provide your answer after \"Answer:\".",
  "B.3Attribute Generation Prompts": "The attribute generation prompts are shown in Ta-ble 11. We divide the attribute generation promptsto three different types: (i) Prompt that generatesthe Web-extracted attributes given the WikipediaAPI retrieved concept documents, (ii) prompt that generates the attributes straight from the modelgiven a text-only input, (iii) prompt that generatesthe attributes from the model given an image-onlyinput. Note that the prompt variance between thetext-only input and image-only input is intention-ally minimized, i.e., minimal change in the inputprompts, to more accurately isolate the effect ofchange in the input modalities. The prompts shown",
  "Dataset:NABirds": "Superordinate-levelWhat is the name of the organism that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Arachnids, Mammals, Reptiles,Animalia, Mollusks, Plants, Amphibians, Ray-finned Fishes, Birds, Insects,Fungi]. Coarse-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a Owl Parrot, give a coarse-grained label for the image Parrot.Provideyour answer after \"Answer:\". Fine-levelWhat is the name of the {concept_placeholder} that appears in this image?For example,if its a picture of a Owl Parrot, give a coarse-grained label for the image Owl Parrot.Provide your answer after \"Answer:\".",
  "Dataset:CUB-200-2011": "Superordinate-levelWhat is the name of the organism that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Arachnids, Mammals, Reptiles,Animalia, Mollusks, Plants, Amphibians, Ray-finned Fishes, Birds, Insects,Fungi]. Coarse-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a Owl Parrot, give a coarse-grained label for the image Parrot.Provideyour answer after \"Answer:\". Fine-levelWhat is the name of the {concept_placeholder} that appears in this image?For example,if its a picture of a Owl Parrot, give a coarse-grained label for the image Owl Parrot.Provide your answer after \"Answer:\".",
  "Dataset:Stanford Cars": "Superordinate-levelWhat is the name of the object that appears in this image?Provide your answer after\"Answer:\" from one of the following categories:[Airplane, Car, Train, Bicycle, CellPhone, Plants, Dogs, Birds, Trucks]. Coarse-levelWhat is the name of the {concept_placeholder} that appears in this image?Provide your answerafter \"Answer:\" from one of the following categories:[Sedan, SUV, Coupe, Convertible,Pickup, Hatchback, Van] Fine-levelWhat is the name of the {concept_placeholder} that appears in this image?For example, ifits a picture of a 2006 Honda Civic LX Coupe, give a fine-grained label for the image 2006Honda Civic LX Coupe.Provide your answer after \"Answer:\".",
  "B.4Coarse-Grained Label GenerationPrompts": "The coarse-grained label generation prompts areshown in . We only generate the coarse-level labels for the following three datasets: (i)Stanford Dogs, (ii) Stanford Cars, (iii) CUB-200-2011, (iv) iNaturalist-2021 because they do not provide concept hierarchy like the rest of the otherthree datasets. For iNaturalist-2021, although thebenchmark does provide the granularity hierarchy,it does so in a taxonomic manner, e.g., order, fam-ily, genus, species, which makes it challenging forthe model to classify the coarse-grained categories;therefore, we generate coarse-grained labels for thedataset as well. By randomly selecting few-shotexamples to guide the coarse-grained label gener-ation, we ensure that the generative model, in this",
  "FINER372K/2.63M161,41611,1713,393,958": ": Overview of the FGVC benchmarks. Our FINER dataset demonstrates richer set of attributes perconcept that enables the evaluation of fine-grained image comprehension. We also augment the benchmarks withoutGranularity Hierarchy with Superordinate Categories and Coarse Categories. case GPT-4V, sticks to the generation of a coarse-grained label. For the faithfulness of the gener-ated coarse-grained labels, we manually evaluatethem for datasets other than iNaturalist-2021. ForiNaturalist-2021, there are 1,103 coarse-grainedcategories, which makes it challenging to evalu-ate each generated coarse-grained label. We there-fore group them together with their correspondingfamily-level category, which serves as a group-ing category for the coarse-grained labels. Forinstance, for Euphaea fraseri, we place its coarse-grained labels, Damselfly and Dragonfly under Eu-phaeidae. By doing so, we not only provide roomfor more encompassing coarse-grained prediction,e.g., Damselflies are also classified as Dragonflies,but also distinguish the granularity setting from thefine-grained level, which requires a more specificcategorization of a given species.",
  "B.5Linear Probing Experiment": "To evaluate the quality of output representations be-fore and after the multimodal projection in LLaVA-1.5 (7B), we perform linear probing over the outputrepresentations of the vision encoder (CLIP-ViT-L/14 (Radford et al., 2021)) and the projected rep-resentations in the textual space. The experimentalsettings are 10 epochs for finetuning and we useaccuracy as the evaluation metric. We train on 4xV100 16GB for 57 GPU hours.",
  "Attribute Gen.(Text-Only)": "What are useful visual features for distinguishing {concept_placeholder} in a photo?Providethe answer as lists of required and likely attributes.For example, for a bengal tiger (FelisTigris) you might say: Required:- yellow to light orange coat- dark brown to black stripes- black rings on the tail- inner legs and belly are white- 21 to 29 stripesLikely:- lives in mangrove, wooded habitat- amber, yellow eyes- large, padded paws- long tail- stout teeth Required attributes are a set of external, physical attributes that allows a human todistinguish it from other similar looking concepts.Likely attributes are a set of attributes that may or may not be visible or are not one ofthe most discriminative features of the concept.In the required (Required:)set, do not include relative, non-visual attributes like size orweight, only the external, visually distinguishable attributes.Provide your response in the above format, saying nothing else.If there are no useful visualfeatures, simply write \"none\".",
  "Likely:- lives in mangrove, wooded habitat- amber, yellow eyes- large, padded paws- long tail- stout teeth": "Required attributes are a set of external, physical attributes that allows a human todistinguish it from other similar looking concepts.Likely attributes are a set of attributes that may or may not be visible or are not one ofthe most discriminative features of the concept.In the required (Required:)set, do not include relative, non-visual attributes like size orweight, only the external, visually distinguishable attributes.Provide your response in the above format, saying nothing else.If there are no useful visualfeatures, simply write \"none\". : Prompts for Attribute Generation. The {concept_placeholder} is replaced with coarse-grained conceptlabels for Image-only case and Web-Extracted cases; for Text-only case, use the fine-grained concept label since wewant to extract the attributes stored in the parametric knowledge by using the fine-grained concept label as a query. Dataset:Stanford CarsGenerate a coarse-grained label for the following fine-grained car types.The coarse-grained car types are as follows:[\"sedan\", \"SUV\", \"coupe\", \"convertible\",\"pickup\", \"hatchback\", \"van\"]."
}