{
  "Abstract": "Large Language Models (LLMs) show remark-able performance on a wide variety of tasks.Most LLMs split text into multi-character to-kens and process them as atomic units with-out direct access to individual characters. Thisraises the question: To what extent can LLMslearn orthographic information? To answer this,we propose a new benchmark, CUTE, whichfeatures a collection of tasks designed to testthe orthographic knowledge of LLMs. We eval-uate popular LLMs on CUTE, finding that mostof them seem to know the spelling of their to-kens, yet fail to use this information effectivelyto manipulate text, calling into question howmuch of this knowledge is generalizable.",
  "Introduction": "Large Language Models (LLMs) attract a lot ofinterest due to their strong performance on manyNLP tasks. They have demonstrated a level offluency rivaling humans. However, it is often over-looked that LLMs lack direct access to the charac-ters composing their tokens. They can only inferknowledge about the characters from the contextduring pretraining or instruction tuning. Whilethere are models that use characters as input units,none of them have been instruction-tuned (to ourknowledge).Our work examines how well LLMs understandthe composition of their tokens. This knowledge en-ables LLMs to better generalize to new languagesand to perform well on a variety of tasks involvingcharacter-level understanding. Tasks such as wordpuzzles, poetry generation (e.g. alliterations), orparsing ciphers all require very explicit use of char-acters to achieve. More popular tasks such as codecompletion, morphological inflection, or spellingcorrection also require character-level informationto a lesser extent, though these tasks also requiresemantic knowledge, which we wish to ablate. We introduce Character-level Understanding ofTokens Evaluation (CUTE)1, a benchmark consist-ing of several tasks designed to be easy for humansto complete, given our ability to process charactersindividually. We evaluate several LLMs rangingfrom 7B to 132B parameters in size on CUTE toanswer the following questions:",
  "Related Work": "Itzhak and Levy (2022) mostly analyze encoder-only models and test if they understand how to spellwords after fine-tuning on 32k examples. Theyconclude that models learn to spell their tokens tosome extent. They also experiment with GPT-2(Radford et al., 2019) which performed similarly tothe other models, and also used training examples.Kaushal and Mahowald (2022) probe models witha task asking if a letter is in a word (similar toour character contains task, see 3). Similarto Itzhak and Levy (2022), their probe requirestraining, as they use models of similar size. Bycontrast, we examine models with 10 to 200 timesas many parameters and apply few-shot promptingwithout fine-tuning.",
  "We release our benchmark open-source at:": "a string of characters. Most of their experimentsconcern a training method they propose, but theyalso include results of GPT-3 (Brown et al., 2020)with few-shot prompting, finding that it performswell on spelling correction, but poorly on othertasks compared to their trained character-basedmodels. Most of their tasks require semantic knowl-edge, which we wish to ablate in our benchmark.Other benchmarks testing orthographic knowl-edge often focus on morphology, such as the SIG-MORPHON inflection tasks (e.g. Goldman et al.(2023)). Inflection is fairly regular for most lan-guages, and could be memorized by a languagemodel. Given that many developers of LLMs donot disclose the sources of their pretraining data,it is possible that LLMs memorize these inflectionpatterns. Therefore, from this we cannot concludethat LLMs can inflect a new word given a newset of rules, and furthermore we cannot concludethat LLMs can apply an arbitrary manipulation toa sequence. Most LLMs are trained with perfor-mance on English in mind, and English being lessmorphological in nature makes inflection a non-ideal measure for a models understanding of thecomposition of tokens.The most similar benchmark to the one we pro-pose is LMentry (Efrat et al., 2023). The purposeof LMentry is similar in that the goal is to test mod-els on tasks that are trivial to humans. Some of thetasks included test orthography and are similar toour tasks, for example, they ask the model to writea word containing a letter, or ask to write the firstor last letter of a given word. Our benchmark isdistinguished from LMentry as most of our tasksexplicitly require knowledge of every character ina word. Testing whether a model knows the firstletter of a word is not sufficient for concluding thatit understands orthography, as there may be morepressure to learn about the first letter of a wordfrom pretraining and/or instruction tuning (e.g. viaalliterations or acronyms). As such, the task towrite a word containing a letter could be more triv-ially solved by writing a word that starts with saidletter.There is an extensive body of research oncharacter-level models, where each character formsa token (Lee et al., 2017; Xue et al., 2022; Tay et al.,2022, inter alia). Several works compared thesemodels to subword models (Libovick et al., 2022;Edman et al., 2022, 2024, inter alia), but they eval-uated on tasks requiring additional training. Weassume that character-based models would perform Spell out the word: there Write the word that is spelled out (no spaces): t h e r e",
  "Input": "Is there a 'c' in 'there'? Is there a 'the' in 'the sky is blue'? Closer in Levenshtein distance to 'happy': glad or apply? More semantically related to 'happy': glad or apply? Add 'b' after every 'e' in 'there' Add 'is' after every 'the' in 'the sky is blue' Delete every 'e' in 'there' Delete every 'the' in 'the sky is blue' Replace every 'e' with 'a' in 'there Replace every 'the' with 'is' in 'the sky is blue' Swap 't' and 'r' in 'there' Swap 'the' and 'is' in 'the sky is blue'",
  "Benchmark": "We split our tasks into 3 categories: understandingcomposition, understanding orthographic similar-ity, and ability to manipulate sequences. shows an example for each task.2 Data gatheringand processing details can be found in Appendix C.Our tasks are synthetically generated from exist-ing corpora. There are non-synthetically gener-ated datasets which partially test our research ques-tions, but these datasets have external factors (e.g.domain and/or language in a translation dataset)that would likely obscure our findings. Some ofthese datasets also might have been leaked intothe LLMs pretraining data and been memorized,resulting in an unrealistically good performance.",
  "Composition": "We start with a straightforward benchmark:spelling. Similar to Itzhak and Levy (2022), weinclude a task where the input is a word given as asingle token3, and the output is the same word withspaces in between, so that each character becomesa separate token. This is the most straightforwardprobe to see whether a model has knowledge ofthe characters forming the tokens. We also addthe inverted task (inverse spelling) to check ifcharacters can also be mapped to tokens.Another method for assessing a models under-standing of composition is to ask if a token containsa certain character. If a model managed the previ-ous tasks, we would expect it to succeed here as 2The prompts shown here are not the full prompts. SeeAppendix B for more details.3This is in the ideal case. We cannot guarantee every LLMtested uses only a single token for each input, but we minimizethe chance of splitting by using frequent words in our task. well. However, a model might not understand therelationship between spelling and membership ofcharacters to a word, so we test this as well. Wealso test whether the LLMs are able to solve thecorresponding word-level task (i.e. is a word ina sentence) to separate the models general under-standing of the task from its ability to solve the taskat the character level.",
  "Similarity": "Since the introduction of word2vec (Mikolov et al.,2013), language models have typically been trainedto predict words from their context. The resultingtoken embeddings mainly reflect the semantic andsyntactic similarity of tokens. Our next tasks exam-ine whether LLMs also comprehend orthographicsimilarity. We ask which one of two candidatewords is orthographically (or semantically) moresimilar to a given word. Our candidate words arechosen to be relatively easy to distinguish for ahuman without explicit knowledge of how to mea-sure orthographic or semantic similarity. For moredetails, see Appendix C.",
  "Manipulation": "Our previous tasks focus on the understanding ofthe model. Now, we turn our focus to acting onthat understanding. The next tasks involve 4 typesof manipulation of the input at the character orword level: Insertion, Deletion, Substitution, andSwapping. We consider these tasks as elementarytasks for modifying a text sequence. InsertionFirst we test how well the model caninsert an element X after every instance of someelement Y in the sequence. Similar modificationsoccur when we replicate letters to emphasize aword (e.g. Yay! vs. Yaaaaay!), or when weadd an adjective next to a noun. DeletionDeletion requires the model to recog-nize an element and remove all instances of it. Thiscan occur in natural language at the character levelwith inflection in languages (e.g. turning an En-glish plural noun into singular), or removing adjec-tives from a sentence. SubstitutionSubstitution replaces all instancesof an element in a sequence with another element.This can occur with spelling or vocabulary varia-tions across dialects or related languages (e.g. de-fense vs. defence, or elevator vs. lift). SwappingSwapping is a simplified case of re-ordering acting on two elements.4 Though reorder-ing is not very common in English, it featuresheavily in languages with free word order, suchas Greek, where stressed words can be moved tothe front of the sentence.",
  "The models perform very well on the tasksspelling and inversespelling,thoughinverse spelling appears slightly more difficult": "4Due to the poor performance on swapping, we leave outmore complex forms of reordering, but these could be easilyadded in the future.5We link all of the models in Appendix A.6We define freely available as those not requiring pay-ment for use, and with available information on the trainingprocess. Command-R+ Llama3-70B DBRX Llama3-8BMistral-47B Aya-35B Command-RLlama2-70BLlama2-13B Mistral-7B Gemma-7B Aya-8B Llama2-7B SpellingInverse SpellingContains WordCharrandom",
  ": Accuracy on each task of CUTE. Models ordered by average accuracy over all tasks": "Although we are not aware of any spelling tasks ininstruction tuning or pretraining datasets, we sus-pect that the similarity of this task with data seenduring training allows the models to perform verywell, in contrast to the following tasks.On the contains tasks, the performance at theword level is quite good, showing that the modelsunderstand the task, but the performance breaksdown at the character level. This indicates themodels do not fully understand the relationshipbetween spelling and membership of a character toa word.",
  "Orthography and Semantic Similarity": "In the semantic similarity task, the models cor-rectly choose the more semantically related word76-93% of the time (with the exception of Aya-8B), and the performance generally increases withmodel size. For orthographic similarity, theperformance is below or near random for all mod-els except Command-R+ and Llama3. It is not yetclear why they perform so well on this task, butapparently it is not solely a scaling effect sinceDBRX fails to perform above random chance. Itmay be due to the amount of training data, howeverDBRX and Command-R+ have not disclosed theseamounts.",
  "In our manipulation tasks, the models strugglemore at the character level than at the word level.The difference is quite profound, with perfor-mance gaps of up to 72.8% on Command-R+ for": "insertion. Larger models such as Command-R+perform well on deleting characters, with 72% ac-curacy, though it should be noted that we are onlytesting on the 1000 most frequent words, makingthe evaluation fairly generous.Like the contains task and orthographicsimilarity task, the manipulation tasks show thatLLMs lack a complete understanding of their to-kens, although they can literally spell them out.The higher word-level performance indicates thatit is not due to a lack of understanding of the taskitself.",
  "Vocabulary Size and Multilinguality": "There appear to be no noticeable effects of vocab-ulary size from the results shown. Looking at the7/8B models, while Llama 3 performs well with avocabulary size of 100k, using a larger vocabulary(i.e. Gemma) does not improve performance, andneither does using a smaller vocabulary (i.e. Llama2 and Mistral). It remains to be seen if noticeableeffects arise as the vocabulary size approaches thenumber of characters. We leave this for future re-search.For multilinguality, the results are similarlymixed. We focus on Aya-35B versus Command-R,as Aya is a fine-tuned version of Command-R, us-ing multilingual instruction tuning data. Overall,Aya makes slight improvements over Command-Ron the character-level, but this could easily be dueto training on additional English data, rather thanthe additional non-English data.",
  "Scaling": "There are 2 major factors to scale: parameter countand training data. In terms of parameter count,larger models clearly tend to perform better. Withrespect to amount of training data, only Llama 2and 3 have disclosed this, and based on the results,it appears that more training data also improvesperformance. This aligns well with the myriad ofworks showing the benefits of scaling and raisesthe question: Is scaling all we need for good per-formance on character-level tasks? Looking at themanipulation tasks, it seems that deletion andsubstitution could become manageable in thenear future, but for insertion and swapping, theperformance gap between word and character leveltasks is large. Many real-world text manipulationtasks are a combination of the tested tasks, so wewill likely need more than just scaling.",
  "Conclusion": "While current LLMs with BPE vocabularies lackdirect access to a tokens characters, they performwell on some tasks requiring this information, butperform poorly on others. The models seem tounderstand the composition of their tokens in di-rect probing, but mostly fail to understand the con-cept of orthographic similarity. Their performanceon text manipulation tasks at the character levellags far behind their performance at the word level.LLM developers currently apply no methods whichspecifically address these issues (to our knowl-edge), and so we recommend more research tobetter master orthography. Character-level modelsare a promising direction. With instruction tun-ing, they might provide a solution to many of theshortcomings exposed by our CUTE benchmark.",
  "Limitations": "We prompt instruction-tuned LLMs without anyfine-tuning on benchmark data. While this can beseen as a limitation, we note that it is not feasible toadd more training data whenever we discover a newissue with LLMs. We expect that the performanceof all models would increase after fine-tuning.We do not evaluate any character-level modelssince there are no instruction-tuned versions (to ourknowledge). Additionally, there are no decoder-only pretrained LLMs available, with the closestmodel being ByT5-XXL (13B), which has a heavyencoder and smaller decoder. Training character-level models also requires a much higher computa- tional budget, as the sequence lengths are roughly5 times longer, resulting in 5 times longer training.As such, training a truly comparable model fallsoutside the scope of this work.Our benchmark does not control whether LLMtokenizers split words into multiple tokens. Weminimize that chance by choosing frequent words,but we can never guarantee that future models willnot split words into multiple tokens. We foundthat the impact of removing split tokens is minimal,with less than 1% change on average (see AppendixF).We only test on English and Russian, with ourprimary focus being on English. While we didnot see any major differences in the LLMs perfor-mance between English and Russian when it comesto character-level versus word-level performance,it is possible that there may be differences in otherlanguages.Lastly, we do not control for generations thatdo not match the pattern of the examples givenin the prompt. Therefore, we cannot guaranteethat all generations considered correct by humansare evaluated as such. Hence the performance ofsome models may be lower than expected. Weprovide the outputs of all models in our repositoryfor further analysis.",
  "Piotr Bojanowski, Edouard Grave, Armand Joulin, andTomas Mikolov. 2017. Enriching word vectors withsubword information. Transactions of the Associa-tion for Computational Linguistics, 5:135146": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020. Language models are few-shot learn-ers. Preprint, arXiv:2005.14165.",
  "Sondos Mahmoud Bsharat, Aidar Myrzakhan, andZhiqiang Shen. 2023. Principled instructions are allyou need for questioning llama-1/2, gpt-3.5/4. arXivpreprint arXiv:2312.16171": "Lukas Edman, Gabriele Sarti, Antonio Toral, Gert-jan van Noord, and Arianna Bisazza. 2024.Arecharacter-level translations worth the wait? compar-ing byt5 and mt5 for machine translation. Preprint,arXiv:2302.14220. Lukas Edman, Antonio Toral, and Gertjan van Noord.2022. Subword-delimited downsampling for bettercharacter-level translation. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2022,pages 981992, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Avia Efrat, Or Honovich, and Omer Levy. 2023. LMen-try: A language model benchmark of elementarylanguage tasks. In Findings of the Association forComputational Linguistics: ACL 2023, pages 1047610501, Toronto, Canada. Association for Computa-tional Linguistics.",
  "Ronen Eldan and Yuanzhi Li. 2023. Tinystories: Howsmall can language models be and still speak coherentenglish? Preprint, arXiv:2305.07759": "Omer Goldman, Khuyagbaatar Batsuren, Salam Khal-ifa, Aryaman Arora, Garrett Nicolai, Reut Tsarfaty,and Ekaterina Vylomova. 2023. SIGMORPHONUniMorph 2023 shared task 0: Typologically di-verse morphological inflection. In Proceedings of the20th SIGMORPHON workshop on ComputationalResearch in Phonetics, Phonology, and Morphology,pages 117125, Toronto, Canada. Association forComputational Linguistics. Jing Huang, Zhengxuan Wu, Kyle Mahowald, andChristopher Potts. 2023. Inducing character-levelstructure in subword-based language models withtype-level interchange intervention training. In Find-ings of the Association for Computational Linguistics:ACL 2023, pages 1216312180, Toronto, Canada. As-sociation for Computational Linguistics. Itay Itzhak and Omer Levy. 2022. Models in a spellingbee: Language models implicitly learn the charactercomposition of tokens. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: Human",
  "Language Technologies, pages 50615068, Seattle,United States. Association for Computational Lin-guistics": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. Preprint, arXiv:2401.04088. Ayush Kaushal and Kyle Mahowald. 2022. What dotokens know about their characters and how do theyknow it? In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 24872507, Seattle, United States.Association for Computational Linguistics. Jason Lee, Kyunghyun Cho, and Thomas Hofmann.2017. Fully character-level neural machine transla-tion without explicit segmentation. Transactions ofthe Association for Computational Linguistics, 5:365378. Jindrich Libovick, Helmut Schmid, and AlexanderFraser. 2022. Why dont people use character-levelmachine translation?In Findings of the Associa-tion for Computational Linguistics: ACL 2022, pages24702485, Dublin, Ireland. Association for Compu-tational Linguistics.",
  "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay": "Kale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy. Preprint, arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "BPrompting Details": "We show an example of a full prompt in .All of our prompts are available with the releaseof our benchmark. For generation, we use greedysearch.For evaluation of the generation, we rely on thegiven start-quote to denote the start of the answer,and we filter out anything after the end quote (e.g.I hope this answer helped!). Some models alsowere prone to starting generation with a genericresponse such as Sure I can do that for you.. Forthese generations, we observe that it would repeatAnswer: , so we filter out all generations beforethis point. Of the remaining generations, somecould be considered correct though did not matchthe desired pattern (e.g. H-E-L-L-O rather thanh e l l o for the spelling task). These we ultimately",
  ": An example of a full prompt to spell the wordcow, with examples, for the task spelling": "consider incorrect so as not to unfairly elevate anymodels performance.Concerning the wording of the orthographic sim-ilarity task, it could be argued that models do notunderstand the concept of Levenshtein distance,and thus it is not comparable to the semantic sim-ilarity task. We also tested using closer in editdistance and closer in spelling in the prompt,and the results were very similar, so we opted forLevenshtein distance as it is more well-defined.Similarly, we used closer in meaning rather thanmore semantically related and achieved similarresults, though it is debatable whether an antonymshould be considered close in meaning, so we optedfor the latter.",
  "Here we detail our exact method for gathering andprocessing the data into our tasks. The scripts forprocessing the data and the resulting data can befound at our Github repository.7": "Data SourcesFor almost all tasks, we require aset of frequent English words that were most likelyto be tokenized into a single token. For this, weuse a dataset derived from the Google Web TrillionWord Corpus.8 For the word-based tasks, we use the TinyStories(Eldan and Li, 2023) dataset, which consists ofstories written by an LLM in a style appropriatefor a 3-4 year old reader. This has the benefit ofusing simple sentences with a limited vocabulary,maximizing the chances of words being tokenizedinto a single token in the models we test, while alsoensuring that the complexity of the sentence is nota confounding factor in a models performance onthe tasks. FilteringFor character-based tasks, we selectthe 1000 most frequent words that are at least 3characters long. For word-based tasks, we similarlyfilter for 1000 sentences of length 3-10 words, inorder to make the length similar to the number ofcharacters in a word seen in the character-leveltasks.For insertion, deletion, and substitution, we ap-ply the modification to those 1000 words, resultingin our dataset.For swapping, we need to sure that the wordor sentence has 2 items that are unique, so as toavoid an ambiguous prompt (e.g. swap the e andg in engineering). As such, we select the 1000most frequent words or the first 1000 sentences thatsatisfy this criteria, as well as satisfying our lengthconstraints. Similarity DataFor our similarity data, we re-quire our candidate pairs to be sufficiently easy fora human to distinguish which is closer orthographi-cally and which is closer semantically.To accomplish this, our candidate words mustsatisfy two thresholds, one based on normalizedLevenshtein distance (for othographic similarity),and one based on cosine similarity to other fastText(Bojanowski et al., 2017) embeddings (for seman-tic similarity). That is to say, the word must besufficiently similar in one metric (0.7+ and 0.5+for Levenshtein and cosine, respectively) and suf-ficiently dissimilar in the other (0.3 and 0.2,respectively). These thresholds are decided em-pirically. We note that this process occasionallyends up with semantic pairs that are antonyms (e.g.good and bad), and thus we refrain from statingthat the pairs are similar in meaning.",
  ": Evaluation of the performance on random strings versus strings in the vocabulary. Vocab is equivalentto the character-level tasks from": "Data ProcessingOur data processing followedour processing in Appendix C. We collected a fre-quency list of Russian words from Wiktionary9,embeddings from FastText, and we translated theEnglish TinyStories dataset using Google Translate.We adjusted the thresholds for gathering our ortho-graphic and semantic similarity pairs to (0.55+,0.55+) and (0.3-, 0.1-) for Levenshtein and cosine,respectively. The thresholds were more difficultto adjust without excluding too many pairs, andas a result, some of the triplets could be argued asunclear or incorrect. For example, for the triplet (, , ) meaning (com-mon, community, joint), it is less clear which of thelast two are more semantically related to the first(more so in Russian), though joint is the intendedsemantically-related word. The prompts were also machine translated withGoogle Translate and post-edited by a native Rus-sian and fluent English speaker, but after testingwith both English and Russian prompts, we foundthe models generally performed better with Englishprompts (with Russian examples in the prompt), sowe only include those in our results. ResultsWe test a subset of the models used inthe English version, focusing mainly on the multi-lingual LLMs (Aya, Command-R(+), and Gemmato the extent of the tokenizer), as well as Llama 2and 3 for reference. shows a similar trend to the Englishresults. Generally speaking, the character-levelperformance lags behind the word-level. An inter-esting difference is that the models struggle muchmore with spelling. Even though the prompt isin English and shows examples of Russian wordsbeing spelled out, this is not enough for most mod-els to understand the concept of spelling. The ad-ditional multilingual instruction tuning done fortraining Aya appears to be necessary for better per-formance.",
  "ERandom String Evaluation": "While we cannot directly assess the performanceof character-level LLMs without training an equiv-alent model from scratch, we can evaluate the per-formance of the existing models when the numberof tokens per word approaches the number of char-acters per word. We can do this by conducting ourtasks using random strings of consonants ratherthan complete words.Since practically every word in English requiresa vowel, random sequences of consonants are typi-cally quite rare, and thus BPE will not dedicate asingular token for sequences like fxqg. As such,we generate random strings for use in our tasks (ex-cepting the similarity tasks). The resulting stringsuse on average 1.6 characters per token, comparedto 5.4 characters per token in the original word list.In , we can see the performance of theLLMs on the character level tasks using regularwords (as shown before in ), as well as ran-dom strings. We can see that, apart from inversespelling, the models perform the same or betteron random strings than on actual words. Thesetasks appear much easier for LLMs to handle whentheir tokenization is close to character-level, sug-gesting that a truly character-level LLM would per-form the best.As for inverse spelling, the decrease maybe a result of the models bias towards generatingwords. Upon inspection of the outputs, we observethat occasionally the model would hallucinate aword, changing a string such as c q n r w to con-quer, essentially filling in what it considers to bethe missing vowels. This phenomenon is particu- Split Error % DBRX Mistral-47B Gemma-7B Llama2-7B Llama3-8B Command-R+ Mistral-7B Aya-8B Llama2-70B Llama3-70B Aya-35B Command-R Llama2-13B",
  "FToken Splitting Impact": "We mention that while we use frequent words forevaluation to maximize the chance they are given asingle token, we cannot guarantee that some wordswill not be split into multiple tokens. Here, weevaluate the impact of this splitting on our results toelucidate whether this issue could affect the trendswe see in the paper. shows the percent change in accuracywere those examples to be removed from each task.All of the tasks are grouped into a violin plot. Herewe can see that the maximum accuracy differenceis around 3.5%, and the median errors for eachmodel are no greater than 0.5%. This is far fromsubstantially affecting the disparity we see betweenthe character-level and word-level tasks. This alsoreveals that even if an LLMs tokenizer splits aword into two or more tokens, the LLM will stillhave difficulty performing the tasks in the CUTEbenchmark."
}