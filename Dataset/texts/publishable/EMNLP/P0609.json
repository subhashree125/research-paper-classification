{
  "Abstract": "Multimodal sentiment detection aims to clas-sify the sentiment polarity of a given image-text pair. Existing approaches apply the samefixed framework to all input samples, lackingthe flexibility to adapt to different image-textpairs. Furthermore, the interaction patterns ofthese methods are overly homogenized, limit-ing the models capacity to extract multimodalsentiment information effectively. In this paper,we develop a Dual-Branch Dynamic RoutingNetwork (D2R), which is the first multimodaldynamic interaction model towards multimodalsentiment detection. Specifically, we design sixindependent units to simulate inter- and intra-modal information interactions without depend-ing on any existing fixed frameworks. Addition-ally, we configure a soft router in each unit toguide path generation and introduce the pathregularization term to optimize these inferencepaths. Comprehensive experiments on threepublicly available datasets demonstrate the su-periority of our proposed model over state-of-the-art methods.",
  "Introduction": "With the growth of the Internet, people increasinglypost multimodal messages on social media plat-forms to share opinions and express emotions (Yueet al., 2019; Mai et al., 2024; Chen et al., 2024).Consequently, multimodal sentiment detection hasattracted significant attention in recent academicand industrial research, proving beneficial for taskssuch as product review analysis and political opin-ion mining (Liang et al., 2021; Zhang et al., 2023).Unlike unimodal data, multimodal data providesricher information to reveal a persons true emo-tions (Zhang et al., 2018; Liang et al., 2021; Zhonget al., 2024).In this work, we focus on multimodal sentimentdetection for image-text pairs in social media posts.",
  ": Examples of multimodal sentiment tweets": "Previous works employed various fusion strategiesto integrate features from different modalities (Xu,2017; Xu and Mao, 2017). Other approaches in-troduced memory networks for inter-modal interac-tions (Xu et al., 2018; Yang et al., 2020). Yang et al.(2021) constructed a multi-channel graph neuralnetwork model for multimodal sentiment detection,Wei et al. (2023) recently utilized a sparse attentionmechanism to enhance fusion by addressing modalheterogeneity. Although these methods have shownpromising results, they predominantly applied ex-isting static networks to handle all samples, withthe fixed structure resulting in a lack of flexibil-ity to adapt to different multimodal inputs, such asattention-based (Yang et al., 2020) and graph-basedapproaches (Yang et al., 2021). Moreover, existingrelated works are relatively homogenised in termsof interaction, focusing on capturing sentiment in-formation through cross-modal alignment. In somecases, complex interaction patterns are unneces-sary for simple image-text pairs and may introducenoise into the model (Qu et al., 2021). As shownin (a), the word \"love\" in the text clearlyindicates a \"Positive\" sentiment, making extensiveinteraction with the image redundant. Similarly, in (b), the image depicts a crying frog, which di-rectly conveys a \"Negative\" sentiment, reducing theneed for textual interaction. Conversely, in otherinstances, varying levels of interaction betweenmodalities are essential. In (c), a local in-teraction between the\" broken house\" in the imageand \"building collapse\" in the text helps identify itas a \"Negative\" example. However, in (d),no affective cues are captured when interacting thecross-modal global and local information, so themodel classifies it as a neutral example.To tackle the above problems, we propose anovel Dual-Branch Dynamic Routing Network(D2R) for multimodal sentiment detection, whichis a fully dynamic neural network. Specifically, toeffectively address complex multimodal sentimentposts, we design six distinct units to implementinteraction operations under various scenarios forboth text and image without depending on any exist-ing fixed frameworks. Each unit is configured witha soft router to generate inference paths. Subse-quently, we stack these six units in width and depthto construct a complete routing space, enabling theexploration of more complex interaction patternsto flexibly adapt to diverse multimodal inputs. Wealso design a path regularization term to measurethe sentiment-path similarity among samples, aim-ing to optimize these inference paths. Finally, weperform block fusion (Ben-Younes et al., 2019)on the multimodal inference information from dif-ferent branches and feed it into the classifier forsentiment classification.The main contributions of this paper are summa-rized as follows: We propose a novel dual-branch dynamic rout-ing network that dynamically selects routingpaths for diverse image-text pairs. To the bestof our knowledge, we are the first to utilize adynamic routing network to capture affectivecues from different modalities for multimodalsentiment detection. We design six independent units to simulateinter- and intra-modal information interac-tions, with each unit integrating a soft routerfor routing learning and inference path opti-mization through path regularization.",
  "Multimodal Sentiment Detection": "Early works on multimodal sentiment detection uti-lized CNN and LSTM to extract and fuse featurerepresentations from different modalities (Xu andMao, 2017; Xu, 2017). Xu et al. (2018) introducedCoMN, a co-memory network iteratively modeledcross-modal interactions. Yang et al. (2020) pro-posed MVAN, which stacked pool module-tunedmemory network to fuse multimodal features. Ad-ditionally, Yang et al. (2021) developed MGNNS, amultichannel graph neural network to capture emo-tions from entire dataset. Recently, Li et al. (2022)proposed CLMLF, a model combined contrastivelearning and multilayer fusion. Another work (Weiet al., 2023) introduced the modal heterogeneityand proposed a multiview calibration network toresolve inherent differences in modalities. Despitepromising results, these networks relied on fixedframeworks with static mechanisms to capture af-fective cues, limiting their capacity to dynamicallyhandle diverse multimodal sentiment posts. In con-trast, we aim to develop a dynamic neural network(Han et al., 2023; Li et al., 2024) to process com-plex image-text pairs adaptively, thereby enhancingthe performance of sentiment classification.",
  "Dynamic Neural Networks": "Unlike common static neural networks, the infer-ence process of dynamic neural networks adjustsdynamically based on different samples (Qu et al.,2021). Early works on dynamic networks focusedon updating model parameters dynamically (Perezet al., 2018; Veit and Belongie, 2018). Subsequentresearch aimed to design dynamic models that en-able automatic tuning of network depth or width(Liu et al., 2017). Dynamic neural networks havealso shown excellent performance in recent mul-timodal tasks (Han et al., 2023; Zhu et al., 2023;Qu et al., 2021). Qu et al. (2021) first appliedrouting mechanisms to the domain of image-textretrieval, Zhou et al. (2021) introduced TRAR, aTransformer-based model to dynamically sched-ule global and local dependencies for VQA. How-ever, TRAR only performed routing on unimodaldata. Tian et al. (2023) proposed DynRT-Net, adynamic routing converter network for multimodalsarcasm detection, which activated different mod-ules through hierarchical collaboration. The workonly achieved local dynamics by modifying sec-tional frames within the Transformer. To the best of our knowledge, the application of dynamic mecha-nisms in multimodal sentiment detection has neverbeen explored. Unlike previous related works, wedesign six interaction units without relying on anyexisting fixed frameworks to model various interac-tion scenarios and employ dynamic routing mecha-nisms to explore novel interaction patterns, achiev-ing truly global dynamics.",
  "Given the input x = (xt, xv), where xt and xv": "denote the text and image. In this work, xt ={si}Pi=1, P is the length of the text, we use thepre-trained BERT model to generate the final wordembedding eti RD, i refers to the i-th word. Dis the hidden dimension. The local text featuredenotes as et RPD. For each image xv, we firstdivide each image into K patches and then use thepre-trained ViT model to generate the final regionembedding evj RD, j refers to the j-th region.The local image feature denotes as ev RKD.We also use [CLS] token representation to get theglobal feature et and ev for text and image.",
  "Dual-Branch Dynamic SentimentInteraction Module": "To capture complex and diverse sentiment informa-tion in multimodal posts, we design six indepen-dent units to realize inter- and intra-modal senti-ment information interaction. These units incorpo-rate existing interaction patterns and can exploremore unexcavated ones based on routing strategies,endowing our model excellent ability to understandemotions and reason sentiment. Formally, the sixunits can be summarized as follows:",
  "S(n)m = {H(n)m (X(n)m ), m = 1 or 2H(n)m (X(n)m , Y ), m = 3, 4, 5, 6(1)": "where S(n)m RMD denotes the output of them-th unit in the n-th layer. H(n)mrepresents theinteraction function of the m-th unit in the n-thlayer. X(n)m RMD is the local input feature ofthe m-th unit in the n-th layer, Y RND denotesthe local input feature from other modality of them-th unit in the n-th layer.In this work, we implement two single symmetri-cal interactive branches. Specifically, in text-image(T2V) branch, we set X = et (M=P) and Y = ev",
  "(M=K) and Y = et (N=P)": "In this section, we take the T2V branch as anexample to detail these six independent units.Simplified Sentiment-Semantic RectifyingUnit. For a simple image or a short sentence, hu-man can judge its sentiment polarity at a glance,and complex interactions are unnecessary. There-fore, we design a rectifiable unit to simplify theoriginal sentiment information. It can be formu-lated as: H(n)1(et) = ReLU(et).Unimodal Sentiment-Semantic ReasoningUnit. There may exist sentiment and semanticsimilarities between the local fragments (differentwords or visual regions), so we design a USSR unitto capture these semantic dependencies. Specifi-cally, we employ a multi-head self-attention mecha-nism to capture intra-modal fine-grained sentimen-tal associations in different subspaces, as follows:",
  "eti = MLP(eti vi + vi ) + eti(8)": "where denotes element-wise multiplication.Combining the above steps, our CLSSM unit canbe summarized as: H(n)3(eti, evj) = [ et1; ; etP ].Cross-modalGlobalSentiment-SemanticAligning Unit. Compared with the local fragmentsinformation, the global holistic information can re-flect the overall sentiment of one text-image pairon a broader level, so we design a CGSSA unitto integrate the global sentiment information ofdifferent modalities to learn valuable cross-modalcoarse-grained representations. Specifically, we in-troduce a special gated fusion mechanism to adap-tively combine the global text representation et andvisual representation ev, which is formulated as:",
  "z =exp(W1(W2 et))": "exp(W1(W2 et)) + exp(W1(W2 ev))(10)where W1 and W2 Rmd are parameter matrices, denotes the Tanh function.After the above processes, our CGSSA unit canbe profiled as: H(n)4(et, ev) = e.Global-Local Sentiment-Semantic FilteringUnit. For complex text-image pairs, relying only on cross-modal global or local information is stillinsufficient to classify sentiment. Therefore, wedesign a GLSSF unit to capture both the cross-modal fine-grained and coarse-grained emotionalcues simultaneously, and compensate for affectivedifferences. In addition, we notice that indiscrim-inately aggregate all possible local comparisonsand global comparisons may cause less-meaningfulcomparisons (such as \"a\" and \"the\" correlation com-parisons), which hinder the models capacity todistinguish sentiment polarity. Therefore, we delib-erately develop a strategy in GLSSF unit to effec-tively suppress invalid comparisons with low affec-tive contributions. Specifically, we first computecross-modal global and local sentimental similarityvector (Diao et al., 2021) as follows:",
  "FuNuFu(15)": "Combining the above processes, our GLSSF unitcan be represented as: H(n)5(et, ev, et, ev) = f.Multi-View Sentiment-Semantic Sensing Unit.Both unimodal and cross-mdoal information arebeneficial for the fianl sentiment classification.Therefore, we design a MVSSS unit that learns uni-modal context-rich cross-modal sentiment featuresfrom two different views, aiming to facilitate inter-active reasoning between unimodal and multimodalsentiment information. Specifically, consideringthe possible common features between unimodaland cross-modal information, we first project wv",
  "Ce = Tanh(Weet + be)(17)": "where Cw and Ce denote the converted multi-viewcross-modal and unimodal sentiment features insame space.Next, we modify the gating mechanism to fil-ter possible sentimental differences noise to inte-grate the common features of unimodal and cross-modal information. And then we learn the uni-modal context-rich cross-modal sentiment features.Specifically, we align Cw based on Ce, and set Cwas Qm = WQCw and the Ce as Ku = WKCe,where WQ and WK are trainable parameters. Thus,the Vm = softmax(QmKTu ), where Vm is queryattended mask. Thereafter, unimodal context-richcross-modal sentiment feature Cwe can be formu-lates as:Cwe = Cw + VmCe(18) After the above processes, our MVSSS unit canbe summarized as: Hn6 (et, wv) = Cwe.Soft Router.To fully utilize the inimitablestrengths of six units, we set up the layers in par-allel and connect them between adjacent layers ina dense manner. This dense connectivity ensuresa multiple and flexible routing space where manyunexcavated interaction patterns can be explored.After constructing the routing space, the routing",
  "r=1h(n)m,r)]}(20)": "where (n)m RC denotes the path probabilityvector of all units in n-th layer, h(n)m,r is the r-th rowvector of H(n)m .Thereafter, the routing process is finished, wecan obtain the final refined feature matrix H16 = H(L)16 through Equation (20) from the last layerL. Then, we take average-pooling operation foraggregating the six units output embeddings H16to obtain the final aggregated single-branch featurerepresentation h16.",
  "Sentiment-Aware Path-Adaptive FusionModule": "Block Fusion. We implement the units and routingprocess on text and image-modality respectively,and obtain two branches of aggregated feature rep-resentation, namely, hT2V16 and hV 2T16 . Then, weadopt a block fusion strategy to fuse hT2V16 andhV 2T16 and use the fusion feature to make the finalprediction. Inspired by Ben-Younes et al. (2019),we project hT2V16 and hV 2T16 into a new feature spacethrough the association tensor T, as follows:",
  "LCE = (ylog(y) + (1 y)log(1 y))(23)": "where y denotes the ground-truth label.Path Regularization. In fact, the sentiment in-formation and semantics of multimodal posts arekey factors affecting the interaction patterns. Sam-ples with similar sentiment polarity should learnsimilar routing paths, while samples with differentsentiment polarity should learn discrepant routingpaths as much as possible. We hope that the rout-ing path distribution can be consistent with thesentiment-semantic distribution. Therefore, we in-troduce the path regularization term to measuretheir correlations among samples.Particularly,we take average-pooling on et RPD to getthe sentiment-semantic representation et RD and compute the sentiment-semantic similarity asSt = et (et). Thereafter, we connect the out-put values of all routers to get the path vectort RC2(L1)+C and compute the path similarityas Sp = t (t).To achieve sentiment-path consistency, we de-velop a path regularization loss function Lpr to cal-culate the distribution gap between the sentiment-semantic representation St and the path vector Sp,which is formulated as:",
  "Experiments results": "We evaluate the effectiveness of our proposedframework by comparing it with the baseline mod-els as shown in and derive the followingobservations. 1). It is evident that both the textand image play crucial roles in sentiment detec-tion. Therefore, it is imperative to fully excavatethe affective cues from different modalities, whichvalidate our tuition of designing two single sym-metric branches of the two-channel interaction. Inaddition, the multi-modal models consistently out-perform the unimodal models on performance be-cause of fusing more sentiment information. 2).Our D2R achieves considerable improvement onAcc and F1 compared with the other strong base-line models on the three datasets, which suggeststhat dynamic routing network have advantages overregular static networks. 3). At last, we find thatD2R achieves better results on HFM dataset com-pared to the MVSA datasets. The reason may bethat for classification tasks with fewer label cate-gories, the interaction patterns contained in the sixunits capture more accurate sentiment information.",
  "Ablation Study": "To further investigate the effectiveness of each com-ponent in D2R, we conduct a series of ablationstudies: 1) w/o 1: we remove the SSSR unit; 2)w/o 2: we remove the USSR unit; 3) w/o 3: weremove the CLSSM unit; 4) w/o 4: we remove theCGSSA unit; 5) w/o 5: we remove the GLSSF unit;6) w/o 6: we remove the MVSSS unit; 7) w/o BF:we remove the block fusion module; 8) w/o PR:we remove the path regularization term. shows the results of ablation study. Itis evident that the performance after removingany of the components is worse than the originalD2R, which demonstrates the effectiveness of eachcomponent. Specifically, for MVSA-Single andMVSA-Multiple datasets, w/o 5 degrades dramat-ically, it drops absolutely 0.0600 and 0.0507 onACC, 0.0477 and 0.0424 on F1, respectively. Thisdemonstrates that GLSSF unit can compensate forsentimental differences by capturing cross-modalglobal and local affective cues. At the same time,it verifies the rationality of calculating the senti-ment similarity vector of cross-modal global andlocal information, suppressing irrelevant ones. ForHFM dataset, w/o 1 has the most significant de-cline which indicates that the simplest SSSR unitplays an important role. We speculate that the rea-son may be that the HFM dataset has more simpletext-image pairs. Moreover, w/o 6 achieve betterresults than others, only decrease 0.0042 on ACCand 0.0043 on F1 as HFM is a simple binary classi-fication task dataset, the unit that capture sentimentinformation from complex multiple perspectivesmay play less important role. The performance ofw/o BF declines distinctly. This suggests that blockfusion benefits our dual-branch model by obtainingbetter sentiment representation. Particularly, forw/o PR, the ACC and F1 of the three datasets alsoshow varying degrees of performance degradation.It proves the effectiveness of our proposed pathregularization which consider the the consistency",
  "MVSA-SingleMVSA-MultipleHFMModelACCF1ACCF1ACCF1": "D2R0.76670.75590.71590.70850.86720.8625Manhattan distance (L1)0.72440.72320.68880.67780.84300.8395Euclidean distance (L2)0.75330.74410.70760.69850.85420.8492Mean Squared Displacement (MSD)0.73330.72920.70760.69220.85590.8510 of routing path and sentiment semantics.We also execute three additional ablation studiesto verify the rationality of our proposed soft router,including: 1) w/o Soft Router: we remove the softrouter instead of selecting routing paths, 2) Ran-dom Router: we replace the soft router with therandom router, deriving the path probability of eachunit from a uniform distribution, 3) Hard Router:we replace the soft router with the hard router, intro-ducing the gumbel-softmax trick to discretize pathvalues. We report the experimental results in and have the following observations: the met-rics of D2R significantly outperform other threemethods on all datasets, demonstrating the effec-tiveness of our proposed soft router equivalent toother methods. Moreover, hard router got the light-est drop in performance compared with randomrouter and w/o soft router, we hypothesize that thereason may be that random router may introduceextra path noise into the model, while not usingrouter deprives the models ability to dynamicallyadapt to different inputs.To validate the reliability of the cosine similar-ity calculations using embeddings obtained fromdifferent pre-trained models in the cross-modal lo-cal sentiment-semantic matching unit, we designmore three ablation studies to compared the effectof the similarity distance calculation formulas weused and others on the final models performance.The experimental results are shown in . Itis clear that using cosine values to compute thesimilarity of embeddings from different pre-trainedmodels is more efficient than other methods andhas the most significant improvement in modelsperformance. At the same time, some previousworks have verified the advantages of using co-sine similarity calculations (Diao et al., 2021; Chenet al., 2022; Zhang et al., 2022).",
  "Hyperparameter Analysis": "To analyze the impact of the number of dynamicrouting layers L in our model, we conduct exper-iments on varying the layer of dynamic routingfrom 1 to 6. The results are shown in . ForMVSA-Single and MVSA-Multiple datasets, wecan see that the performance metric F1 improveswith the increase of dynamic routing layers in therange 1 to 4, and then drops slightly while the rout-ing layers exceed 4. For HFM dataset, F1 improveswith the increase of dynamic routing layers in thefirst 3 layers, and then decreases in the layers 4to 6. The results show that increasing the numberof routing layers in an appropriate range can im-prove the performance as more layers offer broaderpath space, thus increasing the ability of explor-ing more superior interaction patterns. However,when layers exceed 3 or 4, overfitting limits modeloptimization and hinders the path learning ability.In addition, we also carry out several experi-ments on 1 and 2 to research the influence ofthe path regularization parameters LT2Vprand LV 2Tpron the final prediction. The results are shown in. For MVSA datasets, the performancefirst comes and goes before the saturation points(1 = 0.9, 2 = 0.3), and then begins to declinewhen 1 exceed 0.9 and 2 exceed 0.3. We can in-fer that the saturation points (1 = 0.9, 2 = 0.3)can maximize the similarity of dynamic paths forexamples with the same sentiment polarity. ForHFM, the best 1 is 0.6 and 2 is 1.0. Then, aslight drop in performance occurs on other values.Apparently, excessively large 1 and 2 affect theperformance on three datasets, the reason may bethat overly exploring the path diversity leads to aterrible over-fitting phenomenon.",
  ": The influence of hyper-parameters": "reserved and max memory allocated for the modelon three different datasets in . FLOPs referto the floating point operations, which are used toevaluate the models computational complexity; in-ference time reflects the models inference latency;model parameters reflect the models total parame-ter count. On both MVSA datasets, the number ofdynamic routing layers is set to 4, on HFM dataset,it is set to 3. Thus our model has more inferencepaths and parameter count on MVSA datasets thanHFM dataset.",
  "Visualization": "To demonstrate the vital advantage of our dynamicreasoning sentiment methods. We thus show someimages and visualize the path vectors learned inSAPAF module, which indicate that D2R can adap-tively choose the best paths for different examples.Specifically, we use the t-SNE (Van der Maatenand Hinton, 2008) algorithm to map the concatena-tion path vector into a 2-dimensional Euclid space.Afterwards, we clustered these 2-dimensional vec-tors into 6 groups in different 6 colors. As shownin , we could observe that the imagesrelated to obvious positive emotions (the pointsmarked in brown and yellow) and the ones relatedto obvious negative emotions (the points marked inblue and green) can be well distinguished. For in-stance, there exists a large margin between brownpoints (associated with happy crowds) and bluepoints (associated with bad weather), because notonly is there a semantic gap between crowdsand weather, but there is also a sentimental con-flict between happy and bad. Although bothbrown and green points are related to people, thesentiment differences between them are still sig-nificant, as a result, they are still wide apart in a2-dimensional space. Besides, neutral examplesserve as a demarcation between positive and neg-ative examples and are located between the two.Pictures with no emotional inclination (the pointsmarked in red and purple) are also can be welldistinguished. Because the advanced fine-grainedsemantics is much different between red points (re-",
  ": Visualization of the learned path vectors": "lated to logos) and purple points (related to foods).Our proposed soft router can make path choicesbased on these fine-grained semantic and sentimentinformation to the path selection. These resultsreveal that our D2R is able to adaptively learn spe-cific semantic-related and sentiment-aware pathsfor diverse inputs, thus the distribution of learningpaths is to a certain extent consistent with that ofsentiment-semantic.",
  "Case study": "To further verify the adaptability of D2R, we qual-itatively visualize the routing process for severaltypical examples. As shown in , we havethe following observations: 1) Simpler text-imagepairs tend to activate less paths as their sentimentpolarity is obvious. For example, the sentence in (a) conveys a distinctly positive emotion(love), which may not require much image infor-mation; 2) Cross-modal global content analysis canactivate more paths for some examples to obtainaccurate affective cues. In (b), We canttell the sentiment polarity of the post from onlytext-modality (image-modality), but when we fo-cus on the entire text-image pairs, we can see thatits a negative example (a busted pen soiled handslike doing actual manual labor and working on thecar). 3) Additional attention to fine-grained cross-modal local information can activate more pathsto capture nuanced sentiment information. (c) shows two intact strawberries and one damagedstrawberry. We first understand the meaning ofthe entire text, and then paying extra attention tothe comparison between \"destroyed\" in text and\"damaged strawberry\" in image to accurately clas-sify it as a negative example. 4) The elements inthe first three examples are relatively single (book,hand or food), their interaction patterns and rout-ing paths are less complex than the fourth, becausethere exist many elements in (d). (\"bule sky\",",
  "Conclusion": "This paper presents a novel dynamic neural net-work model for multimodal sentiment detectioncalled D2R, which is the first work on exploringdiverse interaction patterns using dynamic routingmechanisms. Specifically, we apply six units tosimulate various levels of inter- and intra-modalinteraction patterns. A soft router is integrated toadapt flexibly to diverse image-text pairs throughrouting path learning. Additionally, we introduce apath regularization term to measure sentiment-pathsimilarity between samples and optimize the in-ference path. Comprehensive experiments demon-strate that our model achieves state-of-the-art per-formance on three benchmark datasets.",
  "Acknowledgement": "This work was supported in part by Guang-dong Basic and Applied Basic Research Founda-tion(2023A1515011370), National Natural ScienceFoundation of China (32371114), CharacteristicInnovation Projects of Guangdong Colleges andUniversities (2018KTSCX049), International Sci-entific and Technological Cooperation Project ofHuangpu and Development Districts in Guangzhou(2022GH01, 2023GH05). Hedi Ben-Younes, Remi Cadene, Nicolas Thome, andMatthieu Cord. 2019. Block: Bilinear superdiagonalfusion for visual question answering and visual rela-tionship detection. In Proceedings of the AAAI con-ference on artificial intelligence, volume 33, pages81028109. Yitao Cai, Huiyu Cai, and Xiaojun Wan. 2019. Multi-modal sarcasm detection in twitter with hierarchicalfusion model. In Proceedings of the 57th annualmeeting of the association for computational linguis-tics, pages 25062515. Jianan Chen, Lu Zhang, Qiong Wang, Cong Bai, andKidiyo Kpalma. 2022. Intra-modal constraint lossfor image-text retrieval. In 2022 IEEE InternationalConference on Image Processing (ICIP), pages 40234027. IEEE. Yifan Chen, Haoliang Xiong, Kuntao Li, Weixing Mai,Yun Xue, Qianhua Cai, and Fenghuan Li. 2024.Relevance-aware visual entity filter network for mul-timodal aspect-based sentiment analysis. Interna-tional Journal of Machine Learning and Cybernetics,pages 114. Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu.2021. Similarity reasoning and filtration for image-text matching.In Proceedings of the AAAI con-ference on artificial intelligence, volume 35, pages12181226. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.In Proceedingsof the International Conference on LearningRepresentations.",
  "Yudong Han, Jianhua Yin, Jianlong Wu, Yinwei Wei,and Liqiang Nie. 2023. Semantic-aware modularcapsule routing for visual question answering. IEEETransactions on Image Processing": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 770778. Lianzhe Huang, Dehong Ma, Sujian Li, XiaodongZhang, and Houfeng Wang. 2019. Text level graphneural network for text classification. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 34443450, Hong Kong,China. Association for Computational Linguistics. Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. 2019. Bert: Pre-training of deep bidirec-tional transformers for language understanding. InProceedings of the naacL-HLT, volume 1, page 2. Yoon Kim. 2014.Convolutional neural networksfor sentence classification. In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 17461751,Doha, Qatar. Association for Computational Linguis-tics. Xingye Li, Jin Liu, Yurong Xie, Peizhu Gong, XiliangZhang, and Huihua He. 2024.Magdra: a multi-modal attention graph network with dynamic routing-by-agreement for multi-label emotion recognition.Knowledge-Based Systems, 283:111126.",
  "Zhen Li, Bing Xu, Conghui Zhu, and Tiejun Zhao. 2022": "CLMLF:a contrastive learning and multi-layer fusionmethod for multimodal sentiment detection. In Find-ings of the Association for Computational Linguis-tics: NAACL 2022, pages 22822294, Seattle, UnitedStates. Association for Computational Linguistics. Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang,and Ruifeng Xu. 2021. Multi-modal sarcasm de-tection with interactive in-modal and cross-modalgraphs. In Proceedings of the 29th ACM interna-tional conference on multimedia, pages 47074715. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,Shoumeng Yan, and Changshui Zhang. 2017. Learn-ing efficient convolutional networks through networkslimming. In Proceedings of the IEEE internationalconference on computer vision, pages 27362744. Weixing Mai, Zhengxuan Zhang, Yifan Chen, KuntaoLi, and Yun Xue. 2024. Geda: Improving trainingdata with large language models for aspect senti-ment triplet extraction. Knowledge-Based Systems,301:112289.",
  "social data. In MultiMedia Modeling: 22nd Inter-national Conference, MMM 2016, Miami, FL, USA,January 4-6, 2016, Proceedings, Part II 22, pages1527. Springer": "Ethan Perez, Florian Strub, Harm De Vries, VincentDumoulin, and Aaron Courville. 2018. Film: Vi-sual reasoning with a general conditioning layer. InProceedings of the AAAI conference on artificial in-telligence, volume 32. Leigang Qu, Meng Liu, Jianlong Wu, Zan Gao, andLiqiang Nie. 2021. Dynamic modality interactionmodeling for image-text retrieval. In Proceedingsof the 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 11041113. Rossano Schifanella, Paloma De Juan, Joel Tetreault,and Liangliang Cao. 2016. Detecting sarcasm inmultimodal social platforms. In Proceedings of the24th ACM international conference on Multimedia,pages 11361145.",
  "Andreas Veit and Serge Belongie. 2018. Convolutionalnetworks with adaptive inference graphs. In Proceed-ings of the European conference on computer vision(ECCV), pages 318": "Yiwei Wei, Shaozu Yuan, Ruosong Yang, Lei Shen,Zhangmeizhi Li, Longbiao Wang, and Meng Chen.2023. Tackling modality heterogeneity with multi-view calibration network for multimodal sentimentdetection. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 52405252. Nan Xu. 2017. Analyzing multimodal public sentimentbased on hierarchical semantic attentional network.In 2017 IEEE international conference on intelli-gence and security informatics (ISI), pages 152154.IEEE. Nan Xu and Wenji Mao. 2017. Multisentinet: A deepsemantic network for multimodal sentiment analy-sis. In Proceedings of the 2017 ACM on Conferenceon Information and Knowledge Management, pages23992402. Nan Xu, Wenji Mao, and Guandan Chen. 2018. A co-memory network for multimodal sentiment analysis.In The 41st international ACM SIGIR conferenceon research & development in information retrieval,pages 929932. Nan Xu, Zhixiong Zeng, and Wenji Mao. 2020. Reason-ing with multimodal sarcastic tweets via modelingcross-modality contrast and semantic association. InProceedings of the 58th annual meeting of the as-sociation for computational linguistics, pages 37773786.",
  "A.1Dataset": "We assess our model by conducting experimentson three publicly available benchmark datasetswhich are MVSA-Single, MVSA-Multiple (Niuet al., 2016), and HFM (Cai et al., 2019). MVSA-Single and MVSA-Multiple datasets collect datafrom Twitter, each text-image pair is labeled bya single sentiment. Both of them have three cate-gories: positive, neutral, and negative. For a faircomparison, we process the original two MVSAdatasets in the same way as Xu and Mao (2017).HFM dataset also collect data from Twitter, whichhas two sentimental categories: positive and neg-ative. Following Cai et al. (2019), we adopt thesame data preprocessing method for experiments.The statistics of these datasets are shown in .",
  "A.2Implementation Details": "For a fair comparison, following the processing inWei et al. (2023), we adopt the pre-trained BERT-base-uncased model (Kenton and Toutanova, 2019)as the text-encoder to embed each word of the text,and utilize the pre-trained ViT model (Dosovitskiyet al., 2020) as the image-encoder to embed eachregion of the image. The learning rate is 1e 5for MVSA-Single and MVSA-Multiple datasets,2e 5 for HFM dataset. We train the model for20 epochs with mini-batch size 64. For MVSA-Single and MVSA-Multiple datasets, we establishthe number of dynamic routing layers L as 4, theJS loss weight 1 as 0.9 and 2 as 0.3. For HFMdataset, we set the number of dynamic routing layerto 3, the JS loss weight 1 to 0.6 and 2 to 1.0.Adam optimizer is also utilized to train the model.Dropout and early stop are used to avoid overfitting.Based on prior configurations, we utilize ACC andWeighted F1 as evaluation metrics for the MVSAdatasets and ACC and Macro-F1 for the HFM toassess the models performance.",
  "A.3Baseline Models": "We compare our model with unimodal baselinemodels and multimodal baseline models.Unimodal Baselines. For text modality, wechoose CNN (Kim, 2014), BiLSTM (Zhou et al.,2016), BERT (Kenton and Toutanova, 2019) andTGNN (Huang et al., 2019) as baselines since theyare well-known models for text classification. Forimage modality, ResNet (He et al., 2016) and ViT(Dosovitskiy et al., 2020) are two popular modelsfor image classification task, ODSA (Yang et al.,2020) is an image sentiment analysis model.Multimodal Baselines.For MVSA-Singleand MVSA-Multiple datasets, the baselines in-clude: MultiSentiNet (Xu and Mao, 2017), a deepattention-based semantic network for multimodalsentiment analysis; HSAN (Xu, 2017), a hierar-chical semantic attentional network based on im-age captions for multimodal sentiment analysis;Co-MN-Hop6 (Xu et al., 2018) utilize co-memorynetwork to iteratively model the interactions be-tween multiple modalities; MGNNS (Yang et al.,2021) adopt multi-channel graph neural networkswith sentiment-awareness for image-text sentimentdetection; CLMLF (Li et al., 2022) propose a con-trastive learning and multi-layer fusion method formultimodal sentiment detection; MVCN (Wei et al.,2023) is the previous SOTA model that design amulti-view calibration network to solve the modal-ity heterogeneity for multimodal sentiment detec-tion. For HFM dataset, we compare two variants ofConcat (Schifanella et al., 2016): Concat(2) meansconcatenating text and image, while Concat(3) in-troduces one more image attribute features; MMSD(Cai et al., 2019) is a hierarchical multimodal fea-tures model for fusing text, image, and image at-tributes; D&R Net (Xu et al., 2020) propose a de-composition and relation network to fuse the text,image, and visual attributes features."
}