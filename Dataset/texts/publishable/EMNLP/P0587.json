{
  "Abstract": "Visual document understanding (VDU) is achallenging task that involves understandingdocuments across various modalities (text andimage) and layouts (forms, tables, etc.). Thisstudy aims to enhance generalizability of smallVDU models by distilling knowledge fromLLMs. We identify that directly promptingLLMs often fails to generate informative anduseful data. In response, we present a newframework (called DocKD) that enriches thedata generation process by integrating exter-nal document knowledge.Specifically, weprovide an LLM with various document ele-ments like key-value pairs, layouts, and descrip-tions, to elicit open-ended answers. Our experi-ments show that DocKD produces high-qualitydocument annotations and surpasses the directknowledge distillation approach that does notleverage external document knowledge. More-over, student VDU models trained with solelyDocKD-generated data is not only compara-ble to those trained with human-annotated dataon in-domain tasks but also significantly excelthem on out-of-domain tasks.",
  "Introduction": "Visual document understanding (VDU) requires ex-tracting and analyzing both textual and non-textualinformation from a document. The textual informa-tion is usually obtained via optical character recog-nition (OCR), which only provides unstructured ornavely ordered text. The non-textual informationis visually-rich, demanding a solution to directlyprocess the document image. Earlier studies ofVDU (Liu et al., 2007; Hao et al., 2016; Soto andYoo, 2019) primarily focused on identifying cer-tain parts of a document using heuristics or simplenetworks. Recent approaches (Huang et al., 2022;Tang et al., 2023) have shifted towards pretraining",
  ": We leverage LLM to generate document anno-tations given the text extracted from a document image": "multi-modal document understanding models to ad-dress the models comprehension of textual, visual,and layout features. However, the existing VDUmethods are limited by training on a small-scale,curated document dataset, compromising the gen-eralizability of VDU models to diverse documents.Thus, their performance heavily relies on the anno-tated training document set for downstream tasks.In this study, we aim to improve the general-izability of VDU models by distilling knowledgefrom large language models (LLMs). In particular,we introduce an open-world document understand-ing problem, where the model needs to addressthe downstream task with a broader scope of doc-uments than covered by the available annotations.LLMs, given instructions to elicit open-ended an-swers, can create rich and diverse annotations, asillustrated in . For instance, we might in-struct the LLM to generate question-answerpairs from this document, along with docu-ment text extracted from OCR. However, this ap-proach entails a critical challenge, since LLMs of-ten struggle to comprehend unstructured OCR text(Wang et al., 2023b), leading to its generation oflow-quality annotations. Moreover, there is a vari-ety of non-textual information within documentswhich is not included in the LLM prompt.To overcome these challenges, we presentDocKD, a document knowledge distillation frame- work that leverages external document informationto enhance LLM data generation. In this frame-work, we extract various document elements (e.g.,key-value pairs, layout, and descriptions) alongwith text and formulate a generation prompt forLLMs with this visual information. The LLM out-puts then serve as annotations to train a small-scaleVDU model. While large multimodal models likeGPT-4V (OpenAI, 2023) are also recognized fortheir visual-language capabilities, they still lag be-hind state-of-the-art OCR systems (Fujitake, 2024),but LLMs that utilize well-structured OCR textexcel in document processing and understanding.Thus, we employ LLMs aided with visual tools fordata generation.We demonstrate the efficacy of DocKD on threedocument understanding tasks: visual question an-swering, entity extraction, and classification. Ineach task, we introduce new tools for incorporatingexternal document knowledge. Our experimentsreveal that DocKD allows student models to attainopen document understanding abilities, generaliz-ing to unseen documents, questions, entities, orcategories. Our contributions are as follows: We introduce DocKD, a framework designed tofacilitate VDU models for open-world documentunderstanding. It boosts the generalizability ofVDU models by leveraging LLMs and externaldocument knowledge to generate training data.",
  "Related Work": "Document understanding models.Research indocument intelligence (Liu et al., 2007; Hao et al.,2016; Subramani et al., 2020; Wang et al., 2022b)has gained significant interest, developing ma-chines to understand document contents and ad-dress associated tasks. Previous studies (Honget al., 2020; Wang et al., 2022a) have proposeddocument understanding models to improve thecomprehension of multi-modality by integrating textual and layout information. These models laterhave evolved to incorporate visual information aswell (Appalaraju et al., 2021; Gu et al., 2021; Penget al., 2022). These models are typically pretrainedthrough self-supervised learning methods, such asword/line alignment (Appalaraju et al., 2023; Tanget al., 2023) or masked text/image modeling (Liet al., 2021; Huang et al., 2022). Subsequently,they undergo a fine-tuning phase for specific down-stream tasks, which entails the manual annotationof documents. To facilitate the training of VDUmodels without the need for human labels, we pro-pose knowledge distillation (Hinton et al., 2015;Gou et al., 2021) approach from LLMs. Leveraging LLMs for data generation.Knowl-edge distillation (KD) from LLMs has been ex-plored across various natural language processingtasks (Gu et al., 2023). LLMs like GPT-3 (Brownet al., 2020) are utilized for guided annotation of un-labeled data (Wang et al., 2021; Ding et al., 2022;Touvron et al., 2023; Chiang et al., 2023) or fordistilling reasoning capabilities (Magister et al.,2022; Hsieh et al., 2023; Zhu et al., 2023) whichis then used to fine-tune smaller language mod-els. Among these, targeted distillation (Jung et al.,2023; Zhou et al., 2023) has demonstrated that iden-tifying and amplifying the LLMs knowledge to ahigh-quality dataset enables student models to at-tain task-specific knowledge. It has the potentialto make specialized language models that outper-form in specific tasks, at the expense of genericperformances (Fu et al., 2023).In visual instruction tuning research (Li et al., 2023a,b,c; Liu et al., 2023b,a), LLMs are employedto generate visual-language instruction-followingdata. For instance, LLaVA (Liu et al., 2023b) istrained on the instruction-following dataset for con-versation, description, and complex reasoning, cre-ated by prompting the LLM with bounding boxcoordinates of objects along with image captions.InstructBLIP (Dai et al., 2023) incorporates diversetasks, such as image question generation and videoquestion answering. Closest to our work is theextension of visual instruction tuning to the do-main of VDU, generating data with document-specific knowledge to fine-tune downstream mod-els. Wang et al. (2023c) use layout-aware doc-uments to answer given questions and fine-tuneLLMs, and Aubakirova et al. (2023) generate cap-tions for patent figures to fine-tune VLMs. Thecommunity has recently focused on directly im-",
  "(3.2)(3.3)": ": Overview of DocKD. (a) To prepare training data, we provide an LLM teacher with a generationprompt pgen given the document text. LLM generates answers agen which are then converted into (ptask, atask). Weexplore methods to inject external document knowledge () into the document text or pgen to obtain high-qualityannotations. (b) We train a student VDU model using the generated task prompt and answer pairs (ptask, atask). proving the VDU performance of LLMs or LMMsby introducing new designs of encoding documentimages (Li et al., 2024; Luo et al., 2024; Tanakaet al., 2024; Liu et al., 2024), which are closelyrelated and complementary to our work that fo-cuses on distilling knowledge from strong LLMsfor VDU. Our work is the first to extract knowl-edge from LLMs for open document understandingtasks, exploring methods to inject visual document-specific knowledge into LLM and produce high-quality data for training VDU models.",
  "Document Knowledge Distillation": "Problem formulation.Similar to prior work(Kim et al., 2022; Appalaraju et al., 2023; Tanget al., 2023), we formulate document understandingproblem under a sequence-to-sequence (seq2seq)generation framework. That is, we design a task-specific prompt ptask which asks a VDU model tosolve the task and output an answer atask. DocKDinvolves an LLM teacher fT to generate theseprompt and answer pairs. Given an image of adocument page, we apply a pre-built OCR engineto extract its words and word bounding boxes. Forsimplicity, we represent a document input as d.The overall pipeline of the DocKD approach isdescribed in . In (a), we first constructa generation prompt pgen for the task. Then, givenpgen and document text dtext as inputs, the LLMgenerates agen, i.e., fT(dtext, pgen) agen. Thiscan be readily parsed into (ptask, atask) by post-processing. Here, we can inject document-specificknowledge into the LLM inputs, so that it can bet-ter understand the document content and generatemore accurate (ptask, atask) pairs. In (b), wetrain a student model fS to output an answer ataskgiven d and ptask, i.e., fS(d, ptask) atask.We exemplify the application of our trainingpipeline on three document understanding tasks: visual question answering (VQA), entity extrac-tion, and document classification. To summarizeeach section, we leverage document knowledge byusing the OCR linearization model to improve dtext(Sec. 3.1), using the key-value detection model toguide pgen (Sec. 3.2), and introducing the docu-ment description into pgen for better class candi-dates (Sec. 3.3). Refer to Appx. B for the full tem-plates of pgen in each task.",
  "Document VQA": "Document VQA (Borchmann et al., 2021; Mathewet al., 2021, 2022; Van Landeghem et al., 2023) isthe task of answering questions about documents.Given a document d and a corresponding question-answer (QA) pair (q, a), we design the task promptas ptask = Document: dtext. Question: q, andatask = Answer: a. To distill knowledge for aVDU model, we investigate a way to prompt LLMsto generate QA pairs from documents.",
  "fT(dtext, pgen) agen = {(q1, a1), (q2, a2), . . . }": "We randomly select one question and its corre-sponding answer from agen and create (ptask, atask)for training the student model. We find that in-cluding an instruction into pgen helps the teacheravoid creating low-quality QAs (e.g., duplicatedquestions or answers inconsistent with context) andenables us to control the generation output so thatit can be easily parsed into (ptask, atask).We also note that pgen instructs the LLM to out-put questions and answers together, which we findfacilitates the generation of accurate QA pairs. Al-ternatively, we may ask the LLM to generate ques-tions first and then answer them, which we observe",
  "that the generated questions are often difficult toanswer, or the answers do not match the questions": "Introducing layout knowledge to OCR text.One limitation of the LLMs QA generation lies onits text-to-text framework, where it requires the textto be organized in a semantically meaningful order.However, OCR text is a simple sequence of wordstypically ordered by raster scanning, which ignoresthe important layout and structural information ofdocument pages. Therefore, QAs generated fromsuch text are usually less challenging and do notcover the spatial relationship between entities.To ensure the LLMs awareness on the text lay-out, we replace the raw OCR text with spatiallylinearized OCR text, where we organize documenttext into a markdown style as displayed in (b).We use the linearization model inspired by (Penget al., 2022), also extracting tables, key-value pairs,and layout information using Textract API1 whichassists the conversion to markdown. Interestingly,an LLM understands this markdown style; thus, thelinearization model supplements document layoutknowledge that is missing and helps the LLM to generate more diverse and higher-quality QAs. Thestudent model trained with these QA pairs achievesnotable VQA performances (). Refer toAppx. C.1 for the examples of generated QAs withraw or linearized OCR text.",
  "Entity Extraction": "Entity extraction aims to identify entities in thedocument that matches a given field name. Sim-ilar to the VQA task, we convert this task intoa seq2seq form. For each field name f and thecorresponding entity e, ptask = Document: dtext.Question:what are entities of < f >?and atask = Answer: e.The challenge of this task lies in that we do notknow which field will be queried for a new docu-ment. Thus, we should generate as many diversefields as possible for different kinds of entities, andtrain the entity extraction model to link those fieldsto the entities. Indeed, LLMs are known to beproficient at the entity recognition task (Li et al.,2019; Wang et al., 2023a) and can even identifytheir names (Zhou et al., 2023). Designing entity generation task.To generatedata for entity extraction, we prompt LLMs to ex-haustively extract any entities present in a doc-ument.We design an entity extraction promptpgen-ent and send it together with the document textdtext as the inputs to an LLM, which then outputs alist of entities along with their field names:",
  "where fi is a generated field name for the i-th entityei. We find that LLMs are able to capture a groupof words into a single entity and generate a fieldbased on the context, as observed in (a)": "Introducing KV entity knowledge to pgen.Al-though LLMs can identify entities from documentsto a certain extent, we notice that they are unableto sufficiently enumerate the entities. They tend tolist mostly the major ones, especially when thereare many potential entities in the document, andfail to identify diverse types. To help LLMs toenumerate them, we propose to leverage a docu-ment expert model that extracts key-value (KV)pairs from documents. KV pairs are frequentlyfound in documents, e.g., the entity Name: XYZis composed of a key Name: and a value XYZ.We detect all KV pairs using an external KVdetection model, and send the detected KV pairs toLLMs to obtain their field names. Because there",
  "Patent Status": ": The templates on the left serve as input prompts to the LLM, for (a) generating non-KV entities and(b) naming KV entities, respectively. For (b), in the iteration n, the n-th KV entity is provided as input as well as theoutput from the previous iteration. On the right, we show the result of generated entities and field names, with blueboxes representing non-KV entities and red boxes representing KV entities.",
  "fT(dtext, pgen-kv, (fi, ei)1:n, en+1)agen-kv = fn+1": "where fn+1 is a field name for the KV entity en+1,as result of the (n + 1)-th generation. This way,we make the LLM focus on the field generationonly for the current KV entity. In addition, it hasaccess to previous generated outputs, so if there aresimilar entities given, it can assign the same field.Note that we do not eliminate the entity genera-tion process by pgen-ent. Not all entities are detectedby the KV detection model, so it is still required toextract non-KV entities. Hence, when generatingnon-KV entities, we provide the OCR text in whichall KV entities are removed.",
  "Document Classification": "We formulate a classification task within a seq2seqframework so that a VDU model can generalize toany novel classes. Specifically, we design the inputprompt as ptask = Document: dtext. Question:what is the class of this document?choose from the following: {candidatelist}, and correspondingly, atask = Answer:class label.The candidate list containsdocument class labels, including the answer class.We collect the LLM-generated labels to fill out theprompt without human annotations.",
  "Designing document class generation task.Wegenerate candidates of class labels that can further": "be used to formulate a downstream classificationtask. For this, we need two types of generationprompts. pgen-pos is used to generate candidates ofa given documents type, and we call this outputlist positive labels that may be used as an answer.In order to build a classification task, we not onlyneed the document types that match the given docu-ment but also the candidate types that do not matchthe document. LLM is instructed with pgen-neg tosuggest these types, which we call negative labels. Introducing knowledge from agen to pgen.Wenotice that when an LLM is directly prompted topredict document classes, it frequently generatesclass labels that are overly general, resulting in lowdiversity. To address this, we incorporate docu-ment descriptions to pgen which we find can facil-itate LLMs to better summarize a document andgenerate more diverse class labels.LLM is instructed with pgen-desc = Describethis document in one sentence. The out-put document description agen-desc is then appendedto the generation prompt for positive labels. Thisstrategy makes the positive labels more diverseand detailed, e.g., letter consumer letter.Subsequently, we also use the output positives inthe negatives generation prompt, in order to avoidgenerating labels that are similar to the positives.We summarize the generation steps as follows:",
  "(3) negatives: fT(dtext, pgen-neg, agen-pos)agen-neg": "While this approach does not directly leverage vi-sual information, it adopts a similar strategy tothe chain-of-thought reasoning (Wei et al., 2022;Hsieh et al., 2023) that encourages better outputsby prompting the instruction steps to LLMs. Candidate list formulation.We select one posi-tive label the list agen-pos, as an answer. For othernon-answer candidates, we randomly sample a fewfrom agen-neg. We train the model to choose oneamong the {positive + negatives} list. In ad-dition, the generated description agen-desc is ap-pended to each positive label to give a hint aboutthe class. We also gather all unique negative classesand use the LLM to produce descriptions for thesetypes, which are also appended to the labels. Referto Appx. B.3 for the prompt we used based on this.",
  "Implementation Details": "Models.We compare the DocKD performancewith the plain KD approach, navely using dtext andpgen without external document knowledge, as aprompt engineering baseline. By default, we useClaude-2 2 as a teacher LLM and DocFormerv2large(Appalaraju et al., 2023) as a student VDU model,while partially using DocFormerv2base to facilitatemore efficient analysis. The training procedure ofDocFormerv2 (DFv2) closely follows that of theoriginal paper, where it jointly encodes documentimage, OCR text, and bounding boxes. The pro-vided query (ptask) is appended to the text (dtext),and the decoder outputs the target answer (atask).For comparison, we also employ Flan-T5large(Chung et al., 2022) as a student language-onlymodel, since the DFv2 structure is based on T5(Raffel et al., 2020). To provide a base comparisonfor each task, we additionally present the zero-shotperformance of instruction-tuned LLMs (Chunget al., 2022; Almazrouei et al., 2023b; Chiang et al.,2023) and a vision-language multi-modal founda-tion model (Liu et al., 2023a). Datasets.For the LLMs data generation, we usea randomly sampled subset of Industry DocumentLibrary (IDL, Lewis et al. (2006)) as unannotateddocument images. To accurately evaluate the open-world capabilities, we have removed all IDL docu-ments that overlap with any of our downstream taskdatasets and excluded them from the data genera-tion phase. For the evaluation datasets and metrics, we use DocVQA (Mathew et al., 2021) validationset in the document VQA task, measured by ANLS(average normalized Levenshtein similarity) (Bitenet al., 2019) and EM (exact match). In the entityextraction, we use two datasets, CORD (Park et al.,2019) and DeepForm (Borchmann et al., 2021),evaluated by entity-level F1 score and ANLS, re-spectively. In the classification task, we use RVL-CDIP (Harley et al., 2015) test set, evaluated by themean accuracy over 16 document categories. Referto Appx. D for more details on each dataset.",
  "Evaluation on Open-World DocumentUnderstanding Tasks": "Document VQA.Claude-2 generates QAs fromrandomly sampled 100K IDL documents.Weprompt Claude-2 to generate three QA pairs perdocument sample, and the trained student modelis evaluated on DocVQA (Mathew et al., 2021). (a) summarizes the DocVQA performancesof the distilled students as well as the LLMs, wherenone of these models have been trained on humanannotations for the document VQA task. We con-firm that knowledge-distilled student models caneffectively answer document questions, being com-parable with much larger-size language models.Compared to the plain KD with raw OCR text,DocKD significantly enhances the performance upto 81.0% ANLS. This result is comparable to us-ing human-labeled annotations (refer to Sec. 4.3),which implies the high quality of generated data.Furthermore, the performance gain is greater withDFv2 (vision + language) than Flan-T5 (language),which shows that the linearization model supple-ments informative visual knowledge. Entity extraction.For generating the entitieswith KV detection, we need documents with richkey and value information. Such documents arefrequently found from forms or invoices. Thus, in-stead of using IDL, we use the invoices subset ofRVL-CDIP (Harley et al., 2015) for entity genera-tion, sampling 5K documents. (b) demon-strates that if the data generation does not involvethe KV detection model but only exploits the en-tity generation prompt pgen-ent, the LLM produceslow-quality entities and field names, leading to thesubpar performance of the student models.",
  "(a) VQA(b) Entity extraction(c) Classificationmodelsizeval ANLSval EMtest F1test ANLStest mAcctest mAcc": "LLM zero-shot predictionFlan-T5large (Chung et al., 2022)750M59.648.80.902.5746.754.0Flan-T5XXL (Chung et al., 2022)11B70.460.021.224.152.058.1LLaVA-1.5 (Liu et al., 2023a)13B49.037.39.125.2036.143.3Vicuna-1.3 (Chiang et al., 2023)33B62.451.924.327.648.457.7Falcon (Almazrouei et al., 2023b)40B72.462.748.538.737.943.3 VDU models trained with only generated dataFlan-T5large + KD750M70.459.424.456.352.359.8Flan-T5large + DocKD750M72.962.755.966.157.071.7DocFormerv2large + KD750M76.967.430.251.858.669.0DocFormerv2large + DocKD750M81.071.961.568.762.473.9 : Document understanding results for LLMs and student VDU models. Note that none of these models weretrained with human-labeled annotations. (a) DocVQA validation performance. KD baseline uses raw OCR text forthe QA generation, while DocKD uses linearized OCR text. (b) Entity extraction performance on CORD (F1) andDeepForm (ANLS). KD baseline generates entities without KV detection. (c) RVL-CDIP test accuracy. For DocKD,both class labels and descriptions are generated. mAcc measures the mean accuracy excluding four ambiguouscategories: memo, filefolder, handwritten, and presentation.",
  ": Top-10 frequently generated document classlabels from IDL (Lewis et al., 2006)": "negative labels. (c) shows that our distilla-tion framework enables the student model to clas-sify novel documents, removing the need to pre-define categories or collect annotated documentsto train a classification model. In addition, wefind that DocKDs description generation inducesmore knowledge on documents compared to theplain KD, improving the accuracy by large margin:58.6% 62.4% mAcc. shows the spectrum of generated class la-bels from the IDL documents. After filtering outinvalid labels (e.g., too long or outliers), it amountsto 49.9K unique positive labels and 10.5K uniquenegative labels. Before introducing the descriptiongeneration, we had 17.2K unique positives, imply-ing that the provision of description contributes toincreasing the label diversity.",
  ": We compare the Claude-2 teacher with Falcon-40B and Falcon-180B teacher models, and the DFv2large(750M) and DFv2base (232M) student models": "tion quality and task performances. In contrast,larger and stronger teacher models like Claude-2 or Falcon-180B (Almazrouei et al., 2023a) cangenerate better data, leading to the highest taskperformances. For instance, Claude-2 better un-derstands the linearized OCR text than Falcon-40Bdoes, so it generates diverse and accurate QAs fromthe layout-aware text. Refer to Appx. C for com-parisons between different teacher models. Visualization and statistics of generated data. visualizes some sample data generated byusing KD and DocKD respectively.For docu-ment VQA, DocKD generates more challengingQA pairs that requires understanding the structureof the table. In (a), the question generatedby DocKD requires understanding the relationshipbetween mean, moisture content %, samplecode and sample point. For entity extraction,we show a common example in (b) where welist the entity names extracted by KD and DocKD.We see that DocKD is able to capture significantlymore entities than KD. For document classification,we note that DocKD generates a document descrip-tion which help to give class labels that aligns betterwith the document content. Additional examplesof DocKD-generated data are available in Appx. C.",
  ": Statistics of data generated by KD and DocKD": "shows some statistics of the data gener-ated by KD and DocKD. For entity extraction, wecalculate the number of unique entity types (# ofent. types) and average number of entities gener-ated per document (# of ent. per doc.). We note thatDocKD can generate significantly more entities andentity types than KD, by leveraging external doc-ument knowledge. Similarly, we also summarizethe number unique document labels generated byKD and DocKD for document classification. Forboth the positive and negative class labels, DocKDgenerates more unique labels than KD. We attributethis to leveraging document descriptions for gen-eration which helps LLMs generating fine-grainedlabels that align better with the document.",
  ": Open-set classification performance. S: su-pervised training with C1 annotations, U: unsupervisedDocKD from LLM-generated class labels": "models, achieving 83.4% ANLS on the DocVQAvalidation set. In a more practical scenario wherehuman-labeled documents have different distribu-tion, we utilize DUDE, a dataset featuring multi-domain documents with diverse VQA annotations(text, numerical, yes/no, lists, etc.). In (b),DocKD-generated data significantly enhances stu-dent model performance, reaching 79.1% ANLS,compared to 66.0% with human annotations alone. Open-set classification.One of the main appli-cations by distilling LLMs knowledge lies in itsopen-set classification ability, i.e., it can classifydocuments of unseen categories. The diversity ofgenerated class labels ensures robustness, while afixed set of annotations makes it hard to adapt tounseen labels. To verify this, let C denote the setof all RVL-CDIP labels, and we split C into twosets: C1 = {email, letter, memo, news article} andC2 = C C1. We train the model with documentsfrom the web, crawled by C1 labels (Larson et al.,2022). shows that this supervised model(S) makes highly biased predictionswhile it pre-dicts known classes accurately (86.1%), it strugglesto identify unknown categories in C2. In contrast,DocKD without any supervised data (U) enablesgeneralization to unseen types of documents. Fur-ther, merging the C1 annotations with the generateddata (S+U) leverages the advantages of both super-vised and unsupervised learning.We also evaluate our model in a more realisticdistribution of data and labels, using the documents out of the domain of IDL or RVL-CDIP. To thisend, we use three evaluation sets, RVL-O (Larsonet al., 2022), IRS-50, and WikiDoc (Fujinuma et al.,2023), all of which contain out-of-domain docu-ments (refer to Appx. D for the details of datasets).While the supervised model cannot handle thesenovel categories, unsupervised DocKD makes thestudent model even adaptable to out-of-domainclassification and outlier detection, following theLLM teachers robust predictions.",
  "Conclusion": "We address the open-world document understand-ing problem by instructing the LLMs to generatedocument annotations, given the generation promptand OCR text. To successfully achieve this, we sug-gest DocKD framework, designing task promptsand answers that LLMs can easily generate, and in-corporate external document knowledge from var-ious sources. Consequently, the student modelsdistilled by DocKD annotations demonstrate re-markable performance improvements compared tothe plain KD approach in various document tasks.The integration with human-labeled annotationsfurther enhances model performance.",
  "Limitations": "This study represents the pioneering work to uti-lize LLMs for open-world document understand-ing, specifically focusing on relatively simpler doc-uments and tasks.We have applied LLMs togenerate document annotations, and subsequently,trained student VDU models using these annota-tions. Our primary focus has been on commondocument understanding tasks such as visual ques-tion answering, entity extraction, and classification,which primarily involve documents containing ta-bles, layouts, and forms.However, extending our approach to handle doc-uments with more complex visual elements, suchas intricate figures, diagrams, or dense equations,remains an area for future exploration. While ad-dressing more sophisticated problems could sig-nificantly enhance the models applicability, suchadvancements would require efforts in developingnew generative prompts. Furthermore, integratingLLMs with document expert models and large mul-timodal models, such as GPT-4V, holds potential tosynthesize visually-rich, informative annotations.This integration has not yet been explored and rep-resents a promising avenue for future research. De-",
  "spite these limitations, our study lays foundationalwork for more complex applications in the field ofdocument understanding using LLMs": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Maitha Alhammadi, Mazzotta Daniele, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.2023a. The falcon series of language models: To-wards open frontier models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Merouane Debbah, Etienne Goffinet, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.2023b. Falcon-40B: an open large language modelwith state-of-the-art performance. Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,Yusheng Xie, and R Manmatha. 2021. Docformer:End-to-end transformer for document understanding.In Proceedings of the IEEE/CVF international con-ference on computer vision, pages 9931003.",
  "Srikar Appalaraju, Peng Tang, Qi Dong, NishantSankaran, Yichu Zhou, and R Manmatha. 2023. Doc-formerv2: Local features for document understand-ing. arXiv preprint arXiv:2306.01733": "Dana Aubakirova, Kim Gerdes, and Lufei Liu. 2023.Patfig: Generating short and long captions for patentfigures. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 28432849. Ali Furkan Biten, Ruben Tito, Andres Mafla, LluisGomez, Maral Rusinol, Minesh Mathew, CV Jawa-har, Ernest Valveny, and Dimosthenis Karatzas. 2019.Icdar 2019 competition on scene text visual ques-tion answering. In 2019 International Conference onDocument Analysis and Recognition (ICDAR), pages15631570. IEEE. ukasz Borchmann, Micha Pietruszka, Tomasz Stanis-lawek, Dawid Jurkiewicz, Micha Turski, KarolinaSzyndler, and Filip Gralinski. 2021. Due: End-to-enddocument understanding benchmark. In Thirty-fifthConference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (Round 2). Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality": "Hyung Won Chung, Le Hou, Shayne Longpre, Bar-ret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong,Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi. 2023. InstructBLIP: Towardsgeneral-purpose vision-language models with instruc-tion tuning. In Thirty-seventh Conference on NeuralInformation Processing Systems.",
  "Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang.2023. Knowledge distillation of large language mod-els. arXiv preprint arXiv:2306.08543": "Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang.2016. A table detection method for pdf documentsbased on convolutional neural networks. In 201612th IAPR Workshop on Document Analysis Systems(DAS), pages 287292. IEEE. Adam W Harley, Alex Ufkes, and Konstantinos G Der-panis. 2015. Evaluation of deep convolutional netsfor document image classification and retrieval. In2015 13th International Conference on DocumentAnalysis and Recognition (ICDAR), pages 991995.IEEE.",
  "Teakgyu Hong, DongHyun Kim, Mingi Ji, WonseokHwang, Daehyun Nam, and Sungrae Park. 2020.Bros: A pre-trained language model for understand-ing texts in document": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.2023. Distilling step-by-step! outperforming largerlanguage models with less training data and smallermodel sizes. arXiv preprint arXiv:2305.02301. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, andFuru Wei. 2022. Layoutlmv3: Pre-training for doc-ument ai with unified text and image masking. InProceedings of the 30th ACM International Confer-ence on Multimedia, pages 40834091. Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. Funsd: A dataset for formunderstanding in noisy scanned documents. In 2019International Conference on Document Analysis andRecognition Workshops (ICDARW), volume 2, pages16. IEEE. Jaehun Jung, Peter West, Liwei Jiang, Faeze Brah-man, Ximing Lu, Jillian Fisher, Taylor Sorensen,and Yejin Choi. 2023. Impossible distillation: fromlow-quality model to high-quality dataset & modelfor summarization and paraphrasing. arXiv preprintarXiv:2305.16635. Geewook Kim,Teakgyu Hong,Moonbin Yim,JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Won-seok Hwang, Sangdoo Yun, Dongyoon Han, andSeunghyun Park. 2022. Ocr-free document under-standing transformer. In European Conference onComputer Vision, pages 498517. Springer. Stefan Larson, Yi Yang Gordon Lim, Yutong Ai, DavidKuang, and Kevin Leach. 2022. Evaluating out-of-distribution performance on document image classi-fiers. Advances in Neural Information ProcessingSystems, 35:1167311685. David Lewis, Gady Agam, Shlomo Argamon, OphirFrieder, David Grossman, and Jefferson Heard. 2006.Building a test collection for complex document in-formation processing. In Proceedings of the 29thannual international ACM SIGIR conference on Re-search and development in information retrieval,pages 665666.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, PeiyiWang, Shuhuai Ren, Mukai Li, Yazheng Yang,Jingjing Xu, Xu Sun, et al. 2023c. M3it: A large-scale dataset towards multi-modal multilingual in-struction tuning. arXiv preprint arXiv:2306.04387. Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu,Handong Zhao, Rajiv Jain, Varun Manjunatha, andHongfu Liu. 2021. Selfdoc: Self-supervised doc-ument representation learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 56525660.",
  "Lucie Charlotte Magister, Jonathan Mallinson, JakubAdamek, Eric Malmi, and Aliaksei Severyn. 2022.Teaching small language models to reason. arXivpreprint arXiv:2212.08410": "Minesh Mathew, Viraj Bagal, Rubn Tito, Dimosthe-nis Karatzas, Ernest Valveny, and CV Jawahar. 2022.Infographicvqa. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vi-sion, pages 16971706. Minesh Mathew, Dimosthenis Karatzas, and CV Jawa-har. 2021. Docvqa: A dataset for vqa on documentimages. In Proceedings of the IEEE/CVF winter con-ference on applications of computer vision, pages22002209.",
  "OpenAI. 2023. Gpt-4v(ision) system card": "Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee,Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. 2019.Cord: a consolidated receipt dataset for post-ocrparsing. In Workshop on Document Intelligence atNeurIPS 2019. Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo,Zhenyu Zhang, Zhengjie Huang, Teng Hu, WeichongYin, Yongfeng Chen, Yin Zhang, et al. 2022. Ernie-layout: Layout knowledge enhanced pre-trainingfor visually-rich document understanding.arXivpreprint arXiv:2210.06155. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551. Carlos Soto and Shinjae Yoo. 2019. Visual detectionwith context for document layout analysis. In Pro-ceedings of the 2019 Conference on Empirical Meth-ods in Natural Language Processing and the 9th In-ternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 34643470.",
  "Nishant Subramani, Alexandre Matton, MalcolmGreaves, and Adrian Lam. 2020. A survey of deeplearning approaches for ocr and document under-standing. arXiv preprint arXiv:2011.13534": "Ryota Tanaka, Taichi Iki, Kyosuke Nishida, KunikoSaito, and Jun Suzuki. 2024. Instructdoc: A datasetfor zero-shot generalization of visual document un-derstanding with instructions.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1907119079. Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,Yang Liu, Chenguang Zhu, Michael Zeng, ChaZhang, and Mohit Bansal. 2023. Unifying vision,text, and layout for universal document processing.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1925419264. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jordy Van Landeghem, Rubn Tito, ukasz Borchmann,Micha Pietruszka, Pawel Joziak, Rafal Powalski,Dawid Jurkiewicz, Mickal Coustaty, Bertrand An-ckaert, Ernest Valveny, et al. 2023. Document under-standing dataset and evaluation (dude). In Proceed-ings of the IEEE/CVF International Conference onComputer Vision, pages 1952819540.",
  "Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, andSandeep Tata. 2022b. A benchmark for structuredextractions from complex documents. arXiv preprintarXiv:2211.15421": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, FuruWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, ChaZhang, Wanxiang Che, et al. 2020. Layoutlmv2:Multi-modal pre-training for visually-rich documentunderstanding. arXiv preprint arXiv:2012.14740.",
  "A.1Statistical Significance of DocumentUnderstanding Results": "We have conducted further experiments to sub-stantiate our findings about statistical significance.Specifically, we reproduced the main results acrossall three tasks () by rerunning the experi-ments for the configurations DocFormerv2large +KD and DocFormerv2large + DocKD using threedifferent random seeds. The results of these ad-ditional runs are summarized in . Theseresults underscore the statistical significance andreliability of our approach.",
  "A.3Data Volume and Quality": "In , we emphasize the significance of the dis-tilled data volume in capturing diverse knowledge.Additionally, the introduction of a small set of hu-man annotations (e.g., DUDE (Van Landeghemet al., 2023)) from a different domain proves ben-eficial, especially when the teacher model size issmall and thus generates data of lower quality.However, it is crucial to note that a larger vol-",
  "Training with Claude-2-generated dataDocFormerv2large + KD QA750M75.8DocFormerv2large + DocKD QA750M80.6DocFormerv2large + DocKD QA (+ DocVQA anno.)750M86.9": ": DocVQA test set performance. The KD base-line uses raw OCR text for the QA generation, whileDocKD uses the linearized OCR text. : reproducedwithout searching hyperparameters. The same hyperpa-rameters were used for training with DocKD QAs. ume of generated data does not always guaranteesuperior performance, i.e., quality of the datasetis also important. For the classification task, weestablished evaluation criteria for generated labels,accounting for both word length and frequencywithin the dataset. Labels exceeding a word lengthof 5 (considered overly specific) or occurring lessthan 3 times throughout the dataset (outliers) wereexcluded. Documents without remaining positivelabels were removed, consequently reducing ourIDL training set size from 50K to 43K. This re-finement enhanced overall data quality, resultingin an improved test accuracy (+3.5%). Similarly,in VQA and entity extraction tasks, we filtered outexcessively long or short questions/answers andfield names identified as outliers.",
  "A.4Using Human-Labeled FUNSD Entities": "For the entity extraction task, we utilized RVL-CDIP invoices (Harley et al., 2015), extractingkeys and values, and applying the entity genera-tion prompts. Here, we use FUNSD (Jaume et al.,2019) dataset, which is a small subset of RVL-CDIP forms, and all the KV entities are manuallyannotated. In this case, we use their annotations forthe KV entity inputs. shows that, althoughFUNSD contains only a small number of documentsamples, an LLM can generate reliable KV entityfields based on the manual annotations. Combin-ing with invoices documents that have abundantentities, the student model is effectively distilledwith diverse knowledge and can exhibit the highestentity extraction performances.",
  ": Entity extraction from FUNSD (Jaume et al.,2019) and RVL-CDIP invoices (Harley et al., 2015)documents. The student model is DFv2base": "tection models key-value extraction ability. Tounveil the individual contributions of each compo-nent, presents an ablation study on differ-ent entity generation methods. Using only pgen-entrepresents the plain KD baseline without externaldocument knowledge. On the other hand, usingonly pgen-kv eliminates the LLMs automatic ex-traction of entities that are not detected as keys orvalues. In addition to these approaches, we con-duct key normalization method, where the LLMgenerates variants for each key name, and thesenormalized variants serve as the field for the KVentities. This method does not utilize KV entityconstraints, which have been used in DocKD as aniterative presentation of KV entities for consistencywith previous entities and fields.The ablation study results confirm the signifi-cace of both pgen-ent and pgen-kv, coupled with KV",
  "pgen-ent + pgen-kv (DocKD)55.1": ": Ablation study on CORD (Park et al., 2019)entity extraction. Entities and field names are generatedfrom 5K RVL-CDIP invoices (Harley et al., 2015) bythe Falcon-40B (Almazrouei et al., 2023b) teacher. Thestudent model is DFv2base. Note that pgen-kv alwaysrequires the KV detection in prior. detection. Notably, providing the LLM with de-tected KV pairs yields substantial improvement(pgen-ent vs. DocKD), while the extraction of non-KV entities also proves to be crucial (pgen-kv vs.DocKD). Injecting context on previous KV entitiesand the generated fields further enhances the relia-bility of subsequent generation (key normalizationvs. DocKD).",
  "B.1Generation Prompt for Document VQA": "In the document VQA task, the generation promptserves as a guidance for the LLM to generatea fixed number of question-answer (QA) pairs,which can be answered by referencing the docu-ments OCR text. To facilitate this process, weprovide two instructive examples and articulateseveral rules. Then, for the specific target docu-ment, which is an IDL (Lewis et al., 2006) doc-ument in our study, we extract OCR text fromthe image, convert it to linearized text (referto Sec. 3.1), and embed this text into the place-holder {LINEARIZED_TEXT_PLACE_HOLDER} inpgen. We set {COUNT_PLACE_HOLDER} to three.",
  ": RVL-CDIP classification results of all 16 categories": "CARDIOVASCULAR DISEASE 2.11.15-19 HyperglycemiaInsulin Hyper a path that leads to increased riskfor MI Resistance Dys TYPE 2 DIABETES EQUALS PRIORMI AS A CHD RISK FACTOR Pr S 7-year incidenceof myocardial infarction (MI) (%) 25% 20% 15%18.8% 20.2% 10% 5% 0% Nondiabetic patients Type2 diabetics with prior MI without prior MI",
  "B.2Generation Prompt for Entity Extraction": "We separate the generation of entities and fieldnames into two parts: for non-KV entities andfor KV entities. For the former, the generationprompt pgen-ent is employed to extract entitiesfrom the document text as well as assigning theirnames. This process is exemplified through twoinstructive examples.Provided with the docu-ment text, the LLM is instructed to extract enti-ties enclosed with <regular> and </regular>tags.Also, each line of entity is delimited bya separator - , followed by the correspond-ing generated field name. Note that, to avoid du-plicated generations for KV entities, we remove",
  "all the detected KV entities from the documenttext: {TEXT_WITHOUT_KV_PLACE_HOLDER} (re-fer to Sec. 3.2)": "For the KV entities identified by a KV detec-tion model, pgen-kv instructs the LLM to gener-ate only the field names for these entities. In theOCR text, the KV entities are enclosed by the tags<kv> and </kv> to provide explicit guidanceto the model regarding which part it should re-fer to. The iterative presentation of each KV en-tity, line by line, involves inputting each line into{CONSTRAINTS_PLACE_HOLDER} in the format of<kv>key value</kv> - . The generatedfield name is then appended to the constraint forthe next iteration.",
  "B.3Generation and Inference Prompts forDocument Classification": "In the document classification task, we need threedistinct generation prompts designed for gener-ating descriptions, positive labels list, and neg-ative labels list, respectively. Initially, pgen-descprompts the LLM to generate a description bycharacterizing the document type based on thedocument text. Subsequently, the generated out-put agen-desc is incorporated into the followingprompt, pgen-pos, specifically within the place-holder {DESCRIPTION_PLACE_HOLDER}.Thisserves the purpose of providing contextual in-formation about the document, thereby facil-itating the accurate generation of positive la-bels. Finally, the output agen-pos is introduced to{POSITIVES_PLACE_HOLDER} in the negative gen-eration prompt pgen-neg. This instructs the LLM toavoid suggesting types similar to those in the posi-tives list.",
  "B.4Connectivity Between the ProposedMethods": "In this study, tailoring generation prompts and doc-ument text formats for specific tasks has been pro-posed, and there is a potential for synergy whencombining these approaches. However, the effec-tiveness of such combination depends on the cho-sen document knowledge injection method and thenature of the task. For instance, we observed thattext linearization did not enhance classification ac-curacy and could not be transferred to entity extrac- tion, as the field name generation also involves dis-tinct modifications to dtext (refer to Appx. B.2). Onthe other hand, leveraging document descriptionsor reasoning steps may hold promise for improvingthe QA generation. Yet, this would require non-trivial efforts in designing new generative prompts,and it is identified as a prospective direction forfuture research.",
  "B.5Improving the Instructions for LLMZero-Shot Prediction": "While numerous strategies exist for enhancingLLM zero-shot predictions through instructionmodulation, the optimal approach varies dependingon the model type. Although we have not exploredoptimal instruction strategies for every languagemodel, our work involves minimal engineering ef-forts to identify the LLMs performance in doc-ument understanding tasks and show that smallstudent models trained by DocKD are as effectiveas the LLMs. In this section, we describe our en-hancements to the prompt for improving zero-shotpredictions of Claude-2 and Falcon-40B models,in document VQA and classification tasks. Essen-tially, we provide the LLM with ptask and dtext asinputs, employing the same design as utilized forthe student models. Within ptask, we input instruc-tions to regulate the output format for each LLM,facilitating the parsing of the answer into the de-sired format. Instructions for DocVQA.We leverage lin-earized OCR text, a method previously employedin generating QA pairs from the LLM. Given theLLMs ability in comprehending linearized text,we convert the OCR text into the linearized formand ask the document question. In addition, sinceDocVQA is an extractive QA dataset, i.e., answersare directly extracted from the provided context,we use the dataset-specific prompt to control theoutputs. To achieve this, we implement instructingrules as suggested in (Wang et al., 2023b). Thisstrategy has significantly increased DocVQA valANLS to 58.3 79.6 for Claude-2, and 52.6 72.4 for Falcon-40B. In summary, the task promptfor DocVQA is provided as follows.",
  "For example,Context: Confidential RJRT PR APPROVAL DATE:": "1/8/93 SUBJECT: Ru IVAs PROPOSED RELEASE DATE: forresponse FOR RELEASE TO: CONTACT: P. CARTER ROUTETO: Name Initials Date Peggy Carter Ace 1/1/15Kaura Payne nt. T/R Return to Peggy Carter, PR, 16Reynolds Building NotAnswer the question: What is the contact personname mentioned in this letter?Answer: P. Carter Rules:- The answers to questions are short text spanstaken verbatim from the document. This meansthat the answers comprise a set of contiguous texttokens present in the document.- Directly extract the answer of the question fromthe document with as few words as possible.",
  "Context: {LINEARIZED_TEXT_PLACE_HOLDER}Answer the question: {QUESTION_PLACE_HOLDER}Answer:": "Instructions for RVL-CDIP.Recognizing thesignificance of document descriptions in enhanc-ing knowledge utilization and improving class la-bel generation, we adopt a 2-step classification ap-proach. In the initial step, the LLM does not clas-sify directly but instead generates the possible docu-ment type according to its own interpretation. Sub-sequently, in the second step, we provide the outputfrom the first step into {TYPE_PLACE_HOLDER} asa suggested document name, and instruct the modelto select the document type from the candidate list.In addition, we recognize that Falcon-40B strug-gles in accurately naming the exact category, evenwhen provided with a list. To address this, we em-phasize all 16 evaluation categories. This strategicmodulation has improved RVL-CDIP test mAcc to31.8 37.9 for Falcon-40B, compared to directclassification. However, Claude-2 does not achievefurther performance gain through this instruction.Additionally, attempts to replace the document textwith linearized text, as done in DocVQA, do notyield improvements in this task.",
  "C.1Generated QAs for Document VQA": "Using raw OCR text vs. linearized OCR text. and describe the generated QAsfrom Claude-2, comparing the results from theplain KD (using raw OCR text) and DocKD (usinglinearized OCR text). In , the documentincludes line numbers for each line of text, butraw OCR text lacks this structural detail, result-ing in misplaced numbers in the middle of text.Consequently, Claude-2 generates inaccurate ques-tions, such as Question 1 erroneously referenc-ing a non-existent question number 2, or Question2 inquiring about the percentage of children, whichcannot be directly answered from the document.In contrast, when linearized OCR text is utilized,questions align with the document context, ensur-ing correct answers. Notably, questions explicitlyrefer to line numbers, e.g., inquiring about the con-tents in line 1 or in lines 58, which requiresvisual knowledge to answer.In , the document contains words andnumbers in a structured form, posing a challengefor the LLM in generating informative QAs fromthe OCR text.In KD QAs, Question 1 andQuestion 3 are easily extracted and straight-forward to answer without visual knowledge.Question 2, which pertains to tabular informa-tion, is paired with Answer 2, which is incorrect.In contrast, Question 2 of DocKD requires ref-erence to the table format, specifically in the thirdrow and the second column, for a correct response.Also, the paired Answer 2 is correct. Similarly,Question 3 and Answer 3 are about the contentsin the second row and the last column of the table.",
  "LLM teachers: Falcon-40B vs. Falcon-180Bvs. Claude-2. and describethe generated QAs from different teacher models,": "using Falcon-40B, Falcon-180B, and Claude-2. Ev-ery teacher utilizes the linaerized OCR text. Thetarget document in corresponds to the oneused in , and the document for cor-responds to the one used in . While Claude-2 adeptly incorporates layout knowledge into QAgeneration, Falcon-40B tends to produce simplequestions and answers, occasionally resulting induplicates or only slight variations. In contrast, theFalcon-180B model better generates diverse QApairs, and they are mostly accurate. The primarydistinction from Claude-2 lies in the observationthat Claude-2 is more inclined to explicitly mentionlayout information in the document. 2-step generation of Q A.In QA generationfor the document VQA task, we have directed theLLM to simultaneously produce both questions andanswers. This approach aims to ensure consistencywith the document contents and establish more ac-curate relationship between the generated questionand its corresponding answer. Alternatively, we ex-plore a 2-step generation process where the LLMinitially generates a list of questions and subse-quently provides answers for them. and delineate questions andanswers generated by Claude-2, comparing thetwo distinct generation schemes: 2-step genera-tion and QA simultaneous generation. In ,the target document features a table with limitedextractable information. During the first step ofquestion generation, Claude-2 manages to producequestions related to the table headers or the index,yet these remain challenging to answer based on thetext. As result, the second step generates randomnumber answers. Conversely, QA pair simultane-ous generation yields better questions and answers,effectively leveraging structural information, e.g.,column headers or numbers and ratios listed in thetable, and creating easy-to-answer questions fromthem.Similar observations are found in ,where the document contains a plot and there is notmuch information other than the header, axes, andaxis labels. In the 2-step generation, questions areformulated regarding the efficiency and percentageof the filtraion, which cannot be addressed usingthe available document content. The resulting an-swers include phrases like not mentioned ornot provided. Conversely, QA pair generationproduces questions that are easily answerable.",
  "C.2Generated Entities and Fields for EntityExtraction": "displays the generated entities and fields forthe RVL-CDIP (Harley et al., 2015) invoice docu-ments. Similar to in the main paper, non-KVentities and their respective field names are repre-sented by blue boxes and text, while detected KVentities and their corresponding field names are de-noted by red boxes and text. It includes an examplewhere the document is non-English (id: jmi32e00);surprisingly, leveraging the multilingual capabil-ity of the LLM, informative entities are extractedand field names are generated in English. Through-out the examples in , a diverse range of fieldnames is observed.Upon generating entities and fields, an ag-gregation process is employed prior to trainingthe student model. There exist multiple entitieswithin a single document sharing the same fieldname. We group these entities under the sharedfield, so that the student model can be trainedto match the field to every entity in the group.Specifically, we gather all generated field-entitypairs {(f1, e1), (f2, e2), . . . } and identify the en-tity group for each field f, {ej} for all j such thatfj = f. Consequently, f is incorporated into ptask,and {ej} is included in atask.",
  "We provide additional information on the datasetsthat were not fully described in the main paper": "Evaluation datasets.In the document VQA task,we use DocVQA (Mathew et al., 2021) as an evalu-ation dataset. The DocVQA validation set containsmanually annotated 5.3K questions related to thereal-world industrial documents. For metrics, weuse ANLS (average normalized Levenshtein sim-ilarity) (Biten et al., 2019) and EM (exact match)which checks if the predicted answers charactersexactly match those of the ground truth.For the entity extraction, we use two evaluationdatasets, CORD (Park et al., 2019) and DeepForm(Borchmann et al., 2021), a collection of restaurantreceipts and invoices for political TV ads, respec-tively. The model should extract entities for thefield such as <menu name> or <total cashprice>for CORD, and <advertiser> or <flight to> forDeepForm. The CORD test set is evaluated byentity-level F1 score, while the DeepForm test setis evaluated by ANLS since DeepForms ground-truth entities are re-formatted from the originaldocument text.In the classification task, we use RVL-CDIP(Harley et al., 2015) test set, where 40K docu-ments are labeled into 16 categories, including let-ter, memo, invoice, form, etc. The performance ismeasured by the mean accuracy of these 16 cate-gories, while mAcc measures the mean accuracyexcluding four ambiguous categories: memo, file-folder, handwritten, and presentation. Open-set classification.In Sec. 4.3, we haveused three out-of-domain datasets for the open-setclassification. Here, we outline their setups. (i)RVL-O (Larson et al., 2022) has documents that donot belong to any of 16 categories of RVL-CDIP.These outliers should be classified (or detected)as other, with the RVL-CDIP labels also given ascandidates. (ii) For IRS-50, we collect 50 typesof forms, instructions, and publications from theUS Internal Revenue Service.3 (iii) WikiDoc (Fu-jinuma et al., 2023) consists of 33K Wikipediascreenshots on 111 different subjects. presents a summary of the 50 IRS classlabels which were used in . Each class labelcorresponds to one document sample sourced fromthe US Internal Revenue Service. We also present",
  "the precdiction results from Falcon-40B (zero-shot)and DocFormerv2base (DocKD)": "WikiDoc categories.The WikiDoc dataset, asdescribed in Fujinuma et al. (2023), comprises111 diverse categories.For each category, thedataset includes screenshots of Wikipedia articles,encompassing a wide range of subjects. Exam-ples of categories in the dataset inlcude Album,BasketballTeam, Cardinal, Dam, Economist, Fish,Glacier, Historian, IceHockeyLeague, Journalist,Lighthouse, Magazine, Noble, OfficeHolder, Poem,Racecourse, School, TradeUnion, University, Vol-cano, and WrestlingEvent. DUDE single-page QAs.Throughout this pa-per, our primary focus was on training the studentmodel using single-page document annotations, i.e.,document annotation is derived from the contentsin a single page. There are document datasets anno-tated with multi-page information, such as DUDE(Borchmann et al., 2021) that is employed for thedocument VQA task in . In this case, weonly used the QA annotations that can be addressedwithin a single page.",
  "Form 1040-NR (Schedule NEC)Form 1040-NR (Schedule NEC)Form 1040-NR (Schedule NEC)Form 1040-NR (Schedule OI)NULLForm 1040-NR": "Form 1040-XTax formForm 1040-XForm 1098-CForm 1098-CForm 1098-CForm 1098-EForm 1098-EForm 1098-EForm 1098-MAForm 1098-MAForm 1098-MAForm 1098-QForm 1098-QForm 1098-QForm 4506Form 4506Form 4506Form 4506-TTax formForm 4506-TForm 4852Form 4852Form 4852Form 8994FormForm 8994Form 9779FormForm 9779Form 9783Form 1000Form 9783Form 15103Form 15103Form 15103Form W-2Form W-2Form W-2Form W-2ASForm W-2ASForm W-2ASForm W-2CForm W-2CForm W-2CForm W-2GForm W-2GForm W-2GForm W-3Form W-3Form W-2",
  "Form W-3SSForm W-3SSForm W-2AS": "Form W-4Form 1040 (Schedule 1)Form W-4Form W-4PForm W-4PForm W-4PForm W-4RForm 1040 (Schedule 1)Form W-4RForm W-4SForm W-4SForm W-4SForm W-7Form W-7Form W-7Form W-7AForm W-7AForm W-7AInstruction 1040 (Schedule A)Form 1040 (Schedule A)Instruction 1040 (Schedule A)Instruction 1040 (Schedule B)Form 1040 (Schedule B)Notice 1016 Instruction 1040-NRFormInstruction 1040-NRInstruction 1098-QInstruction 1098-QInstruction 1098-QInstruction 8994Form 8994Instruction 8994Notice 1015Form 1000Notice 1015Notice 1016NoticeNotice 1016Notice 1027NoticeNotice 1027Notice 1392PublicationNotice 1392Publication 15Publication 15Publication 15Publication 16Publication 16Publication 16Publication 17Publication 17Publication 17Publication 216PublicationPublication 216Publication 1141PublicationPublication 1141Publication 1223PublicationPublication 1223Publication 1516Publication 1516Publication 1516Publication 1518-APublicationPublication 1518-APublication 1546PublicationPublication 1546"
}