{
  "Abstract": "Recent advances in machine learning have sig-nificantly impacted the field of information ex-traction, with Language Models (LMs) play-ing a pivotal role in extracting structured in-formation from unstructured text. Prior workstypically represent information extraction astriplet-centric and use classical metrics such asprecision and recall for evaluation. We reformu-late the task to be entity-centric, enabling theuse of diverse metrics that can provide more in-sights from various perspectives. We contributeto the field by introducing Structured Entity Ex-traction and proposing the Approximate EntitySet OverlaP (AESOP) metric, designed to ap-propriately assess model performance. Later,we introduce a new Multistage Structured En-tity Extraction (MuSEE) model that harnessesthe power of LMs for enhanced effectivenessand efficiency by decomposing the extractiontask into multiple stages. Quantitative and hu-man side-by-side evaluations confirm that ourmodel outperforms baselines, offering promis-ing directions for future advancements in struc-tured entity extraction. Our source code is avail-able at 1IntroductionInformation extraction refers to a broad familyof challenging natural language processing (NLP)tasks that aim to extract structured informationfrom unstructured text (Cardie, 1997; Eikvil, 1999;Chang et al., 2006; Sarawagi et al., 2008; Grish-man, 2015; Niklaus et al., 2018; Nasar et al., 2018;Wang et al., 2018; Martinez-Rodriguez et al., 2020).Examples of information extraction tasks include:(i) Named-entity recognition (Li et al., 2020), (ii) re-lation extraction (Kumar, 2017), (iii) event extrac-tion (Li et al., 2022), and (iv) coreference reso-lution (Stylianou and Vlahavas, 2021; Liu et al.,2023), as well as higher-order challenges, such as",
  "Equal contribution with random order. Work done during an internship at Microsoft Research": ": Illustration of the structured entity extraction,an entity-centric formulation of information extraction.Given a text description as well as some predefinedschema containing all the candidates of entity types andproperty keys, we aim to output a structured json for allentities in the text with their information. automated knowledge base (KB) and knowledgegraph (KG) construction from text (Weikum andTheobald, 2010; Ye et al., 2022; Zhong et al., 2023).The latter may in turn necessitate solving a combi-nation of the former more fundamental extractiontasks as well as require other capabilities like en-tity linking (Shen et al., 2014, 2021; Oliveira et al.,2021; Sevgili et al., 2022). Previous formulations and evaluations of informa-tion extraction have predominantly centered aroundthe extraction of subject, relation, object triplets.The conventional metrics used to evaluate triplet-level extraction, such as recall and precision, how-ever, might be insufficient to represent a modelsunderstanding of the text from a holistic perspec-tive. For example, consider a paragraph that men-tions ten entities, where one entity is associatedwith 10 relations as the subject, while each of theother nine entities is associated with only 1 relationas the subject. Imagine a system that accuratelypredicts all ten triplets for the heavily linked entitybut overlooks the other entities. Technically, thissystem achieves a recall of more than 50% (i.e.,10 out of 19) and a precision of 100%. However, when compared to another system that recognizesone correct triplet for each of the ten entities andachieves the same recall and precision, it becomesevident that both systems, despite showing identi-cal evaluation scores, offer significantly differentinsights into the text comprehension. Moreover,implementing entity-level normalization within tra-ditional metrics is not always easy due to chal-lenges like coreference resolution (Stylianou andVlahavas, 2021; Liu et al., 2023), particularly inscenarios where multiple entities share the samename or lack primary identifiers such as names.Therefore, we advocate for alternatives that canoffer insights from diverse perspectives. In this work, we propose Structured EntityExtraction, an entity-centric formulation of (strict)information extraction, which facilitates diverseevaluations. We define a structured entity as anamed entity with associated properties and rela-tionships with other named-entities. showsan illustration of the structured entity extraction.Given a text description, we aim to first identify thetwo entities Bill Gates and Microsoft. Then,given some predefined schema on all possible en-tity types and property keys (referred to as a strictsetting in our scenario), the exact types, propertykeys, property values on all identified entities inthe text are expected to be predicted, as well as therelations between these two entities (i.e., Bill Gatesco-founded Microsoft). Such extracted structuredentities may be further linked and merged to auto-matically construct KBs from text corpora. Alongwith this, we propose a new evaluation metric,Approximate Entity Set OverlaP (AESOP), withnumerous variants for measuring the similarity be-tween the predicted set of entities and the groundtruth set, which is more flexible to include differ-ent level of normalization (see default AESOP inSec. 3 and other variants in Appendix A). In recent years, deep learning has garnered signif-icant interest in the realm of information extrac-tion tasks. Techniques based on deep learning forentity extraction have consistently outperformedtraditional methods that rely on features and kernelfunctions, showcasing superior capability in fea-ture extraction and overall accuracy (Yang et al.,2022). Building upon these developments, ourstudy employs language models (LMs) to solvestructured entity extraction. We introduce a Multi-stage Structured Entity Extraction (MuSEE) model, a novel architecture that enhances both effective-ness and efficiency. Our model decomposes the en-tire information extraction task into multiple stages,enabling parallel predictions within each stage forenhanced focus and accuracy. Additionally, we re-duce the number of tokens needed for generation,which further improves the efficiency for both train-ing and inference. Human side-by-side evaluationsshow similar results as our AESOP metric, whichnot only further confirm our models effectivenessbut also validate the AESOP metric.",
  "In this section, we first review the formulation of ex-isting information extraction tasks and the metricsused, followed by a discussion of current methodsfor solving information extraction tasks": "Information extraction tasks are generally dividedinto open and closed settings. Open informationextraction (OIE), first proposed by Banko et al.(2007), is designed to derive relation triplets fromunstructured text by directly utilizing entities andrelationships from the sentences themselves, with-out adherence to a fixed schema.Conversely,closed information extraction (CIE) focuses on ex-tracting factual data from text that fits into a pre-determined set of relations or entities, as detailedby Josifoski et al. (2022). While open and closedinformation extraction vary, both seek to convertunstructured text into structured knowledge, whichis typically represented as triplets. These tripletsare useful for outlining relationships but offer lim-ited insight at the entity level. It is often assumedthat two triplets refer to the same entity if theirsubjects match. However, this assumption is notalways held. Additionally, the evaluation of thesetasks relies on precision, recall, and F1 scores at thetriplet level. As previously mentioned, evaluating solely on triplet metrics can yield misleading in-sights regarding the entity understanding. Thus, itis essential to introduce a metric that assesses under-standing at the entity level through entity-level nor-malization. In this work, we introduce the AESOPmetric, which is elaborated on in Sec. 3.2. Various strategies have been employed in existingresearch to address the challenges of informationextraction. TextRunner (Yates et al., 2007) initiallyspearheaded the development of unsupervisedmethods. Recent progress has been made with theuse of manual annotations and Transformer-basedmodels (Vasilkovsky et al., 2022; Kolluru et al.,2020a). Sequence generation approaches, like IMo-JIE (Kolluru et al., 2020b) and GEN2OIE (Kolluruet al., 2022), have refined open information extrac-tion by converting it into a sequence-to-sequencetask (Cui et al., 2018). GenIE (Josifoski et al.,2022) focuses on integrating named-entity recogni-tion, relation extraction, and entity linking withina closed setting where a knowledge base is pro-vided. Recent work, PIVOINE (Lu et al., 2023),focuses on improving the language models gener-ality to various (or unseen) instructions for openinformation extraction, whereas our focus is on de-signing a new model architecture for improving theeffectiveness and efficiency of language modelsinformation extraction in a strict setting. 3Structured Entity ExtractionIn this section, we first describe the structured en-tity extraction formulation, followed by detailingthe Approximate Entity Set OverlaP (AESOP) met-ric for evaluation. We would like to emphasize thatstructured entity extraction is not an entirely newtask, but rather a novel entity-centric formulationof information extraction. 3.1Task FormulationGiven a document d, the goal of structured entityextraction is to generate a set of structured entitiesE = {e1, e2, . . . , en} that are mentioned in the doc-ument text. Each structured entity e is a dictionaryof property keys p P and property values v V,and let ve,p be the value of property p of entitye. In this work we consider only text propertiesand hence V is the set of all possible text propertyvalues. If a property of an entity is common knowl-edge but does not appear in the input document, itwill not be considered in the structured entity ex-traction. Depending on the particular situation, theproperty values could be other entities, although",
  "this is not always the case": "So, the goal then becomes to learn a functionf : d E = {e1, e2, . . . , em}, and we expectthe predicted set E to be as close as possible tothe target set E, where the closeness is measuredby some similarity metric (E, E). Note that thepredicted set of entities E and the ground-truth setE may differ in their cardinality, and our definitionof should allow for the case when |E| = |E|.Finally, both E and E are unordered sets and hencewe also want to define to be order-invariant overE and E. As we do not need to constrain f to pro-duce the entities in any strict order, it is reasonablefor to assume the most optimistic assignment ofE with respect to E. We denote E and E as somearbitrary but fixed ordering over items in predictionset E and ground-truth set E for allowing indexing. 3.2Approximate Entity Set OverlaP (AESOP)MetricWe propose a formal definition of the ApproximateEntity Set OverlaP (AESOP) metric, which focuseson the entity-level and more flexible to includedifferent level of normalization:",
  "i,jFi,j ent( Ei, Ej),(1)": "which is composed of two phases: (i) optimal en-tity assignment for obtaining the assignment matrixF to let us know which entity in E is matched withwhich one in E, and (ii) pairwise entity compar-ison through ent( Ei, Ej), which is a similaritymeasure defined between any two arbitrary enti-ties e and e. We demonstrate the details of thesetwo phases in this section. We implement asa linear sum over individual pairwise entitycomparisons ent, and is the maximum of thesizes of the target set and the predicted set, i.e., = max{m, n}.Phase 1: Optimal Entity Assignment.The op-timal entity assignment is directly derived from amatrix F Rmn, which is obtained by solvingan assignment problem between E and E. Here,the matrix F is a binary matrix where each elementFi,j is 1 if the entity Ei is matched with the entityEj, and 0 otherwise. Before formulating the assign-ment problem, we first define a similarity matrixS Rmn where each element Si,j quantifies thesimilarity between the i-th entity in E and the j-thentity in E for the assignment phase. For practical",
  "implementation, we ensure inclusion of the unionset of property keys from both the i-th entity in E": "and the j-th entity in E for each of these entities.When a property key is absent, its correspondingproperty value is set to be an empty string. Thesimilarity is then computed as a weighted averageof the Jaccard index (Murphy, 1996) for the list oftokens of the property values associated the sameproperty key in both entities. The Jaccard indexinvolved empty strings is defined as zero in ourcase. We assign a weight of 0.9 to the entity name,while all other properties collectively receive a totalweight of 0.1. This ensures that the entity nameholds the highest importance for matching, whilestill acknowledging the contributions of other prop-erties. It is worthy to notice that the weights values0.9 and 0.1 are not universal standards. One cantailor the choices of these weights values for spe-cific requirements. Then the optimal assignmentmatrix F is found by maximizing the followingequation:",
  "j=1Fi,j Si,j,(2)": "subject to the following four constraints to ensureone-to-one assignment between entities in the pre-diction set and the ground truth set: (i) Fi,j {0, 1};(ii) mi=1 Fi,j 1, j {1, 2, . . . , n}; (iii) nj=1 Fi,j 1, i {1, 2, . . . , m}; (iv) mi=1nj=1 Fi,j = min{m, n}.One can take an analogy of maximizing equation. 2to the optimal flow in the Earth Movers Distance(EMD). In EMD, the optimal flow is the one thatminimizes the entire cost of moving the dirt,while in our case, the optimal assignment is theone that maximizes the entire \"similarity\" in thebest possible way.Phase 2: Pairwise Entity Comparison.Afterobtaining the optimal entity assignment, we fo-cus on the pairwise entity comparison. We defineent( Ei, Ej) as a similarity metric between anytwo arbitrary entities e and e from E and E.",
  "properties. We define the score as zero for missingproperties": "It should be noted that while both S and ent areused to calculate similarities between pairs of enti-ties, they are not identical. During the entity assign-ment phase, it is more important to make sure theentity names are aligned, while it is more accept-able to treat all properties equally without differen-tiation during the pairwise entity comparison. Theseparation in the definitions of two similarity mea-sures allows us to tailor our metric more preciselyto the specific requirements of each phase of theprocess. The definition of similarity and differentvariants for our proposed AESOP metric are elabo-rated in Appendix A. We discuss the relationshipbetween traditional metrics, such as precision andrecall, and AESOP in Appendix B.",
  "Multi-stage Structured EntityExtraction using LanguageModels": "In this section, we elaborate on the methodologyfor structured entity extraction using LMs. Weintroduce a novel model architecture leveragingLMs, MuSEE, for Multi-stage Structured EntityExtaction. MuSEE is built on an encoder-decoderarchitecture, whose pipeline incorporates two piv-otal enhancements to improve effectiveness andefficiency: (i) reducing output tokens through intro-ducing additional special tokens where each can beused to replace multiple tokens, and (ii) multi-stageparallel generation for making the model focuson a sub-task at each stage where all predictionswithin a stage can be processed parallelly. Reducing output tokens.Our model condensesthe output by translating entity types and propertykeys into unique, predefined tokens. Specifically,for the entity type, we add prefix ent_type_,while for each property key, we add prefix pk_.By doing so, the type and each property key onan entity is represented by a single token, whichsignificantly reduces the number of output tokensduring generation thus improving efficiency. Forinstance, if the original entity type is artificialobject which is decomposed into 4 tokens (i.e.,_art, if, ical, _object) using the T5 to-kenizer, now we only need one special token,ent_type_artifical_object, to represent the entiresequence. All of these special tokens can be de-rived through the knowledge of some predefinedschema before the model training. : The pipeline of our proposed MuSEE model, which is built on an encoder-decoder architecture. The inputtext only needs to be encoded once. The decoder is shared for all the three stages. All predictions within each stagecan be processed in batch, and teacher forcing enables parallelization even across stages during training. Multi-stage parallel generation.In addition toreducing the number of generated tokens, MuSEEfurther decomposes the generation process intothree stages: (i) identifying all entities, (ii) deter-mining entity types and property keys, and (iii)predicting property values. To demonstrate thispipeline more clearly, we use the same text shownin as an example to show the process ofstructured entity extraction as follows:",
  "[[[Text Description]]] MuSEE pred_val{Microsoft}{ent_type_corporation}{pk_headquarter} Redmond EOS": "Among the three stages depicted, pred_ent_names,pred_type_and_property, and pred_val are specialtokens to indicate the task. For each model pre-diction behavior, the first indicates inputtingthe text into the encoder of MuSEE, while the sec- ond means inputting the encoded outputs intothe decoder. All tokens in blue are the prompttokens input into the decoder which do not needto be predicted, while all tokens in bold are themodel predictions. For the stage 1, we emphasizethat MuSEE outputs a unique identifier for eachentity in the given text. Taking the example in, the first stage outputs Bill Gates only,rather than both Bill Gates and Gates. Thisrequires the model implicitly learn how to do coref-erence resolution, namely learning that Bill Gatesand Gates are referring to the same entity. There-fore, our approach uses neither surface forms, asthe outputs of the first stage are unique identifiers,nor the entity titles followed by entity linkings. Forstage 2, the MuSEE model predicts the entity typesand property keys, which are all represented byspecial tokens. Hence, the prediction can be madeby sampling the token with highest probability overthe special tokens for entity types and property keysonly, rather than all tokens. Notice that we do notneed to predict the value for type and name instage 3, since the type can be directly derived fromthe ent_type_ special key itself, and the name isobtained during stage 1. The tokens in the bracket{..} are also part of the prompt tokens and areobtained in different ways during training and in-ference. During training, these inputs are obtainedfrom the ground truth due to the teacher forcingtechnique (Raffel et al., 2023). During inference,they are obtained from the output predictions fromthe previous stages. The full training loss is a sumof three cross-entropy losses, one for each stage.An illustration of our models pipeline is shown in. More implementation details are elaboratedin Appendix C.Benefits for Training and Inference.MuSEEsunique design benefits both training and inference.In particular, each stage in MuSEE is finely tuned to concentrate on a specific facet of the extraction pro-cess, thereby enhancing the overall effectiveness.Most importantly, all predictions within the samestage can be processed in batch thus largely improv-ing efficiency. The adoption of a teacher forcingstrategy enables parallel training even across dif-ferent stages, further enhancing training efficiency.During inference, the models approach to breakingdown long sequences into shorter segments signifi-cantly reduces the generation time. It is also worthyto mention that each text in the above three stagesneeds to be encoded only once by the MuSEEsencoder, where the encoded output is repeatedlyutilized across different stages. This streamlinedapproach ensures a concise and clear delineationof entity information, facilitating the transforma-tion of unstructured text into a manageable andstructured format.",
  "In this section, we describe the datasets used in ourexperiment, followed by the discussion of baselinemethods and training details": "5.1DataInadaptingthestructuredentityextrac-tion, we repurpose the NYT (Riedel et al.,2010), CoNLL04 (Roth and Yih, 2004), andREBEL (Huguet Cabot and Navigli, 2021)datasets, which are originally developed forrelation extractions.For NYT and CoNLL04,since each entity in these two datasets has apredefined type, we simply reformat them to ourentity-centric formulation by treating the subjectsas entities, relations as property keys, and objectsas property values.REBEL connects entitiesidentified in Wikipedia abstracts as hyperlinks,along with dates and values, to entities in Wikidataand extracts the relations among them. For entitieswithout types in the REBEL dataset, we categorizetheir types as unknown.Additionally, weintroduce a new dataset, named Wikidata-based.The Wikidata-based dataset is crafted using anapproach similar to REBEL but with two primarydistinctions: (i) property values are not necessarilyentities; (ii) we simplify the entity types byconsolidating them into broader categories basedon the Wikidata taxonomy graph, resulting in lessspecific types. The processes for developing theWikidata-based dataset is detailed in Appendix D.The predefined schemas for NYT, CoNLL04, andREBEL are using all entity types and property keys",
  "from these datasets. The details of the predefinedschema for Wikidata-based dataset are provided inAppendix D. Comprehensive statistics for all fourdatasets are available in Appendix E": "5.2BaselineWe benchmark our methodology against two dis-tinct classes of baseline approaches. The first cat-egory considers adaptations from general seq2seqtask models: (i) LM-JSON: this approach involvesfine-tuning pre-trained language models. The inputis a textual description, and the output is the stringformat JSON containing all entities. The secondcategory includes techniques designed for differ-ent information extraction tasks, which we adaptto address our challenge: (ii) GEN2OIE (Kolluruet al., 2022), which employs a two-stage genera-tive model initially outputs relations for each sen-tence, followed by all extractions in the subsequentstage; (iii) IMoJIE (Kolluru et al., 2020b), an ex-tension of CopyAttention (Cui et al., 2018), whichsequentially generates new extractions based onpreviously extracted tuples; (iv) GenIE (Josifoskiet al., 2022), an end-to-end autoregressive genera-tive model using a bi-level constrained generationstrategy to produce triplets that align with a pre-defined schema for relations. GenIE is crafted forthe closed information extraction, so it includesa entity linking step. However, in our strict set-ting, there is only a schema of entity types andrelations. Therefore, we repurpose GenIE for oursetting by maintaining the constrained generationstrategy and omitting the entity linking step. Weomit to compare our method with non-generativemodels primarily due to the task differences. 5.3TrainingWe follow existing studies (Huguet Cabot and Nav-igli, 2021) to use the encoder-decoder architecturein our experiment. We choose the T5 (Raffel et al.,2023) series of LMs and employ the pre-trainedT5-Base (T5-B) and T5-Large (T5-L) as the basemodels underlying every method discussed in sec-tion 5.2 and our proposed MuSEE. LM-JSON andMuSEE are trained with the Low-Rank Adapta-tion (Hu et al., 2021), where r = 16 and = 32.For GEN2OIE, IMoJIE, and GenIE, we follow alltraining details of their original implementation.For all methods, we employ a linear warm up andthe Adam optimizer (Kingma and Ba, 2017), tun-ing the learning rates between 3e-4 and 1e-4, andweight decays between 1e-2 and 0. All experimentsare run on a NVIDIA A100 GPU. It is worthy to mention that MuSEE can also buildupon the decoder-only architecture by managingthe KV cache and modifications to the positionencodings (Xiao et al., 2024), though this requiresadditional management and is not the main focusof this study.6ResultsIn this section, we show the results for both quanti-tative and human side-by-side evaluation.6.1Quantitative Evaluation Effectiveness comparison.The overall effective-ness comparison is shown in . We report tra-ditional metrics, including precision, recall, and F1score, in addition to our proposed AESOP metric.From the results, the MuSEE model consistentlyoutperforms other baselines in terms of AESOPacross all datasets. For instance, MuSEE achievesthe highest AESOP scores on REBEL with 55.24(T5-B) and 57.39 (T5-L), on NYT with 81.33 (T5-B) and 82.67 (T5-L), on CoNLL04 with 78.38 (T5-B) and 79.87 (T5-L), and on the Wikidata-baseddataset with 46.95 (T5-B) and 50.94 (T5-L). Thesescores significantly surpass those of the competingmodels, indicating MuSEEs stronger entity extrac-tion capability. The other three traditional met-rics further underscore the efficacy of the MuSEEmodel. For instance, on CoNLL04, MuSEE (T5-B)achieves a precision of 73.18, a recall of 60.28, anda F1 score of 66.01, which surpass all the otherbaselines. Similar improvements are observed onREBEL, NYT, and Wikidata-based dataset. Nev-ertheless, while MuSEE consistently excels in theAESOP metric, it does not invariably surpass thebaselines across all the traditional metrics of preci-sion, recall, and F1 score. Specifically, within theREBEL dataset, GenIE (T5-B) achieves the highestprecision at 57.55, and LM-JSON (T5-B) recordsthe best recall at 51.29. Furthermore, on the NYTdataset, GenIE (T5-B) outperforms other modelsin F1 score. These variances highlight the uniqueinsights provided by our adaptive AESOP metric,which benefits from our entity-centric formulation.We expand on this discussion in section 6.2. As discussed in Sec. 4, our MuSEE model is cen-tered around two main enhancements: reducingoutput tokens and multi-stage parallel generation.By simplifying output sequences, MuSEE tacklesthe challenge of managing long sequences that of-ten hinder baseline models, like LM-JSON, GenIE,IMoJIE, thus reducing errors associated with se-quence length. Additionally, by breaking down the extraction process into three focused stages,MuSEE efficiently processes each aspect of entityextraction, leveraging contextual clues for moreaccurate predictions. In contrast, GEN2OIEs two-stage approach, though similar, falls short becauseit extracts relations first and then attempts to pairentities with these relations. However, a single re-lation may exist among different pairs of entities,which can lead to low performance with this ap-proach. Supplemental ablation study is provided inAppendix F.Efficiency comparison.As shown in the last col-umn of , we provide a comparison on the in-ference efficiency, measured in the number of sam-ples the model can process per second. The MuSEEmodel outperforms all baseline models in terms ofefficiency, processing 52.93 samples per secondwith T5-B and 33.96 samples per second with T5-L. It shows a 10x speed up compared to IMoJIE,and a 5x speed up compared to the strongest base-line GenIE. This high efficiency can be attributed toMuSEEs architecture, specifically its multi-stageparallel generation feature. By breaking down thetask into parallelizable stages, MuSEE minimizescomputational overhead, allowing for faster pro-cessing of each sample. The benefit of this designcan also be approved by the observation that theother multi-stage model, GEN2OIE, shows the sec-ond highest efficiency. To better illustrate our models strength, we showthe scatter plots comparing all models with variousbackbones in on the effectiveness and effi-ciency. We choose the Wikidata-based dataset andthe effectiveness is measured by AESOP. As de-picted, our model outperforms all baselines with alarge margin. This advantage makes MuSEE partic-ularly suitable for applications requiring rapid pro-cessing of large volumes of data, such as processingweb-scale datasets, or integrating into interactivesystems where response time is critical.Grounding check.As the family of T5 modelsare pre-trained on Wikipedia corpus (Raffel et al.,2023), we are curious whether the models are ex-tracting information from the given texts, or theyare leveraging their prior knowledge to generateinformation that cannot be grounded to the givendescription. We use T5-L as the backbone in thisexperiment. We develop a simple approach to con-duct this grounding check by perturbing the orig-inal test dataset with the following strategy. Wefirst systematically extract and categorize all enti- : Summary of results of different models. Each metric is shown in percentage (%). The last column showsthe inference efficiency, measured by the number of samples the model can process per second. The best is bolded,and the second best is underlined. Our model has a statistical significance for p 0.01 compared to the bestbaseline (labelled with *) based on the paired t-test.",
  ":Grounding check across models on theWikidata-based dataset. MuSEE shows the least perfor-mance drop on the perturbed version of data comparedto other baselines": "ties and their respective properties, based on theirentity types. Then, we generate a perturbed versionof the dataset, by randomly modifying entity prop-erties based on the categorization we built. We in-troduce controlled perturbations into the dataset byselecting alternative property values from the samecategory but different entities, and subsequentlyreplacing the original values in the texts. The ex-periment results from our grounding study on theWikidata-based dataset, as illustrated in , re-veal findings regarding the performance of variousmodels under the AESOP and F1 score. Our model,MuSEE, shows the smallest performance gap be-tween the perturbed data and the original data com-pared to its counterparts, suggesting its stronger",
  "MuSEE prefer61.7559.3257.1361.2845.3337.2440.57": ": Percentage of samples preferred by humansand metrics on MuSEEs results when compared withGenIEs. The first three columns are for human evalua-tion. The next four columns are for quantitative metrics.capability to understand and extract structured in-formation from given texts. 6.2Human EvaluationTo further analyze our approach, we randomly se-lect 400 test passages from the Wikidata-baseddataset, and generate outputs of our model MuSEEand the strongest baseline GenIE. Human evalua-tors are presented with a passage and two randomlyflipped extracted sets of entities with properties.Evaluators are then prompted to choose the outputthey prefer or express no preference based on threecriteria, Completeness, Correctness, and Halluci-nations (details shown in Appendix G). Among all400 passages, the output of MuSEE is preferred61.75% on the completeness, 59.32% on the cor-rectness, and 57.13% on the hallucinations. For acomplete comparison, we also report the percent-age of samples preferred by quantitative metrics onMuSEEs results when compared with GenIEs, assummarized in . As shown, our proposedAESOP metric aligns more closely with humanjudgment than traditional metrics. These observa-tions provide additional confirm to the quantitativeresults evaluated using the AESOP metric that ourmodel significantly outperforms existing baselinesand illustrates the inadequacy of traditional metricsdue to their oversimplified assessment of extrac-tion quality. Case study of the human evaluation isshown in Appendix G.7Discussion and ConclusionWe introduce Structured Entity Extraction (SEE),an entity-centric formulation of information ex-traction in a strict setting. We then propose theApproximate Entity Set OverlaP (AESOP) Met- ric, which focuses on the entity-level and moreflexible to include different level of normalization.Based upon, we propose a novel model architec-ture, MuSEE, that enhances both effectiveness andefficiency. Both quantitative evaluation and humanside-by-side evaluation confirm that our model out-performs baselines. An additional advantage of our formulation is itspotential to address coreference resolution chal-lenges, particularly in scenarios where multipleentities share the same name or lack primary iden-tifiers such as names. Models trained with priortriplet-centric formulation cannot solve the abovechallenges. However, due to a scarcity of relevantdata, we were unable to assess this aspect in ourcurrent study. 8LimitationsThe limitation of our work lies in the assumptionthat each property possesses a single value. How-ever, there are instances where a propertys valuemight consist of a set, such as varying names.Adapting our method to accommodate these scenar-ios presents a promising research direction.",
  "Acknowledgement": "We would like to thank all reviewers for their pro-fessional review work, constructive comments, andvaluable suggestions on our manuscript. This workis supported by the the MSR-Mila Research Grant.We thank Compute Canada for the computing re-sources. Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni. 2007. Openinformation extraction from the web. In Proceedingsof the 20th International Joint Conference on ArtificalIntelligence, IJCAI07, page 26702676, San Francisco,CA, USA. Morgan Kaufmann Publishers Inc.",
  "Italo L Oliveira, Renato Fileto, Ren Speck, Lus PFGarcia, Diego Moussallem, and Jens Lehmann. 2021.Towards holistic entity linking: Survey and directions.Information Systems, 95:101624": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou, WeiLi, and Peter J. Liu. 2023.Exploring the limits oftransfer learning with a unified text-to-text transformer. Sebastian Riedel, Limin Yao, and Andrew McCallum.2010. Modeling relations and their mentions withoutlabeled text. In Machine Learning and Knowledge Dis-covery in Databases, pages 148163, Berlin, Heidel-berg. Springer Berlin Heidelberg. Dan Roth and Wen-tau Yih. 2004. A linear program-ming formulation for global inference in natural lan-guage tasks. In Proceedings of the Eighth Conferenceon Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 18, Boston, Mas-sachusetts, USA. Association for Computational Lin-guistics.",
  "Nikolaos Stylianou and Ioannis Vlahavas. 2021. A neu-ral entity coreference resolution review. Expert Systemswith Applications, 168:114466": "Bayu Distiawan Trisedya, Gerhard Weikum, JianzhongQi, and Rui Zhang. 2019. Neural relation extraction forknowledge base enrichment. In Proceedings of the 57thAnnual Meeting of the Association for ComputationalLinguistics, pages 229240, Florence, Italy. Associationfor Computational Linguistics. Michael Vasilkovsky, Anton Alekseev, Valentin Ma-lykh, Ilya Shenbin, Elena Tutubalina, Dmitriy Sa-likhov, Mikhail Stepnov, Andrey Chertok, and SergeyNikolenko. 2022. Detie: Multilingual open informationextraction inspired by object detection. Yanshan Wang, Liwei Wang, Majid Rastegar-Mojarad,Sungrim Moon, Feichen Shen, Naveed Afzal, Sijia Liu,Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn, et al.2018. Clinical information extraction applications: aliterature review. Journal of biomedical informatics,77:3449. Gerhard Weikum and Martin Theobald. 2010. Frominformation to knowledge: harvesting entities and re-lationships from web sources. In Proceedings of thetwenty-ninth ACM SIGMOD-SIGACT-SIGART sympo-sium on Principles of database systems, pages 6576.",
  "Yang Yang, Zhilei Wu, Yuexiang Yang, ShuangshuangLian, Fengjie Guo, and Zhiwei Wang. 2022. A survey ofinformation extraction based on deep learning. AppliedSciences, 12(19):9691": "Alexander Yates, Michele Banko, Matthew Broadhead,Michael Cafarella, Oren Etzioni, and Stephen Soderland.2007. TextRunner: Open information extraction on theweb. In Proceedings of Human Language Technolo-gies: The Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL-HLT), pages 2526, Rochester, New York,USA. Association for Computational Linguistics. Hongbin Ye, Ningyu Zhang, Hui Chen, and HuajunChen. 2022. Generative knowledge graph construction:A review. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 117.",
  "AVariants of AESOP": "The AESOP metric detailed in section 3.2 matches entities by considering all properties and normalizeswith the maximum of the sizes of the target set and the predicted set. We denote it as AESOP-MultiProp-Max. In this section, we elaborate more variants of the AESOP metric in addition to section 3.2,categorized based on two criteria: the definition of entity similarity used for entity assignment and thenormalization approach when computing the final metric value between E and E. These variants allow forflexibility and adaptability to different scenarios and requirements in structured entity extraction.",
  "AESOP-Max: The denominator is the maximum of the sizes of the target set and the predicted set, i.e., = max{m, n}": "Given these choices, we can obtain 3 3 = 9 variants of the AESOP metric. To avoid excessivecomplexity, we regard the AESOP-MultiProp-Max as default. For clarity, we illustrate the two phases ofcomputing the AESOP metric and its variants in . We also show that precision and recall are specificinstances of the AESOP metric in Appendix B. : An illustration of the AESOP metric, including optimal entity assignment (phase 1) and pairwise entitycomparison (phase 2), and overall metric computation with various similarity and normalization choices.",
  "# of triplets in the target.(5)": "In the framework of the AESOP metric, precision and recall are effectively equivalent to treating eachtriplet as an entity, where the subject as the entity name, and the relation and object form a pair of propertykey and value. For optimal entity assignment (phase 1), precision and recall use the AESOP-MultiPropvariant but match entities based on the similarity of all their properties with a same weight. For pairwiseentity comparison (phase 2), the ent(e, e) (Eq. 3), can be defined as 1 if v = v, otherwise 0, where v and v are the only property values in e and e, respectively. For Eq. 1, aggregation can be defined as alinear sum, which principally results in how many triplets are correctly predicted in this case. If in Eq. 1is set as the number of triplets in the prediction, this corresponds to the calculation of precision. Similarly,when equals the number of triplets in the target, it corresponds to the calculation of recall.",
  "CImplementation Details of MuSEE": "In order to implement the approach of our MuSEE model, one may extend existing models with encoder-decoder architecture by integrating additional modules and processing steps specifically designed forentity and property prediction tasks. Specifically, given a predefined schema, we first add all necessaryspecial tokens to customize the tokenizer as detailed before. The implementation of the generation processinvolves three main stages: entity prediction, property key prediction, and property value prediction. 1. Entity Prediction: We first encode the input sequence using the encoder to obtain the hidden statesfor the entire sequence. We generate a prompt pred_ent_names and transform it to token ids usingthe tokenizer. This prompt, repeated for each sample in the batch, is concatenated with the encodedinput sequence and processed through the decoder to produce entity name predictions as a sequenceof tokens. 2. Property Key Prediction: For each predicted entity name, we generate prompts in the formatpred_type_and_property [entity_name]. These prompts are tokenized, padded to a fixed length, andconcatenated with the encoded input sequence. The concatenated sequences are then passed throughthe decoder to predict entity types and property keys as a sequence of special tokens for entity typesand property keys. We achieve this by sampling the token with highest probability over all specialtokens for entity types and property keys, rather than training a separate classifier head. 3. Property Value Prediction: For each predicted entity and its corresponding property keys, we createprompts in the format pred_val [entity_name] [entity_type] [property_key]. These prompts aretokenized, padded, and concatenated with the encoded input sequence. The concatenated sequencesare processed by the decoder to generate property value predictions.",
  "DDetails of Wikidata-based Dataset": "We build a new Wikidata-based dataset. This dataset is inspired by methodologies employed in previousworks such as Wiki-NRE (Trisedya et al., 2019), T-REx (Elsahar et al., 2018), REBEL (Huguet Cabotand Navigli, 2021), leveraging extensive information available on Wikipedia and Wikidata. The primaryobjective centers around establishing systematic alignments between textual content in Wikipedia articles,hyperlinks embedded within these articles, and their associated entities and properties as cataloged inWikidata. This procedure is divided into three steps: (i) Parsing Articles: We commence by parsingEnglish Wikipedia articles from the dump file1, focusing specifically on text descriptions and omittingdisambiguation and redirect pages. The text from each selected article is purified of Wiki markup toextract plain text, and hyperlinks within these articles are identified as associated entities. Subsequently,the text descriptions are truncated to the initial ten sentences, with entity selection confined to those",
  "The version of the Wikipedia and Wikidata dump files utilized in our study are 20230720, representing the most recentversion available during the development of our work": "referenced within this truncated text. This approach ensures a more concentrated and manageable dataset.(ii) Mapping Wikidata IDs to English Labels: Concurrently, we process the Wikidata dump1 file toestablish a mapping (termed as the id-label map) between Wikidata IDs and their corresponding Englishlabels. This mapping allows for efficient translation of Wikidata IDs to their English equivalents. (iii)Interconnecting Wikipedia articles with Wikidata properties: For each associated entity within the textdescriptions, we utilize Wikidatas API to ascertain its properties and retrieve their respective WikidataIDs. The previously established id-label map is then employed to convert these property IDs into Englishlabels. Each entitys type is determined using the value associated with instance of (P31). Given the highlyspecific nature of these entity types (e.g., small city (Q18466176), town (Q3957), big city (Q1549591)),we implement a recursive merging process to generalize these types into broader categories, referencingthe subclass of (P279) property. Specifically, we first construct a hierarchical taxonomy graph. Each nodewithin this graph structure represents an entity type, annotated with a count reflecting the total numberof entities it encompasses. Second, a priority queue are utilized, where nodes are sorted in descendingorder based on their entity count. We determine whether the top n nodes represent an ideal set of entitytypes, ensuring the resulted entity types are not extremely specific. Two key metrics are considered forthis evaluation: the percentage of total entities encompassed by the top n nodes, and the skewness of thedistribution of each entity types counts within the top n nodes. If the distribution is skew, we then executea procedure of dequeuing the top node and enqueueing its child nodes back into the priority queue. Thisiterative process allows for a dynamic exploration of the taxonomy, ensuring that the most representativenodes are always at the forefront. Finally, our Wikidata-based dataset is refined to contain the top-10 (i.e.,n = 10) most prevalent entity types according to our hierarchical taxonomy graph and top-10 propertykeys in terms of occurrence frequency, excluding entity name and type. The 10 entity types are talk,system, spatio-temporal entity, product, natural object, human, geographical feature, corporate body,concrete object, and artificial object. The 10 property keys are capital, family name, place of death, part of,location, country, given name, languages spoken, written or signed, occupation, and named after.",
  "EStatistics of Datasets": "NYT is under the CC-BY-SA license.CoNLL04 is under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. REBEL is under the Creative Commons At-tribution 4.0 International License. The dataset statistics presented in compare NYT, CoNLL04,REBEL, and Wikidata-based datasets. All datasets feature a minimum of one entity per sample, but theydiffer in their average and maximum number of entities, with the Wikidata-based dataset showing a highermean of 3.84 entities. They also differ in the maximum number of entities, where REBEL has a max of65. Property counts also vary, with REBEL having a slightly higher average number of properties perentity at 3.40.",
  "w/o Multi-stage (T5-L)27.7453.0428.8128.1454.1029.2229.1456.9030.29MuSEE (T5-L)49.3557.9759.6349.8958.6960.3550.9460.1161.68": "generation. By comparing the performance of the full MuSEE model against its ablated version, whereonly the special tokens feature is retained, we aim to dissect the individual contributions of these designchoices to the models overall efficacy. The ablated version simplifies the output format by eliminatingpunctuation such as commas, double quotes, and curly brackets, and by converting all entity types andproperty keys into special tokens. This is similar to the reducing output tokens discussed in Sec. 4. Resultsfrom the ablation study, as shown in , reveal significant performance disparities between thecomplete MuSEE model and its ablated counterpart, particularly when examining metrics across differentmodel sizes (T5-B and T5-L) and evaluation metrics. The full MuSEE model markedly outperformsthe ablated version across all metrics with notable improvements, underscoring the Multi-stage parallelgenerations critical role in enhancing the models ability to accurately and comprehensively extractentity-related information. These findings highlight the synergistic effect of the MuSEE models designelements, demonstrating that both the Reducing output tokens and the Multi-stage parallel generation arepivotal for achieving optimal performance in structured entity extraction tasks.",
  "The details for the three human evaluation criteria are shown below:": "Completeness: Which set of entities includes all relevant entities and has the fewest missing importantentities? Which set of entities is more useful for further analysis or processing? Focus on the set thatcontains less unimportant and/or irrelevant entities. Correctness: Which set of entities more correctly represents the information in the passage? Focuson consistency with the context of the passage. Do extracted properties correctly represent eachentity or are there more specific property values available? Are property values useful?",
  "We provide a case study for the human evaluation analysis comparing the outputs of GenIE (T5-L) and": "MuSEE (T5-L) given a specific text description. MuSEE accurately identifies seven entities, surpassingGenIEs two, thus demonstrating greater completeness. Additionally, we identify an error in GenIEsoutput where it incorrectly assigns Bartolomeo Rastrellis place of death as Moscow, in contrast to theactual location, Saint Petersburg, which is not referenced in the text. This error by GenIE could stemfrom hallucination, an issue not present in MuSEEs output. In this example, it is evident that MuSEEoutperforms GenIE in terms of completeness, correctness, and resistance to hallucinations. Text Description: The ceremonial attire of Elizabeth, Catherine Palace, Tsarskoye Selo; fot. IvonnaNowicka Elizabeth or Elizaveta Petrovna (; ) reigned as Empress of Russia from 1741 until herdeath in 1762. She remains one of the most popular Russian monarchs because of her decisionnot to execute a single person during her reign, her numerous construction projects, and her strongopposition to Prussian policies. The second-eldest daughter of Tsar Peter the Great (), Elizabeth livedthrough the confused successions of her fathers descendants following her half-brother Alexeisdeath in 1718. The throne first passed to her mother Catherine I of Russia (), then to her nephewPeter II, who died in 1730 and was succeeded by Elizabeths first cousin Anna (). After the brief ruleof Annas infant great-nephew, Ivan VI, Elizabeth seized the throne with the militarys support anddeclared her own nephew, the future Peter III, her heir. During her reign Elizabeth continued thepolicies of her father and brought about a remarkable Age of Enlightenment in Russia. Her domesticpolicies allowed the nobles to gain dominance in local government while shortening their terms ofservice to the state. She encouraged Mikhail Lomonosovs foundation of the University of Moscow,the highest-ranking Russian educational institution. Her court became one of the most splendidin all Europe, especially regarding architecture: she modernised Russias roads, encouraged IvanShuvalovs foundation of the Imperial Academy of Arts, and financed grandiose Baroque projects ofher favourite architect, Bartolomeo Rastrelli, particularly in Peterhof Palace.",
  "HMetric Correlation Analysis": "We show the correlation analysis between AESOP metric variants across all models on all four datasets,shown in , , , and , respectively. Specifically, we focus on the correlation analysisof different variants based on entity assignment variants in Phase 1 of AESOP, as described in Sec. 3.For Phase 2, the Max normalization method is employed by default. Observations for the other twonormalization variants are similar. In the associated figures, AESOP-MultiProp-Max is uniformly used asthe x-axis measure, while AESOP-ExactName-Max or AESOP-ApproxName-Max serve as the y-axismetrics. The scatter plots in all figures tend to cluster near the diagonal, indicating a robust correlationamong the various metric variants we have introduced. AESOP-MultiProp-Max AESOP-ExactName-Max LLM-JSON (T5B)GEN2OIE (T5B)IMoJIE (T5B)GenIE (T5B)MuSEE (T5B)LLM-JSON (T5L)GEN2OIE (T5L)IMoJIE (T5L)GenIE (T5L)MuSEE (T5L) AESOP-MultiProp-Max AESOP-ApproxName-Max LLM-JSON (T5B)GEN2OIE (T5B)IMoJIE (T5B)GenIE (T5B)MuSEE (T5B)LLM-JSON (T5L)GEN2OIE (T5L)IMoJIE (T5L)GenIE (T5L)MuSEE (T5L) REBEL Dataset"
}