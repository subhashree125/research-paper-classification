{
  "Abstract": "Large language models (LLMs) can performcomplex reasoning by generating intermedi-ate thoughts under zero-shot or few-shot set-tings. However, zero-shot prompting alwaysencounters low performance, and the supe-rior performance of few-shot prompting hingeson the manual-crafted demonstrations.Inthis paper, we present RoSE (Reasoning withOrchestrated Streaming Experiences), a gen-eral framework for solving reasoning tasks thatcan self-improve without complex external ef-forts. To enable RoSE, we describe an architec-ture that extends an LLM to store all answeredquestions and their thoughts in a streaming ex-perience pool then orchestrates helpful ques-tions from the pool to assist in answering newquestions. To set up a question-aware orches-tration mechanism, RoSE first calculates thesimilarity of each question in the pool with anew test question. Since the solution to eachanswered question is not always correct, RoSEwill sort the questions according to their sim-ilarity with the new question, and then uni-formly divide them into multiple buckets. Itfinally extracts one question from each bucketto make these extracted questions more diverse.To make these extracted questions help RoSEanswer new questions as much as possible, weintroduce two other attributes of uncertaintyand complexity for each question. RoSE willpreferentially select the questions with low un-certainty and high complexity from each bucket.We evaluate the versatility of RoSE in variousreasoning tasks, LLMs, and CoT methods.",
  "* Corresponding author": "Recently, the chain-of-thought (CoT) promptingtechnique (Wei et al., 2022) was proposed to haveLLMs generate intermediate reasoning paths be-fore generating the final answers. The promptingmakes LLMs think deeply before giving an answerand further enhances the reasoning power of LLMs.Besides, the zero-shot CoT prompt (Kojima et al.,2022) \"Lets think step by step\" also enhances thereasoning power of LLMs without any manual-crafting demonstrations. After the CoT promptingwas proposed, more studies tried to manually de-sign better prompts (Zhou et al., 2023; Wang et al.,2023a; Yao et al., 2023a) to further improve theperformance of LLMs in reasoning. However, nomatter how the prompts change, the goal is to haveLLMs generate intermediate reasoning steps.Recent works such as ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), REMEM-BERER (Zhang et al., 2023a), and ExpeL (Zhaoet al., 2023) were presented and have demonstratedthe feasibility of autonomous agents that are builton top of an LLM core. These methods use LLMsto generate reasoning paths and actions. These\"actions\" can be used in API calls and executed inan environment. Besides, some golden feedbackwill be presented to LLMs during the reasoningprocess (Shinn et al., 2023; Zhang et al., 2023a)or labeled samples are needed to collect correct orfalse experiences (Zhao et al., 2023). Overall, thesemethods still require humans to carefully designsome demonstrations and need golden feedback,labeled samples, or external tools to improve thereasoning performance of LLMs.We investigate how to improve the reasoning per-formance of LLMs in a more challenging streamingsetting without any labeled data, pre-set unlabeleddata, feedback signals, and other external help. In-spired by the observation that humans constantlydo various exercises to construct a large experi-ence pool in their minds and use the pool to helpthem quickly and better answer questions in ex- ams, we present RoSE, a general framework forsolving reasoning tasks with only streaming ex-periences. The greatest characteristic of RoSE isthat it can self-improve by constantly collectingand orchestrating streaming experiences like hu-mans. We build an experience pool for RoSE tostore the answered questions and correspondingreasoning paths. We expect these questions canassist LLMs in answering new questions, and con-struct a novel experience orchestration mechanismto extract helpful questions from the pool for eachnew reasoning question. To achieve this, we con-sider three attributes for each question in the poolwhen orchestrating. First, the solution to each ques-tion may be incorrect. If we randomly select someanswered questions as demonstrations, LLMs maydirectly copy the incorrect labels of these questionswhen they are similar to the questions to be an-swered. This phenomenon is also known as thecopy effect (Lyu et al., 2023; Zhang et al., 2023b).To avoid this, we introduce diversity so that theextracted questions are distributed from the highestto lowest similarity to the question to be answered.Second, before a question is appended to the pool,we calculate uncertainty for it according to theoutputs of LLMs. The lower the uncertainty, themore confident RoSE is about its prediction. Wefirst filter questions with higher uncertainty in thepool. However, since the pool is a dynamic system,we also set the dynamic uncertainty threshold toonly filter the questions with relatively higher un-certainty in a pool snapshot. Third, one intuitionis that the more complex the question, the moreit can help RoSE learn how to answer other ques-tions (Fu et al., 2023). Therefore, we introduce thecomplexity as the final attribute. After filtering thequestions with high uncertainty, we select the mostcomplex questions as the final demonstrations.We evaluate the versatility of RoSE on 9 rea-soning tasks, 2 LLMs, and different CoT methods.Experimental results show that RoSE significantlyimproves the reasoning performance of LLMs. Theanalysis experiments verify the importance of eachexperience orchestration process and the stabilityof RoSE across various experimental settings. Wesummarize our contribution as follows: We present RoSE, a general framework forbetter solving reasoning tasks. We build anovel experience orchestration mechanism byintroducing diversity, uncertainty, and com-plexity to extract more helpful questions to as-",
  "Chain-of-Thought Prompting": "Wei et al. (2022) formally presented the CoTprompting in large language models. This tech-nique elicits LLMs to generate a series of interme-diate reasoning steps that lead to the final answer toa question using some manual-crafting demonstra-tions with reasoning steps, so we name it Few-Shot-CoT. Kojima et al. (2022) presented that LLMs canalso perform CoT reasoning when prompted by a\"magic spell\" of \"Lets think step by step\" withoutany other manual-crafting demonstrations, so wename it Zero-Shot-CoT. We categorize promptingmethods as zero- and few-shot settings. Zero-shot SettingSome studies tried to first usezero-shot CoT prompting to obtain the reasoningchain for each unlabeled question and build a re-trieval mechanism to retrieve some helpful ques-tions to construct a few-shot prompt. For example,Auto-CoT (Zhang et al., 2023b) uses the k-meansclustering method to cluster all the test questionsexcept the current question to be answered, thentakes all the questions near each cluster center toconstruct a few-shot prompt using zero-shot CoTprompting. Plan-and-Solve prompting (Wang et al.,2023a) uses a different zero-shot CoT prompt toelicit LLMs to first decompose a question into sub-questions and then solve each sub-question. Few-shot SettingFew-shot CoT promptingachieves better performance by eliciting the CoTreasoning ability with effective manual demonstra-tions. However, designing suitable prompts forall test questions is difficult. Some recent stud-ies mainly focus on manual-crafting more well- designed prompts instead of addressing this limi-tation. Zhou et al. (2023) and Khot et al. (2023)presented similar CoT prompts to first decompose acomplex question into multiple sub-questions andthen solve them one by one. PoT (Chen et al.,2022) uses a CoT prompt to elicit LLMs to gen-erate text and programming language statementswhere the generated program can be executed bya program interpreter to get the final answer. Fuet al. (2023) presented a complexity-based few-shotCoT prompting method that uses more complexdemonstrations (i.e., with more reasoning steps)to obtain better performance than a random few-shot CoT prompt. Yao et al. (2023a) presenteda Tree-of-Thought (ToT) prompting method byconsidering multiple different reasoning paths andself-evaluating choices to decide the next courseof action. MoT (Li and Qiu, 2023) obtains thereasoning paths for each unlabeled question usingfew-shot CoT prompting and filters the questionswith low confidence. MemPrompt (Madaan et al.,2022) also uses few-shot prompting to query LLMsand gathers the interaction histories with user feed-back to concatenate with the original prompt. Be-sides, there are many retrieval-based in-contextlearning methods (Luo et al., 2024) that leverageexisting databases and retrieval systems. Unlikethese methods, RoSE puts more emphasis on theself-improvement of LLMs without any externaldata or feedback.",
  "Reasoning with Language Agents": "Some studies built agents to solve reasoning anddecision-making tasks. ReAct (Yao et al., 2023b)explores the use of LLMs to generate both reason-ing traces and task-specific actions in an interleavedmanner. Reflexion (Shinn et al., 2023) is an agentwith memory and self-reflection and can be usedto solve reasoning and decision-making tasks. Ex-peL (Zhao et al., 2023) is an agent that can learnfrom experiences and insights. However, it needslabeled data to construct experiences and insights.Compared with these agents, RoSE does not re-quire external environments or feedback.",
  "Methodology": "In this paper, we present RoSE, a framework forcollecting and orchestrating streaming experiencesto make LLMs self-improve in various reason-ing tasks. Our setting is zero-shot (i.e., withoutany manual-crafting demonstrations) and stream- ing (i.e., test questions arrive one by one and thereare no pre-set unlabeled questions). showsthe overview of the proposed framework. RoSEincorporates a streaming experience pool to storethe answered questions and their reasoning paths.RoSE will orchestrate the experiences using multi-ple attributes to extract helpful questions to assistitself in better answering new questions. We con-struct a novel experience orchestration mechanismfor RoSE that considers the diversity, uncertainty,and complexity of questions. In this section, we in-troduce how RoSE collects streaming experiencesand how it orchestrates the collected experiences.",
  "Streaming Experience Pool": "The streaming experience pool is a dynamic systemto store the answered questions and their reason-ing paths. After answering a new question, RoSEwill store it and its reasoning path in the streamingexperience pool. Each answered question has twoattached attributes of uncertainty and complexityaccording to the predictions of RoSE. The two at-tributes will be regarded as important measures tofilter collected experiences. UncertaintyThe uncertainty attribute indicateshow confident RoSE is in answering a question. Asshown in , the lower the uncertainty, themore confident RoSE answers the question. RoSEwill filter the questions in the experience pool withhigher uncertainty to guarantee the correctness ofextracted questions. To calculate uncertainty, wemake LLMs generate multiple reasoning paths foreach question. Each reasoning path has a corre-sponding predicted answer. Following Li and Qiu(2023), We calculate an entropy to estimate uncer-tainty according to all predicted answers A:",
  "i=1 p(ai ) log p(ai ),(3)": "where m is the number of reasoning paths andA = [a1, a2, ..., am] is the corresponding answersof each reasoning path for the test question qt.A = [a1, a2, ...] is the set of answers A. uqtrepresents the uncertainty of test question qt andthe higher uqt is, the more uncertain the LLM isabout the question.",
  "Q: Sandy grew 6 carrots. Sam grew 3 carrots.How many carrots did they grow in total?A: Lets think step by step. To find thetotal ... So, they grew a total of 9 carrots": "Q: Sam had 9 dimes in his bank. His dad gave him 7dimes. How many dimes does Sam have ?A: Lets think step by step. To find the total number ofdims, we add these two quantities together: 9 dims + 7dims = 16 dims. So, Sam has totally 16 dims now. Q: Sandy grew 6 carrots. Sam grew 3 carrots. How manycarrots did they grow in total?A: Lets think step by step. To find the total number ofcarrots they grew, we add: 6 + 3 = 9. So, they grew a totalof 9 carrots.",
  ": The relation between accuracy and the mag-nitude of uncertainty value on SVAMP dataset. Wenormalize the range of uncertainty to": "of the reasoning that can better teach LLMs howto reason. Therefore, we introduce the complex-ity attribute for each question as another importantmeasure when filtering experiences. A natural ideais to use the average complexity of the reasoningpaths to represent the complexity of a question.The higher the average path complexity, the morecomplex the question. For example, when a mathword problem is more complex, it may requiremore columns of equations, resulting in more com-plex reasoning paths. Therefore, we measure thecomplexity of a question q as follows:",
  "Experience Orchestration": "RoSE will orchestrate the collected experiences toassist itself in answering new questions. It first con-siders the diversity of experiences, and then filtersuseless questions using the attached attributes ofuncertainty and complexity sequentially. Finally,it constructs a CoT prompt using the orchestratedexperiences. DiversityRecent studies found that LLMs willdirectly copy the wrong labels from the ICL demon-strations (Lyu et al., 2023) or be misled by thewrong predictions in demonstrations (Zhang et al.,2023b) if the demonstrations in prompts are verysimilar to test questions. Therefore, some recentlyproposed methods (Zhang et al., 2023b; Li andQiu, 2023) consider diversity when constructingdemonstrations using unlabeled questions. Differ-ent from these methods that use k-means clustering,we propose a question-aware approach to maintain diversity. Specifically, given a test question qt andthe answered questions (q1, q2, ..., qj) in the expe-rience pool, we first obtain their embedding rep-resentations using an off-shelf semantic embedder.Then we calculate the semantic similarity betweenthe answered questions and the test question usingtheir embedding representations. The answeredquestions are sorted from low to high semantic sim-ilarity and uniformly partitioned into k buckets atthe dimension of similarity, where k is the num-ber of demonstrations. The process of partitioningis summarized in Algorithm 1. RoSE will selectone question in each bucket. This makes the se-lected questions distribute from low similarity tohigh similarity to the test question and guaranteesthe diversity of selected questions. We show thatthis can perform better than Auto-CoT which usesthe k-means clustering method in the latter section. Uncertainty-based FilteringAfter partitioningthe answered questions into k buckets, RoSE willfilter the answered questions with high uncertaintyin each bucket. The streaming experience pool isa dynamic system and the uncertainty distributionamong all buckets is different in different snapshots.Moreover, the uncertainty distribution is also differ-ent for different tasks. Therefore, a fixed filteringthreshold does not necessarily work well for everybucket and we can not find an applicable thresholdfor each task. To ease the awkward situation, wepropose to set a dynamic uncertainty threshold foreach bucket to guarantee that RoSE only filters outthe questions with relatively high uncertainty ineach bucket and there are no empty buckets afterfiltering. Specifically, for each bucket, we adopt the times of minimal uncertainty value in the bucketas the threshold and filter out the questions whoseuncertainty is higher than the threshold:",
  "from test sets to be evaluated. The detailed statis-tics of each dataset can be found in Appendix A": "Method ComparisonSince we mainly focus onthe streaming setting without any labeled data andpre-set unlabeled data, we compare RoSE withZero-Shot-CoT, Few-Shot-CoT, and Auto-CoT. Tomake a more fair comparison, we also comparethe self-consistency (Wang et al., 2023b) versionof these baseline methods. For Auto-CoT, we alsoadopt the same streaming setting as RoSE. Implementation SettingsWe use the tempera-ture T = 1.0 when generating diverse reasoningpaths and 20 reasoning paths will be generated foreach question. We adopt = 1.2 times of minimaluncertainty value in each bucket as the thresholdunless otherwise specified. For the methods thatdo not need to generate multiple diverse reasoningpaths, we use the temperature T = 0. We con-ducted all experiments on 8 Nvidia A100 GPUs.",
  "Main Results": "According to the comparison results in ,RoSE performs better than all baselines overall.For the results on GPT-3.5-Turbo, RoSE exceedsZero-Shot-CoT and Few-Shot-CoT by 8.4 and 5.9points respectively and exceeds Zero-Shot-CoT-SCand Few-Shot-CoT-SC by 4.7 and 2.9 points re-spectively. This directly demonstrates that RoSEcan self-improve by only the collected stream-ing experiences. While Few-Shot-CoT prompting AddSubSingleEqStrategyDate",
  ": The impact of each orchestration process": "uses demonstrations with human annotations, thesedemonstrations do not necessarily work for all testquestions. However, RoSE has a big advantageover Few-Shot-CoT prompting by orchestratinghelpful demonstrations from the experience poolfor each test question. RoSE also shows significantimprovements to Auto-CoT that only considers thediversity of demonstrations, and this indicates theimportance of our proposed well-designed experi-ence orchestration mechanism.Compared to GPT-3.5-Turbo, LLaMA2-13B-Chat has a big capacity gap on all reasoning tasks.However, RoSE also performs better than all base-line methods overall on LLaMA2-13B-Chat modeland the improvement becomes larger than it onGPT-3.5-Turbo. After equipping with RoSE, the",
  "Analyses": "The Effect of Each Orchestration ProcessTobetter understand the contribution of each experi-ence orchestration process, we conduct comprehen-sive ablation studies on four tasks. The ablationresults are shown in . We can observethat through the gradual orchestration process fromdiversity to uncertainty to complexity, the overallperformance of RoSE on four datasets is gradu-ally improved. This means that each process wepropose increases the helpfulness of the extractedexperiences in answering new questions. RoSEthat takes uncertainty into account shows a jumpin performance compared to the one that does notbecause the former generates multiple reasoningpaths for each question and makes a majority voteamong all predicted answers. Besides, RoSE whichonly considers diversity performs better than Auto-CoT overall. This represents the proposed question-aware diversity maintaining method is superior tothe methods that the k-means clustering methodused by Auto-CoT. AddSubSingleEqStrategyDate",
  ": The results on different numbers of reasoningpaths": "The Impact of Different Uncertainty ThresholdsAs shown in , we compare the performanceof RoSE with different uncertainty thresholds. Asintroduced in the previous section, we adopt times the minimal value of uncertainty in a bucketas the uncertainty threshold of the bucket. We firstcompare the performance of RoSE when adoptingdifferent values for . We find that the value oflambda values should not be too large, or RoSEmay retrieve ones with high uncertainty, resultingin lower performance. Moreover, we also evaluatethe performance of RoSE with a fixed uncertaintythreshold for each bucket. Using a fixed thresh-old leads to lower performance than RoSE witha dynamic uncertainty threshold. This representsselecting a suitable fixed threshold for differentbuckets is difficult and also proves that the adopteddynamic threshold is robust. The Impact of Different Complexity ThresholdsAs shown in , we also compare the per-formance of selecting the questions with differentcomplexity and find that the more complex the ex-tracted questions, the more helpful they are. This isalso consistent with our initial intuition mentionedin Sec 3.1, that the more complex a question, themore it includes the details of the reasoning thatcan better teach LLMs how to reason. # Demonstrations Accuracy (%) AddSub Few-Shot-CoT-SCAuto-CoT-SCROSE # Demonstrations Accuracy (%) SingleEq Few-Shot-CoT-SCAuto-CoT-SCROSE",
  ": Results on different demonstration quantities": "Results on Different Temperature ValuesInthis section, we evaluate RoSE under different tem-perature values. shows the results. Weobserve that RoSE consistently outperforms base-line methods across different temperature values,which shows the stability of RoSE. Besides, RoSEperforms worse when adopting a temperature of0.8 than a temperature of 1.0 or 1.2. This is be-cause lower temperatures result in less diversity ofmodel-generating inference paths. Results on Different Number of Reasoning PathsSince RoSE needs to generate multiple reasoningpaths for each question to estimate the uncertainty,we also evaluate RoSE under different numbers ofreasoning paths. shows the results and wecan see that the performance of RoSE increaseswith the increase of the number of reasoning paths.Moreover, RoSE consistently outperforms base-line methods across different numbers of reasoningpaths, which shows the stability of RoSE. Results on Different Numbers of Demonstra-tionsWe also evaluate RoSE under different num-bers of demonstrations. According to the resultsin , we see that RoSE consistently outper-forms Few-Shot-CoT-SC and Auto-CoT-SC acrossdifferent numbers of demonstrations, which showsthe stability of RoSE. Besides, we can find thatFew-Shot-CoT-SC is very unstable across differ-ent numbers of demonstrations, which also indi-cates that dynamically extracting demonstrationsfor each test question is more suitable than manual-crafting demonstrations. Transferability on Different CoT methodsRoSE is a relatively general framework that canbe adapted to many CoT prompting methods. Toverify the versatility of RoSE, we evaluate the per-formance of RoSE on two additional advanced CoTprompting methods: Plan-and-Solve (Wang et al.,2023a) and ToT (Yao et al., 2023a). The detailedimplementation settings are listed in Appendix C.Results on four ablation datasets are shown in . We observe that RoSE leads to consistentimprovements, which shows its generality acrossvarious CoT methods. Moreover, when using themore advanced CoT methods, RoSE can get fur-ther performance improvements, which shows itspotential in the future when the more powerful CoTmethod is proposed.",
  ": Results on different test orders": "Stability Analysis on Different Test OrdersThe order of test questions will influence the perfor-mance because this can lead to different states ofthe experience pool. To verify the stability of RoSE,we conduct 10 evaluations on different test orders,and the distribution of results is shown in .Performance fluctuates as the test order changes,but it is generally better than the baselines.",
  "Conclusion": "We present RoSE, a general framework for im-proving the performance of LLMS on reasoningtasks. RoSE can self-improve by constantly col-lecting questions into an experience pool and doesnot need other complex external help. To extractmore helpful experience from the experience pool,we propose a systematic and novel experience or-chestration mechanism that sequentially regards diversity, uncertainty, and complexity of questionsin the pool as important measures to filter expe-riences. The comprehensive experimental resultson 9 reasoning tasks and 2 LLMs show that RoSEsignificantly improves the reasoning performanceof LLMs. Moreover, we conduct extensive analy-sis experiments and verify the importance of eachprocess and the stability of RoSE across variousexperimental settings.",
  "Ethics Statement": "In this paper, we let LLMs self-improve on reason-ing tasks. only by the collected streaming expe-riences. All datasets used are reasoning type andhave no unsafe samples. Moreover, the LLM can-not access the internet and control external tools.Hence we think the proposed method and all ex-periments are safe enough, which will not causeserious impact and unrecoverable consequences onsociety.",
  "This work was supported by the National NaturalScience Foundation of China (No. 62236004). Thecomputations in this research were performed usingthe CFFF platform of Fudan University": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual.",
  "prompting: Disentangling computation from rea-soning for numerical reasoning tasks.CoRR,abs/2211.12588": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. CoRR, abs/2204.02311. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems. CoRR, abs/2110.14168. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, andTushar Khot. 2023. Complexity-based prompting formulti-step reasoning. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did aristotleuse a laptop? A question answering benchmark withimplicit reasoning strategies. Trans. Assoc. Comput.Linguistics, 9:346361. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,and Laurent Sifre. 2022. Training compute-optimallarge language models. CoRR, abs/2203.15556. Mohammad Javad Hosseini, Hannaneh Hajishirzi, OrenEtzioni, and Nate Kushman. 2014. Learning to solvearithmetic word problems with verb categorization.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2014, October 25-29, 2014, Doha, Qatar, A meetingof SIGDAT, a Special Interest Group of the ACL,pages 523533. ACL. Tushar Khot, Harsh Trivedi, Matthew Finlayson, YaoFu, Kyle Richardson, Peter Clark, and Ashish Sab-harwal. 2023. Decomposed prompting: A modularapproach for solving complex tasks. In The EleventhInternational Conference on Learning Representa-tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.OpenReview.net.",
  "Xiaonan Li and Xipeng Qiu. 2023. Mot: Pre-thinkingand recalling enable chatgpt to self-improve withmemory-of-thoughts. CoRR, abs/2305.05181": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion: Learning to solve and explain algebraic wordproblems. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics,ACL 2017, Vancouver, Canada, July 30 - August 4,Volume 1: Long Papers, pages 158167. Associationfor Computational Linguistics.",
  "Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, andMehran Kazemi. 2024. In-context learning with re-trieved demonstrations for language models: A sur-vey. CoRR, abs/2401.11624": "Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer,and Hannaneh Hajishirzi. 2023. Z-ICL: zero-shotin-context learning with pseudo-demonstrations. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pages 23042317. Association for Computa-tional Linguistics. Aman Madaan, Niket Tandon, Peter Clark, and Yim-ing Yang. 2022. Memory-assisted prompt editing toimprove GPT-3 after deployment. In Proceedings ofthe 2022 Conference on Empirical Methods in Natu-ral Language Processing, EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pages28332861. Association for Computational Linguis-tics.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,John Schulman, Jacob Hilton, Fraser Kelton, LukeMiller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, and Ryan Lowe.2022. Training language models to follow instruc-tions with human feedback. In NeurIPS. Arkil Patel, Satwik Bhattamishra, and Navin Goyal.2021. Are NLP models really able to solve simplemath word problems? In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL-HLT 2021, Online,June 6-11, 2021, pages 20802094. Association forComputational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on Empiri-cal Methods in Natural Language Processing andthe 9th International Joint Conference on NaturalLanguage Processing, EMNLP-IJCNLP 2019, HongKong, China, November 3-7, 2019, pages 39803990.Association for Computational Linguistics.",
  "Reflexion: an autonomous agent with dynamic mem-ory and self-reflection. CoRR, abs/2303.11366": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, AdamFisch, Adam R. Brown, Adam Santoro, AdityaGupta, Adri Garriga-Alonso, Agnieszka Kluska,Aitor Lewkowycz, Akshat Agarwal, Alethea Power,Alex Ray, Alex Warstadt, Alexander W. Kocurek,Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-rish, Allen Nie, Aman Hussain, Amanda Askell,Amanda Dsouza, Ameet Rahane, Anantharaman S.Iyer, Anders Andreassen, Andrea Santilli, AndreasStuhlmller, Andrew M. Dai, Andrew La, Andrew K.Lampinen, Andy Zou, Angela Jiang, Angelica Chen,Anh Vuong, Animesh Gupta, Anna Gottardi, Anto-nio Norelli, Anu Venkatesh, Arash Gholamidavoodi,Arfa Tabassum, Arul Menezes, Arun Kirubarajan,Asher Mullokandov, Ashish Sabharwal, Austin Her-rick, Avia Efrat, Aykut Erdem, Ayla Karakas, andet al. 2022. Beyond the imitation game: Quantifyingand extrapolating the capabilities of language models.CoRR, abs/2206.04615. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,Qinyuan Cheng, Xiangyang Liu, Hang Yan, YunfanShao, Qiong Tang, Shiduo Zhang, Xingjian Zhao,Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li,Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang,Lingling Wu, Zhangyue Yin, Xuanjing Huang, Yu-Gang Jiang, and Xipeng Qiu. 2024. MOSS: an openconversational large language model. Mach. Intell.Res., 21(5):888905. Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge.In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, NAACL-HLT 2019, Minneapolis, MN, USA,June 2-7, 2019, Volume 1 (Long and Short Papers),",
  "pages 41494158. Association for ComputationalLinguistics": "Romal Thoppilan, Daniel De Freitas, Jamie Hall,Noam Shazeer, Apoorv Kulshreshtha, Heng-TzeCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,Amin Ghafouri, Marcelo Menegali, Yanping Huang,Maxim Krikun, Dmitry Lepikhin, James Qin, DehaoChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,Maarten Bosma, Yanqi Zhou, Chung-Ching Chang,Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S.Meier-Hellstern, Meredith Ringel Morris, TulseeDoshi, Renelito Delos Santos, Toju Duke, Johnny So-raker, Ben Zevenbergen, Vinodkumar Prabhakaran,Mark Diaz, Ben Hutchinson, Kristen Olson, Ale-jandra Molina, Erin Hoffman-John, Josh Lee, LoraAroyo, Ravi Rajakumar, Alena Butryna, MatthewLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-hen, Rachel Bernstein, Ray Kurzweil, Blaise Ageray Arcas, Claire Cui, Marian Croak, Ed H. Chi, andQuoc Le. 2022. Lamda: Language models for dialogapplications. CoRR, abs/2201.08239. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.2023a. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large languagemodels. In Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pages 26092634. Association forComputational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-hery, and Denny Zhou. 2023b.Self-consistencyimproves chain of thought reasoning in languagemodels. In The Eleventh International Conferenceon Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InNeurIPS. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Thomas L. Griffiths,Yuan Cao,and KarthikNarasimhan. 2023a. Tree of thoughts: Deliberateproblem solving with large language models. CoRR,abs/2305.10601. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R. Narasimhan, and Yuan Cao.2023b. React: Synergizing reasoning and actingin language models. In The Eleventh InternationalConference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,Yufei Xue, Jidong Zhai, Wenguang Chen, ZhiyuanLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.GLM-130B: an open bilingual pre-trained model. InThe Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. OpenReview.net.",
  "We evaluate RoSE on the following reasoningtasks": "Arithmetic reasoning.We consider 6Math Word Problem datasets, includingAddSub (Hosseini et al., 2014), AQuA (Linget al., 2017), GSM8K (Cobbe et al., 2021),SingleEq (Koncel-Kedziorski et al., 2015),SingleOp (Roy et al., 2015), and SVAMP (Pa-tel et al., 2021). Commonsense reasoning. We use Common-senseQA (CSQA) (Talmor et al., 2019), Strat-egyQA (Strategy) (Geva et al., 2021), andone dataset from BIG-bench (Srivastava et al.,2022): Date Understanding (Date).",
  "CImplementation Details of DifferentCoT Methods": "We verify the versatility of RoSE on two otherCoT prompting methods: Plan-and-Solve (Wanget al., 2023a) and ToT (Yao et al., 2023a). We alsomaintain a zero-shot setting for these two methods,i.e. there are no manual-crafted demonstrations.After combining the two methods with RoSE, weadd each question and the corresponding thoughtsinto the streaming experience pool and orchestratethese collected experiences to assist in answeringeach new question. Although a zero-shot setting isadopted, these two methods have relatively morecomplex zero-shot prompts than traditional CoTmethods. To take full advantage of these methods,we completed the analysis experiment on the gpt-3.5-turbo-16k-0613 model.For the Plan-and-Solve method, we follow theprompts in the original paper and use the sameuncertainty and complexity measures as the tradi-tional CoT method. For ToT methods, we implement a zero-shotToT-BFS that samples multiple thoughts using aCoT prompt and makes a vote for the best oneamong all thoughts. We set the step limit T to 2 andgenerate 5 thoughts every step. To combine withour RoSE framework, we sum the percentage of thetotal votes for each best thought as the uncertaintymeasure and sum the number of steps in each bestthought as the complexity measure. The prompttemplate for ToT is listed in",
  ": Detailed statistics of the datasets utilized in our experiment": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they aredone, there will be 21 trees. How many trees did the grove workers plant today?A: Lets think step by step. There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Lets think step by step. There are 15 trees originally. Then there were 21 trees after some morewere planted. So there must have been 21 - 15 = 6. The answer is 6.Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left intotal?A: Lets think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total, they had32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How manylollipops did Jason give to Denny?A: Lets think step by step. Jason started with 20 lollipops. Then he had 12 after giving some to Denny.So he gave Denny 20 - 12 = 8. The answer is 8.Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toysdoes he have now?A: Lets think step by step. There are 15 trees originally. Shawn started with 5 toys. If he got 2 toyseach from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.Q: There were nine computers in the server room. Five more computers were installed each day, fromMonday to Thursday. How many computers are now in the server room?A: Lets think step by step. There were originally 9 computers. For each of 4 days, 5 more computerswere added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.Q: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. Howmany golf balls did he have at the end of Wednesday?A: Lets think step by step. There are 15 trees originally. Michael started with 58 golf balls. Afterlosing 23 on Tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. Theanswer is 33.Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: Lets think step by step. Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
  ": Few-Shot Demonstrations for AddSub": "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean ofthe numbers is?Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64A: Lets think step by step. If 10 is added to each number, then the mean of the numbers also increasesby 10. So the new mean would be 50. The answer is A.Q: If a / b = 3/4 and 8a + 5b = 22, then find the value of a.Answer Choices: (A) 1/2 (B) 3/2 (C) 5/2 (S) 4/2 (E) 7/2A: Lets think step by step. If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a +20a / 3 = 22, which means 44a / 3 = 22. So a is equal to 3/2. The answer is B.Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?Answer Choices: (A) 53 km (B) 55 km (C) 52 km (D) 60 km (E) 50 kmA: Lets think step by step. The distance that the person traveled would have been 20 km/hr * 2.5 hrs =50 km. The answer is E.Q: How many keystrokes are needed to type the numbers from 1 to 500?Answer Choices: (A) 1156 (B) 1392 (C) 1480 (D) 1562 (E) 1788A: Lets think step by step. There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbersfrom 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. Theanswer is B.",
  ": Few-Shot Demonstrations for AQuA": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they aredone, there will be 21 trees. How many trees did the grove workers plant today?A: Lets think step by step. There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Lets think step by step. There are 15 trees originally. Then there were 21 trees after some morewere planted. So there must have been 21 - 15 = 6. The answer is 6.Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left intotal?A: Lets think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total they had32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How manylollipops did Jason give to Denny?A: Lets think step by step. Jason started with 20 lollipops. Then he had 12 after giving some to Denny.So he gave Denny 20 - 12 = 8. The answer is 8.Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toysdoes he have now?A: Lets think step by step. There are 15 trees originally. Shawn started with 5 toys. If he got 2 toyseach from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.Q: There were nine computers in the server room. Five more computers were installed each day, frommonday to thursday. How many computers are now in the server room?A: Lets think step by step. There were originally 9 computers. For each of 4 days, 5 more computerswere added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. Howmany golf balls did he have at the end of wednesday?A: Lets think step by step. There are 15 trees originally. Michael started with 58 golf balls. Afterlosing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. Theanswer is 33.Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: Lets think step by step. Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
  ": Few-Shot Demonstrations for GSM8K": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they aredone, there will be 21 trees. How many trees did the grove workers plant today?A: Lets think step by step. There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Lets think step by step. There are 15 trees originally. Then there were 21 trees after some morewere planted. So there must have been 21 - 15 = 6. The answer is 6.Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left intotal?A: Lets think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total they had32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How manylollipops did Jason give to Denny?A: Lets think step by step. Jason started with 20 lollipops. Then he had 12 after giving some to Denny.So he gave Denny 20 - 12 = 8. The answer is 8.Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toysdoes he have now?A: Lets think step by step. There are 15 trees originally. Shawn started with 5 toys. If he got 2 toyseach from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.Q: There were nine computers in the server room. Five more computers were installed each day, frommonday to thursday. How many computers are now in the server room?A: Lets think step by step. There were originally 9 computers. For each of 4 days, 5 more computerswere added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. Howmany golf balls did he have at the end of wednesday?A: Lets think step by step. There are 15 trees originally. Michael started with 58 golf balls. Afterlosing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. Theanswer is 33.Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: Lets think step by step. Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
  ": Few-Shot Demonstrations for SingleEq": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they aredone, there will be 21 trees. How many trees did the grove workers plant today?A: Lets think step by step. There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Lets think step by step. There are 15 trees originally. Then there were 21 trees after some morewere planted. So there must have been 21 - 15 = 6. The answer is 6.Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left intotal?A: Lets think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total they had32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How manylollipops did Jason give to Denny?A: Lets think step by step. Jason started with 20 lollipops. Then he had 12 after giving some to Denny.So he gave Denny 20 - 12 = 8. The answer is 8.Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toysdoes he have now?A: Lets think step by step. There are 15 trees originally. Shawn started with 5 toys. If he got 2 toyseach from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.Q: There were nine computers in the server room. Five more computers were installed each day, frommonday to thursday. How many computers are now in the server room?A: Lets think step by step. There were originally 9 computers. For each of 4 days, 5 more computerswere added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. Howmany golf balls did he have at the end of wednesday?A: Lets think step by step. There are 15 trees originally. Michael started with 58 golf balls. Afterlosing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. Theanswer is 33.Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: Lets think step by step. Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
  ": Few-Shot Demonstrations for SingleOp": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they aredone, there will be 21 trees. How many trees did the grove workers plant today?A: Lets think step by step. There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Lets think step by step. There are 15 trees originally. Then there were 21 trees after some morewere planted. So there must have been 21 - 15 = 6. The answer is 6.Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left intotal?A: Lets think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total they had32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How manylollipops did Jason give to Denny?A: Lets think step by step. Jason started with 20 lollipops. Then he had 12 after giving some to Denny.So he gave Denny 20 - 12 = 8. The answer is 8.Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toysdoes he have now?A: Lets think step by step. There are 15 trees originally. Shawn started with 5 toys. If he got 2 toyseach from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.Q: There were nine computers in the server room. Five more computers were installed each day, frommonday to thursday. How many computers are now in the server room?A: Lets think step by step. There were originally 9 computers. For each of 4 days, 5 more computerswere added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. Howmany golf balls did he have at the end of wednesday?A: Lets think step by step. There are 15 trees originally. Michael started with 58 golf balls. Afterlosing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. Theanswer is 33.Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: Lets think step by step. Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
  ": Few-Shot Demonstrations for SVAMP": "Q: What do people use to absorb extra ink from a fountain pen?Answer Choices: (A) shirt pocket (B) calligraphers hand (C) inkwell (D) desk drawer (E) blotterA: Lets think step by step. The answer must be an item that can absorb ink. Of the above choices,only blotters are used to absorb ink. The answer is E.Q: What home entertainment equipment requires cable?Answer Choices: (A) radio shack (B) substation (C) television (D) cabinetA: Lets think step by step. The answer must require cable. Of the above choices, only televisionrequires cable. The answer is C.Q: The fox walked from the city into the forest, what was it looking for?Answer Choices: (A) pretty flowers (B)hen house (C) natural habitat (D) storybookA: Lets think step by step. The answer must be something in the forest. Of the above choices, onlynatural habitat is in the forest. The answer is C.Q: Sammy wanted to go to where the people were. Where might he go?Answer Choices: (A) populated areas (B) race track (C) desert (D) apartment (E) roadblockA: Lets think step by step. The answer must be a place with a lot of people. Of the above choices,only populated areas have a lot of people. The answer is A.Q: Where do you put your grapes just before checking out?Answer Choices: (A) mouth (B) grocery cart (C)super market (D) fruit basket (E) fruit marketA: Lets think step by step. The answer should be the place where grocery items are placed beforechecking out. Of the above choices, grocery cart makes the most sense for holding grocery items. Theanswer is B.Q: Google Maps and other highway and street GPS services have replaced what?Answer Choices: (A) united states (B) mexico (C) countryside (D) atlasA: Lets think step by step. The answer must be something that used to do what Google Maps and GPSservices do, which is to give directions. Of the above choices, only atlases are used to give directions.The answer is D.Q: Before getting a divorce, what did the wife feel who was doing all the work?Answer Choices: (A) harder (B) anguish (C) bitterness (D) tears (E) sadnessA: Lets think step by step. The answer should be the feeling of someone getting divorced who wasdoing all the work. Of the above choices, the closest feeling is bitterness. The answer is C.",
  ": Few-Shot Demonstrations for CommonsenseQA": "Q: Do hamsters provide food for any animals?A: Lets think step by step. Hamsters are prey animals. Prey are food for predators. Thus, hamstersprovide food for some animals. The answer is yes.Q: Could Brooke Shields succeed at University of Pennsylvania?A: Lets think step by step. Brooke Shields went to Princeton University. Princeton University is aboutas academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeedat the University of Pennsylvania. The answer is yes.Q: Yes or no: Hydrogens atomic number squared exceeds number of Spice Girls?A: Lets think step by step. Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 SpiceGirls. Thus, Hydrogens atomic number squared is less than 5. The answer is no.Q: Yes or no: Is it common to see frost during some college commencements?A: Lets think step by step. College commencement ceremonies can happen in December, May,and June. December is in the winter, so there can be frost. Thus, there could be frost at somecommencements. The answer is yes.Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?A: Lets think step by step. The War in Vietnam was 6 months. The gestation period for a llama is11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War inVietnam. The answer is no.Q: Yes or no: Would a pear sink in water?A: Lets think step by step. The density of a pear is about 0.6g/cm3, which is less than water. Objectsless dense than water float. Thus, a pear would float. The answer is no.",
  ": Few-Shot Demonstrations for StrategyQA": "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?Answer Choices: (A) 01/05/2015 (B) 01/06/2015 (C) 01/04/2015 (D) 02/05/2015 (E) 12/05/2015 (F)01/05/2016A: Lets think step by step. If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/05/2015. Theanswer is A.Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today inMM/DD/YYYY?Answer Choices: (A) 01/08/2019 (B) 01/07/2019 (C) 01/06/2019 (D) 02/07/2019 (E) 12/07/2019 (F)01/07/2018A: Lets think step by step. If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday.Today is the first monday, would be six days later. So today is 01/07/2019. The answer is B.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is thedate 10 days ago in MM/DD/YYYY?Answer Choices: (A) 05/22/1943 (B) 05/23/1943 (C) 05/24/1943 (D) 05/25/1943 (E) 05/26/1943 (F)05/27/1943A: Lets think step by step. One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 daysbefore today is 05/23/1943. The answer is B.Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?Answer Choices: (A) 04/23/1969 (B) 04/21/1969 (C) 04/22/1969 (D) 04/20/1969 (E) 04/24/1969 (F)04/25/1969A: Lets think step by step. Today is 04/19/1969. 24 hours later is one day after today, which would be04/20/1969. The answer is D.Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date24 hours later in MM/DD/YYYY?Answer Choices: (A) 03/17/2002 (B) 03/14/2002 (C) 03/15/2002 (D) 03/16/2002 (E) 03/13/2002 (F)03/18/2002A: Lets think step by step. Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. Theanswer is E.Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is thedate yesterday in MM/DD/YYYY?Answer Choices: (A) 03/04/2017 (B) 02/28/2017 (C) 03/01/2017 (D) 03/02/2017 (E) 03/03/2017 (F)02/27/2017A: Lets think step by step. The last day of February is the 28th, so Jane was born on 02/28/2001.Today is her 16-year old birthday.So yesterday was 02/27/2017. The answer is F.",
  ": Few-Shot Demonstrations for Date Understanding": "Answer Formataddsub_format = \"the answer is n\" where n is a numbersingle_format = \"the answer is n\" where n is a numberstrategy_format = either \"the answer is yes\" or \"the answer is no\"date_format = \"the answer is n\" where n is one of \"A, B, C, D, E, F\"Thought FormatAnswer the following question: {input}Make a strategy then write. Your output should be of the following format:Strategy:Your strategy about how to answer the question.Answer:Your answer to the question. It should end with {format}.Voting PromptGiven an instruction and several choices, decide which choice is most promising.Analyze each choice in detail, then conclude in the last line\"The best choice is {s}\", where s is the integer id of the choice."
}