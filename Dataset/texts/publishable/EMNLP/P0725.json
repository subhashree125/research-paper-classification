{
  "Abstract": "How far have we come in mitigating perfor-mance disparities across genders in multilin-gual speech recognition?We compare theimpact on gender disparity of different fine-tuning algorithms for automated speech recog-nition across model sizes, languages and gen-der.We look at both performance-focusedand fairness-promoting algorithms. Across lan-guages, we see slightly better performance forfemale speakers for larger models regardlessof the fine-tuning algorithm. The best trade-off between performance and parity is foundusing adapter fusion. Fairness-promoting fine-tuning algorithms (Group-DRO and SpectralDecoupling) hurt performance compared toadapter fusion with only slightly better per-formance parity. LoRA increases disparitiesslightly. Fairness-mitigating fine-tuning tech-niques led to slightly higher variance in perfor-mance across languages, with the exception ofadapter fusion.",
  "Introduction": "Automatic Speech Recognition (ASR) systemshave become ubiquitous in our daily lives, power-ing virtual assistants, dictation software, customerservice interactions, and more. However, these sys-tems are not always equally effective for all users,and gender disparity in their performance is a sig-nificant concern (Tatman, 2017).One key factor contributing to gender disparityin ASR performance is the way in which these sys-tems are trained. If the training data predominantlyconsist of male voices, for example, ASR systemsmay exhibit higher error rates when transcribingfemale speech. Or vice versa. The downstream so-cietal impact of gender disparity in ASR systems ismultifaceted. It can exacerbate existing inequalitiesby hindering access to information and services forcertain social groups. In professional settings, in-accurate transcription can impede communication : Augmenting Whisper with adapter fusion forbetter performance and gender parity. Adapter fusionis over three adapters one trained with a vanilla loss(ERM), one trained with Group-DRO, and one trainedwith spectral decoupling (SD). and productivity, potentially affecting career ad-vancement opportunities. Finally, the perpetuationof performance disparities in technology can rein-force harmful stereotypes and norms, contributingto broader social inequalities.Mitigating gender disparities is non-trivial. Sev-eral algorithms have been proposed (Koh et al.,2021), but they often introduce significant perfor-mance costs (Kim et al., 2020), and their impacton gender disparity is often small (Chalkidis et al.,2022). In this paper, we evaluate their impact onperformance and gender disparity in multilingualASR, but we also present a simple technique toimprove the robustness of fairness-improving fine-tuning methods. Specifically, we augment multilin-gual Whisper using adapter fusion (Pfeiffer et al.,2021) over several fairness-promoting strategies;see . ContributionsWe evaluate the performance andgender disparity of four different fine-tuning strate-gies for multilingual ASR models, two of whichare fairness-promoting. Our baselines are standardempirical risk minimization (ERM) and so-calledlow-rank adaptation (LoRA) (Hu et al., 2022).The two fairness-promoting algorithms are Group- DRO (Sagawa et al., 2020) and spectral decoupling(Pezeshki et al., 2021). Finally, we evaluate a novelcombination of existing techniques: adapter fusionover ERM, Group-DRO, and spectral decoupling.We evaluate all models across 16 languages withbinary gender as our demographic variable, andfind that only adapter fusion reduces gender dis-parity without incurring a significant performancedrop. In fact, performance improves, and disparityis reduced. Cross-lingual performance gaps widena bit, however.",
  "Fairness-Promoting Finetuning": "Standard expected risk minimization (ERM) fine-tuning in automatic speech recognition (ASR)and fairness-promoting fine-tuning techniques likeGroup-DRO (Sagawa et al., 2020), spectral decou-pling (Pezeshki et al., 2021), and adapter fusion(Pfeiffer et al., 2021) serve different purposes andaddress different aspects of model performance.ERM fine-tuning, our baseline technique, mini-mizes the expected risk or the expected value ofa loss function over the data distribution. In ASR,this typically involves reducing the word error rate(WER). ERM fine-tuning involves iteratively updat-ing the parameters of the ASR model using a gra-dient descent-based optimization algorithm, suchas stochastic gradient descent (SGD) or Adam, tominimize the loss function.Fairness-promoting fine-tuning techniques aimto increase performance parity in ASR systems byaddressing disparities in model predictions acrossdifferent demographic variables (e.g., gender, race,age). Group-DRO (Sagawa et al., 2020) modifiesthe standard fine-tuning objective to optimize forperformance parity by minimizing the worst-caseerror across multiple demographic groups, ensur-ing that no particular group is disproportionatelyaffected by the models predictions. Spectral decou-pling (Pezeshki et al., 2021) involves modifying thetraining process to remove unwanted biases fromthe learned representations, by decoupling sensitivefeatures from the predictive features in the model.The fairness-promoting techniques differ fromERM fine-tuning in their explicit attention to per-formance parity, often introducing additional con-straints or modifications to the training process toachieve more equitable model outcomes. WhileERM fine-tuning seeks to optimize the overall per-formance of the ASR system, fairness-promotingtechniques specifically target performance dispari- ties across social groups.Adapter fusion fine-tunes a pre-trained modelby adding small neural adapters to the model ar-chitecture, which are specifically trained to miti-gate performance disparities and improve empiricalfairness in the models predictions while retain-ing the knowledge learned from the pre-trainingphase. The adapter fusion model explored herecombines ERM, Group-DRO and spectral decou-pling adapters by adding an extra adapter layer.",
  "Experiments": "ModelWhisper (Radford et al., 2022) is a familyof ASR models developed by OpenAI. The mod-els vary in size from 39M parameters in the tinymodel to 1.55B parameters in the large model,and they are all trained on 680,000 hours of webdata across 97 languages. We focus on Whisper-large because it achieves the highest performancefor all groups, but we provide results for all modelsizes in of the Appendix. DataWe use VoxPopuli1 a multilingual, opensource dataset to evaluate the performanceand gender disparity of different fine-tuning andfairness-promoting strategies. VoxPopuli is a col-lection of speeches given in the European Parlia-ment between 20092020, and metadata about thespeakers demographic information is included. Ourstudy of gender disparity is limited by the avail-ability of only the binary genders in this dataset.We use data from 16 languages: English, Slovene,Lithuanian, Italian, French, Polish, Romanian, Ger-man, Finnish, Dutch, Croatian, Hungarian, Slovak,Czech, Spanish, and Estonian. The Whisper mod-els performed relatively poorly out of the box onthis data, so we report word error rates for fine-tuned models only. LoRA and ERM baselinesWe use two differentbaselines. Our LoRA baseline introduce low-rankparameterization to the adapter layers added ontop of the pre-trained model. Low-rank param-eterization reduces the number of parameters re-quired to adapt the pre-trained model, making itmore computationally efficient, yet aims to main-tain the expressiveness of the adapted model. Byconstraining the parameters to be low-rank, LoRAlike sparsity-promoting regularizers encourages the adapter layers to capture essential task-specific in-formation. Our ERM baseline adapter layers trainadditional layers on top of the pre-trained Whis-per model. The purpose of these layers is to adaptthe pre-trained models representations to bettersuit the VoxPopuli data and increase performance.ERM focuses on minimizing the empirical risk,which is the average loss over the training dataset. Group-DRO and Spectral DecouplingOur twofairness-promoting fine-tuning strategies are alsoimplemented as adapter layers. Unlike the base-line adapters, these strategies specifically target andmitigate group disparity, in our case, across gen-ders. All adapter strategies use the same numberof parameters; see for the Appendix forhyper-parameters. Adapter fusionInspired by the adapter fusionframework presented in Pfeiffer et al. (2021), we in-troduce adapter fusion layers to adjudicate betweenthree adapters: one trained with empirical risk min-imization, one trained with group-distributionallyrobust optimization, and one trained with spectraldecoupling. ProtocolWe rely on the standard VoxPopulisplits.For each fine-tuning strategy, we fine-tune a new model for each language. All hyper-parameters are optimized on held-out (develop-ment) data, by doing grid search over a limitedset of values; see of the Appendix for hy-perparameters.",
  ": Word Error Rates for Whisper-large withadapters, averaged across 16 languages. Delta indicatesthe performance disparity between the binary genders": "ResultsThe full set of results is presented in of the Appendix, but the summary forWhisper-large averaging across the 16 languages is presented in . We generally see slightlybetter performance for female speakers than formale speakers. Group-DRO and spectral decou-pling perform on par with standard ERM fine- tuning, but with the added benefit of lower perfor-mance disparity. LoRA also performs comparably,but exhibits higher gender disparity, and Adapterfusion reaches the best performance for both gen-ders, but with higher disparity () than in Group-DRO and Spectral Decoupling. These trends areobserved across all model sizes, but performanceis slightly better for larger models, see for performance examples across the 5 Whispermodels on English and German VoxPopuli.",
  "Discussion": "Fairness and (two flavors of) regularizationWesaw that LoRA exhibits higher performance dispari-ties than vanilla empirical risk minimization. Sincelow-rank adaptation is a form of regularization,this seems at odds with previous work suggestingthat regularization mitigates performance dispari-ties (Sagawa et al., 2020; Petren Bach Hansen et al.,2022). The incongruency is only apparent: It iswell-known that sparsity-promoting regularization like LoRA hurts robustness (Sutton et al., 2006;Globerson and Roweis, 2006; Sgaard, 2013),while 2-regularization, -regularization, noise in-jection (Bishop, 1995), and drop-out (Wager et al.,2013) often improve robustness. The work citedabove (Sagawa et al., 2020; Petren Bach Hansenet al., 2022) both use 2-regularization. Rawlsian fairnessWhether fairness is best mea-sured by the absolute performance of the worst-offgroup (Rawlsian fairness) or by the relative perfor-mance differences across groups (egalitarian fair-ness) is a philosophical question (Jrgensen andSgaard, 2023), but we note that in our case, thishas direct consequences for what approach to rec-ommend: Group-DRO or Adapter Fusion? Adapterfusion has superior performance, also for the worstoff group, so it is preferable on Rawls account.The egalitarian would prefer something like Group-DRO, however, since cross-group differences ()are smaller. Note that minimizing cross-group dif-ferences is preferable even to leveling down (Parfit,2002) from an egalitarian perspective. Linguistic fairnessFairness is usually measuredacross social groups defined by the Cartesian prod-uct of a set of protected attributes. However, globaltechnologies can serve some language communi-ties better than others (Lent et al., 2021; Wang et al.,2022; Paz, 2014). This goes for both error rates anddisparities (Cabello Piqueras and Sgaard, 2022;",
  ": Standard deviations for performance acrosslanguages": "Ramesh et al., 2023). If Whisper or other mul-tilingual ASR models consistently exhibit muchhigher error rates or disparities on data in specificlanguages, their speakers will be disadvantaged.The standard deviations of performances across 16languages for Whisper-large are given in .Fine-tuning cuts more than half of the standard de-viation across languages. The fact that adapter fu-sion leads to the lowest disparity across languages,is promising. Cross-lingual performance gaps suchas the ones observed here are still worrying andcall for more research; see also Rust and Sgaard(2023) for discussion.",
  "The pros and cons of stacked architecturesAdapter fusion adds additional layers of param-": "eters to adjudicate between the three adapter layerstrained with different objectives. If we add param-eters, we slow down inference time, but on theupside, fine-tuning adapter layers is inexpensivein comparison to fine-tuning encoder or decoderlayers of the original Whisper model. Stacking ischeaper than voting in this respect, since we donot have to fine-tune with three different objectives.Finally, the slow inference time can be improvedby adapter pruning (Rckl et al., 2021), withoutcompromising fairness.",
  "Conclusions": "In this paper, we evaluated the impact on perfor-mance and gender disparity of different fine-tuningalgorithms in automated speech recognition, in-cluding fairness-promoting ones.Our analysisconsidered model size, language, gender. In gen-eral, we saw better performance for female speak-ers for larger models and significant performancegaps across languages. Fairness-promoting fine-tuning algorithms (Group-DRO and spectral decou-pling) hurt performance with some improvementsin performance parity. LoRA increases disparitiesslightly. We find the highest performance withacceptable parity in a novel, fairness-promotingvariant of adapter fusion, which had positive ef-",
  "Limitations": "While we cover 16 languages, our study is lim-ited to mostly Indo-European, higher-resource lan-guages.English, Slovene, Lithuanian, Italian,French, Polish, Romanian, German, Dutch, Croat-ian, Slovak, Czech, and Spanish are Indo-European.Hungarian, Finnish, and Estonian are Finno-Ugriclanguages. It is important to extend studies suchas ours to more language families; see, e.g., Abra-ham et al. (2020). Our dataset only contains binarygender (M/F), and as a result our results are lim-ited to these genders only. While we compare twofairness-promoting fine-tuning strategies, we leaveout others, e.g., automatic, worst-case aware cur-riculum learning (de Lhoneux et al., 2022). Suchalgorithms could also be combined using adapterfusion. Obviously, it would be relevant to replicateour findings on other multilingual ASR datasets,and it is extremely important to extend studies suchas ours to more demographic variables (race, age,language proficiency, impairments, etc.) and to in-vestigate the performance on intersections of theseattributes. Basil Abraham, Danish Goel, Divya Siddarth, KalikaBali, Manu Chopra, Monojit Choudhury, Pratik Joshi,Preethi Jyoti, Sunayana Sitaram, and Vivek Seshadri.2020. Crowdsourcing speech data for low-resourcelanguages from low-income workers. In Proceedingsof the Twelfth Language Resources and EvaluationConference, pages 28192826, Marseille, France. Eu-ropean Language Resources Association.",
  "Chris M. Bishop. 1995. Training with noise is equiva-lent to tikhonov regularization. Neural Computation,7(1):108116": "Laura Cabello Piqueras and Anders Sgaard. 2022. Arepretrained multilingual models equally fair acrosslanguages? In Proceedings of the 29th InternationalConference on Computational Linguistics, pages35973605, Gyeongju, Republic of Korea. Interna-tional Committee on Computational Linguistics. Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, LetiziaTomada, Sebastian Schwemer, and Anders Sgaard.2022. FairLex: A multilingual benchmark for evalu-ating fairness in legal text processing. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 43894406, Dublin, Ireland. Association forComputational Linguistics. Miryam de Lhoneux, Sheng Zhang, and Anders Sgaard.2022. Zero-shot dependency parsing with worst-caseaware automated curriculum learning. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 578587, Dublin, Ireland. Associationfor Computational Linguistics. Amir Globerson and Sam Roweis. 2006. Nightmareat test time: robust learning by feature deletion. InProceedings of the 23rd International Conferenceon Machine Learning, ICML 06, page 353360,New York, NY, USA. Association for ComputingMachinery. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations.",
  "FACT: A diagnostic for group fairness trade-offs. InProceedings of the 37th International Conferenceon Machine Learning, volume 119 of Proceedingsof Machine Learning Research, pages 52645274.PMLR": "PangWeiKoh,ShioriSagawa,HenrikMark-lund, Sang Michael Xie, Marvin Zhang, AkshayBalsubramani, Weihua Hu, Michihiro Yasunaga,Richard Lanas Phillips, Irena Gao, Tony Lee, EtienneDavid, Ian Stavness, Wei Guo, Berton Earnshaw, Im-ran Haque, Sara M Beery, Jure Leskovec, AnshulKundaje, Emma Pierson, Sergey Levine, ChelseaFinn, and Percy Liang. 2021. Wilds: A benchmarkof in-the-wild distribution shifts. In Proceedings ofthe 38th International Conference on Machine Learn-ing, volume 139 of Proceedings of Machine LearningResearch, pages 56375664. PMLR. HeatherLent,EmanueleBugliarello,Miryamde Lhoneux, Chen Qiu, and Anders Sgaard. 2021.On language models for creoles. In Proceedings ofthe 25th Conference on Computational Natural Lan-guage Learning, pages 5871, Online. Associationfor Computational Linguistics.",
  "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine McLeavey, and Ilya Sutskever. 2022.Robust speech recognition via large-scale weak su-pervision": "Krithika Ramesh, Sunayana Sitaram, and MonojitChoudhury. 2023. Fairness in language models be-yond English: Gaps and challenges. In Findingsof the Association for Computational Linguistics:EACL 2023, pages 21062119, Dubrovnik, Croatia.Association for Computational Linguistics. Andreas Rckl, Gregor Geigle, Max Glockner, TilmanBeck, Jonas Pfeiffer, Nils Reimers, and IrynaGurevych. 2021. AdapterDrop: On the efficiencyof adapters in transformers. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 79307946, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Phillip Rust and Anders Sgaard. 2023. Differential pri-vacy, linguistic fairness, and training data influence:Impossibility and possibility theorems for multilin-gual language models. In International Conferenceon Machine Learning. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto,and Percy Liang. 2020. Distributionally robust neu-ral networks for group shifts: On the importance ofregularization for worst-case generalization. In In-ternational Conference on Learning Representations(ICLR). Anders Sgaard. 2013. Part-of-speech tagging withantagonistic adversaries. In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 640644, Sofia, Bulgaria. Association for ComputationalLinguistics. Charles Sutton, Michael Sindelar, and Andrew Mc-Callum. 2006. Reducing weight undertraining instructured discriminative learning. In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 8995, New York"
}