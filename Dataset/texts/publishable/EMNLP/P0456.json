{
  "Abstract": "Data annotation and synthesis generally refersto the labeling or generating of raw datawith relevant information, which could beused for improving the efficacy of machinelearning models.The process, however, islabor-intensive and costly.The emergenceof advanced Large Language Models (LLMs),exemplified by GPT-4, presents an unprece-dented opportunity to automate the compli-cated process of data annotation and synthesis.While existing surveys have extensively cov-ered LLM architecture, training, and generalapplications, we uniquely focus on their spe-cific utility for data annotation. This surveycontributes to three core aspects: LLM-BasedAnnotation Generation, LLM-Generated Anno-tations Assessment, and LLM-Generated An-notations Utilization. Furthermore, this surveyincludes an in-depth taxonomy of data typesthat LLMs can annotate, a comprehensive re-view of learning strategies for models utilizingLLM-generated annotations, and a detailed dis-cussion of the primary challenges and limita-tions associated with using LLMs for data an-notation and synthesis. Serving as a key guide,this survey aims to assist researchers and prac-titioners in exploring the potential of the latestLLMs for data annotation, thereby fosteringfuture advancements in this critical field.",
  "Introduction": "In the complex realm of machine learning and nat-ural language processing (NLP), data annotationand synthesis stand out as a critical yet challengingtask, extending beyond simple label attachment toencompass a diverse array of fundamental or aux-iliary information. This detailed process typicallyinvolves categorizing raw data with class or tasklabels for basic classification, adding intermedi-ate labels for contextual depth (Yu et al., 2022), assigning confidence scores to assess annotation re-liability (Lin et al., 2022), applying alignment or",
  "Equal contribution": "preference labels to tailor outputs to specific crite-ria or user needs, annotating entity relationshipsto understand how entities within a dataset interactwith each other (Wadhwa et al., 2023), markingsemantic roles to define the underlying roles thatentities play in a sentence (Larionov et al., 2019), tagging temporal sequences to capture the orderof events or actions (Yu et al., 2023), or Synthe-size data in the format of instruction (Wang et al.,2022b), response (Zhang and Yang, 2023a), rea-soning (Wang et al., 2022a), pairwise (Bai et al.,2022) and textual feedback (Pan et al., 2024) to forlanguage model tuning. Despite its wide applications, data annotationand synthesis poses significant challenges for cur-rent machine learning models due to the com-plexity, subjectivity, and diversity of data (Yanget al., 2023d). This process requires domain ex-pertise and is resource-intensive, particularly whenmanually labeling or creating large datasets. Ad-vanced LLMs such as GPT-4 (OpenAI, 2023),Gemini (Team et al., 2023), and LLaMA-2 (Tou-vron et al., 2023b) offer a promising opportunity torevolutionize data annotation. LLMs serve as morethan just tools but play a crucial role in improv-ing the effectiveness and precision of data annota-tion. Their ability to automate annotation tasks (A,2022), ensure consistency across large volumes ofdata (Hou et al., 2023), and adapt through fine-tuning or prompting for specific domains (Songet al., 2023; Zhang et al., 2024a), significantly mit-igates the challenges encountered with traditionalannotation and synthesis methods, setting a newstandard for what is achievable in the realm ofNLP. This survey delves into the nuances of usingLLMs for data annotation and synthesis, explor-ing methodologies, utilizing strategies, and asso-ciated challenges in this transformative approach.Through this exploration, we aim to shed light onthe motivations behind embracing LLMs as cata-lysts for redefining the landscape of data annotation",
  "Assessing LLM-Generated Annotations: Weexplore various methods for assessing the qualityof annotations and strategies for selecting high-quality annotations from numerous options": "LLM-Generated Annotations Utilization: Weinvestigate the methodologies at different stages,including supervised fine-tuning, alignment tun-ing, and inference time, to train machine learningmodels based on LLM-generated annotations. Social Impact and Future Work: We discussissues ranging from ethical dilemmas, such asbias and implications, to technical limitations,including hallucination and efficiency in LLM-generated annotations. Focusing on this underrepresented aspect of LLMapplication, the survey aims to serve as a valuableguide for academics and practitioners who intendto deploy LLMs for annotation purposes. Notethat in this survey, we primarily focus on pure lan-guage models and do not extensively cover recentlyemerging multimodal LLMs, such as LLaVA (Liuet al., 2023b). illustrates the general struc-ture of this survey. Additionally, a list of potentialtools for utilizing LLMs for annotation is includedin Appendix A, along with explanatory examples.Differences from Other LLM-related Surveys.While existing surveys in the NLP domain ex-tensively cover architectural nuances (Zhao et al.,2023a), training methodologies (Liu et al., 2023d),and evaluation protocols (Chang et al., 2023)associated with LLMs, their main focus lieson the capabilities of models for specific endtasks such as machine translation (Min et al.,2021), alignment (Wang et al., 2023g), code gen-eration (Zan et al., 2023), and medical analy-sis (Thirunavukarasu et al., 2023). In contrast, thissurvey distinguishes itself by focusing primarilyon the application of these potent next-generationLLMs to the intricate realm of annotation synthesis,a domain that is crucial yet underexplored.",
  "Preliminaries": "In this section, we delve into our approach to the an-notation synthesis process. We introduce two coremodels: an annotator model, denoted as A, whichmaps input data to annotations, and a task learner,represented as L, that utilizes or learns from theseannotated data to accomplish specific tasks. Ourprimary focus is on utilizing advanced LLMs likeGPT-4 (OpenAI, 2023) and LLaMA (Touvron et al.,2023a) as annotators (A), while the task learner (L)can be another large model (Chiang et al., 2023a)or a less complex one such as BERT (Devlin et al.,2018), which utilizes these annotated data to per-form designated tasks. LLM-generated annotationsencompass categorical labels and enhance raw datapoints with a comprehensive array of auxiliarysignals. These annotations, including confidencescores, contextual details, and other metadata, ex-tend beyond traditional categorical labels.3LLM-Based Annotation GenerationThe emergence of LLMs has sparked significantinterest in their capacity for high-quality, context-sensitive annotation synthesis. This section dis-cusses various kinds of annotations and data pro-duced via LLMs.",
  "Instruction & Response": "Instruction and response are the two fundamentalcomponents that constitute a dataset for LLM fine-tuning and in-context learning (ICL). Previous NLPdatasets (Li et al., 2017; Wang et al., 2018; Ouyanget al., 2022) mainly rely on human annotators toconstruct. Recently, with the advent of LLMs, au-tomatic and generative methods (Meng et al., 2022;Ye et al., 2022a,b; Wang et al., 2024e; Wu et al.,2024b; Liu et al., 2024a) have gained more focusin data annotation.Instruction Diversity. The diversity of instructionhas been proven crucial for LLM learning (Li et al.,2023e; Song et al., 2024b,a; Tang et al.). Recentstudies have explored various methods to diversifyand augment instructions in the original datasets.For example, Yoo et al. (2021) enhance data diver-sity by mixing two different samples to create anew one. Wang et al. (2022b) use a few manually-written seed instructions and iteratively augmentthem with a generate-then-filter pipeline. Addi-tionally, Meng et al. (2023); Wang et al. (2023f)train an instruction generation model in the origi-nal dataset to augment the diversity of instruction.Gupta et al. (2023) employ a multi-step prompting LLM-Based Annotation",
  ": The proposed taxonomy of existing research on LLM for data annotation": "method to first generate task descriptions, whichare then used as instance seeds to guide LLMs ininstruction generation. To obtain informative anddiverse examples, Wang et al. (2023c) propose anexplain-then-generate pipeline with LLMs for it-erative data synthesis. Besides, Li et al. (2023a)paraphrase the given sample multiple times to helpLLMs understand them from different perspectives.Kksal et al. suggest a clustering-based data se-lection method to ensure diversity in the initialseed data for augmentation. Recently, Yu et al.(2024) introduce AttrPrompt as an effective wayto balance diversity and cost in LLM-based dataannotation. Xu et al. (2024) propose to synthesizehigh-quality instruction data at scale by extract-ing it directly from an aligned LLM and presenta self-synthesis method for generating large-scalealignment data named Magpie. To improve the di-versity, Chan et al. (2024) introduce Persona Hub a collection of 1 billion diverse personas automati-cally curated from web data, to foster the creationof diverse synthetic data at scale for various scenar-ios. Zhu et al. (2024) introduce FANNO, a fullyautonomous, open-sourced framework that revolu-tionizes the annotation process without the needfor pre-existing annotated data. Response Quality. High-quality responses are es-sential for effective fine-tuning and ICL (Luo et al.,2024a). To improve the quality of the generatedresponse, Zhang and Yang (2023a) frame the re-sponse generation as reading comprehension tasksand create detailed prompts for LLMs. Huang et al.(2023) adopt self-consistency (Wang et al., 2022b)in response generation, selecting from the candi-date response with the highest confidence score.Furthermore, Yang et al. (2024b) propose self- distill and augment the instruction tuning dataset byrewriting the original responses. Pang et al. (2024b)conduct social simulations to ensure high-quality,human-valued responses from LLMs. Moreover,Liu et al. (2024c) introduce a multi-step prompt-ing including question analysis, answer guidanceand safe answer production in their response gen-eration pipeline. Guo et al. (2024a) enhance theLLMs outputs quality by implementing retrieval-augmented ICL and providing LLMs with relevantdocuments. To ensure LLMs provide responsesaligned with human values, Sun et al. (2024b)and Wang et al. (2024a) conduct principle-drivenprompting, guiding LLMs with well-crafted anddetailed principles. Besides, Lupidi et al. (2024)propose Source2Synth, which takes as input a cus-tom data source and produces synthetic data pointswith intermediate reasoning steps grounded in real-world sources.",
  "Label": "Label is an important component of the traditionalclassification task in NLP. Nowadays, many re-searchers focus on automating label annotationwith the assistance of LLMs Yadav et al. (2024).Chen et al. (2024a) introduce an innovative ap-proach where we employ LLMs as expert annota-tors for event extraction. Martorana et al. (2024b)propose a method to support metadata enrichmentusing topic annotations generated by several LLMs.Both Wu et al. (2024a) and Ahmed et al. (2024)explores the potential of large language models(LLMs) as automated data annotators to improveefficiency and consistency in label annotation tasks.One interesting work from Li et al. (2023b) pro-poses CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Moreover, Tekumalla and Banda (2023) evalu-ate the utilization of LLM in labeling COVID-19vaccine-related tweets, with the purpose of com-paring performance against human annotators. Toaddress the potential limitation of LLMs annota-tion, Trnberg (2024) propose a comprehensive setof standards and best practices for their reliable,reproducible, and ethical use. Additionally, thereare also some works that utilize LLMs to improvethe original annotation made by human annota-tors Laskar et al. (2023); Flamholz et al. (2024);Wang et al. (2024d). To reduce costs, Schmidt et al.(2024) argue that domain-agnostic knowledge fromLMs, such as linguistic understanding, is sufficientto create a well-curated dataset.",
  "Rationale": "The rationale reflects the detailed thought processand reasoning pathway an individual follows whensolving a given question, being considered valuableauxiliary information for the final answer predic-tion. In early studies (Ling et al., 2017; Cobbeet al., 2021; Wei et al., 2022), the rationale in eachdataset was annotated by human experts, signifi-cantly limiting its availability and scalability. Ko-jima et al. (2022) initially confirm the efficacy ofthe chain-of-thought (CoT) approach in LLMs andboosting LLMs reasoning through the integrationof self-generated rationales.Rationale Structure.Following Kojima et al. (2022), there is a notable interest in abstracting thereasoning process of LLMs into diverse structuresand format, including trees (Hao et al., 2023; Yaoet al., 2024), graphs (Besta et al., 2024; Yao et al.,2023), tables (Wang et al., 2024f), programs (Chenet al., 2023e), recursion (Qi et al., 2023), and con-cepts (Tan et al., 2023).Rationale Quality. To produce high-quality andfine-grained rationale, diverse methodologies havebeen employed. Wang et al. (2022a) prompt frozenLLMs to produce choice-specific rationales to elu-cidate each choice in a sample. Wang et al. (2023b)employ contrastive decoding to foster more plau-sible rationales, taking into account gold-standardanswers. Liu et al. (2023a) curate meticulouslydesigned prompts to derive high-quality rationalesfrom GPT-4 and construct a logical CoT instruc-tion tuning dataset. For attaining fine-grained ra-tionales, Shridhar et al. (2023) introduce SocraticCoT by decomposing the original question into aseries of subquestion-solution pairs and generat- ing CoT for them separately. Additionally, Kanget al. (2024) propose a neural reranker to acquiresupplementary relevant documents for rationalegeneration in knowledge-intensive reasoning tasks.Besides, Zhou et al. (2024) explore the potentialand limitations of using graph-based synthetic rea-soning data as training signals to enhance LLMsreasoning capabilities.Human-like Rationale. Another intriguing av-enue in synthesized rationale delves into makingthe reasoning process more human-like (Gao et al.,2023). Many studies emulate human diverse think-ing in problem-solving, sampling multiple reason-ing pathways for a given question (Gao et al., 2021;Wang et al., 2022b; Chen et al., 2023f; Liu et al.,2023c). Subsequent studies (Tong et al., 2023;Balepur et al., 2023; Ma and Du, 2023) explorethe elimination reasoning in LLMs, checking eachreasoning pathway reversely and removing the in-correct candidates. Moreover, various works (Yinet al., 2023; Liang et al., 2023; Xu et al., 2023d; Liuet al., 2023e) explore the peer collaboration and de-bate among individual LLMs to capture human-likediscussions as rationales.",
  "Pairwise Feedback": "While high-quality human feedback is proven tobe effective in aligning LLMs values and prefer-ences with us humans, recent advancements aim toautomate this pairwise feedback mechanism.Ranking with LLMs. One technique is to samplemultiple responses and have the LLM rank thesecandidates based on various criteria (Bai et al.,2022; Lee et al., 2023b; Yuan et al., 2024). Sunet al. (2023b) sample two responses from the ini-tial policy model and use the model to select thepreferred response based on a human-written prin-ciple (Sun et al., 2024b). Zhang et al. (2024b)propose a self-evaluation mechanism, generatingquestions for each response and measuring factual-ity by the LLMs confidence in the answers. To im-prove synthetic data quality, Pace et al. (2024) com-bine the Best-of-N and Worst-of-N sampling strate-gies and introduce the West-of-N approach. Theyconstructed data pairs by identifying the best- andworst-scored responses according to a pre-trainedpreference model. In robotics, Zeng et al. (2024)iteratively update the reward function with the self-ranked responses from LLMs, enhancing learningefficiency without human supervision.Direct Construction.Another effort towards",
  ": The examples for LLM-based annotation generation": "automatic pairwise feedback generation involvesdirectly generating responses of various quali-ties (Feng et al., 2024; Lee et al., 2024a). To ac-complish this, they typically have to make variousassumptions when determining the factors influ-encing response quality. For example, Kim et al.(2023b) assume larger LLM with more shots willgive better responses and produce synthetic pairsbased on this. Tong et al. (2024b) follow the ruleof thumb that the supervised fine-tuning modelwill perform better than its unfinetuned base model.Adhere to this criterion, they start with a few seeddata, iteratively training the model and synthesiz-ing comparison data pairs. Yang et al. (2023c)create quality differences by prompting LLMs toeither follow or violate given principles. To mea-sure the response quality more subjectively, Xuet al. (2023c) introduce multiple LLMs and utilizebenchmark scores to define superiority.",
  "Textual Feedback": "Textual feedback (Pan et al., 2024) generated byLLMs typically highlights the shortcomings ofthe current output or suggests specific improve-ments, thus offering rich and valuable informationfor polishing or evaluating the generated response.Many existing works tailor appropriate promptsand instruct LLMs to generate such informativefeedback in various tasks, including question an-swering (Madaan et al., 2024; Shinn et al., 2024),machine translation (Chen et al., 2023c; Raunaket al., 2023) and hallucination detection (Yang et al.,2023d; Manakul et al., 2023). Some investigationshave explored leveraging debate and peer review asfeedback to enhance LLMs reasoning (Du et al., 2023a; Xu et al., 2023d; Cohen et al., 2023; Fuet al., 2023) and evaluation (Li et al., 2023d; Chuet al., 2024b; Ning et al., 2024) capabilities. Addi-tionally, efforts have been made to analyze reasonsfor undesired or incorrect responses produced byLLMs, thus facilitating reflection and learning fromtheir previous mistakes (Wang and Li, 2023; Anet al., 2023; Chen et al., 2023a; Tong et al., 2024a).",
  "Other Domain-Specific Data": "Distilling multi-round conversations from LLMspresents a highly cost-effective approach for con-structing high-quality dialogue datasets (Kim et al.,2023a; Xu et al., 2023b; Chen et al., 2023b; Li et al.,2024d; Wang et al., 2024c; Liang et al., 2024a)or enhancing existing ones (Zheng et al., 2023a;Chen et al., 2022; Zhou et al., 2022a; Sun et al.,2024a). In graph and tabular data, several stud-ies prompt LLMs to contextualize these structuraldata (Xiang et al., 2022; Kim et al., 2023a; Li et al.,2024b; Ronzano and Nanavati, 2024; Xiong et al.,2023b, 2024b) or distill structural insights fromraw text (Bi et al., 2024; Li et al., 2024c; Dinget al., 2024; Xiong et al., 2024a; Tuozzo, 2022).Moreover, LLMs have also been widely adoptedin the research of robotics and agents, serving asproficient data annotators to generate plans (Huanget al., 2022; Brohan et al., 2023; Rana et al., 2023;Singh et al., 2023; Lin et al., 2023a), simulationtasks (Wang et al., 2023a; Ha et al., 2023) andsupervised signal (Kwon et al., 2022; Du et al.,2023b). Besides, LLMs are acting as efficient dataannotators in various artificial intelligence domains,including multi-modal (Li et al., 2023f; Yin et al.,2024; Chen et al., 2024b; Luo et al., 2024b; Liu et al., 2024b), recommendation system (Acharyaet al., 2023; Shen et al., 2024; Wei et al., 2024;Zhang et al., 2024c), information extraction (Josi-foski et al., 2023; Jeronymo et al., 2023; Li et al.,2024a; Ma et al., 2024; Bonn et al., 2024), multi-lingual annotation (Frei and Kramer, 2023; Hamer-lik et al., 2024) and etc (Chu et al., 2024a; Bhat-tacharjee et al., 2024; Martorana et al., 2024a; Zhaoet al.).4LLM-Generated AnnotationsAssessment Effective evaluation of annotations generated byLLMs is crucial to fully harness their potential.This section focuses on two main aspects:4.1Evaluating LLM-Generated AnnotationsThis subsection explores various methods for as-sessing annotation quality, ranging from human-ledto automated approaches.General Approaches: Research has investigateddiverse methods for evaluating LLM annotations.The Turking Test by Efrat and Levy (2020), eval-uates LLMs adherence to data annotation guide-lines, with human annotators comparing LLMoutputs against benchmarks like SNLI (Bowmanet al., 2015), SQuAD (Rajpurkar et al., 2016), andNewsQA (Trischler et al., 2016). Similarly, Hon-ovich et al. (2022) manually examined the orig-inality, accuracy, and variety of datasets createdby LLMs, focusing on their response to instruc-tions. Additionally, studies such as by Alizadehet al. (2023) measure the performance of open-source LLMs against human-annotated labels intasks like relevance and topic detection.Task-Specific Evaluations: Methodologies varyby application. For instance, in knowledge graphenhancement, token ranking metrics assess LLMcontributions in fact completion. Additionally, eval-uations of counterfactual generation often utilize di-versity metrics like Self-BLEU (Chen et al., 2023g),while code generation relies on metrics such asPass@k (Nijkamp et al., 2022). In scenarios re-quiring extensive datasets, the quality of LLM-generated annotations is compared to gold standardlabels within a small, labeled subset (Zhao et al.,2021; Agrawal et al., 2022; He et al., 2023).LLM-as-a-Judge: LLM-as-a-judge (Wu et al.,2024c; Zheng et al., 2023b) is a commonly usedmethod in automatic generation evaluation. Toscale the assessment of the synthetic data or anno-tation, there are also some works that adopt LLM-as-a-judge to conduct the evaluation. (Li et al., 2024e) employ multiple LLMs to debate with eachother to evaluate the synthetic datas quality fairly,iteratively improving response quality, while creat-ing a judge LLM to select preferred responses forenhanced instruction tuning. To enhance the qualityof the synthetic instruction tuning data, Liang et al.(2024b) introduce an iterative self-enhancementparadigm (I-SHEEP). During training, they adoptLLM-as-a-judge to score the synthetic responsesand set a threshold to collect high-quality query-response pairs for the subsequent training iteration.",
  "Filtering & Selection": "Selecting high-quality annotations from numerousoptions is crucial. In this section, we categorize thefiltering and selection methods for LLM-generateddata into three types: rule-based filtering, externalsource utilization, and LLMs-driven selection.Rule-Based Methods. Rule-based methods followvarious heuristic assumptions concerning samplelength (Li et al., 2023f; Kim et al., 2023a), keywordoccurrence (Kim et al., 2023b; Zheng et al., 2023a)and specific patterns (Zhang and Yang, 2023a; Guoet al., 2024a; Ding et al., 2024) to filter low-qualityor undesiered synthetic data points. Zheng et al.(2023a); Kim et al. (2023a) establish thresholds forthe number of rounds in generated conversationsto guarantee each synthetic dialogue is informativeenough. Ho et al. (2023); Kang et al. (2024) em-ploy ground truth parsing to filter out incorrect CoTrationales within each candidate reasoning sample.To encourage diversity among the generated datapoints, Wang et al. (2022b); Lee et al. (2023a);Ding et al. (2024) utilize semantic similarity met-rics to identify and remove redundant samples.External-Source-Based Methods.There arealso many works that depend on the externalsources feedback to clean and refine syntheticdatasets (Kim et al., 2023a). With a pre-trainedreward model, Gulcehre et al. (2023); Dong et al.(2023) augment the original dataset only with sam-ples that obtain high reward values. When dis-tilling smaller models, Lin et al. (2023b); Wanget al. (2024e) meticulously select appropriate datathrough the feedback from the student models.Other approaches (Chen et al., 2023g; Zheng et al.,2023a) utilize pre-trained classification models todiscern between target and unwanted data points.LLMs-Driven Methods. The versatility of LLMshas invoked interest in leveraging LLMs them-selves to do data selection. Some approaches usesignals or features produced by LLMs, such as perplexity score (Wang et al., 2023f), confidencelevels (Wang et al., 2022b; Huang et al., 2023),and logits (Pace et al., 2024), as criteria for con-structing data selectors. Others directly promptthe LLMs for this task. For instance, Lu et al.(2023) query the target LLM to assess the qualityof generated samples. Kim et al. (2023a) lever-age ChatGPT to determine if the social common-sense knowledge is appropriately conveyed in thesynthetic dialogues. Additionally, there are alsoworks that adopt the LLMs to rank multiple can-didate annotations and utilize the top ones in thesubsequent stages (Jeronymo et al., 2023; Li et al.,2024c). In pairwise feedback synthesis, Tong et al.(2024b) task the base LLM with judging whetherone response genuinely surpasses another.Be-sides, Jiang et al. (2024b) demonstrate that filteringout correct but with high distribution shift extent(DSE) samples could also benefit the results ofself-improvement. 5LLM-Generated AnnotationsUtilizationLLM-generated annotations provide a valuable re-source of labeled data for NLP models in differentstages. Hereby we explore the methods for utiliz-ing and learning with LLM-Generated Annotations.",
  "Supervised Fine-Tuning": "Supervised fine-tuning can effectively enhancemodels specific capabilities or knowledge. In thissection, we discuss the utilization of generated an-notation for supervised fine-tuning.Self-Evolution. Huang et al. (2023) first proposethe concept of self-improve that utilizes LLMs asboth data annotators and learnable models and it-eratively fine-tune LLMs in their self-annotateddata. Wang et al. (2023e) also tune a GPT3 inthe instruction tuning dataset to improve its zero-shot generalization capability. To foster LLMsevolution, Lu et al. (2023) iteratively fine-tune theLLMs in self-refined synthetic responses. To miti-gate the distribution gap between task datasets andthe LLMs, Yang et al. (2024b) use self-distillationwhich guides fine-tuning with a distilled datasetgenerated by the model itself. Both Chen et al.(2024c) and Cheng et al. (2024) introduce a self-play mechanism, where the LLM refines its capa-bility by playing against instances of itself. More-over, Wang et al. (2024b) demonstrate that thereasoning abilities of small-scale LMs can be en-hanced through self-training, a process where mod- els learn from their own outputs.Distill Smaller Models.For efficiency issues,many studies aim to use the data generated by alarge and powerful LLM to train a flexible andaffordable smaller model. For a better instruction-following ability, many medium and small-sizedLLMs are trained on the synthetic dataset pro-duced by larger LLMs (Taori et al., 2023; Chi-ang et al., 2023b; Xu et al., 2023a). In classifi-cation tasks, Meng et al. (2022, 2023); Wang et al.(2023d) augment the original datasets and trainsmaller bidirectional attention models on them.To foster models reasoning ability, many stud-ies tune smaller models with synthetic rationalescollected from LLMs (Wang et al., 2022a; Shrid-har et al., 2023; Liu et al., 2023a; Kang et al.,2024).Other task-specific capabilities distilla-tion from LLMs include dialogue generation (Xuet al., 2023b), information extraction (Josifoskiet al., 2023; Jeronymo et al., 2023) and code gen-eration (Chaudhary, 2023; Roziere et al., 2023).Moreover, LLMs have been proven to follow ascaling law in terms of their knowledge capacity.Therefore, there is also a growing interest in distill-ing vertical and domain-specific knowledge fromLLMs, including medicine (Zhang et al., 2023;Xiong et al., 2023a), finance (Zhang and Yang,2023b) and science (Luo et al., 2023; Zhao et al.,2024), to smaller models.",
  "Alignment Tuning": "Alignment tuning methods, like RLHF (Ouyanget al., 2022), aim to align the output of LLMs withhuman intentions, ensuring they are helpful, ethical,and reliable. Synthetic data produced by LLMs arewidely adopted in these alignment approaches forreward modeling and policy training.Reward Modeling. LLMs-generated annotationscan be used to train or refine the reward modelfor better alignment. Xu et al. (2023c) proposea data curriculum method that leverages the pair-wise feedback from LLMs to calculate the sampledifficulty level and smooth LLMs learning fromsimple ones to hard ones. Kim et al. (2023b) de-sign reward model guided self-play to iterativelyimprove the reward model with synthesized datagenerated by the policy model. Pace et al. (2024)propose to maximize the probability of correctlylabeling a pair of on-policy responses to a givenquery according to the base preference model. Inrobotics, Zeng et al. (2024) learns a reward func-tion from scratch using the LLMs feedback. With synthetic data pair, Sun et al. (2023b) train an in-structable reward model to generate reward scoresbased on arbitrary human-defined principles.Policy Training. While many direct alignmentmethods (Rafailov et al., 2024; Zhao et al., 2023b)have emerged recently, some works directly ex-plore the use of annotated feedback for policy train-ing. One common strategy is to directly apply DPOwith the synthetic pairwise feedback produced byLLMs (Yuan et al., 2024; Zhang et al., 2024b;Lee et al., 2024b; Tong et al., 2024b; Lee et al.,2024a; Guo et al., 2024b). Besides, Gulcehre et al.(2023); Dong et al. (2023) leverage a pre-trainedreward model to filter low-quality synthetic dataand iteratively tune LLMs with growing datasets.Wang et al. (2024a) propose a bootstrapping self-alignment method to repeatly utilize the syntheticdata. Liu et al. (2024c) introduce the Mixture ofinsighTful Experts (MoTE) architecture, which ap-plies the mixture of experts to enhance each com-ponent of the synthetic response, markedly increas-ing alignment efficiency. With the reasoning pair-wise feedback generated by LLM itself, Pang et al.(2024a) use a modified DPO loss with an additionalnegative log-likelihood term to tune the LLM.",
  "Inference": "In-Context Learning. In-context Learning (ICL)consists of three components: a task description(or prompt), several in-context samples (or demon-stration), and the test case that needs to be inferred.Current studies have applied the annotations anddata generated by LLMs in all these componentsfor refining or augmenting. Zhou et al. (2022b) firstshowed that with a well-designed pipeline, LLMscan be human-level prompt engineers to generateaccurate task descriptions. Following them, Yanget al. (2023b); Li et al. conduct augmentation andexpansion to the original task prompt, making itmore detailed for LLMs to follow. Demonstrationaugmentation (Kim et al., 2022; Li et al., 2023c;Chen et al., 2023d; He et al., 2024) is another usefulskill to enrich and diversify the provided demonstra-tions, especially when the labeled data is limited.For the test sample, one augmentation method isto leverage LLMs to rephrase it once (Deng et al.,2023) or multiple times (Li et al., 2023a; Yanget al., 2024a). Other works study how to polish theoriginal test sample (Xi et al., 2023) or decomposeit into several sub-questions (Wang et al., 2024b).Reasoning. Reasoning plays a crucial role in en-hancing the quality and accuracy of the content generated by LLMs. One efficient manner to boostLLMs reasoning with self-generated annotationis to provide the generated rationale directly be-fore outputting the final answer/ response (Kojimaet al., 2022).To improve LLMs performancewith multiple reasoning pathways, majority vot-ing(Wang et al., 2022b; Chen et al., 2023f) andelimination(Tong et al., 2023; Balepur et al., 2023;Ma and Du, 2023) are adopted to decide the finalanswer among several possible candidates. Post-hoc editing and refining (Madaan et al., 2024; Tonget al., 2024a) is another well-studied direction toutilize textual feedback and analysis for improvingLLMs reasoning capabilities. Additionally, utiliza-tion of LLMs-generated annotations sometimes re-quires additional domain tools. For example, Chenet al. (2023e) use a program interpreter in program-of-thought (PoT) to execute the generated programand convert it to a specific answer. Besta et al.(2024) design a prompter to Build a prompt to besent to the LLM and a parser to extract informationfrom LLM thought. In tree-of-thought (ToT), Haoet al. (2023); Yao et al. (2024) build an additionalstate evaluator by designing specific prompts andrepurposing the base LLM.6Societal Impact and Future WorkIn this section, we outline LLM annotation chal-lenges, including societal implications, technicalconcerns, and bias propagation.",
  "Ethics Consideration": "One critical concern of LLM-generated annotationsis the ethics consideration, especially in high-stakesdecision-making tasks like finance (Yang et al.,2023a), jurisprudence (Cui et al., 2023), and health-care (Eloundou et al., 2023). Despite the efficiencyof LLM annotation, the lack of human insight maylead to biased and unfair results (Wu et al., 2023;Abid et al., 2021; Cheng et al., 2021; Li et al.,2023g; Beigi et al., 2024; Das et al., 2024; Shimabu-coro et al., 2024). Moreover, LLMs make humanannotator roles redundant, potentially increasingsocial disparities (Dillion et al., 2023). Future stud-ies should harmonize technological advancementswith societal consequences, including consideringsocial implications, ensuring ethical use, promotingfairness, and maintaining transparency.",
  "Model Collapse. Model collapse refers to the grad-ual performance decrease of an LLM trained onthe outputs of other LLMs (Sun et al., 2023a; Gu-": "nasekar et al., 2023; Hsieh et al., 2023; Honovichet al., 2022; Chiang et al., 2023a; Geng et al., 2023;Huang et al., 2024a). It is unavoidable since LLM-generated data is occupying the information ecosys-tem. The imitation model often replicates stylisticelements without achieving the factual precisionof superior models (Gudibande et al., 2023; Shu-mailov et al., 2023). This divergence is caused bystatistical approximation error from limited sam-ple sizes and functional approximation error fromconstrained model capacity. Both errors tend toamplify through successive training cycles (Alemo-hammad et al., 2023). Potential Solution. It is important to ensure thatthe training data is diverse and high-quality, with asignificant proportion of human-generated content.Gerstgrasser et al. (2024) avoid model collapseby accumulating real and machine-generated data.This method maintains data diversity, preventingperformance degradation across different LLMs. Hallucinations. Hallucinations in LLMs signif-icantly undermine the integrity and reliability oftheir generated annotations (Alkaissi and McFar-lane, 2023; Azamfirei et al., 2023; Chaudhary et al.,2024). Hullicinated outputs detached from factualinformation can cause the proliferation of misinfor-mation (Jiang et al., 2024a; Chen and Shu, 2023;Chen and Shu; Huang et al., 2024b). Addressinghallucinations requires refining the training pro-cess and implementing validation mechanisms forannotations through automated and manual verifi-cation (Liao and Vaughan, 2023; Pan et al., 2023;Bian et al., 2023). Moreover, the inherent opac-ity of LLMs complicates efforts to investigate thecauses of hallucinations. Potential Solution. Yang et al. (2023d) addresseshallucinations in LLMs with the Reverse Valida-tion method, detecting hallucinations at the passagelevel by constructing a query from the response andchecking for a match within the LLMs internalknowledge. Bertaglia et al. (2023) uses Chain-of-Thought (CoT) prompting and explanation genera-tion, where CoT prompting produces explanationsfor predictions, ensuring logical and verifiable out-puts. Li et al. (2023b) proposes the CoAnnotatingframework, which uses uncertainty-guided workallocation between humans and LLMs, applyingself-evaluation and entropy metrics to assess relia-bility and distribute tasks effectively. Zendel et al.(2024) propose a human-LLM connotation processfor better annotation quality. Efficiency of LLMs. Efficiency in LLMs is crucialdue to their growing size and complexity, which de-mand substantial computational resources (Wonget al., 2024). Efficient models reduce inference la-tency, vital for real-time applications, lower energyconsumption for sustainable AI practices, and cutoperational costs in cloud environments, making AImore cost-effective for researchers. Efficiency tech-niques for LLMs, such as pruning, compression,and distillation, are critical for deploying thesemodels in resource-constrained environments.Potential Solution. Pruning is an efficient tech-nique to reduce the number of parameters in anLLM. For example, Ma et al. (2023) selectively re-moves redundant neurons based on gradient infor-mation while preserving most of the LLMs capabil-ity. Mixture of Experts (MoE) is another promisingtechnique that leverages a set of expert sub-models,where only a subset of these experts is activated forany given input (Artetxe et al., 2021). Researchersalso adopt LLM Quantization to reduce the preci-sion of the numbers used to represent a modelsparameters (Xiao et al., 2023). Instead of using32-bit floating-point numbers, a quantized modelmight use 16-bit floats, 8-bit integers, or even lowerprecision. These techniques can be combined witheach other to achieve further efficiencies.",
  "Conclusion": "The exploration of LLMs for data annotation andsynthesis has revealed an exciting frontier in NLP,presenting novel solutions to longstanding chal-lenges like data scarcity, and enhancing annota-tion quality and process efficiency. This surveymeticulously reviews methodologies, applications,and hurdles associated with LLM employment, in-cluding detailed taxonomy from annotation genera-tion to utilization. It evaluates the effects of LLM-generated annotations on training machine learningmodels while addressing both technical and ethicalconcerns like bias and societal ramifications. High-lighting our novel taxonomy of LLM methodolo-gies, strategies for utilizing LLM-generated anno-tations, and a critical discussion on the challenges,this work aims to steer future progress in this cru-cial area. Additionally, we introduce a compre-hensive categorization of techniques and compileextensive benchmark datasets to support ongoingresearch endeavors, concluding with an examina-tion of persistent challenges and open questions,paving the way for future investigative pursuits inthe domain.",
  "Limitations": "Sampling Bias and Hallucination. LLMs can dis-play sampling bias, leading to incorrect or halluci-nated data, impacting the reliability and quality ofannotations for discriminative tasks.Social Bias and Ethical Dilemmas. The inher-ent biases in training data can be perpetuated andamplified by LLMs, leading to ethical concernsand the propagation of social biases through anno-tated data. This is particularly problematic in tasksrequiring fairness and impartiality.Dependence on High-Quality Data. LLMs use-fulness in generating annotations depends on large,high-quality datasets. But curating these datasets islabor-intensive, posing a scalability challenge forLLM-based annotation efforts.Complexity in Tuning and Prompt Engineering.Successfully leveraging LLMs for data annotationrequires sophisticated prompt engineering and fine-tuning techniques. This can pose a barrier to entryfor practitioners and researchers without extensiveexpertise in NLP and machine learning.Generalization and Overfitting While LLMs canbe powerful tools for annotation, theres a risk ofoverfitting to the training data, limiting their abilityto generalize to unseen data or different contexts.This is a critical limitation for discriminative taskswhere the goal is to develop models that performwell across diverse datasets and domains.Computational and Resource Requirements.The training and deployment of state-of-the-artLLMs for data annotation require substantial com-putational resources, which may not be accessibleto all researchers and organizations, thereby limit-ing widespread adoption.",
  "Acknowledgements": "The material in this presentation is supportedby the National Science Foundation (NSF) un-der grants IIS-2229461, and the U.S. Departmentof Homeland Security under Grant Award Num-ber, 17STQAC00001-08-00 and the U.S. Office ofNaval Research (ONR) under grant N00014-21-1-4002. Lu Cheng is supported by the NationalScience Foundation (NSF) Grant #2312862, NIH#R01AG091762, and a Cisco gift grant. The viewsand conclusions contained in this document arethose of the authors and should not be interpretedas necessarily representing the official policies, ei-ther expressed or implied, of the U.S. Department",
  "Toufique Ahmed, Premkumar Devanbu, ChristophTreude, and Michael Pradel. 2024. Can llms replacemanual annotation of software engineering artifacts?arXiv preprint arXiv:2408.05534": "Sina Alemohammad, Josue Casco-Rodriguez, LorenzoLuzi, Ahmed Imtiaz Humayun, Hossein Reza Babaei,Daniel LeJeune, Ali Siahkoohi, and Richard Bara-niuk. 2023. Self-consuming generative models gomad. ArXiv, abs/2307.01850. Meysam Alizadeh,Mal Kubli,Zeynab Samei,Shirin Dehghani, Juan Diego Bermeo, Maria Ko-robeynikova, and Fabrizio Gilardi. 2023.Open-source large language models outperform crowdworkers and approach chatgpt in text-annotationtasks. arXiv preprint arXiv:2307.02179.",
  "Nishant Balepur, Shramay Palta, and Rachel Rudinger.2023. Its not easy being wrong: Evaluating processof elimination reasoning in large language models.arXiv preprint arXiv:2311.07532": "Alimohammad Beigi, Zhen Tan, Nivedh Mudiam,Canyu Chen, Kai Shu, and Huan Liu. 2024. Modelattribution in machine-generated disinformation: Adomain generalization approach with supervised con-trastive learning. arXiv preprint arXiv:2407.21264. Thales Bertaglia, Stefan Huber, Catalina Goanta, Gerasi-mos Spanakis, and Adriana Iamnitchi. 2023. Closingthe loop: Testing chatgpt to generate model expla-nations to improve human labelling of sponsoredcontent on social media. In World Conference onExplainable Artificial Intelligence, pages 198213.Springer. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gersten-berger, Michal Podstawski, Lukas Gianinazzi, JoannaGajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-otr Nyczyk, et al. 2024. Graph of thoughts: Solvingelaborate problems with large language models. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1768217690.",
  "Amrita Bhattacharjee, Raha Moraffah, Joshua Gar-land, and Huan Liu. 2024. Zero-shot llm-guidedcounterfactual generation for text. arXiv preprintarXiv:2405.04793": "Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo,Huajun Chen, and Ningyu Zhang. 2024. Codekgc:Code language model for generative knowledgegraph construction.ACM Transactions on Asianand Low-Resource Language Information Process-ing, 23(3):116. Ning Bian, Peilin Liu, Xianpei Han, Hongyu Lin, Yao-jie Lu, Ben He, and Le Sun. 2023. A drop of inkmay make a million think: The spread of false in-formation in large language models. arXiv preprintarXiv:2305.04812.",
  "Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi,and Dong Yu. 2024.Scaling synthetic data cre-ation with 1,000,000,000 personas. arXiv preprintarXiv:2406.20094": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.2023. A survey on evaluation of large language mod-els. Manav Chaudhary, Harshit Gupta, and Vasudeva Varma.2024. Brainstorm@ irel at smm4h 2024: Leveragingtranslation and topical embeddings for annotation de-tection in tweets. arXiv preprint arXiv:2405.11192.",
  "Canyu Chen and Kai Shu. 2023. Can llm-generatedmisinformation be detected?arXiv preprintarXiv:2309.13788": "Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han,Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu,Wenyong Huang, Zhenguo Li, et al. 2023a. Gain-ing wisdom from setbacks:Aligning large lan-guage models via mistake analysis. arXiv preprintarXiv:2310.10477. Maximillian Chen, Alexandros Papangelis, ChenyangTao, Seokhwan Kim, Andy Rosenbaum, Yang Liu,Zhou Yu, and Dilek Hakkani-Tur. 2023b. Places:Prompting language models for social conversationsynthesis. In Findings of the Association for Compu-tational Linguistics: EACL 2023, pages 844868. Maximillian Chen, Alexandros Papangelis, ChenyangTao, Andy Rosenbaum, Seokhwan Kim, Yang Liu,Zhou Yu, and Dilek Hakkani-Tur. 2022. Weaklysupervised data augmentation through prompting fordialogue understanding. In NeurIPS 2022 Workshopon Synthetic Data for Empowering ML Research.",
  "Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-neth Heafield. 2023c.Iterative translation refine-ment with large language models. arXiv preprintarXiv:2306.03856": "Ruirui Chen, Chengwei Qin, Weifeng Jiang, andDongkyu Choi. 2024a. Is a large language modela good annotator for event extraction? In Proceed-ings of the AAAI Conference on Artificial Intelligence,volume 38, pages 1777217780. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, andHsin-Hsi Chen. 2023d. Self-icl: Zero-shot in-contextlearning with self-generated demonstrations. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 1565115662. Wenhu Chen, Xueguang Ma, Xinyi Wang, andWilliam W Cohen. 2023e.Program of thoughtsprompting: Disentangling computation from reason-ing for numerical reasoning tasks. Transactions onMachine Learning Research. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Ke-fan Xiao, Pengcheng Yin, Sushant Prakash, CharlesSutton, Xuezhi Wang, and Denny Zhou. 2023f. Uni-versal self-consistency for large language model gen-eration. arXiv preprint arXiv:2311.17311. Yunkai Chen, Qimeng Wang, Shiwei Wu, Yan Gao,Tong Xu, and Yao Hu. 2024b. Tomgpt: Reliabletext-only training approach for cost-effective multi-modal large language model. ACM Transactions onKnowledge Discovery from Data. Zeming Chen, Qiyue Gao, Antoine Bosselut, AshishSabharwal, and Kyle Richardson. 2023g. Disco: Dis-tilling counterfactuals with large language models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 55145528.",
  "Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang,Yong Dai, Lei Han, and Nan Du. 2024. Self-playingadversarial language game enhances llm reasoning.arXiv preprint arXiv:2404.10642": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023a. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgptquality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023b. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023), 2(3):6.",
  "Wang, Chenjie Gu, et al. 2023.Reinforced self-training (rest) for language modeling. arXiv preprintarXiv:2308.08998": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Ce-sar Teodoro Mendes, Allison Del Giorno, SivakanthGopi, Mojan Javaheripi, Piero C. Kauffmann, Gus-tavo de Rosa, Olli Saarikivi, Adil Salim, S. Shah,Harkirat Singh Behl, Xin Wang, Sbastien Bubeck,Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, andYuan-Fang Li. 2023. Textbooks are all you need.ArXiv, abs/2306.11644.",
  "Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xi-aoying Zhang, Zhaoran Wang, and Yang Liu. 2024a.Human-instruction-free llm self-alignment with lim-ited samples. arXiv preprint arXiv:2401.06785": "Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu,Misha Khalman, Felipe Llinares, Alexandre Rame,Thomas Mesnard, Yao Zhao, Bilal Piot, et al. 2024b.Direct language model alignment from online ai feed-back. arXiv preprint arXiv:2402.04792. Himanshu Gupta,Kevin Scaria,Ujjwala Anan-theswaran,ShreyasVerma,MihirParmar,Saurabh Arjun Sawant, Swaroop Mishra, andChitta Baral. 2023. Targen: Targeted data gener-ation with large language models. arXiv preprintarXiv:2310.17876.",
  "Huy Ha, Pete Florence, and Shuran Song. 2023. Scalingup and distilling down: Language-guided robot skillacquisition. In Conference on Robot Learning, pages37663777. PMLR": "Endre Hamerlik, Marek uppa, Miroslav Bltk, JozefKubk, Martin Takc, Marin imko, and AndrejFindor. 2024. Chatgpt as your n-th annotator: Exper-iments in leveraging large language models for socialscience text annotation in slovak language. In Pro-ceedings of the 4th Workshop on Computational Lin-guistics for the Political and Social Sciences: Longand short papers, pages 8189. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, ZhenWang, Daisy Wang, and Zhiting Hu. 2023.Rea-soning with language model is planning with worldmodel. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 81548173.",
  "Chase Harrison. 2022. Langchain": "Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhi-heng Xi, Tao Gui, Qi Zhang, and Xuanjing Huang.2024. Self-demos: Eliciting out-of-demonstrationgeneralizability in large language models.arXivpreprint arXiv:2404.00884. Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin,Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, NanDuan, Weizhu Chen, et al. 2023. Annollm: Makinglarge language models to be better crowdsourcedannotators. arXiv preprint arXiv:2303.16854. Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.Large language models are reasoning teachers. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1485214882.",
  "Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. 2022. Unnatural instructions: Tuning lan-guage models with (almost) no human labor. arXivpreprint arXiv:2212.09689": "Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,Ruobing Xie, Julian McAuley, and Wayne XinZhao. 2023. Large language models are zero-shotrankers for recommender systems. arXiv preprintarXiv:2305.08845. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.2023. Distilling step-by-step! outperforming largerlanguage models with less training data and smallermodel sizes. arXiv preprint arXiv:2305.02301.",
  "Baixiang Huang, Canyu Chen, and Kai Shu. 2024b. Canlarge language models identify authorship? arXivpreprint arXiv:2403.08213": "Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, XuezhiWang, Hongkun Yu, and Jiawei Han. 2023. Largelanguage models can self-improve. In Proceedingsof the 2023 Conference on Empirical Methods inNatural Language Processing, pages 10511068. Wenlong Huang, Pieter Abbeel, Deepak Pathak, andIgor Mordatch. 2022. Language models as zero-shotplanners: Extracting actionable knowledge for em-bodied agents. In International Conference on Ma-chine Learning, pages 91189147. PMLR. Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, andRodrigo Nogueira. 2023. Inpars-v2: Large languagemodels as efficient dataset generators for informationretrieval. arXiv preprint arXiv:2301.01820. Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan Liu.2024a. Disinformation detection: An evolving chal-lenge in the age of llms.In Proceedings of the2024 SIAM International Conference on Data Mining(SDM), pages 427435. SIAM.",
  "Chunyang Jiang, Chi-min Chan, Wei Xue, Qifeng Liu,and Yike Guo. 2024b. Importance weighting canhelp large language models self-improve.arXivpreprint arXiv:2408.09849": "Martin Josifoski, Marija Sakota, Maxime Peyrard, andRobert West. 2023. Exploiting asymmetry for syn-thetic training data generation: Synthie and the caseof information extraction. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 15551574. Minki Kang,Seanie Lee,Jinheon Baek,KenjiKawaguchi, and Sung Ju Hwang. 2024. Knowledge-augmented reasoning distillation for small languagemodels in knowledge-intensive tasks. Advances inNeural Information Processing Systems, 36. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, TaeukKim, Kang Min Yoo, and Sang-goo Lee. 2022.Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration gen-erator. arXiv preprint arXiv:2206.08082. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras,Malihe Alikhani, Gunhee Kim, et al. 2023a. Soda:Million-scale dialogue distillation with social com-monsense contextualization. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1293012949. Sungdong Kim, Sanghwan Bae, Jamin Shin, SoyoungKang, Donghyun Kwak, Kang Yoo, and MinjoonSeo. 2023b. Aligning large language models throughsynthetic feedback. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1367713700. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Abdullatif Kksal, Timo Schick, Anna Korhonen, andHinrich Schuetze. Longform: Effective instructiontuning with reverse instructions. In ICLR 2024 Work-shop on Navigating and Addressing Data Problemsfor Foundation Models.",
  "Minae Kwon, Sang Michael Xie, Kalesha Bullard, andDorsa Sadigh. 2022. Reward design with languagemodels. In The Eleventh International Conferenceon Learning Representations": "Daniil Larionov, Artem Shelmanov, Elena Chistova, andIvan Smirnov. 2019. Semantic role labeling with pre-trained language models for known and unknownpredicates. In Proceedings of the International Con-ference on Recent Advances in Natural LanguageProcessing (RANLP 2019), pages 619628. Md Tahmid Rahman Laskar, Mizanur Rahman, IsratJahan, Enamul Hoque, and Jimmy Huang. 2023. Canlarge language models fix data annotation errors? anempirical study using debatepedia for query-focusedtext summarization. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1024510255. Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White,and Sujay Jauhar. 2023a. Making large languagemodels better data creators. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1534915360. Harrison Lee, Samrat Phatale, Hassan Mansoor, KellieLu, Thomas Mesnard, Colton Bishop, Victor Car-bune, and Abhinav Rastogi. 2023b. Rlaif: Scalingreinforcement learning from human feedback with aifeedback. arXiv preprint arXiv:2309.00267. Kyungjae Lee, Dasol Hwang, Sunghyun Park, Young-soo Jang, and Moontae Lee. 2024a. Reinforcementlearning from reflective feedback (rlrf): Aligning andimproving llms via fine-grained self-reflection. arXivpreprint arXiv:2403.14238.",
  "Dawei Li, Zhen Tan, Tianlong Chen, and Huan Liu.2024b. Contextualization distillation from large lan-guage model for knowledge graph completion. arXivpreprint arXiv:2402.01729": "Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik,Sunkwon Yun, Joseph Lee, Aaron Chacko, BojianHou, Duy Duong-Tran, Ying Ding, et al. 2024c.Dalk: Dynamic co-augmentation of llms and kg toanswer alzheimers disease questions with scientificliterature. arXiv preprint arXiv:2405.04819. Guohao Li, Hasan Hammoud, Hani Itani, DmitriiKhizbullin, and Bernard Ghanem. 2024d. Camel:Communicative agents for\" mind\" exploration oflarge language model society. Advances in NeuralInformation Processing Systems, 36. Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan,Nancy Chen, Zhengyuan Liu, and Diyi Yang. 2023b.Coannotating: Uncertainty-guided work allocationbetween human and large language models for dataannotation. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 14871505.",
  "Ruosen Li, Teerth Patel, and Xinya Du. 2023d.Prd: Peer rank and discussion improve large lan-guage model based evaluations.arXiv preprintarXiv:2307.02762": "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, OmerLevy, Luke Zettlemoyer, Jason E Weston, and MikeLewis. 2023e. Self-alignment with instruction back-translation. In The Twelfth International Conferenceon Learning Representations. Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, BinFu, Guosheng Lin, Chunhua Shen, Ling Chen, andYunchao Wei. 2023f. Stablellava: Enhanced visualinstruction tuning with synthesized image-dialoguedata. arXiv preprint arXiv:2308.10253.",
  "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and YingWang. 2023g. A survey on fairness in large languagemodels. arXiv preprint arXiv:2308.10149": "Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang,Linkun Sun, Bihui Yu, Conghui He, and Wen-tao Zhang. 2024a. Synth-empathy: Towards high-quality synthetic empathy data.arXiv preprintarXiv:2407.21669. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, andShuming Shi. 2023. Encouraging divergent thinkingin large language models through multi-agent debate.arXiv preprint arXiv:2305.19118. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng,Jiawei Guo, Xinrun Du, Zhenzhu Yang, JiahengLiu, Chenghua Lin, Lei Ma, et al. 2024b. I-sheep:Self-alignment of llm from scratch through an iter-ative self-enhancement paradigm.arXiv preprintarXiv:2408.08072.",
  "Yen-Ting Lin, Alexandros Papangelis, Seokhwan Kim,Sungjin Lee, Devamanyu Hazarika, Mahdi Namazi-far, Di Jin, Yang Liu, and Dilek Hakkani-Tur. 2023b": "Selective in-context data augmentation for intent de-tection using pointwise v-information. In Proceed-ings of the 17th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 14631476. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion: Learning to solve and explain algebraic wordproblems. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 158167. Hanmeng Liu, Zhiyang Teng, Leyang Cui, ChaoliZhang, Qiji Zhou, and Yue Zhang. 2023a. Logicot:Logical chain-of-thought instruction tuning. In The2023 Conference on Empirical Methods in NaturalLanguage Processing.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b. Visual instruction tuning. arXiv preprintarXiv:2304.08485": "Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, YanzheZhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, DiyiYang, Denny Zhou, et al. 2024a. Best practices andlessons learned on synthetic data for language models.arXiv preprint arXiv:2404.07503. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu,Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2023c.Plan, verify and switch: Integrated reasoning withdiverse x-of-thoughts. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 28072822. Yang Liu, Yuanshun Yao, Jean-Francois Ton, XiaoyingZhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,Muhammad Faaiz Taufiq, and Hang Li. 2023d. Trust-worthy llms: a survey and guideline for evaluatinglarge language models alignment. arXiv preprintarXiv:2308.05374. Zheng Liu, Hao Liang, Wentao Xiong, Qinhan Yu, Con-ghui He, Bin Cui, and Wentao Zhang. 2024b. Syn-thvlm: High-efficiency and high-quality syntheticdata for vision language models.arXiv preprintarXiv:2407.20756. Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Ji-ahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang,Qun Liu, et al. 2024c.Mixture of insightful ex-perts (mote): The synergy of thought chains andexpert mixtures in self-alignment. arXiv preprintarXiv:2405.00557.",
  "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and DiyiYang. 2023e. Dynamic llm-agent network: An llm-agent collaboration framework with agent team opti-mization. arXiv preprint arXiv:2310.02170": "Jianqiao Lu, Wanjun Zhong, Wenyong Huang, YufeiWang, Fei Mi, Baojun Wang, Weichao Wang, LifengShang, and Qun Liu. 2023. Self: Language-drivenself-evolution for large language model.arXivpreprint arXiv:2310.00533. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-guang Lou, Chongyang Tao, Xiubo Geng, QingweiLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-ardmath: Empowering mathematical reasoning forlarge language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583. Man Luo, Christopher J Warren, Lu Cheng, Haidar MAbdul-Muhsin, and Imon Banerjee. 2024a. Assess-ing empathy in large language models with real-world physician-patient interactions. arXiv preprintarXiv:2405.16402. Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin,Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang,Pengpeng Zeng, Lianli Gao, et al. 2024b. Mmevol:Empowering multimodal large language models withevol-instruct. arXiv preprint arXiv:2409.05840. Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, JaneDwivedi-Yu, Jason Weston, Jakob Foerster, RobertaRaileanu, and Maria Lomeli. 2024. Source2synth:Synthetic data generation and curation grounded inreal data sources. arXiv preprint arXiv:2409.08239.",
  "Potsawee Manakul, Adian Liusie, and Mark JF Gales.2023. Selfcheckgpt: Zero-resource black-box hal-lucination detection for generative large languagemodels. arXiv preprint arXiv:2303.08896": "Margherita Martorana, Tobias Kuhn, Lise Stork, andJacco van Ossenbruggen. 2024a.Text classifica-tion of column headers with a controlled vocabu-lary: leveraging llms for metadata enrichment. arXivpreprint arXiv:2403.00884. Margherita Martorana, Tobias Kuhn, Lise Stork, andJacco van Ossenbruggen. 2024b. Zero-shot topicclassification of column headers: Leveraging llmsfor metadata enrichment. In Knowledge Graphs inthe Age of Language Models and Neuro-Symbolic AI,pages 5266. IOS Press. Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.2022. Generating training data with language mod-els: Towards zero-shot language understanding. Ad-vances in Neural Information Processing Systems,35:462477. Yu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang,Tarek Abdelzaher, and Jiawei Han. 2023.Tun-ing language models as training data generators foraugmentation-enhanced few-shot learning. In Inter-national Conference on Machine Learning, pages2445724477. PMLR. Bonan Min,Hayley Ross,Elior Sulem,AmirPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,Eneko Agirre, Ilana Heintz, and Dan Roth. 2021.Recent advances in natural language processing vialarge pre-trained language models: A survey. ACMComputing Surveys.",
  "Ines Montani and Matthew Honnibal. 2018. Prodigy: Anew annotation tool for radically efficient machineteaching. Artificial Intelligence, to appear": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, HuanWang, Yingbo Zhou, Silvio Savarese, and CaimingXiong. 2022. Codegen: An open large languagemodel for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474. Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao,Zhen-Hui Liu, Yu Wang, Ming Pang, and Li Yuan.2024. Peer-review-in-llms: Automatic evaluationmethod for llms in open-environment. arXiv preprintarXiv:2402.01830.",
  "Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho,He He, Sainbayar Sukhbaatar, and Jason Weston.2024a. Iterative reasoning preference optimization.arXiv preprint arXiv:2404.19733": "Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong,Bolun Zhang, Yanfeng Wang, and Siheng Chen.2024b. Self-alignment of large language models viamonopolylogue-based social scene simulation. arXivpreprint arXiv:2402.05699. Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu,Di Jin, Qifan Wang, and Lifu Huang. 2023. The artof socratic questioning: Recursive thinking with largelanguage models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 41774199. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2024. Direct preference optimization: Your languagemodel is secretly a reward model. Advances in Neu-ral Information Processing Systems, 36.",
  "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016.Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprintarXiv:1606.05250": "Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. 2023. Say-plan: Grounding large language models using 3dscene graphs for scalable robot task planning. In 7thAnnual Conference on Robot Learning. Vikas Raunak, Amr Sharaf, Yiren Wang, HanyAwadalla, and Arul Menezes. 2023. Leveraging gpt-4 for automatic translation post-editing. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 1200912024.",
  "Francesco Ronzano and Jay Nanavati. 2024. Towardsontology-enhanced representation learning for largelanguage models. arXiv preprint arXiv:2405.20527": "Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, et al. 2023.Code llama: Open foundation models for code. arXivpreprint arXiv:2308.12950. MaximilianSchmidt,AndreaBartezzaghi,andNgoc Thang Vu. 2024. Prompting-based syntheticdata generation for few-shot question answering. InProceedings of the 2024 Joint International Con-ference on Computational Linguistics, LanguageResources and Evaluation (LREC-COLING 2024),pages 1316813178. Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu,and Xi Xiao. 2024. Pmg: Personalized multimodalgeneration with large language models. In Proceed-ings of the ACM on Web Conference 2024, pages38333843.",
  "Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, YarinGal, Nicolas Papernot, and Ross Anderson. 2023.The curse of recursion: Training on generated datamakes models forget. ArXiv, abs/2305.17493": "Ishika Singh, Valts Blukis, Arsalan Mousavian, AnkitGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: Generating situated robot task plans usinglarge language models. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA),pages 1152311530. IEEE. Feifan Song, Bowen Yu, Hao Lang, Haiyang Yu, FeiHuang, Houfeng Wang, and Yongbin Li. 2024a. Scal-ing data diversity for fine-tuning language models inhuman alignment. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1435814369.",
  "Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, FeiHuang, Yongbin Li, and Houfeng Wang. 2023. Pref-erence ranking optimization for human alignment.arXiv preprint arXiv:2306.17492": "Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, FeiHuang, Yongbin Li, and Houfeng Wang. 2024b. Pref-erence ranking optimization for human alignment. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1899018998. Renliang Sun, Mengyuan Liu, Shiping Yang, Rui Wang,Junqing He, and Jiaxing Zhang. 2024a. Fostering nat-ural conversation in large language models with nico:a natural interactive conversation dataset.ArXiv,abs/2408.09330.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Ramya Tekumalla and Juan M Banda. 2023. Lever-aging large language models and weak supervisionfor social media data annotation: an evaluation usingcovid-19 self-reported vaccination tweets. In Interna-tional Conference on Human-Computer Interaction,pages 356366. Springer.",
  "Arun James Thirunavukarasu, Darren Shu Jeng Ting,Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large languagemodels in medicine. Nature Medicine, pages 111": "Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, FeiTeng, and Jingbo Shang. 2024a. Can llms learn fromprevious mistakes? investigating llms errors to boostfor reasoning. arXiv preprint arXiv:2403.20046. Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang,Simeng Han, Zi Lin, Chengsong Huang, JiaxinHuang, and Jingbo Shang. 2024b. Optimizing lan-guage models reasoning abilities with weak supervi-sion. arXiv preprint arXiv:2405.04086. Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang,Zi Lin, Simeng Han, and Jingbo Shang. 2023. Elimi-nating reasoning via inferring with planning: A newframework to guide llms non-linear thinking. arXivpreprint arXiv:2310.12342.",
  "Somin Wadhwa, Silvio Amir, and Byron C Wallace.2023. Revisiting relation extraction in the era of largelanguage models. arXiv preprint arXiv:2305.05003": "Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. arXiv preprintarXiv:1804.07461. Danqing Wang and Lei Li. 2023. Learning from mis-takes via cooperative study assistant for large lan-guage models. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 1066710685. Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin,Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, YataoBian, Tingyang Xu, et al. 2024a. Step-on-feet tun-ing: Scaling self-alignment of llms via bootstrapping.arXiv preprint arXiv:2402.07610. Hongru Wang, Boyang Xue, Baohang Zhou, TianhuaZhang, Cunxiang Wang, Guanhua Chen, HuiminWang, and Kam-fai Wong. 2024b. Self-dc: Whento retrieve and when to generate? self divide-and-conquer for compositional unknown questions. arXivpreprint arXiv:2402.13514. Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shrid-har, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu,and Xiaolong Wang. 2023a. Gensim: Generatingrobotic simulation tasks via large language models.In The Twelfth International Conference on LearningRepresentations. PeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,and Xiang Ren. 2022a. Pinto: Faithful language rea-soning using prompt-generated rationales. In TheEleventh International Conference on Learning Rep-resentations.",
  "Peifeng Wang, Zhengyang Wang, Zheng Li, YifanGao, Bing Yin, and Xiang Ren. 2023b.Scott:Self-consistent chain-of-thought distillation. arXivpreprint arXiv:2305.01879": "Ruida Wang, Wangchunshu Zhou, and MrinmayaSachan. 2023c. Lets synthesize step by step: It-erative dataset synthesis with large language modelsby extrapolating errors from small models. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 1181711831. Song Wang, Zhen Tan, Ruocheng Guo, and Jundong Li.2023d. Noise-robust fine-tuning of pretrained lan-guage models via external guidance. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 1252812540.",
  "Song Wang, Peng Wang, Tong Zhou, Yushun Dong,Zhen Tan, and Jundong Li. 2024c. Ceb: Compo-sitional evaluation benchmark for fairness in largelanguage models. arXiv preprint arXiv:2407.02408": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022b. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Yifan Wang, David Stevens, Pranay Shah, WenwenJiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boy-ing Gong, Daniel Lee, et al. 2024d. Model-in-the-loop (milo): Accelerating multimodal ai data annota-tion with llms. arXiv preprint arXiv:2409.10702. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. 2023e. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508.",
  "Yue Wang, Haoke Zhang, Juntao Li, Jinxiong Chang,Qishen Zhang, Zhongyi Liu, Guannan Zhang, andMin Zhang. 2023f. Sass: Self-alignment with semi-supervised instruction data generation": "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xing-shan Zeng, Wenyong Huang, Lifeng Shang, XinJiang, and Qun Liu. 2023g.Aligning large lan-guage models with human: A survey. arXiv preprintarXiv:2307.12966. Zifeng Wang, Chun-Liang Li, Vincent Perot, Long TLe, Jin Miao, Zizhao Zhang, Chen-Yu Lee, andTomas Pfister. 2024e. Codeclm: Aligning languagemodels with tailored synthetic data. arXiv preprintarXiv:2404.05875. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Mar-tin Eisenschlos, Vincent Perot, Zifeng Wang, LeslyMiculicich, Yasuhisa Fujii, Jingbo Shang, Chen-YuLee, et al. 2024f. Chain-of-table: Evolving tables inthe reasoning chain for table understanding. arXivpreprint arXiv:2401.04398. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837.",
  "graph augmentation for recommendation. In Pro-ceedings of the 17th ACM International Conferenceon Web Search and Data Mining, pages 806815": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander M. Rush. 2020. Hug-gingfaces transformers: State-of-the-art natural lan-guage processing.",
  "Jianfei Wu, Xubin Wang, and Weijia Jia. 2024a. En-hancing text annotation through rationale-drivencollaborative few-shot prompting. arXiv preprintarXiv:2409.09615": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-badur, David Rosenberg, and Gideon Mann. 2023.Bloomberggpt: A large language model for finance.arXiv preprint arXiv:2303.17564. Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen,Qihui Zhang, Yao Wan, Tianyi Zhou, XiangliangZhang, Jianfeng Gao, Chaowei Xiao, et al. 2024b.Unigen: A unified framework for textual dataset gen-eration using large language models. arXiv preprintarXiv:2406.18966. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu,Yuandong Tian, Jiantao Jiao, Jason Weston, and Sain-bayar Sukhbaatar. 2024c. Meta-rewarding languagemodels: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594. Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng,Songyang Gao, Jia Liu, Tao Gui, Qi Zhang, andXuan-Jing Huang. 2023. Self-polish: Enhance rea-soning in large language models via problem refine-ment. In Findings of the Association for Compu-tational Linguistics: EMNLP 2023, pages 1138311406. Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, EricXing, and Zhiting Hu. 2022. Asdot: Any-shot data-to-text generation with pretrained language models.In Findings of the Association for ComputationalLinguistics: EMNLP 2022, pages 18861899. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,Julien Demouth, and Song Han. 2023. Smoothquant:Accurate and efficient post-training quantization forlarge language models. In International Conferenceon Machine Learning, pages 3808738099. PMLR.",
  "Siheng Xiong, Ali Payani, Ramana Kompella, andFaramarz Fekri. 2024a.Large language mod-els can learn temporal reasoning.arXiv preprintarXiv:2401.06853": "Siheng Xiong, Yuan Yang, Faramarz Fekri, andJames Clayton Kerce. 2023b. TILP: Differentiablelearning of temporal logical rules on knowledgegraphs. In The Eleventh International Conference onLearning Representations. Siheng Xiong, Yuan Yang, Ali Payani, James C Kerce,and Faramarz Fekri. 2024b. Teilp: Time predictionover knowledge graphs via logical reasoning.InProceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1611216119. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang. 2023a.Wizardlm: Empowering large lan-guage models to follow complex instructions. arXivpreprint arXiv:2304.12244. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.2023b.Baize: An open-source chat model withparameter-efficient tuning on self-chat data. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 62686278. Canwen Xu, Corby Rosset, Luciano Del Corro,Shweti Mahajan, Julian McAuley, Jennifer Neville,Ahmed Hassan Awadallah, and Nikhil Rao. 2023c.Contrastive post-training large language models ondata curriculum. arXiv preprint arXiv:2310.02263. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yun-tian Deng, Radha Poovendran, Yejin Choi, andBill Yuchen Lin. 2024. Magpie: Alignment datasynthesis from scratch by prompting aligned llmswith nothing. arXiv preprint arXiv:2406.08464. Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dong-fang Li, Min Zhang, and Yuxiang Wu. 2023d. To-wards reasoning in large language models via multi-agent peer review collaboration.arXiv preprintarXiv:2311.08152.",
  "Jinghan Yang, Shuming Ma, and Furu Wei. 2023b.Auto-icl: In-context learning without human supervi-sion. arXiv preprint arXiv:2311.09263": "Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng,and Yuandong Tian. 2023c. Rlcd: Reinforcementlearning from contrastive distillation for lm align-ment. In The Twelfth International Conference onLearning Representations. Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023d.A new benchmark and reverse validation method forpassage-level hallucination detection. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 38983908.",
  "Yao Yao, Zuchao Li, and Hai Zhao. 2023.Be-yond chain-of-thought, effective graph-of-thoughtreasoning in large language models. arXiv preprintarXiv:2305.16582": "Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-tao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022a. Zerogen: Efficient zero-shot learning viadataset generation. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1165311669. Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,Tao Yu, and Lingpeng Kong. 2022b. Progen: Pro-gressive zero-shot dataset generation via in-contextfeedback. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, pages 36713683. Zhangyue Yin, Qiushi Sun, Cheng Chang, QipengGuo, Junqi Dai, Xuan-Jing Huang, and Xipeng Qiu.2023. Exchange-of-thought: Enhancing large lan-guage model capabilities through cross-model com-munication. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1513515153. Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi,Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiy-ong Wang, Lu Sheng, Lei Bai, et al. 2024. Lamm:Language-assisted multi-modal instruction-tuningdataset, framework, and benchmark. Advances inNeural Information Processing Systems, 36. Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-WooLee, and Woomyoung Park. 2021. Gpt3mix: Lever-aging large-scale language models for text augmen-tation. In Findings of the Association for Computa-tional Linguistics: EMNLP 2021, pages 22252239. Wenhao Yu, Dan Iter, Shuohang Wang, YichongXu, Mingxuan Ju, Soumya Sanyal, ChenguangZhu, Michael Zeng, and Meng Jiang. 2022. Gen-erate rather than retrieve:Large language mod-els are strong context generators.arXiv preprintarXiv:2209.10063.",
  "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.2024.Self-rewarding language models.arXivpreprint arXiv:2401.10020": "Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu,Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023.Large language models meetnl2code: A survey. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 74437464. Oleg Zendel, J Shane Culpepper, Falk Scholer, and PaulThomas. 2024. Enhancing human annotation: Lever-aging large language models and efficient batch pro-cessing. In Proceedings of the 2024 Conference onHuman Information Interaction and Retrieval, pages340345.",
  "Yuwei Zeng, Yao Mu, and Lin Shao. 2024. Learningreward for robot skills using large language modelsvia self-alignment. arXiv preprint arXiv:2405.07162": "Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, RuiZhao, Yong Jiang, and Fei Tan. 2024a. Balancingspeciality and versatility: a coarse to fine frameworkfor supervised fine-tuning large language model. InAnnual Meeting of the Association for ComputationalLinguistics. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhi-hong Chen, Guiming Chen, Jianquan Li, XiangboWu, Zhang Zhiyi, Qingying Xiao, et al. 2023. Hu-atuogpt, towards taming language model to be a doc-tor. In Findings of the Association for ComputationalLinguistics: EMNLP 2023, pages 1085910885. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou,Lifeng Jin, Linfeng Song, Haitao Mi, and HelenMeng. 2024b. Self-alignment for factuality: Mitigat-ing hallucinations in llms via self-evaluation. arXivpreprint arXiv:2402.09267.",
  "Xuanyu Zhang and Qing Yang. 2023a. Self-qa: Unsu-pervised knowledge guided language model align-ment. arXiv preprint arXiv:2305.11952": "Xuanyu Zhang and Qing Yang. 2023b. Xuanyuan 2.0:A large chinese financial chat model with hundredsof billions parameters. In Proceedings of the 32ndACM International Conference on Information andKnowledge Management, pages 44354439. Chenyang Zhao, Xueying Jia, Vijay Viswanathan, Gra-ham Neubig, and Tongshuang Wu. Self-guide: Bettertask-specific instruction following via self-syntheticfinetuning. In First Conference on Language Model-ing. Haiteng Zhao, Shengchao Liu, Ma Chang, HannanXu, Jie Fu, Zhihong Deng, Lingpeng Kong, andQi Liu. 2024. Gimlet: A unified graph-text model forinstruction-based molecule zero-shot learning. Ad-vances in Neural Information Processing Systems,36. Mengjie Zhao, Fei Mi, Yasheng Wang, Minglei Li, XinJiang, Qun Liu, and Hinrich Schtze. 2021. Lm-turk: Few-shot learners as crowdsourcing workersin a language-model-as-a-service framework. arXivpreprint arXiv:2112.07522. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, ChenYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023a. Asurvey of large language models.",
  "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,Mohammad Saleh, and Peter J Liu. 2023b. Slic-hf:Sequence likelihood calibration with human feed-back. arXiv preprint arXiv:2305.10425": "Chujie Zheng, Sahand Sabour, Jiaxin Wen, ZhengZhang, and Minlie Huang. 2023a. Augesc: Dialogueaugmentation with large language models for emo-tional support conversation. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 15521568. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023b.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36:4659546623. Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma,Yaochen Hu, Soumyasundar Pal, Mark Coates, BinWang, Yingxue Zhang, and Jianye Hao. 2024. En-hancing logical reasoning in large language modelsthrough graph-based synthetic data. arXiv preprintarXiv:2409.12437.",
  "ground improves dialogue response quality. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 1045010468": "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2022b. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations. He Zhu, Junyou Su, Tianle Lun, Yicheng Tao, WenjiaZhang, Zipei Fan, and Guanhua Chen. 2024. Fanno:Augmenting high-quality instruction data with open-sourced llms only. arXiv preprint arXiv:2408.01323.",
  "ALLM-assisted Tools and Software forAnnotation": "LLM-assisted annotation tools and software areinvaluable resources designed specifically to fa-cilitate the annotation process for various NLPtasks. One of their primary attributes is an intu-itive and user-friendly interface, allowing engineersand even non-technical annotators to easily workwith complex textual data. These tools are built tosupport numerous annotation types, from simple bi-nary labels to more intricate hierarchical structures.The main goal of these tools is to simplify the la-beling process, enhance the quality of the labels,and boost overall productivity in data annotation.Below, we will present a selection of the librariesand tools that support Large Language Models forthe annotation process: LangChain: LangChain (Harrison, 2022) isan open-source library1 that offers an arrayof tools designed to facilitate the construc-tion of LLM-related pipelines and workflows.This library specifically provides large lan-guage models with agents in order to interacteffectively with their environment as well asvarious external data sources. Therefore, pro-viding dynamic and contextually appropriateresponses that go beyond a single LLM call. In terms of the annotation process, their powermostly lies in the facilitation of annotationthrough the creation of a modularized struc-ture called chain. In the chaining technique, acomplex problem is broken down into smallersub-tasks. The results obtained from one ormore steps are then aggregated and utilizedas input prompts for subsequent actions in thechain.",
  "Stack AI: Stack AI (Aceituno and Rosinol,": "2022) is a paid service that offers an AI-powered data platform. It is designed explic-itly for automating business processes allow-ing them to maximize efficiency. The essenceof their platform lies in their ability to visuallydesign, test, and deploy AI workflows throughsmooth integration of Large Language Mod-els. Their user-friendly graphical interface() allows the users to create appsand workflows related to diverse tasks fromcontent creation and data labeling to conver-sational AI apps and document processing.Moreover, Stack AI utilizes weakly super-vised machine learning models to expeditethe data preparation process. : UBIAI annotation result on a pdf document.All the entities in the text of the document have beenidentified, annotated, and color-coded based on the type.This image has been borrowed from the videos providedin the UBIAI documentation (Amamou, 2021). UBIAI: UBIAI (Amamou, 2021) is a paidannotation tool that offers multilingual cloud-based solutions and services in Natural Lan-guage Processing. The company aims to aidusers in extracting valuable insights from un-structured documents. This tool not only pro-vides a user interface that facilitates manuallabeling but also offers several auto-labelingfunctionalities such as LLM-assisted zero-and few-shot labeling and model-assisted la-beling. They also provide integration to vari-ous models on huggingface (Wolf et al., 2020)as well as an environment to fine-tune differ-ent models on the users labeled data.",
  "Prodigy: Prodigy (Montani and Honnibal,": "2018), designed by the creators of spaCylibrary (Honnibal and Montani, 2017), of-fers rule-based, statistical models, and LLM-assisted methods for annotation. This tool pro-vides easy, flexible, and powerful annotationoptions such as named entity recognition, spancategorization, and classification/labeling fordifferent modalities including text, audio, andvision. Moreover, it can be easily integratedwith large language models which are capa-ble of zero- or few-shot learning, while alsooffering services and quantifiable methods forcrafting prompts to address any noisy out-comes. This tool is not open-source.",
  "CCollections of Papers on LLM for DataAnnotation": "This collection of tables provides a conciseoverview of using Large Language Models (LLMs)for data annotation, including state-of-the-art tech-niques, methodologies, and practical applications. and lists significant papers on LLM-based data annotation, detailing their methods, coretechnologies, publication venues, and links to re-sources. focuses on assessment and filter-ing of LLM-generated annotations. Tables 4 ex-plore strategies for learning with LLM-generatedannotations, covering supervised fine-tuning, align-ment tuning and inference. Each table clearly out-lines the data type, backbone, computational cost,venues, and available resources, serving as a guideto the latest in LLM-driven data annotation andits implications for the future of automated dataprocessing and machine learning research.",
  "Improving Language Model Reasoning with Self-motivated LearningPairwise FeedbackLLaMA-2Model InferenceLREC24Not Available": "Note: (Yoo et al., 2021); (Wang et al., 2023e); (Meng et al., 2023); (Wang et al., 2023f); (Li et al., 2023a); (Kksalet al.); (Yu et al., 2024); (Zhang and Yang, 2023a); (Huang et al., 2023); (Yang et al., 2024b); (Liu et al., 2024c);(Guo et al., 2024a); (Sun et al., 2024b); (Wang et al., 2024a); (Luo et al., 2024a); (Kojima et al., 2022); (Yaoet al., 2024); (Hao et al., 2023); (Besta et al., 2024); (Yao et al., 2023); (Wang et al., 2024f); (Chen et al., 2023e);(Qi et al., 2023); (Tan et al., 2023); (Wang et al., 2022a); (Wang et al., 2023b); (Liu et al., 2023a); (Shridharet al., 2023); (Kang et al., 2024); (Gao et al., 2021); (Wang et al., 2022b); (Chen et al., 2023f); (Liu et al.,2023c); (Tong et al., 2023); (Balepur et al., 2023); (Ma and Du, 2023); (Yin et al., 2023); (Liang et al., 2023);(Xu et al., 2023d); (Liu et al., 2023e); (Bai et al., 2022); (Lee et al., 2023b); (Yuan et al., 2024); (Sun et al.,2023b); (Zhang et al., 2024b); (Pace et al., 2024); (Zeng et al., 2024); (Kim et al., 2023b); (Tong et al., 2024b);(Yang et al., 2023c); (Xu et al., 2023c); (Lee et al., 2024a); (Feng et al., 2024).",
  "Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichmentMetadataChatGPTAPI CallingArxiv24Link": "Note: (Madaan et al., 2024); (Shinn et al., 2024); (Chen et al., 2023c); (Raunak et al., 2023); (Yang et al., 2023d);(Manakul et al., 2023); (Du et al., 2023a); (Xu et al., 2023d); (Cohen et al., 2023); (Fu et al., 2023); (Li et al.,2023d); (Chu et al., 2024b); (Ning et al., 2024); (Wang and Li, 2023); (An et al., 2023); (Chen et al., 2023a);(Tong et al., 2024a); (Kim et al., 2023a); (Xu et al., 2023b); (Chen et al., 2023b); (Li et al., 2024d); (Zhenget al., 2023a); (Chen et al., 2022); (Zhou et al., 2022a); (Xiang et al., 2022); (Li et al., 2024b); (Ronzano andNanavati, 2024); (Li et al., 2024c); (Ding et al., 2024); (Xiong et al., 2024a); (Tuozzo, 2022); (Huang et al.,2022); (Brohan et al., 2023); (Rana et al., 2023); (Singh et al., 2023); (Lin et al., 2023a); (Wang et al., 2023a);(Ha et al., 2023); (Kwon et al., 2022); (Du et al., 2023b); (Li et al., 2023f); (Yin et al., 2024); (Chen et al.,2024b); (Acharya et al., 2023); (Shen et al., 2024); (Wei et al., 2024); (Zhang et al., 2024c); (Josifoski et al.,2023); (Jeronymo et al., 2023); (Li et al., 2024a); (Ma et al., 2024); (Bonn et al., 2024); [53(Chu et al., 2024a);(Bhattacharjee et al., 2024); (Martorana et al., 2024a).",
  "Optimizing Language Models Reasoning Abilities with Weak SupervisionPairwise FeedbackLLaMAModel InferenceArxiv24Not Available": "Note: (Bai et al., 2022); (Kim et al., 2023a); (Kim et al., 2023b); (Zheng et al., 2023a); (Zhang and Yang, 2023a);(Guo et al., 2024a); (Ding et al., 2024); (Ho et al., 2023); (Kang et al., 2024); (Wang et al., 2022b); (Lee et al.,2023a); (Ding et al., 2024); (Gulcehre et al., 2023); (Dong et al., 2023); (Lin et al., 2023b); (Wang et al., 2024e);(Chen et al., 2023g); (Huang et al., 2023); (Pace et al., 2024); (Lu et al., 2023); (Jeronymo et al., 2023); (Liet al., 2024c); (Tong et al., 2024b).",
  "Reasoning with Language Model is Planning with World ModelRationale - TreeLLaMAModel Inference,424 GB NVIDIA A5000 GPUsEMNLP23Link": "Note: (Huang et al., 2023); (Wang et al., 2023e); (Lu et al., 2023); (Yang et al., 2024b); (Chen et al., 2024c);(Cheng et al., 2024); (Taori et al., 2023); (Chiang et al., 2023a); (Xu et al., 2023a); (Meng et al., 2022); (Menget al., 2023); (Wang et al., 2023d); (Wang et al., 2022a); (Shridhar et al., 2023); (Liu et al., 2023a); (Kanget al., 2024); (Xu et al., 2023b); (Josifoski et al., 2023); (Jeronymo et al., 2023); (Chaudhary, 2023); (Roziereet al., 2023); (Zhang et al., 2023); (Xiong et al., 2023a); (Zhang and Yang, 2023b); (Luo et al., 2023); (Zhaoet al., 2024); (Xu et al., 2023c); (Kim et al., 2023b); (Pace et al., 2024); (Zeng et al., 2024); (Sun et al., 2023b);(Yuan et al., 2024); (Zhang et al., 2024b); (Lee et al., 2024b); (Tong et al., 2024b); (Lee et al., 2024a); (Guoet al., 2024b); (Gulcehre et al., 2023); (Dong et al., 2023); (Wang et al., 2024a); (Liu et al., 2024c); (Chen et al.,2023c); (Zhou et al., 2022b); (Yang et al., 2023b); (Li et al.); (Kim et al., 2022); (Li et al., 2023c); (Chenet al., 2023d); (He et al., 2024); (Deng et al., 2023); (Li et al., 2023a); (Yang et al., 2024a); (Xi et al., 2023);(Wang et al., 2024b); (Kojima et al., 2022); (Wang et al., 2022b); (Chen et al., 2023f); (Tong et al., 2023);(Balepur et al., 2023); (Ma and Du, 2023); (Madaan et al., 2024); (Tong et al., 2024a); (Chen et al., 2023e);(Besta et al., 2024); (Hao et al., 2023)."
}