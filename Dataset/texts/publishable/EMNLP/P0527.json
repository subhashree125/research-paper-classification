{
  "Abstract": "Large Language Models (LLMs) have demon-strated remarkable potential in handling com-plex reasoning tasks by generating step-by-steprationales. Some methods have proven effec-tive in boosting accuracy by introducing ex-tra verifiers to assess these paths. However,existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize therelative merits of intermediate steps, therebylimiting the effectiveness of the feedback pro-vided. To overcome this limitation, we proposeTree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reason-ing trees via a best-first search algorithm andcollects step-level paired data for preferencetraining. Compared to traditional binary clas-sification, step-level preferences more finelycapture the nuances between reasoning steps,allowing for a more precise evaluation of thecomplete reasoning path. We empirically eval-uate Tree-PLV across a range of arithmeticand commonsense reasoning tasks, where itsignificantly outperforms existing benchmarks.For instance, Tree-PLV achieved substantialperformance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% 82.79%), MATH (17.00% 26.80%), CSQA(68.14% 72.97%), and StrategyQA (82.86% 83.25%). Additionally, our study exploresthe appropriate granularity for applying pref-erence learning, revealing that step-level guid-ance provides feedback that better aligns withthe evaluation of the reasoning process.",
  "Corresponding author": "To enhance the reliability of reasoning paths, thebest-of-N decoding strategy (Nakano et al., 2021;Askell et al., 2021; Cobbe et al., 2021) is employed,where N candidate solutions are generated by theLLMs, and the most plausible one is selected basedon specific rules (Golovneva et al., 2022; Prasadet al., 2023), such as coherence, logical consistency,and alignment with known facts. Recently, somestudies have introduced an auxiliary model, termeda verifier (Cobbe et al., 2021), to assess the qualityof candidate solutions. The training of verifiers canbe divided into outcome supervision (Cobbe et al.,2021; Yu et al., 2023a; Hosseini et al., 2024) andprocess supervision (Li et al., 2022; Lightman et al.,2023; Wang et al., 2023) based on the granularityof the supervision signal (see ). Outcomesupervision labels the entire path based on the fi-nal result, while process supervision evaluates thecorrectness of each individual step.Whether employing outcome supervision or pro-cess supervision, verifiers are typically trained us-ing binary classification (Uesato et al., 2022; Light-man et al., 2023; Wang et al., 2023), which may notalign ideally with the goals of optimizing verifiers.In the best-of-N decoding, verifiers are expectedto rank candidate paths accurately. However, bi-nary labels, offering rudimentary correct or incor-rect signals, fail to capture the relative merits ofdifferent paths. Such coarse supervisory signalsare insufficient to provide the detailed feedbacknecessary for verifiers to discern which steps aremore effective, thereby limiting the potential forfurther improvements. Moreover, the annotationsused in training, often derived from answers, in-herently contain some degree of noise. Even ifthe final answer is correct, the reasoning processmay not be entirely accurate. Unfaithful reasoningand spurious shortcuts can also lead to the correctanswer (Creswell and Shanahan, 2022; Lyu et al.,2023; Turpin et al., 2024). Consequently, trainingverifiers using binary classification is particularly",
  ": A comparison of different methods: Traditional verifiers rely on binary labels for outcome and processsupervision, whereas Tree-PLV employs preferences instead of scalar values": "vulnerable to noisy labels, which constrains theverifiers capacity to precisely validate the steps.To tackle these challenges, we propose a shiftfrom a binary to a preference-based verifier.Trained through preference learning, it ranks therelative merits of different reasoning paths, allow-ing for more nuanced partitioning than simply judg-ing them as correct or incorrect. The advantages ofadopting this step-level preference-based verifierfor ranking the reasoning paths include:",
  "Enhancing Model Explainability: The detailedfeedback provided by preference learning offersdeeper insights into the reasoning process, mov-ing beyond mere correctness on the final result": "Therefore, we introduce Tree-based PreferenceLearning Verifier (Tree-PLV), a novel method in-spired by preference learning principles.Tree-PLV transcends traditional verifiers by modelingrewards based on comparisons between paths. Ourmethod not only focuses on instance-level rewardsderived from the outcomes but also emphasizesstep-level optimization. This allows Tree-PLV toutilize intermediate steps to provide more finely grained feedback. Specifically, we employ a best-first search strategy during inference to construct areasoning tree, with the initial problem statementas the root and each step as a node. Upon devel-oping the tree, we construct our dataset by tracingpaths from the root to each leaf node. At each level,we form pairs by conducting pairwise comparisonsamong child nodes, preferring those with higherrewards. This dataset serves to train our verifierusing a ranking loss, greatly enhancing its abilityto discern subtle nuances in reasoning sequences.We conduct an empirical evaluation of Tree-PLVacross diverse reasoning tasks, focusing on arith-metic reasoning with the GSM8K (Cobbe et al.,2021) and MATH (Hendrycks et al., 2021) datasets,and commonsense reasoning on the CSQA (Talmoret al., 2018) and StrategyQA (Geva et al., 2021)datasets. We benchmark Tree-PLV against existingverifiers, including self-consistency (Wang et al.,2022) as a strong baseline. Our results indicatesubstantial performance gains across all datasets.For instance, when compared to the Mistral-7Bself-consistency baseline, our method showed thefollowing increases in accuracy: GSM8K (67.55% 82.79%), MATH (17.00% 26.80%), CSQA(68.14% 72.97%), and StrategyQA (82.86% 83.25%). Notably, Tree-PLV, when trained withdata from GSM8K, demonstrates robust general-ization to the more challenging MATH dataset.",
  "In this section, we introduce the Tree-PLV method,which leverages tree-based preference learning toadvance verification for large language models inthe context of stepwise reasoning processes. We": "Problem: Ann's favorite store was having a summer clearance... She also bought 4 tops, all at the same price. How much did each top cost? 0.75 Step 1: Ann spent $75 in total. 0.25 Step 2: She bought 5 pairs of shorts for $7 each, so she spent 5 * $7 = $35 on shorts. 0.125 Step 2: She spent $7 * 5 = $35 on 5 pairs of shorts. 0.75 Step 2: She spent $75 on 5 pairs of shorts at $7 each and 2 pairs of shoes at $10 ... 1.0 Step 3: So, the total amount she spent on 5 pairs of shorts and 2 pairs of shoes ... 0.25 Step 3: So, 5 * $7 + 2 * $10 = $75, which means 5 * 7 + 2 * 10 = 75, which means 5 * 7... 0.75 Step 3: The total amount spent on the shorts and shoes is 5 * $7 + 2 * $10 = ... Step 3: So, Ann spent $75 - (5 * $7... The answer is 5. Step 3: 5 * $7 + 2 * $10 = $35 + $2... The answer is 5. Step 3: The total amount spent...The answer is 18.75.",
  "Completion": ": The construction process of the reasoning tree. Best-first search consistently selects the child node withhighest reward for further expansion. To evaluate the quality of the i-th step, we sample N completions from it,denoted as Pi. The reward is then calculated based on the proportion of these N paths that yield the correct answer. begin by outlining the problem formulations ( 2.1).Next, we detail how to construct a reasoning treethat represents reward preferences at each step( 2.2). Finally, we describe how we gather paireddata for step-level preference learning and imple-ment this into our verifier training ( 2.3).",
  "Problem Formulations": "Following the best-of-N evaluation protocol pro-posed by Lightman et al. (2023), we generate Ncandidate solutions {y(1), y(2), . . . , y(N)} from thegenerator model for a given input x. Each solutiony consists of a sequence of steps {y1, y2, . . . , yn}.These solutions are then ranked by the verifier, andthe highest-rated one is selected as the most plausi-ble solution.",
  "Reasoning Tree Construction": "To provide precise step-level preference feedback,we implement a best-first tree search algorithmdesigned to generate paired data critical for prefer-ence learning. As depicts, our method con-structs a reasoning tree step-by-step, where eachnode represents a reasoning step. Expansion startsfrom the root of the tree at each search iteration.At step i of the tree expansion, we have apartial solution y1:i1 consisting of the previousi 1 reasoning steps. We use a reward functionR(yi|x, y1:i1) to evaluate the quality of the nextpotential step yi, given the input x and the currentpartial solution y1:i1. The tree search proceedsby expanding the most promising node at eachiteration, i.e., the node whose child (the next po-tential step) has the highest reward according to R. This guided exploration allows us to constructhigh-quality reasoning paths through the tree, pro-viding paired data for preference learning betweencompeting steps.The traditional approaches regard the correct-ness of a step as its quality, relying on metrics likeperplexity (PPL) or self-evaluation by LLMs todesign the reward function R. However, recentstudies have shown that LLMs frequently struggleto effectively recognize errors (Huang et al., 2023;Hong et al., 2023; Ren et al., 2023b), which candegrade performance. To address this, we lever-age the models look-ahead capability to assess asteps quality by its potential to lead to the correctconclusion. Specifically, to evaluate a candidatestep yi, we use the same model to simulate N sub-sequent reasoning trajectories starting from yi, de-noted as N completions Pi = {P 1i , P 2i , . . . , P Ni }.The quality of the step yi is quantified by the pro-portion of trajectories reaching the correct answer:",
  "N(1)": "where a[P ji ] is the outcome of the j-th trajectoryP ji and g represents the golden answer.After determining the node with the highest re-ward value according to R(yi|x, y1:i1), we ex-pand the tree by generating new child nodes. Toachieve this, we sample k potential subsequentreasoning steps yji+1 (yi+1|x, y1:i) for j =1, . . . , k, where is the language model used forreasoning. Each of these candidate steps {yji+1}kj=1becomes a new child node connected to the previ-ously selected node. If the selected node represents the last step, indicating the end of the reasoningchain, we omit the expansion phase, and this itera-tion concludes. Guided by the reward function R,this approach ensures a systematic exploration andexpansion of reasoning paths in the search tree.",
  "Step-Level Pairwise Training": "A reasoning tree illustrates all potential reasoningpaths, starting from the root and branching out tovarious leaf nodes. Our objective is to create adataset Dpair consisting of pairs that express prefer-ences of reasoning paths. We generate this datasetby tracing each unique path from the root to theleaves of the tree. Within this dataset, each en-try consists of a triplet in the form {(x, y+, y)},where x denotes the initial problem statement, y+ is the preferred reasoning sequence that leads toan accurate solution, and y is a less desirablereasoning chain that results in an incorrect answer.To collect the paired data {(x, y+, y)}, we con-duct pairwise comparisons between sibling nodesat each decision point along the tree. Sibling nodesare the various possible next steps in the reason-ing process branching from the same prior contexty1:i1. If the reward difference between a prefer-able step y+i and a less preferable step yi meets theminimum margin , we create a pair (x, y+, y)that includes the initial problem x, the superior rea-soning sequence y+, and the inferior alternative y.Specifically, for the preferable step y+i , we samplea path that leads to a correct solution among itscompletions P+i . This constitutes the complete rea-soning path y+. Conversely, the less preferablestep yi is assembled with one of the subsequentpaths Pi that begins from it to form y, whichultimately leads to incorrect outcomes. Based onthe pairwise instance data {(x, y+, y)}, we gathertwo types of step preferences during our training.The first type is derived from the reward function Rfor the current step yi, which assesses the choicesat each decision point. The second type of pref-erence implicitly decomposes the outcome-guidedinstance preference into step-by-step comparisons,examining each potential path leading to possibleoutcomes. This breakdown allows us to allocatethe instance preference to both the decision pointsand the completion steps in the reasoning process,ensuring that steps on the correct path are preferredover those on the incorrect path.The verifier is built upon a large language modelwith an additional randomly initialized linear layerthat outputs a scalar value. During evaluation, we take the value linked to the end token to representthe aggregate reasoning score.We train the verifier using a step-level rankingloss function designed to promote the choice of themost effective solutions, defined as:",
  "Experimental Setup": "Tasks and DatasetsIn our evaluation, we se-lect benchmarks from two distinct reasoning cate-gories: arithmetic reasoning and commonsense rea-soning. These tasks encompass diverse reasoningparadigms, enabling a comprehensive assessmentof our methods effectiveness. For arithmetic rea-soning, we utilize GSM8K (Cobbe et al., 2021)and MATH (Hendrycks et al., 2021). GSM8Kcomprises grade school math problems, whereasMATH includes complex competition-level mathproblems. We use the entire GSM8K test set anda subset of 500 problems from MATH, which isidentical to the test set of Lightman et al. (2023).For commonsense reasoning, we employ CSQA(Talmor et al., 2018) and StrategyQA (Geva et al.,2021). CSQA challenges the model with multiple-choice questions that often require reasoning basedon complex semantics and prior knowledge. Strate-gyQA involves true-or-false questions that demandimplicit multi-hop reasoning to derive answers. MetricsFollowing the methodology suggestedby Lightman et al. (2023), we adopt a best-of-Nevaluation paradigm. For arithmetic reasoning, wegenerate 64 solutions for each problem. However,for commonsense reasoning, due to the relativelylimited search space for answers in these tasks, wegenerate 10 solutions for each question. ModelsOur experiments leverage a series ofmodels to serve as the generator, includingLLaMA2-7B/13B (Touvron et al., 2023) andMistral-7B (Jiang et al., 2023). For arithmetic rea-soning tasks, we additionally conduct experimentswith two specialized models: a version of Mistral-7B fine-tuned on MetaMATH (Yu et al., 2023b),and WizardMath-7B (Luo et al., 2023).",
  ": Results (accuracy %) of the arithmetic reason-ing task on generators with stronger capabilities": "Data CollectionTo construct the training dataset,we selected 6,000 problems from each of theGSM8K and CSQA training sets to generate paireddata. We applied a margin threshold of = 0.375to filter these pairs, resulting in approximately 100kand 120k valid pairs for GSM8K and CSQA, re-spectively. For additional evaluation, we sampled750 questions from the StrategyQA training set,which yielded 15k pairs. The verifier then trainsfor an epoch on the corresponding dataset basedon task types. See Appendix B for more detailedtraining parameter settings2.",
  "Thecodeisavailableat": "ifiers trained using cross-entropy loss with binarylabels: the outcome-supervised verifier, termedORM (Lightman et al., 2023), and Math-Shepherd(Wang et al., 2023), a state-of-the-art process-supervised verifier that leverages automatically an-notated data. Additionally, we adopted the self-consistency (Wang et al., 2022) as a strong baseline,following Lewkowycz et al. (2022). Regardingpaired data generation, we also considered Self-Explore (Hwang et al., 2024) as a baseline method.It identifies the first erroneous step in a solution anduses the preceding steps to sample a correct path.These sequences are then paired to form trainingdata. We evaluated the quality of data by compar-ing a verifier trained on this data with ours.",
  "Main Results": "Arithmetic ReasoningAs shown in Tables 1,our proposed method significantly outperformsother methods across all scenarios within theGSM8K and MATH500 datasets. For the GSM8Kdataset, Tree-PLV consistently surpasses compet-ing approaches. For instance, using the LLaMA2-13B generator, Tree-PLV achieves an accuracy of76.12%, outperforming the second-best method,Math-Shepherd, by 8.11%. The Tree-PLV, initiallytrained on the GSM8K dataset, yields effective re-sults on the more complicated MATH500 dataset,illustrating its strong generalization capabilities. Itsadaptability stems from the training approach ofTree-PLV, which focuses on comparing the qualityof steps rather than simply identifying correct orincorrect paths. This strategy makes the verifier N = number of solutions per problem % Problems Solved (Best-of-N) GSM8K N = number of solutions per problem 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 % Problems Solved (Best-of-N) MATH500 Self-ConsistencyORMSELF-EXPLOREMATH-SHEPHERDTree-PLV",
  ": Performance of different verifiers across varying numbers of solution (N) generated by Mistral-7B": "more versatile. Additionally, Tree-PLV requiresonly 22.7% of the training data size used by Math-Shepherd, significantly reducing data requirements. displays the results of arithmetic rea-soning tasks using advanced generators, namelyMistral-7B: MetaMATH and WizardMath-7B, onthe GSM8K and MATH500 datasets. In all sce-narios, our method, Tree-PLV, consistently out-performs the others.With Mistral-7B: Meta-MATH, it achieves accuracies of 87.72% onGSM8K and 37.20% on MATH500. Similarly,with WizardMath-7B, Tree-PLV delivers the high-est accuracies of 90.14% and 40.20% on GSM8Kand MATH500, respectively. These results under-line Tree-PLVs robust performance and adaptabil-ity in tackling complex arithmetic reasoning tasksacross various generator settings. Commonsense ReasoningThe results in Ta-ble 1 highlights Tree-PLVs strong performancein commonsense reasoning tasks. Regardless ofthe dataset or the generator used, Tree-PLV consis-tently outperforms other verifiers, demonstratingits versatility and robustness. In the CSQA dataset,specifically, Tree-PLV improves accuracy by up to5.98%, 3.77%, and 3.03%, when compared withthe second-best model. This high level of accuracybecomes even more pronounced with the Strate-gyQA dataset, confirming Tree-PLVs strong ca-pabilities in handling intricate reasoning tasks. Incontrast, both ORM and Math-Shepherd, whichare trained using a binary supervision strategy, failto show similar improvement. This underperfor-mance could indicate the limitations inherent tobinary supervision, especially in tasks that requirecomplex reasoning. It is more prone to introduc-ing noise into the binary supervision signal, whichcould consequently limit the verifiers capabilities.We attribute these improvements to the follow- ing factors: 1) The training method of our verifierutilizes step-level preference learning, allowing fora nuanced evaluation of step quality that is betteraligned with the best-of-N ranking paradigm. 2)The reward function, in our approach, is used tocompare sibling steps instead of annotating them,which minimizes the impact of label noise. 3)Our approach improves the diversity of the databy incorporating comparisons among similar steps,thereby enriching the dataset and improving therobustness of our method.",
  "Different Amounts of Candidate Solutions": "The number of candidate solutions impacts the per-formance of verification. As the number of so-lutions increases, more potentially superior can-didates are introduced. However, theres a trade-off, as more solutions mean higher computationaloverhead and diminishing performance gains. Forour study, we set a maximum of 64 solutions. Asshown in , we found that all methods showan enhancement in performance as the number ofsolutions increases and finally stabilizes at 64, con-sistent with Wang et al. (2023)s findings. Through-out the study, the Tree-PLV method consistentlyoutperforms the others across all quantities of solu-tions. Remarkably, as the number of solutions rises,Tree-PLV continues to widen the performance gap,underscoring its robustness and superior capacityto leverage a greater number of solutions.",
  ": A performance comparison of verifiers trained with different levels of feedback granularity": "training methods (Christiano et al., 2017; Ouyanget al., 2022). Additionally, we include a verifiertrained using instance-level binary classification inour comparison for a comprehensive analysis. Theresults presented in demonstrate that veri-fiers trained with preference learning consistentlyoutperform those trained using binary classificationacross all levels of granularity. This finding sug-gests that preference learning is better aligned withthe ranking evaluation pattern. Furthermore, step-level guidance exhibits the best performance, indi-cating that it represents the most suitable granular-ity. In contrast to instance-level sparse supervision,it provides more detailed and informative feedback.Moreover, in step-by-step inference scenarios, step-wise preference proves to be more reasonable thantoken-level supervision, and such step-level align-ment is also more consistent with the intuitive hu-man process of evaluating reasoning.",
  "Impact on Reasoning Path Distribution": "Recent studies have highlighted the difficultieslarge language models encounter in accuratelyidentifying their errors (Huang et al., 2023; Honget al., 2023). To assess our verifiers effective-ness, we generate reasoning paths using greedydecoding, which are then scored by the originalmodels generation confidence and by Tree-PLV.We compare the score distributions from both meth-ods. shows Tree-PLVs capability to sepa-rate correct and incorrect reasoning paths. In con-trast, generation confidence scores produce overlap-ping distributions for both categories, highlightingtheir limited differentiation capacity. Specifically,Tree-PLV increases the scores of correct paths by35%, whereas the confidence scores show onlya marginal 2% higher score for them. This pro-nounced disparity between the two methods notonly confirms the superior performance of Tree-PLV in evaluating the quality of reasoning paths",
  "Reward Design": "Our proposed reward function R evaluates the qual-ity of the current step by assessing the proportionof its latent complete paths that reach accurate con-clusions. Some research demonstrates the advan-tages of integrating models self-evaluation capabil-ities into inference to enhance accuracy (Hao et al.,2023; Ren et al., 2023a; Xie et al., 2023). Inspiredby this, we also developed a reward function thatrelies on self-evaluation. As per Hao et al. (2023),we first construct a reasoning tree based on MonteCarlo Tree Search (MCTS) that incorporates self-evaluation. We then collect preference data fromeach step of this tree to train a verifier under thesame setting. The results, as shown in ,reveal that the self-evaluation-based reward doesnot perform as effectively as our proposed rewardfunction. This trend is consistent across differentgenerators, where the performance of the verifiertrained with self-evaluation rewards significantlylagged behind our Tree-PLV. Our comparative anal-ysis reveals that relying solely on the models in-trinsic ability to evaluate reasoning steps falls shortin reliability. Additional results on the MATH500dataset are provided in the Appendix C. 0.00.20.40.60.81.00 1200.89 0.87 (a) LLM distribution OverallCorrectWrong 0.00.20.40.60.81.00 0.71 0.36 (b) Verifier distribution OverallCorrectWrong : Distributions of the LLMs generation confidence and verifier score on correct/incorrect reasoning paths.The median scores for correct and wrong paths are highlighted with lines of the same respective colors. 0.1250.2500.3750.500 81.0 81.5 82.0 82.5 % Accuracy (Best-of-64)",
  "Margin Value for Preference Collection": "We delve deeper into the analysis by applying vary-ing margins to filter data. As shown in ,increasing the margin value raises the proportion ofpaired data with more noticeable path differences,thereby reducing noise and improving the perfor-mance of the verifier. However, when the marginvalue reaches 0.5, the verifiers performance de-clines, indicating that excessively high marginscan diminish the comparison of similar paths andreduce the richness of the data. Results on theMATH500 dataset are provided in the Appendix C.",
  "Efficiency of Data Generation": "We further analyze our tree-based frameworkacross the dimensions of data generation efficiency.We compare the average number of training dataper problem generated by different methods over10 iterations, as shown in . Math-Shepherdconsistently labels ten paths.In contrast, Self-Explore struggles with datasets where the modelalready performs well, as it inherently relies onincorrect paths to form pairs. Our framework en-hances the utilization of single paths through thebranching structure of a tree, demonstrating signifi-cant scalability and efficiency in data generation. GSM8KCSQA0 Avg. Training Data per Problem SELF-EXPLOREMATH-SHEPHERD Tree-PLV",
  "Related Work": "Recent studies (Lightman et al., 2023; Cobbe et al.,2021; Yu et al., 2023a) have demonstrated that in-corporating a verifier can significantly enhance per-formance in reasoning tasks. Primarily, there aretwo methods for training verifiers: outcome super-vision and process supervision. Unlike these ap-proaches, which rely on supervised learning withbinary labels, our methodology employs prefer-ence learning to achieve step-level alignment. Weprovide a detailed discussion of additional relatedwork in Appendix A.",
  "Conclusion": "In this paper, we propose Tree-PLV, an innovativeverifier developed through step-level preferencelearning. Utilizing a best-first tree search frame-work to construct reasoning trees, this method cre-ates a diverse and robust dataset, ensuring a richpool of paired data for preference training. Byleveraging step-level preferences instead of con-ventional binary labels, our methodology providesmore granular and detailed feedback, enhancing theverification of reasoning paths. Our empirical find-ings highlight the efficacy of Tree-PLV, showingsignificant performance improvements over exist-ing benchmarks across a variety of arithmetic and commonsense reasoning tasks. In future work, weaim to delve deeper into integrating our verifierwithin the reasoning process by combining it withvarious search algorithms, thereby refining the ver-ification of intermediate reasoning steps.",
  "Limitations": "While Tree-PLV is effective at scoring completesolutions, its potential to provide feedback that as-sists and refines the reasoning process during theinference phase has not been explored in this work.Additionally, aside from assisting in reasoning toencourage successful outcomes, some studies treatverifiers as reward models in reinforcement learn-ing scenarios. However, our current research fo-cuses on developing highly reliable verifiers, ratherthan employing reinforcement learning to enhancethe generator. Addressing these aspects will be thefocus of our future research.",
  "Ethics Statement": "The development of Tree-PLV aims to improvethe accuracy and reliability of reasoning assess-ments in Large Language Models. Although ourmethod poses no immediate ethical concerns, weacknowledge the potential for misuse if applied insensitive areas such as automated decision-making.We recommend rigorous evaluation and oversightto prevent bias and ensure data privacy in all appli-cations. It is crucial to maintain transparency andadhere to ethical standards in the deployment ofsuch technologies.",
  "Acknowledgements": "This work is supported by the \"Pioneer\" and\"Leading Goose\" R&D Programs of Zhejiang (No.2024C01034), the National Natural Science Foun-dation of China (No. 62376245), the FundamentalResearch Funds for the Central Universities (226-2024-00170), the project of the Donghai Labora-tory (Grant no. DH-2022ZY0013) and MOE Engi-neering Research Center of Digital Library. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, Nicholas": "Joseph, Benjamin Mann, Nova Dassarma, NelsonElhage, Zac Hatfield-Dodds, Danny Hernandez, JohnKernion, Kamal Ndousse, Catherine Olsson, DarioAmodei, Tom B. Brown, Jack Clark, Sam McCan-dlish, Christopher Olah, and Jared Kaplan. 2021. Ageneral language assistant as a laboratory for align-ment. ArXiv, abs/2112.00861. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Antonia Creswell and Murray Shanahan. 2022. Faith-ful reasoning using large language models. arXivpreprint arXiv:2208.14271": "Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths,Tommaso Salvatori, Thomas Lukasiewicz, PhilippPetersen, and Julius Berner. 2024. Mathematical ca-pabilities of chatgpt. Advances in Neural InformationProcessing Systems, 36. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did aristotleuse a laptop? a question answering benchmark withimplicit reasoning strategies. Transactions of theAssociation for Computational Linguistics, 9:346361. Olga Golovneva, Moya Chen, Spencer Poff, MartinCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,and Asli Celikyilmaz. 2022. Roscoe: A suite ofmetrics for scoring step-by-step reasoning. arXivpreprint arXiv:2212.07919. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, ZhenWang, Daisy Wang, and Zhiting Hu. 2023.Rea-soning with language model is planning with worldmodel. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 81548173, Singapore. Association for Com-putational Linguistics. Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, and Ja-cob Steinhardt. 2021. Measuring mathematical prob-lem solving with the math dataset. arXiv preprintarXiv:2103.03874. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu,and Changshui Zhang. 2023. A closer look at theself-verification abilities of large language models inlogical reasoning. arXiv preprint arXiv:2311.07954.",
  "JieHuang,XinyunChen,SwaroopMishra,Huaixiu Steven Zheng, Adams Wei Yu, Xiny-ing Song, and Denny Zhou. 2023. Large languagemodels cannot self-correct reasoning yet.arXivpreprint arXiv:2310.01798": "Hyeonbin Hwang, Doyoung Kim, Seungone Kim,Seonghyeon Ye, and Minjoon Seo. 2024.Self-explore to avoid the pit: Improving the reasoningcapabilities of language models with fine-grained re-wards. arXiv preprint arXiv:2404.10346. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Jean Kaddour, Joshua Harris, Maximilian Mozes, Her-bie Bradley, Roberta Raileanu, and Robert McHardy.2023. Challenges and applications of large languagemodels. arXiv preprint arXiv:2307.10169": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Aitor Lewkowycz, Anders Andreassen, David Dohan,Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, TheoGutman-Solo, et al. 2022. Solving quantitative rea-soning problems with language models. Advancesin Neural Information Processing Systems, 35:38433857. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B. Chen,Jian-Guang Lou, and Weizhu Chen. 2022. Makinglanguage models better reasoners with step-awareverifier. In Annual Meeting of the Association forComputational Linguistics. Hunter Lightman, Vineet Kosaraju, Yura Burda, HarriEdwards, Bowen Baker, Teddy Lee, Jan Leike,John Schulman, Ilya Sutskever, and Karl Cobbe.2023.Lets verify step by step.arXiv preprintarXiv:2305.20050.",
  "Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,Delip Rao, Eric Wong, Marianna Apidianaki, andChris Callison-Burch. 2023.Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,Long Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021.Webgpt: Browser-assisted question-answering with human feedback.arXiv preprintarXiv:2112.09332. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Olek-sandr Polozov, Christopher Meek, Dragomir Radev,and Jianfeng Gao. 2022. Learning math reasoningfrom self-sampled correct and partially-correct solu-tions. arXiv preprint arXiv:2205.14318. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744.",
  "Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2018. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge. arXiv preprint arXiv:1811.00937": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Miles Turpin, Julian Michael, Ethan Perez, and SamuelBowman. 2024. Language models dont always saywhat they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural InformationProcessing Systems, 36. Jonathan Uesato, Nate Kushman, Ramana Kumar, Fran-cis Song, Noah Siegel, Lisa Wang, Antonia Creswell,Geoffrey Irving, and Irina Higgins. 2022.Solv-ing math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275.",
  "Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, DamaiDai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.2023.Math-shepherd: Verify and reinforce llmsstep-by-step without human annotations.CoRR,abs/2312.08935": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2022. Self-consistency improves chainof thought reasoning in language models.arXivpreprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "Fei Yu, Anningzhe Gao, and Benyou Wang. 2023a.Outcome-supervised verifiers for planning in mathe-matical reasoning. arXiv preprint arXiv:2311.09724": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,Zhengying Liu, Yu Zhang, James T Kwok, Zhen-guo Li, Adrian Weller, and Weiyang Liu. 2023b.Metamath: Bootstrap your own mathematical ques-tions for large language models.arXiv preprintarXiv:2309.12284. Zheng Yuan, Hongyi Yuan, Chengpeng Li, GuantingDong, Chuanqi Tan, and Chang Zhou. 2023. Scal-ing relationship on learning mathematical reason-ing with large language models.arXiv preprintarXiv:2308.01825.",
  "ARelated Work": "Multi-step ReasoningChallenging reasoningtasks have spurred innovative research in large lan-guage models (LLMs), which are essential for han-dling complex queries (Kaddour et al., 2023; Light-man et al., 2023; Huang et al., 2023). A prominentstrategy employed is the Chain-of-Thought (CoT)prompting technique (Wei et al., 2022), along withits derivatives (Kojima et al., 2022; Wang et al.,2022; Yao et al., 2024). These methods decomposethe reasoning process into sequential steps, system-atically approaching problem-solving by mimick-ing human-like reasoning. To further enhance theaccuracy of these intermediate steps, recent stud-ies leverage extensive synthetic datasets, whichare either distilled from cutting-edge models (Yuet al., 2023b; Luo et al., 2023) or composed of self-generated rationales (Zelikman et al., 2022; Yuanet al., 2023; Ni et al., 2022), to fine-tune the LLMs.Such training effectively sharpens the models abil-ity to produce CoT reasoning that leads to correctanswers.In addition, there is an increasing emphasis ontest-time verification, a process that generates mul-tiple solutions and ranks them through a separateverifier (Cobbe et al., 2021) to select the most accu-rate one. The DIVERSE framework (Li et al., 2022)demonstrates the effectiveness of employing a vari-ety of CoT prompts in conjunction with a verifierto tackle reasoning challenges. CoRe (Zhu et al.,2022) implements a dual-process system by fine-tuning both the generator and the verifier, therebyenhancing the performance of LLMs on math wordproblems. Training VerifiersThe training process for ver-ifiers is typically approached as a binary classifi-cation task. In closely related work, Uesato et al.(2022) introduces two distinct training methodolo-gies for verifiers, differentiated by the granular-ity of the supervision signal: outcome supervision(Cobbe et al., 2021; Yu et al., 2023a; Hosseini et al.,2024) and process supervision (Li et al., 2022; Ue-sato et al., 2022; Lightman et al., 2023; Wang et al.,2023). Verifiers trained under outcome supervisionutilize only the final result of a reasoning path forlearning, while those trained under process super-vision benefit from incremental feedback at eachstep within the CoT.The training objective for this binary classifi-cation does not align well with its intended ap-plication in ranking scenarios. The binary labels provide a coarse supervisory signal, which failsto differentiate the efficacy of various steps, thuscapping potential enhancements in performance.Addressing these issues is not straightforward, asaccurately assessing the quality of each step in-volves subjective judgments that are difficult tostandardize, even with human annotations. Math-Shepherd (Wang et al., 2023) employs heuristicrules to annotate individual steps; however, the ver-ifier trained on these refined labels does not showimprovement compared to its binary version. Ourmethod, Tree-PLV, fundamentally rethinks the ver-ifiers role by incorporating step-level preferencelearning. This approach more effectively capturessubtle differences among paths and better alignswith the ranking evaluation paradigm.",
  "B.1Dataset Description": "presents the reasoning benchmarks usedin this paper, along with examples. We utilize thesame test sets as Wang et al. (2023) for GSM8Kand MATH500, and as Wei et al. (2022) for CSQA.For StrategyQA, there are 2,290 test cases (i.e.,questions paired with TRUE/FALSE labels). How-ever, there are no additional cases that can be lever-aged by Tree-PLC to construct the training set. Toaddress this issue, we randomly sampled 750 testcases to create the training data, leaving the remain-ing 1,540 cases for testing.",
  "B.2Training Data": "To construct the training set for Tree-PLV, we select6,000 problems each from the GSM8K and CSQAtraining datasets to generate paired data. We sam-ple 8 subsequent trajectories (N=8) to calculate thereward for each step. By applying a margin thresh-old of = 0.375, we filter these pairs, resultingin approximately 100k valid pairs from GSM8Kand 120k from CSQA. Additionally, we sampled750 questions from the StrategyQA training set,yielding 15k pairs.",
  "We present a prompt example for generating solu-tions and performing reasoning. We employ few-shot learning to guide the model towards producingoutputs in the step-by-step format": "Solve the questions step by step. Make eachstep clear and logical, leading to the next. Ifa step directly leads to an answer, end it with\"The answer is: \". If not, proceed to the nextstep until you find the solution. Four years ago, Kody was only half asold as Mohamed.If Mohamed is currentlytwice as 30 years old, how old is Kody?Step 1: If Mohamed is currently twice as old as30 years old, then Mohamed is currently 2 * 30= 60 years old.Step 2: Four years ago, Mohamed was 60 - 4 =56 years old.Step 3: Four years ago, Kody was only half asold as Mohamed, so Kody was (1/2) * 56 = 28years old.Step 4: Therefore, Kody is currently 28 + 4 =32 years old. The answer is: 32.",
  "desirable path": "x: One pie costs $4 for a piece. Each pie ishaving 3 pieces. During one hour the bakerycan make 12 pies. Creating one pie costs thebakery $0.5. Considering the bakery would beable to sell all pie pieces, how much moneywould it make? y+:Step 1: Each pie is having 3 pieces.Step 2: 12 pies is equal to 3 * 12 = 36 pieces.Step 3: Each piece costs $4, so the bakerywould earn 36 * $4 = $144 if it could sell all ofits pieces.Step 4: The cost of making one pie is $0.5, sothe cost of making 12 pies is 12 * $0.5 = $6.Step 5: The bakery would make 1446 = $138.The answer is: 138. y:Step 1: Each pie is having 3 pieces.Step 2: 12 pies is equal to 3 * 12 = 36 pieces.Step 3: The bakery can make 36 pieces duringone hour.Step 4: One piece costs $4 and a pie costs $0.5,so it costs the bakery (1/3) * $4 + (2/3) * $0.5 =$1.5 to create one pie.Step 5: The bakery would make 12 * $1.5 =$18 during one hour. The answer is: 18."
}