{
  "Abstract": "While Large language model (LLM)-based pro-gramming assistants such as CoPilot and Chat-GPT can help improve the productivity of pro-fessional software developers, they can alsofacilitate cheating in introductory computerprogramming courses. Assuming instructorshave limited control over the industrial-strengthmodels, this paper investigates the baseline per-formance of 5 widely used LLMs on a collec-tion of introductory programming problems,examines adversarial perturbations to degradetheir performance, and describes the results ofa user study aimed at understanding the effi-cacy of such perturbations in hindering actualcode generation for introductory programmingassignments. The user study suggests that i)perturbations combinedly reduced the averagecorrectness score by 77%, ii) the drop in cor-rectness caused by these perturbations was af-fected based on their detectability.",
  "Introduction": "Large Language Model (LLM)-based tools suchas ChatGPT (OpenAI, 2024) have demonstratedan impressive ability to create high-quality codegiven simple prompts and have the potential forsignificant impact on software development (Barkeet al., 2023). While there are ongoing efforts toincorporate such tools into computer science (CS)education (Jacques, 2023), integrating new tech-nologies into educational curricula can take a longtime (Hembree and Dessart, 1986; Koh and Daniel,2022). Meanwhile, existing CS curricula are underthe threat of LLM-assisted cheating and requireimmediate attention (Finnie-Ansley et al., 2023,2022).Given that educators have little direct controlover the capabilities of industrial-strength LLMs,two possible directions towards addressing this",
  "This function returns which are sensible.[omitted for brevity]": ": Removal of 5 characters from an assignmentprompt caused correctness scores of the generated solu-tions to drop from 100% to 0% in CodeRL, Code Llama,GPT-3.5, and GitHub Copilot. For Mistral, it droppedfrom 33.33% to 0%. threat are (i) to detect and penalize LLM-assistedcheating; and (ii) to modify problem statements toimpede LLM-assisted cheating. The first approachis problematic because it can be difficult to deter-mine reliably whether some given content is LLM-generated or not (Hoq et al., 2023; Orenstrakh et al.,2023), and both false positives and false negativesare possible. In this paper, we explore the secondoption and ask the following question: How can in-structors modify assignment prompts to make themless amenable to LLM-based solutions without im-pacting their understandability to students?While there has been some work on the impact ofadversarial prompts on LLMs (Wang et al., 2023a;Liu et al., 2023a), we are not aware of any researchinvestigating adversarial strategies for impedingLLM-assisted cheating in a Blackbox setting inan academic context. To systematically study theproblem, we break it into the following three steps:",
  "lyze their impact on the quality of LLM-generated solutions to those problems": "Step 3. Run a user study to understand the poten-tial for such perturbation techniques in imped-ing actual LLM-assisted cheating, focusing inparticular on whether students can detect andreverse such perturbations. An overview of these steps is presented in Fig-ure 2. To measure the accuracy of LLM-generatedcode, we use the same test inputs used to evalu-ate student submissions. To modify problem state-ments in a Blackbox setting, we design a set ofperturbation techniques that are informed by ex-isting literature on adversarial perturbation (Bielikand Vechev, 2020; Rauber et al., 2017; Wang et al.,2021b; Zhao et al., 2023). We use SHAP (Lund-berg and Lee, 2017) with a surrogate model toguide the perturbation for better efficacy vs. modi-fication tradeoff. We define efficacy (Definition 1)for a perturbation technique to quantify the portionof lowering the LLM accuracy. To ethically con-duct the user study in Step 3, we select the studygroup from students who have already taken thecourses corresponding to the assignments used forthe study. Our findings suggest that existing LLMs gen-erally struggle to solve assignments requiring in-teractions across multiple functions and classes.Our evaluation of different perturbation techniquesshows a high overall success rate, causing degra-dation of more than 85% of the assignments forall five models (example in ). We findthat high variations in solution generations stronglycorrelate with high success rates. Our user studywith undergraduates shows that the average efficacydropped from 15.43% to 15% when perturbationswere noticed. It also suggests that subtle pertur-bations, i.e., substituting tokens or removing/re-placing characters, when unnoticed, are likely toretain high efficacy in impeding actual solutiongeneration. Additionally, the detectability of ahigh-change perturbation might not imply rever-sion. The implication is that under perturbations,students have to check and modify LLM solutionsrather than adopt them unchanged instructors canuse these perturbations when preparing homeworkproblems to reduce cases where students do notlearn but use ChatGPT as is.",
  "Methodology": "User Study Design. We recruited 30 undergrad-uate students who had previously completed CS1and CS2 courses from the same university to partic-ipate in this IRB-approved user study. Each partici-pant was awarded $20 for their participation. Dur-ing this study, each student was explicitly askedto use ChatGPT to solve 3 assignments over oneweek and submit the entire chat history in a post-study survey. After the experimentation, we askedthe participants to submit their chat history withChatGPT and observed that all of the participantsused ChatGPT-3.5, except for one who used theChatGPT-4.0 version. We discarded the data fromthat user.The details of specific instructions to the stu- dents are added in Appendix G.5. We assign eachassignment-perturbation pair to at least three partic-ipants to cover redundancy and diversity. This in-cludes no perturbation cases, too, which indicatesthe base performance. Our post-study survey alsoasks whether students noticed anything unusualin the assignment description, how they validatedsolutions, etc. (details in ). Note that forethical reasons, we chose to run the study on stu-dents who already took the courses (Demographicinformation in ). We discuss its impact onthe outcome in .Problem Selection. For this study, we select as-signments for which the efficacy score for at leastone perturbation was 80 on GPT-3.5, which powersChatGPT. We chose 6 assignments with at least 3perturbed versions, from this initial list, under 3different techniques. shows the problemand perturbation technique pairs selected for theuser study. Prompt (Original) indicates promptwith no perturbation. We recognize that removalof content (i.e., characters, tokens, etc.) from theassignment text will be easily detected by students.To remedy this, we replace the removed contentwith images of the characters that were removed inan attempt to make the text look as visually iden-tical to the original assignment as possible. Weassume that students will copy and paste the textfrom the assignment into the ChatGPT input box,and because images do not get copied, the textpasted into ChatGPT will be pertubed. inAppendix F shows the distributions of the numberof participants for different variants of the assign-ments.Analyzing the textual Responses. Answers tosome of the questions in our post-study question-naire were open-ended. Thus, to systematically",
  "Oracle": ": Overview of our study, which is conducted in three steps. Here, boxed elements indicate processing units ,and unboxed elements represent input/output data. We used solid arrows through processing units to connect inputsto their corresponding outputs. runs a predefined set of test cases and outputs thepercentage of test cases passed by the solution. Tobuild these scripts, we reuse the test cases obtainedfrom the instructor. We form two groups among theauthors of this paper to create and validate these testoracles. One group creates the scripts for a selectedassignment set, and another validates them.Model Selection.We consider five LLMsforthisstudy:GPT-3.5(OpenAI,2022),GitHub Copilot (GitHub, 2021), Mistral (MistralAI team, 2024), Code Llama (Rozire et al., 2023)and CodeRL (Le et al., 2022). GPT-3.5 is used be-hind ChatGPT, and Mistral-Large is used behindMistral AI chat. GitHub Copilot is an IDE (e.g.,JetBrains IDEs, Visual Studio, etc.) plugin de-veloped by GitHub that is powered by OpenAIsCodex model. We select these five models fortheir availability to fresh CS students. We includedCode Llama and CodeRL for their wide accessibil-ity. The details of our code generation methods andthe model versions and parameters are describedin Appendix D; The most important point hereis that we set any relevant parameters to valuesthat produce the best possible solutions, upload theproblem prompt into the LLM, and evaluate thesolutions generated.",
  "Results: LLM performance": "We use all the short (84) and long (22) problemsto evaluate the performance of the LLMs consid-ered in our assignment corpus. For a given set ofassignments, we define an LLMs performance asthe average correctness scores of the correspond-ing solutions it generates. We generate correctnessscores (the portion of the test cases that pass) withour test oracles.Performance on CS1 Problems. The LLMs wetest do not generate completely correct solutionsto any of the problems in our CS1 problem set.For two short and 5 long problems, GPT-3.5 re-",
  "GitHub Copilot51.470 (26)100 (24)26.990 (6)100 (2)": "Performance on CS2 Problems. The performanceof the LLMs on our CS2 problem set is shown in. By and large, they perform better thanon the CS1 problems. CodeRL has the worst per-formance of the five LLMs tested: while it canconstruct correct solutions for some of the shortproblems with an average score of 12.5% for theshort problems, it fails to solve any of the longproblems. GPT-3.5 does somewhat better, scoring41.6% for the short problems and 8.3% for the longproblems. While Mistrals performance was closer,GitHub Copilot had the best performance, with anaverage score of 51.5% for the short problems and27% for the long problems.Finding 1: All five LLMs fail to solve CS1problems. For CS2, GitHub Copilot per-formed best, with an average score of 51.5%for short and 27% for long assignments.",
  "Discussion on the Findings": "The LLMs lack of success with CS1 problems isunexpected. Possible reasons for this include: (1)many of them are very specific problems unlikely tobe of sufficient general interest to show up in coderepositories and thereby appear in LLM trainingsets, providing a challenge for the LLMs to matchthe output required by the test oracles exactly; (2) information relevant to some of the problems isprovided graphically (60% CS1 problems), some-times in the form of ASCII art (), whichwas difficult for the LLMs to process; and (3) as-signments are often very specific regarding namesof input/output files, classes, methods, etc., and theLLMs had trouble matching these specifics. Theseresults are at odds with other research that suggeststhat LLMs can be effective in solving introductoryprogramming problems (Finnie-Ansley et al., 2022,2023). Possible reasons for this difference include:(1) differences in the problems used in differentstudies, given that there is no consensus on whatthe specific content of CS1 and CS2 courses oughtto be (Hertz, 2010); and (2) methodological dif-ferences between studies, e.g., Finnie-Ansley etal. manually repaired minor errors in the LLM-generated solutions (Finnie-Ansley et al., 2022)while we did not. Although the LLMs do not gen-erate correct solutions for any of the CS1 problems,in some cases, they generate code that is close tocorrect and could potentially be massaged to a cor-rect solution by a student.For the CS2 problems, there is a noticeable dif-ference between LLM performance on short prob-lems, which involve creating a single clearly spec-ified function or class, and long problems, whichare more complex and involve interactions betweenmultiple functions or classes. All of the LLMs gen-erate correct solutions for some short problems butfail to generate correct solutions for others; whileCodeRL fails to generate any correct solutions forany of the long problems. While Code Llama strug-gled too GPT-3.5, Mistral and GitHub Copilotwere able to generate correct solutions for someof the long problems. Once again, for some of theproblems, the LLM-generated code is close to cor-rect, and students could potentially massage themmanually into working solutions.",
  "Exploring Perturbations (Step 2)": "In this section, we explore the following researchquestion: How can we leverage black-box adver-sarial perturbation techniques to impede LLM-assisted solution generation? Towards that end,following existing literature, we design several per-turbation techniques and measure their efficacy onthe assignments that LLMs solved with non-zerocorrectness scores. For a given perturbation tech-nique, we define its efficacy as follows.",
  "Perturbation Methodology": "We design ten perturbation techniques under twobroad categories, core and exploratory.Core perturbations. Under this category, we de-sign seven principled techniques with four end-to-end automated perturbation strategies, i) synonymsubstitution, ii) rephrasing sentences, iii) replac-ing characters with Unicode lookalikes, and iv)removing contents. We apply these strategies todifferent perturbation units, i.e., characters, tokens,words, and sentences. Perturbation units indicatethe unit of changes we make at once. Inspired byexplainability-guided adversarial sample genera-tion literature (Sun et al., 2023; Rosenberg et al.,2020), we use SHAP (SHapley Additive exPlana-tions) (Lundberg and Lee, 2017) with CodeRL asthe surrogate model to select candidate units forperturbations. Specifically, we use Shapley valuesto compute the top-ranked tokens for perturbation.For example, for Character (remove) perturbation,we remove a random character from each token togenerate one variant; for Token (remove) perturba-tion, we remove all 5 tokens to generate one variant,and for the synonym morphs, we may have manysynonyms for one token, and generate many vari-ants. For Token (unicode) perturbation, we replaceall 5 tokens with Unicode characters to generateone variant. For example, we replaced a, c, andy with , c, and , respectively. We use the tokenrank for all the other perturbation units except forsentences. We rank the sentences by accumulatingthe Shapley values of the tokens corresponding toa given sentence for sentence perturbations. We add a detailed description of each technique in theAppendix E.Exploratory perturbations. We design three ad-ditional techniques to explore the potential of twodifferent insights. For example, existing studiesshow evidence that LLMs are prone to memoriz-ing training data (Zhang et al., 2021; Carlini et al.,2021, 2023). Thus, these models are highly sensi-tive to input variations (Zhang et al., 2022; Jin et al.,2022; Reynolds and McDonell, 2021). Under thishypothesis, replacing specific tokens with randomstrings may significantly influence performance, assuch substitution may alter the context (Shi et al.,2023; Liu et al., 2023b; Wang et al., 2021b). Wedesign a new exploratory perturbation techniqueto leverage this insight. Under this technique, wetweak assignments by replacing file names, func-tion names, and class names specified in the prob-lem statement with random words, where thesenames are discovered manually. Another exampleis that to understand the resiliency of LLMs onUnicode lookalikes (Shetty et al., 2018; Boucheret al., 2022), we create a mechanism to replace allpossible characters with Unicode lookalikes in theentire assignment statement. Character (remove) Token (unicode) Token (remove) Token (synonym) Tokens (synonym) Sentences (rephrase) Sentences (remove) Prompt (unicode) Random (insert) Random (replace) 0% 10% 20% 30% 40% 50% Avg. Edit Distance",
  "Results: Perturbation Performance": "We measure the performance of our perturbationtechniques on the assignments that LLMs solvedwith non-zero correctness scores.Perturbation Efficacy. depicts the effi-cacy of all our perturbations. All the perturbationscombined cause performance degradation in allfive models for most of the assignments we tested.Combined perturbation efficacy is the average ef-ficacy of the best perturbation technique for each",
  "Ei is the list of efficacy scores of all the per-turbation techniques on the i-th problem": "The performance is mostly dictated by removesentence and followed by assignment-wide sub-stitution with Unicodes perturbations. However,the average edit distance for these two techniquesis much higher, making them riskier for detection(), which we discuss next.Changes in the original prompt. A higher pro-portion of changes caused by a perturbation tech-nique risks both understandability and detectability.We use the edit distance between the original andperturbed assignment statements to quantify thechanges for a given perturbation technique. Notethat edit distance is not the ideal method to capturethe drifts (if any) caused by Unicode replacements(visual) and synonyms (conceptual); However, itgives a picture of how much the perturbed promptwas altered from the original one. depictsthe average edit distance of the perturbation tech-niques on the assignments with positive efficacy(i.e., causing performance degradation). Exceptfor sentence and prompt-wide perturbations, all theother techniques require a small (<5%) amount ofperturbation to the problem statements. This isbecause they are performed on a small portion ofcharacters or tokens, making them less expensive.Finding 2: The combination of all the pertur-bations covers more than 90% of the problemswith efficacy >80% for all five models. High-change perturbations have high efficacy. Why perturbations failed? To understand whyour perturbation techniques may have failed, westudy the two sets of assignments where they suc-ceeded and failed. Under the succeeded category,we select assignments where the average efficacywas high (greater than 90) for at least half of theperturbation techniques. For failed category, weselect assignments with efficacy 0 for all the tech-niques. Next, we randomly select 10 samples foreach category and study the variety in the generatedsolutions by the LLMs under various perturbationtechniques. For a given assignment, we measurevariety by directly comparing all the solutions andcounting unique variations. We observe that the : Average efficacy of the perturbation techniques. All the perturbations combined caused performancedegradation for a significant portion of assignments, which was dictated by Sentence (remove) and Prompt(unicode) perturbations.",
  "Combined93.7510010010010010097.1491.2190.9180.03": "average number of unique variations per problemis 13.9 and 26.0 for problems where perturbationfailed and succeeded, respectively. To determinethe uniqueness of solutions, we use AST similar-ity. Comparison of the ASTs of the codes that arethe same except for different variable names gets asimilarity score of 100, and formatting differencesbetween solutions will be ignored. We use a thresh-old of 90 when determining if a program is unique.Finding 3: High variations in generated solu-tions strongly correlate with high success ratesfor a given perturbation technique.",
  "Random (replace)---": "analyze those responses, we use thematic analysis,where the goal is to identify the concepts (knownas codebook) and organize them under differentthemes (Jason and Glenwick, 2015; Quaium et al.,2023). Two authors participate in the process toavoid human bias. Our thematic analysis foundthat students use 5 different approaches to neutral-ize perturbations and 11 different approaches tovalidate LLM-generated solutions. We present adetailed description of the method and the code-book in the Appendix F.Analyzing Solutions. The performance of black-box models changes over time. Without takingthis into account, one might come to erroneousconclusions. For example, shows the per-formance of different model checkpoints on theassignment statements we use for the user studysince we computed the efficacy with model check-point 0301. However, to ensure consistency incalculating the efficacy of the perturbation tech-niques in impeding the actual cheating, one needsto calculate the correctness scores for both the per-turbed and unperturbed versions of the assignmentson the same model checkpoints. Thus, we use theaverage correctness scores of unperturbed assign-ments to compute the average efficacy of a givenperturbation technique.",
  "Analysis Results": "In this section, we present the results of our fieldexperiment to answer the following three questions:Q1: How effective are the perturbations, in gen-eral, in impeding LLM-assisted solution genera-tion? Q2: How does the detectability affect effi-cacy? and Q3: What techniques do students adoptto avoid perturbations, and how do they validatetheir generated solutions?Impeding solution generation. Overall, the per-turbations are effective in impeding LLM-assisted",
  "Combined Results76.67%": "solution generation. Although most of the pertur-bations have an efficacy lower than 32%, in com-bination (selecting the best perturbation techniquefor each problem), their efficacy is around 77%,where the base correctness score was 71.28 (). This means perturbation techniques reduced77% of the base score showing promise in imped-ing LLM-assisted cheating. One interesting findingis that the Prompt (unicode) perturbation dropsthe models performance significantly. While moststudents notice it and exercise several strategies,they fail to sidestep it. : Comparison of average efficacy for the per-turbation techniques based on whether they were de-tected or not.For Token (remove) and Sentence(rephase), ChatGPT (GPT-3.5) generated correct solu-tions without any tweaks from the students.",
  "Total1515.43": "Detectability vs. Efficacy. Broadly, participantsnotice unusualness in the assignments for all theperturbations (). In , we show thedifference in efficacy based on whether the studentsnotice a perturbation or not. Overall, the averageefficacy dropped (15.43% to 15%) for detectability.Prompt/assignment-wide substitutions with Uni-code lookalikes that alter a large portion of theassignment are easily noticed (). Despitethe higher risk of being noticed, it still managedto deceive the model. Higher efficacies in noticedcases of perturbations, such as the removal of sen-tences and prompt-wide Unicode substitution, sug-gest that noticing the perturbation does not imply that students were able to reverse the changes, es-pecially if reversing involves some degree of effort.Subtle perturbations, i.e., substitutions of tokensand removal of characters, showed great potentialin tricking both the LLM and students, as they showhigher efficacy when undetected.",
  "Finding 5: The detectability of a high-changeperturbation might not imply reversion": "Handling perturbed assignments. We learn fromthe post-user study questionnaire that even if stu-dents noticed perturbations, in most cases (32 outof 49), they rely on ChatGPT to bypass them (Fig-ure 10). Other strategies they adopt are updat-ing the assignment statement, rewriting incorrectChatGPT-generated solutions, or writing the miss-ing portions. The average efficacy against each ofthe strategies is highest at 31.11% when studentsimpose Update problem statement, followed byNo unusualness found at 15.43% and Expected tobe bypassed at 9.17%. When students try Rewriteincorrect/missing portion, the perturbation effi-cacy is reduced to 0.Validation apporaches. Approaches to validatethe generated solutions also play a crucial role indetecting and fixing accuracy degradation. Moststudents report that they reviewed the generatedcode (72 out of 90 cases) or ran the code with thegiven test cases (55 out of 90 cases). Several ofthem report writing new test cases, too. A heatmapdiagram of the validation approaches is presentedin in Appendix F.",
  "gpt-4-061315.7113.1156.1423.5739.2315.72": "Impact of Model Evolution on perturbations.We run GPT-4.0 on the prompts generated by someof the promising perturbation techniques fromthe user study, i.e., Sentences (remove), Token(unicode), and Prompt(unicode).Out ofthe 1,113 prompts compared, GPT-4.0 outscoredGPT-3.5on 281 problems,while GPT-3.5outscored GPT-4.0 on 107 problems ().We observe that GPT-3.5 has built-in safeguardsfor academic integrity violations.Surprisingly,GPT-4.0 seems to lack such safeguards. For exam-ple, GPT-3.5 refuses to solve 8 problems for trig-gering such safeguards, but GPT-4.0 refuses none.This finding is concerning because it suggests thatGPT-4.0 could potentially be more amenable tomisuse for LLM-assisted cheating.",
  "Related Work": "LLMs in Educational Problem Solving. Finnie-Ansley et al. found that OpenAI Codex producedhigh-quality solutions for a set of CS1 and CS2programming problems (Finnie-Ansley et al., 2022,2023). This suggests that LLM-assisted cheatingin introductory programming courses has the po-tential to be problematic. Other studies note thatLLM-generated code can be of variable quality andsensitive to small changes to the prompt; this hintsat the idea that tweaking the problem prompt can af-fect the usefulness of LLM-generated solutions foracademic dishonesty. For example, Wermelingerobserves that Sometimes Copilot seems to havean uncanny understanding of the problem ... Othertimes, Copilot looks completely clueless (Wer-melinger, 2023), and Jesse et al. discuss Codexstendency to generate buggy code in some situations(Jesse et al., 2023). None of these works consideradversarial perturbation of prompts as a mechanism for hindering LLM-assisted cheating. Sadasivan etal. gives empirical evidence highlighting concernsthat LLM-generated texts can easily evade currentAI detection mechanisms (Sadasivan et al., 2023),underscoring the need for more advanced detec-tion technologies that can follow the continuousadvancements in LLM capabilities and ensuringthe integrity of academic work. Adversarial Attacks on Code Generation LLMs.Real-world applications relying on LLMs can besusceptible to vulnerabilities arising from adver-sarial attacks (Shayegani et al., 2023). Variousstrategies have been proposed to enhance the ad-versarial robustness of LLMs (Jiang et al., 2020;Shetty et al., 2018; Wang et al., 2021a), but thesemethods differ significantly, and there is a lack ofstandardization in the adversary setups used forvaluation (Wang et al., 2021b). Wang et al.s ex-periments show that, despite its relative dominanceover other LLMs, ChatGPTs performance is nev-ertheless sensitive to adversarial prompts and isfar from perfect when attacked by adversarial ex-amples. To the best of our knowledge, our workis the first attempt at studying the Robustness inEducation with adversarial attacks. Other researchshowed that adversarial attacks are also effectivein breaking guards against generating maliciousor unethical content (Zou et al., 2023; Liu et al.,2023a). Incorporating the methods suggested by(Wang et al., 2023b) for generating natural adver-sarial examples could be explored in the future.",
  "Conclusion": "High-performant LLMs pose a significant threat toenable cheating on introductory programming as-signments. It investigates the potential of adversar-ial perturbation techniques to impede LLM-assistedcheating by designing several such methods andevaluating their efficacy in a user study. The resultsuggests that the combination of the perturbationindeed caused a 77% reduction in the correctnessof the generated solutions, showing early promises.Our perturbations show positive results, but theymight only be effective temporarily. Future tech-niques, including rigorous training data and pro-tective layers in the prompting pipeline of LLMs,could counter these results. We hope our study willinspire ongoing efforts to prevent the misuse ofLLMs in academic settings.",
  "Limitations": "Impact of running the user study with studentsexposed to the assignments. One possible limita-tion of our user study is that it was conducted onstudents who already took CS1 and CS2 courses;thus, the finding might not hold for target students.However, as the study aimed to see if studentscan detect and reverse our perturbations, we hy-pothesize that experienced students will be moreequipped to do so than new ones. Thus, if our re-sults suggest that a given perturbation technique iseffective in impeding reversal for the study group,it is likely to be effective on the new students (ac-tual target group) as well. However, if our resultssuggest that a perturbation technique is ineffectivefor the study group, it does not imply that it willbe ineffective for the new students. This meansour study offers a conservative estimation of theefficacy of the perturbation techniques on the stu-dents. Given that designing an ethically acceptableuser study with new students is challenging, weargue this is acceptable. For example, Shalvi etal. (Shalvi et al., 2011) hypothesized that reducingpeoples ability to observe desired counterfactualsreduces lying. Thus, one can argue that expos-ing new students to the ChatGPT way of solvingproblems is ethically more questionable than expos-ing more mature students. This is because a) Thefact that they will know they can get away might in-centivize cheating, as they are likely unaware of thelong-term consequences. The damage is arguablyless for the students with some CS fundamentalknowledge and more insights into the long-termconsequences.We also want to note that even if we ignore theethical challenge mentioned above, designing areasonable study with new students is challenging.For example, all CS students are required to takethe courses from which we took the problems, andthe problems typically address concepts that havebeen discussed in class. So, if we wanted studentswho have not seen those (or similar) problems, wewould have to take non-CS students who have nottaken those classes and who would not have thebackground to solve those problems. This implieseither running the study as part of the course offer-ing or emulating the course for the study. Given theduration and volume it needs, it will be challengingto design such a study while keeping all the otherconfounding factors (i.e., controlling the modelsused) in check. Given these challenges, we chose to use the ChatGPT interface for the user studyinstead of an API-based tool with the trade-off be-tween user comfort and controllability of modelparameters or versions. However, seeing how thefindings hold under different user settings will beinteresting. Considering the complexities and nu-merous factors in designing such studies, they war-rant dedicated independent research efforts.Impact of perturbation on understandability.Perturbations can affect understandability. Ourwork is intended to provide instructors with ad-ditional tools and techniques to deter LLM-assistedcheating; it is up to the instructor to ensure that anyapplied perturbations do not impact the clarity ofthe problem description. For example, a judiciousapplication of the sentence removal perturbationtechnique we describe can be combined with us-ing images to replace the semantic content of theremoved sentences. Additionally, some perturba-tion techniques, such as unicode replacement andcharacter removal may be easily reversed by a stu-dent who notices them, as our user study revealed.Thus for these smart tweak perturbations, thekey requirement is to be as imperceptible as pos-sible, to avoid detection. We also note that this isthe first work to proactively deter the use of LLM-assisted cheating in the academic context, which isan urgent problem. It would be interesting to seewhat other approaches can be more effective forthis purpose in the future or to run studies to findperturbations that do not affect students trying tosolve problems honestly but do affect students whosubmit ChatGPT solutions. Additionally, promptsengineering to reverse the perturbation to under-stand their strengths can be a great complement toevaluating the strength of perturbations, togetherwith user studies, or in cases where user studiesmight be infeasible to run. It would also be interest-ing to run follow-up studies on what factors affectcomprehensibility to develop principles for design-ing understandability-preserving perturbations.\"Investigating all these interesting questions can beboth motivated and enabled by the current work.Other limitations. We use CodeRL as the surro-gate model, which might not be a close approxima-tion of the target models. Despite this limitation,CodeRL is successful in generating perturbed sam-ples to run our field study. Finally, we ran theuser study with only 6 assignments, which mighthurt the generalizability of the findings. ChatGPTprovides personalized answers, which might causevariances in our results. To counter this, we added",
  "Ethical Considerations": "Our study was approved by the IRB of the desig-nated institute. We recruited students who havealready taken CS1 and CS2 to avoid academic in-tegrity violations. Participants were compensatedwith a reward of $20 for their contribution. Duringthe user study, we did not collect any personallyidentifiable data. Lastly, all the experiments onGPT-3.5 and Mistral models were done with pre-mium API access. We also used GitHub Copilotunder an academic subscription to ensure fair andresponsible use. The replication package, whichincludes the data and source code, will be availableto researchers on request.",
  "Acknowledgements": "We thank Genesis Elizabeth Benedith and Lo-gan Michael Sandlin for their involvement dur-ing the initial stage of the project. We also thankthe instructors of CS1 and CS2 who taught thesecourses at the University of Arizona over theyears, including Adriana Picoral, Janalee OBagy,Reyan Ahmed, Russell Lewis, Todd Proebsting,and Xinchen Yu, for sharing the assignments, so-lutions, and syllabus with us. Finally, we thankthe anonymous reviewers for their feedback on theinitial draft of the paper. Malik Al-Essa, Giuseppina Andresini, Annalisa Appice,and Donato Malerba. 2022. An XAI-based Adver-sarial Training Approach for Cyber-threat Detection.In IEEE Intl. Conf. on Dependable, Autonomic andSecure Computing, Intl Conf on Pervasive Intelli-gence and Computing, Intl Conf on Cloud and BigData Computing, Intl Conf on Cyber Science andTechnology Congress, DASC/PiCom/CBDCom/Cy-berSciTech 2022, Falerna, Italy, September 12-15,2022, pages 18. IEEE. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang.2018. Generating natural language adversarial ex-amples. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,Brussels, Belgium, October 31 - November 4, 2018,pages 28902896. Association for ComputationalLinguistics.",
  "Nicholas Boucher and Ross Anderson. 2023. TrojanSource: Invisible Vulnerabilities. In 32nd USENIXSecurity Symposium, USENIX Security 2023, Ana-heim, CA, USA, August 9-11, 2023. USENIX Associ-ation": "Nicholas Boucher, Ilia Shumailov, Ross Anderson, andNicolas Papernot. 2022. Bad Characters: Impercep-tible NLP Attacks. In 43rd IEEE Symposium onSecurity and Privacy, SP 2022, San Francisco, CA,USA, May 22-26, 2022, pages 19872004. IEEE. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,Katherine Lee, Florian Tramr, and Chiyuan Zhang.2023. Quantifying memorization across neural lan-guage models. In The Eleventh International Con-ference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Nicholas Carlini,Florian Tramr,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom B. Brown, Dawn Song, l-far Erlingsson, Alina Oprea, and Colin Raffel. 2021.Extracting Training Data from Large Language Mod-els. In 30th USENIX Security Symposium, USENIXSecurity 2021, August 11-13, 2021, pages 26332650.USENIX Association. James Finnie-Ansley, Paul Denny, Brett A. Becker, An-drew Luxton-Reilly, and James Prather. 2022. Therobots are coming: Exploring the implications ofOpenAI Codex on introductory programming. InACE 22: Australasian Computing Education Con-ference, Virtual Event, Australia, February 14 - 18,2022, pages 1019. ACM. James Finnie-Ansley, Paul Denny, Andrew Luxton-Reilly, Eddie Antonio Santos, James Prather, andBrett A. Becker. 2023. My AI wants to know if thiswill be on the exam: Testing OpenAIs codex on CS2programming exercises. In Proceedings of the 25thAustralasian Computing Education Conference, ACE2023, Melbourne, VIC, Australia, 30 January 2023 -3 February 2023, pages 97104. ACM.",
  "Ray Hembree and Donald J Dessart. 1986.Effectsof hand-held calculators in precollege mathematicseducation: A meta-analysis. Journal for research inmathematics education, 17(2):8399": "Matthew Hertz. 2010. What do \"cs1\" and \"cs2\" mean?investigating differences in the early courses.InProceedings of the 41st ACM Technical Symposiumon Computer Science Education, SIGCSE 10, page199203, New York, NY, USA. Association for Com-puting Machinery. Muntasir Hoq, Yang Shi, Juho Leinonen, Damilola Ba-balola, Collin F. Lynch, and Bita Akram. 2023. De-tecting chatgpt-generated code in a CS1 course. InProceedings of the Workshop on Empowering Educa-tion with LLMs - the Next-Gen Interface and ContentGeneration 2023 co-located with 24th InternationalConference on Artificial Intelligence in Education(AIED 2023), Tokyo, Japan, July 7, 2023, volume3487 of CEUR Workshop Proceedings, pages 5363.CEUR-WS.org.",
  "Leonard A. Jason and David S. Glenwick. 2015. Hand-book of Methodological Approaches to Community-Based Research:Qualitative, Quantitative, andMixed Methods. Oxford University Press": "Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu,and Emily Morgan. 2023. Large language modelsand simple, stupid bugs. In 20th IEEE/ACM Interna-tional Conference on Mining Software Repositories,MSR 2023, Melbourne, Australia, May 15-16, 2023,pages 563575. IEEE. Haoming Jiang, Pengcheng He, Weizhu Chen, Xi-aodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.Smart: Robust and efficient fine-tuning for pre-trained natural language models through principledregularized optimization. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics. Association for Computational Linguis-tics. Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen,and Xiang Ren. 2022.A good prompt is worthmillions of parameters: Low-resource prompt-basedlearning for vision-language models. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2022, Dublin, Ireland, May 22-27, 2022, pages27632775. Association for Computational Linguis-tics. Joyce Koh and Ben Daniel. 2022. Shifting online dur-ing covid-19: A systematic review of teaching andlearning strategies and their outcomes. InternationalJournal of Educational Technology in Higher Educa-tion, 19.",
  "Aiwei Liu, Honghai Yu, Xuming Hu, Shuang Li,Li Lin, Fukun Ma, Yawen Yang, and Lijie Wen": "2022. Character-level White-Box Adversarial At-tacks against Transformers via Attachable SubwordsSubstitution. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2022, Abu Dhabi, United Arab Emirates,December 7-11, 2022, pages 76647676. Associationfor Computational Linguistics. Bowen Liu, Boao Xiao, Xutong Jiang, Siyuan Cen,Xin He, Wanchun Dou, and Huaming Chen. 2023a.Adversarial attacks on large language model-basedsystem and mitigating strategies: A case study onChatGPT. Sec. and Commun. Netw., 2023.",
  "Accessed September25, 2023": "Michael Sheinman Orenstrakh, Oscar Karnalim, Car-los Anbal Surez, and Michael Liut. 2023. Detect-ing LLM-Generated Text in Computing Education:A Comparative Study for ChatGPT Cases. CoRR,abs/2307.07411. Adnan Quaium, Najla Abdulrahman Al-Nabhan, Mas-fiqur Rahaman, Saiful Islam Salim, Tarik Reza Toha,Jannatun Noor, Mainul Hossain, Nafisa Islam, Aaiy-eesha Mostak, Md Shihabul Islam, Md. MasumMushfiq, Ishrat Jahan, and A.B.M. Alim Al Islam.2023. Towards associating negative experiences andrecommendations reported by hajj pilgrims in a mass-scale survey. Heliyon, 9(5).",
  "Jonas Rauber, Wieland Brendel, and Matthias Bethge.2017. Foolbox v0.8.0: A python toolbox to bench-mark the robustness of machine learning models.CoRR, abs/1707.04131": "Laria Reynolds and Kyle McDonell. 2021.Promptprogramming for large language models: Beyond thefew-shot paradigm. In CHI 21: CHI Conferenceon Human Factors in Computing Systems, VirtualEvent / Yokohama Japan, May 8-13, 2021, ExtendedAbstracts, pages 314:1314:7. ACM. Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gor-don, Guillaume Sicard, and Eli (Omid) David. 2020.Generating end-to-end adversarial examples for mal-ware classifiers using explainability. In 2020 Interna-tional Joint Conference on Neural Networks, IJCNN2020, Glasgow, United Kingdom, July 19-24, 2020,pages 110. IEEE. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,Wenhan Xiong, Alexandre Dfossez, Jade Copet,Faisal Azhar, Hugo Touvron, Louis Martin, Nico-las Usunier, Thomas Scialom, and Gabriel Synnaeve.2023. Code llama: Open foundation models for code.CoRR, abs/2308.12950.",
  "Vinu Sankar Sadasivan, Aounon Kumar, Sriram Bala-subramanian, Wenxiao Wang, and Soheil Feizi. 2023.Can ai-generated text be reliably detected?": "Shaul Shalvi, Jason Dana, Michel JJ Handgraaf, andCarsten KW De Dreu. 2011. Justified ethicality: Ob-serving desired counterfactuals modifies ethical per-ceptions and behavior. Organizational behavior andhuman decision processes, 115(2):181190. Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pe-dram Zaree, Yue Dong, and Nael Abu-Ghazaleh.2023. Survey of vulnerabilities in large languagemodels revealed by adversarial attacks.arXivpreprint arXiv:2310.10844. Rakshith Shetty, Bernt Schiele, and Mario Fritz. 2018.A4NT: Author Attribute Anonymity by Adversar-ial Training of Neural Machine Translation.In27th USENIX Security Symposium, USENIX Secu-rity 2018, Baltimore, MD, USA, August 15-17, 2018,pages 16331650. USENIX Association. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H. Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Interna-tional Conference on Machine Learning, ICML 2023,23-29 July 2023, Honolulu, Hawaii, USA, volume202 of Proceedings of Machine Learning Research,pages 3121031227. PMLR. Ruoxi Sun, Minhui Xue, Gareth Tyson, Tian Dong,Shaofeng Li, Shuo Wang, Haojin Zhu, Seyit Camtepe,and Surya Nepal. 2023. Mate! are you really aware?an explainability-guided testing framework for ro-bustness of malware detectors. In Proceedings of the31st ACM Joint European Software Engineering Con-ference and Symposium on the Foundations of Soft-ware Engineering, ESEC/FSE 2023, San Francisco,",
  "Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan,Ruoxi Jia, Bo Li, and Jingjing Liu. 2021a. Infobert:Improving robustness of language models from aninformation theoretic perspective": "Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadal-lah, and Bo Li. 2021b. Adversarial GLUE: A multi-task benchmark for robustness evaluation of languagemodels. In Proceedings of the Neural InformationProcessing Systems Track on Datasets and Bench-marks 1, NeurIPS Datasets and Benchmarks 2021,December 2021, virtual. Jindong Wang, Xixu HU, Wenxin Hou, Hao Chen,Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye,Haojun Huang, Xiubo Geng, Binxing Jiao, YueZhang, and Xing Xie. 2023a.On the robustnessof ChatGPT: An adversarial and out-of-distributionperspective. In ICLR 2023 Workshop on Trustworthyand Reliable Large-Scale Machine Learning Models.",
  "Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, andAnh Nguyen. 2023b. Generating valid and naturaladversarial examples with large language models": "Michel Wermelinger. 2023. Using github copilot tosolve simple programming problems. In Proceedingsof the 54th ACM Technical Symposium on ComputerScience Education, Volume 1, SIGCSE 2023, Toronto,ON, Canada, March 15-18, 2023, pages 172178.ACM. Lei Xu, Alfredo Cuesta-Infante, Laure Berti-quille,and Kalyan Veeramachaneni. 2022. R&r: Metric-guided adversarial sentence generation. In Findingsof the Association for Computational Linguistics:AACL-IJCNLP 2022, Online only, November 20-23,2022, pages 438452. Association for ComputationalLinguistics.",
  "B.1Course Description": "This course provides a continuing introductionto programming with an emphasis on problem-solving. It considers problems drawn from var-ious domains (including Computer Science). Itemphasizes both the broader applicability of therelevant data structures and programming concepts,as well as the implementation of those structuresand concepts in software. Topics include arrays,lists, stacks, queues, trees, searching and sorting,",
  "DLLM Code Generation Methodology": "CodeRL.To initiate code generation withCodeRL, we first create an instance of the tokenizerand model using the HuggingFace API. To ensureobtaining the best solution, we set the temperatureto 0 and the output token limit to its maximum al-lowable limit. Then, we tokenize the prompt andsend it to the model. The model generates a list oftokens from the given prompt of tokens. After deto-kenizing the output, we get a source code, whichserves as the solution to the given assignment prob-lem. In a file jaccard.py write a function jaccard(set1, set2)that takes as arguments two sets set1 and set2 andreturns a floating-point value that is the Jaccardsimilarity index between set1 and set2.The definitionof the Jaccard similarity index is (see also: .Bof the long problem spec; Wikipedia):",
  "(a) Short problem": "In a file update_board.py write the following functions:update_board(board, mov): board is an internalrepresentation of a board position, mov is a tuple ofintegers specifying a move. It returns the internalrepresentation of the board resulting from making themove mov in board board. update_board_interface(board_str, mov): board_str is anexternal representation of a board position (a string of0s and 1s), mov is a tuple of integers specifying a move.This function converts board_str to your internalrepresentation of a board position, calls your functionupdate_board() described above, converts the valuereturned by update_board() to an external representationof a board (a string of 0s and 1s), and returns theresulting string. This function thus serves as theexternal interface to your update_board() function.",
  ": Examples of short and long problems": "GitHub Copilot.To generate code with Copilot,we employ PyAutoGUI to automate VS Code.The step-by-step process starts with opening VSCode in a new window and creating a new Pythonfile.We paste the prompt into the file, sur-rounded by a docstring comment. Next, we askCopilot to generate multiple variations of code ina new window using the custom keyboard short-cut. Then, we close the VS Code after savingthe responses in separate files. The subsequentsteps vary based on the type of problem. For shortproblems, we handle cases where the code caneither be a standalone program generating out-put or a function/class definition.In the lattercase, the code generation is done for that specificcode. Conversely, for standalone programs, weadd the if __name__ == '__main__': blockat the bottom of the file and let Copilot call thegenerated function/class. At this point, Copilotprovides inline suggestions rather than separatewindows for alternatives. For longer problems,we reopen the generated code in VS Code and",
  ": An example CS1 problem where CodeRL,GPT-3.5and GitHub Copilot scored 0%": "allow Copilot to provide up to 15 inline sugges-tions.However, if Copilot generates its ownif __name__ == '__main__': block, we stop,as further code generation may lead to uncompil-able results.As both short and long problems can generateup to 10 solutions for a single prompt, we run allgenerated solutions through autograders and selectthe one with the highest score for evaluation. Thismethodology ensures efficient code generation andselection of the most appropriate solution for thegiven prompt.",
  ": Prompt to generate source code from GPT-3.5": "GPT-3.5.We use the OpenAI API to gener-ate code using GPT-3.5.Specifically, we usethe gpt-3.5-turbo-0301 model to ensure con-sistency throughout our experiments. Similar toCodeRL, we set the temperature to 0 to obtain themost optimal source code deterministically. SinceGPT-3.5 is a general-purpose language model not specifically designed for code generation only, weadd qualifying sentences around the prompt in-structing GPT-3.5 to omit explanations and pro-duce only code (since non-code explanatory textcould induce syntax errors in the autograder). Fig-ure 6 shows the prompt we use to generate codefrom GPT-3.5. This way, we exclusively receivecode outputs from the model. Mistral.We used the Mistral API to gener-ate code using Mistral.Specifically, we usedthe mistral-large-2402 model to ensure consis-tency throughout our experiments. Because Mis-trals API is very similar to OpenAIs API, wefollowed the same methodology and used the samemodel parameters to interact with the API. Code Llama.We used Ollama, a lightweight andextensible framework for running LLMs on lo-cal machines, to host the CodeLlama-7b-instructmodel based on Metas Llama 2.The instructmodel was chosen as it is trained to output human-like answers to given queries, which we believedto be closest to ChatGPT in terms of the generatedsolutions. The steps include installing Ollama andsimply calling ollama run codellama:7b-instruct<prompt> to generate the outputs. To the best ofour knowledge, there isnt a straightforward way totweak the parameters of the models from the pro-vided user manuals, so we used the default model.Although the generated answers often containedcomment blocks as well as codes, most outputswrapped the code blocks with identifiable textssuch as , [PYTHON] or python, we extractedthe codes accordingly. Otherwise, we simply usedthe generated output.",
  "E.1Core perturbations": "Token (remove): Breaking subword tokens pro-foundly impacts LLM performance (Liu et al.,2022; Wang et al., 2021b). By consulting SHAP,in this technique, we remove the top 5 tokens fromthe assignment description and create 1 perturbedvariant of a given assignment. We generated 63short and 12 long variants in total.Character (remove): Following the same princi-ple as Token (remove) to break subwords, in thisperturbation technique, we remove a random char-acter from each of the top 5 tokens to create 1 variant. We generated 63 short and 12 long variantsin total.Random (insert): To break subwords, we alsodesign another perturbation by inserting redundantcharacters, such as hyphens and underscores, in thetop 5 tokens; similarly, we generate 1 variant ofinserting redundant characters, such as hyphens andunderscores, into the top tokens in the assignments.We generated 63 short and 12 long variants in total.Sentence (remove): For sentence removal, we re-move a third of the sentence from the assignmentdescription sequentially. We chose one-third soas to not remove too much relevant information,and we removed sequential sentences to create alarge hole in the information provided to the mod-els. If the assignment description has less than 3sentences, we remove only 1 sentence. This pro-duces a variable number of perturbed variants. Wegenerated 594 short and 857 long variants in total.Sentence (rephrase): Rephrasing of sentences isknown to be effective in degrading LLM perfor-mance (Xu et al., 2022; Morris et al., 2020; Alzan-tot et al., 2018; Wang et al., 2021b). Thus, weleverage rephrasing sentences to design this pertur-bation. First, we rank the sentences by accumulat-ing the Shapley values of the tokens correspondingto a given sentence; then, we remove the top 3 sen-tences to create 3 independent variants. We useGPT-3.5to obtain high-quality phrases. We gener-ated 177 short and 32 long variants in total.Token (synonym): Tokens are the building blocksof language models, which have been used as per-turbation units in context (Boucher and Anderson,2023; Al-Essa et al., 2022; Wang et al., 2021b).Therefore, we design a perturbation technique. tosubstitute tokens with their synonyms. Specifically,we replace the top 5 tokens from the SHAP withtheir synonyms to create 5 different variants. Foreach top-ranked token, we replace all instances ofthat token in the prompt with its synonym, evenif other occurrences are not top-ranked. We dothis to ensure that if the token provides necessaryinformation to the model, it cannot be obtainedfrom another token occurrence in the assignmentdescription. We generate contextual synonyms fora given token using GPT-3.5. We provide the sen-tence containing the token as the context for theGPT-3.5 model and ask for synonyms for the token.We generated 1836 short and 216 long variants intotal.Token (unicode): Recent research shows that ad-versarial attacks can be effective even in a black- box setting without visually altering the inputsin ways noticeable to humans, which includes re-placing characters with Unicode lookalikes (Shettyet al., 2018; Boucher et al., 2022). To leverage this,we create a perturbation method to replace char-acters in the top 5 tokens (from SHAP) with theirUnicode lookalikes to create 1 variant ().We generated 63 short and 12 long variants in total.",
  "E.2Exploratory Perturbations": "Tokens (synonym): To understand the potential ofsynonym-based perturbation, we create a new typeof perturbation method to replace the top 5 tokensfrom the SHAP with their synonyms to create 5different variants. However, we do not replace thetop-ranked occurrences of a given token not alloccurrences in a given assignment prompt. Wegenerated 2373 short and 223 long variants in total.Prompt (Unicode): Similarly, to study the fullpotential of substituting characters with Unicodelookalikes, we apply it to the whole assignmentstatement under this technique. We recognize thatthis perturbation might easily get noticed; however,we add it to understand how detectability mightimpact the actual performance in the field study.We generated 63 short and 12 long variants in total.Random (replace): Existing studies show evi-dence that LLMs are prone to memorizing trainingdata (Zhang et al., 2021; Carlini et al., 2021, 2023).Thus, these models are highly sensitive to inputvariations, and even slight changes in the promptmay lead to substantial differences in the gener-ated output (Zhang et al., 2022; Jin et al., 2022;Reynolds and McDonell, 2021). Under this hypoth-esis, replacing specific tokens with random stringsmay significantly influence performance, as suchsubstitution may alter the context (Shi et al., 2023;Liu et al., 2023b; Wang et al., 2021b). We design anew exploratory perturbation technique to leverage this insight. Under this technique, we tweak as-signments by replacing file names, function names,and class names specified in the problem statementwith random strings, where these names are discov-ered manually. We store the original names andrandom strings, then in the code generated by themodels, replace the instances of the random stringswith the original names. This is to make sure thatthe autograders dont give a score of 0 for a goodsolution that uses the random string. We generated63 short and 12 long variants in total.",
  ": User Study Questions": "QuestionsHow proficient are you in the Python programming language?How hard did the problem seem to you while you were solving it? (For eachproblem)How much time (in minutes) did you spend on this problem? (For eachproblem)How did you validate the ChatGPT-generated solutions? (For each problem)Did you notice anything unusual about the problem statement? (For eachproblem)How did you avoid the unusualness in the problem statement while solvingthe problem? (For each problem)On average, how many hours do you dedicate to coding or problem-solvingper week?How often do you utilize ChatGPT or any other Large Language Model tosolve problems on a weekly basis, on average?What other Large Language Models do you use or previously used?",
  "Random (replace)11": "We manually go through 50% (15 out of 30) re-sponses in this stage. This allows us to performinductive coding to identify potential codes for fur-ther analysis. In the second stage, two authorsgenerated 16 initial codes based on their familiaritywith the data. These codes are data-driven and helporganize information into meaningful units. Twoauthors assign codes to the participants responsesto the specific questions. This coding stage is donemanually. To address disagreements, the authorsfacilitated a consensus-based resolution while com-bining their coding assignments. Consensus-basedresolution is considered important in qualitativestudies to produce meaningful insights. In our case,there were 4 disagreements between the two raterswhile labeling all 30 participants data. After that,one of the authors reviews the students responsesand corresponding conversations with ChatGPT toget the most information and update the coding.This step is iterative until saturation. We considerthe coding to be saturated if no new code is as-signed to the responses. Lastly, the other authorvalidates the final coding to avoid potential bias.In the third stage, after coding the data, we startsearching for themes by bringing together materialunder the same codes. This involves consideringhow codes may form broader themes that are orga-nized hierarchically. In the fourth stage, we reviewand refine the potential themes.Codebook for neutralizing perturbations:",
  "G.1Voluntary Participation": "You are being asked to participate in a researchstudy. Your participation in this research study isvoluntary. You may choose to voluntarily discon-tinue participation in the study at any time withoutpenalty, even after starting the survey. This doc-ument contains important information about thisstudy and what to expect if you decide to partic-ipate. Please consider the information carefully.Feel free to ask questions before deciding whetherto participate.Through this study, we will understand how wellwe can solve CS1 and CS2-level programmingtasks using AI tools such as ChatGPT. The sur-vey consists of three CS introductory assignmentproblems for each student. For each problem, youhave to solve it using ChatGPT and then answer thefollow-up questions. We estimate that the wholeprocess will take around 45-60 minutes. You arefree to take the survey anywhere you choose. Youwill be emailed the survey to complete, and you will need to provide your email address in the sur-vey.By signing up you are agreeing that you tookCS1 and CS2. You will proceed with the studyonce the verification of your historical enrollmentin the CS1 and CS2 courses is confirmed with themoderator of the CS undergraduate listserv (Mar-tin Marquez, Director of Academic and SupportServices, CS). Education records used by this re-search project are education records as defined andprotected by the Family Educational Rights andPrivacy Act (FERPA). FERPA is a federal law thatprotects the privacy of student education records.Your consent gives the researcher permission toaccess the records identified above for researchpurposes.",
  "G.3Incentive": "You will receive a $20 Amazon e-gift card for com-pleting the survey in full. To receive your $20award, please contact the Anonymized author. Hewill then check that you have completed the surveyin full using your email and arrange the payment.You must collect your reward within one month ofcompleting the survey. For any compensation youreceive, we are required to obtain identifiable infor-mation such as your name and address for financialcompliance purposes. However, your name will P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17 P18 P19 P20 P21 P22 P23 P24 P25 P26 P27 P28 P29 P30 Code review w/o run Code review w/ run Given test cases Manual test cases ChatGPT test cases Manual fix ChatGPT fix ChatGPT correct ChatGPT incorrect ChatGPT validation Compare to manual code 0.0 0.5 1.0 1.5 2.0 2.5 3.0 : The vertical axis lists the most frequent validation strategies, while the horizontal axis representsparticipants. Each cells value, capped at 3, indicates the number of times a specific code was applied to aparticipants response across three problems. The color gradient ranges from bright yellow (indicating 0 occurrences)to dark blue (indicating 3 occurrences). Prompt (original) Character (remove) Token (unicode) Token (remove) Sentence (rephrase) Sentence (remove) Prompt (unicode) Random (replace) Number of Occurrences No unusualness foundExpected to be bypassedUpdate problem statementRewrite incorrect/missing portionRewrite incorrect ChatGPT solution",
  "G.4Confidentiality of Data": "Your information may be used for future research orshared with another researcher for future researchstudies without additional consent. In addition,your email addresses will be deleted from the re-sponse spreadsheets, which will be stored on apassword-secured local server computer accessibleonly by the research team members. The form con-taining the list of student emails that signed up toparticipate will be deleted once all surveys are com-plete. Once the entire research project is completeand the conference paper is published, anyone canview the results of the survey by referring to theconference website. The conference at which this paper will be accepted cannot be guaranteed at thismoment.The information that you provide in the studywill be handled confidentially. However, there maybe circumstances where this information must bereleased or shared as required by law. The Insti-tutional Review Board may review the researchrecords for monitoring purposes.For questions, concerns, or complaints about thestudy, you may contact the Anonymized author. Bycompleting the entire survey, you are allowing yourresponses to be used for research purposes."
}