{
  "Abstract": "Transformer-based large language models(LLMs) exhibit limitations such as generat-ing unsafe responses, unreliable reasoning, etc.Existing inference intervention approaches at-tempt to mitigate these issues by finetuningadditional models to produce calibration sig-nals (such as rewards) that guide the LLMsdecoding process. However, this solution in-troduces substantial time and space overheaddue to the separate models required. This workproposes NOn-disruptive parameters insertion(Otter), inserting extra parameters into thetransformer architecture to predict calibrationsignals along with the original LLM output. Ot-ter offers state-of-the-art performance on multi-ple demanding tasks while saving up to 86.5%extra space and 98.5% extra time. Further-more, Otter seamlessly integrates with exist-ing inference engines, requiring only a one-line code change, and the original model re-sponse remains accessible after the parameterinsertion. Our code is publicly available at",
  "Introduction": "Transformer-based Large Language Models (LLM)have demonstrated remarkable prowess in a widerange of natural language processing tasks withhuman-like proficiency, including text genera-tion (Bai et al., 2023; Yang et al., 2024), trans-lation (Wu et al., 2023), summarization (Vassiliouet al., 2023; Yuan et al., 2023; Zhang et al., 2023a;Vaswani et al., 2017), etc. However, extensive in-spections from multiple directions have revealedLLMs are not flawless. For instance, LLM occa-sionally produces unsafe or toxic outputs (Denget al., 2023; Khanov et al., 2024), and in reasoningcapabilities, yielding unreliable results for mathe-matical proofs (Yu et al., 2024).",
  "Inference Intervention": "I read through theproject document. That idea is ... I read through the project document. That idea is perfect calibrate 0.78 terrible0.19 bad... calibrate 0.78 terrible0.19 bad... 0.92 perfect0.05 good... I read through theproject document.That idea is terrible 0.92 perfect0.05 good... : Comparison of inference intervention methodswith and without Otter for harmless response generation.By inserting parameters into the frozen LLM, Otter sig-nificantly reduces space and time costs, while enablingseamless online deployment. To address the above limitations of LLMs, aseries of approaches have been proposed, whichcan be broadly categorized into two directions finetuning and inference intervention.Finetun-ing methods leverage techniques such as knowl-edge graphs (Yuan et al., 2024) and data augmenta-tion (Yu et al., 2024; Bianchi et al., 2024) to curatevast amounts of high-quality data to align LLMswith preferred responses. Nevertheless, prior re-search has highlighted that continual pretrainingor finetuning can lead to unintended consequences,such as catastrophic forgetting (Jian et al., 2022;Zhai et al., 2024; Lu et al., 2024), and halluci-nation (Lin et al., 2023). These side effects canseverely undermine the overall performance andreliability of LLMs. To avoid associated side ef-fects, plug-and-play inference intervention as analternative has been proposed. The method em-ploys a calibration model to produce signals (suchas rewards), which are used to calibrate the de-coding tokens during LLM inference. When the intervention is not required, the calibration modelcan be unplugged, allowing the LLM to return toits original output. Benefiting from this, inferenceintervention achieves reliable reasoning, such asprocess-supervised reward models in mathemati-cal reasoning (Liu et al., 2021; Yu et al., 2023),reduces bias and harmful responses with reward-guided search (Khanov et al., 2024). Therefore,inference intervention paves a promising way toenhance LLMs capabilities.Regretfully, the success of inference interven-tion heavily relies on training additional calibrationmodels, which are typically large-scale models,and causes the space complexity to increase drasti-cally (Liu et al., 2021; Khanov et al., 2024). In ad-dition, as shown in , the inference time is gen-erally longer because the original models output isre-used as input for the calibration model, increas-ing the times of transformer forward passes (Yuet al., 2023). This introduces substantial space andtime overhead, making it challenging to deploythese methods online. Motivated by the powerfulknowledge inherent in LLMs, training an entirelynew model is not necessary. Instead, it is reason-able to utilize this knowledge by re-using the origi-nal parameters. Specifically, our objective is to in-troduce a small set of additional parameters into theoriginal LLM, allowing it to simultaneously gener-ate accompanying output (either reward predictionor language modeling) alongside its primary output.This accompanying output should behave the sameas the additional models in inference interventionmethods.Introducing additional parameters into LLMshas been indeed widely used in parameter-efficientfinetuning/adapting methods (PEFT), such asQLoRA and LoRA (Hu et al., 2022; Dettmers et al.,2023). These methods work by adding adaptedparameters to the original frozen parameters, i.e.,h = (W + W)x, where only the added W istrainable. However, by directly adding parametersto the original parameters, LoRA alters all modeloutputs. Consequently, LoRA is not suitable forinference intervention methods, where the originalmodel outputs (i.e., logits) and a calibration signalare required simultaneously.In this work, we propose a NOn-disruptiveparameters insertion (Otter) method for inferenceintervention in LLMs. The key idea behind Otteris to concatenate the added trainable parametersacross all components of the transformer, includingthe multi-head attention layer and the feed-forward neural network layer. By passing through all layers,the final extended hidden state can be mapped andutilized as an inference intervention signal. Com-pared with existing work, this study offers severalcontributions: 1. SOTA performance with lowest overhead.Otter saves up to 86.5% extra space and 98.5%extra time while obtaining comparable per-formance to state-of-the-art inference inter-vention methods on three high-demandingtasks: generation detoxification, preferencealignment, and inference acceleration. 2. Seamless integration. Otter can integrate newparameters into the existing model so that thewhole model can utilize the existing inferenceengine for efficient decoding with only one lineof code modification. 3. Retain raw model response. The originallanguage models output is always accessiblealongside the intervention signal. When theintervention is not required, Otter will not inter-fere with the original language models output,preventing unexpected performance degrada-tion.",
  "Inference Intervention for LLM": "Existing inference intervention methods employfine-tuned auxiliary models to guide LLM gener-ation. The OVM (Yu et al., 2023) model selectstop-k generations per step, improving mathematicalreasoning. ARGS (Khanov et al., 2024) fine-tunesa reward model to calibrate token probabilities togenerate more harmless responses. DEXP (Liuet al., 2021) uses side models mimicking toxic-discriminative chatbots to guide non-toxic gener-ation. Leviathan et al. (2023) fine-tunes smallerLLMs to generate distributions resembling largerLLMs, enabling parallel candidate token genera-tion for faster inference. These prior works requireadditional models to guide decoding, increasingspace needs, and multiple decoding iterations, lead-ing to time overhead. Our Otter approach insertstrainable parameters directly into the original LLM,serving as the additional model. This enables simul-taneous output of accompanying signals and origi-nal output with fewer parameters, reducing spaceand time overhead compared to existing methods.",
  "Parameter-Efficient Finetuning for LLM": "As pre-trained language models (PLMs) continueto grow in size, fine-tuning them becomes compu-tationally infeasible. To address this issue, pre-vious studies only fine-tuned a few parametersof PLMs, enabling comparable performance tofull fine-tuning. These parameter-efficient fine-tuning methods can be categorized as follows:i)-Low-rank adaption introduces a small num-ber of new parameters that modulate the origi-nal PLM parameters. LoRA (Hu et al., 2022)proposes adding a low-rank adaptor to fine-tuneLLMs. Subsequent QLoRA (Dettmers et al., 2023)and AdaLoRA (Zhang et al., 2023b), aim to fur-ther improve performance by quantizing the PLMand adaptively allocating parameter budgets, re-spectively. ii)-Prompt-tuning methods freeze thePLM and design task-specific prompts, with onlythe prompt-related parameters being fine-tuned,such as soft prompts (Lester et al., 2021) andprefix-tuning (Li and Liang, 2021). iii)-Adaptersalso efficiently transfer knowledge for downstreamtasks, with AdapterFusion (Pfeiffer et al., 2021)combining knowledge from multiple source tasks,K-Adaptor (Wang et al., 2021) infusing differentknowledge types, and AdapterHub (Pfeiffer et al., 2020) integrate pre-trained adapters across tasksand languages. These methods aim to modify themodels output to improve task performance, whichmay also cause unexpected issues like hallucina-tions (Lin et al., 2023). However, Otter overcomesthese problems by expanding the weight matrices.This makes the new model a larger version of theoriginal, adding capabilities without affecting theoriginal output.",
  "Method": "We introduce Otter for predicting calibration sig-nals alongside tokens in transformer-based LLMswithout disrupting their original output and knowl-edge. Our method involves inserting new trainableparameters into the feed-forward network (FFN)and multi-head attention (MHA) layers of the trans-former architecture. Specifically, for i-th blockof the transformer, we expand the original hiddenstate hi into hi = [hi, hi], thereby making it pos-sible to predict the inference intervention signalsbased on the last layer hn.",
  "hffn = Wd((hg) hu) + bd Rdinp(1)": "where () is an activation function and han Rdinp is the hidden state. We define dinp and dinneras the input dim. and inner dim. of the FFN layer,respectively, and refer to them henceforth.In order to insert new trainable parameters whilekeeping the original hidden states intact, we de-sign a method to expand the weight and bias matrixfor each linear projection in the FFN layer, as il-lustrated in (b). Specifically, taking thefirst linear projection in FFN as an example, ourmethod preserves the linear projection from han tohg unchanged and creates a new linear projectionfrom han = [han, han] to hg. To implement it effi-ciently, we expand each original weight matrix Wgby concatenating a frozen all-zero matrix O and atrainable adaptation weight matrix W g. The incor-poration of the all-zero matrix O ensures that theinitial output hg remains unaltered. Moreover, theentire computational process can be implementedin a manner identical to the original linear pro-jection without requiring any modifications to theexisting code. The only difference is the increasedsize of the input and output dimensions.",
  "hmha = Concat(head1, , headn)WO": "where the query Q, key K, and value V are ob-tained by linearly projecting hi, then split into mul-tiple heads (qm, km, vm) for attention computation.To expand the hidden states in the multi-headattention layer, we introduce extra attention headswhile not touching the original heads. As depictedin (c), we insert the new parameters intothe mapping matrices WQ, WK, WV , and WO us-ing the same method described in the FFN layeradaptation section, where the outputs of projectionsare then split into original frozen attention heads,such as q1, k1, v1, and added trainable attentionheads, such as q1,k1,v1. Then we apply the multi-head attention operation on them and subsequently concatenate the outputs from both the original andnewly introduced attention heads at the end of thislayer. Note that, this adaption only extends the sizeof hidden states and the number of attention heads,where no code modification is required, thereforeseamlessly integrating efficient attention implemen-tation such as FlashAttention (Dao et al., 2022) andPagedAttention (Kwon et al., 2023).",
  "Layer Norm Regularization": "In Otter, although the original input and outputremain unchanged within each block of the trans-former model, the layer normalization operationuses the mean and variance of the whole hiddenstates for normalization, which may disrupt theoriginal hidden states. Therefore, we strictly en-force the use of only the original hidden state (hmhaor hffn) to compute the variance for layer nor-malization. This ensures that the models outputremains consistent with the original models be-havior. Following common LLM choice, the nor-malization process after the FFN layer is definedusing RMSNorm (Zhang and Sennrich, 2019) inthe following equation:",
  "mean(h2ffn) +": "where is learnable affine transform weights and is a small constant for stability. Please note that thisis the only part of the method that requires modifi-cation to the inference codes, and this modificationis relatively straightforward and will not affect theinference performance. We show that only one-linemodification is needed in Appendix C.To ensure the training stability of this restriction,a regularization term is introduced in the trainingobjective L during the fine-tuning stage. In ad-dition to the task-specific loss, we minimize thevariance and mean difference between the originalhidden state hi and the entire hidden state hi.",
  "Experiments": "To comprehensively assess Otters capabilities,we consider three tasks: human preference align-ment (Khanov et al., 2024), detoxification (Liuet al., 2021), and inference speed-up (Cai et al.,2024). The evaluation involves various languagemodels, including Llama (Touvron et al., 2023),Vicuna (Zheng et al., 2024), and GPT-2 (Radfordet al., 2019).In addition to task-specific evaluation metrics,we analyze the computational overhead in terms ofspace and time complexity. We define the spaceoverhead as the ratio of GPU memory consumedby the modified model to that of the original modelduring inference. The time overhead is definedas the ratio of the time consumed by the modifiedmodel to the time consumed by the base model.All inference interventions are performed on a sin-gle A100 GPU with the Huggingface transformerslibrary (Wolf et al., 2019).In every task, we introduce an ablation of Otter,denoted as Task Head Only, which solely adds atrainable linear layer on the top of the frozen LLM.This linear layer is then fine-tuned to predict theintervention signals used in each individual task.",
  "Reward-guided Search for Helpful andHarmless Alignment": "The objective of this task is to enhance the capabil-ity of LLM to generate responses that are helpfuland harmless, aligning with human preferences.The reward-guided search-based method involvesfine-tuning a reward model, which is trained to as- sign higher values to text that is deemed helpfuland harmless. During the decoding process, the re-ward model is utilized to adjust the original LLMsprobabilistic predictions, thereby improving thealignment of generated responses with human pref-erences. Hyperparameters and detailed definitionscan be found in Appendix A.1.",
  "Setup": "Our target model is set to be Vicuna-7b. As abaseline, we implemented the original speculativedecoding (Leviathan et al., 2023) by finetuningTinyLlama (Zhang et al., 2024) as the draft model,denoted as Vicuna-draft.We also compare with Medusa model, whichinserts decoding heads into the original model in-stead of separate draft models for inference ac-celeration (Cai et al., 2024). Specifically, theseadditional heads in Medusa and Otter predict thenext tokens in parallel, making the process moreefficient than the traditional speculative decodingmethod that predicts draft tokens sequentially. Notethat Medusa is the Task Head Only baseline in thisexperiment.Dataset: For training the Otter parameters, weutilize the ShareGPT dataset (Zheng et al., 2024),a public website where users share conversationswith ChatGPT. The dataset contains 60k train-ing samples, which are mostly overlapped withtraining samples of Vicuna-7b model. For evalua-tion, following Medusa, we use MT-Bench (Zhenget al., 2024), a multi-turn, and comprehensiveconversational-format benchmark.Evaluation Metrics: Following Medusa, we usethree metrics: a) Average accepted length: theaverage number of tokens decoded per decodingstep (1.0 for standard auto-regressive models). b)Time overhead: The ratio of time cost per modifiedmodel forward pass to the time cost by the basemodel. c) Speedup: the wall-time acceleration rate,",
  "Results": "Otter Significantly Reduces ARGS Overhead.As an inference intervention method, ARGS canbe deployed under both common decoding set-tings: greedy decoding, top-k decoding, and top-p decoding.As shown in , ARGS sig-nificantly improves the base models preferencevalue and response diversity and coherence. How-ever, this achievement comes at a considerable cost.ARGS model doubles both the time and spaceconsumption compared to the base model. Forexample, compared with base model greedy de-coding, ARGS-greedy incurs 2.07 times the timecost and requires 2.02 times space overhead dur-ing inference. In contrast, the Otter model reducesthe time overhead from 2.07x to 1.03x, yielding1 1.031",
  "= 97.2% saving in additional time com-pared to ARGS-greedy. Similarly, Otter reducesthe space overhead from 2.02x to 1.26x, resulting ina 74.5% reduction in additional space over ARGS": "Otter exhibits comparable alignment quality toARGS. As demonstrated in , Topk+ARGSand our method achieve average rewards of 4.787and 4.694, respectively, both surpassing the baseTop-k sampling model. Despite its lower compu-tational complexity, Otter attains a performancecomparable to ARGS reward-guided search taskperformance. To mitigate potential biases intro-duced by the pretrained reward model, followingthe ARGS, we additionally employ a GPT-4-basedevaluation approach for comparing response qual-ity by measuring the win-tie rate. The prompt canbe found in Appendix A.1.3. As shown in ,compared to the base model, both ARGS and Ottermaintain similar winning-tie rates. For example,for all prompts where ARGS wins against the basemodel under greedy decoding, Otter can win 97.5%of those prompts.",
  "Controlled Bi-Experts Generation forReducing Toxicity": "The task aims to mitigate the potential generationof toxic responses from LLMs. The controlleddecoding-time experts generation method (DEXP,Liu et al., 2021) addresses this issue by employingtwo additional models: an expert model trained togenerate non-toxic text, and an anti-expert modeltrained to generate toxic text. During the inferenceprocess of the original model, the next token proba-bility distributions generated by these two auxiliarymodels are utilized to calibrate the original models",
  "token generation probabilities, thereby reducing thelikelihood of generating toxic content. More hyper-parameters details can be found in Appendix A.2": "4.2.1SetupFollowing DEXP, we use GPT-2-large (Radfordet al., 2019) as the base model.Datasets: We follow the setup in DEXP and usethe human-annotated comments from the JigsawUnintended Bias in Toxicity Classification Kagglechallenge to train Otter. we use the 10K non-toxicprompts sampled by DEXP from the RealToxici-tyPrompts dataset (Gehman et al., 2020).EvaluationMetrics:Followingpreviouswork (Liu et al., 2021; Gururangan et al., 2020),we apply Perspective API to measure the toxicitylevel of responses.1We evaluate generationfluency using the mean perplexity of generatedcontinuations based on a larger pre-trained GPT-2XL model. Generation diversity is measured by themean number of distinct n-grams, normalized bytext length, among 25 generations for each prompt.We report Dist-1, Dist-2, and Dist-3 scores fordistinct uni-, bi-, and trigrams, respectively. 4.2.2ResultsAs shown in , Otter achieves similar or bet-ter performance compared to the original DEXPmodel. We explore two settings: 1) using onlythe anti-expert for calibration, and 2) using boththe anti-expert and expert for calibration. In theanti-expert-only setting, DEXP achieves an aver-aged maximum toxicity of 0.352, while Otter per-forms better with a score of 0.314. Notably, Ottermaintains a similar generation quality to DEXP, asdemonstrated by comparable perplexity and diver-sity scores.Importantly, while the (anti-)expert models inDEXP have the same size as the original model,resulting in a 3.05x space overhead and a 2.97x in-ference time overhead, Otter significantly reducesthese overheads to 1.13x and 1.03x, respectively.This makes Otter a more efficient and scalable so-lution for detoxifying text generations.",
  "where Speedup = Average accepted length / Timeoverhead": "4.3.2ResultsOtter significantly outperforms Vicuna-draftand Medusa in terms of inference speed-up. Asshown in , compared to conventional spec-ulative decoding (e.g., Vicuna-draft), Otter andMedusa significantly reduced the space and timeoverhead by reducing inference times and addedparameters. Otter can achieve 2.721.0 1.0= 172%speed gain compared to the base model, yielding a45.5%/14.8% gain compared to Vicuna-draft andMedusa, respectively. Although Otter introducedmore parameters inside the transformer model,leading to slightly higher time and space over-head compared to Medusa, these additional param-eters were well-integrated with the original parame-ters, resulting in a much higher average acceptancelength and speedup.",
  ".64x7.60B256061.071.232.852.66x7.67B512041.071.252.882.69x7.98B12834401.081.142.602.41x7.02B": ": The comparisons of efficient Otter parameters insertion in FFN vs. MHA. Input and Inner dim. are inserteddims to the FFN layer. # AttnHead is the number of inserted attention heads. For the same time overhead, insertingparameters into the MHA layer achieves a 19.9% speedup gain over FFN layer insertion in speculative decoding.",
  "Epochs": "2.2 2.4 2.6 Speed-up Ratio Speed-up Ratio copynormrand Preference AlignmentSpeculative Decoding : The comparisons of initialization methods effectiveness on speculative decoding and preference alignment.copy, norm, and rand denote Parameter Copying, Normal Initialization, and Random Initialization, respectively.Parameter Copying boosts the training efficiency and generalization of Otter compared to others. The loss in thepreference alignment task reflects the training of the reward model. Higher average reward values in this taskindicate better alignment. In speculative decoding, the Top-1 medusa head Acc. measures the average next tokenprediction accuracy of the first medusa decoding head during training. Higher accuracy corresponds to a higheracceleration ratio. The speed-up ratio in this task quantifies the acceleration achieved against the base model,evaluated at each training epoch. of inserted parameters such that the time overheadremains constant in speculative decoding. As illus-trated in , our experiments revealed that forthis task, adding parameters to the MHA layer sig-nificantly improved inference speed compared toaugmenting the FFN layer. These findings suggestthat for better alignment with the original modelin speculative decoding, the multi-head attentioncomponent plays a more crucial role than the FFNlayer. In contrast to previous research that positedthat expanding the FFN parameters should be prior-itized as the FFN layer is responsible for preservingknowledge (De Cao et al., 2021), this observationprovides an alternative insight into parameter ex-tension efficiency.",
  "Normal Initialization: parameters were ini-tialized by sampling from normal distributionswith means and variances matching those ofthe original model parameter groups": "Parameter Copying: parameters were initial-ized by randomly copying from the originalmodel. For inserted FFN dimensions, param-eters were copied from the original FFN layerand truncated as needed. For the MHA layer, ifmultiple new heads were inserted, each head issampled from the original. As illustrated in , both Normal Initializa-tion and Parameter Copying methods outperformedRandom Initialization on speculative decoding andpreference alignment tasks. This outcome is ex-pected, as the former two methods leverage thedense information from the original model parame-ters, reducing the likelihood of converging to localminima during training. Notably, while Normal Ini-tialization and Parameter Copying achieved similarperformance on the training set, the Otter modelinitialized with Parameter Copying exhibited bet-ter generalization capabilities, as evidenced by its",
  "Conclusion": "Otter concatenates a small set of trainable parame-ters to the transformer architecture, enabling addi-tional output for inference intervention while pre-serving the original models behavior and compu-tational efficiency. Experiments demonstrate Ot-ters efficacy in aligning outputs with human pref-erences, mitigating toxicity in text generation, andimproving inference speedachieving these en-hancements with significantly less overhead thanexisting methods. As an extension of the samemodel architecture, Otter can seamlessly integratewith existing infrastructure, providing an appeal-ing solution for enhancing LLM performance effi-ciently for the community.",
  "Limitations": "While the Otter method offers a promising ap-proach to non-disruptive parameter insertion forefficient inference intervention in large languagemodels (LLMs), several aspects warrant furtherconsideration. Firstly, despite the fact that Otterand other intervention methods can reduce harm-ful or unreliable generated responses, there is stilla risk that the original LLM may generate suchresponses, and these methods cannot fully detectand calibrate them. This limitation may necessitateongoing efforts to carefully pre-train the originalLLM, such as cleaning the pre-training data andimplementing safety alignment through reinforce-ment learning from human feedback (RLHF). Sec-ondly, unlike other inference intervention methodsthat rely on separate calibration models, Otters in-sert parameters into the original model; thus, thehyperparameters such as the learning rate and ini-tialization methods might differ from those of otherapproaches. This discrepancy may require extraeffort in exploring the optimal hyperparameter set-tings specific to the Otters method. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609. Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.2022. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXivpreprint arXiv:2204.05862. Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio,Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, andJames Zou. 2024. Safety-tuned LLaMAs: Lessonsfrom improving the safety of large language modelsthat follow instructions. In The Twelfth InternationalConference on Learning Representations. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,Jason D Lee, Deming Chen, and Tri Dao. 2024.Medusa: Simple llm inference acceleration frame-work with multiple decoding heads. arXiv preprintarXiv:2401.10774. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, andChristopher R. 2022.Flashattention: Fast andmemory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems,35:1634416359. Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-ing factual knowledge in language models. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 64916506.",
  "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Li-dong Bing. 2023. Multilingual jailbreak challengesin large language models. In The Twelfth Interna-tional Conference on Learning Representations": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, andLuke Zettlemoyer. 2023. Qlora: Efficient finetuningof quantized llms. In Advances in Neural InformationProcessing Systems, volume 36, pages 1008810115.Curran Associates, Inc. Samuel Gehman, Suchin Gururangan, Maarten Sap,Yejin Choi, and Noah A. Smith. 2020. RealToxi-cityPrompts: Evaluating neural toxic degenerationin language models. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, pages33563369, Online. Association for ComputationalLinguistics. SuchinGururangan,AnaMarasovic,SwabhaSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,and Noah A. Smith. 2020. Dont stop pretraining:Adapt language models to domains and tasks. InProceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages",
  ", Online. Association for ComputationalLinguistics": "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations. Yiren Jian, Chongyang Gao, and Soroush Vosoughi.2022. Embedding hallucination for few-shot lan-guage fine-tuning. In Proceedings of the 2022 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 55225530.",
  "Yaniv Leviathan, Matan Kalman, and Yossi Matias.2023. Fast inference from transformers via spec-ulative decoding. In International Conference onMachine Learning, pages 1927419286. PMLR": "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Lin-guistics. Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, RenjiePi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, HanZhao, Yuan Yao, et al. 2023. Speciality vs gener-ality: An empirical study on catastrophic forgettingin fine-tuning foundation models. arXiv preprintarXiv:2309.06256. Alisa Liu,Maarten Sap,Ximing Lu,SwabhaSwayamdipta, Chandra Bhagavatula, Noah A Smith,and Yejin Choi. 2021. Dexperts: Decoding-time con-trolled text generation with experts and anti-experts.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages66916706.",
  "Keming Lu, Bowen Yu, Fei Huang, Fan Yang, Runji Lin,and Chang Zhou. 2024. Online merging optimizersfor boosting rewards and mitigating tax in alignment.arXiv preprint arXiv:2405.17931": "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rckl,Kyunghyun Cho,and Iryna Gurevych. 2021.Adapterfusion: Non-destructive task composition fortransfer learning. In Proceedings of the 16th Con-ference of the European Chapter of the Associationfor Computational Linguistics: Main Volume, pages487503. Jonas Pfeiffer, Andreas Rckl, Clifton Poth, AishwaryaKamath, Ivan Vulic, Sebastian Ruder, KyunghyunCho, and Iryna Gurevych. 2020. Adapterhub: Aframework for adapting transformers. In Proceed-ings of the 2020 Conference on Empirical Methodsin Natural Language Processing: System Demonstra-tions, pages 4654.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-peng Kong, and Nigel Collier. 2022. A contrastiveframework for neural text generation. Advances inNeural Information Processing Systems, 35:2154821561. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Giannis Vassiliou, Nikolaos Papadakis, and HaridimosKondylakis. 2023. Summarygpt: Leveraging chatgptfor summarizing knowledge graphs. In EuropeanSemantic Web Conference, pages 164168. Springer": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,Xuan-Jing Huang, Jianshu Ji, Guihong Cao, DaxinJiang, and Ming Zhou. 2021. K-adapter: Infusingknowledge into pre-trained models with adapters. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 14051418. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771.",
  "overview of chatgpt: The history, status quo andpotential future development. IEEE/CAA Journal ofAutomatica Sinica, 10(5):11221136": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,Bowen Yu, Chang Zhou, Chengpeng Li, ChengyuanLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-ran Wei, Huan Lin, Jialong Tang, Jialin Wang,Jian Yang, Jianhong Tu, Jianwei Zhang, JianxinMa, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai,Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-qin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni,Pei Zhang, Peng Wang, Ru Peng, Rui Men, RuizeGao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge,Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren,Xinyu Zhang, Xipin Wei, Xuancheng Ren, XuejingLiu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan,Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,Zhifang Guo, and Zhihao Fan. 2024. Qwen2 techni-cal report. Preprint, arXiv:2407.10671.",
  "Chenhan Yuan, Qianqian Xie, and Sophia Ananiadou.2023. Zero-shot temporal relation extraction withchatgpt. arXiv preprint arXiv:2304.05454": "Chenhan Yuan, Qianqian Xie, Jimin Huang, and SophiaAnaniadou. 2024. Back to the future: Towards ex-plainable temporal reasoning with large languagemodels. In Proceedings of the ACM on Web Confer-ence 2024, WWW 24, page 19631974, New York,NY, USA. Association for Computing Machinery. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, QingQu, Yong Jae Lee, and Yi Ma. 2024. Investigating thecatastrophic forgetting in multimodal large languagemodel fine-tuning. In Conference on Parsimony andLearning, volume 234 of Proceedings of MachineLearning Research, pages 202227. PMLR.",
  "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, andWei Lu. 2024. Tinyllama: An open-source smalllanguage model. Preprint, arXiv:2401.02385": "Qingru Zhang, Minshuo Chen, Alexander Bukharin,Pengcheng He, Yu Cheng, Weizhu Chen, andTuo Zhao. 2023b. Adaptive budget allocation forparameter-efficient fine-tuning. In The Eleventh In-ternational Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36.",
  "s(v, x<t) = LM(v|x<t) + w r([x<t, v])(2)": "Here, LM(v|x<t) represents the original modeloutput, r() is the reward model, w is the weight as-signed to the reward scalar, and [x<t, v] denotes theconcatenation of v to the previous context. Consis-tent with ARGS, w is set to 1.5 in our experiments.Two decoding strategies are used in ARGS:greedy and top-k decoding. For greedy-based de-coding, the token with the maximum score is cho-sen. For top-k decoding, tokens are sampled froma renormalized probability distribution among thetop-k candidate tokens, with the following proba-bility:",
  "A.1.3GPT-4 Evaluation": "Previous research (Zheng et al., 2024) has demon-strated that using GPT-4 as a proxy aligns withhuman evaluations over 80% of the time for qualityassessments, offering a scalable method to approx-imate human preferences. Following the ARGSmethodology, we prompt GPT-4 to assign scoresto 300 randomly sampled responses based on help-fulness, harmlessness, relevance, accuracy, and in-sightfulness. We show the system prompt we usedfor GPT-4 evaluation on the win-tie rate of ARGSand Otter against the baseline models. System prompt:You are a helpful,harmless, and precise assistant for check-ing the quality of the answer.Wewould like to request your feedback onthe performance of two AI assistantsin response to the user question dis-played above.Please rate the harm-lessness, helpfulness, and level of de-tail of their responses. Your evaluationshould consider factors such as the help-fulness, harmlessness, relevance, accu-racy, depth, creativity, and level of de-tail of the response. Each assistant re-ceives an overall score on a scale of 1 to10, where a higher score indicates betteroverall performance. Please first outputa single line containing only two valuesindicating the scores for Assistant 1 and2, respectively. The two scores are sepa-rated by a space. In the subsequent line,please provide a comprehensive explana-tion of your evaluation, avoiding any po-tential bias and ensuring that the orderin which the responses were presenteddoes not affect your judgment.",
  "A.1.4ARGS Evaluation Metrics": "Diversity: This metric aggregates n-gram repetitionrates across the generated texts. A higher diversityscore indicates the models capacity to produce abroad spectrum of vocabulary, enhancing the rich-ness and variability of the outputs.Coherence: This metric is estimated by calcu-lating the cosine similarity between the sentenceembeddings of the prompt and its continuation. Weutilize the pre-trained SimCSE sentence embeddingmodel (Su et al., 2022) to obtain the embeddings,quantifying the semantic coherence between theinput prompt and the generated text.",
  "A.2.1Hyperparameter Setting": "The hyperparameter settings for the Otter and train-ing process are summarized in . Duringthe Otter training phase, we first introduce the pos-itive expert Otter parameters and fine-tune theseparameters. Subsequently, we incorporate the nega-tive expert Otter on top of the positive expert Otterand fine-tune only the negative Otter parameters.For the generation hyperparameter settings, pleaserefer to the following section.",
  "A.2.3DEXP Decoding": "The DEXP approach introduces two additionalmodels, with the same size as the original model,to calibrate the model-generated response. Specif-ically, these two additional models are trained ontwo opposite datasets, non-toxic and toxic, respec-tively. Consequently, these two models are catego-rized as the positive expert and the negative expert.During inference, the original models output canbe calibrated by following the equation:",
  "A.3.1Hyperparameter SettingThe hyperparameter for Otter and training is shownin Table. 8.We use the same training hyper-parameters to fine-tune the TinyLlama to obtainVicuna-draft": "A.3.2Speculative DecodingThe Medusa approach trains additional languagemodel heads (i.e. Medusa heads) that are alignedwith the original language model head. This al-lows the newly trained Medusa heads to predictthe next k tokens simultaneously. Consequently,the training objective of speculative decoding is tomaximize the probability of the next i + 1 tokenfor the i-th head. This can be observed from theloss function:",
  "k=1klogp(k)t (yt+k+1)(6)": "where k denotes the k-th Medusa head, and k is ahyperparameter set as the k-th power of a constant,typically 0.8, following the Medusa setting.It is worth noting that in our experiment, we onlyadopt Medusa-1 as the baseline. This is becauseMedusa-2 alters the overall model output distri-bution, which may potentially lead to unintendedconsequences such as catastrophic forgetting andhallucination (Jian et al., 2022; Zhai et al., 2024;Lin et al., 2023).",
  "Draft Stage: We use the Otter-inserted Ht tomap to K new decoding heads. Each decod-ing head k predicts the next k + 1 tokens inparallel. Therefore, we obtained draft tokens:{xt, xt+1, , xt+k+1}": "Verification Stage: The new token list withthe draft tokens:{x1, x2, , xt, xt+1, , xt+k+1}is then verified by the original LLM. Sup-pose the first m tokens are accepted, then wehave the final current generation token list:{x1, x2, , xt, xt+1, , xt+m}. This newlist becomes the input in step 1 for the next it-eration. Note that Ht+m is already computedin Otter with the verification process. In the vanilla speculative decoding setting, theinference process takes several draft model forwardpasses to draft and one base model forward passfor verification. However, in the Otter/Medusa set-ting, the inference process will only use the k extraheads to draft and then take one base model forwardpass for verification. Draft tokens are predictedin parallel, and the draft heads have significantlyfewer parameters than draft models. This makesOtter/Medusa faster and less computationally in-tensive.",
  "DTraining Time Analysis": "In general, the training for Otter is faster than thebaselines, as only a few parameters need to betrained. To illustrate this difference, we providetraining time for the helpful and harmless humanpreference alignment task. All models were trainedon the same 4 NVIDIA A100 80G GPUs withDeepSpeed zero-stage 2. The per-device batch sizeis 1 and the gradient accumulation step is 8.",
  "EAdditional Preference AlignmentResults": "Besides Llama-7b-chat already experimented in Ta-ble 1, here we supplemented the ARGS and Ottermethods on the Llama2-7b-instruct model to eval-uate their performance on the human preferencealignment task. As shown in , the Llama2model itself performs better than Llama in termsof generating more helpful and harmless responses(Llama-7b obtained 3.981 and 3.757 average re-wards), both ARGS and Otter can further enhancethe models performance, leading to higher aver-age reward. More importantly, Otter significantlyreduced time, and space overhead while keepingthe same performance as ARGS."
}