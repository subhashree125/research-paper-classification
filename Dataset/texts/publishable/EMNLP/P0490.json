{
  "Abstract": "Multiple-choicevisualquestionanswer-ing (VQA) is to automatically choose a correctanswer from a set of choices after reading animage.Existing efforts have been devotedto a separate generation of an image-relatedquestion, a correct answer, or challengedistractors. By contrast, we turn to a holisticgeneration and optimization of questions,answers, and distractors (QADs) in this study.This integrated generation strategy eliminatesthe need for human curation and guaranteesinformation consistency. Furthermore, we firstpropose to put the spotlight on different imageregions to diversify QADs.Accordingly, anovel framework ReBo is formulated in thispaper. ReBo cyclically generates each QADbased on a recurrent multimodal encoder, andeach generation is focusing on a differentarea of the image compared to those alreadyconcerned by the previously generated QADs.In addition to traditional VQA comparisonswith state-of-the-art approaches,we alsovalidate the capability of ReBo in generatingaugmented data to benefit VQA models.",
  "Introduction": "Visual Question Answering (VQA) (Antol et al.,2015; Goyal et al., 2017; Krishna et al., 2017)represents a burgeoning research domain that ne-cessitates the development of algorithms capableof responding to arbitrary natural language ques-tions of a given image. A specific subset of VQA,known as multiple-choice (MC) VQA (Zhu et al.,2016; Kembhavi et al., 2017; Lu et al., 2022b),involves the algorithm choosing the correct an-swer from a predefined list of distractors. MC-VQA, which requires vision-language understand-ing and cross-modality reasoning, is the represen-tative benchmark for Large Vision-Language Mod-els (LVLMs) (Zhu et al., 2023; Liu et al., 2024c;Dai et al., 2024). In the era of large models, the imperative for large-scale, high-quality MC-VQAdatasets has become increasingly pronounced.The traditional process of manually generatingdata is both labor-intensive and error-prone. Manyautomated methods are available today to indepen-dently generate questions (Zhang et al., 2016; Fanet al., 2018; Fang et al., 2024), answers (Li et al.,2018), and distractors (Lu et al., 2022a) (QADs)by machines based on images. However, thesemachine-generated QADs are often created inde-pendently, making it challenging to ensure intrinsicdependencies between them. To address this is-sue and enhance the capabilities of large models invision-language understanding and cross-modalityreasoning, our work focuses on the unified genera-tion of QADs.In the process of jointly generating QADs, howto comprehensively understand an image and di-versify its generated QADs is rarely touched. Asillustrated in , the three bounding boxesfocused on by GPT-4o are significantly intersected,inducing redundant questions such as who is inthe photo and what animal is in the photo. Incontrast, the QADs generated by our model, ReBo,are semantically rich and comprehensive for com-prehending the image, as a broad union region withsmall intersections is concentrated on.In the long run, addressing the above challengecomes down to how to align image understandingacross QADs. We tackle this issue in two folds.First, we automate the generation of QADs in aunified manner, ensuring a consistent image under-standing from questions to answers and distractors.Next, we research the generation of a series ofQADs by diversifying their focuses across imageregions, which prevents information redundancyand provides a comprehensive understanding of theentire image.From the methodological point of view, we in-troduce a Recurrent multimodal encoder to gen-erate groups of QADs considering the Bounding",
  ": An example of the vision regions that different QADs focus on. Compared with GPT-4o, our modelgenerates semantically rich QADs and provides a more comprehensive understanding of the entire image": "boxes (ReBo) of the given image. ReBo takes theQADs generated in previous steps as part of theinput to generate QAD in the next step. In addition,ReBo considers the union and intersection of imagebounding boxes, ensuring that each group of QADsfocuses on diverse regions. In this way, ReBo dis-perses its attention on a broad area of the image andboosts the diversity of the generated QADs. Weconduct extensive experiments to validate the per-formance of ReBo in different scenarios. Moreover,a further experimental analysis suggests that theQADs generated by ReBo can be used to promoteexisting VQA models in VQA tasks.Our main contributions are listed as follows:",
  "Related Work": "Most prior research focused on generating a part orparts of QADs, that is, question, answer, or distrac-tors. For instance, the studies of Visual QuestionGeneration aim at generating questions related toan image or a video. Zhang et al. (2016) tookimages and captions as inputs to generate ques-tions with different types. Johnson et al. (2016)introduced Densecap to produce region captions,providing additional context to steer the processof question generation. Krishna et al. (2019) for-mulated a visual question generation frameworkby optimizing the mutual information between the generated question and the pair of image and antic-ipated answer. Shen et al. (2020) explored a visualquestion generation approach based on a DoubleHint strategy concerning textual answers and re-gions of visual interests.On the other hand, the studies of VQA deployattention on generating correct answers by under-standing images, questions, and their interactions.For example, Li et al. (2018) proposed iQAN bytaking Visual Question Generation as a dual taskto improve VQA performance. Xiong and Wu(2020) designed question-generating mechanismsand encouraged collaborative learning interactionsamong question-answering agents. Changpinyoet al. (2022) used neural models to generate tex-tual questions and question answering. In recentyears, some research has broken into the jointgeneration of question-answer pairs. Yang et al.(2021) employed variational inference to generatequestion-answer pairs considering diversity andconsistency. Su et al. (2021) presented an end-to-end Generator-Pretester Network, which generatedquestion-answer pairs from videos.In contrast to Visual Question Generation andVQA, Visual Distractors Generation is a newly ris-ing research field, which targets to generate chal-lenging distractors according to the image, ques-tion, and answer. For example, Lu et al. (2022a)introduced a reinforcement learning approach togenerate distractors in the context of visual images.In this study, we explore a joint generation ofgroups of QADs as well as take into account theirdiversified discriminative correlations. Our pro-posed framework is capable of capturing the infor-mation from a broad region of the image, therebyenhancing the diversity and contextuality of thegenerated QADs.",
  "We propose the unified framework ReBo to gener-ate QADs as diverse as possible. In this section,": ": The model architecture of ReBo. We freeze the Image Encoder and LLM Decoder and introduce aRecurrent Multimodal Encoder to generate various QADs. The Recurrent Multimodal Encoder module takes theprefix and previously generated QADs as text inputs and helps the LLM decoder to generate QADs in each step. Wealso use IoU and UoT to guide the generation. The training processing will be removed during inference.",
  "Model Architecture": "Our model comprises an image encoder, a recur-rent multimodal encoder, and a LLM decoder. Wefreeze the parameters of the image encoder and theLLM decoder, and train the recurrent multimodalencoder.Given n groups of QADs to be generated for agiven image, we divide the generation process inton steps. In each generation step, the recurrent mul-timodal encoder takes all of the QADs generatedin previous steps as part of the text input to helpthe LLM decoder generate the QAD at current step.At each step, the generated QAD will focus on adifferent area of the image. After n steps, the Rebomodel will generate QADs considering the unionand intersection of diverse visual regions.As shown in , an image is fed into thefrozen image encoder to obtain its visual represen-tation. On the other hand, the text representation iscomposed of two elements: a fixed prefix and theground truth QADs. The fixed prefix contains thenumber of QADs and the type information of eachquestion, and the ground truth QADs comprise allof the QADs in previous steps. In specific, the inputtext in step i is the concatenation of the fixed prefixand all of the ground-truth QADs in previous i 1steps. The recurrent multimodal encoder takes both the visual representation and text representation asinputs, and the frozen LLM decoder predicts onesingle QAD in each step.We record the language modeling loss in eachstep and accumulate them as the total languagemodeling loss. An additional cross-entropy loss isintroduced to optimize the predicted QADs, and itscombination with the total language modeling lossis taken as the final loss function of ReBo.To ensure that the generated QADs have a com-prehensive understanding of the total image andshare less redundant information, we present anovel mechanism to analyze the union and inter-section of regions of interest in the image focusedon by various QADs, which will be introduced in.3.",
  "Recurrent Multimodal Encoder": "For a global optimum, simultaneously generatingand optimizing n groups of QADs is suggested. Astraightforward solution is to use only one decoderto generate a unified representation of all groupsof QADs. However, this method cannot modelthe specific representation of each individual QADas well as their inherent correlations. These arecrucial for generating an informative and compre-hensive QADs combination, as will be analyzedin .3. Therefore, we design a recurrentmultimodal encoder module to cyclically generateeach group of QADs from a single input image.To generate n groups of QADs for a given image,we divide the generation process into n steps. In each step, we recurrently utilize the recurrent mul-timodal encoder to help the LLM decoder generatedifferent QADs. To be more specific, the recur-rent multimodal encoder takes the image featureof this image as the visual input, and the text inputin each step is formed by concatenating the prefixand all of the previous ground-truth QADs in thetraining process. As portrayed in , the textinput in step 1 is merely the prefix, that in step 2 isthe prefix and the ground-truth QAD1, and that isthe prefix, ground-truth QAD1, and ground-truthQAD2 in step 3. In contrast, the output of the LLMdecoder in each step is a single group of QAD. Allgroups of QADs will be generated cyclically ac-cording to the recurrent multimodal encoder andLLM decoder for the given image. During the in-ference process, we replace the ground truth withthe predicted result of the LLM decoder in eachstep.",
  "Diversifying QAD Generations": "One bounding box can help induce a group of QAD,and we can obtain n groups of QADs for the givenimage with n bounding boxes. To make the gen-erated QADs focus on diversified image regions,we evaluate the scores of different bounding boxescombinations of and employ these scores to super-vise the QADs generation, as illustrated in .Given an image with n bounding boxes and Rirepresenting the i-th one, we can obtain its bound-ing box combination set C as follows:",
  "C = Rn = R ... R, R = {Ri}ni=1,(1)": "where Rn denotes the n-fold Cartesian productof the bounding box set R. The cardinality of Cis nn, and its each element represents a possiblecombination of bounding boxes based on which wecan induce groups of QADs.Then,weintroduceIntersectionoverUnion (IoU) and Union over Total (UoT) toscore each element in C. The IoU of the k-thbounding box combination Ck is defined asfollows:",
  "IoUk.(4)": "The score vector s can serve as the ground truthto guide ReBo in generating diverse QADs. Thatis, we can minimize the soft cross-entropy lossbetween s and the prediction probability p to gener-ate less redundant and more comprehensive QADs.Suppose the embeddings of n predicted QADsE = [ ei ]ni=1 and the ground-truth embeddingsE = [ ej ]nj=1. Their cosine similarities can becalculated as",
  "Datasets and Metrics": "Visual7W. Visual7W (Zhu et al., 2016) is collectedon 47,300 COCO (Lin et al., 2014) images, consist-ing of 327,939 QA pairs together with 1,311,756multiple-choices. We refer to telling QA of Vi-sual7W in our experiments and take no extra op-erations. Each question starts with one of six Ws,what, where, when, who, why, and how. We onlyselect the QADs that contain bounding boxes fromthe dataset. To cover as many regions of the imagewith as few QADs as possible, for images con-taining QADs up to 3, we calculate the boundingbox scores for all possible combinations of threebounding boxes associated with QADs. The QADscombination with the highest bounding box scoreis selected as the corresponding QADs for eachimage. We also remove the images that only haveone QAD. The final dataset contains 8k/5k imagesand 21k/13k QADs for training and testing.A-OKVQA. A-OKVQA (Schwenk et al., 2022)is a knowledge-based visual question-answeringbenchmark.A-OKVQA is an augmentedsuccessor of OK-VQA (Marino et al., 2019)and contains a diverse set of 17.1k/1.1k/6.7kquestions/answer/rationaletripletsfortrain-ing/validation/testing.We use the A-OKVQAdataset to assess whether the generated QADs ofReBo can enhance existing VQA models.Metrics. We employ BLEU (Papineni et al., 2002),ROUGE (Lin, 2004), METEOR (Banerjee andLavie, 2005), and CIDEr (Vedantam et al., 2015)with ground-truth QADs to evaluate the quality ofthe generated QADs.",
  "VQADG (Ding et al., 2024) first presents togenerate questions, answers, and distractors ina unified way. This paper also incorporates con-trastive learning to improve the quality of QADs": "Qwen-VL (Bai et al., 2023b) is a large vision-language model based on language model (Baiet al., 2023a). We select Qwen-VL-Chat in thispaper, which is a multimodal LLM-based AI as-sistant trained with human alignment techniques. We also compare ReBo with LLMs, includingLlama-2 (Touvron et al., 2023), Mistral (Jianget al., 2023), ChatGPT (Ouyang et al., 2022),Qwen1.5 (Team, 2024b), and Llama-3 (Team,2024a), as well as LVLMs, involving LLaVA-1.5 (Liu et al., 2024a), CogVLM (Wang et al.,2023), and LLaVA-NeXT (Liu et al., 2024b).",
  "Implementation Details": "We adapt our model based on the modular architec-ture of InstructBLIP (Dai et al., 2024). We retainthe image encoder and the LLM decoder whileadapting the Q-Former into a recurrent multimodalencoder. We implement our model with the im-age encoder ViT-g/14 (Fang et al., 2023) and thelarge language model FlanT5-XL (Chung et al.,2024), which is an instruction-tuned model basedon the encoder-decoder Transformer T5 (Raffelet al., 2020). We refer (Ding et al., 2024) to em-ploy an extra contrastive learning loss function tonormalize the embeddings of prediction results andground truth. For the hyper-parameters, we set themaximum text length to 60 and the minimum textlength to 20 for the recurrent generation type and60 to 180 for the concatenation generation type.The image size in all models is resized to 224. Weuse the batch size 8 and 32 for training and testingand fine-tune the datasets for 10 epochs. Otherparameters are set according to the original arti-cles. For Large Language Models, we calculatedthe mean and variance of the results over three runs.For Large Vision-Language Models, we report onlyone result due to consistent outputs. For our modeland all other baselines, we divided the training andtesting data into ten splits and calculated the meanand variance of the results over ten runs. We use theHuggingFace1 transformers library implementationfor LLMs and LVLMs. Our experiments are runon 1 NVIDIA A40 48G GPU. The source code isavailable at",
  "Main Results": "For LLMs and LVLMs, we provide examples andinstruct the LLMs to generate QADs, and im-age captions are employed. We retrain all of theV&L baseline models on the same dataset. Weextend two variants of generation type to con-duct a more comprehensive evaluation of the re-current multimodal encoder. The concatenationgeneration type implies that the QADs associ-ated with one image are generated at once in anaive manner, which means the output would beQAD1<sep>QAD2<sep>QAD3. The recurrentgeneration type entails generating QADs for eachstep using the recurrent multimodal encoder, whichmeans the output would be QADi in step i. AllV&L baseline models are retrained in the concate-nation generation type. We evenly partitioned theentire dataset into ten subsets and calculated themean and variance of the results over ten runs.The experimental results of generating QADs onthe benchmark are summarized in , fromwhich we can observe that: (1) the performanceof ReBo is promising across five metrics, and (2)Llama-3, LLaVA-1.5, and Qwen-VL achieve peakperformance respectively in the families of LLMs,LVLMs, and V&L models. further summa-rizes the separate evaluation results for questions, answers, and distractors. We can conclude that: (1)ReBo can generate more image-related questions,decent answers, and challenging distractors with asuperiority ranging from 2-11%, and (2) the perfor-mance gap of VQADG behind ReBo indicates thatsimply concatenating the single part of QADs isnot a promising strategy, which is consistent withthe argument in Introduction.",
  "Augmenting VQA models": "To verify the boosting effects of ReBo over existingVQA models, we employ the QADs generated byReBo as additional data to train the InstructBLIPon the VQA task in this section.To ensure fairness, we use ReBo to generateQADs according to the images from the validationsplit dataset of the Visual7W, we then train a VQAmodel separately on Visual7W and Visual7W +generated dataset, and finally evaluate the accuracyon the A-OKVQA dataset. To ensure the diversityof the generated QADs, we extract three questiontypes at a time from all six question types (e.g.,what, where, and when for one iteration)for ReBo to generate QADs. 500k QADs can beyielded as training data after 300 iterations. Then,we filter high-quality QADs respectively from theviews of questions and answers: (1) For questions,we select the QADs with less overlapped informa-tion with the ground truth based on their cosinesimilarities; (2) as to answers, we calculate the co-sine similarities between our generated answersand the pseudo-answers generated by InstructBLIP,and preserve those with high similarities as the fi-nal augmented data. After filtering, the final QADs",
  ":Augmenting existing VQA models. Rawdenotes the model trained only on the raw Visual7Wdataset": "are used as the augmented data to train the VQAmodel InstructBLIP.To ensure the generalization of this evaluation,we employ the A-OKVQA dataset for testing inaddition to the QADs generated on the Visual7Wdataset for training as aforementioned. The perfor-mance is depicted in . It can be observedthat the vision-language capability of InstructBLIPis boosted by our generated QADs data over train-ing and validation splits of A-OKVQA. It is note-worthy that our proposed method is model-agnosticand it can be applied to any model on any bench-mark.",
  "We conduct ablation experiments to verify the per-formance of the components of ReBo. We remove": "both bounding box combination scores (BBCS)and recurrent multimodal encoder (RME) to refor-mulate ReBo into the model with concatenationgeneration types. Experimental results in and demonstrate that both modules con-tribute to achieving good performance for ReBo.Excluding BBCS and RME seems not to signif-icantly affect the BLEU-1 and ROUGE-L perfor-mance of ReBo, yet they help generate informativeQADs that focus on diverse regions. More detailscan be found in the case studies in .",
  "Human Evaluations": "To further assess the effectiveness of ReBo, weconducted a human evaluation of 300 images.We generate three QADs separately using BLIP2,VQADG, Qwen, and ReBo for each image. Thetotal human evaluation data comprises 300 imagesand 3600 QADs.We recruit six annotators to rate them from 1to 5 points on five qualitative aspects: (1) QualityThe overall quality of the generated QADs includesquestion relevance, answer accuracy, and the con-fusion level of distractors. (2) Intersection Theintersection score represents whether the seman-tic contents of generated QADs for a given imageare dissimilar. (3) Union The union score repre-sents whether the generated QADs can summarizethe overall content of the image. A higher scoreimplies that the model performs better.",
  "ReBo4.073.723.263.704.02": ":Human evaluation of the generated QADs.Q, A, and D denote the total quality score of questions,answers, and distractors, I denotes the intersection be-tween different QADs, and U denotes the union scorefor all QADs associated with a given image. displays the results of human evaluation, revealingthat ReBo achieves the highest scores across allfive metrics. Experimental results demonstrate thatour recurrent multimodal encoder and boundingbox scores are not only capable of generating high-quality QADs, but also facilitate the generalizationof QADs with small intersections among each otherand cover more information from the image.",
  "Case Studies": "We present case studies to demonstrate the QADsgenerated by GPT-4o, ReBo without BBCS andRME, and ReBo in . For GPT-4o, wedesign the prompt and give examples to generatequestions, answers, and distractors. We presentthree groups of QADs generated by each methodand highlight their focus regions.It shows from the figure that GPT-4o and ReBowithout BBCS and RME can generate completeQADs, yet they may produce some inappropri-ate or incorrect answers and/or distractors. Forexample, GPT-4o generates a distractor a snow-boarder, which is almost indistinguishable fromthe correct answer a skier. ReBo without BBCSand RME generates an incorrect answer yellow for the question What color is the mans jacket?.Our ReBo can generate meaningful questions, cor-rect answers, and misleading distractors. Further-more, the QADs generated by ReBo focus on abroad region of the image, comprising the regionsof people, background trees, and ground snow. Incontrast, GPT-4o and ReBo without BBCS andRME disregard the semantic richness of the gen-erated QADs and are likely to be concerned withoverlapped regions.",
  "Conclusion": "In this paper, we propose a novel framework witha recurrent multimodal encoder and bounding boxscores to generate a series of QADs. The mul-timodal encoder recurrently generates differentQADs for an image, utilizing the previous QADsas part of the input to generate current QADs. Thebounding box scores consider the intersection overunion and the union over total image, which canfacilitate the generation of QADs that attend toas large and diverse areas as possible for one im-age. We conduct experiments on the benchmark todemonstrate a significant advantage of our modelin the evaluation metrics. Additionally, our gener-ated QADs, as supplementary data to the originaldataset, exhibit the capability to promote the per-formance of existing VQA models.",
  "Limitations": "Our focus in this study is devoted on generatingdiverse QADs jointly. This task is challenging asit involves learning interactions between QADs,as well as encoding, generating, and evaluatingQADs. We notice that there is still large room forprogress. For example, how to tailor our modelspecific to different types of question, answer, anddistractors and how to evaluate the generated QADsin a human-like manner remain untouched and willbe tackled in our future study.",
  "masked visual representation learning at scale. InProceedings of CVPR, pages 1935819369": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understanding invisual question answering. In Proceedings of CVPR,pages 69046913. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Ranjay Krishna, Michael Bernstein, and Li Fei-Fei.2019. Information maximizing visual question gen-eration. In Proceedings of CVPR, pages 20082018": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual genome: Connecting language and vi-sion using crowdsourced dense image annotations.IJCV, 123:3273. Junnan Li, Dongxu Li, Silvio Savarese, and StevenHoi. 2023. Blip-2: Bootstrapping language-imagepre-training with frozen image encoders and largelanguage models. In Proceedings of ICML, pages1973019742. Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022.Blip: Bootstrapping language-imagepre-training for unified vision-language understand-ing and generation. In Proceedings of ICML, pages1288812900.",
  "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of ACL,pages 311318": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. JMLR, 21(140):167. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answeringusing world knowledge. In Proceedings of ECCV,pages 146162. Kai Shen, Lingfei Wu, Siliang Tang, Fangli Xu, ZhuZhang, Yu Qiang, and Yueting Zhuang. 2020. Askquestion with double hints: Visual question gener-ation with answer-awareness and region-reference.IEEE-TPAMI. Hung-Ting Su, Chen-Hsi Chang, Po-Wei Shen, Yu-Siang Wang, Ya-Liang Chang, Yu-Cheng Chang,Pu-Jen Cheng, and Winston H Hsu. 2021. End-to-end video question-answer generation with generator-pretester network. T-CSVT, 31(11):44974507."
}