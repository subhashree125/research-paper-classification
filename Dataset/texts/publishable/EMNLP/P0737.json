{
  "Abstract": "There is a scarcity of multilingual vision-language models that properly account for theperceptual differences that are reflected in im-age captions across languages and cultures. Inthis work, through a multimodal, multilingualretrieval case study, we quantify the existinglack of model flexibility. We empirically showperformance gaps between training on cap-tions that come from native German perceptionand captions that have been either machine-translated or human-translated from Englishinto German. To address these gaps, we fur-ther propose and evaluate caption augmentationstrategies. While we achieve mean recall im-provements (+1.3), gaps still remain, indicatingan open area of future work for the community.",
  "Introduction": "Vision-language models (VLMs) such as CLIP(Radford et al., 2021) are predominantly limitedto use in English as a result of the pretraining su-pervision consisting mostly of English captions.This trend naturally poses an accessibility barrierfor non-English speakers. Furthermore, culturesaround the world differ in their salient concepts(Liu et al., 2021) and visual perception (Nisbettand Masuda, 2013). Relying on English supervi-sion in pretraining thus hinders consideration ofcross-cultural concepts in object-based tasks suchas recognition, detection, and image-text retrieval.Example cultural differences present in languagewith respect to object specificity and importance.For example, past literature (Nisbett and Masuda,2013) describes differences in how cultures per-ceive members of an object group (e.g. penguinswithin the group of birds), indicating that certaingroups have stronger associations for specific ratherthan general object terms. Experiments in Nisbettand Masuda (2013) also illustrate differences be-tween East Asians and Americans with respect tothe perceived importance of background objects : Example perception differences between na-tive English and German speakers. Examples are cap-tions from Flickr30K (Young et al., 2014) and Multi30K(Elliott et al., 2016). Note differences in mentioned ob-jects (sand arena, parasol) and specificity (Heuri-gen bench vs. table, horse vs. bronco). Germancaptions here are translated to English. and context as opposed to foreground objects. Dif-ferent cultures notice different objects more; per-ceptual differences may manifest in objects beingincluded/excluded in a caption, and different ob-jects being relevant in tasks. shows examplesof differences in AI datasets for English and Ger-man (Young et al., 2014; Elliott et al., 2016).There has been some progress in multilingual,multimodal modeling (Chen et al., 2022, 2024;Carlsson et al., 2022; Chen et al., 2023) andmultilingual data creation (Elliott et al., 2016;Yoshikawa et al., 2017; Liu et al., 2021; Thapliyalet al., 2022). The models often leverage off-the-shelf machine translation techniques to improvemultilingual functionality. In this work, we investi-gate the performance gaps between training mod-els with translations (which reflect English speakerperception) and natively written captions (whichreflect non-English speaker perception) for a taskin a given language. In line with the observed dif-ferences in Nisbett and Masuda (2013), we reasonthat translation may not account for specificity dif-ferences and may not alter supervision to accountfor importance differences.We quantify potential differences through an ex-ploration of non-English image-text retrieval. In particular, we finetune and benchmark multilingualCLIP (Chen et al., 2023) on Multi30K (Elliott et al.,2016) using German as the target. We exploreMulti30Ks native German captions (reflecting Ger-man speaker perception) and professionally trans-lated captions (from English to German), as wellas use of an external machine translation modelover Flickr30Ks English captions (Young et al.,2014). We find significant performance differencesdepending on the data used to train the model, i.e.(1) English, (2) German translated from English bya machine translation model, (3) German translatedfrom English by humans, and (4) native German.As (2) and (3) have gaps vs. (4), we also attemptto improve upon translation. We test three para-phrasing techniques to diversify object descriptionsin English before translation, and use the resultingtranslations as additional finetuning data. First, weexperiment with a hypernymization data augmen-tation technique, where object terms are updatedbefore translation to represent different levels ofspecificity. Second, we use a large language model(LLM), LLaMA-3 (Touvron et al., 2023), to pro-duce structurally different, but semantically similarparaphrases of English captions before translation.Third, we explore LLM reasoning to produce tar-geted paraphrases that capture the perceptual prop-erties captured in a sample set of captions. Thesetechniques outperform the baselines. However, agap between translation and native perceptionremains, indicating an open problem. We con-clude with analysis in pursuit of this direction.",
  "Background and Related Work": "Cultural differences in perception. Prior workconsiders how culture may influence perceptionand expression. For example, Western and EastAsian cultural differences are found to manifest invisual attention, e.g. Americans appear to pay moreattention to foreground/objects than East Asians,but conversely for background/context (Nisbett andMasuda, 2013). Furthermore, Boroditsky (2006)describes empirical studies that indicate that differ-ent cultures group objects differently (e.g. based onshape or material) and ascribe different propertiesto objects, because of unique grammar (e.g. gen-dered nouns). Since German uses gendered nouns,this observation may manifest in native Germancaptions (and retrieval) as objects being describedwith unique attributes. The work of Berthele et al.(2015) notes that Germanic language speakers de- scribe object relationships with notably specificspatial information (e.g. posture/manner informa-tion in addition to object relationships). Hofstede(2001) conducts analysis to show that there arecultural differences between Germany and UnitedStates in terms of individualism vs. collectivism,which could impact the perception of visual contentas argued by Nisbett and Masuda (2013). Furtherexamples can be found in work on linguistic rela-tivity (Kay and Kempton, 1984).Multilingual multimodal modeling. Our workaligns with works that extend VLMs for use in lan-guages besides English (Chen et al., 2022, 2024;Carlsson et al., 2022; Chen et al., 2023). Modelsnotably often rely on translations, and works do nothave analysis into performance differences betweentranslations and captions of native perception. Incontrast, Kdr et al. (2018) and our work showdifferences in retrieval performance when captionsare natively written in a language or translated intothat language from English. Our work differs fromKdr et al. (2018) as we also explore machinetranslation, with a more modern VLM (Chen et al.,2023). We also explicitly address the lack of tech-niques to overcome gaps by experimenting withparaphrasing augmentations. Our strategies are re-lated to past paraphrasing work (Wieting and Gim-pel, 2018; Hu et al., 2019), but these approaches usemachine translation to generate large-scale Englishparaphrase datasets, while we leverage in-contextlearning and LLMs to generate paraphrases for useas input to machine translation to enhance diver-sity. We are inspired by Fan et al. (2024), as thework shows zero-shot image classification improve-ments with LLM-based caption rewriting.Data-wise, we explore Multi30K (Elliott et al., 2016) as it contains native German captions andparallels the English Flickr30K captions (Younget al., 2014). XM3600 (Thapliyal et al., 2022)also provides natively perceived captions, in 36languages for 3,600 images. Due to size, we donot train with this set, though we provide initialanalysis on it to inspire future work. WebLI (Chenet al., 2022) is another dataset that contains crawledcaptions in 109 languages, though it is proprietary.",
  "Benchmarking Details": "Task. We evaluate on German image-text (I2T) andtext-image (T2I) retrieval. The German captionsused in eval are written directly by native speakersabout images. They are not translated from Englishand represent natural non-English perception.Data. English data is from Flickr30K (Young et al.,2014), and German data is from Multi30K (Elliottet al., 2016). Flickr30K contains 31,014 imagesthat are annotated with 5 independently writtenEnglish captions per image. Likewise, Multi30Kprovides 5 independently written German captionsfor the same images. These German captions arecollected from 185 native speakers using a similarinterface to Flickr (Hodosh et al., 2013). Multi30Kalso provides professional German translations.In particular, for each image, 1 of the 5 Englishcaptions is sampled from Flickr30K, and profes-sional translators produce corresponding captionsin German (just from source text, not using theimages). We refer to the separate caption sets as In-dependently Written (5 sets for each language) andHuman-Translated (1 set per language). For all sets,we randomly split data to create a disjoint referenceset (9,666 samples) to be used with our strategies(Sec. 3.2), as well as retrieval train/val/test sets(9,666/1,014/10,668 samples respectively).Modeling. We explore mCLIP (Chen et al., 2023),an approach which has made CLIP multilingualthrough knowledge distillation-based training ofprojector modules and replacement of CLIPs textencoder with the multilingual text encoder XLM-R(Conneau et al., 2020). We finetune mCLIP withimages and captions for German I2T and T2I re-trieval. For experimentation that involves machine-translating English captions to German, we useopus-mt-en-de (Tiedemann and Thottingal, 2020)from Hugging Face. With this model, we use adeterministic setting, where tokens are generatedaccording to highest token probability, and infer atmost 40 tokens for each caption. mCLIP modelsare trained for 30 epochs on 1 Quadro RTX 5000GPU with batch size 16 and learning rate 0.0005.Metric. We report mean recall as in Chen et al.(2023). Recall@1,5,10 is computed for both T2Iand I2T retrieval on each native German test set (5sets total). Mean recall is the average of these sixvalues. We further average over each set.",
  "Methods Compared": "Baseline finetuning strategies include: ENG, a lower bound: finetuning using datanatively provided in English (in the IndependentlyWritten sets). Since there are 5 sets of captions, weaverage over trials using each set for training. ENG2GER-MT: finetuning on German sen-tences that have been translated from English usingan English-to-German machine translation model(Tiedemann and Thottingal, 2020). English sen-tences come from the Human Translation set. ENG2GER-MT (TRN): same as above, butthe translation model is further trained on captionsfrom the Multi30K disjoint reference split we cre-ate, with the intuition that translation finetuningmay capture caption differences. We train for 10epochs with learning rate 0.00001 and batch size16, using the Human Translation pairs. ENG2GER-HT: finetuning on German captionstranslated from English by professional annotators(in the Human Translation set). This training isdifferent from and expected to perform worse thannative German, but better than naive translation. GER: finetuning using data natively providedfrom German perception (in the IndependentlyWritten sets). Since there are 5 sets of captions,we average over trials using each set for training.Strategies: We find significant gaps between thesemethods, notably ENG2GER-MT and GER, mo-tivating experimentation with potential improve-ments. We test adding training data that has beenaugmented in English then translated to German.Some proposed changes involve object names, sofor this purpose, we define an object vocabularyV including COCO object terms (Lin et al., 2014).Category detection involves consideration of theseterms, synonyms (Lu et al., 2018), plurals, andword sense. For each strategy, mCLIP is trained asin ENG2GER-MT, but with an augmented datasetof captions added. Methods include: HYPER: After identifying each COCO classwith a synset id, if available, object mentions arehypernymized to be a random term above it in theWordNet hierarchy (Miller, 1995). Our goal is toimprove robustness to changes in object naming toaddress challenges in object specificity. PARA-RND (paraphrase-random):Beforetranslation, we ask LLaMA-3 (Touvron et al., 2023)to write each caption in a structurally different man-ner while maintaining meaning. We are motivatedby Fan et al. (2024) which shows English retrieval benefits from diversification. Our approach differsas we diversify before translation to guide transla-tion to more generalizable descriptions. PARA-TGT (paraphrase-targeted):We askLLaMA-3 to paraphrase each caption using exam-ples of object naming style. For each caption,a total of k=100 captions are randomly sampledfrom the reference split of the first native Germanset, such that if possible, sampled captions share atleast one non-person object mention with the cur-rent caption (since most captions mention people).Translations of these are provided in the LLaMA-3 prompt as examples. Then for the input cap-tion, LLaMA-3 is instructed to find relevant nounphrases, and to convert the noun phrases to morealigned representations based on the examples. PARA-CMB combines both sets above.Please refer to the appendix for prompt details.",
  "Key Findings": "In the top block of , zero-shot mCLIP isshown to achieve the lowest recall (24.5). Fine-tuning mCLIP with English Multi30K data im-proves performance to 26.9 (+2.4). English datacan help to a degree on German retrieval due toalignment learned in pretraining the multilingualtext encoder.However, much more significantgains are achieved when the finetuning data isin German. Training with German data that hasbeen translated from English using an off-the-shelftranslation model (ENG2GER-MT) reaches 33.4(second block). Compared to human translation(ENG2GER-HT - fourth block), there is a notablegap from machine translation (3.4), and finetun-ing the translation model only bridges this gap by0.6. These results indicate existing challenges withoff-the-shelf translation for retrieval. Then mostsignificantly, the gap between off-the-shelf trans-lation and native German captions (GER) is 5.0.There is a notable gap between professional trans-lation (ENG2GER-HT) and GER (1.6), which wereason is the gap due to differences in English andGerman perception. For example, these gaps couldbe due to specificity and importance differences.Expert translation does not address these factors.In the third block, our methods are found to besomewhat effective for bridging the gap betweenENG2GER-MT and GER. HYPER improves theresult by 0.3, and PARA-RND and PARA-TGT by0.7. These models are notably more appropriate forlow-resource target languages than ENG2GER-MT",
  ": German I2T/T2I retrieval results. Meanrecall values are averaged over native German cap sets": "(TRN) since they use no/few reference captionscompared to translation finetuning. Further com-bining random and targeted paraphrasing results inthe largest gain of 1.3. The result is still 3.7 awayfrom GER. Addressing differences in the percep-tion of the visual world and the way captions arewritten across cultures is thus an open challenge.",
  "Further Analysis": "ObjectmentionsinEnglish/Germancap-tions.To analyze possible differences in per-ception, we analyze object mention frequency inFlickr30K/Multi30K. We specifically translate Ger-man captions to English and extract nouns in both(original) English and (translated to English) Ger-man captions. The ratio of English and Germanmentions is about 1.5, i.e. English mentions objectnouns 50% more often than German. However,counts vary by object type. For example, Englishmentions clothing more often (pants-143% more,shirt-112%, hat-60%, jacket-43%), and Germanmentions furniture more often (table-37% more,bed-20%, bench-15%). These languages also varyin granularity: English captions often say people,while German ones say workers, athletes, etc.Analysis of other languages. We conduct initialanalysis of the languages and captions in XM3600(Thapliyal et al., 2022). We group XM3600 lan-guages into European, Arabic/Farsi, Hindi/Bengali,Indonesian/Thai, East Asian, and Swahili cate-gories. After translating each language to English,we report average mention counts and standard de-viations per group for various common objects in. Language groups show large differencesin terms of how commonly they mention elementsof nature (e.g. mountains, trees), scenery (streets,buildings), household objects (table, plate, box, bot-tle), and the gender of portrayed people. It is also",
  "woman135.523.71275.711431.1164.5+20.5133.327.7160": ": Language shifts in terms of concept mentions in different languages. We group XM3600 Europeanlanguages (eu), Arabic/Farsi (ar), Hindi/Bengali (hi), Indonesian/Thai (id), East Asian languages (easia), and reportSwahili on its own (sw). The largest two numbers per row are bolded. Observe the differences between the languagewith highest (+) and lowest () counts, which are significantly larger than the within-group standard deviations.",
  "found that the difference between objects countsacross languages is much greater than within-groupstandard deviations. Such results suggest differ-ences in supervision worthy of exploration": "Paraphrasing. LLaMA picks up on granularity dif-ferences. For example, PARA-TGT changes Manin a red shirt riding his bicycle to A bicyclist ina red shirt is riding. Further, LLaMA transformsman on skis into skier, person in blue and redice climbing into ice climber, and men withchildren into family. The model tends to sim-plify, irrespective of the reference. For example,Two young people are approached by a flamboy-ant young woman dressed in a red bikini and a redfeathered headress becomes Two young peopleare approached by a bikini-clad woman. Para-phrasing could thus result in over-simplification. Human evaluation. We extend quantification pastretrieval by asking two German speakers to gaugethe likelihood that captions are made by a Ger-man speaker and their naturalness. We provide 50random captions for each of 3 sets (ENG2GER-MT, ENG2GER-HT, PARA-TGT). Speakers donot know each sets identity and are tasked withscoring captions as 3=great, 2=good, 1=bad. On av-erage, the speakers rate ENG2GER-HT the highestwith a mean ternary score of 2.73 and mean binaryscore (great/good=1, bad=0) of 0.97. For PARA-TGT, the ternary score is 2.19 and binary scoreis 0.79. For ENG2GER-MT, the ternary score is2.16 and binary score is 0.77. These differencesapproximately reflect the recall results in .Recognition. To evaluate object recognition, wecompare objects mentioned in a native Germancaption to ones predicted by the models GER and",
  ": Recognition stats by supercategory. Toprows: mention counts, middle: precision, bottom: recall": "ENG2GER-HT. We take predictions to be oneswith CLIP scores greater than a threshold (the onein range 10:5:50 that maximizes val F1). A pre-diction is correct only if the object is mentioned innative German. shows train-set mentionsand performance for the best-performing COCOsupercategories. We observe large differences inthe number of mentions, precision, and recall forseveral supercategories. GER achieves better re-call (slightly correlated with mention count differ-ences), but ENG2GER-HT better precision. Theseresults suggest potential recognition differenceswhen using translated and native captions.",
  "Conclusion": "We show notable differences in using native vs.translated German captions to train a retrievalmodel, and experiment with three strategies to re-duce the gaps. We plan to extend investigationto more languages. Future work can also involvecreation of data augmentation strategies that takeinspiration from psychology literature (Nisbett andMasuda, 2013; Boroditsky, 2006) and solutions forthe ambiguity challenges of machine translation,such as by using images (Futeral et al., 2023).Acknowledgement. This work was supported byNSF Grants No. 2006885 and 2329992.",
  "Limitations and Ethical Considerations": "We only experiment with one translation model,one non-English language (German), and a smallamount of runs of LLaMA-3. To ensure that in-sights generalize, various models, and languages(especially low-resource ones), should be analyzed.There may be intra-language variance amongst na-tive speakers that should also be considered.We rely on the use of image-caption datasetslike Flickr30K and Multi30K. These datasets arerelatively small (about 30k samples), so the cov-erage of concepts may not be fully representativeof spoken language. Such datasets have also beennoted to contain harmful biases with respect toattributes like race and gender (Van Miltenburg,2016). The use of models like LLaMA-3 carriessimilar biases. There should be careful considera-tion regarding downstream usage of these sets andmodels. We note that a future extension of our para-phrasing strategies could be to mitigate the impactof in-group perspectives in the captions used forpretraining models.Our analysis of differences in languages islimited by the fact that languages are machine-translated to English. It is possible that some differ-ences are amplified and/or missed due to machinetranslation artifacts.Finally, while we conduct initial human evalua-tion, we encourage larger-scale human evaluationthat expands past our limited evaluation. This canbe done to ensure that methods are applicable for agreater amount of people.",
  "Lera Boroditsky. 2006. Linguistic relativity. Encyclope-dia of Cognitive Science": "Fredrik Carlsson, Philipp Eisen, Faton Rekathati, andMagnus Sahlgren. 2022. Cross-lingual and multilin-gual CLIP. In Proceedings of the Thirteenth Lan-guage Resources and Evaluation Conference, pages68486854. Guanhua Chen, Lu Hou, Yun Chen, Wenliang Dai,Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan, and Wen-ping Wang. 2023. mCLIP: Multilingual CLIP viacross-lingual transfer. In Proceedings of the 61st An-nual Meeting of the Association for Computational",
  "Linguistics (Volume 1: Long Papers), pages 1302813043, Toronto, Canada. Association for Computa-tional Linguistics": "Xi Chen, Josip Djolonga, Piotr Padlewski, BasilMustafa, Soravit Changpinyo, Jialin Wu, Car-los Riquelme Ruiz, Sebastian Goodman, Xiao Wang,Yi Tay, et al. 2024. On scaling up a multilingualvision and language model. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1443214444. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-giovanni, Piotr Padlewski, Daniel Salz, SebastianGoodman, Adam Grycner, Basil Mustafa, LucasBeyer, et al. 2022. Pali: A jointly-scaled multilinguallanguage-image model.In The Eleventh Interna-tional Conference on Learning Representations. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Desmond Elliott, Stella Frank, Khalil Simaan, and Lu-cia Specia. 2016. Multi30K: Multilingual English-German image descriptions. In Proceedings of the5th Workshop on Vision and Language, pages 7074, Berlin, Germany. Association for ComputationalLinguistics.",
  "Geert Hofstede. 2001. Cultures consequences: Com-paring values, behaviors, institutions and organiza-tions across nations. Thousand Oaks": "J Edward Hu, Rachel Rudinger, Matt Post, and Ben-jamin Van Durme. 2019. Parabank: Monolingualbitext generation and sentential paraphrasing vialexically-constrained neural machine translation. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 33, pages 65216528. kos Kdr, Desmond Elliott, Marc-Alexandre Ct,Grzegorz Chrupaa,and Afra Alishahi. 2018.Lessons learned in multilingual grounded languagelearning. In Proceedings of the 22nd Conference onComputational Natural Language Learning, pages402412.",
  "Paul Kay and Willett Kempton. 1984.What is theSapir-Whorf hypothesis? American Anthropologist,86(1):6579": "Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. 2014. Microsoft COCO:Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer. Fangyu Liu, Emanuele Bugliarello, Edoardo MariaPonti, Siva Reddy, Nigel Collier, and Desmond El-liott. 2021. Visually grounded reasoning across lan-guages and cultures. Empirical Methods In NaturalLanguage Processing.",
  "Ashish Thapliyal, Jordi Pont-Tuset, Xi Chen, and RaduSoricut. 2022. Crossmodal-3600: A Massively Mul-tilingual Multimodal Evaluation Dataset. In EMNLP": "Jrg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT Building open translation services for theWorld. In Proceedings of the 22nd Annual Confer-enec of the European Association for Machine Trans-lation (EAMT), Lisbon, Portugal. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "Emiel Van Miltenburg. 2016. Stereotyping and bias inthe Flickr30k dataset. Proceedings of the Workshopon Multimodal Corpora (MMC)": "John Wieting and Kevin Gimpel. 2018. ParaNMT-50M:Pushing the limits of paraphrastic sentence embed-dings with millions of machine translations. In Pro-ceedings of the 56th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 451462, Melbourne, Australia. As-sociation for Computational Linguistics. Yuya Yoshikawa,Yutaro Shigeto,and AkikazuTakeuchi. 2017. STAIR captions: Constructing alarge-scale Japanese image caption dataset. In Pro-ceedings of the 55th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 417421, Vancouver, Canada. Associ-ation for Computational Linguistics. Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier. 2014. From image descriptions to visualdenotations: New similarity metrics for semantic in-ference over event descriptions. Transactions of theAssociation for Computational Linguistics, 2:6778. Shown are the prompt templates used for queryingLLaMA-3 (meta-llama/Meta-Llama-3-8B-Instructon Hugging Face). We do not experiment withLLaMA sampling settings and generate outputswith default parameters.",
  "Para-Rnd Prompt TemplateRewrite captions in a structurally differ-ent manner, while closely maintaining se-mantic meaning. Return as Python string.Return no other text": "Para-Tgt Prompt Template1) Given a caption, 1st decompose intonoun phrases, keeping all phrase con-tent (e.g.adjectives) aside from arti-cles. EX: A person is riding a blue bi-cycle down the street on a sunny day.Noun Phrases: [person, blue bicycle,street, sunny day] 2) Based on a provided reference list ofrelated captions, construct a new set ofnoun phrases that alters the original nounphrases to be in the common styles/formsshown in the reference list. EX: If manycaptions say bicyclist, combine per-son and blue bicycle into bicyclist.Do not infer unnecessary information."
}