{
  "Abstract": "Detecting hate speech and offensive language isessential for maintaining a safe and respectfuldigital environment. This study examines thelimitations of state-of-the-art large languagemodels (LLMs) in identifying offensive con-tent within systematically perturbed data, witha focus on Chinese, a language particularly sus-ceptible to such perturbations. We introduceToxiCloakCN1, an enhanced dataset derivedfrom ToxiCN, augmented with homophonicsubstitutions and emoji transformations, to testthe robustness of LLMs against these cloakingperturbations. Our findings reveal that existingmodels significantly underperform in detectingoffensive content when these perturbations areapplied. We provide an in-depth analysis ofhow different types of offensive content are af-fected by these perturbations and explore thealignment between human and model expla-nations of offensiveness. Our work highlightsthe urgent need for more advanced techniquesin offensive language detection to combat theevolving tactics used to evade detection mecha-nisms.",
  "Introduction": "Offensive language, which includes hate speech,cyberbullying, and adult-oriented content, posessignificant risks to user well-being and social har-mony (Davidson et al., 2019). With the rapid ex-pansion and widespread usage of social media plat-forms, the proliferation of offensive language hasbecome a critical issue. Consequently, social mediaplatforms and researchers have explored develop-ing robust machine learning and linguistic analy-",
  "sis solutions to effectively identify and mitigatethe harmful effects of offensive content (Davidsonet al., 2017; Dhanya and Balakrishnan, 2021)": "Recent advances in Natural Language Processing(NLP), particularly with Large Language Mod-els (LLMs), have significantly improved the abil-ity to detect offensive language across multiplelanguages (Pitsilis et al., 2018; Wei et al., 2021;Fatemah and Ozlem, 2021; Battistelli et al., 2020;Beyhan et al., 2022; Dhanya and Balakrishnan,2021; Deng et al., 2022a; Zhou et al., 2023; Awalet al., 2023). However, these models often strug-gle with systematically perturbed data designed toevade detection mechanisms. Common perturba-tion techniques include homophonic substitutions,emoji replacement, insertions, character splits, andsynonyms (Su et al., 2022; Kirk et al., 2022). Thesetechniques, referred to as cloaking, exploit lin-guistic nuances to mask offensive content, posinga substantial challenge to both automated systemsand human moderators. The Chinese language, in particular, is heavily im-pacted by these techniques due to intensive lexicon-based censorship, leading to a new linguistic phe-nomenon (Wiener, 2011) where significant partsof sentences are replaced by either homophonesor emojis to mask underlying offensive content orto circumvent censorship rules. showstwo examples of offensive texts cloaked using ho-mophone and emoji replacement techniques. Inthese examples, the words and phrases highlightedin yellow are replaced with homophones or emo-jis. In the first example, homophones are used toreplace phrases that identify the target (e.g., as the homophone for , which meanspeople from the Henan region in China) and offen-sive terms such as with . Similarly,in the second example, the offensive term (i.e., Simps) is replaced with",
  "Predicted Labels": ": Example of cloaked Chinese offensive lan-guage using homophone and emoji replacement. Byusing such techniques, users will be able to fool the au-tomated offensive language detector into misclassifyingthem as normal sentences. guage detectors into misclassifying these sentencesas non-offensive, even though avid Chinese socialmedia users will have no problem understandingthe offensive context of the text. Addressing thisproblem is crucial to improve the effectiveness ofoffensive language detection systems. As theseevasion techniques evolve, it becomes increasinglyimportant for these offensive langauge detectionsystems to adapt and accurately identify cloakedoffensive content. In this work, we introduce ToxiCloakCN, a novelChinese offensive content dataset that benchmarkcontent moderation models ability to detect of-fensive texts cloaked using homophone and emojireplacements. Specifically, we conduct extensiveexperiments and evaluate state-of-the-art LLMs onthe ToxiCloakCN dataset. The experiments demon-strated that both perturbation methods significantlyaffect the models capabilities in detecting offen-sive text. We also analyze the effect of promptson the experimental results by testing the modelsusing six different prompts. Additionally, we ana-lyze the perturbation effects on different types ofoffensive content: sexism, racism, regional bias,and anti-LGBTQ+. This research underscores thecritical need for developing more robust modelsto effectively moderate cloaked online offensivecontent.",
  "Chinese Offensive Content Dataset": "Several datasets have been developed for Chineseoffensive language detection. The Chinese Offen-sive Language Dataset (COLD) categorizes sen-tences into groups like individual attacks and anti-bias (Deng et al., 2022a). TOCP and TOCAB fromTaiwans PTT platform address profanity and abuse(Chung and Lin, 2021). The Sina Weibo Sexism Re-view (SWSR) focuses on sexism within Chinese so-cial media (Jiang et al., 2021). The ToxiCN datasetfrom platforms like Zhihu and Tieba includes amulti-level labeling system for offensive language,hate speech, and other categories (Lu et al., 2023).In this work, we introduce ToxiCloakCN, a noveldataset capturing cloaked offensive text using ho-mophonic and emoji replacements, built on top ofthe comprehensive ToxiCN dataset.",
  "Chinese Offensive Content Detection": "Offensive language and hate speech detection havebeen explored in various languages, including En-glish (Davidson et al., 2017; Pitsilis et al., 2018;Wei et al., 2021; Cao et al., 2020; Awal et al.,2021; Cao and Lee, 2020), Arabic (Fatemah andOzlem, 2021), French (Battistelli et al., 2020),Turkish (Beyhan et al., 2022), and Asian languages(Dhanya and Balakrishnan, 2021; Ng et al., 2024).In Chinese, techniques include lexicon-based mod-els (Zhang et al., 2010; Deng et al., 2022b), super-vised and adversarial learning models (Jiang et al.,2021; Liu et al., 2020b), knowledge-based models(Liu et al., 2020a), and fine-tuned pretrained mod-els (Deng et al., 2022a) like BERT (Devlin et al.,2019). Cross-cultural transfer learning models alsoadapt to cultural differences (Zhou et al., 2023).",
  "Language Perturbation": "Various perturbation techniques have been pro-posed to investigate the vulnerabilities of NLP mod-els in adversarial scenarios. These include insert-ing emojis (Kirk et al., 2022), token replacementsand insertions (Garg and Ramakrishnan, 2020),and probability-based greedy replacements (Renet al., 2019). While these methods primarily targetEnglish, adapting them to Chinese is challengingdue to linguistic differences, though some attemptshave been made (Liu et al., 2023). For Chinese, Su et al. have highlighted adversarialattacks such as word perturbation, synonyms, andtypos (Su et al., 2022). Subsequent solutions havefocused on BERT-based models to address these at-tacks (Zhang et al., 2022; Wang et al., 2023; Xionget al., 2024). However, previous work mainly eval-uates BERT-based models and lacks robustnessresearch on LLMs and social media-based adver-sarial datasets reflecting current trends. Our workaddresses this gap by providing a new dataset withrealistic perturbations for Chinese offensive lan-guage detection.",
  "Methodology": "The ToxiCloakCN dataset builds upon the Tox-iCN dataset (Lu et al., 2023) through a detailedmulti-step process. First, we sampled a balanceddataset from the base ToxiCN dataset, known asthe base dataset. Next, this balanced base datasetwas perturbed using homophone and emoji replace-ments to produce the ToxiCloakCN dataset. Forsuch After constructing the ToxiCloakCN dataset,we explored pinyin augmentation as a potential so-lution to address the cloaked offensive contentperturbed using homophone replacements. Finally,we defined six different instructions for evaluatingthe performance of state-of-the-art large languagemodels on ToxiCloakCN.",
  ": Base dataset distribution breakdown by contenttopics": "offensive lexicon (i.e., swearwords) identified inToxiCN. Next, we sampled sentences from ToxiCNlabeled as offensive or hateful that containedthe offensive lexicon, resulting in 2,293 offensivesentences. To balance the dataset, we also sam-pled non-offensive sentences from ToxiCN, givingpreference to sentences containing the offensivelexicon but labeled as non-offensive. In total,we sampled 2,289 non-offensive sentences. Thecombined 4,582 sampled sentences form the basedataset that we will use for perturbation. shows the base dataset distribution break-down by content topics, mirroring the statisticaldistribution of the base ToxiCN dataset. It is worthnoting that a substantial amount of offensive con-tent in the base dataset pertains to racism, followedby sexism and anti-LGBTQ content. We have rela-tively lesser regional bias offensive content in thebase dataset.",
  "Homophone Replacement": "From the base dataset, we replaced all offensivewords in the sample set using the offensive lexiconprovided by ToxiCN. To account for typographicalerrors common in real-world scenarios, we intro-duced a random perturbation rate of 0.3, meaning30% of the characters in each sentence were ran-domly replaced with homophones. These replace-ments were guided by a predefined dictionary2. Ahigher perturbation rate rendered sentences unintel-ligible, while a lower rate inadequately simulatedreal-world conditions. Therefore, the 0.3 rate wasselected to balance visible perturbation with read-ability. These homophone-replaced sentences wereadded to our ToxiCloakCN dataset.",
  "Pinyin Augmentation": "While we aim to benchmark the state-of-the-artLLMs ability to detect cloaked offensive contentin our newly constructed ToxiCloakCN dataset,we also explore potential solutions to aid LLMs inthe detection task. Specifically, we explore pinyinaugmentation method as a potential solution to de-tect homophone-replaced offensive sentences inToxiCloakCN. Pinyin is the official romanizationsystem for Standard Mandarin Chinese in main-land China and Taiwan, using the Latin alphabetto represent Chinese characters phonetically. Theintuition for this method is that, given the natureof homophones, the pinyin representation shouldlook alike, if not the same, thus potentially helpingthe model identify the offensiveness. Both Tox-iCN and ToxiCloakCN datasets theoretically sharethe same phonetic data, despite their textual differ-ences. Therefore, we used the pypinyin4 packageto derive pinyin of the sentences in ToxiCloakCN.",
  "Baselines": "Lexicon-based. We employed a lexicon-baseddetection method to identify offensive language,classifying text as offensive if it contained anywords from the ToxiCN offensive lexicon, other-wise marking it as non-offensive (Xiao et al., 2024;Lu et al., 2023). COLDetector.We implemented COLDETEC-TOR (Deng et al., 2022a), a BERT-based modelfor offensive language detection. This approachinvolves feeding the text into the BERT model, ex-tracting the first hidden state from the final layer,and connecting it to a linear layer for the finalprediction. The model is trained on the COLDdataset (Deng et al., 2022a), a popular benchmarkfor Chinese offensive language detection. Large Language Models.We evaluate GPT-4oandthreeopen-sourceLLMsLLaMA-3-8B(AI@Meta,2024),Qwen1.5-MoE-A2.7B (Team, 2024), and Mistral-7B (Jianget al., 2023)for the Chinese offensive languagedetection task.The open-source models werefine-tuned on the COLD training datasets usingthe six proposed instructions. Utilizing the LORAmethod (Hu et al., 2021), we introduced 4.1 millionadditional parameters, which is only 0.06% of thetotal parameters. Fine-tuning was conducted overthree epochs using the LLM-Adapters Toolkit (Huet al., 2023). GPt-4o and the fine-tuned modelswere then evaluated on the base and ToxiCloakCNdatasets. All fine-tuning and inference phases are",
  "Chinese_Text+Pinyin0.630 (17.43%)-0.763": ": Macro F1 scores of benchmark models. Note that Homophone and Emoji denote the homophone-replacedand emoji-replaced sentences in the ToxiCloakCN dataset, respectively. Best performances are bolded. Values in ()represent the difference between the Macro F1 score on the base dataset and the Homophone/Emoji datasets (i.e.,performance decline). All results without asterisk are statistically significant based on a paired t-test. performed on two NVIDIA A6000 GPUs.Toevaluate the impact of different learning paradigmson offensive language detection, we conductedfine-tuning experiments using the LLaMA-3-8B,Qwen-1.5-MoE-A2.7B, Mistral-7B, and GPT-4omodels. Fine-tuning was performed on the COLDtraining dataset using the six proposed instructiontemplates.",
  "Experimental Results": "presents the offensive detection outcomesfor all models, showing that GPT-4o achieves thehighest performance with Chinese-only text instruc-tions. However, all models exhibit a notable per-formance decline on the homophone and emojireplaced sentences in ToxiCloakCN dataset com-pared to the base dataset. This indicates a signif-icant reduction in their ability to detect offensivecontent when the text is perturbed. The drop in performance is primarily due to the probabilisticnature of LLMs, which rely on next-word predic-tion based on statistical probabilities. Perturbationslike homophone and emoji replacements disruptthis probability chain, compromising the modelsability to generate coherent and contextually accu-rate responses. 4.3.1Effects of Pinyin AugmentationWhen pinyin was added to the text, we observeda performance reduction across all models on thehomophone-replaced sentences in ToxiCloakCNdataset compared to text-only inputs. Instead ofenhancing model capabilities, pinyin disruptedperformance, leading to lower classification accu-racy. Pinyin-only input experiments consistentlyshowed lower performance compared to text in-puts, highlighting the models limitations in pro-cessing pinyin. This aligns with Li et al.s find-ing that LLMs recognize pinyin but struggle withpronunciation, making it difficult to correct phono-logical errors. This limitation affects the modelsability to detect offensive language significantlyperturbed through homophone replacements. Theperformance drop with pinyin augmentation is par-tially due to pinyin not being directly associatedwith the original phrase, causing confusion for the : The models error rates comparison between the sentences in the base dataset and the homophone oremoji-replaced sentences using prompt type Chinese_text , broken down by offensive content type. Note that smallererror rates represent better performance.",
  "Effect of Instruction Templates": "Across all models, Chinese instructions consis-tently outperformed English instructions for tasksinvolving Chinese text. Conversely, for pinyin-onlytasks, English instructions yielded better resultsthan Chinese instructions. This is likely due tothe task characteristics: using Chinese instructionsfor Chinese text maintains linguistic consistency,enhancing the models understanding. For pinyin-only tasks, English instructions, which align betterwith the letter-based nature of pinyin, improvedcoherence and model performance.",
  "Case Studies": "The decline in Macro F1 scores across all modelson ToxiCloakCN datasets highlights the challengesthese models face in offensive detection tasks. Inthis section, we aim to empirically explore whyLLMs struggle with offensive content with homo-phone or emoji replacements. Through case studies,we analyzed the effects of the perturbations on var-ious types of offensive content. Additionally, weanalyze specific cases to compare the differencesbetween LLMs and humans understanding of thecloaked offensive content. 1. Text: GPT-4o: Yes, thats offensive, keywords: fu*k pig(), will fu*k()black(), woman()Human Annotator: The phrase is offensive. The speaker is saying that hewould rather fu*k pig () than having sex () with women that have ablack boyfriend () - which is both sexist and racist.2. Text: GPT-4o: - Whilst it is correct to describe a situation of domesticviolence, the direct use of such an acronym may not appear serious andrespectful. - - This is an insulting term that is a personal attack onanother person and is clearly derogatory and abusive in nature. thus, thephrase is offensive.Human Annotator: The phrase is offensive. The speaker is saying that he hasreported () and () roasted () the bit*h ().3. Text: GPT-4o: Your reference to sounds like ahumorous description of something or a situation. If this is a reference to aChick-fil-A toy (the common chocolate egg with small toys), it could bedescribing the fun of such toys, especially when they are rolled around.Human Annotator: The phrase is offensive. The speaker is saying thatoverweight women () is better off () rolling around (). Note that is not a homophone but a slur.",
  "these content types. Note that the performanceis based on the Chinese_Text instruction, and asmaller error rate indicates better performance inthe offensive language detection task": "Generally, we observe that all models have lowererror rates on the base dataset across all offen-sive content types, supporting our initial findingsthat LLMs struggle to detect cloaked offensive lan-guage, regardless of content type. Interestingly, forthe open-source LLMs, we notice a smaller differ-ence between the error rates for regional bias offen-sive content in the base and ToxiCloakCN datasets.This could be due to a generalization issue; theopen-source LLMs are fine-tuned on COLD, whichmay not contain much content related to regionalbias, resulting in poorer performance in detectingthis type of offensive content, regardless of per-turbation. However, for the closed-source model,GPT-4o, we observe performance gaps for regionalbias offensive content when the sentences are per-turbed using homophone and emoji replacements.",
  "To better understand the reasons behind the modelspoor performance on the ToxiCloakCN dataset,we conducted a detailed analysis with the top-performing GPT-4o model, comparing its interpre-": "tations with those of human annotators. Specif-ically, we randomly selected offensive sentencesfrom the ToxiCloakCN dataset to examine howGPT-4o processes these cloaked sentences. Thisanalysis revealed a notable discrepancy betweenthe interpretations made by the model and humanannotators. We focused on capturing a diverserange of examples to illustrate this divergence,highlighting potential weaknesses in GPT-4os abil-ity to accurately detect and interpret subtly alteredoffensive content. The sample was designed toensure coverage of various cloaking techniques, in-cluding homophone substitutions and emoji trans-formations. For this study, we recruited two pro-ficient Chinese speakersan undergraduate and apostgraduate student, both active on Chinese so-cial mediato assess the offensiveness of thesesentences. They provided detailed explanations fortheir judgments, allowing us to directly comparehuman and model interpretations. This side-by-side evaluation helped us empirically identify gapsin GPT-4os comprehension, offering valuable in-sights into areas where the models understandingof cloaked offensive content may fall short. Homophones. presents three homophone-replaced offensive sentences from the Toxi-CloakCN dataset. In the first example, GPT-4ocorrectly identifies the offensive content by rec-ognizing keywords like (fu*k pig). Thissuggests that GPT-4o has some understanding ofhomophones, enabling it to detect cloaked offen-sive language. In the second example, while themodel correctly classifies the sentence as offensive,its explanation does not match the original meaningof the offensive sentence. For instance, it identi-fies as offensive but cannot explain why.The human annotator, however, can reconstruct thesentence and provide an accurate judgment and ex-planation. In the third example, GPT-4o misjudgesand misinterprets the phrase due to its inability tounderstand the cultural background. This exam-ple demonstrates the models limitation in recog-nizing implicit offensive language across differentcultures, whereas human annotators, with their cul-tural understanding, can make accurate judgments. Emoji. presents three emoji-replaced of-fensive sentences from the ToxiCloakCN dataset.In the first example, both GPT-4o and the humanannotator accurately identify the offensive content.This case is relatively simple because offensivekeywords such as (a homophone for di*k) and (AIDS) remain unchanged. In thesecond example, although the model classifies thesentence as offensive, its explanation differs fromthat of the human annotator, indicating a misinter-pretation. This may be due to the models failure tograsp emoji meanings, such as (which meansnot in this context). The third example involvescomplex emoji and homophone replacement, withsimp translated to in Chinese, representedby emojis for (lick) and (dog). (not deserve) was replaced by an emoji ( ) andthe last two characters() are phonetically con-verted to house in English. GPT-4o misclassifiesand misinterprets this complex content, whereasthe human annotators are able identify it, highlight-ing the need for developing more robust solutionscapable of handling such cloaked offensive lan-guages.",
  "Robustness Disparities between Strongand Weak Classifiers": "Through extensive experimentation, we observedthat a lack of robustness in strong base classifiers ismore concerning than in weaker ones. Strong clas-sifiers typically start with higher performance andare expected to handle perturbations more effec-tively. Therefore, a significant performance dropunder perturbation suggests a critical vulnerability,indicating that even high-performing models can beeasily misled. In contrast, the lack of robustness inweaker classifiers is somewhat expected, as thesemodels generally struggle with accuracy even un-der normal conditions. While improving robustnessacross all classifiers is important, the degradationof strong models poses a greater risk, especiallywhen relied upon in high-stakes decision-making. In our experiments, GPT-4o, the strongest classifier,experienced significant performance declines underhomophone and emoji perturbations, as shown in. These perturbations caused a notable dropin Macro F1 scores, revealing a vulnerability evenin robust models. Although other classifiers alsosaw declines, the impact was less severe due totheir relatively lower baseline performance. The distinction between the robustness of strongand weak classifiers is critical. A major perfor-mance drop in strong models like GPT-4o is moreconcerning since these models are expected to bet-ter manage perturbations. This vulnerability under-scores the need for improved robustness, as eventop-performing models can be susceptible to adver-",
  "Conclusion and Future Works": "This study investigated the robustness of Chi-nese offensive language detection models againstcloaking perturbations, specifically homophoneand emoji replacements. We developed the Tox-iCloakCN dataset by augmenting the ToxiCNdataset with these perturbations to simulate real-world evasion tactics. Our experiments showed thatstate-of-the-art models, including GPT-4o, experi-enced significant performance drops when encoun-tering cloaked offensive content. While our pro-posed pinyin augmentation method showed somepromise, its effectiveness varied across models, un-derscoring the complexity of phonetic alignment inoffensive language detection. Case studies further revealed gaps in model com-prehension of cloaked offensive content comparedto human annotators. GPT-4o frequently missedor misinterpreted offensive words disguised withhomophones or emojis, whereas human evaluators,aided by cultural and contextual knowledge, identi-fied the offensive nature of the texts accurately.This highlights the need for models that bettermimic human understanding of nuanced, context-rich language and emphasizes the urgency of devel-oping more advanced techniques to address evolv-ing evasion strategies. Future research should explore cloaking tech-niques beyond homophones and emojis, incorpo-rate broader linguistic variations from real-worldinternet sources, and develop more sophisticatedphonetic alignment methods to enhance model ro-bustness. Additionally, integrating deeper seman-tic understanding and context-awareness into al-gorithms will be critical for effectively managingcloaked offensive language. Given the broader rel-evance of this phenomenon, future work shouldextend these methods to a multilingual setting. Ad-dressing these areas can significantly advance of-fensive language detection, contributing to saferdigital environments.",
  "Limitation": "This study has several limitations. Firstly, whileour dataset includes comprehensive homophoneand emoji perturbations, it may not encompass theentire range of adversarial techniques employed inreal-world scenarios. This limitation could affectthe generalizability of our findings to other pertur-bation forms not examined in this study. Addition-ally, our reliance on the ToxiCN dataset, despite itsrobustness, might not fully capture the diversity ofoffensive language across various Chinese dialectsand regional linguistic nuances. This limitationcould impact the broader applicability of our find-ings. Future research should consider subsamplingperturbed data from real-life internet sources suchas Tieba5 and NGA6 to gain a more accurate andtimely understanding of these perturbed languagesin real life. Lastly, our work does not provide adefinitive solution for addressing all challenges re-lated to cloaked offensive language detection. Fu-ture work should undertake more thorough andadvanced analyses to develop effective solutionsfor these challenges",
  "Ethical Statement": "This research focuses on the detection of offen-sive language, particularly in the context of ho-mophonic and emoji perturbations used to bypassdetection mechanisms. Our primary goal is to high-light the vulnerabilities of current language modelsand enhance their robustness against these cloakingtechniques, thereby contributing to safer and morerespectful online environments. The study involves using systematically perturbeddata to test the limits of existing models. Whilethis approach is crucial for understanding and im-proving detection capabilities, there are inherentrisks associated with the potential misuse of thesefindings. Specifically, the techniques developedto detect cloaked offensive language might alsobe studied to refine evasion tactics further. How-ever, it is important to emphasize that our work issolely aimed at detecting and mitigating offensive",
  "language, not to facilitate censorship or suppressfree speech": "Our dataset and perturbations are derived from ex-isting resources; no new data was collected for thisstudy. The use of ToxiCloakCN aligns with theToxiCN datasets intention, which states, \"All re-sources are for scientific research only.\" We havealso carefully adhered to the Apache-2.0 licenseused by JioNLP and the MIT license for pypinyin. Our research is conducted with the explicit aim ofimproving the detection of offensive language. Ourefforts are directed towards contributing positivelyto the broader field of content moderation, ensur-ing that online platforms can effectively manageoffensive language while respecting the principlesof free and open communication.",
  "AI@Meta. 2024. Llama 3 model card": "Md Rabiul Awal, Rui Cao, Roy Ka-Wei Lee, and San-dra Mitrovic. 2021. Angrybert: Joint learning targetand emotion for hate speech detection. In Pacific-Asiaconference on knowledge discovery and data mining,pages 701713. Springer. Md Rabiul Awal, Roy Ka-Wei Lee, Eshaan Tanwar,Tanmay Garg, and Tanmoy Chakraborty. 2023. Model-agnostic meta-learning for multilingual hate speech de-tection. IEEE Transactions on Computational SocialSystems, 11(1):10861095.",
  "Delphine Battistelli, Cyril Bruneau, and Valentina Dra-gos. 2020. Building a formal model for hate detection infrench corpora. Procedia Computer Science, 176:23582365": "Fatih Beyhan, Buse ark, Inan Arn, AysecanTerzioglu, Berrin Yanikoglu, and Reyyan Yeniterzi.2022. A turkish hate speech dataset and detection sys-tem. In Proceedings of the Thirteenth Language Re-sources and Evaluation Conference, pages 41774185. Rui Cao and Roy Ka-Wei Lee. 2020. Hategan: Ad-versarial generative-based data augmentation for hatespeech detection. In Proceedings of the 28th Interna-tional Conference on Computational Linguistics, pages63276338.",
  "Thomas Davidson, Debasmita Bhattacharya, and In-gmar Weber. 2019.Racial bias in hate speech andabusive language detection datasets.arXiv preprintarXiv:1905.12516": "Thomas Davidson, Dana Warmsley, Michael Macy, andIngmar Weber. 2017. Automated hate speech detectionand the problem of offensive language. In Proceedingsof the international AAAI conference on web and socialmedia, volume 11, pages 512515. Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng,Fei Mi, Helen Meng, and Minlie Huang. 2022a. COLD:A benchmark for Chinese offensive language detection.In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pages 1158011599, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Yong Deng, Chenxiao Dou, Liangyu Chen, DeqiangMiao, Xianghui Sun, Baochang Ma, and Xiangang Li.2022b. BEIKE NLP at SemEval-2022 task 4: Prompt-based paragraph classification for patronizing and con-descending language detection. In Proceedings of the16th International Workshop on Semantic Evaluation(SemEval-2022), pages 319323, Seattle, United States.Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training of deepbidirectional transformers for language understanding.In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 1(Long and Short Papers), pages 41714186, Minneapo-lis, Minnesota. Association for Computational Linguis-tics. LK Dhanya and Kannan Balakrishnan. 2021.Hatespeech detection in asian languages: a survey. In 2021international conference on communication, control andinformation sciences (ICCISc), volume 1, pages 15.IEEE.",
  "Aiqi Jiang, Xiaohan Yang, Yang Liu, and Arkaitz Zu-biaga. 2021. Swsr: A chinese dataset and lexicon foronline sexism detection. Preprint, arXiv:2108.03070": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,Chris Bamford, Devendra Singh Chaplot, Diego de lasCasas, Florian Bressand, Gianna Lengyel, GuillaumeLample, Lucile Saulnier, et al. 2023. Mistral 7b. arXivpreprint arXiv:2310.06825. Hannah Kirk, Bertie Vidgen, Paul Rottger, TristanThrush, and Scott Hale. 2022. Hatemoji: A test suiteand adversarially-generated dataset for benchmarkingand detecting emoji-based hate. In Proceedings of the2022 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Lan-guage Technologies, pages 13521368, Seattle, UnitedStates. Association for Computational Linguistics. Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang,Yangning Li, Feng Zhou, Hai-Tao Zheng, and QingyuZhou. 2023.On the (in)effectiveness of large lan-guage models for chinese text correction.Preprint,arXiv:2307.09007. Hanyu Liu, Chengyuan Cai, and Yanjun Qi. 2023. Ex-panding scope: Adapting English adversarial attacksto Chinese. In Proceedings of the 3rd Workshop onTrustworthy Natural Language Processing (TrustNLP2023), pages 276286, Toronto, Canada. Associationfor Computational Linguistics. Nelson F Liu, Browsing Avci, Andres Abeliuk, RahulAcharya, Kartikeya Ahuja, Klaus Zhuang, Prajit Dhar,Madeleine Fatemi, Sayna Guo, Tanmoy Choudhury,et al. 2020a. Combating negative stereotypes: A compu-tational approach for exposing implicit bias in chinese.In Proceedings of the 28th International Conference onComputational Linguistics, pages 65686581. Nelson F Liu, Tony Wu, Duane S Boning, and TanmoyChoudhury. 2020b. AI bug detector: Adversarial inputdetection for natural language processing models. InProceedings of the 2020 Conference on Empirical Meth-ods in Natural Language Processing: System Demon-strations, page 187196. Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min,Liang Yang, and Hongfei Lin. 2023. Facilitating fine-grained detection of Chinese toxic language: Hierarchi-cal taxonomy, resources, and benchmarks. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1623516250, Toronto, Canada. Association forComputational Linguistics. Ri Chi Ng, Nirmalendu Prakash, Ming Shan Hee, KennyTsu Wei Choo, and Roy Ka-wei Lee. 2024. SGHat-eCheck: Functional tests for detecting hate speech inlow-resource languages of Singapore. In Proceedings ofthe 8th Workshop on Online Abuse and Harms (WOAH2024), pages 312327, Mexico City, Mexico. Associa-tion for Computational Linguistics.",
  "ter data using recurrent neural networks. Applied Intel-ligence, 48:47304742": "Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.2019. Generating natural language adversarial exam-ples through probability weighted word saliency. InProceedings of the 57th Annual Meeting of the Associa-tion for Computational Linguistics, pages 10851097,Florence, Italy. Association for Computational Linguis-tics. Hui Su, Weiwei Shi, Xiaoyu Shen, Zhou Xiao, Tuo Ji,Jiarui Fang, and Jie Zhou. 2022. RoCBert: Robust Chi-nese bert with multimodal contrastive pretraining. InProceedings of the 60th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 921931, Dublin, Ireland. Associationfor Computational Linguistics.",
  "BExamples of Real-World Posts": "In this study, we employed two primary cloakingstrategies: homophone replacement and emoji re-placement. These methods have been widely ob-served as common techniques used to evade of-fensive language detection, especially on Chinesesocial media platforms. presents examples manually collected fromthe Chinese social media platform Tieba. For in-stance, in the first example, a user replaced thephrase (someone dead) with (pri-vate). As the term (private) is harmless incontext, the post successfully evaded detection. Inthe fourth example, a user substituted the character\"\" (mother) with a homophone emoji \"\" andreplaced \"\" (death) with an emoji for \"\" (four),which shares the same pronunciation. This morecomplex replacement also enabled the offensivecontent to avoid detection.From our analysis of manually collected data, weconfirmed that homophone replacement and emojireplacement are the most common strategies usedby Chinese social media users to circumvent detec-tion systems.",
  "CComparison between OffensiveKeyword Replacement and FullPerturbation": "We conducted a comparison between the datasetwhere only offensive keywords were replaced andanother where full replacement was applied, in-volving both keyword replacement and a 30% ran-dom perturbation of the text. Examples from bothdatasets are listed in . When comparingthese examples with real-world data from ,it became clear that the dataset subjected to fullreplacement aligns more closely with actual onlinespeech patterns. As a result, we chose full replace-ment as the preferred approach when creating thedataset. T ok"
}