{
  "Abstract": "Large Multimodal Models (LMMs) excel atcomprehending human instructions and demon-strate remarkable results across a broad spec-trum of tasks. Reinforcement Learning fromHuman Feedback (RLHF) and AI Feedback(RLAIF) further refine LLMs by aligning themwith specific preferences.These methodsprimarily use ranking-based feedback for en-tire generations. With advanced AI models(Teacher), such as GPT-4 and Claude 3 Opus,we can request various types of detailed feed-back that are expensive for humans to provide.We propose a two-stage algorithm ARES thatAlternates REinforcement Learning (RL) andSupervised Fine-Tuning (SFT). First, we askthe Teacher to score how much each sentencecontributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feed-back allows us to consider individual valuablesegments, providing more granular rewards forthe RL procedure. Second, we ask the Teacherto correct wrong reasoning after the RL stage.The RL procedure requires substantial hyper-parameter tuning and often generates errorssuch as repetitive words and incomplete sen-tences. With correction feedback, we stabilizethe RL fine-tuned model through SFT. We con-duct experiments on the multi-modal datasetsScienceQA and A-OKVQA to demonstrate theeffectiveness of our proposal. The ARES ratio-nale achieves around 70% win rate compared tobaseline models judged by GPT-4o. Addition-ally, we observe that the improved rationale rea-soning leads to a 2.5% increase in inference an-swer accuracy on average for the multi-modaldatasets. 1",
  "Correction": ": ARES pipeline: For a given model generating rationale reasoning, we request an AI models sentence-levelscores (ranging from 0.0 to 1.0). The closer the score is to 1.0, the more it helps solve the problem. We proceedwith the RL stage using these sentence-level scores. After RL, the training model may produce incorrect parts(colored in red), so we enhance the training model by requesting correction feedback from the AI model (colored inblue) and then proceed with the supervised fine-tuning stage. Correction Feedback:Given the success ofLLMs and LMMs in a wide range of areas (Brownet al., 2020; Chowdhery et al., 2022; Zhanget al., 2022a), we are not restricted to requestingfeedback in the form of scores.We requestcorrection feedback from advanced AI (Teacher)for sentences containing errors after the RLprocess, and obtain a corrected dataset Xcorrected.Since the supervised fine-tuning is more stable andfinding appropriate hyperparameters is easier thanRL, we proceed with supervised fine-tuning usingXcorrected exactly as in common autoregressivemodel (Vaswani et al., 2023) training to stabilizethe RL fine-tuned model. This reduces the burdenof RLs exhaustive hyperparameter tuning andproperly guides the direction in which the trainingmodel wants to change. How Correction Feedback Helps RL: RL in-creases the probability of positively rewarded ac-tions (or sentences) and decreases the probabilityfor negative rewards. The direction of learning isdetermined by the reward (scalar) value. However,the opposite direction of the reward is sometimesrequired. For example, suppose there is a trun-cated sentence struncated in CoT. struncated gets a neg-ative score because it is an incomplete sentence (Ta-ble 13). If there is no correction stage, the probabil-ity of struncated is simply reduced. What if struncatedcontains some valuable part? This valuable part isignored, and its probability decreases. To alleviatethis issue, we instead receive the corrected sentenceas feedback and encourage the training model to generate complete sentences, which is very chal-lenging to achieve with only RL. showsmore examples of how the correction stage helpsthe RL stage by maintaining the reasoning contextwhile changing the erroneous parts.Additionally, RL is primarily fine-tuned throughPPO (Schulman et al., 2017) to prevent the modelfrom deviating too much from the original model.The KL divergence penalty further prevents de-viation. However, this penalty often causes themodels degeneration. As a solution, InstructGPT(Ouyang et al., 2022) proposes PPO-ptx, wherethe supervised fine-tuning term with the pretrain-ing dataset is included in the loss function. Whilethis aims to align the training model with specificpreferences, it tends to anchor the model to the pre-training dataset. Instead, we perform supervisedfine-tuning through the Teachers correction feed-back to allow the training model to more freelyadapt and meet specific preferences without degen-eration.",
  "Methodology": "This section briefly introduces the preliminaries in.1 and presents our two-stage hybrid algo-rithm ARES that Alternates REinforcement Learn-ing and Supervised Fine-Tuning. 1) We request ascore for each sentence in the Chain-of-Thought(CoT) from the advanced AI model (Teacher) todetermine how much it contributes to solving theproblem (.2). We perform Reinforcement Learning (RL) with the score feedback on our train-ing model. 2) The Teacher corrects minor errorssuch as truncated or slightly incorrect sentences,thereby performing Supervised Fine-Tuning (SFT)(.2).",
  "t=0(yi | x, y<t),(1)": "where y<t indicates previous tokens.To pro-ceed with RL finetuning, Ouyang et al. (2022)train an outcome-supervised reward model (ORM)using ranking-based feedback. With more fine-grained feedback like sentence-level, Lightmanet al. (2023); Wang et al. (2024) train a process-supervised reward model (PRM). Instead of train-ing a reward model, we request score feedbackr(x s<t, st) for each sentence from an advancedAI such as GPT-4 where s<t represents previoussentences.",
  "training model from deviating too far from theoriginal model, thus avoiding degeneration": "Sentence-Level Nuanced Feedback: We requesta score between 0.0 and 1.0 for each sentence inCoT through the advanced AI for RL. The closerthe score is to 1.0, the more relevant and helpfulit is to solving the problem. presents theprompt format. We additionally shift the rewarddistribution by 0.5 to center it at 0 (Zheng et al.,2023). Therefore, the actual range is from 0.5 to0.5. Using these nuanced scores, the RL fine-tunedmodel exhibits emergent behaviors (please referto ). This allows us to understand thedirection in which the model is intended to changethrough RL. Advantages of Using Advanced AI for ScoreFeedback: Although calling the API has disad-vantages, such as incurring costs or facing usagelimits, there exist several advantages to using theadvanced AI for feedback. First, there is no need totrain a reward model. Second, as the RL fine-tunedmodel begins to generate out-of-distributionoutputs that differ from the data used to trainthe reward model, it becomes challenging forthe trained reward model to provide accuraterewards. However, this out-of-distribution problemis effectively addressed with the advanced AI. RL Challenge: One of the challenging factors forRL is hyperparameter tuning (Eimer et al., 2023).This often results in generating repetitive words andtruncated sentences (Ouyang et al., 2022). Addi-tionally, as the model size increases, finding work-ing hyperparameters becomes infeasible for indi-viduals. To alleviate this issue, we utilize correctionfeedback from the advanced AI as the second stage(.3), and proceed with the supervised fine-tuning to stabilize the RL fine-tuned model.",
  "Correction: Supervised Fine-Tuning": "The RL fine-tuning procedure makes modelchanges to maximize the reward sum, such ascorrecting mistakes or explaining why otheroptions cannot be the answer. However, withouthighly tuned hyperparameters (Eimer et al., 2023),the model after the RL phase may result in errorssuch as repeated sentences, truncated sentences,or incorrect content for some data points. (Seeexamples in Appendix D.)",
  "Algorithm Details": "We propose a hybrid algorithm Alternating be-tween REinforcement learning and Supervisedfine-tuning (ARES). illustrates the ARESpipeline. First, we prepare a model with a giventraining dataset and generate rationale reasoningcomposed of several sentences for input. For theRL procedure to align the training model with apreference, we request scores for each sentence.The RL result may include some incorrect parts(colored in red as the 4th sentence in the RL result box), but it aims to maximize the rewards provided.Next, we request correction feedback and create acorrected dataset (colored in blue as the 3rd sen-tence in the Corrected Rationale box) for the super-vised fine-tuning stage. We then repeat the processfrom the RL stage until convergence.",
  "Experimental Setup": "Data: We first evaluate our proposed methodon the ScienceQA (Lu et al., 2022a) dataset, alarge-scale, multi-modal science dataset designedto assess multi-hop reasoning abilities. We chooseScienceQA because it contains reasoning chainsto derive the answer.Each problem consistsof a question, multiple options, multi-modalcontexts, a correct answer, and an annotatedlecture or solution chain (note that around 9.5%lack the solution chain). In addition, we conductexperiments on A-OKVQA (Schwenk et al., 2022),a knowledge-based multi-modal benchmark with adiverse set of challenging questions paired withrationales, demanding non-trivial commonsenseknowledge (see Appendix B). Baselines: We mainly compare our method withMultimodal-CoT (MM-CoT) (Zhang et al., 2023b)as the baseline because it utilizes reasoning chainsto solve multi-modal tasks. MM-CoT leveragestwo distinct models: the first generates a rationalefor a given problem, and the second, an inferencemodel, takes the concatenated input (problem andgenerated rationale). This separated frameworkshows improved performance, even for relativelysmall models such as Flan-AlpacaBase (Chia et al.,2023) (251M) and Flan-AlpacaLarge (790M). Weuse the rationale model provided by MM-CoTfor ScienceQA and retrain the rationale modelourselves for A-OKVQA because there is noprovided model. Prompts for Feedback: Since our proposed ARESrequests different types of feedback for each stage,a corresponding prompt exists separately. We useClaude 3 Haiku for all training to get feedbackbecause it is approximately 20 times cheaper thanthe top competing models, yet still demonstrates de-cent performance. We first request scores rangingfrom 0.0 to 1.0 for each sentence in CoT to proceedwith the RL stage. To obtain reasonable scores, welet Haiku consider the starting point of thought, theprocess of elimination, or true statements. (See .)In order to collect the corrected dataset forthe SFT stage, we let Haiku refer to the givenproblem and correct the answer as the prompt.We ask Haiku to maintain the format of theexisting rationale chains as much as possibleand correct only the parts that require correction.The RL stage often makes the training modelgenerate repetitive sentences.This repetitionis not easily removed even by GPT-4 whenthe repetitive sentence exists in the middle ofrationale reasoning.To reduce the burden offeedback, we simply hard-code the removalof repetitive sentences before adding the gen-erated rationale to the prompt. (See Appendix C.2.) Training Details: For the ARESBase RL stage,we use a learning rate of 2e5 and 10 epochs forPPO with a batch size of 8 for both ScienceQAand A-OKVQA. The learning rate for ARESLargeis 2e5 with 5 epochs for PPO and a batch sizeof 2 for both tasks. We proceed with 2 roundsof our pipeline for ARESBase and 2 rounds forARESLargefor ScienceQA. For A-OKVQA,we proceed with 1 round for both model sizes.For the SFT stage for correction, we followthe hyperparameters used in MM-CoT for bothmodel sizes. Additionally, we replace MM-CoTsinference model, which is the same size as therationale model, with the Low-Rank Adaptation(LoRA) (Hu et al., 2021) added to the rationalemodel (). The LoRA adapter effectivelyutilizes the rationale models features with asmall number of weights,enabling 2x14xfaster inference compared to MM-CoT, whichintroduces a separate inference model (See thetime comparison in and ). For moredetailed settings, please refer to Appendix C. Evaluation Metrics: We use two main metricsto test how our pipeline (ARES) improves ratio-nale reasoning quality. First, we evaluate ARESsrationale reasoning quality against baseline mod-els since we enhance our model based on them.For two different model sizes (Flan-AlpacaBase andFlan-AlpacaLarge) and two tasks (ScienceQA andA-OKVQA), rationale reasoning quality is eval-uated by GPT-4o-2024-05-13 and the win rateis calculated (.3). The GPT-4 series isactively used as an evaluation metric, replacinghuman judgment for various domains (Liu et al.,2023b; Sottana et al., 2023). Second, we assess how the improved rationale reasoning impacts an-swer accuracy (.4). This evaluation is alsoperformed on both model sizes and tasks. Addition-ally, we analyze how the RL stage fine-tunes thetraining model and maximizes the sum of rewardsin .1.",
  "Emergent Behavior Through RL": "Through RL, a training model is aligned to a spe-cific preference. Essentially, the model increasesthe probability of helpful sentences receiving goodrewards and reduces the probability of incorrector meaningless sentences. However, this processproduces some interesting additional results.First, it supplements rationale reasoning forsome problems where rationale reasoning is in-sufficient. In particular, 9.5% of problems in Sci-enceQA have empty rationale reasoning (solution)data. The model generates nothing before the RLstage for these problems but starts generating rea-soning chains afterward (See ). We ob-serve this especially when utilizing PPOs advan-tage normalization or when the learning rate islarge.Second, the training model begins to explain whyother options are not the answer (See ).The process of elimination is a useful method forderiving answers when options are given.",
  "Guide RL with Correction": "Despite the benefits of RL, hyperparameter tun-ing often requires massive effort. Without meticu-lous tuning, the RL fine-tuned model may produceerrors such as repetitive or incomplete sentences.To address these issues, we add a supervised fine-tuning (SFT) stage after RL to correct these errors.SFT is more stable than RL. We evaluate how wellthe SFT stage corrects errors caused by the RLstage for various RL hyperparameters. We test var-ious RL hyperparameters such as learning rate ={5e-6, 1e-5, 2e-5, 5e-5}, batch size = {2, 4, 8, 16,",
  "ARESBase vs MM-CoTBase69.11%ARESLarge vs MM-CoTLarge66.96%": ": We train baseline models, MM-CoT, with theARES pipeline and ask GPT-4o to evaluate which ratio-nale reasoning is better. We compare each baseline fortwo model sizes (ARESBase and ARESLarge) and twotasks (ScienceQA and A-OKVQA). 32}, and PPO epoch = {5, 10, 15}. As a resultof RL, we observe that some of the sentences inrationale chains are repetitive or truncated (see Ta-ble 13 and 12). The SFT stage, with correctionfeedback, reflects the direction in which the modelis fine-tuned through RL and appropriately guidesit ( and 16). However, excessive RL learn-ing rates or epochs cause serious degeneration ofthe model, such as producing no output or gener-ating strange words, and the results of correctionfeedback are also unreasonable.",
  "Rationale Reasoning Comparison": "We check whether ARES improves the quality ofrationale reasoning compared to the baseline model.GPT-4o evaluates which rationale chain is betterbetween the rationale generated by ARES and therationale generated by the baseline model. Werandomly shuffle the rationale chains and providethem as Option A and Option B (see Appendix A.3)for a fair evaluation (Yu et al., 2023). We conductour experiments with two different model sizes,Flan-Base and Flan-Large with ViT feature, onScienceQA and A-OKVQA. shows thatARES achieves around 70% win rate against eachcorresponding baseline model for both datasets.",
  "Inference Accuracy": "We investigate whether the improved rationale alsocontributes to answer inference accuracy. shows the main results of answer inference on theScienceQA. We evaluate our base model against theMM-CoT baseline. ARESBase achieves a 2.79%improvement compared to the corresponding base-line (MM-CoTBase). The large model (ARESLarge)shows some minimal improvement compared to thecorresponding baseline. However, its worth noting",
  "MM-CoTLarge (Zhang et al., 2023b)790M+790M90.7693.5986.5589.6987.8589.5590.9089.1290.26ARESLarge (Ours)790M+76M91.21*92.8089.45*90.27*88.3591.22*91.48*90.3891.09*": ": Main results on the ScienceQA test set (%). Size = backbone size. Question classes: NAT = natural science,SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6= grades 1-6, G7-12 = grades 7-12. Other results are sourced from Lu et al. (2022a) and Zhang et al. (2023b).Results in bold represent the better performance corresponding baseline. (*) indicates the best performance. that despite this seemingly small gain, ARESLargebeats 13B LLaVA (Liu et al., 2023a). This mini-mal improvement may be due to the 9.5% of Sci-enceQA problems needing more rationale reason-ing (around 9.5% problems have empty rationalereasoning). The RL stages can only eliminate someempty rationale reasoning, which requires numer-ous ARES pipeline rounds. Above all, our maingoal is to assess how the RL stage works and howthe SFT stage aids RL. shows the results of answer inferenceon the A-OKVQA. We retrain MM-CoTBase andMM-CoTLarge and evaluate these on the validationset as in (Zhang et al., 2023b) because the test setis hidden. In our experiments, MM-CoT modelsperform around 10% better than the reported ac-curacy in (Zhang et al., 2023b). ARES achieves4.45% gains against MM-CoTBase and 2.35% forMM-CoTLarge.In addition, we demonstrate that two stages, RLand SFT, are essential through an ablation study. shows the rationale reasoning for 4 cases.The baseline model (MM-CoT) produces the samerationale reasoning as the dataset. However, thecorrected reasoning for MM-CoT without the RLstage has insufficient information compared to thereasoning of ARES that performs RL (refer to Ta-ble 17 for more examples). also showsthat inference accuracy gradually improves as eachpart of ARES is executed. 1st RL indicates a sin-gle RL run on MM-CoT, and 1st ARES means one",
  "Related Work": "Chain-of-Thought (CoT) is a multi-step reasoningmethod for problem-solving that encourages LLMsto consider the intermediate reasoning steps. Zero-Shot-CoT (Kojima et al., 2023) promotes CoT byusing prompts such as \"Lets think step by step\" forLLMs. For Few-Shot-CoT (Zhang et al., 2022b;Wei et al., 2023), a few examples with reasoningprocesses are provided, allowing the model to re-fer to these examples and understand how to per-form CoT. Wei et al. (2023) reveal that this CoT",
  "Feedback on MM-CoTBase": "The first sentence, \"The camera isn't working, so the battery must be dead,\" is a compound sentence because it contains two independent clauses (The camera isn't working and the battery must be dead) joined by the coordinating conjunction \"so.\" The second sentence, \"The artist prepared a canvas for a new oil painting,\" is a simple sentence because it contains only one independent clause.",
  ": Comparison of rationales: dataset, baseline,correction feedback for baseline, and ARES": "technique positively impacts the performance oflarge models (> 100B), but has minimal effect onsmaller models. MM-CoT (Zhang et al., 2023b)suggest that CoT is beneficial even for relativelysmall models, such as 200M, if the model that gen-erates intermediate reasoning and the model thatinfers the answer are separated. We find that simplyadding a LoRA adapter (Hu et al., 2021) to the rea-soning model results in comparable performance.This framework enables the LoRA adapter to ef-fectively utilize all features, from raw text to latentfeatures, and generates answers 2x14x faster thanMM-CoT, which uses a separate inference model(See and ). This speed advantagearises from the fact that our framework does notrequire a rationale as input, whereas the separateinference model framework must first generate therationale before using it as input. Reinforcement Learning from Human Feedback(RLHF) (Glaese et al., 2022; Ouyang et al., 2022)and AI Feedback (RLAIF) (Bai et al., 2022) alignLLMs with user preferences. Ouyang et al. (2022)collects ranked feedback from human labelers anduses this feedback to perform Reinforcement Learn-ing (RL). Constitutional AI (CAI) (Bai et al., 2022)collects ranked AI feedback rather than costlyhuman feedback and handles harmfulness withRL. Both approaches learn outcome-supervisedreward models (ORM) using ranking-based feed-back. Lightman et al. (2023), instead, propose a",
  ": Ablation study: The accuracy gradually im-proves as each stage of ARES is added": "process-supervised reward model (PRM) that lever-ages sentence-level feedback for CoT. Lightmanet al. (2023); Luo et al. (2024) evaluate each trainedORM and PRM with searching algorithms such asbest-of-N or Monte Carlo Tree Search (MCTS) byselecting the highest-scored solution, demonstrat-ing that the PRM-selected solution outperforms theORM-selected one. Wang et al. (2024) performRL using PRM, providing heuristic sentence-levelscores for math problems that are simple to grade.As an LLM is trained with RL and starts generat-ing outputs different from the original distribution,these reward models would not correctly providerewards (Pitis, 2023; Byun and Perrault, 2024). In-stead of training a reward model for a more generaltask, we perform RL by requesting sentence-levelrewards from advanced AI models such as GPT-4.",
  "Conclusion": "We propose a hybrid algorithm, ARES, whichAlternates REinforcement Learning (RL) andSupervised Fine-Tuning (SFT) to enhance multi-modal rationale reasoning for ScienceQA and A-OKVQA. ARES leverages two types of feedback:1) ARES requests a score from a Teacher (we usedClaude 3 Haiku) for sentence-level nuanced feed-back and proceeds with RL. 2) ARES requests theTeacher to correct rationale chains after RL, stabi-lizing the RL fine-tuned model with SFT. ARESis designed to aid the RL procedure without mas-sive hyperparameter tuning while properly reflect-ing the desired changes from RL. We evaluate theimprovement in rationale reasoning produced byARES compared to baselines using GPT-4o, andassess how much the improved rationale chains en-hance inference accuracy for the two multi-modaltasks. We hope our work inspires further researchon utilizing various types of AI feedback.",
  "Limitations": "Although we address general multi-modal rationalemodels beyond mathematical problems, receivingfeedback from AI models still needs to be morereliable for more complex tasks such as graduate-level math or expert-level knowledge. For instance,some A-OKVQA problems even contain challeng-ing questions requiring external knowledge beyondthe image alone. This challenge highlights thenecessity for future research to develop methodsthat can effectively incorporate external knowledgesources into the model. Additionally, if the modelis not publicly available for free, using the APIincurs costs, and there are daily usage limits.",
  "We sincerely appreciate the generous support ofcomputational resources provided by the Ohio Su-percomputer Center (Center, 1987)": "Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and Lei Zhang.2018. Bottom-up and top-down attention for imagecaptioning and visual question answering. Preprint,arXiv:1707.07998. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Carol Chen, Catherine Olsson, Christo-pher Olah, Danny Hernandez, Dawn Drain, DeepGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemi Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel R. Bow-man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, andJared Kaplan. 2022. Constitutional ai: Harmlessnessfrom ai feedback. Preprint, arXiv:2212.08073. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario",
  "Yew Ken Chia, Pengfei Hong, and Soujanya Poria. 2023.Flan-alpaca: Instruction tuning from humans andmachines": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. Preprint, arXiv:2204.02311. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Preprint,arXiv:2305.06500. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. Preprint, arXiv:2010.11929.",
  "Amelia Glaese, Nat McAleese, Maja Trebacz, JohnAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,": "Laura Weidinger, Martin Chadwick, Phoebe Thacker,Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, AbigailSee, Sumanth Dathathri, Rory Greig, Charlie Chen,Doug Fritz, Jaume Sanchez Elias, Richard Green,Sona Mokr, Nicholas Fernando, Boxi Wu, RachelFoley, Susannah Young, Iason Gabriel, William Isaac,John Mellor, Demis Hassabis, Koray Kavukcuoglu,Lisa Anne Hendricks, and Geoffrey Irving. 2022.Improving alignment of dialogue agents via targetedhuman judgements. Preprint, arXiv:2209.14375.",
  "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023b. G-eval:Nlg evaluation using gpt-4 with better human align-ment. Preprint, arXiv:2303.16634": "Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022a. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. In The 36th Conference on Neu-ral Information Processing Systems (NeurIPS). Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-ChunZhu. 2022b. Iconqa: A new benchmark for abstractdiagram understanding and visual language reason-ing. Preprint, arXiv:2110.13214. Liangchen Luo, Yinxiao Liu, Rosanne Liu, SamratPhatale, Harsh Lara, Yunxuan Li, Lei Shu, YunZhu, Lei Meng, Jiao Sun, and Abhinav Rastogi.2024. Improve mathematical reasoning in languagemodels by automated process supervision. Preprint,arXiv:2406.06592. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. Preprint, arXiv:2203.02155. Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu,Steven Hoi, Xiaogang Wang, and Hongsheng Li.2019. Dynamic fusion with intra- and inter- modal-ity attention flow for visual question answering.Preprint, arXiv:1812.05252.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2023. Attention is allyou need. Preprint, arXiv:1706.03762": "Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, DamaiDai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui.2024.Math-shepherd: Verify and reinforce llmsstep-by-step without human annotations. Preprint,arXiv:2312.08935. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Preprint,arXiv:2201.11903.",
  "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Ja-son Weston. 2024. Self-rewarding language models.Preprint, arXiv:2401.10020": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hong-sheng Li, and Yu Qiao. 2023a. Llama-adapter: Effi-cient fine-tuning of language models with zero-initattention. Preprint, arXiv:2303.16199. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-haylov, Myle Ott, Sam Shleifer, Kurt Shuster, DanielSimig, Punit Singh Koura, Anjali Sridhar, TianluWang, and Luke Zettlemoyer. 2022a. Opt: Openpre-trained transformer language models. Preprint,arXiv:2205.01068.",
  "Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,George Karypis, and Alex Smola. 2023b.Multi-modal chain-of-thought reasoning in language mod-els. Preprint, arXiv:2302.00923": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, WeiShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,Zhangyue Yin, Rongxiang Weng, Wensen Cheng,Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023.Secrets of rlhf in large language models part i: Ppo.Preprint, arXiv:2307.04964.",
  "A.3Prompt for Win Rate Evaluation": "We prompt GPT-4o(2024-05-13) to choosewhich generated rationale is better for solving thequestion because we dont have gold rationales.Given two generated rationales (e.g., MM-CoTBaseand ARESBase), we ask GPT-4o: \"You are giventwo rationale options (A or B). Your job is to selectthe better rationale between A and B for solvingthe given problem with the given image, choices,hint, and answer. Please output only A or B.\"(see ). Yu et al. (2023) find that ChatGPT",
  "Sentence-LevelNuanced Feedback": "[Prompt when Image is provided]There exists a set comprising Image, Options, Hint, and Answer for aQuestion. The reasoning process used to deduce the answer is providedin JSON format. Fill in \"xxx\" with values ranging from 0.0 to 1.0,in increments of 0.1. The reasoning may include the starting pointof thought, the process of elimination, or true statements, althoughthese may not appear to be directly related to the answer at first glance.A value closer to 0.0 indicates a completely incorrect rationale, 0.5indicates a neutral rationale such as the initial thought process or truestatements that guide later guesses towards the answer, and a value closerto 1.0 denotes a correct or relevant rationale for the question. Please justfill the \"xxx\" parts and only return the JSON format. If a sentence isrepetitive (appeared before), then give 0.0.",
  "BDifficulties with External Knowledge inA-OKVQA": "The A-OKVQA dataset includes challenging ques-tions paired with rationales that demand knowl-edge beyond the information available in the im-age. These questions cannot be answered simplyby querying a knowledge base, as they require adeeper understanding and integration of externalknowledge.Our model faces difficulties with problems that cannot be resolved using only the information fromthe image. While our approach is designed to im-prove rationales by addressing grammatical errorsand incomplete or incorrect statements, it struggleswith questions that necessitate external knowledge. illustrates an example where the ques-tion requires knowledge about the typical PSI rangefor bicycle tires. This information is not visuallyapparent from the image of the bicycle alone. Toanswer this question correctly, one needs exter-nal knowledge about standard bicycle maintenancepractices and the recommended PSI ranges for dif-ferent types of bicycle tires. This highlights a chal-lenge for our model, as it must provide correctrationales and answers without access to such ex-",
  "ternal knowledge, relying solely on the image andits internal knowledge base": "In the second example (), understandingwhat the first two numbers of an identification tagdenote requires specific external knowledge aboutlivestock tagging systems. Such tagging systemsoften use coded information, where the numbersmight represent the birth month, year, or other iden-tification details. This information is not visuallyapparent from the image and requires familiaritywith agricultural practices or livestock manage-ment, illustrating the challenge for our model inproviding correct rationales and answers withoutexternal knowledge.",
  "CTraining Details": "ScienceQA has 21K multi-modal problems, with12K for training, 4K for validation, and 4K fortesting. It also includes various difficulty levelsfrom elementary to high school, covering domainslike natural science, language science, and socialscience. In addition, we conduct experiments onA-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of25K questions A-OKVQA includes 25K questions(17K for training, 1K for validation, and 6K fortesting).We adapt the same T5 encoder-decoder archi-tecture (Raffel et al., 2023) under Base and Largesettings, following (Zhang et al., 2023b), and ini-tialized with Flan-Alpaca (Chia et al., 2023).",
  "For the ARESBase and ARESLarge training on theScienceQA and A-OKVQA dataset, we employthe following settings:": "Common Settings:We use top-k samplingwith k = 50 and sample 4 actions. The initialcoefficient for the Kullback-Leibler (KL) diver-gence is set to 0.0001. The range for clipping theprobability ratios in PPO is 0.2.The discountfactor is set to 1.0. Token length is constrained to512. We train the model using 4 NVIDIA A10080GB GPUs.",
  "C.2Supervised Fine-Tuning": "We use a batch size of 8 and train for 20 epochswith a learning rate of 8e5 for ARESBase,following (Zhang et al., 2023b). For ARESLarge,we use a batch size of 2 and train for 50 epochswith a learning rate of 5e5. The output length isset to 64 tokens. Training for ARESBase utilizes 1A100 GPU, while training for ARESLarge utilizes4 A100 GPUs. In the MM-CoT paper (Zhanget al., 2023b), because the final_eval settingwas not consistent, we retrained the base modelwith final_eval=true and the large model withfinal_eval=false for consistency. Token Cleanup: In order to collect the correcteddataset, we need to identify tokens representingthe end of each sentence, such as periods, questionmarks, and exclamation marks. In the ScienceQAdataset, a newline character often follows the n be-ing added after it. To reduce the burden of feedback,we simply hard-code the removal of repetitive sen-tences before adding the generated rationale to theprompt. We remove this n and also ignore thebackslash (\\) character. For overlapping sentences,we placed each rationale of a problem into a list. Ifa rationale sentence was already in the list, we didnot include it again during this preprocessing step.",
  "C.3LoRA Adapter Training": "MM-CoT utilizes two identically sized models forreasoning and inference tasks. In our approach, wereplace the inference model with a LoRA adapter(), which is added to the rationale modeland consists of only one-tenth of the weights.For LoRA adapter training for ScienceQA and A-OKVQA, we use a LoRA rank of r = 64, a LoRA = 128, and a LoRA dropout rate of 0.05. Thelearning rate is set to 8e5 for both ARESBase andARESLarge. The batch size is 16 for ARESBase and4 for ARESLarge on the ScienceQA dataset. For",
  "C.4Time Comparison between LoRAAdapter and Inference Model": "Rather than introducing a separate model for in-ference, we achieve comparable performance byadding the LoRA adapter to the rationale model,while simultaneously obtaining a 2x14x speedupin inference (ARES) compared to MM-CoT, whichintroduces a separate inference model. We providethe time comparison in and .This speed gap is mainly due to the fact that theseparate inference model requires rationale gen-eration before the inference procedure. However,the LoRA adapter directly refers to the rationalemodels latent features to derive answers, eliminat-ing the need to generate the rationale first.",
  "DComparison of Generated Rationales": "As mentioned in .3 and .1, be-cause RL increases the probability of sentencesreceiving positive rewards and reduces the proba-bility of sentences receiving negative rewards, thetrained model often exhibits specific phenomena.It tends to generate repetitive and incomplete sen-tences ( and ). Before the RLsteps, the model couldnt produce rationales, butafter RL steps, it starts generating meaningful ratio-nale reasoning (). Furthermore, it beginsto generate reasons why other options are not theanswer ().As illustrated in , we compare thesolutions from the ScienceQA original dataset, the rationales generated by the baseline model(MM-CoTBase), the rationales from the baselinemodel with correction feedback applied, and the ra-tionales generated by our model (ARESBase). Thefirst example, \"Which property do these three ob-jects have in common?\" illustrates that the baselinemodel generates incorrect rationales such as \"Thelemon is not (yellow)\" and \"All three objects arerough. The property that all three objects havein common is rough.\" However, when we applycorrection feedback to the rationales generated bythe baseline model and compare it to our proposedmethod, we see that our approach generates correctrationales that include the correct answer and pro-vide explanations on why other options are not theanswer. The second example also shows that ourmethod improves rationale reasoning.",
  "Correction Feedback": "[Prompt when Image is provided]Your task involves reviewing a set that includes an Image, Options, Hint,Answer, and Rationales for a Question. Please follow below 7 rules.1. Preserve any correct original rationales based on the given answerby incorporating them into the final rationale without making anyalterations.2. Preserve any original rationales that represent the starting point ofthought.3. Correct any grammatical errors or incomplete rationales based on thegiven information without your knowledge.4. If there are incorrect rationales based on the given answer, pleasecorrect them without removing them based on the given information.5. Please take into account the content of the options, hint, and answerwhen doing this task.6.Fill the corrected rationales inside the {} in the final_rationaleaccording to the given format below, without any additional explanation.7. Return only the entire set of Rationales within curly braces ({}) belowwith the filled one in the step 6.",
  "Before RL[Rationale reasoning]The sentence is in future tense. You can tell because it uses will beforethe main verb, print. The verb tells you about something that is going tohappen": "After RL[Rationale reasoning]The sentence is in future tense. You can tell because it uses will beforethe main verb, print. The verb tells you about something that is going tohappen. The verb tells you about something that is going to happen. Theverb tells you about something that is going to happen. The verb ends in-s and tells you about something that is going to happen...",
  "Before RL[Rationale reasoning]<empty>": "After RL[Rationale reasoning]Rain is a liquid. A liquid takes the shape of any container it is in. Ifyou put rainwater into a bucket, the rainwater will take the shape of thebucket. But the rainwater will still take up the same amount of space. Thewater in a fishbowl is a liquid. A liquid takes the shape of any containerit is in. If you pour water from a fishbowl into a different container, thewater will take the shape of that container. But the water will still take upthe same amount of space. A hammer is a solid. A solid has a size andshape of its own. Many hammers are made of iron and wood.The waterin a fishbowl is a liquid. A liquid takes the shape of any container it isin. If you put water from a fishbowl into a different container, the waterwill take the shape of that container. But the water will still take up thesame amount of space. A rain is a liquid. A liquid takes the shape of anycontainer it is in. If you put rainwater into a bucket, the rainwater willtake the shape of the bucket. But the rainwater will still take up the sameamount of space. A hammer is a solid. A solid has a size and shape of itsown. Many hammers are made of iron and wood. Both iron and woodare solids.",
  "Example of the Process of Elimination After the RL Stage": "Problem: Which is a compound sentence?Choices: [\"Last night we heard an owl outside in the oak tree.\",\"I ate all of my dinner, so I can have some pie.\"]Answer: \"I ate all of my dinner, so I can have some pie.\" Before RL[Rationale reasoning]The first sentence is the compound sentence. It is made up of two simplesentences joined by a comma and the conjunction so. I ate all of mydinner, so I can have some pie. After RL[Rationale reasoning]The first sentence is the compound sentence. It is made up of two simplesentences joined by a comma and the conjunction so. I ate all of mydinner, so I can have some pie. The second sentence does not have acomma, so it is not a compound sentence. Last night we heard an owloutside in the oak tree.",
  "After RL": "Will these magnets attract or repel? To find out, look at which poles are closestto each other. The north pole of one magnet is closest to the south pole of theother magnet. Poles that are the same repel. So, these magnets will repel eachother. The picture shows two magnets that are the same shape and the samesize. Both magnets are the same size. So, these magnets will repel each other.",
  "CorrectedRationale": "Columbia is the correct answer as it is the capital city of South Carolina. Itis the largest city in the state and serves as the seat of government. The otheroptions, Montgomery (the capital of Alabama), Charleston (a city in SouthCarolina), and Harrisburg (the capital of Pennsylvania), are not the capital ofSouth Carolina.",
  "OriginalSolution": "Look at each object. For each object, decide if it has that property. A roughobject feels scratchy when you touch it. The rain boots and the rubber duck arenot rough. A sticky object can attach or stick to other things. The rain bootsand the rubber duck are not sticky. Yellow is a color. This color is yellow. Allthree objects are yellow. The property that all three objects have in common isyellow.",
  "MM-CoTBase": "Look at each object. For each object, decide if it has that property. A stickyobject can attach or stick to other things. The rubber duck is not sticky. Yellowis a color. This color is yellow. The rubber duck and the rain boots are yellow,but the lemon is not. A rough object feels scratchy when you touch it. All threeobjects are rough. The property that all three objects have in common is rough.",
  "ARESBase(Ours)": "The three objects shown - rubber duck, rain boots, and lemon - all share thecommon property of being yellow in color. The rubber duck is made of asmooth, non-sticky material, the rain boots are also a yellow-colored object,and the lemon is also a yellow-colored liquid. Therefore, the correct answeris that the common property among these three objects is that they are yellow.The other options, rough and sticky, do not apply to all three objects, as therubber duck is not rough, and the rain boots are not sticky."
}