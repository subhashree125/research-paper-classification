{
  "Abstract": "Fine-grained category discovery using onlycoarse-grained supervision is a cost-effectiveyet challenging task. Previous training methodsfocus on aligning query samples with positivesamples and distancing them from negatives.They often neglect intra-category and inter-category semantic similarities of fine-grainedcategories when navigating sample distribu-tions in the embedding space. Furthermore,some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcom-ings, we introduce a method that successfullydetects fine-grained clusters of semanticallysimilar texts guided by a novel objective func-tion. The method uses semantic similarities ina logarithmic space to guide sample distribu-tions in the Euclidean space and to form distinctclusters that represent fine-grained categories.We also propose a centroid inference mecha-nism to support real-time applications. Theefficacy of the method is both theoretically jus-tified and empirically confirmed on three bench-mark tasks. The proposed objective functionis integrated in multiple contrastive learningbased neural models. Its results surpass exist-ing state-of-the-art approaches in terms of Ac-curacy, Adjusted Rand Index and NormalizedMutual Information of the detected fine-grainedcategories. Code and data are publicly availableat",
  "Introduction": "Fine-grained analysis has drawn much attentionin many artificial intelligence fields, e.g., Com-puter Vision (Chen et al., 2018; Li et al., 2021;Wang et al., 2024a; Park and Ryu, 2024) and Nat-ural Language Processing (Ma et al., 2023; Tianet al., 2024; An et al., 2024), because it can pro-vide more detailed features than coarse-graineddata. For instance, as illustrated in , solely",
  "Intent: Buy_Off-roadI recommend buyingan off-road vehicle": ": A fine-grained intent detection example. Left:This panel illustrates the label hierarchy, transitioningfrom coarse-grained to fine-grained granularity. Right:This example demonstrates intent detection in a conver-sation about car choices, showing how coarse-grainedanalysis alone can lead to incorrect recommendationsby a life assistant due to a lack of fine-grained analysis. based on coarse-grained analysis, the chatbot mightincorrectly recommend a roadster, which is un-suitable for field adventures. Detecting the fine-grained intent would allow the chatbot to recom-mend an off-road vehicle that aligns with the usersrequirements. However, annotating fine-grainedcategories can be labor-intensive, as it demandsprecise expert knowledge specific to each domainand involved dataset. Addressing this challenge,An et al. (2022) recently introduced Fine-grainedCategory Discovery under Coarse-grained Super-vision (FCDC) for language classification tasks(details in ). Solving FCDC tasks cansignificantly benefit numerous practical applica-tions, for example, fine-grained classification ofenterprise documents (Chen et al., 2023; Vellmeret al., 2023), fine-grained dialogue intent detectiontasks (Tian et al., 2022; Lichouri et al., 2024), prod-uct labeling on online shopping websites based ontext descriptions (Ghani and Fano, 2002; Parekhet al., 2021), and so on. FCDC aims to reduceannotation costs by leveraging the relative ease ofobtaining coarse-grained annotations, without re- quiring fine-grained supervisory information. Thisapproach has sparked significant research interestin the automatic discovery of fine-grained languagecategories (Ma et al., 2023; An et al., 2023a; Vazeet al., 2024; Lian et al., 2024).Existing methods for addressing FCDC are typi-cally grouped into three groups (An et al., 2024):language models, self-training methods, and con-trastive learning methods. Language models (De-vlin et al., 2019a; Touvron et al., 2023), includingtheir fine-tuned versions with coarse labels, gener-ally perform poorly on this task due to a lack of fine-grained supervision. Self-training methods (Caronet al., 2018; Zhang et al., 2021) and their variants of-ten employ clustering assignments as fine-grainedpseudo-labels, filtering out some noisy pseudo-labels, and training with these labels. Dominantcontrastive learning methods (Chen et al., 2020;Mekala et al., 2021; An et al., 2022, 2023a) typi-cally identify positive and negative samples for agiven query by measuring their semantic distances.The contrastive loss ensures that the query samplemoves closer to positive samples and further awayfrom negative samples. So these methods formclusters of samples in the embedding space, witheach cluster representing a discovered fine-grainedcategory, without requiring fine-grained categorysupervision.However, past methods did not utilize compre-hensive semantic similarities (CSS) in the log-arithmic space to guide sample distributions inthe Euclidean space. We define CSS as the fine-grained semantic similarities measured by bidirec-tional Kullback-Leibler (KL) divergence in the log-arithmic space between the query sample and eachavailable positive or negative sample. Although Anet al. (2024) recently explored similarities mea-sured by rank order between the query sample andpositive samples, they ignore similarities with neg-ative samples.We propose a method (STAR) for detectingfine-grained clusters of semantically similar textsthrough a novel objective function, with the corecomponent considering CSS. This componentguides sample distributions in the Euclidean spacebased on the magnitude of CSS in the logarithmicspace. Large semantic differences (low similarity)in the logarithmic space between the query sam-ple and an available sample push the query samplefurther away in the Euclidean space, while smallsemantic differences bring the query sample closerto the available sample. Thus, samples form dis- tinguishable fine-grained clusters in the Euclideanspace, with each cluster representing a discoveredcategory.Additionally, clustering inference used by pre-vious works (An et al., 2022, 2023a, 2024) cannotsupport real-time scenarios, so we propose a variantinference mechanism utilizing approximated fine-grained cluster centroids, delivering competitiveresults for the tasks considered.Our main contributions in this work can be sum-marized as follows: Method: STAR enhances existing contrastivelearning methods by leveraging comprehen-sive semantic similarities in a logarithmicspace to guide sample distributions in the Eu-clidean space, thereby making fine-grainedcategories more distinguishable. Theory: We interpret STAR from the perspec-tives of clustering and generalized Expecta-tion Maximization (EM). Also, we conductloss and gradient analyses to explain the effec-tiveness of using CSS for category discovery. Experiments:Experiments on three textclassification tasks (intent detection (Lar-son et al., 2019), scientific abstract classi-fication (Kowsari et al., 2017), and chatbotquery (Liu et al., 2021)) demonstrate newstate-of-the-art (SOTA) performance com-pared to 22 baselines, validating the theoreti-cal method.",
  "Fine-grained Category Discovery": "Fine-grained data analysis is crucial in Natural Lan-guage Processing (Guo et al., 2021; Ma et al., 2023;Tian et al., 2024) and Computer Vision (Pan et al.,2023; Wang et al., 2024b). However, effectivelydiscovering fine-grained categories from coarse-grained ones remains challenging (Mekala et al.,2021). Traditional category discovery methods of-ten assume that known and discovered categoriesare at the same granularity level (An et al., 2023b;Vaze et al., 2024).To discover fine-grained categories under the su-pervision of coarse-grained categories, An et al.(2022) introduces the FCDC task. Self-trainingapproaches, such as Deep Cluster (Caron et al.,2018; An et al., 2023a), use clustering algorithmsto detect the fine-grained categories, assign pseudo-labels to the clusters and their samples, and then train a classification model with these pseudo-labels. Its variant, Deep Aligned Clustering (Zhanget al., 2021), devises a strategy to filter out in-consistent pseudo-labels during clustering. Con-trastive learning has become prevalent in FCDCtasks; Bukchin et al. (2021) and An et al. (2022)develops angular contrastive learning tailored forfine-grained classification. An et al. (2022) pro-poses a weighted self-contrastive framework toenhance the models discriminative capacity forcoarse-grained samples. Ma et al. (2023) and Anet al. (2023a) uses noisy fine-grained centroidsand retrieves neighbors as positive pairs, respec-tively, applying constraints to filter noise. An et al.(2024) advances this approach with neighbors thatare manually weighted as positive pairs. However,previous efforts have not leveraged comprehensivesemantic similarities to guide sample distributionsand thereby to enhance fine-grained category dis-covery.",
  "Neighborhood Contrastive Learning": "Contrastive learning enhances representation learn-ing by bringing the query sample closer to posi-tive samples and distancing it from negative sam-ples (Chen et al., 2020). Prior research has focusedon constructing high-quality positive pairs. Heet al. (2020) utilizes two different transformationsof the same input as query and positive sample,respectively. Li et al. (2020) introduces the use ofprototypes, derived through clustering, as positiveinstances. Additionally, An et al. (2022) employsshallow-layer features from BERT as positive sam-ples and introduces a weighted contrastive loss.This approach primarily differentiates data at acoarse-grained level, and the manually set weightslimit its broader applicability.To circumvent complex data augmentation,neighborhood contrastive learning (NCL) is devel-oped, treating the nearest neighbors of queries aspositive samples (Dwibedi et al., 2021). Zhonget al. (2021) extends this by utilizing k-nearestneighbors to identify hard negative samples, whileZhang et al. (2022) selects a positive key fromthe k-nearest neighbors for contrastive represen-tation learning. However, these approaches oftendeal with noisy nearest neighbors that include false-positive samples. An et al. (2023a) addresses thisby proposing three constraints to filter out uncertainneighbors, yet they overlooks semantic similaritiesbetween query sample and each available sample.An et al. (2024) represents semantic similarities us- ing rank order among positive samples but neglectssimilarities among negative samples. In contrast,STAR uses comprehensive semantic similarities toguide sample distributions in the Euclidean space,offering richer features and a superior approach topure contrastive learning.",
  "Problem Formulation": "Given a set of coarse-grained categories Ycoarse ={C1, C2, . . . , CM} and a coarsely labeled trainingset Dtrain = {(xi, ci) | ci Ycoarse}Ni=1, whereN denotes the number of training samples, the taskof FCDC involves developing a feature encoder F.This encoder maps samples into a feature space,further segmenting them into distinct fine-grainedcategories Yfine = {F1, F2, . . . , FK}, without anyfine-grained supervisory information. Here, Yfinerepresents sub-classes of Ycoarse. Model effective-ness is evaluated on a testing set Dtest = {(xi, yi) |yi Yfine}Li=1, with L as the number of test sam-ples, utilizing features extracted by F. For evalua-tion consistency and fairness, only the number offine-grained categories K is used, aligning withmethodologies established in previous research(Ma et al., 2023; An et al., 2022, 2023a).",
  "Method": "STAR leverages comprehensive semantic similar-ities and integrates seamlessly with contrastivelearning baselines by modifying the objective func-tion. We have developed variants for three base-lines: PseudoPrototypicalNet (PPNet) (Boney andIlin, 2017; Ji et al., 2020), DNA (An et al., 2023a),and DOWN (An et al., 2024). This section focuseson STAR-DOWN because DOWN outperformsother baselines, with additional method variantsdetailed in Appendix A.3. DOWN involves three steps: pre-training withcoarse-grained labels (.1), retrieving andweighting nearest neighbors (.2), andtraining with a contrastive loss. STAR-DOWNfollows the same first two steps but replaces thethird with a novel objective function (.3).Like DOWN, STAR-DOWN iterates the last twosteps until the unsupervised metric, the silhouettescore of the clustering into fine-grained clusters,does not improve for five consecutive epochs. Thedetailed algorithm is provided in Appendix A.1.7.",
  "Multi-task Pre-training": "As illustrated in , the baseline DOWN (Anet al., 2024) utilizes the BERT Encoder F to ex-tract normalized feature embeddings qi = F(xi)for input xi, where represents the Encoder pa-rameters. To ensure effective initialization for fine-grained training, DOWN pre-trains the Encoderon the coarsely labeled train set Dtrain with la-bels Ycoarse. DOWN utilizes the sum of a cross-entropy loss Lce and a masked language modelingloss Lmlm for multi-task pre-training of the Encoder(detailed in Appendix A.1.1).",
  "Neighbors Retrieval and Weighting": "The Momentum Encoder is a slowly evolving ver-sion of the Encoder, commonly employed in self-supervised learning (He et al., 2020; An et al.,2023a). DOWN integrates the Momentum Encoderto generate more consistent, stable, and better rep-resentations over time (An et al., 2024).In , the Momentum Encoder Fk withparameters k extracts and stores gradient-free nor-malized neighbor features hi = Fk(xi) in a dy-namic data queue Q. To ensure consistency be-tween the outputs of Fk and F, Fks parametersare updated via a moving-average method (He et al.,2020): k mk + (1 m), where m is the mo-mentum coefficient. For each query feature qi, inorder to facilitate semantic similarity capture andfine-grained clustering, its top-k nearest neighborsNi are determined from Q using cosine similarity(Sim): Ni = {hj | hj argtophlQk(Sim(qi, hl))},",
  "where Sim(qi, hl) =qTi hl": "qihl is the cosine similar-ity function.To counteract potential false positives in Ni,DOWN utilizes a soft weighting mechanism basedon neighbor rank to balance information utilityagainst noise, with weights j of neighbor hj calcu- lated as: j = lijk , where is a normalizingconstant for weights, serves as the exponentialbase, k is the retrieved neighbor count, and lij de-notes the rank of hj as a neighbor to qi.To align with the models evolving accuracy inneighbor retrieval during training, DOWN periodi-cally decreases every five epochs, the values for in j are: set = {150, 10, 5, 2}. The j of eachpositive sample hj is used in Eqs. 3 and 4.",
  "qiNtrainLi2.(6)": "As shown in Eq. 4, the term dKL(qi, hk) in Li2represents the bidirectional KL divergence in a log-arithmic space between the query sample embed-ding qi and the data queue sample embedding hk(detailed in Appendix A.1.2). B is a trainable scalarrepresenting the exponential base.The first term in Li2 minimizes the KL diver-gence between query samples and positive samples(the retrieved top-k nearest neighbors Ni in Sec-tion 4.2) while increasing it for negative samples(the samples in data queue Q apart from the pos-itive samples) in the logarithmic space, with asa balancing hyperparameter. The second term inLi2 uses CSS in the logarithmic space, denoted byBdKL(qi,hk), to guide query sample distribution inthe Euclidean space. qTi hk quantifies the cosinesimilarity between normalized qi and hk, equiva-lent to the negative Euclidean distance (detailedin Appendix A.1.4). The value of the trainablescalar B is updated during loss backpropagation, soBdKL(qi,hk) is fully trainable and can integrate withcontrastive learning methods, making the STARmethod generic. 4.3.2Loss AnalysisThe loss Li2 consists of two terms. The first is acontrastive loss that optimizes sample distributionin logarithmic space, ensuring that similar sampleshave a small KL divergence dKL(qi, hk), while dis-similar samples exhibit a large dKL(qi, hk). Thesesemantic similarities are then used as weights inthe second contrastive loss term qTi hk, optimizingthe sample distribution in Euclidean space. KL divergences grow logarithmically, their scaleincreases slowly, making it challenging to differ-entiate semantic differences. In contrast, exponen-tiation scales rapidly. To address this, we applyexponentiation to amplify semantic distinctions,using a trainable scalar base B and an exponentdKL(qi, hk) from the logarithmic space. This re-sults in weights of BdKL(qi,hk) for qTi hk.Since STAR-DOWN discovers fine-grained cat-egories in the Euclidean space, we analyze the sec-ond term Li22 of the loss Li2, which optimizessample distributions in the Euclidean space:",
  "hkQBdKL(qi,hk) exp(qTi hk/)": "(qTi hj/)).(7)In the loss Li22, BdKL(qi,hk) uses CSS in thelogarithmic space to guide sample distributions inthe Euclidean space. A large dKL(qi, hk) (low se-mantic similarity) causes qi to distance itself fromhk in the Euclidean space, reducing qTi hk, whilea small dKL(qi, hk) allows qi to remain relativelyclose to hk compared to negative samples. Thisresults in the formation of compact fine-grainedclusters, with each cluster representing a discov-ered category.Unlike traditional contrastive loss, which mul-tiplies exp qTi hkby 1, our STAR method in-corporates logarithmic space semantic differences,BdKL(qi,hk), as weights1 for each sample pair. Thisis expressed as BdKL(qi,hk) exp qTi hk. As aresult, distant samples are pushed further apart inEuclidean space, while closer samples remain near,facilitating the formation of more distinct bound-aries. We also analyze the STAR method fromthe perspectives of gradient, clustering, and gen-eralized EM. Detailed analyses are provided inAppendix A.2.",
  ": Statistics of datasets (An et al., 2023a). #:number of samples. |C|: number of coarse-grained cate-gories. |F|: number of fine-grained categories": "an alternative, centroid inference, suitable for bothreal-time and other contexts. Using F, we de-rive sample embeddings from Dtrain and assignfine-grained pseudo-labels through clustering. Foreach fine-grained cluster, only the embeddings ofsamples from the predominant coarse-grained cat-egory (the category with the most samples in thisfine-grained cluster) are averaged to form centroidrepresentations. These approximated centroids areused to determine the fine-grained category of eachtest sample based on cosine similarity. A visualexplanation is in Appendix A.1.5.",
  "Baselines for Comparison": "We compare our methods against the followingbaselines.Language models: BERT (Devlinet al., 2019b), BERT with coarse-grained fine-tuning, Llama2 (Touvron et al., 2023), Llama2with coarse-grained fine-tuning and GPT4 (Achiamet al., 2023). Self-training baselines: DeepClus-ter (DC) (Caron et al., 2018), DeepAlignedClus-ter (DAC) (Zhang et al., 2021), and PseudoPro-totypicalNet (PPNet) (Boney and Ilin, 2017; Jiet al., 2020). Contrastive learning baselines: Sim-CSE (Gao et al., 2021), Ancor (Bukchin et al.,2021), Delete (Wu et al., 2020), Nearest-NeighborContrastive Learning (NNCL) (Dwibedi et al.,2021), Contrastive Learning with Nearest Neigh-bors (CLNN) (Zhang et al., 2022), Soft Neigh-bor Contrastive Learning (SNCL) (Chongjianet al., 2022), Weighted Self-Contrastive Learn- ing (WSCL) (An et al., 2022), Denoised Neigh-borhood Aggregation (DNA), and Dynamic Or-der Weighted Network (DOWN) (An et al., 2023a,2024). We also explore variants incorporating thecross-entropy loss (+CE). 5.1.3Evaluation MetricsTo evaluate the quality of the discovered fine-grained clusters, we use the Adjusted Rand Index(ARI) (Hubert and Arabie, 1985) and NormalizedMutual Information (NMI) (Lancichinetti et al.,2009). For assessing classification performance,we use clustering Accuracy (ACC) (Kuhn, 2010;An et al., 2023a). Detailed descriptions of thesemetrics are provided in Appendix A.5. 5.1.4Implementation DetailsTo ensure fair comparisons with baselines, we usethe BERT-base-uncased model as the backbone forall STAR method variants. We adhere to the hy-perparameters used by the integrated baselines todemonstrate the effectiveness of our STAR method.The learning rate for both pre-training and train-ing is 5e5, using the AdamW optimizer with a0.01 weight decay and 1.0 gradient clipping. Thebatch size for pre-training, training, and testing is64. The temperature is set to 0.07. The expo-nential base B in loss is set to 10. The numberof neighbors k is set to {120, 120, 250} for theCLINC, HWU64, and WOS datasets, respectively.Epochs for pretraining and training are set to 100and 20, respectively. The values are {0.03, 0.05,0.1} for the CLINC, HWU64, and WOS datasets.The momentum coefficient m is set to 0.99. Furtherdetails are provided in Appendix A.4. 5.1.5Research QuestionsThe following research questions (RQs) are inves-tigated: 1. What is the impact of STAR methodon FCDC tasks? 2. What are the effects of theproposed real-time centroid inference comparedto traditional clustering inference? 3. How doeseach component of the STAR method affect perfor-mance? 4. How can we effectively and efficientlyset the base for the exponential function in theSTAR method?",
  "ACCARINMIACCARINMIACCARINMI": "STAR-DOWN (clustering)80.310.2670.220.5987.280.3192.450.3887.050.1796.200.0781.980.6769.270.6079.990.40STAR-DOWN (centroid)79.440.5169.130.7586.970.4092.600.4587.160.5396.210.0981.890.5369.050.3979.780.32 : Comparison of clustering and centroid inference mechanisms. \"Clustering\" clusters test set sampleembeddings to determine each samples fine-grained category, while \"Centroid\" infers the category by comparingeach test samples cosine similarity to fine-grained centroids. 2023; Achiam et al., 2023) (GPT4 prompt in Ap-pendix A.6) perform poorly on the FCDC task dueto the lack of fine-grained supervisory informa-tion. Self-training methods like DC, DAC, and PP-Net (Caron et al., 2018; Zhang et al., 2021; Ji et al.,2020) also struggle because they rely on noisy fine-grained pseudo-labels and overlook comprehensivesemantic similarities (CSS). Contrastive learningmethods such as SNCL (Chongjian et al., 2022) andWSCL (An et al., 2022) perform better by lever-aging positive pairs. DNA (An et al., 2023a) andDOWN (An et al., 2024) further enhance featurequality by filtering false positives and weightingthem by rank. However, these methods still do notuse CSS for sample distributions. Integrating theSTAR method with existing baselines enhances per-formance across all datasets, consistently improv-ing sample distributions in the Euclidean space.The superior performance of STAR is attributedto three factors: First, bidirectional KL diver- gence measures CSS, pushing negative samplesfurther away and relatively bringing positive sam-ples closer based on CSS magnitude, making fine-grained clusters easier to distinguish. Second, thebase B of the exponential in Eq. 4 is a trainablescalar, balancing CSS magnitude and semanticstructure. Third, STAR variants iteratively boot-strap model performance in neighborhood retrievaland representation learning through a generalizedEM process (detailed in Appendix A.2.3).",
  "Inference Mechanism Comparison (RQ2)": "Previous methods (Chongjian et al., 2022; An et al.,2023a, 2024) perform a nearest neighbor searchover the examples of the found fine-grained clus-ters for fine-grained category prediction (we referto this technique as cluster inference). We speed upthis process making it better suitable for real-timetasks by developing a centroid inference mecha-nism (see .4). Results in demon-",
  ": Results (%) of the ablation study for STAR-DOWN on the HWU64 Dataset": "strates that results of centroid inference are compet-itive with cluster inference. When results are of theformer are lower, this is due to two factors: cluster-ing inference leverages inter-relations among testset samples for richer features, while centroid infer-ence depends on centroids derived from noisy sam-ples with fine-grained pseudo-labels. Despite theseissues, centroid inference remains a viable optionfor real-time applications, balancing immediate an-alytical needs with slight performance trade-offs.",
  "Ablation Study (RQ1 & RQ3)": "We examine the impact of various components ofthe STAR method in STAR-DOWN, as detailed in. Our results yield the following insights.(1) Excluding coarse-grained supervision informa-tion during training (w/o CE) reduces model per-formance, as this information is crucial for effec-tive representation learning. (2) Omitting the firstloss term (w/o KL loss) from Eq. 4 diminishesperformance. The KL loss term aligns the KL di-vergence between data samples and the query withtheir semantic similarities. Without it, BdKL(qi,hk) fails to guide the query sample distribution basedon semantic similarities in Eq. 4. (3) Removingthe KL weight BdKL(qi,hk) from Eq. 4 (w/o KLweight) reduces effectiveness. The loss no longerutilizes fine-grained semantic similarities measuredby BdKL(qi,hk) in the logarithmic space to directthe query sample distribution in comparison to allsamples. (4) Eliminating both the KL loss termand the KL weight in Eq. 4 leads to a performancedecline. This omission prevents the optimization ofthe query sample towards positive samples in thelogarithmic space and fails to leverage fine-grainedsemantic similarities in the logarithmic space toinfluence the distribution of query samples relativeto all samples in the Euclidean space.",
  "trainable B (ours)80.310.2670.220.5987.280.31e79.960.1268.890.5586.660.101080.220.2769.610.6587.080.301680.730.3270.140.5887.250.366680.570.3870.200.5287.070.15": ": Averaged results (%) and their standard devia-tions over three runs of multiple STAR-DOWN methodswith five different base values on the HWU64 dataset.To set base value conveniently, we set B as a trainablescalar. similarity in the logarithmic space, as quantifiedby the bidirectional KL divergence. The base B isused to enhance semantic differences, improvingthe discriminability of fine-grained categories. Weexperimented with multiple constant values and atrainable configuration for B, with multiple STAR-DOWN results presented in . The multipleSTAR-DOWN methods with various base valuesconsistently outperform the DOWN method (Ta-ble 2), demonstrating the effectiveness and robust-ness of the STAR method regardless of the basevalue B. Notably, base values that are either toolow (e.g., e) or too high (e.g., 66) disrupt the seman-tic representation by inadequately or excessivelyemphasizing semantic similarities in the logarith-mic space. To set base value conveniently, we set Bas a trainable scalar, achieving favorable outcomesas indicated in .",
  "Inference of Category Semantics": "Prior works (An et al., 2023a, 2024) only discov-ered fine-grained categories and assigned them nu-meric indices without elucidating the categories se-mantics, thus constraining their broader application.We propose utilizing the commonsense reasoningcapabilities of large language models (LLMs) toinfer the semantics of these categories. Specifically,we employ a trained encoder, F, to extract embed-dings from all train set samples and cluster theseembeddings to assign fine-grained pseudo-labelsto each train set sample. For each fine-grainedcategory indicated by a specific pseudo-label, weaggregate all predicted samples from the trainingset and use an LLM to deduce the category seman-tics. Details on the LLM prompt are provided inAppendix A.7.",
  "CategoryACCCategoryACCCategoryACCCategoryACC": "alarm_query63.16datetime_convert100.0general_explain53.85iot_hue_lightoff89.47alarm_remove72.73datetime_query57.89general_joke91.67iot_hue_lighton66.67alarm_set84.21email_addcontact100.0general_negate100.0iot_hue_lightup35.72audio_volume_down87.5email_query73.68general_praise100.0iot_wemo_off88.89audio_volume_mute80.0email_querycontact79.95general_quirky42.11iot_wemo_on85.72audio_volume_up76.92email_sendemail63.16general_repeat73.68lists_createoradd94.74calendar_query63.16general_affirm100.0iot_cleaning100.0lists_query84.21calendar_remove84.21general_commandstop100.0iot_coffee100.0lists_remove94.74calendar_set84.21general_confirm89.47iot_hue_lightchange73.68music_likeness88.89cooking_recipe89.47general_dontcare100.0iot_hue_lightdim58.33music_query63.16music_settings100.0qa_maths92.86transport_taxi100.0news_query78.95qa_stock100.0transport_ticket89.47recommendation_events78.95transport_traffic94.74play_audiobook89.47recommendation_locations100.0play_game89.47play_music89.47qa_currency100.0takeaway_order78.95qa_definition89.47qa_factoid52.63recommendation_movies100.0weather_query89.47transport_query78.95social_post73.68 : The error analysis analyzing the discovered fine-grained categories from STAR-DOWN method on theHWU64 dataset. The numerical values represent the classification accuracy (ACC) for each fine-grained category. grained category samples, such as play_audiobookand qa_currency, are classified with reasonableaccuracy, demonstrating the qualitative effective-ness of our unsupervised method, STAR-DOWN.However, certain fine-grained categories, such asdatetime_query, exhibit lower classification perfor-mance compared to others. A possible reason isthat queries often contain descriptive text, whichcan distract from correctly classifying the text intothe intended query category. For example, thequery \"tell me what time it is in Dallas, Texas\"falls under the datetime_query category, but its de-scriptive nature may lead to misclassification intolocation-related categories. Additionally, some fine-grained categories havevery nuanced semantic differences, making themparticularly challenging for fine-grained discov-ery tasks.Examples include general_quirky,iot_hue_lightdim, iot_hue_lightup, qa_factoid andso on. For instance, the iot_hue_lightup categoryrefers to increasing light brightness, which mustbe carefully distinguished from simply turning thelight on.",
  "Visualization": "We visualize the sample embeddings of STAR-DOWN in . The results demonstrate thatour method forms distinguishable clusters for fine-grained categories, proving STARs effectivenessin separating dissimilar samples and clustering sim-ilar ones. Additionally, we visualize the gener-alized EM perspective of STAR-DOWN in Ap-pendix A.1.6. : The t-SNE visualization of sample embed-dings from STAR-DOWN method on the HWU64dataset, with different colors representing differentcoarse-grained categories. The distinct clusters repre-sent the discovered fine-grained categories.",
  "Conclusion": "We propose the STAR method for fine-grained cat-egory discovery in natural language texts, whichutilizes comprehensive semantic similarities in thelogarithmic space to guide the distribution of tex-tual samples, including conversational intents, sci-entific paper abstracts, and assistant queries, in theEuclidean space. STAR pushes query samples fur-ther away from negative samples and brings themcloser to positive samples based on the comprehen-sive semantic similarities magnitude. This processforms compact clusters, each representing a dis-covered category. We theoretically analyze theeffectiveness of STAR method. Additionally, weintroduce a centroid inference mechanism that ad-dresses previous gaps in real-time evaluations. Ex-periments on three natural language benchmarksdemonstrate that STAR achieves new state-of-the-art performance in fine-grained category discoverytasks for text classification.",
  "Acknowledgements": "We would like to thank all our families and friendsfor their support throughout this work. This re-search was partially supported by the China Schol-arship Council. All content represents the opinionof the authors, which is not necessarily shared orendorsed by their respective employers and/or spon-sors. Marie-Francine Moens is supported by theERC Advanced Grant CALCULUS (788506). Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Wenbin An, Feng Tian, Ping Chen, Siliang Tang,Qinghua Zheng, and Qianying Wang. 2022. Fine-grained category discovery under coarse-grained su-pervision with hierarchical weighted self-contrastivelearning. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 13141323. Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, QinghuaZheng, Qianying Wang, and Ping Chen. 2023a. Dna:Denoised neighborhood aggregation for fine-grainedcategory discovery. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1229212302. Wenbin An, Feng Tian, Wenkai Shi, Haonan Lin,Yaqiang Wu, Mingxiang Cai, Luyan Wang, Hua Wen,Lei Yao, and Ping Chen. 2024. Down: Dynamicorder weighted network for fine-grained category dis-covery. Knowledge-Based Systems, 293:111666.",
  "Rinu Boney and Alexander Ilin. 2017. Semi-supervisedand active few-shot learning with prototypical net-works. arXiv preprint arXiv:1711.10856": "Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar,Rogerio Feris, Raja Giryes, and Leonid Karlinsky.2021. Fine-grained angular contrastive learning withcoarse labels. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 87308740. Mathilde Caron, Piotr Bojanowski, Armand Joulin, andMatthijs Douze. 2018. Deep clustering for unsuper-vised learning of visual features. In Proceedings ofthe European conference on computer vision (ECCV),pages 132149. Tianshui Chen, Liang Lin, Riquan Chen, Yang Wu, andXiaonan Luo. 2018. Knowledge-embedded represen-tation learning for fine-grained image recognition. InProceedings of the 27th International Joint Confer-ence on Artificial Intelligence, pages 627634. Ting Chen, Simon Kornblith, Mohammad Norouzi, andGeoffrey Hinton. 2020. A simple framework forcontrastive learning of visual representations. In In-ternational conference on machine learning, pages15971607. PMLR.",
  "Ze Chen, Wanting Ji, Linlin Ding, and Baoyan Song.2023. Fine-grained document-level financial eventargument extraction approach. Engineering Applica-tions of Artificial Intelligence, 121:105943": "GE Chongjian, Jiangliu Wang, Zhan Tong, Shoufa Chen,Yibing Song, and Ping Luo. 2022. Soft neighborsare positive supporters in contrastive visual repre-sentation learning. In The Eleventh InternationalConference on Learning Representations. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019a. Bert: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019b. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "a little help from my friends: Nearest-neighbor con-trastive learning of visual representations. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 95889597": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.Simcse: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 68946910. Rayid Ghani and Andrew Fano. 2002. Building recom-mender systems using a knowledge base of productsemantics. In Proceedings of the Workshop on Rec-ommendation and Personalization in ECommerce atthe 2nd International Conference on Adaptive Hy-permedia and Adaptive Web based Systems, pages2729. Citeseer. Xiaoting Guo, Wei Yu, and Xiaodong Wang. 2021. Anoverview on fine-grained text sentiment analysis: Sur-vey and challenges. In Journal of Physics: Confer-ence Series, volume 1757, page 012038. IOP Pub-lishing. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, andRoss Girshick. 2020. Momentum contrast for unsu-pervised visual representation learning. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 97299738.",
  "Andrea Lancichinetti, Santo Fortunato, and JnosKertsz. 2009. Detecting the overlapping and hi-erarchical community structure in complex networks.New journal of physics, 11(3):033015": "Stefan Larson, Anish Mahendran, Joseph J Peper,Christopher Clarke, Andrew Lee, Parker Hill,Jonathan K Kummerfeld, Kevin Leach, Michael ALaurenzano, Lingjia Tang, et al. 2019. An evaluationdataset for intent classification and out-of-scope pre-diction. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages13111316. Dan Li, Shuai Wang, Jie Zou, Chang Tian, ElishaNieuwburg, Fengyuan Sun, and Evangelos Kanoulas.2021.Paint4poem: A dataset for artistic visual-ization of classical chinese poems. arXiv preprintarXiv:2109.11682.",
  "Mohamed Lichouri, Khaled Lounnas, and Mohamed Za-karia Amziane. 2024. dzfinnlp at arafinnlp: Improv-ing intent detection in financial conversational agents.arXiv preprint arXiv:2407.13565": "Xingkun Liu, Arash Eshghi, Pawel Swietojanski, andVerena Rieser. 2021. Benchmarking natural languageunderstanding services for building conversationalagents. In Increasing Naturalness and Flexibilityin Spoken Dialogue Interaction: 10th InternationalWorkshop on Spoken Dialogue Systems, pages 165183. Springer. Ruotian Ma, Zhang Lin, Xuanting Chen, Xin Zhou,Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao, andYun Wen Chen. 2023. Coarse-to-fine few-shot learn-ing for named entity recognition.In Findings ofthe Association for Computational Linguistics: ACL2023, pages 41154129. Dheeraj Mekala, Varun Gangal, and Jingbo Shang.2021. Coarse2fine: Fine-grained text classificationon coarsely-grained annotated data. In Proceedingsof the 2021 Conference on Empirical Methods inNatural Language Processing, pages 583594. Zhengxin Pan, Fangyu Wu, and Bailing Zhang. 2023.Fine-grained image-text matching by cross-modalhard aligning network.In Proceedings of theIEEE/CVF conference on computer vision and pat-tern recognition, pages 1927519284. Viral Parekh, Karimulla Shaik, Soma Biswas, andMuthusamy Chelliah. 2021. Fine-grained visual at-tribute extraction from fashion wear. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 39733977.",
  "Sagar Vaze, Andrea Vedaldi, and Andrew Zisserman.2024. No representation rules them all in categorydiscovery. Advances in Neural Information Process-ing Systems, 36": "Jan Vellmer, Peter Mandl, Tobias Bellmann, MaximilianBalluff, Manuel Weber, Alexander Dschl, and Max-Emanuel Keller. 2023. A machine learning approachto enterprise matchmaking using multilabel text clas-sification based on semi-structured website content.In International Conference on Information Integra-tion and Web Intelligence, pages 493509. Springer. Shijie Wang, Jianlong Chang, Zhihui Wang, Haojie Li,Wanli Ouyang, and Qi Tian. 2024a. Content-awarerectified activation for zero-shot fine-grained imageretrieval. IEEE Transactions on Pattern Analysis andMachine Intelligence. Shijie Wang, Zhihui Wang, Haojie Li, Jianlong Chang,Wanli Ouyang, and Qi Tian. 2024b. Accurate fine-grained object recognition with structure-driven rela-tion graph networks. International Journal of Com-puter Vision, 132(1):137160.",
  "Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. 2021.Discovering new intents with deep aligned clustering.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pages 1436514373": "Yuwei Zhang, Haode Zhang, Li-Ming Zhan, Xiao-MingWu, and Albert Lam. 2022. New intent discoverywith pre-training and contrastive learning. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 256269. Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo,Elisa Ricci, and Nicu Sebe. 2021. Neighborhoodcontrastive learning for novel class discovery. In Pro-ceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1086710875.",
  "Lpre = Lce + Lmlm,(8)": "given that Dtrain is the train set and Ycoarse repre-sents the coarse-grained labels for the training set,let Ntrain Dtrain be a training batch with coarselabels Yc.The cross-entropy loss for a single training batchis calculated as the average loss over all samplesin the batch. Here, yi denotes the predicted proba-bility distribution for the i-th sample in the batch,and yi represents the ground truth probability dis-tribution for the i-th sample in the batch. The cross-entropy loss Lce for the training batch Ntrain isgiven by:",
  "c=1yi,c log(yi,c),(9)": "in this context: C is the number of categories,yi,c is a binary indicator (0 or 1) indicating whethercategory label c is the correct classification for sam-ple i, yi,c is the predicted probability for categoryc for sample i.The masked language modeling (mlm) loss Lmlmfor the training batch Ntrain is expressed as theaverage negative log-likelihood of the true tokengiven the masked context for each token in eachsample in the batch:",
  "k = f(hk),k = ELU(f(hk)) + (1 + ),": "where i, k Rl and i, k Rll representthe mean and diagonal covariance of the Gaussianembeddings, respectively. The covariances havenonzero elements only along the diagonal. Thefunctions f and f are implemented as ReLUfollowed by single-layer networks. ELU (exponen-tial linear unit) ensures numerical stability, with e14. Here, l, the Gaussian embedding dimen-sion, is 128.Given the Gaussian distribution parametersi, k Rl and i, k Rll, we define the cor-responding Gaussian distributions Ni = N(i, i)and Nk = N(k, k) for the query sample embed-ding qi and the positive or negative sample embed-ding hk.The bidirectional KL divergence between thequery sample embedding qi and the positive or neg-ative sample embedding hk is calculated using thefunction dKL(qi, hk), which measures fine-grainedsemantic similarities:",
  "r2.(13)": "In astronomy, gravitational force is crucial for de-termining the orbits of celestial bodies. Accordingto the formula, the mass of the bodies significantlyinfluences their orbital paths.Inspired by astronomy, we use fine-grained com-prehensive semantic similarities between the querysample and each available positive or negative sam-ple to guide sample distributions in the embeddingspace, where the similarities are measured by bidi-rectional KL divergence, as shown in Eq. 14:",
  "(14)": "Since bidirectional KL divergence is asymmet-rical, it consists of two components: log M andlog m.In the STAR method, the bidirectional KL di-vergence BdKL(qi,hk) is a key component of theloss function (Eq. 4) that guides sample distribu-tions. It can be decomposed into two divergencecomponents, log(Mm), which is analogous to thegravitational force term m1m2. A.1.4Cosine Similarity ConversionIn trigonometry, cosine rule relates the lengths ofthe sides of a triangle to the cosine of one of itsangles. In , for a triangle with sides a, b,and c, with being the angle opposite side c, thelaw of cosines is expressed as:",
  "c2 c= qTi hk": "The cosine similarity and the negative Euclideandistance represents the similar mathematical mean-ing. Therefore, a smaller Euclidean distance be-tween two samples corresponds to a larger cosinesimilarity. The STAR-DOWN procedure is out-lined in Algorithm 1. In Step 3, STAR-DOWNintroduces a novel contrastive loss, as specified inEq. 5. To ensure fair validation of its effective-ness, STAR-DOWN adheres to Steps 1 and 2 of theDOWN procedure.",
  "A.1.5Centroid Inference": "As shown in , we introduce centroid infer-ence, an alternative inference suitable for real-timeand other contexts. Using F, we derive sampleembeddings from Dtrain and assign fine-grainedpseudo-labels through clustering. For each fine-grained cluster, only the embeddings of samplesfrom the predominant coarse-grained category areaveraged to approximate centroid representations.These approximated centroids are then used to de-termine the fine-grained category of each test sam-ple based on cosine similarity.",
  "A.1.6Visualization": "To verify the generalized EM perspective of STAR-DOWN, we visualize the true neighbor rate andmodel performance curves using three metrics dur-ing the training process, as shown in Figures 6aand 6b. The results indicate that STAR-DOWNprogressively retrieves more accurate neighborsand improves model performance across the threemetrics throughout the training. This improvement is due to the positive feedback loop where moreaccurate neighbor retrieval enhances feature learn-ing, and enhanced feature learning, in turn, leadsto more accurate neighbor retrieval. Thus, STAR-DOWN effectively estimates true neighbors in theE-step and obtains better representations in the M-step, with two steps alternately performed to grad-ually enhance each other. A.1.7Algorithm ProcedureThe STAR-DOWN procedure is outlined in Algo-rithm 1. In Step 3, STAR-DOWN introduces anovel contrastive loss, as specified in Eq. 5. Toensure fair validation of its effectiveness, STAR-DOWN adheres to Steps 1 and 2 of the DOWNprocedure.",
  "A.2STAR-DOWN Analyses": "The STAR method upgrades the original con-trastive loss Li1 in Ltrain to the new loss Li2 asshown in Eq. 16. The first term in Li2 optimizes themethod in the logarithmic space, increasing the KLdivergence magnitude in accordance with seman-tic differences. The second term, Li22, optimizessample distributions in the Euclidean space. Sincefine-grained category discovery occurs in the Eu-clidean space, our analyzes focus on Li22, whichoptimizes the distributions of query samples withinthe Euclidean space:",
  "hkQBdKL(qi,hk) exp(qTi hk/))": "(20)A smaller semantic difference in BdKL(qi,hj) re-sults in a larger gradient magnitude with respectto qTi hj, thereby increasing qTi hj and bringing qicloser to hj.Overall, the gradient optimizes sample distribu-tions in the Euclidean space by leveraging com-prehensive semantic similarities in the logarithmicspace. Large semantic differences (low semanticsimilarities) between the query sample and an avail-able sample push the query sample further away inthe Euclidean space, while small semantic differ-ences (high semantic similarities) bring the query",
  "hjNij hj is the weighted average of": "query qi neighbors embeddings, c= indicates equalup to a multiplicative and/or an additive constant.qi2 = 1 because of normalization, and ci2 is aconstant since the neighbor embedding hj is fromthe dynamic queue without gradient.In Eq. 21, the loss term Li22 indicates that, from a clustering perspective, query samples will clus-ter around the neighbor centroids. These querysamples will be distributed in the Euclidean spacebased on the comprehensive semantic similaritiesin the logarithmic space between the query sam-ple and each available positive or negative sample,as measured by BdKL(qi,hk), effectively distancingdissimilar samples, so that samples could form dis-tinguishable clusters and each cluster represents adiscovered fine-grained category.",
  "Lpre = Lce.(24)": "DNA pre-trains the Encoder F to learn thecoarse-grained information with Eq. 24.Step 2: neighbors retrieval and refinement:DNA retrieves positive samples from the dataqueue Q for each query sample qi and appliesthree principles to eliminate potential false-positiveneighbors:Label Constraint, Reciprocal Con-straint, and Rank Statistic Constraint. The resultingpositive set is Si.Step 3: training:DNA trains the model parameters with the fol-lowing loss:",
  "hkQ exp(qTi hk/)": "(25)D denotes the training batch, Si represents the pos-itive set for qi, and is the temperature parameter.DNA iteratively performs steps 2 and 3 to en-hance model performance. However, DNA doesnot utilize fine-grained semantic similarities toguide the distributions of query samples. To en-sure a fair comparison, STAR-DNA upgrades thetraining loss in step 3 while following the sameinitial two steps as DNA.STAR-DNA introduces a new loss function instep 3 to discover fine-grained semantic similari-ties:",
  "A.3.2STAR-PPNet": "ThePrototypicalNetworkwithpseudo-labels (Boney and Ilin, 2017; Ji et al., 2020)(PPNet) is a widely used approach for fine-grainedunsupervised classification tasks. PPNet typicallyinvolves two steps: in step 1, it employs a cluster-ing algorithm to assign pseudo fine-grained labelsto each query sample qi in the train set Dtrain. Instep 2, it trains using these pseudo-labels with lossEq. 27 to cluster query samples with the samefine-grained pseudo label, thereby discoveringfine-grained categories:",
  "Li = logexp(d(qi, pc))Cc=1 exp(d(qi, pc)).(28)": "Ntrain is the training batch, C is the number offine-grained pseudo-categories, qi is the query sam-ple embedding from the training set Dtrain, d(, )is typically the Euclidean distance, and pc is theprototype embedding of category c, computed asthe mean of the embeddings of the support set ex-amples of category c, pc is the prototype embed-ding of category c, c is a category among C fine-grained categories:",
  "A.4Implementation Details": "For comparison, we use the same BERT model,bert-base-uncased, for feature extraction as in theoriginal baseline papers. We employ GPT4 (ver-sion gpt-4-0125-preview) and Llama2 with 7B pa-rameters. We fine-tune Llama2 with the LoRA tech-nique, where the LoRA rank is 8, and the LoRA is 32. The sample feature dimension in the em-bedding space is 768, and for calculating KL di-vergence, it is 128. The number of neighbors k isset to {120, 120, 250} for the CLINC, HWU64,and WOS datasets, respectively. We use randomseeds {0, 1, 2}. The dimension for Rank StatisticConstraint in the DNA baseline is set to 5. ThePyTorch version is 1.11.0.",
  "max(RI) E(RI),(32)": "where RI is the rand index and the E(RI) is theexpectation of RI. Given a test set with n samples,a sample pair is simply any two distinct sampleschosen from the test set. a: Number of pairs in thesame cluster in both predicted labels and groundtruth labels. b: Number of pairs in different clustersin both predicted labels and ground truth labels. c:Number of pairs in the same cluster in predictedlabels but different in ground truth labels. d: Num-ber of pairs in different clusters in predicted labelsbut same in ground truth labels.",
  "N,(35)": "where I{} is the indicator function, it returns 1 ifthe condition inside the braces is true, and 0 oth-erwise. In this formula, it checks whether the pre-dicted label P( yi) matches the true label yi. yi isthe prediction from clustering and yi is the ground-truth label, N is the number of samples, and P() isthe permutation map function from the Hungarianalgorithm (Kuhn, 2010).",
  "A.7Category Semantics Inference Prompt": "In order to infer the discovered fine-grained cat-egory semantics, we leverage the commonsensereasoning capabilities of GPT4.The following prompt is employed:Can you please check what i last said. Couldyou please check and confirm my last command. Ilike you check question please. Did you get whatjust said correctly? I like you check it please. Didyou get my last instruction correctly? Please canyou check and confirm what just said. Would youcheck the details please. ...... Lower the lightsin the house. Please can you confirm the com-mand. Could you please check and confirm the de-tail. Would you check and confirm question please.Please confirm details. Will you please check andconfirm it. Please would you check instruction.Would you please check question. Could you pleasecheck the detail. Could you please check and con-firm details. Check the last thing said please. Whatyou just said was not so clear to me. I would likeyou check and confirm it please. Did you get whatsaid correctly? I like you to check and confirmwhat i said please. Hello. The provided sentence samples originatefrom the domain of assistant query classification.Predominantly, these samples fit into a specific cat-egory, though a minority are noisy and do not alignwith the primary category. Could you please iden-tify and name this category, and clarify its defini-tion?The ground truth semantics of this discoveredfine-grained category is: general-confirm in theassistant query domain.The outputs from GPT4 are as follows:The primary category for the majority of thesentence samples youve provided seems to be\"confirmation requests.\" This category comprisessentences where the speaker asks the assistant toconfirm if a command or a query was understoodor executed correctly. The requests often seek ver-ification or validation of an action that was pre-sumably communicated earlier, either explicitly orimplicitly.Definition of Confirmation Requests: Confirma-tion requests are communications aimed at verify-ing whether an earlier instruction, command, orstatement was received, understood, and/or pro-cessed correctly. This category is common in user-assistant interactions where clarity and confirma-tion of understanding are critical to ensure accu-rate task execution.Clearly, this method offers a practical and heuris-tic approach to deducing category semantics."
}