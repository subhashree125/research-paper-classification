{
  "Abstract": "We audit how hallucination in large lan-guage models (LLMs) is characterized in peer-reviewed literature, using a critical examina-tion of 103 publications across NLP research.Through the examination of the literature, weidentify a lack of agreement with the term hal-lucination in the field of NLP. Additionally,to compliment our audit, we conduct a surveywith 171 practitioners from the field of NLPand AI to capture varying perspectives on hallu-cination. Our analysis calls for the necessity ofexplicit definitions and frameworks outlininghallucination within NLP, highlighting poten-tial challenges, and our survey inputs providea thematic understanding of the influence andramifications of hallucination in society.",
  "Introduction": "Recent advancements in Natural Language Pro-cessing (NLP) have expanded beyond traditionalMachine Learning tools, evolving into sociotech-nical systems that combine social and technicalaspects to achieve specific goals (Gautam et al.,2024; Narayanan Venkit, 2023). They have nowbecome integral in various domains such as health,policy-making, and entertainment, (Jin and Mihal-cea, 2022; Werning, 2024) showcasing their sig-nificant impact on daily life. However, languagemodels (LM) exhibit negative behaviours such ashallucination and biases (Bender et al., 2021; Guptaet al., 2024). This has catalyzed a surge in researchinvestigating the phenomenon of hallucinations inNLP (Ji et al., 2023a), reflected in the escalatingnumber of peer-reviewed publications on the topic,as illustrated in Fig 1, sourced from SCOPUS.Within the NLP domain, various frameworkshave emerged to define hallucination, primarilyemphasizing its negative aspect. Hallucination hererefers to the models production of references tonon-existent objects or statements, lacking support-ing examples in the training data (Ji et al., 2023a).",
  ": Articles published each year (from 2013 to2023) in SCOPUS that contain the term hallucinationAND (NLP OR AI) in the title, abstract, or keywords": "Despite the growing research on this topic, there isstill a notable divide in our understanding, a lackof a unified framework, and a need for precise defi-nitions (Filippova, 2020a).The necessity to understand this gap is accentu-ated by research demonstrating the societal impactsof hallucinations (Dahl et al., 2024). Hence, thereis a growing need to explore how the field of NLPconceptualizes hallucination. In line with this im-perative, the following questions guide this study:",
  "RQ2: What is the current understanding ofresearchers about hallucinations, and how dothey encounter them in their work?": "To answer the RQ1, we first conduct an auditof the field of hallucinations in NLP by surveying103 peer-reviewed articles1. Subsequently, we con-duct a survey to 171 researchers and academics inthe field to gather their perspectives on this phe-nomenon, providing a novel contribution to theliterature, addressing RQ2. By surveying NLP prac-titioners, the paper incorporates real-world perspec-tives, enriching the theoretical discussions with practical insights. This audit therefore aims tobroaden the communities perspective by presentingpractical insights from researchers employing thesemethods in their work. We also propose an ethicalframework to guide future efforts in comprehend-ing and mitigating hallucinations in LLMs.",
  "Evolution of Hallucination in NLP": "The term hallucination has a long history in ma-chine learning and has been used in various con-texts prior to the LM era. Its earliest documentedusage can be traced to the 2000s when Baker andKanade (2000) applied it in the context of imageresolution enhancement, referring to the generationof new pixel values. Subsequently, \"hallucination\"has been frequently employed in computer visionresearch, including notable works such as Hsu et al.(2010) on face hallucination.In the modern deep learning era, hallucinationwas used first by Andrej Karpathy in his blog fo-cusing on Recurrent Neural Networks (Karpathy,2015). He used the term within the context of LMby illustrating how an LSTM could generate non-existent URLs, effectively hallucinating data. Theterm then gained major traction with the launchof ChatGPT (Wu et al., 2023), where it referredto inaccuracies and factual mistakes produced bymodels (Ji et al., 2023a). However, the field lackeda unified definition, leading to a spectrum of inter-pretations (Filippova, 2020b). In one of the earlierworks, Maynez et al. (2020) divides term usageinto intrinsic and extrinsic hallucination. Intrinsichallucinations are consequences of synthesizingcontent using the information present in the input.Extrinsic hallucinations are model generations thatignore the source material altogether.However, there is a rise in discussion aroundterminology that reflects a deeper inquiry into thephenomena, with recent discourse advocating forconfabulation (Millidge, 2023) or fabrications(McGowan et al., 2023) as a more precise descrip-tor. This reflects the lack of consensus on the termand highlights the importance of looking at the useof hallucination with a more critical lens.",
  "Related Surveys on Hallucination": "We now provide an overview of several key sur-veys in the realm of NLP focusing on the topic ofhallucination and why our work addresses a rele-vant gap in the field. Starting with Ji et al. (2023a),this survey extensively delves into the advance- ments and challenges concerning hallucination inNLG, distinguishing between intrinsic and extrin-sic frameworks of hallucination. Additionally, itsheds light on fundamental terms such as hallu-cination, faithfulness, and factuality, along withprevalent metrics for quantifying these phenomena.Rawte et al. (2023b) categorizes existing workswithin the domain of LMs, covering various as-pects including methods for detecting hallucination,mitigation techniques, datasets used, and evalua-tion metrics. Zhang et al. (2023c) addresses thechallenges of hallucination in LLMs by categoriz-ing hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, divergingfrom traditional viewpoints.Furthermore, Huang et al. (2023) redefines thetaxonomy of hallucination into factuality and faith-fulness, with additional subdivisions, and pro-poses mitigation strategies aligned with underlyingcauses. Tonmoy et al. (2024) offer a comprehen-sive overview of over thirty-two techniques devel-oped to mitigate hallucination in LLMs and finally,Rawte et al. (2023a) present a nuanced categoriza-tion of hallucination into six types, contributing tothe ongoing discourse within the field.While these surveys offer insights into the cur-rent state of hallucination research, they do notpay attention to critical examinations of the fieldsweaknesses arising from a lack of discourse indefining hallucination and challenges due to thesame. This deficiency in discussion reflects thebroader trend within the entire field. Therefore,our audit answers this gap by critically examin-ing how we conceptualize hallucination. We aim tohighlight the challenges stemming from these defi-nitions and to further conduct a practitioner surveywithin the community to understand researchersand developers perspectives on this issue. Our goalis to facilitate a shared understanding of the chal-lenges and mitigation strategies for hallucinationand develop actionable design principles throughan ethics sheet to address these issues effectively.",
  ": Frequency of papers reviewed for each themati-cally grouped NLP tasks": "keywords such as hallucination, NLP (OR) AIAND hallucinations, fabrication, and confab-ulations. We surveyed papers released on and be-fore April 19th, 2024. From this search, a total of164 papers were retrieved. After filtering out pa-pers that were not directly related to hallucinationresearch or those that merely mentioned the termwithout substantial focus on the topic, we arrivedat a corpus of 103 papers. This corpus forms thebasis for our audit and analysis of hallucinationresearch, specifically within the NLP domain.",
  "Conceptualization of Hallucination": "We performed an iterative thematic analysis (Vais-moradi et al., 2013) to uncover the various applica-tions of hallucination research in NLP. To ensureaccuracy and prevent misclassification, this recur-sive process was employed. This resulted in theidentification of seven distinct fields that addressresearch on hallucination (as shown in Table. 1).This taxonomy affords insights into the perva-sive nature of hallucination in NLP. Notably, itreveals that hallucination transcends beyond textgeneration, extending its conceptualization to en-compass broader domains such as Image-VideoCaptioning, Data Augmentation, and Data-to-TextGeneration tasks. This depicts the significance ofhallucination both within and beyond the realm ofNLP. Moreover, our classification framework pro-vides us with a faceted analysis of how each ofthese tasks defines the concept of hallucination.Using thematic categorization, we come acrossdefinite attributes across the definitions of hallu-cination. One set of attributes elucidated how hal-lucinations are associated with the style/languagegenerated by the model: Fluency, Plausibility, andConfidence. The next set of attributes falls underthe effects of hallucinations: Intrinsic, Extrinsic,Unfaithfulness and Nonsensical. The definition ofeach of these attributes is elaborated in .In each paper analyzed within the survey scope,hallucination is defined based on a combination ofthe set of attributes identified. Our survey revealed 31 unique frameworks for conceptualizing hal-lucination, illustrating the diverse approaches andperspectives used. This diversity underscores theambiguity in the terms usage.To illustrate this phenomenon, we present someexamples showcasing the diverse approaches com-monly observed in the literature:Hallucination refers to the phenomenon wherethe model generates false information not sup-ported by the input. - (Xiao and Wang, 2021a)LLMs often exhibit a tendency to produce ex-ceedingly confident, yet erroneous, assertions com-monly referred to as hallucinations. - (Zhanget al., 2023a)Models generate plausible-sounding but un-faithful or nonsensical information called halluci-nations - (Ji et al., 2023c)Hence, within NLP, a notable deficiency persistsin grasping coherent characteristics of hallucina-tion. This shortfall underscores the risk of potentialmisappropriation of the term when employed indivergent contexts. An extensive analysis of hallu-cination for each of the mentioned NLP tasks andits definition is illustrated in the following section.",
  "Hallucination in NLP Tasks": "We now analyze what aspects of the definitions ofhallucination most commonly occur within each ofour identified sub-fields of NLP2 ().Conversational AI: In this sub-field, hallucina-tion encompasses fluency, non-factuality, and bothintrinsic and extrinsic hallucinations. The defini-tions facets highlight that dialogue systems mustbalance conversational fluency with factual consis-tency, aligning both with prior conversation andreal-world truths.Abstractive Summarization: Works in this sub-field mainly focuses on extrinsic and intrinsic hal-lucinations in defining it. Some definitions alsomention the faithfulness of the generation. Despitethe challenges of aligning with real-world facts andsource consistency, prioritizing alignment and ad-herence to the original material has been shown tobe essential in these works.Data2Text Generation: Hallucinations are clas-sified into extrinsic and intrinsic types, similar toabstractive summarization. Here, alignment withthe underlying data is emphasized as the more crit-ical factor when compared to the language used.",
  ": The attributes that appear in the definitions of hallucination": "Machine Translation: Definitions of hallucina-tion predominantly concentrate on extrinsic hallu-cination, with rare mentions of intrinsic hallucina-tions. This observation suggests a lesser concernfor stylistic nuances in text generation within thisfield, with a greater emphasis on comprehendingand conveying translated content accurately. Image and Video Captioning: Models are ex-pected to maintain consistency with the sourcewhile also incorporating real-world knowledgeto address gaps and apply common sense. Con-sequently, the definition of hallucination in thiscontext encompasses intrinsic, extrinsic, and non-factual elements, highlighting these requirements.",
  "Data Augmentation:: Works from this domainoften omit explicit definitions of hallucination, in-dicating a divergence in emphasis or a nascent ex-ploration of this construct within this sub-field": "Miscellaneous: Encompassing tasks such as lan-guage inference and factuality detection, this cat-egorys definitions of hallucination encompass as-pects like factuality, intrinsic and extrinsic halluci-nation, fidelity, and nonsensicality. Its evident thatwithin these subfields, hallucination addresses boththe stylistic aspects of model output and the fidelityand accuracy of generated content. From the analysis of different subfields, it is evi-dent that each perceives hallucination differently,emphasizing specific attributes such as factuality,fidelity, or linguistic styles like confidence, whilepotentially overlooking others. This diversity indi-cates that hallucination as a concept is still in itsearly stages in the field, with various frameworksemerging and a general lack of consensus regard-ing its definition and application. Furthermore, thelack of social aspects in hallucination discussionsin these subfields contrasts with the broader under-standing and research in fields like healthcare.",
  "Audit of Frameworks": "We now scrutinize the dominant frameworks em-ployed in defining hallucination while also assess-ing the extent to which these models accurately cap-ture the phenomenon. We start by looking at howmany of the selected works explicitly define hallu-cination. Out of the 103 papers, just 44 (42.7%)provide a definition of the term, leaving the ma-jority59 papers or 57.3%either altogetheromitting their understanding of hallucination in thecontext of their research or providing no definitionor a framework. This lack of transparency is notonly concerning but also underscores the need forclarity, especially given the varied interpretationsof hallucination across different research domains.Taking our scrutiny a step further, we investi-gate whether the works defining hallucination ref-erence and acknowledge preexisting frameworks.It emerges that only 29 papers or 27% of the se-lected works explicitly acknowledge and adhereto established frameworks of hallucination, whilethe remainder 73% either loosely define the termor devise new definitions tailored to their specificresearch scope. This trend within the field shows alack of consensus on the conceptualization of hal-lucination, leading to disparate interpretations anda shortage of discourse on the subject.We also audit the sociotechnical nature of thedefinitions of hallucination in NLP. Hallucination(elucidated in Appendix 10.1) inherently containssocial dimensions, creating varied perspectivesacross different social contexts. Moreover, giventhe evolution of LMs into social spaces, adopting asociotechnical approach becomes necessary, giventhat the term hallucination is inherently a sharedvocabulary within these domains. Unfortunately,out of the 103 works examined, only 3 acknowl-edge the this nature of hallucination, with noneutilizing this framework to inform their approach.",
  "Audit of Metrics": "In the analysis of the 103 papers, we observed that87 of these works dedicate efforts to measuringhallucination. This observation depicts the prevail-ing trend within NLP, emphasizing the significanceof quantifying the concept of hallucination acrossdiverse research efforts. Building upon prior studiessuch as Ji et al. (2023a), our analysis categorizesthe common approaches in NLP for quantifyinghallucination into four major themes: StatisticalMetrics, Data-driven Metrics, Human Evaluation,and Mixed Methodologies.",
  ": Hallucination evaluation metrics used in NLP": "Statistical metrics calculates a hallucinationscore based on the degree of mismatch, with higherdiscrepancies indicating lower accuracy, factual-ity or faithfulness and hence, higher hallucination(Ji et al., 2023a). Statistical scores such as BLUE,ROUGE, and Error Rate metrics are commonlyused in this approach. Our findings reveal that35.2% of the works that quantify hallucination optfor statistical metrics, employing 25 distinct met- rics (e.g., BERTScore, F1, Perplexity, Cosine Sim-ilarity) developed for this purpose. This variabilityunderscores the lack of a standardized approach.Data-driven metrics utilizes curated datasetsor neural models to gauge hallucination in gen-erated text. This methodology, accounting for cu-rated knowledge/content mismatches, is adopted by26.1% of the works, resulting in the developmentof 18 distinct datasets or models tailored for hal-lucination measurement, such as CHAIR (CaptionHallucination Assessment with Image Relevance)and SelfCheckGPT (Manakul et al., 2023).Human evaluation offers a complementary per-spective by employing human annotators to assesshallucination levels, compensating for apparent er-rors in automated indicators (Ji et al., 2023a). Thisapproach, used by 10.2% of the works, encom-passes scoring and comparison methods, whereannotators rate hallucination levels or compare out-put texts with baselines or ground-truth references.Notably, one outlier paper introduced an innovativeapproach utilizing eye tracking for hallucinationdetection in NLP tasks (Maharaj et al., 2023).Mixed method approach is deployed by 28.4%of the works, combining human evaluation withstatistical metrics to offer a holistic perspective onhallucination quantification. This trend reflects aconcerted effort within the research community toaddress the limitations of individual methodologiesand provide insights into the presence and natureof hallucination in generated texts.The metrics audit reveals significant knowledgegaps and challenges across various approaches.Notably, established research highlights areas forimprovement in standard methods for measuringhallucination. For instance, methodologies likeCHAIR and metrics such as ROUGE scores ex- hibit instability in measuring hallucination due tothe need for complex human-crafted parsing rulesfor exact matching, rendering them susceptible toerrors (Li et al., 2023). Criticisms also extend tohuman evaluation methods, which are prone to in-accuracies in gauging hallucination within thesemodels (Smith et al., 2022).Beyond methodological criticisms, our audit un-covers a trend of employing numerous distinct met-rics and approaches within these frameworks tocategorize hallucinations. Over time, this has led toa diverse set of parameters for measuring halluci-nation, with a general lack of consensus on a stan-dardized measurement approach. This issue furtherhighlights the absence of a unified method, espe-cially as these models have now shifted to becomea sociotechnical solution (Bender et al., 2021).",
  "Practitioner Survey of Hallucination": "In this section, adopting a community-centric ap-proach (Narayanan Venkit, 2023), we conduct asurvey to gain insights into researchers percep-tions of hallucinations in NLP to complement ourtheoretical discussions with practical real-worldperspectives. The primary goal is to demonstratehow researchers and practitioners within the fieldperceive the concept of hallucination and to ex-pand our findings beyond the limitations of existingliterature where real-world perceptions from the re-searchers are missing (Huang et al., 2023; Zhanget al., 2023c; Ji et al., 2023a). This motivates usto gather real-world perspectives from individualsactively engaged in NLP and AI research.",
  "Survey Recruitment and Data Collection": "For our survey, we employed a multi-faceted ap-proach to reach a diverse population of respon-dents. We utilized direct emails, direct messages,and social media platforms such as LinkedIn andTwitter to distribute the survey. Our target audienceincluded graduate students and professors from aca-demic backgrounds as well as individuals from theindustry who work in NLP, aiming to capture awide range of perspectives on hallucinations.To ensure a comprehensive view, we specificallytargeted researchers familiar with AI and ML, pri-marily from disciplines such as computer scienceand information science. However, we also wel-comed participants from other domains to exploretheir perceptions of whether they had the literatureunderstanding of the concept of hallucination as they are also extensively using LLM models. Thesurvey was examined and approved by the Institu-tional Review Board (IRB) for ethical practices.We additionally employed a systematic approachby randomly selecting 15 universities from the top100 in the USA as per the 2023 US News and WorldReport rankings (News, 2023), to then reach outto potential participants. Prior works (Chakravortiet al., 2023) have previously employed this processto identify high-quality participants. We receiveda total of 223 responses, out of which 171 werecomplete and usable for analysis.",
  "Survey Structure": "The survey employed a combination of 14 open-ended and close-ended questions. The survey hasbeen built based on the previous survey design tech-niques (Rosen et al., 2013; Baker, 2016; Van No-orden and Perkel, 2023; Chakravorti et al., 2024).Open-ended questions and free-response text boxesallow us to gather rich opinions from participants.This approach integrates all our findings, providinga broader and deeper understanding of the response.For the analysis of open-ended questions, we uti-lized thematic analysis, drawing from establishedmethodologies outlined in Blandford et al. (2016);Terry et al. (2017). The close-ended questions wereanalyzed using descriptive statistics to summarizeand analyze the numerical data obtained from re-spondents. Throughout the analysis process, theresearch team made collective decisions regardingthe retention, removal, or reorganization of themesderived from open-ended responses. All the surveyquestions have been provided in Appendix 10.3.",
  "Survey Findings": "We now summarize insights from our responsesto explore various perspectives on hallucinationsin LLMs, including perceptions, weaknesses, andpreferences. The breakdown of responses indicatesthat 76.54% of participants were from academia,20.98% from the industry, and 2.47% both.Participants were also asked about their researchareas direct relation to AI and NLP. The analysisrevealed that more than 68.52% of researchers in-dicated that their work is directly related to NLP,while the remaining respondents either exhibitedfamiliarity with or indirectly incorporated NLP andAI methodologies in their work. This highlights thesubstantial involvement of AI experts and practi-tioners within the survey.",
  "Familiarity with Hallucination": "The survey included the question on participantsfamiliarity with the concept of hallucinations inAI-generated text, measured on a 5-point Likertscale. The analysis revealed that 24.07% of re-searchers reported being extremely familiar withthe concept, while 33.33% indicated being very fa-miliar with it (). Participants who indicatednot being familiar with the term hallucination(7.96%) also demonstrated implicit concerns withthis phenomenon by highlighting issues such asgenerating incorrect responses and crafting storiesautonomously. This demonstrates the widespreadimpact of the phenomenon within the community.",
  "Hallucination Frequency": "The survey included a question regarding the fre-quency of encountering hallucinated content, de-fined as content that is factually incorrect or unre-lated to the input, assessed on a 5-point Likert scaleranging from Never to Very frequently (). The analysis revealed that 46.91% of respon-dents reported encountering hallucinated contentoccasionally, while 29.01% indicated experiencingit frequently. The results suggest that a substan-tial portion of practitioners encounter instances ofhallucinated content in AI-generated outputs, indi-cating a prevalent issue in generative NLP models.",
  ": Frequency of encountering Hallucination": "5.3.3Perceptions of HallucinationThe survey findings revealed that more than 92%of respondents perceive hallucination as a weak-ness of LLMs. Subsequently, participants wereasked to provide their own definitions of hallu-cination in generative AI models through an open-ended question. To analyze these responses, weapplied thematic categorization based on attributesgenerated from the literature audit ().The thematic categorization revealed that the ma-jority of respondents categorized hallucination aspertaining to the factuality and faithfulness of input,with relatively lesser emphasis on the extrinsic andintrinsic nature of hallucination concerning the in-put. This trend reflects a common perception ofhow hallucination is understood within the contextof larger-scope generative AI models.Moreover, the analysis identified 12 distinctframeworks regarding how hallucination is de-fined by respondents. For example:Response that appears syntactically and seman-tically believable, but is not based on actual factAcademic Researcher, NLPWhen the model confidently states somethingthat is not trueAcademic Researcher, AIThe diversity of viewpoints underscores the in-consistency within the field regarding the concep-tualization and understanding of hallucination inthe context of generative AI models. 5.3.4Alternative Terms for HallucinationThe survey included a question asking participantsif they prefer an alternate term to describe the phe-nomenon of hallucination in AI-generated contentand to provide an explanation if they do. The anal-ysis revealed that 54.32% of respondents preferredthe term hallucination or had no other term to pro-vide. However, among the remaining responses,40.46% of participants mentioned Fabricationas a better term to describe the phenomenon.This indicates that while the majority of respon-dents did not propose an alternative term, a notableproportion sees fabrications as a more suitable de-scriptor for the phenomenon of hallucination inAI-generated content. For example,Fabrication makes more sense. Hallucinationmakes it feel like AI is human and has the samesensory perceptions that could lead to hallucina-tions.Academic Researcher, AI & EducationIts interesting to note that a few researchers alsoprefer to use the term Confabulations insteadof hallucinations when referring to AI-generated content. Their rationale likely stems from the nu-anced difference in meaning between the two terms.While hallucinations generally convey the idea ofperceiving something that is not based on realityor fact, confabulations specifically refer to the cre-ation of false memories or information without theintention to deceive.By opting for the term Confabulations, re-searchers may be emphasizing the unintentionalnature of the inaccuracies or false information gen-erated by AI models, as opposed to implying delib-erate deceit. For example,I think confabulation works better because itmeans creating a false memory without deceit. Fab-rication gives the idea that it is intentional, whichin the case of generative AI models, it is not.Academic Researcher, AI & HCIIts also insightful to see that respondents pro-posed various alternative terms to describe thephenomenon of hallucination in AI-generated con-tent such as incorrect information/misinformation,Non-factual information, Cognitive gap, hyper-generalization, Overconfidence, and Randomness.These alternatives highlight different aspects andnuances of the inaccuracies or distortions present inthe generated content. Participants also mentionedhow they prefer multiple terms based on the appli-cation in which they are used.As I mentioned there are different types of hal-lucinations. For instruction and context hallucina-tions, I would refer to them as inconsistency in-stead. For factually incorrect hallucinations, theword hallucination is fine.Academia, NLP 5.3.5Creativity and Positive ApplicationsNot all researchers view hallucinations in AI-generated content through a negative lens. Whilethe majority may associate hallucinations with inac-curacies or distortions, a notable minority (12%in our survey) provided insights into how they be-lieve hallucinations in these models can be cor-related with creativity rather than negatively im-pacted behaviors. In fields such as story narrationand image generation, researchers often value thecreative behaviors exhibited by AI models. Halluci-nations, when viewed in this context, may be seenas manifestations of the models ability to thinkoutside the box, generate novel ideas, and exploreunconventional possibilities. These creative outputscan inspire new approaches to storytelling, art, andproblem-solving, contributing to innovation andartistic expression. For example: Hallucinations are just what is needed for mod-els to be creative. In truth, unless AI text-generatorsare factually grounded with external knowledgefor a specific field, they are just story generatorswhich aim to be creative, hencehallucinate.\"National Lab Researcher, NLPFurther supplementary analysis and quotes onthe various external perspectives and the societalramifications of hallucination, obtained through thesurvey, is examined in the Appendix 10.2.",
  "Challenges": "The primary challenges we identify thematicallyand aim to elucidate are as follows:Wide range of vague and absent definitions:The literature and the practitioners survey showdiverse and conflicting frameworks, often lackingclarity or omitting explicit definitions for halluci-nation and how it is perceived in various fields ofNLP and language generation. Ambiguity arisesfrom the use of terms like confabulations, fab-rications, misinformation, and hallucinationsinterchangeably, without clear definitions in thecontext of hallucinations.Lack of standardization in measurement: Theabsence of standardized metrics to quantify hallu-cination results in the use of multiple scales andcategorizations. This makes it challenging to com-pare and interpret results across different modelsand studies, leading to a proliferation of diverseapproaches for evaluating hallucinations.Limited awareness of hallucination in a so-ciotechnical context: Hallucination research of-ten lacks the understanding of how the concept ofhallucination is conceptualized beyond its techni-cal purview. When such analysis is employed insociotechnical systems like healthcare and policymaking (Dahl et al., 2024; Pal et al., 2023), it isimportant to define the socially relevant frameworkof hallucination as well.Multiple sentiment towards hallucination:The perception of hallucination in generative AIvaries depending on the context. For instance, it is often positively regarded as creativity in imagegeneration, whereas in text generation, it is viewednegatively as errors or mistakes. Consequently, fu-ture research efforts should aim to better addressthis disparity to develop a more nuanced frameworkfor understanding hallucination.Lack of standardized nomenclature: Both ourliterature audit and practitioner survey revealed thatthe term hallucination is inadequate to fully cap-ture the behavior exhibited by NLG models. Thereis a need for further investigation into which termsare more appropriate and why they are necessary.For instance, terms like confabulation, fabrica-tions, and misinformations are increasingly be-ing used to describe the same phenomenon. A moreprecise understanding is required to distinguish be-tween these terms and how they are utilized invarious fields within NLP.User trust and reliability: Our survey findingssuggest that users may hesitate to fully utilize LLMcapabilities due to concerns about bias and halluci-nation despite recognizing the potential advantagesthese models offer. Therefore, there is a need tofocus efforts on understanding the human interac-tion aspect concerning hallucination in NLP andlanguage generation.Addressing these issues requires careful consid-eration of the categorization approach, integrationof contextual information, and, efforts towards ro-bust evaluation methodologies in hallucinations.",
  "Recommendations": "Expanding on audits like Blodgett et al. (2020)& Venkit et al. (2023), we examine strategies forNLP practitioners studying hallucination to over-come these challenges. We propose two overarch-ing themes with four associated recommendations. Author-Centric Recommendation.These rec-ommendations prioritize actionable steps for boththe author and developers, emphasizing transparentand accountable development in conceptualizinghallucinations.[R1] Ensure explicit documentation of the hal-lucination framework and analysis methodologyemployed during the development of NLP mod-els. Provide guidelines that outline the expectedmeasurements and quantifications for the model toenhance interpretability and applicability.[R2] Explicitly state the use cases and user pro-files intended to interact with the NLP system. Byconsidering the specific applications and targeted users, it is easier to construct the required frame-work of hallucination that is sensitive to the com-munity in consideration. Raise awareness aboutpotential ramifications introduced by NLP models,emphasizing the importance of fairness and equity.",
  "Community-Centric Recommendation.Theserecommendations prioritize actionable steps for theresearch community to enhance frameworks andunderstanding related to hallucinations": "[R3] Develop clear and standardized definitionsfor terms such as confabulations, fabrications,misinformation, and hallucinations within thecontext of NLP. Establish frameworks that provideclarity and consistency in understanding these con-cepts, particularly regarding hallucinations. Thisrequirement is crucial due to the widespread mis-understanding of hallucination and the misnomersthat have arisen as research progresses. [R4] Promote the creation of methods that offervisibility into the models decision-making process,enabling users to comprehend how hallucinationsor fabrications can occur within the system, thusfostering trust in its use. Facilitating research dis-cussions for transparency through workshops andconferences is one approach to achieving this goal.",
  "Conclusion": "Our work delves into the conceptualization of hal-lucination within the scope of NLP. Our approachinvolved two key methodologies: first, an exhaus-tive audit of 103 peer-reviewed papers in the NLPdomain, and second, a practitioner survey of 171researchers to complement our first study with real-world practical perception and understanding ofhallucination as a unique contribution. Through thisanalysis, we have gained insights into how the NLPcommunity conceptualizes and defines hallucina-tion, showcasing a lack of discourse and agreement.Additionally, our thematic and community-basedapproach highlights potential weaknesses withinthe field, particularly in addressing misrepresenta-tions and inaccurate characterizations associatedwith hallucination, paving way for better advance-ment in language generation. Our work finally con-tributes to a deeper understanding of the challengesand gaps in research related to hallucination withinNLP, paving the way for future advancements inNLP and language generation.",
  "Limitations": "Our study encompasses a selection of 103 papers,incorporating works from primarily the ACL An-thology. While our intention was not to providean exhaustive collection of all published works onhallucination, we aimed to include diverse sourceswithin NLP that cover various aspects of the field.Our intent was to curate peer-reviewed literaturecommonly found in the NLP domain, encompass-ing models, applications, survey papers, and frame-works. We, therefore, did not scope the utility ofhallucination and its impact beyond NLP to otherfields of research, such as Computer Vision. Re-garding the creation of the challenges and recom-mendations, it is important to note that the themespresented are not meant to be exhaustive but ratherserve as a foundational framework to spark addi-tional inquiries and foster further engagement.Our survey was designed to capture the view-points of researchers and practitioners in the AIand ML field, potentially limiting various experi-ences. As such, our analysis is centered on this per-spective. While we did gather additional insightsfrom participants outside this field, our focus wasnot comprehensive in that regard. Our future workintends to explore the publics perspective on hal-lucination.",
  "Ethics Statement": "We are aware of the ethical considerations involvedin our research and have taken measures to ensureresponsible practices throughout the study.Data Publication: All the papers used in our re-search are listed in the Appendix. However, we rec-ognize the importance of transparency and account-ability. Therefore, we publish the complete collec-tion of papers along with our qualitative classifica-tion and annotation, allowing for public scrutinyand examination 3.Mitigating Qualitative Bias: We acknowledgethe potential for bias when performing qualitativecoding of the papers regarding their applications.To address this concern, we ensured that at leastthree different individuals independently reviewedand verified the coding to minimize the possibilityof misclassification. Additionally, we followed thesame approach to verify the presence of various def-initions in each paper, enhancing the reliability andvalidity of our analysis. By disclosing these ethical",
  "Simon Baker and Takeo Kanade. 2000. Hallucinatingfaces. In Proceedings of 4th IEEE International Con-ference on Automatic Face and Gesture Recognition(FG 00), pages 83 88": "Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On thedangers of stochastic parrots: Can language modelsbe too big? In Proceedings of the 2021 ACM confer-ence on fairness, accountability, and transparency,pages 610623. Melania Berbatova and Yoan Salambashev. 2023. Evalu-ating hallucinations in large language models for Bul-garian language. In Proceedings of the 8th StudentResearch Workshop associated with the InternationalConference Recent Advances in Natural LanguageProcessing, pages 5563, Varna, Bulgaria. INCOMALtd., Shoumen, Bulgaria.",
  "hallucinations in abstractive summarization. arXivpreprint arXiv:2109.09784": "Tatiana Chakravorti, Robert Fraleigh, Timothy Fritton,Michael McLaughlin, Vaibhav Singh, ChristopherGriffin, Anthony Mark Kwasnica, David Pennock,C Lee Giles, and Sarah Rajtmajer. 2023. A prototypehybrid prediction market for estimating replicabilityof published work. In 2nd International Conferenceon Hybrid Human-Artificial Intelligence, HHAI 2023,pages 300309. IOS Press BV. Tatiana Chakravorti, Sai Dileep Koneru, and Sarah Rajt-majer. 2024. Reproducibility, replicability, and trans-parency in research: What 430 professors think inuniversities across the usa and india. arXiv preprintarXiv:2402.08796. Qinyu Chen, Wenhao Wu, and Sujian Li. 2023a. Ex-ploring in-context learning for knowledge groundeddialog generation. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1007110081. Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.2021.Improving faithfulness in abstractive sum-marization with contrast candidate generation andselection. In Proceedings of the 2021 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 59355941, Online. Association forComputational Linguistics. Wei-Lin Chen, Cheng-Kuang Wu, Hsin-Hsi Chen,and Chung-Chi Chen. 2023b.Fidelity-enrichedcontrastive search: Reconciling the faithfulness-diversity trade-off in text generation. arXiv preprintarXiv:2310.14981. Prafulla Kumar Choubey, Alex Fabbri, Jesse Vig, Chien-Sheng Wu, Wenhao Liu, and Nazneen Rajani. 2023.CaPE: Contrastive parameter ensembling for reduc-ing hallucination in abstractive summarization. InFindings of the Association for Computational Lin-guistics: ACL 2023, pages 1075510773, Toronto,Canada. Association for Computational Linguistics. Volkan Cirik, Louis-Philippe Morency, and Taylor Berg-Kirkpatrick. 2022. Holm: Hallucinating objects withlanguage models for referring expression recognitionin partially-observed scenes. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages54405453.",
  "Matthew Dahl, Varun Magesh, Mirac Suzgun, andDaniel E. Ho. 2024. Hallucinating law: Legal mis-takes with large language models are pervasive": "Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and PascaleFung. 2023. Plausible may not be faithful: Probingobject hallucination in vision-language pre-training.In Proceedings of the 17th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 21362148, Dubrovnik, Croatia.Association for Computational Linguistics. David Dale, Elena Voita, Loc Barrault, and Marta RCosta-juss. 2022. Detecting and mitigating halluci-nations in machine translation: Model internal work-ings alone do well, sentence similarity even better.arXiv preprint arXiv:2212.08597. David Dale, Elena Voita, Janice Lam, PrangthipHansanti, Christophe Ropers, Elahe Kalbassi, Cyn-thia Gao, Loic Barrault, and Marta Costa-juss. 2023.HalOmi: A manually annotated benchmark for multi-lingual hallucination and omission detection in ma-chine translation. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 638653, Singapore. Associationfor Computational Linguistics.",
  "Souvik Das, Sougata Saha, and Rohini Srihari. 2022": "Diving deep into modes of fact hallucinations in di-alogue systems. In Findings of the Association forComputational Linguistics: EMNLP 2022, pages 684699, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Yue Dong, John Wieting, and Pat Verga. 2022. Faith-ful to the document or to the world? mitigating hal-lucinations via entity-linked knowledge in abstrac-tive summarization. In Findings of the Associationfor Computational Linguistics: EMNLP 2022, pages10671082, Abu Dhabi, United Arab Emirates. Asso-ciation for Computational Linguistics. Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Za-iane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022.Faithdial: A faithful benchmark for information-seeking dialogue. Transactions of the Association forComputational Linguistics, 10:14731490. Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, and Tat-Seng Chua. 2023. Scene graph as pivoting: Inference-time image-free unsupervised multimodal machinetranslation with visual scene hallucination. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 59805994, Toronto, Canada. Associ-ation for Computational Linguistics. Javier Ferrando, Gerard I. Gllego, Belen Alastruey,Carlos Escolano, and Marta R. Costa-juss. 2022.Towards opening the black box of neural machinetranslation: Source and target interpretations of thetransformer. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 87568769, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.",
  "Katja Filippova. 2020a.Controlled hallucinations:Learning to generate faithfully from noisy data.arXiv preprint arXiv:2010.05873": "Katja Filippova. 2020b.Controlled hallucinations:Learning to generate faithfully from noisy data. InFindings of the Association for Computational Lin-guistics: EMNLP 2020, pages 864870, Online. As-sociation for Computational Linguistics. Andreas Fink, Mathias Benedek, Human-F Unterrainer,Ilona Papousek, and Elisabeth M Weiss. 2014. Cre-ativity and psychopathology: are there similar mentalprocesses involved in creativity and in psychosis-proneness? Frontiers in psychology, 5:117336. Lorenzo Jaime Flores and Arman Cohan. 2024. On thebenefits of fine-grained loss truncation: A case studyon factuality in summarization. In Proceedings ofthe 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 138150, St. Julians, Malta.Association for Computational Linguistics. Korbinian Friedl, Georgios Rizos, Lukas Stappen, Mad-ina Hasan, Lucia Specia, Thomas Hain, and BjrnSchuller. 2021. Uncertainty aware review hallucina-tion for science article classification. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 50045009, Online. Associationfor Computational Linguistics.",
  "Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang.2022. Findings of the association for computationallinguistics: Emnlp 2022. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2022": "Javier Gonzlez Corbelle, Alberto Bugarn-Diz, JoseAlonso-Moral, and Juan Taboada. 2022. Dealingwith hallucination and omission in neural natural lan-guage generation: A use case on meteorology. InProceedings of the 15th International Conferenceon Natural Language Generation, pages 121130,Waterville, Maine, USA and virtual meeting. Associ-ation for Computational Linguistics. Javier Gonzlez-Corbelle, Alberto Bugarn Diz, JoseAlonso-Moral, and Juan Taboada. 2022. Dealingwith hallucination and omission in neural naturallanguage generation: A use case on meteorology. InProceedings of the 15th International Conference onNatural Language Generation, pages 121130. Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida,and Andr Martins. 2023a. Optimal transport for un-supervised hallucination detection in neural machinetranslation. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1376613784,Toronto, Canada. Association for Computational Lin-guistics. Nuno M. Guerreiro, Elena Voita, and Andr Martins.2023b. Looking for a needle in a haystack: A com-prehensive study of hallucinations in neural machinetranslation. In Proceedings of the 17th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 10591075, Dubrovnik,Croatia. Association for Computational Linguistics.",
  "Vipul Gupta, Pranav Narayanan Venkit, Shomir Wil-son, and Rebecca J. Passonneau. 2024.Sociode-mographic bias in language models: A survey andforward path": "Chih-Chung Hsu, Chia-Wen Lin, Chiou-Ting Hsu,Hong-Yuan Mark Liao, and Jen-Yu Yu. 2010. Facehallucination using bayesian global estimation andlocal basis selection. In 2010 IEEE InternationalWorkshop on Multimedia Signal Processing, pages449453. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.A survey on hallucination in large language models:Principles, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232. Ann Irvine and Chris Callison-Burch. 2014. Halluci-nating phrase translations for low resource mt. InProceedings of the Eighteenth Conference on Compu-tational Natural Language Learning, pages 160170. Saad Obaid ul Islam, Iza krjanec, Ondrej Dusek, andVera Demberg. 2023. Tackling hallucinations in neu-ral chart summarization. In Proceedings of the 16thInternational Natural Language Generation Confer-ence, pages 414423, Prague, Czechia. Associationfor Computational Linguistics. Saad Obaid Ul Islam, Iza krjanec, Ondrej Duek, andVera Demberg. 2023. Tackling hallucinations in neu-ral chart summarization. In Proceedings of the 16thInternational Natural Language Generation Confer-ence, pages 414423. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023a. Survey of hallu-cination in natural language generation. ACM Com-puting Surveys, 55(12):138. Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, BryanWilie, Min Zeng, and Pascale Fung. 2023b. RHO:Reducing hallucination in open-domain dialogueswith knowledge grounding. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 45044522, Toronto, Canada. Association forComputational Linguistics. Ziwei Ji, YU Tiezheng, Yan Xu, Nayeon Lee, EtsukoIshii, and Pascale Fung. 2023c. Towards mitigatingllm hallucination via self reflection. In The 2023 Con-ference on Empirical Methods in Natural LanguageProcessing. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, EtsukoIshii, and Pascale Fung. 2023d. Towards mitigat-ing LLM hallucination via self reflection. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 18271843, Singapore. Associ-ation for Computational Linguistics. Yiren Jian, Chongyang Gao, and Soroush Vosoughi.2022. Embedding hallucination for few-shot lan-guage fine-tuning. In Proceedings of the 2022 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: HumanLanguage Technologies, pages 55225530, Seattle,United States. Association for Computational Lin-guistics.",
  "Faisal Ladhak, Esin Durmus, and Tatsunori Hashimoto.2022. Contrastive error attribution for finetuned lan-guage models. arXiv preprint arXiv:2212.10722": "Faisal Ladhak, Esin Durmus, Mirac Suzgun, TianyiZhang, Dan Jurafsky, Kathleen McKeown, and Tat-sunori B Hashimoto. 2023. When do pre-training bi-ases propagate to downstream tasks? a case study intext summarization. In Proceedings of the 17th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 32063219. Mateusz Lango and Ondrej Dusek. 2023. Critic-drivendecoding for mitigating hallucinations in data-to-textgeneration. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 28532862, Singapore. Association forComputational Linguistics.",
  "Lisa Legault. 2020. Encyclopedia of personality andindividual differences. Encyclopedia of Personalityand Individual Differences, pages 15": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao,and Ji-Rong Wen. 2023. Evaluating object hallucina-tion in large vision-language models. In Proceedingsof the 2023 Conference on Empirical Methods inNatural Language Processing, pages 292305, Sin-gapore. Association for Computational Linguistics. Hui Liu and Xiaojun Wan. 2023. Models see hallucina-tions: Evaluating the factuality in video captioning.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1180711823, Singapore. Association for Computa-tional Linguistics.",
  "Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.A token-level reference-free hallucination detection": "benchmark for free-form text generation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 67236737, Dublin, Ireland. Associationfor Computational Linguistics. Shayne Longpre, Kartik Perisetla, Anthony Chen,Nikhil Ramesh, Chris DuBois, and Sameer Singh.2021. Entity-based knowledge conflicts in questionanswering. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 70527063, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Kishan Maharaj, Ashita Saxena, Raja Kumar, AbhijitMishra, and Pushpak Bhattacharyya. 2023. Eyesshow the way: Modelling gaze behaviour for hallu-cination detection. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1142411438, Singapore. Association for Computa-tional Linguistics. Himanshu Maheshwari, Sumit Shekhar, Apoorv Sax-ena, and Niyati Chhaya. 2023. Open-world factuallyconsistent question generation. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 23902404, Toronto, Canada. Association forComputational Linguistics.",
  "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023": "SelfCheckGPT: Zero-resource black-box hallucina-tion detection for generative large language models.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages90049017, Singapore. Association for Computa-tional Linguistics. Andreas Marfurt and James Henderson. 2022.Un-supervised token-level hallucination detection fromsummary generation by-products. In Proceedings ofthe 2nd Workshop on Natural Language Generation,Evaluation, and Metrics (GEM), pages 248261, AbuDhabi, United Arab Emirates (Hybrid). Associationfor Computational Linguistics. NL Mason, KPC Kuypers, JT Reckweg, F Mller, DHYTse, B Da Rios, SW Toennes, P Stiers, A Feilding,and JG Ramaekers. 2021. Spontaneous and delib-erate creative cognition during and after psilocybinexposure. Translational psychiatry, 11(1):209. Luca Massarelli, Fabio Petroni, Aleksandra Piktus,Myle Ott, Tim Rocktschel, Vassilis Plachouras, Fab-rizio Silvestri, and Sebastian Riedel. 2020.Howdecoding strategies affect the verifiability of gener-ated text. In Findings of the Association for Compu-tational Linguistics: EMNLP 2020, pages 223235,Online. Association for Computational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 19061919, On-line. Association for Computational Linguistics. Alessia McGowan, Yunlai Gui, Matthew Dobbs, SophiaShuster, Matthew Cotter, Alexandria Selloni, Mar-ianne Goodman, Agrima Srivastava, Guillermo ACecchi, and Cheryl M Corcoran. 2023. Chatgpt andbard exhibit spontaneous citation fabrication duringpsychiatry literature search. Psychiatry Research,326:115334. Nick McKenna, Tianyi Li, Liang Cheng, MohammadHosseini, Mark Johnson, and Mark Steedman. 2023.Sources of hallucination by large language modelson inference tasks. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages27582774, Singapore. Association for Computa-tional Linguistics. Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agentsoverconfidence through linguistic calibration. Trans-actions of the Association for Computational Linguis-tics, 10:857872.",
  "Mathias Mller, Annette Rios, and Rico Sennrich. 2020": "Domain robustness in neural machine translation. InProceedings of the 14th Conference of the Associa-tion for Machine Translation in the Americas (Volume1: Research Track), pages 151164, Virtual. Associa-tion for Machine Translation in the Americas. Feng Nan, Ramesh Nallapati, Zhiguo Wang, CiceroNogueira dos Santos, Henghui Zhu, Dejiao Zhang,Kathleen McKeown, and Bing Xiang. 2021. Entity-level factual consistency of abstractive text summa-rization. In Proceedings of the 16th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Main Volume, pages 27272733,Online. Association for Computational Linguistics. Pranav Narayanan Venkit. 2023. Towards a holisticapproach: Understanding sociodemographic biasesin nlp models using an interdisciplinary lens.InProceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society, pages 10041005. Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-chanadikar, Ting-Hao Huang, and Shomir Wilson.2023. Unmasking nationality bias: A study of humanperception of nationalities in ai-generated articles. InProceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society, pages 554565.",
  "Proceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 26732679, Florence, Italy. Association for ComputationalLinguistics": "Ankit Pal, Logesh Kumar Umapathi, and MalaikannanSankarasubbu. 2023. Med-HALT: Medical domainhallucination test for large language models. In Pro-ceedings of the 27th Conference on ComputationalNatural Language Learning (CoNLL), pages 314334, Singapore. Association for Computational Lin-guistics. Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia,Xinyi Wang, Machel Reid, and Sebastian Ruder.2023.mmT5: Modular multilingual pre-trainingsolves source language hallucinations. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 19782008, Singapore. Associ-ation for Computational Linguistics. Liam van der Poel, Ryan Cotterell, and Clara Meis-ter. 2022. Mutual information alleviates hallucina-tions in abstractive summarization. In Proceedingsof the 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 59565965, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Fina Polat, Ilaria Tiddi, Paul Groth, and Piek Vossen.2023. Improving graph-to-text generation using cy-cle training. In Proceedings of the 4th Conference onLanguage, Data and Knowledge, pages 256261. Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Ponti,and Shay Cohen. 2023. Detecting and mitigatinghallucinations in multilingual summarisation. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 89148932, Singapore. Association for Computational Lin-guistics. Anil Ramakrishna, Rahul Gupta, Jens Lehmann, andMorteza Ziyadi. 2023. Invite: a testbed of automat-ically generated invalid questions to evaluate largelanguage models for hallucinations. In The 2023Conference on Empirical Methods in Natural Lan-guage Processing. Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021. The curious case of hallucinationsin neural machine translation. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 11721183,Online. Association for Computational Linguistics. Vipula Rawte, Swagata Chakraborty, Agnibh Pathak,Anubhav Sarkar, SM Tonmoy, Aman Chadha, Amit PSheth, and Amitava Das. 2023a. The troubling emer-gence of hallucination in large language modelsanextensive definition, quantification, and prescriptiveremediations. arXiv preprint arXiv:2310.04988.",
  "Vipula Rawte, Amit Sheth, and Amitava Das. 2023b. Asurvey of hallucination in large foundation models.arXiv preprint arXiv:2309.05922": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2018. Object hallu-cination in image captioning. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 40354045, Brussels,Belgium. Association for Computational Linguistics. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,Eric Michael Smith, Y-Lan Boureau, and Jason We-ston. 2021. Recipes for building an open-domainchatbot. In Proceedings of the 16th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Main Volume, pages 300325,Online. Association for Computational Linguistics. Larry D Rosen, Kelly Whaling, L Mark Carrier,Nancy A Cheever, and Jeffrey Rokkum. 2013. Themedia and technology usage and attitudes scale: Anempirical investigation. Computers in human behav-ior, 29(6):25012511. Mobashir Sadat, Zhengyu Zhou, Lukas Lange, JunAraki, Arsalan Gundroo, Bingqing Wang, RakeshMenon, Md Parvez, and Zhe Feng. 2023.Delu-cionQA: Detecting hallucinations in domain-specificquestion answering. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages822835, Singapore. Association for ComputationalLinguistics. Farhan Samir and Miikka Silfverberg. 2022. One wug,two wug+s transformer inflection models hallucinateaffixes. In Proceedings of the Fifth Workshop on theUse of Computational Methods in the Study of En-dangered Languages, pages 3140, Dublin, Ireland.Association for Computational Linguistics.",
  "Jianbin Shen, Junyu Xuan, and Christy Liang. 2023": "Mitigating intrinsic named entity-related hallucina-tions of abstractive text summarization. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 1580715824, Singapore. Asso-ciation for Computational Linguistics. Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, and ChengkaiLi. 2023. Hallucination mitigation in natural lan-guage generation from large-scale open-domainknowledge graphs. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1250612521, Singapore. Associ-ation for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,and Jason Weston. 2021. Retrieval augmentationreduces hallucination in conversation. In Findingsof the Association for Computational Linguistics:EMNLP 2021, pages 37843803, Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Aviv Slobodkin, Omer Goldman, Avi Caciularu, IdoDagan, and Shauli Ravfogel. 2023. The curious caseof hallucinatory (un) answerability: Finding truthsin the hidden states of over-confident large languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 36073625. Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller,Y-Lan Boureau, and Jason Weston. 2022. Humanevaluation of conversations is an open problem: com-paring the sensitivity of various methods for eval-uating dialogue agents. In Proceedings of the 4thWorkshop on NLP for Conversational AI, pages 7797, Dublin, Ireland. Association for ComputationalLinguistics. Seonil (Simon) Son, Junsoo Park, Jeong-in Hwang,Junghwa Lee, Hyungjong Noh, and Yeonsoo Lee.2022. HaRiM+: Evaluating summary quality withhallucination risk. In Proceedings of the 2nd Confer-ence of the Asia-Pacific Chapter of the Associationfor Computational Linguistics and the 12th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 895924,Online only. Association for Computational Linguis-tics.",
  "Brent J Steele. 2017. Hallucination and intervention.Global Discourse, 7(2-3):201218": "Bin Sun, Yitong Li, Fei Mi, Fanhu Bie, Yiwei Li, andKan Li. 2023.Towards fewer hallucinations inknowledge-grounded dialogue generation via aug-mentative and contrastive knowledge-dialogue. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 17411750, Toronto, Canada.Association for Computational Linguistics. Anirudh S. Sundar and Larry Heck. 2023. cTBLS: Aug-menting large language models with conversationaltables. In Proceedings of the 5th Workshop on NLPfor Conversational AI (NLP4ConvAI 2023), pages 5970, Toronto, Canada. Association for ComputationalLinguistics.",
  "Gareth Terry, Nikki Hayfield, Victoria Clarke, and Vir-ginia Braun. 2017. Thematic analysis. The SAGEhandbook of qualitative research in psychology, 2:1737": "Alberto Testoni and Raffaella Bernardi. 2021. Iveseen things you people wouldnt believe: Halluci-nating entities in GuessWhat?! In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing: Stu-dent Research Workshop, pages 101111, Online. As-sociation for Computational Linguistics. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-ula Rawte, Aman Chadha, and Amitava Das. 2024.A comprehensive survey of hallucination mitigationtechniques in large language models. arXiv preprintarXiv:2401.01313. Mojtaba Vaismoradi, Hannele Turunen, and Terese Bon-das. 2013. Content analysis and thematic analysis:Implications for conducting a qualitative descriptivestudy. Nursing & health sciences, 15(3):398405.",
  "Richard Van Noorden and Jeffrey M Perkel. 2023. Aiand science: what 1,600 researchers think. Nature,621(7980):672675": "Pranav Venkit, Mukund Srinath, Sanjana Gautam,Saranya Venkatraman, Vipul Gupta, Rebecca J Pas-sonneau, and Shomir Wilson. 2023. The sentimentproblem: A critical survey towards deconstructingsentiment analysis. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1374313763. Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-hit Iyyer, and Noah Constant. 2022. Overcomingcatastrophic forgetting in zero-shot cross-lingual gen-eration. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 92799300, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Jonas Waldendorf, Barry Haddow, and Alexandra Birch.2024. Contrastive decoding reduces hallucinationsin large multilingual machine translation models. InProceedings of the 18th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 25262539,St. Julians, Malta. Association for ComputationalLinguistics. Chaojun Wang and Rico Sennrich. 2020. On exposurebias, hallucination and domain shift in neural ma-chine translation. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 35443552, Online. Association forComputational Linguistics. Orion Weller, Marc Marone, Nathaniel Weir, DawnLawrie, Daniel Khashabi, and Benjamin Van Durme.2024. according to . . . : Prompting language mod-els improves quoting from pre-training data. In Pro-ceedings of the 18th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 22882301,St. Julians, Malta. Association for ComputationalLinguistics.",
  "Stefan Werning. 2024. Generative ai and the techno-logical imaginary of game design. In Creative Toolsand the Softwarization of Cultural Production, pages6790. Springer": "Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-mad Abdul-Mageed, and Alham Aji. 2024. LaMini-LM: A diverse herd of distilled models from large-scale instructions. In Proceedings of the 18th Confer-ence of the European Chapter of the Association forComputational Linguistics (Volume 1: Long Papers),pages 944964, St. Julians, Malta. Association forComputational Linguistics. Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, KangLiu, Qing-Long Han, and Yang Tang. 2023. A briefoverview of chatgpt: The history, status quo and po-tential future development. IEEE/CAA Journal ofAutomatica Sinica, 10(5):11221136.",
  "Yijun Xiao and William Yang Wang. 2021a. On halluci-nation and predictive uncertainty in conditional lan-guage generation. arXiv preprint arXiv:2103.15025": "Yijun Xiao and William Yang Wang. 2021b. On hal-lucination and predictive uncertainty in conditionallanguage generation. In Proceedings of the 16th Con-ference of the European Chapter of the Associationfor Computational Linguistics: Main Volume, pages27342744, Online. Association for ComputationalLinguistics. Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pert-seva, Meng-Hsi Wu, Sina Semnani, and Monica Lam.2023. Fine-tuned LLMs know more, hallucinate lesswith few-shot sequence-to-sequence semantic pars-ing over Wikidata. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 57785791, Singapore. Associa-tion for Computational Linguistics.",
  "Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023": "A new benchmark and reverse validation method forpassage-level hallucination detection. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 38983908, Singapore. Associ-ation for Computational Linguistics. Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, JunyeongKim, and Chang Yoo. 2022. Information-theoretictext hallucination reduction for video-grounded di-alogue. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 41824193, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Jiaxin Zhang, Zhuohang Li, Kamalika Das, BradleyMalin, and Sricharan Kumar. 2023a. SAC3: Reliablehallucination detection in black-box language modelsvia semantic-aware cross-check consistency. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 1544515458, Singapore.Association for Computational Linguistics. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,Yue Zhang, Zheng Zhang, Chenghu Zhou, XinbingWang, and Luoyi Fu. 2023b. Enhancing uncertainty-based hallucination detection with stronger focus.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages915932, Singapore. Association for ComputationalLinguistics. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, et al. 2023c. Sirens song in the aiocean: a survey on hallucination in large languagemodels. arXiv preprint arXiv:2309.01219. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,Maosong Sun, and Qun Liu. 2019.ERNIE: En-hanced language representation with informative en-tities. In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics, pages14411451, Florence, Italy. Association for Compu-tational Linguistics.",
  "Social Perspectives on Hallucinations": "The exploration of hallucination in NLP is solelytechnocentric; however, its conceptual roots andapplications are deeply intertwined with societal in-terpretations. To gain a better understanding of theterm hallucination, it is important to consider itsbroader usage and implications beyond NLP. Hal-lucination has been studied across disciplines likepsychology and neurology (Steele, 2017; Legault,2020). Essentially, hallucinations involve percep-tions arising in the absence of any external reality seeing or hearing things that are not there (Steele,2017). Although a version of this definition is com-monly used in NLP, often with negative connota-tions, hallucinations have a wide scope, originatingfrom fields such as neurology, and philosophy.Hallucination and Medicine: Hallucination isbelieved to have neurological origins, often emerg-ing from induced states such as drug usage, psy-chosis, sensory deprivation, or migraines (Legault,2020). These experiences can encompass vari-ous sensory modalities like auditory, visual, ol-factory, tactile, gustatory, or somatic sensations(Boeving, 2020). Modern neurological researchlike Legault (2020) suggests that while hallucina-tions may not align with external reality, they arelinked to brain regions responsible for processingperceptions from the external world.Hallucination and Creativity: Studies explor-ing hallucination in the context of creativity sug-gest that individuals with mild hallucinatory ex-periences may demonstrate enhanced generativecreativity (Fink et al., 2014; Mason et al., 2021). Another prevalent notion is the use of hallucina-tions as a gateway to accessing intuition, creativity,and novel modes of thinking (Mason et al., 2021).However, there is a call for greater empirical rigorto establish robust connections between specificmental states leading to hallucinations and the cre-ative thinking process (Fink et al., 2014).The analysis of differing perspectives on halluci-nation reveals its diverse interpretations, challeng-ing prevalent assumptions within NLP. However,using the term hallucination without its social con-text can foster misconceptions. Firstly, the hallu-cinations in AI systems result from discrepanciesin input data and prompts rather than an absenceof external senses. Secondly, this metaphor risksperpetuating stigma by linking negative AI phe-nomena with specific mental illness aspects (Palet al., 2023), potentially hindering destigmatiza-tion efforts in mental health domains (Maleki et al.,2024). Lastly, given the widespread use of machinelearning models, especially in medical fields (Jiet al., 2023c), a limited grasp of hallucination con-text may lead to terminology misinterpretations.",
  "Weaknesses of LLM": "Before delving into inquiries about hallucinationsin LLMs, it is crucial to gain insights into the per-ceived weaknesses of these models from the par-ticipants perspective, as well as understand howfrequently they utilize these models in their work.The survey results indicate that a significant por-tion of researchers heavily utilize LLMs in theirdaily life. Specifically, 67.28% of respondents re-ported using these models atleast once a day, while20.37% mentioned using them all the time, high-lighting the ubiquity of these models.Upon analyzing the themes derived from partici-pants responses on the weaknesses of generativeAI tools, it was observed that a substantial majority(55%) of researchers perceive the main weaknessesto be the generation of misinformation and halluci-nations, despite both phenomena being essentiallysimilar in nature. For instance,I have been exploring these models to seewhat they get right and wrong. They get a lot ofthings wrong what some people call hallucina-tions.Emeritus Professor, NLPSome of the other important weaknesses men-tioned by the respondents are: biases, not followingthe prompts correctly, complex language, and not having a long memory. For example,They produce a lot of inaccurate replies withgreat confidence. These models also tend to be verybiased toward many socio-demographic groups.Graduate student, GenerativeAIIt is hard to distinguish whether the informationprovided by them is accurate or not. Sometimes,the models generate text with reasoning makingit sound convincing enough to be true - but endsup being incorrect ultimately.Industry, Genera-tiveAIThe responses highlight a critical concern withinthe research community regarding the reliabilityand accuracy of outputs generated by LLMs, withpotential implications for various applications anddomains, providing us with a strong motivationbehind this study.The widespread use of LLMs, particularly promi-nent models such as GPT 3, 3.5, and 4, highlightstheir importance and impact on research and in-dustry practices. However, its noteworthy that re-spondents also mentioned other LLM models thatthey use or are familiar with. These include Mistral,BERT, LLaMA2, Midjourney, ClaudeAI, Gemini,Vicuna, t5, Falcon, PaLM, Imagen, Dolly, Perplex-ity, among others.",
  "Social Ramifications of Hallucination": "Participants were prompted to explain the effects ofhallucination on their work/daily life. The resultingthemes, from our qualitative analysis of their inputs,are outlined below:Not Good for Education: Respondents raisedconcerns about the extensive use of these mod-els by students for homework, indicating potentialnegative impacts on their performance and learningabilities. The respondents believe that such relianceon these models can lead to a degradation in stu-dents learning. Additionally, respondents expressskepticism about the suitability of these models forchecking homework assignments.I dont actually use AI for my work; I just wantto be aware of what it can do because my studentsare probably using it for their homework. It couldhave an impact on students mastery of the mate-rial.Associate Prof, BiotechnologyNot Good for Scholarly Work: Several respon-dents noted that these models are not effective forscholarly purposes, citing instances where the mod-els generated information that was not present inthe original paper. They express concerns that ifresearchers rely on these models for tasks like liter- ature summarizing, it could lead to a deteriorationin scholarly processes. For example:They tend to generate a lot of misinformed factsabout certain groups or cultures that I have seenhappen often. They also generate facts from schol-arly works where the papers would not have men-tioned the same.Graduate student, NLPStruggle with Code Generation: The modelswere deemed inefficient for code generation by mul-tiple respondents, often producing code that lacksutility due to hallucinations. Respondents high-lighted mismatches between the generated codeand its intended purpose, emphasizing the need forthorough review before utilization. Various con-cerns were raised, including the loss of context dur-ing prolonged interactions, inaccuracies in complexcoding tasks leading to erroneous outputs, fabrica-tion of functions or attributes, inaccuracies in bothcode and associated theoretical concepts, neces-sitating extensive debugging and corrections, anda tendency to cycle back to previously incorrectsuggestions despite error notifications.I was asking an AI to generate me a pieceof code. It ended up picking some code from onewebsite and some from another and combining it.However those two websites (they were linked bychatgpt) were using different versions of the li-brary so the resulting code couldnt be executed.Industry, Network and SecurityIncrease in Time for Task: A common senti-ment among respondents is that these models fre-quently produce errors or false information, result-ing in potential time wastage. While they acknowl-edge occasional helpfulness, theres a consensusthat reliance on these models can often lead to unfa-vorable outcomes, particularly when verifying out-puts. This dependency on verification contributesto increased task duration, adding extra work andtime toward the projects conclusion, as noted byseveral respondents.I use GPT API to conduct analysis for someof my work and accuracy and consistency wouldbe good in my context, and I have to find waysto finetune it before I can trust the results of theanalysis, which added more work on my end.Graduate Student, HCIMisleading and Distrust: Generating incorrectoutputs with confidence can lead to the dissemina-tion of non-existent knowledge, such as mislead-ing information in the literature that may confuseindividuals with incorrect concepts. Most of our respondents mentioned this concern. Moreover, itposes challenges in differentiating between accu-rate AI responses and hallucinations, particularlyfor users lacking expertise in the relevant subjectmatter.It leads to problems if even I do not have anyidea about the work. It is hard to differentiate if itis a genuine output or hallucination.GraduateStudent, Data Science",
  ": Frequency of Text Generation Model Usage": "10.2.3An External ViewpointAdditionally, our survey of 51 researchers who donot specialize in AI revealed that all except 3 haveused text-generation models like various versionsof ChatGPT. Despite their fields not being directlyrelated to AI, a significant number integrate thesetools into their workflow, with 19.6% using themmultiple times daily and 11.76% using them severaltimes per hour. Their extensive usage has allowedthem to identify several limitations in the models;they are: mathematical inaccuracy, outdated infor-mation, misinformation, poor performance withcomplex tasks and creative thinking, lack of speci-ficity in-depth, overconfidence, lack of transparency,bias, and irrelevant responses.Based on the definitions provided, it is observedthat there is a lack of clarity among the respon-dents regarding what constitutes a hallucinationin generative AI models, with perspectives vary-ing widely. Thematic analysis of their responsesindicates that the predominant view associateshallucination with the generation of nonfactualcontent and misinformation by AI systems. Thatmeans these models are generating facts that arenot real and misleading. The remaining themes arefactually incorrect, biased outputs, incompleteness,misinformation with confidence, and nonsensicalbut good-looking texts. The results demonstrate the unclear comprehen-sion and significance attributed to hallucination inLMs beyond the field of NLP and AI. There isa pressing need to enhance public understandingof the concept of hallucination, emphasizing itsmeaning and strategies for mitigation. Given theincreasing prominence of LMs as sociotechnicalsystems (Narayanan Venkit, 2023), it is crucial tograsp their social interactions and potential societalramifications.",
  "Additional Impacts and Concerns": "We analyzed perceptions when participants wereasked about any additional concerns during thesurvey. Participants emphasized the necessity forgreater control and more nuanced mechanisms toaddress and manage AI hallucinations effectively.Presently, the detection and rectification of halluci-nations rely heavily on meticulous human review,highlighting the need for tools designed specifi-cally to identify and mitigate such occurrences.The presence of hallucinations can significantlyimpact the credibility and acceptance of genera-tive models among the general public. These issuesarise due to the inherent limitations of generativealgorithms and the absence of access to real-timeexternal knowledge.Transparency regarding the limitations of gener-ative AI is deemed essential through our findings,and user education is seen as a key factor in mit-igating risks associated with the unchecked useof AI-generated content, as the responsibility foridentifying hallucinations often falls on the user.While inaccuracies in non-critical applications, likemovie suggestions, may be tolerable, according toour survey, they are deemed crucially problematicin contexts such as business decision-making, law,or health (Dahl et al., 2024).",
  ". Do you consider hallucinations to be a weak-ness when using these models? (Yes, No)": "12. How frequently do you encounter that textgeneration models produce hallucinated con-tent that is factually incorrect or unrelated tothe input? (Very frequently, frequently, Occa-sionally, rarely never) 13. If you have an alternate term in mind todescribe the phenomenon instead of hallu-cination (e.g., fabrications, confabulations,etc.), kindly specify it along with an explana-tion(Mention NA if none). (Open-ended)",
  "Works and Application": "We illustrate the examples and categories of worksthat were looked into for understanding the variousapplications of hallucinations. We categorize theresearch on hallucinations into 7 major categories.The definitions and categories of all the applica-tions are mentioned in .Abstractive Summarization: Zhang et al. (2019); Son et al. (2022); Maynez et al. (2020);Choubey et al. (2023); Cao et al. (2021); Mar-furt and Henderson (2022); Akani et al. (2023);van der Poel et al. (2022); Chen et al. (2023b);Dong et al. (2022); Shen et al. (2023); Nan et al.(2021); Chen et al. (2021); Ladhak et al. (2023);Nan et al. (2021); Flores and Cohan (2024)Conversational AI: Liu et al. (2022); Zhou et al. (2020); Ji et al. (2023b); Zhang et al. (2023b);Yang et al. (2023); Das et al. (2022); Bouyamourn(2023); Sun et al. (2023); Sadat et al. (2023); Slo-bodkin et al. (2023); Ramakrishna et al. (2023);Xiao and Wang (2021b); Shuster et al. (2021);Nie et al. (2019); Longpre et al. (2021); Dziriet al. (2022); Maheshwari et al. (2023); Ladhaket al. (2022); Xu et al. (2023); Chen et al. (2023a);Goldberg et al. (2022); Sundar and Heck (2023);Roller et al. (2021); Mielke et al. (2022); Rolleret al. (2021); Massarelli et al. (2020); Weller et al.(2024); Smith et al. (2022)Data Augmentation: Jian et al. (2022); Ji et al.",
  "(2023b); Friedl et al. (2021); Samir and Silfver-berg (2022); Anastasopoulos and Neubig (2019);Narayanan Venkit et al. (2023)Image and Video Captioning: Xiao and Wang": "(2021b); Dai et al. (2023); Rohrbach et al. (2018);Li et al. (2023); Testoni and Bernardi (2021); Sonet al. (2022); Dai et al. (2023); Li et al. (2023); Liuand Wan (2023)Machine Translation: Wang and Sennrich (2020); Raunak et al. (2021); Dale et al. (2022);Guerreiro et al. (2023a); Xu et al. (2023); Pfeifferet al. (2023); Guerreiro et al. (2023b); Dale et al.(2023); Irvine and Callison-Burch (2014); Ferrandoet al. (2022); Vu et al. (2022); Mller et al. (2020);Waldendorf et al. (2024)Data2Text Generation: Gonzlez Corbelle et al. (2022); Shi et al. (2023); Yoon et al. (2022); Fil-ippova (2020b); Kothyari et al. (2023); Lango andDusek (2023); Cirik et al. (2022); Fei et al. (2023);Obaid ul Islam et al. (2023); Qiu et al. (2023);Testoni and Bernardi (2021); Gonzlez-Corbelleet al. (2022); Islam et al. (2023); Polat et al. (2023)"
}