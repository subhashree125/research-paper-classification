{
  "Abstract": "Diffusion models are the state of the art in text-to-image generation, but their perceptual vari-ability remains understudied. In this paper, weexamine how prompts affect image variabil-ity in black-box diffusion-based models. Wepropose W1KP, a human-calibrated measureof variability in a set of images, bootstrappedfrom existing image-pair perceptual distances.Current datasets do not cover recent diffusionmodels, thus we curate three test sets for eval-uation. Our best perceptual distance outper-forms nine baselines by up to 18 points in accu-racy, and our calibration matches graded humanjudgements 78% of the time. Using W1KP, westudy prompt reusability and show that Imagenprompts can be reused for 1050 random seedsbefore new images become too similar to al-ready generated images, while Stable DiffusionXL and DALL-E 3 can be reused 50200 times.Lastly, we analyze 56 linguistic features ofreal prompts, finding that the prompts length,CLIP embedding norm, concreteness, and wordsenses influence variability most. As far as weare aware, we are the first to analyze diffusionvariability from a visuolinguistic perspective.Our project page is at",
  "P2: Orange ball against white background": "As shown in , the first conveys a single par-ticular illustration, while the second elicits multipleinterpretations. Orange could refer to the fruit orthe color, and the scene geometry is underspecified.But how can we quantify and characterize theselinguistic intuitions? : DALL-E 3 images for the prompts a matteorange ball in the center against a pure white back-ground (top) and orange ball against white back-ground (bottom).Our W1KP score quantifies theperceptual similarity for each set of images. It yields0.99 and 0.68 for the top and bottom rows, showing thegreater image variability of the latter. In this paper, we study the connection betweenvisual variability and language in black-box text-to-image models, focusing on state-of-the-art dif-fusion models. Previous work tends to study theperceptual distance (Zhang et al., 2018) betweenpairs of images, while a prompt can generate anear infinite set of images. Furthermore, previousapproaches have not been explicitly calibrated forhuman-friendly grades of similarity. What does ascore of, for example, 0.2 mean in terms of per-ceived similarity? Such calibration is likely crucialfor robust human interpretation.To bridge these gaps in the literature, we firstpropose a straightforward framework for construct-ing human-calibrated perceptual variability mea-sures based on existing perceptual distance met-rics. We call it the Words of a Thousand Picturesmethod, or W1KP ([wIk.pi:]) for short. On ourcrowd-sourced dataset of human-judged imagesfrom DALL-E 3, Imagen, and Stable DiffusionXL (SDXL), we validate our choice of Dream-Sim (Fu et al., 2024), a recent distance trained onStable Diffusion (Rombach et al., 2022) images.Our variant of DreamSim outperforms the best baseline by 0.10.4 points in two-alternative forcedchoice and 0.20.4 points in accuracy. To improveinterpretability, we normalize and calibrate scoresto graded human judgements on four levels of per-ceptual similarity, with cutoff points correspondingto high (0.851.0), medium (0.40.85), low (0.20.4), and no similarity (<0.2), which yield a correctclassification 78% of the time. Next, to ground our academic discourse, we in-vestigate the practical implications of our approach.Suppose a computer graphics practitioner wishesto generate a diverse array of images from a singleprompt, but it is unclear how much it can be reusedwith different seeds before additional images con-tribute little to the variability of the overall set ofimages. Our work provides a quantitative metricfor prompt reusability, as we explore further in Sec-tion 4.1. On DiffusionDB (Wang et al., 2023), anopen dataset of user-written text-to-image prompts,we find that the same prompt can be reused forImagen for 1020 random seeds, while SDXL andDALL-E 3 are more reusable at 100200 seeds. Finally, we study how 56 linguistic features af-fect generation variability. Although research hasexplored optimizing for image variability in diffu-sion (Sadat et al., 2024), they have not investigatedthe contributing linguistic constructs. To under-stand the underlying structure of these 56 features,we perform an exploratory factor analysis over Dif-fusionDB and uncover four factors of keyword pres-ence (e.g., dog walking, 4K, watercolor), syntac-tic complexity (e.g., Yngve depth), linguistic unitlength, and semantic richness. Then, we conductclean-room, single-word generation experimentsover the three strongest features in the semanticrichness factor (concreteness, CLIP embeddingnorm, and number of word senses) to assess theircontribution more precisely. We confirm that allthree linguistic features significantly (p < 0.01)correlate with perceptual variability for all threediffusion models studied. Our contributions are as follows: (1) we pro-pose and validate a human-calibrated frameworkfor building perceptual variability metrics from ex-isting perceptual distance metrics; (2) we examinea new practical application of the method in assess-ing prompt reusability in text-to-image generation;and (3) we provide original insight into the lin-guistic sources of variability in diffusion models,finding that keywords, syntactic complexity, length,and semantic richness influence variability.",
  "Preliminaries": "Text-to-image diffusion models are a family of de-noising generative models broadly consisting oftwo components: a text encoder that produces la-tent representations of language, such as T5 (Raffelet al., 2020) or CLIP (Radford et al., 2021), and adenoising image decoder that transforms randomnoise into an image conditioned on text, e.g., aconvolutional variational auto-encoder (VAE; Rom-bach et al., 2022). To generate an image, we feeda prompt into the text encoder, pass its represen-tation to the image decoder along with randomlysampled noise, then iteratively denoise the noiseinto a meaningful image. Large-scale models aregenerally trained using score matching (Song et al.,2021) on billions of imagecaption pairs (Podellet al., 2024), such as the now-deprecated LAION-5B dataset (Schuhmann et al., 2022).To conduct a general study, we explore diffu-sion in a black-box manner to be able to gener-alize to proprietary models. Formally, let a text-to-image model be G({wi}; s, ) whose codomaincomprises the sample space of all images I anddomain the sequence of words {wi}, random seeds Z to initialize the image noise, and learnedparameters Rp. To generate multiple imagesfrom a single prompt, a standard practice is to runmultiple trials for different random seeds s (Podellet al., 2024), which we follow in our experiments.Our analyses target three state-of-the-art models,one open and two proprietary:",
  "Our General Framework": "We aim to measure the visual variability of a set ofsynthetic images. Toward this, we propose to aggre-gate perceptual distances, which are well studiedin the literature, among all pairs of images in a set.To aid human interpretation of the distances, we ap-ply two steps: first, normalization, which squashespotentially unbounded and odd distributions intothe standard uniform distribution U. For in-stance, a perceptual distance with a tight range of5.105.19 across 1,000 image sets would be dif-ficult to comprehend. Second, we calibrate thedistances to graded human judgements of similar-ity and determine the corresponding cutoff points,giving meaning to score ranges (see ).Concretely, let I := {Ii}ni=1 I be an i.i.d.sample of images generated by G(). We seek afunction (I) such that (I) < (I) if I is moreself-similar than I is. A starting point is percep-tual distance, a symmetric : I I R+ thatassigns larger values to less similar image pairs.Many metrics (Fu et al., 2024) embed Ia, Ib Iusing a feature extractor f : I R, such asViT (Dosovitskiy et al., 2021), then compute a dis-tance d : R R R+ between f(Ia) and f(Ib),e.g., Euclidean distance. To standardize these dis-tances to U for better interpretability, we ap-ply the cumulative distribution function transform,defined as F(x) := P(X x). It has the propertyof F(X) being uniformly distributed:Proposition 2.1. If X is a continuous random vari-able, F(X) is standard uniform U.Hence, a normalized d is",
  "d(Ia, Ib) := F(d(f(Ia), f(Ib))),(1)": "and F is estimated from a sample {d(Iai, Ibi)}mi=1as F(d(Ia, Ib)) := |{d(Iai, Ibi) d(Ia, Ib) : 1 i m}|/m. As our sample, we generate 10,000image pairs per diffusion model for 1,000 randomlyselected DiffusionDB prompts.Equipped with a uniform perceptual distance, wenow construct measures of image set variability ().A natural framework to do this is to define a familyof U-statistics (Li, 2012; Hoeffding, 1948) oversets of images:Definition 2.1. Let h : R R R+ be an-arity kernel parameterized by d. Then a familyof U-statistics for measuring image set variabilitycan be defined as",
  "k-expected maximum (k): let d = d, = k,and h(x1, . . . , x) = min{d(xi, xj) : i = j}.This quantifies the expected maximum similaritybetween a pair of images in a set of size k": "We note a connection to statistical dispersion: if d isthe squared Euclidean distance and h the pairwisemean kernel, Ud,h is proportional to the trace ofthe covariance matrix of f(I1), . . . , f(In), i.e., thetotal variance. A proof is in Appendix B. Further-more, to match the convention of scores in denoting similarity rather than dissimilarity (e.g.,R2), for the rest of this paper we invert and report := 1 instead, calling it the W1KP score.Lastly, we find cutoff points for calibratedto human-judged levels of high, medium, low,and no similarity.For the human judgementdata, we gather a dataset {(Ixi, Iyi, zi)}Ni=1, whereIxi, Iyi I are a pair of generated images fromthe same prompt, and zi {none, low, mid, high}is the human-annotated level of similarity betweenIxi and Iyi (see .2 for details). On thedataset, we optimize the cutoff points low <mid < high to maximize the label accuracy ofthe splits Snone := [0, low), Slow := [low, mid),Smid := [mid, high), Shigh := [high, 1.0]:",
  "Before applying W1KP, we first validate our choiceof the perceptual distance backbone": "Setup. Following prior work in perceptual distanceevaluation (Zhang et al., 2018), we crowd-sourceda dataset of two-alternative forced-choice (2AFC)image triplets using Amazon MTurk (Hauser andSchwarz, 2016). Five unique workers were shownthree generated images from the same prompta reference image, image A, and image Bandinstructed to pick whether A or B resembled thereference more. This was repeated three times eachfor 500 random prompts from DiffusionDB, a largedataset of user-written prompts, for each of SDXL,Imagen, and DALL-E 3, totaling 1,500 triplets permodel. Formally, let {(Iri, Iai, Ibi, yai)}Mi=1 be adataset of M triplets, where Iri, Iai, Ibi I areimages and yai {0, . . . , 5} the number of work-ers choosing Iai over Ibi. We used attention checksthroughout the process; for more details, see Ap-pendix A.3.For our non-neural methods, we evaluated raw-image Euclidean distance (L2) and the struc-tural similarity index (SSIM; Wang et al., 2004).For our neural backbones, we tested the popularLPIPS (Zhang et al., 2018), its shift-tolerant vari-ant ST-LPIPS (Ghildyal and Liu, 2022), and anSSIM-inspired variant DISTS (Ding et al., 2020),all based on the VGG-16 architecture (Simonyanand Zisserman, 2015); SSCD (Pizzi et al., 2022), amodel trained for image copy detection; CoPer (Liet al., 2022), an extension of LPIPS to ViT; rawcosine similarity scores from CLIP (Radford et al.,2019); and lastly, DreamSim (Fu et al., 2024), which ensembles pretrained transformers trainedon Stable Diffusion images for feature extractionand applies cosine distance for measurement. SinceDreamSims domain was closest to ours, we hy-pothesized that it would be most effective. We alsoevaluated our variant, DreamSim2, with Euclideaninstead of cosine distance for d, which benefitsfrom being a true mathematical distance and henceallows for multidimensional scaling analyses, as inAppendix E.We used the standard evaluation metrics of2AFC score, defined as the mean proportion ofworkers agreeing with the backbones scores, i.e.,1MMi=1 I(Iai r Ibi) yai",
  "),where Iai r Ibi if ({Iri, Iai}) < ({Iri, Ibi}),and majority-vote accuracy. We let = mean. SeeAppendix A.3 for further setup details": "Results. We present our results in . Asan upper bound, we report the maximum possi-ble 2AFC and accuracy in row one. In line withintuition, our DreamSim backbones attain the high-est quality, surpassing CLIPL14 raw, the secondbest, by 2.0 points in 2AFC and 2.8 in accuracyon average. Our variant DreamSim2 slightly out-performs the original DreamSim with statisticalsignificance (p < 0.05 on the paired t-test) by0.10.4 in 2AFC and 0.20.4 in accuracy, possiblysince the embedding norm is informative (Oyamaet al., 2023). Thus, we select DreamSim2 as thebackbone for W1KP.Beyond quality assurance, another purpose ofthis evaluation is to ensure that the backbone doesequally well on the three image generators. Asa sanity check, the oracle (row one) has a spreadof 1.4 points (79.380.7) in 2AFC on the threemodels, indicating that humans are unbiased. OurDreamSim2 has a spread of 2.2 points (69.371.5)in 2AFC, which is below the global average spreadof 3.3 points for all the methods. We conclude thatDreamSim2 exhibits less model-wise bias than itscounterparts, possibly due to its increased qualityand in-domain training.A potential issue is that perceptual similarityis inherently subjective and hence challenging tomeasure. Research suggests to also evaluate just-noticeable differences (JND), which is thought tobe cognitively impenetrable due to its viewing-timeconstraint (Acuna et al., 2015). Because of the highcorrelation between 2AFC and JND on syntheticimages (r = 0.94; Fu et al., 2024), 2AFC appearsto be a viable proxy for JND for our study.",
  "We now assess the quality of our human calibrationprocess, as described near the end of .2": "Setup. We collected a dataset of graded imagepairs with MTurk. For 500 random DiffusionDBprompts, three unique workers were presented withtwo images generated from the same prompt andasked to judge the similarity on an integral scaleranging from not similar at all (rating 1) to thesame (5). Afterwards, we merged the last twocategories (same and very similar) since thefifth was mostly reserved for attention checks, re-sulting in the final four categories of high, medium,low, and no similarity. We took the median acrossthe three judgements and repeated the process forSDXL, Imagen, and DALL-E 3, for a total of 1,500median judgements roughly split into 10%, 30%,40%, and 20% for ratings 14. Our evaluation thenconsisted of applying Eqn. (3) with five-fold crossvalidation. For detailed settings, see Appendix A.3. Results. Eqn. (3) yields cutoff points (rounded tothe nearest 0.05 for memorability) of 0.2, 0.4, and0.85 for low, mid and high. Overall, we attainmacro- and micro-accuracy scores of 80% and 78%with DreamSim2 as the backbone. For comparison,the average macro-/micro-accuracy scores of hu-mans are 82%/80%. DreamSim2 also outperformsthe original DreamSim, which has a macro-/micro-accuracy of 79%/77%. Thus, we conclude that ourcalibration yields interpretable cutoffs.",
  "Number of Images per Prompt": ": Visualizing the overlap between the two mostsimilar images (on average) as we generate more imagesfor the two prompts. We remove the green channel forone image (magenta) and keep only the green for theother, then stack the two. Above, Imagen is reusable upto 1050 images, while DALL-E 3 up to 50200. We present qualitative examples of our cutoffsin . The levels appear sensible: highpairs (top row) match in low-level features (e.g.,trees in the same location), high-level composition(e.g., cats in washing machine), artistic style (e.g.,color photography); medium (second) in composi-tion and style; low (third) in style; and none (last)mostly differing in all. This aligns with our quan-titative results in Appendix F.1. We also verifythat normalization (Eqn. 1) is necessary. Beforenormalization, raw W1KP scores have 10th, 50th,and 90th percentiles of 0.4, 0.7, and 1.1, which issignificantly nonuniform (p < 0.01; KS test).One conceivable question is whether calibrationand normalization are essential for downstreamanalysis. It can be argued that analytic conclusionsmay still hold without a normalized, calibrated met-ric. However, as alluded to in .2, thereare two clear benefits to having one: first, normal-ization scales arbitrary scores to the 01 range, inline with other common statistics such as F1 scoreand R2. Our normalized score also has the directinterpretation as the percentile of the raw score ona known ground-truth distribution. Second, cali-bration allows us to interpret scores and aid humanunderstanding. In .1 for example, we usehigh as a cutoff for prompt reusability. Number of Images per Prompt (k) 0.4 0.5 0.6 0.7 0.8 0.9 1.0 k-Exp. Max W1KP (k) Prompt Reusability by Model",
  "Prompt Reusability Analysis": "We first ask how many times a prompt can bereused (under different random seeds) until newimages are too similar to already generated ones.This applies to graphic asset creation in particular,where visual artists are tasked with rendering manyimages of the same concept. To study this quantita-tively, we sampled 50 random prompts from Diffu-sionDB, generated 300 images for each prompt us-ing different seeds on SDXL, Imagen, and DALL-E3, then computed the k-expected maximum k fork = 1, . . . , 300.As visualized in and plotted in ,our diffusion models vary in reusability. DALL-E 3 on average does not generate highly similarimages (k high) until k 200, with our vi-sualization (top two rows in , one prompteach) displaying much green- and magenta-shiftinguntil the last column. On the other hand, Imagentends to produce duplicate images for k 50. At50 images, the two overlaid images are nearly in-distinguishable from the true-color image; see thethird column. corroborates these visualresults, with the red line (high) intersecting Ima-gens green line between 510 and DALL-E 3sblue line at 50100. It also suggests that SDXLresembles DALL-E 3 in prompt reusability; see theoverlap between the two. We conclude that diffu-sion models differ in prompt reusability, possiblydue to different decoder architectures. For example,DALL-E 3 and SDXL share the same U-Net archi-tecture, whereas Imagens is sparsified (Sahariaet al., 2022).",
  "Exploratory Factor Analysis": "Our next two analyses relate various linguisticfeatures of prompts such as syntactic complexityto perceptual variability. First, to understand thesalient structure of these linguistic features, weconduct a factor analysis over DiffusionDB.Setup. Our analysis emulates previous work ininterpreting linguistic features for speech (Fraseret al., 2016). We extracted 56 features for each ofthe 1,000 random prompts: Syntactic complexity: 24 scalar features relatedto syntax comprehension, such as clauses perT-unit and mean T-unit length, extracted usingL2SCA (Lu, 2010). We also added Yngve depth,a measure of embeddedness (Yngve, 1960). Ourmotivation was that sentences with more quali-fiers and nominals may be more visually precise. Keywords: 20 Boolean features indicating thepresence of the top-20 keywords. We had no-ticed that most prompts contained trailing key-word qualifiers after a noun phrase, e.g., catbeside road, 4k (see Appendix C for more);thus, we extracted the top 20 as features. Word order: 3 Boolean features denoting thepresence of the PTB (Marcinkiewicz, 1994) part-of-speech patterns NN VB, NN VB RB, andJJ NN in the prompt. Our purpose was to assessthe effects of adjectives and verbs on nouns. Psycholinguistics: 4 features in mean concrete-ness judgements (Brysbaert et al., 2014), rich-ness (Honores statistic and whether a wordwas in a 100k-word dictionary), and word fre-quency (Brysbaert and New, 2009). Semantic relations: 3 scalars for the mean num-ber of hyponyms, hypernyms, and word senses,from WordNet (Miller, 1995) enhanced withword sense clustering (Snow et al., 2007). Intu-itively, words with many synonyms (e.g., saw)or hyponyms (e.g., animal) may have morevisual representations. Embedding norm:2 scalars for the meansquare GloVe norm (Pennington et al., 2014) andCLIP embedding norm (Radford et al., 2021).Word embedding norms were found to encode in-formation gain (Oyama et al., 2023), which mayaffect perceptual variability through specificity.We generated 20 images per prompt for SDXL,Imagen, and DALL-E 3 and used StanfordCoreNLP (Manning et al., 2014) as our parser (ad-ditional details in Appendix D.1).",
  "Factor 4: Semantic Richness; Mean || = 0.17": "16 Number of words0.120.110.750.30 24.617 CLIP embedding norm0.17-0.61 -0.31 15118 ADJ NOUN0.550.21 0.8219 Percentage of keywords0.200.110.550.20 48.820 Mean concreteness0.470.25 2.3021 Mean # of word senses-0.110.43-0.18 2.5822 Honores statistic-0.38 -0.09 7.3623 Not in dictionary0.290.09 0.9124 Keyword: elegant0.210.04 0.0425 Keyword: fantasy0.150.05 0.04 : Linguistic features grouped by interpreted fac-tors, with high loadings (0.3) in bold and low load-ings (<0.1) removed. All Spearmans are statisticallysignificant (p < 0.05); insignificant features omitted. Results. We present our results in . Follow-ing standard practice (Fraser et al., 2016), we usean oblique promax rotation to enable interfactorcorrelation. Four factors capture sufficient varianceaccording to Kaisers criterion (Kaiser, 1958). Foreach feature, we report its correlation (Spearmans) with the per-prompt perceptual similarity (mean)and compute the mean feature score .As is conventional, we manually explain the fourfactors (F1F4). For F1, 8k, detailed, cine-matic, and digital art describe the art style, cg-society pertains to computer graphics, and art-germ is an artist with a specific style; hence, wecall it style keyword presence. F2s featuresare classic measures of syntactic complexity (Lu,2010) and thus labeled as such. In F3, mean lengthof clauses, sentences, and T-units quantify vari-ous lengths, so we name it linguistic unit length.Lastly, F4 primarily depicts semantic richness, withconcreteness, CLIP embedding norm (related to in-formation gain), number of word senses, and ADJNOUN roughly characterizing visual (non)ambiguityand Honores statistic, the number of words, andnot in dictionary portraying lexical richness.Our feature correlations with W1KP agree with intuition. Having higher concreteness (e.g., housevs. dignity) and fewer word senses (saw vs. tomato)increases similarity (rows 20, 21), likely since ab-stract and polysemous words have more visual in-terpretations. Complex nominals (row 11), adjec-tival modifiers (row 18), and keywords (F1) limitvariability through qualification. Semantic richnesshas the strongest correlated features, with half hav-ing || > 0.2. CLIP norm is the most predictiveof variability ( = 0.31), possibly because textembeddings from vision-language models are usedto initialize image generation (Sec. 2.1). Largernorms may yield more chaotic decoding trajecto-ries in the iterative solver, increasing variability.Factor-wise, linguistic unit length has the highestmean || of 0.19, where sentence length is the thirdmost predictive feature (=0.27). Longer promptspresumably provide more visual information. Weconclude that many features in the linguistic spaceare predictive of variability in the visual space, es-pecially CLIP norm, length, and concreteness.",
  "Confirmatory Lexical Analysis": "The last section studies how prompts relate to vari-ability in the DiffusionDB corpus. While it bene-fits from realism, some experimental control is lost.Thus, to supplement the previous study, this sec-tion uses single-word synthetic prompts, sampledand adjusted for word frequency in a clean-roommanner. We examine the effects of concreteness,CLIP norm, and polysemythree of the strongestfeatures from Sec. 4.2. Setup. For our prompts, we sampled 500 wordsfrom the 10k most common words in the GoogleTrillion Word Corpus (Brants and Franz, 2006). Wenoted each words concreteness rating (xconc), num-ber of word senses (xsens), CLIP embedding norm(xclip), and frequency rank (xfreq) as our explana-tory variables, mirroring the setup of .2.Words without concreteness ratings were resam-pled. We then generated 20 images for each promptwith SDXL, Imagen, and DALL-E 3 and measuredperceptual variability using mean. For our analysis,we fit a linear mixed model with xconc, xsens, xclip,and xfreq as the fixed effects, an intercept for eachdiffusion model as the random effect, and meanas the response variable. Our purpose is to testwhether concreteness, polysemy, CLIP norm, andword frequency independently influence perceptualvariability for each model.",
  ": A plot of mean against frequency, CLIP norm,concreteness, and word senses for single-word prompts.Shaded regions are 95% confidence intervals": "Results. Our linear mixed model reveals statisti-cally significant relationships (p < 0.01) betweenmean and all the predictors, whose coefficientsare 2.4 103, 4.7 104, 7.8 105, and7.2 102 for xsens, xclip, xfreq, and xconc, re-spectively. In other words, polysemy, CLIP norm,word frequency, and concreteness are significant in-dependent factors for perceptual variability, wherepolysemy and CLIP norm are positively correlated,while frequency and concreteness negatively so. In, our feature-wise plots further illustrateeach individual fixed effect. The correlation scoresare consistent in direction across the diffusion mod-els, with similar signs in Spearmans for eachfeature. They also differ by an additive shift, af-firming our random-intercepts mixed model. presents prompts of varying concrete-ness and senses. Cowboy, a concrete prompt, isless variable than concept, an abstract one, sincea cowboy is tangible. Tomato, a monosemousword, has less variability than saw, a polysemousword, because it has a narrow visual representation.In summary, our exploratory findings on concrete-ness, CLIP norm, and polysemy from .2hold in the clean-room single-word prompt setting.",
  ": Four single-word Imagen prompts with vary-ing concreteness (cowboy vs. concept) and numberof word senses (tomato vs. saw)": "precise linguistic features contributing to variabil-ity. One future direction could be to incorporatethese features into the optimization of variability.Previous work has analyzed diffusion modelsusing a mixture of computational linguistics andvision techniques. Tang et al. (2023) conductedan attribution analysis over Stable Diffusion anddiscovered entanglement, to which Rassin et al.(2024) proposed to fix using attention alignment.Separately, Toker et al. (2024) studied the layer-intermediate representations of diffusion, showingthat rare concepts require more computation. Afurther extension could be to study linguistic fea-tures responsible for increased computation, as ourpaper also relates word rarity to variability.Finally, research has previously scrutinizedthe (lack of) variability in older architectures suchas VAEs (Razavi et al., 2019) and generative adver-sarial networks, e.g., mode collapse. In this paper,we extend this analysis to modern diffusion modelswhile taking a visuolinguistic perspective.",
  "Limitations": "One limitation of our work is that while we ana-lyzed the inference-time behavior of various dif-fusion models, we did not trace the training-timecause of perceptual variability due to the scope ofour study. Doing so would require the training ofmultiple diffusion models while varying the train-ing sets, which is beyond our budget.Another limitation is that we have not metic-ulously characterized the precise distribution ofperceptual variability relative to various levels oflinguistic features, with our analyses constrainedto averages due to the moderate sample size. Forinstance, does Imagen yield a higher maximumvariability for certain levels of concreteness, evenif on average it is lower? Are there subgroupswithin each feature that better explain variances inperceptual variability? Such questions require alarger sample size to answer.We also consciously limited our examination torandom seeds and dispensed with comprehensivelyassessing other factors possibly influencing percep-tual variability, such as classifier-free guidance (Hoand Salimans, 2021). We vary the guidance scalein Appendix D.2 to confirm that SDXL is alwaysmore diverse than Imagen regardless of guidance;nevertheless, a study with additional factors otherthan linguistic features and random seeds couldyield more insights.Finally, it should be noted that our work inten-tionally disregards the relationship between qualityand variability, although the two can be conflated.For example, does increased variability reduce im-age quality? Is Imagen a better option than, say,SDXL due to its higher quality, even if it generatesless diverse imagery? Thus, text-to-image modelsshould not be chosen based on the findings of ourstudy alone. Rather, our work supplements imagequality metrics in model selection.",
  "Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravin-dra, Priya Goyal, and Matthijs Douze. 2022. A self-supervised descriptor for image copy detection. InCVPR": "Dustin Podell, Zion English, Kyle Lacey, AndreasBlattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. 2024. SDXL: Improving latentdiffusion models for high-resolution image synthesis.In ICLR. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, et al. 2021.Learning transferable visual models from natural lan-guage supervision. In ICML.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIBlog": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research. Royi Rassin, Eran Hirsch, Daniel Glickman, ShauliRavfogel, Yoav Goldberg, and Gal Chechik. 2024.Linguistic binding in diffusion models: Enhancingattribute correspondence through attention map align-ment. NeurIPS.",
  "A.2Diffusion Model Details": "SDXL. We downloaded stabilityai/stable-diffusion-xl-base-1.0 from HuggingFace zoo.We used the default guidance scale of 7.5 and 30 in-ference steps without the additional refiner module.Each 1024x1024 SDXL image took 45 secondsto generate per card, resulting in a throughput ofroughly 5060 images per minute. Imagen. We selected the imagegeneration@006model, the latest version as of April 2024, and gen-erated four square images per call while varyingthe random seed. This matched our SDXL through-put of 5060 images per minute. Each image was1536x1536 in resolution. DALL-E 3. For DALL-E 3, we used the defaultparameters of hd resolution (1024x1024) andvivid style. To mitigate prompt editing, we fol-lowed the official documentation and prepended INEED to test how the tool works with extremelysimple prompts. DO NOT add any detail, just useit AS-IS: to the prompt. The generation speed ofDALL-E 3 was considerably slower than Imagenand SDXL at approximately 10 images per minute.",
  "not chosen, we rejected all their labels and blockedthem. This resulted in a pass rate of around 90%.For higher quality, we required our workers to beMasters for participation eligibility": "W1KP metric interpretation. We present our an-notation interface for gathering graded similarityjudgements in . For the attention checks,we showed each annotator at least one pair of im-ages that were the exact same. If they did notchoose almost the same, we discarded all theirjudgements, resulting in an acceptance rate of 95%.",
  "D.2Effects of Classifier-Free Guidance": "We briefly confirmed that increasing classifier-freeguidance did not worsen the perceptual variabilityof SDXL below that of Imagen. Imagen and DALL-E 3 do not expose classifier-free guidance as aninput parameter, hence limiting us to SDXL. We in-creased the classifier-free guidance from 5.0 to 30,much higher than the normal range of 5.07.5, andregenerated the images in .1. We arrivedat a mean W1KP score of 0.53 for SDXL, whichwas below Imagens score of 0.62, e.g., SDXL stillhad greater variability.",
  "During peer review, our reviewers provided helpfulfeedback on the paper. We explicitly address a fewof their points below for transparency": "First, it was mentioned that reducing dissimilar-ity to a single numerical score does not do justiceto all the nuances of image perception. To this,we concur. Summarizing a range of phenomenaas a single scalar is a key drawback of any eval-uation metric, and our approach is not differentin this regard from well-established ones such asCLIP, BLEU, BERT score, Spearmans rho, Co-hens kappa, and others. For example, a high BERTscore or BLEU may not mean that translation qual-ity is definitively good. That remains to be judgedon a task-by-task basis. A second point from the reviewers was that ourcomputational contribution in the current work wasunclear, as our DreamSim model is only marginallybetter. In our response, we emphasized that our keycontributions are to propose and validate a human-calibrated framework for building variability met-rics from existing baselines such as DreamSim-L2. We examine a new practical application of themethod and provide new linguistic insight. A third question was about how a variability mea-sure should balance between coverage and unique-ness, and how our measure supports this. Suchnuances are important to the design of the kernelfunction, for which we construct and analyze twochosen measures. In the first pairwise-mean kernel(mean), all-pair similarities are weighted equallyin a set. Intuitively, this should provide a balancedassessment of overall variability (e.g., coverage),as every image pair has equal weight. In the secondk-expected maximum kernel (k), we estimate themaximum expected image-pair similarity out of aset of size k, thus focusing on the nearest pair ofimages (intuitively, the lack of uniqueness, e.g., du-plicates in a set of size k). Our choice of W1KP isfurther grounded by our human alignment, whichprovides interpretation of the scores. Lastly, a few comments centered on the practicalutility of obtaining multiple images from the sameprompt. In the multimedia industry, visual artistsare tasked with storyboarding and brainstorming,which require creating different images of the sameidea. Our approach would assess the reusability ofeach prompt for that purpose before a prompt isconsidered used up.",
  "F.1Metric Interpretation Quantitative Study": "One of the reviewers suggested quantifying the ex-tent to which our W1KP cutoffs corresponded toqualitative features such as composition and stylesimilarity, as claimed in .2. For this, weannotated 50 pairs of images, each from a differentprompt from DiffusionDB, for each model. Foreach image pair, we noted whether the two imagesmatched in low-level features, high-level compo-sition, and artistic style. We found the followingmedians across the models:"
}