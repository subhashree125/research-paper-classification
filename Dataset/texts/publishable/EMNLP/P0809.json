{
  "Abstract": "Academic paper search is an essential task forefficient literature discovery and scientific ad-vancement. While dense retrieval has advancedvarious ad-hoc searches, it often struggles tomatch the underlying academic concepts be-tween queries and documents, which is criti-cal for paper search. To enable effective aca-demic concept matching for paper search, wepropose Taxonomy-guided semantic Indexing(TaxoIndex) framework. TaxoIndex extractskey concepts from papers and organizes themas a semantic index guided by an academic tax-onomy, and then leverages this index as foun-dational knowledge to identify academic con-cepts and link queries and documents. As aplug-and-play framework, TaxoIndex can beflexibly employed to enhance existing dense re-trievers. Extensive experiments show that Tax-oIndex brings significant improvements, evenwith highly limited training data, and greatlyenhances interpretability.",
  "Introduction": "Academic paper search is essential for efficient lit-erature discovery and access to technical solutions.Recently, dense retrieval has advanced in variousad-hoc searches (Karpukhin et al., 2020; Izacardet al., 2021). It encodes queries and documentsas dense embeddings, measuring relevance by em-bedding similarity. These embeddings effectivelycapture textual meanings via pre-trained languagemodels trained on massive corpora. While effectivein general domains like web search, it often showslimitations in paper search (Wang et al., 2023).In paper search, it is crucial to match the under-lying academic concepts between queries and doc-uments, rather than relying on surface text and itsmeanings. Academic concepts refer to fundamentalideas, theories, and methodologies that constitute",
  "*Corresponding author": "the contents of papers. Users often seek informa-tion on specific concepts when searching for papers.For example, consider the query \"learning to winby reading manuals in a Monte-Carlo framework\".This query encompasses various concepts: optimiz-ing decision-making (learning to win), acquiringknowledge from text (reading manuals), and re-inforcement learning using probabilistic sampling(Monte-Carlo). Accordingly, retrievers should findpapers that comprehensively cover these concepts.One critical limitation of existing dense retriev-ers is that such academic concepts are often noteffectively captured, making them insufficientlyconsidered in relevance prediction. Identifying un-derlying concepts from surface text requires an in-herent understanding of domain-specific contents,which is not sufficiently obtained from general cor-pora. This challenge is even greater for queries.As shown in the previous example, user queriesoften encompass various academic concepts inhighly limited contexts. Moreover, queries usuallyhave different expression styles (e.g., terminologychoice, language style) from documents, making itdifficult to match common concepts.(left) shows the results of a dense re-triever for the query, which ranks an irrelevant doc-ument (Paper A) at the top-1. Although the papermentions solving a goal and comprehension oftext, which have similar meanings to learning towin and reading manuals in the query, its focus ison text comprehension and dataset creation, whichlargely differs from the query concepts. This showsthat the overall textual similarity captured by denseretrievers is insufficient to reflect specific academicconcepts, leading to suboptimal results.To address this limitation, we introduce a newapproach that extracts key concepts from papers inadvance, and leverages this knowledge to incorpo-rate academic concepts into relevance predictions.We construct a semantic index that stores semanticcomponents best describing each paper. The pro- [Paper A (irrelevant)] A challenge dataset for the open-domain machine comprehension of text. We present MCTest, afreely available set of stories and associated questions intendedfor research on the machine comprehension of text. Previouswork on solving a more restricted goal [Paper B (relevant)]Using reinforcement learning to learn how to play text-based games. The ability to learn optimalcontrol policies in systems where action space is defined bysentences in natural language optimisation of dialoguesystems. Text-based games with multiple endings Dense retriever ( ): Paper A (Top-1), Paper B (Top-89) [Query] Learning to win by reading manuals in a Monte-Carlo framework with TaxoIndex ( ): Paper A (Top-69), Paper B (Top-2) overall textual similarity+ common academic concepts reinforcement learningrepresentation learning reward technique natural language interfacedomain knowledgeQ learning Monte Carlonatural language processingartificial intelligence Indexed information manuals, domain knowledge, decision making, Monte Carlo, Markov Chain Monte Carlo, sample selection, text-based games,control, reinforcement learning, natural language interface. Papers in the corpus reinforcement learningtext-based gameslearning environment text representation game playingfeedbackdecision bootstrap Monte Carlocontrol policydialogue systems phrase level benchmark datasetcrowdsourcing comprehension approach question answering textual question natural language inference natural language processingartificial intelligence machine comprehension reading comprehension benchmark multiple choice open domain question question answering crowdsourcing screening workers relation extraction phraselevel topiclevel topiclevel infer",
  ": A case study from CSFCube dataset. Results of (left) a dense retriever, (right) with TaxoIndex. For thedense retriever, we use SPECTER-v2 fully fine-tuned on the target corpus": "posed index represents each paper at two differentgranularities: topic and phrase levels, as shownin (right). Topic level provides a broadercategorization of research, such as reinforcementlearning or natural language processing, whilephrase level includes specific details, such as text-based games or control policy, complementarilyrevealing each papers concepts. We leverage thisindex to enhance the existing dense retrievers, en-abling more precise academic concept matching.We propose Taxonomy-guided semantic In-dexing (TaxoIndex) framework. We first introducea new index construction strategy that extracts keyconcepts from papers. To guide this process, wepropose using an academic taxonomy, a hierarchi-cal tree structure outlining academic topics.1 Wethen propose a new training strategy, called indexlearning, that trains a model to explicitly identifythe indexed information for input text. This is acritical technique that enables TaxoIndex to inferthe most related academic concepts in test queries,even if expressed in different terms or not explic-itly mentioned, by associating them with papershaving similar contexts. This inferred informationhelps find relevant papers sharing academic con-cepts, combined with textual similarity from denseretrievers. (right) shows that TaxoIndexfinds Paper B as the top-2 result based on highoverlap in indexed information.TaxoIndex offers several advantages for existingdense retrievers. First, TaxoIndex largely improvesretrieval quality by effectively matching academicconcepts. Notably, index learning offers a new type",
  "Academic taxonomies are widely used for study catego-rization in various institutions (e.g., ACM Computing Classi-fication System) and can be readily obtained from the web": "of supervision to identify key concepts from text,which is not directly provided by query-documenttraining pairs, yielding significant improvementseven with limited training data. Second, TaxoIndexenhances interpretability by explicitly representingeach text with topics and phrases, as shown in ourcase studies. Further, these advancements do notcome at a large cost of model complexity. Tax-oIndex updates only a small module, accountingfor 6.7% of the retriever parameters, yet outper-forms fully fine-tuned models.Our primary contributions are as follows: (1)We propose TaxoIndex, which systematically con-structs and leverages semantic index, for effectiveacademic concept matching in paper search. (2)We design an index construction strategy that rep-resents each paper at both topic and phrase levels,with the guidance of academic taxonomy. (3) Weintroduce an index learning strategy that allows foridentifying the most related concepts from an inputtext. (4) We evaluate TaxoIndex with extensivequantitative and ablative experiments and compre-hensive case studies.",
  "Related Work": "Dense retrieval. The advancement of pre-trainedlanguage models (PLMs) has led to significantprogress in dense retrieval. Recent studies have en-hanced retrieval quality through retrieval-orientedpre-training (Izacard et al., 2021; Gao and Callan,2022), advanced hard negative mining (Zhan et al.,2021; Qu et al., 2021), and distillation from cross-encoder (Zhang et al., 2022). Synthetic query gen-eration has also been explored to supplement train-ing data (Thakur et al., 2021; Dai et al., 2023). Academic taxonomy biology genetics physiology computer science machinelearning natural language processing domain adaptation reinforcement learning * computer vision parsing augmented unparser transition networkapprenticeship learningQ learning",
  ": A conceptual illustration of the taxonomy-guided semantic index construction. We extract and store coretopics and indicative phrases that best represent each paper in the form of a forward index": "On the other hand, many studies have focused onpre-training methods specialized for the academicdomain. In addition to pre-training on academiccorpora (Beltagy et al., 2019), researchers haveexploited metadata associated with papers. Co-han et al. (2020); Ostendorff et al. (2022) use ci-tations, Liu et al. (2022) further utilizes venues,authors, and affiliations. Mysore et al. (2022) usesco-citation contexts, and Singh et al. (2023); Zhanget al. (2023b) employs multi-task learning of taskssuch as citation prediction and paper classification.Complementary to the approach of leveragingsuch paper metadata, we focus on organizing andexploiting knowledge in the textual corpus. Tax-oIndex can be flexibly integrated to enhance theaforementioned models.Indexing for dense retrieval.Indexing refersto the process of collecting, parsing, and stor-ing data to enhance retrieval (Moura and Cristo,2009). Statistical and sparse retrieval often usesinverted indexes for term matching signals (Bruchet al., 2024). Dense retrieval relies on approximatenearest neighbor (ANN) indexes to avoid costlybrute-force searches. Document embeddings arepre-computed offline, and ANN indexes are con-structed by techniques such as hashing (Pham andLiu, 2022), quantization (Baranchuk et al., 2018),and clustering (Zhang et al., 2023a; Li et al., 2023).Our index is designed to extract academic con-cepts and leverage them to improve the accuracy ofdense retrievers. As TaxoIndex encodes each textas an embedding, existing ANN indexes can stillbe applied to accelerate search speed.Enhancing retrieval with additional contexts.Several studies have enhanced retrieval by provid-ing supplementary contexts. Our work falls intothis direction. Pseudo-relevance feedback (Zhenget al., 2020; Wang et al., 2021; Yu et al., 2021) uti-lizes the top-ranked results from an initial retrieval.Recent generative approaches (Mao et al., 2021;Mackie et al., 2023) generate relevant contexts us-ing PLMs. Kang et al. (2024) utilizes topic distri- butions of queries and documents. However, theyare often limited in paper search due to the diffi-culty of generating proper domain-specific contexts.Moreover, these contexts are obtained and addedon-the-fly during inference, making it difficult toprovide information tailored to backbone retriever.",
  "Problem Formulation": "Academic taxonomy.An academic taxonomyT refers to a hierarchical tree structure outliningacademic topics (). Each node representsan academic topic, with child nodes correspondingto its sub-topics. Widely used for study categoriza-tion in various institutions, academic taxonomiescan be readily obtained from the web and automat-ically expanded by identifying new topics from agrowing corpus (Lee et al., 2022; Xu et al., 2023).We utilize the fields of study taxonomy from Mi-crosoft Academic (Shen et al., 2018), which covers19 disciplines (e.g., computer science, biology). Problem definition. To perform retrieval on a newcorpus D, a PLM-based dense retriever is typicallyfine-tuned using a training set of relevant query-document pairs. Our goal is to develop a plug-and-play framework, which facilitates academicconcept matching with the guidance of a giventaxonomy T , to improve the backbone retriever.",
  "Taxonomy-guided Index Construction": "We construct a semantic index that stores semanticcomponents that best describe each paper (). To guide this process, we propose using the aca-demic taxonomy. This ensures that the index orga-nizes knowledge in alignment with the researchersconsensus and greatly improves interpretability. Our key idea is to represent each paper using acombination of core topics and indicative phrasesthat reveal its key concepts at different granularities.Core topics correspond to nodes in the taxonomy,providing a broader view for categorizing papers.Indicative phrases are directly extracted from eachpaper, offering finer-grained information to distin-guish it from other topically similar documents.",
  "Core Topic Identification": "The given taxonomy may contain many topics notincluded in the corpus. To effectively identify coretopics from the vast topic hierarchy, we introducea two-step strategy that first finds candidate topicsand then pinpoints the most relevant ones.Candidate topics identification. Utilizing the hi-erarchy, we employ a top-down traversal approachthat recursively visits the child nodes with the high-est similarities at each level. For each document,we start from the root node and compute its simi-larity to each child node. We then visit child nodeswith the highest similarities.2 This process recursuntil every path reaches leaf nodes, and all visitednodes are regarded as candidates for the document.The document-topic similarity s(d, c) can bedefined in various ways. As a topic includes itssubtopics, we incorporate the information fromall subtopics for each topic node.Let Nc de-note the set of nodes in the sub-tree having cas a root node.We compute the similarity as:s(d, c) =1",
  "|Nc|jNc cos(ed, ej), where ed andej denote representations from PLM for a docu-ment d and the topic name of node j, respectively.3": "Core topic selection.We select core topics byfiltering out less relevant ones from the candidates.We consider two strategies: (1) score-based filter-ing, which retains topics with similarities abovea certain threshold, and (2) LLM-based filtering,which uses large language models (LLMs) to selectcore topics. Our preliminary analysis shows thatboth filtering strategies are effective and lead tocomparable retrieval accuracy. In this work, weopt for LLM-based filtering, as it often handles am-biguous cases better, further enhancing retrieval in-terpretability. Further analysis is provided in 5.3.We prompt the LLM to select core topics fromthe candidates by excluding those that are too broad 2We visit multiple child nodes and create multiple paths, asa document usually covers various topics. For a node at level l,we visit l + 2 nodes to reflect the increasing number of nodesat deeper levels of the taxonomy. The root node is level 0.3We use BERT with mean pooling as the simplest choice. or less relevant.4 After identifying core topics forall documents, we tailor the taxonomy by only re-taining the topics selected as core topics at leastonce, along with their ancestor nodes.In sum, for each document d, we obtain coretopics as ytd {0, 1}|TD|, where ytdi = 1 indicatesi is a core topic of d, otherwise 0. |TD| denotes thenumber of nodes in the tailored taxonomy.",
  "Indicative Phrase Extraction": "From each document, we extract indicative phrasesused to describe its key concepts. These phrases of-fer fine-grained details not captured by topic level,playing a crucial role in understanding detailed con-tent and enhancing retrieval. An indicative phraseshould (1) show stronger relevance to the documentthan to others with similar core topics, and (2) referto a meaningful and understandable notion.We first obtain the phrase set P in the corpus us-ing an off-the-shelf phrase mining tool (Shang et al.,2018). Then, inspired by Tao et al. (2016); Lee et al.(2022), we compute the indicativeness of phrase pin document d based on two criteria: (1) Distinc-tiveness dist(p, Dd) = exp(BM25(p, d))/ (1 +dDd exp(BM25(p, d))) quantifies the relativerelevance of p to the document d compared to othertopically similar documents Dd. Dd is simply re-trieved using Jaccard similarity of core topic anno-tation ytd. (2) Integrity int(p) measures the concep-tual completeness of the phrase, typically providedby most phrase mining tools, preventing the selec-tion of non-meaningful phrases. The final indica-tiveness of p is defined as: (dist(p, Dd) int(p))12 .For each document d, we select top-k indicativephrases and denote them as ypd {0, 1}|P|, whereypdj = 1 indicates j is an indicative phrase of d. Remarks. Compared to recent clustering-based in-dexes for dense retrieval (Zhan et al., 2022; Li et al.,2023), which use cluster memberships from doc-ument clustering, the proposed index has severalstrengths: it effectively exploits domain knowledgefrom taxonomy, offers broad and detailed views viatopics and phrases, and enhances interpretability.",
  "Indexing Network: linking text to index": "A naive approach to using the index information isto append it to each text as additional input context.However, this approach has several limitations. Im-portantly, test queries are not accessible before thetest phase. Annotating topics and phrases during in-ference not only incurs additional latency but also isless effective due to the limited context of queries.As a solution, we propose a new strategy calledindex learning, which trains the indexing networkto identify core topics and indicative phrases fromthe text. We formulate this as two-level classifica-tion tasks, i.e., topic and phrase levels.Extracting topic/phrase information. Given thebackbone retriever embedding hBd Rl, we extractinformation tailored to predict topics and phrases ashtd and hpd, respectively. To exploit the complemen-tarity of topics and phrases, we employ a multi-gatemixture of experts architecture (Ma et al., 2018).We use M different experts, {fm}Mm=1, eachof which is a small feed-forward network fm :Rl Rl. Two gating networks, gt and gp, withSoftmax outputs control the influence of expertsfor topic and phrase prediction, respectively. Letwt = gt(hBd ) and wp = gp(hBd ) denote M-dimensional vectors controlling the influences. Therepresentations for each task are computed as:",
  "m=1wpmfm(hBd ) (1)": "This enables the direct sharing of information ben-eficial for predicting both topics and phrases, mu-tually enhancing both tasks (Ma et al., 2018).Generating class representation. We encode top-ics and phrases to generate class representations for classification learning. For topics, we employgraph neural networks (GNNs) (Kipf and Welling,2016) to exploit their hierarchical information. Ini-tially, each node feature is set as the fixed PLMrepresentation of its topic name, followed by GNNpropagation over the taxonomy structure. Afterstacking GNN layers, we obtain each topic repre-sentation etPLM as ep i. Each phrase is also encoded usingj. For notational simplicity, we pack thetopic and phrase representations, denoting them asEt R|TD|l and Ep R|P|l, respectively.Index learning. For each input text, index learn-ing is applied to predict the corresponding core top-ics and indicative phrases ytd and ypd. We computethe probabilities for topics and phrases as ytd =softmax(Et(htd)T ), ypd = softmax(Ep(hpd)T ), re-spectively. The cross-entropy loss is then applied:",
  "Fusion Network: fusing index knowledge": "The index-based representations (htd, hpd) encodecore topics and indicative phrases comprising theacademic concepts within the text. We fuse themwith the backbone embedding (hBd ), which encodesthe overall textual meanings, to generate hd Rl.We combine the topic and phrase representationsas hId = fI([htd; hpd]) using a small network fI :R2l Rl. The final embedding is obtained as:",
  "esim(hq,hd+) + d esim(hq,hd)(4)": "where d+ and d denote the relevant and irrelevantdocuments. Index learning is applied to both docu-ments and training queries Q. The final objectiveis LCL(Q, D) + IL(LIL(D) + LIL(Q)), whereIL is a hyperparameter to balance the loss. Toensure hId contains high-quality information, weinitially warm up the indexing network using LIL.Core topic-aware negative mining. We devisea new strategy that uses core topics to mine hard-negative documents. Core topics reveal key con-cepts based on taxonomy, which may not be effec-tively captured by the lexical overlap (e.g., BM25)widely used for negative mining (Formal et al.,2022). We utilize both topical and lexical overlapsto select negative documents. For each (q, d+) pair,we retrieve Dd+, a set of topically similar docu-ments to d+, using Jaccard similarity of core topics,as done in 4.1.2. We then select documents withthe highest BM25 scores for q as negative samples.",
  "Retrieval with TaxoIndex": "Based on the index, TaxoIndex incorporates thesimilarity of surface texts and the similarity ofthe most related concepts for relevance prediction.This approach enhances the understanding of testqueries, enables more precise academic conceptmatching, and improves paper search.We introduce advanced inference techniques tofurther enhance retrieval using topic/phrase predic-tions (yt, yp) for queries and documents.Document filtering based on core topics.Be-fore applying the retriever, we filter out irrelevantdocuments that have minimal core topic overlapwith the query. This step enhances subsequent re-trieval by reducing the search space and providingtopical overlap information. We compute the top-ical overlap using the inner product of ytq and ytd. Documents with low topical overlap are excluded,retaining only the top x% of documents from theentire corpus. In this work, we set x = 25%. Weprovide retrieval results with varying x in 5.3.Interpreting search results.The topics andphrases with the highest probabilities reveal the aca-demic concepts captured and reflected in relevanceprediction. Comparing query and document predic-tions allows for interpreting the search results. Weprovide case studies in and Appendix C.3.Expanding query with indicative phrases. Wecan expand a query by appending top-k phrases notincluded in the query. The retrieval results usingthe expanded query are denoted as TaxoIndex ++.",
  "Experiment setup": "We provide further details on setup in Appendix B.Dataset and taxonomy. We use two datasets5:CSFCube (Mysore et al., 2021) and DORIS-MAE(Wang et al., 2023), which provide test query col-lections along with relevance labels on the aca-demic corpus, annotated by human experts andLLMs, respectively. We use training queries gen-erated by Dai et al. (2023), as they are not pro-vided in both datasets. We use the field of studytaxonomy from Microsoft Academic (Shen et al.,2018) which contains 431,416 nodes. After in-dexing, we obtain 1,164 topics and 3,966 phrasesfor CSFCube, and 1,498 topics and 6,851 phrasesfor DORIS-MAE. For core topic selection in Tax-oIndex and baselines that require LLMs, we usegpt-3.5-turbo-0125.Metrics. Following Mackie et al. (2023); Kanget al. (2024), we employ Recall@K (R@K) for alarge retrieval size (K), and NDCG@K (N@K)and MAP@K (M@K) for a smaller K ( 10).Backbone retrievers. We employ two representa-tive models: (1) SPECTER-v2 (Singh et al., 2023)is a highly competitive model trained using meta-data of scientific papers. (2) Contriever-MS (Izac-ard et al., 2021) is a widely used retriever fine-tunedusing vast labeled data from general domains.Baselines. We compare three types of methods forapplying and improving the backbone retriever.(1) Conventional approaches: no Fine-Tuning,Full Fine-Tuning (FFT), add-on module Fine-Tuning (aFT). FFT and aFT follow standard con-",
  "trastive learning with BM25 negatives. FFT up-dates the entire backbone retriever, while aFT onlyupdates an add-on module identical to TaxoIndex.6": "(2) Enhancing retrieval with additional context:GRF (Mackie et al., 2023) generates relevant con-texts by LLMs. We generate both topics and key-words for a fair comparison. ToTER (Kang et al.,2024) uses the similarity of topic distributions be-tween queries and documents, with topics providedby the taxonomy. We apply both methods to FFT.(3) Enhancing fine-tuning using an index: JTR(Li et al., 2023) constructs a tree-based index viaclustering, then jointly optimizes the index andtext encoder. Though focused on efficiency, it alsoenhances accuracy with index-based learning. Weimpose no latency constraints for a fair comparison.We provide details on hyperparameters and im-plementation in Appendix B.4.",
  "Retrieval Performance Comparison": "Main results. In , TaxoIndex performsbetter than all baselines on both backbone modelsacross various metrics. Notably, TaxoIndex consis-tently outperforms FFT despite using significantlyfewer trainable parameters, and aFT despite usingthe same add-on module. This shows the efficacyof the proposed approach using the semantic index.Conversely, GRF often degrades performance. TheLLM-generate contexts are not tailored to targetdocuments, potentially causing discrepancies in ex-",
  ": Results with varying amounts of training data.We report improvements over no Fine-Tuning. denotesp < 0.05 from paired t-test with FFT": "pressions and focused aspects.7 JTR also fails tooutperform FFT. It relies on document clustering,which may be less effective in specialized domains.Among the baselines, ToTER shows competitiveperformance by leveraging topic information. How-ever, it cannot consider fine-grained concepts notcovered by topics, and adds topic information on-the-fly only at inference, failing to fully enhancethe backbone retriever. Lastly, while TaxoIndex++ brings improvements, they are not significantlyhigh, possibly because the phrase information is al-ready reflected by TaxoIndex.For the subsequent analyses, we use SPECTER-v2 for CSFCube and Contriever-MS for DORIS-MAE which show the highest NDCGs in .",
  ": Further analysis for difficult queries. We reportimprovement over no Fine-Tuning. denotes p < 0.05from paired t-test with ToTER": "training data. FFT shows restricted improvementswith limited data and even degrades performance.In contrast, TaxoIndex consistently achieves signif-icant improvements, even with highly limited train-ing data. This is practically advantageous for real-world applications where collecting ample data ischallenging. TaxoIndex trains the model to explic-itly identify the most important concepts based onthe index knowledge, effectively reducing relianceon the training data.Difficult query analysis. In , we furtheranalyze results for difficult queries, which accountfor 20% of total test queries. They are identifiedby two factors complicating query comprehension:(a) high lexical mismatch with documents, (b) highconcept diversity within the query.8 FFT shows lim-ited results and even degrades performance, despitethe overall improvement in . Conversely,TaxoIndex consistently improves the retrieval qual-ity on both types of queries, effectively handlinglexical mismatch and various academic concepts.These results in 5.2 collectively show the effec-tiveness of TaxoIndex in academic paper search.",
  "Study of TaxoIndex": "Ablation study. presents various ablationresults. First, the best performance is achieved byindexing both topics and phrases. Notably, remov-ing phrase information drastically degrades perfor-mance, as phrases enable fine-grained distinctionsof each document. Conversely, the absence of topic-level can be partially compensated by phrases, lead-ing to smaller performance drops. Second, botharchitecture choices improve the indexing network,verifying the efficacy of leveraging the complemen-tarity of topics and phrases and the topic hierarchy.Lastly, both adaptive weight and topic-aware min-ing prove effective. The mining technique shows",
  ": Results with varying retention ratio x": "higher impacts on top-ranked documents and oftenleads to faster convergence in our experiments.Document Filtering based on Core Topics. Fig-ure 4 shows the retrieval performance with varyingretention ratios. We observe that topic-based filter-ing achieves comparable results to a whole corpussearch by examining about 25% of the documents.This result indicates that core topics indeed effec-tively capture the central theme of each document.We expect that core topics can be leveragedto improve recent clustering-based ANN indexes(Zhan et al., 2022; Li et al., 2023), which conductclustering on document embeddings and use clustermemberships to represent documents. As topics arealready discrete categories, this approach can re-duce the need for clustering operations and provideguidance during the clustering process. Addition-ally, it offers interpretability by explicitly usingtopic names. As this is not the focus of our work,we leave further investigation for future research.Impact of LLM-based topic filtering. For coretopic identification, TaxoIndex utilizes LLM-basedfiltering (4.1.1). In (a), we explore itsimpacts by replacing it with score-based filtering,which retains documents with similarity above themedian similarity of all documents assigned to eachtopic. Both score- and LLM-based filtering consis-tently achieve significant improvements over FFT.LLM-based filtering discerns detailed topics bet-",
  ": Taxonomy-related analysis on CSFCube": "ter, aiding in finding more precise order amongthe top-ranked documents, and further enhancinginterpretability. This result also supports the effec-tiveness of our candidate topic generation strategy.Impact of taxonomy quality. TaxoIndex utilizesan academic taxonomy to guide index construc-tion. In (b), we explore the impact of taxon-omy quality by impairing completeness throughrandom pruning, controlling the ratio of removednodes to total nodes. TaxoIndex shows consider-able robustness, outperforming FFT even with 50%pruning. During training, missing topics can bepartially inferred from existing topics and phrases,and detailed phrase information can compensatefor incompleteness not covered by the topics. Thisanalysis shows that TaxoIndex is not heavily depen-dent on taxonomy quality. We expect TaxoIndexto be effective with taxonomies available on theweb, and further improved with existing taxonomycompletion techniques (Lee et al., 2022; Xu et al.,2023; Zhang et al., 2024).",
  "Conclusion": "We propose TaxoIndex to match academic conceptsin paper search effectively. TaxoIndex extracts keyconcepts from papers and constructs a semantic in-dex guided by an academic taxonomy. It then trainsan add-on module to identify and incorporate theseconcepts, enhancing dense retrievers. Extensiveexperiments show that TaxoIndex yields significantimprovements, even with limited training data.We expect that TaxoIndex will effectively im-prove retrieval quality in various domains whereunderlying search intents are not sufficiently re-vealed by surface text. Specifically, e-commerceis an interesting and promising domain. In this do-main, users often express their information needsin various forms rather than searching by the ex- act product name. They might include desired at-tributes, characteristics, or even specific use cases.TaxoIndex can be applied to better capture userssearch intents in such scenarios. We leave furtherinvestigation as future work.",
  "Limitations": "Despite the satisfactory performance of TaxoIndex,our study has three limitations.First, we utilize an academic taxonomy obtainedfrom the web to guide core topic identification(4.1.1). We acknowledge that the taxonomy maynot reflect up-to-date information. However, weare optimistic that this issue can be addressed byleveraging automatic taxonomy construction andcompletion techniques, a well-established researchfields with many readily available tools (Lee et al.,2022; Xu et al., 2023; Shi et al., 2024). Also, ouranalysis in 5.3 shows that TaxoIndex has consider-able robustness to taxonomy coverage by utilizingphrase information directly extracted from papers.Second, for topics and phrase mining process(4.1), we employ relatively simple techniques(e.g., distinctiveness and integrity computations)that have proven effective in recent text miningwork. While these choices show high effectivenessin our experiments, we acknowledge that more so-phisticated techniques could be employed. Our pri-mary contributions lie in representing each papersconcepts at two levels and incorporating them intorelevance predictions, rather than in the specificdetails for obtaining topics and phrases.Lastly, this work focuses on the typical denseretrieval models that represent each text as a singlevector embedding. Applying TaxoIndex to multi-vector representation models (Santhanam et al.,2022) may require additional modifications, whichhave not been explored in this study.",
  "Ninh Pham and Tao Liu. 2022. Falconn++: A locality-sensitive filtering approach for approximate nearestneighbor search. In NeurIPS, pages 3118631198": "Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu,Donald Metzler,Xuanhui Wang,et al. 2023.Large language models are effective text rankerswith pairwise ranking prompting.arXiv preprintarXiv:2306.17563. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, RuiyangRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, andHaifeng Wang. 2021. RocketQA: An optimized train-ing approach to dense passage retrieval for open-domain question answering. In NAACL-HLT, pages58355847.",
  "Amanpreet Singh, Mike DArcy, Arman Cohan, DougDowney, and Sergey Feldman. 2023.Scirepeval:A multi-format benchmark for scientific documentrepresentations. In EMNLP, pages 55485566": "Fangbo Tao, Honglei Zhuang, Chi Wang Yu, Qi Wang,Taylor Cassidy, Lance M Kaplan, Clare R Voss, andJiawei Han. 2016. Multi-dimensional, phrase-basedsummarization in text cubes. IEEE Data Eng. Bull.,39(3):7484. Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In NeurIPS Datasetsand Benchmarks Track. Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Prud-hviraj Naidu, Leon Bergen, and Ramamohan Pa-turi. 2023. Scientific document retrieval using multi-level aspect-based queries. In NeurIPS Datasets andBenchmarks Track.",
  "APrompt for Core Topic Selection": "We instruct LLMs using the prompt provided be-low. Both datasets used in this work contain paperabstracts. In our experiments, the average numberof candidate topics is 28.5, and the average numberof selected topics is 9.4.It is important to note that representing the vastnumber of nodes in the taxonomy within a singleprompt is infeasible. Our two-step strategy, whichfirst identifies candidate topics and then pinpointscore topics, facilitates effective core topic selection. Instruction: You will receive a paper abstractalong with a set of candidate topics for the paper.Your task is to select the topics that best alignwith the core theme of the paper. Exclude topicsthat are too broad or less relevant. You may listup to 10 topics, using only the topic names in thecandidate set. Do not include any explanation.Paper: [DOCUMENT],Candidate topic set: [CANDIDATE TOPICS]",
  "B.1Dataset": "We have surveyed the literature to find retrievaldatasets in the academic domain where relevanceis labeled by experts (or annotators with high capa-bilities). We select two recently published datasets:CSFCube (Mysore et al., 2021) and DORIS-MAE(Wang et al., 2023). They provide test query col-lections along with relevance labels, annotated byhuman experts and LLMs, respectively. They alsorepresent two real-world search scenarios: query-by-example and human-written queries.For both datasets, we conduct retrieval from theentire corpus including all candidate documents.CSFCube dataset consists of 50 test queries, withabout 120 candidates per query drawn from ap-proximately 800,000 papers in the S2ORC corpus.Annotation scores greater than 2 (nearly identi-cal or similar) are treated as relevant. We use thetitle as the query and both the title and abstractfor the documents. DORIS-MAE dataset consistsof 100 test queries, with about 100 candidates perquery drawn similarly to CSFCube dataset. Foreach query, average annotation scores greater than1 (the document answers some or all key compo-nents) are treated as relevant.Lastly, we provide results of Contriever-MSon SCIDOCS (Cohan et al., 2020; Thakur et al.,2021) in Appendix C.1. SCIDOCS dataset con-tains 1, 000 test queries with relevance labels for25, 657 papers. Please note that we exclude thisdataset from main experiments, as it uses citationrelations for relevance labels, which are utilized forthe training of SPECTER-v2.",
  "B.3Metrics": "Following the previous work (Thakur et al., 2021;Mackie et al., 2023; Kang et al., 2024), we em-ploy Recall@K (R@K) for a large retrieval size(K), and NDCG@K (N@K) and MAP@K for asmaller K. Recall@K measures the proportion ofrelevant documents retrieved in the top K results,without consideration of the rank of the documents.Conversely, NDCG@K and MAP@K directlyconsider the absolute rank of each relevant doc-ument, where a higher value indicates that relevantdocuments are consistently found at higher ranks.",
  "B.4Experiment Details": "Backbone models.We use publicly availablecheckpoints: SPECTER-v29 and Contriever-MS10.SPECTER-v2 is trained via multi-task learning us-ing paper metadata from SCIBERT (Beltagy et al.,2019), and Contriever-MS is fine-tuned via mas-sive training queries (MS MARCO) from BERTbase uncased (Devlin et al., 2018). Both backbonemodels have about 110 million parameters.Computational resources and API cost. We con-duct all experiments using 4 NVIDIA RTX A5000GPUs, 512 GB of memory, and a single Intel XeonGold 6226R processor. For ChatGPT API usage,we spent $11.50 on core topic selection in TaxoIn-dex and $39.30 on query generation.Implementation details. For BM25, we use Elas-ticsearch.11 As training queries are not provided inall datasets used in this work, we leverage syntheticqueries generated by using PROMPTGATOR (Daiet al., 2023), the state-of-the-art query generationmethod.12 All compared methods are trained usingthe same queries. We use 10% of training data as avalidation set. We share the generated queries forreproducibility. We report the average performanceover five independent runs. Fine-tuning details. FFT and aFT use top-50hard negatives mined from BM25 for each query,as done in Formal et al. (2022). TaxoIndex usestop-50 hard negatives mined using core topicsand BM25 scores (4.2.3). We use the inner",
  "We use gpt-3.5-turbo-0125 for query generator": "product as a similarity function for relevance pre-diction. The learning rate is set to 1e6 for FFTand 1e4 for aFT and TaxoIndex, after tuningamong {1e7, 1e6, ..., 1e3}. We set the batchsize as 128 and the weight decay as 1e4. GRF, ToTER, and JTR. We utilize the offi-cial implementation for GRF13, ToTER14, andJTR15. For GRF, we generate both topics andkeywords using the same LLMs with TaxoIndexfor a fair comparison. ToTER and TaxoIndexutilize the same given taxonomy. For baseline-specific hyperparameters, we closely follow therecommended values in the original papers andimplementations. TaxoIndex. TaxoIndex only updates an add-on module that has 7.38 million parameters,which account for about 6.72% of backbonemodels. For core topic selection using LLM,we set the temperature as 0.2. The phrase setP is obtained using AutoPhrase.16 In 4.1.2,the number of phrases per document is set ask = min(15, Pd 0.2), where Pd denotes thetotal number of phrases in the document d. Thissimple choice allows for a natural considerationof the document length. The average numberof indicative phrases is 13.2 for CSFCube and13.7 for DORIS-MAE. For TaxoIndex ++, weset k = 15. We set the size of topically similardocument set |Dd| = 100. For the indexing network, we set the number ofexperts as M = 3, each being a two-layer MLP.gt, gp are linear layers with Softmax outputs. Forthe topic encoder, we use a two-layer graph con-volution network (Kipf and Welling, 2016). Forthe fusion network, we use a two-layer MLP forfI, and a linear layer for gI : Rl R. For index learning of training queries, we use theaveraged labels of the relevant documents when aquery has multiple relevant documents. We firstwarm up the indexing network using LIL untilthe training loss converges. We set IL = 0.1.",
  "C.1SCIDOCS Results": "We provide results of Contriever-MS on SCIDOCS.Please note that we exclude this dataset from themain experiments, as it uses citation relations forrelevance labels, which are utilized for the trainingof SPECTER-v2. We conduct automatic evaluationusing LLMs as well as conventional evaluationusing relevance labels.Conventional evaluation. presents the re-trieval results. Overall, TaxoIndex shows higher re-trieval performance compared to FFT and ToTER,despite using significantly fewer trainable parame-ters.",
  "C.3Additional Case Study": "Setup. As discussed in 4.3, topics and phraseswith the highest predicted probabilities reveal aca-demic concepts captured and reflected for retrieval.We interpret search results by comparing predic-tions for queries (ytq, ypq) and documents (ytd, ypd).In our case studies in , , and Ta-ble 8, we use topics and phrases having the highestlogit values. Note that we use y instead of y fordocuments, as unlabeled but relevant classes arenaturally revealed during training. Case study: short query with limited context.In , we present inferred information fortwo example queries.For the query semanticparsing learning with limited labels, TaxoIndexinfers concepts such as syntactic predicate andsemi-supervised learning. Similarly, for the querydomain adaptation approach for machine transla-tion, TaxoIndex identifies related concepts likeparallel corpus and NMT (neural machine trans-lation). This inferred information complementslimited query context, facilitating concept match-ing for paper search. We highlight that our index isconstructed by organizing knowledge in the targetcorpus, and thus these terminologies are actuallyused in the papers that users search. Case study: long and complex query.In Ta-ble 8, we explore how TaxoIndex handles long andcomplex queries by analyzing one that includes var-ious concepts. For this query, we present retrievalresults: (a) an easy case that is well handled byall baselines (document A), and (b) two difficult cases that are not effectively handled by baselines(documents B and C).The query encompasses various academic con-cepts: generative approaches for creating gamelevels, optimization via reinforcement learning orother differentiable methods, and measuring agentperformance. Document A is ranked at the top-1 byall compared methods due to its high lexical over-lap, directly including terms used in the query (e.g.,GAN, generated levels). In contrast, documents Band C are not retrieved near the top. Unlike docu-ment A, they express the concepts using differentterms (e.g., neuroevolutionary system), making itdifficult to find relevance using surface texts. Ad-ditionally, document C specifically focuses on sur-rogate models for a shooter game, which obscuresthe query concepts like level generation.TaxoIndex infers the most relevant topics andphrases from the query (highlighted in yellow) andincorporates them into relevance prediction. Thishelps to match the underlying academic concepts,improving retrieval results. However, it still showslimited effectiveness for document C, as the overlapof indexed information is relatively small. We alsonote that fine-grained aspects of surrogate modeland character class are not fully included in theindexed information, potentially because there arefewer documents covering such concepts in thecorpus. We expect that incorporating other knowl-edge sources (e.g., knowledge bases) can mitigatethese problems. We leave further exploration for fu-ture work.",
  "Querysemantic parsing learning with limited labels": "Inferred core topics and indicative phrasesparsing, labeled data, statistical parsing, parser combinator, top down parsing, syntactic predicate, text annotation, semi-supervised learning, morphological parsing, natural language processing, artificial intelligence semantic parsing, semantic parsers, taggers, syntactic parsing, parse tree, semantic role labeling, treebanks, predicates, semanticrepresentations, formal semantics, predicate argument, linguistic knowledge, training labels, ...",
  "Querydomain adaptation approach for machine translation": "Inferred core topics and indicative phrasesdomain adaptation, machine translation, semantic textual similarity, semantic translation, translation probabilities, weaklysupervised learning, neural machine translation, natural language processing, machine learning, artificial intelligence neural machine translation, statistical machine translation, nmt, smt, cross sentence, adaptation techniques, domain adaptation,parallel sentences, parallel corpus, out of domain, model adaptation, translators, machine translation, ...",
  "Query": "I am seeking a generative modeling approach capable of creating new levels and potentially game settings/environments for avideo game with multiple existing levels of difficulty. Specifically, I am interested in exploring how Generative AdversarialNetworks (GANs) and other generative methods could generate entirely new levels by emulating the style of previous ones. It iscrucial that the newly generated levels are not merely derivative and that my generative model can optimize specific properties,such as the intensity or graphic nature of the game. Given that these properties are non-differentiable, I need a method to eitherrender them differentiable or employ a reinforcement learning-centric approach to optimize these rewards. After generating avariety of levels, I require a method to select some of the best ones. One potential solution could be to evaluate the generatedlevels using an automatic metric, such as the performance of an AI agent playing the level. Alternatively, I am consideringdesigning a derivative-free stochastic optimization algorithm to guide the search across the space of all synthetically generatedlevels, steering towards those that meet specific objectives. Document A: an easy case (Top-1 by all compared methods)Illuminating mario scenes in the latent space of a generative adversarial network. Generative adversarial networks (GANs)are quickly becoming a ubiquitous approach to procedurally generating video game levels. While GAN generated levelsare stylistically similar to human-authored examples, human designers often want to explore the generative design space ofGANs to extract interesting levels. However, human designers find latent vectors opaque and would rather explore alongdimensions the designer specifies, such as number of enemies or obstacles. We propose using state-of-the-art quality diversityalgorithms designed to optimize continuous spaces, i.e. MAP-Elites with a directional variation operator and CovarianceMatrix Adaptation MAP-Elites, to efficiently explore the latent space of a GAN to extract levels that vary across a set ofspecified gameplay measures. In the benchmark domain of Super Mario Bros, we demonstrate how designers may specifygameplay measures to our system and extract high-quality (playable) levels with a diverse range of level mechanics, while stillmaintaining stylistic similarity to human authored examples. An online user study shows how the different mechanics of theautomatically generated levels affect subjective ratings of their perceived difficulty and appearance.",
  "Document B: a successful difficult case (FFT: top-62, ToTER: top-39, TaxoIndex: top-9)": "Co-generation of game levels and game-playing agents. Open-endedness, primarily studied in the context of artificial life, is theability of systems to generate potentially unbounded ontologies of increasing novelty and complexity. Engineering generativesystems displaying at least some degree of this ability is a goal with clear applications to procedural content generation ingames. The Paired Open-Ended Trailblazer (POET) algorithm, heretofore explored only in a biped walking domain, is acoevolutionary system that simultaneously generates environments and agents that can solve them. This paper introduces aPOET-Inspired Neuroevolutionary System for KreativitY (PINSKY) in games, which co-generates levels for multiple videogames and agents that play them. This system leverages the General Video Game Artificial Intelligence (GVGAI) framework toenable co-generation of levels and agents for the 2D Atari-style games Zelda and Solar Fox. Results demonstrate the ability ofPINSKY to generate curricula of game levels, opening up a promising new avenue for research at the intersection of proceduralcontent generation and artificial life. At the same time, results in these challenging game domains highlight the limitations ofthe current algorithm and opportunities for improvement.",
  "Indexed information (Inferred from the query)": "video game development, heuristics, game design, intelligent agent, genetic algorithm, reward technique, optimization problem,evolutionary algorithm, reinforcement learning, heuristic evaluation, generative model, machine learning, artificial intelligencegame playing, game levels, video games, game design, agents, players, general video game ai, exploration and exploitation,GVGAI, simultaneously learn, optimization, autonomous agents, content generation, knowledge acquisition, rewards, . . .",
  "Document C: an unsuccessful difficult case (FFT: top-258, ToTER: top-155, TaxoIndex: top-108)": "Pairing character classes in a deathmatch shooter game via a deep-learning surrogate model. This paper introduces a surrogatemodel of gameplay that learns the mapping between different game facets, and applies it to a generative system which designsnew content in one of these facets. Focusing on the shooter game genre, the paper explores how deep learning can help build amodel which combines the game level structure and the games character class parameters as input and the gameplay outcomesas output. The model is trained on a large corpus of game data from simulations with artificial agents in random sets of levelsand class parameters. The model is then used to generate classes for specific levels and for a desired game outcome, such asbalanced matches of short duration. Findings in this paper show that the system can be expressive and can generate classes forboth computer generated and human authored levels. Indexed information (Inferred from the query)simulation, surrogate model, game design, game mechanics, modeling and simulation, parameter, video game development,intelligent agent, reinforcement learning, generative model, machine learning, deep learning, artificial intelligence surrogate model, artificial agents, game data corpus, simulation, environments, game levels, game playing, content generation,agents, players, characters, character levels, general video game ai, parameters, game design, video games, generators, ..."
}