{
  "Abstract": "Speech Emotion Captioning (SEC) has gradu-ally become an active research task. The emo-tional content conveyed through human speechare often complex, and classifying them intofixed categories may not be enough to fullycapture speech emotions. Describing speechemotions through natural language may be amore effective approach. However, existingSEC methods often produce hallucinations andlose generalization on unseen speech. To over-come these problems, we propose AlignCap,which Aligning Speech Emotion Captioning toHuman Preferences based on large languagemodel (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the diver-gence between the LLMs response predictiondistributions for speech and text inputs usingknowledge distillation (KD) Regularization. 2)Human Preference Alignment, where we de-sign Preference Optimization (PO) Regulariza-tion to eliminate factuality and faithfulness hal-lucinations. We also extract emotional cluesas a prompt for enriching fine-grained infor-mation under KD-Regularization. Experimentsdemonstrate that AlignCap presents strongerperformance to other state-of-the-art methodson Zero-shot SEC task.",
  "Introduction": "The identification and description of speech emo-tions play a crucial role in improving communica-tion efficiency. It also aids in understanding thespeakers intentions. Previous work usually treatsemotion acquisition as a classification task, suchas Speech Emotion Recognition (SER) (Ye et al.,2023; Chen et al., 2023; Shi et al., 2024), which as-signs speech to different emotion categories basedon the emotions such as sadness, anger, and happi-ness contained within the speech. However, theremay be a mixture of emotions within one utter-ance, and classifying speech into a single emotion",
  ": Hallucination and lack of generalization": "category is not enough to capture the true emotion.Moreover, different annotators may assign differentemotion category labels to a piece of speech, lead-ing to the label ambiguity problem in SER task (Liet al., 2017; Lian et al., 2023b). This can result ininaccurate emotion labels in existing SER datasets.Given the limitations of speech emotion classi-fication, employing natural language descriptionsinstead of emotion category labels is a more ac-curately approach. SECap (Xu et al., 2024) firstproposes a speech emotion captioning frameworkto describe speech emotions using natural languageeffectively. It utilizes HuBERT (Hsu et al., 2021)as an audio encoder to extract speech features whileleveraging mutual information learning to decou-ple content and emotion-related features. (Desh-mukh et al., 2024) employs GPT-2 (Radford et al.,2019) as the decoder to generate captions based onthe pre-trained CLAP (Wu et al., 2023) audio en-coder. (Salewski et al., 2023) exploits OPT (Zhanget al., 2022) as the LLM to produce captions thatdescribe the audio content. However, facing withunseen speech, these methods tend to produce hal-lucinations of factuality and faithfulness, resultingin false emotional descriptions and responses thatare inconsistent with user instructions. In addition,the paradigm of text-only training and zero-shotinference on speech like (Kouzelis and Katsouros,2023) will lead to training-inference mismatch, re-sulting in poor model generalization.In this paper, we propose a novel SEC frame- work AlignCap, which aims to generate rich andcoherent captions while maintaining high con-sistency with speech emotion.We design KD-Regularization, which can minimize the distribu-tion gap between LLMs response to speech inputand those to corresponding text inputs. It bridgesthe training-inference mismatch in zero-shot SECand model generalization is improved. AlignCapis the first to align SEC models to human pref-erences via PO-Regularization, which eliminatesfactuality and faithfulness hallucinations of SECmodels on unseen speech. We also utilize a acous-tic prompt generated from emotional clues to en-rich fine-grained information. To summarize, ourcontributions are as follows:",
  "Background and Discussion": "The section describes the speech-text distributiongap of traditional SEC methods. To explore thisgap, we conducted preliminary experiments to an-alyze its potential impact on train-inference mis-match and performance degradation. Furthermore,we discuss the impact of modal alignment positionon downstream SEC performance.Distribution Gap and Alignment.As thecreation of speech-caption pairs is costly, tradi-tional SEC methods usually are trained usingonly text, and employed zero-shot inference onspeech. The distribution of speech and text em-beddings do not exactly coincide, which degradesthe SECs performance.To analyze the effectof modal alignment on eliminating distributiongap, we adopt No-alignment, Contrastive Learn-ing alignment (CL-Align) (Deshmukh et al., 2024),and Projection-based alignment (CL+Proj-Align)(Deshmukh et al., 2024), and evaluate the per-formance of SEC on BLEU@4 (Papineni et al.,",
  ": Results of different alignment methods": "2002), METEOR (Banerjee and Lavie, 2005), andROUGE (Lin, 2004) metrics. We conducted threeexperiments: 1) No-Align: Speech encoder of Pre-trained CLAP model (Wu et al., 2023) is used tozero-shot inference directly. 2) CL-Align: Wefine-tuning the text encoder and speech encoder ofCLAP using contrastive learning on speech-captionpairs Ds ={(xn, yn)} which are randomly selectedfrom SEC datasets. 3) CL+Proj-Align: Based onCL-Align, we add Projection-based decoding toproject the speech embedding into the text embed-ding space through cosine similarity. As shown inFig 2, captions generated from model with Align-ment exhibit superior similarity compared to thatNo-alignment model. This findings proves that thedistribution gap adversely affects the SECs perfor-mance.Align before or after LLM Decoding? Accord-ing to (Jiang et al., 2023), complete alignment be-tween modalities is often not the optimal solutionfor downstream tasks. Such alignment may resultin information loss, especially when the informa-tion provided by the two modalities differs signif-icantly. Traditional SEC models achieve Speech-Text Alignment via fine-tuning encoder on speech-caption pairs, which bridges the distribution gapbefore LLM decoding. However, complete align-ment of speech and text embedding may result ininformation loss, and it lacks a direct measure forassessing speech-text alignment quality.To address these problems, we propose KD-Regularization which achieve Speech-Text Align-",
  "Update": ": The framework of AlignCap. Left: Illustration of Knowledge Distillation Regularization. Acousticprompt Pact is generated from emotional clues, which is extracted by an emotion grammar parser Gparser. Semanticprompt Psem is generated from LLM tokenizer. Right: Illustration of Preference Optimization Regularization. ment and bridge the distribution gap after LLMdecoding. It use the KL-divergence of next-tokenprediction distributions between LLMs responseas a measure of Speech-Text Alignment. As shownin Fig 3, we observe that align after LLM decodingusing knowledge distillation can more effectivelyimprove the speech-text alignment performance.",
  "KD-Regularization": "Our goal is to generate speech emotion captions forspeech clips, we design a student LLM to imple-ment speech tokens to text generation and employa teacher LLMs response to guide student LLMsnext-token generation. LLaMA-7B (Touvron et al.,2023a) is chosen to implement this decoding pro-cess due to its exceptional language understandingand modeling capabilities. We simply choose rankvalue of 8 for LoRA fne-tuning (Hu et al., 2022b)conducted on Student-LLaMA, while the Teacher-LLaMA parameters are frozen.Acoustic Prompt. We first construct a vocabularyof emotional clues, adjectives such as the speakerstone, intonation, pitch, rhythm, and volume in cap-tions are all regraded as emotional clues. We de-sign an emotion grammar parser (based on NLTKtoolkit) to recognize these clues, which are filteredby the vocabulary. Then these clues are insertedinto a prompt template PT: <Feeling e1, e2, ..., anden>, where en is the nth emotion entity. The acous-tic prompt can capture rich and delicate emotion information in emotional clues. It can enrich fine-grained emotional description and enhance the ro-bustness of zero-shot captioning for unseen speech,leveraging its training-agnostic nature, which isdenoted as:",
  "Pact = Insert(PT, idx, e1n)(1)": "Where yi is a series of captions, cmiis the mthcaption of yi. GParser and PT represent emotiongrammar parser and prompt template respectively.We insert the emotional clues e1n into the indexposition idx of PT to get acoustic prompt Pact.Text Token Generation. We denote the captionsin speech-caption pairs as the semantic promptPsem and concat Pact and Psem as a prefix prompt,then we provide the prefix prompt along with aninstruct prompt (users instructions) to the LLMto condition its subsequent generation of speechemotion captions using prefix language modeling.This setup leverages external knowledge and thelanguage understanding and modeling capabilitiesof the teacher-LLM to guide the student-LLM togenerate plausible sentences.Given a caption ci with token Ti, language modelP learns to reconstruct yi conditioned on the Pactand Psem. The probability of generating the nexttoken is calculated as follows:",
  "p (ci | pn, T0,. . ., Tt1)(3)": "Where prefix prompt pn = Pact Psem Pinstruct.Trained on limited data, simply using semanticprompt as prefix prompt may overfit the In-Domaindataset, leading to significant domain shift and per-formance degradation of language model using out-of-domain (OOD) speech. In contrast, the acousticprompt based on emotion-aware clues, inherits thepowerful transferability from captions.Speech Token Generation.For each speech,we adopt the pre-trained SpeechTokenizer (Zhanget al., 2024) to extract discrete representations anddenote the tokens of the first residual vector quan-tization (RVQ) layer as speech tokens. The firstlayer of RVQ can be regarded as a semantic to-ken, which contains more content information fromspeech, resulting in capturing semantically accu-rate emotional clues. We append this speech tokenxt to LLMs input and generate the next token inan autoregressive modeling manner, for each timestep t, the next token Tt is selected according to:",
  "p (ci | xt, T0,. . ., Tt1)(4)": "Modality Alignment. Modality adapters (Desh-mukh et al., 2024; Hu et al., 2024) are often usedto compress the speech encoders feature represen-tations. Similar to (Yang et al., 2023), we treatthe input from speech and text modality as a to-ken sequence and learn a joint embedding spacefor all modalities. Speech tokens are expandedto text tokens codebook in advance so that textand speech share the same codebook. We pad theshorter token sequence to make it the same lengthas the longer token sequence. We use a mask toignore the padding part, ensuring that the modelonly focuses on valid tokens.Knowledge Distillation. As shown in Fig 4, givena Ds ={(xn, yn)}, we treat the LLMs predictiondistribution p (yn |pn, y<n) of the next responsetoken, after having observed the text input pn andgenerated partial response {y0,. . . ,yn1}, as theteacher distribution. Where pn is the concatena-tion of Pact and Psem. In contrast, we considerthe corresponding distribution p (yn |xn, y<n) for",
  "PO-Regularization": "High-quality emotional description needs to con-sider not only the richness of emotions but alsoaspects such as consistency and rationality. Thealignment of SECs output to human preferencesis often neglected. There is a problem that theLLMs response is inconsistent with the users in-structions (faithfulness hallucination) and resultsin false emotional descriptions (factuality halluci-nation). Therefore, we propose PO-Regularizationto solve these problems.Preference Pairs Creation. Inspired by (Ouyanget al., 2022; Yuan et al., 2024), we construct a pref-erence pairs dataset by utilizing GPT-3.5 scoringprompt Pscore on LLMs beam-search decodingoutput. The Pscore to act as reward model is usedto create preference pairs, which is as follow: Review the users question and the corresponding response using the additive scoring system described below. Points are accumulated based on the satisfaction of each criterion:- Add 1 point if the response is relevant to the instruct prompt and provides some emotional relevant information.- Add another point if the response contains rich emotional descriptions and it seems to have been written from an AI Assistants perspective.- Award a third point if the response contains rich, accurate, and consistent descriptions of emotion and appears to be written from a human subjective perspective, reflecting expert knowledge.",
  ": Scoring prompt for candidate responses": "Following above steps, we can get the prefer-ence pairs dataset Dp = {(xn, ycn, yrn)}Nn=1, whichis consisted of chosen response ycn and rejected re-sponse yrn. Finally, we select the highest score asthe ycn and the rest as yrn.Preference Optimization. To solve the hallucina-tion problem of LLMs, the prevalent RLHF meth-ods (Ouyang et al., 2022; Touvron et al., 2023b; Cui et al., 2023) involve fitting a reward modelon the preference data, and the training the policy,value and critic models to maximize the rewardwithout deviating too far from the reference model.However, RLHF method contains four models andhas too many hyperparameters, making the train-ing complex and high computation cost. Inspiredby DPO (Rafailov et al., 2023; Yuan et al., 2024),We propose a simpler equivalent supervised ap-proach PO-Regularization that addresses this rein-forcement learning goal, the policy model can bedirectly optimized on the reward feedback basedon preference pairs:",
  "Dataset": "We select speech-caption paired samples from thelarge-scale video emotion reason dataset MER2023(Lian et al., 2023a) to form the MER23SEC dataset.A Chinese interactive multimodal emotion cor-pus NNIME (Chou et al., 2017) is used to eval-uate the transferability of our model trained onother datasets. Due to the lack of publicly avail-able high-quality SEC task datasets, we proposea new dataset named EMOSEC1, which is about41 hours of Chinese-English Speech Emotion Cap-tioning datasets. It consists of 15 male and 15female speakers and covers 45039 sentences, witha sampling rate of 16kHz. We divide MER23SEC,EMOSEC, and NNIME datasets into training, vali-dation and testing according to the ratio of 8:1:1.",
  "and AESs respectively. The higher the score, thehigher quality of generated captions": "To evaluate the accuracy of the generated cap-tion, we initially adopt traditional supervised met-rics for the Automated audio captioning (AAC)task, containing standard natural language genera-tion metrics BLEU(B@4), METEOR(M), ROUGE-L(R), CIDEr(C) (Vedantam et al., 2015), andSPICE(S) (Liu et al., 2017). B@4 focuses on theappearance frequency of emotional clues and isused to evaluate the emotional consistency and fine-grainedness of generated captions. Compared withB@4, M considers synonyms more, and R paysmore attention to the sufficiency and faithfulnessof output. C and S Compute accuracy of emotioncaptions using human consensus. Therefore, Mcan be used to evaluate factuality hallucinations,while R, C, and S is used to evaluate faithfulnesshallucinations. Baseline Systems. We compare our model withother systems. 1) HTSAT-BART (Mei et al., 2023):a three-stage processing framework, which per-forms exceptionally well in the AAC task.2)NoAudioCap (Deshmukh et al., 2024): a weakly-supervised audio captioning model which requiresa pre-trained CLAP (Wu et al., 2023). 3) SECap(Xu et al., 2024): the fisrt SEC model to generatehigh-quality speech emotion captions. Training.For KD-Regularization, we optimizethe student-LLM with the AdamW optimizer andthe learning rate of 1e-5 on 4*V100 GPUs over50k iterations, the batch size is 16. We employDeepSpeed (Rajbhandari et al., 2020) and LoRA(Hu et al., 2022a) of rank 8 to implement modelparallelism and parameter equivalence, applyingwarmup with 400 steps and gradient accumulationwith 8 steps. For PO-Regularization, the learningrate is set to 5e-7 and train for 1000 steps.",
  "Main Results": "For Zero-shot scenario, we conduct our modelwith baselines on NNIME (Chou et al., 2017) andEMOSEC dataset. Moreover, we evaluated theeffects of two different Human preference align-ments RLHF-PPO and DPO, on eliminating thehallucinations. Quantitative Evaluation. The objective and au-tomatic evaluation about zero-shot SEC methodsare shown in , and we randomly select 25sentences from test set to calculate scores. Ourproposed preference-optimized models, AlignCap-KD-RLHF and AlignCap-KD-PO, outperform thebaseline model in all metrics. The B@4 and Mof AlignCap-KD-PO is higher than that of SECap-PO, which suggests that KD-Regularization canenhance the accuracy of emotional clues modeling.The highest R, C, and S scores demonstrate thatAlignCaps output exhibits greater sufficiency andfaithfulness compared to other baselines. The met-rics of SECap-PO is higher than that of SECap,it is attributed to the PO-Regularization, whicheliminates the faithfulness hallucinations wherethe output is inconsistent with user instructions.AlignCap-KD-PO achieves the highest B@4 score,demonstrating that emotional clues as Pact can gen-erate more fine-grained emotion captions. It out-performs AlignCap-KD-RLHF, indicating superiorperformance in quantitative evaluation. This con-firms that DPO-based PO-Regularization can en-hance the quality of the caption generated by themodel than RLHF-PPO. It also demonstrates thathuman preference alignment is an effective methodfor the SEC model to undergo self-improvement.Compared with NoAudioCap (Deshmukh et al.,",
  ") which also utilizes a similar text-only": "trainingmethod,bothAlignCap-KD-RLHFandAlignCap-KD-POcomprehensivelysur-pass NoAudioCap, attributed to our proposedKD-Regularizationinalleviatingspeech-textdistribution gap after LLM decoding. Zero-shotinference used by NoAudioCap has a training-inference mismatch, which loses generalizationon unseen speech.The KL-divergence in KD-Regularization is used to bridge the mismatch,it suggests that AlignCap can be generalized tounseen speech. Qualitative Evaluation. supports thefindings of by presenting the output ofAlignCap and HTSAT-BART (Mei et al., 2023),NoAudioCap (Deshmukh et al., 2024), SECap (Xuet al., 2024). Our method can produce richer emo-tional clues and more coherent emotion captions.In the Neutral example of , althoughSECap (Xu et al., 2024) can produce rich speechemotion captions, its incorrect emotional cues areinconsistent with the real emotion. In the Surpriseexample, the output of NoAudioCap lacks fine-grained captions of the speakers gender, tone, andintonation. AlignCap-KD-PO not only makes upfor this shortcoming but also outputs the speakerscontent consistent with the transcribed text, whichenhances the understanding of the speech content. In the Angry example in , AlignCap-KD-RLHF simply refers to gender as \"a person\",AlignCap-KD-PO can correctly identify its gen-der by adopting preference optimization, it is alsoattributed to Pact for enriching fine-grained infor-mation about the speaker.",
  ") suffers from LLMs output inconsistentwith user instructions, and both AlignCap-KD-": "Emotion: NeutralTranscription: No, but I love snowboarding very much.HTSAT-BART: The tone of voice indicates that the person in the audio is emotionally calm.NoAudioCap: The audio is of a man speaking, in a neutral tone, at a normal speed, with no voice changes or emotions.SECap: Based on the voice, it sounds like this person is happy in the audio, and they are expressing excitement or enthusiasm. AlignCap-KD-RLHF: The emotion of the person in the audio is neutral, the tone is normal, and the speaking speed is medium.AlignCap-KD-PO: The man in the audio is neutral. His speaking tone is normal, the intonation is calm, and the speaking speed is medium, saying, \\\"No, but I love snowboarding very much.\\\". Emotion: HappyTranscription: Great ! I want to exchange some yen.HTSAT-BART: A woman is speaking, and her tone sounds positive.NoAudioCap: The audio is of a woman speaking, in a fast pace, with a neutral tone, and no apparent emotion.SECap: Based on the voice, it sounds like this person is fearful in the audio, and possibly sad. They might be expressing a need or desire for something, possibly related to money or finances. AlignCap-KD-RLHF: The person in the audio speaks quickly, with a rising intonation and a positive tone.AlignCap-KD-PO: The woman in the audio is happy and cheerful. She speaks quickly, with a rising intonation and a positive tone.",
  "Emotion: SurpriseTranscription: Tom now let our arrows fly!": "HTSAT-BART: The emotion of the person in the audio is angry. His tone is negative and impatient, and his speaking speed is fast.NoAudioCap: The tone of voice indicates that the person in the audio is emotionally argry.SECap: Based on the voice, it sounds like this person is happy in the audio, and their emotions are: happy. AlignCap-KD-RLHF: The person speaks in a happy tone, with a fast speaking speed. The tone of voice indicates the emotion of the person in the audio is surprise.AlignCap-KD-PO: This is a man speaking, in a positive and happy tone. He speaks English with a fast speaking speed, and the content was: \"Tom now let our arrows fly!\".",
  "Emotion: AngryTranscription: What am I, I'm a racer, son of god!": "HTSAT-BART: This is a man speaking, in an excited tone, with a fast speaking speed.NoAudioCap: The content of voice is: \"What am I? I'm a racer, son of God!\".SECap: Based on the voice, it sounds like this person is angry in the audio, possibly expressing disbelief or frustration. AlignCap-KD-RLHF: The person in the audio is in an angry mood, has an angry tone, and speaks at a normal speed. AlignCap-KD-PO: The man in the audio is angry. The voice is vehement and the tone is excited. He speaks at a normal speed. The content of the speech is: \"What am I? I'm a racer, son of God!\".",
  "Ablation Studies": "As shown in , we train AlignCap with spe-cific components selectively removed to evaluatethe effect of the proposed components to eliminat-ing hallucinations and enrich fine-grained informa-tion.The decrease in all objective evaluation scoresshows the significance of acoustic prompt (Pact),KD-Regularization (LKL), and PO-Regularization(LPO). The significant decrease of Pact on B@4proves the positive effect of the emotional cluesextracted by Pact on the emotional consistency ofgenerated captions. The lack of LKL and LPOleads to a significant drop in M and R, indicatingthat they play a crucial role in guiding Zero-shotSEC model to eliminate factuality and faithfulnesshallucinations. The model without Pact also ex-hibits a decrease in R, indicating its effectivenessin generating fine-grained emotional descriptions.The M score of AlignCap-KD-PO is higher thanAlignCap-KD-RLHF without adopting explicit re-ward modeling, allowing it to learn more human-like generated captions.",
  ": Cross-domain SEC results on NNIME andMER23SEC dataset": "captions to fine-tuning AlignCap.In EMOSECNNIME cross-domain scenarios,the results show that AlignCap outperforms allbaselines. The B@4 and M metrics of SECapand HTSAT-BART are lower than NoAudioCapon the NNIME dataset. This is because they allhave encoder-decoder structures and are trained onwell-paired data, lacking components to enhancegeneralization capabilities for cross-domain data.AlignCap outperforms NoAudioCap, demon-strating the superiority of the proposed KD-basedspeech-text alignment over the CLAP-based (con- trastive learning) speech-text alignment used inNoAudioCap for cross-modal mapping. It not onlybridges the audio-text distribution gap, but also im-proves the generalization ability in cross-domainscenarios.Additionally, there is a domain offset betweenthe predicted emotional description generated byLLMs and the real description of the target domain,leading to performance degradation. Equipped withthe PO-Regularization, AlignCap-KD-PO outper-forms the other baselines including AlignCap-KD-RLHF version on most metrics, demonstrating theeffectiveness of the proposed compoents.",
  "Effect of Different Speech-Text Alignmenton Downstream SEC task": "Previous alignment methods, such as GaussianNoise Injection (CL+NI-Align) (Deshmukh et al.,2024) and Project-based Decoding (CL+Proj-Align) (Kouzelis and Katsouros, 2023), achievealignment by adding Gaussian noise variance ormapping based on contrastive learning betweenspeech and text embeddings before LLM decoding.The KD-Regularization (KD-Align) we proposedachieves speech-text alignment after LLM decod-ing and alleviates the information loss in modalityalignment. Fig 9 shows that our method outper-forms other alignment methods in all indicators,attributing to we treat speech-text alignment as aknowledge distillaiton problem. It can ensure thatthe LLMs responses to speech inputs closely mir-ror those to corresponding text inputs.",
  "Performance on Different Preference PairSizes and Steps": "As shown in Fig 9, we examine the effect of differ-ent preference pair sizes and fine-tuning steps forPO-Regularization on the performance of Align-Cap.We set the preference pair sizes to be{0, 25k, 50k, 75k, 100k}. After 500 steps of fine-tuning with DPO for each of these sizes, we assesstheir performance in zero-shot SEC. We can ob-serve notable improvement with increasing sizes from 0 to 50k, which indicates that an increase inpreference pair data can improve zero-shot SEC.However, using more than 50k preference data forDPO does not lead to significant performance im-provements, indicating a threshold beyond whichadditional data does not enhance learning out-comes.",
  ": Left: Performance of AlignCap across dif-ferent preference pair sizes. Right: Performance ofAlignCap of different fine-tuning steps": "Moreover, we set the fine-tuning steps to be01.5k for AlignCap on zero-shot SEC evaluation.As shown in Fig 9, all metrics demonstrate sig-nificant performance improvement when the num-ber of fine-tuning steps is less than 1k. However,when the number of iterations exceeds 1k steps, themodel suffers from overfitting, resulting in perfor-mance degradation, indicating that 1k steps are theoptimal iteration steps for PO-Regularization.",
  "Can PO-Regularization Works with SmallModels?": "We investigate whether PO-Regularization canbring improvements for smaller language mod-els. The preference pair size is 50k and we fine-tuning the models on EMOSEC dataset for 1k steps.We evaluate the zero-shot SEC performance onEMOSEC test set. Tab 4 shows that after 1k it-erations, PO-Regularization significantly boostsOPTs scores but decreases the GPT2-bases scoreson M and R, while improving GPT-2-large verylittle. This indicates that PO-Regularization canimprove caption generation in small language mod-els, although the improvement is not significant formodels with very small parameters.",
  "Conclusion": "We proposed AlignCap, achieving speech-textalignment and human preference alignment. Tominimize the distribution gap between LLMs re-sponse to speech input and those to correspond-ing text inputs, we design KD-Regularization toachieve speech-text alignment. Additionally, wealign emotion captions to human preference byPO-Regularization. This process eliminates thefactuality and faithfulness hallucinations of Align-Cap on unseen speech. Experiments demonstrateAlignCaps superiority in both zero-shot and cross-domain scenarios.",
  "Limitations": "Well-paired speech-caption datasets are difficult toobtain in real-world scenarios. Captions containingemotional descriptions are easy to obtain, but high-quality speech-caption paired data is difficult tocollect, how to solve this mismatch problem willbe left to our future work. In addition, enhancingthe robustness of alignment between speech andtext inputs remains an urgent issue that needs to beaddressed in the future.",
  "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022b. LoRA: Low-rank adaptation of largelanguage models": "Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen,Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li,Sunit Sivasankaran, Linquan Liu, and Furu Wei. 2024.Wavllm: Towards robust and adaptive speech largelanguage model. CoRR, abs/2404.00656. Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen,Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng,and Trishul Chilimbi. 2023. Understanding and con-structing latent modality structures in multi-modalrepresentation learning. In CVPR, pages 76617671.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In NeurIPS, vol-ume 36, pages 5372853741. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,and Yuxiong He. 2020. Zero: memory optimizationstoward training trillion parameter models. In Pro-ceedings of the International Conference for HighPerformance Computing, Networking, Storage andAnalysis, SC, page 20.",
  "Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In CVPR, pages 45664575": "Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-lor Berg-Kirkpatrick, and Shlomo Dubnov. 2023.Large-scale contrastive language-audio pretrainingwith feature fusion and keyword-to-caption augmen-tation. In ICASSP, pages 15. Yaoxun Xu, Hangting Chen, Jianwei Yu, QiaochuHuang, Zhiyong Wu, Shi-Xiong Zhang, GuangzhiLi, Yi Luo, and Rongzhi Gu. 2024. Secap: Speechemotion captioning with large language model. InAAAI, pages 1932319331."
}