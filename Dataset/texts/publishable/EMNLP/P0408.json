{
  "Abstract": "Despite the longstanding adage an image isworth a thousand words, generating accuratehyper-detailed image descriptions remains un-solved. Trained on short web-scraped image-text, vision-language models often generate in-complete descriptions with visual inconsisten-cies. We address this via a novel data-centricapproach with ImageInWords (IIW), a care-fully designed human-in-the-loop frameworkfor curating hyper-detailed image descriptions.Human evaluations on IIW data show ma-jor gains compared to recent datasets (+66%)and GPT-4V (+48%) across comprehensive-ness, specicity, hallucinations, and more. Wealso show that ne-tuning with IIW data im-proves these metrics by +31% against mod-els trained with prior work, even with only 9ksamples. Lastly, we evaluate IIW models withtext-to-image generation and vision-languagereasoning tasks. Our generated descriptions re-sult in the highest delity images, and boostcompositional reasoning by up to 6% on ARO,SVO-Probes, and Winoground datasets. Werelease the IIW-Eval benchmark with humanjudgement labels, object and image-level anno-tations from our framework, and existing im-age caption datasets enriched via IIW-model.",
  "Introduction": "Todays state-of-the-art Vision-Language Models(VLMs) are trained using large, noisy web datasets.WebImageText (Radford et al., 2021), ALIGN (Jiaet al., 2021), Conceptual Captions (Sharma et al.,2018) and LAION (Schuhmann et al., 2022) relyon alt-text scraped from the internet as an imperfectimage caption. Yet alt-text may only mention thephoto location (e.g. Europe), the camera modelused (e.g. Canon EOS R6 Mark II), or is SEO-specic (e.g., keep calm and carry on). Whiledata ltering and post-processing can remove noisytext, alt-text ambiguously captures image contentor intent (Wikipedia contributors, 2023a). There-fore, only using image descriptions from the web is fundamentally awed and limits model capabili-ties (Thrush et al., 2022; Shekhar et al., 2017; Maet al., 2023; Ray et al., 2023; Hsieh et al., 2024).To curate better image-text data, recent work hasreleased dense human written (DOCCI (Onoe et al.,2024), DCI (Urbanek et al., 2023)) or model gen-erated caption datasets (PixLore (Bonilla, 2023),DAC (Doveh et al., 2023)). Both have limitations,as using annotators without comprehensive guide-lines results in outputs that vary by human atten-tion, bias, and effort (Burghardt et al., 2019; Mar-shall and Shipman, 2013; Pandey et al., 2022; Yeet al., 2023). In contrast, model-generated captionsare cheaper but incomplete and rife with hallucina-tions (Rohrbach et al., 2019; Dai et al., 2023b).In this work, we describe ImageInWords (IIW),a human-in-the-loop framework for curating hyper-detailed image descriptions, and its resulting anno-tations. IIW combines the irreplaceable quality ofhuman annotators with seeded metadata from ma-chine generations. First, a VLM generates granularcaptions for each object in the image to seed ourhuman annotation process, where crowd workersaugment and x the object-level captions to makethem richer and hallucination free.Next, at the image-level, a VLM generates aglobal caption to seed the nal image description.Crowd workers consume the image-level seed cap-tion and object-level human annotations to ll incontextual gaps. We design guidelines to attend toconcepts beyond objects, such as visual perspective,spatial arrangement, and human object interactions.To ensure quality, multiple annotators iterate on asample sequentially and we also incorporate activelearning to produce better VLM seeds ().With this process, we construct the IIW datasetof 9018 hyper-detailed image descriptions. We ndIIW has richer statistics than prior dense descrip-tion datasets, with an average of 217.2 tokens, 52.5nouns, 28 adjectives, 5 adverbs, and 19.1 verbs(Tab. 1). We assess quality with human side-by- : ImageInWords Seeded Annotation Framework. Humans enrich and rene outputs sequentially, buildingon prior human or machine inputs. Human annotation starts with ne-grained object captions in Task 1, which areused to compose image-level descriptions in Task 2. VLMs are updated in an active learning loop to produce betterobject and image-level seeds as annotated data becomes available. UI screenshots are in Appendix B.4. side (SxS) comparisons to human-written datasets(DCI, DOCCI) and GPT-4V. Our descriptions arerated as more comprehensive, specic, human-like,with fewer hallucinations and better leading sen-tences at an average of +66% (DCI, DOCCI) and+48% (GPT-4V). We then ne-tune with IIW dataand evaluate generated descriptions with the sameSxS rubric: IIW model outputs are better by +31%compared to models ne-tuned on prior work.To better understand IIW models, we also per-form text-to-image generation and vision-languagereasoning experiments. Images generated with ourmodels descriptions are considered a closer re-construction to the original image than when us-ing other models. For vision-language composi-tionality, we replace images from ARO (Yuksek-gonul et al., 2023), SVO-Probes (Hendricks andNematzadeh, 2021) and Winoground (Thrush et al.,2022) datasets with generated descriptions. IIWmodel descriptions help to better reason over at-tributes, relations, and word order compared toLLaVA-v1.5 and InstructBLIP descriptions.In summary, our contributions include:",
  "Human SxS on comprehensiveness, specicity,": "hallucinations, human-likeness, and tldr-quality.Across these metrics, IIW data is better thanrecent DCI and DOCCI datasets by +66% and+48% better than GPT-4v, and +31% better whenused for ne-tuning than DCI and DOCCI. IIW model evaluations with text-to-image gener-ation and vision-language compositional reason-ing tasks to complement human SxS. IIW modeldescriptions generate images most similar to theoriginal image (ranked 1st) and improve distin-guishing true image-text pairs given attribute, re-lation, or word order differences by up to 6%. An open source IIW-Eval benchmark of humanand model annotations over 2.6k images and theirimage descriptions, and 1.9k object descriptions.We also release human SxS labels between IIW,DCI, and DOCCI for comparison in future work.",
  "Related Work": "Image captioning has been studied for years, start-ing with CNN and LSTM encoder-decoder frame-works for generic captions (Vinyals et al., 2015; An-derson et al., 2018), to the more recent Transformer-based VLMs for more difcult captions (Chenet al., 2023b; Li et al., 2023) (e.g., VizWiz (Gu-rari et al., 2020), NoCaps (Agrawal et al., 2019),TextCaps (Sidorov et al., 2020)). These datasetsand many others contain captions with 15 words or",
  "Count/ Sentence/ Description": "SVP (Krause et al., 2017)19,56111.968.55.717.16.71.15.0LocNar (Pont-Tuset et al., 2020) 873,10715.741.02.610.71.60.43.5DCIextra1 (Urbanek et al., 2023)7,80515.8148.09.335.316.33.610.5DOCCI (Onoe et al., 2024)14,64719.2135.77.134.016.62.79.6IIW (ours)9,01822.1217.29.852.528.05.019.1 : Dataset Statistics Comparing ImageInWords (IIW) to Prior Work. We include the number of descriptionsand the average number of tokens, sentences, nouns (NN), adjectives (ADJ), adverbs (ADV), and verbs (VB). fewer (Desai et al., 2021; Young et al., 2014; Linet al., 2015; Mao et al., 2016; Plummer et al., 2015;Kazemzadeh et al., 2014; Krishna et al., 2016;Plummer et al., 2015) and may differ by captiongrounding level (e.g. whole image or region-levelcaptions) or image domain (e.g. images taken bypeople who are blind or images capturing text). However, few dense image description datasetsexist. PixLore (Bonilla, 2023) used multiple vision-language datasets to generate verbose captions withBLIP-2 (Li et al., 2023). DAC (Doveh et al., 2023)uses a machine-generated approach: pretrainedLLMs expand the original image caption and pre-trained VLMs generate captions over smaller im-age regions. The resulting descriptions are used tone-tune a VLM model for better compositionalreasoning. While model-only approaches are costeffective and avoid the challenges of designing an-notation instructions, they risk introducing halluci-nations and systematic biases. DOCCI (Onoe et al., 2024) collects image de-scriptions with only crowd workers, which we latershow can be considerably improved. Closest to IIWis DCI (Urbanek et al., 2023), which uses humanannotators to reach denser descriptions. DCI usesthe SAM (Kirillov et al., 2023) object detector togenerate smaller regions to be described and thencomposes them into an overall description. DCIs available annotations and metadata canbe concatenated with additional text to reach 1k+length. However, ller text and image labels areused to reach this length, and repeated or highlyoverlapping sentences are often present. As a result,we use their extra_caption eld for fair compari-son as it is the only coherent description available.In contrast to DCI, we also allow crowd workers toupdate or correct every component of the seededinformation. IIW output is then sequentially re-ned over multiple annotation rounds to produce asingle coherent annotation. In comparison to DCIsextra_caption annotation, we collect signicantly",
  "ImageInWords Dataset Collection": "The IIW dataset is composed of 9018 (Train: 8573,Test: 445) images that are sampled from a We-bLI (Chen et al., 2023b) like dataset and humanannotated. Details on the human annotator pool areprovided in Appendix B.1. In 3.1, we briey reviewour foundational guidelines for crowd workers. An-notation methodology and the types of image-textannotations we collect are described in 3.2 and 3.3.",
  "Annotation Guidelines": "We compile an extensive set of guidelines for hu-man annotators and iterate over them with multiplepilot rounds. Appendix A contains the complete setof guidelines due to space. Annotators are asked toonly include details that can be deduced from vi-sual cues, erring on the side of higher precision. Tocompose coherent descriptions, unnecessary frag-mentation of sentences and the use of ller phraseslike in this image, we can see, and there is a,should be avoided since they add no visual detail.While describing the overall image, we instructannotators to start with a newspaper style TLDR(Too Long Didnt Read; meant to serve as a suc-cinct summary). Objects should be described inthe order of their saliency, noting objects and rela-tionships in a well organized manner. Descriptionsshould include the overall setting, background, andstyle, considering the camera angle, overall compo-sition, and rendered text. We also ask to pay spe-cial attention to people, apparel, art pieces, locale-specic, and unique attributes with the following asexample features: function, shape, size, color, de-sign, pattern, texture, material, condition, opacity,orientation, location, relationship to other compo-nents/objects, and text written on objects.",
  "Annotation Round": "(b) Time(sec) perAnnotation Round (1,3)(1,2)(2,3) (c) Jaccard-Similarity b/w Annotation Rounds in the Beginning 0.2 0.4 0.6 0.8 1.0 : Effects of Sequential Annotation: Over annotation rounds, (a) token count goes up as (b) time spentgoes down with (c) higher agreement, measured by Jaccard Similarity (Wikipedia contributors, 2024). (d) Overtime with a constant human annotator pool, each learns from the other via an implicit feedback loop and a highagreement rate in round (1,2) can now be observed as was previously only seen in round (2,3) in (c). highlight that IIW data is meant for supervised ne-tuning rather than pretraining. As a result, our goalwas to annotate a small-scale, high quality dataset.Still, we designed the human-in-the-loop process tobe as efcient and exible as possible. The numberof sequential annotators and the presence of Task 1can be adjusted as time and budget permit.Seeded Annotation Describing images in detail ishighly subjective and complicated. To expedite hu-man annotation, we use PaLI-3 5B outputs to seedthe annotation process instead of crowd workersstarting from scratch. While VLMs have improvedin their ability to capture image details, attemptsto generate a consistent rich output still fall preyto hallucinations and recall issues. Our human an-notation pipeline ensures that VLM hallucinationscan be corrected and missing details lled in.An initial machine generated caption and highprecision, domain specic metadata (e.g., art styleor title of a painting) provide a minimal qualityand coverage guarantee. As data is collected, theVLMs used for seeding are updated to producebetter quality descriptions in an active learning loop(reected with loops in ). After batchesof 1k samples are annotated, we retrain (i.e., re-ne-tune) the PaLI-3 5B models with all availableannotations (for both Task 1 and Task 2).We nd that these updates signicantly improvethe baseline model, with early batches shifting PaLIcaptions from an average of 15 to 150+ words withas few as 3k samples. We do not yet perform spe-cialized sampling for active learning due to thelarge performance gap between the ImageInWordshuman annotations and ImageInWords model (aslater shown in Tab. 8). However, this could beincorporated in the future if performance saturates.Sequential Augmentation We further improveframework efciency with sequential descriptionaugmentations. Humans augment a previous crowdworkers and/or VLMs outputs instead of startingfrom scratch. After the rst augmentation, both the machine-generated seed and prior human annota-tion are provided. The following annotators do notknow which is model output versus human written,which can mitigate preference to model outputs.During the annotation process, it is far more ef-fective in time and quality to read and augmentimage descriptions: in we see that if an-notations were done in parallel, we would have3 competing outputs per image, each with theirown style, perspective, and weaknesses, with eachcontaining 170 words and taking 800 seconds.Whereas, in the sequential process, we get a sin-gle all-inclusive description that has been veriedand augmented by three humans with +20% tokencount in -30% time. Higher Jaccard similarity overrounds suggests a higher inter-annotator agreement,which also serves as a proxy for quality.Finally, our framework has an implicit human-to-human learning loop, as each human annotatorhas the opportunity to read and learn from otherperspectives across the annotation rounds, leadingto improved individual quality. This is seen in the2x improved inter-annotator agreement betweenrounds (1, 2) when comparing (c) and (d) in .",
  "Annotation Framework": "Based on the above guidelines, we present the IIWframework for annotating images across two tasks.The tasks are seeded from VLMs or prior humanannotations (), where each can have multipleannotation rounds. Examples are in Appendix B.4. Task 1: Object-Level Descriptions Similar to Vi-sual Genome (Krishna et al., 2016), we design thisannotation task to capture a (label, bounding box,object description) triplet per salient image object.An objects label is open vocabulary with no ver-bosity restrictions, and its description is focused onthe object but additionally takes the context of theimage into account. The bounding box localizeswhere the object is in the image ( (left)). Toseed the data, we rst used an internal object detec- : IIW Annotation Tasks. Objects and their attributes are rst individually annotated to note the salientobjects and focus on coverage of their attributes in Task 1. These outputs, along with a seed VLM caption, arepassed to humans to build the initial image-level description. The initial caption is then human augmented andrened in N sequential rounds to attain the nal hyper-detailed description in Task 2. tion (OD) model to obtain a list of (label, boundingbox) pairs. Then, object captions are generated bycropping the image to the object bounding box andgenerating a caption via a periodically ne-tunedPaLI-3 5B. Our methodology is agnostic to whichVLM, OD (or image-segmentation) model is used.From the seed list of (label, bounding box, objectcaption), the annotators are rst asked to determinethe salient objects and x the list of (label, bound-ing box) by editing, removing, adding or mergingthe object annotations based on their accuracy, im-portance, and role in the overall image. By limitingthe scope to individual objects, annotators can bet-ter focus and capture details comprehensively. Task 2: Image-Level Descriptions Our secondannotation task is to form the nal hyper-detaileddescription. Task-1 outputs, optional domain spe-cic metadata (e.g., art style of a painting), and aVLM seed caption are used to hint and help theannotators compose the overall image description.The bulk of the annotation responsibility falls onthe rst annotator; note that crowd worker anno-tation order is randomly assigned per sample andthe same annotator is not re-employed for the samesample. This output is then rened and augmentedin sequential rounds to mitigate subjectivity andquality drops. Annotators are encouraged to focuson augmentation and only remove things if they areobvious errors, but are free to re-frame informationto add new details. We started with 3 annotationrounds and monitored the n-gram Jaccard similarity between the outputs. Once a 0.8 round-over-roundoutput similarity was achieved, we reduced thenumbers of rounds. Optionally, early stopping sup-port could be added to the annotation frameworkitself to make this instance specic. Over time, wefound our similarity threshold can be met betweenthe rst two rounds, i.e., (1,2), () suggestingimproved and high individual-annotator quality.",
  "IIW Human-Authored Data Eval": "To evaluate the IIW annotation framework and re-sulting human annotations, we start with humanSxS evaluations to compare our human annotationsto prior work (e.g. DCI, DOCCI, GPT-4V). To runa SxS experiment on human-authored descriptionquality, we rst need a common pool of human an-notated images. For this, we additionally annotatethe DCI test set (112) and a comparable number ofsamples (100) from the DOCCI test set with ourIIW annotation framework. We thus have human-authored IIW annotations for direct comparison onimages in the DCI and DOCCI datasets, which con-tribute to our open-source IIW-Eval benchmark.Our human SxS framework evaluates 5 met-rics: Comprehensiveness, Specicity, Hallucina-tions, quality of the rst few line(s) as a TLDR(Too Long Didnt Read; meant to serve as a suc-cinct summary), and Human-Likeness. Compre-hensiveness concerns whether a description coversall key information and objects present in an image.Specicity is the degree of detail in which each of",
  "C37 19 30 414638 33 19S53420 6832822 65H23 48 32 1501241 34 13Tldr30320 741411 30 54HL11 14 25 591030 46 23": ": Human SxS to Evaluate IIW Human-AuthoredData. We report percentages comparing data from priorwork with data annotated by the IIW framework onComprehensiveness (C), Specicity (S), Hallucinations(H), TLDR-quality, and Human-Likeness (HL). the key objects and details are described in.We also include TLDR quality as one of our met-rics as initial sentences set a precedence for whatdetails to expect, both for the reader and modelstrained on this data. From a practical perspective,we would like hyper-detailed descriptions to stillbe useful in a setting that is constrained by inputtext length; i.e., if we truncate an image descrip-tion, it should contain the most salient informationfor vision-language training. While IIW guidelinesinstruct annotators to include a rst sentence whichprovides an overall summary of the image content,prior work also designed their descriptions to startwith either a short caption that summarizes the fullimage (Urbanek et al., 2023) or have important in-formation covered in earlier sentences (Onoe et al.,2024). As a result, we believe the TLDR metric isreasonable and should be an established practicefor hyper-detailed descriptions moving forward.The evaluation is done on a 5 point scale denedusing substantially better (+ +) or marginally bet-ter (+) ratings on both sides of a neutral (-).Higher numbers indicate higher quality across eachmetric, and our tables report percentages for easeof comparison. We emphasize that this is an ex-tremely challenging human annotation task, whereper image, two text pieces of 100+ words need tobe evaluated across 5 metrics in a SxS setting. Onaverage, we observe each comparison takes 15-20minutes. Details on the annotation setup and UIare in Appendix B.4.",
  "We use the extra_caption eld of DCI annotations and dis-cuss this in choice in . All following DCI referencesrefer to the extra_caption description": "paring IIW to DCI and DOCCI, Comprehensive-ness is higher by +61% and +42%, Specicity by+80% and +82%, Hallucinations are lower by 42%and 35%, TLDR quality is higher by +91% and+79%, and Human-Likeness improves by +82%and +68%, respectively. This indicates that theIIW human-authored image descriptions on imagesfrom DCI and DOCCI are considerably better thanthose originally published with prior work.To further quantify the quality of IIW human an-notations, we compare with GPT-4V outputs (Ope-nAI, 2023) in Tab. 3 (right).We use GPT-4Vto generate image descriptions on 100 IIW-Evalimages. The descriptions are generated with theprompt Generate a detailed image descriptionand no other specications. The results from theModel-Human section of Tab. 3 show that we reachComprehensiveness (+35%), Specicity (+53%),Hallucination (+59%), TLDR (+70%), and Human-Likeness (+21%) improvements over GPT-4V out-puts. Although GPT-4V performs relatively betterthan the human-authored DCI and DOCCI datawhen compared to IIW annotations, we assess thatconsiderable future modeling efforts are needed forVLMs to reach IIW human-authored data quality.",
  "IIW Model Evaluation": "After evaluating IIW human annotations, we turnto quantifying the impact of ne-tuning with IIWdata versus ne-tuning with prior work. We ne-tune separate PaLI-3 5B models on DCI, DOCCIand IIW training splits, with their detailed human-authored text as target. Each model is trained withan identical setup (40 epochs, learning rate 3e-4,batch size 32) and the generic input instruction:Generate a detailed image description. More ne-tuning details are provided in Appendix C and D.As shown in prior work, existing text similar-ity metrics like BLEU (Papineni et al., 2002) andROUGE (Lin, 2004) have been shown to poorlycorrelate with human judgement as they are heavilydependent on n-gram overlaps, and thus ill-suitedfor long texts (Kryscinski et al., 2019; Caglayanet al., 2020). Prior works DAC, DCI, and DOCCIalso are limited by existing image caption met-rics, and use LLM summaries of their descriptionsor human SxS for evaluation. We report BLEU,ROUGE, CIDEr, BERTScore (Zhang et al., 2020),and BLEURT (Pu et al., 2021) in Appendix D.5 butlook to human SxS for more accurate judgements.We also quantify the richness of the IIW model",
  ": Human SxS on Model Predictions. Model Generated compares PaLI-5B ne-tuned with IIW versus priorwork DCI and DOCCI and GPT-4V outputs. Model-Human compares GPT-4V model to IIW human-annotations": "outputs via two downstream evaluations which canhelp us to evaluate IIW model generated descrip-tions in the absence of better metrics. First, in 5.2,we use generated descriptions from DCI, DOCCI,and IIW ne-tuned models to prompt a Text-to-Image (T2I) model for image reconstruction andevaluate which descriptions result in higher delitygenerated images. Then, in 5.3, we quantitativelyshow how IIW models can generate descriptions toaid in vision-language reasoning.",
  "Human SxS Results": "Our rst evaluation uses the same human SxS setupas in . We evaluate the IIW, DCI, andDOCCI ne-tuned models on a random sample ofLocNar Eval images, which can serve as an un-seen test set for each ne-tuning dataset. The re-sults mirror Tab. 2s human-authored statistics: IIWhas gains over (DCI, DOCCI) datasets on Compre-hensiveness (+42, +4)%, Specicity (+54, +37)%,TLDR (+51, +57)% and Human-Likeness (+55,+23)% with a relatively small hallucination trade-off (-9, -7)%, largely dominated by marginal ratedlosses. Overall, compared to DCI and DOCCI, IIWmodel-generated outputs show a higher averagepreference from human judgement by +31%.From Tab. 3 (middle), we see that the IIW PaLI-5B ne-tuned model has clear room for improve-ment compared to GPT-4V, as expected given its5B size. It is worth noting that it competes well onthe Human-Likeness writing-style metric, and ac-tually excels at learning the TLDR concept, whichwe built as a distinct feature of our dataset.",
  ": T2I Reconstruction from Image Descriptions.The original image is compared to images generatedfrom cumulative sentence inputs on relative (MeanRank) and absolute (CLIP image similarity) metrics": "generate descriptions on 240 images from the Loc-Nar eval set. We then split each image descriptioninto sentences as units which are fed as cumula-tive inputs (i.e., sentence 1, sentence 1-2, sentence1-3...) to an Imagen model variant (Saharia et al.,2022). By breaking up the description into sentencechunks, we aim to study IIWs salient descriptionstyle and also debias our results from descriptionlength. We evaluate 1k generated images acrossthe varied input sentence chunks (over 240 randomLocNar images) with a 3-way human ranking eval-uation and CLIP similarity between the originaland reconstructed image (Radford et al., 2021).The results in Tab. 4 indicate that IIWs detailedoutputs consistently lead to better T2I reconstruc-tion, with highest mean rank and CLIP similarityregardless of the length of input units. These re-sults conrm that IIW descriptions capture the mostvisual content with the most detail, and that it isnot strictly due to description length, but ratherthe saliency, comprehensiveness, and specicity ineach sentence that makes IIW impactful. As inputtext length is still a limitation in popular VLMs likeCLIP, these results provide evidence that using onlythe rst sentence of IIW descriptions can still be : Example T2I Outputs and Human Rankings. We show an example output when the rst sentence of theimage description from DCI, DOCCI and IIW PaLI-5B ne-tuned models are fed as input to the same T2I model.",
  "Compositional Reasoning with IIW": "We look to a second downstream evaluation toquantify the impact of our hyper-detailed imagedescriptions. Specically, we use IIW generated de-scriptions to aid in vision-language compositionalreasoning. Probing datasets ARO (Yarom et al.,2023), SVO-Probes (Hendricks and Nematzadeh,2021), and Winoground (Thrush et al., 2022) mod-ify image captions to no longer match the pairedimage2: changing visual attributes or relationships,swapping verbs, or shufing image captions suchthat they contain the same words but reect differ-ent semantics. This is done to evaluate differenttypes of vision-language reasoning, e.g., visual at-tribute understanding or verb understanding.In this experiment we evaluate if IIW descrip-tions can be used to distinguish the real image cap-tion from the incorrect negative caption in ARO,SVO-Probes, and Winoground datasets using anLLM-only setup. We prompt PaLM2-340B (Anilet al., 2023) to select which of the caption options istrue given the image description (see Appendix D.8for exact input prompts). This essentially replacesthe image in these datasets with a generated de-",
  "None56.50 59.9450.7149.88InstructBLIP-7B 83.99 62.7389.3565.25LLaVA-V1.5-7B 84.80 63.7187.8963.38IIW PaLI-3 5B90.37 66.1988.6669.38": ": Vision-Language Compositional ReasoningAccuracy with Image Descriptions. We see if richerIIW descriptions can help distinguish the true match-ing image caption in ARO (Yuksekgonul et al., 2023),SVO-Probes (Hendricks and Nematzadeh, 2021), andWinoground datasets (Thrush et al., 2022). COCO andFlickr30k Order subsets of ARO are not reported dueto a very high language bias baseline of 98%. scription; the amount the description is able toboost accuracy on these compositional reasoningtests should correlate to the descriptions compre-hensiveness and specicity. We compare IIW ne-tuned models to two larger (7B) open source mod-els: InstructBLIP-Vicuna-7B (Dai et al., 2023a)and LLaVA-V1.5-7B (Liu et al., 2023) in Tab. 5,with additional models in Appendix D.8.Our rst baseline is the no-image condition(None in the rst row of Tab. 5), which simplyasks an LLM which image caption is more likely.This serves an important language-bias baseline,and quanties whether the vision-language compo-sitional reasoning task really requires vision at all.Our results show that SVO-Probes and Winogroundhave the lowest language bias (baseline performsnearly at random). On the other hand, ARO vi-sual genome attribution and relation subsets are not",
  ": IIW-Eval Data and Annotation Breakdown": "quite at random baseline; we also note that we donot include the Flickr30k nor COCO order AROsubsets, as the LLM can distinguish the true captionat 98% accuracy without any image description.When incorporating image descriptions, all mod-els perform signicantly better than the language-bias baseline. The IIW model results in the besttask performance for ARO Visual Genome Attribu-tion and Relation (VG-A, VG-R) and Winoground,with accuracy gains of nearly 34%, 6%, and 20%,respectively. Moreover, we can further boost perfor-mance compared to the InstructBLIP and LLaVAimage captions: we improve reasoning accuracy byabout 6%, 2%, and 4% compared to the best imagedescription model-based baseline. This reects therichness of IIW across different parts of speech andcomprehensiveness, as more attributes and relation-ships are captured and can be used to reason aboutimage content. For SVO-Probes, we nd smallerdifferences, with IIW, InstructBLIP, and LLaVAmodels within 1 point of each other.",
  "IIW-Eval Benchmark Release": "We release the IIW-Eval benchmark (Tab. 6) ofhuman- and model-annotated image descriptions,human SxS results on Human-Human and Model-Human pairs of descriptions. IIW-400 is a neweval set of 400 images randomly sampled fromDOCCI-AAR (Onoe et al., 2024). We re-annotateDCI and DOCCI test samples and enrich two ex-isting datasets with new IIW descriptions: Local-ized Narratives (LocNar (Pont-Tuset et al., 2020))and CrossModal-3600 (XM3600 (Thapliyal et al.,2022)). We provide LocNar and XM3600 annota-tions with signicantly improved quality (see statis-tics in Appendix E). The model generated descrip-tions may have hallucinations, information recalllosses, or non-human like writing style artifacts.By releasing this subset along with human SxSjudgements, we encourage the development of new",
  "Future Work": "In future work, robust and effective automatic met-rics are needed to evaluate the quality of detailedimage descriptions. Next steps may include train-ing model-based metrics or preference models (i.e.,autoraters) with human preference data to learn aglobal quality metric. For additional analysis, wecould further break down our current SxS metrics.For example, the human SxS hallucination met-ric could be broken down to capture ne-grainedcategories like how many hallucinations are withrespect to color, size, or spatial location.We are working to extend the ImageInWordsframework to additional languages and geograph-ically diverse images. In next steps, we note thatimages need to be sampled globally (across bothgeographic and cultural identity); this samplingmust also be done across different image topicsand categories, making equal coverage more com-plicated. We are currently working on adaptingour proposed framework to accommodate localespecic annotators, which are required for culturalspecicity. Our continued goal is to make the an-notation guidelines holistic, reduce human effortand dependency in the annotation process, and helpshift the narrative from captions to descriptions.",
  "Conclusion": "In this work, we proposed ImageInWords (IIW),a new framework for hyper-detailed image de-scriptions. Our annotation guidelines and seeded,sequential annotation process lead to human au-thored descriptions that are strongly preferred overboth prior works human annotations (+66%) andprior works ne-tuned models (+31%). Images re-constructed with IIW generated descriptions wereranked 1st more often, regardless of how much ofthe image description was used, reecting highersaliency earlier and better overall quality. Our com-positional reasoning evaluation showed IIW gener-ated descriptions to best contain ne-grained visualdetail needed to decipher true from false visual at-tributes and semantics, with accuracy gains of upto 6% over our most performant baselines. Our re-sults collectively demonstrate the quality and utilityof IIW image descriptions as state-of-the-art.",
  "Limitations": "Finally, we discuss the limitations of our annota-tion framework and evaluations. In our annotationframework, we dene a seeded and sequential anno-tation process, with both aspects having potentiallimitations. The quality of the seeded data is ofhigh importance as it will ultimately affect the restof our human annotation pipeline. Additionally,even with the best possible seeds, they may limitthe scope of what our crowd workers write by bi-asing them towards certain objects or phrases. Weemployed an active learning loop to iteratively im-prove the seed generation quality but signicantroom for improvement still remains. In terms oflimitations for the sequential augmentation used,unnecessary time may be spent by annotators if therst annotator output quality is low. By trainingthe annotators through guidelines and feedback andmonitoring the initially drafted descriptions, qual-ity can be better ensured so that the framework isas efcient as possible.With respect to the evaluation of our human an-notated data and model generated outputs, we doonly perform evaluations on hundreds of samples(as opposed to thousands or more). This is largelydue to the cost and time associated with human SxSevaluations for this task, but we note that IIW israted marginally and substantially better at a muchhigher rate, which would likely scale to more sam-ples. Our work is also inherently limited by thelack of automated metrics available for long de-scriptions. We still report standard text similaritymetrics in Appendix D.5 and complement themwith human SxS, but in future we hope metricsare developed that address the current limitations,as automated metrics can be applied at scale. Wenote that metric limitations were also faced in priorwork, with others opting to use LLM summaries orhuman SxS for evaluation purposes (Urbanek et al.,2023; Onoe et al., 2024).With respect to our trained IIW models, wealso note that all results are reported from a sin-gle model/run for each evaluation included. In thefuture, rerunning models with different seeds oraggregating results over different model variantswould be benecial.While we currently do not plan to open sourceour models or training set, we do release an eval-uation set over images that can serve as a uniedbenchmark for IIW, recent, and future related work.We also open source the human SxS judgements and model enriched samples from Localized Nar-ratives and XM3600. We acknowledge that thefull annotation framework would take substantialtime and effort to rerun from scratch; this is in partdue to needing to reproduce the annotation UI andinfrastructure for seeding. The framework itselfis agnostic to which vision-language models areused for seeding of initial object or image captions,which we hope makes the setup more feasible toreproduce with any open source model of choice.This also becomes increasingly important as newand improved models will continue to be devel-oped, and wed like our framework to be able toincorporate newer models over time. The numberof annotation rounds, annotation volume, and par-ticular set of images can be adjusted to specicuse-cases and budget and time constraints.Lastly, our initial IIW dataset and resulting mod-els are English-only. In the future, we plan toexpand our work to have multilingual and multi-cultural coverage over images sampled globally.We also aim to curate images descriptions whichare annotated by locale specic annotators to cap-ture regional and cultural nuances, so that we donot strictly have descriptions with a western lens.",
  "Ethics Statement": "Our model may have broader societal impact. Itmay contain unknown biases or stereotypes, orpropagate inaccurate or otherwise distorted infor-mation. We used a combination of algorithmicmethods, manual inspection, and other classiersfor identifying and removing Sensitive PersonallyIdentiable Information, pornographic, and vio-lence depicting images. Specically we checkedfor the presence of: (1) any address, email, orphone number; (2) images with high porn scores;(3) images labeled as portraying abuse; (4) textidentied as having certain adult content references.Additionally, we asked human annotators to use anobjective and respectful tone while composing theimage descriptions. While we made all of theseefforts, it is still possible the model may producesome undesirable results.Additionally, image to text VLMs inherently canhave negative impact if the generated image de-scriptions are inaccurate and/or contain hallucina-tions. However, our work specically aims to coverall visual content as comprehensively and accu-rately as possible to improve data quality and theresulting ne-tuned models. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,Rishabh Jain, Mark Johnson, Dhruv Batra, DeviParikh, Stefan Lee, and Peter Anderson. 2019. no-caps: novel object captioning at scale. In Proceed-ings of the IEEE International Conference on Com-puter Vision, pages 89488957. Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and LeiZhang. 2018.Bottom-up and top-down attentionfor image captioning and visual question answering.Preprint, arXiv:1707.07998. Rohan Anil, Andrew M. Dai, Orhan Firat, MelvinJohnson, Dmitry Lepikhin, Alexandre Passos, Sia-mak Shakeri, Emanuel Taropa, Paige Bailey, andZhifeng Chen et al. 2023. Palm 2 technical report.Preprint, arXiv:2305.10403.",
  "Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet,and Geoffrey Hinton. 2022. Pix2seq: A languagemodeling framework for object detection. Preprint,arXiv:2109.10852": "Xi Chen,Xiao Wang,Lucas Beyer,AlexanderKolesnikov, Jialin Wu, Paul Voigtlaender, BasilMustafa, Sebastian Goodman, Ibrahim Alabdul-mohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,Daniel Vlasic, Filip Pavetic, Keran Rong, TianliYu, Daniel Keysers, Xiaohua Zhai, and Radu Sori-cut. 2023a. Pali-3 vision language models: Smaller,faster, stronger. Preprint, arXiv:2310.09199. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-giovanni, Piotr Padlewski, Daniel Salz, Sebas-tian Goodman, Adam Grycner, Basil Mustafa, Lu-cas Beyer, Alexander Kolesnikov, Joan Puigcerver,Nan Ding, Keran Rong, Hassan Akbari, GauravMishra, Linting Xue, Ashish Thapliyal, James Brad-bury, Weicheng Kuo, Mojtaba Seyedhosseini, ChaoJia, Burcu Karagol Ayan, Carlos Riquelme, An-dreas Steiner, Anelia Angelova, Xiaohua Zhai, NeilHoulsby, and Radu Soricut. 2023b.Pali:Ajointly-scaled multilingual language-image model.Preprint, arXiv:2209.06794. Wenliang Dai,Junnan Li,Dongxu Li,AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi.2023a.Instructblip:Towards general-purposevision-language models with instruction tuning.Preprint, arXiv:2305.06500.",
  "Matthew Honnibal, Ines Montani, Soe Van Lan-deghem,and Adriane Boyd. 2020.spaCy:Industrial-strength Natural Language Processing inPython": "Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Anirud-dha Kembhavi, and Ranjay Krishna. 2024.Sug-arcrepe:Fixing hackable benchmarks for vision-language compositionality. Advances in Neural In-formation Processing Systems, 36. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, ZaranaParekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung,Zhen Li, and Tom Duerig. 2021. Scaling up visualand vision-language representation learning withnoisy text supervision. Preprint, arXiv:2102.05918. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara Berg. 2014.Referitgame: Referringto objects in photographs of natural scenes. In Pro-ceedings of the 2014 conference on empirical meth-ods in natural language processing (EMNLP), pages787798. Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C. Berg, Wan-YenLo, Piotr Dollr, and Ross Girshick. 2023. Segmentanything. Preprint, arXiv:2304.02643.",
  "Haotian Liu,Chunyuan Li,Qingyang Wu,andYong Jae Lee. 2023.Visual instruction tuning.Preprint, arXiv:2304.08485": "Zixian Ma, Jerry Hong, Mustafa Omer Gul, MonaGandhi, Irena Gao, and Ranjay Krishna. 2023.Crepe: Can vision-language foundation models rea-son compositionally?In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 1091010921. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L Yuille, and Kevin Murphy. 2016.Generation and comprehension of unambiguous ob-ject descriptions. In Proceedings of the IEEE con-ference on computer vision and pattern recognition,pages 1120. Catherine Marshall and Frank Shipman. 2013. Experi-ences surveying the crowd: Reections on methods,participation, and reliability. In Proceedings of the3rd Annual ACM Web Science Conference, WebSci2013, pages 234243. Yasumasa Onoe, Sunayana Rane, Zachary Berger,Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexan-der Ku, Zarana Parekh, Jordi Pont-Tuset, Gar-rett Tanzer, Su Wang, and Jason Baldridge. 2024.DOCCI: Descriptions of connected and contrastingimages. In ECCV.",
  "OpenAI.2023.Gpt-4v(ision)technicalworkandauthors. 19-February-2024]": "Rahul Pandey, Hemant Purohit, Carlos Castillo, andValerie L. Shalin. 2022.Modeling and mitigat-ing human annotation errors to design efcientstream processing systems with human-in-the-loopmachine learning. International Journal of Human-Computer Studies, 160:102772. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation.In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Bryan A Plummer, Liwei Wang, Chris M Cervantes,Juan C Caicedo, Julia Hockenmaier, and SvetlanaLazebnik. 2015.Flickr30k entities:Collectingregion-to-phrase correspondences for richer image-to-sentence models.In Proceedings of the IEEEinternational conference on computer vision, pages26412649.",
  "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2019.Ob-ject hallucination in image captioning.Preprint,arXiv:1809.02156": "Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mah-davi, Rapha Gontijo Lopes, Tim Salimans, JonathanHo,David J Fleet,and Mohammad Norouzi.2022.Photorealistic text-to-image diffusion mod-els with deep language understanding.Preprint,arXiv:2205.11487. Christoph Schuhmann, Romain Beaumont, RichardVencu, Cade Gordon, Ross Wightman, MehdiCherti, Theo Coombes, Aarush Katta, ClaytonMullis, Mitchell Wortsman, Patrick Schramowski,Srivatsa Kundurthy, Katherine Crowson, Ludwig",
  "Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.2022.Laion-5b:An open large-scale datasetfor training next generation image-text models.Preprint, arXiv:2210.08402": "Piyush Sharma, Nan Ding, Sebastian Goodman, andRadu Soricut. 2018.Conceptual captions:Acleaned, hypernymed, image alt-text dataset for au-tomatic image captioning.In Proceedings of the56th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages25562565, Melbourne, Australia. Association forComputational Linguistics. Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au-rlie Herbelot, Moin Nabi, Enver Sangineto, andRaffaella Bernardi. 2017. Foil it! nd one mismatchbetween image and language caption. In Proceed-ings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers). Association for Computational Linguistics.",
  "Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, andRadu Soricut. 2022.Crossmodal-3600: A mas-sively multilingual multimodal evaluation dataset.Preprint, arXiv:2205.12522": "Tristan Thrush, Ryan Jiang, Max Bartolo, AmanpreetSingh, Adina Williams, Douwe Kiela, and CandaceRoss. 2022. Winoground: Probing vision and lan-guage models for visio-linguistic compositionality.Preprint, arXiv:2204.03162. Jack Urbanek, Florian Bordes, Pietro Astol, MaryWilliamson, Vasu Sharma, and Adriana Romero-Soriano. 2023. A picture is worth more than 77 texttokens: Evaluating clip-style models on dense cap-tions. Preprint, arXiv:2312.08578.",
  "AAnnotation Guidelines": "We now present the full detailed annotation guide-lines used for IIW annotations. Our guidelinesstate that image descriptions should be composedsuch that they paint a vivid mental picture of anactual image in the mind of someone hearing thedescription and has their eyes closed. In order toreach this level of detail composed in an articulatemanner, we compile an extensive set of annotation guidelines. We iterated over these guidelines withmultiple pilot rounds.The annotators are asked to operate as if theyare instructing a painter to paint with their wordsand only include details that can be deduced fromvisual cues, erring on the side of higher precision.Unnecessary fragmentation of sentences should beavoided to compose writing in a owy, coherentstyle, avoiding the use of ller phrases like: Inthis image, we can see, there is a, this isa picture of, since they add no visual detail andcome at a cost of verbosity.Objects form the lego-blocks of an image. In-teractions and spatial arrangements among themhelp to form the context of the image. In complexmulti-object images with dense settings, notingeach and every object independently can becomecumbersome and highly dependent on the effortthe particular human annotator puts in. To denethis better and expect a consistent behavior fromthe annotation outputs, we introduce the notionof salient objects. Key objects without which theimage would lose its context and meaning are con-sidered salient. This can include individual objectsor combinations of them depending on the role theyplay in the image; consider the following 2 casesas examples: Three people in the blurry background of animage, with the scene set inside a coffee shop,who play no concrete role individually can begrouped as people in the background insteadof 3 individual people object annotations. Two people in the foreground and in-focus,engaged in a conversation in the same scene.The two individuals are likely the focus of theimage and hence worth noting individually indetail as separate objects. This is likely whatthe photographer was attempting to capture. While annotating each of these salient objects inan image, the annotators should consider the fol-lowing axes as reference (but not limit themselvesto this list), paying special attention to features thatmake them unique or salient:",
  "Humans typically associate a set of default fea-tures to objects. Consider the following examples:": "Car by default is assumed to have 4 of each:tires, door, windows and 1 of each: trunk,hood, steering wheel, roof. Mentioning themseparately might not be that useful as it addsno specic visual detail that we did not al-ready know as the norm. Now, if the car isa coupe, has a missing window, or containsa door painted with a different color than theoverall color, i.e., making it a unique feature,then that would be worth mentioning in thedescription since it holds specic added visualvalue. The Golden Gate Bridge by default is orange.That being said, it does not hurt to includeextra detail depending on the use-case. If theannotators do not recognize the bridge as afamous well known entity, then it would makesense to include the color and additional at-tributes. When composing the overall image description,start with a newspaper style tldr sentence thatpaints a very clear high level picture. Describethe objects in order of their saliency while notingthe description of individual objects and relation-ships in a coherent manner. Include the overallsetting, background, style, and consider:",
  "Mood or feeling Overall mood or feeling ofthe image": "Camera angle (i.e., the position of the camerain relation to the subject) is crucial, as this setsa precedence for what level and kind of informa-tion to expect. The choice of camera angle canhave a signicant impact on the mood and meaningof a photograph. Different camera angles can beused to create different effects and convey differentmessages, e.g., details about a close-up are differ-ent from those of a wide angle shot. Examples ofcamera angles (see ):",
  ": Camera Angles to Consider when Annotating Images. These are important to set a precedence on thelevel and kind of information to expect in the image description": "multiple lines, if text is in multiple lines whetherthere is mutual alignment, the features of the fontsuch as size, style, color, and orientation (e.g., ver-tical, horizontal, arched), casing (e.g., lower, upper,mixed), and attributes like italics, underlined, bold,written in quotes, clearly visible or blurred. De-scribe the words if they are written.If text is written in multiple lines, we should:",
  "Mention its mutual alignment using referenceslike vertically stacked, aligned to the left, etc": "For example, in , the phrase (Juice,ACROSS THE, Universe) has words Juiceand Universe as capitalized while the phraseACROSS THE is all uppercase, and componentsare aligned along a diagonal. Information on thefont color, type, and shadow effect should be in-cluded. As another example from the same image,the phrase (FREE, ARCADE, GAMES) areall upper-cased, vertically stacked and centrallyaligned.If you have a good idea of the font family andare condent, that would be valuable to note.When people are present, special notes shouldbe kept in mind to mitigate different types of bias. The tone should be respectful to the subject andnot make assumptions or try to guess their gender,identity, ancestry, where they are from, sexuality,religion, etc. We emphasize that the descriptionsshould be noted in objective, neutral and fair lan-guage for related attributes and focus solely on thevisual aspects. Consider the following axes withrespect to attributes here:",
  "Whether they have any unique features likemarks, tattoos, scars on their body that are": ": An Example where Quoting Text in a Detailed Manner can Enable Precise Reconstruction. The word-casing and alignment attributes of the multi-line phrase (Juice, ACROSS THE, Universe) has words Juiceand Universe as capitalized while the phrase ACROSS THE is all upper-cased and all components are alignedalong a diagonal. Information on the font color, type, shadow effect should be included. For the phrase (FREE,ARCADE, GAMES) all words are upper-cased, vertically stacked, and centrally aligned.",
  "visible. If applicable, note the respective posi-tions on their body where each is present": "For professions with known gender biases likenurse, doctor, or construction worker,explicitly include the gender (if clearly de-ducible) and do not operate under the assump-tion that one gender is more common in thatprofession. For any apparel, the descriptions should focuson overall style, unique details, silhouette of thegarment, how it ts, fabric, color, shades, and toneof the garment. If the branding is visually visible, itshould be included while attributes like size shouldbe skipped unless visually veriable.Where applicable use locale specic names ofobjects like clothing (e.g., sherwani, kurta, kimono,saree), food (e.g., shawarma, dosa, paneer tikka)etc. The aim is to capture the locale specic vocab-ulary so the downstream models can pick them up",
  ": Image Category Distribution for the IIW Datasets Train and Eval Splits": "a US majority currently. In the future, we planto intentionally increase diversity in our annota-tor pool to ensure more locale-specic vocabularyin our image descriptions. The annotators werecompensated appropriately taking their skill-set,qualications, location and the complexity of thetask into account. The pool was trained for the annotation task over a period of month to achieve asense of consistency on the annotation guidelinesas well as the downstream tasks to be covered bythe data being collected. The annotators were alsocommunicated clearly on the downstream tasks anddata use cases to get a sense of the importance andquality bar needed for this foundation work. For text-to-image generation rankings, we employedan internal group of six people to rank the imagesgenerated by different model-generated image de-scriptions (i.e., we did not hire crowd workers).People participating are domain experts, familiarwith text-to-image generation technology.",
  "B.2Human Annotation Challenges": "Despite the very detailed annotation guidelines weprovided to the annotators, there were several chal-lenges during the human annotation process. First,we still found individual instances of random qual-ity or judgment lapses. To circumvent this, we de-signed our framework to be sequential (i.e., morethan one annotator works on each sample). Wealso found different challenges with respect to eachimage. For instance, art images require more do-main specic expertise to describe an image withappropriate vocabulary. At the start of our anno-tation process, we observed that annotators had atendency to use ller words and prexes such asThis is a, There is a, or This photo was takenwith, and we provided feedback asking they donot include such phrases.Another challenge during the annotation processwas to encourage annotators to focus on the bigpicture and write a TLDR rst. We also observedsome tendency to use slightly subjective languagewhile describing the images, e.g. using adjectivesthat are not explicitly supported by the visual cues.By providing feedback directly to the annotators,pointing to specic samples, and emphasizing thatcertain language styles do not align with the writingstyle we were aiming for, we were able to consid-erably increase the annotation quality and get thedesired type of image descriptions from the anno-tation process.",
  "(a) Monitor this at the beginning of the an-notation project when the annotators arestill new to the task using metrics likeedit-distance and provide explicit feed-back to the annotators as needed": "(b) Annotators in each round have the optionto start from scratch if they deem thequality from the previous round to beconsiderably low. Use this as feedbackfor the annotator from the previous roundby presenting them the edited output tolearn from. Human-in-the-Loop Learning Our annotationframework implicitly unlocks a feedback loop forthe annotators due to the sequential augmentationprocess discussed above. Each annotator gets anopportunity to read and learn from each othersperspective which in turn improves their individualquality. As an example from , we demon-strate how Annotator-1 get an opportunity to learnfrom Annotator-3 for the rst image and Annotator-2 gets an opportunity to learn from Annotator-1 inthe second image. Model-in-the-Loop Annotation We employ anactive learning loop for the VLMs where after someinitial annotation data is available, a model versionM1 can be trained over the base VLM to improvethe seed description quality. As more data gets an-notated, M1 can be updated to M2, M3, ..., Mn toreduce the human effort needed.Advantages:",
  "We now discuss the annotation framework withconcrete examples and UI illustrations:": "Annotation Task-1: Fine Grained Objects andAttributes In Task-1, the human annotators are pre-sented with seed annotations for the objects froman Object-Detection (OD) model and VLM gener-ated seed captions for each object (see ).The annotators can then annotate to note the salientobjects and their corresponding description (see).Annotators can make the following augmenta-tions to annotate salient objects:",
  "Merge if object(s) are fragmented and/or pre-populated as two or more objects, the anno-tators can remove the individual objects andcreate a new single object": "Closelyplacedobjectsofthesame/similarlabel/typewhichindi-vidually hold low value but can bedescribed as a collection to hold a highercontext value should be combined, e.g.,ve identical cups in an image linedup next to each other do not need tobe tagged as separate objects. If thereare attributes that separate one or moreof them from the others, we expect theannotators to split them in groups andproceed accordingly.",
  ": IIW Annotation UI for Task-1 with VLM seeds. We illustrate the seed object-detection objects and VLMgenerated object-level captions with object cropped image bytes as input": ": IIW Annotation UI for Task-1 after human augmentation. We illustrate the human augmented salientobjects and their human-authored descriptions. The annotations are built on seed information from . Thisexample demonstrates how humans can alter the seed annotations based on the annotation guidelines, which caninclude merging, deleting, editing and adding new salient objects and then describing each.",
  "tal picture than the actual image?, e.g.,doors, windows, or tires of a Car can beomitted unless there is something uniqueabout them, as they are standard expecta-tions from a Car object": "For each (label, bounding box) pair, we ask theannotators to generate a detailed description fo-cused on the object in the context of the imageconsidering the several axes as reference (see Ap-pendix A). Annotation Task-2: Overall Image DescriptionIn Task-2, human annotators are presented withthe annotations from Task-1 and a seeded VLMdescription (see ) which is then rened byhuman annotators in sequential rounds to producethe nal hyper-detailed description (see ).",
  "CIIW Fine-Tuning Tasks": "We dene seven tasks with the IIW Task-1 andTask-2 annotations to ne-tune two IIW basedVLM model variants of PaLI-3 5B (Chen et al.,2023a). Our models include IIW Combined, trainedon a mixture of all seven tasks and IIW-Task-2based aka IIW Model, which is only trained on thenal most detailed image description output. Theseven tasks can be grouped into three categories:image region, salient objects, and detailed descrip-tion based tasks, see for illustration. As we later discuss, we generally nd the IIW(Task 2 only) Model to be preferred over the IIWCombined variant, but include details on the addi-tional training tasks and resulting ablations here forcompleteness. All results in the main paper use theIIW Model.",
  "C.1Image Region Tasks": "Using one object at a time from the list of (label,bounding box, description) Task 1 annotations, weperform three region-based tasks. We use normal-ized bounding boxes in [ymin, xmin, ymax, xmax]format as in Pix2Seq (Chen et al., 2022). Our rsttask is description-label grounding. In multi-objectdense images, a label in itself is not enough touniquely identify an object. Thus, we create agrounding task with (image, label, description) in-puts that are tasked to predict the correspondingnormalized bounding box coordinates. Our second image region task is label prediction,in which we predict an open vocab label for theobject with input (image, bounding box). Lastly,we perform object description generation, whichproduces descriptions for each object in the imagegiven (image, bounding box, label).",
  "C.2Salient Objects Tasks": "Our next category of ne-tuning tasks concerns thesalient objects in an image. We target the aggre-gated list of (label, bounding box) object featuresper image from Task 1. Our rst task is label gener-ation, in which given an image, we aim to generatea text list of the salient object labels. The objectlabels are sorted alphabetically for consistency, butin future work ordering by saliency would be use-ful. Our second object-level task is grounded labelgeneration. The task is to generate the list of (label,bounding box) pairs per object in the image; wesimilarly sort the list alphabetically with respect tolabel name.",
  "C.3Detailed Description Tasks": "Finally, our last ne-tuning tasks relate to the se-quentially annotated descriptions from Task 2. Weperform description elaboration in addition to di-rect description generation. Given the image anddescription from the Nth sentence, descriptionelaboration trains the model to elaborate the cur-rent description to the nal description. We alsocreate synthetically corrupted versions of the naldescription to serve as additional training samples.Specically, we randomly drop X% of sentences.Sentences are dropped starting from the last sen-tence so that the structure of the overall text pieceis maintained (as opposed to random sentence re-moval). For nal description generation, given theimage, a VLM learns to generate the nal mosthyper-detailed description available from the entireannotation framework. This nal task (and not de-scription elaboration), is the only task used to trainthe IIW model (whereas all are used for the IIWCombined ablation).",
  "D.1Seeded Annotation SxS": "We additionally run a human SxS evaluation tocompare the effects of seeding in the IIW anno-tation framework. In , we compare de-scriptions written without and with VLM seedingon a subset of IIW-400 (50 samples). There isa trend across all metrics that seeding improvesdescription quality, as seen with marginal or sub-stantial gains across comprehensiveness (+54%),specicity (+48%), TLDR quality (+28%), andhuman-likeness (+25%). The hallucinations met-ric is primarily neutral with a slight preference toseeded descriptions (+9%). This is somewhat ex- : IIW Annotation UI for Task-2 with seed VLM description. This VLM has been ne-tuned in an activelearning mode as data was collected iteratively. The seed caption from the same VLM (PaLI-5B) without the IIWne-tuning is a pink bicycle with a basket of owers on it. The seed annotation is then rened and augmentedby human annotators, see for the nal resulting description. : IIW Final Annotation UI for Task-2. We illustrate the human annotations available from Task-1 asthe human annotators hover over the salient objects in the image. The annotators can additionally switch betweenhiding all salient objects to view the image properly. Task-2 annotations start with the seed caption from the VLMand is then rened by human annotators in sequential rounds, building on top of the previous rounds output. pected, and afrms that despite model-generatedoutputs having a potential risk for hallucinations,the humans are able to correct and improve on them.Thus, the SxS conrms seeding is advantageous tothe IIW annotation framework.",
  "In , we perform a SxS evaluation on a subsetof IIW-400 (on 100 samples). This compares datafrom the human authored IIW annotation frame-work to descriptions generated by the IIW ne-": "tuned model. Across all metrics there is an ex-tremely high preference to the human annotateddata, with signicant and marginal gains: compre-hensiveness (+78%), specicity (+91%), fewer hal-lucinations (+31%), TLDR quality (+58%), human-likeness (+52%). This conrms the quality of dataproduced by the IIW human-in-the-loop annotationframework, and demonstrates the need for moremodeling efforts to bridge the gap between the IIWhuman authored versus model generated descrip-tion quality. For example, larger capacity models : IIW based VLM Fine-tuning Tasks. We show tasks based on data collected from Task-1 and Task-2 perthe IIW annotation framework. Different tasks enable the ne-tuning to focus on the image at (object, attribute),(image, objects) or (image, hyper-detailed description) levels.",
  "D.3Automatic Readability Measurements": "In addition to our human SxS comparisons, weuse a suite of readability metrics to quantify writ-ing style differences between DCI, DOCCI, andIIW. We run heuristics based readability metricsover both human-authored and model-generated de-scriptions representing each style, and present theresults in . Each metric roughly estimatesthe level of education needed to understand a pieceof written text using different units, e.g. education years or grade-level. While they are proxy signals,a pattern across all can be seen as a clear indicationof a more mature and articulate writing style forIIW in comparison with the other alternatives.For the metrics, we used spaCy (Honnibal et al., 2020) (v3.0.0rc2) to tokenize the text and the imple-mentation in Githubs py-readability-metrics repo(v1.4.1) to calculate the scores. We also includethe readability metric distributions in .The distributions further demonstrate a more ma-ture writing style in both the IIW human-authoreddataset and ne-tuned model generated outputs.",
  "DCI5.85.78.18.12.93.76.26.9DOCCI7.57.19.58.76.46.68.78.2IIW10.49.511.811.59.39.011.311.7": ": Readability Metrics on Human and Model Annotated Data. We include ARI (Wikipedia contributors,2023b), Flesch Kincaid (FK) (Wikipedia contributors, 2023c), Gunning Fog (GF) (Wikipedia contributors, 2023d),and SMOG (Wikipedia contributors, 2023e) metrics. They approximate the grade level needed to comprehend thetext and results indicate a more mature writing style in IIW human-authored and model generated outputs.",
  "First few line(s) as tldr: The rst few line(s)should paint a high level picture of what toexpect in the image and create a succinct sum-mary": "Human-Like: The descriptions should feelas if an educated person wrote them andshould be free from artifacts hinting that amachine generated them (e.g. stuttering, re-peating facts, fragmented chain of thought,etc.). The 5 metrics are dened to capture 3 broad um-brella metrics of precision, recall and writing-style.An overall metric score can further be computed bytaking an average of the 3 umbrella metrics. Eachcan be dened as follows:",
  "D.5Additional Automatic Metrics": "We include evaluations of model-generated outputswith automated text similarity metrics for complete-ness, but note that common text similarity metricsare ill-suited for long texts and more recent image-text metrics are often length limited. We reportthese results simply to emphasize the limitationsof these metrics when measuring the quality ofhyper-detailed image descriptions. Using standardautomatic metrics, illustrates how ne-tuned models largely perform better in replicatingtheir own style.In addition to reporting BLEU-4, ROUGE-1,and ROUGE-2 automatic metrics, we includeCIDEr (Vedantam et al., 2015), BERTScore (Zhanget al., 2020), and BLEURT (Pu et al., 2021) met-rics in .We include BERTScore andBLEURT as they are newer, model-based metricswhich have been shown to correlate more closelywith human judgements. CIDEr, like BLEU andROUGE metrics are not limited by sequence length.BERTScore and BLEURT have a maximum se-quence length of 512 (we specically use thewwm_cased_L-24_H-1024_A-16 BERT check-point and the latest BLEURT-20 model), but forour descriptions, they likely t under this maximumlength, with only outliers being truncated.CIDEr and BERTScore generally show the sametrend of each ne-tuned model performing best onthe same test domain (i.e., DCI ne-tuned mod-",
  "(b) Distribution on the Fine-tuned Model Generated Outputs from DCI, DOCCI and IIW": ": Distribution-based Readability Metrics. We compare both human authored and model generated outputsfrom IIW and prior work to show the distribution of Education based units reected in the writing style. IIW outputsfrom both the human annotators and the model produce a more mature style across the metrics. els perform best on DCI test set, DOCCI mod-els perform best on DOCCI test set, and so on).One anomaly occurs with CIDEr on the DCI testset, where PaLI models ne-tuned with DOCCIslightly outperform the DCI trained model (4.91versus 4.57). Due to how low the metric valuesare, these differences may not be signicant. Whenevaluating the DCI, DOCCI, and IIW test sets withBLEURT, we instead nd a slight preference forIIW models. Across all three datasets, BLEURTshows PaLI-IIW variants perform better or simi-larly to the same-domain test set. Thus, newer met-rics may reveal IIW ne-tuned models generalizebetter than models ne-tuned on other datasets.",
  "D.7Reconstructing Images with IIWDescriptions": "For reconstructing images sentence-by-sentence,we fed the T2I model the rst sentence, rst twosentences, rst three sentences, etc. as promptsfrom each of the three datasets (DCI, DOCCI andIIW). showcases the prompts and the T2Imodel outputs from three descriptions along withthe original image.We then asked human annotators to rank the gen-erated images by how similar they are to the origi-nal image. The image most similar to the originalimage is ranked number 1. We allowed generatedimages to be ranked the same if they are very sim- : Human SxS Annotation UI. Annotators are shown the input image and two input image descriptionsto evaluate side-by-side. The input descriptions could be from any combination of (human, model) sources. Thisinformation is not shared with the annotators and the sources are randomly ipped and marked as A or B to preventany source or order based bias.",
  "DCI4.570.600.414.710.610.420.750.560.40DOCCI4.910.580.3911.090.650.452.400.590.41IIW1.870.560.414.520.590.464.040.610.45IIW Comb.0.610.560.434.150.590.461.770.600.46": ": Additional Automatic Metric Results. We report CIDEr, BERTScore (referred to as BERT in tabledue to space), and BLEURT metrics for all ne-tuned models. We compare DCI, DOCCI, IIW, and IIW Comb.(Combined). ilar. (a) shows the reconstruction rankcounts for all the sentence counts and (b)shows the rank counts when we use sentence 1,sentence 1 and 2, sentence 1, 2 and 3, and sentence1, 2, 3, and 4. Sentences from IIW descriptions areranked rst much more frequently than sentencesfrom DCI and DOCCI descriptions. Specically,for the rst sentence, the difference is most no-table, supporting our claim that IIW descriptionsare higher quality earlier on and IIW rst sentencesare designed to capture a TLDR.",
  ": Ablation Results Comparing IIW Variants on Automatic Metrics": "ne-tuned model, InstructBLIP or LLaVA) and thelist of <CHOICES> are from the correspondingevaluation dataset, respectively. Choices are enu-merated in a list-like fashion, and we ask the modelto generate the number of the most likely caption.We dene a different prompt for the languagebias baseline, which serves as a sanity check thatthe image/image description is truly needed forthese datasets. It provides a lower bound for com-parison, too. While the prompt is different as wedo not input any image description, we try to makeit as similar as possible to the above image descrip-tion based prompt. We set the language bias promptto:",
  "where <CHOICES> are lled in in the same": "format as previously described.Importantly, when lling in the caption choices,we deterministically swap the index of the answer,i.e., the true matching caption, among the choiceslist in the prompt. This is done to ensure an equaldistribution and reduce any order bias (e.g., a LLMmay be more prone to believing the rst option isthe correct option).To obtain the image description which is thenfed into the LLM, we prompt our ne-tuned modelswith Generate a detailed image description. Forthe InstructBLIP and LLaVA models, we denesimilar prompts given the prompts used in theirpublished papers papers: Write a long and detaileddescription for the photo. and Provide a detaileddescription of the given image for InstructBLIPand LLaVA, respectively.We process the LLM outputs as classes, (e.g.,when choosing between image caption choices and , LLM responses are 1 or 2) and calcu-late accuracy with respect to the true image captionclass. If the LLM does not produce a valid class, its considered an incorrect prediction. Note thatthis task set up is different from how VLM modelsare typically evaluated on these reasoning datasets:prior work considers a sample to be correctly rea-soned about if the image-text similarity of the trueimage caption is higher than the image-text simi-larity of the incorrect image caption. Due to thelong length of our descriptions, we cannot com-pute image-text similarity reasonably with modelslike CLIP without signicantly truncating our im-age descriptions. In future work, once input lengthlimitations are mitigated, dual-encoder VLMs likeCLIP can be ne-tuned with our rich data, whichwill help to improve VLM reasoning.Note that ARO and Winoground datasets arebuilt with positive and negative captions for eachimage. SVO-Probes differs in that it originallycontained a positive and negative image for eachpositive caption. For our experiments, we need atrue and false caption associated with an image. Alarge portion (90%) of the SVO-Probes negativeimages also serve as separate samples (where theyare considered positive images, with associatedcaptions). Thus, we can pull these captions to serveas the negative caption for the original sample.For the remaining 10%, we use the negativetriplet (the S, V, O triplet specifying the subject,object, and verb, with one of them being modi-ed) to automatically ip the negative S, V, or Oin the positive caption. Ten of these samples didnot have negative triplets in the dataset, so theywere removed. Lastly, there were 114 samples withpositive captions not containing the S, V, or O thatneeded to be swapped to form the negative caption.This happens as a result of SVO triplets containingroot forms of the words, which were not spelledthe same way in the caption. For example, an SVOmay be man,lie,beach with the caption statingA man lying on a beach. Due to the verb tensedifferences, it would require additional processingto match lie to lying. We remove these edgecases for simplicity.Finally, we include more vision language compo-sitional reasoning results with different PaLI ne-tuned models in . Here we additionally in-clude the models ne-tuned with DCI and DOCCIdatasets. The IIW descriptions still result in high-est reasoning accuracy for ARO VG-A and arecomparable with DOCCI on Winoground. Trendsalso stay the same with SVO-Probes, with DOCCIperforming similarity to IIW, but InstructBLIP per-forming slightly better (by less than 1 accuracy point). Finally, we nd that DOCCI performs beston VG-R, which might be result of its dataset be-ing designed to explicitly contain connected andcontrasting images, which might more frequentlycapture similar images that only differ by the visualrelationship between objects.While performance differences between DCI,DOCCI, and IIW are smaller, this could be an arti-fact of the reasoning datasets; ARO, SVO-Probes,and Winoground are all built upon short captiondatasets, so the utility and quality differences be-tween DCI, DOCCI, and IIW are not fully capturedby these probing datasets.",
  "EEnriching Image Caption Datasets": "As discussed in the main paper, we enrich 1ksamples from two existing image caption datasets,namely, Localized Narratives and CrossModal(XM) 3600, with new image descriptions generatedby IIW ne-tuned models. The goal of releasingthese enriched versions is to provide longer, hyper-detailed image descriptions that can be used forevaluation purposes in future work. The enrichedversions not only allow for ner-grained, full cov-erage evaluations of the content in images (via newmetrics or probing datasets), but also may enableautorater models which learn from the precisionand recall errors in the generated descriptions.In , we report the language statistics onthe original 1k samples from each dataset and theenriched versions. It is clear that the IIW descrip-tions are signicantly longer and richer, as we havehigher counts of tokens, sentences, and each partof speech.",
  "FPercentages Reported in the MainPaper": "We re-quote and dene all analysis percentages re-ported in the main paper for clarity on how theywere calculated in Tables 15-17. The reference lo-cation is dened by the section, paragraph, and lineit appeared in. We only include paragraph numberfor multi-paragraph sections, and only include linenumber if the same percentage occurs more thanonce within a paragraph. For example, S4.3 P2L3 means , Paragraph 2, Line 3. Mostpercentages were rounded to the nearest point inthe main paper.",
  "Image Description ModelAROSVO-Probes WinogroundVG-A VG-R": "None (Language Bias Baseline) 56.50 59.9450.7149.88InstructBLIP-Vicuna-7B83.99 62.7389.3565.25LLaVA-V1.5-7B84.80 63.7187.8963.38PaLI-3 + DCI 5B88.19 66.4786.5064.62PaLI-3 + DOCCI 5B89.70 68.8588.7369.50PaLI-3 + IIW 5B90.37 66.1988.6669.38PaLI-3 + IIW Combined 5B89.46 64.8887.7866.88 : VL Compositional Reasoning Accuracy with Image Descriptions. We evaluate whether rich descriptionscan distinguish the true matching image caption in ARO (Yuksekgonul et al., 2023), SVO-Probes (Hendricks andNematzadeh, 2021), and Winoground (Thrush et al., 2022) datasets. The COCO and Flickr30k Order subsets ofARO are not reported due to a very high language bias baseline of 98%.",
  "Count/ Sent./ Desc": "LocNar (Pont-Tuset et al., 2020)100014.3530.562.128.021.090.162.39IIW Enriched22.19128.875.8032.37 16.021.8211.44XM3600 (Thapliyal et al., 2022)100010.4010.401.003.451.080.040.61IIW Enriched22.25130.565.8633.18 15.821.7211.87 : Dataset Statistics Comparing ImageInWords (IIW) Descriptions of Prior Work to their Original Anno-tations. We include the number of samples (i.e., subset of captions/descriptions that we enrich) and the averagenumber of tokens, sentences, nouns (NN), adjectives (ADJ), adverbs (ADV), and verbs (VB). Language statisticsare averages reported per description unless otherwise noted. : T2I Outputs and Human Ranking Evaluations. We show example T2I results where the rst sentence,rst two sentences, ..., all the sentences of the image descriptions from DCI, DOCCI and IIW models are fedsequentially as inputs, i.e., at each step an additional sentence chunk is fed to the T2I model.",
  "+66%Abstract,Intro P5,Conclu-sion": "Average difference of IIW preference vs. other datasetpreference, averaged over DCI and DOCCI datasetsand averaged over the ve metrics corresponding to(comprehensiveness, specicity, hallucinations, tldr,human-likeness). Differences of IIW marginally andsubstantially better - other dataset marginally andsubstantially better for (comprehensiveness,specicity, hallucinations, tldr, human-likeness)metrics from correspond to DCI (61, 80, 42,91, 82) and DOCCI (42, 82, 35, 79, 68). The nalaverage preference over the ve metrics and twodatasets is 66.2%.",
  "+31%Abstract,Intro P5,S5.1 P1,Conclu-sion": "Average difference of IIW model output preference vs.other ne-tuned model output preference, averagedover DCI and DOCCI ne-tuned models and averagedover the ve metrics corresponding to(comprehensiveness, specicity, hallucinations, tldr,human-likeness). Differences of IIW marginally andsubstantially better - other dataset marginally andsubstantially better for (comprehensiveness,specicity, hallucinations, tldr, human-likeness)metrics from correspond to DCI (42, 54, -9,51, 57) and DOCCI (4, 37, -7, 57, 23). The nalaverage preference over the ve metrics and twodatasets is 30.9%."
}