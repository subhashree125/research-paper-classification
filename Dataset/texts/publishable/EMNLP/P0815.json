{
  "Abstract": "Social science research has shown that candi-dates with names indicative of certain races orgenders often face discrimination in employ-ment practices.Similarly, Large LanguageModels (LLMs) have demonstrated racial andgender biases in various applications. In thisstudy, we utilize GPT-3.5-Turbo and Llama3-70B-Instruct to simulate hiring decisionsand salary recommendations for candidateswith 320 first names that strongly signal theirrace and gender, across over 750,000 prompts.Our empirical results indicate a preferenceamong these models for hiring candidates withWhite female-sounding names over other de-mographic groups across 40 occupations. Addi-tionally, even among candidates with identicalqualifications, salary recommendations vary byas much as 5% between different subgroups. Acomparison with real-world labor data revealsinconsistent alignment with U.S. labor marketcharacteristics, underscoring the necessity ofrisk investigation of LLM-powered systems.",
  "Introduction": "Extensive studies in the social science litera-ture have shown that racism and sexism perme-ate decision-making processes in numerous areas:healthcare, education, criminal justice, and so on(Williams and Wyatt, 2015; Warikoo et al., 2016;Kovera, 2019; Clemons, 2014). Research spanningdecades and continents has shown that discrimina-tion based on race and gender are especially preva-lent in employment practices (Darity Jr and Mason,1998; Bielby, 2000), where Non-White minoritiesand women have consistently been subjected tohiring discrimination (Stewart and Perlow, 2001;Quillian and Midtben, 2021).Biased treatments are not limited to explicitcharacteristicssuch as when a hiring official candirectly observe the race or gender of a candidatebut are also be triggered by proxies, such as their names. Candidates with ethnically or racially dis-tinct names have been subjected to employmentdiscrimination: from getting lower callback ratesto receiving less favorable reviews compared totheir peers (Bursell, 2007; Stefanova et al., 2023).Recently, Large Language Models (LLMs) havebecome the leading architecture for many tasksin Natural Language Processing (NLP) (Kojimaet al., 2022; Zhou et al., 2022; Chang et al., 2024).Despite their class-leading performance, LLMshave been shown to propagate and amplify dif-ferent forms of bias in numerous domains (Wanet al., 2023; Gupta et al., 2023; Poulain et al., 2024;Salinas et al., 2023; Li et al., 2024b), similar tohow more traditional predictive machine learning-based models replicate and exacerbate social bi-ases (Mehrabi et al., 2021).In this paper, we examine LLMs and their poten-tial bias towards first names in making employmentrecommendations. More specifically, our experi-ments prompt LLMs to make hiring decisions andoffer salary compensations for candidates with U.S-based first names that signal their race and gender,sometimes in isolation, and sometimes with a biog-raphy that is otherwise scrubbed for demographicinformation. Our main findings are:",
  "Biases exhibited by LLMs partially mirrorreal-world trends in the United States (U.S)labor force at coarse-grain levels. However, in-tersectional analysis reveals nuanced discrep-": "ancies that favor certain minority groups whilepunishing others, albeit inconsistently.Our work builds directly on that of Haim et al.(2024) and of An et al. (2024). Haim et al. (2024)prompted LLMs to provide assistance for 40 Blackand White named individuals across topics relatedto sports, public office, purchasing etc., finding thatBlack female names received the worst outcomes.An et al. (2024) prompted LLMs to write emails toaccept or reject job candidates with stereotypicallyWhite, Black or Hispanic names (across two gen-ders), and investigated whether those emails choseto accept or reject the candidates. Their work foundthat acceptance rates for the latter 2 groups tendto be lower than the former, even when degrees ofeducation and qualification level were consistentlystated across candidates. Our work augments thesefindings by: 1) exploring alternative hiring-relatedtasks, including salary prediction with full, nat-ural biographiessimilar to rsum studies insociology, and 2) by connecting LLM behaviorsto real-world labor data to reveal intersectional biaswith respect to a range of occupations.",
  "Hiring Recommendation": "In this paper, we study two types of recommen-dations that LLMs could conceivably be applied.The first, discussed in this section, is hiring rec-ommendations: given an occupation and a list ofnames of potential candidates, do LLMs exhibitany racial or gender preferences for selecting whoto give a job to ? The second type is salary rec-ommendation (3): given a candidate name and(potentially) a biography for that candidate, whatsalary is recommended for them ?For hiring recommendations, to investigatewhether GPT-3.5-Turbo (hereafter referred to asGPT-3.5) and Llama 3-70B-Instruct (hereafter re-ferred to as Llama 3) (Meta, 2024) exhibit a pref-erence for names associated with specific demo-graphics, we ask these LLMs to select one namefrom a list of candidates representing various racialand gender groups for 40 different occupations.",
  "Curation of Names": "We limit our study to first (given) names, drawnfrom two datasets: Rosenman et al. (2023) andTzioumis (2018). The former contains 136,000first names compiled form voter registration fileswhile the latter contains 4,250 first names com-piled mortgage information in the United States. System: You are a hiring manager of a company.User:Select only one name from the following list ofcandidates you deem most appropriate for the presentedposition.Thecandidatespossesssimilarlevelsofeducation and experience.You must not provide any additional response besides thebest candidates name.",
  ": Prompt template to select best candidate foran occupation. System denotes system prompt. Userdenote user prompt": "Both datasets supply the conditional probabilitiesof each names association with 5 racial/ethnicgroups: White, Black, Hispanic, Asian, Other. Wedesignate a name as male or female based on itsprobability in the U.S. Social Security Agencysdatabase, with a 0.5 probability threshold. Omit-ting the Other category due to its small size, werandomly select 40 names from each gender of thefour remaining races, whose conditional probabil-ity P(race|name) exceeds at least 0.8. Our finalpool consists of 320 first names. See A.1 foradditional details on the curation process.",
  "Experimental Setup": "Recommendation Without Biographies.Weask the LLMs to recommend an annual compensa-tion for 28 occupations in the BiasinBios dataset tocandidates using the template shown in ain the Appendix. The prompt provides the target oc-cupation, the name of the candidate, and states thatthe candidate meets the qualifications. We promptthe models 2 times for each candidate-occupationpair (over 320 names and 28 occupations) to ac-count for potential variation, leading to a total of17,920 prompts per model. Recommendation With Biographies.We editbiographies from the BiasinBios dataset to mini-mize potential confounding effects of gender-basedexpressions. For each of the 28 occupations, werandomly select 10 male and 10 female biographiesand assign them a unique identifier (BioID). We useGPT-4o to substitute the names of the person ref-erenced in the original biographies with the place-holder string \"{name}\", and replace gender-basedpronouns (he/him, she/her) into gender-neutralcounterparts (they/them) (details in A.4). URLsand social media links that might trigger gender-related associations are also removed. We thenprepend all biographies with the phrase \"The candi-dates name is {name}\" since some texts do not con-tain any name originally. Finally, we perform man-ual qualitative check to verify these 560 rewrittenbiographies for gender-neutrality. For this task, we attorney composer dj physician -0.39 1.45 -1.58 -1.15 Females make moreFemales make less 12.510.07.55.02.50.02.5",
  "Hiring Recommendation Results": "Gender-stratified Hiring. shows thedistribution (normalized to percentages) of frequen-cies where names from each race are chosen (Fullreports in and ). We performthe Chi-square test on the frequency distributionsfor each occupation to compare them against thedefault expected frequency, where all races areequally chosen 50 times out of 200. p-values < = 0.05 indicate statistically significant differ-ences from this baseline for all groups, except forpoet, singer, architect for male names by GPT-3.5,and architect, model, singer, teacher for male nameand janitor for female names by Llama 3. Distribu-tions for the same occupation may not necessarilybe consistent across gender, for example, drywallinstaller, flight-attendant. shows the totalnumber of times reach race emerges the most rec-ommended for the occupations where the LLMsdistributions have statistically significant p-value. Gender-neutral Hiring.Similarly, Chi-squaretests on the output distributions of the 8 race-gender groups reveal statistically significant de-viation from the expected baseline frequency (50out of 400 per group) among all 40 occupations forboth models. shows the distributions of oc-cupations where each of the race-gender groups aremost favored over others. We observe the followingmajor trends:First, LLMs show a strong preference for White-aligned names, particularly favoring White femalenames over other groups. For gender-stratified hir-ing, White female names are preferred in moreoccupations (35 by GPT-3.5, 29 by Llama 3) com-",
  ": Number of occupations where candidates fromthe corresponding of the 8 race-gender groups are mostfrequently chosen for hiring": "pared to White male names (30 and 18) ().For gender-inclusive hiring, White female namesare preferred in 28 (70%) and 26 (65%) occupa-tions by GPT-3.5 and Llama 3 ()Second, Llama 3 exhibits less bias for White-aligned names compared to GPT-3.5. In ,Asian names are the second most chosen groupacross occupations, though not significantly so. Incontrast, Black names are disproportionately hiredas rapper by GPT-3.5, with the addition of singerand social worker by Llama 3. Hispanic names arenever the majority for any occupation by GPT-3.5,and only for 5 and 2 occupations among male andfemale groups by Llama 3. In , Llama 3exhibits more distributed preference for non-Whitenames vs. GPT-3.5, though still far from parity.",
  "Assessment Against U.S Labor Force": "To understand how closely LLMs decisions alignwith real world gender and racial biases, we com-pare the breakdown of their gender-neutral hiringdecisions against published record on labor forcecharacteristics by the U.S Bureau of Labor Statis-tics in 2023 (Bureau, 2023). We are able to matchstatistics for 30 out of 40 occupations (). Gender-based Analysis.We designate each oc-cupation as male or female based on whether thepercentage of names chosen by the LLM exceeds50% for that gender. The Bureaus data is desig-nated similarly 1. shows the contingency ta-ble between LLMs hiring decisions and observeddata. While the U.S labor evenly splits betweenmale and female occupations, GPT-3.5 and Llama3 prefer female names in 23 and 22 (out of 30)occupations respectively ( 70%).",
  "M6978F114114": ": Contingency table for LLM-predicted (withpred suffix) vs. U.S statistics-based male (M) vs female(F) occupations. While labor data shows the occupationssplit evenly between the genders, LLMs favor femalenames in most occupations. (Bureau, 2023). By adjusting the percentages of the3 races in the 2023 U.S labor force to include onlynon-Hispanic constituents, we calculate the MeanAbsolute Errors (MAE) of the LLM-projected(%llm) distribution against recorded statistics (%us)per occupation to quantify the accuracy of theLLMs demographic projections:",
  "MAEoccupation =race |%usrace %llmrace|": "Overall, we find that GPT-3.5 follows U.S statis-tics more closely than Llama 3 in hiring distributionof for White, Black and Asian. GPT-3.5s averageMAE across 30 occupations is 8.3 ( = 5.2), lowerthan Llama 3s average MAE of 9.0 ( = 7.8).For Hispanic group, we calculate Pearson corre-lation coefficient directly between LLM-projectedpercentage distribution and the U.S labor forces(Sedgwick, 2012). Llama 3s coefficient is 0.50,higher than GPT-3.5s 0.43 and congruent with thediversity observed in and . Remarks.Our empirical results reveal thatLLMs display a strong preference to hire White andfemale names when presented a diverse selection ofcandidates noted to possess similar qualifications.While LLMs projections reflect the distributionof U.S labor characteristicsalbeit imperfectlythistendency deviates far from equality, highlightingthe risk of applying LLMs in similar settings.",
  "Salary Recommendation": "In this section, we investigate whether LLMs stillexhibit disparate treatment when candidates namesare presented individually to be offered a targetsalary. To this end, we prompt GPT-3.5 and Llama3 to recommend salary compensation in 2 settings.In the first, only the candidates name is presentedin the prompt, while in the latter, the candidates bi-ography is presented as well, with the expectationthat having access to the additional information System: You are a hiring manager of a company.User: The following candidate has applied for the positionofoccupationatyourcompany.Thiscandidatehaspassed the initial screening for minimum qualifications.RecommendanannualsalaryinU.Sdollarsforthiscandidate as an integer.You must not provide any additional response besides thesalary amount.",
  "Percentage Gap": "-0.56-0.47 -0.92 -0.38 -1.33 -0.50-0.34-0.49 -0.73-0.57 -0.30-0.30 -0.82 -0.30-0.14 -0.72-0.81 -0.26 -0.55 5.56 -0.25 -0.73 -0.33-0.15 -2.35 -0.75 -2.59 -0.84-0.68 -2.32 -0.30 0.13 -0.62 -1.00 -0.23 1.03 -0.85 -1.63 -0.47-0.32 -1.46 -0.56 -0.90 -0.59 3.67 -0.30 -2.63 -1.82 GPT-3.5Llama 3 : Percentage gaps between average salaries offered to female vs. male names by LLMs (as determined byMixedLM model) when biographies are presented. Only careers with statistically significant gaps shown.",
  "Gender-base Analysis": "Without Biographies.First, we determined thesalary offered to each candidate by averaging theamounts recommended across two runs per name-biography pair. We perform a t-test ( = 0.05)to compare the salaries recommended to male vsfemale names per occupation with the null hypoth-esis H0: there exists no difference between themeans of each group. For GPT-3.5, we reject H0and observe statistically significant differences (p-value < ) between gender groups for only 4 outof 28 occupations. In contrast, Llama 3 show dif-ferences for 12 occupations. shows the percentages of differencebetween the mean salaries recommended to eachgender group for the occupations with significantdifferences. GPT-3.5 offers female names morethan their male counterparts for attorney, DJ, physi-cian, and less for composer. Llama 3 offers femalenames less for 11 occupations, and more only forpoet. Furthermore, Llama 3s average magnitudeof gender-based discrepancy in salaries is 3.75%,significantly larger than GPT-3.5s 1.13%.",
  "Sref 100": "where Sref denotes the mean salary offered tothe reference group (male in this case), Sfemaledenotes the average difference in salary offeredto female names with respect to male names, asreturned by the MixedLM model. illus-trates only statistically significant gaps, where theMixedLM determines the associated p-values forboth Sfemale and Smale to be less than = 0.05.Among the 26 presented occupations, candidateswith female names are consistently offered lessthan their male counterparts on average, with the re-verse only true for DJ, model (Llama 3) and rapper(both LLMs). Llama 3 once again exhibits largeraverage magnitude of gender-based gaps (1.17%)versus GPT-3.5 (0.73%).",
  "Intersectional Analysis": "WithoutBiographies.Weperform1-wayANOVA tests to determine whether the meansalaries offered to the 8 intersectional groups differmeaningfully. illustrates the percentagegaps of the race-gender groups relative to theoverall average salary for these occupations.Our first major observation is that White malenames are offered more by both models. In all 9occupations shown in a, GPT-3.5 offersWhite male names salaries higher than averagethan all other groups. Similarly, Llama 3 favorsthis demographic in 9 out of 10 occupations to aneven higher degree of discrepancy (b). Incontrast, Hispanic and Asian names, particularlyfemale, tend to have offers lower than average at ahigher magnitude across both models.Second, GPT-3.5 shows smaller salary gaps com-pared to Llama 3. Pastor is the occupation withthe largest gaps (from -3.31% for AM to 5.85%for WM), followed by physician and composer forGPT-3.5. For Llama 3, surgeon displays even largerdiscrepancy (-10.24% for BF to 13.29% for WM),with comedian, composer, physician and poet show-ing notable gaps. Llama 3 tend to give male nameshigher offers over female names of the same race. accountant architect attorney chiropractor comedian composer dentist dietitian dj filmmaker interior-designer journalist model nurse painter paralegal personal-trainer photographer physician professor psychologist rapper software-engineer surgeon teacher yoga-teacher",
  "(b) Percentage gaps in salaries by Llama 3": ": Heatmaps for intersectional percentage gaps relative to the average salary recommended to all candidatesfor respective occupations, when biographies are not presented. Only occupations with statistically significant resultsare shown. White male names get higher offers by both models. Llama 3 shows significantly higher discrepanciesthan GPT-3.5 along both racial and gender lines. WithBiographies.WeconstructanotherMixedLM analysis with similar setup as inprevious section, but with race-gender as theindependent variable and White male set as thereference group ( = 0.05). The correspondingstatistically significant differences in amountsoffered to the other 7 race-gender groups (inpercentage) are also displayed. presentsthe aggregate number of occupations the LLMsoffer these race-gender groups less (and more)than White male names. shows thecorresponding scatter plots. Full numeric detailsare shown in for all 28 occupations. Compared to their male counterparts, fe-male names are offered lower salaries more fre-quently than White male names. In ,White female names are almost always offeredless than White male names by both GPT-3.5 andLlama 3. Black female names receive lower salaryoffers than White male names in 6 occupationsby GPT-3.5 and 11 by Llama 3, while Black male names only do so in 1 and 2 occupations, respec-tively. Similar patterns are observed for Asian andHispanic female vs. male names. Although theirmagnitudes vary, Llama 3 generally shows largernegative gaps for female names relative to Whitemale names across occupations (a). We observe two major trends. First, compared toother non-White groups, Black names are offeredmore than White male names in significantly highernumber of occupations. For the same gender andmodel, Black names outperform other non-Whitenames in terms of the number of occupations wherethey are favored over White male names (No. Occ.More in ). Second, overall, positive percent-age gaps for names of all other race-gender groupsrelative to White male names cluster at approxi-mately under 2%, though outliers exceeding 4%still exist (b). Though not extremely largein magnitude, the very presence of these dispari-ties in LLMs behaviors is alarming as they canpropagate inequality to stakeholders if deployed.",
  "We quantify the discrepancy between LLMs salaryoffers and recent earning statistics in the U.S": "Comparison of Median Salaries.The latest pub-lished American Community Survey (ACS) in2022 administered by the U.S Census Bureau re-ports the median earnings of various demographicsacross a range of occupations (U.S. Department ofLabor, 2022). We collect and compare the avail-able statistics for 18 out of 28 BiasinBios occupa-tions with the median salaries recommended by theLLMs in the previous experiments (). Overall, we see that LLM-projected mediansalaries highly correlate with the U.S median earn-ings. While all Pearson correlation coefficientsexceed 0.9 (), GPT-3.5-projected salariesMean Average Percentage Errors (MAPE) relativeto their U.S reported counterparts are 13% to 30%less than Llama 3s, with also smaller standard de-viation of errors, depending on whether candidatesbiographies are presented. It is important to notethat the increase in errors might be due to the highvariance within our samples of biography. Comparison of Gender Pay Gaps.As mediansare robust against outliers, the LLM-recommendedmedian salaries are almost identical across genders.Thus, we perform the following analysis using theLLM-projected mean salaries for 16 occupationsagainst U.S reported statistics instead. 2 We see that LLM-projected gender salary gapsare still significantly less than U.S datas on av-erage. The 2022 ACS reports that females makemore than males in only 3 of 16 occupations (di-etitian, interior designer, paralegal), with the aver-age absolute percentage gap between the mediansalaries of the 2 genders at 13.03% (). Incontrast, the average gender gaps between LLMsrecommended mean salaries are all less than 1.01 0.82% (). The average MAEs with re-spect to U.S statistics remain consistent around 12units for both LLMs with comparable variance. Comparison of Intersectional Pay Gaps.Wecompare the overall median earnings of 8 inter-sectional groups as reported by the ACS 2022 in (U.S. Census Bureau, 2022) with the corre-sponding mean salaries recommended by the mod-",
  ": U.S reported median earnings for 8 intersec-tional groups by ACS 2022 (White male as reference,versus corresponding mean salaries offered by LLMsfor names in these groups": "els. In , earnings (from U.S statistics) andsalaries (from models) of all other groups are com-pared against White males median earning.We observe that variance in LLM-projectedsalary differences is much narrower than corre-sponding U.S statistics. The range between thelowest median earning (Hispanic female) and thehighest (Asian male) is 63%, while for all models,this figure does not exceed 5%. White male al-ways receives the highest or second highest salarycompared to other groups, regardless of setting. Incontrast, Hispanic female is always the lowest orsecond lowest paid group. Despite being the high-est earning group in the U.S, Asian male is neveroffered the highest salary by any LLM.Additionally, Llama 3 recommends considerablyhigher salaries than GPT-3.5 and U.S statistics.While both models tend to offer each group highersalaries than the reported median earnings, Llama3s mean offerings exceed the respective GPT-3.5scounterparts on average 9.5% without candidatesbiography. This average jumps to 21.9 when bi-ographies are presented ( ).",
  "Bias Mitigation Strategies": "As our work reveals the potential LLM-propagatedinequality in the allocation of employment due tofirst name preference, the discussion to reduce thisbias becomes even more important. In recent years,bias mitigation techniques have garnered much in-terest in the research community. We discuss threestrategies below that could potentially reduce theobserved disparity in LLM-powered hiring. Name-blind RecruitmentThe simplest ap-proach may be name-blind recruitment, which sim-ply seeks to reduce bias by removing the can-didates name from consideration (Meena, 2016;Vivek, 2022). Having been shown to produce var-ious degrees of success, name-blind recruitmentwould require employers to integrate the name-removal process in their LLM-powered pipeline,which may need further scrutiny to ensure fairnessto applicants (Vivek, 2018). Bias-aware Finetuning and Prompt EngineeringThe first approach involves modifying the LLMs di-rectly to encourage fair behaviors (Garimella et al.,2022; Lin et al., 2024). The latter involves modi-fying the prompt used to interact with the modelto reduce bias (Li et al., 2024a; Dong et al., 2024).These methods could be combined to target biasreduction at multiple checkpoints of deployment. Post-hoc ProcessingThis approach relies onanalysis done on the generated outputs of themodels with respect to certain metrics (Cui et al.,2021). Post-hoc processing may involve human-in-the-loop as a checking-and-balance mechanismto regulate both human and machine factors (Gillet al., 2020). Recent works have investigated us-ing LLMs explanations to aid in enhancing inter-pretable decision-making (Dai et al., 2022).",
  "Name-based biases exhibited by LLMs are notconsistent across settings.For instance, female": "names are preferred over male names in gender-inclusive hiring, yet often offered less salary for thesame position than their male counterparts. In con-trast, Black names are often overlooked in hiring,but are also offered salary higher than average. Incomparison, White-aligned names are consistentlypreferred in both hiring and salary recommenda-tion, with Hispanic names often on the oppositeend. We surmise that this observation may be a po-tential byproduct of the alignment tuning processthat many current LLMs undergo (Street, 2024;Ouyang et al., 2024). Further investigation is war-ranted to understand the underlying mechanism ofthis seemingly counterintuitive artifact. Intersectional bias needs to be closely examined.The gaps in salaries offered to male and femalenames by LLMs may not drastically differ at firstglance. However, our intersectional analyses high-light significant disparity in offers dealt to non-White female names, particularly those of Hispanicbackground. Our findings further underscore theimportance of intersectional analysis to uncoverpotentially unseen disparities. Model selection and calibration for use case isimportant to reduce bias.Our results showcasethat prompting LLMs to choose one among severalcandidates arguably magnify the risk of preferen-tial treatment, and thus should be avoided. ThoughLlama 3 displays larger magnitude of bias thanGPT-3.5, its open-source nature lends itself to moremitigation strategies (Zhou et al., 2023; Qureshiet al., 2023; Wang and Russakovsky, 2023). Con-sideration of the risks, challenges and rewards be-comes crucial in the ethical deployment of LLMs.",
  "Limitations": "We acknowledge the limited number of LLMstested in our work. Though there are many existingmodels, we opt for the 2 most recognizable repre-sentatives of proprietary and open-source modelsat the time of writing. We encourage researchers and Machine Learning practitioners to investigateother models from alternative platforms.Though we attempt to construct a sizable pool offirst names, our collection still does not appropri-ately capture the diversity of names in the UnitedStates, let alone other nationalities. Furthermore,our research is restricted to first names. However,last names may also provide inferential signalsabout the candidates backgrounds, and thus merittheir own investigation.Furthermore, our analysis is limited to 4racial/ethnic groups due to the availability of re-sources and data. In the United States, there existother groups to consider (Native American/AlaskanNative, Native Hawaiian ), and more importantly,people of multi-racial backgrounds. We invite fur-ther research to incorporate these groups.There also exist temporal and geographical con-straints. GPT-3.5-Turbos cutoff date of their train-ing materials is September 2021; Llama 3 is re-leased in early 2024 (Meta, 2024). The U.S statis-tics are available for the years 2022 and 2023. Thus,the LLMs knowledge cutoff may be affected af-ter updates. The analysis in our paper is restrictedto U.S-based names and statistics. It is possiblethat some of the observed disparity in outcomesby LLMs correlate with the popularity of certainnames in the training data. Future studies could ex-pand cross-cultural/national settings to investigatedifferences in trends.Finally, we acknowledge that there are multi-ple ways LLMs could be applied to employmentrecommendation in practice. Though our work fo-cuses only a number of specific use cases to revealbias, our findings serves as a cautionary tale onbias for practitioners who desire to utilize LLMsfor their applications. We encourage researchersto peruse the growing body of literature on biasmitigation in Machine Learning in their use cases(Zhou et al., 2023; Zhang et al., 2024).",
  "Ethics": "This work carries minor risks; it identifies chal-lenges with using LLMs in employment decisionpipelines which hopefully reduces (rather than ex-acerbates) such potential uses. It focuses on En-glish only, and biases from a very U.S. perspective,amplifying the exposure of that language/culture.This project did not include data annotation, andonly used freely available datasets consistent withtheir intended uses.",
  "Acknowledgement": "This work is funded by the University of Mary-lands Institute for Trustworthy AI in Law & Soci-ety (TRAILS). We would like to thank Haozhe Anand Rachel Rudinger at the University of Marylandfor their support during the conceptualization ofthis paper. We thank the service of ACL ARR re-viewers, area chairs and the editors of the EMNLPconference for our papers publication. Haozhe An, Christabel Acquaye, Colin Kai Wang, andRachel Rudinger. 2024. Do large language modelsdiscriminate in hiring decisions on the basis of race,ethnicity, and gender? In Proceedings of the Associ-ation for Computational Linguistics: ACL 2024.",
  "Tyler Rose Clemons. 2014.Blind injustice:Thesupreme court, implicit racial bias, and the racialdisparity in the criminal justice system. Am. Crim. L.Rev., 51:689": "Sen Cui, Weishen Pan, Changshui Zhang, and Fei Wang.2021. Towards model-agnostic post-hoc adjustmentfor balancing ranking fairness and algorithm utility.In Proceedings of the 27th ACM SIGKDD Confer-ence on Knowledge Discovery & Data Mining, pages207217. JessicaDai,SohiniUpadhyay,UlrichAivodji,Stephen H Bach, and Himabindu Lakkaraju. 2022.Fairness via explanation quality: Evaluating dispari-ties in the quality of post hoc explanations. In Pro-ceedings of the 2022 AAAI/ACM Conference on AI,Ethics, and Society, pages 203214.",
  "Xiangjue Dong, Yibo Wang, Philip S Yu, and JamesCaverlee. 2024. Disclosure and mitigation of genderbias in LLMs. arXiv preprint arXiv:2402.11190": "Aparna Garimella, Rada Mihalcea, and Akhash Amar-nath. 2022. Demographic-aware language modelfine-tuning as a bias mitigation technique. In Pro-ceedings of the 2nd Conference of the Asia-PacificChapter of the Association for Computational Lin-guistics and the 12th International Joint Conferenceon Natural Language Processing (Volume 2: ShortPapers), pages 311319. Navdeep Gill, Patrick Hall, Kim Montgomery, andNicholas Schmidt. 2020.A responsible machinelearning workflow with focus on interpretable mod-els, post-hoc explanation, and discrimination testing.Information, 11(3):137. Shashank Gupta, Vaishnavi Shrivastava, Ameet Desh-pande, Ashwin Kalyan, Peter Clark, Ashish Sabhar-wal, and Tushar Khot. 2023. Bias runs deep: Implicitreasoning biases in persona-assigned llms. In TheTwelfth International Conference on Learning Repre-sentations.",
  "Congzhi Zhang, Linhai Zhang, Deyu Zhou, and Guo-qiang Xu. 2024. Causal prompting: Debiasing largelanguage model prompting based on front-door ad-justment. arXiv preprint arXiv:2403.02738": "Fan Zhou, Yuzhou Mao, Liu Yu, Yi Yang, and TingZhong. 2023. Causal-debias: Unifying debiasingin pretrained language models and fine-tuning viacausal invariant learning. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 42274241. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2022. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations.",
  "A.1Curation of Names": "We leverage the dataset by (Rosenman et al., 2023), which provides a compilation of names from voterregistration files of 6 U.S Southern States. This dataset contains 136,000 first names, 125,000 middlenames and 338,000 last names along with imputed probabilities for each names association with 5racial/ethnic groups: White, Black, Hispanic, Asian and Other.We infer the gender for these names by cross-referencing the U.S Social Security Agencys database,which records the total frequency a name is registered by a male or female individual. The probability ofa name being a particular gender {male, female} , if existing in the SSA database, is calculated as:",
  "total frequency": "The majority gender for each name is designated when the corresponding P(gender|name) 0.5.Names whose appeared fewer than 200 times (top 50% of the Rosenman et al. (2023) database) isremoved from the candidate pool. We then randomly select 40 first names for each gender with conditionalprobability P(race|name) 0.9, where race {White, Hispanic, Asian, Black}. We omit theOther category from this analysis. Hispanic male, Asian male and Asian female names yield insufficientoptions. We thus augment these categories with a dataset by Tzioumis (2018), which draws from theUnited States mortgage information and provides similar associated conditional probabilities for 4,250first name for the same racial categories. From this dataset, we select candidate male and female Asiannames with corresponding probability over 0.8 with frequency of appearance in the top 25% among thenames in this dataset. For the Hispanic male category, we select 30 names from the aforementionedRosenman pool of candidates, and 10 from the Tzioumis pool. For Asian male and Asian female categoriesrespectively, we combine the pools evenly (20 from each) to arrive at the required 40 names.",
  "A.2List of Names used in this Work": "White Males: Bradley, Brady, Brett, Carson, Chase, Clay, Cody, Cole, Colton, Connor, Dalton,Dillon, Drew, Dustin, Garrett, Graham, Grant, Gregg, Hunter, Jack, Jacob, Jon, Kurt, Logan, Luke,Mason, Parker, Randal, Randall, Rex, Ross, Salvatore, Scott, Seth, Stephen, Stuart, Tanner, Todd,Wyatt, Zachary White Females: Alison, Amy, Ann, Anne, Beth, Bonnie, Brooke, Caitlin, Carole, Colleen, Ellen,Erin, Haley, Hannah, Heather, Heidi, Holly, Jane, Jeanne, Jenna, Jill, Julie, Kaitlyn, Kathleen,Kathryn, Kay, Kelly, Kristin, Laurie, Lindsay, Lindsey, Lori, Madison, Megan, Meredith, Misty, Sue,Susan, Suzanne, Vicki Black Males: Akeem, Alphonso, Antwan, Cedric, Cedrick, Cornell, Darius, Darrius, Deandre,Deangelo, Demarcus, Demario, Demetrius, Deonte, Deshawn, Devante, Devonte, Donte, Frantz,Jabari, Jalen, Jamaal, Jamar, Jamel, Jaquan, Javon, Jermaine, Malik, Marquis, Marquise, Raheem,Rashad, Roosevelt, Shaquille, Stephon, Tevin, Trevon, Tyree, Tyrell, Tyrone Black Females: Ashanti, Ayanna, Chiquita, Deja, Demetria, Earnestine, Eboni, Ebony, Iesha,Imani, Kenya, Khadijah, Kierra, Lakeisha, Lakesha, Lakeshia, Lakisha, Lashonda, Latanya, Latasha,Latonya, Latosha, Latoya, Latrice, Marquita, Nakia, Octavia, Precious, Queen, Sade, Shameka,Shanice, Shanika, Sharonda, Tameka, Tamika, Tangela, Tanisha, Tierra, Valencia Hispanic Males: Abdiel, Alejandro, Alonso, Alvaro, Amaury, Barbaro, Braulio, Brayan, Cristhian,Diego, Eliseo, Eloy, Enrique, Esteban, Ezequiel, Filiberto, Gilberto, Hipolito, Humberto, Jairo, Jesus,Jose, Leonel, Luis, Maikel, Maykel, Nery, Octaviano, Osvaldo, Pedro, Ramiro, Raymundo, Reinier,Reyes, Rigoberto, Sergio, Ulises, Wilberto, Yoan, Yunior Hispanc Females: Alejandra, Altagracia, Aracelis, Belkis, Denisse, Estefania, Flor, Gisselle, Grisel,Heidy, Ivelisse, Jackeline, Jessenia, Lazara, Lisandra, Luz, Marianela, Maribel, Maricela, Mariela,Marisela, Marisol, Mayra, Migdalia, Niurka, Noelia, Odalys, Rocio, Xiomara, Yadira, Yahaira,Yajaira, Yamile, Yanet, Yanira, Yaritza, Yesenia, Yessenia, Zoila, Zulma Asian Males: Byung, Chang, Cheng, Dat, Dong, Duc, Duong, Duy, Hien, Hiep, Himanshu, Hoang,Huan, Hyun, Jong, Jun, Khoa, Lei, Loc, Manoj, Nam, Nghia, Phuoc, Qiang, Quang, Quoc, Rajeev,Rohit, Sang, Sanjay, Sung, Tae, Thang, Thong, Toan, Tong, Trung, Viet, Wai, Zhong Asian Females An, Archana, Diem, Eun, Ha, Han, Hang, Hanh, Hina, Huong, Huyen, In, Jia, Jin,Lakshmi, Lin, Ling, Linh, Loan, Mai, Mei, My, Ngan, Ngoc, Nhi, Nhung, Quynh, Shalini, Thao,Thu, Thuy, Trinh, Tuyen, Uyen, Vandana, Vy, Xiao, Xuan, Ying, Yoko",
  "A.3LLM Configuration": "For GPT-3.5-Turbo, we accessed this using OpenAIs API. This model costs $0.50 per 1 million inputtokens, and $1.50 per 1 million output tokens 3 at the time of access.For Llama 3 70B-Instruct, we used the weights released by the HuggingFace platform 4. The modelwas loaded on 2 NVIDIA RTX A6000 GPUS, with quantization set to 4 bit. We use the followingconfiguration to prompt our models:",
  "A.4BiasinBios Dataset": "The BiasinBios dataset, proposed by De-Arteaga et al. (2019), contains English biographies createdby the Common Crawl for 28 occupations. For each occupation, there exists a marker that delineateswhether the gender of the original owner of the biography. The original biographies have various lengthswith a long-tail distribution. Thus, we limit our selections to passages that consist between 80 (the 75%percentile) to 120 words to allow the biographies sufficient space to contain relevant details. We first useGPT-4o (version gpt-4o-2024-05-13) with the prompt template in a to replace all referencesto the original personal name with the string \"{name}\". Then, we use the template in b tofurther replace gender-specific pronounces with their gender-neutral counterparts. Finally, we manuallygo through all 560 rewritten biographies to ensure gender-neutrality while still adhere to relevant detailsin the original. a shows a sample data in its original form, and b shows its rewrittengender-neutral version.",
  "OccupationU.S CategoryBiasWomenWhiteBlackAsianHispanic/Latino": "AccountantAccountants and auditors57.073.411.912.78.5ArchitectArchitects, except landscape and naval31.083.63.510.111.3AttorneyLawyers39.586.16.84.45.7BakerBakers65.580.27.45.637.1ChefChefs and head cooks23.358.818.918.520.7ChiropractorChiropractors41.183.66.67.10.7DentistDentists39.577.24.314.58.0DietitianDietitians and nutritionists86.375.913.08.214.5Drywall InstallerDrywall installers, ceiling tile installers, and tapers4.186.87.90.974.3EngineerArchitecture and engineering occupations16.778.06.113.110.1Flight AttendantFlight attendants78.079.716.33.720.0HousekeeperMaids and housekeeping cleaners88.474.016.14.351.9Interior DesignerInterior designers85.390.72.37.09.9JanitorFirst-line supervisors of housekeeping and janitorial workers44.177.217.32.131.8JournalistNews analysts, reporters, and journalists51.374.913.28.815.8NurseRegistered nurses87.472.615.68.98.9ParalegalParalegals and legal assistants83.076.315.35.016.8Personal TrainerExercise trainers and group fitness instructors56.778.910.96.216.8PhotographerPhotographers48.579.49.26.310.4PhysicianOther physicians45.567.49.020.26.7Police OfficerPolice officers14.481.414.22.816.7ProfessorPostsecondary teachers46.678.58.410.97.9PsychologistOther psychologists78.485.57.44.110.7SingerMusicians and singers27.173.615.95.010.9Social WorkerChild, family, and school social workers88.165.826.33.914.2Software EngineerSoftware developers20.254.66.536.26.0SurgeonSurgeons20.075.05.718.62.5TeacherSecondary school teachers56.987.86.12.79.6TranslatorInterpreters and translators74.477.35.712.242.8WaiterWaiters and waitresses68.875.59.98.526.4 : Percentages of employed persons by occupation, sex, race and Hispanic or Latino ethnicity in 2023, aspublished by the U.S Bureau of Labor Statistics for 30 occupations in 2.4 (Bureau, 2023). U.S Category denotesthe original category as published that we match to our list of occupations. Bias indicates whether the occupationappears in the BiasinBios dataset. The percentages of the race groups do not sum to 100% since not all races arepresented. Persons who identified as Hispanic/Latino may be of any race by this methodology.",
  "OccupationU.S CategoryMedian SalaryMenWomenWomen %% Gap": "AccountantAccountants and auditors80,48491,01474,08381.4-18.6ArchitectArchitects, except landscape and naval103,384110,07086,43178.5-21.5AttorneyLawyers153,540162,510134,80583.0-17.0ChiropractorChiropractors85,44691,44264,26870.3-29.7DentistDentists186,740200,421158,30879.0-21.0DietitianDietitians and nutritionists63,25559,93663,446105.95.9Interior DesignerInterior designers63,00659,11763,763107.97.9JournalistNews analysts, reporters, and journalists67,72168,56867,33698.2-1.8NurseRegistered nurses78,93284,87977,58291.4-8.6ParalegalParalegals and legal assistants57,19555,72257,420103.03.0Personal TrainerExercise trainers and group fitness instructors40,98241,79640,10395.9-4.1PhotographerPhotographers48,59552,01441,40879.6-20.4PhysicianOther physicians234,274----ProfessorPostsecondary teachers81,49288,74075,21284.8-15.2PsychologistOther psychologists96,483106,46789,72384.3-15.7Software EngineerSoftware developers126,647129,101115,49589.5-10.5SurgeonSurgeons343,990----TeacherSecondary school teachers63,63666,45361,44892.5-7.5 : Median annual earnings (in U.S dollars) overall and by gender for 18 BiasinBos occupations as reported theAmerican Community Survey (ACS) in 2022 (U.S. Department of Labor, 2022). Women % denotes the percentageof womens median earning over that of men. % Gap denotes the percentage difference between womens earningand mens. Data for physician and surgeon by gender not available as they exceed the 250,000 reporting ceiling byACS methodology. Overall median earning for surgeon extracted from U.S. Bureau of Labor Statistics (2022).",
  "Teacher63,266-0.40.4--0.4-0.5-0.6-0.970,269-1.9-0.81.4-1.31.5-YogaTeacher62,547-0.3------63,856-0.80.70.80.7--": ": Percentage gaps of average salaries offered to 7 intersectional race-gender groups compared to thoseoffered to White Males (WM, listed in US dollars) by 2 LLMs for all 28 occupations, when gender-neutral biographiesare provided. WF: White Female, BM: Black Male, BF: Black Female, HM: Hispanic Male, HF: Hispanic Female,AM: Asian Male, AF: Asian Female. Missing values indicate no statistically significant difference observed.",
  "(b) Llama 3": ": Percentage breakdown for races of names chosen by GPT-3.5 and Llama 3 for 40 occupations by gender.White names are disproportionately favored by LLMs, followed by Asian names. Llama 3 shows less preference forWhite names than GPT-3.5. Distribution of races are not always consistent across genders for the same occupation.",
  ": Sample biographies drawn from the occupation dentist after 2 stages of rewriting by GPT-4o": "The following biography belongs to a person. If explicitly referenced, replaceany instance of this persons name with the string \"name\".Keep pronounreferences like he/she. Do not replace any other entitys name if mentioned.For example, BIO: John Doe starts his work at X this year. Johns work is great. He is nice.Say hi to JoeEDITED: name starts his work at X this year. names work is great. He is nice.Say hi to nameBIO: bioEDITED: _____",
  "Yoga Teacher": "75.04.06.015.0 46.07.05.541.5 73.010.56.010.5 76.55.06.012.5 46.512.513.028.0 60.59.55.025.0 52.517.010.020.5 46.011.515.027.5 55.55.57.032.0 64.05.58.522.0 38.021.514.526.0 20.010.08.561.5 44.08.52.545.0 62.59.57.520.5 55.018.012.015.0 52.513.016.518.0 57.510.510.521.5 46.510.011.032.5 77.59.56.56.5 50.517.519.013.0 69.012.09.010.0 55.59.59.525.5 73.511.56.09.0 60.014.010.515.5 66.013.57.013.5 70.56.08.015.5 52.512.57.527.5 51.517.012.519.0 46.535.05.013.5 63.09.56.521.0 69.011.58.511.0 12.544.011.032.5 44.030.513.512.0 53.524.512.010.0 40.55.53.550.5 42.513.07.537.0 82.55.05.57.0 18.04.014.563.5 59.510.06.524.0 58.011.07.523.5 GPT-3.5-Turbo WhiteBlackHispanicAsian 69.56.08.516.0 23.510.028.038.5 66.514.011.58.0 60.06.518.515.0 30.58.528.532.5 53.514.510.022.0 65.012.55.017.5 35.010.022.532.5 38.510.518.032.5 50.514.520.015.0 16.036.518.029.5 21.512.518.547.5 37.010.59.043.5 34.016.018.032.0 66.512.513.57.5 33.515.040.511.0 36.014.030.020.0 22.524.027.526.0 69.59.011.510.0 33.023.027.516.5 65.010.012.013.0 31.510.027.531.0 61.515.017.06.5 75.014.58.02.5 49.026.016.58.5 44.014.014.028.0 57.014.04.025.0 37.521.017.524.0 65.023.56.05.5 41.018.014.027.0 61.013.514.511.0 5.063.018.014.0 37.529.519.014.0 40.532.023.54.0 32.511.010.546.0 46.09.510.034.5 74.06.010.59.5 4.09.027.060.0 70.57.011.011.5 20.516.531.531.5 Llama 3"
}