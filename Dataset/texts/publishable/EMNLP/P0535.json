{
  "Abstract": "Large language models (LLMs) have demon-strated exceptional abilities across various do-mains. However, utilizing LLMs for ubiqui-tous sensing applications remains challengingas existing text-prompt methods show signifi-cant performance degradation when handlinglong sensor data sequences. We propose a vi-sual prompting approach for sensor data us-ing multimodal LLMs (MLLMs). We designa visual prompt that directs MLLMs to uti-lize visualized sensor data alongside the tar-get sensory task descriptions.Additionally,we introduce a visualization generator thatautomates the creation of optimal visualiza-tions tailored to a given sensory task, eliminat-ing the need for prior task-specific knowledge.We evaluated our approach on nine sensorytasks involving four sensing modalities, achiev-ing an average of 10% higher accuracy thantext-based prompts and reducing token costsby 15.8. Our findings highlight the effec-tiveness and cost-efficiency of visual promptswith MLLMs for various sensory tasks. Thesource code is available at",
  "Introduction": "Large language models (LLMs) have shown re-markable performance in tasks across diverse do-mains, including science, mathematics, medicine,and psychology (Bubeck et al., 2023). The recentadvent of multimodal LLMs (MLLMs), e.g., GPT-4o (OpenAI, 2024), has further expanded their ca-pabilities to images and audio inputs, broadeningtheir use in fields such as industry and medicalimaging (Yang et al., 2023). Meanwhile, sensor dataincluding measure-ments from smartphones, wearables, IoT (Dianet al., 2020), and medical equipment (Pantelopou-los and Bourbakis, 2009)holds potential for ubiq-uitous applications when effectively integrated withMLLMs.Sensory tasks involve extensive and Based on the data, classify whether the user is walking or running.Given sensor data: [[0.65, 0.62, -0.36], [0.65, 0.63, -0.37], [0.65, 0.63, -0.35], [0.65, 0.63, -0.36], [0.65, 0.63, -0.36],[0.65, 0.61, -0.37], [0.64, 0.61, -0.36], [0.64, 0.61, -0.36],",
  ",910 tokens": ": An example of solving a sensory task using anMLLM with visual prompts. The visualization genera-tor generates an appropriate visualization for the givensensor data, and the visualized data is provided as animage to the MLLM for solving the task. significant applications, ranging from authentica-tion (Abuhamad et al., 2020) and healthcare (Wanget al., 2019) to agriculture (Sishodia et al., 2020)and environmental monitoring (Feng et al., 2019).However, MLLMs remain underutilized. The diver-sity of sensors (Wang et al., 2019) and the hetero-geneity among them (Stisen et al., 2015) hinder theimplementation of a foundational model that gener-alizes across various sensing tasks. The expensivedata collection (Vijayan et al., 2021) often resultsin insufficient training data, further complicatingthe development of such capability. Recent studies explored leveraging pre-trainedLLMs to solve general sensory tasks (Yu et al.,2023; Liu et al., 2023; Kim et al., 2024). Oneapproach extracts task-specific features from sen-sor data and composes them as prompts (Yu et al., 2023). However, designing such prompts requiresspecific domain knowledge. Alternatively, incor-porating raw sensor data as text prompts (Kimet al., 2024; Liu et al., 2023) has been a widelyused method to handle sensory data with LLMsas a more generalizable solution. Yet, we empir-ically found that providing raw sensor data withtext prompts shows poor performance in real-worldsensory tasks with long-sequence inputs and incurshigh costs due to an extensive number of tokens.To address these challenges, we propose provid-ing visualized sensor data as images to MLLMsthat support visual inputs. Leveraging MLLMsgrowing ability to interpret visual aids (Yang et al.,2023), we explore their effectiveness in analyzingplots generated from sensor data. We designed avisual prompt comprising visualized sensor dataand task-specific instructions to solve sensory tasks.In addition, we present a visualization generatorthat enables MLLMs to independently generateoptimal visualizations using tools available in pub-lic libraries. This generator filters potential visu-alization methods based on the task descriptionand assesses the resulting visualizations of eachmethod to determine the best visualization. Fig-ure 1 compares the existing text-only prompts withour method for sensory tasks.Evaluations on nine sensory tasks involving fourdifferent modalities showed that visual promptsgenerated from the visualization generator signifi-cantly improved performance by an average of 10%while reducing token costs by 15.8 comparedwith the existing baseline. Our findings highlightthe effectiveness and efficiency of visualized sensordata with MLLMs in various applications.We summarize our contributions as follows:",
  "LLMs with sensor data. Sensory tasks involvesequences of numbers indicating values over time": "Initial research for handling sequential data focusedon time-series forecasting (Zhang et al., 2024c).Converting time-series data into text prompts forforecasting has been proposed in PromptCast (Xueand Salim, 2023) and LLMTime (Gruver et al.,2024). Other studies (Zhou et al., 2023; Jin et al.,2023a) used specialized encoders to create embed-dings compatible with pre-trained LLMs.Beyond forecasting, LLMs have been exploredin healthcare for their ability to answer questionsusing physiological sensor data (Liu et al., 2023).For example, LLMs have been used for ECG diag-nosis (Yu et al., 2023) by integrating ECG-specificfeatures and knowledge from ECG databases. Pen-etrative AI (Xu et al., 2024) and Health-LLM (Kimet al., 2024) have used raw sensor data in textprompts to solve health problems without task-specific processing. Our study examines whetherexisting methods can generalize to broader sens-ing tasks with high-frequency, long-duration data.Building upon these works, we propose visualizingsensor data for MLLMs to improve their perfor-mance and cost efficiency.Multimodal large language models (MLLMs).Advancements in MLLMs (Zhang et al., 2024a)have equipped popular models such as Chat-GPT (OpenAI, 2022) with vision capabilities (Ope-nAI, 2024). Recent studies explored the in-contextlearning (Brown et al., 2020) abilities of MLLMs,showing that they can understand images withthe interleaved text and few-shot examples (Tsim-poukelli et al., 2021; Alayrac et al., 2022). This ca-pability has been applied in medical diagnostics, in-cluding analyzing radiology and brain images withaccompanying text instructions (Wu et al., 2023).Our work explores using MLLMs to analyze visu-alized sensor data for broader applications.Using tools with LLMs. Recent research hasshown that augmenting LLMs with external toolscan extend their capabilities. Toolformer (Schicket al., 2024) enables LLMs to access publicAPIs and search engines, while Visual Program-ming (Gupta and Kembhavi, 2023) uses LLMs togenerate and execute codes. HuggingGPT (Shenet al., 2024) and Chameleon (Lu et al., 2024) in-tegrated multiple expert models to enhance func-tionalities. Recently, Data Interpreter (Hong et al.,2024) enabled LLMs to analyze data and build task-specific models for data interpretation. Building onthem, our work leverages MLLMs to utilize sensordata visualization tools. Importantly, unlike exist-ing approaches that rely on external tools as the pri- mary task solvers, we propose positioning MLLMsas the main solvers for sensory tasks, leveragingtheir in-context learning abilities. Our approachaims to eliminate the need for additional data col-lection and training, thereby addressing the scarcityof public resources for sensory tasks. Furthermore,we introduce a design in which MLLMs performdemonstration-based assessments to evaluate theirtask-solving effectiveness, ensuring optimal visual-ization for specific tasks.",
  "Limitations of Representing SensorData as Text-based Prompts": "Existing approaches for grounding language mod-els with sensor data primarily rely on text-basedprompts (Liu et al., 2023; Jin et al., 2023b; Zhanget al., 2024c; Yu et al., 2023). One approach usesprompts with specialized features extracted fromsensor data for specific tasks, such as R-R inter-vals for ECG-based applications (Yu et al., 2023).While this approach effectively handles known sen-sory tasks, feature-based prompts often require do-main knowledge, which is not generalizable fornon-expert users. Instead, a more common ap-proach (Kim et al., 2024; Xu et al., 2024; Liu et al.,2023) incorporates raw sensor data sequences di-rectly into prompts without data processing. How-ever, most studies focus on short sequences (e.g.,fewer than 100 elements) (Kim et al., 2024) andsimple tasks (e.g., binary classification) (Liu et al.,2023).Real-world sensor data often entail long numericsequences with high sampling rates and long du-rations. For example, arrhythmia detection (Wag-ner et al., 2020) requires ECG data sampled at100Hz over 10 seconds, resulting in 1,000 elements.This section investigates the limitations of usingtext-based prompts to represent such complicatedsensor data in language models. We focus on thecapability to interpret sensor data and the tokenconsumption costs associated with long numericsequences.Language models struggle to interpret longnumeric text sequences. Language models in-terpret simple numeric sequences by performingarithmetic operations (Achiam et al., 2023) and un-derstanding sequential data (Gruver et al., 2024;Mirchandani et al., 2023). However, we empiri-cally revealed that their performance declines sig-nificantly with longer sequences inside the prompt,such as those exceeding 100 numbers, common in",
  ": Performance of GPT-4o on arithmetic opera-tion (mean prediction) and pattern recognition (sine andsawtooth wave classification) tasks for varying lengths": "sensor data.We conducted experiments with two specifictasks: mean prediction to evaluate arithmetic ca-pabilities (Pirttikangas et al., 2006) and wave clas-sification to assess pattern recognition (Liu et al.,2016) in sequences. The defined tasks representedthe basic functionalities for sensor data interpreta-tion, serving as typical feature extraction methods.Using randomly generated sine and sawtooth waveswith varying lengths, we asked a language model,GPT-4o (OpenAI, 2024), to calculate mean valuesand classify wave types using one-shot examplesfor each task. Each task was repeated 30 times toensure robustness. shows the results. In arithmetic op-erations, error rates consistently increased withthe length. In pattern recognition, performancedeclines significantly for sequences longer than100 elements, approaching the performance of arandom classifier at 500 elements. While recentmodels such as GPT-4o, with its 128K context win-dow, support long input lengths, our results indicatethat interpreting sensor data with long numeric se-quences still remains challenging.Sensor data in text is costly. The computa-tional and financial burdens for API users of lan-guage models scale with the number of tokens inthe prompt. Representing sensor data in textual for-mat leads to extensive token usage, thereby increas-ing costs. For instance, performing passive sens-ing to track activities with smartphone accelerome-ters (Stisen et al., 2015) uses sensor data sampledat 100 Hz. Collecting this data over a minute re-sults in a prompt of 18K numbers, translating to90K tokens. This leads to a huge cost of $450 perhour when using GPT-4o API to classify six activi-ties with one example for each. Higher samplingrates or longer durations further increase the costs,making such applications infeasible.",
  "Target data image": "N example images### InstructionYou are an expert in sensor data analysis. Given the sensor data, determine the correct answer from the options listed in the question. Provide the answer with the format of <answer>ANSWER</answer>, where ANSWER corresponds to one of the options listed in the question. If the answer is not in the options, choose the most possible option.",
  "Sensor data": ": Overview of our visual prompt. Sensor data are transformed into annotated images with labels andvisualization methods. Additionally, instructions are provided to the MLLM, detailing the task and relevant datadescriptions. These instructions guide the MLLM on effectively utilizing the provided images to solve the task. Transition to visual prompts. Language mod-els such as ChatGPT (GPT-4o (OpenAI, 2024)) andGemini (DeepMind, 2024) have expanded capabili-ties to include multimodal inputs (e.g., vision andaudio). Recent Multimodal Large Language Mod-els (MLLMs) demonstrate an increasing ability toidentify patterns and interpret visual data (Achiamet al., 2023). This opens new opportunities for sen-sory tasks, as sensor data are often visualized foranalysis. Visualizations make complex data moreinterpretable and condense long data sequencesinto a single image, significantly reducing tokencosts. Building on this capability, we exploit visu-alized sensor data instead of text-based prompts.",
  "Visual Prompt Design": "To leverage MLLMs for sensory tasks, we proposea visual prompt, as illustrated in . The keyidea is to transform numeric sequences of sensordata into visual plots using various methods, suchas raw waveforms and spectrograms. Detailed in-formation about these visualization methods is in.2. For few-shot examples, each plot in-cludes a label as a title above it (i.e., {{Label ofexample X}}). For unlabeled target data used inqueries, the title is simply stated as target data.",
  "Visualization Generator": "In our proposed visual prompt, the choice of vi-sualization method is crucial, as it significantlyinfluences the MLLMs ability to comprehend thesensor data. For example, raw waveform plots areideal for tasks involving amplitude pattern recogni-tion over time, while spectrograms (Ito et al., 2018)are suitable for tasks relying on frequency features.We introduce a visualization generator that auto-matically chooses the most suitable visualizationtool from available public libraries, enabling non-expert users to effectively utilize visual prompts.This generator operates in two main phases: (i) vi-sualization tool filtering and (ii) visualization selec-tion (see ).Visualization tool filtering. Public libraries offera vast array of sensor data visualizations. However,trying each out to identify the optimal visualizationis computationally expensive. To minimize thecost, we employ a filtering approach. By providingavailable visualization tools, descriptions of thetask, and data collection, we ask MLLMs to selecta list of visualization methods useful for the targettask.As shown in (green box), we pro-vide a full list of available visualization methodsfound in public libraries (e.g., Matplotlib (Hunter,2007), Scipy (Virtanen et al., 2020), and Neu-rokit2 (Makowski et al., 2021)) along with task and",
  "Visualization Tools": ": Overview of our visualization generator. First, visualization tool filtering generates a filtered list ofvisualization tools from public libraries based on the task and data descriptions. Next, visualization selectiongenerates and selects the most effective visualization by asking MLLMs to observe visualized sensor data preparedfor the task using all the filtered visualization methods. data collection descriptions to MLLMs. We alsoleverage the in-context learning ability of MLLMsto enhance response quality by providing demon-strations of optimal visualizations chosen for differ-ent tasks. The MLLM is instructed to output a listin JSON format, which is suitable for automatedparsing at a later stage. Appendix E shows the fulllist of available visualization tools and demonstra-tions.Visualization selection Sensor data exhibit vari-ations by instance due to user-specific behaviors,environmental factors, or device settings (Stisenet al., 2015), which cannot be fully captured in taskand data descriptions. This variability limits thereliability of selecting visualizations based solelyon the descriptions. To address this, we visualizethe sensor data using all filtered visualization toolsand ask the MLLM to select the one that providesthe best visual information for the task.The blue box in illustrates this proce-dure. First, different visualizations are generatedusing the filtered tools. With the images, we in-struct the MLLM to select the best visualizationby providing a textual prompt, including the visu-alization methods, task, and data details. We foundthat MLLM often makes incorrect decisions by pri-oritizing the task description over the visual aids.To prioritize visual efficacy, we explicitly instruct the MLLM to avoid relying on prior knowledgeabout sensor data and focus on the provided im-ages. Finally, our automated framework conveysthe selected visualization to the visual prompt fortask solving.",
  "Setups": "We assume a practical scenario where non-expertusers attempt to solve sensory tasks using MLLMs(1) without prior knowledge of relevant featuresand (2) without external resources to fine-tune theMLLM. Given the constraints, we leveraged thefew-shot prompting (Brown et al., 2020) approach.For the main evaluation, we used 1-shot exampleswhere users provide the MLLM with minimal ex-amples to guide task-solving.Sensory tasks We established nine different sen-sory tasks across four sensor modalities:ac-celerometer, electrocardiography (ECG) sensor,electromyography (EMG) sensor, and respirationsensor. We used three datasets for tasks using ac-celerometers: HHAR (Stisen et al., 2015) for basichuman activity recognition (running and walking), : Comparison of the text prompts and visual prompts for solving sensory tasks using GPT-4o. The highestaccuracy values are highlighted in bold. The visual prompts (multi-shot) utilize the maximum number of examplesby matching the token size of the 1-shot text prompts.",
  "(15.8)": "UTD-MHAD (Chen et al., 2015) for complex ac-tivity recognition with fine-grained arm motions,and a swimming style recognition dataset (Brun-ner et al., 2019). We use the PTB-XL (Wagneret al., 2020) dataset for the arrhythmia diagnosistasks that use ECG. The dataset includes detectiontasks for four different types of arrhythmia symp-toms. For EMG data, we used a dataset (Ozdemiret al., 2022) for hand gesture recognition. Finally,we used a stress detection task using respirationsensors provided by the WESAD (Schmidt et al.,2018) dataset. Details on each task, including theclasses, sampling rates, windowing durations, andspecific configurations, are in Appendix F.Data processing. We normalized data using themean and standard deviation values calculated foreach user. Test splits were created by randomlysampling 30 samples per class.For the UTD-MHAD dataset, we sampled 10 samples per classdue to the limited sample availability. Examples offew-shot prompting were randomly sampled, ensur-ing no overlap with the test set. Each task employedthe window sizes and sampling rates specified inthe original dataset descriptions (see Appendix F).Baselines. We set text-only prompts for convey-ing sensor data to MLLMs as the main baselineto be compared with our visual prompts. Text-only prompts represented sensor data as numberswithin the prompt. We designed text-only promptsby following the latest prompting studies incorpo-rating sensor data into LLMs for healthcare (Liuet al., 2023). Additionally, to establish an upperbound for task-specific performance, we includeda fully-supervised baseline using neural networkstrained on 75% of the entire data after excludingthe test and validation sets. We adopted architec-tures widely accepted for each type of sensor data: 1D CNNs for activity recognition with accelerome-ters (Chen et al., 2021) and EMG data (Xiong et al.,2021), as well as for WESAD (Vos et al., 2023),and XResNet-101 for PTB-XL (Strodthoff et al.,2023).Implementation. We used GPT-4o from the Ope-nAI API (OpenAI, 2024) as MLLM. The text-onlyprompts contained the same information as the gen-erated visualization to ensure a fair comparisonbetween text-only and visual prompts. For exam-ple, if the visualization generator outputs a plotwith peak notations, the corresponding text-onlyprompt contains the same features, including thepeak values with their indices. When the informa-tion could not fit within the token limit (128K), weused the raw waveform.Metrics. We evaluated the experimental resultsbased on accuracy. We also assess the number oftokens used by each prompt method. Tokens arecounted using the o200k_base encoding used forGPT-4. To estimate the token cost for images inthe same space as text, we follow the computationguidelines provided by OpenAI (OpenAI, 2024).",
  "Results": "Performance. shows the overall perfor-mance of utilizing visual prompts for solving sen-sory tasks. For the same 1-shot prompting, visualprompts consistently showed enhanced accuraciesthan text-only prompts, achieving an average in-crease of 10%. Notably, the UTD-MHAD taskexhibited a significant accuracy gain of up to 33%.See Appendix G for prompt examples with result-ing visualizations.In addition to achieving higher accuracy, visualprompts are more cost-effective. The number oftokens used for visual prompts in shows a",
  ": Accuracy of arrhythmia detection tasks usingvisual and text-only prompts with different shots": "substantial reduction, averaging 15.8 fewer thantext-only prompts. MLLMs calculate token costsfor images within the same token space as textbut with distinct counting criteria. In our experi-ments, GPT-4o counts tokens for images based onthe number of 512512 pixel blocks (N) coveringthe image input, calculated at 85 + 170 N. Ourvisualized sensor data was represented within a sin-gle 512 512 pixel image, regardless of the sensordata length, significantly reducing costs. Note thatthe number of tokens from visual prompts is onlyaffected by the number of examples, as all imagesare the same size. In contrast, text prompts areheavily influenced by high sampling rates and longdurations.To further understand the effectiveness of visualprompts with small tokens, we analyzed the infor-mation capacity at the same token cost. Consider-ing a budget of 500 tokens, text-based prompts caninclude approximately 2,000 ASCII characters. Incontrast, visual prompts can input two 512 512px images. In terms of bytes, 2,000 ASCII char-acters amount to 2 KB, whereas two RGB imagesoccupy 1.57 MB, which is 785 larger. Althoughthis calculation does not directly translate to theexact amount of useful information, it suggests thatwell-designed visual prompts can convey a widerrange of information than text prompts within thesame cost constraint.Effect of number of examples. To investigate theimpact of varying numbers of examples, we ex- perimented using different numbers of examples(1-shot, 3-shot, and 5-shot) within the prompts. Weused the ECG dataset, allowing multiple exampleswith text-only prompts due to its lower token con-sumption. depicts the results. Prompting meth-ods are color-coded (blue for visual and greenfor text-only), and different markers indicate thenumber of shots. We compared the accuracy andcounted the tokens for each setting. We found thatvisual prompts constantly outperformed text-onlyprompts with the same number of examples, indi-cating the robustness of our method in differentfew-shot examples. Additionally, when comparing visual promptsand text-only prompts under the same token bud-get (5-shot visual prompt versus 1-shot text-onlyprompt), visual prompts often performed signif-icantly better (MI and HYP detection).Thishighlights the advantage of token-efficient visualprompts that can utilize more resources for betterperformance under the same token constraint. Unlike our expectations, additional examplesdid not always result in better performance. Thisresult aligns with existing reports indicating thatmore examples do not always guarantee better re-sults (Perez et al., 2021; Lu et al., 2021). We fur-ther hypothesize that a longer context might hin-der the MLLMs ability to retrieve important in-formation (Liu et al., 2024b). Our findings sug-gest that the impact of shots is data-dependent,and effectively utilizing more examples for con-sistent improvement remains an open question forfurther research.Note that our visual promptsconsistently outperformed text prompts, even ondatasets where additional shots negatively affectperformance. This supports that the main improve-ment of our approach stems from using the visualmodality for data interpretation, not merely fromthe token length reduction.Effect of visualization generator. We conductedan ablation study to assess the impact of the visual-ization generator. We compared the visualizationgenerator against two different baselines: (1) usinga fixed visualization that defaults to raw waveformplots, and (2) a method that selects visualizationsbased solely on a text description of the task anddata. For the second baseline, we utilized our vi-sualization tool filtering prompt (see Appendix Gfor an example) to generate a single visualization,rather than filtering multiple tools.",
  ": Examples of ECG visualizations. The visual-ization generator selected the individual heartbeats plot": "by the baselines and our visualization generator.The selection from the description-based method(Desc.-based) and our visualization generator (Vis-Gen) varied primarily based on sensor modali-ties. Our visualization generator mainly selectedraw waveforms, occasionally spectrograms for ac-celerometer tasks (the same for WESAD). For ECGand EMG datasets, it selected specialized visualiza-tions, such as ECG individual heart beats plot. Thedescription-based method also selected modality-aware visualizations, but these differed from thosechosen by ours. For instance, it selected spec-trograms for the accelerometer tasks and signaland peak plots for ECG datasets. The key differ-ence stemmed from whether the MLLM referencedthe visualized image itself. Note that within eachdataset, the target task and data collection protocolwas consistently controlled, so the visualizationsselected for samples within each dataset remainedalmost identical, despite our design allowing forsample-wise visualization selection.The performance comparison of different visu-alization selection methods is shown in .Overall, our visualization generator achieves the : Performance of using different visualizationmethods for visual prompts. We compare a fixed rawwaveform plot (Fixed), visualizations selected basedsolely on a text description (Desc.-based), and visualiza-tions from our visualization generator (VisGen). Oursis highlighted in blue cells, and performances from vi-sualizations on certain tasks that show significant per-formance drops more than 10% compared to the highestare colored red.",
  "RespirationWESAD0.620.600.61": "best or comparable performance. In contrast, thebaseline methods show significant perforemancedegradation in certain tasks. For example, usinga fixed raw waveform leads to a significant 20%performance drop for ECG tasks, as raw wave-forms fail to provide insightful visual insights dueto the complex structure of ECG (). Thisillustrates that a fixed visualization cannot be gen-eralized across different sensory tasks. Similarly, the description-based method faceschallenges with accelerometer tasks.It selectsspectrograms, likely due to the world knowledgefrom public datasets, which may lead the MLLMto consider frequency features as the optimal infor-mation for motion data analysis. However, thedense and complex features in spectrogram im-ages were difficult for the MLLM to interpret,leading to near-random performances. In contrast,our visualization generator compares visualizedimages, consistently avoiding suboptimal choicessuch as spectrograms for accelerometer tasks. Thisself-assessment mechanism ensures that the visual-ization generator selects the optimal visualizationmethod among the possible options.",
  "Conclusion": "We addressed sensory tasks by providing visualizedsensor data as images to MLLMs. We designed avisual prompt to instruct MLLMs in using visual-ized sensor data, provided with textual descriptionsof the task and data collection methods. Addition-ally, we introduced a visualization generator thatautomatically selects the best visualization methodfor each task using visualization tools available inpublic libraries. We conducted experiments acrossnine different sensory tasks and four sensor modal-ities, each with a distinct task. Our results suggestthat the visual prompts generated by our visualiza-tion generator not only improve accuracy by anaverage of 10% over text-based prompts but alsosignificantly reduce costs, requiring 15.8 fewertokens. This indicates that our approach with visualprompts and a visualization generator is a practicalsolution for general sensory tasks.",
  "Acknowledgements": "This research was supported by the MSIT (Min-istry of Science, ICT), Korea, under the GlobalResearch Support Program in the Digital Field pro-gram (RS-2024-00436680) supervised by the IITP(Institute for Information & Communications Tech-nology Planning & Evaluation). This work wassupported by the National Research Foundationof Korea (NRF) grant funded by the Korea gov-ernment (MSIT) (RS-2024-00337007). MSIT:Ministry of Science and ICT. This project is sup-ported by Microsoft Research Asia.",
  "Limitations": "Our study demonstrates the effectiveness of vi-sual prompts on nine different sensory tasks, pri-marily focusing on classification. While visualprompts effectively highlight patterns over im-ages, for tasks requiring numerical retrieval or pre-cise computationswhere exact values are criti-caltext prompts can be more effective due totheir inclusion of specific numeric data, which areomitted in visual representations. Notably, our ap-proach integrates both images and texts in prompts,allowing the inclusion of numerical values in thetext. Determining the optimal distribution of in-formation between images and text to composea prompt that effectively addresses sensory taskspresents a future direction for this work. More-over, the inclusion of numeric values can result",
  "in long prompts, which affect both cost and per-formance. Extracting only the useful information,such as statistics or specific time splits, requiresfurther research": "Visualizing sensor data as plots often presentschallenges. For instance, brain wave analysis us-ing high-density EEG involves up to 256 chan-nels (Fiedler et al., 2022), complicating their rep-resentation in a single visual plot. We denote dif-ferent channels as distinct notations within a plot,making densely populated plots visually indeci-pherable. An alternative method of plotting distinctchannels across separate subplots was explored butresulted in a significant drop in performance (seeAppendix 6). We hypothesize that this limitationarises from the dispersion of information acrossvarious areas, highlighting that effective visualiza-tion of large-channel datasets remains challenging.This underscores the need for improved visualiza-tion techniques in such scenarios.Our visual prompt design does not incorporateChain-of-Thought (CoT) prompting (Kojima et al.,2022). Experiments using zero-shot CoT on ourdatasets revealed inconsistent benefits (see Ap-pendix A), unlike the widely known effect of CoTfor enhancing performance. We suspect this maybe due to the complexities of reasoning over sen-sory data. Given the observation, further researchis needed to develop methods that effectively inte-grate reasoning and interpretation into the decision-making processes for sensor data analysis.Lastly, the high costs of text-only prompts insensory tasks constrained our testing to 30 samplesper class. Expanding the scale as resources allowcould provide a more robust analysis and poten-tially validate a broader spectrum of applications. Mohammed Abuhamad, Ahmed Abusnaina, DaeHunNyang, and David Mohaisen. 2020. Sensor-basedcontinuous authentication of smartphones users us-ing behavioral biometrics: A contemporary survey.IEEE Internet of Things Journal, 8(1):6584. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Kerem Altun and Billur Barshan. 2010. Human activityrecognition using inertial/magnetic sensor units. InHuman Behavior Understanding: First InternationalWorkshop. Proceedings 1, pages 3851. Springer": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in Neural Information ProcessingSystems, 33:18771901. Gino Brunner, Darya Melnyk, Birkir Sigfsson, andRoger Wattenhofer. 2019. Swimming style recogni-tion and lap counting using a smartwatch and deeplearning. In ACM International Symposium on Wear-able Computers, pages 2331. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712. Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz.2015. Utd-mhad: A multimodal dataset for humanaction recognition utilizing a depth camera and awearable inertial sensor. In International conferenceon image processing (ICIP), pages 168172. IEEE. Kaixuan Chen, Dalin Zhang, Lina Yao, Bin Guo, Zhi-wen Yu, and Yunhao Liu. 2021. Deep learning forsensor-based human activity recognition: Overview,challenges, and opportunities. ACM Computing Sur-veys (CSUR), 54(4):140.",
  "Patrique Fiedler, Carlos Fonseca, Eko Supriyanto, FrankZanow, and Jens Haueisen. 2022. A high-density256-channel cap for dry electroencephalography. Hu-man brain mapping, 43(4):12951308": "Marcus Georgi, Christoph Amma, and Tanja Schultz.2015. Recognizing hand and finger gestures withimu based motion and emg based muscle activitysensing. In International Conference on Bio-inspiredSystems and Signal Processing, volume 2, pages 99108. Scitepress. Ary L Goldberger, Zachary D Goldberger, and AlexeiShvilkin. 2017. Clinical Electrocardiography: ASimplified Approach: Clinical Electrocardiography:A Simplified Approach E-Book. Elsevier Health Sci-ences.",
  "John D Hunter. 2007. Matplotlib: A 2d graphics en-vironment. Computing in science & engineering,9(03):9095": "Chihiro Ito, Xin Cao, Masaki Shuzo, and Eisaku Maeda.2018. Application of cnn for human activity recog-nition with fft spectrogram of acceleration and gyrosensors. In ACM international joint conference and2018 international symposium on pervasive and ubiq-uitous computing and wearable computers, pages15031510. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu,James Y Zhang, Xiaoming Shi, Pin-Yu Chen, YuxuanLiang, Yuan-Fang Li, Shirui Pan, et al. 2023a. Time-llm: Time series forecasting by reprogramming largelanguage models. arXiv preprint arXiv:2310.01728. Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang,Siqiao Xue, Xue Wang, James Zhang, Yi Wang,Haifeng Chen, Xiaoli Li, et al. 2023b. Large modelsfor time series and spatio-temporal data: A surveyand outlook. arXiv preprint arXiv:2310.10196. Yubin Kim, Xuhai Xu, Daniel McDuff, CynthiaBreazeal, and Hae Won Park. 2024.Health-llm:Large language models for health prediction via wear-able sensor digital. In Conference on Health, Infer-ence, and Learning, Proceedings of Machine Learn-ing Research, pages 115. PMLR. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inNeural Information Processing Systems, 35:2219922213.",
  "Li Liu, Yuxin Peng, Shu Wang, Ming Liu, and ZigangHuang. 2016. Complex activity recognition usingtime series pattern dictionary learned from ubiquitoussensors. Information Sciences, 340:4157": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2024b. Lost in the middle: How languagemodels use long contexts. Transactions of the Asso-ciation for Computational Linguistics, 12:157173. Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh,Shun Liao, Paolo Di Achille, and Shwetak Patel.2023. Large language models are few-shot healthlearners. arXiv preprint arXiv:2305.15525. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, andJianfeng Gao. 2024. Chameleon: Plug-and-play com-positional reasoning with large language models. Ad-vances in Neural Information Processing Systems,36. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2021. Fantastically orderedprompts and where to find them:Overcomingfew-shot prompt order sensitivity. arXiv preprintarXiv:2104.08786. Dominique Makowski, Tam Pham, Zen J Lau, Jan CBrammer, Franois Lespinasse, Hung Pham, Christo-pher Schlzel, and SH Annabel Chen. 2021. Neu-rokit2: A python toolbox for neurophysiological sig-nal processing. Behavior research methods, pages18. Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,Danny Driess, Montserrat Gonzalez Arenas, Kan-ishka Rao, Dorsa Sadigh, and Andy Zeng. 2023.Large language models as general pattern machines.In Conference on Robot Learning, pages 24982518.PMLR.",
  "Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.True few-shot learning with language models. Ad-vances in Neural Information Processing Systems,34:1105411070": "Susanna Pirttikangas, Kaori Fujinami, and Tatsuo Naka-jima. 2006. Feature selection and activity recognitionfrom wearable sensors. In Ubiquitous ComputingSystems: Third International Symposium. Proceed-ings 3, pages 516527. Springer. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2024.Toolformer: Language models can teach themselvesto use tools. Advances in Neural Information Pro-cessing Systems, 36. Philip Schmidt, Attila Reiss, Robert Duerichen, ClausMarberger, and Kristof Van Laerhoven. 2018. In-troducing wesad, a multimodal dataset for wearablestress and affect detection. In ACM international con-ference on multimodal interaction, pages 400408. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,Weiming Lu, and Yueting Zhuang. 2024. Hugging-gpt: Solving ai tasks with chatgpt and its friendsin hugging face. Advances in Neural InformationProcessing Systems, 36.",
  "Rajendra P Sishodia, Ram L Ray, and Sudhir K Singh.2020. Applications of remote sensing in precisionagriculture: A review. Remote sensing, 12(19):3136": "Allan Stisen, Henrik Blunck, Sourav Bhattacharya,Thor Siiger Prentow, Mikkel Baun Kjrgaard, AnindDey, Tobias Sonne, and Mads Mller Jensen. 2015.Smart devices are different: Assessing and mitigat-ingmobile sensing heterogeneities for activity recog-nition. In ACM conference on embedded networkedsensor systems, pages 127140. Nils Strodthoff, Temesgen Mehari, Claudia Nagel,Philip J Aston, Ashish Sundar, Claus Graff, Jr-gen K Kanters, Wilhelm Haverkamp, Olaf Dssel,Axel Loewe, et al. 2023. Ptb-xl+, a comprehensiveelectrocardiographic feature dataset. Scientific data,10(1):279. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-timodal few-shot learning with frozen language mod-els. Advances in Neural Information Processing Sys-tems, 34:200212. Yunus Emre Ustev, Ozlem Durmaz Incel, and Cem Er-soy. 2013. User, device and orientation independenthuman activity recognition on mobile phones: Chal-lenges and a proposal. In ACM conference on Perva-sive and ubiquitous computing adjunct publication,pages 14271436.",
  "Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong,Dan Su, Chenhui Chu, and Dong Yu. 2024a. Mm-llms: Recent advances in multimodal large languagemodels. arXiv preprint arXiv:2401.13601": "Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023.Extractive summarization via chatgpt for faithfulsummary generation. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 32703278. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,Kathleen McKeown, and Tatsunori B Hashimoto.2024b. Benchmarking large language models fornews summarization. Transactions of the Associa-tion for Computational Linguistics, 12:3957.",
  "AEffect of Zero-shot Chain-of-Thoughts": "We experimented with zero-shot Chain-of-Thought(CoT) prompting (Kojima et al., 2022) by adding\"lets think step-by-step\" to our prompts, testingthis on two accelerometers and two ECG datasets. shows the findings. While CoT promptingis generally known to enhance LLM response qual-ity, our results showed inconsistent performance bydatasets. Notably, CoT consistently dropped perfor-mance for text-only prompts. We analyzed the re-sults by observing the CoT responses, illustrated asexamples in Figures 8 and , showing wrongpredictions with CoT from the HHAR dataset. Wefound that CoT reasoning in text-only prompts pri-marily focused on simple statistical comparisons,such as whether values were higher or lower. Thissimplistic approach proved inadequate for analyz-ing the complexities of sensor data, leading to sub-optimal responses. Likewise, visual prompts indi-cated reasoning centered around terms like \"vari-ations,\" \"periodic,\" and \"stable,\" but they lackedthe necessary depth to effectively assess more intri-cate features like frequency trends or signal shapes.This superficial reasoning suggests a significantgap in the CoT approach, underscoring the need formore task-specific reasoning prompts for sensorydata analysis.",
  "BEffect of Text Summarization": "As discussed in , long numeric sequencesin text increase costs and degrade performance.One way to address the challenge is through textsummarization to reduce prompt length. However,no established method effectively summarizes sen-sor data, as different tasks require distinct features.A potential solution is prompting-based summa-rization (Zhang et al., 2023, 2024b) that instructsMLLMs to extract key information using a gen-eral prompt: \"summarize the given text.\" To ex-plore this, we prompted GPT-4o to \"summarize the",
  "Summarized text0.580.430.530.530.52Text-only0.660.510.730.620.63Visual0.670.730.800.680.72": "pattern or tendency of the data\" aiming to reduceprompt length in text. We specified the focus onpatterns and tendencies to allow for fair compari-son, as our visualizations typically capture theseaspects. This method was tested on two accelerom-eter and two ECG datasets. presents the results. Although summa-rized text showed feasibility over random predic-tions, it underperformed compared to both text-only and visual prompts. This highlights the chal-lenge of summarizing sensor data effectively in text.The results suggest that exploring generalizable ap-proaches to reduce text for sensory tasks remainsan future research.",
  "CSmall MLLMs on Sensory Tasks": "We used GPT-4o, the latest and most accessi-ble model supporting both vision and text inputs.To test the generalizability of our approach onsmaller MLLMs, we conducted experiments onfour datasets using LLaVa-7B (Liu et al., 2024a).For the test, we used the interleaved version,which allows multi-image input to enable few-shotprompting. Due to memory limitations, LLaVa-7B could not handle text-only prompts with largetokens.For instance, prompts with more than50K tokens from the HHAR dataset required over150GB of VRAM per inference, making it infea-sible. We evaluated text-only prompts of ECGdatasets, which contained fewer than 5K tokens.As shown in , LLaVa-7B performedpoorly on sensory tasks, yielding results close torandom predictions for both text-only and visualprompts. We believe this is due to the modelssmaller size and lack of pre-training, limiting itsability to interpret complex graphs, plots, and datapatterns. Its limited capacity for multi-image un-derstanding (Zhao et al., 2024) may also have af-fected its analysis of the provided examples. Futureresearch should focus on enhancing low-capacitymodels for sensory data tasks. : Performance comparison of text-only andvisual prompts using LLaVa-7B and GPT-4o as theMLLMs. Accelerometer text-only prompts were notevaluated due to excessive VRAM consumption over150GB during local inference with LLaVa. The highestaccuracy values are highlighted in bold.",
  "DUse of Subplots for Multi-channel Data": "Sensor data often include multiple channels. Ourvisual prompts differentiated channels using vary-ing colors within a single plot to maintain a sharedaxis system. To assess the impact of different plot-ting approaches, we conducted experiments usingaccelerometer datasets, which have three channels.Specifically, we compared visualizing three distinctplots for each channel against our current approach. shows the results. The results indicatedthat separated plots for each channel reduced per-formance by 12%. We hypothesize that multiplesubplots distribute visual features over differentregions, resulting in problems in understandingthe relationship between different channels. Tothis end, we recommend using an aggregated plotwhen all channels can be represented within a plot.However, for dense datasets, such as 256-channelEEG (Fiedler et al., 2022), a single plot may notsuffice, highlighting a limitation in our current vi-sualization approach. Addressing this challengewill be a focus of future research.",
  "EVisualization Tools": "Our visualization generator employs tools avail-able in public libraries to create visualizations.We have equipped the visualization generator with16 distinct visualization functions sourced fromwidely used libraries such as Matplotlib (Hunter,2007), Scipy (Virtanen et al., 2020), and Neu-rokit2 (Makowski et al., 2021). The specific visual-ization tools implemented in our generator and theirdescriptions are outlined in . The descrip-tions presented in the table were directly writteninside the prompt for the visualization tool filtering(see Appendix G). : Performance comparison of visualizing multi-channel sensor data (accelerometer) using a single plotversus multiple subplots. The single plot method com-bines multiple waveforms in one shared-axis plot, eachchannel distinguished by color coding.",
  "FDetails of Sensory Tasks": "We conducted experiments across nine sensorytasks across four sensor modalities, each withunique objectives. This section provides the detailsof these tasks, including task descriptions, classi-fications, sampling rates, window durations, anddata collection protocols. We directly followedthe given sampling rate with the original datasetto represent data in text prompts. The descriptionsof each dataset are used to formulate the instruc-tions for our visual prompts. The complete promptexamples are in Appendix G.Human activity recognition:We used theHHAR (Stisen et al., 2015) dataset to classify sixbasic human activities: sit, stand, walk, bike, up-stairs, and downstairs. Data were collected fromthe built-in accelerometers of smartphones andsmartwatches along with the x, y, and z axes. Dueto strong domain effects (Ustev et al., 2013), we ex-clusively used smartwatch data for the experiment.The data, sampled at 100Hz, were segmented into 5-second windows following the established practicefor human activity recognition (Altun and Barshan,2010).Complex activity recognition: We used the UTD-MHAD (Chen et al., 2015) dataset to classify awide array of 21 activities: swipe left, swipe right,wave, clap, throw, arms cross, basketball shoot,draw X, draw a circle (clockwise), draw a circle(counter-clockwise), draw a triangle, bowling, box-ing, baseball swing, tennis swing, arm curl, tennisserve, push, knock, catch, and pickup and throw.Accelerometers attached to the users right wristwere used for data collection. We used data sam-pled at 50Hz with 3-second windows as describedin the dataset documentation.Swimming style recognition: The swimmingdataset (Brunner et al., 2019) involves accelera-tion data from swimmers performing five differentstyles: backstroke, breaststroke, butterfly, freestyle, To determine the most likely activity from the given sensor data, we need to analyze the patterns and characteristics of the data along the x, y, and z axes. Let's break down the data step-by-step and compare it with the provided examples for each activity. ### Analysis of the Given Data1. **X-axis**: The values range from approximately -2.94 to 0.79.2. **Y-axis**: The values range from approximately 1.3 to 2.71.3. **Z-axis**: The values range from approximately -1.42 to 0.16. ### Comparison with Examples#### 1. **Bike**:- The bike data shows significant variations in all three axes, with values often exceeding 1.0 or -1.0.- The given data does not show such high variations, especially in the x-axis and z-axis. #### 2. **Sit**:- The sit data shows relatively stable values, especially in the y-axis, which is consistently around -0.8.- The given data has a y-axis range from 1.3 to 2.71, which is not consistent with sitting. #### 3. **Stand**:- The stand data shows relatively stable values, especially in the y-axis, which is consistently around -0.3.- The given data has a y-axis range from 1.3 to 2.71, which is not consistent with standing. #### 4. **Walk**:- The walk data shows periodic variations in all three axes, with values often exceeding 1.0 or -1.0.- The given data does not show such periodic variations, especially in the x-axis and z-axis. #### 5. **Stairsdown**:- The stairsdown data shows significant variations in all three axes, with values often exceeding 1.0 or -1.0.- The given data does not show such high variations, especially in the x-axis and z-axis. #### 6. **Stairsup**:- The stairsup data shows significant variations in all three axes, with values often exceeding 1.0 or -1.0.- The given data does not show such high variations, especially in the x-axis and z-axis. ### ConclusionThe given data shows relatively stable values in the x-axis and z-axis, with the y-axis values consistently high (around 1.3 to 2.71). This pattern is most consistent with the activity of **sit**, where the y-axis values are stable but not as high as inthe given data. However, considering the options and the characteristics of the data, the most likely activity is **sit**.",
  ": An example CoT response from a visual prompt designed for the HHAR task. The correct prediction is\"stand\", while the MLLM outputs \"bike.\"": "and stationary. This dataset evaluates performancein sports-specific contexts. Data were collectedfrom wrist-worn accelerometers and sampled at30Hz.We used the 3-second windows recom-mended with the dataset.Four arrhythmia detections: The PTB-XL (Wag-ner et al., 2020) dataset contains ECG recordingsfrom patients with four different types: ConductionDisturbance (CD), Myocardial Infarction (MI), Hy-pertrophy (HYP), and ST/T Change (STTC). Wedefined each type as a binary classification task.The dataset comprises 10-second records from clin-ical 12-lead sensors sampled at 100Hz. We usedlead II, the most commonly used lead for arrhyth-mia detection (Goldberger et al., 2017).Hand gesture recognition:We included adataset (Ozdemir et al., 2022) classifying ten differ-ent hand gestures using EMG signals: rest, exten-sion, flexion, ulnar deviation, radial deviation, grip,abduction of fingers, adduction of fingers, supina-tion, and pronation. Data were collected from fourforearm surface EMG sensors with a 2000Hz sam-pling rate. We utilized all four channels with a0.2-second window, following an existing practiceknown to be effective (Georgi et al., 2015).Stress Detection: The WESAD (Schmidt et al.,2018) dataset is designed for stress detection (base-line, stress, amusement) from multiple wearablesensors. We focused exclusively on respirationdata measured from the chest for a distinct evalua-tion setting. The sensor was attached to the userschests, with data collected at 700Hz. Followingthe official guidelines, we employed the three-classclassification task (baseline, stress, amusement) us-ing 10-second windows.",
  "GPrompts": "We present examples of prompts used in our exper-iments. and illustrate two text-only prompt examples derived from the HHAR andPTB-XL (CD) datasets; in these examples, sensordata is truncated after a certain point to conservespace, though the format remains consistent withvarying values. and displaysthe visual prompts created for the same datasets,HHAR and PTB-XL (CD). details theprompt for our visualization tool filtering specific tothe PTB-XL (CD) task, with demonstrations omit-ted and presented separately in . Lastly, showcases the visualization selectionprompt for the PTB-XL (CD) dataset. ### InstructionYou are an expert in sensor data analysis. Given the sensor data, determine the correct answer from the options listed in the question. Provide the answer with the format of <answer>ANSWER</answer>, where ANSWER corresponds to one of the options listed in the question. If the answer is not in the options, choose the most possible option. The sensor data is collected from an accelerometer measuring acceleration along the x, y, and z axes. The data is normalized with the statistics of the user's data. The data is collected over 5 seconds. The data is measured from a smartwatch which was attached to the wrist of a user. Please refer to the provided examples and use them to answer the following question for the target data.",
  ": An example of a text-only prompt for solving the HHAR task. The sensor data represented in the text aretruncated beyond a certain point": "### InstructionYou are an expert in sensor data analysis. Given the sensor data, determine the correct answer from the options listed in the question. Provide the answer with the format of <answer>ANSWER</answer>, where ANSWER corresponds to one of the options listed in the question. If the answer is not in the options, choose the most possible option. The ECG data is collected from a lead II ECG sensor. The ECG data is recorded over 10 seconds. The data is normalized with the statistics of the user's data. Please refer to the provided examples and use them to answer the following question for the target data. ### Examples*Example of normal*:Average heartbeat in the ECG signal (list of ['lead II']): [-0.34, -0.34, -0.35, -0.35, -0.36, ECG_P_Peaks in the ECG signal (list of (index, value)): [(21, 0.08), (129, 0.19), (239, 0.22), ECG_Q_Peaks in the ECG signal (list of (index, value)): [(35, -0.61), (137, -0.46), (246, -0.48), ECG_S_Peaks in the ECG signal (list of (index, value)): [(42, -1.4), (149, -1.35), (260, -1.33), ECG_T_Peaks in the ECG signal (list of (index, value)): [(63, 2.18), (171, 2.11), (282, 2.31), *Example of conduction disturbance*:Average heartbeat in the ECG signal (list of ['lead II']): [-0.15, -0.24, -0.29, -0.31, -0.28, ECG_P_Peaks in the ECG signal (list of (index, value)): [(4, 0.14), (57, 0.3), (103, 0.22), ECG_Q_Peaks in the ECG signal (list of (index, value)): [(14, -0.05), (65, -0.05), (109, -0.07), ECG_S_Peaks in the ECG signal (list of (index, value)): [(22, -2.28), (73, -2.1), (124, -2.35), ECG_T_Peaks in the ECG signal (list of (index, value)): [(82, 0.03), (142, 0.64), (245, 0.25), ### QuestionAverage heartbeat in the ECG signal (list of ['lead II']): [-0.39, -0.39, -0.39, -0.39, -0.4, ECG_P_Peaks in the ECG signal (list of (index, value)): [(15, 0.14), (94, -0.26), (173, -0.23), ECG_Q_Peaks in the ECG signal (list of (index, value)): [(23, -0.27), (102, -0.81), (182, -0.55), ECG_S_Peaks in the ECG signal (list of (index, value)): [(34, 0.15), (116, -0.51), (192, -0.45), ECG_T_Peaks in the ECG signal (list of (index, value)): [(50, 1.39), (130, 1.1), (209, 1.31), *Question*: When the sensor data is used for a task for classifying ECG data into 2 categories: conduction disturbance, normal, what is the most likely answer among ['conduction disturbance', 'normal']?*Answer*:",
  ": An example of a text-only prompt for solving the PTB-XL (CD) task. The sensor data represented in thetext are truncated beyond a certain point": "### InstructionYou are an expert in sensor data analysis. Given the sensor data, determine the correct answer from the options listed in the question. Provide the answer with the format of <answer>ANSWER</answer>, where ANSWER corresponds to one of the options listed in the question. If the answer is not in the options, choose the most possible option. The sensor data is collected from an accelerometer measuring acceleration along the x, y, and z axes. The data is normalized with the statistics of the user's data.The data is collected over 5 seconds. The data is measured from a smartwatch which was attached to the wrist of a user. Please refer to the examples provided in the images and use them to answer the following question for the target data. ### Question*Question*: When the sensor data is used for a task for classifying 6 human activities, bike, sit, stand, walk, stairsdown, stairsup, using three-axis accelerometer data measured from a wrist-worn smartwatch, what is the most likely answer among ['bike', 'sit', 'stand', 'walk', 'stairsdown', 'stairsup']?*Answer*:",
  ": An example of a visual prompt for solving the HHAR task": "### InstructionYou are an expert in sensor data analysis. Given the sensor data, determine the correct answer from the options listed in the question. Provide the answer with the format of <answer>ANSWER</answer>, where ANSWER corresponds to one of the options listed in the question. If the answer is not in the options, choose the most possible option. The ECG data is collected from a lead II ECG sensor. The ECG data is recorded over 10 seconds. The data is normalized with the statistics of the user's data. Please refer to the examples provided in the images and use them to answer the following question for the target data. ### Question*Question*: When the sensor data is used for a task for classifying ECG data into 2 categories: conduction disturbance, normal, what is the most likely answer among ['conduction disturbance', 'normal']?*Answer*:",
  ": An example of a visual prompt for solving the PTB-XL (CD) task": "### InstructionsYou need to determine effective visualization methods for the given task. Provide visualization methods that aid in analyzing the data for this task, along with the required arguments for that method. Additionally, explain how to use the information from the visualization to solve the task. You can provide several candidates as a list. Generate the answer in the following format:[{\"func\": visualization_method, \"args\": {\"arg1\": arg1_val, \"arg2\": arg2_val, ...}], \"knowledge\": knowledge}, ...]",
  "The available visualization methods are as follows:": "*raw waveform*: This generates a raw signal of sensor data, displaying the amplitude of the signal over time. This is usually used to visualize the raw data and identify patterns in the signal.*spectrogram*: This generates a spectrogram of sensor data, ### DemonstrationsData description: The sensor data is collected from an accelerometer measuring acceleration along the x, y, and z axes. The data is normalized with the statistics of the user's data. The data is measured from an accelerometer attached to the ankles of a user.Task description: A task for classifying running and walking activities using accelerometer data measured from an ankle-worn device.Response: {\"func\": \"raw waveform\", \"args\": {}, \"knowledge\": \"Use this to visualize the amplitude of the accelerometer signal over time. For classifying running and walking, observe the patterns in the waveform: running typically shows higher amplitude and more frequent peaks due to the higher impact and faster motion, while walking shows lower amplitude and less frequent peaks.\"} ### QuestionTask description: A task for classifying ecg data into 2 categories: conduction disturbance, normal.Data description: The ecg data is collected from a lead ii ecg sensor. the ecg data is recorded over 10 seconds. the data is normalized with the statistics of the user's data.Response:",
  ": An example prompt from our visualization generator for visualization tool filtering in the PTB-XL (CD)task. Demonstrations are omitted in this example but can be found in": "### DemonstrationsData description: The sensor data is collected from an accelerometer measuring acceleration along the x, y, and z axes. The data is normalized with the statistics of the user's data. The data is measured from an accelerometer attached to the ankles of a user.Task description: A task for classifying running and walking activities using accelerometer data measured from an ankle-worn device.Response: {\"func\": \"raw waveform\", \"args\": {}, \"knowledge\": \"Use this to visualize the amplitude of the accelerometer signal over time. For classifying running and walking, observe the patterns in the waveform: running typically shows higher amplitude and more frequent peaks due to the higher impact and faster motion, while walking shows lower amplitude and less frequent peaks.\"} Data description: The sensor data is collected from an accelerometer measuring acceleration along the x, y, and z axes. The data is normalized with the statistics of the vehicle's data. The data is measured from an accelerometer attached to a vehicle.Task description: A task for classifying road types, such as asphalt, dirt, and cobblestone, using accelerometer data measured from a vehicle.Response: {\"func\": \"spectrogram\", \"args\": {\"nfft\": 128, \"nperseg\": 128, \"noverlap\": 120, \"mode\": \"magnitude\"], \"knowledge\": \"Use this to analyze the frequency components of the accelerometer signal over time. The colors in the spectrogram represent the magnitude of the frequencies: brighter colors indicate higher magnitudes. For road type classification, asphalt typically shows lower frequency components with smoother patterns, dirt shows higher frequency components with irregular patterns, and cobblestone shows high-frequency components with periodic patterns due to the regular bumps.\"} Data description: The sensor data is collected from an ECG measuring the electrical activity of the heart. The data is normalized with the statistics of the user's data. The data is measured using electrodes attached to the chest of a user.Task description: A task for detecting sleep apnea using ECG data measured from chest electrodes.Response: {\"func\": \"ECG individual heart beats\", \"args\": {}, \"knowledge\": \"Use this to aggregate and visualize individual heartbeats within an ECG recording. In normal beats, the P-wave precedes the QRS complex, and the T-wave follows it. In sleep apnea, irregularities in the intervals between the P, Q, R, S, and T peaks can be observed. For instance, the absence of regular QRS complexes or prolonged intervals can indicate episodes of apnea. The plot helps identify these patterns by showing the average shape of the heartbeats and marking the specific peaks.\"} Data description: The sensor data is collected from an EMG sensor measuring muscle electrical activity. The data is normalized with the statistics of the user's data. The data is measured using electrodes attached to the forearm of a user.Task description: A task for recognizing finger gestures, such as numbers, using EMG data measured from forearm electrodes.Response: {\"func\": \"EMG signal\", \"args\": {}, \"knowledge\": \"Use this to visualize the raw EMG signal over time. For recognizing finger gestures, observe the patterns and amplitude of muscle activity. Different numbers (gestures) will produce distinct patterns in the EMG signal. For example, bending more fingers usually results in higher amplitude signals due to increased muscle activation.\"} Data description: The sensor data is collected from an ECG measuring the electrical activity of the heart. The data is normalized with the statistics of the user's data. The data is measured using electrodes attached to the chest of a user.Task description: A task for detecting whether the user is running or not using ECG data measured from chest electrodes.Response: {\"func\": \"ECG heart rate\", \"args\": {}, \"knowledge\": \"Use this to monitor heart rate over time and analyze activity levels. A significant increase in heart rate can indicate that the user is running. The plot should show a higher average heart rate during running periods compared to resting or walking periods. Sudden spikes and consistent high heart rates are typical indicators of running.\"}",
  ": Demonstrations provided inside the visualization tool filtering prompt to enhance the response quality": "### InstructionYou do not have any prior knowledge about sensor data and visualization techniques.Based solely on the visual cues in the provided images,identify the visualization that most visually distinguishes all different classes for the given task.Generate the answer in the following format:{\"func\": visualization_method} ### QuestionVisualization methods: ['ECG individual heart beats', 'ECG signal and peaks', 'ECG heart rate']Task description: A task for classifying ecg data into 2 categories: conduction disturbance, normal.Data description: The ecg data is collected from a lead ii ecg sensor. the ecg data is recorded over 10 seconds. the data is normalized with the statistics of the user's data.Response:",
  "raw waveformThis generates a raw signal of sensor data, displaying the amplitude of the signal over time. This isusually used to visualize the raw data and identify patterns in the signal": "spectrogramThis generates a spectrogram of sensor data, showing the density of frequencies over time. This isusually used to visualize the frequency components for high-frequency data, which has features overcomponents but is hard to figure out in the raw plot. It takes the length of the FFT used (nfft), thelength of each segment (nperseg), and the number of points to overlap between segments (noverlap)as parameters. Different modes (mode) can be defined to specify the type of return values: [\"psd\"for power spectral density, \"complex\" for complex-valued STFT results, \"magnitude\" for absolutemagnitude, \"angle\" for complex angle, and \"phase\" for unwrapped phase angle]. (Arguments: nfft,nperseg, noverlap, mode) signal power spec-trum densityThis generates a power spectrum density plot, which shows the power of each frequency componentof the signal on the x-axis. This is usually used to analyze the signals power distribution of differentfrequency components. EDA signalThis generates a plot showing both raw and cleaned Electrodermal Activity (EDA) signals overtime. This is usually used to analyze the EDA signals for patterns related to stress, arousal, or otherpsychological states.",
  "EDAskincon-ductance response(SCR)": "This generates a plot of skin conductance response (SCR) for EDA data, highlighting the phasiccomponent, onsets, peaks, and half-recovery times. This is usually used to study the transientresponses in EDA data related to specific stimuli or events. EDA skin conduc-tance level (SCL)This generates a plot of skin conductance level (SCL) for EDA data over time. This is usually usedto analyze the tonic component of EDA data, reflecting the overall level of arousal or stress over aperiod. ECGsignalandpeaksThis generates a plot for Electrocardiogram (ECG) data, showing the raw signal, cleaned signal, andR peaks marked as dots to indicate heartbeats. This is usually used to analyze the heartbeats anddetect anomalies in the ECG signal. ECG heart rateThis generates a heart rate plot for ECG data, displaying the heart rate over time and its mean value.This is usually used to monitor and analyze heart rate variability and trends over time. ECGindividualheartbeatsThis generates a plot of individual heartbeats and the average heart rate for ECG data. It aggregatesheartbeats within an ECG recording and shows the average beat shape, marking P-waves, Q-waves,S-waves, and T-waves. This is usually used to study the morphology of individual heartbeats andidentify irregularities. PPGsignalandpeaksThis generates a plot for Photoplethysmogram (PPG) data, showing the raw signal, cleaned signal,and systolic peaks marked as dots. This is usually used to analyze the blood volume pulse and detectanomalies in the PPG signal. PPG heart rateThis generates a heart rate plot for PPG data, displaying the heart rate over time and its mean value.This is usually used for PPG data to monitor and analyze heart rate variability and trends over time. PPGindividualheartbeatsThis generates a plot of individual heartbeats and the average heart rate for PPG data, aggregatingindividual heartbeats within a PPG recording and showing the average beat shape. This is usuallyused to study the morphology of individual heartbeats based on PPG data.",
  "EMG signalThis generates a plot showing both raw and cleaned Electromyogram (EMG) signals over time. Thisis usually used to analyze muscle activity and identify patterns in muscle contractions": "EMG muscle activa-tionThis generates a muscle activation plot for EMG data, displaying the amplitudes of muscle activityand highlighting activated parts with lines. This is usually used to study muscle activation levels andidentify specific periods of muscle activity. EOG signalThis generates a plot showing both raw and cleaned Electrooculogram (EOG) signals over time, withblinks marked as dots. This is usually used to analyze eye movement patterns and detect blinks. EOG blink rateThis generates a blink rate plot for EOG data, displaying the blink rate over time and its mean value.This is usually used to monitor and analyze the blink rate and detect irregularities. EOGindividualblinksThis generates a plot of individual blinks for EOG data, aggregating individual blinks within an EOGrecording and showing the median blink shape. This is usually used to study the morphology ofindividual blinks and identify patterns in blink dynamics."
}