{
  "Abstract": "Recent advances in foundation models have em-phasized the need to align pre-trained modelswith specialized domains using small, curateddatasets. Studies on these foundation modelsunderscore the importance of low-data train-ing and fine-tuning. This topic, well-known innatural language processing (NLP), has alsogained increasing attention in the emergingfield of scientific machine learning (SciML).To address the limitations of low-data train-ing and fine-tuning, we draw inspiration fromHeavy-Tailed Self-Regularization (HT-SR) the-ory, analyzing the shape of empirical spectraldensities (ESDs) and revealing an imbalancein training quality across different model lay-ers. To mitigate this issue, we adapt a recentlyproposed layer-wise learning rate scheduler,TempBalance, which effectively balancestraining quality across layers and enhanceslow-data training and fine-tuning for both NLPand SciML tasks. Notably, TempBalancedemonstrates increasing performance gains asthe amount of available tuning data decreases.Comparative analyses further highlight the ef-fectiveness of TempBalance and its adapt-ability as an add-on method for improvingmodel performance.",
  "Introduction": "Recent surges in foundation models (FMs) havestimulated research on aligning pre-trained mod-els with specialized domains using small-sizeddatasets. This pre-train and fine-tune paradigmis prevalent in natural language processing (NLP)tasks (Wang et al., 2019, 2020; Rajpurkar et al.,2016; Lu et al., 2022). It is also gaining popu-larity in other machine learning (ML) fields, suchas scientific machine learning (SciML) (Subrama-nian et al., 2024; Lanusse et al., 2023; McCabeet al., 2023; Wu et al., 2023; Hao et al., 2024; Chenet al., 2024). From a practical perspective, the",
  "*Equal contribution. Work completed during an internshipat Dartmouth College": "challenge of fine-tuning often lies in curating high-quality datasets (possibly with labeled examples) toachieve alignment with the new domain. In SciML,people often use FMs for training on different typesof partial differential equations (PDEs) (McCabeet al., 2023; Wu et al., 2023; Hao et al., 2024)and fine-tuning it on a certain domain when acces-sible scientific data from that domain is limited.As a concrete example, turbulence simulations atextremely high Reynolds numbers are computa-tionally intensive and time-consuming, often lead-ing to only a few available trajectories. Therefore,training SciML FMs on trajectories with differentReynolds numbers and fine-tuning it on trajecto-ries simulated at extremely high ones is beneficialfor solving the problem of poor training perfor-mance caused by insufficient data volume. UsingSciML FMs, researchers can train these modelsto generalize across a wider range of downstreamtasks, thereby enhancing their applicability andefficiency in diverse scientific scenarios. Prior re-search has shown that strong performance can in-deed be achieved by fine-tuning with a few care-fully selected examples (Zhou et al., 2023), buttraining with low data can still lead to unstable per-formance (Zhang et al., 2021). Therefore, findingfine-tuning algorithms that improve performancein low-data settings, especially few-shot alignment,becomes crucial. In this work, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory (Mar-tin and Mahoney, 2021; Martin et al., 2021), toimprove model performance in low-data regimes.HT-SR theory proposes that well-trained neural net-work (NN) models exhibit strong correlations inweights, resulting in a Heavy-Tail (HT) structurein the Empirical Spectral Density (ESD, usuallyrepresented by a histogram of eigenvalue distribu-tion) of each layers weight matrix. To quantifythe HT structure, we can fit a power law (PL) dis-tribution to the HT part of the ESD and extract",
  "TempBalance": ": Heavy-tail ESD analysis and TempBalance learning rate schedule. To characterize the heavy-tailedstructure of ESD, we fit a power-law exponent PL_Alpha_Hill on the tail part of the ESDs (blue histograms at topleft), shown as the red dashed line on the histogram. Given the imbalanced layer-wise PL_Alpha_Hill (bottomleft), TempBalance assigns lower learning rate to layers with lower PL_Alpha_Hill (more heavy-tailed), andassign higher learning rate to layers with higher PL_Alpha_Hill (less heavy-tailed). TempBalance aims tobalance the PL_Alpha_Hill distribution across layers in low-data regimes (bottom right). its exponent, namely PL_Alpha_Hill (see Fig-ure 1). HT-SR theory suggests that a more HTESD (lower PL_Alpha_Hill) represents bettertraining quality, and vice versa. This estimationof model and layer quality has been shown to beeffective in recent work on model selection (Mar-tin et al., 2021; Martin and Mahoney, 2020, 2022;Yang et al., 2023), layer-wise hyperparameter tun-ing (Zhou et al., 2024), and pruning of large lan-guage models (LLMs) (Lu et al., 2024). Using HT-SR theory, we analyze the limitationsof model training in low-data regimes by measuringthe layer-wise distribution of PL_Alpha_Hill(discussed in 4.2). Our main finding is that whenwe train with sufficient data, PL_Alpha_Hillbecomes more evenly distributed across layers, re-sulting in better layer-wise balance; in this case,high performance can be achieved without layer-specific manipulations.However, when we re-duce the number of training data samples, testperformance decreases, and the standard devia-tion (STD) of PL_Alpha_Hill across layerstends to increase (see ), indicating thatPL_Alpha_Hill is more unevenly distributedwhen training with fewer data, resulting in worselayer-wise balance. This finding indicates that dif-ferent layers training quality becomes more poorlyaligned as we reduce training data.Therefore,layer-wise balancing is beneficial to balance under- trained layers and over-trained layers in low dataregimes.Motivated by this observation, we incorporatethe variance of PL_Alpha_Hill across layerswith the recently proposed layer-wise learningrate scheduling algorithm TempBalance (Zhouet al., 2024), to design a novel method to balancethe training quality across layers. To evaluate itsempirical performance, we use TempBalancein curated low-data regime in LLM fine-tuningand SciML tasks. We compare TempBalancewith commonly used baseline methods and demon-strate that TempBalance not only achieves su-perior performance in low-data training and fine-tuning, but also can be used as a plug-in methodon top of existing optimization methods to achieveeven better test performance and stability, such asSAM (Foret et al., 2021) and AdaFactor (Shazeerand Stern, 2018).Furthermore, in our analy-sis, we reveal that TempBalance successfullybalances training quality across all layers dur-ing training from the HT-SR point of view. Weshow that TempBalance balances the trainingquality of each layer by reducing the STD ofPL_Alpha_Hill of all layers. We summarizeour contributions as follows 1:",
  "Inorderthatourresultscanbereproducedandextended,wehaveopen-sourcedourcode": "We find that low-data fine-tuning is a cru-cial training paradigm that can lead to imbal-anced training quality across different layersof the model, measured by the large STD ofPL_Alpha_Hill values across layers. We focus on low-data training scenariosand demonstrate the effectiveness of usingTempBalance to balance layers and im-prove the performance of both NLP andSciML models. For example, we show thatTempBalance can improve RoBERTa-basetrained on SST2 dataset by at most 9.9% andincrease the test accuracy of LLaMA-7B onScienceQA dataset by at most 1.97%, and re-duce the normalized root-mean-squared-error(nRMSE) of FNO trained on 2D Compress-ible Navier-Stokes(CFD)2 dataset by 14.47%.Furthermore, we show that TempBalanceachieves gradually increased performancegains as the number of data points decreases. In LM fine-tuning tasks, we demonstratethat TempBalance can achieve better fine-tuning performance compared to baselines (in-cluding SAM (Foret et al., 2021) and AdaFac-tor (Shazeer and Stern, 2018)) and can be usedas an add-on method to combine with these ex-isting optimization methods to achieve furtherimprovements.",
  "Related Work": "Heavy-tailed Phenomenon.Recently, severalstudies have observed that a well-trained deep NNexhibits HT spectra in its weight matrices. Manypapers focus on investigating the cause of the emer-gence of HT, and they have attributed HT spectra(or limiting HT distributions of weights) to strongcorrelation in weight elements (Martin and Ma-honey, 2021; Martin et al., 2021), feature learn-ing (Wang et al., 2024b; Kothapalli et al., 2024),the KestenGoldie mechanism (Hodgkinson andMahoney, 2021; Gurbuzbalaban et al., 2021), -stable Lvy process (Gurbuzbalaban et al., 2021;Simsekli et al., 2020), and the maximum-entropyprinciple (Xie et al., 2024). More importantly, sev-eral studies have shown that the heavytailness of theweight spectra is strongly correlated with the qual-ity of neural networks. For example, Martin andMahoney (2021) proposed HT-SR theory, demon-strating that the degree of HT in the ESD of each",
  "CFD means compressible fluid dynamics or, equivalently,the compressible Navier-Stokes equations": "layer can be used to predict model quality: the heav-ier the tail of the ESD, the better the quality of themodel. In addition, Simsekli et al. (2020); Hodgkin-son et al. (2022); Wang et al. (2024a) proved gener-alization bounds dependent on the HT distributionsin either model weights or the ESDs of the weightmatrices, which are validated through extensive ex-periments. Motivated by these studies, some effortshave begun to leverage the degree of HT for modeltraining (Zhou et al., 2024; Li et al., 2024; Qinget al., 2024), model selection (Agrawal et al., 2022;Yang et al., 2023), and model compression (Bars-bey et al., 2021; Lu et al., 2024), as well as toenhance model robustness (Nassar et al., 2020). Resource-constrained Fine-tuning.The pre-training and fine-tuning paradigm has been a pri-mary method for adapting foundation models todownstream tasks for resource-limited users. Whenadapting very large models, people often resortto the Low-Rank Adaptation method (LoRA) (Huet al., 2021), which is also considered in this paper.Our primary focus is on low-data fine-tuning, anincreasingly studied paradigm where the emphasisis often on careful data selection (Zhou et al., 2023).Furthermore, when training models in a few-shotfashion, such as in-context learning (Brown et al.,2020; Logan IV et al., 2021; Zhang et al., 2022),data selection plays a crucial role in improvingmodel performance. Our paper, however, exploreslayer-balancing schemes to improve model perfor-mance. Data-constrainted Training and Fine-tuning inSciML.There has been an increasing interest inthe use of ML methods to solve scientific prob-lems (Raissi et al., 2019; Li et al., 2020; Karni-adakis et al., 2021; Wang et al., 2023). One rep-resentative line of work is on neural operators (Liet al., 2020; Lu et al., 2021; Hao et al., 2023; Raonicet al., 2024). These operators have demonstratedtheir effectiveness in scientific modeling. However,they require extensive scientific datasets. Generat-ing high-fidelity numerical datasets is computation-ally demanding. Hence, to mitigate the costs asso-ciated with simulation, self-supervised pretraininghas been introduced for operator learning (Chenet al., 2024). Additionally, in low-data regimes,researchers also propose to incorporate physicallaws into ML models to facilitate the learning ofthe underlying governing equations, often throughsoft regularization constraints (Raissi et al., 2019).Nevertheless, the physics-constrained ML strategyis limited to specific PDE scenarios (e.g., fixed",
  "HT-SR Theory": "HT-SR theory (Martin and Mahoney, 2021) demon-strates the empirical fact that very well-trainedmodels tend to exhibit strong correlations inweights, resulting in HT structure in the ESD ofeach layer. Its underlying motivation stems fromrandom matrix theory and statistical physics, aswell as the observation that HT ESDs are ubiqui-tous in well-trained NN models. Obtaining the ESD of Weight Matrices.Toobtain the ESDs of a model, we take an NN withL layers and the corresponding weight matricesW1, W2, , WL.For the i-th layer, we cal-culate the eigenvalues of its correlation matrixXi = Wi Wi. Then, we plot the ESD for thatlayer, which is the empirical distribution of theseeigenvalues. During training, the ESD will typ-ically gradually change to have an HT structure.There are many metrics that have been proposed tostudy the properties of ESDs, among which shapemetrics (metrics that depict the shape of ESD) havebeen shown to predict the training quality of eachlayer (Yang et al., 2023). Analyzing ESDs with PL Fitting.To obtainrobust shape metrics that predict layer quality, wefit a PL distribution to the heavy-tailed part of theESD within an interval (min, max). The PL fit hasthe following formula:",
  "where k is an adjustable parameter": "PL_Alpha_HillDistributionandModelQuality.When using PL_Alpha_Hill to an-alyze model performance, related works suggestthat a layer with smaller PL_Alpha_Hill tendsto be relatively overtrained (compared to otherlayers in the model), while layers with higherPL_Alpha_Hill are relatively undertrained.(Zhou et al., 2024) find that in CV tasks, modelstrained with optimized hyperparameter schedulingoutperform baseline methods and yield a more con-centrated PL_Alpha_Hill distribution acrosslayers, suggesting that a more uniformly distributedPL_Alpha_Hill has more balanced trainingquality across layers, leading to better overall qual-ity of the model.",
  "TempBalance Algorithm": "Prior research (Martin and Mahoney, 2021) hasshown that temperature-like parameters signifi-cantly influence the HT structure of individuallayers ESDs. Therefore, to balance the shapeof ESDs across layers, we propose to adapt theTempBalance algorithm, which dynamicallytunes the learning rate on a layer-wise basis, asthe learning rate is the most important temperatureparameter. Smaller learning rates are assigned tolayers with more heavy-tailed ESDs to slow downthe training, while larger learning rates are assignedto those with more light-tailed ESDs to acceleratethe training. We propose a novel method to map thePL_Alpha_Hill of each layer to the layer-wiselearning rate. We first calculate their differencewith the mean PL_Alpha_Hill value across alllayers, then rescale the difference using a sigmoid-like function. Finally, we use the rescaled value asthe exponent to assign the new learning rate ft(i)for the layer. We refer to this scheduling algorithmas TB_Sigmoid. The equations are as follows:",
  "+ e(i) 0.5,(4)": "where t is the base learning rate at step t, iis the PL_Alpha_Hill of layer i, and isthe mean PL_Alpha_Hill across all layers.Note that s and are tunable hyperparametersin experiments, and we often obtain the bestresults when we set = 10. In TempBalance,if a layers PL_Alpha_Hill is higher thanthe mean, a learning rate higher than the baselearning rate is assigned, and if it is lower, a lower Test MetricSTD of PL_Alpha_Hill 0.0005 0.001 0.005 0.01 0.05 0.1 0.25 0.5 1.0",
  ": Test performance and STD of PL_Alpha_Hill across all layers of RoBERTa-base model trained onMNLI (Accuracy) and QNLI (Accuracy) under different subsampling ratios": "learning rate is assigned. Furthermore, layers withPL_Alpha_Hill significantly different fromthe mean receive more substantial adjustments,while those closer to the mean receive minimalchanges. The intuition of this scheduling functionis that it not only controls PL_Alpha_Hill byadjusting the learning rate based on its value, butalso takes the difference of PL_Alpha_Hillto the mean into account to reduce the varianceof PL_Alpha_Hill across layers by assigninglearning rate changes proportional to the differ-ence, finally balancing the training quality.In, we empirically show that TB_Sigmoidworks better than other layer-wise learning ratescheduling methods. Using TempBalance on Transformers.ForTransformer-based architectures, we note eachTransformer block consists of different types oflayers (such as Query, Output, and Down Projec-tion) with different matrix sizes, resulting in dis-tinct ESD shapes. Therefore, we explore a more fa-vorable scheduling method to eliminate unfair com-parison of PL_Alpha_Hill of different ESDshapes. We reschedule each blocks learning rateby averaging the PL_Alpha_Hill across all lay-ers within the block, while in each block we use thesame learning rate across all layers. In inAppendix B, we show that the per-block schedul-ing method consistently outperforms the per-layermethod in different low-data regimes. Given sucha design, we note that a layer used in this workwhen discussing Transformer-based models refersto a Transformer block.",
  "Empirical Results": "In this section, we employ HT metrics to diagnosemodel performance in data-limited regimes anddemonstrate the effectiveness of TempBalancein addressing data limitation in two fields: NLP andSciML. In .1, we describe our experimen-tal setup. In .2, we study the correlationbetween ESD behaviors and model performancewith limited training data. Then, in .3,we evaluate TempBalance in our experimentalsetup. In .4, we compare our methodswith other optimization baselines. We analyze theexperimental results in .6. Finally, weperform ablation studies in .7.",
  "Experimental Setup": "Models and Evaluation.For NLP, we evalu-ate TempBalance with two widely-used fine-tuning methods: Full fine-tuning (FT) and LoRAfine-tuning (Hu et al., 2021) using the Hugging-face framework (Wolf et al., 2020).We se-lect two models with distinct sizes: RoBERTa-base (Liu et al., 2019) and LLaMA2-7b (Tou-vron et al., 2023). We train the models on sub-sampled common fine-tuning datasets, includingGLUE (Wang et al., 2019), SuperGLUE (Wanget al., 2020), SQuAD (Rajpurkar et al., 2016), andScienceQA (Lu et al., 2022). We train with sam-pling ratios ranging from 0.02% to 50% to evaluateour method. We also evaluate TempBalance onlow-resource datasets from three specialized do-mains: BioMed, CS, and News. We choose fivedatasets from these domains: RCT with 500 sam-ples (Dernoncourt and Lee, 2017), SciCite (Co- FTTB FTTB 0.0002 0.0005 0.001 0.005 0.01 0.05 FTTB 0.0002 0.0005 0.001 0.005 0.01 0.05 FTTB",
  "b Trend of improvement": ": (Main Results on PDE Learning). TempBalance (TB) achieves lower nRMSE() than baselinemethod on CFD tasks, especially if subsampling ratio is small. 4a compares test performances of baseline trainedand TempBalance trained FNO and UNet models on 1D and 2D CFD datasets (color-coded as in 4b). 4bdemonstrates the trend of performance improvement brought by TempBalance.",
  "Diagnosing Layer Imbalance Using HTMetrics when Training with Limited Data": "To analyze the performance of models trainedin low-data settings, we employ HT-SR theoryand examine the distribution of PL_Alpha_Hillacross different layers.Our findings revealastrongcorrelationbetweenthetrendofPL_Alpha_Hill distribution and test perfor-mance.We use checkpoints of the RoBERTa-base model trained with subsampling ratios rang-ing from 0.05% to 100% on MNLI and QNLIdataset, and we plot the trend of test performanceand block-wise STD of PL_Alpha_Hill, asshown in .As test performance de-creases with training data samples, we observethat the STD of PL_Alpha_Hill across layersincreases, suggesting a more unevenly distributedPL_Alpha_Hill across different layers. Similartrends are also present in SciML tasks (). Given that PL_Alpha_Hill is a robust pre-dictor of model and layer quality (Yang et al.,2023; Zhou et al., 2024), we propose that mod-els trained on fewer data samples have more un-evenly distributed layer qualities, this layer-wisebalance becomes worse as we reduce the numberof training data points. Training with more datapoints, on the other hand, can make the distributionof PL_Alpha_Hill more balanced. Therefore,when training with limited data, layer balancingis necessary for balancing the training quality ofdifferent layers. 0.1 0.2 BaselineTB 0.20 0.25 0.3 0.35 BaselineTB 0.006 0.025 0.1 0.25 0.5 0.2 0.4 BaselineTB 0.006 0.025 0.1 0.25 0.5 0.30 0.35 BaselineTB",
  "Improving Low-Data Training UsingTempBalance": "Natural Language Understanding.In Fig-ure 3, we report the evaluation result of fine-tuningthe RoBERTa-base model with four larger GLUEdatasets. We compare TempBalance (shown asTB) with Full Fine-tuning (shown as FT) withdifferent subsampling ratios. We also show theresults on smaller GLUE tasks in . Wecan see that TempBalance consistently demon-strates performance improvement in all low-dataregimes. For example, when fine-tuning on thelarger SST2 dataset, TempBalance significantlyoutperforms the baseline with 9.9% improvementin test accuracy with 0.02% subsampling ratio. Re-garding the smaller RTE dataset with 50% trainingdata, TempBalance can improve test accuracyby 3.13%. The detailed results of all GLUE tasksare shown in and 18, in Appendix E.1.Domain-specific Language Modeling. In Fig-ure 5, we report the results of TempBalance onfive domain-specific low-resource datasets. Weshow that when fine-tuned on these datasets inlow-data settings, TempBalance continues toyield better test performance than the baselinemethod.Specifically on Hyperpartisan Newsdataset, TempBalance outperforms baseline FTby 5.13%.This indicates that TempBalancebrings significant improvement when applying tospecialized language modeling domains with lowresources.Neural PDE Solver Training.In , wereport the results of training the FNO and UNetmodel on the 1D and 2D CFD (compressible fluid",
  ":Domain Specific Language Modeling.TempBalance demonstrates significant performancegain when training the RoBERTa-base model on fivelow-resource domain-specific datasets": "dynamics) dataset with a subsampling ratio rangingfrom 0.6% to 100%, evaluated by NormalizedRoot Mean Squared Error (nRMSE). The detailedresults are shown in , Appendix E.4. Wefind that TempBalance achieves lower nRMSEcompared to the baseline on all subsamplingratios. Specifically, TempBalance reduces thenRMSE of the FNO model trained on 10.0%of the 1DCFD dataset significantly by 9.73%and improves the nRMSE of UNet on 2.5% by7.30%. Furthermore, TempBalance can achievecomparable performance gain to increasing thenumber of training data samples. For example,when solving 2DCFD problem using the UNetmodel with 10% data, applying TempBalanceyields comparable performance gain to increasingthe subsampling ratio to 25%.",
  ": Comparing TempBalance with Sharpness-Aware Minimization (SAM) and AdaFactor on RoBERTa-basemodel trained with QNLI dataset. For SAM, we choose hyperparameter in the range of {0.5, 0.25, 0.1, 0.05}": "vide supplementary results on a broader rangeof settings in Appendix E. We first evaluateTempBalance on more full fine-tuning andLoRA fine-tuning tasks of RoBERTa-base andLLaMA-7B, then we explore more SciML settingsby training the FNO and UNet to solve CFD PDEs.We also provide statistical testing to verity the sig-nificance of our results.",
  "Comparison with Other Methods": "Recent works have proposed optimization methodsthat efficiently improve low-data training especiallyon LLMs. For example, Sharpness-Aware Mini-mization (SAM) (Foret et al., 2021) has been shownto effectively improve fine-tuning performancewhen training data is limited, by encouraging con-vergence to flatter local minima (Bahri et al., 2022).Also, AdaFactor is a memory-efficient optimizersuitable for training large models (Shazeer andStern, 2018). We show that TempBalance notonly outperforms these methods in most low-dataregimes, but can be used as an add-on method tofurther enhance model performance.We compare TempBalance with SAM andAdaFactor using RoBERTa-base model trainedwith QNLI on four subsampling ratios, as shown in. We can see that when we have fewer datapoints, SAM achieves worse results than baselineFT. Meanwhile, TempBalance consistently out-performs baseline FT, and achieves better resultsthan SAM in almost all cases. For the AdaFactoroptimizer, we can see that it outperforms baselineand TempBalance in most cases. Still, when wecombine TempBalance with AdaFactor, we canachieve the best results across all low-data regimes,with at most 1.95% test accuracy increase higherthan AdaFactor alone.",
  "Neural PDE Fine-tuning": "To explore diverse scenarios in SciML, we con-duct experiments on low-data fine-tuning usingthe 2DCFD dataset with DPOT-Tiny and DPOT-Small models. In solving PDEs, we utilize founda-tional models pre-trained on various fluid dynamics datasets, which are then fine-tuned on another spe-cific physical scenario. In , we show thatTempBalance (TB) offers better improvementscompared to the baseline FT under different sub-sampling ratios.The experimental settings for SciML tasks areas follows: For TempBalance (TB) and FT, wetrain the models for 500 epochs with a batch size of160 for the Tiny model and 64 for the Small model,and a dropout rate of 1e-6. We test initial learn-ing rates among {0.001, 0.0005, 0.00025, 0.0001,0.00005}. We use the Adam optimizer, and decaythe learning rate by = 0.5 every 50 epochs. Themean and standard deviation of nRMSE across 3random seeds on the test set are reported.",
  "Analysis": "Following section 4.2, we study the effectivenessof TempBalance in overcoming low-data limi-tations. First, we look into the trend of improve-ment brought by TempBalance, and demonstratethat layer-wise tuning like TempBalance bringsmore significant improvement as we train withfewer data. Second, we investigate the distribu-tion of PL_Alpha_Hill across layers, and showthat TempBalance successfully balances layer-wise training quality, resulting in a more uniformPL_Alpha_Hill distribution compared to thebaseline method.Analyzing Performance Gain of TempBalance. As we have shown in our main results, we notethat TempBalance achieves greater performancegain as the subsampling ratio becomes lower. Thistrend suggests that TempBalance is more effec-tive as we train the model with fewer data. Thistrend suggests that when training data is large,model training quality is high without specific ma-nipulations. However, if we only have a few sam-ples, the layer-wise balancing method becomes in-creasingly beneficial and can significantly improvemodel performance.Analyzing PL_Alpha_Hill Distribution. Wecompare the distribution of PL_Alpha_Hillbetween baseline FT and TempBalance.Asobserved in , TempBalance consis-tently shows lower PL_Alpha_Hill variance onRoBERTa-base trained on QNLI under various sub-sampling ratios. Furthermore, in SciML tasks, wecan see a similar trend that is more significant whenwe train the model from scratch ().Following the trend shown previously in Fig-ure 2, this finding suggests that as layer-wise train-ing quality becomes more unevenly distributedas we train with fewer data, TempBalanceeffectively balances training quality across dif-ferent layers (estimated by the variance ofPL_Alpha_Hill).",
  "Ablation study": "Temperature Balancing with Different ESD Met-rics.Recent theoretical works have proposedseveral metrics that measure the shape of theESD (Martin and Mahoney, 2021; Martin et al.,2021; Yang et al., 2023), and we compare theirperformance with PL_Alpha_Hill in assign-ing layer-wise learning rates.We mainly con-sider two shape metrics: Spectral_Norm andStable_Rank.Results are presented in Ta-ble 3.We can see that in all subsampling ra-tios, PL_Alpha_Hill continues to outperformother metrics, while other metrics may performworse than baseline Full FT. We can conclude thatPL_Alpha_Hill have more robust performancethan other shape metrics in assigning layer-wiselearning rates.Different Learning Rate Scheduling functions.In the TempBalance algorithm, we chooseTB_Sigmoid equation as our layer-wise schedul-ing function.To verify the superiority ofTB_Sigmoid function, we evaluate anotherscheduling function TB_Linear_Map, which isproven to have great performance on image classi-fication tasks (Zhou et al., 2024). The results are",
  "Conclusions": "In this work, we leverage HT-SR theory to di-agnose the limitations of low-data training andimprove the learning rate scheduling algorithmTempBalance to balance the training quality ofdifferent layers in low-data regimes. Our exten-sive experiments demonstrate that TempBalanceeffectively balances layer-wise training qualityand improves performance in NLP fine-tuningand SciML training.Our analysis reveals thatTempBalance achieves greater performancegain as we train with fewer data. Furthermore, thecompatibility of TempBalance makes it possibleto add TempBalance to existing optimizationmethods, bringing further performance improve-ments. We show that HT-SR theory brings usefulguidance in low-data training and fine-tuning, andwe expect it to be a more generalized toolbox fordiagnosing model performance in more trainingscenarios.Acknowledgments.Thisworkissup-ported by DOE under Award Number DE-SC0025584, DARPA under Agreement numberHR00112490441, and Dartmouth College.",
  "Limitations": "Despite achieving improvements in NLP andSciML tasks, TempBalance has some potentiallimitations.For computational costs, since TempBalancedynamically reschedules learning rates during train-ing, frequent calculations of ESD of weight matri-ces are required. In our work, the computationoverhead of TempBalance during training theRoBERTa-base model can take up to 25% of thetotal training time: when training on 0.02% SST2dataset, the total training time is 265.73 seconds,in which TempBalance takes up 65.40 seconds.This computational cost could scale up as the modelsize becomes larger. Since the calculation of ESDcontributes to most of the computation cost (theSVD process), we will focus on improving the ef-ficiency of measuring the Heavy-Tail structure ofthe ESD.In addition, we only discuss the schedulingof the learning rate in this work, whereas othertemperature-like parameters can also influence thestructure of ESD during training, such as batch sizeor weight decay. Therefore it would be of interestto explore how HT-SR theory can assist in acquir-ing a comprehensive set of hyperparameter tuningtools.",
  "Ethics Statement": "This paper leverages HT-SR theory to designa layer-wise fine-tuning scheme for LLMs andSciML models. Our study in itself does not poseany negative societal risks or ethical concerns. Onthe contrary, it improves our understanding of theinner mechanisms of training NNs which can po-tentially aid in optimizing the amount of computeresources spent on training large NNs for wide so-cietal use. Kumar Krishna Agrawal, Arnab Kumar Mondal, ArnaGhosh, and Blake Aaron Richards. 2022. $\\alpha$-req : Assessing {\\bf Re}presentation {\\bf Q}uality inself-supervised learning by measuring eigenspectrumdecay. In Advances in Neural Information Process-ing Systems.",
  "tails in sgd and compressibility of overparametrizedneural networks. Advances in neural informationprocessing systems, 34:2936429378": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Wuyang Chen, Jialin Song, Pu Ren, Shashank Subra-manian, Dmitriy Morozov, and Michael W Mahoney.2024. Data-efficient operator learning via unsuper-vised pretraining and in-context learning. Advancesin Neural Information Processing Systems.",
  "Mert Gurbuzbalaban, Umut Simsekli, and LingjiongZhu. 2021. The heavy-tail phenomenon in sgd. In In-ternational Conference on Machine Learning, pages39643975. PMLR": "Zhongkai Hao, Chang Su, Songming Liu, Julius Berner,Chengyang Ying, Hang Su, Anima Anandkumar, JianSong, and Jun Zhu. 2024. Dpot: Auto-regressivedenoising operator transformer for large-scale pdepre-training. arXiv preprint arXiv:2403.03542. Zhongkai Hao, Zhengyi Wang, Hang Su, ChengyangYing, Yinpeng Dong, Songming Liu, Ze Cheng, JianSong, and Jun Zhu. 2023. Gnot: A general neuraloperator transformer for operator learning. In Inter-national Conference on Machine Learning, pages1255612569. PMLR.",
  "Pengxiang Li, Lu Yin, Xiaowei Gao, and Shiwei Liu.2024. Owlore: Outlier-weighed layerwise sampledlow-rank projection for memory-efficient llm fine-tuning. arXiv preprint arXiv:2405.18380": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli,Burigede Liu, Kaushik Bhattacharya, Andrew Stu-art, and Anima Anandkumar. 2020. Fourier neuraloperator for parametric partial differential equations.arXiv preprint arXiv:2010.08895. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. Preprint, arXiv:1907.11692. Robert L Logan IV, Ivana Balaevic, Eric Wallace,Fabio Petroni, Sameer Singh, and Sebastian Riedel.2021.Cutting down on prompts and parameters:Simple few-shot learning with language models.arXiv preprint arXiv:2106.13353. Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang,Michael W. Mahoney, and Yaoqing Yang. 2024. Al-phapruning: Using heavy-tailed self regularizationtheory for improved layer-wise pruning of large lan-guage models. Advances in Neural Information Pro-cessing Systems. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang,and George Em Karniadakis. 2021. Learning non-linear operators via deeponet based on the universalapproximation theorem of operators. Nature machineintelligence, 3(3):218229. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521. Yi Luan, Luheng He, Mari Ostendorf, and Han-naneh Hajishirzi. 2018.Multi-task identificationof entities, relations, and coreference for scien-tific knowledge graph construction. arXiv preprintarXiv:1808.09602. Charles H Martin and Michael W Mahoney. 2020.Heavy-tailed universality predicts trends in test accu-racies for very large pre-trained deep neural networks.In SIAM International Conference on Data Mining. Charles H Martin and Michael W Mahoney. 2021. Im-plicit self-regularization in deep neural networks: Ev-idence from random matrix theory and implicationsfor learning. Journal of Machine Learning Research,22(165):173.",
  "Post-mortem on a deep learning contest: a simpsonsparadox and the complementary roles of scale metricsversus shape metrics. Preprint, arXiv:2106.00734": "Charles H Martin, Tongsu Peng, and Michael W Ma-honey. 2021. Predicting trends in the quality of state-of-the-art neural networks without access to trainingor testing data. Nature Communications, 12(1):4122. Michael McCabe, Bruno Rgaldo-Saint Blancard,Liam Holden Parker, Ruben Ohana, Miles Cranmer,Alberto Bietti, Michael Eickenberg, Siavash Golkar,Geraud Krawezik, Francois Lanusse, et al. 2023.Multiple physics pretraining for physical surrogatemodels. arXiv preprint arXiv:2310.02994.",
  "Josue Nassar, Piotr Sokol, SueYeon Chung, Kenneth DHarris, and Il Memming Park. 2020. On 1/n neuralrepresentation and robustness. Advances in NeuralInformation Processing Systems, 33:62116222": "Peijun Qing, Chongyang Gao, Yefan Zhou, XingjianDiao, Yaoqing Yang, and Vosoughi Soroush. 2024.Alphaexpert: Assigning lora experts based on layertraining quality. In Proceedings of the 2024 Con-ference on Empirical Methods in Natural LanguageProcessing. Maziar Raissi, Paris Perdikaris, and George E Karni-adakis. 2019. Physics-informed neural networks: Adeep learning framework for solving forward andinverse problems involving nonlinear partial differ-ential equations. Journal of Computational physics,378:686707. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ questions formachine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392, Austin,Texas. Association for Computational Linguistics. Bogdan Raonic, Roberto Molinaro, Tim De Ryck, To-bias Rohner, Francesca Bartolucci, Rima Alaifari,Siddhartha Mishra, and Emmanuel de Bzenac. 2024.Convolutional neural operators for robust and accu-rate learning of pdes. Advances in Neural Informa-tion Processing Systems, 36.",
  "Noam Shazeer and Mitchell Stern. 2018. Adafactor:Adaptive learning rates with sublinear memory cost.Preprint, arXiv:1804.04235": "Umut Simsekli, Ozan Sener, George Deligiannidis,and Murat A Erdogdu. 2020. Hausdorff dimension,heavy tails, and generalization in neural networks.Advances in Neural Information Processing Systems,33:51385151. Shashank Subramanian, Peter Harrington, Kurt Keutzer,Wahid Bhimji, Dmitriy Morozov, Michael W Ma-honey, and Amir Gholami. 2024. Towards founda-tion models for scientific machine learning: Charac-terizing scaling and transfer behavior. Advances inNeural Information Processing Systems, 36. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz,Daniel MacKinlay, Francesco Alesiani, Dirk Pflger,and Mathias Niepert. 2022. Pdebench: An extensivebenchmark for scientific machine learning. Advancesin Neural Information Processing Systems, 35:15961611. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R. Bowman. 2020. Superglue: A stickierbenchmark for general-purpose language understand-ing systems. Preprint, arXiv:1905.00537. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019.Glue: A multi-task benchmark and analysis plat-form for natural language understanding. Preprint,arXiv:1804.07461. Hanchen Wang, Tianfan Fu, Yuanqi Du, WenhaoGao, Kexin Huang, Ziming Liu, Payal Chandak,Shengchao Liu, Peter Van Katwyk, Andreea Deac,et al. 2023. Scientific discovery in the age of artificialintelligence. Nature, 620(7972):4760. Yutong Wang, Rishi Sonthalia, and Wei Hu. 2024a.Near-interpolators: Rapid norm growth and the trade-off between interpolation and generalization. In In-ternational Conference on Artificial Intelligence andStatistics, pages 44834491. PMLR. Zhichao Wang, Andrew Engel, Anand D Sarwate, IoanaDumitriu, and Tony Chiang. 2024b. Spectral evolu-tion and invariance in linear-width neural networks.Advances in Neural Information Processing Systems,36. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander M. Rush. 2020. Hug-gingfaces transformers: State-of-the-art natural lan-guage processing. Preprint, arXiv:1910.03771.",
  "Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-berger, and Yoav Artzi. 2021. Revisiting few-sample{bert} fine-tuning. In International Conference onLearning Representations": "Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-tive example selection for in-context learning. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 91349148, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, JiaoSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,Luke Zettlemoyer, and Omer Levy. 2023. Lima: Lessis more for alignment. Preprint, arXiv:2305.11206. Yefan Zhou, Tianyu Pang, Keqin Liu, Michael W Ma-honey, Yaoqing Yang, et al. 2024. Temperature bal-ancing, layer-wise weight analysis, and neural net-work training. Advances in Neural Information Pro-cessing Systems, 36.",
  "APotential Risks": "Our work leverages HT-SR theory as a model di-agnosis tool to analyze the limitations of low-datatraining and fine-tuning, and help the design of animproved learning rate scheduling algorithm. Wedo not see any immediate negative societal impactsor ethics issues stemming from the algorithm it-self. In addition, our analysis could inspire futureresearch on diagnosing performance limitations indifferent scenarios, securing the safe use of LLMs.",
  "BAblation study on granularity ofLearning Rate Scheduling: Per-blockvs. Per-layer": "Following the discussion on scheduling method forTransformer-based models in .2, here wecompare the performance of block-wise and layer-wise scheduling in RoBERTa-base model trainedon QNLI dataset. shows that the block-wise method generally outperforms the per-layermethod in different subsampling ratios. The resultssuggest that block-wise learning rate scheduling isa more favorable method than layer-wise schedul-ing when we use TempBalance on Transformer-based models.",
  "CData Subsampling": "To create low-data regimes, we design sets of sub-sampling ratios based on the size of different train-ing datasets (see and 7). For GLUE fine-tuning, we partition the datasets in GLUE into twogroups: larger datasets (SST-2, MNLI, QNLI andQQP), and smaller datasets (CoLA, MRPC, STS-Band RTE). For larger datasets, we choose subsam-pling ratio from {0.02% 0.05%, 0.1%, 0.5%, 1%,5%}, and for smaller datasets, we choose subsam-pling ratios from {10% 20%, 50%}. For PDEsolving tasks, we use the datasets from PDEBench(Takamoto et al., 2022) and choose different dataratios considering the training difficulty in differ-ent datasets. For DarcyFlow dataset, the range ofsubsampling ratio is {0.6%, 2.5%, 5.0%, 10.0%,",
  "D.1Full Fine-tuning on GLUE andSuperGLUE Datasets": "For full-finetuning,we choose to fine-tuneRoBERTa-base model on GLUE and SuperGLUEdatasets. For each subsampling ratio, we train us-ing the Adam optimizer with a linear learning ratedecay schedule for 10 epochs. We choose the se-quence length of 128, and grid search learning rateand batch size to obtain the best results. Whentraining on four smaller GLUE datasets (CoLA,MRPC, STSB, RTE) and SuperGLUE datasets, wesearch learning rate across {1e-5, 2e-5, 3e-5} andbatch size across{16, 32}; when training on fourlarger GLUE datasets (SST2, MNLI, QNLI, QQP),the search range of learning rate and batch sizeare shown in and 9 respectfully. For otherhyperparameters and model configurations, we usethe same settings following Liu et al. (Liu et al.,2019). We report the mean over 3 random seedsfor each setting, where the results for each run aretaken from the best epoch.",
  ": Batch size range of training RoBERTa-basemodel on subsets of SST2, MNLI, QNLI and QQPdatasets": "In addition to standard training configurations,we report the hyperparameters of TempBalancecorresponding to the best results. Specifically, wereport hyperparameters s. Note that during hyper-parameter search, we find that assigning different svalues to layers with PL_Alpha_Hill higher orlower than the mean PL_Alpha_Hill across alllayers can achieve better results, and in the tables,we show them as a pair (s1, s2), often (2, 1).",
  "D.2LoRA Fine-tuning": "For LoRA fine-tuning, we adopt the training con-figurations from previous works and perform a linesearch around the base learning rate. For trainingRoBERTa-base model on GLUE datasets, we fol-low Hu et al (Hu et al., 2021). and evaluate learningrate at 2e-4 and 6e-4 around the base learning rate(4e-4 or 5e-5). For LLaMA-7B on ScienceQA, wetrained with AdamW optimizer for 50 epochs, andsearch the best learning rate in the range of {2e-4,3e-4, 4e-4}. We set the cutoff length as 256 andbatch size as 128. For LoRA adapters, we set therank to 8, LoRA alpha to 16, and LoRA dropout to0.05.",
  "D.3Neural PDE Solving": "For SciML, we referred to PDEBench(Takamotoet al., 2022) for the hyperparameter settings andselected the appropriate learning rate, weight decayand batch size using a grid search method to makebaseline models achieve good performances. Foreach subsampling ratio, we train the models withthe Adam optimizer, scheduling the base learningrate by decaying the learning rate by = 0.5 ev-ery 100 epochs. We chose to train the models forenough epochs to ensure that the trained modelswere close to a convergent state. For the hyperpa-rameter s in TempBalance, we choose from therange {0.125, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5}.For training the FNO and UNet on DarcyFlow( = 100), the search range of leanring rate andthe selected weight decay are displayed in and the batch size is 50.",
  ": Learning rate range and the selected weightdecay of training FNO and UNet model on subsets ofDarcyFlow( = 100.0) dataset": "When training the FNO on 1D and 2D CFDdatasets, the search range of learning rate and theselected weight decay is shown in . Thebatch size for the subsampling ratio {100%, 50.0%,25.0%, 10.0%} in training on 1DCFD is 25 and 10for {2.5%, 0.6%}, while on the 2DCFD dataset thebatch size is 20.",
  "EComplementary Results": "In this section, we first provide detailed results dis-cussed in .3 in the paper, then further eval-uate TempBalance on NLP and SciML trainingtasks. Also in Section E.2, we provide statisticaltesting results to demonstrate the significance ofimprovement brought by TempBalance. First,in E.1 and E.4 we show detailed results of GLUEfull fine-tuning and two time-dependent PDEs dis-cussed in .3. Second, we present comple-mentary results of TempBalance on fine-tuningRoBERTa-base model on SuperGLUE and SQuADdatasets in E.3. Then, we apply TempBalanceto LoRA fine-tuning, and show the results of LoRAfine-tuning of RoBERTa-base model on GLUEtasks in E.5, and LLaMA-7B model on ScienceQAin E.6. Afterwards, we evaluate TempBalanceon solving DarcyFlow PDEs with FNO and UNetmodel in E.7.",
  "E.2Statistical Testing on the Significance ofImprovement": "We perform statistical testing to verify the effective-ness of our algorithm compared to baseline meth-ods. We define the Null Hypothesis (H0) as Thereis no significant difference in performance betweenour algorithm and the baseline, and the AlternativeHypothesis (H1) as Our algorithm performs sig-nificantly better than the baseline. We run exper-iments of training RoBERTa-base on SST-2 withdifferent subsampling ratios for 10 random seedsand perform t-tests on the results. We present theresults in the table below:",
  "E.3Full Fine-tuning on SuperGLUE andSQuAD": "SuperGLUE. In , we present the resultsof applying TempBalance on training RoBERTa-base model on SuperGLUE tasks. The tasks andtheir corresponding evaluation metrics are: BoolQ(Accuracy), RTE (Accuracy), CB (Accuracy andF1), WiC (Accuracy), MultiRC (F1 and ExactMatch (EM)), COPA (Accuracy). We can see thatTempBalance effectively increases test perfor-mance in most cases, and archives significant over-all improvement.Specifically, TempBalanceachieves 7.14% performance gain when trainingon 50% CB dataset.TempBalance can alsoimprove the overall mean performance by 1.65%when trained with 50% data.SQuAD. In , we present the results of ap-plying TempBalance on training RoBERTa-basemodel on SQuAD (v1.1) dataset across five subsam-pling ratios: 1%, 5%, 10%, 20%, 50%. We trainthe model for 10 epochs with learning rate 2e-5",
  "TB93.690.1683.360.1588.240.0884.000.1587.32(0.35)": ": Evaluation results of RoBERTa-base model trained on larger GLUE tasks. We compare TempBalance(TB) with Full Fine-tuning (FT) trained with Adam optimizer and linear learning rate decay. The tasks and theircorresponding evaluation metrics are: SST-2 (accuracy, ), MNLI (accuracy, ), QNLI (accuracy, ) and QQP(combined score of F1 score and accuracy, )",
  "TB58.600.7488.400.4290.240.0674.851.7878.02(1.51)": ": Evaluation results of RoBERTa-base trained on smaller GLUE tasks using full fine-tuning. We compareTempBalance with baseline FT (Full Fine-tuning) on: CoLA (Matthews Correlation, ), MRPC (combined scoreof F1 score and accuracy, ), STS-B (combined score of Pearson and Spearman Rank, ), and RTE (Accuracy, ) and a batch size of 24 using the AdamW optimizerwith a warmup rate of 0.06 and linear learningrate decay. We follow the detailed hyperparametersettings from (Liu et al., 2019). The mean and stan-dard deviation of test accuracy across 3 randomseeds on the test set are reported. We observe thatTempBalance continues to achieve better testperformance than baseline FT, and significantly out-performs baseline FT in low-data regimes: whentrained on 1% data of SQuAD, TempBalanceincreases the test accuracy by 3.07%.",
  "E.5LoRA Fine-tuning on GLUE": "Measuring the ESD of LoRA Adapters. Somemodels are too large to fine-tune fully, so one oftenneeds to use LoRA. In this case, LoRA adaptersare added to selected layers in the model, and onlythese adapters are trained during fine-tuning, whilethe original weight matrix remains fixed. For alayer with weight matrix W Rdk and LoRAadapters B Rdr and A Rrk, we can-not simply calculate ESD of the product betweenadapters B A, since the rank of the adaptersr min(d, k) are low-rank matrices, which wouldresult in a poor ESD landscape. Therefore, forlayers with LoRA adapters, we calculate the sumof the product of LoRA adapters and the weightmatrix W = W + B A, and then calculate theESD of its correlation matrix X = WW.WepresenttheresultsofapplyingTempBalance on LoRA Adapters in .We can see that TempBalance consistently",
  ": Evaluation results of RoBERTa-base model trained on SuperGLUE tasks using full fine-tuning": "achieves higher test results than LoRA alone. Wenote that our method can at most improve the testaccuracy of 3.29% on 0.02% SST2 dataset, indi-cating a significant improvement. From averageimprovement increases across different tasks, wecan see that as we reduce the subsampling ratio,the average improvement of TempBalance onall tasks continues to increase. This observationaligns with the discussion in .6, thatTempBalance achieves gradually increasedgains in fine-tuning performance as the numberof tuning data points decreases, further provingthe effectiveness of TempBalance in achievingmodel alignment in low-data regimes.",
  "E.7Training FNO and UNet Model onDarcyFlow Dataset": "In we show the test results of trainingthe FNO and UNet model on the DarcyFLowdataset with a subsampling ratio ranging from0.6% to 100%, evaluated by Normalized RootMean Squared Error (nRMSE). We show thatTempBalance achieves lower nRMSE comparedto the baseline on all subsampling ratios. Specifi-cally, TempBalance reduces the nRMSE of theUNet model trained on 2.5% of the DarcyFlowdataset by a significant 10.89%, and improve thenRMSE of FNO on 0.6% by 9.71%.",
  "G.1Different ESD metrics and schedulingfunctions in using TempBalance inSciML": "We compare the performance of using differentESD measuring metrics and scheduling functionsof TempBalance on SciML tasks. re-ports the results of different TempBalance set-tings in training the FNO model on solving the1DCFD task. We can see that TempBalanceoutperforms the baseline method at every sub-sampling ratio, and our proposed scaling functionTB_Sigmoid achieves more stable performancethan TB_Linear_Map. At most subsampling ra-tios, using PL_Alpha_Hill we can achieve re-sults that are comparable to or even better thanthose obtained with other metrics.",
  "H.1Diagnosing the Data Limitation Using HTMetrics": "Following .2, here we further analyzedFNO models test performance using Alpha-relatedmetrics as the training data size decreases. Fig-ure 6 demonstrates that the change of the STDof PL_Alpha_Hill corresponds very closelywith the variations in the models performance.We observe that as the subsampling ratio de-creases, the nRMSE on the 1D and 2D CFDPDEs solving increases, indicating a deteriora-tion in models performance. Simultaneously, theSTD of PL_Alpha_Hill becomes larger, sug-gesting that the training across the model layersis becoming increasingly uneven. Therefore, theSTD of PL_Alpha_Hill effectively captures",
  "H.2More Analysis Study Results in the STDof PL_Alpha_Hill": "In and 8, we compare the STD ofthe PL_Alpha_Hill between the baseline andTempBalance on fine-tuned LLM and trainedFNO models at different subsampling ratios. Whenthe subsampling ratio is relatively large, the STDof PL_Alpha_Hill of models is smaller, andthe impact of the TempBalance method on thismetric is also minimal. However, when the sub-sampling ratio is relatively small, the opposite istrue: the TempBalance method makes the distri-bution of PL_Alpha_Hill across each layer ofthe model more uniform. 0.00020.00050.0010.0050.01",
  "b STD of layer-wise PL_Alpha_Hill in training FNOon 2DCFD": ": Comparing the STD of layer-wise PL_Alpha_Hill measured in using baseline method andTempBalance training FNO model on 1D and 2D CFD datasets. The results demonstrate that TempBalancecan reduce the STD, and this effect is more significant when the subsampling ratio is smaller, indicating that ourapproach helps ensure more uniform training across each layer of the model."
}