{
  "Abstract": "Large language models (LLMs) have demon-strated impressive capabilities, but still sufferfrom inconsistency issues (e.g. LLMs can re-act differently to disturbances like rephrasingor inconsequential order change). In additionto these inconsistencies, we also observe thatLLMs, while capable of solving hard problems,can paradoxically fail at easier ones. To evalu-ate this hard-to-easy inconsistency, we developthe ConsisEval benchmark, where each entrycomprises a pair of questions with a strict or-der of difficulty. Furthermore, we introducethe concept of consistency score to quantita-tively measure this inconsistency and analyzethe potential for improvement in consistency byrelative consistency score. Based on compre-hensive experiments across a variety of existingmodels, we find: (1) GPT-4 achieves the high-est consistency score of 92.2% but is still incon-sistent to specific questions due to distractionby redundant information, misinterpretation ofquestions, etc.; (2) models with stronger capa-bilities typically exhibit higher consistency, butexceptions also exist; (3) hard data enhancesconsistency for both fine-tuning and in-contextlearning. Our data and code will be publiclyavailable on GitHub.1",
  "Introduction": "With the increases in pre-training corpora andthe number of parameters (Radford et al., 2018,2019; Brown et al., 2020), large language mod-els (LLMs) have shown remarkable performanceacross various natural language processing (NLP)tasks, even generating expert-level responses touser queries.The extraordinary capabilities ofLLMs hold potential for further real-world applica-tions (Wang et al., 2023c; Guo et al., 2023; Driesset al., 2023), which necessitate higher requirementsfor model trustworthiness (Wang et al., 2023a; Li",
  ": A hard-to-easy inconsistency case of LLMs.A counter-intuitive phenomenon occurs when an LLM,which can solve a harder problem, surprisingly goeswrong on an easier problem": "et al., 2023a; Sun et al., 2024a) and consistency(Jang and Lukasiewicz, 2023; Elazar et al., 2021).However, LLMs still suffer from inconsistencyissues: semantically equivalent queries (Elazaret al., 2021; Raj et al., 2023) and insignificant orderchanges of inputted contents (Wang et al., 2023b)can lead to divergent outcomes; LLMs can also be-have differently in the generation versus validationof the same content (Li et al., 2023b); moreover,logical transformations like negation and symmetrycan also induce inconsistent behaviors (Jang et al.,2022). In addition to previous work, we also findLLMs able to solve hard problems surprisingly failto solve easier ones (as shown in ), suffer-ing from the hard-to-easy inconsistency. UnlikeLLMs, humans are naturally consistent reasoners,and it is undisputed that an individual proficientin calculus can easily address simpler arithmeticproblems. However, why this difference exists isstill unknown and relevant research to explore hard-to-easy consistency of LLMs is still lacking. To systematically evaluate this consistency ofLLMs, we develop ConsisEval, a Hard-to-easyConsistency Evaluation Benchmark, through au-tomatic generation and human annotation. Consi-sEval encompasses data from three domains: in-struction following, code, and mathematics, eachentry consisting of a pair of questions with a strictorder of difficulty. Considering the absence of anoff-the-shelf metric, we propose a new metric con-sistency score, which is defined as the conditionalprobability of a model correctly answering easyquestions provided that it has correctly answeredharder ones, for quantitative assessment of con-sistency from a probabilistic stance. Further, toanalyze the potential for improvement in consis-tency if model capability remains unchanged, weintroduce the concept of relative consistency score.The calculation of our metrics relies on the proba-bility of a model answering each question correctlythrough a single sampling, for which we designtwo probability estimation methods.Based on our benchmark and metrics, we con-duct extensive experiments on various LLMs.Among evaluated models, GPT-4 (Achiam et al.,2023) achieves the highest CS of 92.2%, demon-strating notable hard-to-easy consistency. Nonethe-less, GPT-4 also exhibits inconsistent behaviorsto specific prompts due to distraction by redun-dant information, misinterpretation of questions,etc. Further, we find models with stronger capa-bilities typically exhibit higher consistency, but ex-ceptions where powerful models demonstrate poorconsistency also exist. Additionally, we discoverthat models show higher consistency when trainedunder hard data than easy data, and that holds thesame under few-shot setting (in-context learningwith harder demonstration examples shows betterconsistency).We summarize our contributions as follows:",
  "ConsisEval Benchmark": "To systematically evaluate the hard-to-easy consis-tency of LLMs, we develop ConsisEval with datafrom code, mathematics, and instruction-followingdomains, which are widely considered to be diffi-cult but of significant importance for LLMs (Weiet al., 2021; Cobbe et al., 2021a,b; Zhou et al.,2023). Different from traditional benchmarks inwhich data are usually individual, there are onlypairwise data in ConsisEval: one datum is com-prised of two questions (an easy question and aharder one) with a strict order of difficulty, andwe present some example data from ConsisEval in. To construct ConsisEval, we collect easydata from some established public datasets (2.1);then we acquire hard data through automatic gener-ation by GPT-4 and human annotation (2.2), andthis process is shown in .",
  "Easy Data Collection": "Mathematicseasy data are collected fromGSM8K (Cobbe et al., 2021a), a linguistically di-verse collection of high-quality grade school mathword problems crafted by human problem writers.The difficulty of these problems varies, requiringfrom 2 to 8 steps to solve, and solving these prob-lems typically requires a series of fundamental cal-culations employing basic arithmetic operations(+). To prevent easy data from being too dif-ficult to be further improved in terms of difficulty,we only select the problems requiring 3 steps tosolve in the test set of GSM8k as our easy data inthe mathematics domain (298 entries). Codeeasy data are collected from HumanEval(Cobbe et al., 2021b), a benchmark aiming at evalu-ating the capability of LLMs to generate standalonePython functions from docstrings. For each cod-ing problem, a check function containing some testcases is provided for automatic correctness evalu-ation of code samples. Since HumanEval is rela-tively small , we select all of the data in HumanEvalas our easy data in code domain (164 entries). Instruction-followingeasy data are collectedfrom IFEval (Zhou et al., 2023), a benchmark com-prised of various instructions for LLMs to follow.Each instruction contains 1-3 verifiable constraints(e.g. maximum number of words in response or",
  "datum": ": The hard data collection process of ConsisEval. An easy datum is fed into GPT-4 with a well-designedprompt and multiple hard data candidates are sampled. Human annotators select the one of best quality, then checkand revise the sample to make it fit our criteria. the appearance of specific keywords in response),whose correctness can be automatically evaluatedby rule-based check functions. We only select theinstructions with only one constraint as our easydata in instruction-following domain (270 entries).",
  "Hard Data Collection": "To build our pairwise dataset in which a strict or-der of difficulty is guaranteed for each pair of easyand hard problems, all of the hard data are modi-fied from easy data. We employ a semi-automaticpipeline that integrates the automatic generation ofGPT-4 with human annotation to acquire hard data,and the whole workflow is illustrated in .Compared to traditional methods that rely solely onmanual annotation, our semi-automatic approachcan significantly alleviate the workload of humanannotators. Automatic generation.Considering the remark-able performance of GPT-4 on various text genera-tion tasks, we employ GPT-4 as a strong modifieddata generator to acquire our hard data candidatesfor human annotators to choose from. To makeGPT-4 understand our criteria better, we insert easydata into a well-designed prompt template (shownin Appendix J) before feeding them into GPT-4.Taking the code domain as an example, the promptconsists of 5 parts: (1) the #Instruction# part ar-ticulates the information we want GPT-4 to know,including but not limited to definition of our modifi-cation task, composition of a datum, and guaranteeof strict order of difficulty; (2) the #Demonstra-tions# part requires insertion of easy and hard datapairs as demonstrations; (3) finally, an easy datumtargeted for modification is decomposed into three",
  "components and inserted into the #Problem#, #An-swer#, and #Check Function# parts, respectively": "Human annotation.Though we have endeav-ored to request GPT-4 to generate hard data thatfully adheres to our criteria through a well-designedprompt, the generated contents may still not meetour standards (e.g. some samples lack a strict or-der of difficulty and check functions of some othersamples are incorrect). To address potential issuesin generated samples, we have engaged human an-notators to inspect, select, and revise these samples.Firstly, the annotators are required to select thesample of the highest quality from multiple candi-dates and discard all the other samples. To ensurecompliance with our criteria, the selected sampleis checked from two aspects:",
  "Evaluation Metrics": "Firstly, we formulate the evaluation problem andintroduce mathematical notations in 3.1. Consid-ering that there is no off-the-shelf metric to utilize,then we propose a new metric named ConsistencyScore (3.2) to measure the hard-to-easy consis-tency quantitatively. Further, we introduce the con-cept of Relative Consistency Score (3.3) to ana-lyze the potential for improvement in consistency.We model sampling an answer from an LLM for agiven question as a stochastic process, wherein theanswer is correct with a fixed probability p. Thecomputation of our metrics requires access to p,and 3.4 discusses how to estimate p by maximumlikelihood estimation.",
  "Problem Formulation and Notation": "Initially, we have a partially ordered set com-prising Npairs of data,denoted as A B = {(a1, b1), (a2, b2), ..., (aN, bN)}, where A ={a1, a2, ..., aN} represents a set of easy questions,and B = {b1, b2, ..., bN} constitutes a set of hardquestions. A stringent guarantee exists that the dif-ficulty order satisfies ai < bi, for bi is derived fromai by increasing the difficulty level. For a givenquestion ai (or bi), the model generates a correct an-swer through a single temperature-based samplingwith probability P(ai) (or P(bi)). We employ to symbolize estimates (e.g. P(ai) represents theestimate of the true value P(ai) ). For convenience,all of the notations mentioned and their meaningsare shown in Appendix A.",
  "Consistency Score": "Can large language models solve easy problemsif they can solve harder ones?To answer thisquestion from a probabilistic perspective, we in-troduce a metric termed Consistency Score (CS),which is the conditional probability of a model cor-rectly answering easy questions given that it hascorrectly answered harder ones. The higher CSindicates the lower probability humans encounterinconsistency phenomena when using LLMs, soCS is almost equal to human perceptions of modelconsistency. Let P(a|b) be the conditional proba-",
  "i=1,...,N P(ai)P(bi)i=1,...,N P(bi)(1)": "The detailed derivation of CS is shown in Ap-pendix B. To intuitively understand the distinctionsbetween consistent and inconsistent models andbetter illustrate CS, we present a Venn diagram in. The more consistent a model is, the largeroverlap area P(a, b) in Venn diagram, and conse-quently the higher CS of the model. Fundamentally,CS represents the ratio of P(a, b) to P(b).",
  "Relative Consistency Score": "In addition to CS that directly reveals consistencyprobability of LLMs, we also endeavor to analyzethe potential for improvement in consistency ifmodel capability remains unchanged. To analyzewhat the CS of an evaluated model M0 should beif it behaves extremely consistently/inconsistently,we formally define a model set = {M0, M1, ...}(detailed definition shown in Appendix C) in whichmodels possess similar capabilities to M0 and de-rive the upper and lower bounds of CS (denoted asCSupp and CSlow) among these hypothetical mod-els. Based on these bounds, we propose RelativeConsistency Score (RCS) (as shown in )to indicate the potential for improvement in consis-tency, and low RCS can reveal high potential forimprovement in CS. The RCS is given by:",
  "Probability Estimation": "For a given question ai and a given model, theprobability P(ai) that the model produces a cor-rect answer in a single sampling is an unknownconstant. We propose two methods for estimat-ing P(ai) based on repeated sampling. For open-source models that can be deployed locally, esti-mate P(ai) is obtained by sampling multiple an-swers independently. For proprietary models thatrequire payment for API calls, an early stoppingstrategy is employed during answer sampling toobtain estimate P(ai) with fewer API calls. Multiple Sampling EstimationFor a given ques-tion ai, answers are sampled m times to obtain asequence a1i , a2i , ..., ami . If the model generatesa correct answer on the jth sampling, we denoteaji = 1; otherwise, aji = 0. In this scenario, aji fol-lows a Bernoulli distribution, and j=1,...,m aji fol-lows a Binomial distribution (i.e. j=1,...,m aji B(m, P(ai))). It can be derived that the maximumlikelihood estimate of P(ai) (refer to Appendix E.1for the derivation details):",
  "m(5)": "Early Stopping EstimationEstimating throughmultiple sampling necessitates generating a multi-tude of answers for the same question (e.g. in 4 weutilize Llama2-7b-chat to sample 20 answers for aquestion). However, considering the high payment for the API calls and the typically high accuracy ofclosed-source models, an early stopping techniqueis employed to estimate with fewer API calls.Details of early stopping strategy: Initially, weset the minimum and maximum number of sam-pling times kmin and kmax. For a given questionai, initially, kmin answers are sampled. If at leastone correct answer exists in these answers, thesampling process will be terminated; otherwise,sampling will continue repeatedly until a correctanswer appears for the first time. Besides, the sam-pling procedure will be forcibly terminated if acorrect answer still does not emerge after samplingkmax answers.The total number of samples in the above processand the number of correct answers are denoted ask and kc, respectively. The maximum likelihoodestimation of P(ai) can be derived as follows (referto Appendix E.2 for the derivation details):",
  "Experimental Setup": "For closed-source models, we evaluate GPT-4Turbo 2 (Achiam et al., 2023), GPT-3.5 Turbo3, Qwen Max (Bai et al., 2023), and Claude-3 Opus 4, which can only be accessed via APIcalls.For open-source models, we experimenton Llama2-(7B,13B,70B) (Touvron et al., 2023),Llama3-(8B,70B) (AI@Meta, 2024), Qwen-1.5-(7B,14B,72B) (Bai et al., 2023), ChatGLM3-6B (Du et al., 2022), DeepseekLLM-(7B,67B)(DeepSeek-AI, 2024), Mistral-7B (Jiang et al.,2023), Baichuan2-(7B,13B) (Baichuan, 2023), andYi-6B (Young et al., 2024). Most of these open-source models are released with two versions, thepre-trained base model and the chat model (basedmodel + instruction tuning and alignment), and wefocus our evaluation solely on chat models. Moreimplementation details can be found in AppendixG.1.",
  "Main Results": "As illustrated in , we evaluate the hard-to-easy consistency of LLMs on ConsisEval and re-port the consistency score (CS) in three domainsand the average consistency score (Avg CS). Theaccuracy (%) on easy and hard sets (indicatingmodel capability) is also shown for comparison.Among the evaluated LLMs, GPT-4 Turbo show-cases outstanding performance in three domainsand achieves the highest Avg CS of 92.2%, closelyfollowed by Claude-3 Opus with an Avg CS is91.8%. Llama3-(8B,70B)-Instruct exhibit high ca-pability and consistency among open-source mod-els, superior to other models of comparable size.For comparison, CS of humans is theoretically100% if not take carelessness cases into consid-eration. Therefore, the potential for further im-provement in consistency still exists.We also observe a strong correlation betweencapability and consistency of LLMs. For example,Kendall rank correlation coefficient between accu-racy on hard set and CS across all evaluated LLMson code domain is 0.801 (further discussion is pro-vided in Appendix G.2). However, higher capabil-ity does not necessarily lead to higher consistency(e.g. in math domain, Claude-3 Oplus outperformsGPT-4 Turbo in capability, yet exhibits a lowerconsistency). Additionally, empirical results alsoshow CS is always larger than easy accuracy across",
  "Relative Consistency Analysis": "To analyze the potential for improvement in consis-tency, we attempt to compare the consistency of anevaluated model with other hypothetical models ofsimilar capability (\"capability\" can be intuitivelybut not strictly understood as \"performance on ac-curacy\", with a formal definition provided in Ap-pendix C). For each evaluated model, we presentits CS, upper and lower bounds of CS along withthe relative consistency score (RCS), which can beutilized to analyze potential improvement in con-sistency within the current capability.The experimental results in code domain are pre-sented in , while the comprehensive resultsacross all domains can be found in Appendix G.3.In code domain, we find that while GPT-4 Turboexhibits high consistency with a CS of 88.1%, thereis still considerable potential for improvement com-pared to the upper bound 93.0%. Furthermore, theRCS for GPT-4 Turbo is 34.8%, indicating a rela-tive improvement potential of 65.2%. Conversely,Llama2-70B-Chat, despite showing a low CS ofmerely 74.7%, achieves an RCS of 81.5%, indicat-ing notable consistency within its current capabil-ity. Baichuan2-7B-Chat Yi-6B-Chat Mistral-7B-Instruct Baichuan2-13B-Chat Llama2-13B-Chat Deepseek-7B-Chat Llama2-7B-Chat Qwen1.5-7B-Chat ChatGLM3-6B Llama3-8B-Instruct Qwen1.5-14B-Chat Llama2-70B-Chat Deepseek-67B-Chat Qwen1.5-72B-Chat GPT-3.5 Turbo",
  "Hard Training Data Benefits Consistency": "To investigate the impact of the ratio between easyand hard data in the training set on model consis-tency, we select 2,500 easy and 2,500 hard entriesfrom the training set of gsm8k (Cobbe et al., 2021a)based on the number of reasoning steps. We adjustthe ratio between easy and hard data while keep-ing the total amount constant at 2,500 entries toconstruct a series of training sets with varying pro-portions. We then fine-tune Llama3-8B on thesetraining sets (each group is repeated three timesunder different random seeds with Lora (Hu et al.,2021)) and observe the consistency behaviors. Asshown in , both the CS and RCS generallyincrease as the proportion of hard data increases,suggesting that hard training data benefits modelconsistency. Moreover, compared to a dataset com-posed entirely of hard data, a combination of 80%hard and 20% easy data yields better consistency,indicating proper easy data also contributes to en-hancing model consistency.",
  "Hard ICL Examples Benefits Consistency": "Similar to 5.1, we also explore the impact of easyand hard in-context learning (ICL) (Brown et al.,2020; Dong et al., 2022; Yang et al., 2023) demon-stration examples on model consistency. The ex-periments are under 1-4 shot setting, and for eachsetting we randomly select 20 easy and 20 hard ICLexamples to evaluate the consistency of Llama-8B- Hard data proportion (%) (Relative) Consistency score (%)",
  "Case Study: Why are LLMs Inconsistent?": "Through investigations on math inconsistencycases (shown in Appendix I), where the probabilityof solving hard problems is higher than that of eas-ier ones, we find even state-of-the-art GPT-4 stillbehaves inconsistently due to the following rea-sons: (1) Distracted by redundant information:As the case shown in , for the easy questionwith redundant conditions, GPT-4 incorrectly pro-ceeds with an additional step after having already",
  ": Consistency behavior of ICL with easy andhard examples under 1-4 shot settings. ICL with harderexamples shows higher consistency": "arrived at the correct answer, leading to a final in-correct result. (2) Data mismatch: As the caseshown in , GPT-4 could accurately analyzethe usage of \"dancing time on Tuesday\" for compu-tation, but it erroneously utilizes \"dancing time onThursday\" when conducting computation. (3) Mis-interpretation of questions: As the case shown in, the easy question requires finding the \"costof travel,\" GPT-4 misinterprets the requirement asthe \"cost of tickets for travel\". (4) Logical error(Off-by-one error): As the case shown in , the initial state should be recorded as \"Day 0\"in the easy question, but GPT-4 erroneously beganrecording from \"Day 1\". (5) Computational er-ror: As the case shown in , GPT-4 encoun-ters computational errors while solving an equationfor the easy question. Superficially, the inconsis-tency of GPT-4 stems from the occurrence of theabove mistakes on the easy questions but not onthe corresponding hard questions. However, deeperunderlying reasons remain unclear.",
  "Related Work": "Consistency of LLMsConsistency constitutesan important part of trustworthiness and reliability(Wang et al., 2023a; Li et al., 2023a; Chai et al.,2024; Liu et al., 2023) of LLMs. Humans are inher-ently consistent reasoners, but LLMs suffer frominconsistency problems. Wang et al. (2023b) findLLMs, when acting as evaluators, show inconsis-tency with insignificant order changes of evaluationcontent; Li et al. (2023b) observe that LLMs alsoshow inconsistency when generating and validatingthe same knowledge; Elazar et al. (2021); Raj et al.(2023) endeavor to evaluate and enhance the consis- tency with semantically identical expressions; Janget al. (2022); Jang and Lukasiewicz (2023) evaluateand analyze consistency to logical transformations,such as negation and symmetry. Different from per-spectives presented in previous works, our researchfocuses on the hard-to-easy consistency of LLMs.",
  "Easy-to-Hard GeneralizationHupkes et al": "(2020); Xu and Wang (2024) study the generaliza-tion ability of models trained on simple elementsto complex element combinations; likewise, Burnset al. (2023); Hase et al. (2024); Sun et al. (2024b)find models trained on easy data exhibit strong gen-eralization capabilities to hard data. However, wehave observed that training models solely on easydata can lead to inconsistent behaviors.",
  "Leveled EvaluationLiu et al. (2024); Xu et al": "(2024a) hierarchically evaluate the capability ofLLMs to solve problems of different difficulty lev-els by data categorized from easy to hard. Simi-larly but differently, we evaluate the consistencyof LLMs by pairwise hard-to-easy data. Unlikeprevious work whose difficulty level is roughlydivided by the number of reasoning steps (Haseet al., 2024), the difficulty order in our work isconstrained to pairwise questions and more strict.",
  "Conclusion": "We observe an anomalous phenomenon whereLLMs able to solve hard problems paradoxicallyfail at easier ones. To evaluate this hard-to-easy in-consistency, we construct ConsisEval by automaticgeneration and human annotation. Furthermore,we propose consistency score to measure this in-consistency quantitatively and relative consistencyscore to analyze the potential for improvement inconsistency. Based on our dataset and metrics, weconduct comprehensive experiments on numerousexisting models, finding that there are exceptionswhere some powerful models demonstrate poorconsistency, though models with stronger capabili-ties usually exhibit higher consistency. Case studyshows though state-of-the-art GPT-4 achieves thehighest CS of 92.2%, still suffers from inconsis-tency due to distraction by redundant information,misinterpretation of questions, etc. Besides, wealso find hard data benefits consistency for bothfine-tuning and ICL. Our benchmark and metricscan facilitate research in consistency of LLMs, ulti-mately paving the way for building more trustwor-thy and reliable AI in the future.",
  "Limitations": "Our evaluation requires repeated sampling for thesame question to estimate the probability, whichis more computationally expensive than traditionalnon-probability evaluation. Our metric CS can onlyreflect the overall consistency of a model and canhardly identify to which types of problems it ismore inconsistent. We also find different modelsbehave inconsistently to totally different questions,and identifying these questions for a given modelstill requires human efforts in case studies.Data contamination (or data leakage) (Magar andSchwartz, 2022; Xu et al., 2024b) can affect ourevaluation. As detailedly discussed in Appendix F,leakage of easy and hard data can lead to higherand lower CS, respectively. Considering that easydata are from public data and thereby suffer froma higher risk of data leakage (e.g. Achiam et al.(2023) reports 25% of HumanEval has been con-taminated in their training data), model consistencycan be overrated.Our evaluation does not include human results.Theoretically, consistency of humans should equateto 100%, yet incorrectness on easy questionscaused by carelessness can diminish this consis-tency. Human evaluation results can vary due tothe variance of carelessness among individuals; be-sides, having humans complete all questions inConsisEval is exceedingly time-consuming. There-fore, determining the human level consistency forLLMs as a reference needs more discussion andexploration.Our benchmark focuses on evaluating the hard-to-easy consistency of LLMs but does not inves-tigate the underlying reasons and how inconsis-tency comes into being. The knowledge acquire-ment process of humans and LLMs is totally dif-ferent, and humans are inherently consistent rea-soners yet LLMs are not. Will pre-training andfine-tuning paradigm of LLMs necessarily lead toinconsistency? Further discussion and explorationis needed. Though our preliminary findings suggestthat hard training data can mitigate this inconsis-tency, how to solve this inconsistency problem isstill unknown, and we leave it to future work.",
  "The easy part of our benchmark originates frompublicly available datasets, which is allowed forresearch usage. Our dataset encompasses code,maths, and instruction-following domains, which": "are safe and can hardly be utilized in harmful ways.Besides, the evaluated LLMs are all publicly avail-able by either parameters or API calls. Therefore,we do not anticipate any ethical concerns in ourresearch. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Baichuan. 2023. Baichuan 2: Open large-scale lan-guage models. arXiv preprint arXiv:2309.10305": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner,Bowen Baker, Leo Gao, Leopold Aschenbrenner,Yining Chen, Adrien Ecoffet, Manas Joglekar, JanLeike, et al. 2023. Weak-to-strong generalization:Eliciting strong capabilities with weak supervision.arXiv preprint arXiv:2312.09390. Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,Jiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai,Tongliang Li, Qiyao Peng, and Zhoujun Li. 2024.xcot:Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. arXiv preprintarXiv:2401.07037, abs/2401.07037. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021a. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021b. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168.",
  "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. 2022. A survey on in-context learning.arXiv preprint arXiv:2301.00234": "Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.2023. Palm-e: An embodied multimodal languagemodel. arXiv preprint arXiv:2303.03378. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:General language model pretraining with autoregres-sive blank infilling. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 320335,Dublin, Ireland. Association for Computational Lin-guistics. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-lasha Ravichander, Eduard Hovy, Hinrich Schtze,and Yoav Goldberg. 2021. Measuring and improvingconsistency in pretrained language models. Transac-tions of the Association for Computational Linguis-tics, 9:10121031. Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,Linzheng Chai, Jiaqi Bai, Junran Peng, XiaorongHu, Chao Chen, Dongfeng Zhang, Xu Shi, TieqiaoZheng, Liangfan Zheng, Bo Zhang, Ke Xu, and Zhou-jun Li. 2023. OWL: A large language model for IToperations. CoRR, abs/2309.09298.",
  "Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and EliaBruni. 2020. Compositionality decomposed: Howdo neural networks generalise? J. Artif. Intell. Res.,67:757795": "Myeongjun Jang, Deuk Sin Kwon, and ThomasLukasiewicz. 2022. BECEL: Benchmark for con-sistency evaluation of language models. In Proceed-ings of the 29th International Conference on Com-putational Linguistics, pages 36803696, Gyeongju,Republic of Korea. International Committee on Com-putational Linguistics. Myeongjun Jang and Thomas Lukasiewicz. 2023. Con-sistency analysis of ChatGPT. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1597015985, Singa-pore. Association for Computational Linguistics. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei,Jinfeng Yi, and Bowen Zhou. 2023a. Trustworthyai: From principles to practices. ACM ComputingSurveys, 55(9):146": "Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tat-sunori Hashimoto, and Percy Liang. 2023b. Bench-marking and improving generator-validator con-sistency of language models.arXiv preprintarXiv:2310.01846. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, HaodongDuan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang,Songyang Zhang, Dahua Lin, and Kai Chen. 2024.Mathbench: Evaluating the theory and applicationproficiency of llms with a hierarchical mathematicsbenchmark. arXiv preprint arXiv:2405.12209. Yang Liu, Yuanshun Yao, Jean-Francois Ton, XiaoyingZhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,Muhammad Faaiz Taufiq, and Hang Li. 2023. Trust-worthy llms: a survey and guideline for evaluatinglarge language models alignment. arXiv preprintarXiv:2308.05374. Inbal Magar and Roy Schwartz. 2022. Data contamina-tion: From memorization to exploitation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 157165, Dublin, Ireland. Associationfor Computational Linguistics.",
  "Harsh Raj, Vipul Gupta, Domenic Rosati, and Sub-habrata Majumdar. 2023. Semantic consistency forassuring reliability of large language models. arXivpreprint arXiv:2308.09138": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu,Qihui Zhang, Chujie Gao, Yixin Huang, WenhanLyu, Yixuan Zhang, Xiner Li, et al. 2024a. Trustllm:Trustworthiness in large language models. arXivpreprint arXiv:2401.05561. Zhiqing Sun, Longhui Yu, Yikang Shen, WeiyangLiu, Yiming Yang, Sean Welleck, and Chuang Gan.2024b. Easy-to-hard generalization: Scalable align-ment beyond human supervision.arXiv preprintarXiv:2403.09472. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie,Mintong Kang, Chenhui Zhang, Chejian Xu, ZidiXiong, Ritik Dutta, Rylan Schaeffer, Sang Truong,Simran Arora, Mantas Mazeika, Dan Hendrycks, Zi-nan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, andBo Li. 2023a. Decodingtrust: A comprehensive as-sessment of trustworthiness in gpt models. In Ad-vances in Neural Information Processing Systems,volume 36, pages 3123231339. Curran Associates,Inc.",
  "Ziyao Xu and Houfeng Wang. 2024. Spor: A compre-hensive and practical evaluation method for composi-tional generalization in data-to-text generation. arXivpreprint arXiv:2405.10650": "Zhe Yang, Damai Dai, Peiyi Wang, and Zhifang Sui.2023. Not all demonstration examples are equallybeneficial: Reweighting demonstration examples forin-context learning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1320913221, Singapore. Association for Computa-tional Linguistics. Alex Young, Bei Chen, Chao Li, Chengen Huang,Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:Open foundation models by 01. ai. arXiv preprintarXiv:2403.04652. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,and Le Hou. 2023.Instruction-following evalu-ation for large language models.arXiv preprintarXiv:2311.07911.",
  "AMathematical Notations": "This section shows all of the mathematical nota-tions used in this paper. If you forget the meaningof any notation, please refer to . We lever-age to symbolize estimates (e.g. P(ai) representsthe estimate of the true value P(ai) ). For sim-plicity, we only show true values in , andestimates are omitted.",
  "(7)": "It is worth noting that for a given question pair(ai, bi), the probability that a model correctly an-swers ai, bi (i.e. P(ai) and P(bi)) are unknownconstants. When sampling answers, whether themodel answers one question correctly does not af-fect answering the other, which allows us to deducethat the simultaneous probability of correctly an-swering both is P(ai, bi) = P(ai)P(bi). However,this does not hold for random questions a and b, asP(a, b) = P(a)P(b).The above derivation process does not specifyhow the random questions a and b are obtained.We provide a more rigorous proof by defining therandom process through which a and b are selected,as well as the random variables P(a) and P(b).Firstly, we outline the following stochastic process:",
  "Randomly sampling a pair of questions (a, b)from A B with equal probability": "Based on this stochastic process, we define therandom variables P(a) and P(b) as the probabil-ities of the model correctly answering a and b re-spectively, through a single temperature-based sam-pling. It is noteworthy that P(a), P(b) are constantin the previous derivation, but here we treat themas random variables. Initially, the prior probabil-ity of selecting bi in the above random process is",
  "CFormal Definition of Models withSimilar Capabilities": "For an evaluated model M0 and a questionpair (ai, bi) from dataset A B, the probabil-ity of M0 answer ai, bi correctly through a sin-gle temperature-based sampling is denoted asPM0(ai), PM0(bi). We define a model set ={M0, M1, ...} in which models have similar capa-bilities (but consistency is not necessarily similar).For any Mj , we have:",
  ". Mset{PM0(a0), ..., PM0(aN)}= Mset{PMj(a0), ..., PMj(aN)},": "where Mset denotes multiset (a.k.a. bag), ageneralization of a set where repetition of elementsmatters.In this scope, we define models with similar abil-ities as models whose correct probability on eachdatum in B are exactly the same and multisets ofcorrect probability on each datum in A are iden-tical to each other. The fact that different modelsfrom demonstrate the same accuracy on A (andB) intuitively makes one feel that these modelshave similar capabilities. It is worth noting thatonly M0 is an existing model in the real world; allother models in are hypothetical for analysis ofconsistency score boundaries.",
  "D.1Mathematical Boundaries": "Without any loss of generality, we assume thatP(b0), ..., P(bN) is an ascending sequence (oth-erwise, the order of elements can be adjustedproperly to meet this condition).After arrang-ing the sequence P(a0), ..., P(aN) in ascend-ing order, we denote the resulting sequence asP(a(0)), ..., P(a(N)). According to the rearrange-ment inequality, we have:i=1,...,N P(a(N+1i))P(bi)i=1,...,N P(bi)",
  "These two hypotheses specify the behavior ofthe model of best and worst consistency. We as-sume that for a model of worst consistency, there": "might be independence between correctly answer-ing easy and hard questions, rather than a negativecorrelation where an increased probability of cor-rectly answering hard questions leads to a lowerprobability of correctly answering easy questions.Conversely, for a model with best consistency, theprobability of correctly answering easy and hardquestions is entirely dependent on the difficultylevel of the questions. Thus, the difference in prob-ability between correctly answering easy and hardquestions, P(ai) P(bi), is solely reliant on thegradient of difficulty from ai to bi. When construct-ing our dataset, its almost impossible to ensure thateach ai scales up in difficulty uniformly to obtainbi; therefore, we hypothesize that the difficulty scal-ing from ai to bi follows a normal distribution (i.e.(P(a) P(b)) N(, )).Based on the Lower Bound Heuristic, we have atighter heuristic lower bound:",
  "(11)": "Substitute actual values , with estimated ones, , then we have the theoretical value of P(ai)in a consistent model: P(ai) = P(bi) + + i.Empirically, the value of does not affect finalresults if averaging on multiple sampling of , sowe directly let = 0. Then by substituting thetheoretical values of P(ai) in consistent model forthe true values of P(ai) used in calculation of CS ,we can obtain the heuristic upper bound as follows:",
  "E.1Multiple Sampling Estimation": "For problem ai, we sample answers m times in-dependently to obtain a sequence a1i , a2i , ..., ami .Let aji = 1 if the model generates a correct an-swer on the jth sampling; otherwise, aji = 0. Inthis case, aji follows a Bernoulli distribution. Letk = j=1,...,m aji, we have the likelihood func-tion:",
  "E.2Early Stopping Estimation": "In Early Stopping Estimation, the minimum andthe maximum number of sampling times kmin andkmax are set as hyper-parameters for a given ques-tion ai. Initially, kmin answers are sampled, andthe sampling process will be terminated if at leastone correct answer exists in these kmin answers;otherwise, answers will be sampled one by oneuntil a correct answer appears for the first time.Besides, the sampling procedure will be forciblyterminated if a correct answer still does not emergeafter sampling kmax answers. Let P(k, kc) be theprobability of sampling k answers in total in whichkc answers are correct, and let L(P(ai); k, kc) bethe likelihood function. The discussion is dividedinto the following three cases based on the differentvalues of k:",
  "E.3More Details about Early StoppingEstimation": "The pseudo-code for Early Stopping Estimation isshown in Algorithm 1. if we set kmax equal to thenumber of sampling m in Multiple Sampling Esti-mation, in the worst-case scenario, the number ofsampling of Early Stopping Estimation could equalthat of Multiple Sampling Estimation, theoretically.However, empirical results suggest that, due to thehigh accuracy of these closed-source models, theactual number of samples required with early stop-ping is typically low. While introducing an earlystopping strategy might slightly reduce the accu-racy of estimation, the reduction in the number ofAPI calls required makes it a worthwhile trade-off.",
  "Return P(ai)": "Multiple Sampling Estimation v.s. Early Stop-ping EstimationIf we sample fewer times inMultiple Sampling Estimation, resulting in aroughly equal total number of samples across theentire dataset for both methods, which methodyields a more accurate estimation? For questionswith a low probability of being answered correctly(near 0%), a large number of samples are requiredto obtain a correct answer and thus accurately es-timate this probability; otherwise, there is a highrisk of erroneously deeming the probability to bezero. On the contrary, for questions that modelshave a high probability of answering correctly (near100%), almost all samples will be correct, andtherefore, fewer samples are needed to accuratelyestimate the probability. The Early Stopping Es-timation method adapts the number of samplingtimes dynamically for different questions, mak-ing better use of each sampling opportunity com-pared to the Multiple Sampling Estimation. Con-sequently, it achieves higher precision in its finalestimates when the sampling times are limited.",
  "FImpact of Data Leakage": "Data leakage can affect our evaluation. We findleakage of easy and hard data can lead to higherand lower CS, respectively. We analyze data leak-ing on datum ai (or bi) by modeling the leakingas an increment in probability P(ai) (or P(bi)).For example, if ai is not leaked, model answersit correctly with probability P(ai); after ai isleaked, model answers it correctly with higherprobability P(ai) + P(ai). The original CS isi=1,...,N P(ai)P(bi)i=1,...,N P(bi), and we numerically analyzethe change of CS after data leakage.",
  "G.1Implement Experiment Details": "For small open-source models with roughly 7B or13B parameters, we employ the Multiple SamplingEstimation and independently sample 20 answersfor each question. As for the large models witharound 70B parameters and closed-source models,we utilize the Early Stopping Estimation to reducecomputational costs and API calls, and we set theminimum number of samples at kmin = 3 and themaximum at kmax = 20. For each small open-source model (7B or 13B), we run the experimentson a single Nvidia A100 80G GPU; for each largemodel (70B), experiments are conducted on threeNvidia A100 80G GPUs. All of the open-sourcemodels are acquired from Huggingface5, and weutilize the default sampling hyper-parameters (e.g.temperature, top-p) released by model developers.All evaluations are under zero-shot setting: formathematics and instruction-following data, ques-tions as fed into LLMs directly; code data are trans-formed into instruction format 6 before inputtedinto models.",
  "HMetric Convergence": "The calculation of our evaluation metric consis-tency score (CS) and relative consistency score(RCS) relies on repeated sampling for a given ques-tion. We show the value change and variance ofthese metrics as the increase in sampling times. Asthe convergence results for Llama3-8B-Instruct onmathematics domain shown in , CS con-verges faster than RCS and achieves a stable valueat about 5 samples. The value of RCS convergesrelatively slower and becomes stable after about 15samples.We also explore leveraging consistent rate asan evaluation metric. Taking the case where theprobability of answering an easy question cor-rectly is larger than that of the hard questionas a consistent case, we have consistent rate =number of consistent cases number of all cases100%. However, we findthat for the case where the probability of answeringeasy and hard questions correctly is close, reach-ing a convergent result requires too many times ofsampling. We abandon this metric due to its highcomputational cost.",
  "easy": "Question: George has 45% more pears than bananas. If George has 200 bananas, how many fruits doesGeorge have?Answer: To find out the number of pears George has, we must first calculate 45% of the bananas, thenadd that to the number of bananas he has. If George has 200 bananas, we will find 45% of this numberlike so:0.45 * 200 = 90. This means George has 90 more pears than bananas. Now, add the extra pears tothe 200 bananas to find the total number of pears: 200 + 90 = 290 pears. Now, to find the total number offruits, add the number of bananas to the number of pears: Number of bananas (200) + Number of pears(290) = Total fruits. 200 + 290 = 490 fruits. Therefore, George has 490 fruits in total.Answer (Number Only): 490",
  "hard": "Question: George has 45% more pears than bananas. If George has 200 bananas, 270 oranges, and 150apples, how much are Georges fruits worth if pears cost 3 dollars each, bananas 2 dollars each, oranges50 cents each, and apples 1 dollar each?Answer: First, lets calculate how many pears George has. Since he has 45% more pears than bananas,and he has 200 bananas, we calculate the number of pears as follows: Number of pears = Number ofbananas + 0.45 * Number of bananas = 200 + 0.45 * 200 = 200 + 90 = 290. Next, lets calculate the totalcost of each type of fruit. Bananas (200 bananas at $2 each): 200 * 2 = $400. Pears (290 pears at $3each): 290 * 3 = $870. Oranges (270 oranges at $0.50 each): 270 * 0.50 = $135. Apples (150 apples at$1 each): 150 * 1 = $150. Finally, to find the total value of all the fruits, we sum these amounts: $400 +$870 + $135 + $150 = $1555. Therefore, Georges fruits are worth $1555 in total.Answer (Number Only): 1555",
  "Code": "Question:def strlen(string: str) -> int:Return length of given string> > > strlen()0> > > strlen(abc)3Answer:return len(string)Check Function:def check(candidate):assert candidate() == 0assert candidate(x) == 1assert candidate(asdasnakj) == 9 Question:def strlen_add(string1: str, string2: str) -> str:Return length sum of two given strings> > > strlen_add(abc, d)4Answer:return len(string1 + string2)Check Function:def check(candidate):assert candidate(abc, d) == 4assert candidate(, z) == 1assert candidate(x, y) == 2assert candidate(hello, !) == 6",
  "First, lets calculate the total hours worked by eachpainter:1. A day is considered to have 24 hours, so 3/8ths of aday is: 3": "8 24 hours. 2.38 of 24 hours = 9 hours perday.Each painter works for 9 hours a day.Now, lets find out the total days worked in 3 weeks.Since there are 7 days in a week:3 weeks 7 days/week = 21 days.Each painter worked for 9 hours a day for 21 days, sotheir total hours worked are:9 hours/day 21 days = 189 hours in total for 3 weeks.Since there are 4 painters:189 hours 4 = 756 hours worked in total by allpainters.Given that each painter earns 80 yuan per hour, the totalwages for all painters are:756 hours 80 yuan/hour = 60,480 yuan.Finally, subtracting the total wages from the initiallyallocated funds gives the remaining amount:70,000 yuan - 60,480 yuan = 9,520 yuan.Therefore, 9,520 yuan will remain after paying the wagesof the 4 painters. : An inconsistency case for GPT-4. Red texts denote wrong reasoning steps. GPT-4 is distracted by theredundant condition of \"4 painters\", leading it to perform an additional unnecessary calculation after arriving at thecorrect answer, ultimately resulting in the wrong final answer.",
  "Questions": "Kenny is selling his Pokemon cards to buy a ticket to anamusement park, which costs $100. He has a collectionof cards and plans to sell them for $1.5 each. He keeps1/3 of them and gets to go to the amusement park with$50 in spending cash. How many cards did he start with?Standard Answer: 150 Kenny is selling his Pokemon cards to buy a ticket toan amusement park, which costs $100. He has a collec-tion of cards and plans to sell them for $1.5 each. Hekeeps 1/3 of them and, after buying the ticket, has $50in spending money for the amusement park. His initialnumber of cards is 5 times the number of cards Eily has.John has 37 more cards than Eily. Mary has 4 times asmany cards as John. How many cards does Mary have?Standard Answer: 268",
  "Model Answers": "Lets denote the original number of Pokemon cards thatKenny has as X.Kenny decides to sell his cards for $1.5 each. However,he keeps one-third of them, which means he sells two-thirds of his cards.The number of cards he sells is 2/3 * X.From the sale of his cards, he earns enough to buy theamusement park ticket, which costs $100, and he has$50 in spending cash. Therefore, the total amount heearns from selling his cards is $100 for the ticket plus$50 spending cash, which equals $150.The amount he earns from selling the cards can be foundby multiplying the number of cards he sells (2/3 * X) bythe price per card ($1.5):(2/3 * X) * $1.5 = $150Lets solve for X:$1.5 * (2X/3) = $150(3/2) * $1.5 * (2X/3) = $150 * (3/2)$2.25 * X = $225Divide both sides by $2.25 to find X:X = $225 / $2.25 X = 100Kenny started with 100 Pokemon cards. Lets break down the problem step by step.1. We know that Kenny sells his cards for $1.5 each andwants to end up with $150 in total ($100 for the ticketand $50 in spending money).2. Lets find out how many cards he sells. We take thetotal money he wants to have, which is $150, and divideit by the price per card: Total cards sold = total moneyneeded / price per card Total cards sold = $150 / $1.5Total cards sold = 100 cards3. Kenny keeps 1/3 of his cards and sells the rest, so 2/3of his cards were sold. Lets calculate the initial numberof cards he had: Let x be the number of cards Kennyinitially had. (2/3)x = 100 cardsTo find x, we solve for x: x = (100 cards) / (2/3) x = (100* 3) / 2 x = 300 / 2 x = 150 cardsSo Kenny originally had 150 cards.4. Kennys initial number of cards is 5 times the numberof cards Eily has. Lets call the number of cards Eily hasE. Therefore: 5E = 150Solving for E gives us: E = 150 / 5 E = 30 cardsEily has 30 cards.5. John has 37 more cards than Eily: Johns cards = E +37 Johns cards = 30 + 37 Johns cards = 67 cards6. Mary has 4 times as many cards as John: Marys cards= 4 * Johns cards Marys cards = 4 * 67 Marys cards =268 cardsSo Mary has 268 cards.",
  "Prompt for Code Data Generation": "#Instruction#:I want you to act as a helpful assistant. Please help me modify some programming problems and make them harder. Aprogramming problem datum consists of three parts: #Problem#, #Answer#, and #Check Function#. The #Problem#includes the name of a python function, function signature, and docstring; the #Answer# is the specific code that fulfillsthe functions purpose; in addition to that, there is a #Check Function# to verify whether the answer is correct. Pleasefollow the format of the following demonstrations, modify the original problem, and make it more challenging. Toensure that there is a strict order in difficulty between the original problem and modified one, steps to solve the originalproblem should be included in that of the modified problem. In other words, steps to solve the original problem is aproper subset of that of the modified problem. Except the modified #Problem#, you should also provide #Answer# and#Check Function# to the modified #Problem#.#Demonstrations#:<insert demonstrations>The above are some demonstrations showing how to modify the original problems. Please follow their format andmodify the following problem:#Problem#:<insert the original problem>#Answer#:<insert the answer>#Check Function#:<insert the check function>Please modified the above #Problem# and then provide #Answer# and #Check Function# to the modified #Problem#: : Our prompt fed to GPT-4 for code data generation. Our prompt is comprised of intention instruction,demonstrations, and one datum to be modified. The instruction offers a clear description of the composition of thedatum and outlines the task we expect the model to accomplish. Demonstrations are provided as a format referencefor the model, followed by the original datum for the model to modify.",
  "Prompt for Math Data Generation": "#Instruction#:I want you to act as a helpful assistant. Please help me modify some grade school math problems and make themharder. A math problem datum consists of two parts: #Problem# and #Answer#. The #Problem# provides a backgrounddescription of a real-world mathematical problem, along with the conditions known and the unknown content to besolved. There is a strict gurrantee that the unknown value can be derived through a few proper computational stepsbased on konwn conditions. The #Answer# encompasses several computational steps based on logical reasoning withthe known conditions, culminating in the numerical value of the final answer. Please follow the format of the followingdemonstrations, modify the original problem and make it more challenging. To ensure that there is a strict order indifficulty between the original problem and modified one, steps to solve the original should be included in that of themodified problem. In other words, steps to solve the original problem is a proper subset of that of the modified problem.Except for the modified #Problem#, you should also provide #Answer# to the modified #Problem#.#Demonstrations#:<insert demonstrations>The above are some demonstrations showing how to modify the original problems. Please follow their format andmodify the following problem:#Problem#:<insert the original problem>#Answer#:<insert the answer>Please modified the above #Problem# and then provide #Answer# to the modified #Problem#:",
  "Prompt for Instruction Following Data Generation": "#Instruction#:I want you to act as a helpful assistant. Please help me modify some instruction following problems and makethem harder. An instruction following problem datum consists of three parts: #Prompt#, #Constraint Type List#,and #Constraint Kwargs#. The #Prompt# consists of several constraints that guide the model to generate text. The#Constraint Type List# and #Constraint Kwargs# include the types and keyword arguments of the constraints containedwithin the #Prompt#, respectively. They are utilized to verify whether the text generated by the model meets theconstraints. We provide a #Candidate Constraint Set# containing a variety of constraints. Please select an appropriateconstraint from this set and follow the format of the demonstrations provided to add to the original #Prompt#. By doingso, you will create a more challenging new #Prompt#. Except for the modified #Prompt#, you should also provide#Constraint Type List#, and #Constraint Kwargs# to the modified #Prompt#.#Candidate Constraint Set#:<insert the candidate constraint set> #Demonstrations#:<insert demonstrations>The above are some demonstrations showing how to modify the original problems. Please follow their format andmodify the following problem:#Prompt#:<insert the original prompt>#Constraint Type List#:<insert the constraint type list>#Constraint Kwargs#:<insert the constraint keyword arguments>Please modified the above #Prompt# and then provide #Constraint Type List# and #Constraint Kwargs# to the modified#Prompt#:"
}