{
  "Abstract": "Recent advances in large language models(LLMs) show the potential of using LLMs asevaluators for assessing the quality of text gen-erations from LLMs. However, applying LLMevaluators naively to compare different systemscan lead to unreliable results due to the inac-curacy and intrinsic bias of LLM evaluators.In order to mitigate this problem, we proposetwo calibration methods, Bayesian Win-RateSampling (BWRS) and Bayesian Dawid-Skene,both of which leverage Bayesian inference tomore accurately infer the true win rate of gen-erative language models. We empirically vali-date our methods on six datasets covering storygeneration, summarization, and instruction fol-lowing tasks. We show that both our methodsare effective in improving the accuracy of winrate estimation using LLMs as evaluators, offer-ing a promising direction for reliable automatictext quality evaluation.",
  "Introduction": "Evaluating the quality of AI-generated text hasbeen a longstanding and evolving challenge in NLP.In recent years, this challenge has become increas-ingly crucial due to the growing interest in the fieldof generative AI. While human judgment is stillconsidered the most reliable form of assessment,common automatic approaches to evaluating qual-ity of AI-generated text include heuristic-basedevaluation metrics (Papineni et al., 2002; Lin, 2004;Pillutla et al., 2021), model-based evaluation met-rics (Zhang et al., 2019; Fabbri et al., 2022; Zhaet al., 2023; Chen and Eger, 2023), and recently,LLM-based evaluations (Kim et al., 2024a,b; Wanget al., 2024). Due to their low cost and high corre-lation with human preferences, LLM-based eval-uations are receiving an increasing amount of at-tention. Most previous studies that apply LLMevaluators (Chiang and Lee, 2023a,b; Dubois et al.,",
  "*Equal contribution": "2024; Kim et al., 2024a,b; Wang et al., 2024) at-tempt to improve the agreement between LLMevaluators and human preference by training ex-pert models for evaluation or improving promptingstrategies. However, such methods often either re-quire compute-expensive finetuning, or suffer fromcommon problems of LLM evaluators such as posi-tion bias (Wang et al., 2023b), self-preference, andmore (Koo et al., 2023). Besides, as we will dis-cuss in .2, directly applying a non-perfectLLM evaluator will result in a bias problem in theestimation of win rate.In this paper, we attempt to address these chal-lenges by proposing two methods, BWRS andBayesian Dawid-Skene. Our methods leverageBayesian inference to infer the true win rate ofone text generator against another using evaluationresults of LLM evaluators and incorporating op-tional prior knowledge about human preferences.By employing these methodologies, we observe acloser alignment between LLM-generated evalua-tions and human judgment. 1",
  "Related work": "LLM as evaluatorsA line of research in LLM-based evaluation evaluated the performance ofLLM evaluators and proposed methods to improvethem. Some works applied various prompting tech-niques to improve the accuracy of LLM evaluation,including chain of thought (Liu et al., 2023a), eval-uation with explanation (Chiang and Lee, 2023b),multi-LLM discussion (Chan et al., 2023; Li et al.,2023), and calibration with human expert (Liu et al.,2023b). Some other works (Wang et al., 2024; Kimet al., 2024a,b) trained expert models in evaluation.As for evaluating the general capability of LLMevaluators, most previous studies (Liu et al., 2023a;Chiang and Lee, 2023a,b; Dubois et al., 2024) usedcorrelation coefficients such as Pearsons correla-tion or Kendalls tau to measure the preference ofdifferent LLM evaluators compared with humanevaluators.On the application side, LLM evaluators are of-ten applied to build LLM rankings. (Dubois et al.,2024) proposed a simple LLM evaluation frame-work by looking at the win rate decided by GPT-4evaluators on a large number of texts generatedby the two generators under the same generationprompts. Auto-Arena(Zhao et al., 2024) used LLMjudge agents to determine the winner of each LLMpair. However, as well discuss in .2, thesemethods can lead to biased win rate estimations,especially when the LLM evaluators do not alignwell enough with human preferences. Annotation modelsIn the field of crowdsourcedannotations, a line of research focuses on simultane-ously modeling the accuracy of individual annota-tors and determining the true labels of tasks. Theseworks mostly target aggregating crowdsourced dataand improving data quality in case of non-expert oradversarial annotators. Dawid-Skene (Dawid andSkene, 1979) is the first model proposed to considerindividual annotator error rates by using maximumlikelihood estimation to infer true labels from anno-tators with different accuracies. Since then, manyother models (Albert and Dodd, 2004; Carpenter,2008; Whitehill et al., 2009; Kim and Ghahramani,2012; Hovy et al., 2013; Passonneau and Carpenter,2014; Zhang et al., 2016) were developed to im-prove performance and efficiency. These methodswere originally proposed to model the accuracy ofhuman annotators, in our paper we instead applythem to model LLM evaluators.",
  "Problem formalization": "3.1.1True win rate and observed win rateConsider two LLMs as text generators (LLM gen-erators) G0 and G1. Let be the set of all possibleinputs to the text generators, and let be the setof all possible outputs given the inputs from .We can then define the LLMs as two functionsG0 : and G1 : . Additionally, letP be a probability distribution on that denotesthe possibility of each input to appear, let Pbe a random input.Let H : {0, 1} be the average humanevaluator function, which assesses the relativequality of two outputs. H(y0, y1) = 0 indicatesthat the output y0 is preferred over y1 by an aver-age human expert, and H(y0, y1) = 1 indicates theopposite. Let Te : {0, 1} be the LLMevaluator function, which represents the prefer-ence of a certain LLM evaluator e. Let P be aprobability measure that encapsulates the stochas-tic nature of , G1, G2, H, and Te.Given the notations above, we define the follow-ing variables:",
  "ke P (Te(G0(), G1()) = 0)(2)": "Intuitively, the true win rate p is the probabilitythat G0 will generate a truly better output thanG1 when they are given the same, arbitrary input,where truly better means being regarded as bet-ter by a human expert on average. Similarly, theobserved win rate k is the probability that G0 willbe evaluated by an LLM evaluator as generatinga better output than G1 when they are given thesame, arbitrary input.Due to the complexity of the stochasticity in pand ke, it is unrealistic to derive them analytically.However, given a large number of input-output pairs evaluated by human and LLM evaluators, wecan approximate p and ke empirically. We formal-ize it as follows.Assume n is a large number. Then for n out-puts y(0)i(i [n]) generated by G0 and n outputsy(1)i(i [n]) generated by G1 given the same ninputs of interest, we let a human evaluator h andthe LLM evaluator e of interest carry out n com-parison tasks, where the i-th comparison task isbetween y(0)iand y(1)i. Then the true win rate pand the observed win rate ke can be empiricallyapproximated with",
  "Te(y(0)i, y(1)i)(4)": "where Hh : {0, 1} is the human evaluatorfunction of a specific human evaluator h (or anaggregation of multiple human evaluators). Notethat in our experiments, in order to make sure thatp is an accurate estimator of p, we assume thatthe preference of h is representative of an averagehuman expert evaluator.",
  "Evaluator accuracy": "We also define two variables qe0 (true positive eval-uation accuracy) and qe1 (true negative evalua-tion accuracy) associated with an LLM evaluatore2. Given two arbitrary outputs generated underthe same arbitrary input where the first output isevaluated as better than the second one by an av-erage human expert, qe0 is defined as the conditionalprobability that e will give the same evaluation asan average human expert. In other words, we have",
  "ni=1 1(Hh(y(0)i, y(1)i) = 1)(8)": "3.1.3Win rate estimationAs we discussed in , the true win rate pcan be used as a metric to compare various genera-tive LLMs. Specifically, for two generative LLMsG0 and G1, G0 outperforms G1 when p > 0.5.Conversely, G1 outperforms G0 when p < 0.5.Furthermore, the absolute value of p signifies thedegree of superiority of one LLM to another. Givena list of LLMs = [Ga, Gb, ...] of interest and acertain baseline generative LLM G, we can use thep values of G with respect to each generator in to compare the LLMs in (1 vs. n comparison).Therefore, it is a meaningful question to derive anaccurate estimation of p. This is the essential goalof this paper.",
  "Bayesian Win Rate Sampling": "First, we propose a sampling-based algorithm,Bayesian Win Rate Sampling (BWRS), which isshown in Algorithm 1. The intuition of the BWRSalgorithm is that, given an LLM evaluator e anda dataset D = {(y(0)i, y(1)i), i [n]} containingoutputs generated by G0 and G1 with respect tothe same set of inputs, we first apply e to generateits annotations {Te(y(0)i, y(1)i), i [n]} on D, andapply Equation 4 to approximate ke. Next, assumewe have access to some human annotations, eitheron a small fraction of D or on a similar dataset F,then we are able to approximate qe0 and qe1 usingEquation 7 and 8. Finally, we apply the followingequation rearranged from Equation 9:",
  "qe0 + qe1 1(11)": "given the assumption that qe0 + qe1 = 1.3 Wecan use the approximated values of ke, qe0, and qe1to infer one sample of p, which characterizes therelative performance between G0 and G1.Note that there is still one key difference betweenthe intuition above and our actual implementationdescribed in Algorithm 1. In our implementation,instead of estimating ke, qe0, qe1 directly using Equa-tions 4, 7, 8, we use Bayesian inference and applyBeta-Bernoulli models to estimate the posteriordistributions for ke, qe0, and qe1. We then obtainN (10000 in our case) samples of p from these",
  "In practice, though this assumption is satisfied under mostcases, some values of evaluator accuracies might cause sam-pling failure. Please refer to Limitations for details": "distributions using Equation 11 and apply KernelDensity Estimation (KDE) on all the p samples toapproximate the distribution of p, and estimate thevalue of p using the mean pmean or mode pmodeof this distribution. The purpose of applying aBayesian setting is to incorporate the uncertaintyof ke, qe0, qe1 into consideration, and also facilitatethe usage of prior knowledge on evaluator accura-cies, which will be discussed in .3.",
  "Bayesian Dawid-Skene model": "The vanilla Dawid-Skene model (Dawid andSkene, 1979) is optimized with the Expectation-Maximization (EM) algorithm. Following (Paunet al., 2018), we instead use a Bayesian Dawid-Skene model with E evaluators. The pseudocodeof our model is shown in Model 1. The parametersin this model include p, p, q0, q0, q1, andq1.We initialize the distribution of p with a uniformdistribution, and thus p, p are initialized as 1.The initialization of the other parameters will bediscussed in .3. We apply the evaluationresults of LLM evaluator e as observations tei, anduse Hamiltonian Monte Carlo (HMC) sampling tofit the model and sample from the posterior distri-bution of p. Similar to BWRS, we use the posteriormean (pmean) and posterior mode (pmode) as twoestimators of p. In order to improve sampling effi-ciency, we employ NUTS sampler (Hoffman andGelman, 2011) and the Binary Gibbs-Metropolissampler implemented in PyMC (Oriol et al., 2023).We tune and sample from the model with 4 chains,with 10000 tuning steps and 10000 sampling steps",
  "Algorithm 1 Bayesian Win Rate Sampling (BWRS) algorithm": "1: Input: Dataset without human annotation: D = {(y(0)i, y(1)i), i [n]}; similar dataset with human annotation (e.g.the OOD set): F = {(z(0)i, z(1)i), i [m]}; annotation by LLM evaluator e on D: De = {Te(y(0)i, y(1)i), i [n]};annotation by LLM evaluator e on F: Fe = {Te(z(0)i, z(1)i), i [m]}; annotation by human evaluator h on F: Fh ={Hh(z(0)i, z(1)i), i [m]}; Number of samples drawn for Bayesian inference: N",
  "Datasets": "The datasets we use in the experiments are HANNA(Chhun et al., 2022), OpenMEVA-MANS (Guanet al., 2021), SummEval (Fabbri et al., 2021),LLMBar (Zeng et al., 2024), MT-Bench (Zhenget al., 2023), and LLMEval2 (Zhang et al., 2023),covering tasks of story generation (HANNA, OpenMEVA-MANS), summarization (SummEval),and instruction following (the other three). All ofthem provide machine-generated content with hu-man annotations. For MT-Bench and LLMEval2,we used the smaller, curated versions preparedby the authors of the LLMBar paper (Zeng et al.,2024). For the three instruction following datasets,since they are presented as a list of (input, output1,output2, human preference) tuples without speci-fying or fixing the output generators, we simulatetwo generators based on these datasets by randomlyattributing 80% of the human-preferred outputs tothe first (simulative) generator and rest 20% to thesecond such that the true win rate between them is80%. The choice of the 80%-20% ratio is arbitrary.A detailed description about each dataset can befound in Appendix A.",
  "Evaluator settings": "For HANNA, OpenMEVA-MANS, and SummEval,we prompt a set of LLM evaluators to comparethe outputs of generator models in the datasets.Specifically, we employ GPT-3.5-turbo-0125 (Ope-nAI, 2023) and Gemini-1.0-Pro (Team, 2024) asthe evaluator models for our experiments. GPT-3.5has been proved to have positive correlation withhuman annotations (Chiang and Lee, 2023a; Wanget al., 2023a), while Gemini-1.0-Pros performanceon meta-evaluation have not yet been widely stud-ied in previous works. For each output pair, we prompted each LLM evaluator to rate the two out-puts that are based on the same input and generatedby two different generator models. For each LLMevaluator, we used three prompting strategies in-cluding Score-only, Rate-explain, and Analyze-ratefollowing (Chiang and Lee, 2023b). For LLMBar,MT-Bench, LLMEval2, the LLM evaluation workhas already been carried out in (Zeng et al., 2024).For these three datasets, we selected the best LLMevaluators among the many ones used, includingevaluators based on GPT-4, PaLM 2, etc. for ourexperiments. More details regarding the specificLLM evaluator modes used for each dataset can befound in Appendix B.",
  "Win rate estimation": "After obtaining the human evaluation and LLMevaluation data, we apply BWRS (.3) andBayesian Dawid-Skene model (.4) to eachdataset described above. Additionally, we calcu-late the observed win rate (k) using Equation 4averaged over the results of all LLM evaluatorscombined. The error of estimating p with the ob-served win rate, i.e. |k p|, acts as a baselinethat shows the aggregated performance of the LLMevaluators applied without any calibration.In order to further study the effectiveness of eachestimation method, we also explore their perfor-mance given the following three different sourcesof human evaluation results. For simplicity, werefer to these human evaluation results as priors,since they act as prior knowledge of human prefer-ences in our methods.No prior4. We assume no prior knowledge ofq, and only depend on the Dawid-Skene model toestimate the accuracy of each evaluator. In this case,we initialize the parameters of evaluator accuraciesin Model 1 with q0 = q1 = 2, q0 = q1 = 1,which is a beta distribution skewed towards higherq0 and q1 values, because we expect our evaluatorsto generally perform better than random guessingsuch that q0 > 0.5 and q1 > 0.5.In-distribution prior. We assume that we haveaccess to human evaluations on a subset of all out-put pairs generated by the two generators of interest.In BWRS, these human evaluation results are usedas Fh in Algorithm 1 to obtain an estimate of eachLLM evaluators accuracies q0, q1. In the BayesianDawid-Skene model, they are instead used as ob-",
  "The no prior setting is not applicable for BWRS, sinceBWRS requires informative priors of evaluator accuracies tobe accurate": "servations (hi in Model 1), while q0, q0, q1, andq1 are initialized in the same way as in the no priorsetting. We refer to the ratio of human-evaluatedoutput pairs over the entire dataset as prior data ra-tio. In our experiments, we try 10 different valuesof prior data ratio (0.1, 0.2, ..., 1.0) and comparethe results. Out-of-distribution (OOD) prior. We assumethat we have access to human evaluations on someother datasets beyond comparing the two genera-tors of interest. These human evaluation resultsare also used to calculate priors for q0 and q1. Inour experiments, we use the generator pair in thedataset that has the closest observed win rate withthe compared generators. For BWRS, these pri-ors are used as Fe and Fh in Algorithm 1. Forthe Bayesian Dawid-Skene model, with the in-distribution prior setting, the priors are used asobservations of ground truth labels hi in Model 1.For the OOD prior setting, they are instead used toderive a prior distribution of the evaluator accura-cies so that the model wont be affected as muchby the distribution shift of evaluator accuracies ondifferent generator models. Specifically, we use aBeta-Bernoulli model similar to the ones we usedin BWRS. The only difference is that we normal-ize the Beta parameters to have a mean value of1 in order to prevent over-confident priors. Con-cretely, we initialize the distributions of qe0 and qe1in Model 1 for each evaluator e as follows:",
  "Results": "In this section, we first analyze the evaluator accu-racies on our datasets, and then list the results ofour experiments, including win rate estimation withno prior, OOD prior, and in-distribution prior. Weshow that both our methods are able to effectivelycalibrate the estimation of win rate given good es-timations of evaluator accuracies. We also showthat even with no or OOD knowledge of humanpreference, our methods are still able to performwell overall.",
  "Evaluator accuracies": "For the three non-instruction following datasets(HANNA, OpenMEVA-MANS, SummEval) onwhich we carry out LLM evaluation by ourselves,the average accuracies of LLM evaluators areshown in .The overall accuracy is de-fined as the proportion of all pair-wise comparisonswhere the LLM evaluation aligns with human eval-uation. For all pairwise comparisons, we actuallymean the 1 vs. n comparisons where the GPT-2 text generator is compared to all the other textgenerators in the dataset. We employ this 1 vs.n comparison strategy because the correspondingn vs. n strategy is much more costly in terms of",
  "In terms of overall accuracy, there is not asignificant difference (>5%) between the threeprompt templates": "There is a significant difference between q0and q1 even though we applied the swap-and-sum strategy (see Appendix A). This can beattributed to the correlation between evalua-tor accuracy and the difference between thegenerators capabilities. When one generatoris significantly better than the other, it is eas-ier for the LLM evaluator to identify caseswhere the better generator does better, andharder when the better generator does worse.Also, Gemini-1.0-Pro evaluators suffer fromthis problem more significantly than GPT-3.5evaluators. This shows the necessity of mod-eling q0 and q1 separately for each evaluatorwhen comparing two generators. For the instruction following datasets (LLMBar,LLMEval2, MT-Bench), the overall evaluator accu-racies are given in the LLMBar paper (Zeng et al.,2024), where the overall evaluation accuracies aregenerally above 70% for the evaluator modes weuse.",
  "(b) BWRS": ": Win rate estimation error with various proportions of the original data used for in-distribution priormeasurement. The results are averaged over all generator pairs over all evaluator modes. The mean and variance ofall results are calculated over ten repetitive runs. The variance of k values in the three instruction following datasetsare results of randomly assigning outputs to two simulative generators, as described in .1",
  "The mode estimator in Bayesian Dawid-Skenewith OOD prior is the overall best estimator.In this setting, estimation of p is more accu-rate than baseline (k) in all datasets exceptHANNA": "The Bayesian Dawid-Skene model with OODprior is more accurate than the model withno prior. This shows that the OOD prior isable to provide some useful information onthe accuracy of each evaluator, which helpsthe Bayesian model converge to a better result. The results of win rate estimation with no prioron LLMBar, LLMEval2, and MT-Bench are shownin . Note that OOD prior is not applicablefor these instruction following datasets due to theabsence of relevant data to act as the OOD set. We can see that the mode estimator in Bayesian Dawid-Skene with no prior outperforms the baseline in alldatasets except MT-Bench.The results of BWRS and Bayesian Dawid-Skene with in-distribution prior are shown in . We can observe the following: As prior data ratio increases, win rate estima-tion accuracy of both BWRS and BayesianDawid-Skene improves. This enhancementarises because having more human annota-tions for in-distribution data allows for a moreprecise assessment of evaluator accuracies andconsequently leads to a more accurate estima-tion of the true win rate p. This shows thatour methods will indeed offer a more accurateestimation of the true win rate p if we havegood estimations of q0 and q1.",
  "Conclusion": "In this paper, we identified the bias problem inwin rate estimation using non-perfect LLM eval-uators, and proposed two methods, BWRS andBayesian Dawid-Skene, in order to address thisissue. We then obtained LLM evaluation resultson six datasets, and used these results to exam-ine the effectiveness of our methods empirically.Our results show that both BWRS and BayesianDawid-Skene can effectively reduce the error inwin rate estimation, especially given good approxi-mations on evaluator accuracies. We also showedthat even without in-distribution prior knowledgeof human preferences, our methods are still ableto effectively calibrate the estimation of win rateunder most cases. The effectiveness of our methodsmanifests the possibility to calibrate the estimationof win rate in a post-hoc manner after LLM eval-uations are completed, and also enlightens futurestudy on applying annotation models for accuratewin rate estimation using LLM evaluators.",
  "Limitations": "There are some limitations of our work. First, dueto budget limit, for the non-instruction followingdatasets, we only examined our methods with GPT-3.5 and Gemini-1.0-Pro as LLM evaluators. Al-though we did incorporate more advanced LLMevaluators such as GPT-4 and PaLM 2 on the in-struction following datasets, it would be illumi-nating to examine how more advanced evaluatormodels would affect our methods performance onthe non-instruction following datasets.Second, the performance of both methods withOOD prior largely depends on the quality of OODdata. Specifically, when there is a large differencebetween evaluator accuracies on the OOD set andon the original dataset, our methods may producehighly-biased results. Therefore, in cases wherehuman evaluation results on datasets with similarobserved win-rates are absent, we would recom-mend against using OOD prior.This paper is an exploratory study on adjustingbias of LLM evaluators. Besides resolving the",
  "< p < 1 1 qe1 < ke < qe0,qe0 + qe1 > 1qe0 < ke < 1 qe1,qe0 + qe1 < 1(14)": "We can see that, in order to make sure p , the evaluator accuracies qe0 and qe1 mustsatisfy one of the conditions in Equation 14.In cases where neither condition is satisfied,our methods can become unstable, and isprone to produce p distributions with highbias and/or variance. We leave it for futureresearch to propose methods that work wellfor LLM evaluators with low or unstable ac-curacies.",
  "Yanran Chen and Steffen Eger. 2023. Menli: Robustevaluation metrics from natural language inference": "Cyril Chhun, Pierre Colombo, Fabian M. Suchanek,and Chlo Clavel. 2022. Of human criteria and au-tomatic metrics: A benchmark of the evaluation ofstory generation. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 57945836, Gyeongju, Republic of Korea. In-ternational Committee on Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2023a. Can largelanguage models be an alternative to human evalua-tions? In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1560715631, Toronto,Canada. Association for Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2023b. A closerlook into using large language models for automaticevaluation. In Findings of the Association for Com-putational Linguistics: EMNLP 2023, pages 89288942, Singapore. Association for Computational Lin-guistics. Alexander Philip Dawid and Allan M Skene. 1979.Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the RoyalStatistical Society: Series C (Applied Statistics),28(1):2028. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyLiang, and Tatsunori B. Hashimoto. 2024. Alpaca-farm: A simulation framework for methods that learnfrom human feedback. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, andCaiming Xiong. 2022. QAFactEval: Improved QA-based factual consistency evaluation for summariza-tion. In Proceedings of the 2022 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 25872601, Seattle, United States. Asso-ciation for Computational Linguistics. Alexander R. Fabbri, Wojciech Kryscinski, Bryan Mc-Cann, Caiming Xiong, Richard Socher, and DragomirRadev. 2021. SummEval: Re-evaluating Summariza-tion Evaluation. Transactions of the Association forComputational Linguistics, 9:391409.",
  "Angela Fan, Mike Lewis, and Yann Dauphin. 2018": "Hierarchical neural story generation. In Proceedingsof the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 889898, Melbourne, Australia. Associationfor Computational Linguistics. Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-biao Ding, Xiaoxi Mao, Changjie Fan, and MinlieHuang. 2021. OpenMEVA: A benchmark for evaluat-ing open-ended story generation metrics. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 63946407, Online.Association for Computational Linguistics. Karl Moritz Hermann, Tom Kocisk, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. 2015. Teaching machines to readand comprehend. In Proceedings of the 28th Interna-tional Conference on Neural Information ProcessingSystems - Volume 1, NIPS15, page 16931701, Cam-bridge, MA, USA. MIT Press.",
  "Hyun-Chul Kim and Zoubin Ghahramani. 2012": "Bayesian classifier combination. In Proceedings ofthe Fifteenth International Conference on ArtificialIntelligence and Statistics, volume 22 of Proceedingsof Machine Learning Research, pages 619627, LaPalma, Canary Islands. PMLR. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,Shayne Longpre,Hwaran Lee,Sangdoo Yun,Seongjin Shin, Sungdong Kim, James Thorne, andMinjoon Seo. 2024a. Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim,Juyoung Suk,Shayne Longpre,Bill Yuchen Lin, Jamin Shin, Sean Welleck, GrahamNeubig, Moontae Lee, Kyungjae Lee, and MinjoonSeo. 2024b. Prometheus 2: An open source languagemodel specialized in evaluating other language mod-els.",
  "Rebecca J. Passonneau and Bob Carpenter. 2014. Thebenefits of a model of annotation. Transactions ofthe Association for Computational Linguistics, 2:311326": "Silviu Paun, Bob Carpenter, Jon Chamberlain, DirkHovy, Udo Kruschwitz, and Massimo Poesio. 2018.Comparing Bayesian models of annotation. Transac-tions of the Association for Computational Linguis-tics, 6:571585. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. Mauve: Measuring the gap be-tween neural text and human text using divergencefrontiers. In Advances in Neural Information Pro-cessing Systems, volume 34, pages 48164828. Cur-ran Associates, Inc.",
  "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, andZhifang Sui. 2023b. Large language models are notfair evaluators": "Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, andYue Zhang. 2024. Pandalm: An automatic evaluationbenchmark for llm instruction tuning optimization. Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, JavierMovellan, and Paul Ruvolo. 2009.Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise. In Advances inNeural Information Processing Systems, volume 22.Curran Associates, Inc.",
  "ADataset details": "HANNA (Chhun et al., 2022) includes 1056 sto-ries annotated by human raters with a 5-point Likertscale on 6 criteria: Relevance, Coherence, Empa-thy, Surprise, Engagement, and Complexity. These1056 stories are based on 96 story prompts fromthe WritingPrompts (Fan et al., 2018) dataset. Foreach story prompt, HANNA collects 11 stories gen-erated by 10 different generation models and a hu-man, respectively. For our purpose of comparingautomatic text generation systems, we did not usethe stories written by humans in our experiments.OpenMEVA-MANS (Guan et al., 2021) is asub-dataset within the OpenMEVA dataset. It con-tains 1000 stories generated by 5 generation modelsbased on 200 prompts from WritingPrompts (Fanet al., 2018). The overall quality of each story israted by five humans on a 5-point Likert scale.SummEval (Fabbri et al., 2021) includes 1600summaries annotated by human expert annotatorswith a 5-point Likert scale on 4 criteria: coher-ence, consistency, fluency, and relevance. These1600 summaries are based on 100 source articlesfrom the CNN/DailyMail dataset (Hermann et al.,2015). For each source article, SummEval collects16 summaries generated respectively by 16 differ-ent automatic summary generation systems. Each summary is scored by three human expert annota-tors.LLMBar (Zeng et al., 2024) consists of 419instances, each containing an instruction pairedwith two outputs: one that faithfully follows theinstruction and another that deviates from it butmay possess superficially appealing qualities. Thedataset is divided into two main parts: the Nat-ural set, which includes instances from existinghuman-preference datasets that have been filteredand modified to ensure objective preferences, andthe Adversarial set, which contains outputs craftedto mislead evaluators by emphasizing superficialqualities. LLMBar aims to provide a more rigorousand objective evaluation of LLM evaluators com-pared to previous benchmarks, achieving a highhuman agreement rate of 94% (Zeng et al., 2024).MT-Bench (Zheng et al., 2023) comprises 80questions and answers to these questions gener-ated by six models. For each question and eachpair of models, an evaluation task was constructed,totaling 1200 tasks. The actual dataset that weused is a subset of the original MT-Bench datasetcurated by the authors of (Zeng et al., 2024), to con-struct which they labelled a human-preferred an-swer for each task using majority vote, removed allthe tie instances, and then randomly sampled 200instances. We found five instances of this curatedsubset repeated themselves once, so we further re-moved these repeated ones and used the remaining195 instances for our experiments.LLMEval2 (Zhang et al., 2023), similar to MT-Bench, is a question answering dataset where eachinstance comprises a question and two answers tothat question. It consists of 2553 instances, each an-notated with human preferences. The actual datasetthat we used is a subset of the original LLMEval2 dataset (Zhang et al., 2023) curated by the authorsof (Zeng et al., 2024), to construct which they re-moved all the tie instances and then randomlysampled 200 instances.For each dataset with multiple human evalua-tions on each piece of generated text, we averagethe human evaluation scores as the final humanevaluation score for each piece of text.",
  "We prepared prompt templates into which the inputand the two outputs would be inserted. Specifically,we used the following three prompting strategiesfollowing (Chiang and Lee, 2023b)": "The Score-only prompting strategy asks theLLM evaluator to only output the attribute scoresof the generated texts without any further explana-tions.The Rate-explain prompting strategy asks theLLM evaluator to rate the generated texts first andthen provide an explanation for its ratings.The Analyze-rate prompting strategy asks theLLM evaluator to first analyze the generated textsand then give the ratings for them.Additionally, it has been reported that LLMevaluators suffer from position bias (Wang et al.,2023b), meaning that their decisions are oftenfalsely correlated with the order of presenting thecompared texts. In order to address this problem,we employ a straightforward swap-and-sum strat-egy inspired by the LLMBar paper (Zeng et al.,2024). For each pair of outputs to be compared,we query the LLM evaluator twice with the origi-nal and swapped ordering of the outputs. We thensum the scores given by the LLM evaluator in thetwo queries and choose the generated text with thehigher total score as the LLM-evaluated winner. Incases where the total score is even for both outputs,we consider their quality to be equal, and randomlyselect one as the winner.The details of the LLM evaluator modes used byour experiments can be found in Tables 4 and 5. Forthe prompting templates used for the three instruc-tion following datasets shown in , pleaserefer to the LLMBar paper (Zeng et al., 2024) fordetailed explanations."
}