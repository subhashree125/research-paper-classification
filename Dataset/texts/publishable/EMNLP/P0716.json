{
  "Abstract": "Document-level relation extraction (DocRE)aims to identify relationships between entitieswithin a document. Due to the vast numberof entity pairs, fully annotating all fact tripletsis challenging, resulting in datasets with nu-merous false negative samples. Recently, self-training-based methods have been introducedto address this issue. However, these meth-ods are purely black-box and sub-symbolic,making them difficult to interpret and proneto overlooking symbolic interdependencies be-tween relations. To remedy this deficiency, ourinsight is that symbolic knowledge, such aslogical rules, can be used as diagnostic toolsto identify conflicts between pseudo-labels.By resolving these conflicts through logicaldiagnoses, we can correct erroneous pseudo-labels, thus enhancing the training of neuralmodels.To achieve this, we propose Log-icST, a neural-logic self-training frameworkthat iteratively resolves conflicts and constructsthe minimal diagnostic set for updating mod-els. Extensive experiments demonstrate thatLogicST significantly improves performanceand outperforms previous state-of-the-art meth-ods. For instance, LogicST achieves an in-crease of 7.94% in F1 score compared to CAST(Tan et al., 2023a) on the DocRED benchmark(Yao et al., 2019). Additionally, LogicST ismore time-efficient than its self-training coun-terparts, requiring only 10% of the trainingtime of CAST. Code is available at",
  "* The first three authors contributed equally. Corresponding author": "Unlike sentence-level relation extraction, whichfocuses on individual entity pairs (Stoica et al.,2021), DocRE is challenged by the vast numberof potential entity pairs. This number increasesquadratically with the number of entities, makingit nearly impossible for annotators to meticulouslyverify the validity of each triplet. Although semi-automatic strategies, such as the recommend-reviseannotation method (Yao et al., 2019), can alleviateannotators workload, they still fail to provide gold-quality datasets. Consequently, these datasets areprone to contain numerous false negative samples.For example, over 60% triplets are not annotated inDocRED (Huang et al., 2022). Therefore, trainingmodels from incompletely annotated datasets iscrucial and practical for DocRE.There has been extensive research aimed at al-leviating the impact of false negative samples (Liet al., 2021; Wang et al., 2022a, 2024). One ofthe most advanced strategies is self-training (Leeet al., 2013), wherein the model reassigns labels toannotated negative triplets based on its predictions.These adjusted labels are then used iteratively torefine the models training process. However, self-training is highly vulnerable to confirmation bias(Arazo et al., 2020). Specifically, inaccurately pre-dicted pseudo-labels may impair the models sub-sequent training. Previous works have attempted tomitigate this issue by sampling the pseudo-labelsbased on class frequencies (Wei et al., 2021) orscores calculated on the development set (Tan et al.,2023a). However, these pseudo-labeling methodsare still far from satisfactory. First, they are purelysub-symbolic approaches. They benefit from thepowerful representations provided by languagemodels (Devlin et al., 2019), but struggle with sym-bolic reasoning among entity pairs. Therefore, theyare prone to making mistakes when logical rea-soning is required, leading to conflicts with theinherent interdependencies among relations. Sec-ond, the purely data-driven nature of these pseudo- \"Have You Ever Been in Love\" is a song recorded byCanadian recording artist Celine Dion, included first on herseventh English studio album \"A New Day Has Come\" (2002)and later, on her eighth English studio album \"One Heart\"(2003). The song is a power ballad, written by Anders Bagg,and Laila Bagge, while production was handled by Bagge. ...",
  "Minimal Diagnosis:1: Flip (Have You Ever Been in Love, part of , One Heart) to False2: Flip (One Heart, has part, Have You Ever Been in Love ) to True": ": An illustration of conflicts between pseudo-labels and logical rules. To revolve this conflict, twopotential minimal diagnostic solutions are proposed,each involving the flip of a binary pseudo-label. labeling methods makes them hard to interpret.Third, to achieve optimal performance, they re-quire multiple rounds of training across variousfolds, significantly increasing the time consump-tion and limiting their practical application. Tran-scending these limitations calls for a fundamentalparadigm shift: i) moving away from independentclassification of each triplet to structured predic-tion; and ii) moving away from pure representationlearning towards neural-symbolic computing.Our key insight is that symbolic knowledge, suchas logical rules, can be utilized as diagnostic toolsto identify conflicts between pseudo-labels. For ex-ample, in , we can identify conflicts suchas asserting that Have You Ever Been in Love ispart of One Heart while simultaneously claimingthat One Heart does not have Have You Ever Beenin Love as a part, which conflicts with the logicalrule (h, part of, t) (t, has part, h). Bycorrectly flipping certain pseudo-labels to resolvethis conflict, we can enhance the quality of pseudo-labels and mitigate the pervasive issue of confirma-tion bias. Building upon this insight, we proposeLogicST, a novel self-training framework that useslogical rules to diagnose pseudo-labels. LogicST isimplemented within a teacher-student framework(Tarvainen and Valpola, 2017), where the teachermodel is first pre-trained to establish a robust ini-tial state. Then, the diagnosed pseudo-labels fromthe teacher iteratively update the student, whoseparameters are in turn used to gradually update theteacher. Given the multitude of potential candi-dates and the high time complexity of computingdiagnoses, LogicST employs a sequential diagnosisapproach. Specifically, LogicST defines a scoring function that dynamically evaluates the probabil-ities and rewards of each diagnosis. It eliminatesthose with lower scores at each updating step andultimately uses the highest-scoring diagnosis to up-date the student model. By doing so, our LogicSTframework i) introduces symbolic reasoning intorepresentation learning, ii) achieves better perfor-mance and interpretability, and iii) reduces the needfor multiple rounds of training and pseudo-labeling,thus significantly improving time efficiency. Ourmain contributions are listed as follows:",
  "Related Work": "Document-Level Relation Extraction. Since theadvent of pre-trained language models (Devlinet al., 2019; Liu et al., 2019), research in DocREhas experienced significant growth. Substantialprogress has been made through the developmentof complex neural networks (Zhou et al., 2021;Jiang et al., 2022; Tan et al., 2022a), the integra-tion of evidence sentences (Huang et al., 2021;Xie et al., 2022), and the exploration of loss func-tions (Zhou and Lee, 2022). More recently, the useof large language models (LLMs) (Brown et al.,2020), has emerged as a promising direction forfurther advancements (Li et al., 2023a; Gao et al.,2023). Despite these advancements, most existingmethods fail to account for the rich logical struc-tures among relations and lack an explicit mech-anism for symbolic reasoning. While some ap-proaches attempt to incorporate logical rules, theyare typically designed for fully supervised settingsand perform poorly in the presence of numerousfalse negative samples (Fan et al., 2022). To thebest of our knowledge, the LogicST frameworkis the first to integrate symbolic knowledge intoDocRE with incomplete annotations.Document-Level Relation Extraction with In-complete Annotations.Existing efforts to ad- dress incomplete labeling problems in DocRE canbe categorized into: negative sampling (Li et al.,2021), positive-unlabeled (PU) learning (Wanget al., 2022a, 2024), and sub-symbolic self-training(Tan et al., 2023a). Negative sampling avoids over-fitting to false negatives but fails to effectively uti-lize the semantic information of unsampled sam-ples. PU learning adjusts the loss weights assignedto relational classes but overlooks the intra-classvariability among distinct samples. Sub-symbolicself-training, representing the current state-of-the-art, iteratively re-annotates negative samples, fullyutilizing all sample information and accounting forwithin-class differences. However, it neglects theinformative structures between entity pairs, lead-ing to sub-optimal extraction performance and ex-tended training times. To address above three limi-tations, this work integrates symbolic knowledgeinto the self-training framework, providing a novelperspective on DocRE with incomplete annota-tions.",
  "Methodology": "Given the training set DTrain = {di}|DT rain|i=1,where each document contains n named entities{ei}ni=1, the objective of DocRE with incompleteannotations is to train a model that fully utilizesboth the annotated positive triplets GP and the an-notated negative triplets GN. Note that GP onlycontains true positives, while GN contains bothtrue negatives and numerous false positives. Thesize of GP is usually small, resulting in an insuf-ficient learning signal. To address this challenge,self-training is proposed to assign pseudo-labels toGN based on model predictions.This paper introduces the LogicST framework,which integrates symbolic knowledge into the self-training process and constructs minimal diagnosticsets to refine pseudo-labels. The overall architec-ture is described in .1. The method fordeveloping minimal diagnostic sets is detailed in.2. To streamline the diagnostic proce-dure and identify the optimal diagnosis, the sequen-tial diagnosis approach and the scoring functionemployed are elaborated in .3. visually outlines the workflow of LogicST.",
  ", 4, : False": ": The workflow of the LogicST framework in-volves three main steps. First, the teacher model pseudo-labels triplets using confidence thresholding. Next, se-quential diagnosis refines these labels. Finally, the re-fined labels are used to update the models. semantic segmentation studies (Wang et al., 2022b),both models share the same architecture but havedifferent parameters. LogicST is compatible withany DocRE backbone network. Following priorwork (Tan et al., 2023a), we adopt ATLOP (Zhouet al., 2021) as the backbone and NCRL (Zhou andLee, 2022) as the loss function. LogicST adopts a two-stage training paradigm.First, the teacher model is pre-trained on DTrain toestablish a robust initial state. In the training stage,the teacher model pseudo-labels each triplet in GN using the method (). Typically, existing self-training methods (Tan et al., 2023a) label tripletsas true if their logits exceed a threshold f0, i.e.,(X) = I(ft(X) > f0). However, this approachtreats triplets independently, ignoring their logicalinterdependency, which may lead to conflicts. To address this limitation, LogicST employsa conflict resolution strategy to correct errors inpseudo-labels produced by confidence threshold-ing, thereby improving the training of neural back-bones. Specifically, the student model updatesits parameters using the corrected pseudo-labels,while the teacher models weights are updated viaan exponential moving average (EMA) of the stu-dent models weights, ensuring a dynamic yet sta-ble learning.",
  "ft is the teacher model that computes logits andbinary pseudo-labels O = {otri}triGP GN forall triplets, where otri is defined as a key-valuepair in the form triplet: boolean value": "K is a finite set of first-order logical rules thatsymbolically capture the logical dependenciesbetween relations. To ensure the quality of thelogical rules, we use a frequency-based approach(Fan et al., 2022) to construct K from the devel-opment set, including implication, composition,and negation rules. Definitions and examples ofused rules are provided in Appendix A.",
  "oflip(o) , (2)": "where signifies that the resulting condition islogically consistent. As shown in , for alogical conflict involving predicates o1, o2, . . . , o,the conflict can be resolved by flipping any indi-vidual predicate oi {o1, . . . , o}. Specifically,flipping a predicate in the antecedent will make therule no longer applicable, whereas flipping the con-sequent predicate will make the rule logically con-sistent. Based on Occams razor (Domingos, 1999),we only consider the minimal diagnosis, where nosubset of the minimal diagnostic set can resolvethe conflict. Inspired by circuit diagnosis theory(Reiter, 1987), we calculate minimal diagnoses byidentifying conflicts. As illustrated in Algorithm 2and , LogicST employs an iterative processto identify and resolve logical conflicts between thepseudo-labels O and the rules K.During each iteration, the algorithm checkswhether the current minimal candidate set canresolve the conflict between the pseudo-labels Oand the logical rules K. This process involves eval-uating whether the intersection {o1, . . . , o} isan empty set, where {o1, . . . , o} represents the setof conflicting pseudo-labels in this iteration:1. Case 1: Conflict Already ResolvedIf the intersection {o1, . . . , o} = , this meansthat already contains at least one new conflictingpseudo-label to be flipped. In this case, the minimalcandidate set can resolve the conflict withoutfurther modification, and no changes are needed.This situation is marked by the in .2. Case 2: Conflict Not ResolvedIf the intersection {o1, . . . , o} = , it indi-cates that none of the conflicting pseudo-labels arepresent in the current set . Therefore, cannotresolve the conflict, and the algorithm replaces with its supersets. Each superset contains one newconflicting pseudo-label oi from {o1, . . . , o}. Thissituation is indicated by the in .Once the minimal candidate sets are expandedand evaluated, LogicST proceeds to remove any",
  "Sequential Diagnosis": "Although Algorithm 2 can compute all minimalcandidate sets, it incurs a time complexity ofO(||2) in each iteration to remove non-minimalsets, which is impractical when dealing with nu-merous conflicts. Moreover, as shown in , there are usually multiple minimal candidatesets that meet the definition, but only one specificset will be used to flip the pseudo-labels duringtraining. To address these challenges, LogicSTintroduces a scoring function F(|S) to evaluatea diagnosis given the input S. After updatingthe minimal diagnostic sets in each iteration, onlythe TopK diagnoses with the highest scores areretained, significantly reducing the time required.Finally, the highest-scoring candidate set is usedto update the student model, a process termed best-first sampling in model-based diagnosis (Rodler,2022).",
  "oP(o|S),(3)": "where + and are sets of triplets to be flipped totrue and false, respectively. P(o|S) is the probabil-ity of the corresponding triplet predicted to be truegiven the input S, and P(o|S) = 1 P(o|S).A naive method to compute this probability isby applying the sigmoid function to the marginbetween the classification logits and the thresholdlogits:P(o|S) = (fi f0),(4) where fi is the teacher models logit for the rela-tion class i corresponding to o, and f0 is the clas-sification threshold score for that triplet. However,DocRE tasks often suffer from severe class imbal-ance (Tan et al., 2022a), causing logits to be biasedtowards popular classes (Menon et al., 2021) and in-troducing confirmation bias (Cho and Roy, 2004).To address this, LogicST adopts an adaptive post-hoc logit adjustment to compensate for minorityclasses. We maintain a margin bank to dynamicallyrecord the training status of each class. At the step-th training iteration, we first use the batch-wisemean margin to evaluate the instant performanceof class i:",
  "P(o|S) = (fi f0 Margini),(7)": "where is a hyper-parameter controlling the inten-sity of compensation.Rewards of Diagnoses. In the early phase ofthe training stage, the teacher models pre-trainedunder numerous false-negative samples tend to gen-erate high-precision but low-recall pseudo-labels(Tan et al., 2022b). Thus, the reward of flipping pseudo-labels from false to true is greater. As train-ing progresses and the teacher model is updated,the importance of precision will gradually increase,while the importance of recall will decrease. There-fore, the reward of a diagnosis is defined as:",
  "Experimental Setup": "Datasets. The experiments are conducted on theDocRED (Yao et al., 2019) and DWIE (Zhang et al.,2022a) datasets.DocRED is a large-scale andwidely-used benchmark, but it is known to havenumerous missing annotations. We use the incom-pletely labeled training set of DocRED and therevised development and test sets of Re-DocRED(Tan et al., 2022b) and DocGNRE (Li et al., 2023b)to validate the models effectiveness. Addition-ally, we experiment on the extremely incompletetraining set, DocRED_ext (Wang et al., 2022a),where the number of labels for each relation typein a document is limited to one. DWIE (Zaporo-jets et al., 2021) is a human-annotated dataset. Tocreate incompletely annotated training sets, we uni-formly sample 20%, 40%, 60%, and 80% of pos-itive triplets to build labels. The original develop-ment and test sets of DWIE are used for evaluation.The dataset statistics are provided in Appendix B.Evaluation Metrics. We utilize F1, Ign F1, preci-sion, and recall as the primary metrics, where theIgn F1 score excludes triplets shared between thetraining and test sets to avoid data leakage. We alsocompute F1 scores for frequent classes (the top 10most common relation types) and long-tail classes(all other relation types), denoted as Freq_F1 andLT_F1, respectively. Additional implementationdetails are provided in Appendix C.",
  "We compare LogicST to the following six typesof baselines: 1) vanilla baselines, including vari-ous top-performing models under fully supervised": "settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tanet al., 2022a); 2) negative sampling methods (Liet al., 2021); 3) PU learning-based methods, includ-ing SSR-PU (Wang et al., 2022a) and P3M (Wanget al., 2024); 4) sub-symbolic self-training methods,such as VST (Jie et al., 2019), CREST (Wei et al.,2021), and CAST (Tan et al., 2023b); 5) methodsbased on large language models (LLMs), includingLLaMA2-7B (Touvron et al., 2023), GPT-3.5 (Ope-nAI, 2022), and GPT-4o (OpenAI, 2024), as wellas techniques utilizing in-context learning (ICL)1 for task-specific adaptation (Dong et al., 2022),natural language inference (NLI) models for fuzzymatching (Li et al., 2023a), and data programmingfor label denoising (Gao et al., 2023); and 6) log-ical frameworks designed for supervised DocRE,including LogiRE (Ru et al., 2021), MILR (Fanet al., 2022), and JMRL (Qi et al., 2024).",
  "Main Results": "Results on DocRED. presents the quanti-tative comparisons on Re-DocRED2, from whichwe draw four observations: First, even with the in-clusion of sophisticated adaptation techniques, allLLMs still underperform compared to specializedmodels. This may be due to LLMs difficulty inhandling complex reasoning, and domain-specificnuances (Pang et al., 2023), as well as the lackof task-specific tuning and sufficient labeled datafor relation extraction tasks (Zhang et al., 2023).Furthermore, we observe that adding more noisyin-context samples can mislead LLMs, degradingtheir performance. Second, LogicST surpasses allbaselines by a large margin, achieving a 7.94%absolute F1 improvement over CAST, establish-ing new state-of-the-art results with 69.26% F1using BERT-base and 73.29% F1 using RoBERTa-large. This significantly narrows the gap with fullysupervised ATLOP-BERTs 74.02% performance.Third, while all weakly-supervised methods aim toenhance overall performance, often at the expenseof precision, LogicST maintains a superior balancebetween precision and recall. Remarkably, Log-icST either achieves or closely approaches the bestperformance in both precision and recall among allthese methods. Finally, by incorporating symboliclogic, LogicST mitigates the confirmation bias in-herent in self-training and the class imbalance prob-lem in the training set, thereby improving perfor-",
  ": Experimental results on the test set of Re-DocRED (%). The reported results are the average of five runs.Results marked with are reproduced from Wang et al. (2024) using the dev set of Re-DocRED": "mance for both frequent and long-tail classes. Ad-ditionally, we compare the performance of CASTand LogicST across various relation classes in Ap-pendix F.Results on DocRED_ext.The experimental re-sults using the DocRED_ext training set and theRe-DocRED test set are shown in . Theproposed LogicST framework consistently outper-forms all strong baselines, surpassing the previousstate-of-the-art, P3M, by 7.94% in F1 score.",
  "Results on DWIE. As illustrated in , Log-icST consistently surpasses all baseline models": "across different sampling ratios, with its superi-ority becoming increasingly evident in scenarios oflimited annotations. Remarkably, it approaches thefully supervised performance of 74.36%. This veri-fies the ability of logical diagnosis to mitigate falsenegative issues. However, LogicSTs improvementis less significant compared to DocRED, whichcan be attributed to differences in dataset construc-tion. The incomplete DWIE dataset is generatedthrough uniform sampling, whereas the missingannotations in DocRED result from distant super-vision, leading to biases towards popular classesand entities (Huang et al., 2022). Consequently, theDWIE dataset is simpler, reducing the performancegap between different frameworks. 0.20.30.40.50.60.70.8 Sampling Ratio of Positive Triplets F1 (%) ATLOP [AAAI 2021]NS [ICLR 2021]CAST [ACL 2023] P3M [AAAI 2024] LogicST [Ours]",
  "Analysis & Discussion": "Comparison with Other Logical DocRE Frame-works. We compare LogicST with LogiRE (Ruet al., 2021), MILR (Fan et al., 2022), and JMRL(Qi et al., 2024) using BERT-base as the encoder.The results in show that these baselinesonly marginally improve the backbones perfor-mance due to the use of noisy labels for calculatingthe classification loss, which inevitably leads tooverfitting. In contrast, LogicST aims to correctthese noisy labels, performing significantly betterunder conditions of incomplete annotations.",
  ": Comparison with other logical frameworks onthe test set of Re-DocRED (%)": "Performance with respect to Entity Pairs Dis-tances. We break down the relation extraction per-formance into four groups based on the distance be-tween entity pairs to analyze the long-range depen-dency capture capabilities. As shown in ,the LogicST framework consistently outperformsall strong baselines across all groups. Moreover,the performance gains from CAST to LogicST in-crease as the distance grows. For distances in theranges [200, 300) and [300, ), LogicST achieves15.45% and 23.67% F1 enhancement, respectively.These results demonstrate the superiority of Log-icST in incorporating rules to capture long-rangedependencies and alleviate confirmation bias.Efficiency Comparison. presents thetraining time of various frameworks. Notably, onlyself-training frameworks such as CAST and Log-",
  ": Training time of ATLOP-backbone frameworkstraining for 30 epochs with batch size 4 on DocRED": "icST require an additional pre-training stage to getbetter initial state. Unlike CAST, which necessi-tates multiple rounds and splits of training, ourLogicST framework pre-trains the teacher modelonly once, significantly reducing this time over-head. During the training stage, LogicST incursonly an additional forward propagation and logi-cal diagnosis step, resulting in an acceptable timeincrease compared to the vanilla training pipeline.Effect of Scoring Functions. To assess the impactof the scoring function, we plot the F1 scores ofdifferent variants over training epochs in . The results reveal that: 1) Without consideringthe probability of diagnoses, the model strugglesto correct false positives. Initially, it corrects falsenegatives, leading to a performance increase, butsubsequently fits extra introduced errors, causinga performance decline; 2) Without considering thereward of diagnoses, the framework fails to correctenough false negatives in the early training stages,resulting in sub-optimal performance. We also per-form a case study of margin bank in Appendix G.",
  ": F1 vs. the number of training epochs on thedevelopment set of DWIE with 40% sampling ratios": "Ablation Study.We conduct an ablation exper-iment to assess the efficacy of LogicSTs compo-nents. Additionally, we introduce a baseline termed\"Fixed Diagnosis\", which employs implication andcomposition rules to complement training labelsbefore the two-stage training, and keeping theselabels fixed. The experimental results in re-veal three key observations. First, the EMA teacheris essential for mitigating the impact of noisy labelsand stabilizing the training process. Second, logi-",
  ": Ablative experiments on DocRED (%)": "cal diagnosis significantly improves the quality andcoverage of pseudo-labels, enhancing performance.Third, while simply adding missing triplets vialogical rules yields competitive results, the \"FixedDiagnosis\" method falls short of LogicST due to itsinability to account for additional false negativesidentified by the backbone during training.Case Study. To further illustrate the effect of logi-cal diagnosis and its contribution to interpretability,we present a typical example in , wherethe pseudo-labels generated by confidence thresh-olding conflict with symbolic knowledge. For in-stance, Have You Ever Been in Love is labeledas part of One Heart, while simultaneously, OneHeart is claimed to not have part of the song,which contradicts the rule: (h, part of, t) (t, has part, h). LogicST resolves this conflictby adding the missing label instead of negating atrue positive, aligning pseudo-labels with logicalrules and enhancing interpretability. Additionally, provides two similar cases. Another casestudy of predictions is provided in Appendix H. \"Have You Ever Been in Love\" is a song recorded byCanadian recording artist Celine Dion, included first on herseventh English studio album \"A New Day Has Come\" (2002)and later, on her eighth English studio album \"One Heart\"(2003). The song is a power ballad, written by Anders Bagg,and Laila Bagge, while production was handled by Bagge. ...",
  "Limitations": "Although making some progress, our LogicSTframework still has several limitations. First, thescoring function of LogicST is designed for set-tings with incomplete annotations and is not appli-cable to distant supervision settings (Mintz et al.,2009; Liu et al., 2022). Secondly, LogicST is onlyapplicable to datasets with clear logical relation-ships between relations, making it unsuitable forbinary datasets such as CDR (Li et al., 2016) andbiomedical datasets such as ChemDisGene (Zhanget al., 2022b). Specifically, if a chemical affects theexpression of a gene and that gene can be used asa marker for a disease, we cannot assume that thischemical is a therapeutic for that disease. Thirdly,LogicST assumes that entities and their mentionsare identified beforehand (Li and Ji, 2014), whichfalls short of real-world applications. We will ad-dress these limitations in future work.",
  "Ethics Statement": "ChatGPT and Grammarly were used for parts ofthe writing. Compared to their sentence-level coun-terparts, DocRE models including the proposedLogicST, demonstrate enhanced capabilities foranalyzing vast volumes of online text and iden-tifying private information across different users.Aware of the associated privacy concerns, we en-sure that all data utilized in this study is publicand devoid of any personal information. Further-more, we strongly advise against using the pro-posed framework for analyzing data that containspersonal privacy elements in future applications.",
  "supervised learning. In Proc. of IJCNN, pages 18.IEEE": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. In Proc. ofNeurIPS, pages 18771901. Curran Associates, Inc.",
  ". Association for Computational Linguis-tics": "Feng Jiang, Jianwei Niu, Shasha Mo, and Shengda Fan.2022. Key mention pairs guided document-level re-lation extraction. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 19041914. Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, andLinlin Li. 2019. Better modeling of incomplete an-notations for named entity recognition. In Proc. ofNAACL, pages 729734. Association for Computa-tional Linguistics.",
  "Dong-Hyun Lee et al. 2013. Pseudo-label: The simpleand efficient semi-supervised learning method fordeep neural networks. In Workshop on challenges inrepresentation learning, ICML, page 896": "Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-aky, Chih-Hsuan Wei, Robert Leaman, Allan PeterDavis, Carolyn J. Mattingly, Thomas C. Wiegers, andZhiyong Lu. 2016. Biocreative V CDR task corpus:a resource for chemical disease relation extraction.Database J. Biol. Databases Curation. Junpeng Li, Zixia Jia, and Zilong Zheng. 2023a. Semi-automatic data enhancement for document-level re-lation extraction with distant supervision from largelanguage models. In Proc. of EMNLP, pages 54955505. Association for Computational Linguistics. Junpeng Li, Zixia Jia, and Zilong Zheng. 2023b. Semi-automatic data enhancement for document-level re-lation extraction with distant supervision from largelanguage models. In Proc. of EMNLP, pages 54955505. Association for Computational Linguistics.",
  "Empirical analysis of unlabeled entity problem innamed entity recognition. In Proc. of ICLR. OpenRe-view.net": "Ruri Liu, Shasha Mo, Jianwei Niu, and Shengda Fan.2022. Ceta: A consensus enhanced training approachfor denoising in distantly supervised relation extrac-tion. In Proceedings of the 29th International Con-ference on Computational Linguistics, pages 22472258. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR.",
  "OpenAI. 2024. Hello gpt-4o. Accessed: 2024-10-03": "Chaoxu Pang, Yixuan Cao, Qiang Ding, and Ping Luo.2023. Guideline learning for in-context informationextraction. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 1537215389. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kpf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Pytorch: Animperative style, high-performance deep learning li-brary. In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Informa-tion Processing Systems 2019, NeurIPS 2019, De-cember 8-14, 2019, Vancouver, BC, Canada, pages80248035. Kunxun Qi, Jianfeng Du, and Hai Wan. 2024. End-to-end learning of logical rules for enhancing document-level relation extraction. In Proceedings of the 62ndAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 72477263.",
  "George Stoica, Emmanouil Antonios Platanios, andBarnabs Pczos. 2021. Re-tacred: Addressing short-comings of the TACRED dataset. In Proc. of AAAI,pages 1384313850. AAAI Press": "Qingyu Tan, Ruidan He, Lidong Bing, and Hwee TouNg. 2022a. Document-level relation extraction withadaptive focal loss and knowledge distillation. InProc. of ACL Findings, pages 16721681. Associa-tion for Computational Linguistics. Qingyu Tan, Lu Xu, Lidong Bing, and Hwee Tou Ng.2023a. Class-adaptive self-training for relation ex-traction with incompletely annotated training data.In Proc. of ACL Findings, pages 86308643. Associ-ation for Computational Linguistics. Qingyu Tan, Lu Xu, Lidong Bing, and Hwee Tou Ng.2023b. Class-adaptive self-training for relation ex-traction with incompletely annotated training data.In Proc. of ACL Findings, pages 86308643. Associ-ation for Computational Linguistics. Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, andSharifah Mahani Aljunied. 2022b. Revisiting docred- addressing the false negative problem in relationextraction. In Proc. of EMNLP, pages 84728487.Association for Computational Linguistics.",
  "Antti Tarvainen and Harri Valpola. 2017. Mean teachersare better role models: Weight-averaged consistencytargets improve semi-supervised deep learning re-sults. In Proc. of NeurIPS, pages 11951204": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Bayu Distiawan Trisedya, Gerhard Weikum, JianzhongQi, and Rui Zhang. 2019. Neural relation extractionfor knowledge base enrichment. In Proc. of ACL,pages 229240. Association for Computational Lin-guistics. Ye Wang, Xinxin Liu, Wenxin Hu, and Tao Zhang.2022a. A unified positive-unlabeled learning frame-work for document-level relation extraction with dif-ferent levels of labeling. In Proc. of EMNLP, pages41234135. Association for Computational Linguis-tics. Ye Wang, Huazheng Pan, Tao Zhang, Wen Wu, andWenxin Hu. 2024. A positive-unlabeled metric learn-ing framework for document-level relation extractionwith incomplete labeling. In Proc. of AAAI, pages1919719205. AAAI Press. Yuchao Wang, Haochen Wang, Yujun Shen, JingjingFei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, andXinyi Le. 2022b. Semi-supervised semantic segmen-tation using unreliable pseudo-labels. In Proc. ofCVPR, pages 42384247. IEEE. Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan L.Yuille, and Fan Yang. 2021.Crest:A class-rebalancing self-training framework for imbalancedsemi-supervised learning. In Proc. of CVPR, pages1085710866. Computer Vision Foundation / IEEE. Yiqing Xie, Jiaming Shen, Sha Li, Yuning Mao, and Ji-awei Han. 2022. Eider: Empowering document-levelrelation extraction with efficient evidence extractionand inference-stage fusion. In Proc. of ACL Find-ings, pages 257268. Association for ComputationalLinguistics.",
  "Klim Zaporojets, Johannes Deleu, Chris Develder, andThomas Demeester. 2021. DWIE: An entity-centricdataset for multi-task document-level informationextraction. page 102563": "Shuang Zeng, Runxin Xu, Baobao Chang, and Lei Li.2020. Double graph based reasoning for document-level relation extraction. In Proc. of EMNLP, pages16301640. Association for Computational Linguis-tics. Dongxu Zhang, Sunil Mohan, Michaela Torkar, and An-drew McCallum. 2022a. A distant supervision cor-pus for extracting biomedical relationships betweenchemicals, diseases and genes. In Proc. of LREC.European Language Resources Association. Dongxu Zhang, Sunil Mohan, Michaela Torkar, and An-drew McCallum. 2022b. A distant supervision cor-pus for extracting biomedical relationships betweenchemicals, diseases and genes. In Proceedings ofthe Thirteenth Language Resources and EvaluationConference, pages 10731082, Marseille, France. Eu-ropean Language Resources Association. Kai Zhang, Bernal Jimnez Gutirrez, and Yu Su. 2023.Aligning instruction tasks unlocks large languagemodels as zero-shot relation extractors. In Findingsof the Association for Computational Linguistics:ACL 2023, pages 794812.",
  "CImplementation Details": "The proposed LogicST framework is compatiblewith any DocRE backbone. Consistent with priorwork (Tan et al., 2023a; Wang et al., 2024), weadopt the ATLOP model (Zhou et al., 2021) as ourbackbone. We use BERT-base (Devlin et al., 2019)and RoBERTa-large (Liu et al., 2019) as the textencoders. All models are implemented in PyTorch(Paszke et al., 2019) and trained on one Tesla V100GPU.For hyper-parameters, we perform a grid searchfor 1 and 2 within {0.99, 0.999, 0.9995}, forTopK within {10, 20, 50}, for within {0.1, 0.3,0.5}, and for within {2, 20, 50, 100, 1000}.All hyper-parameters are selected based on the F1score computed on the development set.",
  "%train(40%)9.9798.87%train(60%)14.7598.34%train(80%)19.5497.81%dev9828.4226.7897.15%test9926.4924.8396.96%": ": Dataset Statistics. #Doc. indicates the number of documents in each dataset. #Rel. denotes the number ofrelation classes. Avg.# Ent. represents the average number of entities per document. Avg. #Triplets indicates theaverage number of annotated true triplets per document. #Negative Rate represents the ratio of negative triplets tothe total number of triplets (negative + positive). For the DWIE dataset, the percentages in parentheses specify thesampling ratios of positive samples.",
  "Ranked Class Index": "F1 (%) * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * CASTLogicST : F1 scores (%) for each class (ranked by class frequency: left (high)right (low)) on the test set ofRe-DocRED of CAST and LogicSTs best models, which are trained on DocRED. Columns marked with * arerelation classes involved in more than 8 logical rules. Better view in color.",
  "tags. Each entity isrepresented by a unique numberenclosed in angle brackets.Please identify all validgiven relation types betweenany two given entities in thedocument": "Target relation type list:{located in the administrativeterritorial entity : \"In the located in the administrativeterritorial entity relation ,the subject , a place , event ,or item , resides or takesplace in the object , anadministrative region. Example: (Harvard University , located Cornelius Ryan 's A Bridge Too Far gives an account of Operation Market Garden, a failed Allied attempt tobreak through German lines at Arnhem in the occupied Netherlands during World War II. The title of the book comes from a comment made by British Lieutenant General Frederick Browning,deputy commander of the First Allied Airborne Army, who told Field Marshal Bernard Montgomery before theoperation, \" I think we may be going a bridge too far.",
  "in the administrativeterritorial entity , Cambridge ,Massachusetts).\"......( all 96 relations andcorresponding description andexample) }": "All non -duplicate valid \"subjectentity \"\" relation type\"\" objectentity\" triples in thedocument (output format: \"entity ID\"\" relation type name\"\" entity ID\", e.g., <1>-country -<2>; one triple perline):[<Entity ID >]-<Relation Name >-[<Entity ID >][<Entity ID >]-<Relation Name >-[<Entity ID >].Please return the triplets in thespecified format directly ,without adding any additionalinformation.",
  "EResults on DocGNRE": "presents the performance of various modelson the DocGNRE dataset (Li et al., 2023b), whichis based on Re-DocRED and enhanced through dis-tant supervision using ChatGPT, followed by hu-man annotation for refinement. As shown in , LogicST demonstrates significant performanceimprovements over existing leading methods inboth precision and recall. Notably, it surpasses thestate-of-the-art P3M model by 7.51% in F1 score.These experimental results further validate the ef-fectiveness of our proposed framework.",
  "FDetailed Comparison with CAST": "We plot the F1 scores of CAST and LogicST forall the classes in , which indicates thatLogicST surpasses CAST in most classes. Notethat CAST samples pseudo labels for all classes,while LogicST only performs logical diagnosis onthe classes involved in the rules. It can be seen thatfor classes involved in many logical rules (markedwith *), LogicST usually has better performance.",
  "HCase Study of Relation ExtractionResults": "illustrates relation extraction casesforATLOPandvariousframeworksbuiltupon it, including NS, CAST, P3M, and Log-icST. This case study involves three logicalrules:(h, part of, t) (t, has part, h),(h, conflict, t)(t, participant, h),and(h, participant, t)(t, participant_of, h).The results demon-strate that ATLOP, CAST-ATLOP, and P3M-ATLOP successfully extract two true positiverelations but fail to infer their logical derivatives.In contrast, the NS-ATLOP method, which dropsmany true negative samples during training, intro-duces an additional false positive error. Notably,LogicST-ATLOP extracts all relevant facts usingthe same architecture and inference method as theother models, highlighting the effectiveness of incorporating logical rules as diagnostic tools toidentify and correct pseudo-label errors. Thesefindings underscore the advantages of usingLogicST to enhance the robustness and accuracyof relation extraction tasks."
}