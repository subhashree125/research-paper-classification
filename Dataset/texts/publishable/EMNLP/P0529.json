{
  "Abstract": "The recent years have witnessed great advancesin video generation. However, the developmentof automatic video metrics is lagging signifi-cantly behind. None of the existing metrics areable to provide reliable scores over generatedvideos. The main barrier is the lack of large-scale human-annotated datasets. In this paper,we release VIDEOFEEDBACK, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videosfrom 11 existing video generative models. Wetrain VIDEOSCORE (initialized from Mantis)based on VIDEOFEEDBACK to enable auto-matic video quality assessment. Experimentsshow that the Spearman correlation betweenVIDEOSCORE and humans can reach 77.1 onVIDEOFEEDBACK-test, beating the prior bestmetrics by about 50 points. Further resultson other held-out EvalCrafter, GenAI-Bench,and VBench show that VIDEOSCORE has con-sistently much higher correlation with humanjudges than other metrics. Due to these re-sults, we believe VIDEOSCORE can serve as agreat proxy for human raters to (1) rate differ-ent video models to track progress (2) simulatefine-grained human feedback in ReinforcementLearning with Human Feedback (RLHF) to im-prove current video generation models.",
  "Introduction": "Powerful text-to-video (T2V) generative modelshave been exponentially emerging these days. In2023 and 2024, we have witnessed an array ofT2V models like Sora (OpenAI, 2024b), RunwayGen-2 (Esser et al., 2023), Lumiere (Bar-Tal et al.,2024), Pika1, Luma-AI2, Kling3, Emu-video (Gird-har et al., 2023), StableVideoDiffusion (Blattmannet al., 2023a). These models have shown their po-tential to generate longer-duration, higher-quality,and more natural videos. Despite the great progressvideo generative models have made, they still sufferfrom artifacts like unnaturalness, inconsistency andhallucination, which calls for reliable fine-grainedmetrics for evaluation and a robust reward modelfor reinforcement learning (RLHF).The recent literature has adopted a wide rangeof metrics to evaluate videos. However, these met-rics suffer from the following issues: (1) some ofthem are computed over distributions and cannotbe adopted to evaluate a single model output. Ex-amples include FVD (Unterthiner et al., 2019) and IS (Salimans et al., 2016). (2) most metrics canonly be used to evaluate visual quality or text align-ment, while failing on other aspects like motionsmoothness, factual consistency, etc. Examples ofsuch metrics include CLIP (Radford et al., 2021b),DINO (Caron et al., 2021), BRISQUE (Mittal et al.,2012a). (2) some metrics focus only on a singlemean opinion score (MOS), failing to provide fine-grained subscores across different multiple aspects.Examples include T2VQA (Kou et al., 2024b),FastVQA (Wu et al., 2022), and DOVER (Wu et al.,2023a). (3) Several works (Ku et al., 2023; Bansalet al., 2024) propose to prompt multi-modal large-language-models (MLLM) like GPT-4o (Achiamet al., 2023) or Gemini-1.5 (Reid et al., 2024) toproduce multi-aspect quality assessment for givenvideos. However, our experiments show that theyalso have low correlation with humans. Thesefeature-based metrics or MLLM prompting meth-ods either fail to provide reliable evaluation or can-not simulate the human feedback from real worldwell, which have lagged behind and restricted usfrom training better video generative models.Since obtaining large-scale human feedback ishighly costly, we can try to approximate human-provided scores with model-based metrics.Tothis end, our work can be divided into two parts:(1) curating VIDEOFEEDBACK, the first large-scale dataset containing human-provided scoresfor 37.6K videos, (2) training VIDEOSCORE onVIDEOFEEDBACK, which is an automatic videometric to simulate human feedback.In preparation of VIDEOFEEDBACK, we solicitprompts from VidProM (Wang and Yang, 2024)and use 11 popular text-to-video models, includingPika, Lavie (Wang et al., 2023c), SVD (Blattmannet al., 2023a), etc, to generate videos of variousquality based on these prompts. We define five keyaspects for evaluation as shown in , and getour videos from VIDEOFEEDBACK annotated foreach aspect from 1 (bad) to 4 (perfect).To build the video evaluator, we select Mantis-Idefics2-8B (Jiang et al., 2024a) as our main back-bone model due to its superior ability to handlemulti-image and video content, accommodatingup to 128 video frames and supporting nativeresolution.After fine-tuning Mantis on VIDE-",
  "BACK-test and 59.5 on EvalCrafter (Liu et al.,2023b) for the text-to-video alignment aspect, sur-": "passing the best baseline by 54.1 and 4.4 respec-tively. The pairwise comparison accuracy gets 78.5on GenAI-Bench (Jiang et al., 2024b) video pref-erence part, and 72.1 in average on 5 aspects ofVBench (Huang et al., 2023), surpassing the pre-vious best baseline by 11.4 and 9.6 respectively.Additional ablation studies with different backbonemodels confirmed that the Mantis-based metric pro-vides a gain of 12.1 compared to using the Idefics2-based metric. Due to the significant improvement,we believe that VIDEOSCORE can serve as the reli-able metrics for future video generative models.",
  "Text-to-Video Generative Models": "Recent progress in diffusion models (Ho et al.,2020; Rombach et al., 2022) has significantlypushed forward the development of Text-to-Video(T2V) generation. Given a text prompt, the T2Vgenerative model can synthesize new video se-quences that didnt previously exist (Wang et al.,2023c; OpenAI, 2024b; Chen et al., 2023a, 2024a;Henschel et al., 2024; Bar-Tal et al., 2024). Earlydiffusion-based video models generally build uponText-to-Image (T2I) models, adding a tempo-ral module to extend itself into the video do-main (Wang et al., 2023c; Chen et al., 2023c). Re-cent T2V generation models are directly trainedon videos from scratch. Among these, modelsbased on Latent Diffusion Models (LDMs) havegained particular attention for their effectivenessand efficiency (Zhou et al., 2022; An et al., 2023;Blattmann et al., 2023b). While the other worksused the pixel-based Diffusion Transformers (DiT)also achieve quality results (Gupta et al., 2023;Chen et al., 2023b; OpenAI, 2024b).",
  "Video Quality Assessment": "As the current progress of Text-to-Video genera-tive models leaves it uncertain how close we areto reaching the objective, researchers have workedon evaluation methods to benchmark the genera-tive models. Common methods involve the useof FVD (Unterthiner et al., 2018) and CLIP (Rad-ford et al., 2021a) to evaluate the quality of framesand the text-frames alignment respectively. How-ever, other aspects like subject consistency, tem-poral consistency, factual consistency cannot becaptured by these metrics.Recent works likeVBench (Huang et al., 2023) proposes to use differ-ent DINO (Caron et al., 2021), optical flow (Horn and Schunck, 1981) to reflect these aspects. How-ever, the correlation with human judgment is rel-atively low. For example, most models have sub-ject/background consistency scores over 97% inVBench, which is a massive overestimation of thecurrent T2V models true capability. Another workEvalCrafter (Liu et al., 2023b) instead resorts tohuman raters to perform comprehensive evaluation.A recent work VideoPhy (Bansal et al., 2024)follows VIEScore (Ku et al., 2023) prompt largemulti-modal models like Gemini (Reid et al., 2024)and GPT-4o (Achiam et al., 2023) to provide qual-ity assessment. However, our later study shows thatthese multimodal language models also achievevery low agreement with human raters. A concur-rent work T2VQA (Kou et al., 2024a) also proposesto train a quality assessment model on human-annotated video ratings. However, there are a fewdistinctions. Firstly, our dataset contains ratingsfor multiple aspects. Secondly, our dataset is 4xlarger than the T2VQA dataset. Thirdly, our metricis built on pre-trained video-language foundationmodels to maximize its performance.",
  "RLHF in image/video generation": "In recent years, reinforcement learning from hu-man feedback (RLHF) has emerged as a signif-icant approach to enhancing the performance ofimage/video generative models. Numerous studieshave focused on training reward models with largedatasets of image-text pairs, such as HPSv2 (Wuet al., 2023b), ImageReward (Xu et al., 2023),RichHF-18K (Liang et al., 2023), or video-textpairs like T2V-Score (Wu et al., 2024). Moreover,some research efforts have combined existing re-ward models to simulate human feedback, such asT2V-Turbo (Li et al., 2024c), while some recentworks (Wang et al. (2024b), Wang et al. (2024a))proposed multi-objective reward model with regres-sion head to provide human preferences. Utilizingthese reward models or feedback simulators, di-verse methods have been proposed to align the out-put of visual generative models with human pref-erences, including RL-based methods (Fan et al.(2024), Zhang et al. (2024)) and reward fine-tuningmethods (Clark et al. (2023), Li et al. (2024b), Yuanet al. (2023)). Additionally, some works adopt datadistillation to fine-tune diffusion models on high-quality data, while others like Diffusion-DPO (Wal-lace et al., 2023), extend Direct Preference Opti-mization (DPO) to train diffusion models based onpreference data. Our VIDEOSCORE aims to ap-",
  "VIDEOFEEDBACK": "This section introduces the construction processof our dataset, VIDEOFEEDBACK. We start byexplaining how we gathered and filtered diversetext prompts for video generation, followed by thevideo-generation processes using 11 selected text-to-video models. Next, we outline the annotationpipeline that guides raters to score videos acrossmultiple aspects defined in . We also in-clude supplementary data to enhance robustness.Finally, we summarize the dataset statistics in Ta-ble 1, with 760 examples as the test set.",
  "Data preparation": "Prompt SourcesWe utilize VidProM (Wang andYang, 2024), a dataset containing extensive text-to-video pairs from different models. VidProMsvideo-generation prompts are diverse and seman-tically rich, derived from real-world user inputs.To create a manageable subset from the 1.04 mil-lion unique prompts, we apply two filters: a lengthfilter and an NSFW filter. The length filter elim-inates prompts with fewer than 5 words or morethan 100 words. The NSFW filter removes promptswith a high probability of containing inappropri-ate content. After filtering, we perform randomdown-sampling to obtain a set of 44.5K prompts,31.6K of them are used in video generation andsome videos may have the same text prompt. Video GenerationWe select 11 text-to-video(T2V) generative models (shown in)with various capabilities so that the quality ofthe generated video ranges from high to lowin a balanced way.Some videos are pre-generated in the VidProM dataset, includingPika, Text2Video-Zero (Khachatryan et al., 2023),VideoCrafter2 (Chen et al., 2024a), and Mod-elScope (Wang et al., 2023a), whereas the othersare generated by ourselves or collected from theInternet (i.e. SoRA). To eliminate differences be-tween models in subsequent annotation stage, wenormalize the videos into a unified format. First,we standardized the frame rate to 8 fps to addressdiscrepancies in temporal consistency betweenhigh and low fps videos. Specifically, for highframe rate model Pika and AnimateDiffusion (Guoet al., 2023) we use frame down sampling, while for",
  "DiDeMo (Hendricks et al., 2017)Real2.0kvarious2.0/3.0s84Panda70M (Chen et al., 2024b)Real2.0kvarious2.0/3.0s84": ": Statistics of our curated VIDEOFEEDBACK for training video-generation evaluator. It consists of 33.6Khuman-scored videos across multiple aspects, with 4k real-world videos collected from DiDeMo (Hendricks et al.,2017) and Panda70M (Chen et al., 2024b) as the supplementary data. Ultimately, we get 37.6K high-quality ratedvideos as the final VIDEOFEEDBACK. low frame rate model like Text2Video-Zero, we em-ployed frame interpolation (Huang et al., 2022) onit. Details are shown in Appendix E. Additionally,we cropped Pika videos to remove the watermark,making them indistinguishable from other models.Ultimately, we obtained 33.6K videos from 11 T2Vmodels, along with their generation prompts.",
  "Annotation Pipeline": "Evaluation DimensionsAs discussed in sec-tion 1, fine-grained and multi-aspect rating ofvideos is crucial for enhancing both the reliabil-ity and explainability of the video evaluator. In-spired by VBench (Huang et al., 2023) and Eval-Crafter (Liu et al., 2023b), and FETV (Liu et al.,2023c), we propose five key dimensions for text-to-video evaluation, detailed in . Thesedimensions encompass both low-level vision as-pects, such as Visual Quality, which evaluates basicvisual impressions, and higher-level aspects, likeText-to-Video Alignment and Factual Consistency,which require a deep understanding of world knowl-edge, is a capability previous metrics do not have.Besides definition, a checklist for error points foreach dimension is also provided to assist the raterin contributing more accurate and consistent rating.Detailed are provided in .",
  "AnnotationWe hired 20 expert raters, with eachrater performing rating for 1K-2K videos. Ourraters are mostly college graduate students. Foreach aspect, there are three available ratings, 1": "(Bad), 2 (Average), and 3 (Good), the score 4 (Per-fect) is post-annotated, as described in the sub-section 3.3. To ensure the consistency and qualityof the annotations, we conducted a system train-ing for each rater. Initially, we conducted a pilottraining session with examples of multi-aspect rat-ings for various videos. Following this, multiplerounds of small-scale annotation were conductedto compute the inter-annotator agreement (IAA)across five aspects, as shown in . The re-sults indicate a high score-matching ratio for allaspects, along with Fleiss (Fleiss and Cohen,1973) and Krippendorffs (Krippendorff, 2011)metrics, with values around 0.4 or 0.5, suggestingsufficient agreement to proceed with large-scaleannotation. The annotation process takes roughly4 weeks to finish. ReviewWe conduct random checks on differentraters during the annotating phase to ensure thealignment. Once we find the exceeded unqualifiedratio in certain rater, we promptly communicatewith the respective rater and review the annotationsfor that segment of the video. This helps calibratethe annotation provided by that rater during therelevant period. For example, we found severalraters are too lenient and tend to give high scoresto unqualified videos. We then step in to makesure they are aligned with our understanding ofevaluation dimensions. With periodical randominspection on annotating, we completed the large-scale annotation of 33.6K videos and moved to the",
  "AspectDefinition": "Visual Quality (VQ)the quality of the video in terms of clearness, resolution, brightness, and colorTemporal Consistency (TC)the consistency of objects or humans in videoDynamic Degree (DD)the degree of dynamic changesText-to-Video Alignment (TVA)the alignment between the text prompt and the video contentFactual Consistency (FC)the consistency of the video content with common-sense and factual knowledge",
  "Dataset Augmentation": "To enhance the robustness of our dataset, weincorporated post-augmentation into the dataset.Firstly, expert raters will review the excellentvideos (all aspects are scored 3) again to selectperfect ones and raise their scoring to 4 (Perfect) incertain aspects, particularly among the SoRA andFastSVD (Blattmann et al., 2023a) videos.Additionally, we gather 4k real-world videosfrom the DiDeMo (Hendricks et al., 2017) andPanda70M (Chen et al., 2024b) with each videoaccompanied by a text description. We select andcut clips from the ones less than 5 seconds to en-sure a strong match between video and its text. Weapply similar normalization in subsection 3.1 andalso use SSIM and MSE between interval sampledframes to filter out the possible static videos, ensur- ing the quality in Dynamic Degree. Finally the 4Kreal videos are scored 4 (perfect) in all aspects.We plot the rating distributions across each di-mension in . which is balanced except forDynamic Degree. We inspected in detail via casestudy and turned out this distribution is expected.Eventually, we get the final 37.6K examples as thetraining split of VIDEOFEEDBACK, and reserve760 validation examples as test set.",
  "Baselines": "To compare with our evaluator model, we selectedtwo categories of video quality metrics. The firstcategory relies on statistical or neural features forevaluation. These metrics typically assess a sin-gle video dimension such as temporal consistency,and then yield a numerical value. The second cate-gory employs advanced MLLMs to evaluate videosacross multiple dimensions. Extensive literaturedemonstrates that MLLMs not only excel in gener-ating content on user instructions but also outper-form traditional metrics in evaluating AI-generatedcontent (AIGC). All baselines are listed in .",
  "Feature-Based MetricsWe list all the experi-mented metrics as follows:": "1. Visual Quality. We use two no-reference im-age quality metrics PIQE (Venkatanath et al.,2015) and BRISQUE (Mittal et al., 2012b).We apply them on all frames of video andtake the average score across frames. 2. Temporal Consistency.In this dimension,CLIP-sim (Radford et al., 2021b) and DINO-sim (Caron et al., 2021) are computed as co-sine similarities of between adjacent framesfeatures, following VBench (Huang et al.,",
  "). Additionally, we calculate SSIM be-tween adjacent frames, denoted as SSIM-sim": "3. Dynamic Degree. We uniformly sample fourframes from the target video and calculatethe average MSE (Mean Square Error) andSSIM (Wang et al., 2004) between adjacentframes in the sample as final score. 4. Text-to-Video Alignment. We include CLIP-Score (Radford et al., 2021b) and X-CLIP-Score (Ma et al., 2022) as metrics in this di-mension. CLIP-Score calculates cosine simi-larity between the feature of each frame andthe text prompt and then averages across allframes, while X-CLIP-Score utilizes the fea-ture of video instead of frames. 5. Factual Consistency. It is challenging to finda feature-based metric to determine whetherthe visual content aligns with common sense.Therefore, we rely on the second category ofmetrics for this dimension. We discretized the continuous outputs of thesemetrics to align with our labeling scores .For instance, for CLIP-sim, values are converted to:4 if raw output in [0.97, 1], 3 if in [0.9, 0.97),2 if in [0.8, 0.9) and 1 otherwise. See for more details.",
  "MLLM Prompting Based MetricsTo under-stand how existing MLLMs perform on the multi-": "aspect video evaluation task, we designed a prompt-ing template in to let them output scoresranging from 1 (Bad) to 4 (Perfect) for each aspect.However, some models, including Idefics2 (Lau-renon et al., 2024), Fuyu (Adept AI, 2023),Kosmos-2 (Peng et al., 2023), and CogVLM (Wanget al., 2023b) and OpenFlamingo (Awadalla et al.,2023), fail to give reasonable outputs. We thus ex-clude them from the tables. MLLMs that follow theoutput format like LLaVA-1.5 (Liu et al., 2023a),LLaVA-1.6 (Liu et al., 2024), Idefics1 (Laurenonet al., 2023), Googles Gemini 1.5 (Reid et al.,2024), and OpenAIs GPT-4o (OpenAI, 2024a).",
  "GenAI-BenchGenAI-Bench(Jiangetal.,": "2024b) is a benchmark designed to evaluateMLLMs ability on preference comparison fortasks including text-to-video generation and others.The preference data is taken from GenAI-Arenafrom user voting. We select the video preferencedata in our experiments. This involves the MLLMjudging which of the two provided videos isgenerally better, measured by pairwise accuracy.We use the averaged scores of the five aspects forMLLM prompting baselines and our models togive the preference. We compute the correlationbetween model-assigned preference vs. humanpreference as our indicator. VBenchVBench (Huang et al., 2023) is acomprehensive multi-aspect benchmark suite forvideo generative models,where they use abunch of existing auto-metrics in each aspect.VBench have released a set of human pref-erence annotations on all the aspects, com-prising videos by 4 models, including Mod-elScope (Wang et al., 2023a), CogVideo (Honget al., 2022), VideoCrafter1 (Chen et al., 2023a),and LaVie (Wang et al., 2023c). We select thesubset from 5 aspects of VBench, like technicalquality, subject consistency, and so on, to com-pute the preference comparison accuracy. For eachaspect, we subsample 100 unique prompts in thetesting. We use the averaged scores of the fiveaspects for MLLM prompting baselines and ourmodels to predict the preference. EvalCrafterEvalCrafter (Liu et al., 2023b) is atext-to-video benchmark across four dimensions:Video Quality, Temporal Consistency, Text-to-Video Alignment, and Motion Quality. We focusedon the first three ones and gathered 2,541 videos byfive models: Pika, Gen2, Floor33 (Floor33, 2024),ModelScope, and ZeroScope (Sterling, 2024). InEvalCrafter, human annotators rated each video ona scale of 1-5, with each scored by three raters.We calculated the average score across raters andnormalized it to . After inference on bench-mark videos, we excluded \"Dynamic Degree\" and\"Factual Consistency\" to match EvalCrafters di-mensions. Finally, we used Spearmans in eachdimension as an indicator.",
  "For VIDEOSCORE, We use two scoring methods:generative scoring and regression scoring. Genera-tive scoring involves training the model to output": "fixed text forms, from which aspect scores are ex-tracted using regular expressions. These scoresare integers corresponding to human annotationscores. In contrast, regression scoring replaces thelanguage model head with a linear layer that out-puts 5 logits representing scores for each aspect.Regression scoring is trained using MSE loss.We select Mantis-Idefics2-8B (Jiang et al.,",
  "Evaluation Results": "We report the Spearman correlation results on theVIDEOFEEDBACK-test and EvalCrafter in and , respectively. For the preference com-parison on videos, we report the pairwise accuracyon the GenAI-Bench and VBench in . VIDEOSCORE achieves the SoTA performanceOn the VIDEOFEEDBACK-test, VIDEOSCOREgets an average of 54.1 improvements on allthe five aspects compared to the baseline GPT-4o. Whats more, on the EvalCrafter benchmark,VIDEOSCORE (reg) has 4.4 improvements on text-to-video alignment. For pairwise preference com-parison, VIDEOSCORE also gets 78.5 accuracy onGenAI-Bench, surpassing the second-best Gemini-1.5-Flash by 11.4 points. on the Vbench, our modelarchives the highest pairwise accuracy on 4 out of5 aspects from VBench, with an average of 16.1improvements.",
  "Feature-based Automatic Metrics are limitedWhile some feature-based automatic metrics aregood at a single aspect, they might fail to evaluatewell on others. For example, on the VIDEOFEED-": "BACK-test, the correlation scores of SSIM-dyn andMSE-dyn achieve 31.5 and 38.0 for the dynamicdegree aspect, but they both get a negative correla-tion for others. Besides, PIQE, BRISQUE, CLIP-Score, and X-CLIP-Score get nearly all negativecorrelations for all 5 aspects. This proves the im-age quality assessment metrics cannot be easilyadapted to the video quality assessment task.",
  ": Spearmans Correlation () of VIDEOSCOREon EvalCrafter (Liu et al., 2023b)": "we conduct a comparison of different T2V modelswith and without employing best-of-k samplingwith VIDEOSCORE on EvalCrafter.We set k = 5 and generated videos using700 prompts from EvalCrafter. For each prompt,the video with the highest average VIDEOSCOREacross five dimensions was selected. We then eval-uated both the \"best videos\" and randomly chosenones using EvalCrafters metrics, averaging the re-sults over 700 videos to obtain the models finalscore. As shown in , compared to the ran-dom sample, most scores on the EvalCrafter bench-mark have increased after the best-of-5 process.",
  "randombestrandombestrandombestrandombestrandombest": "AnimateDiff62.1661.8760.7960.8960.6960.9454.3554.8372.7970.83HotShot-XL53.6961.2257.5660.3958.8560.9451.5254.0546.8369.52LaVie-base57.8859.9057.7958.5254.2157.7052.5153.5366.9969.86VideoCrafter258.3158.9858.4459.7059.1860.7954.3954.6561.2360.77VideoCrafter154.3057.2852.4053.6856.4859.8154.0154.8854.3260.76ModelScope52.4554.4844.8045.0156.7060.3453.2054.4255.0958.17ZeroScope-576w51.0754.0943.3643.8255.9858.7454.5554.6850.3959.12LVDM45.8046.0444.4544.6440.4443.0953.6853.2644.6143.16 : Performace of T2V models on EvalCrafter with and without best-of-5 sampling using VIDEOSCORE.Most EvalCrafter scores have increased compared to the random sample, proving the effectiveness of VIDEOSCORE VBench both have multiple aspects in the bench-marks, we take the average score across these as-pects and report the general performance in .The results show that the Video-LLaVA-basedversion gets the worst performance on the fourbenchmarks, even if it is specifically designed forvideo understanding. The Idefics2-8B-based ver-sion has marginal improvements compared to theVideoLLaVA. After changing to Mantis-Idefics2-8B, the scores on the four benchmarks keep im-proving from 47.5 to 55.6 on average. When thescoring type is regression, the mantis-based ver-sion is still better than the Idefics2-based versionby 12.1 points. Therefore, we select the Mantis-based version as the final choice. Regression scoring or generative scoring?Theprimary difference between regression scoring andgenerative scoring is that regression scoring cangive more fine-grained scores instead of just thefour labels. Results on EvalCrafter, GenAI-Bench,and VBench all indicate that using regression scor-ing can consistently improve the Spearman corre-lation or the pairwise comparison accuracy. Forexample, on GenAI-Bench, VIDEOSCORE (reg)achieves 78.5 accuracy, which is higher than the59.0 of the VIDEOSCORE (gen). The results aresimilar for the other benchmarks. We thus conclude",
  "In this paper, we introduce VIDEOSCORE, which istrained on our meticulously curated dataset VIDE-": "OFEEDBACK for video evaluation and can serveas good simulator for human feedback on gener-ated videos. We hired 20 expert raters to annotatethe 37.6K videos generated from 11 popular text-to-video generative models across 5 key aspects,Visual Quality, Temporal Consistency, DynamicDegree, Text-to-Video Alignment and Factual Con-sistency. Our IAA match ratio gets more than 60%.We test the performance of VIDEOSCORE usingSpearman correlation on VIDEOFEEDBACK-testand EvalCrafter, and using pairwise comparisonaccuracy on GenAI-Bench and VBench. The re-sults show that VIDEOSCORE consistently gets thebest performance, surpassing the powerful base-line GPT-4o and Gemini 1.5 Flash/Pro by a largemargin. Our work highlights the importance of us-ing MLLM for video evaluation and demonstratesthe future prospects of simulating human scores orfeedback in generative tasks, due to its rich worldknowledge and the high-quality rating dataset withfine-grained and multiple dimensions.",
  "Acknowledgement": "We express our gratitude to StarDust for providingvideo raters and to DataCurve for supplying theGPU compute resources. Additionally, we expressour thanks to all the raters who offered valuablefeedback and suggestions, which were instrumentalin completing this work. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Adept AI. 2023. Fuyu-8B: A Multimodal Architec-ture for AI Agents": "Jie An, Songyang Zhang, Harry Yang, Sonal Gupta,Jia-Bin Huang, Jiebo Luo, and Xi Yin. 2023. Latent-shift: Latent diffusion with temporal shift for ef-ficient text-to-video generation.arXiv preprintarXiv:2304.08477. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, Je-nia Jitsev, Simon Kornblith, Pang Wei Koh, GabrielIlharco, Mitchell Wortsman, and Ludwig Schmidt.2023. Openflamingo: An open-source framework fortraining large autoregressive vision-language models.arXiv preprint arXiv:2308.01390. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong,Michal Yarom, Yonatan Bitton, Chenfanfu Jiang,Yizhou Sun, Kai-Wei Chang, and Aditya Grover.2024. Videophy: Evaluating physical commonsensefor video generation. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-rmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Jun-hwa Hur, Yuanzhen Li, Tomer Michaeli, et al. 2024.Lumiere: A space-time diffusion model for videogeneration. arXiv preprint arXiv:2401.12945. Andreas Blattmann, Tim Dockhorn, Sumith Ku-lal, Daniel Mendelevitch, Maciej Kilian, DominikLorenz, Yam Levi, Zion English, Vikram Voleti,Adam Letts, et al. 2023a.Stable video diffu-sion: Scaling latent video diffusion models to largedatasets. arXiv preprint arXiv:2311.15127. Andreas Blattmann, Robin Rombach, Huan Ling, TimDockhorn, Seung Wook Kim, Sanja Fidler, andKarsten Kreis. 2023b.Align your latents: High-resolution video synthesis with latent diffusion mod-els. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages2256322575. Mathilde Caron, Hugo Touvron, Ishan Misra, HervJgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin. 2021. Emerging properties in self-supervisedvision transformers. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages96509660. Haoxin Chen, Menghan Xia, Yingqing He, YongZhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing,Yaofang Liu, Qifeng Chen, Xintao Wang, et al.2023a.Videocrafter1:Open diffusion modelsfor high-quality video generation. arXiv preprintarXiv:2310.19512. Haoxin Chen, Yong Zhang, Xiaodong Cun, MenghanXia, Xintao Wang, Chao Weng, and Ying Shan.2024a.Videocrafter2: Overcoming data limita-tions for high-quality video diffusion models. arXivpreprint arXiv:2401.09047. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong,Sen He, Yanping Xie, Animesh Sinha, Ping Luo,Tao Xiang, and Juan-Manuel Perez-Rua. 2023b.Gentron: Delving deep into diffusion transformersfor image and video generation.arXiv preprintarXiv:2312.04557. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,Ekaterina Deyneka, Hsiang-wei Chao, Byung EunJeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. 2024b. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479. Xinyuan Chen, Yaohui Wang, Lingjun Zhang, ShaobinZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin,Yu Qiao, and Ziwei Liu. 2023c. Seine: Short-to-longvideo diffusion model for generative transition andprediction. arXiv preprint arXiv:2310.20700.",
  "Kevin Clark, Paul Vicol, Kevin Swersky, and David J.Fleet. 2023. Directly fine-tuning diffusion models ondifferentiable rewards. ArXiv, abs/2309.17400": "Patrick Esser, Johnathan Chiu, Parmida Atighehchian,Jonathan Granskog, and Anastasis Germanidis. 2023.Structure and content-guided video synthesis withdiffusion models. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages73467356. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu,Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mo-hammad Ghavamzadeh, Kangwook Lee, and KiminLee. 2024. Reinforcement learning for fine-tuningtext-to-image diffusion models. Advances in NeuralInformation Processing Systems, 36.",
  "Floor33. 2024. Floor33 pictures: Ai video generator": "Rohit Girdhar, Mannat Singh, Andrew Brown, QuentinDuval, Samaneh Azadi, Sai Saketh Rambhatla, Ak-bar Shah, Xi Yin, Devi Parikh, and Ishan Misra.2023. Emu video: Factorizing text-to-video genera-tion by explicit image conditioning. arXiv preprintarXiv:2311.10709. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,Yu Qiao, Dahua Lin, and Bo Dai. 2023. Animated-iff: Animate your personalized text-to-image diffu-sion models without specific tuning. arXiv preprintarXiv:2307.04725.",
  "Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni,Shizhuo Sun, Rongqi Fan, and Wenhu Chen. 2024b.Genai arena: An open evaluation platform for gener-ative models. arXiv preprint arXiv:2406.04485": "Levon Khachatryan, Andranik Movsisyan, VahramTadevosyan, Roberto Henschel, Zhangyang Wang,Shant Navasardyan, and Humphrey Shi. 2023.Text2video-zero: Text-to-image diffusion modelsare zero-shot video generators.arXiv preprintarXiv:2303.13439. Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, ChunyiLi, Haoning Wu, Xiongkuo Min, Guangtao Zhai,and Ning Liu. 2024a. Subjective-aligned dataset andmetric for text-to-video quality assessment. ArXiv,abs/2403.11956. Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, ChunyiLi, Haoning Wu, Xiongkuo Min, Guangtao Zhai, andNing Liu. 2024b.Subjective-aligned dateset andmetric for text-to-video quality assessment. arXivpreprint arXiv:2403.11956.",
  "Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, SugatoBasu, Wenhu Chen, and William Yang Wang. 2024c.T2v-turbo: Breaking the quality bottleneck of videoconsistency model with mixed reward feedback": "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,Ishaan Gulrajani, Carlos Guestrin, Percy Liang, andTatsunori B. Hashimoto. 2023. Alpacaeval: An au-tomatic evaluator of instruction-following models. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Ar-seniy Klimovskiy, Nicholas Carolan, Jiao Sun, JordiPont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Kr-ishnamurthy Dvijotham, Katie Collins, Yiwen Luo,Yang Li, Kai Kohlhoff, Deepak Ramachandran, andVidhya Navalpakkam. 2023. Rich human feedbackfor text-to-image generation. ArXiv, abs/2312.10240.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023a.Visual instruction tuning.ArXiv,abs/2304.08485": "Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng,Raymond Chan, and Ying Shan. 2023b. Evalcrafter:Benchmarking and evaluating large video generationmodels. Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao,Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou.2023c. Fetv: A benchmark for fine-grained eval-uation of open-domain text-to-video generation.Preprint, arXiv:2311.01813. Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan,Ji Zhang, and Rongrong Ji. 2022. X-clip: End-to-end multi-grained contrastive learning for video-textretrieval. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 638647.",
  "Gretchen Krueger, and Ilya Sutskever. 2021a. Learn-ing transferable visual models from natural languagesupervision. In International Conference on MachineLearning": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021b. Learning transferable visual modelsfrom natural language supervision. In Internationalconference on machine learning, pages 87488763.PMLR. Machel Reid, Nikolay Savinov, Denis Teplyashin,Dmitry Lepikhin, Timothy Lillicrap, Jean-baptisteAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions oftokens of context. arXiv preprint arXiv:2403.05530. Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjrn Ommer. 2022.High-resolution image synthesis with latent diffusion mod-els. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages1068410695.",
  "Thomas Unterthiner, Sjoerd van Steenkiste, Karol Ku-rach, Raphal Marinier, Marcin Michalski, and Syl-vain Gelly. 2019.FVD: A new metric for videogeneration": "N.Venkatanath,D.Praneeth,MaruthiChan-drasekhar Bh,Sumohana Channappayya,andSwarup Medasani. 2015.Blind image qualityevaluation using perception based features. 201521st National Conference on Communications, NCC2015. Bram Wallace, Meihua Dang, Rafael Rafailov, LinqiZhou, Aaron Lou, Senthil Purushwalkam, StefanoErmon, Caiming Xiong, Shafiq R. Joty, and NikhilNaik. 2023. Diffusion model alignment using directpreference optimization. ArXiv, abs/2311.12908. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang,Shizhe Diao, Shuang Qiu, Han Zhao, and TongZhang. 2024a.Arithmetic control of llms fordiverse user preferences:Directional preferencealignment with multi-objective rewards.ArXiv,abs/2402.18571.",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero PSimoncelli. 2004. Image quality assessment: fromerror visibility to structural similarity. IEEE transac-tions on image processing, 13(4):600612": "Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.2022. Fast-vqa: Efficient end-to-end video qualityassessment with fragment sampling. In EuropeanConference on Computer Vision, pages 538554. Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen,Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan,and Weisi Lin. 2023a. Exploring video quality as-sessment on user generated contents from aestheticand technical perspectives. In Proceedings of theIEEE/CVF International Conference on ComputerVision, pages 2014420154. Jay Zhangjie Wu, Guian Fang, Haoning Wu, XintaoWang, Yixiao Ge, Xiaodong Cun, David JunhaoZhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, WeisiLin, Wynne Hsu, Ying Shan, and Mike Zheng Shou.2024. Towards a better metric for text-to-video gen-eration. ArXiv, abs/2401.07781. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen,Feng Zhu, Rui Zhao, and Hongsheng Li. 2023b. Hu-man preference score v2: A solid benchmark for eval-uating human preferences of text-to-image synthesis.arXiv preprint arXiv:2306.09341. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong,Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.2023. Imagereward: Learning and evaluating hu-man preferences for text-to-image generation. ArXiv,abs/2304.05977. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei,Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu,Samuel Albanie, and Dong Ni. 2023. Instructvideo:Instructing video diffusion models with human feed-back. ArXiv, abs/2312.12490.",
  "BRisks and Limitation": "Although we have designed systematic pipelines torecruit expert raters and annotate the video evalua-tion scores, we still find out that some annotationscontain errors and may harm the overall qualityof the dataset. Our IAA score computation is onlybased on a small number of trial examples and, thusmight not represent the actual IAA of the whole an-notations. Besides, while VIDEOSCORE is provento be able to effectively give reasonable scores onour defined five aspects, it can still sometimes out-put wrong scores that do not match our expecta-tions. We admit this drawback and list that as oneof our future works.",
  "CDataset Licence": "We have used VidProM (Wang and Yang, 2024)to collect the prompts used for video generation,whose usage LICENSE is CC BY-NC 4.0 license.For other evaluation datasets, We did not find li-cense for EvalCrafter (Liu et al., 2023b) humanannotations. GenAI-Bench (Jiang et al., 2024b)is under MIT licence, and VBench (Huang et al.,2023) is under Apache 2.0 license. We are thusable to utilize these datasets in our experiments.We also release our curated dataset, VIDE-",
  "EVideo Format Normalizing Details": "To mitigate difference of videos format from dif-ferent generative models, we normalize the framerate of all the generated videos to 8 fps (framesper second).Specifically, for high frame ratemodel Pika and AnimateDiffusion (Guo et al.,2023), we use uniform down-sampling to nor-malize Pika from 24 fps to 8fps, and Animate-Diffusion from 23 fps to 8 fps. For low frame rate model Text2Video-Zero (Khachatryan et al.,2023), we use video frame interpolation modelRIFE (Huang et al., 2022) to interpolate frames,adding the frame rate from 4 fps to 8 fps. Forreal-world videos from DiDeMo (Hendricks et al.,2017) and Panda70M (Chen et al., 2024b) in postaugmentation of VIDEOFEEDBACK, we use thesame down-sampling as Pika and AnimateDiffu-sion to reduce their frame rate from 30 fps to 8fps.Additionally, since video from Pika are alwaysattached a watermark \"PIKA-LABS\", we croppedall the Pika videos from the resolution of (1088,640) to (768, 480), making Pika video indistin-guishable from videos from other models.",
  "FAnnotation Details": "Additional annotation details are put in this sectionfor the reference.Firstly we show the user interface of our anno-tating website in and . In bothwelcome page and working page, we list the defini-tion and a checklist of error points in five evaluationdimensions, as shown in . Additionally wealso provide many Good/Average/Poor videos asexamples in each dimension for raters to quicklyunderstand each dimension and align well with ourunderstanding.",
  "GPrompting Template": "In process of training Mantis (Jiang et al., 2024a)for generation scoring and the testing with \"MLLMPrompting\" baselines, we use the same prompt tem-plate provided in .For training Mantis with regression scoring, wemake modification to the above template accord-ingly, instructing model to output a float numberranges from 1.0 to 4.0, as shown in .",
  "HFeature-based Baselines Discretization": "As described in subsection 4.1, we employ sev-eral statistical or neural feature-based metrics asbaselines for comparison with our model. The con-tinuous float-format outputs of these metrics arediscretized into labels , aligning with ourannotation data format. The discretization rules arepresented in . Metrics with a symbolindicate that higher values are better, while thosewith a symbol indicate that lower values are bet-ter.",
  "ICase study of VIDEOFEEDBACK": "We showcase the annotations examples in .The first example depicts a clear video of a womanwith her hair moving, thus scoring 3 in all 5 aspects.The second example shows a distorted video, thusscoring 1 across all the aspects except the dynamicdegree. We further analyzed the correlations be-tween the designed aspects in . We foundthat visual quality achieves a high correlation of 0.6with temporal consistency, while dynamic degreehas a very low correlation with all other aspects.",
  "VisualQuality": "Expected Case:(1) The video looks clear and normal on its appearance.(2) The features like Brightness, Contrast, Color, etc, are appropriate and stable.Error point:(a) local obvious unclear or blurry, (b) too low resolution, (c) some speckles or black patches, (d) appearanceof video is skewed and distorted, (e) unstable optical property, such as brightness, contrast, saturation,exposure etc, (f) flickering color of main objects and backgroundNote:Some videos have watermark, we can ignore that.",
  "TemporalConsis-tency": "Expected Case:(1) The main objects, main characters and overall appearance are consistentacross the video.(2) The appearance of video as well as the movements of humans and objectsare smooth and natural.Error points:(a) The person or object suddenly disappears or appears, (b) The type or class of objects has obvious changes,(c) There is an obvious switch in the screen shot,(d) the appearance of video or movements in it is laggy andun-smooth, (e) local deformation or dislocation of human or objects due to the motion.(for large scale deformation, the video should also be rated as bad in \"1. visual quality\").Note:For a video almost static or with small dynamic degree, as long as it does not have error points, then it shouldbe scored as good.",
  "Text-to-VideoAlignment": "Expected Case:The characters, objects, motions, events etc. that are mentioned in text input prompts all exist reasonably.Error points:(a) The people and objects in prompt do not appear in video, (b) The actions and events in prompt do notappear in video, (c) The number, size, shape, color, state, movement and other attributes of the objects in thevideo do not match the prompt, (d) Text mentioned in prompt is not displayed correctly in the video, suchas \"a placard saying No Smoking\" but \"No Smoking\" is not spelled correctly in the video, (e) The videoformat (such as width, height, screen ratio, duration) does not match the format in prompt.",
  "FactualConsis-tency": "Expected Case:(1) Overall appreance and motion are consistent with our common-sense,physical principles, moral standards, etc.Error points:(a) static ones: Content in video goes against common sense in life, such aslighting a torch in the water, standing in the rain but not getting wet, etc.(b) static ones: The size, color, shape and other basic properties of objects violate scientific principles(c) dynamic ones: The overall movement of people or objects violates common-sense and laws of physics,such as spontaneous upward movement against gravity, abnormal water flow, etc.(d) dynamic ones: Partial movements of people or objects violate common-sense and laws of physics, such asthe movement of hands or legs is anti-joint, etc.Notes:Relation with 5. text-to-video alignment:Some text prompts express fictional and unrealistic content, for example, \"a dog plays the guitar in the sky\"or \"an astronaut rides a horse in space\". In this case, regardless of the veracity of the text prompt, you shouldonly consider whether the other content in the video makes sense.",
  ": Expected cases and error cases for each aspect that annotators can see during the annotation": "Suppose you are an expert in judging and evaluating the quality of AI-generated videos,please watch the following frames of a given video and see the text prompt for generating the video,then give scores from 5 different dimensions:(1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color(2) temporal consistency, the consistency of objects or humans in video(3) dynamic degree, the degree of dynamic changes(4) text-to-video alignment, the alignment between the text prompt and the video content(5) factual consistency, the consistency of the video content with the common-sense and factual knowledge For each dimension, output a number from ,in which 1 means Bad, 2 means Average, 3 means Good,4 means Real or Perfect (the video is like a real video)Here is an output example:visual quality: 4temporal consistency: 4dynamic degree: 3text-to-video alignment: 1factual consistency: 2",
  ": Prompting template in generation format used for VIDEOSCORE training and the MLLM promptingbaselines": "Suppose you are an expert in judging and evaluating the quality of AI-generated videos,please watch the following frames of a given video and see the text prompt for generating the video,then give scores from 5 different dimensions:(1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color(2) temporal consistency, the consistency of objects or humans in video(3) dynamic degree, the degree of dynamic changes(4) text-to-video alignment, the alignment between the text prompt and the video content(5) factual consistency, the consistency of the video content with the common-sense and factual knowledge For each dimension, output a float number from 1.0 to 4.0,higher the number is, better the video performs in that dimension,the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)Here is an output example:visual quality: 2.24temporal consistency: 3.89dynamic degree: 3.17text-to-video alignment: 1.86factual consistency: 2.16"
}