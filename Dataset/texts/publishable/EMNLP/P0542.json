{
  "Abstract": "To effectively use large language models(LLMs) for real-world queries, it is imperativethat they generalize to the long-tail distribution,i.e., rare examples where models exhibit lowconfidence. In this work, we take the first steptowards evaluating LLMs in the long-tail distri-bution of inferential knowledge. We exemplifylong-tail evaluation on the Natural LanguageInference task.First, we introduce Logic-Induced-Knowledge-Search (LINK ), a sys-tematic long-tail data generation framework, toobtain factually-correct yet long-tail inferentialstatements. LINK uses variable-wise prompt-ing grounded on symbolic rules to seek low-confidence statements while ensuring factualcorrectness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains108K statements spanning four domains. Weevaluate popular LLMs on LINT; we find thatstate-of-the-art LLMs show significant perfor-mance drop (21% relative drop for GPT4) onlong-tail data as compared to on head distribu-tion data, and smaller models show even moregeneralization weakness. These results furtherunderscore the necessity of long-tail evaluationin developing generalizable LLMs.1",
  "Introduction": "Generalization, especially to unfamiliar and novelsituations, is a cornerstone for the usability oflarge language models (LLMs) in addressingvaried real-world inquiries. This imminent demandnecessitates evaluation of LLMs in the long-tail distribution (the space consisting of unfamiliarexamples on which the model has low confidence).Previous works, mostly focusing on model mem-orization issue, define long-tail knowledge usingthe frequency of entities in a knowledge base (Caoet al., 2020), in the pre-training dataset (Kandpal et al., 2023), or in Wikipedia search (Mallenet al., 2022). Godbole and Jia (2022) introducesa general definition for long-tail statements, wherelong-tail examples are assigned lower likelihoodby a pre-trained language model. We follow thisdefinition which applies to various data formatand language task for any set of statementswith similar length and format, those in thelong-tail distribution cannot be generated or aregenerated with low confidence by the models,compared to those in the head distribution.Recent works have noticed that LLMs have amarked decline in performance when facing inputsfrom the long-tail (McCoy et al., 2023; Razeghiet al., 2022). Hallucination, for example, is foundto be correlated with data being in the long-tail dis-tribution (Li et al., 2024; Yu et al., 2024). LLMsineffective utilization of long-tail knowledge im-pacts its reasoning capabilities and raises reliabilityconcerns in downstream tasks (Huang et al., 2023).Evaluation in the long-tail distribution requiressystematic generation of long-tail data. However,obtaining examples in the long-tail is non-trivial.With state-of-the-art LLMs being trained on vastvolume of data on the internet (OpenAI, 2023; Tou-vron et al., 2023b), it is increasingly difficult to findunseen examples that can effectively test modelgeneralization to its low-confidence end. Crowd-sourcing long-tail data is also difficult because ofhuman cognitive bias (Tversky and Kahneman,1973, 1974), and LLMs generation in the long-tail distribution is hindered by their pretraining taskof most likely\" next token (McCoy et al., 2023).While demonstrating long-tail evaluation acrossall applications and domains is not feasible withinthe scope of one paper, this work focuses on infer-ential knowledge statement in the form of NaturalLanguage Inference (NLI) task (Bowman et al.,2015; Zellers et al., 2019): NLI requires extensiveknowledge and complex reasoning about entitiesand events, and is one of such tasks on which LLMs",
  "First, we make long-tail inferential knowledgegeneration possible.We propose a novel andlightweight long-tail inferential knowledge gen-eration framework, Logic-Induced-Knowledge-Search (LINK": ") ( 2), a variable-wise prompt-ing framework grounded on symbolic rules. Thisframework enables us to obtain long-tail knowl-edge statements from existing LLMs. Our eval-uation shows that by taking simply instructionsChatGPT(gpt-3.5-turbo) and GPT4(gpt-4) canonly produce statements in the head distributionthat also have lower factual correctness, but usingLINK with the LLMs improves on both distributionconformity and factual correctness ( 3). Second, we test LLMs long-tail generalizationcapability on data generated by LINK( 4). We pro-duce a large-scale dataset, Logic-Induced-Long-Tail (LINT), which contains 108k knowledge state-ments spanning across 4 different domains().In the long-tail test set of LINT, GPT4s capabilityin identifying incorrect knowledge drop by 21%relative to the test set in the head distribution, andthe gap is even larger for other models we tested(ChatGPT, llama2-70b). At the same time, humanperformance significantly outperforms LLMs inboth distributions, and stays consistent betweenhead and long-tail test set.",
  ": Examples of inferential knowledge in each do-main of LINT, in a premise (P), conclusion (C) format": "Our work is the first to propose a systematicframework that generates data in the long-tail dis-tribution. Using NLI as an example, we show thatgenerating data in the long-tail distribution is aneffective way for curating evaluation examples forLLM generalization. Our work serves as a startingpoint for the series of research on long-tail datadiscovery and generation, and motivates thecommunity to incorporate long-tail evaluation intomodel building pipelines.",
  "Advantages of Generating InferentialKnowledge from Symbolic Rules": "Due to the fact that LLMs are pretrained with thetask of generating the most likely\" next token, itis fundamentally challenging for them to directlygenerate long-tail data through prompting that arefactually correct and have low likelihood. Usingsymbolic rules to guide the generation of knowl-edge statements have three benefits: (1) Symbolicrules are designed to be correct, so we alleviatethe pressure of ensuring the deductive plausibilityof the statement throughout the entire generationprocess. (2) The generation process can be brokendown into multiple steps, each of which is condi-tioned on only one variable. Generating for onevariable at a time will be much easier for the model,and it is easier to manipulate the distribution of in-dividual values than the entire sentence. (3) Fromone symbolic rule, one can get abundant combi-nations of variable values as long as they satisfyeach predicate in the rule, making the generationprocess scalable.",
  "Curating Symbolic Rules": "A symbolic rule consists of a premise and a conclu-sion. The conclusion is a single predicate, whilethe premise contains a set of predicates connectedby & operators. Each predicate is a triple of a verbphrase, a subject and an object, and each variablein the symbolic rule has a designated data type.While there are infinitely many ways to constructsymbols rules, we create ours using the principlesof Compatibility and Mutual Exclusivity. Com-patibility refers to a scenario in which one or moreevents enables another event to occur. We constructthe premise and conclusion such that if all predi- cates in the premise are true, then the predicatein the conclusion can occur. Mutual Exclusivityrefers to a scenario in which two or more events orconditions cannot occur simultaneously. We con-struct the premise and conclusion such that if allpredicates in the premise are true, then the predi-cate in the conclusion will not occur. To constructsuch conditions, we add constraints such as time,location, or outcomes to variables in the symbolicrules to make their interaction possible/desirable orimpossible/undesirable. In other words, the conclu-sion describes an event (certain interaction betweenthe variables) and the premise depicts some combi-nation of conditions.For example, a symbolic rule constrained bycompatibility looks like:",
  "The symbolic rule should not contain pred-icates out of scope of LLMs knowledge": "has_height(Tree X, Height Y) is not a valid predi-cate, because it is unlikely that LLMs have knowl-edge about the exact height of one tree. This is toavoid hallucination.We create symbolic rules that span across fourdomains (of constraint type): temporal, locational,outcome and effect, and natural properties, totaling149 person-related rules and 268 object-relatedrules. More about symbolic rules in Appendix B.",
  "Knowledge Beam Search": "Defining search order.Since all variables arelinearly chained, we can search them one by onewithout repetition. We always start with the subjectof the sentence the person or the object, repre-sented as Datatype X. In the rule in , forexample, we start with Person X in the premiseand find a chain of variables that connects it to theobject in the conclusion: X, A, Z, B.For some rules that call for factual knowledgewith only one correct answer, such as age, height,year, etc., we empirically find that it increases theknowledge quality to start from the subject in theconclusion and end with the object in the premise. Constructing Prompt.For each variable, weconstruct a prompt using all predicates that con-tain that variable and other previously searchedvariables. For example, to search variable B inthe rule in , we include predicate ingredi-ent_in(Ingredient Z, Dish B). We assume Z=butterand construct the prompt as follows:Give me 50 values of B to fill in the sentenceingredient_in(butter, B) in the format 1. value.,where B is a Dish. Prompting for knowledge.For each partiallysearched beam, we obtain 200 values of the cur-rent variable from the knowledge model 2. We callOpenAI API 4 times, generating 50 values eachtime (temperature=0.7 3). After each call, we ver-ify the values using a critic model (see paragraphbelow). To prevent duplicates, we explicitly in-struct the model not to generate verified correctvalues and set logit_bias=-100 for incorrect val-ues. We implement an early stop mechanism: iffor two consecutive calls we do not get any correctvalues, we terminate the search for the beam.",
  "We use text-davinci-003 but one can use any model.3We keep top_p=1 for maximum diversity, and top_k isunchangable. Ablation on temperature in Appendix D.1": "Verifyingvalueswithacritic.WeusehuggingfacedefaultimplementationofFlan-T5-XXL (Chung et al., 2022), an instruction-tuned model that can be used zero-shot, as thecritic that checks data type conformity and factualcorrectness of the values. We ask the model tooutput yes/no on the correctness of a given state-ment. For data type conformity, the statement is{value} is a {data type}. For factual correctness,we convert the symbolic predicate into a naturallanguage statement.We obtain the yes tokenprobability and dynamically adjust the thresholdfor accepting values for different predicates. Moreabout the critic model in Appendix C. Pushing values to long-tail distribution withreranking.At each search step, we convert sym-bolic predicates into natural language statements(ingredient_in(butter, saag chicken) Butter isan ingredient in saag chicken) and concatenatethem with and. We obtain the sentence likeli-hood using huggingface default implementationof llama-7B (Touvron et al., 2023a)4 and rerankthe sentences from the lowest likelihood to thehighest likelihood. We take top the 75% valuesunless there are more than 200 values, in whichcase we take the top 200 values. Then we moveon to the next variable. To control data distribu-tion during evaluation, we also generate statementsin the head distribution, by ranking the sentencesfrom the highest to the lowest likelihood.From 149 person-related rules and 268 object-related rules across four domains, we curate ourdatasetLogic-Induced-Long-Tail(LINT)thatconsists of 54K long-tail knowledge statements.We also release 54K head distribution statementsthat are also searched with the LINK framework.Domain-wise statistics of LINT in Appendix H.",
  "Prompt": "1Use less frequent terms of A and B and Z.2Use terms of A and B and Z that are lesscommon.3Use terms with lower frequency for A andB and Z.4Use terms of A and B and Z that have lowerprobability in language model distribution.5Use less frequent words of A and B and Z.6Use words of A and B and Z that are lesscommon.7Use words with lower frequency for A andB and Z.8Use less frequent entities of A and B and Z.9Use entities of A and B and Z that are lesscommon.10Use entities with lower frequency for A andB and Z.",
  ": An illustration of prompts for zero-shot LLMs,containing a symbolic rule in natural language and itsvariables with data type specified": "prompt the LLMs with a natural language versionof the symbolic rule with data types of the variablesspecified, and ask them to populate the rule withall variables simultaneously(). Generationsfrom this prompt serve as the head distributionbaseline, to which we compare model generationswhen they are instructed to generate in the long-tail distribution.To instruct models to generate long-tail knowl-edge from symbolic rules, we append Use lessfrequent terms of A and B and C in the prompt.5",
  "LINK Generations Consistently Fall inLong-tail Distribution": "Following Godbole and Jia (2022)s definitionof long-tail statements, we use the most capableLLM that was producing log likelihood at the timeof experiments (text-davinci-003) to assignlikelihood to generated data. We compare howdifferent models assign distributions to the samestatements, in order to ensure that the distributionstays consistent among all models despite theabsolute log likelihood difference (Appendix E.2).WecalculatetheloglikelihoodoverInstructGPT of all statements generated by LINK,compared with instruction-only ChatGPT and GPT4.We calculate = mean(D(H)) mean(D(L))for each set of statements generated from eachsymbolic rule, where D() means the log likeli-hood distribution of the probability model, H isthe set of statements as head distribution baseline,and L is the set of the statements intended to",
  ": Only LINK generations fall in the correct dis-tributions on the log likelihood scale of InstructGPT": "illustrates that while ChatGPT and GPT4are not able to generate long-tail statements merelyfrom prompting, LINK is able to generate long-tail statements with much lower likelihood. Eachgrid on the x-axis represents a unique symbolic rule,and each grid on the y-axis represents [1.5, 2].A close to 0 means that the intended long-tail dis-tribution generations have the same log likelihoodsas the statements in the head distribution, beinglarger than 0.3 empirically means a decent drop inlikelihood, and being negative means the intendedlong-tail distribution data have even high likelihoodthan the head distribution data.Averaged across 200 sampled rules, LINK hasa positive of 0.48, while ChatGPT and GPT4 eachhas a delta of -0.14 and -0.02. The values forLINK (blue line) float above 0 for most of the rules,some even being above 0.5. On the other hand, values for ChatGPT (red line) and GPT4 (green line)mostly locate around 0, with many being negative.To better illustrate the distribution of statements",
  ": LINK has both the highest factual and datatype accuracy in human evaluation": "generated by LINK, ChatGPT and GPT4, we plotthe log likelihood of the generated statements forone symbolic rule from the three methods .To eliminate noise from incorrect statements on thedistribution, we only plot the statements that aremarked as correct in human evaluation (explainedin 3.3). The likelihood distribution of more rulescan be found in .The long-tail statements from LINK clearlyfall in a much lower probability distribution thanGPT4s long-tail generations. Moreover, GPT4sgeneration in the long-tail distribution in factfalls in the same probability distribution as its headdistribution generations.",
  "LINK Achieves Higher Data Correctnessthan Instruction-Only LLMs": "In addition to distribution correctness, we alsoevaluate data type conformity and factual correct-ness of LLMs long-tail knowledge generationsusing crowdworkers from Amazon Mechanic Turk(AMT). For data type conformity, we ask an AMTworker Is {variable} a {data type}? for each vari-able in the symbolic rule. For factual correctness,we ask an AMT worker Does the premise entailthe conclusion? We sample 4,000 statements fromLINT for human evaluation, of which 2,025 arefrom head distribution and 1,975 are from long-tail distribution. We take 3 annotations for eachstatement and take the majority vote. Annotatoragreement can be found in Appendix I.3. The AMTtemplate can be found in Appendix I.2. shows that instruction-only ChatGPT andGPT4 underperform LINK in both data type confor-mity and factual correctness. Without LINK, bothmodels struggle more with factual correctness, aforeseeable behavior in the low likelihood realm.For domain wise performance see . Forexamples of failure cases, see .",
  "rectness. In this section, we perform ablation stud-ies on the role of reranker, critic, and knowledgemodels in LINK": "Ablationonreranker.Ourvariable-wisereranker is essential for pushing LINK generationsinto the long-tail distribution. presents thedistribution comparison of generated statementsby LINK and the variant without the reranker.Without the reranker, the statements for the long-tail distribution are pulled towards the head distri-bution, making the two completely inseparable.Post-hoc reranking over LLM generations doesnot have the same effect as variable-wise rerankingin LINK. illustrates the distributionof generated statements by LINK, comparedto instruction-only GPT4 and instruction-onlyGPT4 reranked by InstructGPT. Post-hoc rerank-ing barely changes the distribution of generations,even when using the same model as the evaluation.Even though log likelihood is both used byreranker and evaluation, they are taken fromdifferent models and using different inputs. Thestatements we use for ranking the knowledgebeams are shorter than the final statement, as theyonly consist of partial predicates. Despite thesedifferences, variable-wise reranking that uses asmaller model achieves the separation that post-hocfiltering with the evaluation model cannot achieve.Our findings highlight the importance of per-forming variable-wise reranking in LINK. Ablation on the critic model.Critic models areessential for guaranteeing the generation quality,especially in the long-tail distribution. inAppendix D.2 shows that removing the critic andremoving both reranker and critic leads to signif-icant drops of data type conformity and factualcorrectness of generations in the long-tail distri-bution. Note that LINK w/o reranker + critic hashigher generation quality than LINK w/o critic.This is because without a reranker the model isonly able to generate statements in the head dis-tribution, making it easier to be factually correctthan in the long-tail distribution. This observationfurther suggests that critic models are essential forgeneration qualities in the long-tail distribution. Ablation on Knowledge Model.Our analysisabove has shown that by simply providing an in-struction to ChatGPT and GPT4, we cannot effec-tively generate knowledge statements that are bothhigh quality and in the long-tail distribution. By adding LINK, we can improve both distributioncorrectness and generation quality, as shown in Fig-ure 8 and .Interestingly, we find that the generations fromLINK + GPT4 do not show a big improvement overLINK + InstructGPT, both on distribution correct-ness and generation quality. This suggests that theimprovement a stronger knowledge model brings toLINK is marginal compared to that of the rerankerand critic model. This finding highlights the effec-tiveness of LINK in facilitating long-tail generationregardless of the knowledge model.",
  "LLMs (Lack of) Generalization in theLong-tail Distribution": "Using data from LINT, we evaluate LLM general-ization through an entailment classification task oninferential knowledge in the long-tail distribution.We use all human evaluated knowledge state-ments except for those with incorrect data typesin LINT, with 1,925 statements in head distribu-tion and 1,856 statements in long-tail distribution.Statements rated as factually correct has entailmentbetween premise and conclusion, and statementsrated as factually incorrect has contradiction be-tween premise and conclusion.In order to prevent LLMs template biases frommisleading the evaluation, we convert each state-ment into 13 question templates, where each ques-tion templates corresponds to a positive label(Yes, True, or Right) or a negative label(No, False, or Wrong). The question tem-plates are summarized in Appendix G.1. We con-sider a model answering accurately about one state-ment only if the model answers all question tem-plates correctly.WeevaluatethreeLLMs:llama2-70B,ChatGPT and GPT4. In order to enforce the modelto predict the target token sets and minimizeformat noncompliance, we use Chain-of-Thought(CoT) (Wei et al., 2022) prompting that includes 2in-context examples with randomly shuffled ordersof positive label and negative label.For each domain, we report aggregated (All)performance of each model as well as humanbaseline performance in . We also includeperformance on positive labels only and negativelabels only. We also mark relative performancedrop =th",
  "h , where h and t are head and taildistribution aggregated performance.We obtain human performance on the same set": "of statements. We recruit 17 AMT workers whodo not participate in the evaluation task (and thushave not seen the task data). The workers see theknowledge statements in premise, conclusion for-mat and are asked to select yes/no to whether thepremise entails the conclusion. The workers areasked to use search engines to verify their answers.See AMT templates in Appendix I.2.We make the following observations on LLMgeneralization in long-tail NLI. Performance drops in the long-tail distribution.All models exhibit a large relative drop in perfor-mance in the long-tail distribution. The most com-petitive model, GPT4, has a 21% overall drop fromhead to long-tail distribution, while other modelsexhibit a even larger drop. Human Performance does not drop for long-tail distribution.Performance drop in thelong-tail distribution does not occur to humans for3 out of 4 domains. It is expected because humanscan verify their knowledge using search engines, soinfrequent knowledge does not challenge humansas much as models (discussion in Appendix A).The exception with the locational domain maybe due to some relations being less availableonline(eg. banned_in(Food, Country)). Brittleness towards question templates.Thehuge gap between model and human baseline per-formance indicate that LLMs cannot reliably rea-son on the same statement when question templateschange. We find that model performance betweenpositive and negative labels can be very differentfor certain domains, indicating that models are mis-calibrated for positive and negative answer tokens.Although this phenomenon is not entirely due to theshift of distribution of the knowledge statements, itis caused by models unfamiliarity of certain ques-tion templates. For example, we find that template5 and template 12 are the same question (Premise:... Conclusion: ... Does the Premise entail theConclusion?\") with opposite answers (Yes\" andNo\"), but all models performance on template5 is significantly lower than that on template 12.This suggests that models are more likely to createfalse negatives in such context, another evidence ofperformance drop due to long-tail distribution.In summary, our analysis shows that while hu-mans inferential reasoning is not affected by theirfamiliarity to the data (provided that they knowthe entities involved), models inferential reasoning",
  "Related Work": "Works on model generalization analysis have fo-cused on generating adversarial examples formodel evaluation (Zhang and Li, 2019; Ziegleret al., 2022; Perez et al., 2022; Casper et al., 2023),flagging abnormal inputs that are likely to triggerbad behavior. Recently, the community has real-ized the importance of testing language modelsabilities in the long-tail distribution (Godboleand Jia, 2022). Works reveal that LLM perfor-mance is affected by input data probability. (Mc-Coy et al., 2023; Razeghi et al., 2022), and moreworks have focused on generating less commondata for probing LLMs. RICA (Zhou et al., 2020)proposes to include novel entities in self-containedcommonsense statements to evaluate robust infer-ence capabilities. UnCommonSense (Arnaout et al.,2022) proposes to evaluate models on informativenegative knowledge about everyday concepts in ad-dition to positively expressed commonsense knowl-edge. Razeghi et al. (2022) observe a correlationbetween the model performance on math problemsand the frequency of numeric and temporal termsfrom those instances in the pretraining data.In addition to probing models on less commondata, recent works also test LLMs generatingless common data. Chen et al. (2023) propose a negative knowledge generation task where modelsgenerate uncommon knowledge with negationconditioned on constrained keywords. Tang et al.(2023) introduce the less likely brainstormingtask that asks a model to generate outputs thathumans think are relevant but less likely to happen.Generating uncommon data is challenging notonly for LLMs, but also for humans because ofour cognitive bias. Tversky and Kahneman (1974)observe that humans are prone to more systematicerrors when facing uncertain events, and Tverskyand Kahneman (1973) reveal that humans tend toevaluate the frequency of classes or the probabil-ity of events by availability, i.e., by the ease withwhich relevant instances come to mind. These traitsmake it difficult for humans to come up with novelassociations (Kray et al., 2006), a crucial ability tocreate data in the long-tail distribution.",
  "Conclusion": "Using NLI as a case study, we illustrated the signif-icant potential of long-tail data in uncovering thegeneralization limitations of LLMs. We introducedthe first systematic framework designed to gener-ate inferential data within the long-tail distribution,and then demonstrated a noteworthy performancedrop of LLMs in the long-tail examples. Our workinitiates a new line of research focused on long-tail data discovery and generation, urging the re-search community to adopt long-tail evaluation inthe development of LLMs.",
  "Limitation": "Limitation on knowledge statement format.Long-tail knowledge statements may come in mul-tiple shapes and forms. Our work focuses onlyon premise, conclusion format, as the first step to-wards the generation of knowledge statements. Thesymbolic rules do not have high complexity, due tothe limited number of variables and predicates, andbeing under the constraint for the symbolic rules tobe linearly chained. Therefore, the effectiveness ofour framework on generating more complex knowl-edge statements has not been tested. Limitation on testing with open-source models.Our work did not include open-source models inevaluations of long-tail statement generation andentailment classification task. While ChatGPT andGPT4 are arguably the strongest models, open-source models may exhibit new behaviors in thelong-tail realm that are worth exploring. Limitation on ablating with different criticand reranker model settings.While we per-formed extensive ablation studies on the critic andreranker models and established their importance inthe LINK framework, we did not explore a diverseset of model options as well as hyperparameter set-tings. Using other models may or may not affectthe performance of LINK. Limitation on sample size.Due to constraintfrom human annotation resources, we were onlyable to evaluate models on 200 rules uniformlysampled from the LINT. Although the general trendshould remain the same, model performance evalu-ated on all rules may result in some deltas.",
  "Risk": "Generation of harmful values.LINK might beused on mal-intention-ed rules or searching fortoxic and harmful values, where researchers mayreplace our reranker with another model trained toprefer more harmful values. Environmental tax.Another potential risk is in-creasing environmental burdens because we exten-sitvely call OpenAI APIs to large language modelsduring search; however, one can replace the largelanguage models with smaller open source modelswith less environmental tax. Factualerrorsingenerations.Be-cause LINK operates in the long-tail realm,its generations are no guaranteed to be correct100% of the time. If one uses the generationsdirectly without verification, one my introducefalse information into their system.",
  "Use and Distribution": "All data we collected through LLMs in our workare released publicly for usage and have been dulyscrutinized by the authors. Data for all human stud-ies that we conduct are also publicly released withthis work, with appropriate annotator anonymiza-tions.Our framework LINK may only be used for gen-erations that follow the ethics guideline of the com-munity. Using LINK on mal-intention-ed rules orsearching for toxic and harmful values is a poten-tial threat, but the authors strongly condemn doingso. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Hiba Arnaout, Simon Razniewski, Gerhard Weikum,and Jeff Z Pan. 2022. Uncommonsense: Informativenegative knowledge about everyday concepts. In Pro-ceedings of the 31st ACM International Conferenceon Information & Knowledge Management, pages3746. Samuel Bowman, Gabor Angeli, Christopher Potts, andChristopher D Manning. 2015. A large annotatedcorpus for learning natural language inference. InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages 632642.",
  "Ameya Godbole and Robin Jia. 2022. Benchmarkinglong-tail generalization with likelihood splits. arXivpreprint arXiv:2210.06799": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.A survey on hallucination in large language models:Principles, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Nikhil Kandpal, Haikang Deng, Adam Roberts, EricWallace, and Colin Raffel. 2023. Large languagemodels struggle to learn long-tail knowledge. In In-ternational Conference on Machine Learning, pages1569615707. PMLR. Laura J Kray, Adam D Galinsky, and Elaine M Wong.2006.Thinking within the box: The relationalprocessing style elicited by counterfactual mind-sets. Journal of personality and social psychology,91(1):33. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng,Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.2024. The dawn after the dark: An empirical studyon factuality hallucination in large language models.arXiv preprint arXiv:2401.03205. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2022.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. arXiv preprint arXiv:2212.10511. R Thomas McCoy, Shunyu Yao, Dan Friedman,Matthew Hardy, and Thomas L Griffiths. 2023. Em-bers of autoregression: Understanding large languagemodels through the problem they are trained to solve.arXiv preprint arXiv:2309.13638.",
  "Yasaman Razeghi, Robert L Logan IV, Matt Gardner,and Sameer Singh. 2022. Impact of pretraining termfrequencies on few-shot reasoning. arXiv preprintarXiv:2202.07206": "Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,Brendan Roof, Noah A Smith, and Yejin Choi. 2019.Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI con-ference on artificial intelligence, volume 33, pages30273035. Liyan Tang, Yifan Peng, Yanshan Wang, Ying Ding,Greg Durrett, and Justin F Rousseau. 2023. Lesslikely brainstorming: Using language models togenerate alternative hypotheses.arXiv preprintarXiv:2305.19339. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Amos Tversky and Daniel Kahneman. 1974. Judgmentunder uncertainty: Heuristics and biases: Biases injudgments reveal some heuristics of thinking underuncertainty. science, 185(4157):11241131": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wen-tao Ye, Bosheng Qin, Siliang Tang, Qi Tian, andYueting Zhuang. 2024. Hallucidoctor: Mitigatinghallucinatory toxicity in visual instruction data. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1294412953. Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics, pages 47914800.",
  "Jiliang Zhang and Chen Li. 2019. Adversarial examples:Opportunities and challenges. IEEE transactions onneural networks and learning systems, 31(7):25782593": "Pei Zhou, Rahul Khanna, Seyeon Lee, Bill YuchenLin, Daniel Ho, Jay Pujara, and Xiang Ren.2020. Rica: Evaluating robust inference capabili-ties based on commonsense axioms. arXiv preprintarXiv:2005.00782. Daniel Ziegler, Seraphina Nix, Lawrence Chan, TimBauman, Peter Schmidt-Nielsen, Tao Lin, AdamScherlis, Noa Nabeshima, Benjamin Weinstein-Raun,Daniel de Haas, et al. 2022. Adversarial training forhigh-stakes reliability. Advances in Neural Informa-tion Processing Systems, 35:92749286.",
  "AMore Discussions": "Our main experiments on entailment classificationtask assume that the LLMs have the specific knowl-edge necessary to answer the questions. While thisis a fair implication, given that their training dataare known to contain essentially anything availableon the web, one may also worry that the modelshandling of such knowledge is impacted by thememorization of this knowledge. As knowledgegets more long-tail , it means it appears in the train-ing data less and is thus harder to memorize. Giventhe imperfect memorization of current LLMs, thismay impact their performance on such knowledge,and our current experiments suggest that. To ver-ify this, for future experiments one can count thefrequency of this long-tail knowledge in the train-ing data to measure how imperfect memorizationmakes certain knowledge long-tail .",
  ". Select Compatibility (conclusion will be pos-itive) or Mutual Exclusivity (conclusion willbe negative)": "2. Defining Constraints.Our constraints canbe categorized as temporal, locational, nat-ural properties, and desirable/undesirable out-comes and effects. Temporal constraints referto time period or age, locational constraintsrefer to geographic location such as countriesor cities as well as climates (tropical or po-lar, etc), natural properties refer to physicalproperties of objects such as temperature, size,density, speed, and outcomes and effects in-clude allergies, cure of disease, etc.",
  "CCritic Model": "We find that while the critic model usually verifiesdata type conformity with high accuracy, it oftencreates false negatives when verifying factual cor-rectness. Moreover, even within false negativesthat result from the same predicate, the correctvalues get higher yes token probabilities than theincorrect values. We hypothesize that while thecritic model is less confident about certain knowl-edge because it is trained on a smaller portion ofthe knowledge than text-davinvi-003, it can stillrank the values inherently. Therefore, we extractthe probability of the yes token instead of takingthe argmax. We also implement a dynamic criticthreshold that adjusts the threshold for acceptingvalues for different predicates. The algorithm is asfollows:",
  ". For data type conformity, we set a minimumthreshold of 0.65 because we expect the modelto be more confident": "In this way, we can find the maximum availablethreshold for each beam, which guarantees preci-sion while reducing false negatives.To verify the effectiveness of our critic model,we use crowd workers from AMT to evaluatethe data type conformity and factual correctnessof predicates.Specifically, for each symbolicpredicate that contains two variables (e.g., ex-ist_during(Location X, Historical Time Period Y)),we will present a statement in natural language(e.g., Saigon existed during The Cold War.) with3 types of questions: (1) clear reference: Q1 andQ2. (2) factual correctness: Q3. (3) data typeconformity: Q4 and Q5.",
  "D.1Hyperparameters on Knowledge Model": "When constructing LINT, we used InstructGPT asthe knowledge model with temperature=0.7 andtop_p=1. Since top_p=1 maximizes sampling diver-sity and top_k is hidden from the OpenAI API, weconduct ablation studies on whether temperatureaffects the result of knowledge search, comparingtemperature of 0.5 (low diversity), 0.7 (mediumdiversity) and 1.0 (high diversity), using a fewsampled rules.In this ablation study, we usegpt-3.5-turbo-instruct checkpoint as knowl-edge model and llama-2-70b as the approxima-tion of the language distribution.",
  ": Different temperatures result in similar datatype conformity and factual correctness": "shows similar data type conformity andfactual correctness among the three ablated tem-perature, with temperature=1.0 having the lowestaccuracy among the three settings. shows that all three temperature settingscan successfully generate knowledge statements inthe long-tail distribution, except for when tempera-ture=0.5 in one of the six sampled rules.This phenomenon reflects that higher tempera-ture helps generating more diverse values and there-fore more likely to generate long-tail values, whilerisking lowering factual salience.",
  "D.2Effect of Critic on LINK": "To investigate the effectiveness of the critic, weprovide an ablation study on a few sampled rulesby removing the critic in LINK. showsthe generation quality of LINK and several vari-ants in long-tail distribution. Without the critic, thegeneration quality decreases significantly. How-ever, the performance drop is less significant in thehead distribution . Besides, if we replacethe reranker with a random sampling method, thegenerated statements cannot lie in the long-tail dis-tribution (which will be further explained in D.3)and have higher correctness without the critic. Itindicates that it is harder for models to generatecorrect statements from the long-tail distributionthan the head distribution without LINK.",
  "D.4Ineffectiveness of Post-hoc Reranking forLLM generated knowledge": "To further highlight the importance of perform-ing step-wise reranking in LINK, we confirm thatapplying a post-hoc reranker on the GPT4 gener-ations from instructions does not have the sameeffect as LINK. We use InstructGPT to rerankthe GPT4 generations from the lowest to the high-est likelihood and take the top 75% results as thelong-tail distribution. For the head distribution,we rerank the generations from the highest to thelowest likelihood and take the top 75% results.We evaluate on the same set of rules as in 3.2 as an example. illustrates the distributionof generated statements by LINK, prompt-basedGPT4 and prompt-based GPT4 with reranker. Weobserve that using post-hoc reranker still cannotachieve a separation between the generation of thehead distribution and the long-tail distribution, evenwith the same model as the evaluation. It demon-strates that maneuvering the distribution during thesearching process is necessary and more effectivethan post-hoc filtering.",
  "E.2Distribution Comparison of DifferentModels": "In this section, we show that the long-tail distri-bution of different language models overlap, andthat this evidence supports our assumption that auniversal natural language distribution exists; sub-sequently, the long-tail distribution of a languagemodel can be used to approximate the long-tail dis-tribution of other language models.We sample knowledge statements generated byLINK from six rules and calculate their proba-",
  ": Post-hoc reranking of GPT4 does not helpmove the distribution towards the long-tail distribution": "bilities with llama-7b, llama2-7b, llama-2-70b,and InstructGPT. , and Fig-ure 12 respectively show the distribution compar-ison between InstructGPT and the three open-source models over the sampled statements fromeach rule. For every rule, we note that if a set of state-ments falls into the low-probability distribution ofInstructGPT, it also falls into the low-probabilitydistribution of the open-source model. Therefore,the categorization on long-tail distribution by onelanguage model can effectively approximate thecategorization on long-tail distribution by othermodels; hence, we use InstructGPT as the approx-imation of the written natural language distributionin our distribution evaluation.",
  "(f) rule122": ": An illustration on the distribution of generated statements by LINK, ChatGPT and GPT4. While LINKslong-tail generations fall into a lower probability distribution than those of GPT4, GPT4s long-tail distribution\"overlaps with the head distribution, indicating that these generations are not truly long-tail. 4.54.03.53.02.5 Likelihood from text-davinci-003 3.75 3.50 3.25 3.00 2.75 2.50 2.25 2.00 Likelihood from llama-7b rule0 Long-tail DistributionHead Distribution",
  "G.1Probing template": "shows templates we used for the entail-ment classification task. As mentioned in 4, wedivide the templates into positive templates andnegative templates. Positive templates are thosewith a positive label (i.e., Yes, Right and True) andnegative templates are those with a negative label(i.e., No, Wrong and False).Most of the templates have definite labels acrossall rules. However, the label of Template 7 depends on the rules. If the rule has a positive conclusion(e.g., Person X can use ChatGPT), the answer tothe question should be positive, i.e., Yes. On thecontrary, if the rule has a negative conclusion(e.g.,Person X cannot use ChatGPT), the answer to thequestion should be negative, i.e., No.",
  "G.2Accuracy averaged over templates": "One main concern of not using accuracy of ourmain metric is because accuracy is not favorablefor label imbalance. Because we have 13 templateswhere for each knowledge statement we will havean uneven number of positive or negative answers,the negative label rate in total is around 51.58%.Therefore, we decide to consider the template bi-ases and deem a model correct only if it answersall templates of a statement correctly, as our goalis to test the true knowledge of models.When evaluating the accuracy across all tem-plates, the models performance also drops overlong-tail knowledge. shows the accuracyacross all templates of GPT4. Even if LLMs are brit-tle to templates, they exhibit a performance drop inthe long-tail distribution among all domains.",
  ": Accuracy over all templates, GPT4": "search the periods of The Paleolithic Era andLion Gate of Mycenae, and answer easily. Thusthe human errors in the head distribution may bedue to carelessness. For those containing long-tail knowledge, even with search engines, it is notso easy to infer the answer for human annotators. Itis also likely that models do not have such knowl-edge either.",
  "H.1Domain-wise human evaluation": "As mentioned in 3.3, we uniformly sample 4,000statements from LINT for human evaluation. Ta-ble 17 provides more detailed domain-wise statis-tics on the data type conformity and factual cor-rectness performance of LINT long-tail knowledgegeneration. While Natural Properties has thehighest overall accuracy and factuality, model per-formance on positive templates in is thelowest while model performance on negative tem-plates is the highest in this domain. This suggeststhat these LLMs might have been most aligned inthis domain during pre-training.",
  "I.1Recruiting Workers": "We recruit workers from all English-speaking coun-tries (US, UK, New Zealand, Australia, Canada),although AMT workers are mostly US-based. Weuse a qualification task to recruit AMT workers. Inthe qualification task, all workers will be presentedwith three manually selected statements, which areclear and representative. Each statement has fiverelated questions as described in Appendix C. Onlyworkers who answer all the questions correctly willbe recruited. In the end, we recruited 38 workers toevaluate the quality of generation and 17 workersas human baselines for the entailment classificationtask. We paid the workers $0.47 per annotation forevaluating the quality of generations and $0.11 perannotation for the entailment classification task, tomatch $15 per hour based on their working time.",
  "shows the agreement of annotations inthe evaluation task. The high agreement of thedata type conformity and factual correctness forLINT ensures the reliability of our results. The": "Statement:Yes or no: if Plant X vanished in Paleolithic Era, Plant X cannot surroundLion Gate of Mycenae. (Head)True label:YesHuman answer:NoModel answer (GPT4 + COT):Yes. The Paleolithic Era ended around 10,000 BC and the Lion Gate ofMycenae was built around 1250 BC. If PlantX vanished in the PaleolithicEra, it could not have surrounded the Lion Gate of Mycenae which was builtmuch later. Statement:Yes or no: if Bag X has trouble containing Pandeiro, Dhak cannot fit in BagX. (Long-tail )True label:YesHuman answer:NoModel answer (GPT4 + COT):No. The size of the Pandeiro does not directly relate to the size of the Dhak.They are different objects and may have different shapes and sizes. Therefore,just because BagX has trouble containing a Pandeiro doesnt necessarilymean that a Dhak cannot fit in BagX.",
  ": Agreement of annotations in the evaluationtask": "Rule 172LocationalRule: is_located_in(Person A, Location X) & is_forbidden_in(FoodItem B, Location X) cannot_eat(Person A, Food Item B)Premise: Person X is located in HoustonConclusion: Person X cannot eat ChocolateIs Houston a location? Annotation: YesIs Chocolate a food item? Annotation: YesDoes the premise entail the conclusion? Annotation: NoReason: It is a factual error. Chocolate is not actually forbiddenin Houston, so People in Houston can eat chocolate. Rule 371Capabilityand AdviceRule: can_treat(Drug B, Name of Disease X) & has(Person A,Name of Disease X) should_take(Person A, Drug B)Premise: Person X has HepatitisConclusion: Person X should take SofosbuvirIs Hepatitis a name of disease? Annotation: YesIs Sofosbuvir a drug? Annotation: YesDoes the premise entail the conclusion? Annotation: NoReason: It is a factual error. There are different types of hepatitisviruses. Sofosbuvir is a medication used primarily for the treatmentof hepatitis C. For other types of hepatitis, different medications ortreatments may be necessary. Rule 274TemporalRule: vanished_in(Plant A, Historical Time Period X) &was_invented_in(Weapon B, Historical Time Period Y) &is_earlier_than(Historical Time Period X, Historical Time Period Y) cannot_be_used_to_conceal(Plant A, Weapon B)Premise: Plant X vanished in MongolConclusion: Plant X cannot be used to conceal M92 ZoljaIs Mongol a historical time period? Annotation: NoIs M92 Zolja a weapon? Annotation: YesDoes the premise entail the conclusion? Annotation: YesReason: It is a data type error. The Mongols are an East Asian ethnicgroup native to Mongolia, not a time period. The Mongol Empire mayrefer to a period of the 13th and 14th centuries, but Mongol cannot. Rule 204NaturalPropertiesRule: has_trouble_containing(Drawer B, Tool C) & is_larger_than(Tool A, Tool C) cannot_be_placed_in(Tool A, Drawer B)Premise: Drawer X has trouble containing Scroll sawConclusion: Car cannot be placed in Drawer XIs Scroll saw a Tool? Annotation: YesIs Car a Tool? Annotation: NoDoes the premise entail the conclusion? Annotation: YesReason: It is a data type error. Car is a vehicle instead of a tool."
}