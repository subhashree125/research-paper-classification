{
  "Abstract": "Commercially available models dominate aca-demic leaderboards. While impressive, this hasconcentrated research on creating and adapt-ing general-purpose models to improve NLPleaderboard standings for large language mod-els. However, leaderboards collect many indi-vidual tasks and general-purpose models oftenunderperform in specialized domains; domain-specific or adapted models yield superior re-sults. This focus on large general-purpose mod-els excludes many academics and draws atten-tion away from areas where they can makeimportant contributions. We advocate for arenewed focus on developing and evaluatingdomain- and task-specific models, and high-light the unique role of academics in this en-deavor.",
  "Introduction": "Natural language processing (NLP) research hashistorically produced domain- and task-specific su-pervised models. The field has shifted course in thepast few years, with a singular focus on general-purpose generative large language models (LLMs)that, rather than focusing on a single task or domain,do well across many tasks (Brown et al., 2020;Chowdhery et al., 2022; Workshop et al., 2022;Zhang et al., 2022; Touvron et al., 2023b). By train-ing on massive amounts of data from many sources,these models can do well on extremely broadprofessional and linguistic examinations (Achiamet al., 2023; Anil et al., 2023), college-level knowl-edge questions (Hendrycks et al., 2021; Lai et al.,2023), and collections of reasoning tasks (Suzgunet al., 2023).While the trend to develop a single, general-purpose generative model is a net positive changethat has resulted in impressive results, it has alsoslowed down progress in other areas of NLP. First,we are less focused on problems that cannot be",
  "*The project was completed during work at Bloomberg": "solved with a chat-like interface. Second, the best-performing LLMs are often commercial systems,which are sometimes opaque about training data,system architecture, and training details. Third,frequent model updates hinder reproducibility. The resources required to train large generallanguage models naturally constrain research tolarge organizations, and researchers (or academics)outside of these organizations have become de-pendent on closed commercial systems, or opensystems with limited transparency regarding theirtraining data. This is partly reflected in broaderAI trends: Zhang et al. (2021) found that roughly30% of papers at AI conferences (including *CL)have a Fortune 500 tech affiliation. Increased re-sources contribute to the success of transformer-based LLMs (Vaswani et al., 2017), with availablehardware (Hooker, 2021) and benchmarks (De-hghani et al., 2021) both playing a deciding rolein what models end up being developed. By opti-mizing the average score across hundreds of shal-low tasks, we are smoothing out any signal thatwould be gained from deeply engaging with indi-vidual tasks. Developing domain-specific modelscan help identify model and training choices thatyield improvements on tasks within those domains. In this paper, we argue for renewed attention todomain-specific models with rigorous and domain-expert informed evaluations. Because many aca-demics are excluded from LLM development due toresource constraints, attention has been drawn awayfrom research areas where academics can makethe greatest contributions: deep dives on specificchallenging problems. Thus, we propose severalresearch questions to reorient the research commu-nity towards developing domain-specific modelsand applications, where academics are uniquelysuited to lead.",
  "LLMs: A Brief History": "While modern LMs date back to Jelinek (1976), wesummarize very recent history to describe the cur-rent environment. In the wake of the popularizationof neural word embeddings by word2vec (Mikolovet al., 2013), contextualized representations of lan-guage as features for supervised systems were re-alized by ELMo (Peters et al., 2018) followed byBERT (Devlin et al., 2019; Liu et al., 2019). BERTand subsequent models became the base modelsfor supervised systems utilizing task-specific fine-tuning and continued pre-training for new domains(Gururangan et al., 2020), e.g., for clinical tasksELMo (Schumacher and Dredze, 2019) and clini-calBERT (Huang et al., 2019).Parallel work utilized transformers for autore-gressive LLMs, resulting in GPT (Radford et al.,2018), GPT-2 (Radford et al., 2019), BART (Lewiset al., 2020a; Liu et al., 2020), CTRL (Keskar et al.,2019), T5 (Raffel et al., 2020; Xue et al., 2021),and XGLM (Lin et al., 2021). These models hadsome few-shot capabilities, but they could each beadapted (fine-tuned) for a specific task of interest.Some models were available to academics, thoughtraining a new model was beyond reach for many.GPT-3 (Brown et al., 2020) greatly increasedmodel size and changed our understanding ofLLMs. Impressive in-context (few-shot) learningpushed the idea that a single large model couldsolve a wide range of tasks. While the cost ofresources meant training was restricted to a fewgroups, work focused on training bigger models(Chowdhery et al., 2022; Anil et al., 2023; Zhanget al., 2022; Touvron et al., 2023a; Rae et al., 2021).While only a few could train large models, manystudied how best to use them: prompt engineer-ing (Liu et al., 2023), prompt tuning (Han et al.,2022; Wei et al., 2022), evaluation (Liang et al.,2022), among many other topics.CommercialLLM APIs, and eventually open source models(Zhang et al., 2022; Workshop et al., 2022; Touvronet al., 2023a,b; Groeneveld et al., 2024), facilitatedthis work. Ignat et al. (2024) noted the massiveresearch shift to LLMs reflected in Google Scholarcitations. Subsequent work in instruction tuning(Ouyang et al., 2022) and fine-tuning (Wei et al.,2022; Chung et al., 2022; Longpre et al., 2023)have further centralized research around general-purpose models. Many consider fine-tuning forspecific applications to be obsolete: why would youtune a model for a specific task when you can tune",
  "a single model to do well on all tasks?1": "Despite this view, multiple domain-specificLLMs have demonstrated that domain-specific dataleads to models that outperform much larger mod-els (Wu et al., 2023; Taylor et al., 2022). Med-PaLM has shown that adapting even giant LLMs toa specific domain leads to vastly increased perfor-mance (Singhal et al., 2022, 2023).2 Furthermore,the release of LLaMA (Touvron et al., 2023a) ledquickly to Alpaca (Taori et al., 2023) and a waveof new fine-tuned versions of LLaMA for specifictasks. This trend strongly indicates that domain-specific models, especially for constrained sizes,are still highly relevant.To be clear, our concern is not with closed mod-els, which play an important role in the modelecosystem. Models range from full to limited to noaccess, with some closed models providing incred-ibly detailed information (Hoffmann et al., 2022;Rae et al., 2019; Wu et al., 2023) and others pro-viding none (Achiam et al., 2023). Our lamentover this focus on general models, either open orclosed, is that it draws attention away from work ontask- and domain-specific models and evaluations.Academics have become product testers, insteadof focusing on tasks where they can play a uniquerole. Moreover, existing academic benchmarks in-creasingly serve a reduced purpose for commercialmodels; we are hill-climbing on benchmarks with-out a way to ensure existing LLMs have not beentrained to excel on these benchmarks (Dodge et al.,2021). Furthermore, we rely on benchmarks inplace of deep engagement with an application andits stakeholders.",
  "The Need for Domain-Specific LLMs": "In general, web data does not reflect the needs ofall NLP systems. Historically, the community hasdeveloped systems for specialized domains such asfinance, law, bio-medicine, and science. Accord-ingly, there have been efforts to build LLMs forthese domains (Wu et al., 2023; Taylor et al., 2022;Singhal et al., 2022; Bolton et al., 2023; Luo et al.,2022; Lehman et al., 2023; Garca-Ferrero et al.,2024). We need a deep investment in how best todevelop and evaluate these models in partnershipwith domain experts. How should we best integrate 1Distillation for task-specific models remains popular ifsmaller models are desired (Hsieh et al., 2023).2We acknowledge that the biomedical domain is a rapidlydeveloping area, and GPT-4 without fine-tuning was reportedto surpass MedPaLM 2 (Nori et al., 2023).",
  "insights gained from the development of general-purpose models with these efforts? We proposeseveral research directions": "Howcangeneral-purposemodelsinformdomain-specificmodels?Buildingdomain-specific models should benefit from insights andinvestments into general-purpose models. Thereare several strategies: training domain-specificmodels from scratch (Taylor et al., 2022; Boltonet al., 2023), mixing general and domain-specificdata (Wu et al., 2023), and fine-tuning existingmodels (Singhal et al., 2022, 2023).Focusingon domain-specific needs,applications,andknowledge with guidance from topic experts willbenefit us in acquiring a better model for specificNLP tasks. Which approach yields the best resultsfor task performance and overall cost? What is the role of in-context learning and fine-tuning?Both LIMA (Zhou et al., 2023) and Med-PaLM (Singhal et al., 2022) use a small numberof examples to tune a model. With expanding con-text size, we may soon rely entirely on in-contextlearning (Petroni et al., 2020). This blurs the linesbetween changing model parameters and condi-tioning during inference. Beyond inference speedtradeoffs between the two, there may be value intuning on tens of thousands (or more) of exam-ples. Which domain-specific examples are the mosteffective to include and in what manner? How can LLMs be integrated with domain-specific knowledge?Specialized knowledge iskey in many domains. RAG (Lewis et al., 2020b;Guu et al., 2020) and KILT-derived works (Petroniet al., 2021) focus on knowledge-intensive tasksby including retrieval steps. Work on attributedQA (Bohnet et al., 2022) takes a similar approach,as do search LLMs that require interaction withretrieved data (Nakano et al., 2021). Rich updatedknowledge sources will always exist beyond themodel, especially in environments like medicine,finance, and many academic disciplines.",
  "Evaluation of Domain-Specific Models": "The evaluation of NLP systems is at a crossroads,and the downstream usage of LLMs and evaluationapproaches have diverged. Benchmarks assumethat their results translate to insights into similartasks and usefulness for commercial applications.But benchmarks have become increasingly narrow in scope, oftentimes assessing one metric on a sin-gle, often flawed, dataset (Mitchell et al., 2019;Kiela et al., 2021; Ethayarajh and Jurafsky, 2020).The primary evaluation approach for LLMs hasbeen to evaluate on a broad set of these narrowbenchmarks (Liang et al., 2022, HELM) (Srivas-tava et al., 2022, BIG-Bench). High average per-formance argues for a broad range of capabilities;however, one size may not fit all. Since specificuses of LLMs are typically much more narrow, weidentify three major issues and associated researchopportunities with this approach. Depth-first EvaluationCurrent approaches fo-cus on a single model doing everything well onaverage instead of being useful in a single do-main. However, it is widely acknowledged thatthe standard benchmarks for most tasks are insuffi-cient (e.g., for summarization, Fabbri et al., 2021;Goyal et al., 2022). Task-specific evaluations havethus adopted additional protocols that measure howwell models transfer to different domains, how ro-bust they are, and whether they stand up to con-cept drift (Mille et al., 2021; Dhole et al., 2021).These details disappear when benchmarking on100+ tasks. Yet, a models usefulness is not solelydefined by doing okay on everything but rather byhow well it performs in specific and narrow tasksthat provide value. This value is only realized if themodel does not suffer from catastrophic failures.Exemplar studies that perform deep dives onLLMs for specific tasks exist in healthcare (Zacket al., 2024; Eriksen et al., 2023; Ayers et al., 2023;Han et al., 2024; Chen et al., 2024; Strong et al.,2023), law (Blair-Stanek et al., 2023b,a; Mageshet al., 2024), and physics (Kim et al., 2024), amongother areas. We encourage more work on eval-uation practices for specific tasks that can han-dle various model setups and yield informativeinsights (Zhang et al., 2023; Liang et al., 2022). Sound MetricsFor convenience, most bench-mark tasks are formulated as multiple choice ques-tion answering or classification. This is not howLLMs are often used. For much more common gen-eration tasks, researchers have been ringing alarmsabout broken evaluations (Gehrmann et al., 2023).It is dubious whether we gain insights into non-task-specific generation through NLU benchmarks.If we are performing the depth-first evaluation ofa generation task, a remaining hurdle and whyresearchers fall back to NLU tasks is the lack ofrobust metrics. While there is much recent work on better metrics (Celikyilmaz et al., 2020; Gehrmannet al., 2023), a troubling trend is the use of LLMs asevaluators (e.g., Sellam et al., 2020; Chiang et al.,2023). This approach poses many risks, includingthe implicit assumption that the evaluating modelhas access to the ground truth judgment. Whilethere are some promising results, using an LLMout of the box should be avoided (e.g., Wang et al.,2023a,b). Moreover, it is unclear how to evaluatethe evaluator when it is a non-deterministic API, orhow to scale the development of learned metricsand quantify the strength of a metric. Products are not BaselinesIf we really do wantto evaluate 100+ tasks, there are many issues withthe soundness of evaluation setups. At this scope,it is impossible to run careful ablation studies orto assess the effect of changes to methodologyin a causal manner. Moreover, different LLMsrespond differently to prompts.The BLOOMevaluation averaged over multiple prompts andfound significant variance (Workshop et al., 2022).This variance leads to a lack of reproducibility:LLaMA (Touvron et al., 2023a) claimed highMMLU (Hendrycks et al., 2021) performance butdidnt release the prompts that led to them. 3 Sim-ilarly, the evaluation scheme makes a difference(Liang et al., 2022, ). High evaluation costsmean benchmarks pick a small number of setups(sometimes only one) for each task, which intro-duces further bias, making it hard to construct fairbenchmarks on many tasks.An additional issue with the current benchmark-ing approach is that the best-performing models areoften commercial APIs. With limited transparencyregarding data and training, we cannot fairly evalu-ate these models (e.g., data leakage). Furthermore,task-specific tuning may have been selected basedon these specific benchmarks. Moreover, the un-derlying models change frequently, so it is unclearwhether a result will hold for long.These evaluation issues prompt significant openquestions: 1) How do we develop consistent evalu-ation setups across models that give true measuresof performance? 2) How do we develop evaluationsetups and metrics more closely aligned with down-stream usage? 3) How do we develop evaluationsuites that support depth-first evaluation and notbreadth-first benchmarking?",
  "The Role of Academics": "A focus on general-purpose LLMs has forced aca-demics to work with large base models and perhaps,shifted the focus to solve problems of immediateindustrial interest. Many academics feel excludedfrom current research trends (Ignat et al., 2024) andthe academic and industry relationship is chang-ing (Littman et al., 2022). Shifting attention backto domain-specific applications emphasizes areaswhere academics hold an advantage: partnershipswith domain experts to invest in specific tasks, andconsideration of broader societal needs.Developing domain-specific models requires do-main expertise and universities are diverse aca-demic environments that house experts in manydomains. Collaborations with these experts canidentify data sources, tasks, and challenges impor-tant within each domain. Furthermore, these collab-orations are the best avenues for better alignmentof evaluations with use cases (Winata et al., 2024),and can support the development of proper met-rics. These collaborations are necessary to explorewide open interdisciplinary topics, such as modelsfor protein structure prediction (Tunyasuvunakoolet al., 2021; Vig et al., 2021) and games as proxiesfor reasoning (Silver et al., 2016; Agostinelli et al.,2019; Schrittwieser et al., 2020). This includes de-veloping domain-specific resources, which requiredomain experts to properly design and constructthe datasets. Further, areas where industry underin-vests are those where academics could focus atten-tion. For example, low-resource languages are notserved by a general-purpose multilingual LLM, norwill we reasonably have enough data to support cur-rent LLM training methods. Dialects and variationsin languages are still wide open topics (Aji et al.,2022; Winata et al., 2023; Nicholas and Bhatia,2023).General-purpose LLMs are unlikely to solveproblems in many important domains, with manyopen research problems that can only be solved bydomain-specific approaches. Focusing on domain-specific knowledge will benefit us in acquiringa better model and developing application strate-gies more aligned with how humans learn domain-specific knowledge (Tricot and Sweller, 2014). Formany interdisciplinary areas, subject matter expertsare essential, and the problems must be definedclearly. The first pass from an LLM is often im-pressive, but it hides the trenches and areas wherethings are most interesting. We need a renewed fo- cus on developing and evaluating domain-specificmodels and applications, an area where academicscan play a leading role. Let us not be distractedby claims that a single model solves all tasks, andinstead deeply explore and understand the needsand challenges of specific domains.",
  "Forest Agostinelli,Stephen McAleer,AlexanderShmakov, and Pierre Baldi. 2019. Solving the Ru-biks Cube with deep reinforcement learning andsearch. Nature Machine Intelligence, 1(8):356363": "Alham Aji, Genta Indra Winata, Fajri Koto, SamuelCahyawijaya, Ade Romadhony, Rahmad Mahendra,Kemal Kurniawan, David Moeljadi, Radityo Eko Pra-sojo, Timothy Baldwin, et al. 2022. One country,700+ languages: NLP challenges for underrepre-sented languages and dialects in Indonesia. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 72267249. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023. Palm 2 technical report. arXivpreprint arXiv:2305.10403. John W Ayers, Adam Poliak, Mark Dredze, Eric CLeas, Zechariah Zhu, Jessica B Kelley, Dennis JFaix, Aaron M Goodman, Christopher A Longhurst,Michael Hogarth, et al. 2023. Comparing physicianand artificial intelligence chatbot responses to pa-tient questions posted to a public social media forum.JAMA Internal Medicine, 183(6):589596. Andrew Blair-Stanek, Nils Holzenberger, and BenjaminVan Durme. 2023a.Can gpt-3 perform statutoryreasoning? In Proceedings of the Nineteenth Interna-tional Conference on Artificial Intelligence and Law,pages 2231.",
  "Andrew Blair-Stanek, Nils Holzenberger, and BenjaminVan Durme. 2023b. Openai cribbed our tax exam-ple, but can gpt-4 really do tax?arXiv preprintarXiv:2309.09992": "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni,Daniel Andor, Livio Baldini Soares, Jacob Eisen-stein, Kuzman Ganchev, Jonathan Herzig, Kai Hui,Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster,William W. Cohen, Michael Collins, Dipanjan Das,Donald Metzler, Slav Petrov, and Kellie Webster.2022. Attributed question answering: Evaluationand modeling for attributed large language models.CoRR, abs/2212.08037.",
  "Hanjie Chen, Zhouxiang Fang, Yash Singla, and MarkDredze. 2024. Benchmarking large language modelson answering and explaining challenging medicalquestions. arXiv preprint arXiv:2402.18060": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023), 2(3):6. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. CoRR, abs/2204.02311. Hyung Won Chung, Le Hou, Shayne Longpre, Bar-ret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv, 2210.11416.",
  "Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, ZheZhao, Neil Houlsby, Fernando Diaz, Donald Metzler,and Oriol Vinyals. 2021. The benchmark lottery.CoRR, abs/2107.07002": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. KaustubhD.Dhole,VarunGangal,SebastianGehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-hamood, Abinaya Mahendiran, Simon Mille, AshishSrivastava, Samson Tan, Tongshuang Wu, JaschaSohl-Dickstein, Jinho D. Choi, Eduard H. Hovy,Ondrej Dusek, Sebastian Ruder, Sajant Anand, Na-gender Aneja, Rabin Banjade, Lisa Barthe, HannaBehnke, Ian Berlot-Attwell, Connor Boyle, Car-oline Brun, Marco Antonio Sobrevilla Cabezudo,Samuel Cahyawijaya, Emile Chapuis, WanxiangChe, Mukund Choudhary, Christian Clauss, PierreColombo, Filip Cornell, Gautier Dagan, MayukhDas, Tanay Dixit, Thomas Dopierre, Paul-AlexisDray, Suchitra Dubey, Tatiana Ekeinhor, Marco DiGiovanni, Rishabh Gupta, Rishabh Gupta, LouanesHamla, Sang Han, Fabrice Harel-Canada, AntoineHonore, Ishan Jindal, Przemyslaw K. Joniak, DenisKleyko, Venelin Kovatchev, and et al. 2021.Nl-augmenter: A framework for task-sensitive naturallanguage augmentation. CoRR, abs/2112.02721. Jesse Dodge, Maarten Sap, Ana Marasovic, WilliamAgnew, Gabriel Ilharco, Dirk Groeneveld, MargaretMitchell, and Matt Gardner. 2021. Documentinglarge webtext corpora: A case study on the colossalclean crawled corpus. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 12861305.",
  "Alexander V Eriksen, Sren Mller, and Jesper Ryg.2023. Use of gpt-4 to diagnose complex clinicalcases": "Kawin Ethayarajh and Dan Jurafsky. 2020. Utility is inthe eye of the user: A critique of NLP leaderboards.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 48464853, Online. Association for Computa-tional Linguistics. Alexander R. Fabbri, Wojciech Kryscinski, Bryan Mc-Cann, Caiming Xiong, Richard Socher, and DragomirRadev. 2021. SummEval: Re-evaluating summariza-tion evaluation. Transactions of the Association forComputational Linguistics, 9:391409. Iker Garca-Ferrero, Rodrigo Agerri, Aitziber AtutxaSalazar, Elena Cabrio, Iker de la Iglesia, AlbertoLavelli, Bernardo Magnini, Benjamin Molinet, Jo-hana Ramirez-Romero, German Rigau, et al. 2024.Medical mT5: an open-source multilingual text-to-text LLM for the medical domain. arXiv preprintarXiv:2404.07613. Sebastian Gehrmann, Elizabeth Clark, and Thibault Sel-lam. 2023. Repairing the cracked foundation: Asurvey of obstacles in evaluation practices for gener-ated text. Journal of Artificial Intelligence Research,77:103166.",
  "News summarization and evaluation in the era ofGPT-3. CoRR, abs/2209.12356": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-gia, Rodney Kinney, Oyvind Tafjord, Ananya HarshJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,et al. 2024. Olmo: Accelerating the science of lan-guage models. arXiv preprint arXiv:2402.00838. SuchinGururangan,AnaMarasovic,SwabhaSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,and Noah A Smith. 2020. Dont stop pretraining:Adapt language models to domains and tasks. InProceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages83428360. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,and Ming-Wei Chang. 2020. Retrieval augmentedlanguage model pre-training. In Proceedings of the37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, volume119 of Proceedings of Machine Learning Research,pages 39293938. PMLR.",
  "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, andMaosong Sun. 2022. PTR: Prompt tuning with rulesfor text classification. AI Open, 3:182192": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, et al. 2022. An empirical analy-sis of compute-optimal large language model training.Advances in Neural Information Processing Systems,35:3001630030.",
  "Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.2019.Clinicalbert: Modeling clinical notes andpredicting hospital readmission.arXiv preprintarXiv:1904.05342": "Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester,Santiago Castro, Naihao Deng, Xinyi Gao, Aylin EceGunal, Jacky He, Ashkan Kazemi, et al. 2024. Hasit all been solved? open nlp research questions notsolved by large language models. In Proceedings ofthe 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Evalu-ation (LREC-COLING 2024), pages 80508094.",
  "Frederick Jelinek. 1976. Continuous speech recognitionby statistical methods.Proceedings of the IEEE,64(4):532556": "Nitish Shirish Keskar, Bryan McCann, Lav R Varshney,Caiming Xiong, and Richard Socher. 2019. Ctrl: Aconditional transformer language model for control-lable generation. arXiv preprint arXiv:1909.05858. Douwe Kiela, Max Bartolo, Yixin Nie, DivyanshKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-shia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,Zeerak Waseem, Pontus Stenetorp, Robin Jia, MohitBansal, Christopher Potts, and Adina Williams. 2021.Dynabench: Rethinking benchmarking in NLP. InProceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 41104124, Online. Association for Computa-tional Linguistics. Eun-Ah Kim, Haining Pan, Nayantara Mudur, WilliamTaranto, Subhashini Venugopalan, Yasaman Bahri,and Michael Brenner. 2024. Performing Hartree-Fock many-body physics calculations with large lan-guage models. Bulletin of the American PhysicalSociety. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen,Franck Dernoncourt, Ryan Rossi, and Thien Nguyen.2023. Okapi: Instruction-tuned large language mod-els in multiple languages with reinforcement learningfrom human feedback. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing: System Demonstrations, pages318327. Eric Lehman, Evan Hernandez, Diwakar Mahajan,Jonas Wulff, Micah J Smith, Zachary Ziegler, DanielNadler, Peter Szolovits, Alistair Johnson, and EmilyAlsentzer. 2023. Do we still need clinical languagemodels?In Conference on health, inference, andlearning, pages 578597. PMLR. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020a.BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 78717880. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-tus, Fabio Petroni, Vladimir Karpukhin, NamanGoyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih,Tim Rocktschel, Sebastian Riedel, and DouweKiela. 2020b. Retrieval-augmented generation forknowledge-intensive NLP tasks. In Advances in Neu-ral Information Processing Systems. Percy Liang, Rishi Bommasani, Tony Lee, DimitrisTsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-mar, Benjamin Newman, Binhang Yuan, Bobby Yan,Ce Zhang, Christian Cosgrove, Christopher D. Man-ning, Christopher R, Diana Acosta-Navas, Drew A.Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak,Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,Keshav Santhanam, Laurel J. Orr, Lucia Zheng, MertYksekgnl, Mirac Suzgun, Nathan Kim, NeelGuha, Niladri S. Chatterji, Omar Khattab, PeterHenderson, Qian Huang, Ryan Chi, Sang MichaelXie, Shibani Santurkar, Surya Ganguli, TatsunoriHashimoto, Thomas Icard, Tianyi Zhang, VishravChaudhary, William Wang, Xuechen Li, Yifan Mai,Yuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-uation of language models. CoRR, abs/2211.09110. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, TianluWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-man Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.Few-shot learning with multilingual language models.arXiv preprint arXiv:2112.10668. Michael L Littman, Ifeoma Ajunwa, Guy Berger, CraigBoutilier, Morgan Currie, Finale Doshi-Velez, GillianHadfield, Michael C Horowitz, Charles Isbell, Hi-roaki Kitano, et al. 2022. Gathering strength, gather-ing storms: The one hundred year study on artificialintelligence (ai100) 2021 study panel report. arXivpreprint arXiv:2210.15767. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing.ACM Computing Surveys, 55(9):135. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.Transac-tions of the Association for Computational Linguis-tics, 8:726742.",
  "Roberta: A robustly optimized BERT pretrainingapproach. arXiv": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The FLANcollection: Designing data and methods for effectiveinstruction tuning. arXiv, 2301.13688. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, ShengZhang, Hoifung Poon, and Tie-Yan Liu. 2022.Biogpt:generative pre-trained transformer forbiomedical text generation and mining. Briefingsin bioinformatics, 23(6):bbac409. Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suz-gun, Christopher D Manning, and Daniel E Ho.2024. Hallucination-free? assessing the reliabilityof leading ai legal research tools. arXiv preprintarXiv:2405.20362.",
  "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013.Efficient estimation of wordrepresentations in vector space.arXiv preprintarXiv:1301.3781": "Simon Mille, Kaustubh D. Dhole, Saad Mahamood,Laura Perez-Beltrachini, Varun Gangal, Mihir San-jay Kale, Emiel van Miltenburg, and SebastianGehrmann. 2021. Automatic construction of eval-uation suites for natural language generation datasets.In Advances in Neural Information Processing Sys-tems. Margaret Mitchell, Simone Wu, Andrew Zaldivar,Parker Barnes, Lucy Vasserman, Ben Hutchinson,Elena Spitzer, Inioluwa Deborah Raji, and TimnitGebru. 2019. Model cards for model reporting. InProceedings of the Conference on Fairness, Account-ability, and Transparency, FAT* 2019, Atlanta, GA,USA, January 29-31, 2019, pages 220229. ACM. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,Long Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021.Webgpt: Browser-assisted question-answering with human feedback.arXiv preprintarXiv:2112.09332.",
  "Matthew E. Peters, Mark Neumann, Mohit Iyyer, MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer. 2018. Deep contextualized word repre-sentations. arXiv, 1802.05365": "Fabio Petroni, Patrick Lewis, Aleksandra Piktus, TimRocktschel, Yuxiang Wu, Alexander H. Miller, andSebastian Riedel. 2020. How context affects lan-guage models factual predictions. In AutomatedKnowledge Base Construction. Fabio Petroni, Aleksandra Piktus, Angela Fan, PatrickLewis, Majid Yazdani, Nicola De Cao, James Thorne,Yacine Jernite, Vladimir Karpukhin, Jean Maillard,Vassilis Plachouras, Tim Rocktschel, and SebastianRiedel. 2021. KILT: a benchmark for knowledgeintensive language tasks. In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 25232544, Online.Association for Computational Linguistics.",
  "Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,and Timothy P Lillicrap. 2019. Compressive trans-formers for long-range sequence modelling. arXivpreprint arXiv:1911.05507": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hu-bert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis,Thore Graepel, et al. 2020. Mastering Atari, Go,Chess and Shogi by planning with a learned model.Nature, 588(7839):604609.",
  "Elliot Schumacher and Mark Dredze. 2019. Learningunsupervised contextual representations for medicalsynonym discovery. JAMIA Open, 2(4):538546": "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.BLEURT: Learning robust metrics for text genera-tion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78817892. David Silver, Aja Huang, Chris J Maddison, ArthurGuez, Laurent Sifre, George Van Den Driessche, Ju-lian Schrittwieser, Ioannis Antonoglou, Veda Pan-neershelvam, Marc Lanctot, et al. 2016. Masteringthe game of Go with deep neural networks and treesearch. Nature, 529(7587):484489. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mah-davi, Jason Wei, Hyung Won Chung, Nathan Scales,Ajay Kumar Tanwani, Heather Cole-Lewis, StephenPfohl, Perry Payne, Martin Seneviratne, Paul Gamble,Chris Kelly, Nathaneal Schrli, Aakanksha Chowdh-ery, Philip Andrew Mansfield, Blaise Agera y Arcas,Dale R. Webster, Gregory S. Corrado, Yossi Matias,Katherine Chou, Juraj Gottweis, Nenad Tomasev,Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christo-pher Semturs, Alan Karthikesalingam, and VivekNatarajan. 2022.Large language models encodeclinical knowledge. CoRR, abs/2212.13138. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,Heather Cole-Lewis, Darlene Neal, Mike Schaeker-mann, Amy Wang, Mohamed Amin, Sami Lachgar,Philip Andrew Mansfield, Sushant Prakash, BradleyGreen, Ewa Dominowska, Blaise Agera y Arcas,Nenad Tomasev, Yun Liu, Renee Wong, Christo-pher Semturs, S. Sara Mahdavi, Joelle K. Barral,Dale R. Webster, Gregory S. Corrado, Yossi Matias,Shekoofeh Azizi, Alan Karthikesalingam, and VivekNatarajan. 2023. Towards expert-level medical ques-tion answering with large language models. CoRR,abs/2305.09617. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, AdamFisch, Adam R. Brown, Adam Santoro, AdityaGupta, Adri Garriga-Alonso, Agnieszka Kluska,Aitor Lewkowycz, Akshat Agarwal, Alethea Power,Alex Ray, Alex Warstadt, Alexander W. Kocurek,Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-rish, Allen Nie, Aman Hussain, Amanda Askell,Amanda Dsouza, Ameet Annasaheb Rahane, Anan-tharaman S. Iyer, Anders Andreassen, Andrea San-tilli, Andreas Stuhlmuller, Andrew M. Dai, An-drew D. La, Andrew Kyle Lampinen, Andy Zou,Angela Jiang, Angelica Chen, Anh Vuong, Ani-mesh Gupta, Anna Gottardi, Antonio Norelli, AnuVenkatesh, Arash Gholamidavoodi, Arfa Tabassum,Arul Menezes, Arun Kirubarajan, Asher Mullokan-dov, Ashish Sabharwal, Austin Herrick, Avia Efrat,Aykut Erdem, Ayla Karakacs, Bridget R. Roberts,Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski,Batuhan Ozyurt, Behnam Hedayatnia, BehnamNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-mekci, Bill Yuchen Lin, Blake Stephen Howald,Cameron Diao, Cameron Dour, Catherine Stinson,Cedrick Argueta, Cesar Ferri Ramirez, ChandanSingh, Charles Rathkopf, Chenlin Meng, ChittaBaral, Chiyu Wu, Chris Callison-Burch, Chris Waites,Christian Voigt, Christopher D. Manning, Christo-pher Potts, Cindy Tatiana Ramirez, Clara Rivera,Clemencia Siro, Colin Raffel, Courtney Ashcraft,Cristina Garbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, DanielFreeman, Daniel Khashabi, Daniel Levy, DanielGonzalez, Danny Hernandez, Danqi Chen, DaphneIppolito, Dar Gilboa, David Dohan, D. Drakard,David Jurgens, Debajyoti Datta, Deep Ganguli, De-nis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,Derek Tam, Dieuwke Hupkes, Diganta Misra, Dil-yar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk,Elad Segal, Eleanor Hagerman, Elizabeth Barnes,Elizabeth P. Donoway, Ellie Pavlick, EmanueleRodol, Emma FC Lam, Eric Chu, Eric Tang,Erkut Erdem, Ernie Chang, Ethan A. Chi, EthanDyer, Ethan J. Jerzak, Ethan Kim, Eunice EngefuManyasi, Evgenii Zheltonozhskii, Fan Xia, Fate-meh Siar, Fernando Martinez-Plumed, FrancescaHappe, Franois Chollet, Frieda Rong, GauravMishra, Genta Indra Winata, Gerard de Melo, Ger-mn Kruszewski, Giambattista Parascandolo, Gior-gio Mariani, Gloria Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galija-sevic, Han Sol Kim, Hannah Rashkin, Hanna Ha-jishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,Hinrich Schtze, Hiromu Yakura, Hongming Zhang,Hubert Wong, Ian Aik-Soon Ng, Isaac Noble, JaapJumelet, Jack Geissinger, John Kernion, Jacob Hilton,Jaehoon Lee, Jaime Fernndez Fisac, J. BrookerSimon, James Koppel, James Zheng, James Zou,Jan Kocon, Jana Thompson, Jared Kaplan, JaremaRadom, Jascha Narain Sohl-Dickstein, Jason Phang,Jason Wei, Jason Yosinski, Jekaterina Novikova,Jelle Bosscher, Jenni Marsh, Jeremy Kim, JeroenTaal, Jesse Engel, Jesujoba Oluwadara Alabi, Ji-acheng Xu, Jiaming Song, Jillian Tang, Jane WWaweru, John Burden, John Miller, John U. Balis,Jonathan Berant, Jorg Frohberg, Jos Rozen, JosHernndez-Orallo, Joseph Boudeman, Joseph Jones,Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua,Kamil Kanclerz, Karen Livescu, Karl Krauth, KarthikGopalakrishnan, Katerina Ignatyeva, Katja Markert,Kaustubh D. Dhole, Kevin Gimpel, Kevin OchiengOmondi, Kory Wallace Mathewson, Kristen Chia-fullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-Donell, Kyle Richardson, Laria Reynolds, Leo Gao,Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella,Luca Lam, Lucy Noble, Ludwig Schmidt, LuhengHe, Luis Oliveros Colon, Luke Metz, Lutfi KeremcSenel, Maarten Bosma, Maarten Sap, Maartje terHoeve, Madotto Andrea, Maheen Saleem Farooqi,Manaal Faruqui, Mantas Mazeika, Marco Baturan,Marco Marelli, Marco Maru, M Quintana, MarieTolkiehn, Mario Giulianelli, Martha Lewis, MartinPotthast, Matthew Leavitt, Matthias Hagen, MatyasSchubert, Medina Baitemirova, Melissa Arnaud,Melvin Andrew McElrath, Michael A. Yee, MichaelCohen, Mi Gu, Michael I. Ivanitskiy, Michael Star-ritt, Michael Strube, Michal Swkedrowski, MicheleBevilacqua, Michihiro Yasunaga, Mihir Kale, MikeCain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mo-hit Bansal, Moin Aminnaseri, Mor Geva, MozhdehGheini, T MukundVarma, Nanyun Peng, NathanChi, Nayeon Lee, Neta Gur-Ari Krakover, NicholasCameron, Nicholas S. Roberts, Nicholas Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff,Nitish Shirish Keskar, Niveditha Iyer, Noah Con-stant, Noah Fiedel, Nuan Wen, Oliver Zhang, OmarAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,Pablo Antonio Moreno Casares, Parth Doshi, Pas-cale Fung, Paul Pu Liang, Paul Vicol, PegahAlipoormolabashi, Peiyuan Liao, Percy Liang, Pe-ter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S. Patil, PouyaPezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU,Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph,Raefer Gabriel, Rahel Habacker, Ramon RiscoDelgado, Raphal Millire, Rhythm Garg, RichardBarnes, Rif A. Saurous, Riku Arakawa, RobbeRaymaekers, Robert Frank, Rohan Sikand, RomanNovak, Roman Sitelew, Ronan Le Bras, RosanneLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-dinov, Ryan Chi, Ryan Lee, Ryan Stovall, RyanTeehan, Rylan Yang, Sahib J. Singh, Saif M. Mo-hammad, Sajant Anand, Sam Dillavou, Sam Shleifer,Sam Wiseman, Samuel Gruetter, Sam Bowman,Samuel S. Schoenholz, Sanghyun Han, SanjeevKwatra, Sarah A. Rous, Sarik Ghazarian, SayanGhosh, Sean Casey, Sebastian Bischoff, SebastianGehrmann, Sebastian Schuster, Sepideh Sadeghi,Shadi S. Hamdan, Sharon Zhou, Shashank Srivas-tava, Sherry Shi, Shikhar Singh, Shima Asaadi,Shixiang Shane Gu, Shubh Pachchigar, ShubhamToshniwal, Shyam Upadhyay, Shyamolima Deb-nath, Siamak Shakeri, Simon Thormeyer, SimoneMelzi, Siva Reddy, Sneha Priscilla Makini, Soohwan Lee, Spencer Bradley Torene, Sriharsha Hat-war, Stanislas Dehaene, Stefan Divic, Stefano Ermon,Stella Rose Biderman, Stephanie C. Lin, S. Prasad,Steven T. Piantadosi, Stuart M. Shieber, SummerMisherghi, Svetlana Kiritchenko, Swaroop Mishra,Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A.Ali, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes,Theodore Rothschild, Thomas Phan, Tianle Wang,Tiberius Nkinyili, Timo Schick, T. N. Kornev, Tim-othy Telleen-Lawton, Titus Tunduny, Tobias Ger-stenberg, Trenton Chang, Trishala Neeraj, TusharKhot, Tyler OBrien Shultz, Uri Shaham, VedantMisra, Vera Demberg, Victoria Nyamai, Vikas Rau-nak, Vinay Venkatesh Ramasesh, Vinay Uday Prabhu,Vishakh Padmakumar, Vivek Srikumar, William Fe-dus, William Saunders, William Zhang, W Vossen,Xiang Ren, Xiaoyu Tong, Xinyi Wu, Xudong Shen,Yadollah Yaghoobzadeh, Yair Lakretz, Yang Song,Yasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao,Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yun-tao Bai, Zachary Seid, Zhao Xinran, Zhuoye Zhao,Zi Fu Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu,Sahib Singh, and Uri Shaham. 2022. Beyond the imi-tation game: Quantifying and extrapolating the capa-bilities of language models. arXiv, abs/2206.04615. Eric Strong, Alicia DiGiammarino, Yingjie Weng, An-dre Kumar, Poonam Hosamani, Jason Hom, andJonathan H Chen. 2023. Chatbot vs medical studentperformance on free-response clinical reasoning ex-aminations. JAMA Internal Medicine, 183(9):10281030. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc Le, Ed Chi, DennyZhou, et al. 2023. Challenging big-bench tasks andwhether chain-of-thought can solve them. In Find-ings of the Association for Computational Linguistics:ACL 2023, pages 1300313051.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Ross Taylor, Marcin Kardas, Guillem Cucurull, ThomasScialom, Anthony Hartshorn, Elvis Saravia, An-drew Poulton, Viktor Kerkez, and Robert Stojnic.2022. Galactica: A large language model for science.CoRR, abs/2211.09085. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Andr Tricot and John Sweller. 2014. Domain-specificknowledge and why teaching generic skills does notwork. Educational Psychology Review, 26:265283": "Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu,Tim Green, Michal Zielinski, Augustin dek, AlexBridgland, Andrew Cowie, Clemens Meyer, AgataLaydon, et al. 2021. Highly accurate protein struc-ture prediction for the human proteome.Nature,596(7873):590596. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, pages 59986008. Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong,richard socher, and Nazneen Rajani. 2021. BERTol-ogy meets biology: Interpreting attention in proteinlanguage models. In International Conference onLearning Representations.",
  "Iz Beltagy, et al. 2023b. How far can camels go?Exploring the state of instruction tuning on open re-sources. arXiv preprint arXiv:2306.04751": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V Le. 2022. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations. Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-jaya, Rahmad Mahendra, Fajri Koto, Ade Romad-hony, Kemal Kurniawan, David Moeljadi, Radi-tyo Eko Prasojo, Pascale Fung, et al. 2023. NusaX:Multilingual parallel sentiment dataset for 10 Indone-sian local languages. In Proceedings of the 17th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 815834. Genta Indra Winata, Hanyang Zhao, Anirban Das, Wen-pin Tang, David D Yao, Shi-Xiong Zhang, and Sam-bit Sahu. 2024. Preference tuning with human feed-back on language, speech, and vision tasks: A survey.arXiv preprint arXiv:2409.11564. BigScience Workshop, Teven Le Scao, Angela Fan,Christopher Akiki, Ellie Pavlick, Suzana Ilic, DanielHesslow, Roman Castagn, Alexandra Sasha Luc-cioni, Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-badur, David Rosenberg, and Gideon Mann. 2023.BloombergGPT: A large language model for finance.arXiv preprint arXiv:2303.17564. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mt5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498. Travis Zack, Eric Lehman, Mirac Suzgun, Jorge A Ro-driguez, Leo Anthony Celi, Judy Gichoya, Dan Ju-rafsky, Peter Szolovits, David W Bates, Raja-Elie EAbdulnour, et al. 2024. Assessing the potential ofGPT-4 to perpetuate racial and gender biases in healthcare: A model evaluation study. The Lancet DigitalHealth, 6(1):e12e22. Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, JohnEtchemendy, Deep Ganguli, Barbara Grosz, TerahLyons, James Manyika, Juan Carlos Niebles, MichaelSellitto, et al. 2021. The AI index 2021 annual report.arXiv preprint arXiv:2103.06312. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068."
}