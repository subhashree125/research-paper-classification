{
  "Abstract": "Word sense disambiguation (WSD) is a keytask in natural language processing and lexicalsemantics. Pre-trained language models withcontextualized word embeddings have signifi-cantly improved performance in regular WSDtasks. However, these models still strugglewith recognizing semantic boundaries and of-ten misclassify homonyms in adversarial con-text. Therefore, we propose FOOL: FOur-foldObscure Lexical, a new coarse-grained WSDdataset, which includes four different test setsdesigned to assess the robustness of languagemodels in WSD tasks. Two sets feature typicalWSD scenarios, while the other two includesentences with opposing contexts to challengethe models further. We tested two types of models on the proposeddataset: models with encoders, such as theBERT and T5 series of varying sizes by prob-ing their embeddings, and state-of-the-art largedecoder models like GPT-4o and the Llama3family, using zero shot prompting. Across dif-ferent state-of-the-art language models, we ob-served a decrease in performance in the lattertwo sets compared to the first two, with somemodels being affected more than others. Weshow interesting findings where small modelslike T5-large and BERT-large performed betterthan GPT-4o on Set 3 of the dataset. This in-dicates that, despite excelling in regular WSDtasks, these models still struggle to correctlydisambiguate homonyms in artificial (Set 3) orrealistic adversarial contexts (Set 4).",
  "*These authors contributed equally to this work": "tasks like machine translation, text annotation, andquestion answering (Agirre and Edmonds, 2007).In order to comprehend the intended meaning ofhomonyms, it is necessary to consider the context,in which they are used. Consequently, the accuratedisambiguation of homonyms provides evidence ofthe models comprehension of the context and, inturn, of language.Contextualized language models, such as BERT(Devlin et al., 2019), produce word embeddingsthat reflect the words meaning based on its con-text (Wiedemann et al., 2019). This has led tosignificant improvements in WSD performance inboth fine-grained or coarse-grained WSD (Wiede-mann et al., 2019; Reif et al., 2019; Loureiro et al.,2021).While fine-grained WSD addresses thenuanced senses a word can have, coarse-grainedWSD focuses on broader, unrelated word mean-ings (Haber and Poesio, 2024). The emergenceof context-based language models suggests thatthe challenge of regular WSD has largely beenresolved. However, it is still unclear if these mod-els can understand context well enough to disam-biguate homonyms effectively. Let us consider thefollowing sentence:",
  "\"I eat an apple while holding my iPhone.\"": "For a human it is clear that \"apple\" refers to thefruit, and not the technology company. The ques-tion remains whether todays language models candifferentiate these senses in this adversarial con-text.Even though there are many existing WSDbenchmarks, such as the Unified Evaluation Frame-work by Raganato et al. (2017) or CoarseWSD-20by Loureiro et al. (2021), none of them considersthe distinction between different types of contextnor the use of opposing context in the sentences.For this purpose we introduce FOOL, a coarse-grained WSD dataset that differentiates betweenfour distinct categories of context changes. The",
  ": Example sentences from the dataset for the word apple": "dataset includes one training set and four test setsas illustrated in .* The first two test setsprovide sentences for regular WSD, while the othertwo contain sentences with additional context thatopposes the anticipated meaning of the homonym.This structure allows for the testing of state-of-the-art (SOTA) language models in both regularhomonym disambiguation settings and adversarialcontext settings. Therefore, this dataset can be usedto investigate the robustness of language models todifferent context changes.We investigated two types of language models:models with encoders, from which we probed theirembeddings using kNN algorithm, and state-of-the-art models that we prompted to classify the targetword into one of two possible meanings. Our find-ings indicate that current SOTA models struggle toaccurately disambiguate coarse-grained homonymswhen adversarial contexts are added. We observeda performance decrease across all models whencomparing results from Set 1 with those from Sets3 and 4.In models containing encoders, this effect ismost significant in smaller models like BERT-baseand T5-base, and less significant in larger mod-els like T5-FLAN-xxl. Conversely, advanced andlarger language models such as GPT-3.5 Turbo andLlama3-70b show a dramatic performance declinewhen faced with adversarial context changes, withperformance drops of 25.6% and 10.4%, respec-tively in Set 4 compared to Set 1. However, modelslike GPT-4o exhibit more robustness against re-alistic opposing context examples (Set 4), with aperformance drop of only around 4%, but morevulnerability for adding adversarial adjective (Set3). Additionally, our findings suggest that modelsthat contain encoder, such as those from the BERT",
  "*Thefulldatasetcanbedownloadedfrom:": "or T5 family, tend to perform better in these tasks,specifically in Set 3. For instance, the BERT-largemodel with 340 million parameters outperformedthe Llama3-8b model, which has around 8 billionparameters on both adversarial tasks (Set 3 and 4).In addition, T5-large and BERT-large performedbetter than GPT-4o on Set 3 of the dataset. Toconclude, our contributions can be summarized asfollows:",
  "Related Work": "Word Sense Disambiguation (WSD) is a well-studied task in natural language processing, focus-ing on fine-grained polysemy disambiguation. Themajority of standard WSD benchmarks, such as theUnified Evaluation Framework by Raganato et al.(2017), heavily rely on WordNet (Miller, 1994).This dependence on WordNet, known for its fine-grained classification, poses a challenge even forhumans to distinguish all possible senses. To tacklethis issue, Loureiro et al. (2021) introduced thedataset \"CoarseWSD-20\", which extracts sentencesfrom Wikipedia articles to create a coarse-grainedsense inventory WSD dataset.The performance of pre-trained language modelshas been tested on both fine-grained and coarse-grained datasets. Especially, BERT (Devlin et al., 2019) achieved overall good results with over 94%accuracy in coarse-grained WSD (Loureiro et al.,2021). For example, Du et al. (2019) fine-tunedBERT on a WSD task and tested it on a variety ofdifferent fine-grained WSD benchmarks (Edmondsand Cotton, 2001; Moro and Navigli, 2015; Nav-igli et al., 2013; Pradhan et al., 2007; Snyder andPalmer, 2004), achieving promising results withaccuracies ranging from 74% to 78%.Additionally, without fine-tuning Wiedemannet al. (2019) and Reif et al. (2019) showed thatBERT can effectively perform fine-grained WSDby combining its contextualized word embeddingswith a kNN classification algorithm. Moreover,Loureiro et al. (2021) employed a kNN BERTclassifier and reported human-like performanceon their coarse-grained noun WSD dataset, withover 94% accuracy. More recently, Proietti et al.(2024) tested different BERT-based models, in-cluding BERT (Devlin et al., 2019) and RoBERTa(Liu et al., 2019) on coarse-grained WSD. Theyclustered WordNet senses to match coarse-grainedhomonym sense distinction and found that BERTsaccuracy is as high as 95%.In our work, we conduct an extensive analysison two types of models. On the one hand, we testedmodels that include encoder, such as the BERT andT5 family (Raffel et al., 2020), by probing theirword embeddings. On the other hand, we analyzedstate-of-the-art language models including GPT-3.5 Turbo (OpenAI, 2022), GPT-4 Turbo (OpenAI,2023) , and GPT-4o (OpenAI, 2024), Llama3-8b,Llama3-70b (Meta, 2024) and Mixtral-8x7b (Jianget al., 2024) using prompts in zero shot settings.While Kocon et al. (2023) investigated GPT-3.5sperformance on WSD among other tasks, our worksignificantly differs in that we have created a newcoarse-grained adversarial dataset and tested var-ious models from different families. To the bestof our knowledge, this paper is the first to con-duct an extensive analysis comparing models likeBERT and T5 with models like GPT and Llama onadversarial WSD tasks.Furthermore, it is evident that there is no existingdataset that aligns with the one proposed in this pa-per. Despite this, there have been some attempts totest models on adversarial sentences. For example,Emelin et al. (2020) considered adversarial attacksin WSD. They changed adjectives in sentences infront of homonyms and checked the performancein a machine translation task. These changes leadto translation errors in LSTM (Luong et al., 2015), Transformer (Vaswani et al., 2017) and ConvS2S(Gehring et al., 2017). Inspired by Emelin et al.(2020) approach, we adopted the idea of modifyingadjectives in order to test the resilience of morerecent pre-trained language models based on theircontextualized word embeddings.Moreover, Reif et al. (2019) incorporated oppos-ing context words in their study. In their paper, theauthors analyzed the performance of pre-trainedlanguage models, primarily BERT, on SemCor(Miller et al., 1993), a fine-grained sense dataset.While they succeeded in this task, they also com-bined two sentences with distinct meanings of ahomonym to create sentences with opposing con-texts. Thereby they found a higher number of clas-sification errors than in normal conditions. Thistest was done using fine-grained senses of words.Although this represents a promising initial step,there is a need to further extend this idea. Weanalyze coarse-grained WSD performances of dif-ferent state-of-the-art models beyond BERT andhave developed an entire human-made test set toevaluate our approach.",
  "Dataset": "In this section, we introduce our dataset FOOL,a coarse-grained WSD dataset that is designed todifferentiate between four different categories ofcontext changes. This design allows us to test bothregular homonym disambiguation settings and ad-versarial context settings. Therefore, this datasetserves as a tool to evaluate the robustness of largelanguage models against different context changesand their ability to discern between various coarse-grained homonym senses.",
  "Dataset Split": "In order to assess the efficacy of distinct pre-trainedmodels across different levels of contextual com-plexity, four different sets of sentences were cre-ated, with an additional set designated as the train-ing set. Each set is associated with a specific con-text and serves a unique purpose. Examples fromthese sets are illustrated in .",
  "Set 2: This set extends the sentences fromSet 1 by adding an adjective directly beforethe homonym, which aligns with the antici-pated meaning of the homonym in that sen-tence": "Set 3: This set modifies the sentences fromSet 2 by changing the adjectives preceding thehomonyms. The new adjectives are typicallyassociated with the opposite meaning of thehomonym, introducing an artificial adversarialcontext. Set 4: This set includes sentences that havebeen specifically crafted with realistic con-text that opposes the anticipated meaning ofthe homonym, further challenging the modelsdisambiguation capabilities. While the context provided in Sets 1 and 2 isdesigned to facilitate the models ability to distin-guish between homonym senses, Sets 3 and 4 in-clude adversarial examples to challenge the models.The goal of Set 3, which uses artificial adversarialcontext, is to simulate an adversarial attack thatdoes not have to be realistic but could still foolor confuse the language model. To achieve this,we employed a simple approach that adds an ad-jective (distractor) typically used in the oppositecontext and studied its effect. On the other hand,the goal of Set 4, which uses realistic adversarialcontext, is to simulate a normal sentence that mightbe used in daily conversations but could still be con-sidered adversarial due to its wording (e.g., \"fruitapple\" and \"iPhone\" in the same sentence). Overall,the dataset allows testing models in regular WSDwith coarse-grained homonym senses and assess-ing their response to adversarial examples. Thisdual approach tests not only basic disambiguationcapabilities but also the resilience of models undermore complex and potentially confusing linguisticscenarios.",
  "Statistics": "shows an overview of all words used in thedataset, which comprises 20 homonyms in total.Each homonym is confined to exactly two broadword senses that are unrelated to each other. It iscrucial that in both senses, the word remains a noun,which is essential for the application of adjectives in Sets 2 and 3. The distribution of sentences perword sense is well balanced across each set. InSets 1 to 3 the number of sentences ranges from40 to 60 sentences per word sense in each set. Set4 consists of 25 to 30 sentences per word sense,reflecting the higher complexity and cost associ-ated with creating these sentences. The trainingdata includes 20 to 40 sentences per word sense.This structured approach ensures that each sense isadequately represented and tested throughout thedataset. in the Appendix shows the fullstatistics of the dataset with the number of sen-tences for every word sense in each set is shown.",
  "Data Collection": "The construction of the dataset is mostly done bymanually creating and revising sentences that aresuitable for the desired sense of the homonym. No-tably, Set 4 is entirely crafted by hand to includehomonyms in their anticipated use along with op-posing contexta task that cannot be automatedusing tools like ChatGPT or sourced from existingliterature. This manual approach ensures that thesentences are fluent and meaningful, fulfilling theirintended purpose in the dataset.Thereareninecommonwords(popularhomonyms) between FOOL and CoarseWSD-20by Loureiro et al. (2021), while the other elevenwords are specific to our dataset. For the ninecommon words, we used some of the sentencesfrom the CoarseWSD-20 dataset for the trainingset and our Set 1. However, for words not coveredby CoarseWSD-20, we sourced example sentencesfrom platforms like Word Hippo (Kat IP Pty Ltd)and YourDictionary (LoveToKnow Media), whichwere then adapted to meet our criteria. Addition-ally, Set 1 was generated using both examples fromthese platforms and sentences created with Chat-GPT (OpenAI, 2022) and GPT 4 (OpenAI, 2023).Nevertheless, the adjectives in Set 2 and 3 are man-ually added by humans to ensure a diverse andcontextually appropriate use of adjectives, tailoredto our specific needs. Furthermore, all labels forthe above mentioned sentences were generated byhuman annotators. To summarize the difference be-tween our dataset FOOL and CoarseWSD-20,only the training set and a small subset of Set 1sentences containing the common words (9 words)were adopted from CoarseWSD-20, and everythingelse is specific to our dataset.The dataset was developed by three contributorsto this study, comprising two doctoral candidates",
  ": All homonyms used in the dataset listed with their senses": "and one undergraduate student from the Depart-ment of Cognitive Science. It is noteworthy thatEnglish is not the first language of any of the re-searchers. The workload was evenly distributedbetween one doctoral candidate and the undergrad-uate student, with each responsible for creatingten words for the dataset. Each researcher cross-verified the work of their peers, and the final datasetwas subsequently reviewed by the third doctoralcandidate.",
  "Contextualized Language Models": "For our evaluation we selected a variety of knownlanguage models that are proven to be efficient inWSD tasks. Besides well tested BERT-based mod-els like BERT (Devlin et al., 2019), RoBERTa (Liuet al., 2019), Distil-BERT and Distil-RoBERTa(Sanh et al., 2020), we included T5 (Raffel et al.,2020) and FLAN-T5 (Chung et al., 2022).The T5-based models have an encoder-decoderarchitecture which proved to be useful in differentbenchmark tasks (Raffel et al., 2020). T5 modelshave been pre-trained on 750GB of cleaned data,significantly more than the 16GB and 160GB usedfor BERT and RoBERTa, respectively.To get a comprehensive overview of all models,we tested different sizes from small to xxl in T5and FLAN-T5 and distil, base and large in BERTand RoBERTa. The parameters and embeddingvector sizes are detailed in . All models areutilized in their original, unmodified form from theHuggingFace library (Wolf et al., 2019) and werenot specifically fine-tuned for this purpose.",
  "To evaluate the performance of all models on theintroduced dataset, a binary classification task isemployed. All of the following is performed for": "each model in each set. For each sentence in aset, all words are converted to lower case, and theembedding vector for the homonym is extracted.To ensure the best results, it is recommended tosum and average the word embeddings from thefinal four layers of the encoder in BERT (Loureiroet al., 2021). This approach is also adopted for T5and FLAN-T5 to ensure better comparability. Wevisualized the word embeddings of the homonym\"crane\" produced by BERT-base (first row) andT5-base (second row) in for every testset, with embeddings color-coded by their correctlabel. We include in the Appendix more visualiza-tions of different words and models (figs. 2 to 7)The averaged word embeddings are categorizedinto one of the designated labels using k-nearestneighbor (kNN) algorithm (Cover and Hart, 1967),which uses our training data as a basis for classi-fication. This algorithm takes a plurality vote ofa samples nearest labeled neighbors, in our casek = 3, and decide based on the 3 nearest neigh-bors which sense to assign the homonym to. Testsvarying k showed no significant differences on theoutcome, which is consistent with the findings ofWiedemann et al. (2019). Cosine similarity wasused as the distance measure, and the macro F1-score as the performance measure. The kNN modelwas trained on the averaged embeddings producedby the corresponding model for the Train Set ofour dataset. Accordingly, the k-nearest neighbor(kNN) algorithm is employed to classify the datafrom the four test sets. For each word in a set, theF1-score is calculated and then averaged over allwords in a set, resulting in four different F1-scoresfor each model",
  "T5": ": The visualization depicts the word embeddings of the word \"crane\" produced by BERT-base (first row)and T5-base (second row) for different sentences in each Set 1 to 4. Orange depicts all embeddings with the label\"crane_bird\" and blue all the ones labeled \"crane_machine\". We used tSNE (Van der Maaten and Hinton, 2008) fordimensionality reduction. One can see that the models are able to cluster the different senses in Set 1 and 2, whilethey struggle to differentiate them in Set 3 and 4. all models show good performances in Set 1 andSet 2. Almost all models score higher than 90% inthe first two settings and some T5-based modelseven up to 99%. The T5-based models score ingeneral higher than the BERT-based models withthe same size. Model SizeIn almost all cases, it is noticeablethat as model size increases, so do the performanceacross all four sets. While T5-small achieves only87.7%, T5-xxl shows results as good as 99.3%.This effect is seen in all models except in FLAN-T5-large and FLAN-T5-xl which show worse re-sults in settings 1, and 3 than FLAN-T5-base. SettingsAs previously stated, all models demon-strate a good performance on Sets 1 and 2. How-ever, the performance of the models declines whenevaluated on Sets 3 and 4. A comparison of theresults observed in Set 1 with those in Set 3 re-veals a decline in the F1-Score from 6% to up to11%, even though only one additional adjective isintroduced in this setting. Nevertheless, the per-formance drops from Set 1 to Set 4 are even moresevere, with a decrease ranging from 20% to 33%.The most significant effect is observed in smallermodel sizes, while in larger models, the difference between Set 1 and Set 4 is smaller, with approxi-mately 20%. Overall the best performance is shownin FLAN-T5-xxl which has the best performancein all four settings and one of the smallest perfor-mance drop to Set 4.",
  "Experimental Settings": "We evaluate FOOL, using state-of-the-art largelanguage models including GPT-3.5 Turbo (Ope-nAI, 2022), GPT-4 Turbo (OpenAI, 2023) , andGPT-4o (OpenAI, 2024), Llama3-8b, Llama3-70b(Meta, 2024) and Mixtral-8x7b (Jiang et al., 2024).Since these models are decoder models, we utilizedprompt-based classification for testing. We inputeach sentence from the set and ask the model toclassify the target word by providing two choices.For example, to classify the meaning of the word\"apple\" the prompt for GPT-4o would be: \"In this sentence: She used iCloud to store photosfrom her visit to the apple orchard, ensuring shenever lost a memory, classify the occurrence ofthe word apple for fruit or for a company.Answer only by one of these options: fruit orcompany.\"",
  ": Results (F1-Scores) for all encoder models, including their parameters and embedding sizes, are presented": "The outputs were manually evaluated because,although models like GPT-4 and GPT-4 Turbostrictly adhere to instructions by outputting only\"fruit\" or \"company\" other models such as GPT-3.5Turbo occasionally respond with explanations thatinclude both categories, such as It is obviously notapple the fruit that is meant, but the company com-plicating the extraction of the correct answer. Suchresponses were considered correct if the classifica-tion was accurate. However, outputs that includedboth classes, such as The word apple could meancompany or fruit in this sentence, were markedas incorrect. This manual process was adopted toaddress the inherent tendency of models like GPT-3.5 Turbo, Mixtral, and Llama 3-8b not to strictlyadhere to instructionsa factor unrelated to ourpapers objective. We conducted initial testing withmultiple runs for the same sentences and observedlittle variance; therefore, the reported results arefrom a single run for each word with each model.The manual validation was performed by two ofthe authors, who cross-validated each others work.",
  "Results": "The results in show that state-of-the-artmodels can distinguish perfectly between twohomonyms in a regular context.All modelsscore above 98%, indicating no difficulty in dis-tinguishing homonyms. Adding an adjective to thehomonym makes the performance even better forall models to score almost perfectly with an accu-racy around 99.9% for models like GPT-4o. How-",
  ": F1-Scores showing the performance of largedecoder models on FOOL using prompt-based classifi-cation": "ever, results from Set 3, where only one adversarialadjective is added to the sentences of Set 1, couldfool the models and affect their performance. Forexample, the score of GPT4-o drops from 99.8% toaround 87% showing vulnerability to a simple ad-versarial context change. However, GPT-4o showsmore robustness to a realistic opposing context testin Set 4 with F1-score of 95.6%. In addition, mod-els like GPT-3.5 Turbo, Llama3-8b and Mixtral-7bx8 experience significant performance drops inSet 4 with F1-score around 70%.",
  "Set 1/2 vs. Set 3/4One of the main findings fromthe analysis above is that there is a major perfor-mance gap between Set 1 and Set 2 compared toSet 3 or Set 4. The significant decline in perfor-": "mance observed between Set 1 and Set 3 in theWSD test, despite the only change being the re-placement of one adjective, appears to be out ofproportion. Also the performance decrease in Set 4is disproportionate. Adding opposing yet realisticcontext while still remaining the overall meaningof the homonym can lead to a decrease in the F1-score to up to 30% even for advanced models likeLlama3-8b and GPT-3.5 Turbo. One explanationfor the changing results could be that contextu-alised language models do not pay attention to se-mantic boundaries like Reif et al. (2019) mentionedin their paper about BERT. This could be extendedby the findings of Tang et al. (2018) who state thatlanguage models do not learn which context wordsare useful and pay attention mostly to the homonymitself. Unimportant context words, which humanscan successfully sort out, have a major impact onthe word embedding produced by language models.This could be one factor language models have toimprove in order to achieve human-like results alsoin smaller model sizes. Model SizeAnother finding is the correlation be-tween model sizes and WSD performance in allfour sets. The results indicate a positive correlationbetween model size and F1-score. Larger mod-els with more parameters store more training datainformation and have bigger embedding vectorsthat capture extensive contextual details, improvingdisambiguation. Furthermore, a decline in perfor-mance is observed in Sets 3 and 4, with smallermodels experiencing a larger drop than larger mod-els. This supports the hypothesis that larger modelsare more robust to adversarial attacks. This robust-ness is likely due to larger models ability to recallmore information and recognize different contexts. T5 vs. BERTThe best overall performance isseen in the encoder of FLAN-T5-xxl. In general,the T5-based models show the best overall resultsnot least because of the bigger model sizes. Evenin the base size FLAN-T5 surpasses BERT-largewhich has more model parameters than FLAN-T5.This may suggest that T5-based language modelsare an optimal choice for the task of word sensedisambiguation. One potential explanation for theenhanced performance is that T5 employs a dis-tinct masking approach distinct from BERT. WhileBERT can only mask one word at a time, T5 masksmultiple words at the same time. Additionally, T5was trained on a larger data corpus than BERTwhich could also improve the performance in WSD",
  "since more knowledge about words in different us-ages is collected": "Embeddings vs. Prompt-based ClassificationIn this paper, we tested the performance of twotypes of models: those that include an encoder,which provides bi-directional context of the sen-tence and thus reflects it in their embeddings, andlarge decoder models known for their ability whenprompted. It is evident that having bi-directionalcontext is an advantage, as reflected in the resultswhen comparing models by size. We can see thateven state-of-the-art models like Llama3-8b, whichis trained on around 15 trillion tokens, performworse than T5-large, which is trained on around1 trillion tokens and has approximately ten timesfewer parameters than Llama3-8b. Furthermore,we believe that the bi-directional context abilityof T5 and BERT family models makes them lessvulnerable to simple adversarial context changes,such as altering one adjective in a sentence. Thisis evidenced by the less significant performancedrop in Set 3 compared to decoder models likeLlama3-8b or even GPT4-o. For example, GPT-4os performance drops by about 12% from Set 1 toSet 3, whereas even a simple BERT-base modelsperformance drops only by about 9%. Additionally,the performance of GPT-4o in Set 3 is comparableto that of T5-base and lower than T5-large, whichhave approximately 220 million and 770 millionparameters, respectively. While the types of mod-els were tested differently, one could argue thatencoder models are better suited to these types oftasks. On the other hand, both GPT4o and GPT-4Turbo models show greater robustness in realisticopposing contexts when tested on Set 4. In thisscenario, we believe that the set involves more rea-soning abilities, which some claim these types ofmodels possess, and smaller models like T5-baseand BERT are less equipped for. Error AnalysisIn this section, we analyze themistakes made by the models and identify specificwords that the models struggled to disambiguatein Set 4. There are many factors that affect modelperformance, but we will discuss a few key ones.Firstly, there are words that are predominantly usedin one meaning and less so in another, such as\"digit\". We observed that performance for thesetypes of words is generally lower. Another categoryof challenging words includes those that share sim-ilar contexts across different meanings, like \"gum\"and \"letter\". For instance, \"gum\" in both mean- ings involves the context of the mouth and chewing,making it more difficult for the model to distinguishbetween them. Similarly, \"letter\" involves writingin both contexts. Conversely, for words like \"Java\"where we intended two meaningsJava the pro-gramming language and Java the islandthe mod-els performed well. Even though \"Java the island\"is not widely used, the contexts of the two mean-ings are completely different, making it harder tocreate sentences that fool the models. Additionally,some models exhibit a bias towards a particularmeaning; for example, Mixtral-7bx8 shows a biastowards interpreting \"pitcher\" as a container and\"rock\" as stone. The performance of the models oneach word in Set 3, and 4 is detailed in figures 8and 9 in Appendix A.3.",
  "Conclusion": "In this paper, we introduce FOOL, a new coarse-grained WSD dataset featuring various types ofcontexts, which serves as both a benchmark forassessing model performance on WSD tasks and atool for evaluating context comprehension by mod-els. Our experiments using this dataset demonstratethat SOTA language models still struggle to under-stand context and disambiguate homonyms in thepresence of opposing contexts, compared to theirperformance in regular WSD tasks. This effect ismost prominent not only in smaller models likeBERT-base and T5-base but also in larger modelslike Llama3 and GPT-3.5 Turbo. Among the seriesof models that include an encoder, our results showthat T5, especially FLAN-T5 is a better alterna-tive to BERT-based models. With more than 99%score in Set 1, FLAN-T5-xxl shows human-likedisambiguation skills. Furthermore, we showedthat models incorporating an encoder are less vul-nerable to adversarial addition of context (Set 3)with the best performing model being FLAN-T5-xxl, which outperforms GPT-4o and GPT-4 Turbo.Interestingly, small models like BERT-large andFLAN-T5-base outperform GPT-4o on the sameset. However, these small models struggle withSet 4, which includes realistic opposing contextusage of words, which we believe requires a deeperunderstanding of language and some degree of rea-soning abilities. In the future, we plan to extendthe FOOL dataset to include sentences with fine-grained homonyms to investigate how languagemodels perform on them. Additional adversarialsettings could also be added to further challenge the models, potentially exposing new weaknessesin their contextual understanding and disambigua-tion capabilities. This will provide further insightsinto the limitations of current language models andguide the development of more robust systems.",
  "Limitations": "While our study presents significant findings in thefield of Natural Language Processing, several lim-itations should be acknowledged to contextualizethe results.Our approach deals with homonymous nounsin a coarse-grained manner, which may oversim-plify the complexities of word sense disambigua-tion. Our coarse-grained homonym resolution doesnot consider the nuanced differences between thevarious meanings of a word that are closely relatedto each other; instead, it focuses on only two dis-tinct senses. This limitation might affect the preci-sion of our models understanding and processingof the context. Moreover, the exclusive focus onnouns, while ignoring other word types, such asverbs, adjectives, or adverbs, may result in limitedgeneralizability.Furthermore, one limitation of this paper is thatpart of a subset of the dataset (Set 1) was generatedusing ChatGPT and then used to evaluate the samedata. This approach introduces a bias that maydistort the evaluation results. However, this issue islimited to one of the four sets and affects only oneof the 21 models tested in this study.",
  "Eneko Agirre and Philip Edmonds. 2007.WordSense Disambiguation: Algorithms and Applications.Springer Science & Business Media": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. Preprint, arxiv:2210.11416 [cs].",
  "Janosch Haber and Massimo Poesio. 2024. Polysemyevidence from linguistics, behavioral science, andcontextualized language models. Computational Lin-guistics, pages 167": "Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. Preprint, arXiv:2401.04088.",
  "Kanclerz, et al. 2023. Chatgpt: Jack of all trades,master of none. Information Fusion, 99:101861": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A robustly optimized BERT pretrainingapproach. Preprint, arxiv:1907.11692 [cs]. Daniel Loureiro, Kiamehr Rezaee, Mohammad TaherPilehvar, and Jose Camacho-Collados. 2021. Anal-ysis and evaluation of language models for wordsense disambiguation. Computational Linguistics,47(2):387443.",
  "George A. Miller. 1994. WordNet: A lexical databasefor english. In Human Language Technology: Pro-ceedings of a Workshop held at Plainsboro, NewJersey, March 8-11, 1994": "George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker. 1993.A semantic concordance.In Human Language Technology: Proceedings ofa Workshop Held at Plainsboro, New Jersey, March21-24, 1993. Andrea Moro and Roberto Navigli. 2015. SemEval-2015 task 13: Multilingual all-words sense disam-biguation and entity linking. In Proceedings of the9th International Workshop on Semantic Evaluation(SemEval 2015), pages 288297. Association forComputational Linguistics. Roberto Navigli, David Jurgens, and Daniele Vannella.2013. SemEval-2013 task 12: Multilingual wordsense disambiguation. In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 222231. Association for Computational Lin-guistics.",
  "OpenAI. 2024. GPT-4o": "Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer. 2007. SemEval-2007 task-17: En-glish lexical sample, SRL and all words. In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 8792.Association for Computational Linguistics. Lorenzo Proietti, Stefano Perrella, Simone Tedeschi,Giulia Vulpis, Leonardo Lavalle, Andrea Sanchietti,Andrea Ferrari, and Roberto Navigli. 2024.An-alyzing homonymy disambiguation capabilities ofpretrained language models. In Proceedings of the2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion (LREC-COLING 2024), pages 924938. ELRAand ICCL. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Alessandro Raganato, Jose Camacho-Collados, andRoberto Navigli. 2017. Word sense disambiguation:A unified evaluation framework and empirical com-parison. In Proceedings of the 15th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Volume 1, Long Papers, pages99110. Association for Computational Linguistics. Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda BViegas, Andy Coenen, Adam Pearce, and Been Kim.2019. Visualizing and measuring the geometry ofBERT. In Advances in Neural Information Process-ing Systems, volume 32. Curran Associates, Inc.",
  "Gregor Wiedemann, Steffen Remus, Avi Chawla, andChris Biemann. 2019. Does BERT make any sense?interpretable word sense disambiguation with con-textualized embeddings. Preprint, arxiv:1909.10430[cs]": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,Joe Davison, Sam Shleifer, Patrick von Platen, ClaraMa, Yacine Jernite, Julien Plu, Canwen Xu, TevenLe Scao, Sylvain Gugger, Mariama Drame, QuentinLhoest, and Alexander M. Rush. 2019. Hugging-Faces transformers: State-of-the-art natural languageprocessing. Publication Title: arXiv e-prints ADSBibcode: 2019arXiv191003771W.",
  "WordsSensesDefinitions": "appleapple_apple_inc\"Apple Inc. (formerly Apple Computer, Inc.) is an American multinational corporation andtechnology company headquartered in Cupertino, California, in Silicon Valley.\"apple_fruitthe round fruit of a tree of the rose family, which typically has thin green or red skin andcrisp flesh. bankbank_bank\"a financial establishment that uses money deposited by customers for investment, pays it outwhen required, makes loans at interest, and exchanges currency..\"bank_river\"the land alongside or sloping down to a river or lake..\" batbat_mammal\"a mainly nocturnal mammal capable of sustained flight, with membranous wings that extendbetween the fingers and limbs..\"bat_equipment\"an implement with a handle and a solid surface, typically of wood, used for hitting the ball ingames such as cricket, baseball, and table tennis..\" cellcell_prison\"a small room in which a prisoner is locked up or in which a monk or nun sleeps.\"cell_biology\"the smallest structural and functional unit of an organism, which is typically microscopic andconsists of cytoplasm and a nucleus enclosed in a membrane.",
  "javajava_program\"a general-purpose computer programming language designed to produce programs that willrun on any computer system.\"java_island\"a large island that forms part of Indonesia\"": "letterletter_alphabet\"a character representing one or more of the sounds used in speech; any of the symbols of analphabet.\"letter_mail\"a written, typed, or printed communication, sent in an envelope by post or messenger.\" matchmatch_sports\"a contest in which people or teams compete against each other in a particular sport.\"match_lighter\"a short, thin piece of wood or cardboard used to light a fire, being tipped with a compositionthat ignites when rubbed against a rough surface.\" nailnail_metal\"a small metal spike with a broadened flat head, driven into wood to join things together or toserve as a hook.\"nail_finger\"a horny covering on the upper surface of the tip of the finger and toe in humans and otherprimates.\" pitcherpitcher_jug\"a large, round container for liquids that has a flat base, a handle, and a very narrow raisedopening at the top for pouring\"pitcher_sports\"the player who delivers the ball to the batter.\" pupilpupil_student\"a person who is taught by another, especially a schoolchild or student in relation to a teacher.\"pupil_eye\"the dark circular opening in the centre of the iris of the eye, which varies in size to regulatethe amount of light reaching the retina.\" ringring_arena\"an enclosed space, surrounded by seating for spectators, in which a sport, performance, orshow takes place.\"ring_jewelry\"a small circular band, typically of precious metal and often set with one or more gemstones,worn on a finger as an ornament or a token of marriage, engagement, or authority.\" rockrock_music\"a type of popular music with a strong, loud beat that is usually played with electric guitarsand drums\"rock_stone\"the dry solid part of the earths surface, or any large piece of this that sticks up out of theground or the sea\" rulerruler_governor\"the leader of a country; a person who is in charge of a country\"ruler_measure\"a straight strip or cylinder of plastic, wood, metal, or other rigid material, typically marked atregular intervals and used to draw straight lines or measure distances.\" sealseal_animal\"a large mammal that eats fish and lives partly in the sea and partly on land or ice\"seal_close\"something fixed around the edge of an opening to prevent liquid or gas flowing through it\" springspring_season\"the season after winter and before summer, in which vegetation begins to appear, in thenorthern hemisphere from March to May and in the southern hemisphere from September toNovember.\"spring_device\"an elastic device, typically a helical metal coil, that can be pressed or pulled but returns to itsformer shape when released, used chiefly to exert constant tension or absorb movement.\""
}