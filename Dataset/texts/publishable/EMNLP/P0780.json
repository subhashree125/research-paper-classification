{
  "Abstract": "KnowledgeGraphQuestionAnswering(KGQA) methods seek to answer NaturalLanguagequestionsusingtherelationalinformation stored in Knowledge Graphs(KGs).With the recent advancements ofLarge Language Models (LLMs) and theirremarkable reasoning abilities, there is agrowing trend to leverage them for KGQA.However, existing methodologies have onlyfocused on answering factual questions, e.g.,In which city was Silvio Berlusconis firstwife born?, leaving questions involvingcommonsense reasoning that real-world usersmay pose more often, e.g., Do I need separatevisas to see the Venus of Willendorf and attendthe Olympics this summer?unaddressed.In this work, we first observe that existingLLM-based methods for KGQA struggle withhallucination on such questions, especiallyon queries targeting long-tail entities (e.g.,non-mainstream and recent entities), thushindering their applicability in real-worldapplications especially since their reasoningprocesses are not easily verifiable. In response,we propose Right for Right Reasons (R3),a commonsense KGQA methodology thatallows for a verifiable reasoning procedure byaxiomatically surfacing intrinsic commonsenseknowledge of LLMs and grounding everyfactual reasoning step on KG triples. Throughexperimental evaluations across three differenttasksquestion answering, claim verification,and preference matchingour findings show-case R3 as a superior approach, outperformingexisting methodologies and notably reducinginstances of hallucination and reasoning errors.",
  "Introduction": "Knowledge Graphs (KGs) have been widely usedas a structured format for storing and represent-ing relational information. Efficiently queryingKGs to obtain the required knowledge is a long-standing problem, for which query languages such as RQL (Karvounarakis et al., 2002) andSPARQL (Prudhommeaux and Seaborne, 2008)have been developed. However, writing queriesin these languages requires expertise which limitsthe accessibility of KGs to inexpert users. Knowl-edge Graph Question Answering (KGQA) (Zhenget al., 2017; Berant et al., 2013a; Yih et al., 2016)is an established research field that facilitates ac-cess to KGs by providing factual answers to naturallanguage (NL) questions using KGs.Recently, the promising performance of LargeLanguage Models (LLMs) in reasoning-relatedtasks has encouraged their application in KGQAresearch (Baek et al., 2023; Guan et al., 2023b; Liet al., 2023a). While these works have significantlyenhanced the performance of KGQA systems, theirprimary focus has been on addressing factoid ques-tions, such as \"In which city was Silvio Berlusconisfirst wife born?\", which can be answered usingonly the knowledge graph (KG) facts. However,real-world user queries often extend beyond thefactoid knowledge stored in the KG. For example,answering a question such as \"Do I need separatevisas to see the Venus of Willendorf and attend theOlympics this summer?\" requires both KG triplesindicating the locations of Venus of Willendorf andthe place where this summers Olympics is takingplace, as well as commonsense reasoning abouthow one can identify whether traveling to thosecountries requires separate visas or not.Commonsense reasoning is one of the most sig-nificant capabilities offered by LLMs (Shen and Ke-jriwal, 2021; Zhao et al., 2024). Therefore, it mayseem straightforward to leverage the LLMs to rea-son over a set of retrieved KG facts to perform com-monsense KGQA. However, LLMs are still suscep-tible to introducing ungrounded or incorrect infor-mation to their reasoning process a phenomenoncalled hallucination (Ye et al., 2023; Tonmoy et al.,2024). In conducting commonsense KGQA, LLMsmay exhibit hallucinations both by introducing un- grounded factual information as well as makingincorrect commonsense inferences. Hence, veri-fiability of the reasoning process is crucial to en-sure the reliability of the final answer, especiallyin high-stakes applications. Regrettably, none ofthe existing LLM-enhanced KGQA methodologiesanswer queries following a verifiable scheme.In this paper, we experimentally show that theperformance of existing KGQA methods is crit-ically hindered by the hallucination issue whenfaced with questions involving commonsense rea-soning. This issue is particularly exacerbated forquestions about long-tail knowledge, i.e., questionstargeting obscure or recent entities, and personal-ized questions. To address this challenge, we in-troduce Right for Right Reasons (R3), a verifiablemethodology for performing KGQA using LLMs.R3 makes both aspects of commonsense KGQAreasoning, factoid steps and commonsense infer-ences, verifiable. For the commonsense inferenceaspect, it axiomatically surfaces the commonsenseknowledge required for answering the question thatis intrinsic to the LLM parameters. Also, it caststhe KGQA task into a tree-structured search inwhich all factual reasoning steps are enforced tobe grounded on a subset of the relevant KG tripleswhich enables the verification of factual reasoningsteps. We compare R3 against current LLM-basedKGQA methodologies and pure LLM methods onthree different tasks: question answering, claimverification, and KG-based preference matching.The results demonstrate that R3 leads to a con-siderable reduction in hallucination and reasoningerrors while often improving accuracy and offeringrobustness to entity popularity.",
  "Reasoning with Large Language Models": "Despite being originally designed for text gen-eration, LLMs have shown outstanding perfor-mance when applied to several other NLP sub-fields (Chang et al., 2023). Particularly, the rea-soning capability of LLMs has attracted consider-able interest in AI research (Arora et al., 2022; Sunet al., 2022; Xu et al., 2023). Several works havestudied different reasoning skills of LLMs such asarithmetic reasoning (Yuan et al., 2023), logicalreasoning (Liu et al., 2023), and commonsense rea-soning (Bian et al., 2023; Shen and Kejriwal, 2023).These abilities make LLMs apt candidates for beingused as a reasoner in specialized tasks (Ren et al.,",
  "Commonsense Question Answering": "The general knowledge and conception about theworld that humans possess, and their ability to rea-son about it is called commonsense reasoning andis a crucial cognitive ability of humans. It is alsoan important reasoning skill based on which AIagents are evaluated (Liu et al., 2021; ?; Wanget al., 2023). LLMs have shown outstanding com-monsense reasoning skills and the gap betweentheir performance and humans on available datasetshas narrowed substantially (Guan et al., 2023a;Bian et al., 2023). Most of these datasets suchas CommonsenseQA (Talmor et al., 2018) andPhysicalQA (Bisk et al., 2020) contain questionsabout concepts rather than entities. Recently, Strat-egyQA (Geva et al., 2021a) and Creak (Onoe et al.,2021b) have been proposed as datasets for com-monsense reasoning about entities that can be usedto introduce commonsense reasoning to KGQA.",
  "Knowledge Graph Query Answering": "Answering questions using the relational informa-tion of KGs has recently gained significant atten-tion (Wang et al., 2024; Toroghi and Sanner, 2024),with its applications ranging from healthcare (Guoet al., 2022) to recommendation (Toroghi et al.,2023). Most existing works on the task of an-swering NL queries using the KG facts, knownas KGQA, focus on converting the NL query intoa structured formal query in a language such asSPARQL, executing the query to retrieve the re-quired knowledge, and finally reasoning over theretrieved facts to obtain the final answer. This idea,referred to as semantic parsing (Lan et al., 2021;Gu and Su, 2022; Cheng et al., 2022), often in-volves the data and computationally expensive pro-cess of fine-tuning with thousands of labeled exam-ples (Chen et al., 2021; Shu et al., 2022). Recently,KB-BINDER has suggested a training-free seman-tic parsing methodology using the in-context learn-ing ability of LLMs with few-shot examples (Liet al., 2023b).Novel LLM-based methods be-yond semantic parsing approach have also beenproposed. KAPING (Baek et al., 2023) introducedan efficient LLM-enhanced KGQA model that findsthe relevant sub-graph to the query via dense re-trieval and uses the LLM to reason over it in azero-shot manner. KGR (Guan et al., 2023b) pro-posed the idea of allowing LLMs to make claims,retrofitting those claims on the KG facts, and finally",
  "Problem Formulation": "In this paper, we propose a methodology for per-forming commonsense KGQA that is easily ex-tended to other related tasks such as KG-basedpreference matching. The input to the problem is aNL sentence posed by the user that can be either aquestion in the form of an interrogative sentence,or a claim or need expressed as an imperative sen-tence. We use the term query, denoted by q, torefer to the input in all cases. The query mentionsa set of anchor entities Eq. A KG K = (E, R) isassumed to be given, where E and R denote itsset of entities and relations respectively, such thatEq E. The objective is to follow a sequence ofreasoning steps Sq to find aq Oq, the answerto the query, such that verifying the correctness ofevery sqi Sq is possible. Here, Oq denotes theset of possible options.",
  "Right for Right Reasons": "Our proposed method casts the problem of com-monsense KGQA as a tree-structured search, inwhich every reasoning step is either grounded onKG facts, or based on surfaced commonsense ax-ioms, a key property that makes the reasoning pro-cedure completely verifiable. The overall workflowof R3 for answering a query is shown in .In brief, R3 first identifies the anchor entities of aquery and obtains the relevant sub-graph for theseentities. Next, it surfaces a commonsense axiomfrom the LLM that will guide the reasoning steps inthat branch of the search tree. Then, at each depthlevel of the tree, it checks whether the common-sense axiom can be satisfied with the available KGfacts, and if possible, provides an answer groundedon a subset of them. If the available KG triples areinsufficient, by backward-chaining from the axiom,it selects the next entity to obtain its relevant KGsub-graph to continue the search. Each branch cancontinue up to a maximum depth, and if an answeris not obtained at its bottom, a new commonsenseaxiom will be surfaced which will guide search ina new branch until the search tree reaches its max-imum breadth. Components of R3 are explainedhere, and a series of analyses on their roles and",
  "Obtaining Relevant Sub-graph": "The query answering process begins by extractingEq from q. Most existing works perform this ex-traction using entity linking techniques (Li et al.,2020; Ayoola et al., 2022). However, since existingentity linkers may fail to extract recent or obscureentities from the query, we also use an LLM-basedmodule with few-shot examples to obtain anotherset of entity names, and consider the union of thetwo sets as the final set of entities. Formally,",
  "Surfacing Commonsense Axioms": "The commonsense knowledge that LLMs have ob-tained during their training process is intrinsic totheir parameters, and they can use it to answerqueries given a set of retrieved facts. ExistingLLM-based methods that are designed for tack-ling the factoid KGQA problem can approach com-monsense KGQA using this intrinsic capability oftheir LLM component. However, since the set ofcommonsense axioms the reasoner has used is notknown, the reasoning process is not verifiable. Toaddress this issue, R3 axiomatically surfaces thisintrinsic knowledge of the reasoner and uses it toguide the reasoning process. In other words, itsreasoner is enforced to state the premises requiredfor concluding an answer as a set of atomic factoidclauses and try to find the answer by identifyingwhether those clauses are satisfied when their vari-ables are grounded on the KG entities, and theirpredicates and functions on KG relations. For ex-ample, when given a query \"Would it make sensefor Virginia Raggi to ask for a quinceaera?\", thereasoner surfaces the axiom: \"If Virginia Raggi is agirl from Latin America and her age is near 15, itwould make sense for her to ask for a quinceaera.\"Formally, given Eq = {eq1, ..., eq|Eq|}, a common-sense axiom Iq is an NL representation of the First-",
  "Austria": ": Workflow of commonsense KGQA procedure using R3. After extracting entities from the query andretrieving their relevant sub-graphs, a commonsense axiom is surfaced from the LLM that guides the reasoningbranch. After pruning sub-graphs to obtain their relevant facts to the axiom, an iterative process using the LLM isexecuted to either provide a fact-grounded answer or identify missing information and retrieve it from the KG. If theanswer is not found after a certain depth, a new axiom is surfaced to guide a new branch.",
  "= aq,(3)": "in which P = {P1, ..., P|P|} is the set of predi-cates, F = {F1, ..., F|F|} is the set of functions,opij {=, =, <, , >, } is a (dis)equality op-erator or comparison operator if the function valueis numeric, eij is the entity compared to the func-tion evaluation, and aq is the answer to the query orclaim. These relations and functions are all atomicclauses that can be checked against the KG triples. 3.2.3Sub-graph PruningOnce a commonsense axiom is surfaced, R3 triesto identify the satisfiability of the premises basedon the KG triples. Since the number of triples inKq may be large, we need to first prune the set ofavailable KG triples. To this end, as in (Baek et al.,2023), we use off-the-shelf dense retrievers (Songet al., 2020; Karpukhin et al., 2020; Xiong et al.,2020) to obtain T qi Kq, the subset of triples that have the most semantic similarity to the common-sense axiom Iqi . Since filtering triples by only con-sidering semantic similarity may lead to a high riskof losing some useful triples, we also use an LLMmodule with few-shot examples to pick relevanttriples to the axiom from a subset of the sub-graphtriples to reduce the chance of this information loss.Formally we have the Sub-graph Pruning moduleSGP as",
  "(4)": "in which sim denotes the Euclidean similarity be-tween t and Iqi , the embedding vectors of the triplet and the axiom Iqi , top-k operator returns the firstk elements of the sorted list of triples by their sim-ilarity score in descending order, and LLMT is anLLM-based module that returns a subset of Kq thatare relevant to Iqi .",
  "Fact-Grounded Answer SelectionAfter surfacing the commonsense axiom Iqi , andobtaining the set of relevant triples T qi , R3 tries to": "identify whether all premises in the axiom can besatisfied by grounding them on the relevant triples,in which case the answer to the query is \"True\", orat least one of the premises is unsatisfied, makingthe answer \"False\". If the axiom is in a disjunc-tive form, the answer becomes \"True\" as soon aseach disjunctive clause is completely satisfied. Inall these cases, R3 returns the answer, and the rea-soning process is terminated. For multiple-choicequeries, the process is repeated for each optionuntil an option satisfies all premises. However,if the satisfiability of any of the premises is notidentifiable by the current set of facts, instead ofreturning a guessed answer that encourages hallu-cination, the answer will remain undetermined. Inthis case, the set of current facts is insufficient forgrounding all premises, so the reasoning processmust continue to the next depth level. Formally,the answer aq {\"True\", \"False\", \"I dont know\"}is determined by",
  "where answer is the LLM-based module determin-ing the final answer": "3.2.5Missing Evidence IdentificationThe set of retrieved facts may be insufficient intwo cases: either the query targets a different en-tity, as in multi-hop questions, or the facts requiredfor grounding at least one premise were mistak-enly pruned. In this step, the reasoner is askedto consider the set of unsatisfied premises and theexisting facts to first identify what additional evi-dence must be obtained that is currently missing.Then, it has to identify the anchor entity em thatits triples can provide the missing information. Ifthe anchor entity is already in, Eq, the next top krelevant facts about it will be picked for the nextstep. Otherwise, the reasoner is asked to proposethe next entity and extract its name from Kq. Thenext entity is then added to Eq, and the process ofsub-graph extraction and pruning is executed for it.Formally,",
  "Comparison to Existing KGQA Methods": "R3 is the first KGQA approach that supports com-monsense queries in a verifiable manner, since ev-ery factual reasoning step is grounded on partic-ular KG triples, and its commonsense reasoningassumptions are surfaced in the form of axioms.Although KGR (Guan et al., 2023b) retrofits itsfactual claims on the KG, its commonsense rea-soning process is implicit. Semantic parsing meth-ods are only designed for factoid queries and can-not address commonsense queries. Finally, KAP-ING (Baek et al., 2023), despite its strong perfor-mance on single-hop factoid queries, cannot answermulti-hop questions because it has no particularmechanism for traversing the KG. A summary ofkey properties of existing KGQA methods and theircomparison to R3 is provided in .",
  "Question Answering. In this task, a question re-quiring commonsense reasoning formed aroundsome KG entities is asked. The reasoner is requiredto find the answer, which is either \"Yes\" or No": "Claim Verification Claim verification is very sim-ilar to question answering. Here, an imperativesentence including a claim about some entities isstated. The reasoner has to use the KG facts to de-cide whether the claim is \"Correct\" or \"Incorrect\". KG-based Preference Matching In this task, aquery explaining the users preference and a per-sonal KG containing evidence about the users pref-erences and restrictions is presented to the reasoner.The reasoner has to choose the item that matchesboth the users query and their personal restrictions.",
  "our tasks and make them publicly available to en-courage research on commonsense KGQA. Exam-ples of these modifications are shown in": "Question Answering Early KGQA datasets con-sisted of simple questions that can be answeredusing a single KG triple. Recently, datasets contain-ing more complex questions by introducing multi-hop reasoning have been proposed (Berant et al.,2013b; Trivedi et al., 2017; Gu et al., 2021). How-ever, all KGQA datasets contain factoid questions,which do not require commonsense reasoning toanswer (Guo et al., 2024). Some datasets exclu-sively focus on evaluating commonsense reason-ing (Talmor et al., 2018; Boratko et al., 2020; Sapet al., 2019), but their questions target concepts(e.g., river, mountain, etc.) rather than KG entities(e.g., specific people, locations, etc.).To overcome this challenge we modify Strat-egyQA (Geva et al., 2021b), a QA datasetwith Yes/No questions that target entities fromWikipedia2 articles. We select a subset of 150questions for which the required factual knowl-edge for answering them is present in Wikidata3 or that can be rewritten as such queries by target-ing them on new entities. The questions mostlytarget famous entities that LLMs can answer usingtheir internal knowledge without hallucinating oreven needing a KG. Since we are particularly in-terested in studying the hallucination behavior ofLLM-based KGQA methods on long-tail knowl-edge, for each query, we also write a counterparttargeting long-tail knowledge by substituting itsentities with obscure entities of the same types.We use the number of Wikidata triples and GoogleSearch results as measures of popularity. Claim Verification For KG-based claim verifica-tion, we use Creak (Onoe et al., 2021a), a datasetcontaining True/False claims written by crowdworkers using Wikipedia. We follow a similarprocedure applied to the QA dataset to select 150 claims and write their long-tail counterparts.KG-basedPreferenceMatchingRecipe-MPR (Zhang et al., 2023) is a preference matchingdataset that contains NL queries expressing a userspreference toward recipes and often targetingmultiple aspects. The reasoner has to choose therecipe that satisfies all aspects among five options.The multi-aspect nature of its queries and thenecessity for performing logical reasoning makeit a relevant dataset to our work. However, itsqueries are not personalized, meaning that thecorrect recipe does not require reasoning over theusers preferences and restrictions beyond thosestated in the query. In real-world applications, the\"correct\" item is different for each user consideringtheir personal preferences and restrictions.Tobridge this gap, we first extract 100 queries fromRecipe-MPR dataset that require commonsensereasoning and add a synthetic personal KG for theuser posing the query. We also add a sixth optionthat matches every preference aspect of the querybut violates at least one personal requirement thatcan be inferred from the users personal KG.",
  "Experimental Setup": "We compare R3 against LLM baselines withChain-of-Thought (CoT) prompting, both in zero-shot (Kojima et al., 2022) and few-shot (k = 2)settings (Wei et al., 2022) to evaluate the needfor a KG to answer these queries, and three re-cent LLM-based KGQA models, KB-BINDER (Liet al., 2023a), KGR (Guan et al., 2023b), and KAP-ING (Baek et al., 2023). For question answeringand claim verification tasks, we evaluate all meth-ods on both original queries (targeting famous en-tities) and modified queries (targeting long-tail en-tities) to study their robustness to popularity shift.We use GPT-3.5 Turbo as the LLM for all models.In addition to accuracy, we perform human evalua-tion to measure factual and reasoning faithfulness.In particular, we use FActScore (Min et al., 2023),which measures the percentage of atomic facts in",
  "Personal KG(Sam, occupation, painter), (Sam, age, 29), (Sam, medical condition, allium allergy), . . ., (Sam, religion, Christianity), (Sam, medical condition, lactose intolerance)": ": Exemplar queries form Datasets used for each task and modifications applied to them. Modified queries inQuestion answering and Claim verification target obscure entities to evaluate robustness to popularity shift. Thesynthetic KG and the new option add personalization aspect to the Preference Matching task. an LLMs response supported by a knowledge base,and Reasoning score, which measures the propor-tion of LLM responses in which there are no logicalreasoning errors. For preference matching, our hu-man evaluation consists of measuring Accuracy ofReasons which is the fraction of correct answersthat were obtained from correct reasons.",
  "Question Answering": "The results for the question answering task are pre-sented in table 2. R3 beats all baselines, achiev-ing an accuracy of 0.82 and 0.73 in the originaland long-tail settings respectively. Although thestrongest baseline, KAPING, achieves compara-ble accuracy, human evaluation reveals that KAP-INGs answers are far less reliable than those of R3. KB-BINDERs performance is much lower thanother methods, because KB-BINDER is a semanticparsing method that only supports factoid queriesand not ones that require commonsense reasoning.Although 0-shot and few-shot CoT achieve 0.70accuracy on the original queries, their accuraciesdrop significantly in the long-tail setting to 0.32 and0.43 respectively. We also observe in the long-tailsetting a sharp increase in the number of questionsfor which the LLM responds I dont know. Among all methods, R3 hallucinates the least,with the highest FActScores, 0.97 and 0.96, in theoriginal and long-tail settings respectively. KAP-INGs FActScores, 0.74 and 0.59, are significantlylower than R3, despite leveraging dense retrieval.This is because KAPINGs retrieval is limited to en-tities in the question, which works only for single- hop queries. For multihop queries, KAPING re-sorts to the LLMs internal knowledge. From ourLLM baselines, we observe low FActScores, in-dicating that LLMs internal knowledge is insuffi-cient. In contrast, R3 enforces strict grounding onthe KG for reasoning, and has an iterative mech-anism for identifying what additional facts are re-quired, which leads to near perfect FActScores.Not only are the FActScores of baseline meth-ods significantly lower than R3, but we also ob-serve for all baselines a significant decrease inFActScore on long-tail queries. For instance, KAP-INGs FActScore drops by 0.15 from 0.74 to 0.59.These results show that baseline method hallucina-tions are exacerbated in the long-tail setting dueto LLMs being unable to faithfully recall long-tailknowledge. For KAPING, we also observe that theentity linker fails more often to identify long-tailentities, which inevitably leads to ungrounded hal-lucinated answers in the absence of relevant triples.In contrast, R3 maintains a high FActScore in boththe original and long-tail settings with respectivescores of 0.97 and 0.96, which indicate its robust-ness to shifts in entity popularity.R3 also maintains the highest reasoning scorecompared to all baselines, achieving a score of 0.97and 0.95 in the original and long-tail settings re-spectively, compared to the next best method, few-shot CoT, which achieves reasoning scores of 0.92and 0.90. Because R3 makes the commonsenseinference process explicit by axiomatically surfac-ing the commonsense inference rules, R3 providesboth more verifiable and faithful chains of reason-ing with less errors. In contrast, KAPING has alow reasoning score. We qualitatively observe thatdue to the low precision of the facts retrieved byKAPING, the LLM is frequently misled by theirrelevant facts. Elsewhere, KGR has the lowestreasoning score. Without CoT, KGRs initial re-sponse often contains poor reasoning, which thenleads to poor retrofitting and thus a low FActScoreas well. Note that we do not perform human evalua-tion for KB-BINDER since it is a semantic parsingmethod that outputs SPARQL queries which areincompatible with FActScore and reasoning scores.",
  "Claim Verification": "The results for the claim verification task are pre-sented in . Although 2-shot CoT beats ourmethod on the original queries, our method is ro-bust in the long-tail setting, achieving the same ac-curacy as the original setting whereas 2-shot COTs accuracy drops significantly by 0.51.We observe that again R3 maintains the highestFActScore, 0.98, in both the original and long-tailsettings. In contrast, similar to the question answer-ing task, all baseline methods have significantlylower FActScores that also decrease significantlyin the long-tail setting. The low and decreasingFActScores in both the question answering andclaim verification task crucially demonstrate thatLLMs suffer from high rates of hallucination whichare exacerbated in long-tail settings.R3 also maintains the highest reasoning scoreamong all methods, 0.04 better than the next-bestmethod which is few-shot CoT. Interestingly, withfew-shot CoT, we qualitatively observe that theLLM at times erroneously follows the reasoningstrategies in the examples. We believe that explic-itly surfacing commonsense axioms is crucial forcorrectly guiding the subsequent reasoning. Again,KAPINGs low precision KG retrieval misleadsthe LLM, resulting in low reasoning scores, andKGRs poor reasoning leads to suboptimal initialresponses that KGR has difficulty retrofitting.A statistical analysis of these results is providedin Appendix B, which verifies that R3 statisticallysignificantly reduces sources of hallucination onthree of the studied datasets. We also provide anec-dotal examples of R3s performance in addressingLLM misbeliefs in Appendix C. 4.4.3Preference MatchingResults of the preference matching task are pro-vided in . Since the personal KG does notsupport SPARQL queries, KB-BINDER cannot beevaluated on it. KGR and pure LLM baselinesalso cannot be evaluated on this task since they canonly make claims or provide answers about entitiesthat LLMs are aware of, and not about users in asynthetic dataset. So, the only relevant baseline isKAPING. Results of this comparison vividly iden-tify that on the challenging task of personalizedpreference matching, R3 obtains a considerablyhigher accuracy. We also observe that the Accu-racy of reasons for R3 is more than double the num-ber for KAPING, which again reflects its strongercommonsense reasoning ability due to its specialapproach for surfacing commonsense axioms.",
  ": Results of Accuracy and Accuracy of reasons(%) for preference matching task": "iomatically surfacing their intrinsic commonsenseknowledge. Key experimental results exhibit the ef-ficacy of R3 across different tasks related to KGQAand its superior performance to existing baselines.The promising performance of R3 combined withits verifiability and robustness to entity popularityopens up possibilities for versatile future extensionto address broader ranges of tasks and improve theflexibility and accessibility of KGs and reliabilityof LLM-based reasoners.",
  "Limitations": "While we believe this work has made significantforward progress in leveraging KG content for com-monsense question answering (QA), our methodR3 (like any QA method) has natural limitationsthat we hope will encourage further investigationand future work.The quality of the reasoning process in R3 re-lies on the quality of the natural language axiomsgenerated. We observe through our experimentsthat in cases where the quality of axioms is in-sufficient, the reasoner is misled resulting in anundetermined answer at the end of the explorationbudget identified. Furthermore, due to the impor-tance of avoiding hallucination, our model takesa conservative and rigorous approach to groundevery factual premise on KG triples. Therefore,our model typically leaves more questions unan-swered than other baselines (which we consideredan incorrect response in calculating the accuracy).Furthermore, as in most LLM-based models, forhaving a proper performance, LLM-based compo-nents of our model require clear explanation of thetask provided in the prompts to them, as well as anumber of few-shot examples that clarify the intentof the task description further.We consider further studies into the above limi-tations as open areas of future work. Studying thetrade-off between rigor and the rate of unansweredquestions, as well as studying the robustness ofour model to different prompting styles are keyresearch questions that we consider for future.",
  "Ethics Statement": "This work intends to provide a solution for improv-ing the correctness and faithfulness of LLMs inthe task of commonsense KGQA. Additionally, itseeks to improve the verifiability of the generatedanswers, thereby aiding in the detection and miti-gation of incorrect or potentially harmful content.However, it is important to acknowledge that thisapproach (a) relies on LLMs that may hallucinateand (b) presumes the accuracy of the knowledgegraph (KG) data and lacks any capacity to correcterroneous or noisy information present within theKG. Hence, it is imperative to ensure accuracy ofthe KG and that the reasoning steps introduced byR3s LLM are free of both hallucinations and oth-erwise incorrect, biased, or unethical conclusionsthat may be harmful to downstream users. Simran Arora, Avanika Narayan, Mayee F Chen, LaurelOrr, Neel Guha, Kush Bhatia, Ines Chami, FredericSala, and Christopher R. 2022. Ask me anything:A simple strategy for prompting language models.arXiv preprint arXiv:2210.02441. Tom Ayoola, Shubhi Tyagi, Joseph Fisher, ChristosChristodoulopoulos, and Andrea Pierleoni. 2022.Refined: An efficient zero-shot-capable approachto end-to-end entity linking.arXiv preprintarXiv:2207.04108.",
  "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.Knowledge-augmented language model promptingfor zero-shot knowledge graph question answering.arXiv preprint arXiv:2306.04136": "Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013a. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013conference on empirical methods in natural languageprocessing, pages 15331544. Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013b. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2013, 18-21 October2013, Grand Hyatt Seattle, Seattle, Washington, USA,A meeting of SIGDAT, a Special Interest Group of theACL, pages 15331544. ACL. Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, YaojieLu, and Ben He. 2023. Chatgpt is a knowledgeablebut inexperienced solver: An investigation of com-monsense problem in large language models. arXivpreprint arXiv:2303.16421. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Michael Boratko, Xiang Lorraine Li, Rajarshi Das, TimOGorman, Dan Le, and Andrew McCallum. 2020.Protoqa: A question answering dataset for proto-typical common-sense reasoning.arXiv preprintarXiv:2005.00771. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. 2023. A sur-vey on evaluation of large language models. ACMTransactions on Intelligent Systems and Technology. Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin,Jian-Guang Lou, and Feng Jiang. 2021. Retrack:A flexible and efficient framework for knowledgebase question answering. In Proceedings of the 59thannual meeting of the association for computationallinguistics and the 11th international joint conferenceon natural language processing: system demonstra-tions, pages 325336. Zhoujun Cheng, Tianbao Xie, Peng Shi, ChengzuLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,et al. 2022. Binding language models in symboliclanguages. arXiv preprint arXiv:2210.02875. Jan Clusmann, Fiona R Kolbinger, Hannah SophieMuti, Zunamys I Carrero, Jan-Niklas Eckardt,Narmin Ghaffari Laleh, Chiara Maria Lavinia Lffler,Sophie-Caroline Schwarzkopf, Michaela Unger, Gre-gory P Veldhuizen, et al. 2023. The future landscapeof large language models in medicine. Communica-tions Medicine, 3(1):141. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021a. Did aristotleuse a laptop? a question answering benchmark withimplicit reasoning strategies. Transactions of theAssociation for Computational Linguistics, 9:346361. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021b. Did aristotleuse a laptop? A question answering benchmark withimplicit reasoning strategies. Trans. Assoc. Comput.Linguistics, 9:346361. Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler,Percy Liang, Xifeng Yan, and Yu Su. 2021. BeyondI.I.D.: three levels of generalization for question an-swering on knowledge bases. In WWW 21: The WebConference 2021, Virtual Event / Ljubljana, Slovenia,April 19-23, 2021, pages 34773488. ACM / IW3C2.",
  "Willis Guo, Armin Toroghi, and Scott Sanner. 2024.Cr-lt-kgqa: A knowledge graph question answeringdataset requiring commonsense reasoning and long-tail knowledge. arXiv preprint arXiv:2403.01395": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020.Dense passage retrieval foropen-domain question answering. arXiv preprintarXiv:2004.04906. GregoryKarvounarakis,SofiaAlexaki,VassilisChristophides, Dimitris Plexousakis, and MichelScholl. 2002.Rql: a declarative query languagefor rdf. In Proceedings of the 11th internationalconference on World Wide Web, pages 592603. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In Advancesin Neural Information Processing Systems 35: An-nual Conference on Neural Information ProcessingSystems 2022, NeurIPS 2022, New Orleans, LA, USA,November 28 - December 9, 2022. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang,Wayne Xin Zhao, and Ji-Rong Wen. 2021. A sur-vey on complex knowledge base question answering:Methods, challenges and solutions. arXiv preprintarXiv:2105.11644.",
  "Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, QijiZhou, and Yue Zhang. 2023. Evaluating the logicalreasoning ability of chatgpt and gpt-4. arXiv preprintarXiv:2304.03439": "Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-ter West, Ronan Le Bras, Yejin Choi, and HannanehHajishirzi. 2021.Generated knowledge prompt-ing for commonsense reasoning.arXiv preprintarXiv:2110.08387. Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of factualprecision in long form text generation. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023, Sin-gapore, December 6-10, 2023, pages 1207612100.Association for Computational Linguistics. Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, andGreg Durrett. 2021a. CREAK: A dataset for com-monsense reasoning over entity knowledge. In Pro-ceedings of the Neural Information Processing Sys-tems Track on Datasets and Benchmarks 1, NeurIPSDatasets and Benchmarks 2021, December 2021, vir-tual.",
  "Maarten Sap, Hannah Rashkin, Derek Chen, RonanLeBras, and Yejin Choi. 2019.Socialiqa: Com-monsense reasoning about social interactions. arXivpreprint arXiv:1904.09728": "Ke Shen and Mayank Kejriwal. 2021. On the gener-alization abilities of fine-tuned commonsense lan-guage representation models. In Artificial Intelli-gence XXXVIII: 41st SGAI International Conferenceon Artificial Intelligence, AI 2021, Cambridge, UK,December 1416, 2021, Proceedings 41, pages 316.Springer. Ke Shen and Mayank Kejriwal. 2023.An experi-mental study measuring the generalization of fine-tuned language representation models across com-monsense reasoning benchmarks. Expert Systems,page e13243. Yiheng Shu, Zhiwei Yu, Yuhan Li, Brje F Karlsson,Tingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022.Tiara: Multi-grained retrieval for robust question an-swering over large knowledge bases. arXiv preprintarXiv:2210.12925. Chan Hee Song, Jiaman Wu, Clayton Washington,Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023.Llm-planner: Few-shot grounded planning for em-bodied agents with large language models. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 29983009.",
  "Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2018. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge. arXiv preprint arXiv:1811.00937": "SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-ula Rawte, Aman Chadha, and Amitava Das. 2024.A comprehensive survey of hallucination mitigationtechniques in large language models. arXiv preprintarXiv:2401.01313. Armin Toroghi, Griffin Floto, Zhenwei Tang, and ScottSanner. 2023. Bayesian knowledge-driven critiquingwith indirect evidence. In Proceedings of the 46thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages18381842.",
  "Armin Toroghi and Scott Sanner. 2024. Bayesian in-ference with complex knowledge graph evidence. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 2055020558": "Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey,and Jens Lehmann. 2017. Lc-quad: A corpus forcomplex question answering over knowledge graphs.In The Semantic Web - ISWC 2017 - 16th Interna-tional Semantic Web Conference, Vienna, Austria,October 21-25, 2017, Proceedings, Part II, volume10588 of Lecture Notes in Computer Science, pages210218. Springer. Weiqi Wang, Tianqing Fang, Wenxuan Ding, BaixuanXu, Xin Liu, Yangqiu Song, and Antoine Bosse-lut. 2023. Car: Conceptualization-augmented rea-soner for zero-shot commonsense question answer-ing. arXiv preprint arXiv:2305.14869. Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, RuiyiZhang, and Tyler Derr. 2024.Knowledge graphprompting for multi-document question answering.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1920619214.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought prompting": "elicits reasoning in large language models. In Ad-vances in Neural Information Processing Systems 35:Annual Conference on Neural Information Process-ing Systems 2022, NeurIPS 2022, New Orleans, LA,USA, November 28 - December 9, 2022. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul Bennett, Junaid Ahmed, and ArnoldOverwijk. 2020. Approximate nearest neighbor neg-ative contrastive learning for dense text retrieval.arXiv preprint arXiv:2007.00808. Yudong Xu, Wenhao Li, Pashootan Vaezipoor, ScottSanner, and Elias B Khalil. 2023.Llms and theabstraction and reasoning corpus: Successes, failures,and the importance of object-based representations.arXiv preprint arXiv:2305.18354.",
  "Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,and Songfang Huang. 2023. How well do large lan-guage models perform in arithmetic tasks?arXivpreprint arXiv:2304.02015": "Haochen Zhang, Anton Korikov, Parsa Farinneya, Mo-hammad Mahdi Abdollah Pour, Manasa Bharadwaj,Ali Pesaranghader, Xi Yu Huang, Yi Xin Lok, ZhaoqiWang, Nathan Jones, et al. 2023. Recipe-mpr: Atest collection for evaluating multi-aspect preference-based natural language retrieval. In Proceedings ofthe 46th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 27442753.",
  "framework. To further substantiate the significance": "of each part and assess its impact on overall perfor-mance, we conduct a series of ablation studies andexperiments in this section. This analysis contraststhe functionality of each component against alter-native design choices, providing deeper insightsinto the necessity of each element in the R3 archi-tecture.Utilizing the KG facts and grounding the factsused in reasoning on the KG is a cornerstone of theR3 framework. Ablating the use of KG effectivelyreduces R3 to the few-shot CoT baselines, whichwe previously compared in . There arethree major steps in answering a commonsensequery based on KG:",
  "Answering the question using these relevantfacts": "R3 adds a critical step that governs its searchprocess for answering the query, which is surfacingthe commonsense axiom. The importance of thisstep was shown through experiments conducted in. Removing the surfaced commonsenseaxioms and the tree-structured search that R3 em-ploys to answer queries simplifies it to KAPING,one of the baseline we evaluated in andshowed that it was outperformed by R3.In this section, we study the options and designchoices that can be considered for each of the threeenumerated steps and examine the influence of ab-lating components utilized in the R3 framework ineach step.",
  ": Accuracy of R3 compared to its variants with ablated entity extraction modules. The higher success rate ofR3 in extracting queries also results in a higher accuracy": "Existing KGQA methodologies often rely onentity-linking techniques (Li et al., 2020; Ayoolaet al., 2022) that efficiently extract well-knownentities. However, since these methods were nottrained on sufficient data from long-tail and recententities that R3 aims to address, they might notbe able to perform successful entity extraction forthose queries. To address this possible issue, R3 also leverages an LLM-based entity extractor. Inthis analysis, we study the role and importance ofeach of these entity extraction techniques.To this end, we first compare the entity extrac-tion performance of each of these entity linkingmethodologies by using each of them to extractentities for all queries of all subsets of the dataset,and comparing the sets of retrieved entities againstthe set of ground truth entities that are contained inall queries. Results of this experiment are shownin . In the first row of this table, we use Re-FinED, a standard entity linking methodology thatis specialized for Wikipedia and Wikidata entities,and in the second row, we just use our LLM-basedentity extractor. The final row refers to the finalset of entities that we use in R3 which is basicallythe union of the entities retrieved by each of thesemethods. From this table, we can verify that al-though both entity extraction methods have a highsuccess rate in extracting the entities, they are bothimperfect and fail to extract a fraction of the entitiesfrom some queries. However, when their union isused in R3, all entities can be successfully retrievedto extract their relevant sub-graph. This means that on every query that one of these methods fails toextract the correct entity, the other method success-fully compensates for it. We note that this perfectentity extraction result that is obtained for R3 isconfined to the datasets that we studied in this pa-per and across other datasets, there might be casesin which both entity extraction methods fail. How-ever, using both methods considerably increasesthe chance of successful retrieval. This table alsovalidates our hypothesis that the standard entitylinking mythologies may be challenged more inextracting the long-tail entities, but the LLM-basedentity extractor is more robust to entities popular-ity.To further verify the importance of utilizingboth sub-graph extraction methodologies, we ex-amine the role of each method in the overall per-formance of R3. We repeat all experiments forboth tasksquestion answering and claim verifi-cationwhile ablating the two entity extractionmethodologies. The results of this experiment arepresented in . These findings underscorethe significance of the entity extraction scheme em-ployed in R3. In every case, the combined useof both entity extraction methodologies (as imple-mented in R3) enhances the accuracy across alltasks. Additionally, this table highlights the con-tribution of the LLM-based entity extractor intro-duced in this work to the methods overall perfor-mance.In conclusion, for extracting the relevant sub-grapha crucial first step in answering common-",
  "A.2Sub-graph Pruning": "Due to the potentially large size of the relevantsub-graph that is retrieved, it is crucial to prune itto enable the use of an LLM-based reasoner thathas a limited context length. However, it is crucialnot to prune out the essential KG facts from therelevant sub-graph that are essential in answeringthe query. We consider two possible approaches inthis regard:",
  "Using more intelligent approaches such as se-mantic similarity to identify the more relevantfacts": "In R3, we used an approach based on the seman-tic similarity between the commonsense axiom andfacts in the relevant sub-graph. In order to verifythe efficacy of this approach in preserving the es-sential KG facts while pruning the irrelevant ones,we perform an experiment in which we ablate thissemantic similarity-based approach of sub-graphpruning. However, due to the large size of the re-trieved sub-graph, we truncate the set of facts to fitthe context size of the LLM.The results of comparing the outcome of thissub-graph pruning method against the semanticsimilarity-based approach used in R3 are presentedin . Evidently, truncating the sub-graphleads to a significant drop in accuracy across alldataset splits, as it often prunes essential facts.These results confirm the necessity of the sub-graphpruning approach employed in R3 for judiciouslyselecting the facts that are useful in answering thequeries.",
  "A.3Iterative Process for AnsweringMulti-hop Queries": "R3 is equipped with a tree-structured search mech-anism for answering queries. As illustrated in theworkflow of R3 in , each branch of thetree undergoes an iterative process of sub-graph re-trieval and pruning, attempting to answer the query, and identifying missing information at deeper lev-els of the tree. This iterative process enables R3 toperform multi-hop reasoning on the KG, therebyproviding fact-based answers.In this experiment, we validate the necessityof the tree-structured search process in answeringcommonsense queries for question answering andclaim verification tasks. To achieve this, we varythe maximum depth of the search tree and conductexperiments on the long-tail subsets of the questionanswering and claim verification datasets. Resultsof this experiment are presented in .We first completely ablate this iterative processand try to answer queries on the first try. Results ofthis experiment are shown in the first row (depth =0) which shows a considerably lower accuracy thanthe original R3 performance that we reported inthe paper using depth = 2. By increasing the treedepth which is equivalent to an increased number ofiterations for performing multi-hop reasoning, theaccuracy in both tasks increases, until it plateausat depth = 2 as there are limited queries requiringmore reasoning steps on these two datasets.The results of this study underscore the criticalimportance of the iterative process for effectivelyanswering multi-hop commonsense queries withinthe R3 framework.",
  "BStatistical Analysis": "In order to evaluate the statistical significance of thesuperior performance of R3 in comparison to thebaselines that were reported in of ,we conducted a statistical test. Each subset of thisdataset contains 150 queries, resulting in a total of600 queries across the two tasks with the originaland long-tail settings.In this test, we consider queries of each col-umn with the responses provided by R3 and an-swers given by the best-performing baseline acrossall queries per column, resulting in a total of300 query-answer pairs for each column. SinceFActScore is a numerical metric, we employed thepaired t-test to obtain the statistical significance,while for the Accuracy and Reasoning metrics, weutilized McNemars test (also for paired data) con-sidering the binary nature of the data. We also triedcalculating the Fishers exact test and it providedmuch more favorable p-values indicating a strongersignificance of R3s superiority, but we do not be-lieve it is appropriate for this paired comparison ofeach method on the same queries and therefore, do",
  "not include its results": "Results of the p-values reflecting the statisticalsignificance test are presented in . Whilethe p-values are not high enough to make strongstatistical claims that R3 performs statistically sig-nificantly better than the best baseline in terms ofAccuracy, we note that the purpose of Right forthe Right Reasons (R3) is to maintain the accu-racy of existing state-of-the-art QA methods whilereducing fact and reasoning hallucinations. Factand reasoning hallucinations are respectively mea-sured by the FActScore and Reasoning metrics. Onthese metrics, p-value results show very encour-aging statistical results. Reasoning results for R3 appear significantly better for the Original versionsof the datasets (p-value < 0.05) and just miss the0.05 significance level for the Long-tail version bya small margin. Critically, for the FActScore, R3 outperforms the best baseline with high statisticalsignificance (p-value < 0.001) for both the Originaland Long-tail variants of our datasets indicatinga highly statistically significant reduction in facthallucination for R3. In summary, this statistical analysis shows thatR3 potentially outperforms and at least matches theAccuracy of state-of-the-art methods and it statisti-cally significantly reduces sources of hallucinationon three out of the four datasets (and almost signif-icantly on the fourth).",
  "COverriding LLM Misbeliefs with KGFacts": "During their training process, LLMs acquire sub-stantial factual knowledge about various objectsand entities. However, as observed in the exper-imental results presented in , utilizingtheir internal knowledge in answering common-sense queries is prone to hallucination, especiallyin answering queries about long-tail entities. R3 addresses this challenge by grounding its answerson the facts from the KG that are more reliable. Infact, it is likely that the internal knowledge of theLLM disagrees with a fact that R3 obtains from theKG, and in these circumstances, R3 overrides theLLMs potentially mistaken belief with a credibleKG fact, which resulted in higher accuracy and fac- tual correctness of the R3s responses compared tothe LLM-based baselines.In order to verify that R3 succeeds in overridingmistaken beliefs of the LLM by correct and rele-vant facts from the KG and observe the contributionof this approach to the superior performance of R3,in this section, we provide anecdotal examples ofresponses provided by R3 and the LLM-based base-lines to 30 queries. These queries are chosen fromthe subsets of the benchmark datasets on which R3 outperforms the baselines, i.e., question answer-ing task in both original and long-tail settings andthe claim verification task in the long-tail setting.We choose these queries among the queries thatthe LLM bases its answers on facts and does notrespond by just Yes, No, or I dont know.We also provide the complete sets of responses pro-vided by R3 and all baselines on all queries in ourrepository.Each anecdotal example is provided in a tablethat contains the query, the correct answer, and theset of responses provided by each method followedby a brief discussion comparing those responses.Correct facts that are used in each response areindicated by green text colour and incorrect onesare shown in red. Also, incorrect reasoning stepsare indicated by red underline.",
  "Question Answering: Long-tail": "Query: Could someone in Vaughan take a taxi to the Jamjuree Art Gallery? (No)0-Shot CoT: First, lets find out if there are taxis available in Vaughan. In Vaughan, taxis areavailable for local transportation. Next, lets check the location of the Jamjuree Art Gallery to seeif its within a reasonable distance from Vaughan. The Jamjuree Art Gallery is located in Toronto,which is adjacent to Vaughan. With this information, it is reasonable to conclude that someone inVaughan could take a taxi to the Jamjuree Art Gallery, given the proximity of the locations. Finalanswer: Yes. 2-Shot CoT: The Jamjuree Art Gallery is located in Bangkok, Thailand. Vaughan is a city in Ontario,Canada. Since they are in different countries and continents, it is not feasible to take a taxi fromVaughan to the Jamjuree Art Gallery. Final Answer: No. R3: Based on the general rule, the Jamjuree Art Gallery must be within a reasonable distance fromVaughan and accessible by road for someone in Vaughan to take a taxi to it. Fact (Jamjuree ArtGallery, country, Thailand) tells us that the Jamjuree Art Gallery is located in Thailand, which isquite far from Vaughan, Ontario, Canada. Fact (Vaughan, is, city in Ontario, Canada) tells us thatVaughan is a city in Ontario, Canada. Although the facts dont provide explicit information aboutthe distance between Vaughan and the Jamjuree Art Gallery, we can make a plausible assumptionbased on general knowledge. Thailand is located on the other side of the world from Canada, and itis not within a reasonable distance for someone in Vaughan to take a taxi to the Jamjuree Art Gallery.Therefore, the answer is: No, someone in Vaughan cannot take a taxi to the Jamjuree Art Gallery. In the 0-shot response, the LLM has hallucinated about the location of the Jamjuree Art Gallery andbelieved that it is located in Toronto. Although the reasoning about the possibility of traveling bytaxi between two adjacent cities is correct, the hallucinated fact has led to an incorrect final answer.The 2-shot response is correct and does not include this hallucination. The response provided byR3 is completely correct and based on KG facts. Also, it points to the specific KG facts used in itsreasoning which allows for verification of the response. Query: Is the Its Hyorish an older album than Nasime vasl? (No)0-Shot CoT: First, lets determine the release dates of the albums Its Hyorish and Nasime vasl.Once we have this information, we can compare the release dates to see which album is older. Thealbum Its Hyorish by Nasimevasl was released on October 30, 2019. On the other hand, Nasimevasl by Marjan Farsad was released on November 23, 2017. Comparing the release dates, Nasimevasl is older than Its Hyorish. Final answer: Yes, Nasime vasl is an older album than ItsHyorish.",
  "-Shot CoT: The album Its Hyorish was released in 1990, while Nasime vasl was released in1978. Therefore, Its Hyorish is not older than Nasime vasl. Final Answer: No": "R3: Based on the general rule, we need to compare the publication dates of Its Hyorish andNasime Vasl to determine which album is older. Fact (Its Hyorish, publication date, 2008) tellsus that \"Its Hyorish\" was published in 2008, and Fact (Nasime Vasl, publication date, 2003/05/21)tells us that Nasime Vasl was also published in 2003. Comparing the publication dates, we can seethat Nasime Vasl was published earlier than Its Hyorish. Therefore, the answer is: No, NasimeVasl is an older album than Its Hyorish. In the 0-shot CoT response, the LLM makes multiple factual hallucinations. First, it states that ItsHyorish is an album by Nasimevasl, whereas such singer does not exist and Nasime vasl is thename of another album. It also states that Its Hyorish was released on October 30, 2019 which isincorrect. It also states that Nasime vasl is an album by Marjan Farsad and that Nasime vaslwas released on November 23, 2017, which are both incorrect. Also, in the 2-shot CoT response, theLLM hallucinated about the dates the two albums were released. However, R3 correctly overridesthe mistaken beliefs of the LLM by KG facts and provides the correct answer. Query: Did Ilary Blasi celebrate a silver anniversary with Francesco Totti? (No)0-Shot CoT: To the best of my knowledge, Ilary Blasi did celebrate a silver anniversary withFrancesco Totti. Silver anniversary usually refers to the 25th anniversary, and in this contextcould imply a significant milestone in their relationship. Ilary Blasi and Francesco Totti are knownpublic figures and have been married for many years, so it is possible that they celebrated a silveranniversary.",
  "-Shot CoT: Yes. Ilary Blasi and Francesco Totti celebrated their silver (25th) anniversary in 2020.Final answer: Yes": "R3: According to the general rule provided, Ilary Blasi and Francesco Totti must have been marriedfor 25 years to celebrate a silver anniversary together. Fact (Francesco Totti, spouse, Ilary Blasi;start time, 2005; end cause, divorce; end time, 2022) states that Ilary Blasi and Francesco Tottiwere married from 2005 to 2022, which is a total of 17 years, not 25 years. Therefore, based on theinformation given, Ilary Blasi and Francesco Totti did not celebrate a silver anniversary together.Hence, the answer is: No, Ilary Blasi did not celebrate a silver anniversary with Francesco Totti. Both 0-Shot and 2-Shot responses hallucinate about the length of Ilary Blasi and Francesco Tottismarriage. However, R3 overrides the mistaken belief by grounding its response on the KG fact abouttheir divorce which leads to correctly calculating their length of marriage. Query: Is it likely for the director of Demitri Cantemir to fast during Ramadan? (No)0-Shot CoT: First, Demitri Cantemir was a Moldavian historian, composer, and scholar of Ottomanhistory and music. It is known that he was of Orthodox Christian faith, which doesnt require fastingduring Ramadan. So, it is unlikely for the director of Demitri Cantemir to fast during Ramadan.Final answer: No. 2-Shot CoT: Demitri Cantemir was a Moldavian philosopher, historian, composer, and a man of arts.He was also a convert to Islam and lived in Constantinople for a portion of his life. Since Ramadanis an important month of fasting for Muslims, it is likely that Demitri Cantemir, being a Muslim,observed fasting during Ramadan. Final answer: Yes. R3: The general rule states that the likelihood of the director of a film fasting during Ramadandepends on whether the director observes the Islamic faith. Fact (Demitri Cantemir, director, VladIovita) tells us that Vlad Iovita is the director of Demitri Cantemir. Fact (Vlad Iovita, religionor worldview, Eastern Orthodox Church) tells us that Vlad Iovitas religion is Eastern OrthodoxChurch, not Islam. Since Vlad Iovita does not follow the Islamic faith, it is not likely for him to fastduring Ramadan. Therefore, the answer is: No, it is not likely for the director of Demitri Cantemirto fast during Ramadan. The 0-Shot answer includes a reasoning error. Although the facts it provides about who DemitriCantemir was and the religion that he believed in are correct, the fact that Demitri Cantemir followedChristianity does not imply that the director of a movie about him is not likely to fast. Also, theresponse provided by 2-Shot CoT contains the same reasoning error by using information aboutDemitri Cantemirs religous beliefs to judge about the religous beliefs of the director of the movieabout him. This response also includes a hallucinated fact about Demitri Cantemirs conversionto Islam. The response provided by R3 fixes both errors. First, it has a mechanism for identifyingthe required missing information that allows it to answer multi-hop questions such as this one. R3 determines that information about Demitri Cantemirs director is required and correctly retrievesthe required information from the KG and uses it to generate the response. Second, the response isgrounded on the KG facts and does not contain hallucinations as the one in the 2-shot CoT. Query: Was Darya Dadvar from the same city that used to be Afsharid Empires center of power?(Yes)0-Shot CoT: The capital of the Afsharid Empire was Mashhad. Darya Dadvar was born in Mashhad.Therefore, the answer is Yes. 2-Shot CoT: Darya Dadvar was born in Rasht, Iran. The center of power of the Afsharid Empirewas in Mashhad. Since Rasht and Mashhad are different cities, Darya Dadvar is not from the samecity that used to be the center of power of the Afsharid Empire. Final Answer: No. R3: Based on General Rule Darya Dadvar must have been born in the city that used to be the centerof power of the Afsharid Empire to be from the same city., Darya Dadvar must have been born inthe city that used to be the center of power of the Afsharid Empire to be from the same city. Fact(Darya Dadvar, place of birth, Mashhad) tells us that Darya Dadvar was born in Mashhad. Fact(Afsharid Empire, capital, Mashhad) tells us that Mashhad was the capital of the Afsharid Empire.Therefore, the answer is: Yes, Darya Dadvar is from the same city that used to be the center of powerof the Afsharid Empire, which is Mashhad. The response provided by 0-Shot CoT is correct, but the one provided by 2-Shot CoT, contains afactual hallucination about the birthplace of Darya Dadvar which shows that LLM is unreliablein answering. However, R3 provides a response which is grounded on the KG and overrides theincorrect belief that was reflected in 2-Shot CoTs response. Query: Does Toyoharu Tsutsuis alma mater have more history than Newfoundland and Labrador?(Yes)0-Shot CoT: 1. Toyoharu Tsutsuis alma mater is Seijo University, which was founded in 1943.2. Newfoundland and Labrador is a province in Canada, known for its rich history dating backthousands of years, including Indigenous peoples, Viking settlements, and European exploration.Based on this information, it is clear that Newfoundland and Labrador has significantly more historythan Seijo University. 2-Shot CoT: Toyoharu Tsutsuis alma mater is Tokyo University, which was established in 1877.Newfoundland and Labrador has a long history dating back thousands of years, including Indigenousoccupation, Viking settlements, and European exploration. Therefore, Newfoundland and Labradorhas a longer history than Tokyo University. Final answer: No. R3: To determine whether Toyoharu Tsutsuis alma mater has more history than Newfoundland andLabrador, we need to compare the histories of both. Fact (Newfoundland and Labrador, inception,1949/03/31) tells us that Newfoundland and Labrador was established in 1949. On the other hand,Fact (Harvard University, inception, 1636/09/08) tells us that Harvard University was founded in1636. Therefore, based on the information provided, Harvard University has a longer history thanNewfoundland and Labrador. Therefore, the answer is: Yes, Harvard University has more historythan Newfoundland and Labrador. In the response provided by 0-Shot CoT, the LLM hallucinates about Toyoharu Tsutsuis alma mater,and also the date Seijo University was founded. Also, there are hallucinations about ToyoharuTsutsuis alma mater, and the date Tokyo University was founded in 2-Shot CoT. In the responseprovided by R3, these hallucinations are correctly overridden by the respective KG facts, and thefinal provided answer is also corrected. Query: Could people have read Mjegull during the Perak war? (No)0-Shot CoT: To answer the question, we can take a step by step approach. First, lets determine thetime period of the Perak war and the publication date of Mjegull. Then we can investigate if therewere any circumstances that could have allowed people to read Mjegull during the Perak war. ThePerak War occurred from 1875 to 1876. Meanwhile, Mjegull was published in 1884. Based on thetimeline, it is unlikely that people could have read Mjegull during the Perak war, as the publicationof Mjegull came after the war. Final answer: No. 2-Shot CoT: Mjegull is a fictional language used in the video game \"The Witcher 3: Wild Hunt\". Asthe Perak War took place in the late 19th century, it is impossible for people to have read Mjegullduring that time because it is a creation for a modern video game. Final answer: No. R3: The general rule states that people during the Perak War must have had access to the novelMjegull, which was published in 2009, in order to have read it during that time. Fact (Perak War,end time, 1876) tells us that the Perak War ended in 1876, while Fact (Mjegull, publication date,2009; place of publication, Tirana) states that the novel Mjegull was published in 2009. Since thepublication date of Mjegull is much later than the Perak War, it is not possible for people during thePerak War to have read Mjegull. Therefore, the answer is: No, people could not have read Mjegullduring the Perak War. The 0-Shot CoT response includes a factual hallucination about the time Mjegull was published.Also, 2-Shot CoT response includes an error about the Mjegull entity and attributes it to a fictionallanguage whereas Mjegull here refers to a book. By grounding the response on the KG, R3 succeedsin providing a correct answer to the question which is correct in terms of reasoning and factuality. Query: Did Ivan Shuisky suffer the same fate as Benny Frey? (Yes)0-Shot CoT: First, lets identify the fate of Benny Frey. Benny Frey was a character in the televisionshow \"Hannibal\" who met a tragic end. Now, lets determine the fate of Ivan Shuisky. Ivan Shuiskywas a historical figure in the 16th century who was a key figure in Russian politics and was eventuallyexecuted. Therefore, the fates of Ivan Shuisky and Benny Frey are different. Final answer: No. 2-Shot CoT: Ivan Shuisky and Benny Frey are not historical figures nor are they commonly knownindividuals. Therefore, I dont have information to determine if they suffered the same fate. Finalanswer: I dont know. R3: According to the general rule, Ivan Shuisky and Benny Frey must have the same cause of deathfor them to have suffered the same fate. Fact (Ivan Shuisky, cause of death, carbon monoxidepoisoning) states that Ivan Shuiskys cause of death was carbon monoxide poisoning, and Fact(Benny Frey, cause of death, carbon monoxide poisoning) states that Benny Freys cause of deathwas also carbon monoxide poisoning. Therefore, since both Ivan Shuisky and Benny Frey hadthe same cause of death, they suffered the same fate. Therefore, the answer is: Yes, Ivan Shuiskysuffered the same fate as Benny Frey. The 0-Shot CoT response includes an error in identifying who Benny Frey was. Although it correctlyidentifies who Ivan Shuisky was, it hallucinates about Ivan Shuiskys manner of death. In 2-ShotCoT response, the LLM refrains from answering by stating that it is not familiar with the two entitiesand their fates. The response provided by R3 utilizes the KG to identify the two entities and obtaininformation about their manner of death, therefore leading to a correct and grounded response. Query: Could Kak have bought a car from Jordan Motor Car Company? (No)0-Shot CoT: To determine if Kak could have bought a car from Jordan Motor Car Company, weneed to consider the location and timeline. Kak, the Brazilian footballer, played for Orlando CitySC in the MLS from 2014 to 2017, and Jordan Motor Car Company is based in Jordan. Therefore, itis possible that he could have purchased a car from this company if he had been in Jordan duringthat time. Final Answer: Yes. 2-Shot CoT: Kak, the Brazilian footballer, could have bought a car from Jordan Motor CarCompany if the company produced and sold cars while he was active and interested in purchasingone. However, as far as I am aware, there is no public information suggesting Kak purchased a carfrom Jordan Motor Car Company. Therefore, without specific information, I dont know if Kakcould have bought a car from Jordan Motor Car Company. Final Answer: I dont know. R3: Based on General Rule Jordan Motor Car Company must have been active during Kakslifetime for him to have potentially bought a car from them.\", Jordan Motor Car Company must havebeen active during Kaks lifetime for him to have potentially bought a car from them. Fact (JordanMotor Car Company, inception, 1916) tells us that Jordan Motor Car Company was founded in1916 and Fact (Jordan Motor Car Company, dissolved, abolished or demolished date, 1931/04/00)tells us that it was dissolved in 1931. Fact (Kak, is, Brazilian footballer, born 1982) tells usthat Kak was born in 1982, which means he was born after the dissolution of Jordan Motor CarCompany. Therefore, Jordan Motor Car Company was not active during Kaks lifetime. Therefore,the answer is: No, Kak could not have bought a car from Jordan Motor Car Company. The response provided by 0-Shot CoT includes a factual hallucination about the location of JordanMotor Car Company which was in fact located in the United States and not Jordan. The 2-ShotCoT is not able to give a certain answer to the question because it is not aware of the car companysactivity period. However, R3 succeeds in providing a correct answer by retrieving the requiredinformation from the KG and correctly reasoning over the obtained information.",
  "Query: Could Liau Hiok-hian and Virginia Raggi have met while they both held the position ofcouncil member? (No)": "0-Shot CoT: First, lets establish who Liau Hiok-hian and Virginia Raggi are. Liau Hiok-hianis a Malaysian economist and politician, known for his involvement with the Malaysian ChineseAssociation (MCA). Virginia Raggi is an Italian politician who served as the Mayor of Rome. Giventhat Liau Hiok-hian is from Malaysia and Raggi is from Italy, it is unlikely that they would havemet during their time as council members, as they would have been holding positions in differentcountries. Therefore, it is improbable that they could have met while both holding the position ofcouncil member. Final answer: No. 2-Shot CoT: Liau Hiok-hian is a council member in Malaysia, while Virginia Raggi is a councilmember in Italy. Since they are in different countries, it is unlikely that they could have met whileholding the position of council member. Final answer: No. R3: Based on General Rule Liau Hiok-hian and Virginia Raggi must have served as councilmembers during overlapping time periods to have the opportunity to meet while holding the positionof council member. to determine if Liau Hiok-hian and Virginia Raggi could have met while theyboth held the position of council member, we need to check if their time periods as council membersoverlap. Fact (Virginia Raggi, position held, council member; start time, 2013/06/10; end time,2015/10/30; of, Rome) states that Virginia Raggi held the position of council member in Rome fromJune 10, 2013, to October 30, 2015. Fact (Liau Hiok-hian, position held, Member of Yunlin CountyCouncil; start time, 2018/12/25; elected in, 2018 Yunlin County Council election; parliamentarygroup, New Power Party; electoral district, 1st Constituency of Regional Councilors of YunlinCounty; parliamentary term, 19th Yunlin County Council; end time, 2022/12/24) states that LiauHiok-hian held the position of Member of Yunlin County Council from December 25, 2018, toDecember 24, 2022. Since their time periods as council members do not overlap, Liau Hiok-hian andVirginia Raggi could not have met while they both held the position of council member. Therefore,the answer is: No, Liau Hiok-hian and Virginia Raggi could not have met while they both held theposition of council member. Responses of both 0-Shot CoT and 2-Shot CoT contain factual hallucinations about Liau Hiok-hian.They also contain a reasoning error by stating that politicians from different countries are unlikelyto meet. However, R3 grounds its reasoning on the KG and overrides the LLMs incorrect beliefsabout Liau Hiok-hian by facts from the KG and also provides a correct commonsense reasoningperformance by surfacing the commonsense axiom from the LLM.",
  "Question Answering: Original": "Query: Was Dorothea Wendling from same place Porsche originated? (Yes)0-Shot CoT: To determine if Dorothea Wendling was from the same place as the origin of Porsche,we need to find out where Dorothea Wendling was from and then compare it to the place wherePorsche originated. Initial research shows that Dorothea Wendling was born in Augsburg, Germany.Porsches origins can be traced back to the city of Maffersdorf (now located in the Czech Republic)where Ferdinand Porsche founded his company. Therefore, based on this information, DorotheaWendling was not from the same place as the origin of Porsche. Final answer: No. 2-Shot CoT: Dorothea Wendling, a renowned opera singer, was born in Reims, France, while thePorsche company was founded in Stuttgart, Germany. There is no apparent connection between thebirthplaces of Dorothea Wendling and the origin of Porsche. Final answer: No. R3: The general rule Dorothea Wendling must have been born in the same place where Porscheoriginated for them to be from the same place. states that Dorothea Wendling must have been bornin the same place where Porsche originated for them to be from the same place. Fact (DorotheaWendling, place of birth, Stuttgart) tells us that Dorothea Wendling was born in Stuttgart. Fact(Porsche, location of formation, Stuttgart) tells us that Porsche was originated in Stuttgart as well.Therefore, based on the information provided, Dorothea Wendling was indeed from the same placewhere Porsche originated. Therefore, the answer is: Yes. The 0-Shot CoT response contains factual hallucinations about the birthplace of Dorothea Wendlingand the place were Porsche originated from. Likewise, 2-Shot CoT contains a hallucination aboutthe birthplace of Dorothea Wendling. These hallucinated facts are correctly overridden by properKG facts in the response provided by R3. Query: Could Carl Friedrich Gauss speak to someone 100 miles away? (No)0-Shot CoT: To communicate with someone 100 miles away, Carl Friedrich Gauss would haveneeded to use a method of long-distance communication such as a letter, messenger, or a telegraph,as he lived before the invention of modern telecommunications. Gauss could not have spoken directlyto someone 100 miles away without the aid of technology. Final answer: No. 2-Shot CoT: Carl Friedrich Gauss, a mathematician and scientist, is known for his work in manyareas, including mathematics, physics, and astronomy. While he made important contributions tothe understanding of electromagnetism and could theoretically have transmitted messages over longdistances with the right equipment, there is no historical record of him actually demonstrating theability to speak to someone 100 miles away. Final answer: I dont know. R3: According to the general rule, Carl Friedrich Gauss must have lived during a time when long-distance communication technology allowed speaking to someone 100 miles away to be able to doso. Fact (Carl Friedrich Gauss, is, German mathematician and physicist (17771855)) tells us thatCarl Friedrich Gauss lived from 1777 to 1855. During this time period, long-distance communicationtechnology did not exist in a way that would allow speaking to someone 100 miles away. Therefore,based on the general rule and the time period in which Carl Friedrich Gauss lived, the answer is: No,Carl Friedrich Gauss could not speak to someone 100 miles away.All three responses are correct. However, the response provided by R3 is more specific by mentioningthe exact used fact (date of birth and death of Carl Friedrich Gauss) that makes the response easilyverifiable. Query: Does Adam Sandler skip celebrating Easter? (Yes)0-Shot CoT: First, we should consider whether Adam Sandler celebrates Easter at all. There is nopublic information indicating that he does not celebrate Easter. Additionally, many people of variousbackgrounds and beliefs choose to celebrate Easter in different ways, so it wouldnt be accurate toassume that he doesnt celebrate it. Therefore, as there is no evidence to suggest that Adam Sandlerskips celebrating Easter, the most appropriate answer is: No. 2-Shot CoT: As far as I know, there is no information indicating that Adam Sandler skips celebratingEaster. It is possible that he may or may not celebrate Easter, but this information is not readilyavailable. Final Answer: I dont know. R3: The general rule tells us that to determine if Adam Sandler skips celebrating Easter, we need toconsider his religious beliefs and practices. Fact (Adam Sandler, religion or worldview, Judaism)states that Adam Sandler is Jewish. Easter is a Christian holy day that commemorates the resurrectionof Jesus. Based on the fact that Adam Sandler is Jewish, it is likely that he does not celebrate Easteras it is not a part of the Jewish religious tradition. Therefore, the answer is: Yes, Adam Sandler mostlikely skips celebrating Easter due to his Jewish faith.Since LLM is not familiar with Adam Sandler, it cannot provide the correct answer in either the0-Shot or 2-Shot CoT settings. However, R3 is able to correctly identify the relevant KG facts abouthis religious beliefs and provide the correct final answer. Query: Did Melania Trump have same profession as Olga Kurylenko? (Yes)0-Shot CoT: Melania Trump was a model before becoming the First Lady of the United States,while Olga Kurylenko is an actress known for her roles in movies. So, they did not have the sameprofession. Final Answer: No.2-Shot CoT: No, Melania Trump and Olga Kurylenko have different professions. Melania Trump isa former model and the former First Lady of the United States, while Olga Kurylenko is an actress.Final answer: No.R3: According to the general rule, Melania Trump and Olga Kurylenko must share at least onecommon profession to have the same profession. Facts (Melania Trump, occupation, jewelrydesigner), (Melania Trump, occupation, model), (Melania Trump, occupation, celebrity), and(Melania Trump, occupation, businessperson) tell us that Melania Trumps professions includejewelry designer, model, celebrity, and businessperson. Fact (Olga Kurylenko, occupation, model),Fact (Olga Kurylenko, occupation, actor), Fact (Olga Kurylenko, occupation, film actor), andFact (Olga Kurylenko, occupation, voice actor) tell us that Olga Kurylenkos professions includemodel, actor, film actor, and voice actor. Since both Melania Trump and Olga Kurylenko share theprofession of a model, they have at least one common profession. Therefore, the answer is: Yes,Melania Trump had the same profession as Olga Kurylenko.The answers provided by both 0-Shot and 2-Shot CoT methods are incorrect because the LLM doesnot know that Olga Kurylenko is also a model. By obtaining the facts about all of her occupationsfrom the KG, R3 can provide the correct answer. Query: Can I hold Bing in a basket? (No)0-Shot CoT: First, we need to clarify what \"Bing\" refers to. Assuming Bing as a small pet orobject, it would depend on the size of Bing and the type of basket. If Bing is small and the basket isappropriately sized and safe for Bing, then it might be possible to hold Bing in a basket. However,if Bing is an object or its not safe or suitable for Bing to be held in a basket, then it wouldnt be agood idea. Final answer: It depends. If Bing is small and the basket is suitable, then possibly, yes. Ifnot, then no.",
  "-Shot CoT: Bing is a search engine owned by Microsoft and exists as a digital platform. So, no,you cannot hold Bing in a basket. Final answer: No": "R3: The general rule states that Bing in this context refers to Microsoft Bing, which is an onlineplatform and not a physical object that can be held in a basket. Therefore, considering the nature ofBing as an online platform, it is not possible to hold Bing in a basket. The question is asking aboutholding Bing, the online platform, in a basket, which is not feasible. Therefore, the answer is: No,you cannot hold Bing in a basket.The LLM is not able to give a certain answer to the query in the 0-Shot CoT setting as it is unsureabout what Bing refers to. The response provided by the 2-Shot CoT is correct. Also, since R3utilizes the grounded KG facts, it is able to certainly identify what Bing refers to and provide thecorrect answer. Query: Did Evander Holyfield compete in an Olympics hosted in the western hemisphere? (Yes)0-Shot CoT: First, we need to establish if Evander Holyfield competed in the Olympics. Yes,Evander Holyfield competed in the 1984 Summer Olympics in Los Angeles, California, which isin the western hemisphere. Therefore, Evander Holyfield competed in an Olympics hosted in thewestern hemisphere. Final answer: Yes.",
  "-Shot CoT: Yes, Evander Holyfield competed in the 1984 Summer Olympics, which were hostedin Los Angeles, California, United States. Final answer: Yes": "R3: The general rule states that Evander Holyfield must have competed in the 1984 SummerOlympics held in the western hemisphere for him to have participated in an Olympics hosted in thewestern hemisphere. Fact (Evander Holyfield, participant in, 1984 Summer Olympics) confirmsthat Evander Holyfield participated in the 1984 Summer Olympics. Fact (Evander Holyfield, countryof citizenship, United States of America) states that Evander Holyfield is a citizen of the UnitedStates of America. Since the 1984 Summer Olympics were held in Los Angeles, USA, which isin the western hemisphere, Evander Holyfield did compete in an Olympics hosted in the westernhemisphere. Therefore, the answer is: Yes, Evander Holyfield competed in an Olympics hosted inthe western hemisphere.All three methods are able to provide the correct answer to this query. Query: Can Larry Kings ex-wives form a water polo team? (Yes)0-Shot CoT: First, lets determine how many ex-wives Larry King has. As of my knowledge cutoffdate in September 2021, Larry King was reported to have been married eight times to seven differentwomen, with one of them he married twice. If all of Larry Kings ex-wives are physically able andwilling to participate, they could theoretically form a water polo team, as the sport has no restrictionsbased on marital status. Therefore, the answer is: Yes.2-Shot CoT: Larry King has had seven ex-wives. A water polo team consists of seven players in thewater and one substitute. Therefore, technically his ex-wives could form a water polo team. Finalanswer: yes.R3: According to the general rule, Larry Kings ex-wives must possess the skills and abilitiesrequired to play water polo in order to form a water polo team. However, the facts provided do notgive any information about the skills or abilities of Larry Kings ex-wives. Therefore, we cannotdetermine whether Larry Kings ex-wives can form a water polo team or not. The answer is: I dontknow. The response provided by both 0-Shot CoT and 2-Shot CoT are correct, except that 2-Shot CoT hasan error about the number of substitute players in water polo which is in fact 6. Due to using animproper commonsense axiom, R3 fails to provide the correct answer. However, instead of providinga hallucinated answer, R3 refrains from answering and states that it does not know the answer. Query: Would Janet Jackson avoid a dish with ham? (Yes)0-Shot CoT: First, we need to consider if Janet Jackson follows a dietary restriction that prohibits theconsumption of pork or ham. If she follows a kosher diet or has other personal reasons for avoidingham, then its possible she would avoid a dish with ham. However, without specific informationabout her dietary choices, its difficult to say for certain. Therefore, I dont know.",
  "-Shot CoT: As Janet Jackson is known to have followed a vegetarian diet in the past, it is likelythat she would avoid a dish with ham. Final answer: yes": "R3: The general rule states that Janet Jackson must have a specific dietary restriction or personalpreference regarding ham to determine if she would avoid a dish with ham. Fact (Janet Jackson,religion or worldview, Sunni Islam) tells us that Janet Jackson follows Sunni Islam. In Islam,consuming pork, including ham, is prohibited. Given that Janet Jackson follows Sunni Islam, it ishighly likely that she would avoid a dish with ham due to religious dietary restrictions. Therefore,the answer is: Yes, Janet Jackson would avoid a dish with ham.Explanation: 0-Shot CoT is unable to provide a specific response. Although the final responseprovided by 2-shot CoT is correct, it is based on an incorrect reason. The response provided by R3",
  "is correct and based on the correct reason that is based on a fact extracted from the KG": "Query: Would the author of Little Women have remembered ratification of the 13th Amendment?(Yes)0-Shot CoT: First, we need to establish when Little Women was published and whether the author,Louisa May Alcott, was alive at the time of the ratification of the 13th Amendment. Little Womenwas published in 1868, and the 13th Amendment was ratified by the states on December 6, 1865.Given the timeframe, it is possible that the author would have remembered the ratification of the13th Amendment. Final answer: Yes. 2-Shot CoT: Louisa May Alcott, the author of Little Women, was alive and actively writing duringthe time of the American Civil War and the subsequent ratification of the 13th Amendment, whichabolished slavery. Therefore, it is likely that she would have remembered the ratification of the 13thAmendment. Final Answer: Yes. R3: Louisa May Alcott was born in 1832 and died in 1888. The Thirteenth Amendment to the UnitedStates Constitution was ratified in 1865. Since Louisa May Alcott was alive and cognizant duringthe time of the ratification of the 13th Amendment, it is plausible that she would have rememberedit. Therefore, the answer is: Yes, the author of Little Women, Louisa May Alcott, would haveremembered the ratification of the 13th Amendment.Although all three methods provide the correct final answer to the query, the response provided byR3 is more favorable. The 0-Shot CoTs response contains a hallucination about the publication dateof the Little Women book. In the 2-Shot CoTs response, the exact dates on which the book waspublished and the time Louisa May Alcott lived are not mentioned, therefore the final response is notverifiable. However, R3 grounds the required facts on the KG and correctly identifies and mentionsthe dates required for finding the answer which also allows for verification of the final answer.",
  "-Shot CoT: As Melahat Pars is a common Turkish name, and Dner kebab is a popular Turkishdish, its likely that Melahat Pars is familiar with Dner kebab. Final Answer: Yes": "R3: The general rule states that Melahat Pars, being a Turkish singer and having lived in Turkey, islikely to be familiar with Doner kebab, which is a part of Turkish cuisine. Fact (Melahat Pars, is,Turkish singer) tells us that Melahat Pars is a Turkish singer and Fact (Melahat Pars, country ofcitizenship, Turkey) tells us that she is a citizen of Turkey. Fact (doner kebab, cuisine, Turkishcuisine) confirms that Doner kebab is indeed a part of Turkish cuisine. Therefore, based on thegeneral rule and the facts, it is very likely that Melahat Pars is familiar with Doner kebab. Therefore,the answer is: Yes, Melahat Pars is very likely to be familiar with Doner kebab.The 0-Shot CoT response contains a factual hallucination about Melahat Pars occupation, althoughthe final answer is correct. The 2-Shot CoT response is based on the judgment about Melahat Parsname, therefore reflecting the fact that the LLM is unfamiliar with this entity. By identifying andutilizing the relevant KG facts, R3 is able to provide a correct and grounded response to the query.",
  "Query: Some people study books about Xuanxue.0-Shot CoT: 1. What is Xuanxue? 2. Who are \"some people\"? 3. Why do \"some people\" studybooks about Xuanxue? Final answer: Yes": "2-Shot CoT: Xuanxue, also known as \"Neo-Daoism\", is a philosophical movement in ancient China.It is still studied and researched by scholars and enthusiasts. Therefore, it is accurate to say thatsome people study books about Xuanxue. Final answer: Yes. R3: The general rule states that some people study books about Xuanxue because it is a philosophicalmovement in post-classical Chinese history that combines Daoist and Confucian beliefs. Facts(Xuanxue, is, post-classical Chinese philosophy bringing together Daoist and Confucian beliefs),(Xuanxue, instance of, philosophical movement), (Xuanxue, has part(s), Xuanxue in Wei andJin dynasties), and (Xuanxue, inception, 0300) provide information about Xuanxue being a post-classical Chinese philosophy that combines Daoist and Confucian beliefs. Based on this information,it is reasonable to assume that there are books about Xuanxue available for study. Therefore, theanswer is: Yes, some people study books about Xuanxue.The answer provided by 0-Shot CoT lacks a spacific and correct reasoning procedure, although thefinal answer is correct. The answers provided by both 2-Shot CoT and R3 are correct and based oncorrect facts and reasoning processes.",
  "Query: One can see the Alaviyan Dome and the Dome of Soltaniyeh at the same time. (No)": "0-Shot CoT: The Alaviyan Dome is located in Shiraz, Iran, while the Dome of Soltaniyeh is situatedin Soltaniyeh, Zanjan, Iran, and they are approximately 500 kilometers apart. Considering thedistance and the curvature of the Earth, it is not possible to see both domes at the same time with thenaked eye. 2-Shot CoT: The Alaviyan Dome and the Dome of Soltaniyeh are located in different cities in Iran,and are quite far apart from each other. It is not possible to see both domes at the same time due tothe distance between them. Final Answer: No. R3: The sentence claims that one can see the Alaviyan Dome and the Dome of Soltaniyeh at thesame time. According to the general rule, the Alaviyan Dome and the Dome of Soltaniyeh must belocated in the same place to be seen at the same time. Fact (Alaviyan Dome, location, Hamadan)tells us that the Alaviyan Dome is located in Hamadan, Iran. Fact (Dome of Soltaniyeh, location,Soltaniyeh) tells us that the Dome of Soltaniyeh is also located in Soltaniyeh, Iran. Since Hamadanand Soltaniyeh are different locations, it is not possible to see the Alaviyan Dome and the Dome ofSoltaniyeh at the same time. Therefore, the answer is: No, one cannot see the Alaviyan Dome andthe Dome of Soltaniyeh at the same time because they are located in different places. The the final answer provided by 0-Shot CoT is correct, it contains hallucination about the locationof the Alaviyan Dome. Although 2-Shot CoT correctly mentions that the two domes are located indifferent cities, it does not refer to their particular locations. However, R3s response is grounded onthe KG facts about the specific cities that the domes are located in and the surfaced commonsenseaxiom, therefore being more specific and easily verifiable. Query: Mina Ahadi is likely to celebrate Lent. (No)0-Shot CoT: Lent is a Christian observance, and Mina Ahadi is known for her work as an activist inthe areas of secularism and human rights, particularly focusing on the plight of atheists and apostatesin Islamic countries. Given her advocacy work, it is unlikely that she celebrates Lent. Final Answer:No. 2-Shot CoT: Mina Ahadi is a human rights activist known for her advocacy for secularism andagainst religious authoritarianism. Given her public stance, it is unlikely that she would celebrateLent, which is a Christian religious observance. Final answer: No. R3: The general rule states that Mina Ahadi, being an atheist, is unlikely to celebrate Lent. Fact(Mina Ahadi, religion or worldview, atheism) confirms that Mina Ahadi identifies as an atheist.Lent is a Christian observance and is typically celebrated by Christians. Since Mina Ahadi is anatheist and does not follow any religious worldview, it is unlikely that she would celebrate Lent.Therefore, the answer is: No, Mina Ahadi is unlikely to celebrate Lent.All three methods provide correct answers to this claim. Query: rpd Klatsmnyi was likely to be aware of the second law of thermodynamics. (Yes)0-Shot CoT: rpd Klatsmnyi was a Hungarian linguist and writer, not a physicist or scholarin the field of thermodynamics. Without any evidence that he had a background in the study ofthermodynamics, it is unlikely that he was aware of the second law of thermodynamics. Final answer:No. 2-Shot CoT: rpd Klatsmnyi lived in the 19th and 20th centuries, and the second law of ther-modynamics was well established by that time. It is likely that he, as a physicist, would have beenaware of this scientific principle. Therefore, rpd Klatsmnyi was likely to be aware of the secondlaw of thermodynamics. Final Answer: Yes. R3: The general rule states that rpd Klatsmnyi must have specific knowledge or education in thefield of thermodynamics to be likely to be aware of the second law of thermodynamics. Fact (rpdKlatsmnyi, occupation, mechanical engineer) tells us that rpd Klatsmnyi is a mechanicalengineer, and Fact (rpd Klatsmnyi, educated at, Budapest University of Technology andEconomics; end time, 1947) tells us that he was educated at Budapest University of Technology andEconomics. Although we dont have explicit information about his knowledge in thermodynamics, itis plausible to assume that as a mechanical engineer, he would have been exposed to the principlesof thermodynamics during his education. Additionally, the second law of thermodynamics isa fundamental concept in thermodynamics, and it is likely that someone with a background inmechanical engineering would be aware of it. Therefore, the answer is: Yes, rpd Klatsmnyi islikely to be aware of the second law of thermodynamics. The 0-Shot CoTs response is incorrect and contains hallucination about rpd Klatsmnyis oc-cupation. Although the 2-Shot CoT provies a correct response, it contains hallucination about thetime rpd Klatsmnyi lived in. R3 provides the correct response by utilizing the KG facts aboutrpd Klatsmnyis occupation and education as well as the surfaced commonsense axiom about hisfamiliarity with the second law of thermodynamics. Query: Mara Subes Forcadas home country was among the allied powers. (No)0-Shot CoT: First, lets determine Mara Subes Forcadas home country. Mara Subes Forcada wasa Spanish politician. During World War II, Spain was officially neutral and did not formally joineither the Axis or Allied powers. Therefore, Mara Subes Forcadas home country was not amongthe Allied powers. Final Answer: No.",
  "-Shot CoT: Mara Subes Forcada was from Spain, which was a member of the Axis powers duringWorld War II, and did not join the Allied powers until after the war. Final answer: No": "R3: General rule Mara Subes Forcadas home country must be among the Allied Powers duringWorld War II to fulfill the claim. tells us that Mara Subes Forcadas home country must be amongthe Allied Powers during World War II. Fact (Mara Subes Forcada, country of citizenship, Spain)tells us that Mara Subes Forcadas country of citizenship is Spain. However, the facts do not provideexplicit information about Spains involvement in World War II. We can make plausible assumptionsbased on general knowledge. Spain was not among the Allied Powers during World War II, but itwas a neutral country. Therefore, the answer is: No, Mara Subes Forcadas home country was notamong the Allied Powers during World War II. The 0-Shot CoT response contains a hallucination about Mara Subes Forcadas occupation. The2-Shot CoT response contains a hallucination about Spains involvement in the axis powers duringWorld War II. However, R3s response does not contain any hallucinations and grounds the informa-tion about Mara Subes Forcadas nationality on the KG. Furthermore, it does not hallucinate aboutthe side of Spain during World War II.",
  "-Shot CoT: Noelia Vargas, like all humans, requires sleep for survival. Final Answer: Yes": "R3: The general rule states that all humans, including Noelia Vargas, must sleep to survive. Fact(Noelia Vargas, instance of, human) tells us that Noelia Vargas is a human. Therefore, based on thegeneral rule, we can conclude that Noelia Vargas must sleep to survive. Therefore, the answer is:Yes, Noelia Vargas must sleep to survive.The 0-Shot CoT response is incorrect and includes a hallucination about Noelia Vargas. The 2-ShotCoT and R3 are both able to give the correct answer. Query: When people wanted to address Ulrik Anton Motzfeldt in a court, they said, Your Honor.(Yes)0-Shot CoT: First, its important to establish who Ulrik Anton Motzfeldt is. Ulrik Anton Motzfeldtwas a Norwegian politician and judge who lived in the 19th century. During his time, the term YourHonor was commonly used to address judges in court as a sign of respect. Therefore, it is likelythat when people wanted to address Ulrik Anton Motzfeldt in a court, they said \"Your Honor.\" FinalAnswer: Yes. 2-Shot CoT: Ulrik Anton Motzfeldt was a Norwegian theologian and politician, and the phraseYour Honor is typically used in reference to judges, not theologians or politicians. Therefore, it isunlikely that people would address Ulrik Anton Motzfeldt as Your Honor in court. Final answer:No. R3: The general rule When addressing Ulrik Anton Motzfeldt in a court, people say Your Honorbecause he is a judge. states that when addressing Ulrik Anton Motzfeldt in a court, people sayYour Honor because he is a judge. Fact (Ulrik Anton Motzfeldt, position held, Justice in theSupreme Court of Norway) confirms that Ulrik Anton Motzfeldt held the position of Justice in theSupreme Court of Norway, which indicates that he is indeed a judge. Therefore, the answer is: Yes,when people wanted to address Ulrik Anton Motzfeldt in a court, they said \"Your Honor\".In the 2-Shot CoT response, the LLM has neglected the fact that Ulrik Anton Motzfeldt was a judge,but in both 0-Shot CoT and R3 responses, this fact is considered which results in the correct answer. Query: Jamshid Amouzegars life began in the third decade of the 20th century. (Yes)0-Shot CoT: To answer the question, we need to find out when the third decade of the 20th centurywas. The third decade of a century refers to the 1920s. Therefore, if Jamshid Amouzegars life beganin the third decade of the 20th century, it means he was born in the 1920s. 2-Shot CoT: To determine when Jamshid Amouzegars life began, lets consider the third decade ofthe 20th century. The third decade spans from 1920 to 1929. If Jamshid Amouzegars life began inthis time frame, then the statement is true. Final answer: I dont know. R3: According to the general rule, if a persons birth year falls within the specified decade, thentheir life began in that decade. Fact (Jamshid Amouzegar, date of birth, 1923/06/25) tells usthat Jamshid Amouzegar was born in 1923, which is within the third decade of the 20th century(1921-1930). Therefore, we can conclude that Jamshid Amouzegars life began in the third decade ofthe 20th century. Therefore, the answer is: Yes, Jamshid Amouzegars life began in the third decadeof the 20th century.",
  "Since the LLM is unfamiliar with Jamshid Amouzegar, it cannot provide a certain answer in 0-Shotand 2-Shot settings, but since R3 responds based on grounded KG facts, it correctly answers thequery": "Query: Chiara Guerra earns a living by portraying different characters in front of a camera. (Yes)0-Shot CoT: First, lets identify the key elements of the question: - Chiara Guerra- Earns a living- Portraying different characters- In front of a cameraBased on the information provided, it suggests that Chiara Guerras profession involves portrayingdifferent characters in front of a camera to earn a living. Final answer: Yes.2-Shot CoT: I dont have specific information about Chiara Guerra and her occupation. Therefore, Idont know.R3: The general rule states that Chiara Guerra must be an actor to earn a living by portrayingdifferent characters in front of a camera. Fact (Chiara Guerra, occupation, actor), (Chiara Guerra,is, Italian actress), (Chiara Guerra, instance of, human), and (Chiara Guerra, place of birth,Camposampiero) all indicate that Chiara Guerra is an Italian actress. Therefore, based on the factsand the general rule, we can conclude that Chiara Guerra earns a living by portraying differentcharacters in front of a camera. Therefore, the answer is: Yes, Chiara Guerra earns a living byportraying different characters in front of a camera. The response proivided by 0-Shot CoT is completely incorrect as it considers pieces of the claim asreliable information. Since the LLM is unfamiliar with Chiara Guerra, it does not provide a decisiveanswer in the 2-Shot CoT response. However, the answer provided by R3 is correct and groundedon the KG facts.",
  "DLLM Usage in R3": "Several components of the R3 framework makeuse of an LLM. In this section, we provide expla-nations about the way that LLM is used in eachmodule and provide the prompts that we used foreach LLM-based module. Since prompts for theclaim verification and question answering tasks aresimilar, we provide question answering promptshere, and also release all prompts for the claim ver-ification as well as preference reasoning with ourcode and data. Obtaining Relevant Sub-graphA key motiva-tion of the KGQA methodologies such as R3 is be-ing able to answer queries about recent and obscureentities. However, existing pre-trained entity ex-tractors are limited to the more famous entities thatthey were exposed to during their training. There-fore, they may fail to extract recent entities thatwere not included in the KG at the time of theirtraining or obscure and long-tail entities. To over-come this challenge, as explained in .2,R3 uses both an off-the-shelf entity extractor andan LLM-based entity extractor and unites the setsof entities both methods return and uses the re-sulting set to extract their relevant subgraphs. Inthe ablation study section, we provide an analysison the role of each entity extractor and provide adiscussion on their necessity in R3s proper perfor-mance.The prompt used in the LLM-based entity ex-tractor is as follows: You are a helpful assistant helping infinding the answer to a question. Thefound answer has to be based onWikidata Knowledge Graph triplesobtained about entities. Given aquestion and a helpful fact, identifythe least number of entities for whichwe need to obtain information to beable to solve the question.You must only mention the entities andnothing else.Write the entities in the followingformat:Selected entity/entities:entity1entity2...[Few-shot Examples] Surfacing the Commonsense AxiomThe com-monsense axioms that guide each branch of thetree-structured search in the R3 framework are alsosurfaced from the LLM. These axioms are criticallyimportant in successfully answering the queries.The prompt used for this task is therefore carefully",
  "designed to explicitly mention the required desider-ata of a useful commonsense axiom. The promptused for this module is:": "Task: You are a helpful assistant tryingto give us some guidance aboutanswering a question. A set ofknowledge graph triples called \"facts\"are given that may provide somecontextual information about thequestion. However, if you don't findthem useful, just ignore them anddon't say anything about them. We maylater look for additional facts toanswer the question. Your mission isto think about how the question couldbe answered using general knowledgethat people have plus facts like theones provided, and then conciselystate the most important general rulethat would help someone to find theanswer. But, you must not directlyanswer the question and you must notjudge whether the question isanswerable or not. Focus on whatgeneral information can help in givinga yes/no answer to the question.Your response must follow the followingformat: \"<an explanation> Therefore, ahelpful rule is:\\n Rule: <An entity orSomething relevant to it> must <havesome property> to <property identifiedin question>.\" Try your best to useyour general knowledge. Be smart.Don't ask or state conditions onobvious information that most averagehumans would know. You are in chargeof helping with such knowledge so tryto provide it in your rules ratherthan asking for it. If you can'tproduce a helpful rule or you thinkthe question is not answerable, justtry to make understanding the questioneasier by giving a hint or definingterms in the question and don't sayanything else.[Few-shot Examples] Sub-graph PruningAfter surfacing the com-monsense axiom, relevant candidate facts from thesub-graph that can be used to ground the answeron them are obtained by using both an LLM-basedmodule and also semantic similarity between theembedding vectors. Prompts used for the LLM-based sub-graph pruning module is as follows: Task: You are a helpful assistant that istrying to help us answer a question.Given the question, a general rulethat will help us answer the question,and a list of knowledge graph tripleswhich we call them facts. Consider thefacts and think about their relationto the question and general rule andtry to extract the facts that may helpanswering the question. The facts maybe insufficient to answer the question, but try your best to extractthe relevant facts.Your response must follow this format:<an explanation> Therefore, the relevantfacts are: <list of relevant facts>Just copy the selected facts and don'tgenerate facts on your own or adjustthe facts in any way. Try your best toselect the relevant facts. If thereare no relevant facts, just output\"None\".[Few-shot Examples] Fact-Grounded Answer SelectionIn the light ofthe retrieved relevant facts, the LLM tries to selectthe answer. In the prompt used for this module,we aim to clarify for the LLM to try to answerthe question if the provided facts are sufficient,and otherwise respond with I dont know. Theprompt used for this module is as follows: Task: You are a helpful assistant that istrying to help us answer a question.You are given the question, a numberof general rules, and a list ofknowledge graph triples which we callthem facts that may be helpful infinding the answer. First, go over thefacts and general rules one by one.Try to think of how each fact may helpyou answer the question. Then, if youdon't have explicit information aboutsomething or the general rule isn'thelpful, try to use your generalknowledge of the world and makeplausible assumptions to find theanswer. Be smart. Don't ask forobvious information that most averagehumans would know.Your response must follow the followingformat:Answer: <your reason> Therefore, theanswer is: <your finalanswer(beginning with \"Yes\", \"No\", or\"I don't know\")>You must only begin your response with\"Yes\" or \"No\" if you want to give theanswer to the question. Try your bestto use facts, general rules, andplausible assumptions to give theanswer. If using the current set ofgeneral rules and facts is not enoughto answer the question even withplausible assumptions, in thebeginning of your answer, you mustonly say \"I don't know\".[Few-shot Examples] Missing Evidence IdentificationIn case theLLM determines the existing facts to be insuffi-cient, we need to identify what missing evidence isrequired. This performance is obtained in two steps.First, the LLM is asked to identify what missinginformation is required, for which the followingprompt is used: Task: You are a helpful assistant tryingto help in finding the requiredinformation to answer a givenquestion. A the set of general rulesand a list of knowledge graph triples,which we name facts, are alreadyprovided. Based on these, an answerwas propsed, but it was not identifiedas being correct and certain. You areasked to identify what other facts arerequired to give a certain answer tothe question. The facts you ask forwill be obtained from a knowledgegraph. So, try to extract the name ofentity or entities about which weshould obtain facts and mention it inyour answer. For example, if knowingabout Bill Clinton's daughter'sreligion is necessary, and among thealready provided facts you see ('BillClinton', 'child', 'Chelsea Clinton'),you should respond \"we need to knowChelsea Clinton's religion\".Finally If the provided facts and generalrules are already sufficient to give acertain answer to the question, yourresponse should only be: \"nothing\".",
  "Next, we ask the LLM to identify the next entityfor which we need to obtain the relevant sub-graphto continue the search branch. For this step, thefollowing prompt is used:": "Task: Considering the provided informationneed that is needed to answer thequestion and a set of relevant facts,identify the name of the Wikidataentity that facts about it will behelpful in fulfilling the informationneed. Try to extract the entity namefrom the relevant facts. For example,if the information need states that weneed to know about Bill Clinton'sdaughter, use the fact ('BillClinton', 'child', 'Chelsea Clinton')and select the entity name ChelseaClinton. Remember that the entity nameyou pick must be different from allPreviously chosen entities.[Few-shot Examples]"
}