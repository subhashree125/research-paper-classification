{
  "Abstract": "Teaching large language models (LLMs) to gen-erate text with citations to evidence sourcescan mitigate hallucinations and enhance verifi-ability in information-seeking systems. How-ever, improving this capability requires high-quality attribution data, which is costly andlabor-intensive. Inspired by recent advancesin self-improvement that enhance LLMs with-out manual annotation, we present START, aSelf-Taught AttRibuTion framework for iter-atively improving the attribution capability ofLLMs. First, to prevent models from stagnatingdue to initially insufficient supervision signals,START leverages the model to self-constructsynthetic training data for warming up. Tofurther improve the models attribution abil-ity, START iteratively utilizes fine-grained pref-erence supervision signals constructed fromits sampled responses to encourage robust,comprehensive, and attributable generation.Experiments on three open-domain question-answering datasets, covering long-form QAand multi-step reasoning, demonstrate signif-icant performance gains of 25.13% on aver-age without relying on human annotations andmore advanced models. Further analysis re-veals that START excels in aggregating infor-mation across multiple sources.",
  "Introduction": "The rapid development of large language models(LLMs) (OpenAI, 2023; Zhao et al., 2023) has ledto their prosperity as indispensable tools for infor-mation seeking. Despite their remarkable capabil-ity to generate fluent and informative responses touser queries, LLMs also struggle with hallucina-tions (Huang et al., 2023). To facilitate factualityverification, recent research (Bohnet et al., 2022)has explored attributed text generation, a paradigmthat enables LLMs to generate responses with cita-tions. By attributing models output to verifiable",
  "*Corresponding Author": "sources, it can improve the explainability and cred-ibility of LLM-generated content (Li et al., 2023).While beneficial, the ability to attribute con-textual sources is not inherent in LLMs. Mostwork induces LLMs to generate text with citationsvia in-context learning (Gao et al., 2023), whichis far from satisfactory (Liu et al., 2023). Thecurrent winning recipe for accurate attribution in-volves fine-tuning on high-quality attribution re-sponses1 (Li et al., 2024). However, acquiringsuch data typically requires either manual cura-tion (Malaviya et al., 2023), or distilled from themost advanced LLMs (Huang et al., 2024a,b), bothof which are costly and not scalable, thus limit-ing the growth of models attribution capability.One promising solution is self-improvement (Yuanet al., 2023), which has demonstrated the poten-tial to boost model performance by learning fromself-generated high-quality samples.Inspired by this, we aim to explore the poten-tial of self-improvement in bootstrapping the at-tribution ability of LLMs. However, achievingthis goal presents several challenges.One sig-nificant challenge lies in the risk of model stag-nation during the self-improvement process, pri-marily due to the insufficient supervision signalsobtained in the early stage. Concretely, consider-ing the inferior performance of LLMs in handlingthe attribution task (Gao et al., 2023), generatingsufficient high-quality attribution responses solelythrough sampling proves difficult. This scarcity ofhigh-quality samples limits the opportunities forLLMs to self-improve effectively. Another chal-lenge stems from the limitation of weak supervi-sion signals. Current self-improvement approaches(Yuan et al., 2023) primarily involve supervisedfine-tuning on high-quality samples while discard-ing low-quality ones. When applied to LLM attribu-",
  "Attribution responses refers to responses with in-linecitations, e.g.,": "tion, these high-quality samples provide only weaksupervision signals, mainly teaching LLMs on thesurface form of attribution (e.g., proper citationformat) (Li et al., 2024). Such practice may ne-glect the potential of exploring fine-grained signalsfrom low-quality samples to learn what constitutesa desirable attribution response.To address these challenges, we present START,a Self-Taught AttRibuTion framework designedto bootstrap the attribution capabilities of LLMs.To prevent models from stagnating early due toinsufficient supervision signals, we first leveragethe model to self-construct high-quality syntheticattribution data (3.1). The data synthesis processfollows reverse attribution thinking: the modelinitially generates a response to a given query,then breaks it into atomic claims, and finally ran-domly combines them to create synthetic docu-ments.This process not only simulates multi-source information-seeking scenarios but also en-sures precise attribution, as each document can bedirectly traced back to the specific claim it origi-nated from. These high-quality synthetic data arethen utilized for warming up, providing a goodstarting point for LLMs to self-improve. Further-more, to better explore fine-grained supervisionsignals for LLM attribution, we introduce an itera-tive self-improving recipe (3.2). Specifically, theframework meticulously designs fine-grained re-wards tailored for LLM attribution, covering robust-ness, comprehensiveness, and attributability. Byscoring multiple candidates through sampling andselecting those with the highest holistic rewardsfor supervised fine-tuning, the framework subse-quently utilizes low-quality samples to constructfine-grained preference pairs with diverse optimiza-tion rewards for preference optimization. This iter-ative process further fosters the self-improvementof attribution capabilities.We conduct extensive experiments across threeopen-domain question-answering datasets, cover-ing long-form QA and multi-step reasoning. Re-sults indicate that START achieves significant per-formance gains of 25.13% on average in citationquality. Moreover, START successfully achievesself-improvement in LLM attribution, showing pro-gressive improvements across iterations. Ablationstudies confirm that each component significantlycontributes to the improvement. Further analysisshows that START not only excels in generating su-perior attributable responses but also in effectivelyaggregating information across multiple sources.",
  "Large Language Model Attribution": "Attribution has gained significant attention for en-hancing the interpretability and verifiability ofLLMs (Gao et al., 2023; Li et al., 2023). Recentstudies have focused on improving LLM attribu-tion in a supervised way. Asai et al. (2023) firstdistill GPT-4 to collect high-quality attribution data,aiming to teach the model to generate grounded an-swers with citations through self-reflecting. Simi-larly, Huang et al. (2024a) develop a training frame-work starting with distilling ChatGPT, followedby designing reward models to teach the LLM togenerate highly supportive and relevant citations.Additionally, Li et al. (2024) model the attributiontask from a preference learning perspective, wherethey first fine-tune the model on human-labeled at-tribution datasets and then perform preference op-timization using synthesized preference data. Fur-thermore, Huang et al. (2024b) take this further byextending the attribution format to a fine-grainedcitation level, primarily distilled from ChatGPT. Itenables the model to first ground the fine-grainedquotes within the context and then condition thegeneration process on them. In contrast to thesemethods, START aims to bootstrap attribution ca-pability without relying on human-labeled data ordistilling from more capable LLMs.",
  "Self-Improvement for LLMs": "High-quality data either human-crafted or distilledfrom advanced LLMs has proven effective in en-hancing the performance of LLMs. However, ac-quiring such high-quality data can be prohibitivelyexpensive. Recently, self-improvement approaches(Glehre et al., 2023; Yuan et al., 2024), whereLLMs learn from self-generated samples haveemerged as a viable solution to compensate forthe scarcity of high-quality data. These methodstypically involve employing heuristic rules (Zelik-man et al., 2022), self-critique (Tian et al., 2024), ortraining additional verifiers (Hosseini et al., 2024)to assess the quality of model-generated samples.Such practices are particularly effective in rea-soning tasks, e.g., mathematical reasoning, whereLLMs already demonstrate capable abilities andcan receive precise feedback on correctness. How-ever, these advantages are absent in the attributiontask, due to its challenging nature. To bridge thegap, we take an initial step towards exploring thepotential of self-improvement in LLM attribution.",
  "Generation": ": The data synthesis pipeline consists of five steps: given a user query, the LLM first generates an informativeresponse without citations in a closed-book setting. Subsequently, the LLM decomposes this response into atomicclaims. These claims are then randomly grouped into specific sets, which serve as the basis for generatingdocuments that cover all included claims. Finally, we trace back to the initial response to relabel the citations.",
  "Problem Formulation and Methodology": "We follow a formulation of attributed text gener-ation as described in Gao et al. (2023). This taskinvolves processing a user query q for information-seeking, given a corpus of retrieved documentsD, to generate a response S with in-line cita-tions. We assume the response S as consistingof n statements, such that S = {s1, s2, . . . , sn}.Each statement si S cites a list of passageCi = {ci1, ci2, . . .}, where cij D. Citations arepresented in the form of , which representthe attribution to specific documents in D.Next, we present an overview of START, a train-ing framework designed to teach LLMs to self-improve their attribution ability, as illustrated in. START consists of two essential stages:synthetic data warm-up (3.1) and self-improvingfor LLM attribution (3.2).",
  "Synthetic Data Warm-Up": "The core of self-improvement lies in generatinghigh-quality samples and iteratively learning fromthem.Intuitively, a high-quality attribution re-sponse should not be distracted by irrelevant doc-uments (robustness) and capture high coverage ofviewpoints across multiple documents (comprehen-siveness) while maintaining high citation quality(attributability). However, existing LLMs typicallyshow inferior performance in the attribution task,significantly hindering their ability to generate suchhigh-quality samples. This limitation poses sub-stantial challenges to enhancing their attributioncapabilities through self-improvement.In this stage, we propose utilizing the model toself-construct high-quality synthetic data for warm- ing up, enabling the model to have the basic abilityto generate robust, comprehensive, and attributableresponses across multiple sources. The pipelineconsists of the following steps, shown in .More details can be found in Appendix A. Step 1: Response GenerationGiven an arbitrarymodel, we first sample a query q from seed ques-tions Q and then generate a long-form answer Sutilizing the parametric knowledge of the model it-self. The model is required to produce informativeanswers that cover multiple perspectives. Step 2: Claim DecompositionPrior work (Minet al., 2023) has explored using atomic claims asa fundamental unit in long-form text generation.Thus, for the response S, we ask the model todecompose it into atomic claims. Each atomicclaim represents a distinct piece of information. Step 3: Claim CombinationTo ensure that theresponse behaves as an aggregation of informationfrom multiple documents, we randomly combinedifferent claims into one claim set. This processhelps simulate the natural diversity of viewpointsand sources, thus enhancing the comprehensivenessand realism of the synthesized responses. Step 4: Document GenerationFor each claimset, we prompt the model to generate a syntheticdocument D that provides a comprehensive dis-cussion of the grouped claims. Additionally, toenhance the robustness of the response, we intro-duce irrelevant documents by uniformly samplingdocuments generated from other queries.",
  "comprehensive": ": Overview of our self-improving framework, which consists of two stages. The model is first warmedup using synthetic data (3.1). This provides a good starting point to enable the model to generate high-qualitysamples in the subsequent iterative training. Next, the model is further trained via rejection sampling fine-tuningand fine-grained preference optimization iteratively (3.2). This iterative process bootstraps the models attributioncapability by fully utilizing the supervision signals from its sampled generations. the generated documents. This process ensuresthat each claim within the response is explicitlyattributed to its source. In this way, for each queryq, and documents set D, we can obtain an infor-mative and attributable response while maintainingrobustness against irrelevant documents.Next, the model is fine-tuned for warming upwith the MLE objective on the synthesized dataset,which consists of N data entries, each containinga query qi, a document set Di, and a high-qualityattributable response yi:",
  "Self-Improving for LLM Attribution": "In this stage, we propose to iteratively boost themodels attribution capability by exploring morefine-grained supervision signals, rather than solelyrelying on golden responses in synthetic data. Thisinvolves leveraging rejection sampling for datagrowing and fine-grained preference optimizationfor capability evolution. 3.2.1Rejection Sampling Fine-tuningAfter warming up, we first sample N candidatesfor each query in the synthetic dataset and thenscore each candidate with fine-grained rewards thatcover three key dimensions: robustness, compre-hensiveness, and attributability.",
  "(2)where S is the total number of statements in theresponse and Entail returns 1 if the statement i isentailed by cited documents, and 0 otherwise": "Robustnessmeasures the degree to which amodel-generated response is influenced by irrel-evant contexts. Considering that we can identifyrelevant documents dr within the document set Dfor each query q, thus we quantify robustness bycalculating the probability difference of the modelM to generate the response y under different con-texts. The robustness score is defined as follows:",
  "Here, I is an indicator function that returns 1 ifAttrScore = 1, and 0 otherwise": "3.2.2Fine-grained Preference OptimizationThe common way of self-improvement focuseson updating the model with high-quality sampleswhile discarding low-quality ones. For LLM attri-bution, simply supervised fine-tuning with highlyattributable responses only teaches the LLM tolearn surface characteristics of attribution, e.g., thecorrect form of citation. Inspired by human cog-nition, learning from mistakes provides more fine-grained signals to understand the mechanisms thatdrive successful attribution than simply imitatingcorrect examples. Thus, we aim to fully unlock thepotential of low-quality samples by constructingfine-grained preference pairs with different opti-mization rewards for preference optimization.Given the multi-objective nature of LLM attri-bution, our focus is specifically on attributabilityand comprehensiveness, utilizing correspondingrewards functions to construct preference data re-spectively3. Specifically, we pair samples that ex-hibit high attributability but low comprehensive-ness with the top-ranked sample selected using aholistic reward, and vice versa. These preferencepairs, each addressing different optimization objec-tives, are then aggregated to further train the LLMvia DPO (Rafailov et al., 2023):",
  "Datasets": "Following previous work (Ye et al., 2023; Li et al.,2024), we conduct our experiments using two long-form question-answering datasets: ASQA (Stel-makh et al., 2022) and ELI5 (Fan et al., 2019), aswell as a multi-step reasoning dataset, StrategyQA(Geva et al., 2021). Both ASQA and ELI5 featurefactoid long-form answers that require synthesiz-ing highly relevant documents in response to a userquery. In StrategyQA, answers demand a combina-tion of information-seeking and implicit reasoning.Further details on the data statistics, knowledgecorpus used for retrieval, and examples for eachdataset are provided in Appendix B.",
  "Evaluation": "Following previous research (Gao et al., 2023), weevaluate model-generated responses mainly on twodimensions: Citation Quality and Correctness.Our evaluation methodology combines both auto-mated metrics and human evaluation. Automatic Evaluation.To assess citation qual-ity, we calculate the citation precision, citationrecall, and its harmonic mean citation F1 basedon the definition in Gao et al. (2023). We useTRUE (Honovich et al., 2022), a T5-11B modelfine-tuned on a collection of natural language infer-ence (NLI) datasets to examine whether the citeddocuments entail the generated statement. For cor-rectness, different datasets are measured differently.For ASQA, we report the exact match recall (EMRec.) of correct short answers. For ELI5, we re-port the claim recall (Claim) by checking whetherthe model output entails the sub-claims generatedby text-davinci-003. For StrategyQA, the for-mat of answers begins with yes/no, we evaluatecorrectness by reporting the accuracy (Acc.). SeeAppendix C for more details. Human Evaluation.We collected a total of150 instances from the test sets of ASQA, ELI5,and StrategyQA for human evaluation, with eachdataset providing 10 instances from five differentsystems. The evaluation is divided into two parts:",
  "(2023), we enable the LLM to generate citationsvia in-context learning. For each query, we firstretrieve five relevant documents and then promptthe LLM with two-shot demonstrations": "Post-hoc Attribution (PostAttr).Following Yeet al. (2023), given a query, we first instruct theLLM to generate an initial response leveraging itsparametric knowledge. For each statement in theresponse, we use the NLI model4 to find the maxi-mally supported document and cite accordingly. Training-based Methods.Training on high-quality data serves as a strong baseline to unlockthe attribution ability of LLMs. We consider thefollowing training-based methods.Knowledge Distillation employs the most capa-ble LLMs, e.g., Llama-3-70B-Instruct and Mixtral-8x7B-Instruct, as teacher models to train a studentmodel on distilled attribution data.Self-RAG (Asai et al., 2023) first collect datadistilled from GPT-4, then teach the LLM to re-trieve on-demand while reflecting on its generationto improve both generation quality and attributions.AGREE (Ye et al., 2023) trains the LLM to self-ground its response in retrieved documents using",
  "We use the same NLI model during citation evaluation": "automatically collected data and then leverages test-time adaptation to reinforce unverified statements.APO (Li et al., 2024) models LLM attribu-tion as a preference learning task, where theyfirst supervised-fine-tuned on human-labeled high-quality data and then automatically collect prefer-ence data for preference optimization.FGR (Huang et al., 2024a) first collects attribu-tion data distilled from ChatGPT and then designsrewards tailored for LLM attribution to teach theLLM to generate supportive and relevant citations.",
  "We provide the main results and the performanceof START across different iterations in": "START effectively improves performance.Asshown in , START shows superior perfor-mance across three datasets and achieves state-of-the-art results in citation quality. Specifically,START shows significant improvements over bothICL and Post-hoc approaches, highlighting thebenefits of supervised signals in unlocking theattribution ability of LLMs. Notably, comparedwith methods that rely on distilling from more ad-vanced LLMs or training on human-annotated data,START achieves performance improvement of at",
  ": The pass rate comparison between START andSTART (w/o. warm-up) across different iterations duringthe rejection sampling stage": "least 8.0%, 20.4%, and 47.0% in citation qualityfor ASQA, ELI5, and StrategyQA respectively. Re-garding correctness, START also achieves gains ofat least 9.1% and 7.2% on both ASQA and Strate-gyQA, despite a slight decrease on ELI5. START successfully achieves self-improvement.We compare the performance of START from itera-tion 0 to 3 in , and the results demonstrateconsistent improvements across iterations. Initially,at iteration 0 (after warm-up), thanks to the syn-thetic training data, the model shows decent per-formance after warm-up. By iteration 1, STARTexhibits remarkable effectiveness in improving itsperformance by leveraging its own generated sam-ples (e.g., 23.5 72.0 on ASQA, 10.0 48.9 onELI5, 9.5 46.4 on StrategyQA). Subsequent it-erations continue this trend of incremental improve-ment, reaching a convergence point at iteration 3.",
  "We conduct comprehensive ablation studies andanalyses to understand how each component inSTART contributes to the significant improvement": "Effect of synthetic data warming-up.Todemonstrate the importance of utilizing syntheticdata for initial warm-up in START, we con-duct a comparative ablation study employingLlama-2-13b for self-improvement, omitting theinitial warm-up stage. shows the ablation results (w/o. warm-up) across three iterations. Weobserve that omitting the initial warm-up stage canlead to a significant performance drop in the first it-eration. Additionally, as the iteration increases, theperformance of the model without warm-up showsonly modest improvements and remains substan-tially inferior to the model that underwent warm-up. Moreover, we also calculate the pass rate ofsampled response in each iteration as shown in Ta-ble 3. The findings indicate that the model withwarm-up exhibits a higher pass rate in the first it-eration, which allows the model to utilize moresupervised signals for self-improvement. These re-sults suggest that warming up effectively facilitatesthe bootstrapping of supervised data, thus prevent-ing early model stagnation. Its worth noting thatwhile the warm-up strategy effectively enriches themodel with supervision signals at an early stage, itdoes not lead to noticeable improvements in cita-tion quality, as shown in . We hypothesizethat this limitation stems from the inherent diffi-culty LLMs face in synthesizing information frommultiple sources to generate comprehensive andattributable responses solely through direct super-vised fine-tuning. Effect of fine-grained preference optimization.To further understand the significance of fine-grained preference optimization, we compare anablation of START that solely relies on high-qualitysamples for iteratively supervised fine-tuning, dis-carding low-quality samples for fine-grained pref-erence optimization. As shown in , thereis a significant decline in performance when fine-grained preference optimization is removed. Thishighlights the effectiveness of START in fully un-locking the potential of low-quality samples to en-",
  "ITER-1ITER-0": ": The impact of supervision signals from different stages (synthetic data v.s. self-improvement) onattribution performance across ASQA, ELI5, and StrategyQA. The blue line represents the model that undergoesonly supervised fine-tuning use synthetic data at iteration 0. The red line represents the model that first trains fortwo epochs with synthetic data at iteration 0, followed by one iteration of self-improvement.",
  ": Ablation study on the effect of synthetic datasize on attribution and correctness performance. Wesample 1k, 3k, and 5k user queries for data synthesis.hance attribution performance": "Effect of synthetic data size.We investigate theeffect of varying synthetic data sizes on the per-formance of START. demonstrates theireffect on citation quality and correctness after threeiterations of self-improving. Specifically, we sam-ple 1k, 3k, and 5k unlabeled queries to generatesynthetic training data accordingly, which providesdifferent levels of supervision signals. As shown in, even with 1k synthetic data points, STARTdemonstrates comparable performance. Moreover,as the training size increases, START achieves no-table improvement in citation quality and exhibitsstability in correctness. Supervision signals from synthetic data v.s. iter-ative self-improvement.We further investigatethe differential impact of supervision signals de-rived from data synthesis versus those from theiterative self-improvement stage. We utilize syn-thetic training data to train the model for multipleepochs, extending up to 10 epochs, and compareits performance to that of a model that undergoesonly the first iteration of self-improvement. As de-picted in , training with synthetic data dur-ing the initial iteration yields minimal performancegains. The attribution performance climbs slowly",
  "Human Evaluation": "Human evaluation results, detailed in , in-dicate that START generates significantly more at-tributable responses compared to all baselines, evensurpassing ChatGPT5. Specifically, 76.2% of thestatements generated by START are fully supportedby the cited documents, which outperforms Chat-GPT by 11.24%. Additionally, 18.3% of the state-ments are partially supported, with only 5.5% un-supported. In terms of factuality, START outper-forms all training-based baselines, slightly inferiorto ChatGPT. Moreover, START achieves the high-est score in comprehensiveness, demonstrating itsexceptional ability to generate responses that ex-tensively cover information from multiple sources.Overall, these findings are in line with the auto-matic evaluation results in .",
  "We utilize gpt-3.5-turbo-0125 version": "tify two key limitations for LLM attribution self-improvement. To address these, START first lever-ages self-constructed synthetic data for warmingup, aiming to prevent models from early stagna-tion due to insufficient supervision signals. To ex-plore more fine-grained supervision signals, STARTconstructs fine-grained preference supervision sig-nals from low-quality samples for preference opti-mization. Both automatic and human evaluationsdemonstrate significant improvement in attributionwithout relying on human annotations and moreadvanced LLMs.",
  "Limitations": "Despite significant performance improvements, ourwork presents several limitations worth noting.Firstly, while our data synthesis process providesa good starting point for the model to self-improveand demonstrate some generalization on existingbenchmarks, it may not cover all scenarios en-countered in user information-seeking. This limita-tion raises concerns regarding the generalizabilityof synthetic data in a more complex information-seeking environment. Secondly, the iterative train-ing pipeline of our self-improving framework istime-consuming, presenting a significant trade-off between performance and training duration.Thirdly, although our self-improving frameworkdoes not rely on human annotations and more ad-vanced LLMs, it still necessitates the integration ofoff-the-shelf NLI models to guarantee the qualityof attribution in the generated samples. The perfor-mance of the NLI model significantly impacts thequality of our outputs to a certain extent. To movetowards a fully self-improving framework that doesnot rely on external judgment, future research couldinvestigate the use of intrinsic attribution signalsderived directly from the LLM itself.",
  "Acknowledgements": "Xiaocheng Feng is the corresponding author ofthis work.We thank the anonymous review-ers for their insightful comments.This workwas supported by the National Natural ScienceFoundation of China (NSFC) (grant 62276078,U22B2059), the Key R&D Program of Hei-longjiang via grant 2022ZX01A32, the Interna-tional Cooperation Project of PCL, PCL2022D01and the Fundamental Research Funds for the Cen-tral Universities (Grant No.HIT.OCEF.2023018).",
  "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.CoRR, abs/2310.11511": "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni,Daniel Andor, Livio Baldini Soares, Jacob Eisen-stein, Kuzman Ganchev, Jonathan Herzig, Kai Hui,Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster,William W. Cohen, Michael Collins, Dipanjan Das,Donald Metzler, Slav Petrov, and Kellie Webster.2022. Attributed question answering: Evaluationand modeling for attributed large language models.CoRR, abs/2212.08037. Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. 2019. ELI5:long form question answering. In Proceedings ofthe 57th Conference of the Association for Compu-tational Linguistics, ACL 2019, Florence, Italy, July28- August 2, 2019, Volume 1: Long Papers, pages35583567. Association for Computational Linguis-tics. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.2023. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 64656488. Association for Compu-tational Linguistics. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did aristotleuse a laptop? A question answering benchmark withimplicit reasoning strategies. Trans. Assoc. Comput.Linguistics, 9:346361. aglar Glehre, Tom Le Paine, Srivatsan Srini-vasan, Ksenia Konyushkova, Lotte Weerts, AbhishekSharma, Aditya Siddhant, Alex Ahern, MiaosenWang, Chenjie Gu, Wolfgang Macherey, ArnaudDoucet, Orhan Firat, and Nando de Freitas. 2023.Reinforced self-training (rest) for language modeling.CoRR, abs/2308.08998. Or Honovich, Roee Aharoni, Jonathan Herzig, HagaiTaitelbaum, Doron Kukliansy, Vered Cohen, ThomasScialom, Idan Szpektor, Avinatan Hassidim, andYossi Matias. 2022. TRUE: re-evaluating factualconsistency evaluation. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL 2022, Seattle, WA,United States, July 10-15, 2022, pages 39053920.Association for Computational Linguistics.",
  "text with citations via fine-grained rewards. CoRR,abs/2402.04315": "Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu,Weihong Zhong, Xiachong Feng, Weijiang Yu, Wei-hua Peng, Duyu Tang, Dandan Tu, and Bing Qin.2024b. Learning fine-grained grounded citations forattributed large language models.In Findings ofthe Association for Computational Linguistics, ACL2024, Bangkok, Thailand and virtual meeting, Au-gust 11-16, 2024, pages 1409514113. Associationfor Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. CoRR, abs/2311.05232. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples. Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xin-shuo Hu, Xuebo Liu, and Min Zhang. 2024. Improv-ing attributed text generation of large language mod-els via preference learning. CoRR, abs/2403.18381.",
  "Chaitanya Malaviya, Subin Lee, Sihao Chen, ElizabethSieber, Mark Yatskar, and Dan Roth. 2023.Ex-pertqa: Expert-curated questions and attributed an-swers. CoRR, abs/2309.07852": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of factualprecision in long form text generation. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023, Sin-gapore, December 6-10, 2023, pages 1207612100.Association for Computational Linguistics. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernndez brego, Ji Ma, Vincent Y. Zhao,Yi Luan, Keith B. Hall, Ming-Wei Chang, and YinfeiYang. 2022. Large dual encoders are generalizableretrievers. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2022, Abu Dhabi, United Arab Emirates, De-cember 7-11, 2022, pages 98449855. Associationfor Computational Linguistics.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Dmytro Okhonko, Samuel Broscheit, Gautier Izacard,Patrick S. H. Lewis, Barlas Oguz, Edouard Grave,Wen-tau Yih, and Sebastian Riedel. 2021. The webis your oyster - knowledge-intensive NLP against avery large web corpus. CoRR, abs/2112.09924. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,and Yuxiong He. 2020. Deepspeed: System opti-mizations enable training deep learning models withover 100 billion parameters. In KDD 20: The 26thACM SIGKDD Conference on Knowledge Discoveryand Data Mining, Virtual Event, CA, USA, August23-27, 2020, pages 35053506. ACM. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: factoid questions meetlong-form answers. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 82738288.Association for Computational Linguistics.",
  "Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, DianYu, Haitao Mi, and Dong Yu. 2024. Toward self-improvement of llms via imagination, searching, andcriticizing. CoRR, abs/2404.12253": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.",
  "Zheng Yuan, Hongyi Yuan, Chengpeng Li, GuantingDong, Chuanqi Tan, and Chang Zhou. 2023. Scalingrelationship on learning mathematical reasoning withlarge language models. CoRR, abs/2308.01825": "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.Goodman. 2022. Star: Bootstrapping reasoning withreasoning. In Advances in Neural Information Pro-cessing Systems 35: Annual Conference on NeuralInformation Processing Systems 2022, NeurIPS 2022,New Orleans, LA, USA, November 28 - December 9,2022. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, ZikangLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. CoRR,abs/2303.18223.",
  "A.1Data Sources": "The queries employed for data synthesis aresourced from the Wish-QA (Yehudai et al., 2024),which provides high-quality grounded data suit-able for content-grounded generation tasks suchas long-form question-answering and summariza-tion. Specifically, we utilize the ELI5 subset ofthe WishQA, noted for its high lexical diversity,comprising a total of 8,413 queries. Notably, werandomly sample 5,000 user queries for our datasynthesis, resulting in the creation of 5,000 syn-thetic data points.",
  "A.3Implementation Details": "In our work, we use Llama-2-13b-base for datasynthesis, as our goal is to realize self-improvingfor the attribution ability of LLMs, the models usedin the data synthesis stage and the subsequent mainexperiment need to be consistent without introduc-ing additional more powerful models. To enhancethe LLMs ability to accurately follow instructionsat each step, we utilize in-context learning, incorpo-rating two demonstrations for response generation,claim decomposition, and document generation.",
  "A.4Quality of Synthetic Data": "We focus on evaluating the attributability of thefinal response. Specifically, we employ an off-the-shelf Natural Language Inference (NLI) model,TRUE (Honovich et al., 2022), to verify whethereach statement in the response is fully supported bythe cited documents and to check for the presenceof any irrelevant citations. The results indicate thatthe synthetic data are of significantly high qual-ity: 92.3% of the statements are fully supportedby the cited documents, and 94.1% are free fromirrelevant citations.",
  "BDetails of evaluation datasets": "Our evaluation utilizes the ASQA, ELI5, and Strat-egyQA datasets. For both ASQA and StrategyQA,Wikipedia serves as the external knowledge base,specifically employing the Wikipedia snapshotfrom 2018-12-20. For the ELI5 dataset, the ex-ternal knowledge source is Sphere (Piktus et al.,2021). Regarding the retrievers, we use the denseretriever GTR (Ni et al., 2022) for Wikipedia andthe sparse retriever BM25 for Sphere. Detailedstatistics for these datasets are presented in .In line with previous research by Gao et al. (2023),we use the same evaluation datasets for ASQA andELI5. Regarding StrategyQA, we adopt the settingsof Ye et al. (2023), utilizing a randomly split sub-set of 490 test instances for evaluation. To furtherclarify, we provide an example from each datasetin .",
  "Documents: [The retrieved documents are omitted here]": "Answer: No. \"Curiosity\" rover has outlasted its expected lifespan. The rover was designedto last for months, but is still operating after years on Mars. In August 2017, \"Curiosity\"celebrated its fifth anniversary on Mars and is expected to continue its mission for years to come.The longevity of \"Curiosity\" can be attributed to the advanced technology used in the roversdesign and the meticulous planning and preparation done by the engineers and scientists. With theadvancement of technology and the continued refinement of the mission, \"Curiosity\" is likely tocontinue operating for many more years to come.",
  "(a) Prompt template of ASQA and ELI5": "Instruction: Write an accurate, engaging, and concise answer for the given question using onlythe provided search results (some of which might be irrelevant) and cite them properly. Usean unbiased and journalistic tone. Always cite for any factual claim. When citing severalsearch results, use . Cite at least one document and at most three documents in eachsentence. If multiple documents support the sentence, only cite a minimum sufficient subset ofthe documents.",
  "We provide a detailed description of the evaluationmetrics employed to assess the quality of the model-generated responses": "Citation Quality.Citation Quality is a criticalevaluation dimension in attributed text generation,assessing whether the answer is fully supported bythe cited documents and that no irrelevant docu-ments are cited. Following Liu et al. (2023) andGao et al. (2023), the evaluation of citation qualityis typically divided into two parts: Citation Recalland Citation Precision.Citation Recall evaluates whether all generatedstatements are fully supported by the cited docu-ments. Specifically, for each statement si S,its citation recall is scored as 1 if there is atleast one valid citation (Ci = ) and the concate-nation of cited documents concat(Ci) fully sup-port the statement ((concat(Ci), si) = 1), where(premise, hypothesis) is an NLI model that out-puts 1 if the premise entails the hypothesis. Thefinal citation recall is calculated by averaging overall statements in S.Citation Precision assesses whether any citationsin the response are irrelevant. A citation ci,j is determined as irrelevant if (a) ci,j alone cannotsupport statement si and (b) removing ci,j does notaffect the rest of the citations to support si.Citation F1 is a metric that combines citationprecision and citation recall by calculating theirharmonic mean. In our work, we utilize this metricto evaluate the overall citation quality of the re-sponse, where a higher Citation F1 score indicatesa more accurately and comprehensively attributedresponse.",
  "citation precision + citation recall, (7)": "Correctness.Correctness is crucial in long-formQA tasks.Given the ambiguous nature of theASQA dataset, where each question requires mul-tiple short answers to cover different aspects, wefollow Stelmakh et al. (2022) and calculate the re-call of correct short answers using exact match.As for the ELI5 dataset, evaluating the cor-rectness of long-form answers is challenging.Thus, the ALCE benchmark employs Instruct-GPT (text-davinci-003) to generate three \"sub-claims\" based on the human-annotated answers. Toassess correctness, we use a T5-11B model6 thathas been fine-tuned on a collection of NLI datasetsto check whether the model-generated outputs en-tail these sub-claims.",
  "DHuman Evaluation Details": "Considering the open-ended nature of long-formQA tasks, automatic evaluation of correctness maynot cover all possible answers. Furthermore, theevaluation of citation quality is constrained by thecapabilities of the off-the-shelf NLI model, whichmay not adequately detect cases of partial sup-port. Therefore, we conduct a human evaluationto assess the attribution quality and correctness ofSTART. We recruited two annotators, holding atleast a bachelors degree to participate in our study.To evaluate citation quality, annotators are askedto verify whether each statement in the responsesis fully supported, partially supported, or not sup-ported by the cited documents and identify errortypes if the statement is not fully supported.Next, we evaluate the overall quality of the re-sponses, focusing on comprehensiveness and cor-rectness. Annotators are asked to rate both compre-hensiveness and correctness using a 5-point Likertscale, capturing different levels of content coverageand factuality.",
  "EBaselines": "Knowledge Distillation:We employ supervisedfine-tuning to teach Llama-2-13B to generate re-sponses with citations, utilizing training data dis-tilled from the most advanced LLMs. Specifically,the queries and documents are sourced from oursynthetic dataset and the attributed responses aregenerated by Llama-3-70B-Instruct / Mixtral-8x7B-Instruct. Self-RAG (Asai et al., 2023):The method in-volves training the LLM to generate text with re-flection tokens, which are categorized into retrievaland critique tokens to indicate the need for retrievaland the attributability of its generation, respec-tively. Specifically, it first collects over 145,619supervised data by prompting GPT-4 with specificinstructions to generate responses with reflectiontokens for knowledge-intensive queries.Thesedata are then used to train the LLM to generateresponses with self-reflection via supervised fine-tuning. AGREE (Ye et al., 2023):The method involvestraining the LLM to generate grounded claims withcitations and to identify unverified claims. Specif-ically, it first collects 4,500 attribution data viapost-hoc attribution with the help of an NLI model. These data are then used to train the model to gen-erate grounded responses with citations and alsoclearly state the unsupported statements. An iter-ative retrieval process is employed to search foradditional information for the unsupported state-ments via a test-time adaptation (TTA) strategy. APO (Li et al., 2024):This method models theattributed text generation task as a preference learn-ing task. Specifically, the model is first trainedusing 6,330 human-labeled high-quality attributiondata for supervised fine-tuning to learn the basicability of attribution. It then leverages automat-ically constructed preference data for preferencelearning, where a positive response is generatedfrom relevant documents accompanied by a posi-tive prompt, while a negative response is generatedusing irrelevant documents or a negative prompt. FGR (Huang et al., 2024a):The method firstcollects 3,000 in-domain user queries along with re-trieved documents and then leverages ChatGPT togenerate high-quality attributed responses. Thesedata then serve as training data to teach the modelthe basic ability of citation generation via super-vised fine-tuning. Subsequently, the method de-signs reward models to teach the model to gen-erate well-supported and accurate responses viafine-grained reinforcement learning.To ensure a fair comparison, we employ the samebase model (Llama-2-13b-base) for evaluatingall baselines. For Self-RAG, AGREE, and APO,we directly utilize their published experimental re-sults. In the case of FGR, which does not pro-vide Llama-2-13b-base results, we reproduce theexperiments using the official code and the samesettings provided by the authors.",
  "FImplement Details": "In all experiments, training is conducted using eightA100-80GB GPUs, leveraging Deepspeed stage3 (Rasley et al., 2020) for multi-GPU distributedtraining, with training precision Bfloat16 enabled.During the initial warm-up stage, we employ theAdamW (Loshchilov and Hutter, 2019) optimizerwith a warm-up ratio of 0.03. The total batch sizeis set at 64, and the learning rate is maintained at2e-5. The maximum input sequence length is con-figured to 2048 tokens. The model is trained withonly 20% of the synthetic dataset for two epochs inthis stage. This strategy is designed to prevent themodel from overfitting to the synthetic data during the warm-up stage, enabling it to generate morediverse samples in the subsequent rejection sam-pling fine-tuning stage. In the self-improving stage,we conduct rejection-sampling fine-tuning for threeepochs at each iteration, maintaining the same train-ing settings as those used during the warming-upstage. To get the highest quality responses dur-ing rejection sampling, we set the threshold forattributability reward at 1.0, ensuring that everystatement in the response is fully supported by thecited documents. For comprehensive, we set thethreshold to 0.8, which means that at least 80% ofthe statements need to be cited. Subsequently, dur-ing the fine-grained preference optimization, themodel is further trained for one additional epochusing a learning rate of 1e-5.During the evaluation, we utilize the vLLMframework (Kwon et al., 2023) for efficient infer-ence. Without special instructions, the samplingparameters are specifically configured with a tem-perature of 1.0 and a top-p setting of 0.95. Wepresent detailed prompts used during the evalua-tion process in ."
}