{
  "Abstract": "We introduce LogicAsker, a novel approachfor evaluating and enhancing the logical rea-soning capabilities of large language models(LLMs) such as ChatGPT and GPT-4. DespiteLLMs prowess in tasks like writing assistance,code generation, and machine translation, as-sessing their ability to reason has been chal-lenging. Traditional evaluations often prioritizeaccuracy on downstream tasks over direct as-sessments of reasoning processes. LogicAskeraddresses this gap by employing a set of atomicreasoning skills grounded in propositional andpredicate logic to systematically examine andimprove the reasoning prowess of LLMs. Ourmethodology reveals significant gaps in LLMslearning of logical rules, with identified reason-ing failures ranging from 29% to 90% acrossdifferent models. Moreover, we leverage thesefindings to construct targeted demonstration ex-amples and fine-tune data, notably enhancinglogical reasoning in models like GPT-4o by upto 5%. To our knowledge, this is the first effortto utilize test case outcomes to effectively refineLLMs formal reasoning capabilities. We makeour code, data, and results publicly available1",
  "Introduction": "Large language models (LLMs), such as OpenAIsGPT series have significantly impacted natural lan-guage processing, excelling in a variety of tasksincluding text generation, machine translation, andcode generation (Gao et al., 2022, 2023a; Jiao et al.,2023).Reasoning, defined as the cognitive processof using logic to draw conclusions from givenfacts (Wei et al., 2022b,a), is crucial for complexinteractions that go beyond text generation. Ac-curately assessing this ability in LLMs is essen-",
  "Both authors contributed equally to this research.Wenxiang Jiao is the corresponding author.1": "tial, yet challenging, as models may correctly per-form tasks merely relying on shortcuts such as pat-tern recognition without truly engaging in logicalreasoning (Huang and Chang, 2022; Huang et al.,2023; Liu et al., 2023a). Consider the followinginference example: Either it is raining, orTom will play football; if it rains, thenthe floor will be wet; the floor is dry;therefore, Tom will play football. We mayencounter the following challenges: 1) Its unclearif a correct LLM response is due to reasoning orsimple heuristics like word correlations (e.g., dryfloor is more likely to correlate with playing foot-ball). 2) If an LLM fails, pinpointing the specificbreakdown in reasoning is difficult (i.e., inferringnot raining from the floor being dry or inferringplaying football from not raining). 3) Current sys-tems lack comprehensive test cases that encompassvarious formal reasoning types beyond implica-tion, such as logical equivalence (e.g., A and B aretrue; therefore, B and A are true. 4) Evaluatingan LLMs reasoning on such cases offers limitedinsight into enhancing its reasoning capabilities.To better handle these challenges, a well-performing testing framework should be able todefine a set of skills that a) directly correspond tothe reasoning process, b) cannot be further divided,c) cover all formal logical reasoning scenarios, andd) can identify LLMs weaknesses and facilitateimproving LLMs performance. Property a) en-sures that the task cannot be accomplished by otherapproaches, such as inferring from the correlationsof words, and the evaluation result directly reflectsthe models reasoning ability. Property b) and c)ensure that the set of skills is fundamental and com-prehensive, which can provide helpful insights toaccomplish Property d).We introduce LogicAsker, an automatic frame-work designed to evaluate and enhance LLMs for-mal reasoning skills using Minimum Functional-ity Tests (MFTs) (Ribeiro et al., 2020), akin to",
  ": Overview of the LogicAsker framework": "software engineerings unit tests, which utilizestraightforward examples to assess specific behav-iors. These tests help identify when models relyon shortcuts rather than genuinely mastering askill (Ribeiro et al., 2020). Specifically, LogicAs-ker builds a set of atomic skills from foundationalprinciples of propositional and predicate logic, twofundamental systems used to formalize reasoningprocedures (Partee et al., 1990), together with com-mon logical fallacies (Hurley and Watson, 2020).Based on the skill set, LogicAsker generates rea-soning questions by translating standard logic ex-pressions into natural language, assesses LLMsaccuracy per skill, pinpoints weaknesses, and cre-ates in-context-learning (Brown et al., 2020) ex-amples and fine-tuning data to bolster reasoningabilities. In addition, for each skill, LogicAskeruses diverse vocabulary to frame various naturallanguage queries, computing average performanceto minimize biases from word correlations. demonstrates that LogicAsker comple-ments existing frameworks by providing a compre-hensive evaluation scope and utilizing outcomesto enhance LLMs reasoning capabilities, whileother datasets often face data leakage and are scope-limited. LogicAsker serves as an extensive diagnos-tic tool for LLMs formal reasoning, significantlyexceeding the coverage of comparable tools andenabling detailed assessments across diverse rea-soning rules such as inferences, quantifiers, andfallacies. Scaling up the scope presents significantchallenges due to the complexity of designing algo-rithms capable of processing various logical rulesand translating them into natural language. Despitethese complexities, LogicAsker uniquely integrates all formal logical rules and common fallacies, facil-itating robust testing and refinement of reasoningcapabilities.WeevaluatedLogicAskersperformancethrough extensive testing on six state-of-the-art(SOTA) LLMs (Hugging Face, 2024), includingfourclosed-sourceLLMs(GPT-4o,GPT-4,ChatGPT, and Gemini-1.5) and two open-sourceLLMs (Llama3 and Mixtral). Our findings revealthat LogicAskers test cases effectively pinpointlogical reasoning failures across these models, witherror rates (i.e., 1 accuracy) between 29% and90%. These test cases also facilitate the creation ofin-context learning examples and fine-tuning data,thereby enhancing logical reasoning capabilities.For instance, applying LogicAskers cases toGPT-4o improved its reasoning accuracy from 92%to 97%. All resources are released for reproductionand further research2.We summarize the main contributions of thiswork as follows:",
  "Formal Analysis of Reasoning Abilities": "Reasoning can be characterized into formal rea-soning and informal reasoning. The former is asystematic and logical process that follows a set ofrules and principles, and the reasoning within thesesystems will provide valid results as long as onefollows the defined rules (e.g., all A are B, all B areC; therefore, all A are C). The latter is a less struc-tured approach that relies on intuition, experience,and common sense to draw conclusions and solveproblems (Huang and Chang, 2022; Bronkhorstet al., 2020) (e.g., Hong Kong residents have a highlife expectancy; this is probably because they havehealthy living habits). Generally, formal reasoningis more structured and reliable, whereas informalreasoning is more adaptable and open-ended butmay be less reliable. In this paper, we focus on theformal reasoning process to systematically analyzeLLMs reasoning abilities.To formalize reasoning procedures, two funda-mental systems are usually adopted, namely, propo-sitional logic and predicate logic. The former onedeals with propositions or statements that can beeither true or false, and utilizes logical operatorsincluding (and), (or), (not), (inference),and (bidirectional) to connect these statements.The latter one, in contrast, extends propositionallogic to deal with more complex statements thatinvolve variables, quantifiers, and predicates. Bothpropositional logic and predicate logic contain vari-ous rules for the reasoning process. These rules canbe categorized into equivalence rules and inferencerules. Equivalent rules summarize the basic expres-sions that are equivalent in terms of truth value (e.g.,(P Q) (P) (Q)). Inference rules sum-marize the basic valid inference rules (e.g., fromthe premises: A B, and A, we can infer B ).We refer to (Partee et al., 1990) for a more de-tailed explanation. -9 in Appendix A listcommon inference rules in predicate logic andpropositional logic. Besides inference rules, for-mal logic systems can also express common logicalfallacies, i.e., arguments that may sound convinc-ing but are based on faulty logic and are, therefore,invalid. We list the common logical fallacies in.",
  "Minimum Functionality Test": "In this paper, we adopted the concept of MinimumFunctionality Tests (MFTs), introduced in (Ribeiroet al., 2020), to evaluate the reasoning ability ofLLMs. MFTs are analogous to unit tests in soft-ware engineering, where a collection of simple ex-amples is used to check a specific behavior withina capability. These tests involve creating small andfocused datasets that are particularly effective indetecting whether models resort to shortcuts to han-dle complex inputs, rather than truly mastering thecapability.To apply MFTs in evaluating the reasoning abil-ity of LLMs, we treated each formal logical ruleas an independent task and generated abundant testcases for each task. Each test case was designedto trigger logical failures in the LLMs, allowing usto assess the strengths and weaknesses of LLMs inthe logical reasoning process, and providing a solidfoundation for further analysis and improvement.",
  "LogicAsker": "In this section, we introduce the design and imple-mentation of LogicAsker, a novel tool to triggerlogical reasoning failures in large language models. overviews the workflow of LogicAsker,which consists of three main modules: test casegeneration, weakness identification and in-contextlearning (ICL) demonstration. In particular, thetest case generation module utilizes atomic skillsdefined on the two formal logic systems and aninference synthesis approach to generate questionsas test cases. Then, the generated cases are fedinto the LLMs to reveal weaknesses and provide in-sights into the LLMs by the weakness identificationprocess. Finally, LogicAsker utilizes these insightsto construct ICL demonstrations to improve thereasoning abilities of the LLMs.",
  "Reasoning Skills": "Atomic skills. As described in .1, propo-sitional and predicate logic are two fundamentalsystems that formalize the reasoning process. Theinference rules and equivalence laws in these twosystems are atomic and can cover all correct reason-ing scenarios; therefore, we define these 34 rulesas the set of atomic skills an LLM should possessto perform formal reasoning.Extended skills. Predicate logic extends propo-sitional logic to deal with more complex statementsthat involve variables, quantifiers, and predicates.",
  ": Test case generation procedure": "In this regard, besides the unique equivalence andinference laws in predicate logic, we add quanti-fiers and variables to every rule in propositionallogic to form the predicate version of the laws. Us-ing this approach, we expand the set of 34 atomicskills into a set of 208 extended skills. In AppendixB, we provide some concrete examples of theseextended rules.",
  "Test Case Generation": "To generate logical questions, LogicAsker firstadopts a rule-based method to generate logical ex-pressions systematically based on reasoning skillsand then translates the logical expressions into nat-ural language. provides an overview ofthe procedure.Logic expression generation. To better controlthe process of logic expression generation, we firstdefine the length of an inference problem by thenumber of syllogisms it involves. We use the in-ference rules described in .1 to generateinference expressions with length one. When alonger inference (> 1) is specified, we start with abase expression E0 := P1 P2 C1 with lengthone and expand the inference chain. Specifically,we substitute the premises (either or both) of thefirst inference with the conclusion of some other syllogism and append the premises of those syllo-gisms into the list of all premises. For example, wecan find another syllogism E1 := P3 P4 P2with P2 as the conclusion and then obtain a newexpression Enew := P1 P3 P4 C1 with theinference length of two. We can obtain inferenceexpressions of any length by recursively expandingthe inference chain as above. During the genera-tion process, one can specify the desired rules andlength to allow complete control over expected testcases.In addition to the correct inference expressioncreated above, we generate three kinds of false in-ference expressions: contradiction, unrelated, andfallacy. A contradiction is generated by negatingthe conclusion of a correct inference expression andan unrelated is generated by replacing the conclu-sion of a valid inference expression with an irrele-vant statement. For example, for E0 := P1 P2 C1, a contradiction is Ec := P1 P2 C1, anunrelated can be Eu := P1 P2 U1. We createa fallacy by directly using the fallacy rules listedin .1 for an inference length of one. Fora fallacy with a more extended length, we select afallacy rule as the base expression and expand theinference chain using correct rules, ensuring theexpressions incorrectness. Natural language translation.Partially in-spired by (Ontan et al., 2022), translating aclause into natural language involves a series ofpatterns that depend on the structure of the clause.Simple propositions are transformed into one of thetemplate patterns, such as subject verb-action,subject predicate, or impersonal-action with apredefined set of subjects, verbs, predicates, andimpersonal actions that can be chosen randomlywithout repetition. For predicate clauses that in-volve constants or variables, we employ templatessubject verb-action, subject predicate to trans-late them. Furthermore, each clause can be ren-dered in various modes, such as the present, past,or negated forms. Additionally, connectives like\"or,\" \"and,\" \"implies,\" and \"if and only if\" alsoadhere to their designated patterns. For quantifiedclauses, we adopt patterns like \"for all x, X\", \"thereis at least one x for which X\", and \"some Xs areY ,\". To facilitate the generation process, we cu-rate extensive lists of potential subjects, includingcommon names in English, and compile plausiblepredicates, actions, and impersonal actions. Com-pared to Ontanon et al. (Ontan et al., 2022), ourmethod provides a more natural and less ambigu-ous translation. We provide a detailed illustrationof the translation process in Appendix C.",
  "Weakness Identification": "To measure the reasoning abilities of the LLMs, wecalculate the accuracy of LLMs answers Acc =NcorrectNtotal . Where Ntotal denotes the total number ofresponses, and Ncorrect denotes the number of re-sponses that are correct. In particular, since allgenerated queries are formulated as yes-or-no ques-tions, LogicAsker adopts an automatic approachthat searches for pre-defined keywords (e.g., \"yes\"and \"no\") in sentences to identify correct answers. To reveal the weaknesses of LLMs, we gen-erate n test cases for each leaf node in the ruletree depicted in . Then, we calculated theresponse accuracy of an LLM of each leaf node.Based on the result, we can identify the weaknessesof LLMs by listing the leaf nodes that receive thelowest accuracy. In addition, by grouping the ac-curacy by different attributes in the rule tree, wecan gain insights into the strengths and weaknessesof LLMs on these attributes (e.g., performance onpredicate logic vs. propositional logic).",
  "Improving LLMs": "In-context learning (ICL) is a paradigm that en-ables LLMs to learn tasks with examples in theform of demonstrations (Brown et al., 2020). Itleverages task instructions and a few demonstra-tion examples to convey the task semantics, whichare then combined with query questions to createinputs for the language model to make predictions.ICL has demonstrated impressive performance invarious natural language processing and code in-telligence. However, the performance of ICL isknown to rely on high-quality demonstrations (Gaoet al., 2023b) strongly. To fully unleash the po-tential of ICL, LogicAsker utilizes the weak skillsof each LLM to construct both correct and incor-rect examples with expected answers and explana-tions as demonstrations to facilitate the reasoningof LLMs. The generation process follows a similarapproach to the test case generation described in 3.2, with the difference being that we append abrief explanation and the correct answer at the endof each case. We show an instance of the demon-stration example in Appendix D.Fine-tuning is another widely used tech-nique to enhance model performance on specifictasks (Moslem et al., 2023; Wei et al., 2021). Thisprocess involves taking a pre-trained model andfurther training it on a smaller, task-specific dataset.The rationale behind fine-tuning is to leverage thelearned features and knowledge of the pre-trainedmodel, adapting it to particular nuances and charac-teristics of a targeted domain or task. In this paper,we directly utilize the data generated by LogicAs-ker to fine-tune LLMs to improve their reasoningability.",
  ": Overall accuracy": "two open-source models. lists brief infor-mation on these systems. All of them are rankedwithin the top 50 in the LMSYS Chatbot ArenaLeaderboard 3 according to the assessment resultsin June 2024. We leave details of how we accessthe model, the parameters used, and the prompt weused in Appendix E.We conduct two iterations of experiments for acomprehensive assessment. In the first iteration, wefollow the setting in 3.3 and set n = 25, resultingin 5,200 cases. Statistics of the sampled data aredescribed in Appendix F. The second iteration isbased on the first one, which focuses on the iden-tified weaknesses of each LLM, i.e., the ten leafnodes in with the lowest accuracy. Wegenerated 25 additional test cases for each weak-ness. These 250 test cases comprise our weaknessdataset, which will be utilized for further evalua-tion in 4.5.",
  "Effectiveness of LogicAsker": "We demonstrate the effectiveness of LogicAskerthrough the overall performance of LLMs on thetest cases. The overall performance of LLMs in thefirst and second iteration is shown in . Theresult reveals that our framework can effectivelyexpose logical failures in the first iteration, withLLMs accuracy ranging from 78%-98%. Whenfocusing on the weak skills of LLMs in the seconditeration, we further reduce the accuracy to 10%-71% for the LLMs. Whats surprising is that mostof these LLMs show accuracy even lower than ran-dom guesses (i.e., 50% here) when confronted withlogical questions involving specific logical rules.This contradicts their remarkable performance invarious LLM benchmarks, for example, achievingtop 50 ranks on the LLM Arena Leaderboard. Itsuggests that existing benchmark datasets are notcomprehensive enough to assess the generalizationability of LLMs in reasoning.",
  "Insights into Reasoning Abilities": "We conducted a comprehensive analysis to gaininsights from the failures exposed by LogicAsker,obtaining three key observations from the evalua-tion:Most LLMs are better at easier logical skills.We compared the performance of LLMs on propo-sitional logic and predicate logic, the former ofwhich is simper in form while the latter involvesmore complex quantifier manipulations. illustrates the difference between the accuracy ob-tained for the two logic systems. Notably, we ob-served that most LLMs are better at propositionallogic, implying their limited ability in complex rea-soning scenarios.Most LLMs are weak in recognizing logicalfallacies. presents the accuracy of LLMsunder different skill categories. Interestingly, wediscovered that among three types of skills, recog-nizing fallacies has the lowest accuracy for mostLLMs, with GPT-4 and Mixtral-8x7B being theexceptions. It suggests that current LLMs are over-confident even in fallacies, which may be learnedfrom the mistakes in pretraining data.Case study: GPT-4 did not learn all logic ruleswell. To provide a direct impression of what skillsLLMs cannot perform well, we list three atomicrules in which GPT-4 has the lowest accuracy in. While GPT-4 has an average accuracy of98% over all skills, it only achieves 60% - 68%accuracy on these skills, indicating that it cannotperform these atomic skills smoothly.We also discovered that longer inference chains",
  "Count108018Percentage1.92%1.54%0.00%3.46%": "are more challenging for LLMs, the details are pro-vided in Appendix G. These insights provide a valu-able understanding of the strengths and weaknessesof each LLM when handling logical questions, al-lowing us to uncover specific areas that requireimprovement and potential avenues for enhancingoverall performance. We provide a full breakdownlist of the LLMs performance on various skills inAppendix H.",
  "The Quality of Test Cases": "Since the test cases are automatically generated, weconduct a human evaluation to measure the qual-ity of the generated test cases by LogicAsker. Toachieve this, we randomly sampled 10% (520) ofthe test cases generated during the first iteration ofthe experiment in 4.2 and conduct manual inspec-tion. Two annotators with bachelors degrees wererecruited to answer the questions manually. Eachtest case was annotated as either valid or invalidbased on the following three questions: a) Is thequestion grammatically correct? b) Is the questionunderstandable and has only one interpretation? c)Can the target answer be derived from the ques-tion? A test case is considered valid only whenboth annotators answer to the above questions arepositive. The results of the annotation are presentedin . This result is statistically sufficient toprove that the probability of LogicAsker generat-ing understandable and solvable logical questionsis larger than or equal to 0.94 (with p-value 0.05),indicating that the test cases generated by Logi-cAsker are highly reliable and valid.",
  "LogicAsker to Improve Reasoning": "In this section, we explore the potential of Logi-cAsker in further improving the reasoning abilityof LLMs through in-context learning (ICL) andfine-tuning.We first employ LogicAsker to generate ICLdemonstrations tailored to address the weaknessesdataset uncovered in the experiments in 4.2. Foreach inference problem, we generated ICL demon-strations that provide both the expected answer andan explanation as described in 3. We evaluate theeffectiveness of the ICL demonstrations generatedby LogicAsker by comparing the following prompt-ing strategies: a) Zero-Shot: We provide only taskinstructions without any ICL demonstrations. b)Zero-Shot Chain-of-Thouhgt (CoT): We use the in-struction \"Please think step-by-step\" (Kojima et al.,2022) to elicit the zero-shot reasoning ability ofthe LLMs. c) Random ICL Demonstrations: Inaddition to the task instruction, we also includefour ICL demonstrations selected randomly fromthe available rules with balanced answer labels,i.e., two correct and two incorrect. d) WeaknessICL Demonstration: Instead of random demonstra-tions, we include four ICL demonstrations usingthe weakness rules identified in 4.2 with balancedanswer labels.We perform testing with the 5.2k sampled dataon all models and list the result in . In gen-eral, the weakness ICL demonstrations are moreeffective than those random ICL demonstrations,and both ICL methods bring more performancegain than CoT, indicating that the test cases gen-erated by LogicAsker can improve reasoning.To further demonstrate the effectiveness of Logi-cAsker, we fine-tune ChatGPT on 5.2k separatelygenerated data on all skills and 2.8k separatelygenerated data on weaknesses of ChatGPT, respec-tively. We test the two fine-tuned model on bothLogicAsker and another dataset, LogiQA, a chal-lenging dataset for machine reading comprehension",
  "Average89.5290.3590.6791.63": "with logical reasoning (Liu et al., 2020). We usethe \"test\" split of LogiQA which contains 651 testdata. The results are presented in . We canobserve that models fine-tuned on LogicAsker caneffectively enhance the models reasoning abilityon both datasets, suggesting the generalizabilityof LogicAsker. These findings demonstrate theeffectiveness of LogicAsker in improving thereasoning ability of LLMs.",
  "Discussion": "A surprising result from our experiments is thelower performance of CoT prompting in someLLMs, such as GPT-4, despite the widespread be-lief in CoTs ability to enhance logical reasoning.One possible explanation lies in the fundamen-tal difference between how CoT operates and thestructure of LogicAskers test cases. CoT functionsby breaking down complex reasoning tasks intointermediate, manageable steps, guiding the modeltoward a final conclusion through a series of naturallanguage reasoning steps. However, LogicAskertests focus on assessing LLMs mastery of the mostbasic, indivisible reasoning skills. These fundamen-tal reasoning skills are the building blocks for morecomplex reasoning processes like CoT. Since thesebasic skills cannot be further subdivided into in-termediate steps, the benefits of CoT are limitedwhen applied to LogicAskers simpler test cases.In contrast, improving an LLMs understanding ofthese core skills can enhance its performance onmore complex tasks where CoT is applicable, asdemonstrated by ChatGPTs improved results onthe LogiQA dataset (see ). Further analysis of failure cases reveals anotherintriguing observation. In certain CoT-promptedscenarios, GPT-4 tends to rely on common senserather than logical deduction. Given that Logi-cAskers data is synthetic and not aligned withcommon-sense reasoning, this tendency leads toperformance degradation. This is illustrated bythe following example, where CoT prompts GPT-4to incorrectly infer weather conditions based onpremises, while a zero-shot approach provides amore accurate deduction by sticking to strict logicalinterpretation:",
  "Q: Consider the following premises: If Lindawere sad, then it is sunny, and if it is sunny, thenLinda is sad. Can we infer the following fromthem? Answer yes or no: It is not overcast": "Ans (Zero-shot): From these premises, we caninfer that Linda is sad and it is sunny are equiva-lent; ... This means that Linda is sad if and onlyif it is sunny. However, without additional infor-mation about whether Linda is actually sad or not,we cannot definitively conclude the state of theweather (overcast or not). Therefore, based on theinformation given and without knowing whetherLinda is sad, the answer is no. Ans (CoT): ... The statement It is sunny di-rectly contradicts the possibility of it being over-cast at the same time, assuming a typical under-standing of weather where sunny conditions andovercast conditions are mutually exclusive. ...Therefore, if either is true, it must be sunny, andthus it cannot be overcast. ... Since the premiseslead us to conclude that it is sunny, we can inferthat it is not overcast. Final answer: yes. This observation underscores the effectivenessof LogicAsker in revealing situations where mod-els default to heuristics or memorized knowledgerather than true logical reasoning. While CoT hasbeen shown to improve logical reasoning in generaltasks, LogicAskers framework exposes when mod-els fail to genuinely reason and instead fall backon familiar or remembered patterns. This insightsuggests that strengthening LLMs mastery of ba-sic reasoning skills is a necessary foundation forimproving performance on tasks that benefit fromCoT strategies.",
  "Related Work": "Significant advancements in NLP reasoning havebeen achieved through methods such as Chain-of-Thoughts (CoT) prompting (Wei et al., 2022b),which enables models to generate reasoning stepswith minimal training. Enhancing this, the Pro-gram of Thoughts (PoT) prompting (Chen et al.,2022) leverages external interpreters like Python to handle complex mathematical problems. Furtheraugmenting reasoning validity, the Logic Agentframework transforms LLMs into logic agents thatcan dynamically apply propositional logic rulesto convert natural language inputs into structuredlogic forms (Liu et al., 2024).Recent studies have focused on evaluating thereasoning capabilities of Large Language Models(LLMs) by measuring their performance acrossvarious reasoning tasks. These include arithmetic(Cobbe et al., 2021; Hendrycks et al., 2021; Aminiet al., 2019; Patel et al., 2021; Miao et al., 2020;Ling et al., 2017; Roy and Roth, 2016), common-sense (Talmor et al., 2019; Geva et al., 2021; Clarket al., 2018), symbolic (Wei et al., 2022b), andtable reasoning (Nan et al., 2021), as well as un-derstanding words, dates, and causal relationships(Srivastava et al., 2022), and generalization (Lakeand Baroni, 2017; Anil et al., 2022). Despite theseefforts, it remains uncertain whether LLMs trulyreason or rely on simple heuristics, since most as-sessments focus only on accuracy and do not thor-oughly evaluate the reasoning processes.Efforts to develop metrics for more formal rea-soning analysis in LLMs include creating datasetswith first-order logic problems (Han et al., 2022),generating test cases using a single predicate in-ference rule (Saparov and He, 2022), buildinginstruction-tuning dataset designed for CoT reason-ing with GPT-4 (Liu et al., 2023b), and employingpropositional logic with randomized methods (On-tan et al., 2022). These methods, however, oftenlack generalizability or focus on limited deductionrules. Saparov et al. (Saparov et al., 2023) intro-duced a comprehensive approach by using all de-duction rules in propositional logic to assess LLMsdeductive reasoning across complex proofs. Our re-search expands further, incorporating all rules andequivalent laws in both propositional and predicatelogic, aiming to enhance understanding of eachrules impact on LLM performance and using theseinsights for improvement.",
  "Conclusion": "In this paper, we present LogicAsker, an automatedtool designed to comprehensively evaluate and im-prove the formal reasoning abilities of LLMs undera set of atomic skills.Our research demonstrated the efficacy of Logi-cAsker in identifying logical reasoning failures in adiverse set of widely deployed LLMs, we achieved a substantial error detection rate in revealing rea-soning flaws in these models, ranging from 29% to90%. Additionally, we utilized the test cases fromLogicAsker to design in-context learning demon-strations, which effectively enhance the logical rea-soning capabilities of LLMs, e.g., improving from92% to 97% for GPT-4o.By providing insights into the strengths andweaknesses of LLMs in reasoning, we are ableto improve the reliability and trustworthiness ofthese models. The release of all the code and dataaims to facilitate replication and encourage furtherresearch in this crucial area.",
  "This paper identifies two primary limitations thathighlight areas for future research:": "Although our ICL (In-Context Learning) methodsignificantly enhances the logical reasoning capa-bilities of large language models (LLMs), thereremains a performance gap compared to human-level reasoning. Further refinements and innova-tions in model training and architecture may benecessary to bridge this gap. Our method is currently applicable only to LLMsthat possess robust in-context learning capabil-ities. LLMs lacking this feature may not ben-efit from our approach. Future studies couldexplore fine-tuning methods to extend the ap-plicability of our improvements across a broaderspectrum of LLMs, potentially enhancing modelswith weaker or no inherent in-context learningabilities.",
  "generalization in large language models.ArXiv,abs/2207.04901": "Hugo Bronkhorst, Gerrit Roorda, Cor J. M. Suhre,and Martin J. Goedhart. 2020. Logical reasoningin formal and everyday reasoning tasks. Interna-tional Journal of Science and Mathematics Educa-tion, 18:16731694. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, T. J. Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler,Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.ArXiv,abs/2005.14165.",
  "Shuzheng Gao, Xinjie Wen, Cuiyun Gao, WenxuanWang, and Michael R. Lyu. 2023a. Constructing ef-fective in-context demonstration for code intelligencetasks: An empirical study. ArXiv, abs/2304.07575": "Shuzheng Gao, Xinjie Wen, Cuiyun Gao, WenxuanWang, and Michael R. Lyu. 2023b. What makes goodin-context demonstrations for code intelligence taskswith llms? 2023 38th IEEE/ACM International Con-ference on Automated Software Engineering (ASE),pages 761773. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did aristotleuse a laptop? a question answering benchmark withimplicit reasoning strategies. Transactions of theAssociation for Computational Linguistics, 9:346361.",
  "Google. 2024.Gemini. Accessed: 2024-06-16": "Simeng Han, Hailey Schoelkopf, Yilun Zhao, ZhentingQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-terina Zubova, Yujie Qiao, Matthew Burtell, DavidPeng, Jonathan Fan, Yixin Liu, Brian Wong, Mal-colm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai,Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab-bri, Wojciech Kryscinski, Xi Victoria Lin, CaimingXiong, and Dragomir R. Radev. 2022. Folio: Natu-ral language reasoning with first-order logic. ArXiv,abs/2209.00840. Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn XiaodongSong, and Jacob Steinhardt. 2021. Measuring math-ematical problem solving with the math dataset.ArXiv, abs/2103.03874.",
  "Abulhair Saparov and He He. 2022. Language modelsare greedy reasoners: A systematic formal analysisof chain-of-thought. ArXiv, abs/2210.01240": "Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-makumar, Nitish Joshi, Seyed Mehran Kazemi, Na-joung Kim, and He He. 2023. Testing the generaldeductive reasoning capacity of large language mod-els using ood examples. ArXiv, abs/2305.15269. Koustuv Sinha, Shagun Sodhani, Jin Dong, JoellePineau, and William L. Hamilton. 2019. Clutrr: Adiagnostic benchmark for inductive reasoning fromtext. In Conference on Empirical Methods in NaturalLanguage Processing.",
  "Commutative lawsP Q Q PIt is cold and it is winter It is winter and it is cold.P Q Q PYou can go to the party or you can study You can study oryou can go to the party": "Associative laws(P Q) R P (Q R)It is raining and it is cold, and also it is winter It is raining,and also, it is cold and it is winter.(P Q) R P (Q R)Either I will go to the park or I will go to the library is true, or Iwill go to the cinema I will go to the park or either I will goto the library or I will go to the cinema is true. Distributive lawsP (Q R) (P Q) (P R)It is raining and either I have an umbrella or I have a raincoat It is raining and I have an umbrella, or it is raining and I have araincoat.P (Q R) (P Q) (P R)Either I will go to the park, or it is cloudy and it is cold EitherI will go to the park or it is cloudy is true, and either I will go tothe park or it is cold is true.",
  "Complement laws(P) PIt is not the case that it is not raining It is raining": "Conditional lawsP Q P QIf it rains, then Ill stay at home It doesnt rain or I stay athome.Bidirectional laws(P Q) (P Q) (P Q)Ill go to the park if and only if its sunny Either its sunnyand I go to the park, or its not sunny and I dont go to the park.",
  "x((P(x) Q(x))) x(P(x) Q(x))": "In this example, the goal is to extend the proposi-tional equivalence law to its predicate version byadding quantifiers. To achieve this goal, we firstnote that DeMorgans law states that \"P and Q can-not both be true\" (e.g., Alice is happy and Bob ishappy cannot both be true) is equivalent to \"eithernot P or not Q\" (e.g., either Alice is not happy orBob is not happy). Since the two expressions areequivalent, we can add the same quantifier to bothsides and the equivalence will still hold. There-fore, by adding a \"for all\" quantifier to both sides,we obtain \"for all x, P(x) and Q(x) cannot both betrue\" (for all persons in the room, the person likesCharley and the person likes David cannot both betrue) is equivalent to \"for all x, either not P(x) ornot Q(x)\" (e.g., for all person in the room, either theperson doesnt like Charley or the person doesntlike David). Before the extension, the law can onlybe applied to simple propositions (e.g., P = \"Aliceis happy\", Q = \"Bob is happy\"), but after extension,the law can be applied to predicates with variablesand quantifiers (e.g., P(x) = \"x likes Charley\", Q(x)= \"x likes David\") The same also applies to the",
  "Quantifier NegationxP(x) xP(x)It is not the case that all birds can fly There exists abird that cannot fly.xP(x) xP(x)There is no human that can live forever All humanscannot live forever": "Quantifier Distributionx(P(x) Q(x)) xP(x) xQ(x)Every student is smart and diligent Every student issmart, and every student is diligent.x(P(x) Q(x)) xP(x) xQ(x)There is a person who is either a doctor or a lawyer There is a person who is a doctor, or there is a personwho is a lawyer. Quantifier Movementx(P Q(x)) (P xQ(x))For every child, if it is raining then they are inside Ifit is raining, then every child is inside when the notionof raining doesnt depend on the specific child.x(P Q(x)) (P xQ(x))There exists a student who is tall and a good basketballplayer There is a tall student and there exists a studentwho is a good basketball player when the notion of beingtall doesnt depend on the specific student.",
  "{AB C} {x, (A)x, (B) x, (C)}": "(i.e., if A and B imply C, there exists x such thatA is true and for all x, B is true implies thereexists x such that C is true). Since all proposi-tional inference rules are of the form P Q C,we can transform them into their predicate formx, P(x)x, Q(x) x, C(x) and x, P(x)x, Q(x) x, C(x) following similar procedurein the previous section.",
  ". Negation: If the clause starts with a op-erator, the algorithm then translates the restof the clause based on a negation template,making sure to negate the statement": "3. Quantifiers: For clauses that start with (meaning for all items) or (meaning thereis at least one item), it translates these into nat-ural language, adjusting the phrasing based onwhether were asserting something positivelyor negating it. 4. Logical Connectives: If the clause combinespropositions using logical operators like ,, (implies), or (if and only if),the function translates these into natural lan-guage phrases that express the relationshipbetween the propositions.",
  "Inference RuleLogical FormExample": "Universal InstantiationxP(x) P(c)All humans are mortal. Hence, Socrates is mortal.Existential GeneralizationP(c) xP(x)This apple is red. Hence, there exists a red apple.Universal Generalization{P(x)} yP(y)Any particular human is mortal. Hence, all humans are mortal.Modus Ponens{P Q, P} QIf it rains, the street gets wet. It is raining. Hence, the street iswet.Modus Tollens{P Q, Q} PIf it rains, the street gets wet. The street is not wet. Hence, itis not raining.{P Q, Q} PIf it rains, the street does not get wet. The street is wet. Hence,it is not raining.Transitivity{P Q, Q R} P RIf I study, I will pass the test. If I pass the test, I will get areward. Hence, if I study, I will get a reward.Disjunctive Syllogism{P Q, P} QEither its raining or its snowing. Its not raining. Hence, itssnowing.Addition{P} P QIt is raining. Hence, it is raining or snowing.Simplification{P Q} PIt is raining and it is cold. Hence, it is raining.{P Q} QIt is raining and it is cold. Hence, it is cold.Conjunction{P, Q} P QIt is raining. It is cold. Hence, it is raining and it is cold.Resolution{P Q, P R} Q REither it is raining or snowing. If it is not raining, then it iscloudy. Hence, either it is snowing or it is cloudy.Disjunction Elimination{P R, Q R, P Q} RIf it rains, I will stay home. If it snows, I will stay home.Either it will rain or snow. Hence, I will stay home.Biconditional Introduction{P Q, Q P} P QIf I study, I pass. If I pass, I studied. Hence, I study if andonly if I pass.Biconditional Elimination{P Q} P QI study if and only if I pass. Hence, if I study, I pass.{P Q, P} QI study if and only if I pass. I didnt study. Hence, I didntpass.{P Q, Q} PI study if and only if I pass. I didnt pass. Hence, I didntstudy.",
  "Predicates": "a cashier, a janitor, a bartender, a server, anoffice clerk, a mechanic, a carpenter, an elec-trician, a nurse, a doctor, a police officer, ataxi driver, a soldier, a politician, a lawyer,a scientist, an astronaut, a poet, an artist, asailor, a writer, a musician, poor, rich, happy,sad, fast, curious, excited, bored, tired, joy-ful, intelligent, skilled, efficient, meticulous,creative.",
  "Actions": "make tea, makes tea, making tea, drink wa-ter, drinks water, drinking water, read a book,reads a book, reading a book, play tennis,plays tennis, playing tennis, play squash,plays squash, playing squash, play a game,plays a game, playing a game, go running,goes running, running, work, works, working,sleep, sleeps, sleeping, cook, cooks, cooking,listen to a song, listens to a song, listening toa song, write a letter, writes a letter, writinga letter, drive a car, drives a car, driving a car,climb a mountain, climbs a mountain, climb-ing a mountain, take a plane, takes a plane,taking a plane, paint a picture, paints a picture,painting a picture.",
  "Impersonal Candidates": "snowing, snows, doesnt snow, snow, raining,rains, doesnt rain, rain, sunny, is sunny, isnot sunny, be sunny, cloudy, is cloudy, is notcloudy, be cloudy, windy, is windy, is notwindy, be windy, cold, is cold, is not cold,be cold, late, is late, is not late, be late, over-cast, is overcast, is not overcast, be overcast,foggy, is foggy, is not foggy, be foggy, humid,",
  "NameLogical FormExample": "Affirming the Consequentp q, q pIf I study, I will pass the test.I passed the test. Therefore, Istudied.Denying the Antecedentp q, p qIf it rains, the street gets wet.It is not raining. Therefore, thestreet is not wet.Affirming a Disjunctp q, p qEither I will study or I will failthe test. I studied. Therefore, Iwill not fail the test.Denying a Conjunct(p q), p qIm not both hungry and thirsty.Im not hungry. Therefore, Imthirsty.Illicit Commutativityp q q pIf I am in Paris, then I am inFrance. Therefore, if I am inFrance, I am in Paris.Existential Fallacyx(P(x) Q(x)), x(P(x)) x(Q(x))All birds can fly.No birdsare present. Therefore, noth-ing can fly.Illicit Majorx(P(x) Q(x)), x(Q(x)) x(P(x))All humans are mortal. Some-thing is mortal.Therefore,something is human.Illicit Minorx(P(x) Q(x)), x(P(x) R(x)) x(R(x) Q(x))All men are mortal. All menare humans. Therefore, all hu-mans are mortal.Undistributed Middlex(P(x) Q(x)), Q(a) P(a)All dogs are animals. My cat isan animal. Therefore, my catis a dog.",
  "Q:Considerthefollowingpremises:It is not raining.It is raining or itis late. Can we infer the following fromthem? Answer yes or no: It is not late": "A:LetAbetheclaimthat\"itisraining\",B be the claim that \"it islate\", then the premises are \"not A, Aor B\". We can infer \"B\", which is \"it islate\". Therefore, we cannot infer \"it isnot late\". The answer is ==no==. Q:Considerthefollowingpremises:For all x, x will write a letter, and xwill climb a mountain and x is a musician.Can we infer the following from them?Answer yes or no: For all x, x will writea letter and x will climb a mountain, andx is a musician.",
  "GAccuracy Versus Inference Length": "To assess the impact of inference length, we gen-erated test cases of varying lengths (i.e., rangingfrom 1 to 9) using randomly selected rules. Foreach length, we generated 100 test cases. shows the performance of LLMs in these test cases.Generally, LLMs perform gradually worse as theinference length increases, indicating the increasedcomplexity introduced by longer inference chains.",
  "LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeak": "PropositionalEquivalentComplement lawsContradiction1.001.001.001.00PropositionalEquivalentComplement lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsUnrelated1.000.921.001.00PropositionalInferenceConjunctionContradiction1.001.001.001.00PropositionalInferenceDisjunction eliminationContradiction1.001.001.000.20PropositionalInferenceDisjunction eliminationInference1.000.600.921.00PropositionalInferenceDisjunction eliminationUnrelated1.001.001.000.96PropositionalEquivalentDistributive lawsContradiction1.000.841.001.00PredicateEquivalentExistential de morgans lawsInference1.000.760.240.80PredicateInferenceExistential additionContradiction1.001.001.001.00PredicateInferenceExistential additionInference1.000.120.200.00PredicateInferenceExistential biconditional eliminationContradiction1.001.001.000.96PredicateInferenceExistential biconditional introductionContradiction1.000.841.001.00PredicateEquivalentExistential commutative lawsInference1.000.640.920.88PredicateEquivalentExistential complement lawsContradiction1.001.001.001.00PredicateEquivalentExistential complement lawsInference1.001.001.001.00PredicateInferenceExistential disjunction eliminationContradiction1.001.001.000.84PredicateInferenceExistential disjunction eliminationInference1.000.720.681.00PredicateInferenceExistential disjunction eliminationUnrelated1.001.001.000.96PredicateInferenceExistential disjunctive syllogismInference1.000.360.480.44PredicateEquivalentExistential distributive lawsContradiction1.001.001.001.00PredicateInferenceExistential generalizationContradiction1.001.001.001.00PredicateInferenceExistential generalizationUnrelated1.001.001.001.00PredicateEquivalentExistential idempotent lawsContradiction1.001.001.001.00PredicateEquivalentExistential idempotent lawsInference1.001.000.880.88PredicateFallacyExistential illicit commutativityInference1.001.001.001.00PredicateInferenceExistential modus ponensContradiction1.001.001.001.00PredicateInferenceExistential modus tollensContradiction1.001.001.001.00PredicateInferenceExistential resolutionContradiction1.000.961.001.00PredicateInferenceExistential resolutionInference1.000.640.520.84PredicateInferenceExistential simplificationContradiction1.001.001.001.00PredicateInferenceExistential simplificationInference1.000.880.960.68PropositionalEquivalentIdempotent lawsContradiction1.001.001.001.00PropositionalEquivalentIdempotent lawsInference1.001.000.560.52PropositionalFallacyIllicit commutativityInference1.001.001.001.00PropositionalInferenceModus ponensContradiction1.001.001.000.76PropositionalInferenceModus tollensContradiction1.001.001.001.00PropositionalInferenceResolutionContradiction1.000.721.000.92PropositionalInferenceResolutionInference1.000.601.001.00PropositionalInferenceSimplificationContradiction1.001.001.001.00PropositionalInferenceSimplificationInference1.001.001.000.92PredicateFallacyUndistributed middleInference1.001.001.001.00PredicateEquivalentUniversal de morgans lawsInference1.000.320.680.88PredicateFallacyUniversal affirming a disjunctInference1.001.001.000.92PredicateFallacyUniversal affirming the consequentInference1.001.001.000.96PredicateEquivalentUniversal biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal commutative lawsContradiction1.001.001.000.96PredicateEquivalentUniversal commutative lawsInference1.000.561.001.00PredicateEquivalentUniversal commutative lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal complement lawsContradiction1.001.001.001.00PredicateEquivalentUniversal complement lawsInference1.001.001.001.00PredicateEquivalentUniversal complement lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal conditional lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal idempotent lawsContradiction1.001.001.001.00PredicateEquivalentUniversal idempotent lawsInference1.000.880.680.80PredicateFallacyUniversal illicit commutativityInference1.001.001.001.00PredicateInferenceUniversal resolutionInference1.000.160.240.88PredicateInferenceUniversal simplificationContradiction1.001.001.001.00PredicateInferenceUniversal transitivityUnrelated1.001.000.960.36"
}