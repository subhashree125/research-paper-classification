{
  "Abstract": "Image-text contrastive models like CLIP havewide applications in zero-shot classification,image-text retrieval, and transfer learning.However, they often struggle on compositionalvisio-linguistic tasks (e.g., attribute-binding orobject-relationships) where their performanceis no better than random chance. To addressthis, we introduce SDS-CLIP, a lightweightand sample-efficient distillation method to en-hance CLIPs compositional visio-linguisticreasoning. Our approach fine-tunes CLIP us-ing a distillation objective borrowed from largetext-to-image generative models like Stable-Diffusion, which are known for their strongvisio-linguistic reasoning abilities. On the chal-lenging Winoground benchmark, SDS-CLIPimproves the visio-linguistic performance ofvarious CLIP models by up to 7%, while onthe ARO dataset, it boosts performance by upto 3%. This work underscores the potential ofwell-designed distillation objectives from gen-erative models to enhance contrastive image-text models with improved visio-linguistic rea-soning capabilities.",
  "Introduction": "In recent years, multimodal models like CLIP (Rad-ford et al., 2021a) have excelled in tasks such aszero-shot classification, image-text retrieval, andimage-captioning (Mu et al., 2021; Yu et al., 2022;Li et al., 2022; Mokady et al., 2021). These mod-els are also crucial components in various state-of-the-art pipelines for tasks like segmentation andobject detection (Wang et al., 2021; Lddeckeand Ecker, 2021; Minderer et al., 2022; Zhonget al., 2021). However, they struggle with visio-linguistic reasoning tasks, such as determiningthe spatial relationships between objects in animage (Yuksekgonul et al., 2023; Huang et al.,2023). Notably, CLIPs performance on the chal-lenging Winoground (Thrush et al., 2022; Diwanet al., 2022), a benchmark designed to assess visio-linguistic reasoning, is close to random chance. This shortcoming is attributed to CLIPs contrastiveobjective which prioritizes shortcuts for retrieval,and thus impacts its ability to understand fine-grained object details and their positions (Diwanet al., 2022; Thrush et al., 2022).In contrast, text-to-image models like StableDiffusion (Rombach et al., 2021) excel in visio-linguistic tasks, likely due to their text condition-ing enhanceing semantic consistency in its cross-attention maps (Li et al., 2023; Clark and Jaini,2023). Li et al. (2023) recently demonstrated thison the Winoground benchmark, reliably matchingcaptions to images with fine-grained spatial differ-ences using denoising diffusion scores (see Fig 1).Similar results have been shown for other text-to-image models, including Imagen (Clark and Jaini,2023), with almost all of these methods outperform-ing CLIP variants on the same tasks.While these works have shown the potential ofusing generative text-to-image models for visio-linguistic tasks, it remains computationally inten-sive. For instance, computing the denoising diffu-sion score for image-text matching involves multi-ple passes through a UNet model (approximately892M parameters) with varying noise levels andtime-steps. On an entry-level GPU, this can take upto a minute for a single image-text matching task,making it impractical for real-world and real-timeapplications. In contrast, CLIP models can classifyimages up to 18 times faster (see Fig 1), requir-ing only one pass through both image and text en-coders. A promising research direction, therefore,lies in finding methods that combine the strongvisio-linguistic capabilities of text-to-image mod-els with the rapid inference of CLIP.To this end,we introduce SDS-CLIP, alightweight and sample-efficient fine-tuning ap-proach for CLIP which distills knowledge from Sta-ble Diffusion, and enhances CLIPs visio-reasoningcapabilities. Specifically, we add a regularizationterm to CLIPs standard contrastive loss based",
  "Diffusion Score": ":CLIP variants underperform onWinoground, a visio-linguistic reasoning benchmark,compared to Diffusion Score from Stable Diffusion.The diffusion score is computed from Stable Diffusionsloss function. Note that Diffusion Score takes 18more time than CLIP variants for inference (using 50samplings during diffusion score computation). on score-distillation sampling (SDS) (Poole et al.,2022). This regularization encourages CLIPs em-beddings to be aligned with the denoising diffusionloss from a text-to-image model. By fine-tuningCLIP with this regularized objective on a smallpaired image-text dataset, specifically 118k image-text pairs from MS-COCO, we demonstrate an 1.5-7% performance gain compared to vanilla CLIPon Winoground and ARO, two highly challengingvisio-linguistic reasoning benchmarks. Notably,this is achieved by only updating CLIPs Layer-Norm parameters. Furthermore, we show that SDS-CLIPs zero-shot performance is not impacted on awide range of downstream datasets.In summary, our contributions are as follows:",
  "The Winoground benchmark establishes a challeng-ing image-text matching task to measure a modelsvisio-linugistic reasoning abilities: given an im-age x, the model must match it with the correct": "caption c from a set of captions C = {ci}ni=1,where all caption contains the same words but eachdescribes a different spatial arrangement of theobjects, with only one being correct. Concurrentworks (Clark and Jaini, 2023; Li et al., 2023; Kro-jer et al., 2023) to this paper have showed that it ispossible to use the denoising diffusion score fromtext-to-image generative models to perform suchan image-matching task. This can be formalized asfollows: for an image x and caption c, the denois-ing diffusion score, denoted by d(x, c), is definedas:",
  "(2)": "where t is the sampled time-step, is the noiseprediction UNet, v is an encoder (e.g., VQ-VAE)which maps the image x to a latent code and is thesampled Gaussian noise. Previous works (Krojeret al., 2023) have demonstrated that by adoptingthis approach, text-to-image models performingstrongly on visio-linguistic reasoning benchmarkslike Winoground, outperforming contrastive mod-els like CLIP by a significant margin (see Fig 1).For ARO, we obtain an accuracy of 0.63 with thediffusion score which is better than CLIP models.",
  "SDS-CLIP: Our Method": "The core idea of our approach is to regularize thecontrastive objective in CLIP with the denoisingdiffusion score from Stable Diffusion (see Eq.(1)).Our method builds on the recent work of (Pooleet al., 2022) which maps the output of a 3D NeRFmodel into the input space of Stable DiffusionsUNet and optimizes its parameteres with the de-noising diffusion loss, also known as the score-distillation sampling (SDS). In a similar vein, wefine-tune the parameters of CLIP using SDS. In-tuitively, our set-up can be viewed as a form ofknowledge distillation where the teacher is the text-to-image model and the student is CLIP. As a re-sult, in inference, CLIP can benefit from the visio-linguistic reasoning capabilities that are alreadylearned by text-to-image diffusion models.Formally, we map the output of CLIPs image en-coder to the input space of Stable Diffusions UNet.Specifically, we pass a given image x through",
  "FT with LCLIP + LSDS0.2650.300.210.420.290.190.600.550.66": ": Our fine-tuning method SDS-CLIP improves CLIP performance on the Winoground benchmark by1.5% to 7% and upto 3% for the ARO-Relation and Attribution tasks across various CLIP variants. Specifically,we find that our method improves on the sub-categories involving object-swap and relational understanding whichcomprise of the majority of the tasks in Winoground. Note that only fine-tuning with image-text pairs from MS-COCO without the distillation loss does not lead to any improvements. OpenCLIP results in Appendix I.",
  "CLIPs image encoder f and map its <CLS> em-bedding through a linear map hw Rd46464": "into the input space of Stable Diffusions UNet. This can be formalized as (hw(f(x)), t, c)where t is the time step and c is the correspondingtext caption for the given image. We then use thisterm in place of (v(x), t, c) in Eq. (2) to arriveas a denoising diffusion loss LSDS which encour-ages image-text binding with feedback from thediffusion loss:",
  "Ltotal = LCLIP + LSDS(4)": "where LCLIP is defined in Appendix C.1 and isa hyper-parameter that can be set with a grid search.We note that there are multiple ways to incorporatea diffusion loss into CLIPs objective. We foundthat as an additional loss term led to the best results,however, we include the full set of design choiceswe considered in the Appendix.Similar to differentiable image parameteriza-tions (Mordvintsev et al., 2018) where a given func-tion is optimized by backpropogation through theimage generation process, the UNet parameters are kept frozen during the optimization process.Specifically, given Ltotal(, , w, ):",
  "Experiments": "In this section, we empirically validate our pro-posed method SDS-CLIP on two types of tasks:i) visio-linguistic reasoning using two challengingbenchmarks (Winoground, ARO) and ii) zero-shotimage classification using a suite of downstreamdatasets (ImageNet, CIFAR-100, and others). Over-all, we show that our method improves CLIPs per-formance significantly on Winoground and somekey tasks in ARO, while also marginally improvingdownstream zero-shot classification performance.",
  "Experimental Setup": "CLIP Models. We consider the following CLIPvariants in our experiments: (i) CLIP ViT-B/16; (ii)CLIP ViT-B/32; (iii) CLIP-ViT-L-14; (iv) CLIP-ViT-L-14 336px; (v) CLIP-ResNet-50.Implementation Details. Due to computationallimit, we fine-tune CLIP from a publicly avail-able checkpoint instead of training from scratch.Notably, we only fine-tune CLIPs LayerNormparameters following (Basu et al., 2023) alongwith the linear transformation hw accountingfor only 8M trainable parameters. We fine-tune these parameters using image-text pairs fromMSCOCO (Lin et al., 2014). In particular, wechoose MSCOCO as it is relatively small and lessnoisy than other image-text datasets such as CC-12M (Sharma et al., 2018). Both these factorsmake our fine-tuning method extremely sample- : Our fine-tuning method does not harm the zero-shot abilities of CLIP. In fact for certain downstreamdatasets (e.g., ImageNet, CIFAR-10, MNIST, Aircraft) we observe an improvement in the zero-shot performancebetween 1% 8% for ViT-B/16. For other CLIP models, we find no drop in zero-shot performance. and parameter-efficient.Baselines. We compare our method with twodifferent baselines: (i) pre-trained (vanilla) CLIPcheckpoints; and (ii) CLIP fine-tuned on MS-COCO with the standard contrastive loss withoutthe regularization term.4.2Results Winoground.We evaluate SDS-CLIP on thechallenging visio-linguistic reasoning benchmark,Winoground (Thrush et al., 2022). In Table (1),we find that our approach consistently improvesperformance across all Winoground sub-categoriesand CLIP variants, yielding absolute improvementsranging from 1.5% to 7%. The largest gain of 7% isobserved in ViT-B/16 (CLIP), with other CLIP vari-ants showing consistent improvements of 1.5% to2%. In the Appendix( ), we provide resultsfor CLIP variants pre-trained on public data, wheresimilar improvements are observed. On further in-spection of the Winoground sub-categories, we findthat SDS-CLIP shows consistent improvementsin object-swap\" and relation\". It is worth not-ing that the both sub-category, which combinesboth object-swap\" and relation\" tags, makes uponly 5% of all tasks, thus are potentially not fullyrepresentative of all scenarios involving both ob-ject swaps and relational understanding. We alsoanalyse SDS-CLIPs robustness to the number ofpredicates in captions and find that overall, it en-hances performance in tasks where there are bothone and two predicates.ARO.TheAROdataset(Yuksekgonulet al., 2023) comprises tasks for (i) attribute-understanding and (ii) relational-understanding.In , we find that SDS-CLIP enhancesperformance by 1%-3% in the \"attribute-binding\"and \"relational understanding\" tasks. Impact on CLIPs zero-shot performance.From Fig 2, we find that SDS-CLIPs zero-shotclassification capbilities are not impacted, relativeto vanilla CLIP. In fact, we find that ViT-B/16szero-shot performance improves across a range ofdownstream datasets (with up to 8% improvementfor MNIST).While Stable-Diffusion is pre-trained on a muchlarger set of image-text pairs than CLIP, in Ap-pendix K, we show that the CLIP variants pre-trained on LAION-2B still suffer on Winoground.In fact, we show that using SDS-CLIP can im-prove compositional reasoning of such CLIP vari-ants. In Appendix H, we show results with fine-tuning on the larger CC-3M (Sharma et al., 2018).",
  "Related Works": "While CLIP models (Radford et al., 2021a) arerenowned for their robust zero-shot classifica-tion, recent research (Thrush et al., 2022; Di-wan et al., 2022) has exposed their limitationsin visio-linguistic reasoning. In contrast, recentstudies have demonstrated that text-to-image mod-els (Clark and Jaini, 2023; Li et al., 2023; Krojeret al., 2023; Chen et al., 2023) outperform CLIPin reasoning tasks. These models in fact lever-age scores computed from the diffusion objective.We note that while (Poole et al., 2022) use score-distillation sampling for text to 3D generation, oursis the first work to adapt the formulation as a regu-larizer and improve compositional abilities in CLIP. 6ConclusionOur paper introduces SDS-CLIP, a novel dataand parameter-efficient method that effectively en-hances CLIPs visio-linguistic reasoning abilitiesby distilling knowledge from text-to-image models,without compromising its zero-shot abilities.",
  "Limitations": "The primary limitation of our method is the inabil-ity to use large batch-sizes on moderate size GPUs.This is due to the fact that the regularizer LSDSrequires a full backward pass through the UNet,even though its parameters are frozen. We also findthat while the original diffusion score is good atobject-understanding, attribute-understanding andrelational-understanding tasks, it does not performwell on ordering tasks from the ARO dataset. Forthis reason, distillation from Stable-Diffusion po-tentially may not be effective in improving CLIPsperformance on ordering tasks. Similar results arealso observed in concurrent works such as (Krojeret al., 2023).",
  "Ethical Considerations": "Vision-language models such as CLIP have beenknown for inheriting biases (Agarwal et al., 2021)due to their training data. Our work uses a well-known widely used dataset (MS-COCO) for thefine-tuning procedure and therefore does not in-troduce any additional bias. In fact, our distilla-tion method mitigates some of the inherited bias inCLIP which earlier did not lead to good reasoningcapabilities. Sandhini Agarwal, Gretchen Krueger, Jack Clark, AlecRadford, Jong Wook Kim, and Miles Brundage. 2021.Evaluating CLIP: towards characterization of broadercapabilities and downstream implications. CoRR,abs/2108.02818.",
  "Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath,and Kyle Mahowald. 2022.Why is winogroundhard? investigating failures in visuolinguistic compo-sitionality": "Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang,Xinfeng Zhang, Weijie Chen, Zeng Zhao, Tangjie Lv,Zhipeng Hu, and Wen Zhang. 2023. Structure-clip:Enhance multi-modal language representations withstructure knowledge. Justin Johnson, Bharath Hariharan, Laurens van derMaaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.Girshick. 2016. CLEVR: A diagnostic dataset forcompositional language and elementary visual rea-soning. CoRR, abs/1612.06890.",
  "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-ung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.Coca: Contrastive captioners are image-text founda-tion models": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou. 2023. When and whyvision-language models behave like bags-of-words,and what to do about it? In The Eleventh Interna-tional Conference on Learning Representations. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-yuan Li, Noel Codella, Liunian Harold Li, LuoweiZhou, Xiyang Dai, Lu Yuan, Yin Li, and JianfengGao. 2021.Regionclip: Region-based language-image pretraining. CoRR, abs/2112.09106.",
  "A.1Benchmark datasets": "Winoground (Thrush et al., 2022; Diwan et al.,2022) is a challenging vision-language datasetfor evaluating the visio-linguistic characteristicsof contrastively trained image-text models. Thedataset consists of 400 tasks, where each task con-sists of two image-text pairs. The objective is toindependently assign the correct text caption toeach image. Each task is also annotated with meta-data corresponding to whether the task requiresobject-understanding, relational-understanding orboth. The tasks in Winoground are challengingas the images differ in fine-grained ways and as-signing the correct text captions requires inherentcompositional visual reasoning.ARO (Yuksekgonul et al., 2023) similarly testsvisio-linguistic reasoning and consists of threetypes of tasks: (i) Visual Genome Attribution to testthe understanding of object properties; (ii) VisualGenome Attribution to test for relational under-standing between objects; and (iii) COCO-Orderand Flickr30k-Order to test for order sensitivity ofthe words in a text, when performing image-textmatching. We highlight that Winoground thoughslightly smaller in size than ARO is more challeng-ing as it requires reasoning beyond visio-linguisticcompositional knowledge (Diwan et al., 2022).",
  "A.2Does distilling features directly fromUNet help?": "Previous works such as (Xu et al., 2023) find thatthe frozen features of the UNet contain structuralinformation about the image. Motivated by this,we also investigate if distilling knowledge directlyfrom the frozen UNet features is beneficial, Givenan image x and its caption c, the frozen featuresf from the UNet (where I(x, c) = (v(x), t, c),similar to (Xu et al., 2023)) can be extracted. Wethen use these frozen internal representations fromthe UNet to regularize features of the image en-coder in CLIP. In particular:",
  "C.1CLIP": "CLIP (Radford et al., 2021b) is a image-text modelwhich is pre-trained using a contrastive objective,typically on internet-scale data. The core intu-ition of the training objective is to align the textand image embeddings of image-text pairs in ashared embedding space. To do this, CLIP con-sists of two components: (i) an image encoderf which transforms a raw image xi into an im-age embedding eimg(xi) = f(xi) Rd, alsodenoted by the <CLS> token; and (ii) a text en-coder g which transforms a raw text caption ciinto a text embedding etext(ci) = g(ci) Rd also denoted by <EOS> token, both of whichmap to an embedding dimensionality d. Givena dataset D = {(xi, ci)}Ni=1 of image-text pairs,where (xi, yi) is the ith image-text pair, CLIP usesa contrastive objective to pull the image and textembeddings of matched pairs together, while push-ing those of unmatched pairs apart. Formally, thecontrastive objective can be defined as:",
  "DWhen does distillation not help CLIP?": "While we find that distilling knowledge fromStable-Diffusion to CLIP helps in object-swap,relational-understanding and attribution-bindingvisio-linguistic tasks, it does not help on taskswhere the order of the text is perturbed (e.g. theCOCO-Order and Flickr-Order tasks in the AROdataset). In fact, we find that the denoising diffu-sion score in Equation (1) leads to accuracies of0.24 for COCO-Order and 0.34 for Flickr-Orderwhich is in fact lower than CLIP models. Concur-rent works (Krojer et al., 2023) has shown similarlylow performance for text-ordering tasks. A poten-tial reason could be that ordering tasks only testfor grammatical understanding which current textencoders cannot effectively model. Another reasoncould be that the denoising diffusion score is notaffected by word ordering as the image semanticsare not changed as a result.",
  "FMore Experimental Details": "Hyper-parameters. We perform a hyperparametersweep for the learning rate and the regularizationhyperparameter for ViT-B/16. We use these samehyperparameters for different CLIP variants in-cluding ViT-B/32, ViT-B/14, ViT-L/14-336px andResNet-50. In particular, we set = 0.001 and setthe learning rate as 5 105. We use a batch-sizeof 32 for all the different CLIP models. We useStable-Diffusion v1-4 as the teacher model in ourexperiments.Note on Full Fine-tuning. All our experimentswere primarily done by fine-tuning only the Layer-",
  ": Additional results on Winoground withViT-B/16 CLIP pre-trained on public data (LAION-400M)": "Norm parameters. In the initial phase of the project,we also fine-tune all the parameters of the text andimage encoder in CLIP, however it results in worseperformances than those reported in Table. (1). Po-tentially, this can be due to overfitting issues whenused in conjunction with the new regularizer. Wetherefore run all the experiments with LayerNormtuning as it leads to the best results.Total GPU Hours. For all our experiments weuse NVIDIA-A6000 and each fine-tuning experi-ment takes 6 hours.",
  "GAdditional Results withStable-Diffusion-v2-1": "In particular, with our distillation strategy withStable-Diffusion v-2.1 as a teacher we obtainthe following results on Winoground: (i) ViT-B/16:0.35; (ii) ViT-B/32: 0.33; (iii) ViT-L/14: 0.31; (iv)ViT-L/14-336px: 0.31; (iv) ResNet-50: 0.28; Allthe scores are higher than the fine-tuned modelwith Stable-Diffusion-v1-4 as the teacher, there-fore highlighting that a teacher with better com-positional generation capabilities will be a betterchoice.",
  "HFine-tuning with Conceptual Captions": "We primarily use MS-COCO as : (i) Its a rela-tively small dataset which can keep the fine-tuningsteps relatively smaller and scaling the fine-tuningdataset will increase fine-tuning time; (ii) Its awell-established, relatively diverse and well anno-tated image-text dataset which is used by the com-munity. We also fine-tuned with CC-3M (Sharmaet al., 2018), but found the improvements to besimilar in lines to that using MS-COCO. For e.g.,On Winoground with CC-3M, we find the fol-lowing performance after distillation with Stable-Diffusion-v1-4: (i) ViT-B/16: 0.32; (ii) ViT-B/32:0.32; (iii) ViT-L/14: 0.30; (iv) ViT-L/14-336px:0.28; (iv) ResNet-50: 0.27.These scores areonly marginally better than using MS-COCO, al-though the dataset size is more than 30 times which shows that a high-quality dataset such as",
  "JAdditional Results on CLEVR": "We apply our fine-tuned model on the CLEVRtask (Johnson et al., 2016) which consists of im-ages of 3D shapes isolating phenomena such asspatial reasoning or attribute binding. We find thatthe diffusion-score leads to a score of 0.67, whereasthe best CLIP variant in our test-bed (CLIP ViT-L/14) scored 0.63. With our distillation loss duringfine-tuning this score improved to 0.65 with a 2%gain.",
  "KIs it the Scale of Pre-Training DataWhich Helps?": "In , we show that CLIP models evenwhen trained at the same scale of pre-training dataas Stable-Diffusion (LAION-2B) struggle on theWinoground dataset. We specifically highlight thatCLIP (when pre-trained on 2B image-text pairs)obtain a score of 0.27, whereas the diffusion modelwhen trained on similar pre-training corpus obtainsa score of 0.35. This clearly shows that at a similarpre-training scale, diffusion models (with their dif-fusion objective) are better compositional learnersthan CLIP like models. Our distillation methodfrom Stable-Diffusion improves the Winogroundscore from 0.27 to 0.31 on CLIP(pre-trained on 2Bimage-text pairs).",
  "LBeyond CLIP": "We find that Open-CoCa (Yu et al., 2022) pre-trained on 2B image-text pairs obtains a score of0.30 on Winoground. With our distillation strategy,we find that the score improves to 0.33 highlightingthat our distillation strategy can be used for modelsbeyond CLIP. A full investigation of the impact ofour distillation method on various vision-languagemodels is deferred towards future work."
}