{
  "Abstract": "We present a new perspective on how readers in-tegrate context during real-time language com-prehension. Our proposals build on surprisaltheory, which posits that the processing effortof a linguistic unit (e.g., a word) is an affinefunction of its in-context information content.We first observe that surprisal is only one out ofmany potential ways that a contextual predictorcan be derived from a language model. Anotherone is the pointwise mutual information (PMI)between a unit and its context, which turns outto yield the same predictive power as surprisalwhen controlling for unigram frequency. More-over, both PMI and surprisal are correlated withfrequency. This means that neither PMI nor sur-prisal contains information about context alone.In response to this, we propose a techniquewhere we project surprisal onto the orthogo-nal complement of frequency, yielding a newcontextual predictor that is uncorrelated withfrequency. Our experiments show that the pro-portion of variance in reading times explainedby context is a lot smaller when context is rep-resented by the orthogonalized predictor. Froman interpretability standpoint, this indicates thatprevious studies may have overstated the rolethat context has in predicting reading times.",
  "Introduction": "Surprisal theory (Hale, 2001; Levy, 2008) positsthat the amount of effort it takes to process alinguistic unit is an affine function of its in-contextinformation content, i.e., its surprisal. Numerousstudies have found empirical support for surprisaltheory across different reading measurementmethods, languages, and language models (Smithand Levy, 2013; Wilcox et al., 2020; Kuribayashiet al., 2021; Meister et al., 2021; Wilcox et al.,2023; Shain et al., 2024), particularly whencontrolling for additional effects such as frequency.In this work, we take a critical look at surprisaltheory as an adequate explanation for the role of context in reading time prediction, starting froma simple observation: Surprisal is but one quantitythat can be derived from a language model torepresent the effect of context (Giulianelli et al.,2024). We first show that, as an alternative tosurprisal, one could take an association-based viewon real-time language comprehension and model itas a function of the pointwise mutual information(PMI) between a unit and its context. BecausePMI, surprisal, and frequency are collinear, alllinear models with just two of these covariates areequivalent in terms of their predictive power. Thissimple identity therefore implies that all empiricalvalidation of surprisal theory based on linearregression modeling also lends support for anassociation-based theory of language processing.This raises the question of whether there isa more suitable way to estimate the effect thatcontext has on reading time. We argue that, giventhat frequency is known to play an important rolein processing effort (Broadbent, 1967; Inhoff andRayner, 1986; Rayner and Duffy, 1986; Bybee,2006), a more interesting construct to analyzeshould be what context contributes beyond whatis already captured by frequency.To obtain apredictor that represents just that, we proposea technique where we project surprisal onto theorthogonal complement of frequency, ensuringthat they are uncorrelated. This process effectivelydisentangles the contextual and non-contextualinformation into different covariates in ourregressions and closely resembles residualization.1 To test whether the choice of contextual predic-tor matters empirically, we measure how much thevariance in reading times explained by the contex-tual predictor changes when substituting surprisalfor the orthogonalized context predictor. We findthat our proposed predictor results in much smallerexplained variance. Our results suggest that em-pirical work on surprisal theory has overestimatedthe effect that context has on reading times.",
  "Predictive Models of Reading Behavior": "We seek to model the cognitive processing diffi-culty of a unit u, e.g., a word, drawn from an al-phabet . Additionally, we augment to includea unique EOS symbol which indicates the endof an utterance; we further define def= {EOS}.Let be the set of all strings over the alphabet ;we write u for a string, ut for the tth unit inu, |u| for the number of units in u, and uv for theconcatenation of u with another string v. Given astring cu, we are interested in how us processingeffort is shaped by its context of preceding units c.A common psychometric proxy for the cognitiveprocessing difficulty of u is the time it takes ahuman to read u, typically, as measured in an eye-tracking study (Rayner, 1998). In general terms,we are interested in empirically assessing sometheory of cognitive processing difficulty, which canbe thought of as a collection of unit-level propertiesthat are implicated in determining processingeffort as measured by reading times. The mostcommon type of evidence adduced to support suchtheories comes from (generalized) linear modeling.We define a predictor function as a functionof type X: RD, i.e., a function thatmaps a contextunit pair to a D-dimensional realvector. We model the reading time measurementsas a linear model f conditioned on X(c, u), i.e.,r(c, u) f ( | X(c, u)) where RD is areal-valued parameter vector.A model whoseexpected value, r(c, u) = X(c, u), achieveshigh likelihood on held-out data lends empiricalsupport to the theory that the factors measured bythe predictors in X underlie the process of reading.",
  "The subscript H suggests that pH is the human LM": "probability exists when the expected string lengthis finite, which is the case for transformer-basedLMs (Du et al., 2023). For simplicity, we furtherassume that pH(u) > 0 for all u . Thisassumption holds true in practice due to thesoftmax function (Boltzmann, 1868; Gibbs, 1902),which enforces the probability estimates to bestrictly positive. Then, for all u ,",
  "Frequency as a Predictor of Reading Time": "Previous studies (e.g., Shain, 2019, 2024) haveinvestigated the effect of frequency, operational-ized as unigram surprisal, on reading time. Aunigram LM qH is a distribution over where,when a string is sampled autoregressively, eachunit is conditionally independent of the context. Innotation, we write qH(u) for the probability of uindependent of context.We now consider the unigram model that bestapproximates the human LM pH in the sense of theforward KullbackLeibler divergence KL(pH || q).We can compute the minimizer qH in closed form.We define the following function that counts thenumber of occurrences of a unit u in u:",
  "H(c, u)def= log pH(u | c).(8)": "Indeed, this claim has received much empirical sup-port (Hale, 2001; Demberg and Keller, 2008; Smithand Levy, 2008, inter alia). Importantly, there isevidence that the particular functional relationship,called the linking function, between contextualsurprisal and reading time is affine5 (Smith andLevy, 2013; Wilcox et al., 2023; Shain et al., 2024),justifying the use of linear regression modeling.",
  "PMI as a Predictor of Reading Time": "Next, we point out an alternative way of derivinga contextual predictor from an LM, namely, as thepointwise mutual information (PMI; Fano, 1961)between a unit and its context. PMI measuresassociation, and has been an important notion inNLP (Church and Hanks, 1990; Levy and Gold-berg, 2014) and, more recently, psycholinguistics(Tsipidi et al., 2024; Wilcox et al., 2024). The PMIbetween a unit u and its context c is",
  "H(c)qH(u).(9)": "The probability that c and u occur together is ex-pressed in the numerator (rewritten using Eqs. (3)and (4)). The denominator expresses what thisprobability would be if c and u were independent.If PMI is predictive of reading times, then thatwould suggest a theory positing that the strengthof association that the observed unit has with itscontext is part of what determines the effort it takesto process it. It turns out that many of the empirical 4Additional common controls include unit length (as mea-sured, e.g., by its orthographic representation), as well aslength and frequency of previous units. The latter are includedto account for spillover effects, where reading-time slowdownstriggered by a particular unit appear after a time delay.5Previous work often refer to this affine function as linear. results that have been published in support of sur-prisal theory, actually, by courtesy of the assumedaffine linking function, provide an equal amountof evidence for a PMI-based theory. To see this,first note that we can rewrite PMI as the differencebetween frequency and surprisal:",
  "r(c, u) = 0 + (H + H)H(u) HH(c, u).(12)": "(We suppressed an intermediate step, given inApp. B.1.) Thus, it turns out that the very same co-efficient that is typically taken to indicate the effectof surprisal also has an alternative interpretationas the negative effect of PMI. Furthermore, the pre-dictive power of a linear model with surprisal andfrequency is the same as that of a linear model withPMI and frequency. In other words, if frequencyis provided as a predictor, additionally addingsurprisal as a predictor is no more predictive thanadding PMI ceteris paribus. However, two suchmodels will differ in the coefficient assigned to fre-quency: H in the surprisal and frequency model,versus H + H in the PMI and frequency model.As a consequence, they will also differ in terms ofthe strength of the effect attributed to the predictorthat stands in for context, i.e., surprisal or PMI.",
  "Disentangling the Effect of Context": "As there is a large and established body of workshowing that frequency plays a major role in ex-plaining the effort it takes to process words (see,e.g., Bybee, 2006), we argue that the interest of sur-prisal theory lies in understanding what additionaleffect there is of contextual information beyond fre-quency. The exposition above implies that neithersurprisal nor PMI should receive special status asa measure of the effect of context. Moreover, both",
  "surprisal and PMI are correlated with frequency6": "and all three are collinear.We now present a simple technique to decorre-late frequency from surprisal, resulting in a newpredictor that is engineered to be disentangled fromfrequency. Importantly, our technique attributes theshared effect of frequency and surprisal on readingtime to frequency, and then creates a new predictorwhich represents the effect of surprisal that is notshared with frequency. We argue that this new pre-dictor is more relevant to study than either surprisalor PMI since it represents only the effect of context.Our technical exposition starts with an underly-ing probability space (, P(), H pH).Next, consider the following random variables un-der this probability space: IH encoding the dis-tribution over surprisals H(c, u) of the next unitgiven a context, MH encoding the distribution overPMIs H(c, u) between the next unit and a context,and YH encoding the distribution over frequenciesH(u) of a unit. Note that IH, MH and YH are real-valued random variables and that YH is constantin c. They are elements of a Hilbert space H overR containing all random variables under the aboveprobability space that have finite second moment(Rudin, 1987). The inner product on H is given by",
  "YH, YHYH.(14)": "Projecting in this manner results in an orthogonal-ization in the sense that YH, projYH (IH) = 0,as a consequence of the Hilbert projection theorem(Rudin, 1991, pp. 306-9). See App. C.2 for a proof.If the expected values of at least one of the randomvariables X and Y is 0, which can be achieved bya simple mean-centering transformation, then",
  "Variance Explained by Context": "We now seek an empirical understanding of howthe new orthogonalized surprisal predictor influ-ences the importance attributed to context in read-ing time prediction. In addition to the experimentpresented here, we also compared the predictivepower for nonlinear models across different predic-tors. Those experiments are discussed in App. F. Dataset and Predictors.We use gaze durationtimes from the Multilingual Eye-movement Corpus(MECO; Siegelman et al., 2022), which consistsof word-level eye-tracking measurements from sev-eral languages.To obtain surprisal estimates, we ap-proximate pH with mGPT (Shliazhko et al., 2024),which is a multilingual LM based on GPT-2. Thefrequency estimates are from Speer (2022)8 andPMI is computed through the decomposition givenin Eq. (10b). Orthogonalized surprisal is obtainedby approximating IH, YH and YH, YH usingonly the words that occur in a training corpus.We transform surprisal and frequency to be mean-centered, and use the transformed data to computeunbiased sample variances and covariances. Thosesample estimates are then plugged into Eq. (14).App. B.2 gives the formulaic details of this approx-imation. We also include a word length predic-tor. Word length is correlated with frequency (Zipf,1949), so we orthogonalize it in the same manner assurprisal. App. E gives more details on the datasetand predictor variables. Experimental Setup.We fit three linear mod-els using ordinary least squares, with predictors:(i) surprisal, frequency, and length, (ii) PMI, fre-quency, and length, and (iii) orthogonalized sur-prisal, frequency, and orthogonalized length. (Analternative perspective would be to use orthogo-nalized frequency, which assumes that surprisal is 7The method presented can in principle be applied to anypair of predictor variables that live in H, e.g., taking MHinstead of IH yields an equivalent context predictor. We alsoconsider orthogonalized unit length in our experiments in 4.8See Nikkarinen et al. (2021) for a more nuanced estima-tion technique.",
  "predictorFrequency (Unigram Surprisal)Context (Contextual Surprisal / PMI / Orthogonalized Surprisal)Length / Orthogonalized Length": ": Proportion of total variance explained by the predictors (Kruskal, 1987), across languages and linearregression models. Summing the values represented by the four bars yields the coefficient of determination R2. Notethat the R2 values are the same across the three models, as a consequence of the collinearity discussed in 2.4. Errorbars are 95% confidence intervals across folds of data. We observe only a small proportion of explained variance bycontext when excluding the variance it shares with frequency, as in the orthogonalized surprisal predictor. the more fundamental predictor. We provide suchresults, which are are consistent with the currentconclusions, in App. F.1.) We include spillovereffects from the previous word and fit models overten folds of cross-validation. We then measure therelative importance of predictors by averaging overthe proportion of variance explained by the predic-tors across orderings in which they are added to themodel (Lindeman et al., 1980; Kruskal, 1987), atechnique known as LMG (Grmping, 2007).9 Oneadvantage of this technique, compared to previousmethods (e.g., Delta Log Likelihood; Goodkindand Bicknell, 2018) is that LMG gives a betterabsolute sense of predictive power. Results.The LMG values for the different pre-dictors across these models are visualized in .Comparing the plots on the bottom row to the plotson the top row, we observe that the explained vari-ance for orthogonalized surprisal is much lowerthan for surprisal. These results are consistentacross languages. For most languages, when rank-ing the importance of the predictors, orthogonal- 9The intuition behind LMG is simple: Given a responsevariable y and two predictors x1 and x2, the proportion of thevariance of y explained by x1 can be taken to either depend onthe correlation between x1 and y (if x1 is added to the modelbefore x2), or to depend on the partial correlation between x1and y when controlling for x2 (if x1 is added to the modelafter x2). For a model with p predictors, the LMG averagesthe explained variance over all p! such orderings. ization shifts the third most important predictorfrom context to the previous words frequency. Ourresults suggest that using surprisal therefore overes-timates the importance that context has on readingtime.10 We observe values for PMI that for mostlanguages lie between those for surprisal and or-thogonalized surprisal, indicating that the extentto which that PMI overestimates the context effectis smaller compared to surprisal. We observe thatthe mean R2 values of the models across LMGorderings, i.e., the sums of the bars within eachfacet, range between 0.60.8, indicating that thelinear models capture a fairly large proportion ofthe variance observed during the reading process.",
  "Conclusion": "This article discusses predictors that capture howthe processing effort of a unit is shaped by itscontext. We made the observation that there existalternatives to the widely used surprisal predictor.Surprisal is correlated with non-contextual fre-quency, so we provided a technique to disentanglecontextual and non-contextual information in lan-guage models. In so doing, we found that the effectthat context has on reading times appears to besmall in comparison to non-contextual frequency.",
  "Limitations": "Our approach takes one predictor to remainuntouched (i.e., frequency), and modifies othersto reflect effects that are disassociated from thefirst. As suggested above, it would thus be naturalto ask what happens in the alternative setting,where surprisal remains untouched and frequencyis projected onto the orthogonal complement ofsurprisal. We provide such an analysis in App. F.1.It turns out that even when attributing the sharedeffect of frequency and surprisal on reading timesto surprisalwhich is the case when replac-ing frequency by an orthogonalized frequencypredictorthe variance explained by the frequencypredictor is still higher for most languages in com-parison to the surprisal predictor. This gives furthersupport to our conclusion that context appears toplay a small role in reading time prediction.Furthermore, our presentation of ideas anddiscussion largely ignores effects of word lengths.We find word lengths to explain the most variancein reading times in our surprisal and PMI models.In addition,after residualizing word lengthagainst frequency, we find length to be the secondstrongest predictor, with an explained varianceranging from around 10%21%. One hypothesisis that readers may make multiple saccades withinfirst passes of longer words, and the time it takesto plan and execute these saccades could be theunderlying reason why orthogonalized word lengthremains explanatory even after residualization.Future work could control for this by adding in thenumber of saccades within a word as an additionalpredictor into models.We are unaware of any efficient algorithm tocompute IH, YH and YH, YH exactly, so inpractical settings we must rely on estimation. Thus,it may be that the orthogonalized surprisal pre-dictor is only close to being orthogonal to fre-quency in practice. The manner in which esti-mation is performed makes our technique similarto residualizationsee App. D for a discussion.Moreover, our method only provides guaranteesfor predictor variables that live in H.Importantly, an off-by-one issue was detectedin the trial, sentence and word (interest area) IDscheme for a handful of tokens in MECO (Versions1.1 and 1.2). This was corrected for our analysis,but should be taken into consideration by otherresearchers using this dataset. The code to correctthe data is included in our repository. In addition, we also identified a few instances of repeated wordswithin a sentence (e.g., als als), but given theirconsistency across participants, we assumed thesewere typos. These were retained in the analysis,but could have resulted in atypical responses.In addition, the current paper makes use of afixed-effects linear regression model with aver-aged data, as opposed to the more standard mixed-effects regression. Estimation of R2 values frommixed-effects models can differ depending on theresearcher assumptions and has historically beenunder-reported due to this limitation (Nakagawaand Schielzeth, 2013). Nevertheless, some propos-als have been made regarding best practices (Nak-agawa and Schielzeth, 2013; Rights and Sterba,2018). Future research should investigate the fea-sibility of our approach, particularly with the useof partial effect sizes (i.e., the LMG approach), butusing mixed-effects models.Another limitation of this work is that, while weinvestigate several languages, these are still biasedtowards Indo-European languages. For example,we present results from one language only for Fino-Uralic, Semitic, Turkic, and Koreanic languagefamilies, but seven Indo-European languages. Ex-panding these results to even more languages wouldfurther broaden the impact of this work. In addi-tion, we observe somewhat unique effects for Ko-rean, where, in orthogonalized models, frequencyaccounts for a lower proportion of the variance, andlength and context account for higher proportions,at least compared to other languages. One possi-ble reason for this is the Korean script (Hangul),which combines features of both alphabetic and syl-labic writing systems. Future work should conductsimilar analyses on different Korean datasets to de-termine whether this trend is a property of Korean,or just our particular Korean language dataset.",
  "Josiah Willard Gibbs. 1902. Elementary Principles inStatistical Mechanics. Charles Scribners Sons": "Mario Giulianelli, Andreas Opedal, and Ryan Cotterell.2024. Generalized measures of anticipation and re-sponsivity in online language processing. In Find-ings of the Association for Computational Linguistics:EMNLP 2024, Miami, Florida, USA. Association forComputational Linguistics. Adam Goodkind and Klinton Bicknell. 2018. Predictivepower of word surprisal for reading times is a linearfunction of language model quality. In Proceedingsof the 8th Workshop on Cognitive Modeling and Com-putational Linguistics (CMCL 2018), pages 1018,Salt Lake City, Utah. Association for ComputationalLinguistics.",
  "Victor Kuperman, Raymond Bertram, and R. HaraldBaayen. 2008.Morphological dynamics in com-pound processing.Language and Cognitive Pro-cesses, 23(7-8):10891132": "Tatsuki Kuribayashi, Yohei Oseki, Ana Brassard, andKentaro Inui. 2022. Context limitations make neurallanguage models more human-like. In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1042110436,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, RyoYoshida, Masayuki Asahara, and Kentaro Inui. 2021.Lower perplexity is not always human-like. In Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics and the 11th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 52035217,Online. Association for Computational Linguistics.",
  "Byung-DohOhandWilliamSchuler.2023a": "Transformer-basedlanguagemodelsurprisalpredicts human reading times best with abouttwo billion training tokens.In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 19151921, Singapore. Association forComputational Linguistics. Byung-Doh Oh and William Schuler. 2023b.Whydoes surprisal from larger transformer-based lan-guage models provide a poorer fit to human readingtimes? Transactions of the Association for Computa-tional Linguistics, 11:336350.",
  "Byung-Doh Oh, Shisen Yue, and William Schuler. 2024": "Frequency explains the inverse correlation of largelanguage models size, training data amount, andsurprisals fit to reading times. In Proceedings ofthe 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 26442663, St. Julians, Malta.Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167.",
  "Claude E. Shannon. 1948. A mathematical theory ofcommunication. The Bell System Technical Journal,27(3):379423": "Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova,Anastasia Kozlova, Vladislav Mikhailov, and TatianaShavrina. 2024. mGPT: Few-Shot Learners Go Mul-tilingual. Transactions of the Association for Compu-tational Linguistics, 12:5879. Noam Siegelman, Sascha Schroeder, Cengiz Acartrk,Hee-Don Ahn, Svetlana Alexeeva, Simona Amenta,Raymond Bertram, Rolando Bonandrini, Marc Brys-baert, Daria Chernova, et al. 2022. Expanding hori-zons of cross-linguistic research on reading: The mul-tilingual eye-movement corpus (MECO). BehaviorResearch Methods, 54(6):28432863.",
  "Robyn Speer. 2022. rspeer/wordfreq: v3.0": "Eleftheria Tsipidi, Franz Nowak, Ryan Cotterell,Ethan Gotlieb Wilcox, Mario Giulianelli, and AlexWarstadt. 2024. Surprise! Uniform information den-sity isnt the whole story: Predicting surprisal con-tours in long-form discourse. In Proceedings of the2024 Conference on Empirical Methods in NaturalLanguage Processing, Miami, Florida, USA. Associ-ation for Computational Linguistics. Ethan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, PengQian, and Roger P. Levy. 2020. On the predictivepower of neural language models for human real-time comprehension behavior. In Proceedings ofthe 42nd Annual Meeting of the Cognitive ScienceSociety, page 17071713.",
  "upH(cu).(18d)": "The last step follows from the fact that a string u can be segmented into two substrings c and u in 1 + |u|ways. Note the two cases where either c = and u = u, or c = u and u = (with denoting theempty string). It is then easy to see that ZH is a valid normalizing constant:",
  "B.1Rewriting Linear Models with Surprisal in Terms of PMI": "In this section, we demonstrate the equivalence between two linear models, one with surprisal andfrequency as predictors, and the other with PMI and frequency as predictors. In particular, we considera linear model of reading times f ( | X(c, u)), in which X(c, u): RD includes surprisal,frequency and additional baseline predictors Xb(c, u): RD3 and show that it is equivalent toa model in which surprisal is replaced with PMI. Consider the prediction r(c, u) which is the expectedvalue of f ( | X(c, u)). To demonstrate the equivalence, we simply add and subtract an additionalfrequency term:",
  "n=1(H(cn, un) H)(H(un) H),(22)": "where H and H are the sample means computed over C for surprisal and frequency, respectively. Inwords, this means that we only use the wordcontext pairs present in the training data for approximatingthe inner products in Eq. (14). Recall that H is infinite-dimensional, and we are not aware of efficientalgorithms for exact computations of inner products on H.",
  "C.1Predictor Variables as Elements of a Hilbert Space": "Let ( , P( ), H pH) be a probability space. We further require that H(c)pH(u | c) > 0for all c and u , which is equivalent to the assumption that pH(u) > 0 for all u byEqs. (3) and (4). We construct a Hilbert space H over R of all random variables of type X: Rwith the restriction that E[X2] < . We require the second moment to be finite since / R. In thispaper, we are particularly interested in the random variables IH(c, u) = H(c, u), MH(c, u) = H(c, u),YH(c, u) = H(u), for c and u . They encode the distributions over surprisal, frequency andPMI, respectively, and are all distributed according to H pH. We have",
  "DOur Method in Relation to Residualization": "The technique presented in 3 closely resembles another method used to decorrelate predictorsresidualization (see, e.g., Kuperman et al., 2008; Jaeger, 2010; Garca et al., 2019). Consider a linearregression setting with response y RN and design matrix X RN2, i.e., N data points for twopredictors x1, x2 RN, being column vectors. The idea behind residualization is to decorrelate thepredictors by replacing one of them, say x1, by the residuals obtained from the ordinary least squaressolution of the regression model in which x1 is the response and x2 is the (only) predictor. The newpredictor will thus take the value",
  "x2x2x2.(34)": "In Eq. (14) we had the inner product between two vectors in the Hilbert space H, but here we have theinner product between two vectors in Euclidean space (x1 and x2), which is the dot product. Indeed,residualization is defined only over a sample of data points. Our technique can thus be viewed as afunctional generalization of residualization. From a statistical perspective, Eq. (34) provides estimatesof residual values which can, ideally, be generalized to data beyond the sample of N data points fromwhich they were estimated. In a Hilbert space, on the other hand, there is no question of generalization.For the predictors we use, which are derived from a language model, we know their true values. We onlyapproximate the inner products in Eq. (14) since computing their exact value would be intractable. Thatis, the reason for our approximation, which yields the same formula as residualization, is computational,rather than statistical. Arguments Against Residualization.Residualization has received criticism as a way to obtain moreinterpretable model coefficients (York, 2012; Wurm and Fisicaro, 2014). Consider the following threelinear regression models estimated by ordinary least squares:",
  "Model C:y = C0 + C2 x2 + ,(37)": "where RN is a vector of Gaussian noise variables. In the models above, we have that A1 = B1and A2 = B2 = C2 (Wurm and Fisicaro, 2014). That is, the effect of the residualized predictorx1 inthe example aboveremains the same after residualization (Model A vs. Model B). On the other hand,the estimated effect of the residualizing predictor on the responsex2 in the example abovechangesbetween Model B which regresses y on x1-res and x2 and Model A which regresses y on x1 and x2. Theestimated effect of x2 in Model B instead becomes equal to what it would have been under a model witha single predictor, regressing only on x2 (Model C). These outcomes are contrary to what one mightexpect: The effect of the modified, residualized predictor x1 is the same while the effect of the untouchedpredictor x2 changes. This has indeed been a source of confusion in the literature (Wurm and Fisicaro,2014). However, in our case, the fact that A2 = B2 = C2 is actually the desired consequence: Wesought to estimate the effect of context that is not correlated with frequency. In other words, we wantthe covariance between frequency and surprisal to be attributed to frequency, as it would be in a modelthat only regresses reading time on frequency (corresponding to Model C). Moreover, we argue that theestimated coefficient is the wrong quantity to look at when measuring importanceit is the role of thepredictor in relation to the others that should matter. While A1 = B1 does not indicate a difference inimportance, a measure of explained variance like LMG does, as we demonstrate by our results in . We thus advocate for the use of such a measure instead of analyzing coefficients. Another remark relatesto whether a residualized variable is interpretable as anything at all. For instance, Breaugh (2006) givesan example of height and weight of basketball players: Residualizing height with respect to weight wouldresult in a residualized predictor that would involve a notion of height disentangled from weight, which istricky to conceptualize in a real-world context. However, in our case, we are addressing the question ofwhat predictor should be extracted in the first place, i.e., from a language model as a stand-in for context.Our paper argued that orthogonalized surprisal gives a better interpretation for the effect of context thancontextual surprisal does.We hope that our work and this discussion can help shed further light on when residualization is, and isnot, suitable.",
  "E.1Dataset": "We use the Multilingual Eye-movement Corpus (MECO; Siegelman et al., 2022), which consists ofeye-tracking-based reading-time data across 13 different languages for 12 Wikipedia-style articles aboutvarious topics. The articles have been carefully constructed to contain the same content across languages.Word-level reading time is recorded for between 3254 participants per language, using several differentreading variables. Of these, for our main experiments, we use gaze duration, which is the total fixationtime on a word during its first pass (i.e., before the first time the gaze leaves the word). However, belowwe also show results for two other reading metrics: first fixation duration, which is the duration ofthe first fixation that lands on the word, and total fixation duration, which is the sum over all fixationdurations of a word. In our experiments, we give a reading time of zero to words that were skipped onthe first pass. We take the average over the by-subject reading times to obtain the response variables wemodel.",
  "E.2Predictors": "Our predictor variables are estimated in the following way: Surprisal estimates are obtained from mGPT(Shliazhko et al., 2024), which is a multilingual variant of GPT-3 that was trained on Wikipedia and theC4 corpus (Raffel et al., 2020). It supports 61 languages, which intersected with the MECO dataset yields11 languages for our experiments: Dutch, English, Finnish, German, Greek, Hebrew, Italian, Korean,Russian, Spanish, and Turkish. Estonian and Norwegian, which are present in MECO, are unfortunatelynot supported by mGPT. For each word in MECO, we sum the surprisals estimated by mGPT for each ofthe tokens that make up that word. We use the estimates from Speer (2022) to obtain word frequency (i.e.,unigram surprisal) and length, following previous work. Finally, PMI estimates are obtained from surprisaland frequency through Eq. (10b). All predictors are standardized (i.e., set to mean zero and standarddeviation one) before computing orthogonalized surprisal values and fitting the regression models.",
  "FAdditional Experiments": "We complement the main experiments presented in 4 with three additional empirical analyses. InApp. F.1, we take an alternative position to the one in the main text, using an orthogonalized frequencypredictor. In App. F.2, we exclude the length predictor and perform the same analysis as we did in the maintext. In App. F.3, we compare the predictive power of nonlinear models across different sets of predictors.",
  "F.1Orthogonalized Frequency": "Here, we provide an additional analysis in which we derive a new frequency predictor, swapping the twovariables in Eq. (14). In this case, the shared effect of frequency and surprisal on reading time is attributedto surprisal, and the frequency effect represents the effect beyond what is already explained by surprisal.We present the results in . Comparing this analysis to may be considered analogous to Shains(2019) study, which compares the independent effects of surprisal and frequency by adding them in aspredictors on top of baseline models that contain the other. However, in contrast to Shain (2019), we findthat frequency appears to be more important than contextual effects in explaining reading times.",
  "predictorFrequency (Frequency / Orthogonalized Frequency)Context (Surprisal)": ": This figure is analogous to , with the difference that frequency is projected onto the orthogonalcomplement of surprisal, resulting in an orthogonalized frequency predictor. Even when the shared effect offrequency and surprisal is attributed to surprisal, we still find that the frequency predictor explains more variancethan surprisal for most languages.",
  "F.2Results without Length": "We complement the experiments in 4 with an additional analysis which excludes length. We follow thesame experimental setup as described in the main portion of the text, this time with three linear models thatinclude predictors: (i) surprisal and frequency, (ii) PMI and frequency, and (iii) orthogonalized surprisaland frequency. We include spillover variables from the previous word. The results are shown in .We observe that the implications on context found in do not change when excluding length.",
  "F.3Psychometric Predictive Power": "While the surprisal and PMI models are equivalent under a linear model, that relationship does notnecessarily need to hold under a nonlinear one. Therefore, we compare the psychometric predictivepower by fitting generalized additive models (GAMs). GAMs are a class of additive models that canlearn non-linear relationships between predictor and response variables. All the terms in our GAMs arespline-based smooth terms, that include either a contextual predictor variable (i.e., surprisal, PMI, ororthogonalized surprisal), or frequency. We restrict our smooth terms to six basis functions, following thelogic outlined in (Hoover et al., 2023). GAMs are fit using the mgcv package in R. Two example calls aregiven below:",
  "'cr', k = 6) + s(prev_frequency, bs = 'cr', k = 6), data = .)": "We consider an additional baseline model which is the average reading time estimated from the trainingset. We compare delta log-likelihood llhthe average difference in likelihood between the target modelsmentioned above and the baseline modelas estimated over ten folds of cross-validation across severaldifferent sets of predictor variables.Results are visualized in . We observe three big trends: First, we find that all predictors lead topositive llh, indicating that they are useful for predicting reading times. However, second, when lookingat models with just one predictor variable, we observe that frequency alone leads to higher llhthan any",
  ": This figure is analogous to , except that it shows results when excluding the word length predictor": "other single variable, and that surprisal and PMI tend to result in higher llhthan orthogonalized surprisal.This is to be expected. We know from prior research that frequency plays a large role in explainingby-word processing effort, and because orthogonalized variables, by definition, are decorrelated withfrequency, they are not expected to be strong predictors of reading times, alone.The right three facets of show models that combine contextual and non-contextual predictorsinto a single model. Here, we observe only insignificant, nearly invisible differences between the modelsllh. We conclude that all three implementations of context are equally good at predicting reading times.In , we show our generalized additive modeling results, broken down by language, across thex-facets. We also show results for reading time metrics other than gaze duration, including first fixationduration (top row) and total reading times (bottom row). These results are consistent with those reportedin . We find that of the four individual predictors, frequency leads to the highest llh, followedby surprisal, PMI, and then orthogonalized variants. When combining our non-contextual predictor(frequency), alongside these contextual predictors, we do not observe differences in llh.",
  "GConnection with Model Size": "The results presented in this article may help explain a trend recently observed in the computationalpsycholinguistics literature: Surprisal values of larger LMs provide a worse fit to human reading-timedata compared to those of medium-sized models (Oh and Schuler, 2023a,b). Specifically, Oh et al. (2024)suggest that this is because larger models are incredibly accurate at predicting rare words in context.Medium-sized models, on the other hand, are not as good at predicting rare words in context. Therefore,surprisal estimates for these words are closer to their unigram frequencies, i.e., non-contextual surprisal.If reading times are primarily driven by frequency effects, as suggested by our analysis, the surprisalpredictor shouldon its ownyield stronger predictive power if it is closer to frequency, as is the casefor medium-sized models. Thus, this could explain why the decoupling of surprisal and frequency in theselarger models results in poorer fits to human reading times.Similarly, this could be a reason for why surprisal estimates derived from lossy contexts have beenshown to be more predictive of reading times (Futrell et al., 2020; Kuribayashi et al., 2022): Restrictingthe context might make the surprisal estimates more similar to unigram frequencies. du duduen enenfi fi fi gege ge grgrgr he hehe it it it kokoko sp spsp tr trtr ru ru ru dududuenenen fififigegege grgrgrhehehe itititkokoko spspsp trtrtr rururudududuenenen fififigegege grgrgr hehehe ititit kokoko spspsp trtrtr rururu dududuenenen fififigegege grgrgr hehehe itititkokoko spspsp ru trtrtrruru du duduen enenfifi fi gege ge grgrgr he hehe it itit kokoko sp spsp tr trtr ru ru ru du duduen enenfi fi fi gege ge grgrgr he hehe it itit kokoko sp spsp tr trtr ru ru ru du duduen enenfi fi fi gege ge grgrgr he hehe it itit kokoko sp spsp tr trtr ru ru ru"
}