{
  "Abstract": "Natural language explanations represent aproxy for evaluating explanation-based andmulti-step Natural Language Inference (NLI)models. However, assessing the validity of ex-planations for NLI is challenging as it typi-cally involves the crowd-sourcing of appositedatasets, a process that is time-consuming andprone to logical errors. To address existing lim-itations, this paper investigates the verificationand refinement of natural language explana-tions through the integration of Large LanguageModels (LLMs) and Theorem Provers (TPs).Specifically, we present a neuro-symbolicframework, named Explanation-Refiner, thatintegrates TPs with LLMs to generate and for-malise explanatory sentences and suggest po-tential inference strategies for NLI. In turn,the TP is employed to provide formal guaran-tees on the logical validity of the explanationsand to generate feedback for subsequent im-provements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explana-tory reasoning, autoformalisation, and error cor-rection mechanisms of state-of-the-art LLMsas well as to automatically enhance the qual-ity of explanations of variable complexity indifferent domains.1",
  "Introduction": "A recent line of research in Natural Language Infer-ence (NLI) focuses on developing models capableof generating natural language explanations in sup-port of their predictions (Thayaparan et al., 2021;Chen et al., 2021; Valentino et al., 2022a; Bostromet al., 2022; Weir et al., 2023). Since natural lan-guage explanations can be used as a proxy to evalu-ate the underlying reasoning process of NLI models(Kumar and Talukdar, 2020; Zhao and Vydiswaran,2021; Chen et al., 2021), researchers have proposed",
  "Code and data are available at:": "different methods for assessing their intrinsic qual-ity (Camburu et al., 2020; Wiegreffe and Maraso-vic, 2021; Valentino et al., 2021; Atanasova et al.,2023; Quan et al., 2024; Dalal et al., 2024), includ-ing the adoption of language generation metricsfor a direct comparison between models generatedexplanations and human-annotated explanations.However, this process is subject to differenttypes of limitations. First, the use of languagegeneration metrics requires the crowd-sourcingof explanation corpora to augment existing NLIdatasets (Wiegreffe and Marasovic, 2021), a pro-cess that is time-consuming and susceptible to er-rors (Valentino et al., 2021; Liu et al., 2022; Zhaoet al., 2023). Second, language generation metricshave been shown to fail capturing fine-grained prop-erties that are fundamental for NLI such as logicalreasoning, faithfulness, and robustness (Camburuet al., 2020; Chan et al., 2022; Atanasova et al.,2023; Quan et al., 2024). Third, human explana-tions in NLI datasets tend to be incomplete andcontain logical errors that could heavily bias theevaluation (Elazar et al., 2021; Valentino et al.,2021).In this paper, we investigate the integration ofstate-of-the-art LLM-based explanation generationmodels for NLI with external logical solvers tojointly evaluate explanatory reasoning (Pan et al.,2023a; Olausson et al., 2023; Jiang et al., 2024b)and enhance the quality of crowd-sourced explana-tions. In particular, we present a neuro-symbolicframework, named Explanation-Refiner, that in-tegrates a Theorem Prover (TP) with Large Lan-guage Models (LLMs) to investigate the followingresearch questions: RQ1: Can the integration ofLLMs and TPs provide a mechanism for automaticverification and refinement of natural language ex-planations?; RQ2: Can the integration of LLMsand TPs improve the logical validity of human-annotated explanations?; RQ3: To what extentare state-of-the-art LLMs capable of explanatory reasoning, autoformalisation, and error correctionfor NLI in different domains?. To answer thesequestions, Explanation-Refiner employs LLMs togenerate and formalise explanatory sentences andto suggest potential inference strategies for build-ing non-redundant, complete, and logically validexplanations for NLI. In turn, the TP is adopted toverify the validity of the explanations through theconstruction of deductive proofs and the generationof fine-grained feedback for LLMs.We instantiate Explanation-Refiner with state-of-the-art LLMs (i.e., GPT-4 (OpenAI, 2023), GPT-3.5 (Brown et al., 2020), LLama (Touvron et al.,2023), and Mistral (Jiang et al., 2024a)) and theIsabelle/HOL proof assistant (Nipkow et al., 2002)utilising Neo-Davidsonian event semantics (Par-sons, 1990) coupled with First-Order Logic (FOL)to effectively and systematically translate naturallanguage sentences into logical forms.Our empirical analysis, carried out on three NLIdatasets of variable complexity (i.e., e-SNLI (Cam-buru et al., 2018), QASC (Khot et al., 2019), andWorldTree (Jansen et al., 2018)), reveals that ex-ternal feedback from TPs is effective in improv-ing the quality of natural language explanations,leading to an increase in logical validity usingGPT-4 from 36% to 84%, 12% to 55%, and 2%to 37% (on e-SNLI, QASC, and WorldTree respec-tively). At the same time, the results demonstratethat integrating external TPs with LLMs can re-duce errors in autoformalisation, with an averagereduction of syntax errors of 68.67%, 62.31%, and55.17%. Finally, we found notable differences inperformance across LLMs and NLI datasets, withclosed-sourced LLMs (i.e., GPT-4 and GPT-3.5)significantly outperforming open-source models(i.e., Mistral and LLama) on both explanatory rea-soning and autoformalisation, along with a sharedtendency of LLMs to struggle with increasing ex-planation complexity.To summarise, the main contributions of thispaper are: 1. We introduce Explanation-Refiner, a novelneuro-symbolic framework that integratesLLMs with an external theorem prover. Thisframework automatically verifies and refinesexplanatory sentences in NLI tasks using anobjective external feedback.",
  ". We integrate Neo-Davidsonian event seman-tics coupled with FOL to effectively translatenatural language sentences into logical forms": "to minimise semantic information loss. Ad-ditionally, we introduce a novel method thatleverages a theorem prover and a proof as-sistant for verifying NLI explanations and asyntactic refiner to minimise syntax errors inresponses generated by LLMs. 3. We conduct a comprehensive series of exper-iments with Explanation-Refiner across fiveLLMs and three datasets, including 1 to 16explanatory sentences, covering tasks fromtextual entailment to complex multiple-choicequestion answering in various domains. 4. We perform extensive analyses to explore theexplanation refinement process, characteris-ing the LLMs inference capabilities and re-vealing the strengths and limitations of differ-ent models in producing verifiable, explain-able logical reasoning for NLI.",
  "Explanation Verification andRefinement": "Explanation-based NLI is widely adopted to eval-uate the reasoning process of multi-step inferencemodels via the construction of natural languageexplanations. In this work, we refer to the fol-lowing formalisation for Explanation-based NLI:given a premise sentence pi, a hypothesis sentencehi, and an explanation Ei consisting of a set offacts {f1, f2, ..., fn}, the explanation Ei is logi-cally valid if and only if the entailment piEi |= hiholds. This entailment is considered verifiable if{pi, Ei, hi} can be translated into a set of logicalformulae that compose a theory . The validityof the theory is subsequently determined by atheorem prover, verifying whether , where represents a logical consequence derived fromthe logical form of hi.In this paper, we aim to automatically verify thelogical validity of an explanation Ei. To this end, if is rejected by the theorem prover, a furtherrefinement stage should be initiated to refine thefacts {f1, f2, ..., fn} based on external feedback,resulting in an updated explanation Ei. Thus, anexplanation is accepted if all the facts are logicallyconsistent, complementary and non-redundant tosupport the derivation.",
  "Rough Inference": "1. To infer the hypothesis, we need to find the information of a man and the action of public speaking. .... 5. By combining these steps, we can infer the hypothesis by satisfying the information of man (from premise) and public speaking (from premise and explanation 1). There are no redundant or not directly related explanation sentences. The proof steps use explanation 1 and the premise sentence.",
  "Iteration": "Construct the theory with formalised axioms, theorems and proof methods IsabelleTheory : The overall pipeline of Explanation-Refiner: An NLI problem is converted into axioms and theorems for atheorem prover, along with some proof steps derived from a preliminary inference. In case the proof fails (logicallyinvalid), the erroneous steps along with the constructed proof strategy are used as feedback to refine the explanationin a new iteration. present a neuro-symbolic framework that iterativelychecks and refines the explanation Ei based on ex-ternal feedback. shows an overview of ourproposed framework. Given an NLI task, to evalu-ate the logical validity of the entailment, the LLMis prompted to perform an autoformalisation pro-cess that transforms natural language sentences intoformal language represented in the form of an Is-abelle/HOL theory. Each fact f Ei is convertedinto an axiom ai, where each ai is an element ofthe set A = {a1, a2, ..., an}. The premise pi andcorresponding hypothesis hi, is converted into atheorem for proving pi B hi, where B A.A syntax refinement mechanism is subsequently ap-plied to the previously transferred symbolic forms.The theorem prover is implemented as a checker toidentify any syntax errors and provide these errordetails as feedback to an LLM, enabling the LLMto iteratively correct the syntax errors over a fixednumber of iterations, denoted by t. We can then perform automated reasoning viathe theorem prover. To this end, in step 3 we usethe LLM to generate a rough inference that states apreliminary proof strategy in natural language andelicit the facts f Ei which are sufficient and nec-essary for entailing the hypothesis hi. Based on this preliminary proof strategy, the LLM is prompted toconstruct and formalise the proof steps for provingthe theorem. In step 5, the theorem prover will ver-ify the constructed theory by attempting to provethe theorem. If it is solvable, we consider it a logi-cally valid explanation. If the prover failed at oneof the proof steps, we adopt the failed steps alongwith the applied axioms B A as an external feed-back for the LLM. This feedback is used to refinethe logical errors and consequently refine the factsf Ei.",
  "In order to formally verify the logical validity of theexplanations, we adopted Neo-Davidsonian event-based semantics and FOL": "Neo-Davidsonian Event SemanticsPreventingthe loss of semantic information during the repre-sentation of natural language sentences in logicalforms, such as FOL, poses significant challengeswhen using LLMs, particularly with long and com-plex sentences that are crucial for logical reasoning(Olausson et al., 2023). Neo-Davidsonian event se-mantics (Parsons, 1990) focused on event variablesto represent the verb predicates and their corre-sponding object arguments as semantic roles. This",
  "(1)": "In 1, the verbs are represented as the events eatingand hunting, where the agent and patient argu-ments correspond to the entities performing andreceiving the actions within these events, respec-tively. The logical form example(e1, e2) explicitlycaptures the semantic meaning of this sentence: theevent of a wolf eating a sheep as an exemplar of apredator hunting prey. Similarly, whenever thereare no action verbs involved in a sentence, we useFOL to represent the static or descriptive aspects.For instance:",
  "explanation_1: \"x. Violin x Instrument x\"": "In addition, as illustrated in , both thepremise and the hypothesis constitute parts of thetheorem to be proven. In particular, the assumesasm clause includes unquantified, specific propo-sitions or conjunctions of propositions which arerecognised as known truths (i.e., premises). On theother hand, the show clause denotes the conclu-sion (i.e., hypothesis) for which we seek to builda proof through logical deductions based on theassumed propositions and axioms. Syntax Error RefinerRecent studies (Olaussonet al., 2023; Gou et al., 2024) have revealed per-sistent syntax errors when prompting LLMs forcode and symbolic form generation tasks. We cat-egorised the syntax errors into two distinct sub-domains based on feedback from Isabelle: typeunification errors and other syntax errors. Typeunification errors primarily arise from mismatchesbetween declared and actual argument types in log-ical clauses. Other syntax errors typically involvemissing brackets, undefined entity names, or in-valid logical symbols. Our process involves usingIsabelle to identify syntax errors in the transferredtheory, extracting these error messages, and thenprompting the LLM with these messages alongwith few-shot examples. This guides the modelon how to correct each type of syntax error over aseries of iterations, allowing for continuous verifi-cation and refinement. Details of the autoformali-sation prompts are described in Appendix A.4.1.",
  "A proof provides a detailed step-by-step strategythat elucidates the logical connections and unifica-": "tion among axioms to support the reasoning processaimed at achieving the solvers goal. Initially, weprompt the LLM to create a preliminary proof innatural language to assess how it infers the hypoth-esis and to identify which explanatory sentencesare relevant, redundant, or unrelated. Based onthis initial inference, we then guide the LLM to de-velop a formal proof () that integrates withIsabelle/HOL to verify the explanatory sentences(axioms) that are required to derive the hypothe-sis. The general proof steps generated by an LLMare in the format show X using Y by Z, wherethe theorem prover is asked to prove X given theassumptions Y , using the automated proof tacticZ. The proof tactic often applied is blast, whichis one of broader Isabelles FOL theorem provingtactics(Paulson, 1999). Additional details of theproof construction process and the prompts used toguide the LLMs are described in Appendix A.4.2.",
  "Verification and Refinement": "Finally, the constructed theory, which includes ax-ioms, theorems, and proof steps, is submitted tothe theorem prover for verification. If the theory isvalidated, it outputs a logically valid explanation.If the proof fails or timeouts, we extract the firsterror from the solvers error message, identify thecorresponding proof step, and locate the related ex-planatory sentences (axioms) from the theory. Webegin by removing redundant and irrelevant factsthat are not present in the preceding Isabelle/HOLproof steps or are declared as such in the text infer-ence strategy. Then, we prompt the LLM to refinethe explanatory sentences by providing it with theerror message, the failed proof step, the associatedproof strategy, and the relevant explanatory sen-tences for further iteration. This process is iterativeand progressive; with each iteration, the frameworkaddresses one or more logical errors, continually re-fining the explanatory sentences to ultimately yielda logically valid and verifiable explanation. Addi-tional details on the prompts used for refinementare described in Appendix A.4.3.",
  "Datasets": "We adopted three different NLI datasets for evalua-tion: e-SNLI, QASC, and WorldTree, using a totalof 300 samples selected via the sampling strategydefined in Valentino et al. (2021), which maximisesrepresentativeness and mutual exclusivity across syntactic and semantic features expressed in thedatasets. For multiple-choice question answering,the task includes a question q accompanied by a setof candidate answers C = {c1, c2, ..., cn}, with ciidentified as the correct answer. To cast this prob-lem into NLI, we simply convert q and the correctanswer ci into a hypothesis hi. On the other hand,the questions context, if present, is used to buildthe premise pi.",
  "Models": "To integrate Isabelle/HOL as a real-time verifica-tion tool with LLMs, we employ a Python client(Shminke, 2022) which communicates with Is-abelle/HOL as a server backend. This enables thecommunication of the constructed theory files andthe extraction of the response messages from Is-abelle. We conducted experiments using five LLMswithin the proposed framework. The models in-clude two open-sourced models: Llama2-70b (Tou-vron et al., 2023) and Mixtral-8x7b (Jiang et al.,2024a), as well as Mistral-small (mistral-small-latest) (Mistral AI, 2024), GPT-3.5 (gpt-3.5-turbo)(Brown et al., 2020), and GPT-4 (gpt-4-0613) (Ope-nAI, 2023).",
  "Results": "Detailed feedback from an external theoremprover effectively guides LLMs in verifying andrefining explanations for NLI.To assess theeffectiveness of employing an external theoremprover to verify and refine explanations in NLItasks, we conducted a comparative analysis acrossvarious LLMs (). The initially valid ex-planations represent the percentage of explanationsthat can be verified as logically valid without anyfurther iteration. Although the initial verificationresults varied among different models, all LLMsdemonstrated a consistent improvement in refiningthe logical validity of the explanations. This pro-cess highlights the positive impact of the externalfeedback but also shows significant differences be-tween models. We found that lower rates of initialvalid explanations often resulted from syntactic er-rors, which impeded the theorem provers abilityto generate proofs. Despite this initial variability,all models demonstrate a consistent improvementin the refinement process across the datasets. No-tably, GPT-4 outperformed other models, improv-ing the validity of explanations by 48%, 43%, and35% across the three datasets, respectively, withina maximum number of ten iterations (). Llama2-70b Mixtral-8x7bMistral-smallGPT-3.5GPT-4 Number of Valid Explanations 0.0 16.0 22.0 19.0 36.0 7.0 32.0 36.0 55.0 84.0 Number of Iterations 6.0 3.28 1.58 2.93 1.96 e-SNLI Initially Valid ExplanationsFinally Valid ExplanationsNumber of Iterations",
  ": Number of successfully refined explanations at each iteration step": "shows the number of explanations refinedat each iteration across the e-SNLI, QASC, andWorldTree datasets. On average, we found that anincreasing number of iterations leads to increasingrefinement, with models requiring an average offive iterations across the datasets. Explanation length/complexity impacts formal-isation and verification.The e-SNLI dataset,which includes only a single explanatory sentenceper example, shows the best overall performance.In contrast, the multiple-choice question answeringdatasets, QASC and WorldTree, exhibit compara-tively lower performance. QASC typically contains2 explanatory sentences, while WorldTree rangesfrom 1 to 16 sentences. As the number of explana-tory sentences increases, so does the complexity ofthe logical reasoning required. Models show lowerrefinement performance in WorldTree when com-pared to e-SNLI and QASC, with only 3%, 5%, and5% of Llama-70b, Mixtral-8x7b, and Mistral-smallexplanations being refined in WorldTree. Mean-while, 29% and 35% of explanations are refinedby GPT-3.5 and GPT-4 in WorldTree, respectively.This process involves synthesising multiple ex-planatory sentences to fulfill sub-goals, which mustthen be integrated to meet the overall hypothesisgoal. Iterative and categorical refinement can mono-tonically reduce syntactic errors in autoformal-isation.To evaluate the syntax error refinementstage, we quantified the presence of syntax errorsin the Isabelle theories both before and after theiterative refinement process. After a maximumof three iterations, all models showed significantreductions, with maximum reductions of 68.67%,62.31%, and 55.17% from 7.82 to 2.45, 20.27 to7.64, and 22.91 to 10.27 across the three respec-tive datasets (see ). While models likeLlama2-70b and Mixtral-8x7b still exhibit somesyntax errors in the refined theories code, this isprimarily due to their inability to perform complexautoformalisation, especially for multiple and morecomplex explanatory sentences such as those in theWorldTree dataset. This result is consistent withthe percentage of explanations that were success-fully refined across the models, which suggests thatthe autoformalisation process plays a critical rolein the models logical reasoning capability.",
  "Ablation Study": "We conducted an ablation study to further evaluateand disentangle the impact of autoformalisation onperformance. To this end, we adopted GPT-4 exclu-sively for the autoformalisation component, whileretaining the original models for explanation refine-ment and proof strategy generation. As shown in Llama2-70bMixtral-8x7b Mistral-smallGPT-3.5GPT-40 Avg. Number of Theories Contain Syntax Errors 75.18 58.55 47.0 33.27 7.82 64.55 31.82 23.0 17.64 2.45 e-SNLI",
  "(c)": ": AF represents the autoformalisation components, and TI represents the textual inference components.TI+AF (Base Model) indicates the use of the base model for both the autoformalisation and textual inferencecomponents. TI+AF (GPT-4) indicates the use of GPT-4 for the autoformalisation components, while the basemodel is used for textual inference. , integrating GPT-4 for autoformalisationled to a significant increase in the number of expla-nations successfully refined across all models. Forinstance, Llama2-70b with GPT-4 as the formali-sation component refined explanations from 7% to65% in the e-SNLI dataset. For the multiple-choicequestion answering dataset, GPT-3.5 showed a rela-tively smaller increase from 44% to 48% and from29% to 34%. Despite these improvements, a perfor-mance gap persists between GPT-4 and the othermodels, which is attributed to GPT-4s superiorsymbolic reasoning capabilities required for expla-nation refinement from the identified logical errors. Explanations are progressively made more com-plete and consistent through iterative refine-ment.In order to deliver step-wise logical con-sistency, explanations need to be made completeand self-contained, leading to the introduction ofadditional explanatory sentences, which increasesthe total number of suggested proof steps. There-fore, we further evaluated how the proof steps varywhen the total number of suggested proof stepsincreases, contrasting both refined and unrefinedcases. illustrates this trend. In general,all models show a positive trend, as the total sug-gested proof steps increase, the average number ofproof steps processed by the proof assistant alsoincreases. Models like Mistral-small and GPT-3.5 tend to suggest more proof steps to accomplish thelogical goal, which can result in some redundantsteps, such as the significant pulse shown in c. For unrefined explanations, as shown in Fig-ure 7d, 7e and 7f, the progression is steadier butretains a positive trend, where the models gener-ally suggest more proof steps in response to theadditional explanatory sentences introduced to cor-rect a logical error identified from the erroneousstep. We analysed the correlation between averagesuccessful explanatory sentences and total plannedsentences in proofs, detailed in Appendix A.3. Ex-amples of refined and unrefined explanations are inAppendix A.5.",
  "Factual Errors and Trivial Explanations": "In addition to evaluating the logical validity of ex-planations, we also conducted a human evaluationof the refined explanations, considering factual cor-rectness and explanation triviality for the two best-performing models (GPT-3.5 and GPT-4). Thisevaluation focused on two questions: Are therefined explanatory sentences factually correct?and Is the explanation trivial, merely repeatingor paraphrasing the content of the premise andhypothesis to achieve logical validity?. As illus-trated in , our findings indicate that allrefined explanations in the e-SNLI and WorldTree",
  ": Human evaluation of refined explanations in terms of factuality and triviality": "datasets are consistent with commonsense knowl-edge. In the QASC dataset, 2.27% and 1.82% of theexplanation refined by GPT-3.5 and GPT-4 containsentences misaligned with true world knowledge.We found that the majority of these errors resultfrom over-generalisation, such as the sentence Alltetrapods are defined to have four limbs, whichinaccurately includes snakes. Finally, we found a relatively low number of ex-planations that repeat or paraphrase the content ofpremise and hypothesis. This phenomenon is ab-sent in e-SNLI and becomes more evident when theexplanatory sentences increase in complexity (i.e.,WorldTree), leading models sometimes to gener-ate explanations that do not include any additionalinformation for the entailment to hold.",
  "LLMs Self-Refinement from ExternalFeedback": "Self-refinement of LLMs has demonstrated promis-ing effectiveness in generating faithful and trust-worthy responses (Pan et al., 2023b). The use ofexternal feedback to guide LLMs has been exten-sively studied (Yu et al., 2023; Akyurek et al., 2023;Olausson et al., 2024a). Previous work such asPeng et al. (2023) have employed facts retrievedfrom external knowledge bases as sources of feed-back, while Paul et al. (2024) developed a criticmodel to provide feedback for reasoning refine-ment. Additionally, Nathani et al. (2023) have ex-plored the use of feedback models for automatedfeedback generation. Various works have also in-vestigated tasks related to code generation (Chen et al., 2023; Olausson et al., 2024b) and the creationof either synthetic or expert-written logical naturallanguage expressions (Olausson et al., 2023). Quanet al. (2024) use a differentiable logic reasoner forverifying and refining explanations via abductivereasoning, improving logical consistency in ethicalNLI tasks. This paper focuses on the automatedverification and refinement of natural language ex-planations created by human annotators in NLItasks. Our method leverages feedback from exter-nal solvers to iteratively refine explanations, whichrequire specific modelling interventions such asextracting the exact erroneous steps from the theo-rem prover to effectively refine logical errors in theexplanatory sentences.",
  "Explanation Generation": "Existing work has explored robust and effective ap-proaches for multi-hop reasoning tasks in explana-tion generation (Thayaparan et al., 2021; Valentinoet al., 2022b; Neves Ribeiro et al., 2022). In priorresearch, metrics such as Mean Average Precision(MAP) (Valentino et al., 2022a) have been em-ployed to assess the ranking of facts in explana-tion generation tasks against gold-standard explana-tions. Although these metrics effectively measureprecision relative to these standards, they inade-quately capture the logical consistency and com-pleteness of the explanations generated. Such short-comings are particularly critical in tasks that re-quire not only factual accuracy but also coherenceand inferential soundness, as in natural languageinference and explanation generation. Our pro-posed metrics address this gap by incorporatingassessments of logical validity. Although somemetrics have been proposed to manually evaluatethe logical validity of explanations (Valentino et al.,2021; Yuan et al., 2024), such as non-redundancyor logical errors, these require significant effortfrom domain experts in formal languages. In thiswork, we use human-annotated explanations as afoundational dataset to detect and correct logicaldiscrepancies, offering a framework adaptable forautomatically enhancing both the precision and log-ical integrity of outputs across multi-step inferencetasks.",
  "Autoformalisation": "Autoformalisation refers to the process of trans-lating natural language descriptions into symbolicrepresentations. Research in this area has includedthe formalisation of mathematical proofs (Cunning- ham et al., 2022; Wu et al., 2022; First et al., 2023;Jiang et al., 2023), and efforts to transform nat-ural language sentences into logical forms usingLLMs (Pan et al., 2023a; Olausson et al., 2023;Jiang et al., 2024b; Dalal et al., 2024). However,contextual information is frequently lost when sen-tences are translated in these logical frameworks.To mitigate semantic loss during the transforma-tion process, we leverage Neo-Davidsonian eventsemantics, which aims to maximise the preserva-tion sentence-level content. This representationparadigm can facilitate a more systematic content-preserving translation to logical forms, which ismore independent from particular choices of repre-sentation schema.",
  "Conclusion": "In this work, we present a novel neuro-symbolicframework, Explanation-Refiner, which integratesLLMs and theorem provers for automatic verifi-cation and refinement of natural language expla-nations through iterative cycles. Extensive exper-iments on textual entailment and multiple-choiceQA tasks showed improved logical validity ofhuman-annotated explanations. We investigatedthe models performance from simple to complexexplanatory/sentence structures and introduced amethod to prevent the loss of semantic informationin autoformalisation tasks with error correction.In future work, we aspire to enhance the frame-works robustness towards complex and unstruc-tured explanations with fewer iterations required toimprove the models efficiency.",
  "Limitations": "While this work have demonstrated significant im-provements in terms of enhancing the logical con-sistency of explanations, the connection betweenlogical consistency and AI safety still needs fur-ther investigation. While the idea of using for-mal solvers in conjunction with LLMs delivers apromise avenue to improve the consistency of rea-soning within LLMs, these methodologies need tobe further developed and critically assessed as amechanism which can provide guarantees of cor-rectness, consistency and completeness within crit-ical application domains.",
  "This work was partially funded by the Swiss Na-tional Science Foundation (SNSF) project Neu-": "Math (200021_204617), by the EPSRC grantEP/T026995/1, EnnCore: End-to-End Concep-tual Guarding of Neural Architectures under Secu-rity for all in an AI enabled society, by the CRUKNational Biomarker Centre, and supported by theManchester Experimental Cancer Medicine Centreand the NIHR Manchester Biomedical ResearchCentre. Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan,Peter Clark, Derry Tanti Wijaya, and Niket Tandon.2023. RL4F: Generating natural language feedbackwith reinforcement learning for repairing model out-puts. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 77167733, Toronto,Canada. Association for Computational Linguistics. Pepa Atanasova, Oana-Maria Camburu, Christina Li-oma, Thomas Lukasiewicz, Jakob Grue Simonsen,and Isabelle Augenstein. 2023. Faithfulness testsfor natural language explanations. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 283294, Toronto, Canada. Association forComputational Linguistics. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, andGreg Durrett. 2022.Natural language deductionthrough search over statement compositions. In Find-ings of the Association for Computational Linguistics:EMNLP 2022, pages 48714883, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Oana-Maria Camburu, Tim Rocktschel, ThomasLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-ral language inference with natural language explana-tions. In Advances in Neural Information ProcessingSystems, volume 31. Curran Associates, Inc.",
  "Proceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 41574165, Online. Association for Computational Lin-guistics": "Chun Sik Chan, Huanqi Kong, and Liang Guanqing.2022. A comparative study of faithfulness metricsfor model interpretability methods. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 50295038, Dublin, Ireland. Association forComputational Linguistics. Qianglong Chen, Feng Ji, Xiangji Zeng, Feng-LinLi, Ji Zhang, Haiqing Chen, and Yin Zhang. 2021.KACE: Generating knowledge aware contrastive ex-planations for natural language inference. In Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics and the 11th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 25162527,Online. Association for Computational Linguistics.",
  "Proceedings of the Eleventh International Confer-ence on Language Resources and Evaluation (LREC2018), Miyazaki, Japan. European Language Re-sources Association (ELRA)": "Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024a.Mixtral of experts. Preprint, arXiv:2401.04088. Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,Wenda Li, Jiacheng Liu, Mateja Jamnik, TimotheeLacroix, Yuhuai Wu, and Guillaume Lample. 2023.Draft, Sketch, and Prove: Guiding formal theoremprovers with informal proofs. In International Con-ference on Learning Representations. Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen.2024b.Leanreasoner: Boosting complex logicalreasoning with lean. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies.",
  "Tushar Khot, Peter Clark, Michal Guerquin, Pe-ter Alexander Jansen, and Ashish Sabharwal. 2019.QASC: A dataset for question answering via sentencecomposition. In AAAI": "Sawan Kumar and Partha Talukdar. 2020. NILE : Natu-ral language inference with faithful natural languageexplanations. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 87308742, Online. Association forComputational Linguistics. Haochen Liu, Joseph Thekinen, Sinem Mollaoglu,Da Tang, Ji Yang, Youlong Cheng, Hui Liu, andJiliang Tang. 2022. Toward annotator group bias incrowdsourcing. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 17971806,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Tobias Nipkow, Markus Wenzel, and Lawrence C Paul-son. 2002.Isabelle/HOL: a proof assistant forhigher-order logic. Springer": "Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,Armando Solar-Lezama, Joshua Tenenbaum, andRoger Levy. 2023. LINC: A neurosymbolic approachfor logical reasoning by combining language modelswith first-order logic provers. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 51535176, Singapore.Association for Computational Linguistics. Theo X. Olausson, Jeevana Priya Inala, ChenglongWang, Jianfeng Gao, and Armando Solar-Lezama.2024a. Is self-repair a silver bullet for code gen-eration? In International Conference on LearningRepresentations (ICLR).",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Liangming Pan, Alon Albalak, Xinyi Wang, andWilliam Wang. 2023a. Logic-LM: Empowering largelanguage models with symbolic solvers for faithfullogical reasoning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages38063824, Singapore. Association for Computa-tional Linguistics. Liangming Pan, Michael Saxon, Wenda Xu, DeepakNathani, Xinyi Wang, and William Yang Wang.2023b.Automatically correcting large languagemodels: Surveying the landscape of diverse self-correction strategies. Preprint, arXiv:2308.03188.",
  "Boris Shminke. 2022. Python client for isabelle server.Preprint, arXiv:2212.11173": "Mokanarangan Thayaparan, Marco Valentino, andAndr Freitas. 2021.Explainable inference overgrounding-abstract chains for science questions. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 112, Online.Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Marco Valentino, Ian Pratt-Hartmann, and Andr Fre-itas. 2021. Do natural language explanations repre-sent valid logical arguments? verifying entailment inexplainable NLI gold standards. In Proceedings ofthe 14th International Conference on ComputationalSemantics (IWCS), pages 7686, Groningen, TheNetherlands (online). Association for ComputationalLinguistics. Marco Valentino, Mokanarangan Thayaparan, DeborahFerreira, and Andr Freitas. 2022a. Hybrid autore-gressive inference for scalable multi-hop explanationregeneration. Proceedings of the AAAI Conferenceon Artificial Intelligence, 36(10):1140311411.",
  "Nathaniel Weir, Peter Clark, and Benjamin Van Durme.2023. Nellie: A neuro-symbolic inference engine forgrounded, compositional, and explainable reasoning.Preprint, arXiv:2209.07662": "Sarah Wiegreffe and Ana Marasovic. 2021. Teach me toexplain: A review of datasets for explainable naturallanguage processing. In Thirty-fifth Conference onNeural Information Processing Systems Datasets andBenchmarks Track (Round 1). Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, MarkusRabe, Charles Staats, Mateja Jamnik, and ChristianSzegedy. 2022. Autoformalization with large lan-guage models. In Advances in Neural InformationProcessing Systems, volume 35, pages 3235332368.Curran Associates, Inc.",
  "Li Yuan, Yi Cai, Haopeng Ren, and Jiexin Wang. 2024": "A logical pattern memory pre-trained model for en-tailment tree generation.In Proceedings of the2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion (LREC-COLING 2024), pages 759772, Torino,Italia. ELRA and ICCL. Wenting Zhao, Justin Chiu, Claire Cardie, and Alexan-der Rush. 2023. Abductive commonsense reason-ing exploiting mutually exclusive explanations. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1488314896, Toronto, Canada.Association for Computational Linguistics.",
  "A.2Scalability": "shows the average Isabelle/HOL solvingtime against the number of planned explanatorysentences in a proof and the length of suggestedproof steps, including theories that have syntaxerrors, respectively. In some cases, the theoremprover may get stuck on a proof step, and we haveset a termination time if the solving time exceeds65 seconds. Number of Explanatory Sentences Avg. Isabelle Solving Time/s",
  "A.4.1Autoformalisation": "displays the prompts used to identify ac-tion verbs (events) within the premise, explanation,and hypothesis sentences, representing events inDavidsonian-event semantics. displaysthe prompts used to transfer natural language tological forms based on the identified events verbs. shows how to convert logical forms intoIsabelle/HOL code (axioms and type declaration). shows how to convert the premise and hy-pothesis sentences into the Isabelle/HOL theoremcode, based on the previously constructed axiomscode. shows how to refine the syntaxerrors based on the types of errors, the providedcode, the error messages, and the locations of theerrors within the code. A.4.2Proof Construction shows the prompts for making a pre-liminary inference strategy, which also identifiesredundant and related explanatory sentences thatwill be used for proof generation. showsthe prompts for building the proof steps used forIsabelle/HOL Proof assistant based on the providedinference strategy.",
  "A.5Examples of Explanation Refinement": "shows an example from the e-SNLI datasetof how the explanation changes after each iteration.Figures 21, 22, and 23 illustrate the Isabelle/HOLtheory code changes during the refinement process. with Figures 24, 25, and 26 also showanother example of how the explanation is refinedafter each iteration.Green code indicates the proof steps that havesuccessfully progressed, while red code showswhere the proof failed at that step. More examplescan be found at",
  "A.6Datasets and Theorem Prover": "The datasets used in our experiments, includingsamples from e-SNLI (Camburu et al., 2018),QASC (Khot et al., 2019), and WorldTree (Jansenet al., 2018), are all sourced from open academicworks.We employed Isabelle as the theoremprover, which is distributed under the revised BSDlicense. Additionally, the TCP client used for theIsabelle server (Shminke, 2022) is licensed underApache-2.0. Number of Planned Explanations Avg. Processed Explanations Refined e-SNLI Mixtral-8x7bMistral-smallGPT-3.5GPT-4Llama2-70b",
  "return E": "SYSTEM: You are an expert in linguistics. You will be provided with some sentences, find any action verbs of these sentences.You need to ignore auxiliary verbs and modal verbs. Some instructions:1. You must give me the answer for all provided sentences.2. Do not add any notes.3. If no premise sentence provided, include it in the answer as none. 4. Retain the answer words in their original form within the provided sentence.USER: Here are some examples:###Hypothesis Sentence: 1. A woman is playing an instrument.Has action: YesActions: 1. playing",
  ": Prompts for detecting event-related words inthe given sentences": "SYSTEM: You are an expert in semantics, formal language and neo-davidsonian event semantics. You will be provided with some sentences and the action verbs involved in those sentences. You need to transfer the sentences into symbolic language. If the sentence has no action, transfer it into formal language using first-order language. If the sentence has one action, transfer it using first-order language and davidsonian event semantics within one event. If the sentence has two more actions, transfer it using first-order language and davidsonian event semantics within at most two events.Some instructions:1. Capture All Information: Ensure the logical form reflects",
  "every detail from the sentence": "2. Use '' for Certain Verbs: Represent actions like 'cause', 'lead', 'help' that represent an implication, causal relation with '' for clarity.3. Event Variable 'e': Use 'e' for events, actions, with action predicates having 'e' as their sole argument....USER: Here are some examples:###Sentence: Grass is a kind of plant.Has action: NoActions: Logical form: x. Grass(x) Plant(x)###Sentence: Squirrels typically eat nuts for energy.Has action: YesActions: 1. eatLogical form: x y z. Squirrels(x) Nuts(y) (e. Eat(e) Agent(e, x) Patient(e, y) ForEnergy(y, x))###...<<<<<<<<<<<<<<<<<<<<<<<Strictly followed the instructions that I have claimed.",
  ": Prompts for converting natural languagesentences into logical form representations": "SYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with some sentences and corresponding logical forms (first-order logic and davidsonian event semantics) of those sentences. You need to transfer such logical forms into Isabelle axioms code and define the consts and of the symbolic forms.Some instructions:1. Isabelle axioms code use , , , , , , as logic symbols. Please write the axiom code with these logic symbols.2. Isabelle consts code use as logic symbols. Please define...The code structure for axioms is:```begin",
  ": Prompts for converting logical form intoIsabelle/HOL code format for building the axioms andtype declaration": "SYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with a Hypothesis sentence and a Premise sentence with their corresponding logical forms (first-order logic and davidsonian event semantics). ...Some instructions:1. Isabelle code use , , , , , , as logic symbols. Please write the code with these logic symbols....The code structure for theorem hypothesis is:```theorem hypothesis: (* Premise: [provided premise sentence in natural language] *)...end```USER: Here are some examples:###Provided sentences:...Provided code:...Answer:```imports Main",
  ": Prompts for building the theorem code partof the Isabelle/HOL theory": "SYSTEM: You are an expert in the Isabelle theorem prover and familiar with HOL session syntax and Davidsonian event semantics. You will be provided with Isabelle code containing some syntax errors, along with details of the errors and their locations in the code. You need to fix the code (logical form) of the related error.Some instructions:1. Do not change code structure, you just need to fix the syntax error.2. Type unification failed errors indicates the defined consts and the acutal preidcates are not consistent. There are only two types: event and entity. The type defined in the consts should be same as the type represented in the logical form codes....USER: Here are some examples:###Provided code:...Error Identified:",
  "explanation_1: \"x c. Cute x Couple x Club c At x c (y. Couple y Club c At y c)\"": "Answer:From the error found, the error type is type unification failed, which means the operand type defined in consts is not consistent with the operand in the code.As the error indicates the Operator 'At' in code is defined as At :: \"event entity bool\" but in the code it is stated as At x c where x is defined as entity. That's the reason cause type unification failed.It should have the same type which can be refined as:```...Couple :: \"entity bool\" Club :: \"entity bool\" At :: \"entity entity bool\" Cute :: \"entity bool\"",
  ": Prompts for how to refine the identifiedsyntax errors in the constructed code": "SYSTEM: You are an expert in natural language inference, textual entailment and linguistic semantics. You will be provided with a premise sentence, some explanatory sentences and a hypothesis sentence. The premise sentence and explanatory sentences should entail the hypothesis sentence.You need to write a step-by-step natural language inference to state how the explanatory sentences will entail the hypothesis sentence from the premise sentences.Instructions:1. You must elicit the explanatory sentences which are redundant and not directly related (if there are no redundant or all related state it as no).2. You must state on which step of the proof each explanatory sentence is used. 3. You must elicit the used explanatory sentences in the natural language inference steps.USER: Here are some examples:###Provided Premise Sentence:A group of students are studying non-contact force. Provided Explanation Sentences:1. Non-contact forces can affect objects that are not touching. 2. A magnet attracts magnetic and ferromagnetic metals through magnetism.3. Magnetism does not require contact between objects to act.4. A paper clip is a kind of object.5. A magnet is a kind of object.6. Magnetism is a kind of force.7. A kind of something is an example of that something.",
  "Provided Hypothesis Sentence:A paper clip attracted to a magnet is an example of a non-contact force acting on an object": "Natural Language Inference Steps:1. As we need to infer the hypothesis, we need to find the information of paper clip, magnet, non-contact force and object. The action event of attracted and acting. The relationship of is an example of.2. From the premise, we can get the information of non-contact force.3. From explanation 4 and 5, we deduce that both a paper clip and a magnet are objects.4. Explanation 2 establishes that a magnet can attract certain metals through magnetism, which is a force (due to explanation 6)....",
  ": Prompts for how to make a step-by-steppreliminary inference strategy": "SYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with an Isabelle code which consistent of some axioms, a theorem hypothesis that needs to be proven. The logical form of axioms indicates some explanatory sentences, the logical form after \"assume asm:\" indicates a premise sentence and the logical form after \"shows\" indicates a hypothesis sentence. ...Some instructions:1. 'sorry' and fix command is not allowed. ...USER: Here are some examples:###Provided Isabelle Code:```...begintypedecl entitytypedecl eventconsts PlantReproduction :: \"entity bool\"...(* Explanation 1: Plant reproduction often requires pollen. *)axiomatization where explanation_1: \"x y e. PlantReproduction x Pollen y Require e Agent e x Patient e y\" theorem hypothesis: (* Premise: Students are studying plant reproduction process. *) assumes asm: \"Students x PlantReproduction y Studying e Agent e x Patient e y\" (* Hypothesis: Plant reproduction often requires bees. *) shows \"x y e. PlantReproduction x Bee y Require e Agent e x Patient e y\"proof - qed",
  "end```Provided Natural Language Inference Strategy:": "1. As we need to infer the hypothesis, we need to find the information of plant, reproduction process, requires action and bees.2. From explanation 1, we get the information of plant reproduction, which requires pollen....Explanation 3 and 4 is not related and Explanation 5 is redundant.The proof steps use explanation 1 and explanation 2. Answer: ```proof - from asm have \"PlantReproduction x\" by simp then obtain e1 where e1: \"Require e1 Agent e1 x Patient e1 y\" using explanation_1 by blast then have \"Bee y\" using explanation_2 by blast have conclusion: \"Require e1 Agent e1 x Patient e1 y\" using e1 by simp show ?thesis using asm conclusion `Bee y` by blastqed```###...<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.",
  ": Prompts for how to build a proof for Is-abelle/HOL proof assistant": "SYSTEM: You are an expert in Isabelle theorem prover, first-order, Davidsonian event semantics and natural language inference. You will be provided with three types of sentences: Premise Sentence, Explanation Sentence and Hypothesis sentence. ...Some instructions:1. Only refine the related axioms/explanatory sentence in natural language sentences....USER: Here are some examples:###Provided Premise Sentence:...Natural Language Inference steps:1. To infer the hypothesis, we need to identify the information related to a tennis ball, water, and the action of floating. The relationship of \"will\" indicates a future or potential action....",
  "Proof failed at:then have \"Object x\" using explanation_1 by blast": "Refine strategy:From the provided error location, it failed at the step of \"then have \"Object x\" using explanation_1 by blast\" using explanation 1. ...Updated explanatory sentences:1. a table tennis ball is a kind of object.2. a tennis ball contains air.3. something that contains air is usually buoyant.4. buoyant means able to float in a liquid or gas.5. water is a kind of liquid. ###...<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.",
  "begintypedecl entitytypedecl event": "consts Bartender :: \"entity bool\" Person :: \"entity bool\" DressedInBlack :: \"entity bool\" InBlack :: \"entity bool\" Male :: \"entity bool\" SleevesRolledUpToElbowHeight :: \"entity bool\" Drink :: \"entity bool\" MartiniGlass :: \"entity bool\" Making :: \"event bool\" Agent :: \"event entity bool\" Patient :: \"event entity bool\" In :: \"entity entity bool\"",
  "(* Explanation 1: The lady is looking through a photo album which is a type of book. *)axiomatization where explanation_1: \"x y e. Lady x PhotoAlbum y Book y Looking e Agent e x Patient e y\"": "theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: \"Woman x Glasses x BlackFramed x PhotoAlbum y Chair z Red z Wicker z Peruses e Agent e x Patient e y Sitting x z\"(* Hypothesis: There is a lady with a book. *)shows \"x y. Lady x Book y With x y\"proof - from asm have \"Woman x PhotoAlbum y\" by blast from this have \"Lady x Book y\" using explanation_1 by blast then show ?thesis using `Lady x` `Book y` by blastqed",
  "(* Explanation 2: A photo album is a type of book. *)axiomatization where explanation_2: \"x. PhotoAlbum x Book x\"": "theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: \"Woman x Glasses x BlackFramed x PhotoAlbum y Peruses e Agent e x Patient e y Chair z Red z Wicker z SittingIn x z\"(* Hypothesis: There is a lady with a book. *)shows \"x y. Lady x Book y With x yproof - from asm have \"Woman x\" by blast then have \"Lady x\" using explanation_1 by blast from asm have \"PhotoAlbum y\" by blast then have \"Book y\" using explanation_2 by blast show ?thesis using `Lady x` `Book y` by blastqed",
  "(* Explanation 3: If a woman is perusing a photo album, then the woman is with a book. *)axiomatization where explanation_3: \"x y e. Woman x PhotoAlbum y Perusing e Agent e x Patient e y With x y\"": "theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: \"Woman x Glasses x BlackFramed x PhotoAlbum y Chair z RedWicker z Perusing e Agent e x Patient e y SittingIn x z\"(* Hypothesis: There is a lady with a book. *)shows \"x y. Lady x Book y With x y\"proof - from asm have \"Woman x PhotoAlbum y Perusing e Agent e x Patient e y\" by blast then have \"With x y\" using explanation_3 by blast from asm have \"Woman x\" by blast then have \"Lady x\" using explanation_1 by blast from asm have \"PhotoAlbum y\" by blast then have \"Book y\" using explanation_2 by blast show ?thesis using `Lady x` `Book y` `With x y` by blastqed",
  "(* Explanation 1: A bartender, who is a person, is wearing black. *)axiomatization where explanation_1: \"x e. Bartender x Person x Black x Wearing e Agent e x Patient e x\"": "theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: \"Bartender x Male x Black x Drink y MartiniGlass z Making e Agent e x Patient e y In z y\"(* Hypothesis: A person in black *)shows \"x. Person x Black x\"proof - from asm have \"Bartender x Black x\" by blast from this have \"Person x Black x\" using explanation_1 by blast then show ?thesis by blastqed",
  "(* Explanation 2: If a person is wearing black, then the person is in black. *)axiomatization where explanation_2: \"x e. Person x Wearing e Agent e x Black e InBlack x\"": "theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: \"Bartender x Male x Black e1 Dress e1 Agent e1 x Sleeves x RolledUp e2 Agent e2 x ElbowHeight e2 Drink y MartiniGlass z Making e3 Agent e3 x Patient e3 y In z y\"(* Hypothesis: A person in black *)shows \"x. Person x InBlack x\"proof - from asm have \"Bartender x\" by blast then have \"Person x\" using explanation_1 by blast from asm have \"Black e1 Dress e1 Agent e1 x\" by blast then have \"InBlack x\" using `Person x` explanation_2 by blast show ?thesis using `Person x` `InBlack x` by blastqed",
  "(* Explanation 2: If a person is dressed in black, then the person is in black. *)axiomatization where explanation_2: \"x. Person x DressedInBlack x InBlack x\"": "theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: \"Male x Bartender x DressedInBlack x SleevesRolledUpToElbowHeight x Drink y MartiniGlass z Making e Agent e x Patient e y In z y\"(* Hypothesis: A person in black *)shows \"x. Person x InBlack x\"proof - from asm have \"Bartender x DressedInBlack x\" by blast then have \"Person x DressedInBlack x\" using explanation_1 by blast then have \"Person x InBlack x\" using explanation_2 by blast then show ?thesis by blastqed"
}