{
  "Abstract": "Large language models (LLMs) have demon-strated remarkable progress in leveraging di-verse knowledge sources. This study inves-tigates how nine widely used LLMs allocateknowledge between local context and globalparameters when answering open-ended ques-tions in knowledge-consistent scenarios. Weintroduce a novel dataset, WikiAtomic1, andsystematically vary context sizes to analyzehow LLMs prioritize and utilize the providedinformation and their parametric knowledge inknowledge-consistent scenarios. Additionally,we also study their tendency to hallucinate un-der varying context sizes. Our findings revealconsistent patterns across models, including aconsistent reliance on both contextual (around70%) and parametric (around 30%) knowledge,and a decrease in hallucinations with increas-ing context. These insights highlight the im-portance of more effective context organizationand developing models that use input more de-terministically for robust performance.",
  "Introduction": "Large language models (LLMs) have significantlyadvanced the capabilities of natural language pro-cessing. When generating responses, LLMs canuse the contextual information provided in aprompt along with or instead of the parametricknowledge embedded during pretraining (Petroniet al., 2019; Brown et al., 2020; Heinzerling andInui, 2021).In order to generate accurate and coherent re-sponses, LLMs need to effectively combine theirparametric knowledge with provided contextual in-formation, and understanding the balance betweenthese two sources of information is crucial (Nee-man et al., 2022; Li et al., 2024). Most prior work",
  "The dataset is available at": "has explored this question through the lens of coun-terfactual data where the parametric and contextualknowledge are in conflict (Longpre et al., 2021;Xu et al., 2024a). Some studies suggest that themodels prefer contextual knowledge, while otherssuggest they prioritize parametric knowledge (Kr-ishna et al., 2021; Zhou et al., 2023a).In many real-world scenarios such as questionanswering or summarization, however, contextualknowledge may complement rather than conflictwith parametric knowledge. Understanding howmodels integrate different sources of knowledge inknowledge-consistent scenarios is critical.Moreover, while parametric knowledge enablesLLMs to generate coherent text, it also contributesto the risk of hallucinations where responses are co-herent and confident but factually incorrect or irrel-evant (Ji et al., 2023; Tian et al., 2023; Wang et al.,2023; Luo et al., 2024). As such, our work alsoexplores the hallucination tendency in knowledge-consistent scenarios.Real-world applications often present LLMswith varying amounts of context. Building robustmodels requires understanding how effectively theyleverage this information. This research investi-gates how the volume of context influences knowl-edge preference and hallucination tendencies inLLMs within a question answering (QA) frame-work. We study nine widely used LLMs to deter-mine how different models navigate varying con-text sizes in knowledge-consistent settings, aimingto answer the main question: How do LLMs pri-oritize contextual and parametric knowledge whengenerating responses? Additionally, we investi-gate the likelihood of generating hallucinationswith increasing context, which parts of contextsare used, how similar are various types of knowl-edge, and further analyses exploring potentiallyunseen knowledge. provides an overview of our inves-tigation pipeline.We create a new dataset,",
  ": Overview of the dataset creation and model evaluation pipeline": "WikiAtomic, of atomic sentences serving as ourcontext. We systematically adjust the amount offactual context provided to the models and promptthem in an open-ended QA task to study the re-sponses generated by various models. Then, wemeasure the amount of contextual knowledge mod-els recall, the additional parametric knowledge theyrely on, and the extent of hallucinations found intheir responses.Our results suggest that all models behave verysimilarly, integrating up to 30% parametric knowl-edge in their responses. For smaller contexts, mod-els recall information from all parts of the con-text. However, for longer contexts, they predomi-nantly focus on the first half, potentially missingkey information from other parts of the input. Thesequence of information in the responses largelymirrors the context and the additional parametricknowledge included is moderately similar to thecontextual knowledge. Lastly, the hallucinationscore decreases as more context is added. Thesefindings highlight the importance of effective con-text organization and models that utilize input morepredictably.Our key contributions include:",
  "Task and Terminology": "We structure our investigation as an open-endedquestion answering task to gain deeper insightsinto how LLMs utilize and integrate contextualknowledge with their parametric knowledge. Byincrementally increasing the amount of context, weanalyze the models behavior, focusing on theirability to prioritize different types of informationand their tendency toward factual hallucination inknowledge-consistent scenarios.We next introduce some key terminology usedthroughout the paper.Topic: This is the title of the Wikipedia arti-cle. The WikiAtomic dataset includes 200 distinctarticles, and questions are asked about these topics.Atomic Sentence: This is a sentence that con-tains a single piece of information that cannot bebroken down into simpler components without los-ing its meaning (Liu et al., 2023).Context: For each topic, a context consists ofk atomic sentences provided in a prompt. In ourexperiments, 2 k 50, where contexts consistof increments of 2 sentences from 2 to 30 (e.g., 2,4, 6, etc.), followed by increments of 5 sentencesuntil reaching a total of 50 sentences. For eachtopic, this results in contexts of 20 different sizes,creating a total of 4,000 topic-context instances.Response: Given the context, the model gen-erates a response that includes some degree ofboth contextual and parametric knowledge. Thisresponse is then atomized. A sentence directly de-rived from the provided context is considered ascontextual (local) knowledge. In contrast, a sen-tence that is not entailed from the context is consid- ered as parametric (global) knowledge. Follow-ing prior work (Neeman et al., 2022), contextualknowledge comes from external sources providedduring inference, while parametric knowledge isknowledge encoded (or memorized) in the modelparameters. As such, if a sentence introduces infor-mation not present in the context, it is considered aparametric knowledge sentence.",
  "WikiAtomic Dataset": "We created a novel dataset WikiAtomic consist-ing of 200 articles from Wikipedia. We selectedWikipedia as the basis for creating our knowledge-consistent dataset because its extensive data hashistorically been integral to the training of numer-ous LLMs, explicitly in models like GPT-2, GPT-3,BERT, T5, and BLOOM, and implicitly in large-scale aggregate corpora such as The Pile (Gao et al.,2020) and Common Crawl (Luccioni and Viviano,2021). This extensive use allows us to reasonablyassume that information from Wikipedia, particu-larly from older articles, is present in the pretrain-ing data of several LLMs we study. Extracting Wikipedia ArticlesWe selected 200high-quality articles from Wikipedia2, each over1000 words, covering diverse topics from scienceand technology to history, culture, and prominentfigures. After manually removing low-quality texts,we ensured a diverse collection for our open-endedquestion answering task. Converting to Atomic SentencesTo preciselycontrol the number of contexts in our question,all articles are decomposed into a set of atomicsentences (Liu et al., 2023). Sentences contain-ing multiple pieces of information can complicateevaluations, particularly those involving entailment(Kim et al., 2024). By breaking down sentencesinto atomic sentences, we ensure that each unitpresents a single, unambiguous piece of informa-tion which enhances the accuracy of entailmentassessments. Following Min et al. (2023), we useGPT-4o to extract atomic sentences by providingthe definition of atomic facts and instructing themodel to perform multiple passes on an article tobreak each sentence down to individual atomic sen-tences (the prompt is included in Appendix A).Consequently, k atomic sentences are extracted foreach article (in our experiments, k = 50) for atotal of 200 50 = 10000 atomic sentences in",
  "Atomic Sentences:1.Mariah Carey was born on March 27,1969.2.Mariah Carey was born in Huntington,New York": "We manually verify a subset of 1000 atomic sen-tences (20 articles, each with 50 atomic sentences)to ensure that the sentences (i) were correctly at-omized and, (ii) originate from the correspondingWikipedia article. Only 12 out of 1000 sentenceswere found to be insufficiently atomic or had theirmeaning changed. GPT-4o tended to struggle themost with sentences that have complex structures,such as those with multiple clauses or extensive useof commas. This led to errors in atomic sentenceextraction where the sentences either contained toomuch information to be considered atomic or themeaning got altered in the process. Here is an ex-ample of each failure case:",
  "Experiments": "illustrates a sample instance of our ques-tion answering task that includes the input (a list ofatomic sentences as context along with a questionprompt) and the output which is a models responsefurther atomized.Question FormulationWe prompt the models toanswer a question given the contexts using a semi-restrict format: With this information, tellme about {Topic}3. The effects of varying thequestion format are further explored in .5.Response GenerationThe responses generatedby the LLMs are converted into atomic sentencesusing the same method described earlier in Sec-tion 34.Ultimately, we obtain two lists for each question-answer pair: atomic contexts (atomic sentencesfrom the context) and atomic responses (atomicsentences from the response), allowing us to di-rectly compare them and minimize discrepancies.",
  ": Knowledge-consistency between paramet-ric knowledge and input context of WikiAtomic topics,computed using SBERT (Reimers and Gurevych, 2019)": "When contextual and parametric knowledgelargely align with subtle differences, it becomeschallenging to distinguish between the two. Whenno context is provided, the models response servesas a baseline of global parametric knowledge. Wecalculate the overlap between the response (atk = 0) and the longest input context (k = 50)to estimate knowledge consistency. As shown in, parametric knowledge across five top-ics shows high consistency with WikiAtomic. Thisshows that the model can produce similar responseswhether it is provided with detailed context or not,supporting our experiments in a non-conflict set-ting. This differentiates our work from previousstudies that relied on counterfactual datasets.Using the atomic contexts as a reference, wecategorize the atomic responses as either contex-tual or parametric knowledge. We use the NaturalLanguage Inference INFUSE framework5 (Zhanget al., 2024) to assess the faithfulness of responses.INFUSE calculates entailment scores for each sen-tence in what is considered the summary (responses",
  "(d) Mistral 8x22B": ": Contextual (local), parametric (global), and total sentences in responses for (a) GPT-4o, (b) Claude Opus,(c) Llama 3 70B, and (d) Mistral 8x22B. On the x-axis, k = 0 serves as the baseline when no context is provided. from the model), with scores ranging from 0 to 1. Ascore of 0 indicates that the sentence is not entailedfrom the context, whereas a score of 1 signifiesfully entailed from the context. We empirically setthe threshold at 0.5 based on an examination ofpreliminary results, thus helping us identify sen-tences in the response as either contextual or para-metric6.This process involved three annotatorswho independently reviewed a subset of these am-biguously scored sentences. They assessed whethereach of these sentences were contextual or paramet-ric based on the definition and whether the scoresalign with their decisions. The threshold of 0.5was chosen based on a consensus from this process,as it most effectively distinguished between thetwo categories. Preliminary experiments with othermetrics such as ROUGE, METEOR, and partialmatch yielded similar patterns (results included inAppendix B).Hallucination detectionFor sentences classi-fied as parametric knowledge, we further assesswhether they are factually accurate or hallucinatedas LLMs are known to hallucinate (Huang et al.,2023; Xu et al., 2024b; Bai et al., 2024). We use theFActScore framework7 (Min et al., 2023) that usesan external knowledge source to verify each sen-tence with scores ranging from 0 to 100; a higherscore indicates fewer hallucinations and more fac-tually accurate responses.",
  "Models": "We study nine models ranging from small mod-els to state-of-the-art LLMs, including both open-source and closed-source options (for implementa-tion details, see Appendix C). These models in-clude: GPT-4o (gpt-4o-2024-05-13), Claude3 Opus (claude-3-opus-20240229),Sonnet(claude-3-sonnet-20240229) and Haiku (claude-3-haiku-20240307)), Llama 38 70B (Meta-Llama-3-70B-Instruct) and 8B (Meta-Llama-3-8B-Instruct), Mixtral 8x22b (Mixtral-8x22B-Instruct-v0.1), Mistral 7B (Mistral-7B-Instruct-v0.2) (Jiang et al., 2023), and Phi-39",
  "Modelk = 10k = 20k = 30k = 40k = 50": "GPT-4o0.69 / 0.310.68 / 0.320.69 / 0.310.68 / 0.320.67 / 0.33Claude 3 Opus0.48 / 0.520.67 / 0.330.73 / 0.270.74 / 0.260.72 / 0.28Claude 3 Sonnet0.50 / 0.500.64 / 0.360.68 / 0.320.67 / 0.330.66 / 0.34Claude 3 Haiku0.69 / 0.310.75 / 0.250.75 / 0.250.71 / 0.290.72 / 0.28Llama 3 70B0.73 / 0.270.75 / 0.250.74 / 0.260.74 / 0.260.72 / 0.28Llama 3 8B0.62 / 0.380.63 / 0.370.65 / 0.350.65 / 0.350.67 / 0.33Mixtral 8x22B0.58 / 0.420.65 / 0.350.66 / 0.340.69 / 0.310.69 / 0.31Mistral 7B0.48 / 0.520.61 / 0.390.66 / 0.340.67 / 0.330.69 / 0.31Phi-30.24 / 0.760.37 / 0.630.42 / 0.580.46 / 0.540.49 / 0.51",
  ": Ratios of contextual/parametric knowledge in responses": "show consistently similar patterns. This uniformityamong the models suggests a shared underlyingmechanism in processing and responding to con-textual information.With no context provided (k = 0), models tendto be most verbose as expected from an initiallyopen prompt. They also peak in parametric knowl-edge, which drops drastically with the first fourcontexts. Interestingly, total response lengths gen-erally match context lengths. From k = 2 onward,the total sentences and local contextual knowledgesteadily increase as context increases. More con-texts were utilized but none of these models uti-lized 100% of them in their responses. Global factsshow a consistent trend: after the initial open-endedprompt, the global knowledge drops sharply and re-mains low, though never to zero, with some modelsslowly increasing or decreasing but always includ-ing some global knowledge in their responses. presents the proportions of contextualand parametric knowledge in responses. Largercontexts generally increase the models relianceon contextual knowledge (about 70%) while reduc-ing dependence on parametric knowledge (about30%). However, in smaller contexts, models showdifferent preferences, with some prioritizing con-textual knowledge (GPT-4o, Claude Haiku, Llama3 70B, Llama 3 8B, and Mixtral 8x22B), and oth-ers, parametric knowledge (Claude Opus, Mistral7B, Phi-3). Moreover, the average proportion re-mains similar for k = 30, 40, and 50 suggestingthat the ratio of contextual/parametric knowledgeis maintained at these different context lengths.",
  ": Percentage of each quartile of context recalledin response (GPT-4o)": "context an entailment score, considering sentenceswith scores greater than 0.5 to be included in themodels response. For k > 4, inputs were split intoquartiles for clearer analysis. (additional results in Appendix E)shows that for smaller contexts (k < 10), the modeltreats all portions of the context equally. As con-text increases, the model predominantly focuseson the first quartile. For k 25, the preferencegap between quartile 1 and all others further in-creases, with quartile 2 becoming the next mostpreferred section. We observe selective attentionwhere certain parts of the data receive more focusthan others (Liu et al., 2024), and find that modelsstruggle more with recalling the bottom half of thedata rather than the middle, further confirming theimportance of initial contexts.To determine where provided contexts are usedin the responses, we divide contexts and responsesinto four quartiles and compare each section usingcosine similarity of SBERT embeddings (Reimersand Gurevych, 2019), essentially mapping the con-text quartiles to response quartiles. From ,we observe that the first context quartile maps mostclosely to the first, and to a lesser extent, the ad-jacent response quartile (additional results in Ap-",
  "How similar are various types ofknowledge?": "Our analysis so far suggests that LLMs consistentlyadd parametric knowledge regardless of the amountof context provided in the question. This promptedus to investigate the relationships between the dif-ferent types of knowledge. The graphs were gener-ated as follows: For each Wikipedia topic contain-ing 20 questions (up to 50 contexts), we obtained20 responses. Each response contains sentencesmarked as contextual or parametric knowledge. Local vs. LocalWe perform a pairwise compar-ison of contextual knowledge in responses acrossvarious context sizes (a, full set of resultsin Appendix G). We observe that the local contextremains moderately similar in smaller contexts butbecomes increasingly similar with larger contexts.This pattern suggests that models tend to focus oncertain types of context, often the earlier parts.",
  "Global vs. GlobalLet us now turn to b": "(full set of results in Appendix H) which showspairwise similarity between parametric knowl-edge in each response. The darker plots near thediagonal line indicate that the models parametricknowledge remains similar when context sizes areclose, such as context size 8s parametric knowl-edge being most similar to that of sizes 6 or 10.Initially, with minimal context, models provide alarger variety of information. As context size in-creases, some models show increased similarityacross broader ranges, suggesting responses alignmore closely with the overall theme of the context.This indicates that models tend to repeatedly addcertain pieces of information from a small pool of",
  "knowledge, though it is not as uniformly homoge-neous as local knowledge": "Local vs. GlobalOur analysis naturally leads usto our next question: what type of global paramet-ric knowledge is incorporated with local contex-tual knowledge? Our assumption is that low simi-larity indicates complementary parametric knowl-edge, while high similarity suggests it is supple-mental. c (detailed graphs in AppendixI) shows consistent trends across all models. Forsmaller contexts, the similarity between contextualknowledge and parametric knowledge is muchlower, suggesting that models incorporate comple-mentary information not directly provided by thecontext. As context increases, the similarity rises",
  "In knowledge-consistent setting, (howmuch) do models hallucinate?": "In , we present the FActScore hallucina-tion scores across all models. For smaller contexts,models have higher hallucination rate, which im-proves with additional context and converges withas little as 10 sentences in context. Larger mod-els (GPT-4o, Claude Opus, Claude Sonnet, ClaudeHaiku, Llama 3 70B, and Mistral 8x22B) consis-tently show higher FActScores, indicating they areless prone to hallucinations and benefit significantlyfrom additional context. Among smaller models,Mistral 7B and Llama 3 8B perform well, show-ing significant improvement with increased context,while Phi-3 tends to hallucinate more, although italso improves with context. Larger models gen-erally stabilize at higher FActScores with fewerfluctuations, indicating a stronger ability to lever-age additional context to minimize hallucinations.Smaller models show a more gradual improvement,with some like Phi-3, displaying more variabilityand a lower overall FActScore, suggesting they aremore context-sensitive but can still improve withmore information (for detailed results of false para-metric knowledge for each model, see Appendix J).",
  "Further Analyses": "(Potentially) Unseen KnowledgeIn knowledge-consistent scenarios, models rely on a mix of con-textual and parametric knowledge. But in unseenknowledge scenarios, where the context containsnew information that the models have potentiallynot seen, how do models respond? To explorethis, we create a recent example around the pro-Palestinian university protests of April 2024 andprompt the models for information on this topic.Without context, most models stated they lackedinformation on the topic and provided a generalbackground on protests related to Palestine (see, with additional results in Appendix K).However, some models, Claude Haiku, Llama 8B,and Mixtral 22x8B, all confidently respond to theprompt as if they have seen this knowledge in theirpre-training data. We hypothesize that this maybe due to similar events that have occurred in thepast that the model has seen in its pretraining data.With just two contexts, all models referenced thecontext but some included facts not in the context.",
  ": Unseen knowledge results (GPT-4o)": "With 50 contexts, they effectively incorporated thecontextual knowledge without much additional in-formation. Our findings suggest that care must betaken when querying LLMs on new information aswithout context or with minimal context, modelsmay still confidently provide seemingly accurateinformation not in their parametric knowledge. Prompt SensitivityOur naturalistic semi-restrict question prompt (With this information,tell me about [topic].) encourages the models toconsider the provided contexts while still allowingthem to use their parametric knowledge. Consider-ing that LLMs are sensitive to prompts (Lu et al.,2021), we tested two alternative phrasings: a no-restrict prompt which allows the models completefreedom to draw on their parametric and contextualknowledge as they see fit (Tell me about [topic])and a strict prompt which tries to restrict the mod-els to use only the contexts provided (Using theprovided context only, tell me about [topic].).To conduct this ablation study, we randomly se-lected 20 topics from our dataset, resulting in atotal of 400 instances of varying context sizes, andobtained responses different prompts. While theno-restrict plots look similar to the semi-restrictprompts presented earlier, the strict plots showthat the models are entirely focused on contextualknowledge with very little parametric knowledge(supporting graphs in Appendix L). These resultsconfirm model sensitivity to prompts and highlightthat, with simple prompt adjustments, models can",
  "be guided to leverage different ratios of contextualand parametric knowledge": "Disregard Ambiguous SentencesWhen weused INFUSE to generate entailment scores, asmall number of sentences had scores close to 0.5,indicating lower confidence in categorizing themas entailed or not. We decided to exclude thesesentences to assess whether the patterns from Sec-tion 5.1 remained valid. After ignoring sentenceswith scores between 0.3 and 0.7 (see Appendix M),the patterns persisted, confirming the robustnessof our approach and the reliability of the INFUSEframework for this task.",
  "Related Work": "Contextual GroundingA persistent challengefor LLMs lies in reconciling contradictory or su-perseded information between the provided con-text and their internal knowledge base (Li et al.,2022; Zhou et al., 2023a), with most recent workexploring this interplay between local and globalknowledge in knowledge-conflict scenarios usingcounterfactual datasets (Si et al., 2022; Qian et al., 2023; Feng et al., 2024; Yang et al., 2024). Thepreference for contextual or parametric knowledgein counterfactual settings is not straightforward.Larger models seem better at adapting to counter-factual context, while smaller ones often prioritizetheir learned knowledge (Si et al., 2022). Whenfaced with counterfactual prompts, LLMs adjusttheir responses to align with the given context, evenif it contradicts their pretrained parametric knowl-edge (Li et al., 2022; Feng et al., 2024). In othercases, however, steering LLMs away from gen-erating outputs aligned with their vast but poten-tially inaccurate pretrained knowledge, even whencontradicted by the context, has been challenging(Zhou et al., 2023b). Our work contributes to con-textual grounding studies by prompting LLMs ina knowledge-consistent scenario and is the first tosystematically analyze how the amount of atomicinformation in a prompt affects context utilization.HallucinationThere has been extensive work onhallucination detection with methods falling intotwo general categories: internal parameter basedmethods (Chen et al., 2024; Su et al., 2024; Duanet al., 2024) and external response based methods(Manakul et al., 2023; Min et al., 2023; Yu et al.,2024; Manakul et al., 2023; Sun et al., 2024). Sim-ilar to our work, Hu et al. (2024) analyze the ten-dency of LLMs to generate factual hallucinationsin the presence of varying local context in ques-tion answering. Unlike our approach, they studythe effects of entirely switching out local context,whereas our approach incrementally increases theamount of context while comparing hallucinationtendency between these responses.",
  "Conclusion": "Understanding how LLMs handle different con-text sizes is crucial for developing robust mod-els. Our evaluation of nine widely used LLMswith our WikiAtomic dataset showed that all mod-els process context similarly, balancing contex-tual and parametric knowledge, while adjustingresponse lengths consistently. They never use allprovided contexts, always include some parametricknowledge, and prioritize information sequentially.As context increases, while contextual knowledgeremains similar, parametric knowledge becomesmore aligned with the context, and hallucinationrates decrease. These insights highlight the impor-tance of organizing context effectively and develop-ing models that utilize input more deterministically.",
  "Limitations": "Our work, while yielding some interesting findings,is not without limitations. Our method of extract-ing atomic sentences from Wikipedia articles oftensplit the source sentences into multiple, occasion-ally creating sentences that begin with an indirectreference to a subject. Because of the natural flowof presenting information in such a format, we didnot randomize the order. Further work could lookat how the order of contexts used by models com-pared when the provided contexts are shuffled.We utilized the INFUSE method to classifymodel response contexts into contextual andparametric. We empirically set the threshold fordetermining the categorization based on manualverification. Further work could look at more so-phisticated methods to set this threshold for evenbetter results. A limitation of the metric we adoptfor hallucination detection is that it relies on a sin-gle knowledge source Wikipedia as its knowledgesource. Information that is factually accurate, butnot present in the knowledge source could be incor-rectly classified as a hallucination. We thank the anonymous reviewers as well as themembers of PortNLP lab for their insightful com-ments that helped improve this paper. This researchwas supported by the National Science Foundationgrant SAI-P 2228783. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-min Bao, Harkirat Behl, Alon Benhaim, MishaBilenko, Johan Bjorck, Sbastien Bubeck, Qin Cai,Martin Cai, Caio Csar Teodoro Mendes, WeizhuChen, Vishrav Chaudhary, Dong Chen, DongdongChen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,Matthew Dixon, Ronen Eldan, Victor Fragoso, DanIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,Abhishek Goswami, Suriya Gunasekar, EmmanHaider, Junheng Hao, Russell J. Hewett, JamieHuynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Ma-houd Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-den, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,Arindam Mitra, Hardik Modi, Anh Nguyen, BrandonNorick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmi-lac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,Olli Saarikivi, Amin Saied, Adil Salim, Michael San-tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,Swadheen Shukla, Xia Song, Masahiro Tanaka, An-drea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,Yu Wang, Rachel Ward, Guanhua Wang, PhilippWitte, Haiping Wu, Michael Wyatt, Bin Xiao, CanXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-wen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technicalreport: A highly capable language model locally onyour phone. Preprint, arXiv:2404.14219.",
  "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He,Zongbo Han, Zheng Zhang, and Mike Zheng Shou.2024. Hallucination of multimodal large languagemodels: A survey. Preprint, arXiv:2404.18930": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Hanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do llmsknow about hallucination? an empirical investigationof llms hidden states. Preprint, arXiv:2402.09733": "Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Bal-achandran, Tianxing He, and Yulia Tsvetkov. 2024.Knowledge card: Filling LLMs knowledge gapswith plug-in specialized language models. In TheTwelfth International Conference on Learning Repre-sentations. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020.The pile: An800gb dataset of diverse text for language modeling.Preprint, arXiv:2101.00027. Benjamin Heinzerling and Kentaro Inui. 2021. Lan-guage models as knowledge bases: On entity repre-sentations, storage capacity, and paraphrased queries.In Proceedings of the 16th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics: Main Volume, pages 17721791, Online.Association for Computational Linguistics. Xinshuo Hu, Baotian Hu, Dongfang Li, XiaoguangLi, and Lifeng Shang. 2024.Does the genera-tor mind its contexts?an analysis of generativemodel faithfulness under context transfer. Preprint,arXiv:2402.14488. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. Preprint, arXiv:2311.05232. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023. Survey of halluci-nation in natural language generation. ACM Comput.Surv., 55(12). Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825. Yekyung Kim, Yapei Chang, Marzena Karpinska,Aparna Garimella, Varun Manjunatha, Kyle Lo,Tanya Goyal, and Mohit Iyyer. 2024. Fables: Evaluat-ing faithfulness and content selection in book-lengthsummarization. Preprint, arXiv:2404.01261.",
  "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021": "Hurdles to progress in long-form question answering.In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 49404957, Online. Association for Computa-tional Linguistics. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, XinWang, Michal Lukasik, Andreas Veit, Felix Yu,and Sanjiv Kumar. 2022. Large language modelswith controllable working memory. arXiv preprintarXiv:2211.05110.",
  "Yongqi Li, Mayi Xu, Xin Miao, Shen Zhou, andTieyun Qian. 2024. Prompting large language mod-els for counterfactual generation: An empirical study.Preprint, arXiv:2305.14791": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2024. Lost in the middle: How language mod-els use long contexts. Transactions of the Associationfor Computational Linguistics, 12:157173. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Liny-ong Nan, Ruilin Han, Simeng Han, Shafiq Joty,Chien-Sheng Wu, Caiming Xiong, and DragomirRadev. 2023. Revisiting the gold standard: Ground-ing summarization evaluation with robust humanevaluation. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics",
  "(Volume 1: Long Papers), pages 41404170, Toronto,Canada. Association for Computational Linguistics": "Shayne Longpre, Kartik Perisetla, Anthony Chen,Nikhil Ramesh, Chris DuBois, and Sameer Singh.2021. Entity-based knowledge conflicts in questionanswering. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 70527063, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2021. Fantastically orderedprompts and where to find them:Overcomingfew-shot prompt order sensitivity. arXiv preprintarXiv:2104.08786.",
  "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023": "SelfCheckGPT: Zero-resource black-box hallucina-tion detection for generative large language models.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages90049017, Singapore. Association for Computa-tional Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100, Singa-pore. Association for Computational Linguistics. Ella Neeman, Roee Aharoni, Or Honovich, LeshemChoshen, Idan Szpektor, and Omri Abend. 2022.Disentqa: Disentangling parametric and contextualknowledge with counterfactual question answering.Preprint, arXiv:2211.05655. Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language models as knowl-edge bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, China. Associationfor Computational Linguistics.",
  "Hallucination is inevitable: An innate limitation oflarge language models. Preprint, arXiv:2401.11817": "Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, andXindong Wu. 2024. Give us the facts: Enhancinglarge language models with knowledge graphs forfact-aware language modeling. IEEE Transactionson Knowledge and Data Engineering, PP:120. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao,Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xi-aohan Zhang, Hanming Li, Chunyang Li, ZheyuanZhang, Yushi Bai, Yantao Liu, Amy Xin, KaifengYun, Linlu GONG, Nianyi Lin, Jianhui Chen, ZhiliWu, Yunjia Qi, Weikai Li, Yong Guan, KaishengZeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, YuanYao, Ning Ding, Lei Hou, Zhiyuan Liu, Xu Bin, JieTang, and Juanzi Li. 2024. KoLA: Carefully bench-marking world knowledge of large language models.In The Twelfth International Conference on LearningRepresentations. Huajian Zhang, Yumo Xu, and Laura Perez-Beltrachini.2024. Fine-grained natural language inference basedfaithfulness evaluation for diverse summarisationtasks. In Proceedings of the 18th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages17011722, St. Julians, Malta. Association for Com-putational Linguistics.",
  "AWikipedia Article Contexts ExtractionPrompt": "Figure A.1 shows the exact prompt we used toextract atomic facts from Wikipedia articles. Wehave tested multiple versions of prompts to breakdown sentences into atomic sentences. We foundif we only asking the model directly to break downsentences to atomic sentences, or only adding thedefinition of atomic sentence and asking it to breakdown sentences. The model performed very poorly,the main problem with these approaches was thisversion of atomic sentences werent atomic enough.For example:April is the fourth month of the year inboth the Julian and Gregorian calendars.This sentence could be treated as a single infor-mation. But this definitly could be further brokedown to smaller pieces:1.April is the fourth month of theyear in the Julian calendar2. April is the fourth month of the yearin the Gregorian calendar.Figure A.1s prompt was the most effective ver-sion in our experiments, it could handle the above example very well and at the same time add theappropriate subject to each atomic sentences. Weused this version of prompt also to break downresponses from each model into atomic sentences(Figure A.2).",
  "CImplementation Detail": "Here we go through implementation details, modelparameters. We used default parameters acrossall models where temperature and top_p set to 1,presence_penalty and frequency_penalty set to 0.For atomic sentence extraction both for Wikipediaarticles and models responses, we set GPT4-osthe max_tokens to 2048 because the output is aJSON object that contains 50 atomic sentences.We think this number is sufficient for this size ofoutput. When generating responses from differentmodels once we have all the questions set up, weset the max_token to 512 to mimic the real worldapplication setting.API calls were made to OpenAI and Anthropicto obtain responses from GPT-4o, Claude 3 Opus,Sonnet, and Haiku, completing in 4 hours. Forlarge open-source models Mixtral 8x22B andLlama 3 70B, we used a third-party service, whichtook 2 hours. The INFUSE evaluation of responsesfrom 9 LLMs was conducted on 9 A100 GPUs over10 hours. Inference for smaller models (Llama 3 8Band Phi 3) was performed on an M1 Ultra, taking 5hours.",
  "LFurther Analyses": "This section shows the difference in the models useof local contexts when using a strict vs unrestrictedprompt (Figure L.13, L.14 and L.15). The strictprompt instructs the model to only use the providedcontext when answering the question. The unre-stricted prompt provides the context but doesntinstruct the model to use it in any way."
}