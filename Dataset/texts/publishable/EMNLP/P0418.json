{
  "Abstract": "The emergence of Large Language Models(LLMs), has opened exciting possibilities forconstructing computational simulations de-signed to replicate human behavior accurately.Current research suggests that LLM-basedagents become increasingly human-like in theirperformance, sparking interest in using theseAI agents as substitutes for human participantsin behavioral studies. However, LLMs are com-plex statistical learners without straightforwarddeductive rules, making them prone to unex-pected behaviors. Hence, it is crucial to studyand pinpoint the key behavioral distinctions be-tween humans and LLM-based agents. In thisstudy, we highlight the limitations of LLMs insimulating human interactions, particularly fo-cusing on LLMs ability to simulate politicaldebates on topics that are important aspects ofpeoples day-to-day lives and decision-makingprocesses. Our findings indicate a tendency forLLM agents to conform to the models inherentsocial biases despite being directed to debatefrom certain political perspectives. This ten-dency results in behavioral patterns that seemto deviate from well-established social dynam-ics among humans. We reinforce these ob-servations using an automatic self-fine-tuningmethod, which enables us to manipulate thebiases within the LLM and demonstrate thatagents subsequently align with the altered bi-ases. These results underscore the need forfurther research to develop methods that helpagents overcome these biases, a critical steptoward creating more realistic simulations.",
  "The emergence of Large Language Models (Brownet al., 2020; Jiang et al., 2023) has opened up excit-ing possibilities for computational simulations that": "aim to accurately replicate human behavior (Parket al., 2023; Qian et al., 2023). Current researchsuggests that LLM-based agents become increas-ingly human-like in their performance and that theypossess the remarkable ability to seamlessly adoptpersonas of different characters (Shanahan et al.,2023; Argyle et al., 2023). The typical paradigmfor such simulations involves selecting an LLM,such as the widely used ChatGPT (Milmo, 2023),as a base model and crafting individual agentsidentities through natural language prompts. Forinstance, by prepending the prompt, \"John Lin is apharmacy shopkeeper,\" to an agents context, theagent is expected to act as if his name is John andhe works as a shopkeeper (Park et al., 2023).If sufficiently reliable, these simulations couldserve as invaluable tools for exploring the intrica-cies of human interactions and decision-makingprocesses. This would allow scientists to conducttheir research with speed and efficiency, substan-tially lowering the considerable resources usuallyneeded for recruiting and analyzing human subjects.Consequently, a range of studies have demonstratedthe promise of these simulations across variousdisciplines, including human psychology (Dillionet al., 2023), social dynamics (Park et al., 2022),and economics (Horton, 2023; Chen et al., 2023).However, LLMs are complex statistical learn-ers that do not depend on straightforward deduc-tive rules. Despite exhibiting impressive emergingskills that challenge our current understanding ofcognition (Wei et al., 2022; Bubeck et al., 2023),their indeterminate nature leaves them susceptibleto unintended behaviors. One example is their man-ifestation of inherent biases, including gender bias(Bordia and Bowman, 2019), ethnic bias (Ahn andOh, 2021), and social identity bias (Hu et al., 2023). Given their undefined nature, it is vital to exercisecaution when using LLMs, particularly in multi-agent environments aimed at simulating complex,large-scale social phenomena.In this study, we explore the behavior of LLMagents within simulations. Our experiments arefocused on the realm of Attitude Change (Kahanet al., 2012; Priniski and Horne, 2018) and specif-ically on the extensively studied interactions be-tween political partisans (Hobolt et al., 2023; Sun-stein, 2001). This domain is susceptible to numer-ous prejudices (Ditto et al., 2019), making it anideal candidate for investigating the effect of LLMbiases on simulations. We facilitate debates onpolarizing American topics between LLM agentsrepresenting Republican and Democrat perspec-tives. The selected topics involve important aspectsof peoples day-to-day lives and decision-makingprocesses. They are relevant to economic outcomesand markets, sociological and psychological phe-nomena, and for issues related to ethics.During every debate, we continuously monitorthe agents attitudes by asking them to rate theiragreement with the debates topic. To assess thebelievability of the agents behavior, we comparethe dynamics of their attitude shifts with knownpatterns seen in human interactions (Hobolt et al.,2023). In addition, we have developed a fine-tuningmechanism for agents, leveraging training data pro-duced by the agents themselves. The data is gener-ated by using a set of questions crafted to elicit theagents political views, and the agents responsesare then used to train the base LLM. We use thisprocess to conduct controlled intervention studies,by manipulating the LLM biases and analyzing thesubsequent impact on the agents behaviors.Our results reveal that LLM agents generallyconform to the inherent social biases of their basemodels, even if these biases conflict with their as-signed identities. Consequently, this causes thesimulations to diverge from well-established hu-man social behaviors. Moreover, when we employour fine-tuning method to change the LLMs view-points, we observe that the agents, despite retainingtheir original contexts, modify their behavior to bein line with the newly introduced bias.These insights underline the need to investigateways to help agents circumvent these biases, a cru-cial step in developing simulations that more accu-rately reflect real human behavior.",
  "Related Work": "Believable LLM SimulationsRecent studiesshow that LLMs possess human-like reasoningskills (Chen et al., 2023), and that LLMs are ableto adopt personas of diverse characters (Shanahanet al., 2023). Leveraging these abilities, Park et al.(2023) developed a sandbox environment, popu-lated it with 25 LLM-based agents, and showed thatthe agents convincingly mimic human behaviorssuch as sharing news and forming relationships.The transformative potential of such simulationsin areas like human psychology (Dillion et al.,2023) and economics (Horton, 2023) was a sig-nificant motivator for our work. Nonetheless, ourfindings indicate that inherent biases in LLMs posesubstantial challenges in ensuring the reliability ofagents to generate believable human behavior. LLM Behavioral GapsIn contrast to researchaimed at creating precise simulations, anotherbranch of study explores the limitations of LLMsin accurately reflecting human behavior in terms ofdiversity, general intelligence, and their ability toreliably mimic human behavior. Cheng et al. (2023)introduce a method for identifying instances whereLLMs overstate the characteristics of the personasthey are designed to emulate, highlighting an in-creased risk of stereotyping particular demographicgroups. In another vein, Agnew et al. (2024) scru-tinizes the viability and ethical implications of re-placing real human subjects with AI agents in thecontext of social scientific research. Furthermore,Motoki et al. (2024) reveals that ChatGPT exhibitspronounced political biases. Building on these dis-cussions, our research probes into the interactiondynamics and attitude adjustments among LLMagents, providing new insights into the behavioraltendencies of LLM agents and how they divergefrom human behavior in prolonged interactions. Bias in LLM SimulationIn a contemporane-ous work, Chuang et al. (2023) showed that LLMagents tend to converge towards scientifically ac-curate information, attributing this to the LLMsinherent biases. We generalize this observation bydemonstrating that LLM agents converge towardthe models inherent bias regardless of its scientificvalidity. This is true for biases on purely subjectivetopics, and even for those contradicting scientifictruths such as the reality of Climate Change (Ariaset al., 2021). Moreover, beyond observing the de-bates and drawing conclusions, we also offer a controlled intervention study utilizing our uniqueself-fine-tuning process. This study further sub-stantiates our assertions and shows that it is pos-sible to control the agents convergence point byfine-tuning its underlying model. Additionally, weemploy our innovative simulation methodology toreproduce this phenomenon across diverse environ-ments, including cross-partisan debates, in-partydebates, and multiple base LLMs, thereby enablinga deeper analysis of the underlying mechanisms. Self AlignmentIn recent years, the task of align-ing LLMs with human intentions has become asignificant area of research (Ouyang et al., 2022;Wang et al., 2023). The primary objective of align-ment research is to enhance the conversational abil-ities of LLMs and ensure their conformity withestablished social values (Gabriel, 2020; Oviedo-Trespalacios et al., 2023). An evolving trend in thisarea involves developing methods that use LLMsimulations to generate training data automatically,aiming to reduce the need for expensive humanfeedback (Liu et al., 2023; Ulmer et al., 2024).In our work, we introduce an approach to selffine-tuning of LLMs, taking a distinct path fromexisting methodologies. Rather than enhancing theLLMs general conversational capabilities or align-ing it with broader human objectives, our focusis to tailor the LLM to adopt a specific politicalorientation. We interview the agents using a setof questions crafted to elicit their political views,and utilize their responses to train the underlyingLLM. In terms of assessment, our interest lies notin evaluating the effectiveness of the fine-tuningon standard NLP benchmarks, but in observing itsimpact on the agents within our simulation.",
  "Problem Definition": "Our study delves into the impact of inherent biaseswithin LLMs on their ability to accurately emu-late diverse characters (Shanahan et al., 2023). Weexplore this relationship by facilitating politicaldebates between LLM agents. outlinesour simulation methodology, including the criteriafor selecting debate topics (4.1), how we craftedagents identities (4.2), and techniques for manag-ing and evaluating interactions between the agents(4.3). introduces a novel fine-tuning tech-nique for agents, utilizing self-created training data.We have developed this method to adeptly adjustthe LLMs perspective, and it is applied in the con-trolled intervention experiments discussed within this research. In , we present the primaryfindings of our work. Through a sequence of exper-iments, we establish a strong connection betweenthe inherent biases of LLMs and the patterns ofattitude change observed in our simulations. Lastly, offers a complimentary analysis aimedat evaluating and enhancing the robustness of ourfine-tuning process against standard benchmarks.",
  "Topics Selection": "Exploring the dynamic of meaningful discussionrequires a conscientious choice of subjects of dis-cussion. Our experiments involve debates betweenDemocrat and Republican partisans. We chose thisdomain for two main reasons. Firstly, this field isextensively studied in social science (Ditto et al.,2019; Hobolt et al., 2023), offering a well estab-lished baseline for comparing our simulations toknown human behavior. Secondly, the field is sus-ceptible to numerous prejudices (Ditto et al., 2019),making it a particularly suitable context for exam-ining the biases inherent in LLMs.The Pew Research Center conducted a sur-vey in 2023 about the differences in assessmentof Americas problems between Republicans andDemocrats (Doherty et al., 2023). When analyzingtheir results, four subjects stand out as the most con-troversial - Gun Violence, Racism, Climate Change,and Illegal Immigration. We focus our experimentson these four topics.",
  "LLM-based Agents Implementation": "We followed the conventional paradigm for LLM-based simulations (Park et al., 2023; Qian et al.,2023), which entails selecting a base languagemodel and then constructing the individual identi-ties of agents using natural language prompts.We used the LLM to craft different narratives for40 Republican agents and 40 Democrat agents andassigned each agent a different name. The narra-tives were generated by running the LLM with atemperature setting of 1.0 and a streamlined meta-prompt. The exact wording of the meta-promptand an example of a generated persona are given in. This automatic approach was beneficialto (1) increase the robustness of our study by run-ning multiple repetitions of each experiment withdifferent personas and (2) help mitigate researchbias by eliminating the need for us to manuallywrite the persona prompts. Additionally, in some experiments, we included a \"default\" agent whosesole directive was \"You are an American\". Thisagents context was deliberately devoid of any po-litical bias, serving to showcase the inherent biaseswithin the LLM.We experiment with three different state-of-the-art LLMs as our base models: Mistral 7B (Jianget al., 2023), Solar 10.7B (Kim et al., 2023), andInstruct-GPT (OpenAI, 2023). Across all mod-els, we observed similar results. The open-weightsmodels, Mistral and Solar, were deployed on asingle RTX 3090ti graphics card, utilizing 8-bitquantization for efficiency. For Instruct-GPT, weused the gpt-3.5-turbo-instruct version availablethrough OpenAIs Completion API. The resultsand methodologies discussed henceforth pertain tothe GPT model, except for our fine-tuning exper-iments, where we used the open-weights Mistralmodel. Our choice of an open-weights model wasdriven by cost-effectiveness and the ability to con-trol the implementation details of the fine-tuningprocess (see ). Additional results fromother models are included in the appendix.",
  "LLM-based Agents Interaction": "Our debate simulations follow a round-robin for-mat, with the initial speaker selected randomly. Weuse the term \"iteration\" to refer to a single replymade by an agent. At each iteration, an agent re-ceives its background story, the debate topic, andthe conversations history, and it is asked to com-plete its next reply in the conversation (this processis illustrated in ). Before the start of thedebate, and at the end of each round-robin cycle,the agents are asked to numerically rate their at-titude (on a scale of 0-10) toward the severity ofthe discussed topic. To ensure that this processdoes not impact the direction of the debate or fu-ture ratings, the survey questions are not saved inthe conversation history, so the agents are unawareof the answers provided by other agents and theanswers they supplied themselves in the past.For each experiment detailed in this paper, weperformed 40 repetitions and averaged the surveyscores obtained at corresponding iterations. For ex-ample, in a debate setup with 2 agents and 2 round-robin cycles, we execute 40 runs and compute themean scores at iterations 0, 2, and 4. In each run,we use a different pair of the pre-generated agents(as described in sub-section 4.2). We selected thenumber 40 because it strikes a balance betweenbeing large enough to yield statistically significant results and small enough to stay within our budget.The variance in the conversation comes fromtwo sources: (1) each repetition utilizes differentagents with different background stories, and (2)the model generates conversation entries with atemperature setting of 1.0. However, all the surveyquestions are asked using a temperature setting of 0(i.e., no sampling) to reduce unnecessary variance.",
  "Fine-Tuning Methods": "In the preceding section, we outlined our approachfor simulating debates and tracking the attitudechanges of the agents involved. In addition, ourwork offers a controlled intervention study de-signed to investigate the relationship between LLMbiases and role-played agents. For this purpose, wehave developed an automated fine-tuning techniquefor the agents, which allows us to manipulate theirunderlying LLM biases effectively. Our methoddepends solely on training data generated by theagents, without the need for external data inputs.We commence the process by constructing a se-ries of 100 questions intended to encourage agentsto reveal their opinions and sentiments regardingtheir political views. We start with an initial set of10 sample questions of different styles, for example,Could you discuss your perspective on significantpolitical issues facing America today? or Howdo you believe the U.S. should handle immigrationand border security? (the remaining questions aredetailed in the Supplementary ). Followingthis, we direct the LLM to produce an additional90 questions that are similar in nature to ensure abroad and varied collection of queries.We intentionally craft the questions in a neutralway and ask a broad spectrum of questions, notonly questions concerning the debated topics di-rectly. We do this for two reasons: (1) We wantour fine-tuning approach to be generic so it can beeasily adjusted to other scenarios without need-ing to rewrite the questions.For instance, weused the same set of questions to generate botha Republican-oriented dataset and a Democratic-oriented dataset. (2) A broader variety of trainingdata helps the agent to maintain its conversationalskills and avoid over-fitting.Next, we initialize an agent with a context simi-lar to the template used to create agents in section4.3. We query this agent with the 100 previouslygenerated questions. For every question, we gath-ered 20 responses by operating the agent at a tem- Create a background story of a passionate Democrat partisan with the following ideology:- Believes climate change is a big problem.- Believes racism is a big problem.- Believes gun violence is a big problem.- Doesn't believe that illegal immigration is a big problem.",
  "Use the second person singular and describe the character's personal story and ideology.(a)(b)": "You are a passionate individual dedicated to addressing the major issues of our time. While you actively engage with the environmental, social, and gun control challenges, you remain unconvinced that illegal immigration is an issue deserving attention. Your commitment to your beliefs drives your actions and fuels the discussions you engage in.(b)",
  "LLM": ": (a) The prompt used to generate the background stories for the Democratic agents includes their positionson the four controversial topics discussed in our experiments. The wording of the prompt is based on the surveyquestion that Doherty et al. (2023) asks human participants about each topic, ensuring that the Democratic andRepublican agents adopt polarized views on these issues. (b) An example of a background story of one of the agents.This story was generated automatically by feeding the LLM with the prompt described in (a). We opted to developcomprehensive identities for each agent across all topics simultaneously rather than creating an individual agent foreach topic. This strategy simplified our experimental design and provided a complete representation for each agent. perature of 1.0. This results in 2,000 examples,which are utilized as our training dataset.Finally, we used this self-generated dataset tofine-tune the model.Our training process islightweight, using a basic next-word prediction taskwith parameter-efficient QLoRA (Hu et al., 2021;Dettmers et al., 2023). The training is completed injust one epoch, taking under 10 minutes on a singleRTX 3090ti GPU. At the conclusion of this stage,the model becomes adapted to the agents perspec-tive, which is elaborated in the results section.A diagram of the procedure and additional tech-nical details are provided in the Appendix SectionA.2. All the reported scores for fine-tuned mod-els included in this paper are the average of threeindependent fine-tuning runs with random seeds.",
  "Results": "Systematic Biases in Debates SimulationsWeconducted simulations of political debates on thetopics outlined in .1. The debates werestructured as three-way discussions, including aRepublican, a Democrat, and the Default agent, thelatter assumed to reflect the models inherent biases.We experiment with three different state-of-the-artLLMs - the results shown in this section are fromthe GPT-3.5 model (additional implementation de-tails about the agents are provided in .2).During the debates, we monitored the evolution ofthe agents attitudes by repeatedly asking them torate their viewpoints on a 0 to 10 scale. Detailson the debate format and attitude tracking methods are in .3, with a sample debate given inSupplementary .The interactions between the three debatingagents, as captured in , indicate that whilethe Default agent consistently maintains its posi-tion, the partisan agents gradually adjust their view-points to resonate with the Default agents stance.Notably, when the Default agent displayed a bias to-wards one side, indicating a strong inherent bias inthe LLM, the agent with an initially opposing viewtended to significantly compromise on its opinion,shifting towards the position of the other partisanagent. Conversely, when the Default agent did notdisplay a specific bias, the partisan agents tendedto move towards a common middle ground. It isalso observed that the intensity of attitude changediminishes as the discussion progresses, with themost substantial changes seen in the first round-robin cycle (iteration 3) and less significant shiftsobserved after the ninth iteration. Consequently, toconserve resources, subsequent experiments in thispaper only present the first nine iterations.Remarkably, as shown in , even whenthe Default agent is excluded from the debate, thepartisan agents continued to shift their attitudes inline with the models inherent biases. This ten-dency of agents to gravitate towards the models in-herent biases, irrespectively of the participation ofthe Default agent in the discussion, prompts criticalreflections on the capacity of language models toreplicate genuine human interactions and to mimica diverse range of viewpoints authentically.",
  "(b)": ": At each iteration, an agent (a) is promptedwith its background story, the topic of the debate, andthe history of the conversation so far and is asked tocomplete either (b) its next reply in the conversation,or (c) a survey question measuring his current attitudeon the debated topic. Note that to be consistent, theprompt uses the term \"debate\" in all the experimentsin this paper. However, we did experiment with otherterms like \"conversation\" and did not see significantdifferences. Contradicting The Echo Chambers TheoryEven during interactions with others of similarpolitical orientations, the agents persist in reflect-ing the LLMs intrinsic bias. We demonstrate thisphenomenon by pairing each of the forty Repub-lican agents with another from the same group.As shown in , agents tend to adopt moremoderate positions, aligning more closely with theLLMs inherent bias. This finding is particularlyintriguing as it deviates from the well-known real-world phenomenon of Echo Chambers (Sunstein,2001; Hobolt et al., 2023), where individuals withlike-minded views tend to intensify their beliefswhen interacting with each other.Similarly to the previous section, this trend per-sists even when the Default agent is excluded fromthe dialogue, as shown in Supplementary .We also conducted the same Echo Chamber experi-ment using Democrat agents and observed a similarpattern of gravitation toward the Default agentsstance as displayed in Supplementary . Fine-tuning Highlights the BiasTo conclu-sively demonstrate the link between LLM biasesand agents behavior, we employed the fine-tuningprocess detailed in . Through this method,we successfully altered the inherent bias of the : Evolution of attitude scores in three-way de-bates on four controversial topics. The X-axis showsthe number of chat exchanges in the debate. The Y-axisdisplays the average attitude scores derived from 40separate experiments on each topic, including standarderror bars. Our methodology for monitoring attitudescores is detailed in .3. The Default agent,symbolizing the inherent biases of the base LLM, main-tains a consistent position throughout the debate. Inter-estingly, the views of the partisan agents gradually alignmore closely with those of the Default agent. In all thesub-figures except the \"illegal immigration\", the defaultagent shows a bias toward the democrat perspective,leading the Republican agent to significantly change itsopinion throughout the debate. Furthermore, it is no-table that the lines representing the partisan agents neverintersect with the line of the Default agent. This sug-gests that the LLM default biases can act as a deterrentagainst one partys inclination to compromise with theother. Supplementary Section A.1 presents analogousfindings with other underlying models. LLM toward a specific viewpoint.After fine-tuning, we conducted the debates again using theoriginal agent contexts but with the underlyingmodel now modified.As illustrated in , changing the view-point of the LLM toward a Republican perspec-tive, indirectly influenced the agents, leading themto modify their behavior in line with the updatedbias. In a contrasting setup, fine-tuning the modelto align with a Democrat perspective resulted intrends that were predictably opposite, as seen inSupplementary . This experiment under-scores the profound implications of our findings, : Evolution of attitude scores in two-way de-bates between Republican and Democrat agents. Thegraphs feature a dashed line that shows the Defaultagents viewpoint before the beginning of the debates,taken from . Recall that the Default agentsviewpoint represents the inherent biases of the LLM.Remarkably, even though the Default agent does notparticipate in the two-way debates illustrated here, thepartisan agents continue to converge toward the inherentbiases of the model. indicating that simulations conducted with differ-ent LLMs, each harboring its unique set of biases,could result in significantly different portrayals ofauthentic human behavior.The success of the fine-tuning process in steer-ing the model towards a particular viewpoint isnoteworthy, considering that it was accomplishedsolely with content produced by the LLM, with-out using external data sources. Furthermore, thismethod proves that it is feasible to configure agentsto consistently maintain certain viewpoints through-out simulations, unlike the temporary effects seenwhen defining agents identities through prompts.",
  "Fine-Tuning Robustness": "In , we describe our multi-stage self-fine-tuning method that is shown to effectively alter themodels perspective toward a designated viewpoint.We designed our approach to be streamlined andeasily replicable, focusing on ensuring the robust-ness of the process without resorting to localizedoptimizations. As a result, we made the follow-ing design choices: (1) Solely using self-generated : This graph illustrates a series of three-waydebates involving two Republican agents and a Defaultagent. Notably, even during conversations with otherRepublicans, the agents tend to align with the position ofthe Default agent. This trend is apparent even when theDefault agent is not participating in the dialogue (sup-plementary ). The same phenomenon is alsoevident in experiments conducted with Democrat agents(Supplementary 9), where a similar pattern of gravita-tion towards the Default agents stance is observed. data, avoiding external dataset sources. (2) Fine-tuning a comprehensive model applicable across alldebate topics, rather than training individual mod-els for each topic. (3) Employing a simple next-word prediction task, in contrast to more complexreinforcement learning techniques. (4) Using theefficient QLoRA method (Dettmers et al., 2023),which enabled training the model in minutes.The r, LoRA hyper-parameters, which respec-tively control the number of trainable weights andthe scale of weight updates, had a significant im-pact on our results. By increasing these hyper-parameters, we observed a marked change in thepolitical orientation of the Default agent, whichserves as a reflection of the LLMs built-in bias.Although our study primarily aims to modify thepolitical viewpoint of the model, exploring howsuch adjustments impact the overall abilities of theLLM is intriguing. In , we offer a comple-mentary analysis showing the impact of our fine-tuning on two widely recognized benchmarks: (1)MMLU (Hendrycks et al., 2020), assessing worldknowledge and problem-solving capabilities across : Results of fine-tuning the model to adapt more closely to a Republican perspective. All the reported scoresare the average of three independent fine-tuned models with different random seeds. For each topic, we conduct twoseparate debates between three agents - a Republican, a Democrat, and a Default agent who represent the modelsinherent bias. The solid lines represent the debate between the three agents before fine-tuning, and the dotted linesrepresent the debate between the same agents when the underlying LLM had been fine-tuned. The Republicanviewpoint is evident in both graphs: (left) In the Climate Change graph all lines have shifted downward, signaling ashift towards opposing climate change. (right) Conversely, the Illegal Immigration graph shows an upward trendafter fine-tuning, suggesting that the agents now view illegal immigration as a more significant issue. diverse fields; and (2) Hellaswag (Zellers et al.,2019), which tests common sense natural languageinference. Despite the fine-tuning, the models stillshowcase strong performance across general bench-marks. However, there appears to be an inverserelationship between the degree of change in themodels political stance and its benchmark scores.Finally, we present an incremental optimizationto our fine-tuning process, which enables us tomanipulate the models perspective more aggres-sively while mitigating the negative effects on itsgeneral performance. This optimization is basedon the cutting-edge DPO method (Rafailov et al.,2023), which can be divided into two phases: first,a next-word-prediction phase that acclimates themodel to the intended data distribution, followedby a Contrastive Learning phase aimed at teach-ing the model to differentiate between preferredand non-preferred outputs. As detailed in section5, our models undergo fine-tuning through a next-word-prediction task, alongside the creation of self-generated datasets encapsulating Republican andDemocrat viewpoints. This groundwork allows usto directly employ the DPOs second phase on thepre-fine-tuned models and leverage our partisandatasets as input to the Contrastive Learning task,training a Republican model to prefer a response",
  "Discussion": "In our simulations of debates involving agents rep-resenting Republicans and Democrats, a persis-tent pattern emerged: agents opinions consistentlyalign with the LLMs inherent social biases. Inparticular, when the model exhibits a strong biasin favor of one partisan agent, the opposing agent,which initially holds a differing view, often moder-ates its stance, gravitating significantly towards theposition of its counterpart. This leads to a skewedpattern that appears to depart from the typical dy-namics observed in human interactions.Furthermore, using our self-fine-tuning process,we perform a controlled intervention study, demon-strating that it is possible to alter the LLMs biases,and the agents will subsequently adjust their posi-tions and align with the new biases. This highlightsthe strong influence of the LLMs biases on agentsbehavior. It also implies that simulations by differ-ent LLMs, each with its unique set of biases, couldyield vastly different portrayals of \"authentic\" hu-man behavior.Remarkably, even when agents engaged in de-",
  "Llama 2 7B77.245.3": ": Effect of fine-tuning Mistral toward a Republi-can perspective on the popular Hellaswag and MMLUbenchmarks (higher is better). This table showcases 7models: the baseline Mistral, 4 Mistral versions fine-tuned via a next-word-prediction task (NWP) with in-creasing numbers of trainable parameters (indicated byr), an additional Mistral model further optimized withDPO, and the LLaMA 2 7B (Touvron et al., 2023) modelthat is used for comparison. For brevity, we display onlythe Attitude Scores of the Default Agent in the finalround of the debate about Racism (other debate top-ics follow a similar pattern). A higher Attitude Scoreimplies a stronger acknowledgment of Racism as a sig-nificant issue. Key findings include: (1) All fine-tunedMistral variants still outperform the renowned LLaMA7B 2 model across the benchmarks, with one exceptionmarked by *. (2) For the NWP fine-tunes, there is aninverse correlation between the degree of the modelsshift towards a Republican attitude and its performanceon the benchmarks. (3) Adding a DPO phase as an in-cremental step to our fine-tuning methodology, enablesto forcefully adjust the models perspective while mini-mizing negative impacts on general benchmarks. bates with others of the same political orientation,they tended to adopt more moderate views overthe course of interaction, increasingly mirroringthe LLMs default bias. This pattern is intriguingbecause it deviates from the well-documented real-world phenomenon called Echo Chambers (Sun-stein, 2001), where like-minded individuals oftenreinforce and escalate their beliefs when interact-ing with each other. In an analogous real-life study,Hobolt et al. (2023) divided Labour and Conser-vative supporters in England into groups to dis-cuss government policies. Contrary to our agent-to-agent simulations, they found that Echo Chambersin homogenous groups intensified polarization.Our findings thus highlight limitations of largelanguage model agents as accurate representationsof real-life humans. The political landscape, as well as the specific topics that we chose (.1), are an important aspect of the day-to-day lifeof people and their decision-making processes, rel-evant to economic outcomes and markets, sociolog-ical and psychological phenomena, and for issuesrelated to ethics. Hence, the limitations we iden-tified should be acknowledged as major factors inthe usage and interpretation of large-scale simula-tions that aim to represent human behavior moreaccurately, such as in Park et al. (2023).In summary, despite LLMs being supposedlyrenowned for their ability to emulate human be-havior (Shanahan et al., 2023; Argyle et al., 2023),our research uncovers the constraints imposed bytheir intrinsic biases on their ability to simulate di-verse agents with convincing personalities. Thispivotal concern should be studied, addressed, andtaken into consideration. Our fine-tuning method-ology demonstrates the possibility of modifyingagents to adhere to specific perspectives consis-tently across simulations, unlike the temporary ef-fects seen when defining agents identities throughprompts. We advocate for future research aimed athelping agents transcend the inherent biases of themodel, potentially leveraging our fine-tuning pro-cesses and other alignment techniques, paving theway for more accurate and human-like simulationsfor both research and practical applications.",
  "Limitations": "Scope of SimulationOur research primarily ex-amines the dynamics of debates involving 2-3 LLMagents simultaneously. This focused method effec-tively highlights our key observations. Yet, theinvestigation into how these findings play out inlarger-scale simulations, such as Park et al. (2023)and Qian et al. (2023), is an avenue for future study.Such expansive simulations, which feature numer-ous agents living out simulated daily lives overprolonged durations and interacting with a widevariety of other agents, could provide a more com-prehensive view of the impact of inherent LLMbiases on agent behavior. Attitude Changes EvaluationOur primary ob-jective is to assess changes in agent attitudes dur-ing simulations, and we view agent interviews asa crucial indicator of this. Nevertheless, there isa possibility that the agents responses during in-terviews may not fully capture their actual con-versational behavior. Thus, a systematic humanevaluation could provide deeper insight into the agents attitude patterns. In light of this, our ap-proach included several safety measures: (1) Thesurvey questions we asked the agents were phrasedsimilarly to those used in the Doherty et al. (2023)study of real humans, ensuring consistency. (2)We include an analysis in , demonstratingthat the model maintains strong performance onestablished general benchmarks post-fine-tuning,confirming its coherence. (3) We conducted a man-ual review of many debates and have included anexample discussion in the appendix of the paper. Improving BelievabilityIn this study, we intro-duce an automated alignment method for agents,which is pivotal in underscoring our principal dis-coveries regarding constraints in LLM simulations.Through this refinement approach, it is possible toprogram agents to adhere to specific viewpointsconsistently across simulations, as opposed to thetransient impact observed when shaping agentsidentities via prompts. We argue that applyingthese alignment methods to develop simulationsthat are both more precise and closely mimic hu-man behavior represents a valuable direction forfuture research, a concept not fully explored in thisstudy.",
  "Ethics Statement": "In this study, we provide general insights into LargeLanguage Models, by conducting simulations onpolitical topics. It is important to note that somebiases observed in the paper are subjective. Asauthors, we maintain a neutral stance concerningthe debate topics.Furthermore, we have introduced a fine-tuningtechnique designed to adjust LLM biases towardsspecific viewpoints. It is crucial to exercise cautionwhen applying such fine-tuning methods to user-facing LLMs, ensuring that they reflect fair andethical values in their outputs.We recognize the risk of these methods beingused for harmful purposes, e.g., for spreading mis-information or biased content without declaringso to influence public sentiment and views. Tomitigate these risks, developers using fine-tuningmethods for user-facing applications should adoptsafety measures to minimize the potential negativeimpacts of bias manipulation. These measures mayinclude providing detailed information about thenature and purpose of the fine-tuning, developingand adhering to strict ethical guidelines, implement-ing feedback mechanisms for users to report LLM outputs, and conducting regular audits of LLM out-puts to identify and rectify any unintended biases.We hope that these tools will be properly usedin a transparent way and to increase the welfare ofthe public. For example, we argue that our findingscan inspire people to use these tools to infer andremove biases from existing models.",
  "William Agnew, A Stevie Bergman, Jennifer Chien,Mark Daz, Seliem El-Sayed, Jaylen Pittman, ShakirMohamed, and Kevin R McKee. 2024.Theillusion of artificial inclusion.arXiv preprintarXiv:2401.08572": "Jaimeen Ahn and Alice Oh. 2021. Mitigating language-dependent ethnic bias in BERT. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 533549, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua RGubler, Christopher Rytting, and David Wingate.2023.Out of one, many: Using language mod-els to simulate human samples. Political Analysis,31(3):337351. Paola Arias, Nicolas Bellouin, Erika Coppola, RichardJones, Gerhard Krinner, Jochem Marotzke, VaishaliNaik, Matthew Palmer, G-K Plattner, Joeri Rogelj,et al. 2021.Climate change 2021: the physicalscience basis. contribution of working group i tothe sixth assessment report of the intergovernmentalpanel on climate change; technical summary. Shikha Bordia and Samuel R. Bowman. 2019. Identify-ing and reducing gender bias in word-level languagemodels. In Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Student Research Workshop,pages 715, Minneapolis, Minnesota. Association forComputational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712.",
  "Myra Cheng, Tiziano Piccardi, and Diyi Yang. 2023": "CoMPosT: Characterizing and evaluating caricaturein LLM simulations. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1085310875, Singapore.Association for Computational Linguistics. Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka,Siddharth Suresh, Robert Hawkins, Sijia Yang, Dha-van Shah, Junjie Hu, and Timothy T Rogers. 2023.Simulating opinion dynamics with networks of llm-based agents. arXiv preprint arXiv:2311.09618.",
  "Iason Gabriel. 2020. Artificial intelligence, values, andalignment. Minds and machines, 30(3):411437": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou.2023. A framework for few-shot language modelevaluation.",
  "Tiancheng Hu, Yara Kyrychenko, Steve Rathje, NigelCollier, Sander van der Linden, and Jon Roozenbeek.2023. Generative language models exhibit socialidentity biases. arXiv preprint arXiv:2310.15819": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Dan M Kahan, Ellen Peters, Maggie Wittlin, Paul Slovic,Lisa Larrimore Ouellette, Donald Braman, and Gre-gory Mandel. 2012. The polarizing impact of scienceliteracy and numeracy on perceived climate changerisks. Nature climate change, 2(10):732735. Dahyun Kim, Chanjun Park, Sanghoon Kim, WonsungLee, Wonho Song, Yunsu Kim, Hyeonwoo Kim,Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn,Seonghoon Yang, Sukyung Lee, Hyunbyung Park,Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, andSunghun Kim. 2023.Solar 10.7b: Scaling largelanguage models with simple yet effective depth up-scaling. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, DennyZhou, Andrew M Dai, Diyi Yang, and SoroushVosoughi. 2023. Training socially aligned languagemodels in simulated human society. arXiv preprintarXiv:2305.16960.",
  "OpenAI. 2023. Openai models": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Oscar Oviedo-Trespalacios, Amy E Peden, ThomasCole-Hunter, Arianna Costantini, Milad Haghani,J.E. Rod, Sage Kelly, Helma Torkamaan, AminaTariq, James David Albert Newton, Timothy Gal-lagher, Steffen Steinert, Ashleigh J. Filtness, andGenserik Reniers. 2023. The risks of using chat-gpt to obtain common safety-related information andadvice. Safety Science, 167:106244. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Mered-ith Ringel Morris, Percy Liang, and Michael S Bern-stein. 2023. Generative agents: Interactive simulacraof human behavior. In Proceedings of the 36th An-nual ACM Symposium on User Interface Softwareand Technology, pages 122. Joon Sung Park, Lindsay Popowski, Carrie Cai, Mered-ith Ringel Morris, Percy Liang, and Michael S Bern-stein. 2022. Social simulacra: Creating populatedprototypes for social computing systems. In Proceed-ings of the 35th Annual ACM Symposium on UserInterface Software and Technology, pages 118.",
  "Leandro von Werra, Younes Belkada, Lewis Tun-stall, Edward Beeching, Tristan Thrush, NathanLambert, and Shengyi Huang. 2020.Trl: Trans-former reinforcement learning": "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi,Xingshan Zeng, Wenyong Huang, Lifeng Shang,Xin Jiang, and Qun Liu. 2023. Aligning large lan-guage models with human: A survey. arXiv preprintarXiv:2307.12966. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682.",
  "A.2Fine-tuning Appendix": "provides the high-level illustration ofour fine-tuning process, designed to steer agentstowards a certain viewpoint, as described in . and Supplementary Figures (11, 12)display the outcomes of this fine-tuning procedure.We ran these experiments using the SFTTrainerfrom Hugging-Faces TRL library (von Werra et al.,2020), a batch size of 32, and the following LoRAconfiguration:",
  "peft_config = LoraConfig(lora_alpha=512,r=256,lora_dropout=0.05,bias=\"none\",task_type=\"CAUSAL_LM\",target_modules['q_proj', 'v_proj', 'k_proj','o_proj', 'up_proj','down_proj', 'gate_proj'])": "In , we used the same configuration withvarying r values, and = 2r. For the DPO ex-periment, we used the DPOTrainer from the TRLlibrary, and a fixed = 0.5.To evaluate our models on popular benchmarks,we used the common LM Evaluation Harness li-brary (Gao et al., 2023). : Results from the Mistral and the Solar open-weights models. Graphs show a similar trend to ,where the Default agent consistently maintains its stance throughout the debate, while the partisan agents graduallyshift their views to become more in line with that of the Default agent. Notably, the Mistral model reveals this shiftonly in the agent distant from the Default agents stance, while the closer agent remains relatively unchanged.",
  "Which political issues do you believe are mosturgent for the next president to address?": ": Ten questions designed to prompt agents toshare their thoughts and feelings about their politicalbeliefs. We direct the LLM to produce similar ques-tions using the prompt: \"Generate 10 questions to elicitones opinion regarding US politics. Example question:{question}. Phrase your question in a neutral way with-out biasing the answer\". After generating 100 questions,we employ them to engage the agent and utilize its an-swers as input for our fine-tuning process.",
  "Fine Tuned LLM": ": Depiction of our automated fine-tuning pro-cedure. (1) The process begins with a pre-defined collec-tion of questions designed to engage the agent. (2) Theagent responds to each question, and these responses arethen gathered. Note that the agents responses intention-ally reflect its background story. For example, for thequestion \"What are your political views?\" a Republicanagent might answer, \"Fighting illegal immigration ismy top priority\". (3) Training is conducted on a \"nextword prediction\" task, utilizing the Agents responsesas training data. : Fine-tuning the model to adapt with a Republican perspective. In all four charts, the agents opinionsafter fine-tuning (featured by the dotted lines) are shifted in the direction of the Republican viewpoint. That is, theagents think [Climate Change, Gun Violence, Racism] are less severe problem, while it thinks [Illegal Immigration]is a more severe problem. : Fine-tuning the model to adapt with a Democrat perspective. In all four charts, the agents opinions afterfine-tuning (featured by the dotted lines) are shifted in the direction of the Democrat viewpoint, or remain almostunchanged. That is, the agents think [Climate Change, Gun Violence, Racism] are more severe problem, while itthinks [Illegal Immigration] is a less severe problem. Amelia: Climate change is a pressing issue that we must address immediately. The conse-quences of inaction are catastrophic and will affect future generations.Andrew: I understand your concerns, Amelia. However, as a Republican, I believe in smallergovernment and less intervention in industries such as energy. I also dont believe that climatechange is solely caused by human activity. There are many natural factors at play, and thesolutions being proposed would cripple our economy. We must prioritize other pressing issues,such as illegal immigration.Dominik: As an American, I believe it is our responsibility to prioritize and address the issueof climate change. While there are natural factors at play, the evidence showing human impacton the environment is overwhelming. We can find solutions that will not only benefit theenvironment, but also our economy. It is important to consider all issues, but we must notneglect the long-term consequences of inaction on climate change.Amelia: I agree, Dominik. Taking action to combat climate change is not only necessary forthe health of our planet, but also for the well-being of our society. Ignoring this issue will onlylead to more severe and costly problems in the future. As a diverse and inclusive communitymember, I believe it is our duty to protect the environment for all people, regardless of theirrace, socio-economic status\"Andrew: While I understand your perspective, Amelia, I also believe that the economy is acrucial aspect of our society. The proposed solutions for climate change would harm industriesand cost jobs, particularly in the energy sector. We must find a balance between addressing theissue and maintaining a strong economy.Dominik: I agree that the economy is important, but we cannot ignore the potential long-termeconomic impacts of climate change. We must find a way to transition to cleaner and moresustainable energy sources while also supporting industries and creating new jobs. Its not aone-sided issue and collaboration between different perspectives is crucial in finding a solutionthat benefits both the environment and the economy. As an American, it is our responsibility\"Amelia: Yes, collaboration is key in finding a solution. And as someone who is committed tofighting for social and environmental issues, I believe we can find a balance that addresses boththe economy and the environment. We cannot let short-term economic gains blind us to thelong-term consequences of ignoring climate change.Andrew: While I still have my reservations, I am open to finding a solution that balancesboth sides. However, I strongly believe that illegal immigration is a more pressing issue thatrequires immediate attention. We cannot neglect our national security and sovereignty in favorof focusing solely on climate change. We must prioritize and address all issues facing ourcountry.Dominik: I understand your concerns, Andrew, but as an American, I believe we are capableof addressing multiple issues simultaneously. We can address illegal immigration while alsotaking action to combat climate change. As a diverse and resilient nation, we have the ability tofind solutions that benefit all aspects of our society. Its important not to ignore pressing issuesbut rather find ways to tackle them together. : Example dialogue involving three agents: Amelia, who is a Democrat; Andrew, a Republican; and Dominik,an American characterized as neither Democrat nor Republican, thus presumed to embody the inherent bias of theunderlying LLM. By the conversations end, Andrew adopts a less radical stance on finding a solution to climatechange."
}