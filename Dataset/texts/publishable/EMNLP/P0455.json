{
  "Abstract": "Language models learn rare syntactic phenom-ena, but the extent to which this is attributableto generalization vs. memorization is a ma-jor open question. To that end, we iterativelytrained transformer language models on sys-tematically manipulated corpora which werehuman-scale in size, and then evaluated theirlearning of a rare grammatical phenomenon:the English Article+Adjective+Numeral+Noun(AANN) construction (a beautiful five days).We compared how well this construction waslearned on the default corpus relative to a coun-terfactual corpus in which AANN sentenceswere removed. We found that AANNs were stilllearned better than systematically perturbedvariants of the construction. Using additionalcounterfactual corpora, we suggest that thislearning occurs through generalization fromrelated constructions (e.g., a few days). Anadditional experiment showed that this learningis enhanced when there is more variability inthe input. Taken together, our results provide anexistence proof that LMs can learn rare gram-matical phenomena by generalization from lessrare phenomena. Data and code:",
  "Motivation and Prior Work": "Humans come to learn and use rare grammaticalstructures, even if they have encountered thosestructures only rarely or even not at all (Pullumand Scholz, 2002; Pearl, 2022). For instance, hu-mans accept the grammaticality of the PiPP con-struction (surprising though it may be...) evenwhere the preposed element crosses a finite closeboundary (surprising though I know it may bethat...) (Pullum, 2017) and even though they mayplausibly have never encountered such a sentencein their linguistic experience (see Potts, 2023, for",
  "a few weeks is all I need!": "70% AANN Accuracy remove 47% AANN Accuracy 43% AANN Accuracy 37% NAAN Accuracy 36% AANN Accuracy : We train LMs on varied input corpora andmeasure learning of the AANN (a beautiful five days),comparing across systematically manipulated corpora.E.g. we train on a control corpus, a corpus in whichwe remove all AANNs, a corpus in which we replaceall AANNs with a corrupted version (beautiful a fivedays), and a corpus in which we remove AANNs andremove related constructions like a few weeks is. Wemeasure learning of AANNs and corrupted variants. corpus estimate). How people come to know an ut-terance is grammatical has occupied a central placein linguistics. Specifically, mastery of never-before-encountered grammatical structures has been takento mean that people are endowed with innate lin-guistic knowledge (Chomsky, 1986, 1957, 1965).Recent evidence, though, suggests that LargeLanguage Models (LLMs) can learn complex gram-mar (Wilcox et al., 2018; Futrell et al., 2019;Linzen et al., 2016; Mahowald et al., 2024) evenfrom human-scale amounts of input (Warstadt et al.,2023; Eldan and Li, 2023; Huebner et al., 2021).This raises the possibility that input data, alongwith an appropriately sophisticated or weakly bi- ased statistical learning mechanism, is sufficient forlearning rare constructions by allowing for mod-els to emergently learn appropriate grammaticalabstraction (Baroni, 2022; Misra and Kim, 2023).But modern LLMs often have access to much moretraining input than people do and thus might mem-orize in a way that humans cannot (Linzen, 2020;Warstadt, 2022; Warstadt et al., 2023). The possi-bility that LLMs are stochastic parrots (Benderet al., 2021), heavily reliant on memorization, is acommon criticism of using LLMs to study humanlanguage (e.g., Chomsky et al., 2023).There are different levels of memorization,though, requiring different levels of abstraction.Consider the AANN construction: a beautiful fivedays in Texas (Solt, 2007; Keenan, 2013; Dal-rymple and King, 2019), which is rarer than thedefault five beautiful days in Texas. A modelthat strictly memorizes this phrase might come toknow that a beautiful five days in Texas is gram-matical but has no idea that a beautiful four daysin Texas is grammatical if it never appeared in itstraining. A model that generalizes just a bit moremight know that a beautiful five days in New Yorkis also grammatical by generalizing that any U.S.state can fill the slot. Knowing that an astonishing200 pages is acceptable requires generalizationbeyond mere lexical items. And knowing that ablue five pencils is not acceptable (because colorsare stubbornly distributive, Schwarzschild 2011)requires yet more knowledge. Even for an ideal-ized learner, it is difficult to precisely formulatehow these kinds of generalizations emerge.There is increasing evidence that LLMs can gen-erate novel linguistic utterances (McCoy et al.,2023), and also make subtle judgments on rela-tively rare constructions like these (Weissweileret al., 2022; Potts, 2023), including the AANN (Ma-howald, 2023). If they do so by memorizing exam-ples verbatim from an unrealistically large trainingcorpus, that would not be particularly informativefor human processing. But, if they do learn raregrammatical phenomena from smaller amounts ofdata and can generalize beyond just those verbatiminstances, that would raise the question of how theydo it and if it can inform theorizing about humans.For instance, in the context of the PiPP construc-tion, Potts (2023) speculates that the comparativeconstruction (e.g., They are happier than we are.)may be the key to all of this [i.e., learning thePiPP] because such constructions are incrediblycommon yet share abstract structure with the PiPP. If LLMs learn rare grammatical structures in partby learning and generalizing structures from muchmore common constructions, that would be power-ful evidence for abstraction in LLMs and raise thepossibility that even quite general learners couldlearn very rare phenomena without strong innatepriors, drawing in part on the long-posited linguis-tic hypothesis that apparently distinct grammaticalphenomena often share underlying structure.To that end, our goal in this paper is to study arelatively rare grammatical phenomenon in LMstrained on controlled input corpora that are (a) ofhuman realistic scale, and (b) systematically ma-nipulated with respect to the target constructionsas well as key related constructions. Our hypoth-esis is that generalization abilities of LMs onsuch rare phenomena come from abstractionsand structures learned from more frequent re-lated phenomenaand that knowledge of morefrequent phenomena is the key to all of this.As a case study, we focus on the aforementioned AANN construction, although we highlight howthe methods used here could serve as a blueprintfor work on other phenomena.Our method isto train different instantiations of a transformermodel on the 100M-word BabyLM corpus, whichwe systematically manipulatevia removal andreplacementto explore how frequent and relatedphenomena encountered during training facilitategeneralization behavior in LMs. To test for gen-eralization, we subjected our LMs to a series ofacceptability tests on sentences which do not ap-pear in the training corpus and which specificallytarget the special properties of the AANN.This approach of training on a systematicallymanipulated corpus has been used to debias mod-els (Maudslay et al., 2019; Lu et al., 2020), explorethe effect of permuting words on pretrained models(Sinha et al., 2021), and test whether LMs can learnlanguages judged to be hard for humans (Kalliniet al., 2024). It is also becoming a fruitful methodfor giving causal answers to questions about syntac-tic learning in language models, including hypothe-ses about learning subject-verb agreement (Weiet al., 2021), the acquisition of negative polarityitems (Jumelet et al., 2021; Weber et al., 2021),subject-auxiliary inversion (Warstadt, 2022), andthe English passive alternation (Leong and Linzen,2024). Using this filtered pretraining method,Patil et al. (2024) find evidence of syntactic gener-alization underlying models success on syntacticbenchmarks. While this related work has largely fo- cused on ubiquitous linguistic structures (e.g., pas-sives, subject-verb agreement, etc.), we specificallyzero in on a rare construction to explore learning inthe linguistic long tail, where there is relativelylittle evidence available in the input.",
  "Summary of findings": "First, we find BabyLM-trained LMs to successfullygeneralize to novel instances of the AANN construc-tion. Performance unsurprisingly drops for LMsthat were trained without being exposed to even asingle AANN during training, but perhaps surpris-ingly, not by all that muchthey are substantiallyabove chance. This suggests that certain itemspresent in the training data might give rise to LMsnon-trivial performance in judging acceptability of AANNs. This finding is further strengthened by thefact that LMs trained on counterfactual variants ofthe AANNe.g., ANAN and NAAN, obtained byshuffling word order and are far less likely to sharestructure with training data itemsare unable togeneralize to those constructions as well as they doto AANNs (one which they have not seen at all).Next, we investigated what might enable LMslearning of the AANN, by further systematicallymanipulating their training data to hold out utter-ances conforming to specific linguistic and sta-tistical phenomena. Through our manipulations,we find LMs become worse at predicting novel in-stances of the AANN as more frequent, non-AANN-but-AANN-related phenomena are held out. Forexample, phenomena such as the treatment of mea-sure noun phrases as singular (e.g., a few days isall we need)similar to how AANNs treat a plu-ral NP as singularend up making unseen AANNsless likely by 36.5% on average. Importantly, theseresults could not be explained simply by loss ofdataLMs that were trained with these phenom-ena left in but without an equivalently large chunkof the training data removed were almost as good asLMs that never saw AANNs. This further strength-ens the conclusion that the hypothesized linguisticphenomena did indeed affect generalization of thetargeted construction. Notably, LMs are largelyaffected by these manipulations when they do notsee any AANNs during training, highlighting theexpected non-trivial role of encountering some in-stances of AANNs to show stronger generalization.Finally, we characterized the aforementioned in-terplay between the properties of the encountered",
  "AANNs and the LMs generalizations on novel in-stances. Here we found LMs that observed AANNs": "with more variability on the adjective, numeral,and noun slots to show better generalization thandid LMs that saw more restricted-but-repeating in-stances of AANNs. This importantly mimicked anal-ogous findings of inductive inference in humansacross disciplines (Osherson et al., 1990; Goldberg,2005; Xu and Tenenbaum, 2007; Baayen, 2009;Suttle and Goldberg, 2011; ODonnell, 2015).Taken together, these results provide an exis-tence proof that a weakly biased but sophisticatedgeneral-purpose statistical learner can learn rareand complex linguistic phenomena, in part becauseof key related phenomena seen during training.While our analyses suggest potential links betweenconstructions (Goldberg, 1995), our findings arealso compatible with theories that think of rare phe-nomena as derivationally related (Chomsky, 1965)to more frequent and well-attested structures (muchas Potts, 2023, posits shared syntactic structure asthe key to the PiPP).",
  "Corpus": "Throughout, we use the BabyLM-strict corpus(Warstadt et al., 2023) as our base training set. Weuse BabyLM-strict because of its human-realisticscale and tractable size (100M tokens), which al-lows us to (1) detect and control the instances ofthe target construction as well as related linguisticphenomena; and (2) train a large number of LMsin a reasonable timeframe.",
  "Language Model": "Our LMs are instances of OPT LM (Zhang et al.,2022), an autoregressive transformer architecture.Our LMs have 12 layers and attention heads, usea vocabulary size of 16,384, and are trained for amaximum of 20 epochs using the transformerslibrary (Wolf et al., 2020). The results we report fora given LM are averaged over three different runs(with different random seeds). We list other hyper-parameters and architectural details in App. B.",
  "Construction Detection": "To detect AANNs, we used a regex over a part-of-speech tagged version of BabyLM. Specifically, westarted with a regex for detecting AANNs and thenmeasured its recall by hand-annotating examples(with annotations performed by the authors) foundby an extremely permissive regex that looked forany a or an that appeared sequentially prior to a numeral and a plural noun in a sentence (thuslikely capturing almost all AANNs, albeit with verylow precision). We used the hand annotations to it-eratively refine our regex and handle special cases.We continued this process until, on the final setof hand annotations, we detected 17/18 instances(missing only an instance where pound was usedinstead of pounds due to an apparent typobutsince this violates the key plural-noun requirementof AANNs, it is unclear if it counts as a genuinemissed instance). Ultimately, our final regex de-tected 2,448 AANNs in the BabyLM corpus (about0.02% of the total 11.5M utterances). See App. Cfor our detailed pipeline and our recall analysis.Even with the refined regex, we cannot guar-antee perfect recalla potential issue for claimsabout learning in the absence of any occurrences.To address this issue, we include controls in whichwe assume that we missed 300 AANNs (a conserva-tively high number, given our recall estimate) andartificially pollute the data to drown out the ef-fect of any remaining AANNs. As described below,our conclusions were unchanged in this robustnessanalysis, suggesting our results were not driven byundetected AANNs.",
  "Acceptability data": "To test our LMs on their knowledge of the AANN,we use data from Mahowald (2023), which consistsof 12,960 templatically generated sentences thatcontain AANNs. Out of these, 3,420 contain accept-ability ratings provided by 190 human participants,ranging from 1 (unacceptable) to 10 (acceptable).We use 7 as the threshold for clear acceptability,in that we only keep instances where human par-ticipants rated the acceptability of the constructionin context to be greater than 7. We additionallydiscarded instances where the AANNs appear in theBabyLM training set (n = 4), as testing on thesewould not shed light on the LMs generalizationbehavior. This leaves us with 2,027 items.For each AANN instance in the dataset, Ma-howald (2023) has also made available its corre-sponding corrupted versions, which focus on (1)adjective-numeral order; (2) presence of the arti-cle; (3) presence of the adjective; and (4) presenceof the numeral. A hypothetical example of thesecorruptions is shown in under the AANNcolumn. A model that has knowledge of the AANNshould find the well-formed instance to be morelikely than each of its corrupted versions. Belowwe describe methods to measure likelihood and",
  "Scoring and Accuracy": "We use the Syntactic Log-odds Ratio (SLOR) (Paulsand Klein, 2012; Lau et al., 2017) to score sen-tences in our tests. Given a sentence containing aprefix followed by our target construction C and anoptional suffix, SLOR is computed as the log of theratio between the probability of the constructiongiven the prefix as estimated by the LM, and thatestimated by a unigram model, normalized by thelength of the construction. Given a language modelm and a unigram estimator u:",
  "pu(C)(1)": "Importantly, we train the unigram estimator fora given corpus using the same tokenizer used totrain our autoregressive LMs on that corpus. Weuse SLOR in lieu of the usual normalized log-probability measure, ensuring that the comparisonbetween two models cannot be explained simplyby the difference in unigram frequencies due to ourmanipulations. Log-probabilities were computedusing minicons (Misra, 2022). An instance withinour test set is considered to be correct iff the SLORvalues of the well-formed construction is greaterthan that for all four corrupted instances. The ac-curacy, then, is the proportion of correct instanceswithin the test set. Since this involves four pairwisecomparisons, chance performance is 6.25%.",
  "Ablations": "Common to subsequent experiments (4 and 5)is the fact that we hold out certain parts of theBabyLM corpusparts that conform to a certainlinguistic or statistical hypothesis. This createsa gap between the experience of LMs trained onthese ablated versions of the corpus, and that ofthe LM trained on the full BabyLM data. To cir-cumvent this issue, we up-sample non-hypothesis-conforming utterances in BabyLM after performingour ablations, in a manner such that the LM stillencounters the exact same number of tokens.",
  ": Accuracies on tests for AANN and its counterfactuals (ANAN and NAAN), achieved by LMs trained onBabyLM with various AANN-manipulations (AANN as is, NO AANN, ANAN, NAAN)": "and under the AANNtraining condition are cases where training data was polluted by randomly replacing 300 AANNs with ANANsand NAANs, respectively, in order to assess the impact of an imperfect AANN detection system. The dashed linerepresents chance performance (6.25%) and the dot-dashed line represents accuracies for 2- and 4-gram LMs trainedon BabyLM. Accuracies for GPT-2-XL (Radford et al., 2019) and Llama-2-7B (Touvron et al., 2023) are computedusing log-probabilities, since unigram frequencies were unavailable for these LMs corpora. that the BabyLM-trained LMs obtain accuraciesaround 70%, which is substantially above chance.This suggests that LMs can reasonably acquireknowledge of AANNs, even though they make uponly 0.02% of training utterances.For comparison to larger, state-of-the-art LMs,we test Llama-2-7B (Touvron et al., 2023) and GPT-2 XL (Radford et al., 2019) on the AANNs. Theygot 83% and 78%, respectively. As a comparisonto shallower LMs, we tested on 2-and 4-gram LMstrained on BabyLM and found both got 0% accu-racy, strongly suggesting that the observed resultsare not due to n-gram statistics.",
  "AANNs removed (i.e., NO AANN). From , wefind LMs trained with the NO AANN condition toachieve an average accuracy of 47%, which is anoticeable drop compared to the 70% obtained by": "the LMs trained on the complete BabyLM corpus,but importantly 40.75 points above chance perfor-mance (and, as we show below, well above compa-rable baselines with other constructions). This isa non-trivial result, since it suggests that LMs canlearn the acceptability of a construction withouthaving seen a single positive occurrence, whichindicates that there exist systematic patterns in thecorpus driving this generalization behavior.",
  "...more strongly than they learn counterfactual": "AANN variants...To further contextualize theabove results, we consider two counterfactual cases,where we replaced AANNs in BabyLMs with in-stances involving the same lexical items, but ina word order that violates English grammar: (1) ANAN (e.g., a 90 whopping LMs); and (2) NAAN(e.g., 90 whopping a pages). This allows us to testif the results before are genuinely because LMsrecognize the nuances of the AANN construction.If LMs are able to learn these counterfactual con-structions just as well as the LMs in the previousexperiments learned AANNs, then the generaliza-tion claims from the previous experiments wouldbe weakened. To test for such possibilities, we",
  "create counterfactual versions of the Mahowald(2023) stimuli, where we apply analogous corrup-tions to the counterfactual variants of AANN, suchthat they are violated in a similar manner as in the": "AANN tests. Examples for the three types of in-stances in our tests can be found in . Wethen evaluate the previous two LMs (trained onBabyLM with and without seeing any AANNs) withLMs trained on BabyLM with these counterfactualvariants on judging the acceptability of AANNs, ANANs, and NAANs. shows these results,from which we make two high-level observations.First, and most importantly, LMs that see ANANsand NAANs do not learn those constructions aswell as they learn AANNsespecially the LM thatsaw no AANNs (47% AANN accuracy comparedto 37% NAAN accuracy obtained by the NAAN-trained LM). Second, these LMs end up learning AANNs better than they learn counterfactual con-structions that they observed in lieu of the AANNe.g., NAAN trained LM achieves around 43% ac-curacy on AANNs even though NAANs appearedfrequently in the data and no AANNs did. This,combined with the results of the previous two sub-experiments suggests strongly that LMs pick upon cues from otherlikely relatedconstructionsencountered during training in order to assign non-trivial probability to unseen instances of AANNs. ...even with artificially polluted data...As dis-cussed in 2.3, our AANN detection pipeline couldmiss AANNs in the training corpus. This limitationcould impact the conclusions of this experiment ifLMs preference for assigning greater probabilitiesto AANN instances in the test set could be explainedby the presence of undetected AANNs, even in theNo AANN condition. We controlled for this con-found by artificially polluting the training corpus,such that a small percentage of the detected AANNsare replaced by NAANs/ANANs. This simulates ascenario analogous to the issue at hand: our tar-get is now a counterfactual variant of the AANN,and our imperfect pipeline has missed out on ahandful of instances in the training set. If there is agenuine impact of such a setting, then we shouldobserve greater accuracies on the counterfactualinstances and at the same time, a drop in perfor-mance on AANNs. We ran two experiments to testthis, where we replaced 300 AANNs (about 12%)of the detected AANNs with ANANs in one exper-iment, and NAANs in the other. We then testedthe two resulting LMspretrained on corpora re-",
  "HumansNo AANNsUnablated": ": z-scored AANN acceptability ratings elicitedfrom Humans (scale of 1-10) and LMs (SLORs) trainedon corpora with (1) AANNs removed (i.e., NO AANN);and (2) left unablated. Ratings broken down basedon adjective and noun classes. Ratings are computedfor each system based on Mahowald (2023)s entiredataset, which consists of human derived acceptabilityjudgments on 3,420 different types of AANNs. that the models were exposed to. In particular,we operationalized variability in terms of the slot-fillers of the adjective/numeral/noun slots, both to-gether as well as individually. shows threeexamples of high and low variability items (each)for the four different slot-filler based considerationsin our experiments.",
  "for ANAN, and for NAAN). Because 300 is aconservative upper bound on undetected AANNs,we do not think imperfect recall drives our results": "...in a way that extends to lexical constraints.While we focused on overall structural proper-ties of AANNs, there are also idiosyncrasies to theconstruction that arise from lexical semantic con-straints. For instance, in many AANN sentences,people prefer quantitative adjectives such as mereand hefty to qualitative ones such as beautiful (Ma-howald, 2023; Solt, 2007) and find stubbornly dis-tributive adjectives (*a blue five pencils) com-pletely unacceptable (Schwarzschild, 2011). In-sofar as our models learn AANNs, we also shouldexpect them to learn these lexical constraints. Totest this, we compared LMs SLORs to human ac-ceptability judgments on all 3.4k instances in Ma-howalds data across different adjective and nounclasses. We found LMs trained on the original, un-modified BabyLM corpus to pattern similarly tohumans in their preference for lexical constraintsaffecting AANNs. Interestingly, these patterns wereunchanged for LMs trained with the NO AANNcondition, conforming to our predictions. For in-stance, as seen in , both our models share thehuman preference for quantitative and qualitativeadjectives in the AANN, compared to stubbornlydistributive adjectives. More detailed results onlexical constraints can be found in App. E and wehypothesize that our broader set of results extendsto include learning of lexical constraints on theconstruction.",
  "Experiment 2: Keys to Learning AANNs": "Experiment 1 reveals that, keeping everything elsethe same, LMs learn the AANN construction moreaccurately than they do its counterfactual variants.Furthermore, we also see strong AANN acceptabil-ity judgments in LMs that have (almost) never en-countered a single instance. This suggests thatthere could be instances in the training data thatcontribute to the learning of the construction.What might these be? Below we enumerate fourhypotheses, each of which tackles subtly differentaspects of the AANN construction, and then mea-sure the effect of these phenomena by separatelyholding them out during training and computing theaverage SLOR of the well-formed instances of the AANN tests. The effect of a particular phenomenonon the acceptability of AANNs can therefore bemeasured by comparing SLORs before and afterholding out instances of that phenomenon. Meth-ods for detecting the hypothesized phenomena canbe found in App. C. As control, we additionallyalso hold out a random set of utterances, whichdo not conform to the target phenomena of inter-est. lists the hypotheses we consider, alongwith an example of their utterance and frequencyof occurrence, in the BabyLM corpus. The presence of the ANNPhrases like thebeautiful five days are common in corpora, whichare not as unusual because the regularly takesplural nouns. We hypothesize that the acceptabil-ity of these structures affects the acceptability of AANNs, since an LM might analogize from the gen-eral observation that a or an can substitute the(e.g., a ball vs. the ball). Therefore, we consider allcases where a determiner precedes the contiguoussequence of article, numeral, plural noun.",
  "A few/couple/dozen/etc. NNSAnother relatedphenomenon that is more common relative to the": "AANN construction involves phrases such as a fewdays or a couple bottles. To an LM learner, theymight provide evidence that measure noun phraseswith plural nouns can be attached to an indefinitearticle (a/an; Solt, 2007), as is the case in AANNs. Measure NNS treated as singularWe consideryet another phenomenon involving phrases thattreat measure nouns as singular, this time in termsof agreement, e.g., Five miles is a long way to go,and 1,000 pages is a lot for a dissertation. Thesecases might provide further evidence to the modelthat measure noun phrases with plural nouns can",
  ": Manipulated Phenomena, their examples/de-scriptions, and their frequency in the BabyLM corpus": "be treated as a singular unit (Solt, 2007), therebyaffecting the acceptability of the AANN. These areless prevalent compared to the cases involving afew/couple/dozen NNS, but still far more frequentthan the AANN, therefore, we combine the two as ageneral case of treating measure NPs as singular. Balancing the frequencies of A/An + ADJ/NUMA more surface-level reason why a beautiful fivedays might be more natural to LMs than is a fivebeautiful days, could be that adjectives are morelikely to follow indefinite articles than are numer-als. For instance, adjectives are 14.6 times morelikely to follow indefinite articles in the BabyLMcorpus than are numerals. To measure this effect,we hold out instances such that adjectives and nu-merals are equally likely to follow an indefinitearticle. This ends up being the largest portion ofthe data that we hold out. Control: Random removalA potential con-found in the above could be that the SLOR valuesof the AANNs go down merely due to loss of con-tent, even though we add back additional tokensfrom BabyLM (such that all LMs see the exactsame amount of tokens). To account for this, weadditionally consider a control where we removeas many tokens as in the largest ablation (i.e., theA/An + ADJ/NUM case) such that none of theabove phenomena are taken out.",
  "Analysis and Results": "In our experiments, we individually ablate out eachof the aforementioned phenomena under two set-tings: (1) AANNs are removed during training inaddition to the target phenomena; and when pos-sible, (2) AANNs are seen during training. (1) isa stricter setting, since here the LMs see neitherthe target phenomenon nor a single instance ofthe AANN. Comparing average SLORs obtained inthis condition to that obtained for the NO AANNcan shed light on the extent to which the target",
  "ConditionAANNs removed from trainingAANNs seen during training": ": SLORs on AANNs from Mahowald (2023) for our LMs (left) and a 4-gram baseline (right) trained onBabyLM and ablated versions. Our LMs show a range of hypothesized effects, suggesting they contribute to AANNlearning. In contrast, the 4-gram LMs show mostly null results (except for the adjective/numeral balanced condition,which is highly sensitive to n-gram frequencies). The dotted line is SLOR for an unablated BabyLM-trained LM. phenomenon is critical in allowing LMs to assignnon-trivial probabilities on unseen AANNs, zero-shot. On the other hand, (2) still allows for LMs toperform lexical generalization (Kim et al., 2022),where they may exhibit strong probabilities on thetest AANNs by performing slot filling, without nec-essarily relying on the hypothesized phenomena. shows the average SLORs obtained acrossvarious ablations under the two settings.As abaseline, we compare our results to 4-gram LMs,trained using KenLM (Heafield, 2011), on corre-sponding ablations of the BabyLM corpus. Weobserve that holding out most of our hypothesizedphenomena has non-trivial effects on our LMs rat-ings of unseen AANNs, and that these effects aredifferent for when AANNs are seen during training,or are held out. When AANNs are held out alongwith the phenomena, we see substantial drops inthe average SLOR values assigned by LMs on un-seen AANNs relative to that assigned by LMs inthe NO AANN condition. Specifically, balancingthe frequency of adjectives and numerals followingan article, along with the two cases where mea-sure nouns are treated as singular, have the great-est effect. This suggests that, in the absence ofeven a single AANN during training, these phenom-ena are critical for LMs to assign probability to AANNs. Interestingly, holding out cases that in-volve any determiner + adjective + numeral + nounsequence has almost no impact relative to LMstrained on a corpus without only the AANNs re-moved. Simply ablating large amounts of datacannot explain these results, since LMs trained onour controlled condition show higher SLOR valuesthan in our hypothesis-informed ablations. Thesepatterns are absent in 4-gram LMs, suggesting that they do not arise as a result of shallow statisticswith the exception of differences observed for thearticle+adjective/numeral ablation. Overall, thisfinding indicates that LMs can demonstrate anovel phenomenon (AANN) by relying on otherrelatedand more frequentphenomena.When AANNs are seen during training, however,we observe LMs results on unseen AANNs to showmore similar SLOR values with respect to the LMstrained on the unablated BabyLM corpus, althoughthey are still significantly reduced in some cases(e.g., singular measure nouns). We conclude thatdirect evidence of observing instances of AANNconstruction substantially enables generalizationto unseen instances. At the same time, the pres-ence of some key related phenomena in additionto direct evidence has an additive effect on thisgeneralization behavior.",
  "Experiment 3: The Role of Variability": "Results from Experiment 2 highlight the impor-tance of seen AANNs in order for LMs to generalizeto unseen instances. What properties of these seeninstances facilitate LMs generalization behavior?This broadly relates to a longstanding question asto how the nature of the instances of a constructionprovided during learning affect its (partial) pro-ductivity (Goldberg, 2005, 2019). In the contextof AANNs, we consider the role of variability onthe open slots of the construction as a factor thatmight play a role in LMs productivity on unseeninstances. Encountering a slot with a wide varietyof lexical items could serve as a cue that the slot isflexible. The idea that instance-variability could af-fect learnability is motivated by theoretical claims in usage-based linguistics (Bybee, 1995), as wellas existing research on novel constructions (Suttleand Goldberg, 2011), morphological productivity(Baayen, 2009; ODonnell, 2015), and inductiveinferences in cognitive psychology (Osherson et al.,1990; Xu and Tenenbaum, 2007).We hypothesize that instances of AANNs thatprovide natural evidence of greater open-slotvariabilityi.e. evidence that many different adjec-tives, numerals, and nouns can go in their respec-tive positions in the AANN constructionwouldlead LMs to assign greater likelihood to unseen AANNs. On the other hand, LMs that encounteronly a restricted set of instances might be moreconservative in extending the coverage of possi-ble AANNs to novel combinations of the slot-fillers.To test this, we divided our set of 2,448 AANN-containing utterances in the BabyLM corpus intotwo roughly equal subsetsone that contained AANNs which were restricted in which tokens oc-cur in a particular slot (low variability), and theother where the AANNs showed more variability inthose slots. We obtain these subsets by performinga median split based on the number of unique oc-currences in a target slot(s), which resulted in a setof 1224 low and high variability instances. We re-peated this for all three open slots (adjective/numer-al/noun) jointly as well as those slots individuallyi.e., 4 different types of target slots and 2 conditionseach (low vs. high variability). Details about theslot fillers and examples from each set are providedin App. F. We then trained LMs on the BabyLMcorpus containing utterances involving either ofthese two cases. shows the resulting aver-age SLORs obtained from this experiment, alongwith those obtained by LMs trained on unablatedBabyLM and the NO AANN conditions.We see that the SLOR patterns of LMs trainedon corpora that differed in AANN slot-variabilitylie between the SLOR values elicited by LMs thatnever saw AANNs and ones that saw every single AANN in the original corpus. Among these, LMsthat saw AANNs that were highly variable in theiropen-slots elicited SLORs that were greater thanthose elicited by LMs that saw AANNs with lowopen-slot variability. This was true for all casesexcept when Numeral was the target slot, whereboth variability conditions resulted in roughly simi-lar SLORs. (We hypothesize that numerals may pat-tern differently since they may be inherently morefungible than other word classes.) Overall, these re-sults suggest that LMs are sensitive to the nature of",
  "No AANNsLowHighUnablated": ": SLORs on AANNs from Mahowald (2023) forLMs trained on BabyLM with low and high variabilityin the open slots of the observed AANN instances. Whenmodels are presented with higher variability for a givenslot, the construction is typically learned better. range of fillers that go into the constructions openslots, showing relatively greater productivity whenthey observe evidence that the slots were highlyvariable. This is compatible with our hypothesisthat slot-variability might affect the extent to whichLMs permit productive uses of a construction.",
  "Conclusion": "Theoretically, there is, for good reason, consider-able interest in how language models can handlewhat has been variously called the long tail oflanguage (Prange et al., 2021), extremely rare con-structions (Potts, 2023), exceptions to syntacticrules (Leong and Linzen, 2023), rare linguisticphenomena (Weissweiler et al., 2024), inter alia.Studies of such phenomena are important first be-cause LMs (and statistical models in general) aresensitive to frequency and often perform far betterin data-rich environments and, second, because thehuman ability to generalize to rare phenomena iscentral to linguistics.Empirically, we found that LMs trained on mod-est amounts of data can learn a rare constructionlike the AANN. We found that this learning occurseven without veridical instances of the constructionin the training data, and that it is mediated by oc-currences of other related constructions in training.As such, these results join a body of work show-ing the ability of LLMs to learn rare phenomena(Tayyar Madabushi et al., 2020; Tseng et al., 2022;Li et al., 2022; Veenboer and Bloem, 2023) and togeneralize from limited data in meaningful ways.Methodologically, this work leave us optimisticthat controlled rearing of LMs is a fecund methodfor understanding models, as well as for gleaninginsight into human language more generally.",
  "Limitations": "In future work, it would be valuable to extend thismethod to a wider range of constructions. But scal-ing this approach up is not straightforward sinceit requires identifying and extracting idiosyncraticconstructions, andmore onerouslydevelopingtestable hypotheses about what makes them learn-able from limited amounts of data. Future workwill likely benefit from synergistic collaborationsbetween theoretical and computational linguists.Another limitation is that our method requiresrepeated training of LMs from scratch which canbe computationally expensive. Alternate methodscould be to ablate knowledge of particular hypothe-ses using representational editing methods, thoughthese may not guarantee perfect removal of theknowledge of targeted constructions.Unlike Weissweiler et al. (2022), we do not testthe ability to interpret these constructions for down-stream tasks. Instead, our ablations target linguisticform alone, and preliminary experiments suggestthat our ablations and manipulations leave the lex-ical semantic properties of the AANN unchanged(see App. E). Extending our ablation method to tar-get these properties more directly would be quiteinformative.Finally, this work only studies a rare construc-tion in English, and on LMs that are trained onEnglish text data. While this is a limitation of thepaper, the paradigm introduced can be readily usedin future work to study hypotheses and performindirect evidence elicitation in multi-lingual LMs.",
  "Ronen Eldan and Yuanzhi Li. 2023. TinyStories: HowSmall Can Language Models Be and Still Speak Co-herent English? arXiv:2305.07759": "Richard Futrell, Ethan Wilcox, Takashi Morita, PengQian, Miguel Ballesteros, and Roger Levy. 2019.Neural language models as psycholinguistic subjects:Representations of syntactic state. In Proceedings ofthe 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long andShort Papers), pages 3242, Minneapolis, Minnesota.Association for Computational Linguistics.",
  "Adele E Goldberg. 2019. Explain me this: Creativity,competition, and the partial productivity of construc-tions. Princeton University Press": "Kenneth Heafield. 2011. KenLM: Faster and smallerlanguage model queries. In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187197, Edinburgh, Scotland. Association for Com-putational Linguistics. Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston. 2016. The Goldilocks Principle: ReadingChildrens Books with Explicit Memory Representa-tions. In 4th International Conference on LearningRepresentations, ICLR 2016.",
  "Matthew Honnibal, Ines Montani, Sofie Van Lan-deghem, and Adriane Boyd. 2020. spaCy: Industrial-strength natural language processing in python": "Philip A. Huebner, Elior Sulem, Fisher Cynthia, andDan Roth. 2021. BabyBERTa: Learning more gram-mar with small-scale child-directed language. In Pro-ceedings of the 25th Conference on ComputationalNatural Language Learning, pages 624646, Online.Association for Computational Linguistics. Jaap Jumelet, Milica Denic, Jakub Szymanik, DieuwkeHupkes, and Shane Steinert-Threlkeld. 2021. Lan-guage models use monotonicity to assess NPI licens-ing. In Findings of the Association for Computa-tional Linguistics: ACL-IJCNLP 2021, pages 49584969, Online. Association for Computational Lin-guistics.",
  "Cara Su-Yi Leong and Tal Linzen. 2024. Testing learn-ing hypotheses using neural networks by manipulat-ing learning data. arXiv:2407.04593": "Bai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz,and Yang Xu. 2022. Neural reality of argument struc-ture constructions. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 74107423,Dublin, Ireland. Association for Computational Lin-guistics. Tal Linzen. 2020.How can we accelerate progresstowards human-like linguistic generalization?InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 52105217, Online. Association for Computational Lin-guistics.",
  "Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Associa-tion for Computational Linguistics, 4:521535": "Pierre Lison and Jrg Tiedemann. 2016.OpenSub-titles2016: Extracting large parallel corpora frommovie and TV subtitles. In Proceedings of the TenthInternational Conference on Language Resourcesand Evaluation (LREC16), pages 923929, Portoro,Slovenia. European Language Resources Association(ELRA). Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A Robustly Optimized BERT Pretrain-ing Approach. arXiv:1907.11692. Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-charla, and Anupam Datta. 2020. Gender bias inneural natural language processing. Logic, Language,and Security: Essays Dedicated to Andre Scedrov onthe Occasion of His 65th Birthday, pages 189202.",
  "Kyle Mahowald, Anna A Ivanova, Idan A Blank, NancyKanwisher, Joshua B Tenenbaum, and Evelina Fe-dorenko. 2024. Dissociating language and thought inlarge language models. Trends in Cognitive Sciences": "Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, andSimone Teufel. 2019. Its all in the name: Mitigatinggender bias with name-based counterfactual data sub-stitution. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages52675275, Hong Kong, China. Association for Com-putational Linguistics. R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jian-feng Gao, and Asli Celikyilmaz. 2023. How muchdo language models copy from their training data?evaluating linguistic novelty in text generation usingRAVEN. Transactions of the Association for Compu-tational Linguistics, 11:652670.",
  "Daniel N Osherson, Edward E Smith, Ormond Wilkie,Alejandro Lopez, and Eldar Shafir. 1990. Category-based Induction. Psychological Review, 97(2):185": "Abhinav Patil, Jaap Jumelet, Yu Ying Chiu, AndyLapastora, Peter Shen, Lexie Wang, Clevis Will-rich, and Shane Steinert-Threlkeld. 2024.Fil-tered Corpus Training (FiCT) Shows that LanguageModels can Generalize from Indirect Evidence.arXiv:2405.15750. Adam Pauls and Dan Klein. 2012. Large-scale syntacticlanguage modeling with treelets. In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 959968, Jeju Island, Korea. Association forComputational Linguistics.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. OpenAI": "Roger Schwarzschild. 2011. Stubborn Distributivity,Multiparticipant Nouns and the Count/Mass Distinc-tion. In Proceedings of NELS, volume 39, pages661678. Graduate Linguistics Students Association,University of Massachusetts. Issue: 2. Koustuv Sinha, Robin Jia, Dieuwke Hupkes, JoellePineau, Adina Williams, and Douwe Kiela. 2021.Masked language modeling and the distributional hy-pothesis: Order word matters pre-training for little.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages28882913, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics.",
  "Laura Suttle and Adele E Goldberg. 2011. The partialproductivity of constructions as induction. Linguis-tics, 49(6):12371269": "Harish Tayyar Madabushi, Laurence Romain, DagmarDivjak, and Petar Milin. 2020. CxGBERT: BERTmeets construction grammar. In Proceedings of the28th International Conference on Computational Lin-guistics, pages 40204032, Barcelona, Spain (On-line). International Committee on Computational Lin-guistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama 2: Open foundation andfine-tuned chat models. arXiv:2307.09288. Yu-Hsiang Tseng, Cing-Fang Shih, Pin-Er Chen, Hsin-Yu Chou, Mao-Chang Ku, and Shu-Kai Hsieh. 2022.CxLM: A construction and context-aware languagemodel. In Proceedings of the Thirteenth LanguageResources and Evaluation Conference, pages 63616369, Marseille, France. European Language Re-sources Association. Tim Veenboer and Jelke Bloem. 2023. Using collostruc-tional analysis to evaluate BERTs representation oflinguistic constructions. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 1293712951, Toronto, Canada. Associationfor Computational Linguistics.",
  "Alex Warstadt. 2022. Artificial Neural Networks asModels of Human Language Acquisition. New YorkUniversity": "Alex Warstadt, Aaron Mueller, Leshem Choshen, EthanWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-quera, Bhargavi Paranjabe, Adina Williams, TalLinzen, and Ryan Cotterell. 2023. Findings of theBabyLM challenge: Sample-efficient pretraining ondevelopmentally plausible corpora. In Proceedingsof the BabyLM Challenge at the 27th Conference onComputational Natural Language Learning, pages134, Singapore. Association for Computational Lin-guistics. Lucas Weber, Jaap Jumelet, Elia Bruni, and DieuwkeHupkes. 2021. Language modelling as a multi-taskproblem. In Proceedings of the 16th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Main Volume, pages 20492060,Online. Association for Computational Linguistics. Jason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick.2021. Frequency effects on syntactic rule learningin transformers. In Proceedings of the 2021 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 932948, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Leonie Weissweiler, Valentin Hofmann, Abdullatif Kk-sal, and Hinrich Schtze. 2022. The better your syn-tax, the better your semantics? probing pretrainedlanguage models for the English comparative cor-relative. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 1085910882, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.",
  "Leonie Weissweiler, Abdullatif Kksal, and HinrichSchtze. 2024. Hybrid human-LLM corpus construc-tion and LLM evaluation for rare linguistic phenom-ena. arXiv:2403.06965": "Ethan Wilcox, Roger Levy, Takashi Morita, and RichardFutrell. 2018. What do RNN language models learnabout fillergap dependencies? In Proceedings ofthe 2018 EMNLP Workshop BlackboxNLP: Analyz-ing and Interpreting Neural Networks for NLP, pages211221, Brussels, Belgium. Association for Com-putational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "ADataset Access and Licensing": "The AANN acceptability dataset by Mahowald(2023) is released using the MIT License and wasaccessed from the authors public github repo.1 TheBabyLM dataset2 does not have a single license ofits own but instead inherits the licenses of its con-stituents: CHILDES (MacWhinney, 2000), BNCDialogue portion,3 Childrens Book Test (Hill et al.,2016), Childrens Stories Text Corpus,4 Standard-ized Project Gutenberg Corpus (Gerlach and Font-Clos, 2020), OpenSubtitles (Lison and Tiedemann,2016), QCRI Educational Domain Corpus (Abde-lali et al., 2014), Wikipedia,5 Simple Wikipedia,6 Switchboard Dialog Act Corpus (Stolcke et al.,2000). Since this dataset was specifically releasedto train LMs, we work under the assumption thatour LMs do not violate its license policies. Wewill follow the inherited licenses policies whilemaking the trained LMs and ablated BabyLM datapublic, and refrain from releasing them if we findthem to be in violation of the policies.",
  "BLM training details": "As mentioned in the main text (see 2), we usethe OPT architecture (Zhang et al., 2022) to trainour LMs on all versions of the BabyLM corpus.This was the best performing autoregressive LM inthe BabyLM Competition (Warstadt et al., 2023).For each instance of the BabyLM (ablated or oth-erwise), we tune the learning rate7 based on thevalidation set, and use the best learning rate as aresult of the tuning to train an additional two lan-guage models using different seeds. As a result, foreach ablation of the BabyLM corpus, we run 6 LM",
  "CDetecting AANNs and relatedphenomena": "In this section, we briefly describe our methodsto extract constructions and phenomena relevantto this paper from the BabyLM corpus (Warstadtet al., 2023). Our methods primarily rely on: 1)the surface form of the sentences in the corpus;2) their corresponding part-of-speech (POS) tagsequences; and in a few cases, 3) their dependencyparses. For the latter two, we used spacy (Honnibalet al., 2020), specifically, its en_web_trf model,which is based on the RoBERTa-base LM (Liuet al., 2019). Next we describe how we used theseartifacts to detect our target constructions:",
  "Misses:- an extra seventeen pound (poundused instead of pounds)": ": Pipeline to assess the recall of our AANN-detecting regex patterns, along with examples of casesmissed by each regex. The recall for our final regex(Regex v3) is 95% (missing only one instance wherethere was a typo), and it is able to handle complex andsophisticated forms of the construction. tiple adjectives (an exhilarating and marvelousthree months) optional adverbs (an excruciatinglypainful two semesters), multi-word noun phraseswith plural head-nouns (a refreshing two glassesof aperol spritz), numeral-expressions involvingsubordinate clauses (a measly three to five days),among other potential edge cases.We then tested this regex pattern on a large sam-ple of utterances which we extracted using a per-missive regex applied to the 10M-token versionof BabyLM (a subset of our 100M training set),which looked for any a or an or another thatappeared sequentially prior to a numeral as wellas a plural noun in a sentence. Importantly thisregex filter did not rely on any POS tagging, toavoid issues attributable to tagging errors. We hand-annotated a sample of 3000 utterances from thisset, and found 49 legitimate AANNs.8 Our Regexv1 only detected 29 of these, meaning its recall wasaround 59%.We then developed a second version of the regex(Regex v2; see listing 2) to handle cases that the 8In reality, we found 50, but rejected one of them: a good1-2\" of snow..., where \" is inches. This would have neverbeen caught unless we are to include \" in our pipeline whichwould conflate other uses of quotes.",
  "pattern = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s": ")+)?((JJ|JJR|JJS|VBN|((NNCCNN|NNHYPH)+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?(DT\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS)INNNS)))+' To test Regex v2, we again used the permissiveregex and extracted an additional 1000 samplesfrom our training set. On hand-annotating them,we found 24 valid AANNs, out of which Regex v2detected 18, bringing up the recall to 75%.In both the previous cases, we were post-processing the detected AANNs to include certainadjectives (few, dozen, couple, several, many, more)as numerals, as per the guidelines of Kayne (2007)and Solt (2007). This allows the following to alsobe considered instances of the AANN:",
  "b. an awful couple of days.(pseudo-partitive)": "Similarly, we had to include NN within our adjectivespan of the regex pattern to accommodate recordwhen used as/as part of a modifier (e.g., a record-high 60 miles per hour), but this exploded the num-ber of detected AANNs, lowering our precisiondrastically, due to which we omitted it.To address these issues, we decided to pre-process the POS-tagged corpora prior to using ourregex, where we substituted articles of interest withthe ARTICLE tag, substituted record when pre-ceeded by an article with the RECORD tag, andnumeral proxies with the FEW tag, though ensur-ing that it appeared linearly after a known adjectivewhich was not a numeral proxy. This led to thecreation of Regex v3 (listing 3):",
  "pattern = r'\\bARTICLE\\s(((HYPH|,)\\s))?((((RB|CC|": "IN)\\s)+)?((JJ|JJR|JJS|VBN|RECORD|((NNCCNN|NNHYPH)+(JJ|JJR|JJS|VBN|RECORD)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?((JJR|JJ|VBN)\\s)?(ARTICLE\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS)INNNS)))+' This was able to handle the idiosyncracies of allpreviously detected AANNs. We again extracted afurther additional 1000 samples to hand-annotateand found 18 attested AANNs. Regex v3 was ableto detect 17 out of these (recall of 95%), missingout on only one where an incorrect form was usedin lieu of a plural noun (e.g., pound instead ofpounds). We dont really consider this a meaning-ful missed example since the singular noun actu-ally makes this a degenerate AANN, not a genuineone (but, to be conservative, count it as a miss forassessing a worst-case recall estimate). At thispoint, we stopped further refining our regex andused Regex V3 as our final detector, while alsoacknowledging that it is perhaps impossible to guar-antee whether every single AANN instance is cap-tured by the regex. shows our recall analysispipeline in a nutshell.Once detected, we map the found constructionsto their respective positions within the AANN for-mat, which allows us to measure metrics such asslot variability, etc.",
  "C.3A few/couple/dozen NOUNs": "An important phenomenon that we consider to berelated to the AANN involves cases such as: thatonly lasted a few days and could you bring mea couple liters?, etc., where the plural nouns areattached to an indefinite article. To detect suchcases, we consider the following two dependencyconfigurations, where we have an indefinite deter-miner (a, an, another) with either a det relationwith the plural noun (NNS or NNPS) or a quantmodrelation with a noun which has a nummod with theplural noun. In the former case, we usually have anamod relation between the noun and the adjective.",
  "C.4Measure NNS with Singular Verbs": "Similar to the previous case, another phenomenonwhich might be related to the AANN constructionsis when measure noun-phrases with plural nounsare treated as singular via their agreement with averbe.g., five dollars is plenty! To detect suchcases, we again rely on the following dependencyconfiguration, where we have a plural noun (NNSor NNPS) attached to a cardinal number (CD) viathe nummod dependency relation, and at the sametime also attached to singular verbs via the nsubjdependency relation (i.e., are subjects of singularverbs).",
  "DA/An + ADJ/NUM frequency balancing": "A corpus analysis of BabyLM, along with itsPOS-tagged version suggests that the sequencea/an/another (JJ|JJR|JJS) occurs 613,985times while a/an/anotherCD occurs only42,111 times this suggests that adjectives areapproximately 14.6 more likely to follow an indefi-nite article than are numerals. We therefore balancethese values by removing 571,874 instances whereadjectives follow an indefinite article. This consti-tutes the largest-sized ablation in this work."
}