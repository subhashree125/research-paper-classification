{
  "Abstract": "In recent years, large language models (LLMs)have achieved remarkable success in the field ofnatural language generation. Compared to pre-vious small-scale models, they are capable ofgenerating fluent output based on the providedprefix or prompt. However, one critical chal-lenge the hallucination problem remainsto be resolved. Generally, the community refersto the undetected hallucination scenario wherethe LLMs generate text unrelated to the inputtext or facts. In this study, we intend to modelthe distributional distance between the regularconditional output and the unconditional out-put, which is generated without a given inputtext. Based upon Taylor Expansion for thisdistance at the output probability space, our ap-proach manages to leverage the embedding andfirst-order gradient information. The resultingapproach is plug-and-play that can be easilyadapted to any autoregressive LLM. On thehallucination benchmarks HADES and otherdatasets, our approach achieves state-of-the-artperformance.",
  "Introduction": "In recent years, large language models, such asGPT-4 (OpenAI, 2023), LLaMa (Touvron et al.,2023) and PaLM (Chowdhery et al., 2022), haveachieved tremendous successes. LLMs generatefluent output based on given texts and prompts.They have been applied in various real-world sce-narios, such as dialogue systems and informationretrieval systems (Mialon et al., 2023).How-ever, despite their powerful generative capabilities,LLMs still suffer from the problem of hallucina-tions: Given an input, the model generates textunrelated to the source or even contradictory (Jiet al., 2023). Even the most advanced languagemodel currently available, GPT-4, has been shownto experience hallucinations (Bang et al., 2023).These hallucination issues severely compromisethe fairness and safety of LLMs, hindering their large-scale deployment.Indeed, to solve this issue, the task of hallu-cination detection, aiming to determine whetherhallucination occurs, has attracted more and moreattention from the community. The existing meth-ods for hallucination detection primarily focus ondetecting at the token level in the output (Zhanget al., 2023). Some methods utilize uncertaintymetrics at the token level to determine hallucina-tion (Yuan et al., 2021; Fu et al., 2023), while someother methods design prompts to query the modelmultiple times to assess the hallucination (Mndleret al., 2023). Despite the prior efforts, we be-lieve that white-box hallucination detection isnot much studied. The white-box detection settingcan both benefit the open-sourced LLM community such as LLaMA or Mistral (Jiang et al., 2023) and prompt the LLM owners to provide extraservices upon the current forms of API. Contraryto the black-box setting, a white-box setup mayutilize the full token probability matrices and theembedding layers in the transformer. In this work,we dive into this white-box setup and prove thatthis internal information may significantly benefitdetecting hallucinations.However, given the current parameter scale ofthe LLM, devising a white-box detector is non-trivial. This means we need to systematically deter-mine what portion of the information in the LLM isdiscriminative to raise the hallucination case. Themotivation for this study stems from a rather simpleand straightforward idea, as follows. We translatethe verbal definition of hallucination to be more for-mal. Take the QA task as an example, where Q andA represent the question and the models answer,respectively. P(A|Q) and P(A) represent the prob-ability distributions of the model outputting A un-der conditional Q and unconditional scenarios. Thehallucination detection task can be considered as atask to determine how much the model answer Adepends on the question Q, that is, modelling the : Two query forms of our method. Conditional query jointly inputs the question-answer text, whileunconditional query only inputs the answer text. We use zero padding in the unconditional query to align thedimension.",
  "difference between P(A) and P(A|Q) as denotedas D(P(A), P(A|Q))": "Indeed, one can directly use metrics like KLdivergence, cross-entropy or others to calculatethe difference between P(A) and P(A|Q). How-ever, these measurements would only offer a single-dimensional output as the indicator, which wefound quite imprecise towards the detection of hal-lucination. We argue that simply using the probabil-ity space for this problem overlooks the complex-ity of the hallucination issue due to the massiveamount of information wasted and dropped. Inour approach, by contrast, we first conduct Tay-lor series of D(P(A), P(A|Q)). We further showthat the subitems from the resulting series can bemodelled by the embeddings in the transformercombined sophistically with the first-order gradientinformation. This result paves the way to effec-tively detect hallucinations. provides a visual example of our ap-proach, named EGH (Embedding and Gradient-based Hallucination detection method).Morespecifically, inspired by Filippova (2020), we con-struct a dual form, (i)-the conditional tuple jointingthe source-output text versus (ii)-its unconditionalcounterpart with only the output text. After feed-ing both forms through the LLM, we yield theD(P(A), P(A|Q)) mentioned above. We cachethe corresponding necessary information, includ-ing embedding and gradient, alongside the twofeedforward passes. On top of them, we train asimple classifier to discriminate if the hallucinationoccurs. Our method is a model-agnostic solution forwhite-box hallucination detection, which relies onmodel internal signals (embedding and gradient in-formation) to perform. In order to make our methodsolid, we managed to experiment with multipleLLMs as base models. On several common hallu-",
  "Hallucination in LLMs": "In recent years, large language models, such asGPT-4 (OpenAI, 2023), LLaMa (Touvron et al.,2023), and PaLM (Chowdhery et al., 2022) havebecome the mainstream research direction in thefield of natural language processing. Accordingto the scaling law (Kaplan et al., 2020), as thenumber of parameters increases, the capabilities ofthese large language models (LLMs) also improve,allowing them to generate fluent output based onexisting text. However, due to the uncertainty in theoutputs, large language models (LLMs) also facehallucination issues, which significantly hinder thedevelopment of LLMs. Ji et al. (2023) define hallucination as naturallanguage generation models generating unfaithfulor nonsensical text. In a more granular classifi-cation, we can categorize hallucinations into twotypes: intrinsic hallucination and extrinsic halluci-nation. Intrinsic hallucination refers to generatedtext that contradicts the input, while extrinsic hallu-cination refers to outputs that cannot be derivedfrom the input, meaning unrelated to the input.While Zhang et al. (2023) categorize hallucina-tions into three types: Input-conflicting hallucina-tion, where LLMs generate content that deviatesfrom the source input provided by users; Context-conflicting hallucination, where LLMs generatecontent that conflicts with previously generated in-formation by itself; Fact-conflicting hallucination,where LLMs generate content that is not faithful toestablished world knowledge.Recent work indicates that hallucinations in",
  "Hallucination Detect Method": "One simple detection method utilizes numericalmetrics such as ROUGE (Lin, 2004) and PAR-ENT (Dhingra et al., 2019). Some researchershave also explored using models to detect hallu-cinations. FActScore (Min et al., 2023) retrievesexternal knowledge based on the question, and af-ter obtaining this external knowledge, it utilizes alanguage model (such as LLaMa-65b) to assess theconsistency between the answer and the externalknowledge in order to determine the hallucination.Filippova (2020) proposed a language model-baseddetection method where they trained a conditionalLM and an unconditional LM separately, usingtheir respective losses to measure the presence ofhallucination.Besides, some methods allow LLMs to au-tonomously assess hallucinations by designingprompts to query the model multiple times.Mndler et al. (2023) design dedicated promptsto query an evaluator LLM (e.g.ChatGPT) whetherthe subjective LLM contradicts itself under thesame context and report classification metrics, in-cluding precision, recall, and F1 score.Existing hallucination detection methods mostlyperform hallucination detection at the token level.However, when the model loses much of the in-ternal information while outputting tokens, it un-doubtedly significantly increases the difficulty ofhallucination detection. Therefore, this paper de-signs a white-box hallucination detection methodthat utilizes the internal information of the modelto detect hallucinations.",
  "Preliminary": "To simplify representation, in this section, we takea question-answering task as an example for illus-tration. The input source text is the question in theQA task, denoted as Q = {Q1, Q2, , Qm}, andthe text generated by the model is the answer tothe question, denoted as A = {A1, A2, , An}.Here, m and n represent the number of tokensof Q and A, respectively. Our task is to classifyA and obtain its hallucination label yhal, whereyhal {0, 1}.",
  "Overview": "Our method is based on the assumption that dur-ing the process of generating hallucinations, themodel tends to incorporate less information fromthe source text, and the output is unrelated to thesource text. Therefore, the extent to which themodel accesses the source text can, to a certaindegree, represent the level of hallucination. Fora language model LLM, question Q, and outputtext A, we feed Q and A into the model in two dif-ferent forms,where represent the concatenationoperation:",
  "D([Q, A]) = Difference(P(A|Q), P(A|0)) (1)": "where Difference() is the differentiable indicatorsrepresenting differences in probability distributions,such as KL divergence, cross-entropy, and so on.During the generation of hallucinated outputs, webelieve that the model receives less informationfrom Q, resulting in a smaller relationship betweenthe models output and Q. The gap D([Q, A]) issmaller. Conversely, when the model outputs nor-mally, it fully utilizes information from Q, leadingto a larger gap D([Q, A]). Therefore, D([Q, A])can be the extent to which the model accesses the : The algorithm schematic of the EGH. The step 1 part of the figure represents the extraction process offeature E = E(A|Q) E(A|0), while the step 2 part represents the extraction process of feature G = D([0, A]).Only the step 3 part (Hallucination Detector) in the figure undergoes parameter updates during the training process.",
  "+ R1([Q, A])(2)": "ItsclearthatD([0, A])isaconstant.R1([Q, A]) is a term of higher order, suchas a second derivative, with a high computationalcost to model. Considering the high computationalcost and difficulty of R1, we ignore the use ofR1 in this article and only focus on whetherhallucination detection can be performed based onthe other terms. And the remaining two terms inEquation 2, [Q, A] [0, A] and D([0, A]), canbe used as factors influecing D([0, A]). EGH aimsto identify which internal parts affect hallucinationgeneration rather than fully modelling the entiretyof D([0, A]). Therefore, we only extract featuresto represent the two factors above. In the followingtwo sections, we demonstrate the features used torepresent both entities.",
  "Embedding Information": "For the factor [Q, A][0, A], as shown in step 1 of, it means the difference between [Q, A]and [0, A]. When it comes to the token level, sub-tracting two tokens has no practical significance.Therefore, we consider using embedding layersas an alternative. E(A|Q) means the embeddingunder condition Q, while E(A|0) means the em-bedding under condition 0 (unconditional). Thedifference vectors of two embeddings can be usedas representations of [Q, A] [0, A]. Let E(A|Q)and E(A|0) be the embedding vectors correspond-ing to the answer part in two input modes, we usethe differential embedding vector between E(A|Q)and E(A|0) to represent this feature, denoted asE = E(A|Q) E(A|0).",
  "Gradient Information": "For the factor D([0, A]), as shown in step 2 of, since we used the embedding layer above,we calculate the gradient of its corresponding layeras the representation. Consider the probability dis-tributions of the answer part in two input modes,D([Q, A]) can be expressed as the sum of KL di-vergences corresponding to each token in the twoprobability distributions:",
  "After computing the KL divergence, we performgradient backpropagation to obtain the gradients": "Algorithm 1 Algorithm of EGHInput: The Question: Q = {Q1, Q2, , Qm},The Answer A = {A1, A2, , An}, The Func-tion eLLM() is to get the final embedding of theLLM. The Function pLLM() is to get the Proba-bility of the LLMs outputs. The Function dKL()means Calculate KL Divergence. The Weight Pa-rameter .Output: The hallucination label yhal {0, 1}.",
  "Embedding and Gradient-basedHallucination Detection Method": "The overall algorithm is shown in . Follow-ing the input methods described in .2, weinput Q and A to the LLM in two different forms.For feature E, we take the last hidden layer as theembedding, extract them, and obtain the differencevector E Rnh, where h is the hidden size. Forfeature G, we first calculate the Kullback-Leiblerdivergence between the two output probability dis-tributions and then perform gradient backpropaga-tion to obtain G Rnh. So far, our operationshave been solely focused on obtaining the two fea-tures, and the parameters involved during this pro-cess are frozen.After extracting the aforementioned two features,E and G, the label of hallucination, yhal {0, 1},can be represented as:",
  "Dataset": "HaluEval (Li et al., 2023) HaluEval is one of thebenchmarks for hallucination detection. It consistsof 5000 general examples and 30000 task-specificexamples, covering three tasks: question answer-ing, knowledge-grounded dialogue, and text sum-marization. For general examples, the questionscome from the Alpaca (Taori et al., 2023) instruc-tion fine-tuning dataset, where each data consistsof ChatGPTs response and hallucination labels.For task-specific examples, each data includes aquestion, ChatGPTs generated correct answer, anda hallucinated answer.SelfcheckGPT (Manakul et al., 2023) The datasetcontains 1908 sentences from 298 articles gener-ated by GPT-3. Each sentence is labelled with oneof the three veracity labels: Accurate, MinorInaccurate and Major Inaccurate. In the experi-ments, Major Inaccurate and Minor Inaccuratesentences are labelled as hallucination class, whilesentences labelled as Accurate are considerednon-hallucination class.HADES (Liu et al., 2022). HADES is a token-level hallucination detection dataset consisting of8,754 training and 1,000 validation examples. Thedataset annotates certain specific tokens in a sen-tence to determine whether these labelled tokenscontain hallucinations.",
  "EGHtrained on HaluEval87.23trained on SelfCheckGPT89.82": ": Results on the SelfCheckGPT dataset. We traintwo models on two different datasets, 10% of the Self-CheckGPT and HaluEval. The latter results demonstratethat our method possesses a certain degree of general-ization. since we need to leverage white-box information.We chose LLaMa-2-7B and OPT-6.7B as base lan-guage models for the feature extraction step. Forthe HADES dataset, to align the settings in thebaselines, we use BERT, RoBERTa and GPT-2 asthe base model, which are the same as baselines.Training Process For the HaluEval dataset, dueto there being four tasks in HaluEval (for uniformrepresentation, we also consider the general exam-ples as a task), we trained a separate model foreach task. For a specific task, We randomly select10% of the data as the training set to train the hal-lucination detector and use the remaining 90% ofthe data as the testing set for testing. For the Self-CheckGPT dataset, we conducted separate testswith and without training: the untrained test uti-lized a model trained on HaluEval for evaluation,while the trained test involved training on 10% ofthe data from SelfCheckGPT and testing on the re-maining 90% of the data. For the HADES dataset, we use the training data of HADES to obtain thehallucination detector model and test it on the vali-dation set.Baselines For the HaluEval dataset, we adopt theLLM test methods from the original paper as thebaselines. The baseline experiments utilized thefollowing prompts: You should try your best to de-termine if the answer contains non-factual or hallu-cinated information according to the above hallu-cination types. The answer you give MUST be Yesor No. We evaluate four state-of-the-art LLMs asbaselines: GPT-3(davinci), text-davinci-002, text-davinci-003 and ChatGPT. We reproduced the ex-periments in the baseline on our partitioned test set(as described above) as the final baseline result. Wealso compare the results in Zhang et al. (2024) andLei et al. (2023). Additionally, since our methodis a white-box method, we used the method fromAzaria and Mitchell (2023) as a white-box baselinefor comparison with our approach. For the Self-CheckGPT Dataset, we use the results from theoriginal paper as the baseline. We also comparethe results in Wang et al. (2023). For the HADESdataset, since the HADES dataset does not have apublicly available labelled test set, we replicatedthe benchmark experiments on the validation set,which serves as the baseline.Training Hyperparameters Our hallucination de-tector is a three-layer MLP, where the first twolayers scale the feature dimension to half of thepreceding layer, and the final layer transforms itinto probabilities. The ReLU (Glorot et al., 2011)function is chosen as the activation function. InEquation 5, is set to 0.8, respectively. In Section",
  "Results in HaluEval": "presents the results of our experimentson HaluEval. EGH has achieved state-of-the-art(SOTA) results in three task-specific tasks.Inthe QA task, EGH achieved a 97% accuracy rate,which is 35% higher than the baseline and 10%higher than the CONLI method. In the Dialoguetask, EGH achieved a 77.1% accuracy rate, whichis 5% higher than the baseline. In the Summarytask, EGH achieved a 95% accuracy rate, whichis 35% higher than the baseline. In the generalquestion-answering task, our approach achievedan accuracy of 82%, slightly lower than the base-line. Due to the imbalance of labels in the generalquestion-answering task, with only 815 out of 5000data instances being hallucination data, we considerthis result to be a normal phenomenon. Comparedwith the white-box method SAPLMA, EGH has a2-3% improvement on all four tasks.",
  "Results in SelfCheckGPT": "presents the results of our experiments onSelfCheckGPT. The model trained on the HaluEvalachieved performance comparable to the baselineand method Wang et al. (2023) on PR-AUC. Aftertraining on 10% of the data, our approach outper-formed the baseline and method Wang et al. (2023)by 2-3% on PR-AUC. The results indicate that ourmethod exhibits a certain degree of generalizationand is applicable to various tasks and datasets.",
  "Weights for the embedding and Gradient": "Recalling Equation 5, yhal = f(E + (1 )G).For the two extracted features, the difference em-bedding E and the gradient G, we respectively useweight to perform weighted summation. To ver-ify the effectiveness of the two features, we conductablation experiments using different , with a setof values for 0, 0.2, 0.5, 0.8, and 1. Since the accu-racy on QA and summary tasks is higher, makingthe ablation effects less noticeable, we conductedexperiments on the dialogue task instead. The ex-perimental results are presented in the : As shown in , when using only embed-ding or gradient ( = 0 or = 1), both accuracyand F1 score are lower than when using both fea-tures simultaneously. Therefore, both features con-tribute to the classification process. When the valueof is relatively large, the classification accuracyis higher. Specifically, when = 0.8, the accuracyis the highest, indicating that embedding plays aprimary role in classification.",
  ": Average(mean) and standard deviations(std) ofKL divergence and cross entropy": "model tends to rely less on information from theinput and engage in more free expression. Due tothese two input forms representing conditional andunconditional scenarios, respectively, the extent ofthe models creativity, namely its access to sourceconditions, can be indicated by the difference be-tween these two outputs. Therefore, we introduceDifference(P(A), P(A|Q)) to represent the differ-ence between the output probability distributionsunder two input scenarios. Mathematical quan-tities such as KL divergence, cross-entropy, etc.,can directly express the difference between twoprobability distributions, thus serving as a formof Difference(P(A), P(A|Q)). Consequently, wecompute the values of these two statistical quan-tities in both hallucination and non-hallucinationsamples and plot the probability distribution figure.As shown in , we randomly sam-pled 1,000 examples for each task in HaluEvaland obtained the probability distributions corre-sponding to hallucinated and non-hallucinateddata. The distribution of KL divergence and CrossEntropy have certain differences between non-hallucination and hallucination samples.Sincehallucinatory responses tend to receive less",
  ": The results of directly comparing KL diver-gence and cross-entropy using logistic regression andEGH indicate that applying these two features directlyfor hallucination detection is not advisable": "information from the questions, the disparityDifference(P(A), P(A|Q)) between the two out-put probability distributions will be relativelysmaller. Therefore, the KL divergence and cross-entropy will be smaller for hallucination samplescompared to non-hallucination samples. The meanvalues in the above also confirm this ob-servation.However, as mentioned earlier, using a thresholdvalue for the KL divergence or cross-entropy is im-precise to determine whether hallucinations occur.Besides, due to the loss of a significant amount ofinformation during the process of outputting prob-abilities, directly utilizing features such as proba-bilities, KL divergence, cross-entropy, etc., cannoteffectively model hallucinations. To demonstratethis, we designed a simple experiment: we used KLdivergence and cross-entropy as two-dimensionalfeatures, with Logistic Regression as the classifierfor detection. The results of the QA and Summarytasks in HaluEval are shown in . The exper-imental results support our viewpoint. Therefore,our approach extracts deeper features from thesetwo input modes for hallucination detection.",
  "Conclusion": "This paper proposes a white-box hallucination de-tection method named EGH that utilizes the inter-nal embedding and gradient of the model to deter-mine hallucination. EGH is based on the followingassumptions: during hallucination generation, themodel tends to generate responses without con-sidering the input question directly. The modelsunderstanding of the input question represents thedegree of hallucination. We designed both condi-tioned and unconditioned inputs and utilized theTaylor expansion method to demonstrate that em-beddings and gradient features can represent thisdegree. EGH achieves state-of-the-art results onhallucination detection datasets such as HaluEval,SelfCheckGPT, and HADES, validating the effec-tiveness of our method. Our work provides valu-able insights into detecting hallucinations in LLM.",
  "Limitation": "The main limitation of this method lies in the factthat, as our approach requires leveraging gradientinformation and necessitates gradient backpropaga-tion, it undoubtedly increases the time and spacecomplexity of the method. In terms of time, ourmethod requires two inputs, although it can be al-leviated through batch input. In terms of space,our method requires gradient calculation and addi-tional space to store information. In the future, wewill explore more lightweight methods to detecthallucinations.",
  "Acknowledgements": "This work is supported by the Pioneer R&D Pro-gram of Zhejiang (No.2024C01035), and theNSFC Grants (No. 62206247), and in part by theFundamental Research Funds for the Central Uni-versities 226-2024-00049. This work is also par-tially supported by ZJU Kunpeng&Ascend Centerof Excellence. Amos Azaria and Tom Mitchell. 2023. The internalstate of an LLM knows when its lying. In Find-ings of the Association for Computational Linguistics:EMNLP 2023, pages 967976, Singapore. Associa-tion for Computational Linguistics. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpt",
  "on reasoning, hallucination, and interactivity. arXivpreprint arXiv:2302.04023": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scalinglanguage modeling with pathways. arXiv preprintarXiv:2204.02311. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William W Cohen.2019.Handling divergent reference texts whenevaluating table-to-text generation. arXiv preprintarXiv:1906.01081.",
  "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and PengfeiLiu. 2023. Gptscore: Evaluate as you desire. arXivpreprint arXiv:2302.04166": "Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011. Deep sparse rectifier neural networks. In Pro-ceedings of the fourteenth international conferenceon artificial intelligence and statistics, pages 315323. JMLR Workshop and Conference Proceedings. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023. Survey of halluci-nation in natural language generation. ACM Comput-ing Surveys, 55(12):138. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Katie Kang, Eric Wallace, Claire Tomlin, Aviral Ku-mar, and Sergey Levine. 2024. Unfamiliar finetuningexamples control how language models hallucinate.arXiv preprint arXiv:2403.05612": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models.arXivpreprint arXiv:2001.08361. Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, EmilyChing, Eslam Kamal, et al. 2023. Chain of natu-ral language inference for reducing large languagemodel ungrounded hallucinations. arXiv preprintarXiv:2310.03951. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-YunNie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for largelanguage models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64496464.",
  "Potsawee Manakul, Adian Liusie, and Mark JF Gales.2023. Selfcheckgpt: Zero-resource black-box hal-lucination detection for generative large languagemodels. arXiv preprint arXiv:2303.08896": "Grgoire Mialon, Roberto Dess, Maria Lomeli, Christo-foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,Baptiste Rozire, Timo Schick, Jane Dwivedi-Yu,Asli Celikyilmaz, et al. 2023. Augmented languagemodels: a survey. arXiv preprint arXiv:2302.07842. Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of factualprecision in long form text generation. arXiv preprintarXiv:2305.14251.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Xiaohua Wang, Yuliang Yan, Longtao Huang, XiaoqingZheng, and Xuan-Jing Huang. 2023. Hallucinationdetection for generative large language models bybayesian sequential estimation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1536115371.",
  "AAnalysis of Our Experimental Results": "Our method achieved state-of-the-art results on var-ious datasets. Among all tasks, the QA task yieldedthe best performance, with an accuracy reaching97%. In QA tasks, answers are highly dependenton the questions, and there is a certain degree ofsimilarity between questions. Models are likely tomake \"heuristic errors,\" answering questions basedon past learned knowledge and experiences with-out looking at the question stem. Therefore, ourapproach focuses on extracting the models under-standing of the question, addressing the commonhallucination type in QA tasks: Input-conflictinghallucination. We have achieved excellent results inQA tasks. For dialogue tasks they typically involvelonger contexts, and the questions from differentsamples are relatively independent. Instances ofencountering the same question are less frequent,so the model is less likely to rely on \"memoriz-ing answers\". Consequently, occurrences of Input-conflicting hallucinations are less common, result-ing in less effective performance compared to QAtasks. In general QA tasks, our approach falls be-low the baseline. We believe there are two mainreasons for this: First, there is an imbalance be-tween hallucination and non-hallucination samples,with only 815 hallucination samples available. Thisleads to a significant bias towards non-hallucinationsamples in the classifier, resulting in poor classi-fication of hallucination samples. Second, in gen-eral datasets, questions tend to be common andrelatively short. Under this task, models are mostlikely to experience Fact-conflicting hallucination,where the output answers contain factual errors thatcontradict objective facts. Since our approach doesnot rely on external knowledge bases, its ability todetect such hallucinations is not very strong. Weconsider this to be a normal phenomenon."
}