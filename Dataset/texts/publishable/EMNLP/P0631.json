{
  "Abstract": "Modern language models (LMs) need to followhuman instructions while being faithful; yet,they often fail to achieve both. Here, we pro-vide concrete evidence of a trade-off betweeninstruction following (i.e., follow open-endedinstructions) and faithfulness (i.e., ground re-sponses in given context) when training LMswith these objectives. For instance, fine-tuningLLaMA-7B on instruction following datasetsrenders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performanceat following instructions when further opti-mized on tasks that require contextual ground-ing. One common remedy is multi-task learn-ing (MTL) with data mixing, yet it remainsfar from achieving a synergic outcome. Wepropose a simple yet effective method that re-lies on Rejection Sampling for Continued Self-instruction Tuning (RESET), which signifi-cantly outperforms vanilla MTL. Surprisingly,we find that less is more, as training RESETwith high-quality, yet substantially smaller data(three-fold less) yields superior results. Ourfindings offer a better understanding of objec-tive discrepancies in alignment training of LMs.",
  "Introduction": "Aligning language models (LMs) with human pref-erences becomes increasingly important. One mainobjective is to train LMs to follow human instruc-tions (e.g., answering open-ended questions) whilebeing faithful (e.g., grounding responses in thegiven context). However, LMs often suffer fromfailing to follow human instructions (Kadavathet al., 2023; Chen et al., 2023; Ji et al., 2023)or making up facts that are not grounded in con-text (Zhang et al., 2023; Wang et al., 2023a; Huanget al., 2023; Ghosh et al., 2024).We trace this problem back to commonly usedalignment training datasets, often collected fromnaturalistic conversations covering a wide range of",
  "*Equal contribution. Work done at AWS AI Labs": ": Faithfulness scores on context-dependenttasks (QA and summarization) decrease when we fine-tune grounded LLaMA-7B checkpoint with instructionfollowing datasets (orange), and instruction followingscores (assessed by GPT-4) decrease when we fine-tuneVicuna-7B with context-dependent tasks (blue). Ourmethod, RESET surpasses the vanilla MTL with datamixing, approaching the North Star (upper right corner). domains (Taori et al., 2023; Geng et al., 2023; Chi-ang et al., 2023; Christiano et al., 2017; Ouyanget al., 2022). For instance, Alpaca (Taori et al.,2023) and Dolly-15K (Conover et al., 2023) covertasks from creative writing to context-dependentQA. Particularly, these tasks may have distinct ob-jectives, and, when mixed, may induce potentialconflicts of interest during alignment.In this context, we examine the interaction be-tween instruction following (i.e., how well doesthe LM follow open-ended instructions) and faith-fulness (i.e., is the LMs response grounded in thecontext) during alignment training. Specifically, westudy how instruction following and faithfulnessscores change when adapting LMs to two typesof datasets: 1) instruction-tuning datasets that arecommonly used to train chat-models such as Chat- GPT (OpenAI, 2022) and Llama-2-Chat (Touvronet al., 2023); and 2) context-dependent datasets thatrequire grounding to a provided context, and arecommonly used to train Retrieval-Augmented Gen-eration (RAG) models (Lewis et al., 2020) suchas Atlas (Izacard et al., 2023) and DSPy (Khat-tab et al., 2024). We observe a clear trade-offbetween the two scores, as shown in :a fine-tuned LM with a competitive faithfulnessscore becomes much less faithful when separatelyfine-tuned on instruction following datasets. Con-versely, an instruction-tuned LM becomes worse atfollowing instructions when fine-tuned on context-dependent datasets.Our findings suggest fine-tuning an LM with either instruction following orcontext-dependent datasets exclusively may impairits original ability in the other aspect.One natural mitigation strategy is to use multi-task learning (MTL) by mixing datasets, whichwe find as a strong yet sub-optimal baseline. Toachieve a more synergic outcome, we propose asimple yet effective method Rejection Samplingfor Continued Self-instruction Tuning (RESET).Inspired by recent works in self-instruct (Zelikmanet al., 2022; Wang et al., 2023b), RESET leveragesthe LM to sample generations for instruction fol-lowing and task-specific datasets, which is differentfrom vanilla MTL. Generations are then rated byexternal judges for instruction following and faith-fulness scores, where top-rated generations are col-lected and used to further fine-tune the LM. Ourexperiments show that when trained with RESETusing only a single iteration and 8,000 additionalfine-tuning examples, LMs see substantial gains infaithfulness scores (up to +18.8%) compared to theMTL baseline, while maintaining their instruction-following scores. Furthermore, we find that less ismore: training with RESET on higher quality yetthree-fold less data yields up to 31.3% improve-ments on faithfulness among datasets compared toMTL. Our analyses shed lights on finding and miti-gating objective discrepancies in alignment train-ing where datasets encompass different or evenconflicting goals.1",
  "NQ69,6398,75713,3686CNN/Daily Mail287,11313,36811,49076MS MARCO153,7252,50012,46623BioASQ--1,9569SearchQA--31,7604WikiSum--2,000140": ": Data statistics of two-stage fine-tuning experi-ments. Italicized datasets are held-out unseen evaluationdatasets, while the rest are used for training. Instruc-tion Datasets here consist of publicly avaliable datasetssuch as Dolly-15K (Conover et al., 2023), ShareGPT,Self-Instruct (Wang et al., 2023b) and OASST-1 (Kpfet al., 2023). Average length is the averaged responsetoken length, which is calculated with training or testingsets for each dataset with the LLaMA-7B tokenizer. To do this, we first construct two separate groupsof datasets, each resembling a setting above. Wethen adopt a two-stage fine-tuning paradigm thatallows us to closely study the effect of each type oftraining. We introduce the setup in this section.",
  "Datasets": "We outline different datasets used for instructionfollowing and context-dependent fine-tuning. Ta-ble 1 shows data statistics. For each dataset wecarefully design the instructions to well-align theinput and output (see .3). 2.1.1Instruction Following DatasetsWe curated an instruction following training datasetby compiling unique examples from publicly avail-able datasets such as Dolly-15K (Conover et al.,2023), ShareGPT2, Self-Instruct (Wang et al.,2023b), and OASST-1 (Kpf et al., 2023). For eval-uation, we gather unique examples from Alpaca-15K (Taori et al., 2023), Vicuna-eval (Chiang et al.,2023), and Koala-eval (Geng et al., 2023). Data Pre-processingWe exclude examples thatoriginally come with context (e.g., examples la-beled as summarization type in Dolly-15K are fil-tered out) to prevent overlaps with our context-dependent datasets. We retain only unseen exam-ples in our evaluation sets. For OASST-1, we onlyinclude examples with an average human ratinghigher than 0.5 (i.e., the higher the rating is, the",
  "###Response:Model Generated Answer Goes Here": ": The prompt template we used for training andevaluation for LLaMA-7B. The instruction field con-tains task-specific instruction, the input field containscontexts if applicable, and the response field is followedby models generation. We use a different templatefor Vicuna-7B since it uses a different template duringinstruct-tuning phase. Actual input examples for eachdataset and model are included in Appendix A.6. higher the quality is) and that are rated by at leasttwo annotators. To refine ShareGPT, we includeonly examples with responses that are longer than10 words, split by whitespace. For other instructionfollowing training datasets, we exclude exampleswith empty instructions or responses. 2.1.2Context-Dependent DatasetsWe aim to evaluate the models generation forfaithfulness to the given input context. We se-lect a range of context-dependent datasets fromthree task domains: (1) extractive QA includ-ing NQ (Kwiatkowski et al., 2019), BioASQand SearchQA, taken from the RobustQA bench-mark (Han et al., 2023); (2) abstractive QA withMS MARCO (Bajaj et al., 2016) where the answersare well-formed sentences grounded in context;and (3) abstractive summarization including CNNDailyMail (Hermann et al., 2015; Nallapati et al.,2016) and WikiSum (Liu et al., 2018). BioASQ,SearchQA and WikiSum are hold out for evaluationand the rest are for training. One crucial advantageof using these context-dependent datasets is thatthey provide us with a reliable way of measuringfaithfulness, in terms of how well the response isgrounded in the given context.",
  "Evaluation Metrics": "In the context of our datasets, we evaluate our mod-els with three metrics: instruction following score,faithfulness score and task performance score. Thestandard methods for measuring instruction follow-ing and faithfulness of language models are subjectto ongoing debate. In this work, we employ widelyadopted approaches for these measurements andcompile a set of metrics for more stable evalua-tions.4 To present our findings, we report macro-averaged results across all test datasets. Instruction Following ScoreWe adopt the com-monly used evaluation paradigm proposed by LLM-as-a-Judge (Zheng et al., 2023), and zero-shotprompt GPT-4 to provide a rating followed by ashort explanation (i.e., named as LLM-as-a-Judge(R) in the paper).5 For the GPT-4 evaluator, we setthe temperature to 0 for stability with a maximumgeneration length of 512. We check instruction fol-lowing scores only for instruction following eval-uation datasets. See Appendix A.1 for our actualevaluation prompt. Faithfulness ScoreFor extractive QA datasets,we utilize the span coverage score as our metric(i.e., whether the predicted answer is a span withinthe context). We apply standard normalization toboth the predicted answer and the context (see Ap-pendix A.2 for details). A score of 1.0 is assignedif the span is covered, and 0.0 otherwise. Addi-tionally, we include unigram and bigram coveragefor selected datasets to further refine our faithful-ness evaluation in the Appendix (see and). For abstractive QA and summarizationdatasets, we employ SummaC-ZeroShot (SummaC-ZS; Laban et al. (2022)) to assess whether the pro-vided context (with the question concatenated as aprefix for QA datasets) entails the model-generatedanswer. Specifically, we segment both the context MARCO (Bajaj et al., 2016).4Throughout the paper, we sample a subset of the fullevaluation data which include 6,000 examples (1,000 exam-ples from each context-dependent evaluation set), and sample300 examples (100 examples from each instruction followingevaluation set) due to limited compute resources.5We use gpt-4-0613.",
  "Instruction Template": "Our datasets follow the same instruction-tuningtemplate format as used in the Alpaca setting (Taoriet al., 2023).This template includes a headerthat outlines general guidelines, followed by task-specific instructions, as illustrated in . ForQA tasks, the question is presented after the re-trieved passages. Objective-Aligned InstructionsWe design ourtask-specific instructions to minimize objective-conflict among datasets. For example, in prompt-ing our model for a context-dependent extractiveQA task, we explicitly instruct the model to ex-tract a specific text span from the given passages.This template helps models to reduce hallucinationwhen fine-tuned with instruction following datasets:as models getting better at understanding humaninstructions, they also get better at understandinghow to extracting a span which ensures the answerto be faithful. The instructions for all datasets aredepicted in . Our task-specific templatesensure there is no objective conflict when we fine-tune our models with mixed of datasets. We usethe same template for training and evaluation foreach task.",
  "Two-stage Fine-tuning Paradigm": "To understand the trade-off between instructionfollowing and faithfulness when training LMswith both objectives, we formulate a two-stagefine-tuning paradigm to answer our research ques-tions (as illustrated in ).6 For our firstpipeline, we initially fine-tune our LM with context-dependent datasets that require grounding. We thentake the best checkpoint from the initial stage tofurther fine-tune it on instruction following datasets(CDIF). Conversely, in our second pipeline, wefine-tune instruct-tuned LM (e.g., Vicuna-7B) withcontext-dependent datasets (IFCD). For bothpipelines, we measure instruction following andfaithfulness scores before and after training, togauge the impact of the second-stage training onboth capabilities. We follow this paradigm to findevidence of the trade-off in and . ModelsWe use two models in our two-stagefine-tuning paradigm. We use a base LM LLaMA-7B (Touvron et al., 2023), one of the most widelyused open-source LM, for the CDIF pipeline. Forour IFCD pipeline, we use Vicuna-7B off-the-shelf as our instruct-tuned LLaMA-7B without re-training one from scratch. Vicuna-7B is one of themost competitive open-source chat-model, and is a",
  "Does Fine-tuning with InstructionFollowing Data Hurt Faithfulness?": "To answer this question, we follow the first pipelineoutlined in , where we take our LLaMA-7B that is fine-tuned on context-dependent datasetsand further fine-tune it with instruction followingdatasets. Our results are shown in . First ofall, instruction following scores increase drasticallyas expected, from 0.30 to 0.74. Meanwhile, faith-fulness scores on abstractive datasets drop 33.0%from 0.82 to 0.55, while the task performance islargely maintained (from 0.34 to 0.32). For ex- tractive datasets, both faithfulness scores and taskperformance see a relatively small drop. To providea fine-grained view of these datasets, we provide ad-ditional analysis of per-task and per-metric changesof faithfulness score as well as task performance inAppendix A.4 and Appendix A.5.One potential confounding factor for faithfulnessscores dropping on abstractive tasks is the lengthof the generated response. The model could simplyhave generated longer responses () as a re-sult of training on intruction tuning data with longerresponses (). To rule this out, we re-evaluateour models separately with two contrasting groups:evaluated with only those examples with shortergeneration length (less than or equal to), and thoseexamples with much longer generation length (100more tokens) compared with the golden answer. Asshown in , both short and long generationssee very substantial drops in faithfulness, whilelonger generations indeed see larger drops. Thisnevertheless supports our conclusion that instruc-tion following training hurts faithfulness.",
  ": Macro-averaged faithfulness, instruction following, and task performance scores on correspondingevaluation datasets before and after fine-tuning with context-dependent datasets": ": Instruction following scores for 1,000 ran-domly selected examples from Alpaca-15K (left), andfor filtered examples with similar generation lengths(maximally 10 tokens longer) before and after fine-tuning with context-dependent datasets (right). 37.9%, suggesting our model becomes much lesshuman-aligned. Similar to previous findings, onenatural concern is that context-dependent trainingmakes our model generate much shorter responses,which leads to lower instruction following scores.To rule out this concern, we randomly sample 200examples from the Alpaca-15K dataset and onlykeep those with a minimal sequence length change(a maximum of 10 new tokens) compared to thecorresponding Vicuna-7B zero-shot generation. Asshown in , instruction following scores con-sistently drop considerably for the filtered setting,regardless of length (36.8% vs. 22.0%). Overall,context-dependent fine-tuning negatively affectsthe instruction following score, as we expected.",
  "RESET starts with a model that is fine-tuned ona mixture of instruction following and context-dependent datasets and reconciles the two objec-tives with the following steps ():": "Sample GenerationsFor a random subset of thetraining datasets, we sample generations from thecheckpoint with different decoding settings. Specif-ically, we focus on two hyperparameters by ran-domly changing one of them at a time by enumer-ating all possible values. For the decoding temper-ature, it takes on a value from {0.1, 0.2, 0.3, 0.4,0.5, 0.6, 0.7}. For top-k, we set it to take on a valuefrom {5, 10, 20, 50, 70, 90, 100}. When we varythe temperature, we fix top-k to be 0. Similarly,when we vary the top-k, we fix the temperatureat 0. For each example, if we run the samplingprocedure once, it will sample 7 examples in total. External JudgesWe then use a set of externaljudges to rate our collected generations as in Sec-tion 2.2. These judges are offline evaluators. Weevaluate generation based on instruction follow-ing score, faithfulness score and task performance.One potential limitation is that our model may over-fit to existing judges on seen datasets. We thusinclude unseen datasets to test generalizability. Top-1 Weighted ScoreFor each generation, wecollect a set of scores from our external judges.Then, we take a weighted sum of these scores be-fore ranking the generations and picking the toprated one. The score is weighted score of taskperformance stask, instruction following sinstr andfaithfulness scores sfaith,",
  "score = stask + 2.0 (Iinstr sinstr + Ifaith sfaith)": "where we use Iinstr and Ifaith to indicate whether theexample is from our instruction following datasetsor context-dependent datasets. We pick the toprated sample per example and combine them into acontinued fine-tuning dataset as described next. : The illustration of our proposed method RESET. It samples generations from the initial vanilla multi-tasklearning (MTL) checkpoint with seen examples from instruction following and context-dependent datasets. Foreach example, it generates a set of possible responses with different decoding strategies. Generations are rated byexternal judges with a weighted scores of task performance, faithfulness and instruction following scores. Then, thetop rated generations will be collected to further fine-tune the initial model. Continued Fine-tuningFor RESET, we ran-domly sample training data from each of our fourdataset in and collect 2,000 additional fine-tuning examples per dataset. In total, our new fine-tuning dataset has 8,000 examples. With this smallcollected dataset, we further fine-tune our startingcheckpoint model for a single epoch with a smallerlearning rate to avoid overfitting. This continuedfine-tuning step is very lightweight as the train-ing data is usually less than 1% of the MTL train-ing step. Overall, RESET resembles a reject sam-pling based preference learning paradigm, whichhas been proven effective (Touvron et al., 2023)while drastically saving training costs as well as in-creasing stability. Other experimental setup detailsare included in Appendix A.3. Supercharged RESET (RESET-S)In addition,we evaluate RESET with a different setting to testthe impact of the quality of our continued fine-tuning dataset. Specifically, we supercharge thequality of our additional fine-tuning dataset by sam-pling 1 more generations. We swap our instruc-tion following judge from GPT-4 to the weakerChatGPT7. One potential benefits is that ChatGPTis a weaker judge with a lower recall for good gen-erations. As a result, examples rated high by Chat-GPT may have higher quality. While samplingfor generation, we decrease the curated fine-tuningdataset by 3-fold (2,000 examples in total). We fine-tune our checkpoint model with the same setup.",
  "We use gpt-4-0613": "We fine-tune Vicuna-7B with context-dependentdatasets without any MTL objective. The purposeis to establish a potential upper bound of faithful-ness score without any further training for instruc-tion following. (3) w/ MTL: We fine-tune Vicuna-7B with baseline MTL by mixing instruction fol-lowing and context-dependent datasets together.(4) w/ RESET: We fine-tune Vicuna-7B directlywith our collected continued fine-tuning dataset(8,000 examples) from our RESET pipeline with-out MTL training first. Note that the curated datasetcomprises of sampled generations from our MTLcheckpoint. The purpose is to evaluate whetherMTL is necessary for model improvements. (5) w/MTL+RESET: We follow to fine-tuneour MTL checkpoint with curated dataset. (6) w/MTL+RESET-S: We follow to fine-tuneour MTL checkpoint with the high-quality versionof our curated dataset (2,000 examples).We evaluate our models on both seen and unseentesting datasets. As shown in , fine-tuningVicuna-7B with a mixture of datasets close the gapon instruction following score substantially (from0.49 to 0.75) while leaving headroom across theboard. Next, models fine-tuned with RESET signif-icantly outperform the MTL baseline on both seenand unseen testing datasets. Our results also sug-gest directly fine-tuning Vicuna-7B performs worsecompared to fine-tuning a model checkpoint afterMTL training. Last but not least, our results withRESET-S provide strong evidence that data qualityis more important than data quantity and that usingmultiple iterations of sampling helps, as RESET-Sachieves similar or better performance with 3-foldless training data. Due to space limit, we presentqualitative model generations in Appendix A.7.",
  "Related Work": "Instruction Following of LMsThere are a vari-ety of instruction following training datasets (Taoriet al., 2023; Geng et al., 2023; Chiang et al., 2023;Wang et al., 2023b) covering tasks from wildly dif-ferent domains such as poetry creation to context-dependent summarization. In this paper, we focuson a particular trade-off between instruction follow-ing and model faithfulness, which are fundamentalobjectives for modern LMs. In addition to train-ing datasets, LM generations are often assessed byhuman experts (Ouyang et al., 2022) or model scor-ers (Zheng et al., 2023; Ethayarajh et al., 2022). Weuse GPT-4 to access instruction following scoresas in Chiang et al. (2023). Faithfulness and Groundedness of LMsBe-ing faithful or grounded is crucial for tasks likecontext-dependent QA or summarizations, reduc-ing hallucinations for LMs (Zhang et al., 2023;Wang et al., 2023a; Huang et al., 2023; Kadavathet al., 2023; Chen et al., 2023; Ji et al., 2023)Unlike task performance, faithfulness measureswhether generated answers are based on the givencontext (Rashkin et al., 2021; Dziri et al., 2022;Paranjape et al., 2022). For context-dependent QAand summarization benchmarks, common metricsinclude subsequence-based lexical matching (Pap-ineni et al., 2002; Lin, 2004; Banerjee and Lavie,2005), natural language inference (NLI; Labanet al., 2022; Fabbri et al., 2022), and more recently,LLMs as factuality scorers (Chiang and Lee, 2023;Liu et al., 2023; Kamalloo et al., 2023). We uselexical matching and NLI to measure faithfulnessby checking whether the answer is grounded. Instruction Following Training with LMsBaseLMs are often tuned to follow human instruc-tions (Wei et al., 2022; Mishra et al., 2022; Wanget al., 2022; Chung et al., 2022). Various post-hoc fine-tuning techniques have been proposed to align base LMs with human preferences, suchas supervised fine-tuning on instruction-followingdatasets (Taori et al., 2023; Chiang et al., 2023)or variances of RLHF and RLAIF (Schulmanet al., 2017; Ouyang et al., 2022; Lee et al., 2023;Rafailov et al., 2023; Touvron et al., 2023). Thesetechniques can require training policy and rewardmodels which is not cost-efficient. Inspired byalignment training with reject-sampling (Touvronet al., 2023), we propose a simple yet effectivemethod that uses reject-sampling by self-instruct.On the other hand, there exist various advance MTLtechniques (Liu et al., 2019b; Crawshaw, 2020)prior to the development of LMs. However, RE-SET is the only variant that leverages LMs self-instruct generations to further fine-tune the LM.More importantly, RESET is complimentary to dif-ferent instruct-tuning techniques by leveraging theLM and external evaluators to generate high-qualitycontinued fine-tuning datasets. Compared with re-cent works showing how instruct-tuning may causehallucination (Ghosh et al., 2024), we provide amore rigorous analysis with our two-stage fine-tuning paradigm and focus on the interaction be-tween instruction following and faithfulness.",
  "Conclusion": "In this paper, we demonstrate a clear trade-off be-tween instruction following and faithfulness whenfine-tuning LMs on datasets with differing objec-tives. To alleviate this trade-off, we proposed RE-SET, a simple yet effective iterative method thatsignificantly outperforms the MTL baseline in bothinstruction following and faithfulness scores. Thelightweight and iterative nature of RESET makesit extensible for future refinement at minimumcost and integrable with recent instruct-tuning tech-niques. We contribute to the broader goal of creat-ing more reliable, accurate, and user-aligned lan-guage technologies.",
  "The limitations of our work are as follows:": "Our study primarily focuses on the LLaMA-7B and Vicuna-7B models, that are among thebest open-source models at the time of thiswork. While we posit that our findings andthe proposed RESET method could generalizeacross other language models, our findingscould remain speculative without evaluatingon more current model types (e.g., LLaMA-2,Mistral or Mixtral at various scales). The datasets chosen for fine-tuning and evalu-ation, though comprehensive, are not exhaus-tive. There are other interesting datasets thatare not covered in this study. For instance,long-form QA where the answers are muchlonger than 1-2 phrases or sentences. Ourinstruction following datasets can also be fur-ther categorized into creativity-driven, world-knowledge driven and others to help us todisentangle discrepancies in objectives betteracross datasets. Our evaluation relies heavily on automatedmetrics and external judges like external LMsfor assessing instruction following and faith-fulness. While these methods are standard,they cannot fully encapsulate the nuanced un-derstanding and preferences of human evalua-tors. For future research, evaluating responseswith human annotators would provide addi-tional validations. Although the purpose of our study is to studythe objective discrepancies in the datasets andcome up with mitigation strategies withoutanother novel training paradigm, it wouldstrengthen our results if we can compare ourmethod with more recent alignment trainingmethods. Although we evaluate RESET on unseendatasets, our method still has the potential tooverfit to certain evaluators. Future work mayuse a different set of evaluators for a morerobust evaluation. Human evaluation is chal-lenging for our unseen testing datasets, suchas WikiSum, because the input and responseare extremely long, which could also makehuman ratings unstable.",
  "Ethics Statement": "In this paper, we delve into the nuances of instruct-tuning LMs, a process that involves balancing atrade-off between instruction following and faith-fulness. We feel that the immediate ethical andsocietal consequences of our research may be lim-ited, and none which we feel must be specificallyhighlighted here. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-drew McNamara, Bhaskar Mitra, Tri Nguyen, MirRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,and Tong Wang. 2016. Ms marco: A human gen-erated machine reading comprehension dataset. InAdvances in Neural Information Processing Systems(NeurIPS). Satanjeev Banerjee and Alon Lavie. 2005. METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments. In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Translationand/or Summarization. Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern,Siyang Gao, Pengfei Liu, and Junxian He. 2023.Felm: Benchmarking factuality evaluation of largelanguage models. In Advances in Neural InformationProcessing Systems (NeurIPS) Datasets and Bench-marks Track.",
  "Cheng-Han Chiang and Hung-yi Lee. 2023. Can largelanguage models be an alternative to human evalua-tions? In Association for Computational Linguistics(ACL)": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-tic, Shane Legg, and Dario Amodei. 2017. Deepreinforcement learning from human preferences. InAdvances in Neural Information Processing Systems(NeurIPS). Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowd-hery, Alex Castro-Ros, Marie Pellat, Kevin Robin-son, Dasha Valter, Sharan Narang, Gaurav Mishra,Adams Yu, Vincent Zhao, Yanping Huang, An-drew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,Jeff Dean, Jacob Devlin, Adam Roberts, Denny",
  "Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-lace, Pieter Abbeel, Sergey Levine, and Dawn Song.2023. Koala: A dialogue model for academic re-search. Blog post": "Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Ku-mar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ra-mani Duraiswami, and Dinesh Manocha. 2024. Acloser look at the limitations of instruction tuning. InarXiv preprint arXiv:2402.05119. Rujun Han, Peng Qi, Yuhao Zhang, Lan Liu, JulietteBurger, William Yang Wang, Zhiheng Huang, BingXiang, and Dan Roth. 2023. RobustQA: Benchmark-ing the robustness of domain adaptation for open-domain question answering. In Findings of Associa-tion for Computational Linguistics (ACL). Karl Moritz Hermann, Tom Kocisk, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. 2015. Teaching machines to readand comprehend. In Advances in Neural InformationProcessing Systems (NeurIPS). Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.A survey on hallucination in large language models:Principles, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232.",
  "Grave. 2023. Atlas: Few-shot learning with retrievalaugmented language models. In The Journal of Ma-chine Learning Research (JMLR)": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023. Survey of hal-lucination in natural language generation. In ACMComputing Surveys. Saurav Kadavath, Tom Conerly, Amanda Askell, TomHenighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, EliTran-Johnson, et al. 2023. Language models (mostly)know what they know. In Findings of Associationfor Computational Linguistics (ACL). Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke,and Davood Rafiei. 2023. Evaluating open-domainquestion answering in the era of large language mod-els. In Association for Computational Linguistics(ACL). Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,Zhiyuan Zhang, Keshav Santhanam, Sri Vard-hamanan, Saiful Haq, Ashutosh Sharma, Thomas T.Joshi, Hanna Moazam, Heather Miller, Mateia Za-haria, and Christopher Potts. 2024. DSPy: Com-piling declarative language model calls into self-improving pipelines. In International Conferenceon Learning Representations (ICLR). Andreas Kpf, Yannic Kilcher, Dimitri von Rtte,Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-ley, Richrd Nagyfi, et al. 2023.Openassistantconversationsdemocratizing large language modelalignment. In Advances in Neural Information Pro-cessing Systems (NeurIPS) Datasets and BenchmarksTrack. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. In Transactions of the Association of Com-putational Linguistics (TACL). Philippe Laban, Tobias Schnabel, Paul N. Bennett, andMarti A. Hearst. 2022. Summac: Re-visiting nli-based models for inconsistency detection in sum-marization. In Transactions of the Association ofComputational Linguistics (TACL). Harrison Lee, Samrat Phatale, Hassan Mansoor, ThomasMesnard, Johan Ferret, Kellie Lu, Colton Bishop,Ethan Hall, Victor Carbune, Abhinav Rastogi, andSushant Prakash. 2023. Rlaif: Scaling reinforcementlearning from human feedback with ai feedback. InarXiv preprint arXiv:2309.00267. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. In Advances inNeural Information Processing Systems (NeurIPS).",
  "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-feng Gao. 2019b. Multi-task deep neural networksfor natural language understanding. In Associationfor Computational Linguistics (ACL)": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023. G-eval:NLG evaluation using gpt-4 with better human align-ment. In Empirical Methods in Natural LanguageProcessing (EMNLP). Swaroop Mishra, Daniel Khashabi, Chitta Baral, andHannaneh Hajishirzi. 2022. Cross-task generaliza-tion via natural language crowdsourcing instructions.In Association for Computational Linguistics (ACL). Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,Caglar Gulcehre, and Bing Xiang. 2016. Abstrac-tive text summarization using sequence-to-sequenceRNNs and beyond.In Proceedings of the 20thSIGNLL Conference on Computational Natural Lan-guage Learning, pages 280290, Berlin, Germany.Association for Computational Linguistics.",
  "OpenAI.2022.Introducingchatgpt.URL": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems (NeurIPS).",
  "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation. In Association forComputational Linguistics (ACL)": "Ashwin Paranjape, Omar Khattab, Christopher Potts,Matei Zaharia, and Christopher D Manning. 2022.Hindsight: Posterior-guided training of retrievers forimproved open-ended generation. In InternationalConference on Learning Representations (ICLR). Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D. Manning, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems (NeurIPS). Hannah Rashkin, David Reitter, Gaurav Singh Tomar,and Dipanjan Das. 2021. Increasing faithfulness inknowledge-grounded dialogue with controllable fea-tures. In Association for Computational Linguisticsand International Joint Conference on Natural Lan-guage Processing (ACL-IJCNLP).",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schel-ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-ramanian, Xiaoqing Ellen Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Au-relien Rodriguez, Robert Stojnic, Sergey Edunov,and Thomas Scialom. 2023. Llama 2: Open founda-tion and fine-tuned chat models. In arXiv preprintarXiv:2307.09288. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-gru Tang, Tianhang Zhang, Cheng Jiayang, YunzhiYao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.2023a. Survey on factuality in large language mod-els: Knowledge, retrieval and domain-specificity. InarXiv preprint arXiv:2310.07521.",
  "model with self generated instructions. In Associa-tion for Computational Linguistics (ACL)": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran,Anjana Arunkumar, David Stap, Eshaan Pathak,Giannis Karamanolakis, Haizhi Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,Shailaja Keyur Sampat, Siddhartha Mishra, SujanReddy A, Sumanta Patro, Tanay Dixit, and XudongShen. 2022. Super-naturalinstructions: Generaliza-tion via declarative instructions on 1600+ nlp tasks.In Empirical Methods in Natural Language Process-ing (EMNLP). Association for Computational Lin-guistics. Jason Wei, Maarten Bosma, Vincent Y. Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M. Dai, and Quoc V. Le. 2022. Finetuned lan-guage models are zero-shot learners. In InternationalConference on Learning Representations (ICLR).",
  "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-man. 2022. Star: Bootstrapping reasoning with rea-soning. In Advances in Neural Information Process-ing Systems (NeurIPS)": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, et al. 2023. Sirens song in the ai ocean:A survey on hallucination in large language models.In arXiv preprint arXiv:2309.01219. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judgingllm-as-a-judge with mt-bench and chatbot arena. InAdvances in Neural Information Processing Systems(NeurIPS) Datasets and Benchmarks Track.",
  "A.2Evaluation Metric: Faithfulness Score": "Text NormalizationWe use regular expressionsto replace spaces around hyphens, slashes, and be-fore s. We then remove all the articles (e.g.,a and the) and punctuations. We lowercase allletters for simplicity. Common Failure ModeThe authors also humanlabel models generation to check whether modelsgenerations are paraphrased version of the goldenanswers. The most common failure mode is themodel extracting a non-existent span or a wrongspan. This supports our findings of models beingunfaithful.",
  "A.3Experimental Setup": "We train our model for a maximum of a singleepoch8 across all training jobs. We up-sample thesmaller datasets to match the number of examplesin the larger ones when combing datasets for train-ing. The learning rate is set at 1 105 with abatch size of 16 for our faithfulness-driven training.For training jobs involving instruction followingdatasets, we increase the batch size to 32. Trainingis conducted using bfloat16 precision with a max-imum sequence length of 2048. The weight decayis set to 0.05, with a cosine learning rate schedulerand a warm-up ratio of 0.03. We save checkpointsevery 100 training steps and evaluate them basedon perplexity scores on the evaluation set. The best-performing checkpoint is then selected for the nextstage of training. Our models are trained using thestage-3 deepspeed library. We train each modelwith three random seeds and average the resultsfor consistency. For each training job, our modelsare trained on 8A100 GPUs within a single-nodesetup, with the total training time not exceeding 24hours. For model generation, we employ greedy de-coding with a maximum generation length of 480,which aligns with the maximum response lengthacross the training datasets.For the contined fine-tuning step in RESET, weuse a smaller learning rate of 8 106 and keepother settings the same.",
  "Experiments with up to three epochs showed minimalchanges in results": "[System]Please act as an impartial judge and evaluate the quality of the response provided by an AIassistant to the user question displayed below. Your evaluation should consider factors such as thehelpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Beginyour evaluation by providing a short explanation (strictly 1-2 short sentences). Be as objectiveas possible. After providing your explanation, please rate the response on a scale of 1 to 10 bystrictly following this format: [[rating]], for example: Rating: [].",
  "A.5Quandary of Mixed Training onAbstractive and Extractive QA andSummarization Datasets": "shows faithfulness score and task per-formance across related datasets when we fine-tune LLaMA-7B on context-dependent datasets.One suprising finding is that throughout the fine-tuning process, the faithfulness scores on extractiveQA datasets gradually decrease, while task per-formance scores gradually increase. On the otherhand, this trend is not salient for abstractive tasksas shown in .",
  "and show two qualitative examplesof actual model generations from our experiments": ": Individual faithfulness score and task performance with extractive QA datasets evaluated with threedistinct model checkpoints of LLaMA-7B fine-tuned on context-dependent datasets. The middle checkpoint is theone with lowest in-training evaluation loss. : Individual faithfulness score and task performance with extractive QA datasets across evaluated withthree distinct model checkpoints through the instruction following fine-tuning. The middle checkpoint is the onewith lowest in-training evaluation loss. : Individual instruction following score on alignment datasets evaluated with three distinct modelcheckpoints of LLaMA-7B fine-tuned on context-dependent datasets. The middle checkpoint is the one with lowestin-training evaluation loss. : Individual instruction following score on alignment datasets evaluated with three distinct modelcheckpoints through the instruction following fine-tuning. The middle checkpoint is the one with lowest in-trainingevaluation loss. : Individual faithfulness score and task performance with abstractive QA and summarization datasetsacross evaluated with three distinct model checkpoints of LLaMA-7B fine-tuned on context-dependent datasets.The middle checkpoint is the one with lowest in-training evaluation loss. : Individual faithfulness score and task performance with abstractive QA and summarization datasetsacross evaluated with three distinct model checkpoints through the instruction following fine-tuning. The middlecheckpoint is the one with lowest in-training evaluation loss.",
  "Testing Example from WikiSum Dataset": "PassageTake your pencil and on the top of the page, about two inches from the right start drawing a line to 2 inches(5.1cm) below the right top corner. The line should loop down and have points. (see the picture) Draw straightlines from the points in your first line to the corner. Make lines parallel to your fist line going all the way up.You should have 5 or 6 lines. Get a paper and make a cross on it, try to make both lines a similar length (using aruler will help) Draw diagonal lines through the centre, dividing the paper up from 4 to 8 sections. Make surethey are smaller than the cross you made before. Start connecting the lines with inverted arcs, this is an arc ),from the inside out. Once you have reached the end of the web, elongate the diagonal lines, (this will makeit look like it has supports). Draw a spider by making a fuzzy ball, then drawing legs (eight of them) on yourweb. Or see the spider drawing tip. Finished. Draw a circle and draw a cross section which also extends outsidethe circle. Draw two diagonal lines at the midpoint of the cross-sections which form an X-mark. Draw squareswhich descend in size as it nears the centre point. Draw the corners or vertices of the square along the diagonallines. Draw diamond shapes descending in size as it nears the centre point. Draw the vertices along the lines ofthe cross-section. Draw curves to connect the lines 2013 from the squares to the diamonds, much like formingbridges. Trace with a pen and erase unnecessary lines. You may add drawings for spiders. Color to your liking!",
  "Golden Answer": "ResponseTo draw a spider web, start by drawing a cross with 2 lines of equal length. Next, draw diagonal lines throughthe center to divide your web into 4 to 8 equal sections. Then, connect the lines with inverted arcs, or curvedlines, from the inside out. When you reach the end of the web, elongate the diagonal lines to make the web looklike its hanging from something, like a tree or shrub. Finally, draw a spider in your web by making a fuzzy ballwith 8 legs.",
  "### Input:Question: a bond that the issuer has the right to pay off before its maturity date": "Passage: does well. Companies also reserve the right to call their bonds, which mean they can callit sooner than the maturity date. Often there is a clause in the contract that allows this; forexample, if a bond issuer wishes to rebuy a 30-year bond at the 25th year, they must pay a premium.If a bond is called, it means that less interest is paid out. Failure to pay a bond effectivelymeans bankruptcy. Bondholders who have not received their interest can throw an offending companyinto bankruptcy, or seize its assets if that is stipulated in the contract.Passage:Callable bond A callable bond (also called redeemable bond) is a type of bond (debtsecurity) that allows the issuer of the bond to retain the privilege of redeeming the bond at somepoint before the bond reaches its date of maturity. In other words, on the call date(s), the issuerhas the right, but not the obligation, to buy back the bonds from the bond holders at a definedcall price. Technically speaking, the bonds are not really bought and held by the issuer but areinstead cancelled immediately. The call price will usually exceed the par or issuePassage: options embedded. Callable bond A callable bond (also called redeemable bond) is a typeof bond (debt security) that allows the issuer of the bond to retain the privilege of redeemingthe bond at some point before the bond reaches its date of maturity. In other words, on the calldate(s), the issuer has the right, but not the obligation, to buy back the bonds from the bondholders at a defined call price. Technically speaking, the bonds are not really bought and held bythe issuer but are instead cancelled immediately. The call price will usually exceed the parPassage: the amount on which the issuer pays interest, and which, most commonly, has to be repaidat the end of the term. Some structured bonds can have a redemption amount which is different fromthe face amount and can be linked to the performance of particular assets. The issuer has to repaythe nominal amount on the maturity date. As long as all due payments have been made, the issuerhas no further obligations to the bond holders after the maturity date. The length of time untilthe maturity date is often referred to as the term or tenure orPassage: Bond (finance) In finance, a bond is an instrument of indebtedness of the bond issuer tothe holders. The most common types of bonds include municipal bonds and corporate bonds. The bondis a debt security, under which the issuer owes the holders a debt and (depending on the termsof the bond) is obliged to pay them interest (the coupon) or to repay the principal at a laterdate, termed the maturity date. Interest is usually payable at fixed intervals (semiannual, annual,sometimes monthly). Very often the bond is negotiable, that is, the ownership of the instrument can be",
  "### Instruction:Summarize the text in a few sentences. Using original phrases or paraphrasing them if necessary.Do not include new information beyond the given passages": "### Input:. Condition score the horse Condition scoring is a process in which you assess the amount of fatthe horse has on it. Condition scoring requires that you look at and feel the horses body andassess the amount of fat it is carrying in specific areas. With condition scoring, you can evaluatewhether the horse is in ideal condition or not. Once you look at and feel an area, you will writedown your assessment in a chart made specifically for condition scoring. Condition scoring can takesome instruction and practice, so you may want to consult with your veterinarian for some guidanceon the procedure. A horse needs some fat to get it through the winter but it shouldnt have so muchfat on it that it becomes obese. Use a weight tape. A weight tape is a tool used to approximatea horses weight. It is a measuring tape that is wrapped around a horses back and chest and themeasurement markings are in pounds or kilograms. Using a weight tape will not give you a completelyaccurate measurement. It is only an estimate. It is best used for assessing change over time. Havethe horse weighed. If you are bringing your horse to a veterinary clinic or a center that has ahorse scale, then you can have the horse actually weighed. However, this is not usually availableto horse owners on a regular basis.Using a scale is the most accurate way to weigh a horse.Measure the horses weight regularly. In order to get an accurate understanding of changes to thehorses weight over time, you will need to measure it on a regular basis. If you are very concernedabout a horses weight, this can be every week. If you think the horse is maintaining weight fine,then every couple of weeks should be fine. Use the same type of weight measurement every time youmeasure the horse. This is the only way to really assess changes in weight over time. Take thehorses winter coat into consideration. When a horse is in the cold during the winter, it can growa thick winter coat. This can interfere with weight tape measurements and condition scoring, whichis why it is important to put your hands on your horse as you take these measurements. This willenable you to feel for fat pockets and ribs beneath the horses wooly coat. With this coat changein mind, try to be consistent with the pressure and placement of weight tape and how you feel thehorses body when condition scoring. If you focus on consistency, any changes you document willusually signal a real trend in weight change. Be sure to remove a horses blanket daily to assessweight gain or loss. Record your horses weight over time. To track the horses weight, you willneed to know what its weight was before the winter. Then, you will need to keep a record of itsweight throughout the winter. Make sure to write down each weight along with the date in a journalor notebook. This will be helpful to share with your horses veterinarian later on. Be sure thatyou measure the horses weight the same way every time.For instance, if you use a horse tapeto measure the weight in the Fall, then use it to make subsequent measurements. Get professionalhelp. If you are unsure if your horses weight loss is of concern or you are unsure how to helpyour horse gain weight, you should get some professional advice. Talk to your veterinarian aboutwhat and how much you should be feeding your horse to fatten it up. Your veterinarian will alsobe able to recommend further bloodwork and diagnostics if they suspect that something else, suchas parasites or disease, may be causing your horses weight loss. Weight loss, if at all, shouldbe gradual. Contact your veterinarian immediately if you notice extreme changes in your horsesweight over a short period of time. Check your horses manure. Your horses manure can providesome helpful clues about their eating and drinking habits. If you notice anything different, thencall your veterinarian. Some instances where you would want to call your horses vet include stoolthat is: Runny or wetter than usual, such as diarrhea.Drier than usual and/or less frequent,which may indicate constipation. This could be caused by not being able to access water becauseit is frozen.Check your horses water often in cold weather to make sure it is not frozen.Adifferent texture or color than is normal for your horse. Identify the signs of an emergency. Ifyour horse has lost too much weight it can be an emergency that requires immediate veterinarycare. Assess your horses vital signs. Make sure that it is breathing regularly and that it hasa strong pulse. A horse with irregular breathing or a weak pulse, or that is weak and unable tomove, is in desperate need of emergency veterinary care. If any of its vital signs are not strong,you should call a veterinarian immediately. Help your horse regain weight. Horses should eat 2 -2.5% of their body weight in good quality bulky foods, such as grass or hay, during winter. Forthe average 1,000 pound horse, this would mean eating 25 pounds.Make sure that your horse isgetting enough. Getting your horse to gain weight requires that you feed it more calories everyday than you have been. Discuss making one of the following changes to your horses diet with yourveterinarian: Give the horse unrestricted access to hay around the clock. Change to a higher gradeof hay. Add in a complete feed. Add 4 to 8 ounces of corn or cocosoya oil to a small amount ofgrain. Add beet pulp, sweet feed, or concentrates to your horses diet.",
  "### Input:Question: 2015 college half term holiday dates": "Passage: UK School Half Term Dates 2015, 2016 and 2017. Research the latest school holidays andterm dates for England, Scotland, Wales and Ireland. Welcome to Half Term Dates. The place to viewSchool Holidays for the UK, Ireland, France and Australia. We list the latest published half termtimes on one handy website.Passage: UK School Half Term Dates 2015, 2016 and 2017. Research the latest school holidays andterm dates for England, Scotland, Wales and Ireland. Welcome to Half Term Dates. The place to viewSchool Holidays for the UK, Ireland, France and Australia.Passage: UK School Half Term Dates 2015, 2016 and 2017. Research the latest school holidays andterm dates for England, Scotland, Wales and Ireland.Passage: Find your child2019s school term, half term and holiday dates on your local council 2019swebsite.Passage:School term dates-guide.These are the school term and holiday dates for schools inLambeth. The dates shown do not take account of the five professional development days when schoolsare closed to pupils, or any other changes.Passage: Find your child 2019s school term, half term and holiday dates on your local council 2019swebsite. School term and holiday dates vary across the UK.Passage: Irish School Holidays, Mid Term Dates. Research the official half term breaks and schoolholidays in Ireland. The Irish authorities do not enforce the same holidays as the UK. Noticeabledifferences being the lack of early June half term and a 9 week long summer holiday. Commencingend of June for most Irish schools.Passage:School term dates-guide.These are the school term and holiday dates for schools inLambeth. The dates shown do not take account of the five professional development days when schoolsare closed to pupils, or any other changes. Please check with your childs school for more detailedinformation.Passage: School term dates and holidays 2014/15. Here are the school term dates and holidays forSandwells primary, secondary and special schools in 2014/15. Some academies and voluntary aidedchurch schools may not follow this schedule. You are advised to check with these schools directly.Passage: Holiday Dates for college students*: 1 Autumn Half Term: 26 October 2015 to 30 October2015 (teaching re-starts from 3/4 November 2015 due to staff P&D days 2013 check with your tutor).2 Winter Break: 18 December 2015 to 5 January 2016 (teaching re-starts from 5/6 January 2016 dueto staff P&D days 2013 check with your tutor).",
  "### Input:Question: abnormalities in which chromosomes were linked to the moyamoya disease?": "Passage: moyamoya disease. We postulate that a protein encoded on chromosome 21 may be related tothe pathogenesis of moyamoya disease. Although the neuronal substrate is abnormal in Down syndromepatients, recovery from hemiplegic stroke in patients with MM-DS is comparable to recovery inpatients with primary moyamoya.Passage: Moyamoya disease (MIM 252350) is characterized by stenosis or occlusion of the terminalportions of the bilateral internal carotid arteries and by abnormal vascular networks at thebase of the brain.There is a high incidence of moyamoya disease in Asia, especially in Japan.Multifactorial inheritance is estimated with lambda(s)>40. Previous linkage studies have indicatedthat susceptibility loci for the disease are located on chromosomes 3p, 6q, and 17q. In the presentstudy, we searched for loci linked to the disease in 12 Japanese families using 428 microsatellitemarkers and found significant evidence for linkage to 8q23 [maximum LOD scorePassage: We reported an autopsy case of Downs syndrome with moyamoya syndrome. A 30-year-old malewith Downs syndrome suffered from a cerebral infarction and died of brain herniation. Cerebralangiography showed vascular abnormalities that were the same as moyamoya disease.Pathologicalfindings revealed multiple stenosis of main trunk of the cerebral arteries. Pathologically, thestenosed vessels showed eccentric intimal thickness with cholesterin deposit, unlike moyamoyadisease. There are only two previous reports of autopsied cases of Downs syndrome with moyamoyasyndrome. We postulate that a protein encoded on chromosome 21 may be related to the pathogenesisof Downs syndrome with moyamoyaPassage: other). The karyotype was normal. No mutation in the RFN213 gene was found, and none ofthe HLA types linked to moyamoya disease or described in similar familial cases were identified.By describing these multisystemic associations, polycystic kidney disease for the second time, andintestinal malformation for the first time in the literature, our report expands the phenotypicvariability of moyamoya syndrome. The coexistence of disparate malformations among close relativessuggests an underlying common genetic background predisposing to structural or physiologicalabnormalities in different tissues and organs.Passage: OBJECTIVE: We report a detailed description of a family affected by a hereditary multisystemdisorder associated with moyamoya syndrome.METHODS: In this family case report, we evaluated 9membersofthesamefamilyoriginatingfromAlgeria.Investigationsincludedneuroimaging,cardiologicandophthalmologicevaluation,hormonaltesting,hemoglobinelectrophoresis,chromosomal karyotyping, muscle biopsy for morphology, immunohistochemistry and enzyme assays,mtDNA mutation screening, and haplotype analysis of 2 loci previously linked to moyamoya, onchromosomes 10 (ACTA2) and 17.RESULTS: Five males related through a maternal lineage were affected,suggesting an X-linked inheritance. Four of them had symptomatic moyamoya syndrome with an onsetof acute",
  "### Instruction:Answer to the question by extracting a specific text span from the given passages. Do not includenew information beyond the given passages": "### Input:Question: 30 days in the holef you cant name this old peter frampton band whose name refers to aforced apologyPassage: CLASSIC YUMMY ROCKERS | 30 Days in the Holef you cant name this oldPeter Frampton band whose name refers to a forced apology | Humble Pie. right:.Passage: 30 Days in the Hole by Humble Pie song meaning, lyric interpretation, video and... Steve Marriot had said that Peter Frampton had heard the early stages of this ...of mine with the Dead End Kids from the movie of the same name, Dead End,... Dirty room: one full of evidence that can get you busted; especially if you...Passage: 30 Days in the Holes the seventh single by English rock group Humble Pie,from the bands ... The song refers to Borstal - some seeds and dust, and you gotBorstal- referring to Borstal Prison and its borstal ilk ... Humble Pies GreatestHits {Featuring Peter Frampton & Steve Marriott]; Best of Humble Pie; ClassicsVolume...Passage: Apr 1, 2012 ... Perhaps it was my new loose and mellow attitude, perhaps I had ... As Iwroteearlier this month, it is now illegal to sell an old piano with ... Needless to say thiswould devastate the antique industry and force a ...... 30 Days In The Hole ... Thissong of Humble Pie was recorded after Frampton left the band...Passage: The Irish town of Kerry lends its name to this colorful breed of pooch | A Kerryblue .... Peter Frampton band whose name refers to a forced apology | HumblePie."
}