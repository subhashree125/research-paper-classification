{
  "Abstract": "Understanding how Transformer-based Lan-guage Models (LMs) learn and recall informa-tion is a key goal of the deep learning commu-nity. Recent interpretability methods projectweights and hidden states obtained from theforward pass to the models vocabularies, help-ing to uncover how information flows withinLMs. In this work, we extend this method-ology to LMs backward pass and gradients.We first prove that a gradient matrix can becast as a low-rank linear combination of its for-ward and backward passes inputs. We thendevelop methods to project these gradients intovocabulary items and explore the mechanicsof how new information is stored in the LMsneurons.Our code is available at: .",
  "Introduction": "Deep learning models consist of layers, which areparameterized by matrices that are trained using amethod known as backpropagation. This processinvolves the creation of gradient matrices that areused to update the models layers. Backpropaga-tion has been playing a major role in interpretingdeep learning models and multiple lines of study ag-gregate the gradients to provide explainability (Si-monyan et al., 2014; Sanyal and Ren, 2021; Cheferet al., 2022; Sarti et al., 2023; Miglani et al., 2023).Recent interpretability works have introducedmethods to project the weights and intermediateactivations of Transformer-based LMs (Vaswaniet al., 2017) into the vocabulary space. The semi-nal Logit Lens method (nostalgebraist, 2020) haspaved the way to explaining LMs behavior dur-ing inference (Geva et al., 2022a; Dar et al., 2022;Ram et al., 2023), including directly interpretingindividual neurons (Geva et al., 2021; Katz andBelinkov, 2023). Our work is the first, as far as wecan ascertain, to project LM gradients to the vo-cabulary space. Furthermore, modern LMs contain : An illustration depicting the tokens promotedby a single LMs MLP layer and its gradient during theforward and backward pass when editing the model toanswer Paris for the prompt Lionel Messi plays for.The gradients (in green) of the first MLP matrix, FF1,attempt to imprint into the models weight (in blue) theinformation that FF1 encountered during the forwardpass. Utilizing a vocabulary projection method, wereveal that this information represents the token team.The gradients of the second MLP matrix, FF2, aim toshift the information encoded within FF2 towards theembedding of the new target. thousands of neurons in each layer, while certainfeatures are likely distributed across multiple neu-rons (Elhage et al., 2022; Cunningham et al., 2023).These issues are handled in our examination of thegradient matrices by performing a decompositionof provably low-rank matrices. Despite the popularity of LMs, our understand-ing of their behavior remains incomplete (Benderet al., 2021; Dwivedi et al., 2023), particularly re-garding how LMs acquire new knowledge duringtraining and the mechanisms by which they storeand recall it (Dai et al., 2022; Geva et al., 2021,2023; Meng et al., 2023). We identify a mecha-nism we refer to as imprint and shift, which cap-tures how information is stored in the feed-forward (MLP) module of the transformer layer. This mod-ule has two fully connected layers, FF1 and FF2.The imprint refers to the first layer, to or fromwhich the learning process adds or subtracts copiesof the intermediate inputs encountered during theforward pass. The shift refers to the second ma-trix, where the weights are shifted by the embed-ding of the target token; see .In summary, our contributions are: (i) Analyzingthe rank of gradients. (ii) Interpreting gradientsby inspecting relatively small spanning sets. Inparticular, we examine the Vector-Jacobian Product(VJP) obtained during the backward pass, which isthe equivalent of the hidden states of the forwardpass. (iii) Investigating the embedding of these setsby projecting them into tokens. (iv) Revealing atwo-phase mechanism by which models learn tostore knowledge in their MLPs, which we termedimprint and shift. (v) Exploring a novel editingmethod based solely on a single forward pass.",
  "Related Work": "Developing methods to explain LMs is centralto the interpretability community (Belinkov andGlass, 2019; Srivastava et al., 2023). Initially in-spired by interpretability efforts in vision models(Sundararajan et al., 2017; Samek et al., 2017;Zhang and Zhu, 2018; Indolia et al., 2018; Olahet al., 2020), LMs have also benefited from the abil-ity to operate in the language domain. This includesleveraging projections of vectors into readable con-cepts (nostalgebraist, 2020; Simhi and Markovitch,2023) or clustering them into the idioms they pro-mote (Cunningham et al., 2023; Tamkin et al.,2023; Tigges et al., 2023; Bricken et al., 2023).Reverse engineering the gradients role in shift-ing model behavior has been a primary method tocomprehend the mechanics of deep learning mod-els. Recent work (Ilharco et al., 2022; Gueta et al.,2023; Tian et al., 2023) demonstrates that cluster-ing the weights that models learn during training orfine-tuning reveals patterns that connect the tasksand their training data. Other approaches employ-ing Saliency Maps (Simonyan et al., 2014) explorethe relationship between a gradient matrix and partsof its corresponding forward pass input, where wehave observed that gradients are spans, linear com-binations, of those inputs. Our work differs fromprevious studies in two key aspects: (1) we inter-pret gradients based on the components that gener-ate them, primarily the VJPs, rather than analyzing entire gradient matrices, and (2) we elucidate the in-formation stored in the gradients and project theminto tokens, instead of solely examining their im-pact on the models predictions or their connectionto the training data. To the best of our knowledge,we are the first to explore both projecting gradi-ents into tokens and examining them through VJPs,offering a perspective that has yet to be explored.Our experiment regarding LM editing adds tothe line of work that utilizes interpretability forknowledge editing. The closest idea to our imple-mentation was introduced by Dai et al. (2022), whoidentified activated neurons for specific idioms inencoder LMs and altered them by injecting the em-bedded target. We show that gradients work ina very similar way. Other state-of-the-art modelediting methods include Mitchell et al. (2021) andMeng et al. (2022, 2023).Our work analyzes the gradients rank.Itwas previously known that gradients are low-rank(Mitchell et al., 2021), but the utilization of thischaracteristic for interpretability study or predict-ing the rank of an edited prompt have remained un-explored. Optimizers and adaptors, such as LoRA(Hu et al., 2022), were created to constrain the rankof gradients, making fine-tuning faster. In contrast,our work demonstrates that the ranks of gradientscan be predicted based on the edited prompt length.",
  "Background": "We first provide background on transformers, fo-cusing on the components that play a role in ouranalysis and omitting other concepts, such as LayerNorms and positional embedding, which are ex-plored in full by Vaswani et al. (2017) and Rad-ford et al. (2019). We then provide the necessarybackground on the backward pass; a more compre-hensive description is given by Clark (2017) andBishop (2006). Finally, we discuss the buildingblocks of the Logit Lens method.",
  "Transformer LMs": "Generative Pre-trained Transformer (GPT), is anauto-regressive family of architectures containingmultiple transformer blocks. Given a prompt se-quence of n tokens, GPT predicts a single token.The architecture maintains an embedding dimen-sion d throughout all layers. First, the input tokensare embedded using an embedding matrix E intovectors X = [x1, , xn] Rnd. This is mir-rored at the final stage, in which a decoding matrix D projects the output of the last transformer blockinto a score for each token within the vocabulary.Each transformer block comprises an attentionlayer (Attn) and a Multi-Layer Perceptron (MLP)layer, interconnected by a residual stream. Theattention mechanism transfer vectors (information)from each of the preceding inputs to the currentforward pass. In our study, we do not delve intothis module and refer the reader to Radford et al.(2018) for more details.The MLP layer (also known as FFN, Feed-Forward Network) consists of two fully connectedmatrices FF1, FF T2 Rddm, with an acti-vation function f between them: MLP(X) =f(XFF1)FF2.Hence, the calculation that the l-th trans-former block performs on its input hidden state,Xl, is given by Xl+1=Xl + Attn(Xl) +MLP(Attn(Xl) + Xl).",
  "Backpropagation": "Backpropagation (Rumelhart et al., 1986; Le Cun,1988) is an application of the chain rule to computederivatives and update weights in the optimizationof deep learning network-based models. The pro-cess begins with the model executing a forwardpass, generating a prediction y, which is subse-quently compared to a desired target by quantifyingthe disparity through a loss score L. Following this,a backward pass is initiated, iterating through themodels layers and computing the layers gradientsin the reverse order of the forward pass.For a given layer of the model that during the for-ward pass computed z = xW, where x Rd1, z Rd2 are its intermediate input and output, we com-pute its gradient matrix using the chain rule:",
  "W = x. Theother derivative = L": "z Rd2 is known as theVector-Jacobian Product (VJP) of z.It can bethought of as the hidden state of the backward passand is the error factor that later layers project back.In LMs, the output of the model is an unnormal-ized vector, y R|vocabulary|, representing a scorefor each of the models tokens. We denote the tar-get token by an index t [|vocabulary|]. Typicallythe Negative Log-Likelihood (NLL) loss is used:",
  "Vocabulary Projection Methods": "nostalgebraist (2020) discovered that we can trans-form hidden states from LMs forward passes intovocabulary probabilities, thereby reflecting theirintermediate predictions. Termed as Logit Lens(LL), this method projects a vector x in the sizeof the embedding space d by applying it with theLMs decoding, the process that transforms the lasttransformer blocks output into a prediction:",
  "LL(x) = Softmax(lnf(x)D) R|vocabulary|(7)": "where lnf is the models last Layer Norm beforethe decoding matrix D.The projection captures the gradual building ofLMs output (Millidge and Black, 2022; Haviv et al.,2023), and projections from later layers are moreinterpretable than earlier ones. Efforts such as Dinet al. (2023) and Belrose et al. (2023) try to solvethis gap by incorporating learned transformationsinto LL. However, to emphasize our main discov-eries, we have not included such enhancements,which primarily aim to shortcut the models compu-tations and require dedicated training procedures.An artificial neuron performs a weighted sumof its inputs, and appears as a column or a row of the models matrices taken along a direction thathas a dimensionality d. Static neurons can also beprojected into tokens using LL: Geva et al. (2021),Geva et al. (2022b) observe that neurons of the firstMLP matrix FF1 determine the extent to whicheach neuron in FF2 contributes to the intermediateprediction. Dar et al. (2022), Geva et al. (2023)employ the same approach to investigate the at-tention matrices. Elhage et al. (2021), Katz andBelinkov (2023) demonstrate how these neuronscan elucidate model behavior, Wang et al. (2023),Millidge and Black (2022), Todd et al. (2023) useit to explore circuits and in-context learning.Despite the growing interest in this approach, weare only aware of works that have applied it to thestatic weights of models or the hidden states of theforward pass. In contrast, our work is focused onthe backward pass of LMs.",
  "The Rank of the Gradient Matrices": "Hu et al. (2022) and Mitchell et al. (2021) haveobserved the low-rank of MLP layers gradientswith a single input. However, they did not explainthis phenomenon in the context of a matrix with asequence of inputs, nor did they predict this rank.Building on the observations of Kiani et al. (2022),who bounded the rank of gradients in convolutionaland vanilla recurrent networks, we define the fol-lowing lemma for GPT:",
  "Proof. According to Equation 6, the gradient of amatrix is L": "W = x. Assuming x, are non-zerovectors, the rank of the gradient matrix is 1, givenits interpretation as a span, a linear combinations,of a single column vector x, or equivalently, as aspan of a single row . In the case when x or is azero vector, the rank of the gradient matrix is 0.In LMs, an input prompt comprises a sequenceof n tokens, each of which introduces an interme-diate input (xi) at every layer. In this case, the",
  "i=1xi i(8)": "The maximum rank of the summed gradient matrixis n given each xi, or i, is linearly independent,since we sum n distinct rank-1 matrices. This rankwould be lower than n if there are linear dependen-cies between xi or between i, with 0 being theminimum possible rank. For instance, if a lineardependency exists between only two is, the rankof the gradient matrix would be n 1.",
  "Gradient Matrices as Spans of Vectors": "In our analysis we focus on the MLP layers, dueto recent interest in identifying and editing theknowledge stored in these layers neurons (Gevaet al., 2022b, 2021; Dai et al., 2022; Mitchell et al.,2021; Meng et al., 2022). Consider the matricesof the MLP modules, FF1 and FF2, each havingdm neurons, which are d-sized vectors. Explor-ing all the modules dimensions is prohibitive, seeAppendix C. However, Equation 8 reveals that ev-ery gradient matrix is a sum of n outer productsxi i. This view allows us to examine every gradi-ent matrix as a sum of n pairs of vectors. Therefore,assuming we edit a sequence of inputs with undera few hundred tokens, our analysis would focus ononly n d < dm vectors in Rd.Every matrix formed by xi i can be interpretedin two ways simultaneously: (1) as a span (linearcombination) of xi and (2) as a span of i. illustrate the two viewpoints. We utilize this dualityand examine gradients as the spans of n vectors,xi or i, guided by the observation that for eachmodule there is only one span with vectors that arein the size of the modules neurons. The gradients of FF1i is not of size d and thusare not suitable for methods that examine vectors ofthe hidden state size. However, xi are d-sized vec-tors and were previously explored using LL (Gevaet al., 2022b; Dar et al., 2022). Therefore, for FF1we chose to observe the gradient matrix as a spanof xi, considering i as the factor determining theextent of the update each xi will introduce (the up-per option in ). Explicitly, we refer to xias FF1s spanning set since the j-th neuron of the : The calculation of gradient matrix by theouter product of x . Each row consists of the samevalues, but above we describe the matrix as a span (linearcombinations) of , while below as a span of x. Thedisplayed vectors are presented transposed to emphasizethe spanning effect.",
  "The VJPs of the Top Layer": "In this section, we analyze the initial VJPs that arecreated during the editing of a single prompt, aswe describe in .2. The last matrix of anLM, which is the final model parameter used be-fore calculating the loss score, is the decoding ma-trix D Rd|vocabulary|. During the forward pass,this matrix calculates the output vectors xiD = yi.When editing a prompt, we only use the final pre-diction of the last prompts token xnD = yn tocalculate the loss score.We calculated Ds VJP of the last token n. Ac-cording to Equation 5, the backward pass VJP tothe layer that preceded D is given by the followingbackward step:",
  "and using the notation p = Softmax(yn), t fromEquation 2:": "Lemma 5.1. The VJP nD passed at the begin-ning of a backward pass is a vector in Rd that is asum of weighted token embeddings. It is dominatedby the embedding of the target token, D[t], multi-plied by a negative coefficient n[t] = p[t]1. Theembedding of all other tokens k = t are scaled bya positive coefficient p[k]D[k]. The VJP nD is the initial vector to be passedin the backward pass, if we simplify the effect ofLayer Norms, which are discussed in Appendix B.In particular, this is the only VJP to span the lastMLP layers gradient.TheLLofnisprovidedasSoftmax(lnf(nD).Except for lnf,thisbehaves similarly to Softmax(nD).As thelemma shows, nD[t] is negative, while nD[k] is positive for all tokens k = t. Therefore, we canexpect the target embedding to have the lowestprobability in the softmax.In practice, sincerelated tokens have more similar embeddings andsimilar entries in yn, this effect is expected to beeven more pronounced.",
  "Storing Knowledge in LMs": "In .2 we observed that each neuron in theMLPs gradients is a sum of vectors in Rd fromthe forward and backward passes, xi and i respec-tively. Based on this observation, we aim to under-stand how LM editing with a single prompt and asingle backward pass changes the internal knowl-edge of a model. Explicitly, we study the implica-tions of updating a weight matrix with its gradients:W W + L",
  "W = ni=1 xi iwe are interesting on the direct effect of updatinga layer by the gradient of a single token from theedited prompt, xi i": "Lemma 5.2. When updating an MLP layer of anLM using backpropagation only with the VJPs ofthe i-th token, i, and rerunning the layer with thesame inputs xi from the forward pass of the promptwe used for the editing, the following occurs: (i)The inputs, xi, are added or subtracted from theneurons of FF1, thereby adjusting how much theactivations of each corresponding neuron in FF2would increase or decrease. (ii) The VJPs i aresubtracted from the neurons of FF2, amplifying inFF2s output the presence of the VJPs after theyare multiplied with negative coefficients. See Appendix D for the proof.Since the change in FF1 uses the given inputsxi to amplify future activation, we term this mecha-nism imprint. The modification of FF2 is termedas the shift, since it represents a process of al-tering the output of the layer. In summary, theimprint and shift mechanism depicts the MLPslearning process during a single backward pass ashaving two phases: Given the layers original inputand the new target, the process imprints a similarinput through the update of FF1 and subsequentlyshifts the output of FF2 towards the new target. illustrates this process.Updating with the full gradient: In practice,the gradient matrix is the sum of individual contri-butions, each given by the outer product of a token,xi i. However, as we will demonstrate in Sec-tion 6, only a few tokens dominate the full update. : The percentage of occurrences where the rankof FF1s gradient equals the length of the prompt usedfor editing. To show different models in the same plot,we normalize the layer indices. Except for the last layer,all layers and models exhibit the above equality morethan 98.5% of the time. Therefore, for the simplicity of our explanation, wecan regard the complete update as consisting of justa few individual steps of imprint and shift.LL ranking refers to the index assigned to thevocabularys tokens when ordered by the proba-bility scores generated by LL (Equation 7). Up-dating FF1 involves adding or subtracting xi fromweights, focusing on the most probable tokens fromthe LL ranking. Conversely, for FF2, updating en-tails subtracting i, effectively adding 1 i. Thissubtraction reverses the LL rankings, turning pre-viously least probable tokens into most probableones. Thus, when utilizing LL with FF2s i, at-tention should be given to the least probable tokensfrom the projections.",
  "Experiments": "We conduct a series of experiments to support theresults of Sec. 4 and 5, as well as to briefly demon-strate their application to LM analysis.We employ GPT2 (Radford et al., 2019) andLlama2-7B (Touvron et al., 2023) in our experi-ments. We randomly sampled 100 prompts andtheir corresponding editing targets from the Coun-terFact dataset (Meng et al., 2022). For each modeland prompt, we conducted a single backpropaga-tion using SGD and without scaling optimizers,such as Adam (Kingma, 2014), and no batching. TherankofthegradientsToexamineLemma 4.1, we measure the rank of each layergradient matrix. As depicted in , for everyprompt with the length of n tokens, the modelsgradient matrices are almost always exactly rankn. The only exceptions are the last MLP layers,which have a rank of 1, as predicted in .Although unnoticeable from the figure, once in afew dozen examples, there is a drop of one or two",
  "(b) Jack Dorsey - IBM": ": The gradient of GPT2-small FF2 when (a) editing the model to answer Paris for the prompt Obamagrew up in, and (b) when editing the model to answer IBM for Jack Dorsey founded. Each cell shows theLogit Lens projection of the gradients VJP (i) for a token input and a layer. Non-English characters are replacedwith a question mark, and long tokens are truncated with ... According to .2, instead of showing the mostprobable token in each cell, we display the least probable one. The color indicates the norm of the VJP, with whitecells indicating that almost no editing is done in practice.",
  "in the rank of the gradients, indicating linear de-pendency in xi or i, see .1. This is nota result of a repeated token, since the positionalencoding would still lead to a different xi": "Logit Lens of GradientsNext, we present exam-ples of our gradients interpretation through LL in and in Appendix E. In each cell of theseplots, the LL projections of the chosen spanningset (FF1s xi and FF2s i) are presented for aspecific layer and a token from the prompt that wasused for the editing. Prior studies that projected the forward pass ex-amine the LL projections of hidden states, high-lighting the gradual change in the projected tokensbetween layers (nostalgebraist, 2020; Haviv et al.,2023). Similarly, a presents a gradualchange in the backward pass VJP. Across most lay-ers, LL reveals that the gradients represent the em-bedding of Paris. Other projections have seman-tics that are related to Paris such as Macron,the family name of the President of France. Thenorm of the VJP is indicted by color, and, in thetop layers, the only meaningful updates are for thetoken Paris. Some of the edits in the lower layersare harder to explain, similarly to the situation inthose layers for the vanilla LL of the forward pass.Another example, presented in b, showssimilar patterns: when attempting to edit the modelto suggest that Jack Dorsey, known as one of Twit-ters founders, founded IBM, we observe that mostgradients correspond to the embedding of IBM.",
  ": The norm of GPT2-xls FF2s VJPs (i) as afunction of the layers index and segments of the editedprompts. White color represents close-to-zero updateswith almost no effect on the models weights": "Impact of Different Segments of the PromptWe observe that while all the prompts tokens con-tribute to the gradient construction (Equation 8),the majority of these contributions are done byVJPs, i, with a close-to-zero norm. Furthermore,upon examining the LL of every individual neuronfrom the gradient matrix (Appendix C), we foundthat all the projected tokens are correlated with only1-2 vectors we can identify from the spanning setspresented in .2.To discern the relative importance of tokens andlayers in the gradient reconstruction, we divideeach prompts tokens into segments and plot theiri mean norm. This experiment is done with GPT2-xl, due to its extensive use in prior work on inter-pretability research.Results are depicted for FF2 in , seeAppendix F.1 for FF1. Evidently, predominant updates occur in two main areas: (1) by the sub-jects tokens in the initial layers, and (2) by thelast prompts token around the second quarter ofthe layers. The majority of other tokens exhibit anorm close to zero throughout the layers, indicat-ing that they have almost no effect on the updating.We hypothesize that the changes to the last subjecttoken may involve editing the information trans-ferred by the subjects token through attention, asdemonstrated by Geva et al. (2023).A complementary view is provided by consider-ing the LL rank of the target token for each VJPi (labeled by the segment of token i of the input). illustrates that the VJP of the last tokenfrom the edited prompts, n, consistently ranks thetarget token among the least probable ones. TheVJPs of other tokens from the edited prompt, i,exhibit comparable behavior, generally ranking thetarget token as improbable.The result reveals that along the first and lastlayers, some of the i show degradation in theirranking of the target token, which we attributeto their low norms as reflected by . Wedemonstrate in Appendix H that normalizing i be-fore LL magnifies the presence of the target token.Specifically, the drop at the models last layer isdue to the fact that apart from the last promptstoken, all the others have a zero vector i at thatlayer (Appendix A).Please note that the degradation of this rank inthe first few layers might be related to the gap in LLinterpretability for the earlier layers discussed in.3. In Appendix F.2 we provide a similaranalysis for FF1s gradients.",
  "Application: Editing Based on theShift Mechanism": "Prior work (Mitchell et al., 2021; Meng et al., 2022,2023) introduced editing methods that change onlyFF2s matrices. In .2 we identify theshift mechanism of editing FF2. In weobserve that the dominant components in construct-ing the gradients are derived from the outer productof the last tokens input and VJP, xn n, and thatn contains the embedding of the target token.We hypothesize that we can edit LMs internalknowledge by updating only a single FF2 matrixwith a single forward pass and an approximationof the VJP, thereby eliminating the need for thebackward pass. Based on Lemma 5.1, the embed-ding of the editing target is annotated by D[t], : The Logit Lens rank of the target token forGPT2-xl FF2s VJPs, i. Most gradients tend to rankthe target token as one of the least probable tokens, withthe last token consistently ranking it as such. We foundthat the degradation of the first and last layers can beattributed to the proximity of certain i norms to zero.In the initial layers, Logit Lens is less effective, therebyresulting in lower readability for the earlier layers. where D is the decoding matrix and t is the in-dex of the target token. Our experimental methodworks as follows: (i) We choose an MLP layer wewish to edit,FF2 (predefined as a hyper-parameteraccording to Appendix I). (ii) We run a singleforward pass with the prompt whose output wewant to edit. (iii) During the forward pass, wecollect the last token input for the layer we wantto edit, xn. (iv) We collect the embedding of thetarget token D[t] and (v) update the MLP ma-trix byFF2 FF2 + xn D[t], where isthe learning rate. We name this method forwardpass shifting. As motivation, please observe thatxn( FF2 + xn D[t])D[t]xn FF2D[t] = xn22 D[t]22. We examined our method on 1000 samples fromCounterFact (; the full results and addi-tional implementation details are presented in Ap-pendix I), and found that for single editing our ap-proach is on par with the state-of-the-art methodsMEND (Mitchell et al., 2021), ROME (Meng et al.,2022) and MEMIT (Meng et al., 2023), in editinga given prompt, but it falls short in comparisonto ROME in generalization (editing paraphrases)and specificity. However, our method has muchlower runtime complexity and does not employ amulti-step (iterative) execution. Overall, our resultssuggest we might be able to find shortcuts in theimplementation of fine-tuning by injecting tokensdirectly into LMs layers.",
  "MEND71.417.67.73623.94ROME99.471.910.91622.78MEMIT79.440.710.98627.18FORWARD PASS SHIFT99.441.66.02622.45": ": GPT2-xl single editing results for CounterFact. Efficacy represents the editing success rate (accuracy).Paraphrase denotes the accuracy of predicting the new target for phrases derived from the edited prompt. Neigh-borhood examine the model accuracy on prompts we wish the edit will not change but have similar domain to theedited one. N-gram measures generation fluency using weighted bi- and tri-gram entropies.",
  "Conclusions": "Other LL-type interpretability contributions shedlight on LMs through the forward pass. Here, weshow that gradients can be projected into the vocab-ulary space and utilize the low-rank nature of thegradient matrices to explore the backward pass inan interpretable way. As we show, the gradients arebest captured by a spanning set that contains eitherthe input to each layer, or its VJP. These two com-ponents, which are accessible during the forwardand backward passes, are used to store informationin the MLP layers, using a mechanism we call im-print and shift. We provided experimental resultsto substantiate the results of our analysis, includingan editing method that only requires a single for-ward pass, but is on par with the SOTA knowledgeediting methods. Future Work:The focus of this work is in under-standing the mechanism behind the simplest formof LM editing. We hope our observations can servefurther research on how knowledge is embeddedinto the models parameters. Scaling our experi-ments to multiple editing tasks or full fine-tuningwould result in losing the low-rank characteristicthat we utilize in this work. However, our find-ings in suggest that only a small set ofVJPs and subspaces dominate the editing process.Therefore, future research could interpret complexediting scenarios by examining only the most dom-inant VJPs, or by decomposing the full gradientsinto low-rank components using techniques suchas singular value decomposition (SVD).Notably, this work did not explore the similaritybetween gradients of different sentences editing.The question of how subspaces are shared between different edits and tasks remains an open question,which has been recently discussed in the contextof superposition (Elhage et al., 2022). While mostwork has examined superposition phenomenon bylooking at the forward pass of LMs, exploring howinformation is embedded into the same subspacesmight shed light on how LMs work within theirconstrained embedding space.In addition, in this work we utilize Logit-Lens(LL) projection, which is known to have reduced in-terpretability abilities in the earlier layers of modelscompared to later ones. Future research could buildon our findings by interpreting VJPs using alterna-tive projection methods, such as those proposed byDin et al. (2023), or by employing techniques likesparse autoencoders (Bricken et al., 2023).",
  "Limitations": "Our use of LL in projecting gradients has limita-tions when it comes to explaining the gradientsof earlier layers. At this point, it remains unclearwhether gradients operate in the same embeddingspace across all layers or if another transforma-tion is required for projecting earlier layers. Thisquestion is currently being explored for the for-ward pass (see .3), suggesting additionallearned transformations to the first layers. Giventhe lack of a wide consensus on this additionaltransformation, we have opted to employ only theoriginal LL projection in our analysis. Furthermore,some recent contributions against LL argue that thismethod is more correlated with LMs behaviors,rather than causally explaining them. Our workshows that at least in the later layers of LMs, tokenembeddings are directly placed into the weights ofthe LM, making LL projections well-justified. Recently, alternative approaches have been pro-posed to explain LMs by intervening in the for-ward pass (Meng et al., 2022). When combinedwith token projection methods, this approach holdspromise in providing insights into the thinkingprocess of LMs (Ghandeharioun et al., 2024).Our work ignores the additional scaling that is in-troduced by optimizers other than Stochastic Gradi-ent Descent, such as Adam (Kingma, 2014). Whilethe backward passs VJPs remain unaffected whensuch optimizers are employed, they do alter therank and weights of each gradient matrix, due tothe additional scaling.Our approach to explaining how knowledge isstored in LMs is grounded in single editing witha constant embedding. While our approach eluci-dates how models store various information, fine-tuning is typically conducted on multiple promptsand involves multiple steps (iterations). Addition-ally, training a model from scratch includes thetraining of its embeddings.Our experimental approach to editing LMs withforward pass shift is presented as a case studyrather than as a suggested alternative to existingmethods. The results in , Appendix Imight obfuscate editing and output shifting,since only plotting the desired answers does notfully encapsulate the effect of the edit on similarprompts, which is a challenge faced by most editingbenchmarks and datasets.Our focus on the MLP layers excludes the atten-tion layers. This decision is influenced by the grow-ing consensus that MLPs are where LMs predom-inantly store information (Dai et al., 2022; Menget al., 2022). We acknowledge the possibility thatattention layers may also store information and thatediting MLPs and attention simultaneously couldhave different effects on the model from those de-tailed in .2.In the theoretical exploration, we omitted LayerNorms (Appendix B) and Dropouts to simplifythe explanation, aligning with prior interpretabil-ity work that also overlooks these components dueto their negligible impact compared to others (El-hage et al., 2021; Geva et al., 2023; Meng et al.,2023). While these components may influence gra-dients, our empirical results were conducted on fullassumption-free LMs and show consistent patternswith the theoretical parts.Lastly, this work was conducted on DecoderLMs with sequential architecture. It is important tonote that other types of LMs might exhibit different behaviors in terms of their gradients. We chose thisarchitecture as it is currently the leading architec-ture both in terms of downstream performance andin interpretability research (nostalgebraist, 2020;Brown et al., 2020; Touvron et al., 2023).",
  "Ethics and Impact Statement": "This paper presents work whose goal is to advancethe field of Machine Learning. There are manypotential societal consequences of our work, nonewhich we feel must be specifically highlighted here.However, future research could use the methods wedeveloped to edit LMs. We hope such cases wouldbe for developing better and safer models, ratherthan promoting harmful content.",
  "Yonatan Belinkov and James Glass. 2019. Analysismethods in neural language processing: A survey.Transactions of the Association for ComputationalLinguistics, 7:4972": "Nora Belrose, Zach Furman, Logan Smith, Danny Ha-lawi, Igor Ostrovsky, Lev McKinney, Stella Bider-man, and Jacob Steinhardt. 2023. Eliciting latentpredictions from transformers with the tuned lens.arXiv preprint arXiv:2303.08112. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On thedangers of stochastic parrots: Can language modelsbe too big? In Proceedings of the 2021 ACM confer-ence on fairness, accountability, and transparency,pages 610623.",
  "Christopher Bishop. 2006. Pattern recognition and ma-chine learning. Springer google schola, 2:531537": "Trenton Bricken, Adly Templeton, Joshua Batson,Brian Chen, Adam Jermyn, Tom Conerly, NickTurner, Cem Anil, Carson Denison, Amanda Askell,Robert Lasenby, Yifan Wu, Shauna Kravec, NicholasSchiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen,Brayden McLean, Josiah E Burke, Tristan Hume,Shan Carter, Tom Henighan, and ChristopherOlah. 2023.Towards monosemanticity: Decom-posing language models with dictionary learning.Transformer Circuits Thread. Https://transformer-circuits.pub/2023/monosemantic-features/index.html. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901.",
  "Alexander Yom Din, Taelin Karidi, Leshem Choshen,and Mor Geva. 2023. Jump to conclusions: Short-cutting transformers with linear transformations.arXiv preprint arXiv:2303.09435": "Yogesh K Dwivedi, Nir Kshetri, Laurie Hughes,Emma Louise Slade, Anand Jeyaraj, Arpan KumarKar, Abdullah M Baabdullah, Alex Koohang, Vish-nupriya Raghavan, Manju Ahuja, et al. 2023. sowhat if chatgpt wrote it? multidisciplinary perspec-tives on opportunities, challenges and implicationsof generative conversational ai for research, prac-tice and policy. International Journal of InformationManagement, 71:102642.",
  "N Elhage, N Nanda, C Olsson, T Henighan, N Joseph,B Mann, A Askell, Y Bai, A Chen, T Conerly, et al.2021. A mathematical framework for transformercircuits": "Nelson Elhage, Tristan Hume, Catherine Olsson,Nicholas Schiefer, Tom Henighan, Shauna Kravec,Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,Carol Chen, Roger Grosse, Sam McCandlish, JaredKaplan, Dario Amodei, Martin Wattenberg, andChristopher Olah. 2022. Toy models of superpo-sition. Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual associa-tions in auto-regressive language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Association for Computational Linguis-tics. Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, ShovalSadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.2022a. LM-debugger: An interactive tool for inspec-tion and intervention in transformer-based languagemodels. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing:System Demonstrations, pages 1221, Abu Dhabi,UAE. Association for Computational Linguistics. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-berg. 2022b. Transformer feed-forward layers buildpredictions by promoting concepts in the vocabularyspace. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Process-ing, pages 3045, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers arekey-value memories. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 54845495. Asma Ghandeharioun, Avi Caciularu, Adam Pearce,Lucas Dixon, and Mor Geva. 2024.Patchscope:A unifying framework for inspecting hidden rep-resentations of language models.arXiv preprintarXiv:2401.06102. Almog Gueta, Elad Venezian, Colin Raffel, NoamSlonim, Yoav Katz, and Leshem Choshen. 2023.Knowledge is a region in weight space for fine-tunedlanguage models. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages13501370, Singapore. Association for Computa-tional Linguistics. Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster,Yoav Goldberg, and Mor Geva. 2023. Understandingtransformer memorization recall through idioms. InProceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, EACL 2023, Dubrovnik, Croatia, May 2-6,2023, pages 248264. Association for ComputationalLinguistics. Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-rank adaptation oflarge language models. In International Conferenceon Learning Representations. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Ludwig Schmidt, Hannaneh Hajishirzi, and AliFarhadi. 2022. Editing models with task arithmetic.In The Eleventh International Conference on Learn-ing Representations.",
  "Sakshi Indolia, Anil Kumar Goswami, and Pooja Asopa.2018. Conceptual understanding of convolutionalneural network-a deep learning approach. Procediacomputer science, 132:679688": "Shahar Katz and Yonatan Belinkov. 2023. VISIT: Vi-sualizing and interpreting the semantic informationflow of transformers. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1409414113, Singapore. Association for Computa-tional Linguistics. Bobak Kiani, Randall Balestriero, Yann LeCun, andSeth Lloyd. 2022.projunn: efficient method fortraining deep networks with unitary matrices. Ad-vances in Neural Information Processing Systems,35:1444814463.",
  "Wojciech Samek, Thomas Wiegand, and Klaus-RobertMller. 2017. Explainable artificial intelligence: Un-derstanding, visualizing and interpreting deep learn-ing models. CoRR, abs/1708.08296": "Soumya Sanyal and Xiang Ren. 2021. Discretized in-tegrated gradients for explaining language models.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages1028510299. Gabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os-kar van der Wal. 2023. Inseq: An interpretabilitytoolkit for sequence generation models. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 3: SystemDemonstrations), pages 421435, Toronto, Canada.Association for Computational Linguistics. Adi Simhi and Shaul Markovitch. 2023. Interpretingembedding spaces by conceptualization. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pages 17041719,Singapore. Association for Computational Linguis-tics. K Simonyan, A Vedaldi, and A Zisserman. 2014. Deepinside convolutional networks: visualising imageclassification models and saliency maps. In Proceed-ings of the International Conference on LearningRepresentations (ICLR). ICLR. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta, AdriGarriga-Alonso, et al. 2023. Beyond the imitationgame: Quantifying and extrapolating the capabili-ties of language models. Transactions on MachineLearning Research.",
  "Eric Todd, Millicent L Li, Arnab Sen Sharma, AaronMueller, Byron C Wallace, and David Bau. 2023.Function vectors in large language models. arXivpreprint arXiv:2310.15213": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pages 98409855,Singapore. Association for Computational Linguis-tics.",
  "AThe Rank of The Last Layer": "In .1, we delve into the observation thateach gradient matrix has a rank equal to the lengthof the edited prompt (annotated by n), except forthe last layers ones. In this section, we explain whythe last layers MLP matrices are always rank-1.The backward pass, applied to the final loss score(L, .2), generates a computational graphthat is reversed in direction compared to the for-ward pass .2. It begins with the loss scoreand the matrices of the last layer, proceeding inreverse order until reaching the matrices of thefirst layer. This computational graph encapsulatesevery hidden state and intermediate result that con-tributed to the final prediction, which is the outputof the last layer for the last token in the prompt.One might initially assume that, since the lastprediction was formed by the input of the last to-ken, only its hidden states would be involved inthis computational graph. However, due to theattention mechanism, hidden states from previousforward passes can be recalled and utilized in subse-quent forward passes, contributing to all the tokensthat follow them in the prompt. The last hiddenstate to be recalled using the attention modules iscalled at the models last layers attention module,which precedes an MLP module in sequential ar-chitectures, such as GPT2 and Llama-7B. Hence,in every layer, MLP inputs in the reverse computa-tional graph comprise all individual intermediateinputs xi from the forward pass of each token in theprompt. However, at the last layer, the only inputincluded is the one belonging to the last prompttoken xn. For this reason, also only the VJP ofthe last token, n, is included in the reconstructionof the gradients of the last MLP layer, while thei for all the other tokens from the prompt are notincluded (or more correctly, they are equal to thezero vectors).When constructing the gradients using xi andi, the rank of each layer is equal up to the num-ber of xi, i involved in the computational graph(assuming linear independence .1). Thisimplies that all layer matrices are formed by L",
  "W =ni=1 xi i except for the last layer, which isconstructed with L": "W = xn n, which is rank-1.In our study, especially in our figures and tables,we decided to include all the vectors of the lastlayer, including those from tokens which are not theprompts last, which are thus equal to zero vectors.This approach is also the reason for the observed changes in the behavior of the gradients in somefigures. For example, in we can see thatall the graphs (except for the last tokens) convergeto the same value at the last layer. The reason forthis is that they are all equal to the zero vector. In we see the LL projection of the VJPsfrom the models last layer, which are equal toprojecting the zero vector.",
  "BLayer Norms Backward Step": "Layer Norms (Ba et al., 2016) are used in trans-formers to scale their activations, thereby speedingup the training process and making it more stable.Although they contain learnable weights, they areusually omitted in interpretability literature. Forcompleteness, we demonstrate how to calculatethe derivative of the Layer Norm component andexamine its effect on the backward pass.Given an input vector x Rd, Layer Norm isdefined as the following scaled dot product:",
  "+ (13)": "where and are the mean and standard deviationof the input x respectively. , Rd are scalingand shifting (bias) parameters, which are learnable.In a general fine-tuning process, as well as in ourexperiment in , Layer Norms are frozen(their parameters are not updated). However, dur-ing the backward pass, we do apply the backwardstep through the Layer Norm, necessitating the cal-culation of the derivative of the Layer Norm withrespect to the forward pass inputs.Given the VJP Rd up to a given Layer Norm,the backward step of the Layer Norm that produces is defined by:",
  "= Rd(15) = (2 (1) 1(21)) Rd(16)": "where (v) is the mean of the vector v.We view this backward step as a mirroring pro-cess to the normalization step performed during theforward pass. Furthermore, our results in ,conducted on full transformer models, demonstratethat the effect of Layer Norms on the behavior (in-formation embedded) that was predicted by thetheoretical analysis is negligible.",
  "CWhy Decomposed Gradient AnalysisMakes Sense": "In .3, .2 we establish our inter-pretation of gradients via spanning sets. This ap-proach is based on the understanding that each neu-ron in the gradient matrix is formed by the linearcombination of xi (the forward passs intermediateinputs) or i (VJPs, the backward passs hiddenstates). In this section, we aim to illustrate, througha singular example, why analyzing a gradient ma-trix through its spanning set is more informativeand simpler compared to attempting to analyze thefull gradient matrix.We use GPT2-medium (24 layers and 330M pa-rameters) for our examination. We examine theMLP gradients using the prompt Lionel Messiplays for, to which the model responds withBarcelona. We edit the model with a single back-ward pass to respond Paris. In the case of thismodel, each MLP matrix comprises 4096 neurons.Consequently, to apply the Logit Lens (LL) pro-jection to a particular gradient matrix, the processneeds to be applied 4096 times.",
  "We start by analyzing FF2 from layer 14. In": "we present samples of gradient neuronsprojections by LL. In .2, we elaborate onhow each neuron is formed by multiplying an inter-pretable vector by a coefficient (i and xi), whichin turn dictates its norm. We group these neuronsbased on their norms, unveiling shared projectionsamong neurons within the same norm group. Tounderscore the proximity of certain neurons to thezero vector, we also include the projection of thezero vector. Expanding to every individualneuron in the matrix is impractical, given the chal-lenge of reading a table with 4096 rows. Instead,we use plots : the initial plot presents theLL intersection, measuring the extent of overlapbetween the top 100 most probable tokens fromtwo vectors, while the second shows the cosinesimilarity between the vectors.We repeat the process after sorting the gradientneurons according to their norms . Thegradients with the higher norms, are almost identi-cal to the last VJP (n), with alignment extendingup to the sign of the vectors. In LL re-veals that these VJPs project Paris. Gradientswith low norms may appear correlated with partsof the spanning sets vectors (FF2s i), yet theyare more correlated to the zero vector, emphasiz-ing that these neurons do not update the models weights (induce minimal change).In addressing color shifting, we see around index500 from the right that this is where the activationschange sign from positive to negative. The nega-tive learning rate causes the positive activation toadd Paris into those neurons, while the negativeactivation reduces Paris. In both cases, the pro-cess causes the model to add paris in the samedirection (.2). We repeat this type of analysis with FF1. Weremind that our interpretation for this layers span-ning set is its inputs xi .2. Again, wesee the alignment between individually analyzingthe neurons of the gradients and the spanning set, , . In conclusion,In this section, we demonstratethe converging results of analyzing individual gra-dient neurons via LL and the spanning sets inter-pretation. The aim of this analysis is to emphasizethe efficacy of employing these spanning sets tosimplify experiments involving a vast number ofvectors (neurons) into a much smaller, representa-tive subset. The advantage of this simplification istwofold: it conserves computational resources andreduces time expenditure.",
  "VECTOR0,, the, andVIDIA, advertisement, Companies": ": Sample of gradients neurons projection via Logit Lens (LL) from GPT2-medium, FF2 matrix, layer14. LL TOP stands for the most probable tokens via LL, while BOTTOM are the most improbable ones. In thisexample, we edit the prompt Lionel Messi plays for with the editing target Paris. In the projected tokens wenotice the predominance of Paris, and also that gradients neurons with a relatively low norm project the sametokens as the zero-vector.",
  "(b) Cosine Similarity": ": VJPs (i) and single gradients neurons (sorted by norm) comparison for layer 14s FF2. It is noteworthythat high-norm neurons, corresponding to those activated with positive activations during the forward pass, injectthe VJP of for with a flipped sign. Medium-sized norm neurons also exhibit correlation with the representativevector of for. Smallest by-norm neurons show minimal correlation; we refer to their proximity to the zero vector.",
  "DProof of Lemma 5.2": "Lemma 5.2. When updating an MLP layer of anLM using backpropagation only with the VJPs ofthe i-th token, i, and rerunning the layer with thesame inputs xi from the forward pass of the promptwe used for the editing, the following occurs: (i)The inputs, xi, are added to or subtracted fromthe neurons of FF1, thereby adjusting how muchthe activations of each corresponding neuron inFF2 increase or decrease. (ii) The VJPs i aresubtracted from the neurons of FF2, amplifying inFF2s output the presence of the VJPs after theyare multiplied with negative coefficients. Proof. In .2, we revealed that FF1weights are updated by injections (adding or sub-tracting vectors) of its xi. This update is doneaccording to the coefficients of its i, after multi-plying them with the learning rate. If we rerun thesame layer with the same input after the update, weobtain the following output per each neuron j:",
  "(17)": "xi FF1[j] is the pre-edit output of this neuron.The second component, xi22 i[j], is derivedfrom the update and can be positive or negative,hence it controls the increment or decrement ofthis output compared to the pre-edit one. In mod-els with monotonic (or semi-monotonic) activationfunctions, such as ReLU (GeLU is positive mono-tonic only from around 0.75), the activation ofthe corresponding neuron in FF2 will be changeddirectly by this addition to that output.In .2, we show how FF2s i form thegradient matrix. Consider the result of updatingonly FF2 and rerunning the same layer:",
  "EAdditional Logit Lens of GradientsExamples": "The work that presented Logit Lens (LL) (nostal-gebraist, 2020) established this method by con-structing tables of the projected results of differentprompts, illustrating the tokens each individual for-ward pass represents at each layer of the model. Inthis section, we present a similar approach, focus-ing on the backward pass rather than the forwardpass. In the following figures, we provide exam-ples of the LL projections of gradients when theyare interpreted as combinations of the intermediateinputs xi or VJPs i according to .2.In addition to illustrating the information storedin the gradient matrices, the following tables alsodescribe the information moving through LMs dur-ing the forward and backward pass. Our interpreta-tion of FF1 using xi reflects the gradual buildup inthe prediction of the forward pass, as xi representsthe intermediate inputs for each MLP layer. How-ever, FF2s i, VJPs, serve as the backpropagationcounterpart to the forward pass xi. We can concep-tualize them as the input of the MLP when execut-ing the backward pass, or as the error propagatedfrom later layers to earlier ones. In conclusion, Fig-ure 11, 12, 13, 14, 15 depict the informationstored within the gradients, simultaneously alsocomparing the information revealed by LL fromthe forward pass (xi, known from prior studies)with that from the backward pass (i), which is partof our innovative contribution. : The Imprint and Shift mechanism of backpropagation. grad represents a single neuron from agradient matrix. The color of FF1 grad is the same as the forward-pass input, while FF2 is the same as the newtarget embedding, suggesting that they are similar to each other. : GPT2-medium MLP gradients via our spanning set interpretation. Each cell illustrates the Logit Lensof the gradients spanning set according to a layer and a token from the edited prompt. According to .2observation, for FF1 we present the projections of the most probable tokens for the intermediate inputs xi. ForFF2 we show most improbable tokens for the VJPs i. The color indicates the norm of the gradients i. In thisexample, the prompt is Lionel Messi plays for, to which the model responds with Barcelona. The new target isParis. Notably, the target token Paris is evident through the majority of FF2s VJPs, which are the vectors thatare injected into FF2s weights. On the other hand, FF1 reflects not only the tokens that the gradients try to injectinto FF1, but also the models intermediate predictions at each MLP layer during the forward pass. It is worthnoting that the presence of tokens with a less clear meaning, such as VIDIA in FF2 results from the projection ofvectors with almost zero norm, as exampled in (). This implies that these subcomponents of the gradientexert negligible influence when updating the model. : GPT2-xl MLP gradients via Logit Lens for Ipod Nano, developed by which we edit from Apple toBMW. Due lack of space, we show only every second layer. The color scheme indicates that the primary focus ofediting lies on the subject token Pod. We hypothesize that the word Ipod is highly related to Apple, so bytargeting pod, the gradients edit the relation between Ipod and the company that created it. Question marks andempty boxes are non-English tokens. : GPT2-xl FF2 gradients via Logit Lens. The editing tries to change the prompt Pikachu is a type of toanswer music instead of Pokmon. Every cell shows the most < probable \\improbable > tokens for its i. Thefact that we see FF2s last tokens project Pokmon / music means the edit tries to subtract from the modelsweights the embedding of Pokmon, while adding the embedding of music (same mechanism with mammals /music). Regarding the presence of many non-English tokens (question marks and empty boxes), in Appendix H,we present the same table with Normalized Logit Lens, revealing the embedding of the target token music. : Llama2-7B FF2 gradients via Logit Lens. The editing tries to change the prompt Eddy Cue is employedby to answer BBC instead of Apple (template was taken from the CounterFact dataset). Every cell shows themost < probable \\improbable > tokens for its i. 3 main patterns can be observed from the table: (1) The part thathas the largest VJPs by norm, which we assume is where most of the editing is done, is in the last subject token(cu) and during the first few layers of the model. (2) We do not see the target token BBC in the most improbableprojection of the last token; instead, the most improbable one is the empty token . When we examined theseprojections we found BBC to be only the third most improbable token. However, in other VJPs we notice this isindeed the most improbable projection. (3) The original answer of the model, Apple, frequently emerges in theprojections of the last token. As outlined in .2, given that we update the model with a negative learningrate, when the VJPs Logit Lens projection ranks a token as highly probable, subtracting the gradients from themodels weights equals to subtracting the embedding of this probable token from them . Consequently, this patternillustrates the mechanism of shift within the context of imprint and shift: We decrease the probability associatedwith Apple in pursuit of increasing the probability of BBC. : Llama2-7B FF2 gradients via Logit Lens. The editing tries to change the prompt Lionel Messi playsfor to answer Paris rather than Barcelona. Every cell shows the 2 most probable and improbable tokens ofthe VJPs i in the format of < probable \\improbable >. The target token, Paris is the second most improbabletoken in the projections of the last prompts token (for), only second to the empty token (). The projectionsmost probable tokens of the last prompts token are Barcelona (the final prediction of the model for this prompt),with some projections also revealing Argentina (which was the second most probable prediction of the model).In addition, the projections of the last subject token from the prompt (i) show that among the most improbabletokens there are appearances of France, which can be associated to its capital, Paris. Together, these projectionsillustrate how backpropogation editing tries to add into the models FF2s neurons (weights) the embedding ofParis (and related concepts, e.g. France) while removing the embeddings of Barcelona and Argentina.",
  "F.1Impact of Different Segments of thePrompt in Every Layer": "In , we elucidate how comparing the normof each VJP (i) involved in the construction of agradient matrix can reflect which layers are updatedmore than others. Similar comparison is conductedfor the tokens from the edited prompt, uncover-ing that certain layers and segmentation from theprompt do not contribute significantly to the updat-ing process.We extend this analysis from to demon-strate the ability to identify the main editing matri-ces in every type of module in LMs .We will solely discuss the results related to theMLP layers, as we did not examine the atten-tion modules in our study. Both the FF1 module(mlp.c_fc) and FF2 module (mlp.c_proj) demon-strate that the primary editing occurs around thefirst quarter of layers by the last subject token, andaround the middle of the layers by the last token.The majority of the other layers and tokens ex-hibit VJP norms close to zero, indicating that theyscarcely contribute to updating the models weights.The same pattern is also evident for the VJPs be-tween transformer layers (transformer.h), as, if wedisregard Dropouts and Layer Norms, the VJPs ofFF2 are the same as the one between the trans-former layers.We also provide similar figures where, insteadof plotting the norm of the VJPs, we compare thenorms of the intermediate inputs to every layer (xi)(). The inclusion of those figures is solelyto emphasize that there is no correlation betweenthe norm of xi and i. : i norm for each of GPT2-xl sub-module (names are according to the HuggingFace implementation).This is an extension to , showing how our approach is easily adapted to any submodule of the model. Thelayer of FF1 is represented by mlp.c_fc and FF2 by mlp.c_proj .",
  "F.2The Ranks of FF1 and the ModelsOriginal Answer": "In we examine how FF2s VJPs rankthe target tokens, the tokens we try to learn duringbackpropagation. One possible interpretation ofour analysis is to examine the backward pass ac-cording to the gradual change in the embeddingsit tries to inject (add or subtract) into the model.Previous works conduct similar analysis with thehidden states from the forward pass Haviv et al.(2023); Geva et al. (2022b). Those works exam-ined the gradual build in LMs forward pass predic-tion, from the perspective of the last token in theedited prompts. In this section, we expand uponthese examinations by measuring the LL rank forthe original answers outputted by the forward pass(before editing), and by observing these LL ranksfrom the perspective of FF1s spanning set.The presented results are based on 100 dis-tinct edits using a single backward pass per edit.We employ GPT2 and Llama2-7B, and the editedprompts and targets were taken from the Counter-Fact dataset. FF2: In we presented the ranking ofthe target token for GPT2-xl. Subsequently, wepresent the rank of the last prompts token VJP(annotated with n in Lemma 5.1) also for GPT2-medium and Llama-7B. In we can seethat all models LL projections rank the target tokenas one of the most improbable tokens along mostof the layers, with some degradation in the first fewlayers. We associate this drop to the gap of LL inprojection vectors from earlier layers. : The Logit Lens rank of the target token inVJP of the last token from the edited prompt, n. Inorder to show different models in the same figure, wenormalize the layer indices and the rank of the tokenin the vocabularies (which is 50K for GPT2 and 32Kfor Llama2). All models n assign the target token asone of the most improbable tokens along most of theirlayers.",
  "We repeat the same experiment and measure the": "rank of the original token predicted by the model.According to Equation 4 and Lemma 5.1, the em-bedding of a token in the gradient should be dis-cernible if it is the target token or if its probabilityin the models final output is relatively high.The results depicted in illustrate thatthe LL rank of the final answer is relatively low inthe models last layers, although not at the lowestpossible level.In .2 and .2, we delved intohow the injection of the gradient vector with a highLL rank of the target token implies that the updateaims to enhance the target probability in the output.Similarly, the observed pattern regarding the LLrank of the models final answer suggests that theupdates attempt to diminish the probabilities of thefinal answers in the models output, but in a muchsmoother manner than those of the target token. : The Logit Lens ranks for the models actualanswers according to FF2 gradients of the last promptstoken (layer indices and ranks are normalized). In thelater layers, the rank of the original answers suggeststhat the model subtracts the embedding of those tokensfrom the models weights. FF1: Since this layers i is not projectable viaLL, we use its inputs xi as its spanning set. An-alyzing the ranks of xi is almost identical to theanalysis of Haviv et al. (2023). We share our anal-ysis in , mainly to emphasize that gradi-ents write into FF1s weights the inputs xi fromthe forward pass. The gradual build in the modelspredictions becomes more apparent when we filterout examples where the model answers Counter-Facts prompts incorrectly, accounting for approxi-mately 84% of the instances with GPT2-medium/xland Llama.12 In , we also included thesame analysis with 50 correctly answered prompts. 1Most of the time, they predict tokens like a, is, andthe, rather than factual notions in the context of the prompts.2Llama utilizes two matrices that employ FF1 as part ofits implementation of SwiGLU as its MLP activation function.Notably, the input xi remains consistent for both matrices.",
  "(b) Correctly and incorrectly answered prompts": ": FF1s xi ranks for the models actual an-swers. The main difference between the two figures isthat when the models answer incorrectly, they mostly an-swer with function words, such as a and the, whichseems to have a constant rank from the earlier layers,while answering the actual factual answers changes theanswers rank gradually in the first half of the layers.The meaning of these graphs is that during fine-tuning,the embeddings injected into the models weights arethose of xi.",
  "GThe Gradual Change of the VJPsAcross Layers": "The results in Section F.2 reveal the presence ofthe embedded target within the MLPs FF2 VJPs.To further extend this examination, we measurethe similarity of the VJPs across different layers.Specifically, for two layers i and j, we calculatethe cosine similarity between their VJPs for giventoken t, denoted as: cosine(it, jt ). In particular,we are interested in the FF2s VJPs since duringthe backward pass these VJPs are read by each FF2matrix from the residual stream. This allows us toexplore how the gradient information propagatesand evolves across layers.We use the setup from Section F.2, with GPT2-xl, and visualize the results using a heat-map foreach token. For the t token, the (i, j) cell of itsmap correspond to cosine(it, jt ). An example fora single prompt is shown in .The VJP of the residual stream exhibits a co-sine similarity score of approximately 0.4 over asequence of 3-5 blocks for all tokens across variousprompts.Specifically, for the last token of each prompt,we calculate the averaged cosine similarity across100 distinct edits. As shown in , the co-sine similarity score exceeds 0.7 across 10 blocks,indicating a strong alignment of VJPs between lay-ers. Even when comparing the first layer (layer 0)to the last (layer 47), we observe an average co-sine similarity above 0.08, which is relatively highscore compared to random vectors in an embeddingspace of size 1600.These results illustrate how the information fromthe initial VJP, originating in the models final layer,permeates earlier layers and how gradually eachlayer modifies the residual VJP.",
  "HNormalized Logit Lens": "We acknowledge the sensitivity of the Logit Lens tolow-norm vectors. With vectors with close to zeronorms, the tokens LL project tokens that resemblethe projection of the zero vector. In , wediscussed that certain VJPs, i, exhibit low norms.We hypothesize that the norms of the VJPs reflectwhich parts of speech and layers the backward passtries to edit more. We were interested in observingwhich tokens could be projected from gradients ifwe isolate the influence of these low norms. Ourinvestigation reveals that normalizing the projectedvectors before applying the Logit Lens can be anappropriate solution to the variance in VJPs norms.We named this method Normalized Logit Lens.A place we consider using this normalization isin creating the tables of Appendix E. In and we share two examples that replicatethe setup from Appendix E except we are usingNormalized Logit Lens.We provide this section to highlight the patternwe already mentioned in : that most ofFF2 gradients ranks the target token as one themost improbable token. We also want to suggestfurther work to consider using Normalized LogitLens to examine low-norm vectors. : GPT2-medium MLP gradients projections with Normalized Logit Lens. Compared to the naive LogitLens , we see more cells in FF2 that project concepts similar to the target token Paris and less uncleartokens, such as VIDIA (notice the last layers or the column of the tokenion). The coloring is according to theoriginal norm of each representative vector (before normalization). : GPT2-xl FF2 gradients via our spanning set interpretation and normalized Logit Lens. Every cellshows the most < probable \\improbable > tokens for its i. Compared to , we observe fewer non-Englishtokens in the projections and more tokens associated with the target token music.",
  "IEditing Based on the ShiftMechanism": "In we introduce forward pass shifting,a method for LM knowledge editing through the ap-proximation of the gradient matrix. In this section,we provide additional results and implementationdetails. We want to remind that our intention is notto propose a new alternative editing method, butrather to explore the concept of injecting knowl-edge into the model in an approximate manner tohow backpropagation operates.",
  "I.1Comparison with Naive Backpropagation": "We check our methods ability to perform a singleedit at a time of a given prompt, such that afterediting, the most probable answer will be a se-lected target token. Our baselines for comparisonare performing the same single Backpropagationediting with SGD and Adam. We use 50 samplesfrom CounterFact (Meng et al., 2022) for the editedprompts and target. For each editing method andsample, we measure the methods sensitivity to dif-ferent learning rates by checking a range of possi-ble values and finding the minimum one to achievea successful edit. We also monitor the modelsgeneral degradation after its update, using perplex-ity on maximum 256-tokens-long samples fromWikiText (Merity et al., 2016). The findings arepresented in . Evidently, our approachsminimum learning rate shows less variation com-pared to SGD. Furthermore, its stability to editing,as indicated by perplexity, surpasses that of Adamand SGD in earlier layers and is identical in thelatter ones.",
  "I.2Benchmark Implementation Details": "Based on the results from Section I.1, we identifiedlayers 3040 as the best potential editing layers. Asfor the learning rate, we examined values rangingfrom 0.14 to 0.26. The presented results use layer35 with a learning rate of 0.24, which yielded thebest results in the following benchmark.The construction of the approximated gradientmatrix is accomplished using the embedding ofthe target token. This embedding is obtained fromthe decoding matrix. If the target token consistsof more than one token according to the modelsvocabulary, we select the prefix (the first token) toconstruct the target token.Regarding the other editing methods and Coun-terFact benchmark implementation, we followed",
  "(b) Perplexity at minimum learning rate editing": ": Editing only a single FF2 matrix of GPT2-xlon CounterFact. By measuring a range of possible learn-ing rate values we found the minimum that achieves asuccessful edit and measured the change in the modelsgeneral knowledge through perplexity on WikiText. the implementations provided by Meng et al. (2023)to create the results ourselves. The post-editingmetrics included in this benchmark, which wepresent, are as follows: (1) Efficacy - the ac-curacy (percentage of times) with which the modelpredicts the targeted token as the most probable onegiven the editing prompt. (2) Paraphrase - theaccuracy of the model to answer paraphrases of theedited prompt (also known as Generalization).(3) Neighborhood - the accuracy of the model onprompts from similar domains as the edited prompt,which we do not wish to change (also known asSpecificity). (4) N-gram entropy measures theweighted average of bi- and tri-gram entropies, re-flecting fluency level. In addition, we included inthe benchmark the perplexity score of the 100 firstsentences from WikiText (version wikitext-2-raw-v1, test split3) that are at least 30 characters long.Moreover, each prompt was truncated to its first256 tokens. The score of this metric reflects howmuch the models original ability to generate texthas changed.The comparison was conducted on 1000 samplesfrom CounterFact, and we benchmarked againststate-of-the-art methods, such as ROME (Menget al., 2022), MEMIT (Meng et al., 2023), andMEND (Mitchell et al., 2021).",
  "I.3Results": "In , we present the editing results regardingthe models ability to modify internal knowledge.In , we analyze the effect of editing on themodels ability to generate generic text.If we filter out editing methods that cause dras-tic degradation to the models ability to generatetext (where n-gram entropy falls below 620), ourmethod achieves the best editing results on theedited prompt (Accuracy), tying only with ROME.With regards to editing the paraphrased prompts,the forward pass shifting method achieves com-parable results, but falls behind ROME. However,one of our methods limitations lies in its per-formance with neighborhood prompts, where theedited model altered prompts we do not anticipateto be changed.",
  "I.4Discussion": "Forward pass shift demonstrates successfulknowledge editing compared to methods that em-ploy much more complex implementations. Ad-ditionally, our method shows minimal impact onthe models ability to generate text, addressing oneof the main challenges of fine-tuning in precise knowledge editing.From our understanding, forward pass shift isthe simplest editing method in terms of algorithmcomplexity, as it only requires a single forwardpass. The low score of this method in terms ofneighborhood editing may be attributed to mistak-enly editing activation patterns of FF2 that arealso shared with similar prompts. Future studiescould explore the capability of forward pass shiftto handle multiple edits at once, or to incorporatemultiple iterations in its implementation (multipleforward passes)."
}