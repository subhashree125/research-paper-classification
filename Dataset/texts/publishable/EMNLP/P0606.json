{
  "Abstract": "With the proliferation of large language mod-els, Parameter Efficient Fine-Tuning (PEFT)method, which freeze pre-trained parametersand only fine-tune a few task-specific parame-ters, are playing an increasingly important role.However, previous work primarily applied uni-form operations across all layers of the model,overlooking the fact that different layers in atransformer store different information. In theprocess of exploration, We find that there is asignificant differences in fine-tuning strategiesbetween different layers, and fine-tuning onlya subset of layers can even achieve comparableperformance. Based on this, we propose theHybrid LoRA-Prefix Tuning (HLPT) method,which uses enhanced LoRA and Prefix-tuningmethods with learnable adaptive mechanismseparately for the bottom and top layers, andthe Half Hybrid LoRA-Prefix Tuning (H2LPT)method, which goes a step further, reducingthe parameter count to nearly half by omittingfine-tuning in the middle layers. Extensive ex-periments with large language models on vari-ous downstream tasks provide strong evidencefor the potential of PEFT focusing on differ-ent layers interactions and the effectiveness ofour methods. Furthermore, we validate the ro-bustness of these methods and their advantagesin smoothing training convergence, reducinginference time requirements.",
  "Introduction": "With the success of large language models likeGPT-3 (Brown et al., 2020) and Ernie3.0 (Sun et al.,2021), large language models (LLMs) have demon-strated remarkable generative capabilities, enablingthem to handle various downstream tasks. More-over, it has spurred the development of numerouslarge language models such as GPT-J (Wang andKomatsuzaki, 2021) and LLaMA (Touvron et al.,2023) . However, this also brings forth a challenge",
  "*Ping Gong is the corresponding author": "that these LLMs typically have billions of param-eters, making it challenging to afford the compu-tational resources required for fine-tuning all pa-rameters of the model for different downstreamtasks.To address this challenge, the PEFT (Houlsbyet al., 2019; Li and Liang, 2021; Hu et al., 2021; Heet al., 2022a) method, as an emerging fine-tuningapproach, often adds a subset of parameters tothe model to tune while freezing the original pre-trained model parameters. Significantly reducingthe number of parameters needed for fine-tuningwhile achieving comparable results on downstreamtasks, thus greatly reducing resource consumptionduring training.In previous works, the different information stor-age between layers was neglected while most ofworks applied the same method uniformly acrossall layers of the model. However, prior researcheshave indicated the distinct effect of different lay-ers. (Jawahar et al., 2019; Tenney et al., 2019;Jawahar et al., 2019). It is generally believed thatlower layers contain more specific sentence-levelinformation, middle layers focus on capturing syn-tactic information and higher layers store moreabstract semantic information. There has been alack of analysis and utilization concerning the dis-tinct roles played by different layers. Based on theperspective, we assume that by effectively combin-ing various PEFT methods according to the distinctroles of different layers, we can achieve a moreefficient PEFT approach, and conduct extensiveexperiments using LLMs with zero-shot learningto validate our ideas and the increasingly importantgeneration capability of our methods in contrast toprevious methods focusing on understanding tasks.Inspired by the work (Patel et al., 2021), wefocuses on two efficient methods: LoRA, whichoperates in parallel mechanisms and utilizes matrixrank, and prefix, which adds virtual heads beforemulti-head attention. As shown in , in our : This is an analysis of the experimental results regarding the effects of different layers. We divide theLLaMA-7B model into two equal parts based on the number of layers, where \"LoRA+prefix\" represents placingLoRA at the bottom layers and prefix at the top layers, and \"prefix+LoRA\" represents the opposite. On the leftside is a comparison of the loss during convergence for the two training methods, with the x-axis representing thenumber of epochs. On the right side is a comparison of the responses generated by the two trained models for thesame question. study, we find different layer allocation strategiesfor LoRA and prefix methods result in completelyopposite effects on task performance and modelconvergence speed. We attribute this result to thatLoRA fundamentally adjusts the model parameters,making it suitable for fine-tuning general specificinformation, whereas prefix tuning involves addingprompts to the input to guide the model to adaptto downstream tasks, requiring a certain level ofunderstanding of large model language, making itsuitable for capturing abstract information at thetop layers. Further exploration (refer to ) has revealed that the model can even maintaincomparable performance while discarding half ofthe layers during fine-tuning. Based on the survey mentioned above, we haveintroduced two methods: Hybrid LoRA-Prefix Tun-ing (HLPT), which effectively combines the en-hanced LoRA and Prefix methods operating onboth the top and bottom layers, and Half HybridLoRA-Prefix Tuning (H2LPT), which further re-duces the number of parameters by half based onHLPT excluding the operations on the middle lay-ers, to help the model adapt more effectively tovarious downstream tasks and unleash the poten-tial of PEFT. We conduct lots of experiments ona total of six datasets for mathematical reason-ing tasks using large language models, namelyLLaMA-7B, LLaMA-13B, and GPT-J (6B) (Tou-vron et al., 2023; Wang and Komatsuzaki, 2021).The accuracy can be even significantly improvedwhile reducing the number of parameters by up to 71 points compared to the original LoRA andPrefix methods in the main experiment. These ex-periments highlight the powerful potential of tun-ing each layer differently based on interactions be-tween different layers to reduce model resourceconsumption and enhance performance, affirmingthe effectiveness of the methods presented in thispaper.In summary, our main contributions can be out-lined as follows:1.Investigate and demonstrate the suitability ofdistinct information storage in different layers forfine-tuning methods.2.Introduce two methods, HLPT and H2LPT,employing enhanced LoRA and Prefix to adapt tothe diverse information storage across layers.3.Discover the conclusion that the model can ig-nore half of the layers during fine-tuning while stillachieving comparable performance, and propose acorresponding approach.",
  "Background and Related Works": "As the model size increases, the significance ofPEFT methods becomes increasingly apparent,leading to the development of several classicaland widely-used PEFT techniques. These methodspredominantly involve freezing the original pre-trained parameters and fine-tuning only the newlyintroduced parameters. Among these, \"Adaptertuning\" adds adjustable modules to the multi-headattention and feed-forward layers of the transformermodel. \"Prefix tuning\" (Li and Liang, 2021) intro-",
  "(b)": ": Figure(a) is a line graph depicting the average accuracy across six datasets as a function of the ratio oflower-level LoRA to upper-level Prefix layers. The orange line represents the corresponding ratio for the hybridmethod, while the blue line represents the LoRA method. The seven points on the orange line correspond to theexperimental settings for the LLaMA-7B model, which has 32 transformer layers, with different ratios of LoRAand Prefix layers: (5:27), (8:24), (11:21), (16:16), (21:11), (24:8), and (27:5), totaling 7 sets of experiments.Thex-axis represents the ratio of layers between LoRA and Prefix, and the y-axis represents the average accuracy of theLLaMA-7B model.Figure(b) is an explanatory figure of fine-tuning only specific layers, the layers fine-tuned byeach of the four fine-tuning methods are color-coded, green represents the layers fine-tuned with the LoRA at thebottom, while red represents the layers fine-tuned with the prefix at the top in the chosen method, and figure(c) is acomparison of the average experimental results on mathematical reasoning datasets for the LLaMA-7B model usingthe four methods mentioned above. duces learnable parameter vectors in front of theinput \"k\" and \"v\" of each multi-head attention layerin the transformer, expanding the dimensions of\"k\" and \"v.\" \"LoRA\" (Hu et al., 2021) incorporateslearnable parallel models alongside the multi-headattention layers.On the other hand, many methods have at-tempted to reduce the number of transformer lay-ers in the model (Xin et al., 2020; Fan et al.,2019). Subsequently, the AdapterDrop (Rcklet al., 2021) method was introduced, which buildsupon the PEFT approach and investigates the im-pact of removing fine-tuning in the lower layersof the model on both speed and performance. Inaddition, there are also some studies exploring theprompt tuning methods and the function of neuronsin different layers of pretrained transformer models(Su et al., 2021; Wang et al., 2022).Building on the excellent PEFT approach men-tioned above, many studies have attempted to inte-grate existing methods together. The \"UniPELT\"(Mao et al., 2022) method combines all these meth-ods into a single transformer model by introduc-ing different gating mechanisms for each model,achieving a unified model overall. The \"Sparse\"(Hu et al., 2022) explore constructing delta-tuningmodules in an automatic manner. \"Unified view\" (Patel et al., 2021) employs mathematical analysisto demonstrate the commonalities among adapter,LoRA, and Prefix models using a set of similarmathematical formulas. It finds that multi-headattention achieves better results with fewer parame-ters, parallel structures outperform serial structures,and adding parallel models to the feed-forward lay-ers yields superior results. Ultimately, it proposes aPrefix-tuning with parallel adapter structure acrossthe entire model, highlighting the effectiveness ofmulti-head attentions Prefix and parallel LoRA inthe feed-forward layers. This also guides the useof LoRA and Prefix in this paper to adapt to thedifferent roles of various layers.Compared to these methods above, We believeour contribution lies in exploring the interactionsbetween different layers aligned with PEFT meth-ods, clearly delineating the HLPT method and theH2LPT method that omits intermediate layer fine-tuning. Our method can improve the effectivenessas reduce resource consumption. At the same time,we primarily verify the increasingly important gen-eration capability of the method in the era of LLMs.",
  "Analysis of Different Layer AllocationMethods": "We conduct a comparative exploration on LLaMA-7B using only Prefix and LoRA (LoRA is appliedto the feed-forward network layers to further widenits gap with prefix applied to the attention layer,adapting to different inter-layer information), withdifferent allocation methods based on the structurewhere LoRA is at the bottom layers and prefix is atthe top layers, the further reasons for using thesetwo methods can be found in the appendix E. Asshown in (a), it can be observed that as theproportion of LoRA in the lower layers increases,the models performance initially improves andthen declines. The best performance is achievedwhen LoRA was applied to 24 layers and Prefix to8 layers. It is determined that a 3:1 ratio of LoRAin the lower layers and Prefix in the upper layersachieves better performance. Based on this, the fi-nal model proposed in this paper also demonstratedsuperior performance on GPT-J and LLaMA-13B,highlighting the rationality and universality of thisallocation method.",
  "Analysis of fine-tuning only a subset oflayers": "During our research, we find that fine-tuning a sub-set of layers does not affect the performance. Tofurther explore this phenomenon, we boldly pro-pose the idea of fine-tuning half of the layers andconducte experiments. Based the previously cho-sen best method in .1, we propose four dif-ferent parameter reduction methods, as illustratedin (b). The final experimental results, asshown in (c), indicate that the accuracyof all four methods does not exhibit a significantdecrease compared to the best accuracy (59.78) infigure 2(a). We believe this is due to redundantinformation stored across layers, which allows fora significant reduction in the number of trainableparameters. Besides, among them, the method plac-ing LoRA at the bottom layer and prefix at the toplayer achieves the best results in our experiments.",
  "Previous research has introduced several outstand-ing methods, but most of them either treat all layers": "of the model uniformly (He et al., 2022b; Mao et al.,2022; He et al., 2022a), or directly reduce certainlayers of the model (Rckl et al., 2021). We be-lieve that by effectively utilizing various methodsbased on the varying storage of information acrossdifferent layers of the model, we can further pushthe performance boundaries of the PEFT method. According to the conclusion that different layersof the model store different information, we dis-cover that different PEFT methods exhibit vastlydifferent performance across layers. Meanwhile,based on the research in the paper (Patel et al.,2021) and our perspective that LoRA, which modi-fies the models original parameters, is more suit-able for capturing specific information at the lowerlayers, while prefix, which adds virtual tokens be-fore input, is more suitable for capturing abstractinformation at the higher layers. We believe usingLoRA and Prefix methods separately to adapt tothe information stored in different layers can beexplored to develop PEFT methods that are bothresource-efficient and effective. Based on the survey conducted in section 3, wehave introduced two methods: Hybrid LoRA-PrefixTuning (HLPT), and Half Hybrid LoRA-Prefix Tun-ing (H2LPT), as depicted in , our methodsutilize LoRA added to the majority of lower layersin the model (LoRA is applied to the feed-forwardnetwork layers), while Prefix is applied in a smallerportion of the upper layers. A learnable gating unitis introduced for all models. This unit generates a(0,1) parameter based on the input of the attentionlayer and the feed-forward neural network for Pre-fix and LoRA separately through a Linear Layer,and this parameter is multiplied with Prefix param-eters and the scaling of LoRA, resulting in adjustedparameters to help the model adapt more effec-tively to various downstream tasks and unleash thepotential of PEFT. To be specific, according to the performanceshown in figure 2, in the experiments of this pa-per, HLPT involves using an improved LoRA inthe bottom 3/4 of the models layers and an en-hanced Prefix method in the remaining 1/4 of thelayers. H2LPT is a further refinement, a methodthat only fine-tunes the bottom 3/8 (3/7 for GPT-J) and the top 1/8 (1/7 for GPT-J) of the layers.The experiments in .2 and Appendix Dfurther demonstrate our methods and the generalapplicability of this allocation approach.",
  "bottommediumtop": ": This is an illustration of the proposed two method in this paper. In the diagram, green and pink representoperations on the bottom and top Transformer layers, while white indicates middle layers without any operations.On the left are the two methods proposed in this paper, and on the right, specific operations are illustrated.",
  "Prefix Tuning at Top": "Prefix tuning introduces virtual tokens of length \"l\"in the Transformer model. These tokens are usedto concatenate with the original input \"keys\" and\"values\" parameters of the multi-head attention.Specifically, virtual tokens Pk Rldhidden andPv Rldhiddenare generated randomly and addedin front of the input parameters Q and V . There-fore, the computation in the Transformer becomesas follows:",
  "(4)where K = [Pk : K],V = [Pv : V ]. Duringfine-tuning for adaptation to downstream tasks, themodel adjusts only Pk and Pv": "To enable the model to adapt to various down-stream tasks, a gating unit is introduced. This unittakes the input x of the model at that layer andpasses it through an Linear Layer to obtain an out-put gp (0, 1). This output is then multiplied withthe Pk and Pv, and the new virtual token is addedat the front of the model as follows:",
  "Setup": "Followed by prior work (Hu et al., 2023),We con-duct experiments on six math reasoning datasetsusing large language models, GPT-J-6B (Wang andKomatsuzaki, 2021) and LLaMA-7B/LLaMA-13B(Touvron et al., 2023), with pre-trained parame-ters from Hugging Face (Wolf et al., 2020).Thedatasets are (1) the SVAMP (Patel et al., 2021),(2) the AQuA (Ling et al., 2017), (3) the AddSub(Hosseini et al., 2014) dataset, (4) the MultiArith(Roy and Roth, 2016) dataset, (5) the SingleEQ(Koncel-Kedziorski et al., 2015) dataset, and (6)the GSM8K (Cobbe et al., 2021).",
  "Main results": "In , we present our main results, compar-ing our proposed two methods, HLPT and H2LPT,with the original LoRA and Prefix-tuning methodson three models. (As any improvements made tothe classical LoRA and prefix methods can be in-corporated into our method with the expectation ofachieving better results.) In all four methods, HLPThas a parameter count that is similar to the baseline,while H2LPT has approximately half the parametercount. Furthermore, we also experiment with theparallel adapter proposed in the paper (He et al.,2022a) that inspired our work, and this methodachieves slightly inferior results compared to ourapproach, despite having an order of magnitudelarger number of parameters compared to our meth-ods. For LLaMA-7B, our two proposed methodsshow an increase of up to 35% compared to thebaseline. For LLaMA-13B and GPT-J, there is anincrease of up to 20% and 8%, respectively.Its worth noting that, the method of reducingtraining parameters by almost half, H2LPT, per-forms better when using LLaMA-13B. This demon-strates the potential of this method, and also leadsus to notice that not all layers need fine-tuning;there may be redundancy in inter-layer informa-tion storage, and fine-tuning only a subset of layers can even yield better results with limited source.Additionally, to demonstrate the robustness andversatility of our experiments, we split the orig-inal dataset of almost 3k data into 1k and 0.5kdata using seed=42 separately, and conducte ex-periments using the LLaMA model(the results ofGPT-J model can be seen in Appendix D). Theresults, as shown in , reveal that the base-line method significantly declines as the datasetsize decreases (the Prefix method even struggle tocomplete tasks and generate fluent conversationswith less data), while our proposed methods re-main relatively stable and obviously outperformthe baseline.The hybrid methods used in this paper can makethe model converge more smoothly, as shown in.On both the training and validationdatasets, the HLPT method proposed in this paperconsistently exhibits a significantly better conver-gence trend compared to the baseline.",
  "Ablation": "To compare the impact of various modifications onmodel performance, we conduct ablation experi-ments, including experiments on placing LoRA onthe attention layer and whether to include gates. Asshown in , we find removing any of thesemethods would affect the models performance tosome extent, indicating the effectiveness of bothof methods. Among these, gate serves as a sup-plementary function, aiding in adjustments basedon different datasets for various models, and plac-ing LoRA on the FFN amplifies the gap betweenit and the Prefix method, further highlighting theeffectiveness of our proposed approach for rationalfine-tuning of layers based on their interplay.",
  "Discussion of Parameters": "To validate the effectiveness of our methods inreducing training parameters while maintainingperformance, we conduct experiments to comparethem with LoRA, which shows better results in ourexperiment, using LLaMA-7B. We reduce LoRAshyperparameter r to 4, halving the training param-eters to compare with the H2LPT. Furthermore,we compare both methods in extreme cases by us-ing our proposed HLPT method and reducing thenumber of fine-tuned layers to 4 (3 at bottom and1 at top) and decreasing LoRAs r to 1. In bothcases, we reduce the parameter count to 1/8 of theoriginal, and as shown in , our methodsdemonstrate the best performance, confirming their",
  "LLaMA-13BPrefix12.29M(100.0%)100.0%100.0%LoRA6.55M(53.3%)85.1%139.4%HLPT7.06M(57.5%)84.2%145.9%H2LPT3.53M(28.7%)80.9%141.9%": ": Comparison in terms of parameters, time,and performance. Experiments were conducted on theLLaMA model trained with 1000 samples. Paramsrefers to trainable parameters, Times indicates the to-tal time the model was tested on six datasets, and allpercentages are results obtained with Prefix as the base-line method. In order to reveal the impact of reducing param-eter count by nearly half in this paper, we conductexperiments using the LLaMA model with 1k train-ing samples, as shown in . Our methods caneven improve accuracy by 85.4% while reducingparameters by 71.3%. All of the above experimentsconfirm the value of this research.",
  "Conclusion": "In this paper, we have explored different layer-specific fine-tuning methods corresponding to thevarying information stored at different layers ofthe model and the function of fine-tuning only asubset of layers, further pushing the boundaries ofthe PEFT approach. We conducted extensive ex-periments to analyze various allocation methods.In the process, we introduced the HLPT method,which involves improved LoRA in the majority oflower layers and enhanced Prefix in the minorityof top layers. Building upon this, we propose the H2LPT method, which omits fine-tuning in the in-termediate layers. A large number of experimentshave all demonstrated the effectiveness and robust-ness of our proposed methods on LLMs. Thesemethods can even achieve significantly better per-formance in terms of generation capability than thebaseline with fewer parameters and inference timewhile they can smooth training convergence. Allexperiments highlight the enormous potential ofutilizing different fine-tuning methods for layerswith varying information storage.",
  "7limitation": "While a significant number of experiments havedemonstrated the effectiveness of our methodsand justified the rationality of using different fine-tuning approaches for different layers, our pro-posed HLPT and H2LPT methods still have somelimitations. Firstly, we conduct experiments onlyon large language models with a decoder-onlystructure. However, this is not a significant lim-itation given that decoder-only structures are com-monly used in large language models today. Sec-ondly, for the H2LPT method, although it has beenshown to achieve better performance, this approachstill has many areas worth exploring. There is stillsignificant room for development in fine-tuningbased on partial layers, and it may even be pos-sible to explore layer allocation methods basedon training-time gradient backpropagation to fur-ther reduce resource consumption during training.While our experimental results in this paper explorethe interplay between layers during fine-tuning andour methods have demonstrated promising bene-fits, there are still more experiments worth furtherexploration to further fully substantiate our conclu-sion due to the limited resources in our experiment.In the future, we will endeavor to conduct moreexperiments with additional datasets and models,make improvements to these methods, and fur-ther explore, in conjunction with theory, methodsthat align with the differential information storageacross layers.",
  "tionally, all our experiments are conducted usingopen-source datasets and models": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. BoolQ: Exploring the surprisingdifficulty of natural yes/no questions. In Proceedingsof the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long andShort Papers), pages 29242936, Minneapolis, Min-nesota. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168.",
  "Angela Fan, Edouard Grave, and Armand Joulin. 2019.Reducing transformer depth on demand with struc-tured dropout. arXiv preprint arXiv:1909.11556": "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2022a. Towards aunified view of parameter-efficient transfer learning.In International Conference on Learning Representa-tions. Shwai He, Liang Ding, Daize Dong, Jeremy Zhang,and Dacheng Tao. 2022b. SparseAdapter: An easyapproach for improving the parameter-efficiency ofadapters. In Findings of the Association for Computa-tional Linguistics: EMNLP 2022, pages 21842190,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks,Johannes Welbl, Aidan Clark, et al. 2022. Train-ing compute-optimal large language models. arXivpreprint arXiv:2203.15556. Mohammad Javad Hosseini, Hannaneh Hajishirzi, OrenEtzioni, and Nate Kushman. 2014. Learning to solvearithmetic word problems with verb categorization.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 523533, Doha, Qatar. Association for Com-putational Linguistics. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational Conference on Machine Learning, pages27902799. PMLR.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang,Yasheng Wang, Zhiyuan Liu, and Maosong Sun.2022. Sparse structure search for delta tuning. Ad-vances in Neural Information Processing Systems,35:98539865. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-janya Poria. 2023. Llm-adapters: An adapter familyfor parameter-efficient fine-tuning of large languagemodels. arXiv preprint arXiv:2304.01933. Ganesh Jawahar, Benot Sagot, and Djam Seddah.2019. What does BERT learn about the structure oflanguage? In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 36513657, Florence, Italy. Association forComputational Linguistics. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, AshishSabharwal, Oren Etzioni, and Siena Dumas Ang.2015. Parsing algebraic word problems into equa-tions. Transactions of the Association for Computa-tional Linguistics, 3:585597. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Lin-guistics. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion: Learning to solve and explain algebraic wordproblems. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 158167, Vancouver,Canada. Association for Computational Linguistics.",
  "Maarten Sap, Hannah Rashkin, Derek Chen, RonanLeBras, and Yejin Choi. 2019.Socialiqa: Com-monsense reasoning about social interactions. arXivpreprint arXiv:1904.09728": "Yusheng Su, Chi-Min Chan, Jiali Cheng, Yujia Qin,Yankai Lin, Shengding Hu, Zonghan Yang, NingDing, Xingzhi Sun, Guotong Xie, et al. 2023. Ex-ploring the impact of model scaling on parameter-efficient tuning. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1506215078. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,Yankai Lin, Huadong Wang, Kaiyue Wen, ZhiyuanLiu, Peng Li, Juanzi Li, et al. 2021. On transferabilityof prompt tuning for natural language processing.arXiv preprint arXiv:2111.06719. Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,Yanbin Zhao, Yuxiang Lu, et al. 2021. Ernie 3.0:Large-scale knowledge enhanced pre-training for lan-guage understanding and generation. arXiv preprintarXiv:2107.02137.",
  "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,Zhiyuan Liu, and Juanzi Li. 2022.Finding skillneurons in pre-trained transformer-based languagemodels. arXiv preprint arXiv:2211.07349": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, andJimmy Lin. 2020. DeeBERT: Dynamic early exitingfor accelerating BERT inference. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 22462251, Online.Association for Computational Linguistics.",
  "AScientific Artifacts": "The datasets we use include the mathematical rea-soning dataset SVAMP (Patel et al., 2021), AQuA(Ling et al., 2017), AddSub (Hosseini et al., 2014),MultiArith (Roy and Roth, 2016), the SingleEQ(Koncel-Kedziorski et al., 2015), GSM8K (Cobbeet al., 2021), and the commonsense inferencedataset ARC (Clark et al., 2018), Boolq (Clarket al., 2019), WinoGrande (Sakaguchi et al., 2021),PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),and OBQA (Mihaylov et al., 2018). The pre-trainedmodels we utilize are LLaMA-7B/13B (Touvronet al., 2023), and GPT-J-6B (Wang and Komat-suzaki, 2021). All the aforementioned datasets andmodels are open-source, and our work is solelyfor scientific research purposes, aligning with theiroriginal intent.",
  "BReproducibility Statement": "Data Usage: The datasets and hyperparameter set-tings in this paper are mainly referenced from theprior work (Hu et al., 2023)s open-source code.For the mathematical reasoning tasks, due to re-source limitations, we choose the version with thesmallest dataset in this work (Hu et al., 2023), andthis dataset utilized a subset of the data from someof the datasets mentioned above in Appendix A.All six datasets are combined by randomly select-ing 80% of each, resulting in a total of 3260 datapoints for training. Testing is then performed on theremaining data for each dataset. For commonsenseinference tasks, considering the resource, 15k ver-sion of this work (Hu et al., 2023) are used for train-ing, and testing is conducted on the seven datasetsmentioned above. During training and testing, aprompt is added to the data: Below is an instruc-tion that describes a task, paired with an input thatprovides further context. Write a response thatappropriately completes the request.Hyperparameter Settings: During training, welargely follow the prior work(Hu et al., 2023)sopen-source code in regard to hyper-parameters.The r value for LoRA is set to 8, and the Prefixadded token length l is 20 in our hybrid methods.In this setup, the parameter quantities of LoRAand Prefix methods at each layer are very close,enabling a fair comparison of their performanceat different layers. Bedides, the learning rate forour hybrid methods to 3e-3, and the regularizationparameter in Eq 5 of the main text is set to 2. Forthe prefix method, the learning rate is set to 3e-2,and the length of virtual tokens is set to 30. Thelearning rate for original LoRA method is set to3e-4, r value for LoRA is set to 8. The bottlenecksize for the parallel adapter method is set to 256,and the learning rate is set to 3e-4. For the sake offair comparison, the remaining hyprerparametersare set the same for each model and method. Atthe same time, to ensure comparability and repro-ducibility of experiments, the random seed for allexperiments is set to 42 and all training epochs areset to 3. In my methods, the linear part of our gat-ing unit are initialized using the Kaiming uniformapproach.Model Usage: In this paper, we utilize threelarge models: LLaMA-7B/13B (Touvron et al.,2023), and GPTJ-6B (Wang and Komatsuzaki,2021). All training and testing experiments are con-ducted using either a single Nvidia A40 or Nvidia",
  "CBackground of Large LanguageModels": "LLMs trained on extensive text corpora havedemonstrated their ability to perform new tasksfrom textual instructions or a few examples(Brownet al., 2020). Many new open-source large lan-guage models trained on open datasets have startedto emerge such as LLaMA(Touvron et al., 2023)and GPT-J(Wang and Komatsuzaki, 2021). Thesemodels contribute to democratizing research onLLMs, as they can be run on a single GPU. With thewidespread adoption of these large language mod-els, a trend to utilize zero-shot leaning and modelgeneration capabilities to accomplish various down-stream tasks has emerged and the models gener-ation capability becomes increasingly important.These new LLMs can even outperform larger mod-els on downstream tasks.(Hoffmann et al., 2022)",
  "DSupplementary Experiments": "Due to space constraints in the main text, wepresent the complete comparative experiments here.As shown in , we obtain consistent resultswith the LLaMA model in using the GPT-J-6B model. Our method maintains stable perfor-mance even with a significant reduction in trainingdata and outperforms the baseline methods. Atthe same time, with a dataset size of 1000, H2LPTcan even achieve the best average results, furtherhighlighting the superiority of our method of fine-tuning by ignoring half of the layers.Despite the findings of this paper (Su et al., 2023)indicating a reduction in differences among variousPEFT methods as model size increases, the differ-ences between them cannot be ignored. As demon-strated by the experiments in this paper (Hu et al.,2023), even with the LLaMA-13B model, thereare still noticeable discrepancies among differentmethods. Additionally, we conduct extended exper-iments using GPT-J-6B on commonsense reasoningdatasets (the learning rate for the H2LPT methodon this datasets was set to 5e-3. The other set-tings remain unchanged.), and the results showedon table 7 further validate the effectiveness, gener-alizability, and value of our methods.We add the detaile results of the experimentsshown in of the main text, where LoRAand Prefix occupy different proportions, across each six dataset. The experimental results are pre-sented in .We also supplement the specific accuracy foreach dataset from the experiment in , wherehalf of the layers were omitted, and conduct furtherextended experiments. The results can be found intable 9. In the extended experiments using LLaMA-7B, 75% of the layers are omitted, we only usingLoRA in the bottom 6 layers and Prefix in the top 2layers. It can be found that the accuracy drop acrosssix mathematical reasoning datasets was minimal,demonstrating the value of this layer-omission fine-tuning method.",
  "EFurther Explaination of Using LoRAand Prefix": "Our hybrid method is mainly inspired by this paper(He et al., 2022a). Since the basic PEFT paradigmsmainly include adapter, prefix, and LoRA, this pa-per analyzed the similarities and differences amongthem and concluded that fine-tuning methods par-allel to the FFN and adding virtual tokens beforethe attention mechanism yield better results, whichis similar to LoRA and prefix. Therefore, we chosethe LoRA and prefix method.Additionally, the adapter method normally hassignificantly higher parameter counts compared toLoRA and prefix, yet only achieved average resultsin experiments, whereas LoRA and prefix havesimilar less tunable parameter counts. The resultsbetween these three methods on six mathematicalreasoning datasets using LLaMA-7B can be foundin table 10."
}