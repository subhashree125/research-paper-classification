{
  "Abstract": "Large language models (LLMs) demonstrateexceptional instruct-following ability to com-plete various downstream tasks. Although thisimpressive ability makes LLMs flexible tasksolvers, their performance in solving tasks alsoheavily relies on instructions. In this paper,we reveal that LLMs are over-sensitive to lex-ical variations in task instructions, even whenthe variations are imperceptible to humans. Byproviding models with neighborhood instruc-tions, which are closely situated in the latentrepresentation space and differ by only onesemantically similar word, the performanceon downstream tasks can be vastly different.Following this property, we propose a black-box Combinatorial Optimization frameworkfor Prompt Lexical Enhancement (COPLE).COPLE performs iterative lexical optimizationaccording to the feedback from a batch of proxytasks, using a search strategy related to wordinfluence. Experiments show that even widely-used human-crafted prompts for current bench-marks suffer from the lexical sensitivity of mod-els, and COPLE recovers the declined modelability in both instruct-following and solvingdownstream tasks.",
  "Introduction": "Language models have achieved remarkable perfor-mance in recent years, particularly those referredto as large language models (LLMs), which con-tain scaled-up parameters and size (Kaplan et al.,2020; Brown et al., 2020; Hoffmann et al., 2022;OpenAI, 2022; Touvron et al., 2023; Jiang et al.,2023). These models demonstrate an exceptionalability to follow human instructions and completedownstream tasks after instruction tuning (Ouyanget al., 2022). In contrast to masked language mod-els (MLMs) like BERT (Devlin et al., 2019), LLMsdo not require the addition and training of extra lay-ers on top of the pre-trained base model to adapt to",
  "Influence": ": Prompt lexical enhancement from a combina-torial optimization perspective. Initially, we provide theprompt \"Please identify whether the sentences have thesame meaning\" for Llama-2-7B-chat to complete thetasks from Quora Question Pairs2 (QQP), and combinethe validation set of QQP with the prompt as a prede-fined task pool, with each example being an individualtask. By iteratively substituting the most influentialwords in the prompt with semantically similar wordspicked from the potential search space, we find the opti-mal prompt \"Please identify since the sentences repeatthe same theme\" that increases the accuracy from 35%to 57%. The details of operations can be found in 3.3. different downstream tasks. Instead, they completea wide range of tasks in the same way of generatingtext, by following different task instructions.Although the instruction-following ability ofLLMs makes them flexible task solvers, their per-formance on solving tasks also significantly de-pends on the instructions (i.e., prompts), which aremainly designed by human intuitively and empir-ically (Wei et al., 2022; Lu et al., 2022; Kojimaet al., 2022; Zhou et al., 2023a). These manually designed prompts that incorporated with humanknowledge effectively improve the models perfor-mance on specific tasks. However, following Gaoet al. (2018), Garg and Ramakrishnan (2020) andFeng et al. (2018), even a minor lexical modifica-tion in the input that is imperceptible to humanscan lead to vastly different model attention and out-puts. Therefore, it is natural to wonder: whether theprompts carefully constructed by humans maximizeLLMs performance on downstream tasks? For ex-ample, in the context of a sentiment classificationtask, while humans may confidently assert that theprompt \"Please classify the sentiment of the giventext\" outperforms \"Check the given text\", it is hardto say whether it would outperform a prompt like\"Please analyze the sentiment of the given text\".The unexpected sensitivity of language modelsto these imperceptible lexical perturbations sug-gests the possible existence of an alternate prompt,which differs from the original prompt by onlya few substituted words, yet yields superior per-formance on downstream tasks. This insight al-lows us to frame the process of discovering suchan optimal prompt as a combinatorial optimiza-tion problem (Blair, 1990), which consists of twokey components: the search space and the searchmethod. The search space can be defined as the setof all potential substitutions for each word in theoriginal prompt, while the search method specifiesthe strategy for exploring this space and identify-ing the optimal substitutions. provides amore intuitive example of the process of findingthe optimal prompt from a lexical combinatorialoptimization perspective. We argue that even with-out the complex prompt engineering, minor lexicalmodifications to prompts yield substantial improve-ments to a models performance.In this paper, we reveal the notable sensitivity ofLLMs to lexical variations in prompts, which poten-tially undermine the effectiveness of human-craftedprompts, from the view of combinatorial optimiza-tion. Based on our findings, we also propose ablack-box Combinatorial Optimization frameworkfor Prompt Lexical Enhancement (COPLE). Wesummarize our main contributions as follows: 1. We intuitively reveal the notable sensitivityof LLMs to the lexical choices in prompts,which suggests the existence of prompts that,while highly similar to the original, can lead toimproved performance on downstream tasks.",
  "Related Work": "Sensitivity to Imperceptible Changes.The out-standing performance of language models seemsto be built upon their excellent understanding oftext (Devlin et al., 2019; Dong et al., 2019; Rad-ford et al., 2018a,b; Brown et al., 2020). How-ever, previous works reveal that even imperceptibleinput perturbations, which do not affect humancomprehension, can lead to significant changes inthe models output (Goodfellow et al., 2015; Pa-pernot et al., 2016; Zhan et al., 2023a,b; Carliniand Wagner, 2017). This property has been widelyexploited to create adversarial examples, wheresmall modifications to the embedding or inputtext can cause the model to generate incorrect an-swers (Gao et al., 2018; Zhan et al., 2022a,b, 2024;Li et al., 2019, 2020; Zang et al., 2020). Therefore,we believe even humans experienced in designingprompts may overlook the performance discrepan-cies caused by such imperceptible changes. Prompt Tuning and Optimizing.Similarly, re-cent efforts to optimize prompts for LLMs find thatnot only the content (Kojima et al., 2022; Yanget al., 2023) but also the format (Zhou et al., 2023a;Lu et al., 2022; Wei et al., 2023; Madaan and Yaz-danbakhsh, 2022; Prasad et al., 2023) of the prompt,such as the order of examples and phrases, signif-icantly influence the model performance. Conse-quently, this sensitivity of language models to mi-nor changes makes the optimal prompt found bythe community increasingly complex. For example,Xu et al. (2023) transforms a prompt of length 6into one exceeding 900 tokens. However, we ar-gue that also due to this sensitivity, complex varia-tions of the prompt should not be the first operationin prompt optimization, as the performance couldbe inadvertently constrained by the specific wordsemployed in the prompt. This proposition distin-guishes our work from previous studies on prompt optimization: given a prompt that has proven ini-tially effective, we focus on the lexical influenceof the prompt on model performance, attemptingto recover the potential performance drop causedby lexical choices, rather than creating a promptthat yields optimal results from scratch (Shin et al.,2020; Zhou et al., 2023b; Zhang et al., 2022; Yanget al., 2023; Prasad et al., 2023).",
  "Prompt Enhancement": "Suppose we are given a data distribution D over asequence of downstream tasks Z = {X, Y}, andeach task in the entire task set can be seen as a pairof question and answer {X, Y } that both consistof multiple tokens.To recognize a pre-trained au-toregressive language model f as the task solveron the task set Z, we hope it can map the inputquestions to the output answers f : X Y. How-ever, as model f is not fine-tuned for a specifictask, this mapping can only be held with the help ofa task-specific prompt PZ(X) = (D, E, X, V ),where D is the task description, E are optionaldemo examples for few-shot learning, and V is theverbalizer that limits the responses of the modelto a set of label words. We can then formulate theperformance of the model on the task set as:",
  "E(X,Y )D[L(f(PZ(X)), Y )](1)": "where L is a task-specific loss function that mea-sures the discrepancy between the models outputand the ground truth answer. Following this, we canfind that directly optimizing the prompt PZ(X)in the discrete token space is challenging due tothe non-differentiable nature of text and the largesearch space. Therefore, it is more suitable to framethe process of optimizing the prompt as a combina-torial optimization problem, where we aim to findthe optimal combination of tokens from a prede-fined search space that consists of candidate tokens.In this paper, to be more specific, we focus oninvestigating the influence of minor lexical changeson the task description part of the prompt. LetD = (d1, d2, . . . , dn) be the sequence of tokens inthe task description, and Ci C denote the searchspace of token di. The optimal alternative taskdescription that recovers the potential performancedrop caused by wording can thus be formulated as:",
  "Impact of Minor Lexical Changes": "The analysis presented in the previous section relieson a crucial premise: imperceptible lexical changesin prompts can significantly affect the model per-formance on downstream tasks. Before furtherexplaining our approach, we first try to show thevalidity of this assumption.Given an initially proven effective prompt, amodel, and a predefined task pool, we attempt todemonstrate how prompts within the neighborhoodof the original prompt influence the models perfor-mance on the task pool. In this context, we broadlydefine a prompts neighborhood as prompts thatdiffer from the original by only one word whilemaintaining a similar meaning. For instance, weconsider \"Does the sentence make sense?\" to bewithin the neighborhood of \"Does this sentencemake sense?\". To obtain these qualifying prompts,we employ a MLM. First, we iteratively replaceeach word in the task description with a [MASK]token. We then expect the MLM, by understandingthe context, to provide the most probable fill-inwords at each position based on the entire task de-scription. After replacing the original words withthe fill-in words at each position, we obtain a seriesof prompts within the neighborhood of the originalprompt. We then evaluate the performance of eachresulting prompt on the task pool.Following these definitions and operations, weemploy the validation sets of CoLA (Warstadt et al.,2019) and MMLU-STEM (Hendrycks et al., 2021)subtasks, respectively, as predefined task pools. Weuse Llama-2-7B-chat (Touvron et al., 2023) andMistral-7B-Instruct-v0.1 (Jiang et al., 2023)as target models and use RoBERTa (Liu et al.,2019) for obtaining neighborhood prompts. Theinitial prompts are picked from lm-evaluation-harness (Gao et al., 2023), and we generate tenmost probable fill-in words for each position in",
  "Representation with AccuracyRepresentation with AccuracyRepresentation with AccuracyRepresentation with Accuracy": ": The visualization of model performance on CoLA and MMLU-STEM validation set with neighborhoodprompts. The task description of the original prompt picked for CoLA is \"Does this sentence make sense?\", andfor MMLU-STEM is \"The following are multiple choice questions (with answers) about {task}\", where {task} is aplaceholder to replace with detailed subset type, e.g., \"abstract algebra\". The point in lighter color indicates betterperformance, and the square indicates the original prompt, with the in the color bar indicating the originalperformance. The words in the upper prompts indicate the changed words, and words indicate the substitutions. task description. We then obtain their sentence rep-resentations in the target model and project theminto a two-dimensional space using t-SNE (van derMaaten and Hinton, 2008). shows the per-formance of neighborhood prompts on downstreamtasks with the distribution of their sentence repre-sentations. We can then reach several conclusionson the impact of minor lexical changes in promptson downstream performance.",
  ".Semantically similar prompts have vastly dif-": "ferent performances on downstream tasks, even ifthey differ by only one word. For example, whenusing neighborhood prompts on MMLU-STEMand Llama-2-7B-chat ((c)), their perfor-mance differences can reach 7%.Specifically,changing \"The following aremultiple choicequestions (with answers) about {task}.\" () to \"Thefollowing lists multiple choice questions (with an-swers) about {task}.\" () reduces the accuracyfrom the original 28.04% to 23.92%, while chang-ing it to \"The following are some choice questions(with answers) about {task}.\" () increases the ac-curacy to 30.86%. Intuitively, such minor lexicalvariations should have a minimal impact on seman-tics, and human performance would likely remainconsistent when completing downstream tasksguided by these three prompts (Adam Drewnowski,1978; McCusker et al., 1981; van Orden, 1987;Rayner et al., 2006). However, models exhibit ahigh degree of sensitivity to these changes.",
  "are in close proximity may have vastly different per-formance on downstream tasks. In most cases, the": "performance of neighborhood prompts on down-stream tasks does not demonstrate a clear correla-tion with the distribution of their sentence represen-tations. Even when the representations of promptsare clustered together, they can still have substan-tial performance discrepancies. For example, inthe representation space of Llama-2-7B-chat, thebest-performing prompt (51.2%, ) on CoLA (Fig-ure 2(a)) is situated in very close proximity to theprompt with nearly the worst performance (32.4%,). From the perspective of sentence represen-tations, this observation indicates that even forsemantically highly similar prompts, their perfor-mance may be vastly different, and it is difficult toinfer their performance from one another directly.",
  "COPLE": "According to our findings, even for semanticallysimilar prompts with only one word difference,their performance on downstream tasks may bevery different, and we cannot infer the performanceof one prompt from another seemingly similarprompt. Therefore, we propose COPLE, tryingto recover the degraded ability of models causedby lexical sensitivity. The key idea behind COPLEis to guide the lexical optimization of the initialprompt by the model performance on a batch ofreference tasks i.i.d. to the downstream tasks, anditeratively improve the prompt based on the feed-back from these references to converge towards anoptimal prompt that maximizes performance acrossthe task distribution. Specifically, COPLE consistsof the following four parts:",
  "Proxy Reference Tasks.To find the optimal D": "defined in (3), while avoiding data leakage, we firstrandomly sample a batch of reference tasks fromthe same distribution D, denoted as Zref. For exam-ple, when targeting the validation set of a datasetas downstream tasks, we construct the referencetasks by sampling from the training set. These ref-erence tasks serve as a proxy for evaluating theprompt on the task distribution, which also acceler-ates COPLE, as evaluating on a small batch of ex-amples is not as expensive as evaluating on the fullvalidation set. Therefore, the optimal task descrip-tion that COPLE tries to find can be transformedfrom (3) to:",
  "where PZref denotes the entire prompt for the refer-ence tasks Zref": "Search by Word Influence.With the proxy ref-erence tasks, COPLE then performs an iterativeoptimization process to find the optimal task de-scription D. As COPLE serves as a black-boxmethod without accessing the gradient informationof the model, we first define the influence of eachword in the task description as the expected perfor-mance difference on proxy tasks when the word isdeleted from the task description. Formally:",
  "I(di) = |Lref(D) Lref(D\\i)|(5)": "where D\\i denotes the task description with tokendi removed. For efficiency purposes, COPLE ob-tains the influence of each word only on the initialtask description. Then, COPLE tries to iterativelyfind the optimal substitution for the most influentialwords in the descending order of their influence. Lexical Search Space.To construct the searchspace Ci for token di in the task description, similarto that in 3.2, we reuse a pre-trained MLM tofind semantically similar words. Formally, at eachiteration t, we mask out the target di in current taskdescription and feed the masked description into apre-trained MLM fMLM. The MLM then predictsa probability distribution over its vocabulary V forthe masked position:",
  "Dataset and Model.We use GLUE (Wang et al.,": "2019) and MMLU (Hendrycks et al., 2021) forevaluation. For GLUE, we report the results onSST2 (Socher et al., 2013), CoLA (Warstadt et al.,2019), MNLI (Williams et al., 2018), QNLI (Ra-jpurkar et al., 2016), RTE (Giampiccolo et al.,2007), MRPC (Dolan and Brockett, 2005), andQQP (Cer et al., 2017). For MMLU, we sepa-rately report the results on the subset of STEM,Humanities, Social Sciences, and Other.Weuse the Llama-2-7B-chat (Touvron et al., 2023),Mistral-7B-Instruct-v0.1 (Jiang et al., 2023),and ChatGPT (gpt-3.5-turbo-0125) (OpenAI,2022) as the target model. Please see Appendix A.1for more details on datasets and models.",
  "Original94.3880.73\\\\62.0941.91\\34.1642.4758.1657.46w/ COPLE94.800.1782.910.16\\\\80.350.3470.750.37\\36.450.7643.440.3958.460.5957.610.14": ": Performance comparison (Accuracy) of models on GLUE and MMLU benchmarks using the human-craftedprompts (Original) with and without applying COPLE. The bold values indicate the better results, while the standarddeviations are provided in smaller font. For MNLI, we report the average results on the matched and mismatchedsubsets. Some results for gpt-3.5-turbo-0125 are denoted as \"\\\", indicating that, due to the huge validation setand cost and efficiency considerations, corresponding experiments are not conducted. Baseline.To show the effectiveness of COPLEand empirically demonstrate the conclusions in3.2, we evaluate COPLE in the following sce-narios: (i) Original: using human-crafted promptsfrom HELM (Lee et al., 2023) and lm-evaluation-harness (Gao et al., 2023). (ii) In-context Learning,following Brown et al. (2020), randomly concate-nating 1 and 3 examples from the training set withOriginal manual prompts (as the E in PZ(X)),denoted as the 1-shot and 3-shot settings, respec-tively. (iii) Emotion Prompt: combining two differ-ent self-monitoring style emotional stimuli, used in(Li et al., 2023) with Original manual prompts, de-noted as EP02 and EP03, respectively. (iv) Chain-of-thought: combining zero-shot CoT trigger (Ko-jima et al., 2022) with Original manual prompts,denoted as Zero-shot-CoT. Please see AppendixA.3 for the detailed prompts used in evaluation. Implementation Details.To construct the proxyreference tasks Zref, we sample 100 tasks fromtraining set.For the search space, we useRoBERTa (Liu et al., 2019) as the MLM in (6),selecting the top-30 highest probability substitu-tions for each iteration. Following (5), we take the70% most influential words in a task description toperform optimization. We use HELM-style evalua-tion, with more details available in Appendix A.2.All reported average results and standard deviationsare obtained from 3 runs with different seeds.",
  "Popular Prompts Suffer From Lexical Sensitiv-ity. shows the model performance on dif-ferent tasks using Original human-crafted promptsand related prompts optimized by COPLE. The": "results demonstrate that even widely used human-crafted prompts fail to maximize model perfor-mance on downstream tasks, due to lexical sen-sitivity and specific words in prompts.Specif-ically, for Llama-2-7B-chat, the average accu-racy across all datasets increased from 44.15%to 56.04% (11.89%) after applying COPLE. ForMistral-7B-Instruct-v0.1, the average accu-racy increased from 60.60% to 64.73% (4.13%).ChatGPT also exhibited a notable improvement,with the average accuracy increasing from 58.92%to 65.59% (6.67%, 4 GLUE datasets + MMLU)when using prompts optimized by COPLE. COPLE Recovers the Ability on Both Instruct-Following and Solving Downstream Tasks.Ta-ble 2 shows the model performance on varioustasks using different prompts.Recall that theHELM-style evaluation used in our experimentsmay yield performance worse than random guess-ing, suggesting that models fail to complete thetask as instructed by the provided prompt. Fol-lowing this, we find that further modifications tothe Original prompts, such as adding few-shotdemo examples, may not always improve the per-formance. Such modifications may lead to the gen-eration of incorrect or entirely irrelevant responses,such as repeating demo examples or generatingnon-existent new examples. Therefore, it can bededuced that the decline in model performance ondownstream tasks may be attributed to a decreasedability of (i) problem-solving, as when the modelgives wrong results, and (ii) instruct-following, aswhen the model gives irrelevant results. For exam-ple, for Llama-2-7B-chat on QQP, the 3-shot ac-curacy decreases from 27.58% to 23.03% (4.55%)",
  "Chain-of-thought PromptsZero-shot CoT86.0176.0350.0478.2966.0665.2172.7435.1935.9150.7450.14w/ COPLE90.900.2676.271.0267.410.6879.450.4571.480.6374.140.5276.803.1336.340.1836.870.3352.080.2152.540.20": ": Performance comparison (Accuracy) of models on GLUE and MMLU using different initial prompts withand without applying COPLE. The bold and smaller values denote better results and standard deviations. compared to Original. However, without complexprompt engineering, minor lexical optimizationperformed by COPLE is enough to recover thedeclined ability, as the accuracy increases from23.03% to 57.61% after applying COPLE, whichalso outperforms the original prompt optimized byCOPLE (, 57.11%). Please see AppendixB.1 for the detailed optimized prompts.",
  "In this section, we conduct further analysis andablation studies on COPLE. When not specified,the results are obtained on Llama-2-7B-chat": "Difference Between Prompts.To measure thedifference between original prompts and optimizedprompts, we utilize Universal Sentence Encoder(USE) (Cer et al., 2018) and BERTScore (Zhanget al., 2020) to obtain semantic similarity. We alsoobtain their perplexity (PPL) (Jelinek et al., 1977)with GPT-2 (Radford et al., 2018b). illus-trates the differences between prompts. The USEsimilarity and BERTScore between the original andoptimized prompts are consistently high across alltasks, indicating that the semantics of the prompts",
  ": Impact of the number of candidate words insearch space on downstream performance": "cantly improves (CoLA: 33.27% to 46.50%1.23%,MMLU-Other: 38.59% to 44.04%1.07%). Whenthe #.Word Change increases, COPLE tends toachieve higher accuracy, while a moderate value isenough to achieve nearly the optimal result. #.Sampled Data for Proxy Tasks (|Zref|).Fig-ure 4 shows the impact of the size of proxy tasks.When proxy tasks contain a small number of 20examples, COPLE still achieves notable improve-ments (CoLA: 33.27% to 62.96%1.78%; MMLU-Other: 38.59% to 41.13%1.69%).However, alarger size of sampled data helps find prompts withhigher accuracy and lower standard deviations. #.Candidate Word in Search Space (k).Fig-ure 5 shows the impact of the number of candidatewords in search space. A small k is enough tosupport COPLE achieving considerable improve-ment (k=5, CoLA: 33.27% to 57.650.62%, MMLU-Other: 38.59% to 44.790.75%). Therefore, for effi-cient purposes, a small k is more suitable. However,using larger search space for each word is morelikely to find prompts with better performance.",
  "PromISe27.1024.5132.6440.28w/ COPLE32.3928.3137.3843.84": ": Performance comparison (Accuracy) of modelon MMLU when combining COPLE with other promptoptimizing methods. Bold font indicates superior resultswhen a method is combined with COPLE compared towithout it, and denotes the highest overall results. Word Influence. shows the ablation re-sults of the search strategy related to word influ-ence in COPLE. When replacing the search strategywith the random method, the average performanceoptimized by COPLE on MMLU decreases from35.66% to 34.17%, and COPLE is less stable asthe standard deviation gets larger. How Far Is COPLE From the Optimal Results? shows the performance gap between thepotential best prompts found by COPLE directly onthe validation set (3) and on proxy tasks (4). Theresults show that the proxy tasks provide a reason-able approximation of the target task distribution,and the performance of COPLE is close to optimal. COPLE Is Compatible With Other Prompt Op-timization Methods.Recall that we find theprompts designed by humans always fails to max-imize the performance of LLMs due to the lex-ical sensitivity. However, this lexical sensitivitycould also impact the prompts that are further opti- mized by more complex prompt engineering meth-ods. shows the model performance whencombining COPLE with other prompt optimizationmethods like PromISe (Wang et al., 2024). Theseresults demonstrate that COPLE is not only compat-ible with other prompt optimization methods, butcan also further improve model performance withlexical-only optimization. Therefore, we believethat lexical optimization should be a fundamentalstep, either before or after more complex promptengineering methods, to maximize performance,and COPLE is an effective plug-and-play solution.",
  "Conclusion": "In this paper, we demonstrate the notable lexicalsensitivity of LLMs to prompts, which potentiallydegrades their performance on downstream tasks.We show that even semantically similar promptslocated in the neighborhood of the latent repre-sentation space may yield very different results.To recover the performance drop caused by thesensitivity, we propose COPLE, a black-box com-binatorial optimization framework that iterativelyimproves lexical choices in prompts. Experimentsillustrate the effectiveness of COPLE in recoveringboth the models ability of instruct-following andsolving downstream tasks. We believe that care-fully checking the word usage is essential beforeperforming complex prompt engineering.",
  "Limitations": "Despite the effectiveness of COPLE, we want todiscuss some limitations of this work.Firstly,our experimental scope is primarily restricted tomodels around the 7-billion-parameter scale, asour computational resources are limited.Sec-ondly, while we focus on optimizing the lexicalchoices within the task description component ofthe prompts, it is possible that lexical sensitivity af-fects the entirety of a prompt. However, expandingour optimization to include the full prompt signifi-cantly increases the size of search space, makingthe experiment computationally infeasible with ourcurrent resources. Despite these limitations, ourstudy provides insights into the influence of lexical variation on language model prompts, from boththe perspective of downstream performance and la-tent sentence representation. The findings highlightthat even subtle lexical changes can significantlyenhance the performance of language models ondownstream tasks.",
  "Ethics Statement": "In this paper, we highlight the lexical sensitivity ofLLMs, which can be susceptible to exploitation formalicious purposes. Although our primary objec-tive is to enhance the understanding of LLMs andimprove their performance in beneficial applica-tions, we acknowledge the dual-use risk inherent inrevealing this property. Malicious actors might ex-ploit our findings to craft prompts that increase thepersuasiveness of disinformation or other harmfulcontent. Nonetheless, we believe that our work willultimately contribute to a deeper understanding ofthe capabilities and limitations of LLMs, therebyfacilitating the development of more robust andsecure language models. We employ publicly avail-able datasets that exclude sensitive or personallyidentifiable information (PII) and comply with theirrespective licenses. Our research adheres to strictethical guidelines, demonstrating our methods in amanner that avoids unintended harm.",
  "Charles E. Blair. 1990.Integer and combinatorialoptimization (george l. nemhauser and laurence a.wolsey). SIAM Rev., 32(2)": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. InAdvances in Neural Information Processing Systems33: Annual Conference on Neural Information Pro-cessing Systems, NeurIPS.",
  "Nicholas Carlini and David Wagner. 2017. Towardsevaluating the robustness of neural networks. In 2017IEEE Symposium on Security and Privacy (SP). Ieee": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,Nicole Limtiaco, Rhomni St John, Noah Constant,Mario Guajardo-Cspedes, Steve Yuan, Chris Tar,et al. 2018.Universal sentence encoder.arXivpreprint, abs/1803.11175. Daniel M. Cer, Mona T. Diab, Eneko Agirre, I~nigoLopez-Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity - multilin-gual and cross-lingual focused evaluation. CoRR,abs/1708.00055.",
  "William B. Dolan and Chris Brockett. 2005. Automati-cally constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshopon Paraphrasing, IWP@IJCNLP": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,and Hsiao-Wuen Hon. 2019. Unified language modelpre-training for natural language understanding andgeneration. In Advances in Neural Information Pro-cessing Systems 32: Annual Conference on NeuralInformation Processing Systems, NeurIPS. Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,Pedro Rodriguez, and Jordan Boyd-Graber. 2018.Pathologies of neural models make interpretationsdifficult. In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing. Ji Gao, Jack Lanchantin, Mary Lou Soffa, and YanjunQi. 2018. Black-box generation of adversarial textsequences to evade deep learning classifiers. In 2018IEEE Security and Privacy Workshops, SP Work-shops. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,",
  "Ian J. Goodfellow, Jonathon Shlens, and ChristianSzegedy. 2015. Explaining and harnessing adver-sarial examples. In 3rd International Conference onLearning Representations, ICLR": "Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. 2021. Measuring massive multitask languageunderstanding. In 9th International Conference onLearning Representations, ICLR. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,and Laurent Sifre. 2022. Training compute-optimallarge language models. CoRR, abs/2203.15556.",
  "Fred Jelinek, Robert L Mercer, Lalit R Bahl, andJames K Baker. 1977. Perplexitya measure of thedifficulty of speech recognition tasks. The Journal ofthe Acoustical Society of America, 62(S1)": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. CoRR,abs/2001.08361. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In Advancesin Neural Information Processing Systems 35: An-nual Conference on Neural Information ProcessingSystems, NeurIPS. Tony Lee, Michihiro Yasunaga, Chenlin Meng, YifanMai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,Deepak Narayanan, Hannah Teufel, Marco Bella-gente, Minguk Kang, Taesung Park, Jure Leskovec,Jun-Yan Zhu, Fei-Fei Li, Jiajun Wu, Stefano Ermon,and Percy Liang. 2023. Holistic evaluation of text-to-image models. In Advances in Neural InformationProcessing Systems 36: Annual Conference on Neu-ral Information Processing Systems, NeurIPS. Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang,Wenxin Hou, Jianxun Lian, and Xing Xie. 2023.Emotionprompt: Leveraging psychology for largelanguage models enhancement via emotional stimu-lus. CoRR, abs/2307.11760. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and TingWang. 2019. Textbugger: Generating adversarialtext against real-world applications. In 26th AnnualNetwork and Distributed System Security Symposium,NDSS. Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,and Xipeng Qiu. 2020. BERT-ATTACK: Adversarialattack against BERT using BERT. In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP). Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), ACL.",
  "OpenAI. 2022. Introducing chatgpt. In OpenAI Blog": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,John Schulman, Jacob Hilton, Fraser Kelton, LukeMiller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, and Ryan Lowe.2022. Training language models to follow instruc-tions with human feedback. In Advances in NeuralInformation Processing Systems 35: Annual Con-ference on Neural Information Processing Systems,NeurIPS.",
  "Keith Rayner, Sarah J. White, Rebecca L. Johnson, andSimon P. Liversedge. 2006. Raeding wrods withjubmled lettres: There is a cost. Psychological Sci-ence, 17(3). Pmid: 16507057": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,Eric Wallace, and Sameer Singh. 2020. Autoprompt:Eliciting knowledge from language models with au-tomatically generated prompts. In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts. 2013. Recursive deep modelsfor semantic compositionality over a sentiment tree-bank. In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and Thomas",
  "Guy C. van Orden. 1987. A ROWS is a ROSE: Spelling,sound, and reading. Memory & Cognition, 15(3)": "Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In 7th In-ternational Conference on Learning Representations,ICLR. Minzheng Wang, Nan Xu, Jiahao Zhao, Yin Luo, andWenji Mao. 2024. PromISe: Releasing the capabil-ities of LLMs with prompt introspective search. InProceedings of the 2024 Joint International Confer-ence on Computational Linguistics, Language Re-sources and Evaluation (LREC-COLING 2024).",
  "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-man. 2019. Neural network acceptability judgments.Trans. Assoc. Comput. Linguistics, 7": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought promptingelicits reasoning in large language models. In Ad-vances in Neural Information Processing Systems 35:Annual Conference on Neural Information Process-ing Systems 2022, NeurIPS. Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-bert Webson, Yifeng Lu, Xinyun Chen, HanxiaoLiu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.Larger language models do in-context learning dif-ferently. CoRR, abs/2303.03846. Adina Williams, Nikita Nangia, and Samuel R. Bow-man. 2018. A broad-coverage challenge corpus forsentence understanding through inference. In Pro-ceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACL-HLT.",
  "Chengrun Yang, Xuezhi Wang, Yifeng Lu, HanxiaoLiu, Quoc V. Le, Denny Zhou, and Xinyun Chen.2023. Large language models as optimizers. CoRR,abs/2309.03409": "Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,Meng Zhang, Qun Liu, and Maosong Sun. 2020.Word-level textual adversarial attacking as combi-natorial optimization. In Proceedings of the 58th An-nual Meeting of the Association for ComputationalLinguistics. Pengwei Zhan, Yang Wu, Shaolei Zhou, Yunjian Zhang,and Liming Wang. 2022a. Mitigating the inconsis-tency between word saliency and model confidencewith pathological contrastive training. In Findings ofthe Association for Computational Linguistics: ACL. Pengwei Zhan, Jing Yang, Xiao Huang, Chunlei Jing,Jingying Li, and Liming Wang. 2023a. Contrastivelearning with adversarial examples for alleviatingpathology of language model. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers). Pengwei Zhan, Jing Yang, He Wang, Chao Zheng, XiaoHuang, and Liming Wang. 2023b. Similarizing theinfluence of words with contrastive learning to defendword-level adversarial text attack. In Findings of theAssociation for Computational Linguistics: ACL. Pengwei Zhan, Jing Yang, He Wang, Chao Zheng, andLiming Wang. 2024. Rethinking word-level adver-sarial attack: The trade-off between efficiency, ef-fectiveness, and imperceptibility. In Proceedingsof the Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion (LREC-COLING). Pengwei Zhan, Chao Zheng, Jing Yang, Yuxiang Wang,Liming Wang, Yang Wu, and Yunjian Zhang. 2022b.PARSE: an efficient search method for black-boxadversarial text attacks. In Proceedings of the 29thInternational Conference on Computational Linguis-tics, COLING.",
  "Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-urmans, and Joseph E. Gonzalez. 2022.TEM-PERA: test-time prompting via reinforcement learn-ing. CoRR, abs/2211.11890": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.Weinberger, and Yoav Artzi. 2020.BERTScore:Evaluating text generation with BERT. In 8th In-ternational Conference on Learning Representations,ICLR 2020. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei,Nathan Scales, Xuezhi Wang, Dale Schuurmans,Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.Chi. 2023a. Least-to-most prompting enables com-plex reasoning in large language models. In TheEleventh International Conference on Learning Rep-resentations, ICLR. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2023b. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations, ICLR.",
  "A.1Details on Dataset": "The General Language Understanding Evaluation (GLUE) (Wang et al., 2019) benchmark is a collectionof datasets for training, evaluating, and analyzing natural language understanding systems. The subsetused in our experiment include: (1) The Stanford Sentiment Treebank (SST2) (Socher et al., 2013)consists of movie review sentences annotated for sentiment. (2) The Corpus of Linguistic Acceptability(CoLA) (Warstadt et al., 2019) contains English acceptability judgments drawn from linguistic theorypublications. (3) The Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) corpusincludes sentence pairs annotated with textual entailment information. (4) The Question-answering NLI(QNLI) (Rajpurkar et al., 2016) is derived from SQuAD, converted to a binary sentence pair classificationtask. (5) The Recognizing Textual Entailment (RTE) datasets come from a series of textual entailmentchallenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).(6) The Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) contains sentencepairs annotated for semantic equivalence. (7) The Quora Question Pairs2 (QQP) (Cer et al., 2017)includes question pairs from Quora annotated for semantic equivalence. The Massive Multitask LanguageUnderstanding (MMLU) dataset (Hendrycks et al., 2021) contains multiple-choice questions that cover 57tasks, which can be divided into four main subsets: STEM, Humanities, Social Sciences, and Other, with14,042 test and 1,531 validation examples. In our experiment, for constructing the proxy reference tasks,we sample from the test set on MMLU. More information about the datasets is provided in .",
  "Dataset#.Training example#.Validation exampleMission Type#.Category": "SST267 349872Sentiment Analysis2CoLA85511043Linguistic Acceptability2MNLI392 70219 647Natural Language Inference3QNLI104 7435463Natural Language Inference2RTE2490277Natural Language Inference2MRPC3668408Semantic Equivalence2QQP363 84940 430Semantic Equivalence2MMLU14042 (Test)1531Question Answering4 : Summary of datasets used in the experiments. For MNLI, 19,647 validation examples consists of 9,815from the matched in-domain section and 9,832 from the mismatched cross-domain section. For MMLU, we reportthe size of test set rather than training set.",
  "A.2Details on Evaluation": "We follow the same evaluation style as HELM (Lee et al., 2023), which expects the model to output thecorrect label word (for GLUE) or the letter of the correct option (for MMLU), rather than directly usingthe probability of the output token for judgement. For example, on SST2, for a sentence with positivesentiment, we expect the model output to be \"positive\". Similarly, on MMLU, the model is expected tooutput one of the letters \"A\", \"B\", \"C\", or \"D\" that matches the correct answer. Otherwise, we consider themodel makes incorrect decisions. Note that this evaluation approach may result in model performanceworse than that of random guessing. However, we believe that it provides a more accurate indication ofthe models ability on instruction following and on solving downstream tasks. For all datasets, we reportthe performance with Accuracy.",
  "A.3Details on Baseline Prompts": "In the main text, we perform COPLE on various scenarios, including Original, 1-shot, 3-shot, EP02,EP03, and Zero-shot-CoT; here we report the detailed prompts of these scenarios. The initial promptsfor GLUE are in -13, and the initial prompts for MMLU are in -15. It should also benoted that for scenarios of Emotion Prompts, following Li et al. (2023), we insert the additional text atthe end of the task description and verbalizer, but before the demo examples, and we do not consider theadditional text to be a part of the task descriptions to perform optimization.",
  "Question: {content}Answer:": ": Detailed baseline prompts for QQP. In the prompt, brown text indicates the task description, which is thetarget that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples forfew-shot learning. {text} denotes the placeholder, which will be replaced with the text sampled from the dataset.",
  "Answer:": ": Detailed baseline prompts of Emotion Prompt and Chain-of-thought setting for MMLU. In the prompt,brown text indicates the task description, which is the target that COPLE performs on, green text indicates theverbalizer, and blue text indicates the demo examples for few-shot learning. {text} denotes the placeholder, whichwill be replaced with the text sampled from the dataset."
}