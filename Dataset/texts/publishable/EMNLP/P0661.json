{
  "Abstract": "In multi-person communications, conflicts of-ten arise. Each individual may have their ownperspective, which can differ. Additionally,commonly referenced offensive datasets fre-quently neglect contextual information and areprimarily constructed with a focus on intendedoffenses. This study suggests that conflictsare pivotal in revealing a broader range of hu-man interactions, including instances of unin-tended offensive language. This paper proposesa conflict-based data collection method to uti-lize inter-conflict cues in multi-person commu-nications. By focusing on specific cue postswithin conversation threads, our proposed ap-proach effectively identifies relevant instancesfor analysis. Detailed analyses are providedto showcase the proposed approach, efficientlygathers data on subtly offensive content. Theexperimental results indicate that incorporat-ing elements of conflict into data collectionnot only significantly enhances the comprehen-siveness and accuracy of detecting offensivelanguage but also enriches our understandingof conflict dynamics in digital communication.",
  "Introduction": "Social media platforms enable communication thattranscends physical boundaries and temporal limita-tions, allowing people from diverse backgrounds tointeract regardless of direct connections. However,this diversity can also lead to challenges. Con-flicts often arise from varying interpretations ofresponses, result in misunderstandings or offense.While the definitions of offensive languagevary, most focus on profanity and the receiversemotional reaction rather than the senders inten-tion (Caselli et al., 2020). Consequently, offensive-ness is often interpreted as subjective to the receiver.However, many datasets on offensive language con-sider only individual texts in their construction,without taking subsequent responses into account. Additionally, numerous studies on offensive lan-guage employ intended affective datasets, usingintention-related language markers such as #hateand #bully for data collection. These markersindicate the senders subjective emotional intent,not necessarily how the receivers interprets thesemessages. Consequently, recipients may not per-ceive the message with the intended emotionalintensity. Furthermore, a message not intendedto offend by the sender but perceived as offen-sive by the receiver constitutes an unintended of-fense. Examples of datasets that overlook suchunintended offenses include the Waseem Datasetor known as HSHP (Hateful Symbols or HatefulPeople) Dataset (Waseem and Hovy, 2016), David-son Dataset or known as AHSD (Automated HateSpeech Detection) Dataset (Davidson et al., 2017),Founta dataset or known as AYR (Are You a Racistor Am I Seeing Things) Dataset (Founta et al.,2018), Kumar/TRAC 2018 Dataset (Kumar et al.,2018), HateEval Dataset (Basile et al., 2019), Of-fense/OLID Dataset (Zampieri et al., 2019) andSocial Bias Inference Corpus (SBIC) (Sap et al.,2020).Unintended offensive expressions are understud-ied compared to the intended one. Often, theseunintended expressions are not meant to harmand may even appear neutral or positive (Huckin,2002); however, recipients can still perceive offen-sive implications that were not intended by thesender.The interpretation of such expressionscan vary greatly depending on the context. De-spite widespread acknowledgment that context in-fluences perceived offensiveness (Breitfeller et al.,2019), existing datasets typically isolate offensiveexpressions from their conversation context. Thisapproach limits the analysis of offensive languageand the development of detection models, prompt-ing on an overly simplistic understanding of howoffensiveness occurs in real interactions (Meniniet al., 2021). Therefore, it is crucial to incorpo-",
  ": Inter-conflict in the conversation": "rate contextual information, such as conversationreplies, when creating datasets focused on unin-tended and implicitly offensive language.One method to address this is by capturing in-stances from the perspective of the reader, specif-ically through perceived emotion. Shmueli et al.(2020) introduced reactive supervision, a method togather posts based on the perspective of a second orthird person, by utilizing cue responses. These arereplies that underscore the perceived emotion in aprevious post. Building on this reactive supervisionmethod, our study proposes a human-interaction-based approach for collecting offensive language.This method extends beyond the individual whoinitiates the post, re-conceptualizing human inter-actions in light of a common phenomenon, humanconflict. We identify two primary types of con-flicts: intra-conflict, which represents internal in-consistency within an individual, and inter-conflict,which relates to perceived disagreements betweenindividuals. illustrates an example of inter-conflict in a conversation, where user A takes of-fense at remarks made by user B, despite the lackof intended offense. This paper introduces a noveldata collection method and a corresponding datasetfor studying inter-conflict, called the UnintendedOffense Dataset1.Exploring human conflict deepens our under-standing of the dynamics of offensive language. Byanalyzing inter-conflict instances, we gain insightsinto how offensive language emerges within socialinteractions. Our data collection strategy capturesa broader spectrum of implicit offensiveness thanexisting datasets and shows less topic bias. Re-evaluating human interactions through the lens ofconflict allows for a more thorough exploration ofthe complex interplay between human expression",
  "The dataset is available at": "and perception.Our main contributions are threefold: (1) As faras our knowledge, this work is the first to focuson identifying expressions of conflict using inno-vative data collection methods for inter-conflictbased on perceived emotion. (2) We developedthe Unintended Offense dataset, which includesdialogue context and exhibits reduced samplingbias in topic distribution. (3) We demonstrate thatmodels trained on the existing datasets are limitedin their ability to identify instances of UnintendedOffense.",
  "Related Works": "Research on offensive language has been widelyconducted, including studies on dataset construc-tion. The data collection methods for these datasetsvary and include distant supervision, crowdsourc-ing, data aggregation, and context- or reaction-based approaches.Distant supervision methods employ languagemarkerssuch as keywords, hashtags, emoticons,and emojisto capture intended affective behav-iors. These markers are often explicit and subjec-tive. For instance, Waseem Dataset (Waseem andHovy, 2016) collected the Twitter (currently X)data by searching common slurs and terms oftenused in religious, sexual, gender and ethnic minori-ties such as MKR and asian drive. The David-son dataset (Davidson et al., 2017) utilized the Hate-base lexicon for search queries and later annotatedas hate speech and offensive. Similarly, Golbecket al. (2017) collected data using derogatory terms,hashtags, and phrases related to race and religion.A Hindi-English code-mixed offensive dataset wascreated using popular hashtags on sensitive top-ics like beef ban and election result (Kumaret al., 2018). For Evalita 2018s Automatic Misog-yny Identification task, the dataset was collectedusing explicit words like bi**h (Fersini et al.,2018), later extending to form the HatEval datasetthrough keyword filtering (Basile et al., 2019). TheOLID dataset (Zampieri et al., 2019), collected databy searching for topic-related keywords such asMAGA and topic-unrelated keywords (e.g. sheis) and was later expanded to distinguish betweenimplicit and explicit offenses (Caselli et al., 2020).A popular offensive public dataset by Fountaet al. (2018) was created by randomly samplingTwitter data, then extracting negative sentimentsand offensive words. Kumar et al. (2018) also gathered data from Facebook fanpages discussingcontroversial topics, alongside Twitter data. Dataaggregation is another method used in offensivedataset collection, exemplified by Sap et al. (2020),where data from various existing sources anddatasets like the Waseem dataset was compiled.The ISHate (Ocampo et al., 2023) dataset enhancedseven existing datasets to address implicit and sub-tle hate speech.Another data collection approach involves iden-tifying hate accounts, as done in Kwok andWang (2013), where data was collected from self-identified or perceived racist accounts, and in ElSh-erief et al. (2021), which gathered tweets, retweets,and replies from three hate groups with the mostfollowers. Some research employed crowdwork-ers to generate offensive language instances, re-sulting in implicitly abusive comparison (Wiegandet al., 2021) and euphemistic abuse (Wiegand et al.,2023) datasets. Additionally, a toxic dataset lever-aging large language models has also been created(Hartvigsen et al., 2022).The datasets mentioned earlier typically consistof standalone posts and do not encompass con-versations or consider responses as contextual in-formation. An exception is the Reddit conversa-tion corpus augmented with automatic responses,called ToxiChat (Baheti et al., 2021). Another of-fensive conversational dataset is CONAN (Chunget al., 2019), which pairs hate comments withtheir counter-narratives. Menini et al. (2021) in-corporated context by extracting previous mes-sages from posts in the Founta dataset. However,some messages might have been deleted or couldnot be recovered, resulting in varied context sizes.The dataset known as the Identification of Conver-sational Hate-Speech in Code-Mixed Languages(ICHCL) (Madhu et al., 2023) is claimed to bethe first to incorporate conversation context fromthe outset. However, while these datasets do takecontext into account, their collection methods pri-marily concentrate on intended offenses, leavingunintended understudied.",
  ": Overall framework": "ously, offensive language is more reflective of thereceivers reaction than the authors intention. Con-sequently, individuals may unintentionally offendothers through their posts.One example that we found from the wild, as il-lustrated in as mentioned previously, UserA experiences offense from User Bs comment: Ihear losing weight helps reduce knee pain (high-lighted in gray), and responds with, You dontknow one thing about me.. User B then followsup with, Didnt mean to offend you (highlightedin yellow), indicating an unawareness of the po-tentially offensive nature of their prior message.This scenario suggests that User Bs initial com-ment likely contained implicit offensiveness. UserBs subsequent statement, didnt mean to offend...signifies that any perceived offense in the previousmessage was unintended.Therefore, phrases such as Didnt mean to of-fend you can function as response cues for identi-fying the presence of unintended offense in onlinecommunication. In order to capture the expres-sion of implicit offense, this phrase is selected asquery template for the data collection. Various di-alogue contexts containing this response cue aresubsequently gathered.",
  "Proposed Framework": "This paper introduce a framework to construct of-fensive language dataset which take into accountthe possible unintended and implicit offense thatmight happen in social media conversation. Theframework can be seen in figure 2.We leverage Twitter API 2.0 academic versionto collect the cue responses by using sentence tem-plate as query. To enhance data quality and reducenoise, two specific filters are applied to the col-lected posts: the quotation filter and the ambiguouspronoun filter. The quotation filter is designed toexclude posts with a high likelihood of quoting oth-ers , while the ambiguous pronoun filter eliminatesposts that focus on clarifying another users inten-tions rather than the authors own. These filtersensure that each response cue is directly associated",
  "with the same authors prior dialogue in the thread.The dialogue contexts remaining after this filteringare referred to as response target threads": "Response Target Threads.Inter-conflict typi-cally unfolds across several exchanges within multi-turn conversations. Our proposed method involvesa comprehensive examination of these conversa-tions, starting from the most recent cue posts andtracing back to earlier parts. This approach is pred-icated on the understanding that indicators of un-intended, implicitly offensive content are likely tohave appeared in the initial segments of the conver-sation.A filtering process is applied to the response tar-get threads before extracting the target offensiveposts. We only included dialogue threads involvingexactly two users. This allows for a more directidentification of offended users. Additionally, con-versations where root posts mention other users areexcluded to avoid confusion, as these often repre-sent extensions of other dialogues. Multi-turn Threads with Context.In scenarioswhere offensive threads are multi-turn, it is ob-served that the same author of the cue posts mightpublish multiple posts in the thread. This can leadto ambiguity regarding which post is being referredto by the cue. To resolve this, a novel conversationdynamic filter is introduced, designed to removeconversations where this ambiguity arises. Definition 1. Conversation Dynamic Filter. Givenan author sequence AC = {aCi } for a dialoguethread C, the author of response cue post aCn isrepresented with X. For authors {aCi }n1i=1 of the re-maining post in the thread, each author is also rep-resented with X if aCi = aCn ; otherwise, aCi is rep-resented by Y. A regular expression Y+(X)Y+X$ is then proposed to match the author sequence AC",
  "represented by X and Y. If AC mismatches the regu-lar expression, dialogue thread C is then removed": "To achieve this, for each thread, the post indi-cated by the cue post is identified as the targetpost. The posts written by the other user beforethe target post are regarded as the context posts,and the post that appears between the target postand the cue post is referred to as the follow-uppost. Examples of this structure are illustrated in. Furthermore, we employ a language filterand a URL filter. The language filter is used to en-sure that only conversations in English are retained.Meanwhile, the URL filter is designed to exclude",
  "Human Annotation": "Searching cue posts with a query template enabledus to crawl 201k posts from the complete Twitterhistory. During the process of reconstructing con-versations, we observed that posts are often missing.This is typically due to users deleting their postsor altering their privacy settings. This observationaligns with findings from previous studies (Klu-bicka and Fernndez, 2018; Menini et al., 2021).We used the previously mentioned ConversationDynamic Filter to retain conversations that matchedthe pattern. As a result, we successfully recon-structed approximately 42k conversations. How-ever, after filtering, a total of 4, 027 fine-grainedthreads remained. Detailed statistics regarding thecollected threads are presented in .Among the collected data, a total of 2, 401 con-versations were randomly sampled and annotatedusing Amazon Mechanical Turk (AMT) with eachconversation receiving at least three annotations.The instruction page outlined how social mediaconversations may be perceived differently by in-dividuals, covering the scope of offensiveness andthe number of conversations, including a warningabout adult content and advising workers to ex-ercise discretion. It also provided examples in atemplate similar to the question layout. Annota-tors were paid approximately $0.40 per assignment,with payment issued within three days.We asked the annotators to assume themselves asthe author of the context posts, this is because Hube(2020) claimed that it would increase the qualityof the annotation. We also asked the annotators toassume the writer of the target posts as their unfa-miliar friends, making them focus on the semanticmeaning instead of inferring the relationship. Theannotation schema ranged from not offensive atall to extremely offensive (range 0 - 100) andthe annotators are required to provide their con-fidence rating with the range of 0 - 100. Conse-",
  ": The results of the implicitness measurement": "quently, an offensiveness greater than 0 indicatedthe presence of offense. For annotator agreement,we calculated the average score, and if it exceeded50, the post was considered offensive.From the annotation process, we found that 80%(around 1, 920 conversations) of offensiveness rat-ing were made with confidence 50.Mean-while, the average offensiveness in our dataset isdescribed by a mean of 51.71, a median of 53.66,and a standard deviation of 16.81. These figuressupport the methods ability to detect offensive lan-guage, as shown by the fact that more than halfof the posts have an average offensiveness scoreabove 50. Additionally, the concentration of scoresnear the median value of 50 indicates that mostinstances of language in the study are rated aroundthis median. This aligns with the understandingthat unintended implicit offensiveness, while sub-tle, can still be harmful.",
  "Implicitness": "Following the procedure developed by Wiegandet al. (2019), we calculate the proportion of im-plicitly offensive messages among the offensivemessages for each dataset above. In the existingdatasets with more fine-grained classes for offen-sive language sub-types, the offense-related cat-egories were consolidated into a single offensivecategory. Subsequently, the classification for mea-suring implicitness was simplified to binary cate-gories: non-offensive and offensive.For the Unintended Offense dataset, target postswith an average offensiveness score equal to or ex-ceeding a predefined offensiveness threshold T anda confidence score of 50 or more were classifiedas offensive. Instances not meeting these criteriawere labeled as non-offensive. Accordingly, the",
  "lower bound of implicitness in this dataset wasdetermined by excluding these subtle instances": "The results of the implicitness measurement aredetailed in . Among the existing datasets,Kumar and SBIC exhibited highest levels of im-plicitness. Notably, the implicitness in datasetsobtained through biased sampling was significantlyhigher than the Founta dataset, a finding that isin line with the research of Wiegand et al. Wie-gand et al. (2019). Overall, the implicitness in ourdataset surpassed that of Kumar and SBIC by 5 and13 points, respectively. This indicates that the datacollection method employed in this study is moreeffective in capturing implicitly offensive languagecompared to traditional biased sampling methods.",
  "Topic Selection Bias": "Many of the offense datasets are constructed bybiased sampling over the manually defined top-ics (Wiegand et al., 2019), and this manually de-fined process might introduce bias. However, as nomethod exists to evaluate such selection bias, com-paring the bias level among datasets is challenging.To address this issue, a measurement method wasproposed to reflect the level of topic selection biasby comparing dissimilarity in topics between eachdataset and reference dataset. The dataset constructed by boosted random sam-pling is considered to be the reference dataset(denoted by Dref). We used Founta dataset as ourreference dataset, because they have a similar topicdistribution to the overall social media posts. Weleveraged the topic distribution of Dref to measurethe degree of topic selection bias in our datasetand the dataset constructed by biased sampling.The key concept is to calculate the gap betweenthe topic distribution of Dref and the dataset tobe measured, which is called the target dataset(denoted by Dtar). The gap represents how signifi-cantly the topic distribution of Dtar deviates fromthat of Dref. Thus, we can regard the gap as themeasurement of topic selection bias. To model the topic distribution in datasets, apre-trained Latent Dirichlet Allocation (LDA)with N topics is leveraged. For each instance inDref, a topic distribution vector vrefi RN isgenerated by LDA, where i = 1, ..., |Dref|. Thetopic distribution vector of the whole referencedataset, vref, is aggregated by the followingequation.",
  "||vref|| ||vtar||(3)": "We leverage the proposed approach to comparethe topic selection bias of our dataset with thatof datasets constructed by biased sampling. Thelarge-scale Wikipedia Corpus is utilized to train thenoun-only LDA with topic number N = 100 togenerate the coherent topics (Martin and Johnson,2015).To generate the topic vector representing thetopic distribution of each existing dataset, the mes-sages were solely input into the pre-trained LDA.We aggregated the outputs by Equation 1 or Equa-tion 2 to approximate the topic distribution in eachdataset. Additionally, we concatenated the contextposts and target posts in our dataset before apply-ing the pre-trained LDA. The intuition is that thecontext posts and target posts in the same threadswould discuss the same topics. Concatenating theposts might provide more information for the LDAto model the topics.For comparison, Kumar and Waseem were se-lected, since they have the highest implicitnessamong the biased-sampling baseline datasets. Asshown in , the cosine distance of the topicdistributions between our dataset and Founta wasthe smallest among all datasets. This finding shows",
  ": The proportion of positive and negative influ-ence on implicitly offensive tweets with offensiveness T": "that the topic distribution of our method is muchcloser to the dataset constructed by boosted randomsampling (Founta). In other words, the proposedmethod introduces less topic selection bias than thebiased sampling.Previously, Wiegand et al. (2019) found that bi-ased sampling can be more efficient in capturingimplicit offensiveness. Taking implicitness intoconsideration, it was discovered that the implicit-ness of Kumar is the highest of those considered,and its topic selection bias is also much higher thanour dataset and Waseem. In other words, the mea-surement results showed that our data collection notonly achieve the higher implicitness than existingdatasets, but also introduce less topic selection bias.Thus, the proposed approach seems to alleviate thetrade-off between capturing implicit offensivenessand introducing less topic selection bias, which arethe limitations of boosted random sampling andbiased sampling.",
  "Context Influence": "We further investigate the difference of the per-ceived offensiveness between situations where con-text is provided and those where it is absent. Usingour annotation schema, we obtain offensivenessratings both with and without context from eachannotator.Let Runcon denote the rating of offensivenesswithout context, and Rcon denote the rating withcontext. The influence of context on perceivedoffensiveness is denoted by . The calculation ofcontext influence is formulated as follows:",
  ": The BERT Offense Classification Results. Teston Founta (-) and Ours (50+). N=262/262": "Conversely, a negative value suggests that the in-clusion of context results in a decrease in perceivedoffensiveness.In this study, we randomly sample two hundredconversation, each data instance is annotated byfive independent crowd-workers from a pool of 72annotators, each contributing an average of 2.78AMT Human Intelligence Tasks (HIT). At first,annotators were shown only the target post andasked to rate its offensiveness, without consider-ing the previous post for context. Afterward, theywere given the full conversation and asked to ratethe offensiveness again. The evaluation of contextinfluence on perceived offensiveness is based onEquation 4, focusing on the differences in offensive-ness ratings with and without context, as providedby the same annotator for each target post. illustrates the impact of context on theperception of implicitly offensive posts under vary-ing offensiveness thresholds T. The analysis re-veals that regardless of the offensiveness levels,the implicit offensiveness is generally more proneto escalation. As T increases, there is a notablegrowth in the instances of positive context influ-ence, with the disparity between positive and nega-tive influences widening. This suggests that postswith higher offensiveness are often deemed lesssevere when the context is disregarded. Conse-quently, this emphasizes the significance of consid-ering contextual elements in assessing the implicitoffensiveness of posts.",
  ": The GPT Offense Classification Results. Teston Founta (-) and Ours (50+). N=262/262": "itive, two public datasets were used to constructlabel-balanced datasets through random sampling.These additional datasets also served as bench-marks. Since the two public datasets do not includecontext, we omitted the context post during theexperiment and used only the target post.In each dataset, BERT was trained for fiveepochs using cross-entropy loss. In the experimen-tal setup where our dataset involved, our datasetrepresented the positive category, while Fountadataset was used for the negative category. Theaggregated dataset was then randomly divided intotraining and testing sets with an 8:2 ratio, ensuringlabel balance. Our annotated dataset was evenlydistributed across these sets, and the number ofnegative examples was adjusted to match the pos-itives, ensuring a balanced representation of bothcategories for analysis.For GPT-4, both zero-shot and few-shot prompt-ing with randomly sampled examples were tested.The number of positive and negative examplesshown to GPT-4 in few-shot prompts was equal.The test set consisted of our dataset as the posi-tive category and the Founta dataset as the negativecategory.",
  "Result and Discussion": "RQ1: Can models trained on existing datasetsdetect Unintended Offense? The performanceof single run fine-tuned BERT models is summa-rized in . When trained solely on the OLIDand Founta datasets, these models demonstratedlimited success in identifying Unintended Offenseposts that were more obviously offensive, indicatedby offensive values exceeding 50. The majorityof posts were classified as non-offensive by thesemodels. Notably, the model trained with OLID datashowed slightly better performance, which might be attributed to the general keywords used in itsdata collection process.The performance of GPT-4, as shown in Ta-ble 5, revealed that under zero-shot prompting, itfaced challenges in recognizing Unintended Of-fense posts with higher offensive values (e.g., >50). The introduction of positive and negative ex-amples from the Founta dataset, in both one-shotand five-shot prompting scenarios, led to only mi-nor improvements. In these cases, the models pri-marily predicted posts as non-offensive. Anotherexperiment with GPT-4 involved introducing exam-ples from the OLID dataset for five-shot prompt-ing, resulting in a low macro-average F1 score ofaround 0.474. Similar to the results using Founta,the recall score was not high enough, indicatingthat Unintended Offense posts were not correctlypredicted.These experiments suggest that using existingdatasets for training or introducing examples solelyfrom existing datasets is insufficient to identify Un-intended Offense posts effectively. RQ2: Can the collected data improve Unin-tended Offense detection?To improve Unin-tended Offense detection, the study integratednewly collected data into the fine-tuning processof BERT. As shown in the results, BERT was bet-ter able to detect Unintended Offense even whentrained on just 2, 000 high-quality labeled posts,although it remained more likely to predict postsas non-offensive.Incorporating a larger dataset, consisting of5, 300 unlabeled samples from the collection, ledto a significant improvement in the macro F1 score,highlighting the utility of the collected data despitepotential noise. The most notable enhancementwas observed when the model was trained on theentire dataset, excluding the test split, which in-cluded less obvious Unintended Offense samples.The fully trained model successfully identified Un-intended Offense, achieving a macro F1 score of0.8.To further investigate RQ2, the collected datawere utilized as examples in few-shot prompts forGPT-4. For comparison with the performance whenusing only Founta data as examples, the collecteddata with a labeled offensive value of 50 or higherwere chosen as positive examples, while Fountasnegative data served as negative examples. As in-dicated in , the inclusion of the collecteddata enhanced classification performance under both one-shot and five-shot conditions. Specifi-cally, with five-shot prompting, the collected dataachieved a macro F1 score close to 0.73, demon-strating that GPT-4 could more effectively detectUnintended Offense when informed by the col-lected data, even with limited examples. A sim-ilar result was observed when introducing samplesfrom the collected dataset and the OLID datasetin five-shot prompting, with the macro average F1score close to 0.6, better than five-shot promptingwith OLID dataset samples (F1 of 0.47). Thesefindings suggest that the collected data, even inthe absence of human labeling, substantially con-tributes to the effectiveness of Unintended Offensedetection. To explore unintended dataset detection, we ana-lyze the classification results of unintended offen-sive posts that were accurately predicted as offen-sive when the collected dataset was introduced, butnot predicted as such when it was not introduced.One example involves a context where someone isdiscussing an athlete from their favorite team, mak-ing the response Lives with his Mum! inappropri-ate. In another scenario, an adult mentioning drink-ing Mt. Dew receives the response I thoughtonly 13-year-old boys drank Mt. Dew??? This isconsidered offensive due to its biased assumption,even if the responders intent was genuine curiosity.Lastly, when someone shares their daily routine,the response OMG, are you narrating your day?Wicked may be perceived as offensive for poten-tially mocking the individual.",
  "Conclusion": "This work proposes a data collection frameworkthat considers the context based on post reac-tions.The framework captures unintended of-fensive tweets by leveraging the concept of inter-conflict. Using this approach, a dataset of Unin-tended Offense posts was successfully compiled.Comprehensive analyses and experiments con-ducted across various datasets revealed that ourdataset exhibits rich emotional depth, higher im-plicitness, and reduced topic bias. The findings alsoindicate that models trained on existing datasetshave difficulty to accurately recognize UnintendedOffense. We believe that our open-source datasetand methodology will facilitate more comprehen-sive data analysis and research opportunities in thisdomain.",
  "Limitations": "In this paper, we employed human annotators to la-bel our collected data. While some quality controlswere implemented, such as requiring three anno-tations for each instance, subjectivity may still bepresent. This is partly due to the subjective natureof offensiveness, which depends on the receiversperception and open for personal interpretation.Furthermore, since we used Amazon Mechan-ical Turk, which relies on non-expert judgment,variability in the annotations might still occur. Toensure the consistency and quality of the data, onlynative English speakers from the USA, UK, andAustralia with an approval rate higher than 90%and more than 100 approved submissions were al-lowed to participate. However, the selection ofannotators from specific countries could lead tobiases influenced by their unique backgrounds andcultural perspectives, potentially limiting the gen-eralization of the findings. Additionally, we usedthe average score for annotator agreement, whichoverlooks any extreme cases of high and low scoreswithin a single post, though such occurrences wererare during our annotation phase.The models used to evaluate the datasets are lim-ited to BERT and ChatGPT version 4.0, and therange of datasets we compared is also restricted.Additionally, the classification performed is binary,rather than a more refined classification that differ-entiates between implicit and explicit categories.These conditions, if considered, might result indifferent performance outcomes.",
  "Ethics Statements": "We collected our dataset from Twitter, complyingto its policies at the time of collection. Since ourdataset includes responses, we initially retainedthe usernames. However, after constructing theneeded conversation, we replaced all usernameswith @user. Despite this, the data might still retainsome personal information, so responsible use ofthe data is encouraged.This dataset is intended to enhance offensive lan-guage detection. Nonetheless, we acknowledgethat because the dataset still contains offensive lan-guage, it is susceptible to misuse, such as generat-ing harmful language, which is not the intention ofthe authors.We used existing datasets for comparison andevaluation with our dataset. Therefore, the use ofthese existing datasets complied with their intended",
  "This research is supported by National Science andTechnology Council 108-2221-E-007-064-MY3,111-2221-E-007 -110 -MY3": "Ashutosh Baheti, Maarten Sap, Alan Ritter, and MarkRiedl. 2021. Just say no: Analyzing the stance of neu-ral dialogue generation in offensive contexts. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 48464862, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics. Valerio Basile, Cristina Bosco, Elisabetta Fersini,Debora Nozza, Viviana Patti, Francisco ManuelRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.2019. SemEval-2019 task 5: Multilingual detectionof hate speech against immigrants and women inTwitter. In Proceedings of the 13th InternationalWorkshop on Semantic Evaluation, pages 5463, Min-neapolis, Minnesota, USA. Association for Compu-tational Linguistics. Luke Breitfeller, Emily Ahn, David Jurgens, and YuliaTsvetkov. 2019. Finding microaggressions in thewild: A case for locating elusive phenomena in socialmedia posts. In Proceedings of the 2019 conferenceon empirical methods in natural language processingand the 9th international joint conference on naturallanguage processing (EMNLP-IJCNLP), pages 16641674. Tommaso Caselli, Valerio Basile, Jelena Mitrovic, IngaKartoziya, and Michael Granitzer. 2020. I feel of-fended, dont be abusive! implicit/explicit messagesin offensive and abusive language. In Proceedingsof the Twelfth Language Resources and EvaluationConference, pages 61936202, Marseille, France. Eu-ropean Language Resources Association. Yi-Ling Chung, Elizaveta Kuzmenko, Serra SinemTekiroglu, and Marco Guerini. 2019.CONAN -COunter NArratives through nichesourcing: a mul-tilingual dataset of responses to fight online hatespeech. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 28192829, Florence, Italy. Association forComputational Linguistics. Thomas Davidson, Dana Warmsley, Michael Macy, andIngmar Weber. 2017. Automated hate speech detec-tion and the problem of offensive language. Proceed-ings of the International AAAI Conference on Weband Social Media, 11(1):512515.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of": "the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish-navi Anupindi, Jordyn Seybolt, Munmun De Choud-hury, and Diyi Yang. 2021. Latent hatred: A bench-mark for understanding implicit hate speech. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 345363,Online and Punta Cana, Dominican Republic. Asso-ciation for Computational Linguistics.",
  "Elisabetta Fersini, Debora Nozza, and Paolo Rosso.2018. Overview of the evalita 2018 task on automaticmisogyny identification (ami). In EVALITA@CLiC-it": "Antigoni Founta, Constantinos Djouvas, DespoinaChatzakou, Ilias Leontiadis, Jeremy Blackburn, Gi-anluca Stringhini, Athena Vakali, Michael Sirivianos,and Nicolas Kourtellis. 2018. Large scale crowd-sourcing and characterization of twitter abusive be-havior. Proceedings of the International AAAI Con-ference on Web and Social Media, 12(1). Jennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo,Alexandra Berlinger, Siddharth Bhagwan, Cody Bun-tain, Paul Cheakalos, Alicia A. Geller, Quint Ger-gory, Rajesh Kumar Gnanasekaran, Raja Rajan Gu-nasekaran, Kelly M. Hoffman, Jenny Hottle, VichitaJienjitlert, Shivika Khare, Ryan Lau, Marianna J.Martindale, Shalmali Naik, Heather L. Nixon, PiyushRamachandran, Kristine M. Rogers, Lisa Rogers,Meghna Sardana Sarin, Gaurav Shahane, JayaneeThanki, Priyanka Vengataraman, Zijian Wan, andDerek Michael Wu. 2017. A large labeled corpus foronline harassment research. In Proceedings of the2017 ACM on Web Science Conference, WebSci 17,page 229233, New York, NY, USA. Association forComputing Machinery. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.ToxiGen: A large-scale machine-generated datasetfor adversarial and implicit hate speech detection.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 33093326, Dublin, Ireland.Association for Computational Linguistics.",
  "Thomas N. Huckin. 2002. Critical discourse analysisand the discourse of condescension": "Filip Klubicka and Raquel Fernndez. 2018. Examininga hate speech corpus for hate speech detection andpopularity prediction. In 4REAL 2018 Workshop onReplicability and Reproducibility of Research Resultsin Science and Technology of Language, page 16. Ritesh Kumar, Aishwarya N. Reganti, Akshit Bha-tia, and Tushar Maheshwari. 2018.Aggression-annotated corpus of Hindi-English code-mixed data.In Proceedings of the Eleventh International Confer-ence on Language Resources and Evaluation (LREC2018), Miyazaki, Japan. European Language Re-sources Association (ELRA).",
  "Reactive supervision: A new method for collectingsarcasm data. CoRR, abs/2009.13080": "Zeerak Waseem and Dirk Hovy. 2016. Hateful symbolsor hateful people? predictive features for hate speechdetection on Twitter. In Proceedings of the NAACLStudent Research Workshop, pages 8893, San Diego,California. Association for Computational Linguis-tics. Michael Wiegand, Maja Geulig, and Josef Ruppenhofer.2021. Implicitly abusive comparisons a new datasetand linguistic analysis. In Proceedings of the 16thConference of the European Chapter of the Associ-ation for Computational Linguistics: Main Volume,pages 358368, Online. Association for Computa-tional Linguistics. Michael Wiegand, Jana Kampfmeier, Elisabeth Eder,and Josef Ruppenhofer. 2023. Euphemistic abuse a new dataset and classification experiments for im-plicitly abusive language. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1628016297, Singapore.Association for Computational Linguistics. Michael Wiegand, Josef Ruppenhofer, and ThomasKleinbauer. 2019. Detection of abusive language:the problem of biased datasets. In Proceedings ofthe 2019 conference of the North American Chap-ter of the Association for Computational Linguistics:human language technologies, volume 1 (long andshort papers), pages 602608. Marcos Zampieri, Shervin Malmasi, Preslav Nakov,Sara Rosenthal, Noura Farra, and Ritesh Kumar.2019. Predicting the type and target of offensiveposts in social media. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), pages 14151420, Minneapolis, Minnesota.Association for Computational Linguistics."
}