{
  "Abstract": "In this paper, we apply a method to quantifybiases associated with named entities from var-ious countries. We create counterfactual exam-ples with small perturbations on target-domaindata instead of relying on templates or specificdatasets for bias detection. On widely usedclassifiers for subjectivity analysis, includingsentiment, emotion, hate speech, and offen-sive text using Twitter data, our results demon-strate positive biases related to the languagespoken in a country across all classifiers stud-ied. Notably, the presence of certain countrynames in a sentence can strongly influence pre-dictions, up to a 23% change in hate speechdetection and up to a 60% change in the pre-diction of negative emotions such as anger. Wehypothesize that these biases stem from thetraining data of pre-trained language models(PLMs) and find correlations between affectpredictions and PLMs likelihood in Englishand unknown languages like Basque and Maori,revealing distinct patterns with exacerbate cor-relations. Further, we followed these correla-tions in-between counterfactual examples froma same sentence to remove the syntactical com-ponent, uncovering interesting results suggest-ing the impact of the pre-training data was moreimportant for English-speaking-country names.Our anonymized code is available here.",
  "Introduction": "Recent trend in Natural Language Processing re-search, like in works published at conference suchas ACL (Rogers et al., 2023), is to provide open-source data and models (Scao et al., 2022). Thispractice not only enhances its value for generalresearch purposes but also facilitates the deploy-ment of these models in diverse operational set-tings by companies or stakeholders. Applicationssuch as customer experience, CV screening, So-cial Media analyses and moderation are exampleof applications that will directly impact the users in different ways. For this reason, the models ap-plied at large scale should be scrutinized in orderto understand their behavior and should tend tobe fair by passing successfully a series of test toreduce their biases toward various target groups.Past study (Ladhak et al., 2023) showed that PLMsare impacted by names, and Barriere and Cifuentes(2024) proposed a method to quantify this to detectbiases of the model toward specific countries, usingthe country most common names as a proxy. Weare showing in this paper that this bias is systematicin several widely-used off-the-shelf classifiers onEnglish data, and propose a method to directly linkthe bias level with the perplexity of the PLM ContributionsWe propose an investigation intobiases related to country-specific names in widelyused off-the-shelf models (Barbieri et al., 2020,2022), commonly deployed in production envi-ronments for Twitter data.1 Our analysis revealsdistinct biases in sentiment, emotion, and hatespeech classifiers, showing a propensity to favornames from certain countries while markedly dis-favoring those from less Westernized nations, of-ten by a large margin. Furthermore, we establisha global-level correlation between the perplexityof associated PLMs and model predictions acrossboth known and unknown (i.e., Out-of-Distribution;OOD) languages, demonstrated through examplesin English, Basque, and Maori. At a local level,we mitigate the influence of syntax on perplexityby examining the correlation among counterfactualexamples generated through minor perturbations.Notably, our findings suggest that the frequencyof a names occurrence during the training phasedirectly impacts the sentiment models tendency toproduce positive outputs, which highly disadvan-tage the non-English (i.e., OOD) persons in a world",
  "Regarding the number of monthly downloads ofcardiffnlp models from Barbieri et al. (2020, 2022) in theHuggingface Model Hub at the time of writing (>4m forsentiment)": ": Overview of the counterfactual example creations. We show examples with sentiment and hate speech forvariation of the name \"Alexander\" and two sentences S1 and Sn. S1 : \"I do not like you [PER] you fucking bitch\".The NER is applied to the production data to create templates, which are then filled randomly with most commonnames from gazeeters of different countries to create a pool of counterfactuals. The discrepancies in probabilitiesis quantified using metrics such as .",
  "Related Work": "As it is known that models still learn bias whenfine-tuned on downstream tasks and that the cor-relation is low between the intrinsic bias scores ofthe initial model and its extrinsic bias scores af-ter fine-tuning (Kaneko et al., 2022a, 2024), weuse a method to evaluate an already trained classi-fier and not the pre-trained language model. Someworks propose such thing as general \"unit-test\" forNLP models (Ribeiro et al., 2020) or even apply-ing a battery of fairness tests (Nozza et al., 2022).However, extrinsic methods mainly relies on tem-plate or datasets (Czarnowska et al., 2021; Kuritaet al., 2019; Guo and Caliskan, 2021), which havebeen proven to influence considerably the bias es-timation and conclusion across template modifica-tion (Seshadri et al., 2022). A potential solutionis to apply perturbation on the test data. Pertur-bations can be used for attribution methods (Felet al., 2023), but also for testing a models robust-ness (Ribeiro et al., 2020). They allow getting ridof the aforementioned template issue and data col-lection methodology: directly used on the targetdomain data, it prevents for not properly evaluatingthe intended notion of bias (Blodgett et al., 2020).The origin of the bias generally comes from thetraining data (Caliskan et al., 2017), as a lot ofinformation can be stored in the network (Petroniet al., 2019; Carlini et al., 2021, 2018) due to repe-titions of the same sentences or concepts. This typeof over-representation in the training data involve arepresentation bias, such as the one demonstratedby Kaneko and Bollegala (2022) regarding the gen- der as masculine was over-represented. This wasfound out to be correlated with the likelihood of themodel. For example, Barikeri et al. (2021) proposea perplexity-based bias measure meant to quantifythe amount of bias in generative language mod-els along several bias dimensions. For this reason,Kaneko et al. (2022b) propose to use the likelihoodas a proxy to estimate the bias on gender. In ourcase, we validate that the bias is already present inthe PLM, by calculating the correlation between thelikelihood and different classes for country-name.This technique is even more efficient with genera-tive models (Ouyang et al., 2022; Jiang et al., 2024)as one can apply it directly on production model. Although names are not inherently linked to aspecific nationality, research has revealed the pres-ence of nationality biases within them. Delvinginto this underexplored domain, Venkit et al. (2023)shed light on the influence of demographic on bi-ases associated with countries in language models.An and Rudinger (2023) offer insights into the in-tricate relationship between demographic attributesand tokenization length, particularly focusing onbiases related to first names. Zhu et al. (2023)propose to mitigate name bias by disentangling itfrom its semantic context in machine reading com-prehension tasks. Ladhak et al. (2023) investigatethe propagation of name-nationality bias, demon-strating through intrinsic evaluation with templateshow names and nationalities are intrinsically linkedand how biases manifest as hallucinations. Lastly,Barriere and Cifuentes (2024) showed that usingnames as proxy works to detect country-relatedbiases depends on the sentences language, in mul-tilingual sentiment and stance recognition models(Barriere and Balahur, 2023; Barriere and Jacquet,2022; Barriere et al., 2022).",
  "Method": "We first rely on Named Entity Recognition (NER)to create counterfactual examples from the target-domain, specific of target groups, following themethodology of Barriere and Cifuentes (2024). Thebias is assessed by quantifying the differences inthe model outputs. Second, we ran a series ofexperiences studying the correlation between theoutput variations and the perplexity. showsan overview of the bias detection.",
  "Perturbation-based Counterfactuals": "Counterfactual GenerationA set of counter-factual examples are constructed from the target-domain data using a NER system combined witha list of most common names from different coun-tries. Each named entity automatically tagged asperson is substituted by a random common namefrom a specific country. Note that the original en-tity is conserved, by looking in our gazeeters itscorresponding gender. More details are found inthe Appendix A. Bias CalculationIn order to assess the bias, wecalculate the percentage of change in terms oftagged examples, using the confusion matrices. Forsentiment, we also computed the change in differ-ence in probability between positive predictionsand negative predictions .",
  "Perplexity and Likelihood": "General and Pseudo-PerplexityThe perplex-ity of a language model measures the likelihoodof data sequences and represents how fluent it is(Carlini et al., 2018). In simpler terms, perplexityreflects how unexpected a particular sequence is tothe model. A higher perplexity suggests that themodel finds the sequence more surprising, whilea lower perplexity indicates that the sequence ismore likely to occur. We refer to the definition ofpseudo-log-likelihood introduced by Salazar et al.(2020), the pseudo-perplexity being the opposite ofit. For a sentence S = w1, w2, ..., w|S|, the pseudo-log-likelihood (PLL) score given by Eq. 1, can beused for evaluating the preference expressed by anMLM for the sentence S.",
  "(2018) is the negative log likelihood, hence we": "use pseudo-log-perplexity as simply the oppo-site of the PLL.2 More details are provided in Ap-pendix B. In the following, we will not use the termpseudo- when talking about the pseudo- perplexityor likelihood. Bias quantificationWe calculated the Pearsoncorrelation between the probabilities output andlikelihood in two ways. First, what we call globalcorrelation, i.e., between all the examples of thedataset, in order to shed lights on a general patternbetween perplexity and subjectivity. Second, whatwe call local correlations, i.e., between elementscoming from the same original sentence, beforeaveraging them. In this way, we can disentanglethe syntactic aspect of the sentences that have animpact in the likelihood calculation. This is sim-ilar to normalizing the perplexity and likelihoodof every examples coming from the same sentencebefore calculating the Pearson correlation.",
  "Experiments": "Bias DetectionOur first experiment focuses onquantifying the country names bias for differentoff-the-shelf models previously learned on tasksthat are related to affects, looking at the probabilityof positiveness and the percentage of change innumber of predicted examples per class. Global PerplexityThe second experiment aimsto show that the model predictions are in generalintricately linked with the perplexity even for un-known languages. We first create datasets in theseunknown languages using Machine Translation(MT) in order to preserve the semantic content in-between the different languages, as they did in inBalahur and Turchi (2013). We then calculate the\"global\" correlation between perplexity and outputprobabilities in English and unknown languagessuch as Maori and Basque, which we obtain usingGoogle Translate.3 More details in Appendix C. Local PerplexityTo remove the syntactic aspectinfluencing both perplexity and predictions, we con-duct experiments focusing on what we call \"local\"correlation, which is between the relative probabil-ities of each class among counterfactual examples 2Contrary to the definition of Salazar et al. (2020) definingit on a complete corpus, summing between all the sentencesbefore passing it to exponential.3Google MT is based on the LLM PaLM 2 (Google, 2023),which should work reasonably well for these two languagesalready used in production.",
  "Experimental Protocol": "GazeetersWe used the dataset collected fromWikidata Query Service.4by the authors ofChecklist, composed of common first and lastnames as well as the associated cities from sev-eral countries. This makes a total of 16,771 malefirst names, 12,737 female first names, 14,797 lastnames from 194 countries. NERWe use a multilingual off-the-shelf NERsystem available on the Spacy library (AI,2023) and created for social media (namedxx_ent_wiki_sm) to identify entities for removalin target-domain data, aligning with the data usedduring model deployment.",
  "PerturbationFor every sentence x, we create 50random perturbations of this sentence for each ofthe target countries": "DatasetIn order to apply our method to data sim-ilar to production data, we collected 8,891 randomtweets in English by using the IDs from the Eu-rotweets dataset (Mozetic et al., 2016). The 8,891tweets used in the experiment correspond to a ran-dom selection of 10% of the English tweets of theEuroTweets dataset (Mozetic et al., 2016) down-loaded in June 2020.5",
  "Results": "Bias Detection provides a comprehen-sive overview of the impact of country-specificnamed entities on sentiment, emotion, hate speech,and offensive text classifications across diverseclassifiers. Notably, it reveals significant variationsin model predictions based on the presence of dif-ferent country names within textual data. For senti-ment analysis, it is striking to observe substantialshifts in sentiment probabilities ()6 across coun-tries. For instance, countries like India, Turkey orSpain exhibit noteworthy deviations in sentimentprobabilities, indicating potential biases in classi-fier outputs concerning specific national contexts.7",
  ": Global correlations between PPL and classesfor different languages, tasks or pre-trainings": "sistently receiving more positive or negative senti-ment classifications compared to others. Emotionanalysis reveals intriguing patterns in the distribu-tion of predicted emotions across countries. Opti-mism shows an interesting pattern where the non-English names highly decrease this prediction, upto -33% for Moroccan. It is also notable that Mo-roccan names provoke a very high increase (60%)of anger predictions at the expense of the otherclasses. Finally, a similar pattern can be seen for thehate speech and offensive text classifiers. English-speaking countries names highly favor hate speechdetection, even as a false positive, compared toother countries. For offensive text detection, thereis an increase of 6.1% with counterfactuals usingUS names and a decrease of 4.9% and 2.1% usingMoroccan and Hungarian names. Global Subjectivity-Perplexity CorrelationTa-ble 2 shows the correlations between the perplex-ity and the labels for Sentiment and Hate speechtasks using tweets from different languages, ob-tained using Machine Translation. For the hatespeech model, the global correlation between thehate speech class and the perplexity is almost closeto zero for English data, which is good since show-ing no spurious pattern between perplexity and hatespeech prediction. However, the correlations arehigher for the unknown language such as Basqueand Maori, where it reaches more than 22%. Themodel tends to classify as hate speech more eas-ily texts having a higher perplexities, i.e., that areoutside the training distribution. For the Sentimentmodel, the pattern for Basque and Maori languageis the same, high positive/negative correlation forthe negative/positive class, which means that theless the sentence is similar to the train distribution,the more negative it would be. Additional exper-iments using other languages are confirming theresults, and are available in Appendix D.",
  ": Correlations between the relative perplexity ofthe model and the relative output probabilities": "plexity of the model and the probabilities of dif-ferent classes. The results are very different fromglobal correlations. Notably, there is a negativecorrelation between perplexity and positiveness ofthe sentiment, which implies that names that aremore similar to what was seen during the PLM pre-training will imply a more positive output of thesentiment classifier. This trend is particularly pro-nounced among English-speaking countries. Dueto lack of space, more details and results can befound in Appendix E.",
  "Conclusion": "Bias at the nationality level can also occur withthe most common entities of the country such asnames. We show its occurrence in this paper fora set of tasks that are related with affect and sub-jectivity classification, using several transformermodels widely used on Twitter data. Motivatedby prior research, we studied the link between thisbias and the perplexity of the PLM showing (i) ex-acerbate correlations in unknown languages, and(ii) verify that correlation can be related to namesusing counterfactual sentences. We found out inter-esting patterns using the Pearson correlations be-tween the classes and perplexity, revealing highercorrelations for English-speaking country names,meaning that the exposition bias on names impactsthe predictions also in-between a country.",
  "Limitations": "First, our method only relies on Named Entities,so it does miss all the implicit hate speech. Nev-ertheless, it is a system with low recall but highprecision as when it detects a change, meaning thatthe classifier behavior is biased. Second, even ifour method slightly perturbates the data from thetarget distribution, it does not explicitly keep it in-side, creating examples that might be a bit outsidethe distribution of the production data. We thinkthat is the reason why we see a general shift to-ward a more negative sentiment when comparingperturbated examples and true examples (negativepredictions always augment while positive predic-tions always decrease). It would be more naturalto use target-data-specific lexicons, or use a gen-erative model to do the job. However, we thinkthat this is a fair comparison toward all the coun-tries and it can drive a pertinent conclusion on arelative bias between the different countries. An-other bias induction can also come from the factthat some names can be non gendered in some con-text, such as Claude as a first-name or Jane as asurname (for a man) that would be tagged as fem-inine. Co-reference resolution could mitigate thisissue, even though we believe it is uncommon. Fi-nally, we compare a masked language model, butfurther experiments are left for future workusinggenerative models such as flan-T5 (Chung et al.,2022) or Mixtral (Jiang et al., 2024) where the samemodel computes both label and perplexity, for ex-ample using label tokens probabilities to estimatethe probabilities (Hegselmann et al., 2023).",
  "Acknowledgements": "The authors thank the reviewers for the variouscomments that helped to improve the manuscript.This work has been partially funded by Na-tional Center for Artificial Intelligence CENIAFB210017, Basal ANID. Martn Abadi, Paul Barham, Jianmin Chen, ZhifengChen, Andy Davis, Jeffrey Dean, Matthieu Devin,Sanjay Ghemawat, Geoffrey Irving, Michael Isard,Manjunath Kudlur, Josh Levenberg, Rajat Monga,Sherry Moore, Derek G. Murray, Benoit Steiner,Paul Tucker, Vijay Vasudevan, Pete Warden, Mar-tin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.TensorFlow: A system for large-scale machine learn-ing. Proceedings of the 12th USENIX Symposium",
  "Haozhe An and Rachel Rudinger. 2023. Nichelle andNancy : The Influence of Demographic Attributesand Tokenization Length on First Name Biases. InACL, volume 2, pages 388401": "Alexandra Balahur and Marco Turchi. 2013. Improv-ing sentiment analysis in twitter using multilingualmachine translated data. International ConferenceRecent Advances in Natural Language Processing,RANLP, (September):4955. Francesco Barbieri, Luis Espinosa Anke, and JoseCamacho-Collados. 2022. XLM-T: A MultilingualLanguage Model Toolkit for Twitter. In Workshop onComputational Approaches to Subjectivity, Sentiment& Social Media Analysis @ ACL. Francesco Barbieri, Jose Camacho-Collados, LeonardoNeves, and Luis Espinosa-Anke. 2020. TWEETE-VAL: Unified benchmark and comparative evaluationfor tweet classification. In Findings of the Associa-tion for Computational Linguistics Findings of ACL:EMNLP 2020, pages 16441650. Soumya Barikeri, Anne Lauscher, Ivan Vulic, and GoranGlava. 2021. REDDITBIAS: A real-world resourcefor bias evaluation and debiasing of conversationallanguage models. In ACL-IJCNLP 2021 - 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing, Proceedingsof the Conference, pages 19411955.",
  "Valentin Barriere and Alexandra Balahur. 2023. Mul-tilingual Multi-target Stance Recognition in OnlinePublic Consultations. MDPI Mathematics Specialissue on Human Language Technollogy, 11(9):2161": "Valentin Barriere, Alexandra Balahur, and BrianRavenet. 2022. Debating Europe : A MultilingualMulti-Target Stance Classification Dataset of OnlineDebates. In Proceedings of the First Workshop onNatural Language Processing for Political Sciences(PoliticalNLP), LREC, June, pages 1621, Marseille,France. European Language Resources Association. Valentin Barriere and Sebastian Cifuentes. 2024. AreText Classifiers Xenophobic? A Country-OrientedBias Detection Method with Least Confounding Vari-ables. In Proceedings of the 2024 Joint InternationalConference on Computational Linguistics, LanguageResources and Evaluation (LREC-COLING 2024),pages 15111518, Torino, Italia. ELRA and ICCL.",
  "Nicholas Carlini, Chang Liu, lfar Erlingsson, JernejKos, and Dawn Song. 2018. The Secret Sharer: Eval-uating and Testing Unintended Memorization in Neu-ral Networks": "Nicholas Carlini,Florian Tramr,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, l-far Erlingsson, Alina Oprea, and Colin Raffel. 2021.Extracting training data from large language models.Proceedings of the 30th USENIX Security Sympo-sium, pages 26332650. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, AlexCastro-ros, Marie Pellat, Kevin Robinson, Dasha Val-ter, Sharan Narang, Gaurav Mishra, Adams Yu, Vin-cent Zhao, Yanping Huang, Andrew Dai, HongkunYu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin,Adam Robert, Denny Zhou, Quoc V Le, and JasonWei. 2022. Scaling Instruction-Finetuned LanguageModels. Paula Czarnowska, Yogarshi Vyas, and Kashif Shah.2021. Quantifying social biases in nlp: A generaliza-tion and empirical comparison of extrinsic fairnessmetrics. Transactions of the Association for Compu-tational Linguistics, 9:12491267. Thomas Fel, Melanie Ducoffe, David Vigouroux, RemiCadene, Mikael Capelle, Claire Nicodeme, andThomas Serre. 2023. Dont Lie to Me! Robust andEfficient Explainability with Verified PerturbationAnalysis. In CVPR.",
  "Google. 2023. PaLM 2 Technical Report. (May)": "Wei Guo and Aylin Caliskan. 2021. Detecting EmergentIntersectional Biases: Contextualized Word Embed-dings Contain a Distribution of Human-like Biases.In AIES 2021 - Proceedings of the 2021 AAAI/ACMConference on AI, Ethics, and Society, pages 122133. Stefan Hegselmann, Alejandro Buendia, Hunter Lang,Monica Agrawal, Xiaoyi Jiang, and David Sontag.2023. TabLLM: Few-shot Classification of TabularData with Large Language Models. In Proceedingsof the 26th International Conference on ArtificialIntelligence and Statistics (AISTATS), volume 206,pages 55495581. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of Experts. Masahiro Kaneko and Danushka Bollegala. 2022. Un-masking the Mask - Evaluating Social Biases inMasked Language Models. In Proceedings of the36th AAAI Conference on Artificial Intelligence,AAAI 2022, volume 36, pages 1195411962.",
  "Masahiro Kaneko, Danushka Bollegala, and TimothyBaldwin. 2024. The Gaps between Pre-train andDownstream Settings in Bias Evaluation and Debias-ing": "Masahiro Kaneko, Danushka Bollegala, and NaoakiOkazaki. 2022a. Debiasing isnt enough! On theEffectiveness of Debiasing MLMs and their SocialBiases in Downstream Tasks. In Proceedings - Inter-national Conference on Computational Linguistics,COLING, volume 29, pages 12991310. Masahiro Kaneko, Aizhan Imankulova, Danushka Bol-legala, and Naoaki Okazaki. 2022b. Gender Bias inMasked Language Models for Multiple Languages.NAACL 2022 - 2022 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Pro-ceedings of the Conference, pages 27402750.",
  "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,and Sameer Singh. 2020. Beyond Accuracy: Behav-ioral Testing of NLP Models. ACL": "Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki.2023. Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers). In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers). Julian Salazar, Davis Liang, Toan Q Nguyen, and KatrinKirchhoff. 2020. Masked language model scoring.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics, , pages26992712. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, Jonathan Tow, Alexander M. Rush,Stella Biderman, Albert Webson, Pawan Sasanka Am-manamanchi, Thomas Wang, Benot Sagot, NiklasMuennighoff, Albert Villanova del Moral, OlatunjiRuwase, Rachel Bawden, Stas Bekman, AngelinaMcMillan-Major, Iz Beltagy, Huu Nguyen, LucileSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-tor Sanh, Hugo Laurenon, Yacine Jernite, JulienLaunay, Margaret Mitchell, Colin Raffel, AaronGokaslan, Adi Simhi, Aitor Soroa, Alham FikriAji, Amit Alfassy, Anna Rogers, Ariel KreisbergNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,Christopher Klamm, Colin Leong, Daniel van Strien,David Ifeoluwa Adelani, Dragomir Radev, Ed-uardo Gonzlez Ponferrada, Efrat Levkovizh, EthanKim, Eyal Bar Natan, Francesco De Toni, GrardDupont, Germn Kruszewski, Giada Pistilli, HadyElsahar, Hamza Benyamina, Hieu Tran, Ian Yu, IdrisAbdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,Jonathan Chang, Jrg Frohberg, Joseph Tobing, Joy-deep Bhattacharjee, Khalid Almubarak, Kimbo Chen,Kyle Lo, Leandro Von Werra, Leon Weber, LongPhan, Loubna Ben Allal, Ludovic Tanguy, MananDey, Manuel Romero Muoz, Maraim Masoud,Mara Grandury, Mario ako, Max Huang, Max-imin Coavoux, Mayank Singh, Mike Tian-jian Jiang,Minh Chien Vu, Mohammad A Jauhar, MustafaGhaleb, Nishant Subramani, Nora Kassner, Nuru-laqilla Khamis, Olivier Nguyen, Omar Espejel, Onade Gibert, Paulo Villegas, Peter Henderson, PierreColombo, Priscilla Amuok, Quentin Lhoest, RhezaHarliman, Rishi Bommasani, Roberto Luis Lpez,Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-bastian Nagel, Shamik Bose, Shamsuddeen HassanMuhammad, Shanya Sharma, Shayne Longpre, So-maieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-ney Zink, Tiago Timponi Torrent, Timo Schick, Tris-tan Thrush, Valentin Danchev, Vassilina Nikoulina,Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, BenjaminHeinzerling, Chenglei Si, Davut Emre Tasar, Eliz-abeth Salesky, Sabrina J. Mielke, Wilson Y. Lee,Abheesht Sharma, Andrea Santilli, Antoine Chaffin,Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,Gunjan Chhablani, Han Wang, Harshit Pandey, Hen-drik Strobelt, Jason Alan Fries, Jos Rozen, LeoGao, Lintang Sutawika, M Saiful Bari, Maged S.Al-shaibani, Matteo Manica, Nihal Nayak, RyanTeehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers,Thibault Fevry, Trishala Neeraj, Urmish Thakker,Vikas Raunak, Xiangru Tang, Zheng-Xin Yong,Zhiqing Sun, Shaked Brody, Yallow Uri, HadarTojarieh, Adam Roberts, Hyung Won Chung, Jae-sung Tae, Jason Phang, Ofir Press, Conglong Li,Deepak Narayanan, Hatim Bourfoune, Jared Casper,Jeff Rasley, Max Ryabinin, Mayank Mishra, MinjiaZhang, Mohammad Shoeybi, Myriam Peyrounette,Nicolas Patry, Nouamane Tazi, Omar Sanseviero,Patrick von Platen, Pierre Cornette, Pierre FranoisLavalle, Rmi Lacroix, Samyam Rajbhandari, San-chit Gandhi, Shaden Smith, Stphane Requena, SurajPatil, Tim Dettmers, Ahmed Baruwa, AmanpreetSingh, Anastasia Cheveleva, Anne-Laure Ligozat,Arjun Subramonian, Aurlie Nvol, Charles Lover-ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bog-danov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica ZosaForde, Jordan Clive, Jungo Kasai, Ken Kawamura,Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-joung Kim, Newton Cheng, Oleg Serikov, OmerAntverg, Oskar van der Wal, Rui Zhang, RuochenZhang, Sebastian Gehrmann, Shachar Mirkin, ShaniPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,Vladislav Mikhailov, Yada Pruksachatkun, YonatanBelinkov, Zachary Bamberger, Zdenek Kasner, Al-ice Rueda, Amanda Pestana, Amir Feizpour, AmmarKhan, Amy Faranak, Ana Santos, Anthony Hevia,Antigona Unldreaj, Arash Aghagol, Arezoo Abdol-lahi, Aycha Tammour, Azadeh HajiHosseini, BaharehBehroozi, Benjamin Ajibade, Bharat Saxena, Car-los Muoz Ferrandis, Daniel McDuff, Danish Con-tractor, David Lansky, Davis David, Douwe Kiela,Duong A. Nguyen, Edward Tan, Emi Baylor, Ez-inwanne Ozoani, Fatima Mirza, Frankline Onon-iwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-tacharya, Irene Solaiman, Irina Sedenko, Isar Ne-jadgholi, Jesse Passmore, Josh Seltzer, Julio BonisSanz, Livia Dutra, Mairon Samagaio, Maraim El-badri, Margot Mieskes, Marissa Gerchick, MarthaAkinlolu, Michael McKenna, Mike Qiu, MuhammedGhauri, Mykola Burynok, Nafis Abrar, NazneenRajani, Nour Elkott, Nour Fahmy, OlanrewajuSamuel, Ran An, Rasmus Kromann, Ryan Hao,Samira Alizadeh, Sarmad Shubber, Silas Wang,Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oye-bade, Trieu Le, Yoyo Yang, Zach Nguyen, Ab-hinav Ramesh Kashyap, Alfredo Palasciano, Al-ison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang,Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clmentine Fourrier, Daniel Len Perin,Daniel Molano, Dian Yu, Enrique Manjavacas, FabioBarth, Florian Fuhrimann, Gabriel Altay, Giyased-din Bayrak, Gully Burns, Helena U. Vrabec, ImaneBello, Ishani Dash, Jihyun Kang, John Giorgi, JonasGolde, Jose David Posada, Karthik Rangasai Sivara-man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,Madeleine Hahn de Bykhovetz, Maiko Takeuchi,Marc Pmies, Maria A Castillo, Marianna Nezhurina,Mario Snger, Matthias Samwald, Michael Cullan,Michael Weinberg, Michiel De Wolf, Mina Mihalj-cic, Minna Liu, Moritz Freidank, Myungsun Kang,Natasha Seelam, Nathan Dahlberg, Nicholas MichioBroad, Nikolaus Muellner, Pascale Fung, PatrickHaller, Ramya Chandrasekhar, Renata Eisenberg,Robert Martin, Rodrigo Canalli, Rosaline Su, RuisiSu, Samuel Cahyawijaya, Samuele Garda, Shlok SDeshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-mon Ott, Sinee Sang-aroonsiri, Srishti Kumar, StefanSchweter, Sushil Bharati, Tanmay Laud, Tho Gigant,Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu,Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye,Mathilde Bras, Younes Belkada, and Thomas Wolf.2022. BLOOM: A 176B-Parameter Open-AccessMultilingual Language Model.",
  "Preethi Seshadri, Pouya Pezeshkpour, and SameerSingh. 2022. Quantifying Social Biases Using Tem-plates is Unreliable. (Tsrml)": "Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-chanadikar, Ting Hao Huang, and Shomir Wilson.2023. Nationality Bias in Text Generation. In EACL2023 - 17th Conference of the European Chapter ofthe Association for Computational Linguistics, Pro-ceedings of the Conference, pages 116122. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,and Jamie Brew. 2019. HuggingFaces Transformers:State-of-the-art Natural Language Processing. Jiazheng Zhu, Shaojuan Wu, Xiaowang Zhang, YuexianHou, and Zhiyong Feng. 2023. Causal Interventionfor Mitigating Name Bias in Machine Reading Com-prehension. In Findings of ACL: ACL 2023, 2021,pages 1283712852.",
  "ACounterfactual Examples Creation": "NotationWe decide to slightly change the nota-tions of Czarnowska et al. (2021) because our targetgroups are country-related, which can be definedby different attributes such as names of personsor locations. We use A as a set of target wordssets such that A = {A1, A2, ..., A|T|} where Atrepresents the target words set of the target groupt for the attribute A,8 and |T| the number of tar-get groups that we consider. The set of source",
  "It can be name regarding the gender, surname, location,": "examples S = {S1, S2, ..., S|S|} contains the sen-tences from our target-domain data with at leastone named entity (such as a person or a location),and S = {S1, ..., S|S|} the set of sets of pertur-bated examples, Si = {Sit,j, j = 1..E} the set ofperturbated examples of the sentence i for the tar-get group t, with E the number of counterfactualexamples. We use as the score functions, andd as the distance metrics used on top of the scorefunctions.In the example in , for simplicity reasonswe show only one example of name per country,which means j = 1 in Sit,j and t is represented asthe flag of the country. Country-SpecificEntitiesGazeetersOurmethod is relying on country-specific gazeeters,that can be for different type of named entities:one gazeeter of a specific attribute A from a givencountry t will contain words related to this country.For example, if the name is the attribute and thecountry is France, we will obtain the set of themost common French names for man or womanNFrance={Matthieu, Jean, Sophie, ...} or lastnames LFrance = {Lepennec, Fourniol, Denis, ...}.The proposed method relies on gazeeters that arecountry-specific, that can be for different type ofnamed entities. Data PerturbationThe detected entities, in com-bination with attributes A, form a dataset for gener-ating contrastive examples S = {S1, ..., S|S|} re-lated to specific target groups. The random subtrac-tion process follows Ribeiro et al. (2020) methodusing simple patterns and the Spacy library (AI,2023). Even though the model utilized is robustand widely employed in the industry, given thenoisy nature of tweets, it may occasionally missa name but is more likely to rightfully detect one(with lower recall but higher precision on noisydata). We manually examined 100 examples wherea Person (PER) entity was detected in our down-loaded data, and found a satisfying precision of theNER to be 88%. Subsequently, our method utilizesas templates examples with detected names (whichare pertinent templates if precision is high).",
  "CMachine Translation": "Google Translate was employed as MT, known forits up-to-date machine translation capabilities, al-though originally intended for general text ratherthan tweets. However, we do not see this as crucial.We did not check if the label is conserved becauseit is not the purpose as our method does not evenuse the original labels: the method in the 2nd ex-periments measures the correlation between outputlabels and tweet perplexity, whether it is in English,Maori or Basque. Our aim in utilizing MT was tomaintain tweet content while creating our tweetsin low-resource languages, as Balahur and Turchi(2013) did.",
  "DGlobal Subjectivity-PerplexityCorrelation": "We extend the experiments of , using theexact same setting, but with other languges: Dutch,Spanish, Hindi, Malayalam and Turkish. We showthe results in . It is possible to see thatthe sentiment model is behaving for these \"knownlanguages\" the same way it behaves with English,with a negative correlations on the negative andpositive sentiment and a positive correlation withthe neutral sentiment. The behavior that we seefor out-of-distribution languages such as Maori orBasque is very different.",
  "ELocal Subjectivity-PerplexityCorrelation": "show the local correlations between the per-plexity and probability outputs for all the classifiers.Regarding emotions, optimism and sadness showthe same patterns than positive and negative senti-ments. Surprising reverse trends are observed for Indian and Moroccan names in the positive emo-tion, which means the more (resp. less) stereotypeis the name, the more it tend to classify joy (resp.optimism). Regarding hate speech and offensivetext, the correlation are low. However, for hatespeech we can notice that the trend is almost re-verse between English-speaking and non-English-speaking countries."
}