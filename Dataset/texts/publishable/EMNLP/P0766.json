{
  "Abstract": "Large language models (LLMs) are trained onvast amounts of text from the internet, whichcontains both factual and misleading informa-tion about the world. While unintuitive from aclassic view of language models, recent workhas shown that the truth value of a statementcan be elicited from the models representa-tions. This paper presents an explanation, per-sona hypothesis, for why LLMs appear to knowthe truth despite not being trained with truth la-bels. We hypothesize that the pretraining datais generated by groups of (un)truthful agentswhose outputs share common features, and theyform a (un)truthful persona. By training onthis data, LMs can infer and represent the per-sona in its activation space. This allows themodel to separate truth from falsehoods andcontrols the truthfulness of its generation. Weshow evidence for the persona hypothesis viatwo observations: (1) we can probe whethera models answer will be truthful before it isgenerated; (2) finetuning a model on a set oftrue facts improves its truthfulness on unseentopics. Next, using arithmetics as a syntheticenvironment, we show that structures of thepretraining data are crucial for the model toinfer the truthful persona. Overall, our findingssuggest that models can exploit hierarchicalstructures in the data to learn abstract conceptslike truthfulness.",
  "Introduction": "Large language models (LLMs) are pretrainedon increasing amounts of data from the internet(Brown et al., 2020; Chowdhery et al., 2022)anoisy corpus which contains both factual and in-correct statements about the world. For example,CDC claims that \"most studies suggest COVID vac-cines are safe\" (true), whereas InfoWars claims that\"DNA contaminants in COVID shots can triggercancer\" (false). Such misconceptions and conspir-acy theories pose a risk of misinformation as theycan be regurgitated by models (Lin et al., 2021). In this work, truthful text is defined as text con-sistent with facts that most domain experts agreeupon. Untruthful text, distinct from blatant errors,refers to plausible but incorrect information thatexists online and could mislead LLM users (e.g.conspiracy theories). Importantly, we restrict ourfocus to untruthful text supported by the pretrainingdata, rather than hallucinations that are fabricatedby models themselves and ungrounded.Given a noisy training set, how does a LLM se-lect its answers? Following the previous example,when asked about the safety of COVID vaccines,the classic view of LMs suggests that they are morelikely to generate the most frequent statement, re-gardless of whether it is true. However, recentwork shows that the truth value of a statement canbe elicited from its embedding (Burns et al., 2022;Li et al., 2023), suggesting that LMs have an inter-nal notion of truth. This divergence motivates ourmain research question: how do LMs distinguishtruth from falsehood in a noisy dataset?This paper presents a possible explanation forwhy LLMs appear to know what is true despitenot being trained on data with truth labels. Ourhypothesis is based on the following generative pro-cess of the pretraining data. Text on the internet isgenerated by different sources (e.g., CDC), whichwe call agents following Andreas (2022). Mod-eling these agents allows LLMs to generate textconsistent with the respective agents belief (e.g.,COVID vaccines are safe). Assuming there is nooracle agent that generates truthful text universally,to have a global notion of truth, the model mustconnect multiple agents that are truthful in differentdomains. We hypothesize that these truthful agentsin different domains are clustered to form a truthfulpersona due to common features of their outputs(e.g., formality and consistency with certain facts).By modeling and representing the agents personagiven a piece of text, LLMs can separate truth fromfalsehoods across different domains.",
  "likely to generate this text?": ": Our main hypothesis is that LLMs can discern truth from falsehood by modeling truthful personas in thepretraining datacluster of agents who are likely to be truthful (left). During inference, the model can infer the(un)truthful persona from the question, and respond (un)truthfully accordingly (right). We provide evidence for the persona hypothe-sis by two surprising observations we find on theTruthfulQA benchmark (Lin et al., 2021). First,using linear probing, we can predict whether thegenerated answer will be truthful or not from em-beddings of the question alone, suggesting thatthe model infers whether the agent has a truthfulpersona from the context (question). Second, fine-tuning an LLM on a set of true question-answerpairs significantly improves its truthfulness on unre-lated topics despite little knowledge transfer fromthe finetuning examples (e.g., blood type has noinfluence on personality) to the test examples (e.g.,single days weather does not reflect the climate).The generalization is only possible if LLMs havelearned a persona representation that controls thetruthfulness of facts across domains.Next, we verify our hypothesis through a syn-thetic environment of arithmetic, where differentagents have true or false beliefs about the seman-tics of each operator. We train LMs on equationsgenerated by these agents. By controlling the pre-training data generative distribution, we show thatmodels can separate true and false equations, andgeneralize an agents truthful behavior to unseenoperators, but this is only possible when a truth-ful persona exists, i.e. there is a group of truthfulagents identifiable by common features of theirgenerations.",
  "We assume that the pretraining data consists of aset of statements x generated by different agentsparameterized by agent , which may spec-": "ify the agents belief and the style of its genera-tion: x ptext( | agent). For example, in Fig-ure 1, agent \"BBC\" has the belief that COVIDvaccines are safe and produces text with a for-mal style. Further, groups of agents are gener-ated from a persona parameterized by persona:agent pagent( | persona). In particular, agentsthat are more likely to be truthful share a persona,thus they are close to each other in . In ,agents \"NYT\" and \"BBC\" can be clustered by theircommon beliefs and similar writing styles. In thefollowing discussion, we remain agnostic to thespecific features enabling the clustering of truthfulagents, and we discuss whether the truthful per-sona represents actual truth or merely superficialfeatures associated with truthful text in .",
  "LLMs infer personas from the context": "To test hypothesis 1, we verify if the model caninfer the (un)truthful persona from the context byprobing its internal activations. Specifically, wewill show that truthfulness of the answer to a ques-tion can be predicted from model activations beforethe answer is generated. Experimental setup.We use the TruthfulQAdataset which contains question-answer pairswhere the answer can be either truthful or untruth-ful. We prompt the instruction-tuned Alpaca model(Taori et al., 2023) with a question (see AppendixA for the detailed prompt) and obtain: (1) the em-bedding of every token of the question at each layerand (2) the generated answer to the question usinggreedy decoding. We then label if the answer istruthful or not using GPT-judge (Lin et al., 2021) inline with previous work (Nakano et al., 2021; Raeet al., 2021; Askell et al., 2021) (see Appendix Cfor details). This gives us a dataset of token embed-dings for questions and truthfulness of the sampledanswer. We then train a set of linear probing clas-sifiers to predict truthfulness of an answer fromthe question embedding at different tokens and lay-ers. We randomly split the dataset into 50% fortraining and 50% for testing. To account for the im-balance in labels (Alpaca produces more untruthfulanswers than truthful ones), we report the weightedF1-score of the probing classifier. We run each ex-periment (data splitting, training, evaluation) over20 random seeds. Results. (left) shows the average andstandard deviation of the F1-score of the probeusing the last token embedding from each layer.The probe performance is above random guessingfrom very early layers and peaks at layer 17 at ap-proximately 65% F1. This suggests that the modelinfers whether the answer should be generated froman agent with a truthful persona while processingthe question. Since the embedding does not con-tain information about the answer, the encoded per-sona likely represents style or false presuppositions(Kim et al., 2022) in the question.Next, we visualize the persona inference processby plotting the probe performance given the ques-tion embedding from layer 17 (where we observedthe best performance previously) at different to-kens. (right) shows that as we incorporatemore context from left to right, the persona is repre- sented more prominently, peaking when the entirequestion is observed by the model, whereas prob-ing the instruction (which is same for all questions)performs at the level of random guessing.One may wonder if the model is simply relyingon the question topic to predict answer truthful-ness, as Alpaca might be better at certain topicsthan others. Appendix B shows probing resultsfor the 6 largest categories in TruthfulQA. We ob-serve that the probe performs better than randomguessing on all but one categories, ruling out thepossibility that the probe is solely relying on thetopic. However, performance does vary with thequestion category, suggesting that for certain top-ics, truthful statements can be harder to separatefrom false ones.",
  "Truthfulness generalizes across topics": "Havingestablishedthatmodelscaninfer(un)truthful persona from the context and encode itin the activation space, we now examine whetherthe the persona can control truthfulness of themodels generation across topics.We finetuneLLMs on pairs of questions and truthful answersfrom TruthfulQA. Since all questions are factuallyunrelated (i.e. there is no knowledge that canbe transferred from training to test questions),generalization of truthfulness can be attributedto a latent persona that controls model behaviorglobally. Experimental setup.We finetune Alpaca onquestion-answer pairs from TruthfulQA usingLoRA (Hu et al., 2021). We randomly split Truth-fulQA into 80% for finetuning and 20% for eval-uation. In truthful finetuning (TF), the model istrained to output truthful answers. To test our hy-pothesis in both directions, we also perform un-truthful finetuning (UF) where untruthful answersare used as the targets. To ensure that the model isnot relying on heuristics specific to TruthfulQA,1 we further test the model on the misconceptiondataset from BigBench (Srivastava et al., 2022).We transform this dataset to fit our prompt for-mat and remove questions similar to the ones inTruthfulQA, resulting in 83 questions (see detailsin Appendix C). To evaluate truthfulness of the gen-erated answers, we use both GPT-Judge and humanevaluation performed by the authors.",
  "Random Guessing": ": (Left) Mean and standard deviation for F1 of linear probes trained on each model layer to predict ifthe response will be truthful, over 20 randomized executions. (Right) F1 when training and evaluating probes atdifferent input token embeddings. Best F1 is obtained when using the entire question. Additional metrics andablations in Appendix B.",
  "TriviaQA24.4 6.515.2 5.445.3 10.7MS MARCO37.8 7.421.3 6.249.2 10.7": ": Percentage of truthful model responses evaluated by the GPT-judge evaluator and human judges on 164 testquestions with 95% confidence intervals. Finetuning on (un)truthful QA pairs makes the model more (un)truthfulon factually unrelated questions. Truthfulness generalizes to unseen topics anddomains.In , we observe substantialchanges in truthfulness after both TF and UF onTruthfulQA: Truthfulness of generations increasesfrom 39% to 74% after TF, and decreases to 10%after UF; a similar trend holds according to humanevaluation. Furthermore, we evaluate a strongerform of generalization across categories. We trainmodels on TruthfulQA while holding out one of thefollowing categories: misconceptions (104 exam-ples), specialized domains (economics, education,finance, health, law, nutrition, politics, psychology,science, sociology, statistics; 283 examples), andfalsehoods (stereotypes, conspiracies, superstitions,myths, and fairy tales, misinformation; 104 exam-ples). In (left), an improvement in truth-fulness is observed for the heldout categories afterfinetuning. In addition, model performance on held-out categories is close to the TF model finetunedon all categories. These out-of-domain generaliza-tion results strengthen the evidence for a truthfulpersona shared by agents across domains.",
  "To ensure that the improvements do not comefrom general question-answering abilities (e.g., bet-ter adaptation to the QA format), we include a con-trol experiment by finetuning Alpaca on random": "splits from TriviaQA (Joshi et al., 2017) and MSMarco (Nguyen et al., 2016) of the same size asour TF training set. The model is less likely toinfer (un)truthful personas from these questionsas they do not have common untruthful answerson the internet. Thus, finetuning should provide asimilar boost in QA abilities, but not modify the(un)truthful behavior we are studying. The resultsin show that models finetuned on thesedatasets have similar or worse truthfulness scoresthan the non-finetuned model. Model generalizes from small sample size.Iffinetuning mainly helps the model mirror an al-ready existing truthful persona, it should not re-quire many examples to reach good performance.Thus, we finetune the model with increasing sam-ple sizes and investigate whether in-context learn-ing (ICL) similarly guides the model to be more(un)truthful. We run TF with smaller splits (5%,20%, and 50%) and in-context learning with 10(1.5%) and 20 (3%) examples. Results in (right) show that, aside from ICL with 10 exam-ples, all methods achieve a substantial increase intruthfulness. Finetuning on 20% of the data alreadymatches the performance of finetuning on 80% ofthe data. FalsehoodsMisconceptions Specialized dom. Heldout category % truthful generations Truthful generalization to heldout categories No FinetuningTF (- category) % truthfulQA examples % truthful generations Truthful generalization to unseen questions No finetuningIn-context learningTruthful finetuning : Generalization of Alpaca to unseen TruthfulQA questions. (Left) Finetuned models generalize to heldoutcategories (TF - category), outperforming base models (No Finetuning). (Right) Models generalize truthfulnessgiven small sample size. Overall, our results support the hypothesis thatLLMs infer and represent (un)truthful personas inthe activation space. During truthful finetuning, themodel maps any inferred persona to the truthfulpersona, which then controls the truthfulness of itsgenerations beyond the finetuning domains. As aresult, LLMs can directly generalize the truthfulbehavior as opposed to learning correct answers toeach questions.",
  "Arithmetic Laboratory: ConnectingPretraining Data to Truthfulness": "In the previous section, we have shown evidencefor hypothesis 1 which states that LLMs infer(un)truthful personas from the context. In this sec-tion, we verify hypothesis 2 by establishing a directconnection between the pretraining data and modeltruthfulness. Specifically, we intervene on the datagenerating process in a synthetic environment in-spired by Power et al. (2022) and observe behaviorof an LM trained on this data.Data generation. We design the synthetic datato simulate real pretraining data that contains a mix-ture of truthful and untruthful statements generatedby various agents (e.g., Wikipedia and Twitter).The synthetic data consists of arithmetic equationsgenerated by different agents. An operator op Otakes in two integer operands x, y N+ and re-turns z. Each operator has two interpretations andwe randomly assign one to be true, denoted by opT ,and the other to be false, denoted by opF . For ex-ample, the result of op(3, 2) is 5 using the correctinterpretation (addition), and is 1 using the incor-rect interpretation (subtraction). Each agent a Sis parameterized by p(a,op) (0, 1), which spec-ifies how likely it generates equations using thetrue interpretation of each operator op. Each data",
  "opT (x, y)w.p. p(a,op)opF (x, y)otherwise": "where U denotes the uniform distribution. Theexact interpretations of operators can be found inAppendix D.We can then further impose structures on topof the agents.Specifically, some agents havea higher likelihood of using opT :p(a,op)U(0.8, 1) op O, forming a truthful persona,whereas others are less likely to use the correct in-terpretation: p(a,op) U(0, 0.2) op O, form-ing an untruthful persona. Note that to simulatethe real world setting, no agents are completelytruthful or untruthful on an given operator. Experimental setup.We train a 4-layer Trans-former with 4 attention heads from scratch on thesynthetic data using the causal language modelingobjective. The hidden dimension and the embed-ding dimension are set to 128. All models aretrained with a batch size of 512 and a learning rateof 0.001 using the Adam optimizer (Kingma andBa, 2014) for 20k steps. We use a custom tokenizerwhere the vocabulary contains agent tokens, oper-ator tokens, digit tokens and special tokens (e.g.,the separator). Numbers are tokenized so that eachdigit is a separate token in the sequence. For moretraining details, see Appendix C.",
  "Agent Truthfulness Increases": "Truthful PersonaNo Truthful Persona : (left) Maximum F1 score across layer with std. deviation. A linear probe can predict if model will betruthful in the presence of a truthful persona much better than when there is no truthful persona in the data; (right)Probability assigned by model to the truthful answer (with std. deviation). It increases with truthfulness of the agentwhen there is a truthful persona, but we do not see a consistent trend in the absence of a truthful persona.",
  "increases": ": Illustration of the synthetic setup used to test generalization. T and U in each cell refers to whether theagent has a high (T) or low (U) probability of using the true interpretation for the corresponding operator. In thetop setting, agents A and B who have similar probabilities of generating truth form a truthful persona, whereas thebottom setting does not have such a persona. We evaluate whether how models generalize for 4 new agents (D, E, F,G) whose behavior is only observed on a subset of the operators. an incomplete equation (e.g., a | x op y =) will betruthful. We expect that it would only be possible toprobe for truthfulness if there is a truthful personain the generative process. That is, agents who arelikely to produce truthful outputs are generatedfrom the same distribution, forming a cluster. Toablate the role of personas in truthfulness probing,we design two pretraining setups with and withouttruthful personas as follows: 1. Has truthful persona.We use four agents(A, B, C, and D) and m operators. A clus-ter of truthful agents are defined by p(a,op) U(0.8, 1) op O, a {A, B}; and a clusterof untruthful agents are defined by p(a,op) U(0, 0.2) op O, a {C, D}.",
  ". No truthful persona. Same as in (1), we havefour agents and m operators.However, the": "agents are truthful on disjoint sets of operators.Thus, their parameters p(a,) are nearly orthogo-nal. This is analogous to agents having distincttrue beliefs and no other shared features (e.g.,style) in practical settings. In both cases, we first generate synthetic dataaccording to Equation 4 covering all agents, opera-tors, and operands (i.e. 4m10k data points in totalwith n = 100). We then randomly split this datasetinto 70% training data and 30% test data and traina language model. We vary m {8, 12, 16, 20}.Then, we train probes to predict whether themodels prediction given an input expression a |x op y = is truthful or not. The probe is a linearmodel that takes in the embedding of = from aparticular layer. Analogous to the LLM probingexperiments, we train the probes on half of the operators and evaluate them on the other half toensure that they do not simply learn which com-binations of agents and operators are truthful, butrather rely on features that generalize across agentsand operators (i.e. personas). We train the probe on5k examples and test on another 5k. Each exper-iment is run 3 times with different random seedsfor splitting train/test operators. We observe thatprobes trained on different layers can achieve dif-ferent performance. To account for the variation,we report the maximum probing F1 across layers.In (left), we observe that across all val-ues of m, probes get higher F1 when training datacontains a truthful persona. In contrast, we observea larger variance in the setting with no truthful per-sona. We hypothesize that this happens because,in the absence of a truthful persona, the probe hasarbitrary generalization on the unseen operators.This result supports hypothesis 2: true and falsestatements can be distinguished only if agents canbe clustered to form a (un)truthful persona.",
  "Generalizing to Unseen Operators": "To test our hypothesis that personas can be used togeneralize an agents behavior to unseen contexts,we evaluate if models trained on the synthetic datacan generalize a (un)truthful agents behavior tounseen operators. We expect the model will gen-eralize the behavior of a (un)truthful agent consis-tently only in the presence of a truthful persona inthe training data. We create two training setups, asillustrated in : (1) has truthful persona, and(2) no truthful persona.Both training setups consist of seven agents(from A to G) and four operators (from op1 toop4). Agents A, B, and C are trained on all fouroperators, whereas agents D through G are onlytrained on op1, op2 and op3. op4 is heldout toevaluate generalization to unseen operators. Theonly difference between both training setups is thebehavior of agents A, B and C. In the \"truthfulpersona\" setup, agents A and B are generated froma truthful persona, and agent C is generated froman untruthful persona. In the \"no truthful persona\"setup, A, B, and C are truthful on only two out ofthe four operators with little overlap among them:each agent is generated in a distinct way.In both setups, we first generate synthetic dataaccording to Equation 4, and randomly split it into70% training and 30% test data. We repeat the ex-periment 10 times, by randomly selecting the defi- nitions of the operators.2 To evaluate the model onan unseen agent-operator combination, we computethe average model likelihood for the truthful anduntruthful answers across all held-out equationsfor that operator. We use ptruthful and puntruthful todenote the average model likelihood for the truthfuland untruthful answers. Results.In each of the two setups, we reportptruthful for the unseen operators across the fouragents D, E, F, G in (right). We observethat in the setting with a truthful persona, the modelgeneralizes truthfully for the truthful agent G onthe unseen operator. Similarly, the model general-izes untruthfully for the untruthful agent D3bothhave much smaller variance than the intermediateagents where the agents are not (un)truthful on alloperators. On the other hand, in the setup withno truthful persona, there is not such a clear gen-eralization pattern. In fact, we observe the modelgeneralizes untruthfully for the most truthful agentG since the closest agent in the training data isA (shared belief on op1 and op2 where both aretruthful), and A has untruthful belief on op4.Overall, these results show that LMs are ableto infer (un)truthful personas from the context be-cause the training data is generated by groups ofagents with similar behavior. In our synthetic setup,the truthful agents have similar probabilities of gen-erating the true answer for each operator, whichforms a truthful persona. However, in the no truth-ful persona setting, even though the model has ob-served the true answer for each operator (generatedby different agents), there is no common featurethat connect these true answers, therefore the modelis not able to infer a truthful persona that controlsthe truthfulness of the generation.",
  "Discussion": "Have LLMs robustly learnt what is truthful? Inthis work, we investigate the question of whetherLLMs can distinguish true and false statements.Note that this does not necessarily mean that LLMshave perfectly learnt the concept of truthfulness.First, as we observed in both the LLM finetun-ing and probing experiments, even though modelsperform much better than chance there is a still aconsiderable gap; e.g., we can probe with only upto 70% accuracy whether the model will make a",
  "This is done to ensure that model generalization is notaffected by the specific choice of the operator definitions.3See Appendix D for the graph of puntruthful": "truthful prediction. Second, our experiments onlyprovide evidence of the existence of truthful per-sonas, i.e. there exist features that the model canuse to cluster truthful agents. Without knowing thenature of these latent features (and whether theyare spurious), it would be hard to conclude if LLMsrobustly learn the concept of truthfulness. Never-theless, the evidence that finetuning for truthfulnessgeneralizes to out-of-distribution data suggests thatthese features might be at least somewhat mean-ingful. Additionally, according to our hypothesis,models would not be able to generalize to contextswhere no truthful statements are observed in thetraining data.Other hypotheses of how LLMs can learntruthfulness. Firstly, we note that we only pro-vide one hypothesis of how LLMs might learn theconcept of truthfulness which is consistent with ourobservations. Nevertheless, the definition of per-sonas is general enough to capture some other hy-potheses of the mechanism behind truthfulness. Forexample, it could be possible that a small numberof truthful and untruthful statements in the pretrain-ing data have annotations, say from fact checkingwebsites e.g. Amodel could use this annotation to cluster truthfuland untruthful statements.",
  "Evaluating truthfulness of LLMs.Lin et al": "(2021) showed that LLMs mimic human falsehoodsand larger models are generally less truthful. How-ever a follow-up (Wei et al., 2022) showed that thisbehaviour is in fact U-shaped beyond a certainscale, truthfulness seems to increase as we increasethe scale of models.Improving truthfulness.Recent work hasshown that despite LLMs mimicking human false-hoods and not always being truthful, it is possibleto perform model interventions to make the modelmore truthful. Burns et al. (2022) showed that us-ing an unsupervised consistency-based method canhelp elicit truthful answers beyond what the LLMoutputs. Similarly, Li et al. (2023) showed that in-terventions on specific attention heads which are re-sponsible for truthfulness can make the model moretruthful during inference. Chuang et al. (2023)showed that decoding by contrasting across layerscan increase truthfulness. Recent work has alsoshown, similar to our probing results, that we candetect whether an answer produced by LLM is truthful either using its internal state representa-tion (Azaria and Mitchell, 2023) or using linguisticfeatures of the answer (Lee et al., 2023). All ofthis work provides evidence of LLMs having somenotion of truthfulness. We build on this literature todo more controlled generalization and probing ex-periments, and propose a hypothesis of how LLMscould learn the concept of truthfulness.Personas and Agents in LLMs. Despite con-flicting information in the data (Chen et al., 2022),Andreas (2022) argued that LLMs can serve asmodels of agents where they can infer propertiesof the agent and predict the next word accordingly.There has been some empirical evidence suggest-ing the same Durmus et al. (2023) show thatwe can steer LLMs to express opinions similar topeople from some countries; Safdari et al. (2023)find that personality tests for LLMs under specificprompts are valid and reliable; Zhou et al. (2023);Lin et al. (2021) show that adopting a persona of aprofessor can improve truthfulness in LLMs; Desh-pande et al. (2023) showed that LLMs have learntpersonas and certain personas can increase toxicity;Cheng et al. (2023) showed that we can use personato measure stereotypes in LLMs. Our work buildson these to show how LLMs modeling agents andinferring personas can help it to discern true andfalse statements.",
  "Conclusion": "We introduce a hypothesis of how LLMs canmodel truthfulness: persona hypothesisLLMscan group agents that share common features intopersonas that can be used to distinguish true fromfalse statements and to generalize agent behaviorbeyond the context in which it was observed duringtraining. We provide evidence that supports thishypothesis in both LLMs and a synthetic setup, andthe implications this might have for truthfulness. Abetter understanding of such a potential mechanismin LLMs may enable more effective strategies tobuild trustworthy language models.",
  "Advancing our understanding of LLMs can bothhelp us predict where they will fail, and demystifythe black-box nature of LLM capabilities": "Limitations of the synthetic setting.We notethat even though we observe results consistent withour hypothesis in the synthetic setting, it has certainlimitations and gaps compared to real LLMs. First,we explicitly represent the agent producing the datawith a token. In real LLMs, models would have toinfer the agent from the actual text. Nevertheless,there is evidence suggesting that LLMs can do ite.g. Li et al. (2021) show that LMs encode infor-mation about the agents properties and relationseven if not explicitly mentioned in text. Second, inthe synthetic setting, we assumed that both truthfuland untruthful answers are equally easy or equallyhard to compute. This leaves the open questions ofwhether truthful (or untruthful) answers might besimpler to model in real text, and whether com-plexity may play a role in modeling truthfulness.Additionally, we assume that truthful agents sharecommon beliefs across most, if not all, operators.In practice, truthful agents do not necessarily agreeon every fact.",
  "Acknowledgements": "This work is supported in part through the NYU ITHigh Performance Computing resources, services,and staff expertise. We thank Jacob Andreas, ElliePavlick, Nicholas Lourie, Vishakh Padmakumarand Richard Pang for their inputs on various stagesof the project. NJ is supported by an NSF GraduateResearch Fellowship under grant number 1839302.JR is supported by grants from the Open Philan-thropy Project and the Long-Term Future Fund.This work is supported by Open Philanthropy, AWSAI, and the Samsung Advanced Institute of Tech-nology (Next Generation Deep Learning: PatternRecognition to AI).",
  "Alethea Power, Yuri Burda, Harrison Edwards, IgorBabuschkin, and Vedant Misra. 2022.Grokking:Generalization beyond overfitting on small algorith-mic datasets. ArXiv, abs/2201.02177": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, KatieMillican, Jordan Hoffmann, Francis Song, JohnAslanides, Sarah Henderson, Roman Ring, Susan-nah Young, et al. 2021. Scaling language models:Methods, analysis & insights from training gopher.ArXiv, abs/2112.11446. Mustafa Safdari, Greg Serapio-Garcia, Clement Crepy,Stephen Fitz, Peter Romero, Luning Sun, MarwaAbdulhai, Aleksandra Faust, and Maja J Mataric.2023. Personality traits in large language models.ArXiv, abs/2307.00184. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R. Brown, Adam Santoro, Aditya Gupta, andAdri Garriga-Alonso et al. 2022. Beyond the imita-tion game: Quantifying and extrapolating the capa-bilities of language models. ArXiv, abs/2206.04615.",
  "BProbing Ablations": "We run some additional experiments to better un-derstand the probing results from .1. First,as described before, we analyze the performanceof the probe across different topics in . Weobserve that the performance of the probe varies bytopic e.g. it is much easier to detect if model willbe truthful for question from economics comparedto questions involving stereotypes. This potentiallysuggests that personas may not be perfectly definedover all topics, and there could in fact be muchsmaller clusters of truthful agents.Next, to expand on the results in , weuse the same tokens to obtain the representationbut instead of using a specific layer (layer 17), weplot the performance of the probe across differentlayers in . reports accuracy as an alternative prob-ing metric for .",
  ": F1 obtained when training and evaluatinglinear probes at different input and generation tokenembeddings as an extension of results in": "Finally, reports probing results over thegenerated tokens as a baseline for results in . Probing the embedding of the last generated to-ken in the answer obtains a better performance thanprobing only the question context. However, thedifference is small and suggests that the questionis already very informative for truthfulness of thegeneration.",
  "CExperiment Details": "TruthfulQA Evaluation. We use GPT-Judge forautomatically evaluating if the model generation istruthful, in line with previous work (Nakano et al.,2021; Rae et al., 2021; Askell et al., 2021). To ob-tain the GPT-Judge model, we use the OpenAI fine-tuning API at using the datasets re-leased in the TruthfulQA work - We use the defaulthyperparameters and prompt suggested by the orig-inal authors.Finetuning for TruthfulQA. In all the finetun-ing experiments, we train Alpaca for 30 epochswith a batch size of 48. We use the Adam opti-mizer (Kingma and Ba, 2014) with a learning rateof 9e5 and a warmup ratio of 0.03. To finetuningmodels with a smaller compute, we use LORA (Huet al., 2021) we apply it to the query and keyprojection matrices where we set the rank to 16, adropout rate of 0.05.Transforming the BigBench misconceptionsdataset. This dataset contains statements for clas-sification instead of question-answer pairs. Wecovert these statements into QA pairs using GPT-3.5 (Brown et al., 2020), and manually correctsome generated questions which were not correct.Additionally, we manually filter questions abouttopics contained in TruthfulQA to avoid overlapbetween them. The resulting dataset contains 83examples. Training in the synthetic setup. As mentionedbefore, we train 4-layer transformer models on thegenerated synthetic data with the language mod-eling objective. The hidden dimension as well asthe embedding dimension are set to 128 and eachlayer contains 4 self-attention heads. All modelsare trained with a batch size of 512 and learningrate of 0.001 using the Adam optimizer (Kingmaand Ba, 2014) for a total of 20k steps. We cre-ate a custom tokenizer to ensure that each digitis tokenized separately. Specifically, the tokenizercontains the following tokens one token for eachagent, separator token (|), start of sequence token,end of sequence token, tokens corresponding toeach digit (0-9), one token for each operator in thedata and a token for =.",
  "D.1Probing for Truthfulness": "In this experiment we have two training data setups,one with truthful persona and one without a truthfulpersona as described in .1. In each setup,we have m operators where m {8, 12, 16, 20}.Instead of manually defining all the operators, weuse the following to sample truthful and untruthfulinterpretations of the operators:",
  "opF (x, y) = x + y + r2(2)": "where r1, r2 are randomly sampled for each ofthe operators from the range (0, 70). Note that r1and r2 are different for all the operators.We use n = 100 (i.e. range 100 for x, y) andrandomly select the generation parameters. Specifi-cally, if an agent a is truthful on operator op, we setp(a,op) to be a random value > 0.8 and vice versawe set it to < 0.2 if the agent is untruthful.",
  "D.2Generalization to Unseen Operators": "This experiment contains two setups, one withtruthful persona and one without truthful personaas described in .2. Both setups containfour operators, op1 to op4.Notation. In the following, first() and last() areused for functions that denote the first and last digit of the argument respectively. We use ; to denotethe concatenation of the two numbers (e.g. 2; 3 23). We use first2() for the function denoting thefirst two digits of the argument (e.g. first2(123) =12).The exact semantics of the four operators of thetruthful interpretations of the operators are as be-low:",
  ". op3F (x, y) = last(x + y) + first2(y)": "We designed these operators, so that the mod-els we are using can learn these operations. Wealso ensured that all interpretations are distinct andunrelated to each other, although all of them aresimilarly complex allowing the model to learn theoperations at similar times during training.We use n = 200 (i.e. range 200 for x, y) andrandomly set the generation parameters. Specifi-cally, if an agent a is truthful on operator op, we setp(a,op) to be a random value > 0.8 and vice versawe set it to < 0.2 if the agent is untruthful.",
  ": Probability that the model assigns to theuntruthful answer puntruthful decreases as the truthful-ness of agent increases in the first setup, whereas thebehavior widely varies in the second setup": "to an (un)truthful persona based on the cluster theagent belongs to, and generate (un)truthful continu-ations accordingly. An interesting question here isthe mechanism used to perform the persona-basedcomputationdo LLMs first infer the persona andthen compute the corresponding answer? Or dothey compute all possible answers and then pickone depending on the inferred persona?To answer this question, we train two linearprobes. One probe predicts the truthful answer andthe other predicts untruthful answer to the equa-tion, respectively. All probes are trained on theembedding of a token before the complete answeris generated. We expect that if both the truthfuland untruthful probes get high accuracy, the modelcomputes both answers and then picks one depend-ing on the inferred persona. We also train controlprobes to predict an answer of an unrelated oper-ation as a baselinethis helps to control for thepossibility of the LLM encoding answers to all op-erators in the representation, or the probe learningto perform the task. Experiment Details.We use the model from Fig-ure 5 with truthful personas (top), and embeddingsfrom the last layer to train the linear probes. Sincethe answers can span multiple digits, we train theprobe to predict the first different digit between thetruthful and untruthful answers. e.g. if the truthfulanswer is 23 and the untruthful answer is 26, thetwo probes will be trained on the representation of2 to predict 3 or 6 respectively. This is done toreduce the output space of the probe. To train thecontrol probe for the truthful answer, we select ananswer based on the truthful operator for a different",
  "Untruthful Answer96.38%94.73%90.78%79.33%Control Answer24.58%25.03%24.98%23.91%": ": Probing accuracy to predict the truthful answer, the untruthful answer or a control answer. Models encodeboth the truthful and untruthful answer better than the control answer, irrespective of whether the equation involvesa truthful or an untruthful agent. randomly sampled operator. Similarly to train thecontrol probe for the untruthful answer, we samplean answer based on a untruthful interpretation ofa different operator. All the probes are trained on50k randomly sampled examples, and evaluated onheld-out equations for op4. Results.In , we find that irrespective ofwhether we condition on a truthful or an untruth-ful agent, models encode both the truthful and un-truthful answers much better than the control an-swer. This indicates that models compute and storeboth possible answers to an input equation andthen pick an answer based on the inferred per-sona. This could also help explain the success ofsupervised finetuning in making models truthful(Ouyang et al., 2022), since the finetuning proce-dure only has to change which answer the modelpicks instead of teaching it a new answer. We leavemore investigation along this direction on largermodels as future work."
}