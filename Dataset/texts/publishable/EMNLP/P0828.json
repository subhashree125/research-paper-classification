{
  "Abstract": "This study demonstrates that incorporating the written explanations provided byindividuals when making predictions enhances the accuracy of aggregated crowd-sourced forecasts. The research shows that while majority and weighted votemethods are effective, the inclusion of written justifications improves forecastaccuracy throughout most of a questions duration, with the exception of its finalphase. Furthermore, the study analyzes the attributes that differentiate reliable andunreliable justifications.",
  "Introduction": "The concept of the \"wisdom of the crowd\" posits that combining information from numerous non-expert individuals can produce answers that are as accurate as, or even more accurate than, thoseprovided by a single expert. A classic example of this concept is the observation that the medianestimate of an oxs weight from a large group of fair attendees was remarkably close to the actualweight. While generally supported, the idea is not without its limitations. Historical examplesdemonstrate instances where crowds behaved irrationally, and even a world chess champion was ableto defeat the combined moves of a crowd. In the current era, the advantages of collective intelligence are widely utilized. For example, Wikipediarelies on the contributions of volunteers, and community-driven question-answering platforms havegarnered significant attention from the research community. When compiling information fromlarge groups, it is important to determine whether the individual inputs were made independently. Ifnot, factors like group psychology and the influence of persuasive arguments can skew individualjudgments, thus negating the positive effects of crowd wisdom. This paper focuses on forecasts concerning questions spanning political, economic, and socialdomains. Each forecast includes a prediction, estimating the probability of a particular event, anda written justification that explains the reasoning behind the prediction. Forecasts with identicalpredictions can have justifications of varying strength, which, in turn, affects the perceived reliabilityof the predictions. For instance, a justification that simply refers to an external source withoutexplanation may appear to rely heavily on the prevailing opinion of the crowd and might be consideredweaker than a justification that presents specific, verifiable facts from external resources. To clarify the terminology used: a \"question\" is defined as a statement that seeks information (e.g.,\"Will new legislation be implemented before a certain date?\"). Questions have a defined start andend date, and the period between these dates constitutes the \"life\" of the question. \"Forecasters\"are individuals who provide a \"forecast,\" which consists of a \"prediction\" and a \"justification.\" Theprediction is a numerical representation of the likelihood of an event occurring. The justificationis the text provided by the forecaster to support their prediction. The central problem addressed inthis work is termed \"calling a question,\" which refers to the process of determining a final predictionby aggregating individual forecasts. Two strategies are employed for calling questions each daythroughout their life: considering forecasts submitted on the given day (\"daily\") and considering thelast forecast submitted by each forecaster (\"active\"). Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing writtenjustifications to assess the quality of individual or collective forecasts, this paper investigates theautomated calling of questions throughout their duration based on the forecasts available each day.The primary contributions are empirical findings that address the following research questions: * When making a prediction on a specific day, is it advantageous to include forecasts from previousdays? (Yes) * Does the accuracy of the prediction improve when considering the question itselfand the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurateprediction toward the end of a questions duration? (Yes) * Are written justifications more valuablewhen the crowds predictions are less accurate? (Yes) In addition, this research presents an examination of the justifications associated with both accurateand inaccurate forecasts. This analysis aims to identify the features that contribute to a justificationbeing more or less credible.",
  "Related Work": "The language employed by individuals is indicative of various characteristics. Prior research includesboth predictive models (using language samples to predict attributes about the author) and modelsthat provide valuable insights (using language samples and author attributes to identify differentiatinglinguistic features). Previous studies have examined factors such as gender and age, political ideology,health outcomes, and personality traits. In this paper, models are constructed to predict outcomesbased on crowd-sourced forecasts without knowledge of individual forecasters identities. Previous research has also explored how language use varies depending on the relationships betweenindividuals. For instance, studies have analyzed language patterns in social networks, online commu-nities, and corporate emails to understand how individuals in positions of authority communicate.Similarly, researchers have examined how language provides insights into interpersonal interactionsand relationships. In terms of language form and function, prior research has investigated politeness,empathy, advice, condolences, usefulness, and deception. Related to the current studys focus,researchers have examined the influence of Wikipedia editors and studied influence levels withinonline communities. Persuasion has also been analyzed from a computational perspective, includingwithin the context of dialogue systems. The work presented here complements these previous studies.The goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,without explicitly targeting any of the aforementioned characteristics. Within the field of computational linguistics, the task most closely related to this research is argumen-tation. A strong justification for a forecast can be considered a well-reasoned supporting argument.Previous work in this area includes identifying argument components such as claims, premises,backing, rebuttals, and refutations, as well as mining arguments that support or oppose a particularclaim. Despite these efforts, it was found that crowdsourced justifications rarely adhere to theseestablished argumentation frameworks, even though such justifications are valuable for aggregatingforecasts. Finally, several studies have focused on forecasting using datasets similar or identical to the one usedin this research. From a psychological perspective, researchers have explored strategies for enhancingforecasting accuracy, such as utilizing top-performing forecasters (often called \"superforecasters\"),and have analyzed the traits that contribute to their success. These studies aim to identify and cultivatesuperforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,the present research develops models to call questions without using any information about theforecasters themselves. Within the field of computational linguistics, researchers have evaluated thelanguage used in high-quality justifications, focusing on aspects like rating, benefit, and influence.Other researchers have developed models to predict forecaster skill using the textual justificationsfrom specific datasets, such as the Good Judgment Open data, and have also applied these modelsto predict the accuracy of individual forecasts in other contexts, such as company earnings reports.However, none of these prior works have specifically aimed to call questions throughout their entireduration.",
  "Dataset": "The research utilizes data from the Good Judgment Open, a platform where questions are posted, andindividuals submit their forecasts. The questions primarily revolve around geopolitics, encompassingareas such as domestic and international politics, the economy, and social matters. For this study, allbinary questions were collected, along with their associated forecasts, each comprising a predictionand a justification. In total, the dataset contains 441 questions and 96,664 forecasts submittedover 32,708 days. This dataset significantly expands upon previous research, nearly doubling thenumber of forecasts analyzed. Since the objective is to accurately call questions throughout theirentire duration, all forecasts with written justifications are included, regardless of factors such asjustification length or the number of forecasts submitted by a single forecaster. Additionally, thisapproach prioritizes privacy, as no information about the individual forecasters is utilized.",
  "# tokens81620284821.94# entities0235113.47# verbs022362.26# days open224599847574.16": "provides a basic analysis of the questions in the dataset. The majority of questions arerelatively lengthy, containing more than 16 tokens and multiple named entities, with geopolitical,person, and date entities being the most frequent. In terms of duration, half of the questions remainopen for nearly two months, and 75% are open for more than three weeks. An examination of the topics covered by the questions using Latent Dirichlet Allocation (LDA)reveals three primary themes: elections (including terms like \"voting,\" \"winners,\" and \"candidate\"),government actions (including terms like \"negotiations,\" \"announcements,\" \"meetings,\" and \"passing(a law)\"), and wars and violent crimes (including terms like \"groups,\" \"killing,\" \"civilian (casualties),\"and \"arms\"). Although not explicitly represented in the LDA topics, the questions address bothdomestic and international events within these broad themes. : Analysis of the 96,664 written justifications submitted by forecasters in our dataset. Thereadability scores indicate that most justifications are easily understood by high school students (11thor 12th grade), although a substantial amount (>25%) require a college education (Flesch under 50 orDale-Chall over 9.0).",
  "Flesch-49.6850.3365.7680.62121.22Dale-Chall0.056.727.959.2019.77": "presents a fundamental analysis of the 96,664 forecast justifications in the dataset. The medianlength is relatively short, consisting of one sentence and 23 tokens. Justifications mention namedentities less frequently than the questions themselves. Interestingly, half of the justifications containat least one negation, and 25% include three or more. This suggests that forecasters sometimes basetheir predictions on events that might not occur or have not yet occurred. The sentiment polarity of the justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scoressuggest that approximately a quarter of the justifications require a college-level education for fullcomprehension. Regarding verbs and nouns, an analysis using WordNet lexical files reveals that the most commonverb classes are \"change\" (e.g., \"happen,\" \"remain,\" \"increase\"), \"social\" (e.g., \"vote,\" \"support,\"\"help\"), \"cognition\" (e.g., \"think,\" \"believe,\" \"know\"), and \"motion\" (e.g., \"go,\" \"come,\" \"leave\").The most frequent noun classes are \"act\" (e.g., \"election,\" \"support,\" \"deal\"), \"communication\" (e.g.,\"questions,\" \"forecast,\" \"news\"), \"cognition\" (e.g., \"point,\" \"issue,\" \"possibility\"), and \"group\" (e.g.,\"government,\" \"people,\" \"party\").",
  "Experiments and Results": "Experiments are conducted to address the challenge of accurately calling a question throughoutits duration. The input consists of the question itself and the associated forecasts (predictions andjustifications), while the output is an aggregated answer to the question derived from all forecasts.The number of instances corresponds to the total number of days all questions were open. Bothsimple baselines and a neural network are employed, considering both (a) daily forecasts and (b)active forecasts submitted up to ten days prior. The questions are divided into training, validation, and test subsets. Subsequently, all forecastssubmitted throughout the duration of each question are assigned to their respective subsets. Itsimportant to note that randomly splitting the forecasts would be an inappropriate approach. This isbecause forecasts for the same question submitted on different days would be distributed across thetraining, validation, and test subsets, leading to data leakage and inaccurate performance evaluation.",
  "Neural Network Architecture": "A neural network architecture is employed, which consists of three main components: one to generatea representation of the question, another to generate a representation of each forecast, and an LSTMto process the sequence of forecasts and ultimately call the question. The representation of a question is obtained using BERT, followed by a fully connected layer with 256neurons, ReLU activation, and dropout. The representation of a forecast is created by concatenatingthree elements: (a) a binary flag indicating whether the forecast was submitted on the day the questionis being called or on a previous day, (b) the prediction itself (a numerical value between 0.0 and 1.0),and (c) a representation of the justification. The representation of the justification is also obtainedusing BERT, followed by a fully connected layer with 256 neurons, ReLU activation, and dropout.The LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecastsas its input. During the tuning process, it was discovered that providing the representation of thequestion alongside each forecast is more effective than processing forecasts independently of thequestion. Consequently, the representation of the question is concatenated with the representation ofeach forecast before being fed into the LSTM. Finally, the last hidden state of the LSTM is connectedto a fully connected layer with a single neuron and sigmoid activation to produce the final predictionfor the question.",
  "Architecture Ablation": "Experiments are carried out with the complete neural architecture, as described above, as well aswith variations where certain components are disabled. Specifically, the representation of a forecastis manipulated by incorporating different combinations of information: * Only the prediction. * The prediction and the representation of the question. * The prediction andthe representation of the justification. * The prediction, the representation of the question, and therepresentation of the justification.",
  "Quantitative Results": "The evaluation metric used is accuracy, which represents the average percentage of days a modelcorrectly calls a question throughout its duration. Results are reported for all days combined, as wellas for each of the four quartiles of the questions duration. : Results with the test questions (Accuracy: average percentage of days a model predicts aquestion correctly). Results are provided for all days a question was open and for four quartiles (Q1:first 25% of days, Q2: 25-50%, Q3: 50-75%, and Q4: last 25% of days).",
  "BaselinesMajority Vote (predictions)77.2768.8373.9277.9887.44Weighted Vote (predictions)77.9772.0472.1778.5388.22": "Neural Network VariantsPredictions Only78.8177.3178.0478.5381.11Predictions + Question79.3576.0578.5379.5682.94Predictions + Justifications80.8477.8679.0779.7486.17Predictions + Question + Justifications81.2778.7179.8181.5684.67 Despite their relative simplicity, the baseline methods achieve commendable results, demonstratingthat aggregating forecaster predictions without considering the question or justifications is a viablestrategy. However, the full neural network achieves significantly improved results. **Using Daily or Active Forecasts** Incorporating active forecasts, rather than solely relying onforecasts submitted on the day the question is called, proves advantageous for both baselines and allneural network configurations, except for the one using only predictions and justifications. **Encoding Questions and Justifications** The neural network that only utilizes the predictionto represent a forecast surpasses both baseline methods. Notably, integrating the question, thejustification, or both into the forecast representation yields further improvements. These resultsindicate that incorporating the question and forecaster-provided justifications into the model enhancesthe accuracy of question calling. **Calling Questions Throughout Their Life** When examining the results across the four quartiles ofa questions duration, its observed that while using active forecasts is beneficial across all quartilesfor both baselines and all network configurations, the neural networks surprisingly outperform thebaselines only in the first three quartiles. In the last quartile, the neural networks perform significantlyworse than the baselines. This suggests that while modeling questions and justifications is generallyhelpful, it becomes detrimental toward the end of a questions life. This phenomenon can be attributedto the increasing wisdom of the crowd as more evidence becomes available and more forecasterscontribute, making their aggregated predictions more accurate. : Results with the test questions, categorized by question difficulty as determined by the bestbaseline model. The table presents the accuracy (average percentage of days a question is predictedcorrectly) for all questions and for each quartile of difficulty: Q1 (easiest 25%), Q2 (25-50%), Q3(50-75%), and Q4 (hardest 25%).",
  "Predictions + Question79.3594.5888.0178.0458.73Predictions + Justifications80.8495.7193.1879.9957.05Predictions + Question + Justifications81.2794.1790.1178.6764.41": "**Calling Questions Based on Their Difficulty** The analysis is further refined by examiningresults based on question difficulty, determined by the number of days the best-performing baselineincorrectly calls the question. This helps to understand which questions benefit most from the neuralnetworks that incorporate questions and justifications. However, its important to note that calculatingquestion difficulty during the questions active period is not feasible, making these experimentsunrealistic before the question closes and the correct answer is revealed.",
  "This section provides insights into the factors that make questions more difficult to forecast andexamines the characteristics of justifications associated with incorrect and correct predictions": "**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectlyon at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and ahigher number of active forecasts per day (31.0 vs. 26.7). This suggests that the models errors alignwith the questions that forecasters also find challenging. **Justifications** A manual review of 400 justifications (200 associated with incorrect predictionsand 200 with correct predictions) was conducted, focusing on those submitted on days when the bestmodel made an incorrect prediction. The following observations were made: * A higher percentage of incorrect predictions (78%) were accompanied by short justifications(fewer than 20 tokens), compared to 65% for correct predictions. This supports the idea that longeruser-generated text often indicates higher quality. * References to previous forecasts (either by thesame or other forecasters, or the current crowds forecast) were more common in justifications forincorrect predictions (31.5%) than for correct predictions (16%). * A lack of a logical argumentwas prevalent in the justifications, regardless of the predictions accuracy. However, it was morefrequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *Surprisingly, justifications with generic arguments did not clearly differentiate between incorrect andcorrect predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English wereinfrequent but more common in justifications for incorrect predictions (24.5%) compared to correctpredictions (14.5%).",
  "Conclusions": "Forecasting involves predicting future events, a capability highly valued by both governments andindustries as it enables them to anticipate and address potential challenges. This study focuses onquestions spanning the political, economic, and social domains, utilizing forecasts submitted by acrowd of individuals without specialized training. Each forecast comprises a prediction and a naturallanguage justification. The research demonstrates that aggregating the weighted predictions of forecasters is a solid baselinefor calling a question throughout its duration. However, models that incorporate both the questionand the justifications achieve significantly better results, particularly during the first three quartiles ofa questions life. Importantly, the models developed in this study do not profile individual forecastersor utilize any information about their identities. This work lays the groundwork for evaluating thecredibility of anonymous forecasts, enabling the development of robust aggregation strategies that donot require tracking individual forecasters."
}