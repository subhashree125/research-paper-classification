{
  "Abstract": "Recent progress in Spoken Language Model-ing has shown that learning language directlyfrom speech is feasible. Generating speechthrough a pipeline that operates at the text leveltypically loses nuances, intonations, and non-verbal vocalizations. Modeling directly fromspeech opens up the path to more natural andexpressive systems. On the other hand, speech-only systems require up to three orders of mag-nitude more data to catch up to their text-basedcounterparts in terms of their semantic abilities.We show that fine-tuning speech representationmodels on phoneme classification leads to morecontext-invariant representations, and languagemodels trained on these units achieve compa-rable lexical comprehension to ones trained onhundred times more data.",
  "Introduction and related work": "Recent advances in Self-supervised Speech Repre-sentation Learning (SSL) (Mohamed et al., 2022;Chen et al., 2022; Hsu et al., 2021; Baevski et al.,2020) have enabled the development of label-freerepresentations that are valuable for various down-stream tasks (wen Yang et al., 2021). These repre-sentations can be discretized and treated as pseudo-text, allowing for the training of language modelsdirectly from raw audio (Lakhotia et al., 2021),which capture both prosody and linguistic con-tent (Kharitonov et al., 2022). Applications ofthese audio-based language models include dia-logue modeling (Nguyen et al., 2023b), emotionconversion (Polyak et al., 2021), and direct speech-to-speech translation (Lee et al., 2022). They canbe trained not only on discretized SSL representa-tions but also on continuous word-size tokens (Al-gayres et al., 2023) or on a combination of acousticand semantic tokens (Borsos et al., 2023). How-ever, these models still lag behind their text-basedcounterparts in terms of capturing semantics whentrained with similar data quantity (Nguyen et al., 7.58.08.5 MCD (resynth. distortion) sWUGGY Acc. (LM quality) Base L11* FT 100h L12* FT 100h L13 7.58.08.5 MCD (resynth. distortion) sBLIMP Acc. (LM quality) Base L11* FT 100h L12* FT 100h L13",
  ": Trade-off between language modeling andexpressive resynthesis. *: embeddings initialized fromunit centroids": "2020), with scaling laws up to three orders of mag-nitude slower (Cuervo and Marxer, 2024). Recentapproaches tackled this issue by jointly trainingspeech and text Language Models (LMs) (Nguyenet al., 2024; Maiti et al., 2024; Chou et al., 2023)or by using existing LMs as a warm initialization(Hassid et al., 2023).One hypothesis for the data inefficiency of spo-ken language models is that they must at the sametime perform language modeling and process ir-relevant acoustic variations.. Recent works haveaddressed this issue for background noise (Chenet al., 2022), speech rate change (Gat et al., 2023),and speaker change (Qian et al., 2022; Chang et al.,2023; Chang and Glass, 2024). However, con-textual variations due to coarticulation remain achallenge (Hallap et al., 2023): SSL units alignmore closely with contextual phone states (Younget al., 1994) than with linguistic units (Dunbar et al.,2022), which may affect the LMs capacity to learnhigher-order representations of language.Here, we test a simple idea: using supervisedfine-tuning on a phoneme classification task to helpthe model remove its contextual dependency. Wefirst show that fine-tuned models learn represen-tations that are much more context-invariant thanthe original SSL representations, even with as lit-tle as a few hours of labels. Next, we show thatthese representations can be used to train a LM that 123456789 10 11 12 13",
  "Phoneme classification": "We started from the pretrained HuBERT (Hsu et al.,2021) Base model, with 95M parameters, and fine-tuned it on a frame-wise phoneme classificationtask with a forced aligned gold transcription. Wechose this objective to give the model full infor-mation about phoneme identity and boundaries, toenforce the learning of context-invariant represen-tations. An alternative would have been to use aCTC objective (Graves et al., 2006), which has theadvantage of not requiring forced-alignment, butmay result in alignment errors hindering context-invariance. As shown in Appendix A.1, CTC fine-tuning results in slightly lower performance thanphone classification.We added one fully connected layer on top of theHuBERT backbone that maps the 768-dimensionalrepresentation to our phoneme space of dimen-sion 40. We fine-tuned this model on LibriSpeechtrain-clean-100 (Panayotov et al., 2015). Wealso reported results for models fine-tuned on Lib-riLight Limited 10 h, 1 h, and 10 min (Kahn et al.,2020). The forced alignments are those used inNguyen et al. (2020), obtained with the Abkhazialibrary1. The fine-tuning hyperparameters are de-rived from those used in Hsu et al. (2021) for ASR.Input frames are partially masked as in pretraining,but the prediction loss is computed over all outputframes, not just the masked ones. We trained for20 000 steps with a batch size of 32 on a singleNVIDIA V100 GPU.",
  "Quantization": "We selected the best layer in terms of TriphoneABX score for the standard HuBERT base andthe model fine-tuned on train-clean-100. Wetrained k-means models on the features of a 10 hsubset of train-clean-100 extracted from thoselayers, with k = 500. We also quantized the logitsof the fine-tuned model by simply setting the labelsas the predicted phonemes for each frame.",
  "Language modeling": "Finally, we trained LMs on the discretized units.The language model is a 3-layer LSTM, followingthe low-budget baseline of Nguyen et al. (2020),only changing the embedding dimension from 200to 768.It was trained on the discrete units ofLibriSpeech 960 h, for 30 000 steps on a singleNVIDIA V100 GPU. This 26M parameters lan-guage model is two orders of magnitude smallerboth in terms of number of parameters and hours oftraining data than Spoken LMs like TWIST (Hassidet al., 2023) or SpiRit-LM (Nguyen et al., 2024).Our fine-tuned units can in principle benefit anyother LM, including these larger ones.",
  "Speech resynthesis": "For speech resynthesis, we trained a HiFi-GAN(Kong et al., 2020; Polyak et al., 2021) on theEXPRESSO dataset (Nguyen et al., 2023a), con-ditioned on the HuBERT discrete speech unitsand one-hot speaker embeddings from one of EX- PRESSOs voices. We trained for 250 000 steps ontwo NVIDIA V100 GPUs and followed the otherhyperparameters used in EXPRESSO. In this setupthe HiFi-GAN has a different training domain thanthe HuBERT, the k-means, and the LM, whichwere trained on the audiobooks of LibriSpeech.EXPRESSO is rich in expressive variations, paralin-guistics and nonvocals, making it well-suited toevaluate whether the discrete units preserve expres-sivity along with phonemic content.",
  "Evaluation metrics": "We evaluate continuous and discrete units usingABX discriminability (Schatz et al., 2013; Schatz,2016). This task quantifies the discriminabilitybetween two sound categories, A and B, as theprobability that a token x of category A will becloser to another a A than to a b B. Thedissimilarity function is the dynamic time-warpingaligned angular distance between the models rep-resentations of two sounds. The ABX error rate iscalculated by averaging the discriminabilities forall pairs of categories and subtracting it from 1. Inthe standard evaluation, each token is a triphoneand triphones differ only by the central phonemein a triplet. In the within speaker condition, a,b, and x come from the same speaker, while in theacross speaker condition, a and b come from thesame speaker, and x from another one.Following Hallap et al. (2023), we also evaluateour models on the Phoneme ABX task, where eachtoken is a phoneme. We examine two conditions:within context (constant preceding and follow-ing phonemes) and any context (no constraintson context). This task assesses context-invariancein speech representations, revealing that currentself-supervised systems struggle with context in-dependence. Notably, in Hallap et al. (2023) theperformance drop when removing the constant con-text condition is larger than the gaps observed inspeaker independence or clean versus less-cleanspeech conditions. By fine-tuning at a frame levelwithout taking into account the context, our ap-proach is a way to directly tackle this issue. Forcomplementary analysis of the discrete units, seeAppendix A.2.We evaluate spoken language modeling at thelexical and syntactic levels using the sWUGGYand sBLIMP metrics from the ZeroSpeech 2021challenge (Nguyen et al., 2020).sWUGGY isa spot-the-word task, where the network is pre-sented with a word and a matching non-word, andevaluated on its ability to assign a higher proba-bility to the true word. We also report results forin-vocab pairs, which only contains words fromLibriSpeech. sBLIMP assesses the networks abil-ity to prefer grammatically correct sentences overincorrect ones, given a pair of matching sentences.We evaluate content preservation in resynthe-sized speech by following (Nguyen et al., 2023a)and running wav2vec 2.0 Large ASR (Baevskiet al., 2020) on the resynthesized speech, report-",
  "ing the Word Error Rate (WER). We assess thison EXPRESSO-READ the reading subset of EX-": "PRESSO in-domain for the vocoder but out-of-domain for the HuBERT backbone and the k-meansmodule and on LibriSpeech, which is out-of-domain for the vocoder. On EXPRESSO the targetvoice is the same as the input voice, while on Lib-riSpeech the target voice is sampled from the fourvoices. We also compute the mel cepstral distortion(MCD) (Kubichek, 1993) between the original andresynthesized samples of EXPRESSO-READ usingSternkopf and Taubert (2024).",
  "Results at the phonemic level": "As shown in , we computed the ABX errorrate for each Transformer layer of the base modeland the fine-tuned models, including the addedfully connected layer (layer 13). We calculatedboth triphone- and phoneme-level ABX error rates.Fine-tuning mainly improves the last layers ABXerror rates, with near-perfect scores for the 10hand 100h fine-tuned models in the within contextcondition. SSL representations generally strugglemore in the any context condition: there the gainin error rate is the most significant, dropping from9.4% to 2.4% after fine-tuning on as little as 10 min-utes. Fine-tuning pushes representations to becomemore context-independent.",
  ": Zero-shot language comprehension scores (in%), for LMs with an embedding table either initializedrandomly or from the unit centroids": "We selected the best layers for the base model(layer 11) and fine-tuned 100h model (layer 12)based on the Triphone ABX score, as well as thelast layer of the fine-tuned 100h model (layer 13).We trained k-means on these representations and re-port the results in . We compare these to theABX error rates of the best layers of wav2vec 2.0(Baevski et al., 2020), WavLM (Chen et al., 2022),ContentVec100 (Qian et al., 2022) and HuBERT+ Spin2048 (Chang et al., 2023). For the centroidscores, each representation is replaced by the con-tinuous representation of the closest centroid ink-means. For the one-hot scores, each representa-tion is replaced by a one-hot vector with a 1 at itslabel position. We use the same distance to com-pute the ABX as for continuous representations. Inthe case of the base models layer 11 (Base L11)and the fine-tuned 100h models layer 12 (FT 100hL12), the representations are of dimension 768,while for the fine-tuned 100h models layer 13 (FT100h L13) they have a dimension of only 40. Fine-tuning improves both triphone and phoneme ABX scores, particularly in reducing the context effect inthe any context condition, as observed earlier. Inthe case of the ABX of one-hot representations, theerror rates increase across all conditions, but thehighest increase is when the context is not sharedbetween the phones in the triplet. This is a sign thatthe k-means clusters not only are organized accord-ing to the phonemes but also to the surroundingcontext. Clusters are grouped according to theirmost probable phoneme, and within each group,clusters encode different contexts. By going fromcentroid representations to one-hot representations,all 500 clusters are now equidistant, which leads tothe dramatic loss in any context compared to themore modest ones in the other two conditions.",
  "Results above the phonemic level": "We report in the zero-shot sWUGGY (lexi-cal level) and sBLIMP (syntactic level) scores forthe base and fine-tuned models, as well as for anLSTM trained on the gold phonemes. Followingthe observation regarding the ABX error rates ofthe centroids, which remained within 1 percentagepoint of the standard continuous units, we trainLSTMs by initializing their embedding table di-rectly with the associated centroid representationof dimension 768. Apart from this change, thetraining process is the same between the two con-ditions. Fine-tuning for phoneme classification im-proves spoken language modeling in terms of zero-shot comprehension evaluations. Overall, the gapbetween training from speech and training withgolden phonemes is now halved. Fine-tuning forphoneme classification results in models that are onpar in terms of lexical comprehension with muchlarger baselines, which were trained on orders ofmagnitude more of data.However, shows that this comes at thecost of the quality of resynthesis. Notably, thereis a cost in content preservation, illustrated by the WER. It exists both for the LibriSpeech dataset andfor the EXPRESSO-READ, while these two datasetscorrespond to the training domain of different com-ponents of our pipeline. makes directlyvisible the trade-off between language modelingand speech generation quality.",
  "Conclusion": "We showed that fine-tuning SSL representationswith a phoneme classification task is an effectiveand simple procedure to improve context indepen-dence. LMs trained on these units achieve compa-rable lexical comprehension to models trained onhundred times more data. And we also found thatinitializing the embeddings of the discrete tokensof the LMs with the centroids of the units furtherhelps with LM scores. This shows that the unitsfound are meaningfully placed relative to one an-other in this representation space. Our work alsohighlights the trade-off between language modeling(which works best with abstract units), and speechgeneration (which works best with specific units).Fine-tuning on phoneme classification can adjustthis trade-off.",
  "Limitations": "Further work is needed to improve on the trade-off, perhaps by combining SSL, resynthesis, andfine-tuning objectives concurrently. More compre-hensive studies could explore the role of the en-coder in the spoken language modeling pipeline byexamining the impact of fine-tuning methods ondownstream language modeling, comparing self-supervisedand supervised speech models with dif-ferent kinds of supervision. Another important di-rection to consider is the application of this methodin a multilingual setting. The benefits of fine-tuningare visible after training on as little as a few hoursof aligned data, making it applicable to low re-source languages. This work was performed using HPC resourcesfrom GENCI-IDRIS (Grant 2023-AD011014368)and was supported in part by the Agence Nationalepour la Recherche (ANR-17-EURE-0017 Frontcog,ANR10-IDEX-0001-02 PSL*, ANR19-P3IA-0001PRAIRIE 3IA Institute) and a grant from CIFAR(Learning in Machines and Brains) awarded to E.D.in his EHESS capacity. M. P. acknowledges Ph.D.funding from Agence de lInnovation de Dfense. Robin Algayres, Yossi Adi, Tu Nguyen, Jade Copet,Gabriel Synnaeve, Benot Sagot, and EmmanuelDupoux. 2023. Generative spoken language modelbased on continuous word-sized audio tokens. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 30083028, Singapore. Association for Computational Lin-guistics. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,and Michael Auli. 2020. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations.In Advances in Neural Information Processing Sys-tems, volume 33, pages 1244912460. Curran Asso-ciates, Inc. Zaln Borsos, Raphal Marinier, Damien Vincent, Eu-gene Kharitonov, Olivier Pietquin, Matt Sharifi,Dominik Roblek, Olivier Teboul, David Grangier,Marco Tagliasacchi, and Neil Zeghidour. 2023. Au-diolm: A language modeling approach to audio gen-eration. IEEE/ACM Transactions on Audio, Speech,and Language Processing, 31:25232533. Heng-Jui Chang and James Glass. 2024. R-spin: Effi-cient speaker and noise-invariant representation learn-ing with acoustic pieces. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers),pages 642662, Mexico City, Mexico. Associationfor Computational Linguistics.",
  "Heng-Jui Chang, Alexander H. Liu, and James Glass.2023. Self-supervised Fine-tuning for Improved Con-tent Representations by Speaker-invariant Clustering.In Proc. INTERSPEECH 2023, pages 29832987": "Sanyuan Chen, Chengyi Wang, Zhengyang Chen,Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, LongZhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022.Wavlm: Large-scale self-supervised pre-training forfull stack speech processing. IEEE Journal of Se-lected Topics in Signal Processing, 16(6):15051518. Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu,Karen Livescu, Arun Babu, Alexis Conneau, AlexeiBaevski, and Michael Auli. 2023. Toward joint lan-guage modeling for speech units and text. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 65826593, Singapore.Association for Computational Linguistics.",
  "Santiago Cuervo and Ricard Marxer. 2024. Scalingproperties of speech language models.Preprint,arXiv:2404.00685": "Ewan Dunbar, Nicolas Hamilakis, and EmmanuelDupoux. 2022. Self-supervised language learningfrom raw audio: Lessons from the zero resourcespeech challenge. IEEE Journal of Selected Topicsin Signal Processing, 16(6):12111226. Itai Gat, Felix Kreuk, Tu Anh Nguyen, Ann Lee, JadeCopet, Gabriel Synnaeve, Emmanuel Dupoux, andYossi Adi. 2023. Augmentation invariant discreterepresentation for generative spoken language model-ing. In Proceedings of the 20th International Confer-ence on Spoken Language Translation (IWSLT 2023),pages 465477, Toronto, Canada (in-person and on-line). Association for Computational Linguistics. Alex Graves, Santiago Fernndez, Faustino Gomez, andJrgen Schmidhuber. 2006. Connectionist temporalclassification: labelling unsegmented sequence datawith recurrent neural networks. In Proceedings ofthe 23rd International Conference on Machine Learn-ing, ICML 06, page 369376, New York, NY, USA.Association for Computing Machinery.",
  "Mark Hallap, Emmanuel Dupoux, and Ewan Dunbar.2023. Evaluating context-invariance in unsupervisedspeech representations. In Proc. INTERSPEECH2023, pages 29732977": "Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat,Alexis CONNEAU, Felix Kreuk, Jade Copet, Alexan-dre Defossez, Gabriel Synnaeve, Emmanuel Dupoux,Roy Schwartz, and Yossi Adi. 2023. Textually pre-trained speech language models.In Advances inNeural Information Processing Systems, volume 36,pages 6348363501. Curran Associates, Inc. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460. J. Kahn, M. Rivire, W. Zheng, E. Kharitonov, Q. Xu,P.E. Mazar, J. Karadayi, V. Liptchinsky, R. Col-lobert, C. Fuegen, T. Likhomanenko, G. Synnaeve,A. Joulin, A. Mohamed, and E. Dupoux. 2020. Libri-light: A benchmark for asr with limited or no super-vision. In ICASSP 2020 - 2020 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 76697673. Eugene Kharitonov, Ann Lee, Adam Polyak, YossiAdi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen,Morgane Riviere, Abdelrahman Mohamed, Em-manuel Dupoux, and Wei-Ning Hsu. 2022. Text-freeprosody-aware generative spoken language modeling.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 86668681, Dublin, Ireland.Association for Computational Linguistics.",
  "of IEEE Pacific Rim Conference on CommunicationsComputers and Signal Processing, volume 1, pages125128 vol.1": "Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-AnhNguyen, Jade Copet, Alexei Baevski, AbdelrahmanMohamed, and Emmanuel Dupoux. 2021. On gen-erative spoken language modeling from raw audio.Transactions of the Association for ComputationalLinguistics, 9:13361354. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne,Holger Schwenk, Peng-Jen Chen, Changhan Wang,Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, andWei-Ning Hsu. 2022.Textless speech-to-speechtranslation on real data.In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 860872, Seattle,United States. Association for Computational Lin-guistics. Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-WeonJung, Xuankai Chang, and Shinji Watanabe. 2024.Voxtlm: Unified decoder-only models for consolidat-ing speech recognition, synthesis and speech, textcontinuation tasks. In ICASSP 2024 - 2024 IEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 1332613330. Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt,Jakob D. Havtorn, Joakim Edin, Christian Igel, Ka-trin Kirchhoff, Shang-Wen Li, Karen Livescu, LarsMaale, Tara N. Sainath, and Shinji Watanabe. 2022.Self-supervised speech representation learning: Areview. IEEE Journal of Selected Topics in SignalProcessing, 16(6):11791210. Tu Anh Nguyen,Maureen de Seyssel,PatriciaRoz, Morgane Rivire, Evgeny Kharitonov, AlexeiBaevski, Ewan Dunbar, and Emmanuel Dupoux.2020. The Zero Resource Speech Benchmark 2021:Metrics and baselines for unsupervised spoken lan-guage modeling. Preprint, arxiv:2011.11588. Tu Anh Nguyen, Wei-Ning Hsu, Antony DAvirro,Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-mez, Jade Copet, Gabriel Synnaeve, Michael Has-sid, Felix Kreuk, Yossi Adi, and Emmanuel Dupoux.2023a. Expresso: A Benchmark and Analysis ofDiscrete Expressive Speech Resynthesis. In Proc.INTERSPEECH 2023, pages 48234827. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, YossiAdi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello,Robin Algayres, Benot Sagot, Abdelrahman Mo-hamed, and Emmanuel Dupoux. 2023b. Generativespoken dialogue language modeling. Transactionsof the Association for Computational Linguistics,11:250266.",
  "Sagot, and Emmanuel Dupoux. 2024. SpiRit-LM:Interleaved Spoken and Written Language Model.Preprint, arxiv:2402.05755": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-jeev Khudanpur. 2015. Librispeech: An asr corpusbased on public domain audio books. In 2015 IEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 52065210. Adam Polyak,Yossi Adi,Jade Copet,EugeneKharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-delrahman Mohamed, and Emmanuel Dupoux. 2021.Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. In Proc. Interspeech2021, pages 36153619. Kaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni,Cheng-I Lai, David Cox, Mark Hasegawa-Johnson,and Shiyu Chang. 2022. ContentVec: An improvedself-supervised speech representation by disentan-gling speakers. In Proceedings of the 39th Interna-tional Conference on Machine Learning, volume 162of Proceedings of Machine Learning Research, pages1800318017. PMLR. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine Mcleavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In Proceedings of the 40th InternationalConference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages2849228518. PMLR.",
  "Jasmin Sternkopf and Stefan Taubert. 2024.mel-cepstral-distance": "Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu,Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li,Shinji Watanabe, Abdelrahman Mohamed, and Hungyi Lee. 2021. SUPERB: Speech Processing Univer-sal PERformance Benchmark. In Proc. Interspeech2021, pages 11941198. S.J. Young, J.J. Odell, and P.C. Woodland. 1994. Tree-based state tying for high accuracy modelling. In Hu-man Language Technology: Proceedings of a Work-shop held at Plainsboro, New Jersey, March 8-11,1994. 123456789 10 11 12 13",
  "A.2Discrete units quality": "In addition to the ABX scores reported in .1, the quality of the discrete units and their rela-tionship to phonemes can also be assessed with thethree metrics proposed in Hsu et al. (2021): Clus-ter Purity, Phone Purity, and PNMI. Cluster purityis the conditional probability of a k-means labelgiven a phone label, phone purity is the conditionalprobability of a phone label given a k-means label,and PNMI is the phone-normalized mutual infor-mation between units and phone labels. The unitsare obtained from the cluster assignments given bythe k-means with 500 clusters trained on the outputof the considered model. The evaluation is doneon the combination of LibriSpeech dev-clean anddev-other. We have for the Base L11 and FT 100hL12 models: a PNMI of 0.669 and 0.846, ClusterPurity of 0.093 and 0.131, and Phone Purity of0.685 and 0.858, respectively."
}