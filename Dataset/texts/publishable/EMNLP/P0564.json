{
  "Abstract": "Emotion classification has wide applications ineducation, robotics, virtual reality, etc. How-ever, identifying subtle differences betweenfine-grained emotion categories remains chal-lenging. Current methods typically aggregatenumerous token embeddings of a sentenceinto a single vector, which, while being anefficient compressor, may not fully capturetheir complex semantic and temporal distri-butions. To solve this problem, we proposeSEmantic ANchor Graph Neural Networks(SEAN-GNN) for fine-grained emotion clas-sification. It learns a group of representative,multi-faceted semantic anchors in the token em-bedding space: using these anchors as globalreference, any sentence can be projected ontothem to form a semantic-anchor graph, withnode attributes and edge weights quantifyingsemantic and temporal information, respec-tively.The graph structure is well alignedacross sentences and, importantly, allows forgenerating comprehensive emotion representa-tions regarding K different anchors. Messagepassing on the anchor graph can further inte-grate the semantic and temporal informationand refine the learned features. Empirically,SEAN-GNN produces meaningful semantic an-chors and discriminative graph patterns, withpromising classification results on 6 popularbenchmark datasets against state-of-the-arts.",
  "These authors contribute equally.Corresponding Author": "more detailed distinctions. For example, the twolargest fine-grained emotion classification datasetscontain 32 (Rashkin et al., 2019) and 27 (Demszkyet al., 2020) categories, respectively.The difficulty of fine-grained emotion classifica-tion mainly arises from learning faithful emotionrepresentations, in particular in terms of captur-ing both the semantic and temporal distribution ofemotion-related vocabulary in the sentence:Semantically, human emotions are expressed byhighly diverse word vocabulary (emotion-relatedadjectives, nouns, verbs and adverbs describingthe intensity of the situation). Capturing the dis-tribution of this rich vocabulary, and the subtledifference between similar emotions (e.g., afraidand terrified) is still an important challenge forfine-grained emotion classification.Temporally, the meaning of a sentence is re-lated to the meanings of its parts and the way theyare combined (Pagin, 2016); in particular, subtledifferences of emotion categories are in many casespresented by the relationship among the words(Waugh, 1977). Therefore capturing the temporal(or positional) word relations is crucial for emo-tion classification. Unfortunately, since differentsentences have different word compositions, di-rectly quantifying and comparing word relation-ships across sentences is practically challenging.Numerous methods have been proposed for fine-grained emotion classification, see a review in. Despite the technical diversity, thesemethods typically use pre-trained language mod-els (PLMs) or those enhanced with contrastivelearning (Suresh and Ong, 2021) or LSTM (Zan-war et al., 2022) to obtain the token embeddings,and then aggregate them into a single vector for Consider two sentences: I feel extremely sad when I seeanimals abandoned and left to suffer. and I feel sad whenI see extremely pitiful animals abandoned and left to suffer.In the former, extremely describes sad and indicates a deeperemotion like devastated, while in the latter, extremely modifiespitiful, hence the emotion conveyed remains sadness.",
  "th input sentenceith input sentencei": ": The structure of SEAN-GNN. (1) The K semantic anchors are learned end-to-end to cover emotionrelevant vocabulary. (2) For an input sentence, the content-projector and the temporal projector are used to instill itssemantic distribution and token relationship into an anchor graph. (3) A message passing GNN is used to integratethe semantic and temporal information and refine the anchor representations for final classification. sentence-level representation.Well-known ag-gregating schemes include average pooling (Suet al., 2021), sum-pooling (Alvarez-Gonzalez et al.,2021), and [CLS] token (Sosea and Caragea, 2021;Suresh and Ong, 2021; Chen et al., 2023).Although PLMs provide informative token em-beddings for a sentence, aggregating them into asingle vector may lead to significant informationcompression. From data distribution point of view,average pooling of the tokens is like approximatingtheir distribution with the first-order statistics, andhigher-order information (e.g. relationship amongthe tokens) may not be fully quantified. However,the semantic distribution and the temporal relation-ship among the tokens are important informationfor accurate emotion classification.In this paper, we explore new ways for com-puting sentence-level representations to capturecomplex semantic distributions and temporal rela-tionship of the tokens. Unlike current methods thatcompress all the tokens of a sentence into one vec-tor, we use a set of semantic anchors to extractsentence information in a more delicate manner.Our method is called semantic anchor graph neuralnetwork (SEAN-GNN), as in .The SEAN-network has three building blocks.(1) Learning semantic anchors, a set of vectorsshared globally in the token embedding space cov-ering emotion-related vocabulary. (2) Projecting",
  "The [CLS] embedding can be deemed as a weightedaverage of the token embeddings, so with similar observation": "a sentence onto the anchors by content projector(which projects token embeddings to anchors bytheir semantic similarity) and temporal projector(which projects the positional token relationshiponto pairs of anchors). Then a sentence of arbitrarylength can be expressed as a constant-sized anchor-graph, where node attributes and edge weights inturn quantify semantic and temporal information.(3)Using GNN to integrate semantic and temporalinformation to refine graph representations. The semantic anchors provide a flexible and fine-grained basis for learning emotion representations.The anchors are learned end-to-end to cover emo-tion related vocabulary adaptively. Besides, an-chors are shared globally, and so complex tokenrelations from sentences of different word com-positions, which are otherwise hard to compare,can now be easily quantified using the anchors asa common ground. This is beneficial since sub-tle positional relations of words can be importantemotion features. Most importantly, rather thancompressing all the tokens into a single vector, thesemantic anchor graph allows one sentence to beencoded by multiple vectors each associated withone semantic anchor, being a highly enriched rep-resentation for fine-grained emotion classification. Main contributions of the paper are listed below: We proposed SEAN-GNN to extract emotion-related features in a more delicate manner for fine-grained emotion classification. We show that SEAN-GNN learns meaningful semantic anchors and discriminative graph patternsfor different emotion categories. We show that SEAN-GNN has promising resultsagainst state-of-the-arts across various base PLMsand 6 popular benchmark datasets.",
  "Related Work": "Numerous methods have been proposed for fine-grained emotion classification.Typically, pre-trained language models (PLM) are used to gettoken embeddings; these embedding are then fur-ther refined/updated before aggregated into a singlevector for emotion classification. Various strategieswere designed to refine either of these 3 steps.For PLMs, Sosea and Caragea (2021) presentedemotion masked language modeling, which onlymasked off those emotion-related tokens in the pre-training stage; Yin and Shang (2022) incorporateda whitening method and nearest neighbor retrievalto PLMs to improve retrieval efficiency so as tobetter differentiate semantically similar sentences.For token embedding refinement, Suresh andOng (2021) modified the supervised contrastiveloss (Khosla et al., 2020) and propose a label-awarecontrastive loss to improve token embedding; Chenet al. (2023) proposed HypEmo, which learnedlabel embedding in hyperbolic space and integratedit with RoBERTa fine-tuned in Euclidean space.For token aggregation, a common method in-volves pooling all token embeddings into a singlevector through averaging, summation, or takingthe maximum/minimum. For instance, Zanwaret al. (2022) employed Bi-LSTM to process tokenembeddings derived from a pre-trained languagemodel, and concatenated the hidden representa-tions from Bi-LSTMs final layer to form the con-text representation. Alvarez-Gonzalez et al. (2021)suggested that a pooling function such as attention,mean or max can be used to aggregate token em-beddings to a vector; The [CLS] token embeddingis also commonly used as the sentence-level fea-ture (Devlin et al., 2018). However, Su et al. (2021)showed that averaging the token embeddings isbetter than only utilizing [CLS] token. Both aresub-optimal as demonstrated by Choi et al. (2021).Despite the technical diversity, most of thesemethods mix up the token embeddings of a sen-tence into a single vector. Such aggregation is aconvenient way for sentence-level representation,but it may not be sufficiently effective in capturingthe semantic and temporal distribution, which can be crucial to accurate emotion classification.In the NLP literature, concept of anchors havebeen explored in various tasks, but with motiva-tions and implementations very different from ourapproach.For example, Arora et al. (2012) se-lected words that are uniquely associated with atopic as anchors to accelerate topic modeling anal-ysis. Liu et al. (2020) adopted the average contex-tual representations of each word as the anchors toenhance contextualized representations. Wang et al.(2023) used the class labels/words as anchors andused anchor re-weighting to improve in-contextlearning performance. In these works, anchors arelinked to predefined words, while our anchors arelearned adaptively through data.GNN models have also been applied in NLPtasks, like encoding word relations (Yao et al.,2019), recognizing named entities (Luo and Zhao,2020), modeling syntactic structures (Luo andZhao, 2020), etc.A main difference is that ourGNN is built on semantic anchors rather than rawtokens. Using anchors as GNN nodes allows gener-ating emotion representations that are not only richand multi-faceted, but also well-aligned across dif-ferent sentences without token padding or cutting(an undesired perturbation of their embeddings).GNN models themselves could also benefit fromthe use of anchors. These methods use anchors toimprove the computational efficiency of GNNs orgraph-based clustering/semi-supervised learning(Liu et al., 2010; Nie et al., 2022; You et al., 2019,etc), to better encode relative positional relationbetween the nodes (You et al., 2019), or to improvegraph embedding in case of noisy/inaccurate edges(Tu et al., 2022). These anchor based GNN modelsare different from ours in both their motivationsand methodology. They mainly consider graphslike similarity graph (clustering), protein networksand communication networks (link prediction andcommunity detection), without temporal (sequen-tial) relation between the nodes; in comparison, achallenge in our context is how to properly projectthe temporal relationship between pairs of tokensonto their corresponding anchors. Furthermore,our anchors are learned end-to-end, instead of be-ing directly selected from existing nodes or com-puted through an off-line procedure.",
  "Semantic Anchors": "Given m sentences each shaped to the same lengthof n tokens, as X(i) = {x(i)1 , x(i)2 , ..., x(i)n }. Herex(i)j Rd1 is the embedding of the jth token ofthe ith sentence. In order to account for the diver-sity of emotion-related vocabulary in the trainingdata, we propose to learn a global semantic anchorset, Z = {z1, z2, ..., zK} to facilitate the represen-tation learning for emotion classification. Eachzk Rd1 is a vector in the word embeddingspace. Preferably, the learned anchors should be di-verse enough to cover different emotional aspects,while in the mean time discriminative enough togenerate good features for accurate classification.To promote diversity of semantic anchors, wecollect token embeddings from all (or a randomsubset of) the input sentences obtained throughpretrained language model, and then initialize theanchors as their K-means clustering centers. TheK-means algorithm is known to distribute cluster-ing centers to minimize the reconstruction errorof the input samples. We further optimize the se-mantic anchors in the end-to-end architecture in. By doing this, the semantic anchors willbe iteratively optimized and updated to facilitateextraction of discriminative semantic and temporalfeatures for emotion classification.",
  "Using the K anchors {zks} as global reference,we can project the information of sentence X(i)": "onto it and obtain a graph representation as G(i) =(A(i), W(i)). We call G(i) the semantic-anchorgraph (SEAN-graph) for sentence X(i), which hasexactly K nodes corresponding to the K anchors.The node attribute matrix A(i) RKd and ad-jacency matrix W(i) RKK respectively en-codes the semantic (first-order) and the temporal(second-order) distribution of the input sentence.Since anchors are shared across sentences withwide coverage and discriminative power, the se-mantic anchor graph G(i) serves as an informativeand well-aligned emotion representation.To project the input sentence X(i) onto the K an-chors to extract its semantic/temporal information,we have devised the following two projectors:",
  "Temporal projector. The temporal relationsbetween the words of a sentence are projectedas edge weights (W(i)) of the SEAN-graph": "Content projector.Suppose we are givenan input sentence with token embedding matrixX(i) = {x(i)1 , x(i)2 , ..., x(i)n }. The goal of the con-tent projector is to project each token to the Kanchors zks in a probabilistic manner. We use aprobability matrix P(i) RnK whose jkth entrydenote the probability that the jth token in the ithsentence belongs to the kth anchor, such that",
  "(1)": "with the bandwidth of the Gaussian. In otherwords, each row of P(i) specifies the probabilityof one token belonging to the K anchors. It canalso be deemed as the cross-attention matrix be-tween tokens and anchors. After quantifying theprobabilistic association between n tokens and theK anchors, we can project the token embeddingsonto the anchors as,",
  "A(i) = (P(i)) X(i).(2)": "The matrix A(i) RKd can be used as the at-tribute matrix of the semantic anchor-graph G(i).Intuitively, the kth row in A(i) summarizes the con-tent of the sentence that are most relevant to thekth semantic anchor. If the tokens are all irrelevantto that anchor, the kth row of A(i) approaches 0.Temporal Projector. The goal of the temporalprojector is to project the temporal/positional re-lation between pairs of tokens in a sentence ontopairs of anchors. This allows token relationshipin each sentence to be expressed globally as therelationship among the K semantic anchors.Suppose we have projected a sentence X(i) ontoK anchors, with the token-anchor probability ma-trix P(i) (1). We normalize it such that kth columnin P(i) becomes a probability simplex describingthe probabilities that a word similar to the kth an-chor appears in the n locations of the sentence X(i).It can be deemed as the positional distribution ofthe kth anchor in the sentence. We will use thesecolumns to evaluate the relations between any pairof anchors for input sentence X(i), as follows.Let p(i)aand p(i)bdenote two columns of P(i),i.e., p(i)a= P(i)[:,a], p(i)b= P(i)[:,b], as illustrated in . For each of the n entries/locations in p(i)a ,say, the sth entry with (large) probability p(i)a (s),we will examine the entries inside the location win-dow [s l, s + l] in the probability vector p(i)b . Ifthere exists a large probability in this window, thatmeans two words whose meanings are similar tothe two anchors respectively appear in close vicin-ity to each other within the input sentence X(i).This should contribute positively to the temporalrelation between the two anchors. We will exam-ine all the entries in pa and accumulate the scores.Mathematically, the temporal relation between theath anchor and the bth anchor due to the inputsentence X(i) can then be computed as follows,",
  "W(i)ab =K(i)ab C1(3)": "Empirically, revising the 1-norm in (3) to themixed-norm ,1 (summation of the maximumentry of each row) gives more robust result. Thismeans that for a token in one of the two positionaldistributions, p(i)a and p(i)b , we emphasize only themost significant word pairs across the two distribu-tions. This, however, breaks the symmetry so we",
  "Message Passing on the Anchor-Graph": "Having encoded the semantic/temporal informa-tion of sentence X(i) as an undirected anchor graphG(i), with node attribute matrix A(i) and adjacencymatrix W(i), we employ GNNs (Kipf and Welling,2016; Velickovic et al., 2017; Hamilton et al., 2017,etc.) to perform message passing among the an-chor nodes. The procedures using GCN (Kipf andWelling, 2016) is as follows.",
  "H[l][l](5)": "Here, H[l] is node feature matrix at layer l, and the0th layer is initialized by A(i); D is normalized de-gree matrix, W is chosen as the adjacency matrixbetween anchors, is the ReLU (Krizhevsky et al.,2012), and [l]is the transform at layer l.The message passing on semantic anchor graphwill aggregate the features of those anchor-nodeshaving close temporal relations with each otheraccording to the input sentence. In other words,the temporal and semantic information of the inputsentence are integrated through GNN to enhancethe anchor features, and the resultant attribute ma-trix H[l] will uniquely determine representation ofthe input sentence. We concatenate H and final-layer H[l] as the sentence-level representation, andflatten it to a long vector with a 3-layer FFN andcross entropy loss for classification. Note that ourmethod allows any GNN model for message pass-ing, lending itself great flexibility in applications.",
  "Datasets": "We evaluate our model on altogether 6 benchmarkdatasets widely used for emotion classification.Among them, the first two are fine-grained clas-sification (the two largest and most challengingdatasets we could find in the literature), and theother 4 datasets are course-grained classification.The way we pre-process each dataset follows pre-vious works (Chen et al., 2023; Suresh and Ong,2021; Yin and Shang, 2022). Brief data statisticsare listed below (see more details in Appendix B). (1) Empathetic Dialogue (Rashkin et al., 2019)consists of dialogues between a speaker and a lis-tener with 32 single emotion label.(2) GoEmotion (Demszky et al., 2020) are Redditcomments from 27 emotions and neutral.(3) CancerEmo (Sosea and Caragea, 2020) com-poses of 8500 sentences sampled from an onlinecancer survivors network with 8 emotion labels.(4) ISEAR (Scherer and Wallbott, 1994) containssentences of personal reports on emotional eventslabelled with one of 7 emotions.(5) GoEmotion-EK (Ekman et al., 1999) anno-tates data originally constructed by (Demszky et al.,2020) into Ekmans 6 basic emotions.(6) EmoInt (Mohammad and Bravo-Marquez,2017) comprises tweets of 4 emotion classes.",
  "Experimental Settings and Baselines": "Metrics. For fine-grained emotion classification,we adopt Accuracy and Weighted F1 by followingthe setting in (Suresh and Ong, 2021) and (Chenet al., 2023). For coarse-grained emotion classifica-tion, we use Macro F1 following the common prac-tice in (Yin and Shang, 2022; Singh et al., 2023).Baselines. We incorporated 12 baseline meth-ods.Baseline methods (1-6) are three PLMs(BERT, RoBERTa, and ELECTRA) with two sizes(base, large), all using the [CLS] token embed-ding as the sentence-level feature. The remaining6 baselines are recent state-of-the-art methods, in-cluding: (7) LCL (Suresh and Ong, 2021) usinglabel-aware contrastive loss; (8) Hypemo (Chenet al., 2023), using label-aware weighting and hy-perbolic distance metric; (9-10) PLM-BiLSTMand PLM-DNN (Alvarez-Gonzalez et al., 2021)using Bi-LSTM and DNN to update token em-beddings from PLMs with summation pooling;(11) PsyLing (Zanwar et al., 2022) use Bi-LSTMtrained on psycholinguistic features to improve thegeneralizability for emotion classification of tokenembeddings from PLMs; (12) KNNEC (Yin andShang, 2022) using whitening method and nearestneighbor retrieval for emotion classification.Method (7-12) and ours need a base PLM tocompute token embeddings. For fairness of com-parison, we used RoBERTabase for all, which wasalso the majority of their official choices. For thoseofficially reported results using BERTbase, our com-parisons with them are in Appendix C.1.Our algorithm used batch size 64 and AdamWoptimizer, with a learning rate 2e5 and a weightdecay 0.01. Graph convolutional network (Kipf and Welling, 2016) is used for message passing.The number of semantic anchor K was chosenfrom {50, 100, 150, 200} using validation set. Theparameter settings of other methods follow theiroriginal papers, see details in Appendix C.2.",
  "Classification Results": "Results are reported in . Each evaluationis based on 5 repeats of different seeds, with theaverage score and standard deviation.Our results surpassed over PLMs (base and largeversions), with weighted-F1 being 4.0% and 2.2%higher than the best among them (RoBERTa large)in two fine-grained classifications. In the 4 coarse-grained tasks, our model surpassed RoBERTalargein Macro-F1 by 1.1% - 2.9%. Note that our modelwas only based on the base version of RoBERTa.Therefore these performance gains are clearly at-tributed to the use the semantic anchor graph inaggregating token embeddings.Our model also outperforms other advanced al-gorithms with an improvement of 1.2% and 1.1%in weighted F1 , 1.6% and 2.2% in accuracy againstthe best competitor on 2 fine-grained tasks; on4 coarse-grains datasets, an improvement around1.1% - 2.2% in Macro F1 was observed. Overall,our method has shown promising results across allmetrics and datasets.",
  "Impact of Base PLMs and Anchor-set Size": "reports the results of our method usinganchor-based sentence features when the raw to-ken embeddings are obtained from different basePLMs (BERTbase, RoBERTabase, ELECTRAbase).It also reports the results of these PLMs using[CLS] embedding as sentence features. As canbe seen, SEAN-GNN can enhance performance ir-respective of the PLM employed, with an improve-ment around 3.3% - 9.4%. This shows that ourapproach is PLM-agnostic and can be versatilelyintegrated with any PLMs to improve performance.We also investigate how the number of semanticanchors, K, affects the performance. Using thetwo fine-grained datasets, we plot the WeightedF1 score of our method when K is chosen from 1to 500. Here, K=1 can be deemed as the standardpooling. As shown in , the performance ex-hibits a significant improvement when K increasesfrom 1 to 100, validating the effectiveness of intro-ducing semantic anchors to emotion classification.When K increases to 200, the performance remainssteady, meaning that the gains due to larger num-",
  "AccWeighted F1AccWeighted F1Macro F1": "BERTbase50.4 0.351.8 0.2160.9 0.462.9 0.570.1 1.469.2 0.871.1 1.184.8 0.6RoBERTabase54.5 0.756.0 0.462.6 0.664.0 0.273.6 1.369.4 0.971.9 0.785.4 0.6ELECTRAbase47.7 1.249.6 1.059.5 0.461.6 0.672.1 0.569.9 1.271.4 1.385.2 0.9BERTlarge53.8 0.154.3 0.164.5 0.365.2 0.472.3 0.770.2 1.471.6 0.985.6 0.5RoBERTalarge57.4 0.558.2 0.364.6 0.365.2 0.274.7 1.073.1 0.573.0 0.886.0 0.7ELECTRAlarge56.7 0.657.6 0.663.5 0.264.1 0.373.5 0.972.5 1.472.0 0.785.3 0.7 PLM-BiLSTM55.3 1.156.9 0.963.4 1.464.6 0.873.8 0.769.9 1.272.3 0.685.6 0.6PLM-DNN55.1 0.757.2 1.363.0 0.564.3 1.474.4 0.970.3 1.172.6 0.885.4 0.5PsyLing56.5 1.257.0 1.163.0 0.664.6 1.374.7 0.771.7 1.473.0 0.985.7 0.3KNNEC58.0 0.958.5 0.864.0 1.264.5 1.074.4 0.670.7 1.173.5 1.386.0 0.7LCL59.5 0.659.2 0.564.5 0.365.1 0.375.0 0.872.1 1.072.8 1.286.3 0.3",
  "+ 1.6%+ 1.2%+ 2.2%+ 1.1%+ 2.2%+ 1.1%+ 1.2%+ 1.3%": ": Classification results (in %) for all methods, with weighted F1 and accuracy for fine-grained task (Sureshand Ong, 2021; Chen et al., 2023), and Macro F1 for coarse-grained task (Yin and Shang, 2022; Singh et al., 2023).The best/second-best results highlighted in bold/ underline. \"\" indicates we present results using RoBERTabase asbackbone for fairness. CE, IS, EK, EM stands for CancerEMO, ISEAR, GoEmotion-EK, EmoInt; numerals are thenumber of classes. represents the improvement of our model over the second-best.",
  "Case Study": "In this subsection, we examine whether SEAN-GNN can generate meaningful semantic anchorsfor emotion classification, as well as unique graphpatterns for different emotion classes. Moreover,we report comparative results using 4 most diffi-cult subsets of Empathetic Dialogues to furtherdemonstrate the effectiveness of our method.We choose three pairs of emotion classes withsubtle difference: {Afraid vs Terrified}, {Angry vsFurious} and {Sad vs Devastated}. First, we pullout top-6 semantic anchors most relevant to eachemotion, and annotate the anchor with two wordswith closest embeddings to it (see Appendix A.2).As shown in , the learned anchors en-compass verbs (run, shout, cry), nouns (murder,betrayal, despair) and adjectives (severe, unfair, up-set). Their semantics look quite reasonable witheach emotion class, like Terrified: {murder, crime,scream, frighten}, and Furious: {disrespect,insult}.Interestingly, for the intense emotion in each pair(e.g., furious, terrified), they are often associatedwith anchors of adverbs such as {so, really, very,quite}, which are absent in less-intense emotions(afraid, sad). Intense emotions may also employ an-chors like {murder, yell} to describe the fierce state.These observations are consistent with our under-standing of the emotions from a natural languageperspective. A longer list is in Appendix A.3. visualizes the averaged adjacency ma-trix (4) (edges with the top-10% highest weights) Emotion Related AnchorsAnchor Graph Pattern",
  "vsSadDevastatedvsvs": "AfraidTerrifedAngryFurious unhappy /heartbroken failure /sorrow lose /miss crushed /shocked divorce / cancer cry /weep depressed /sentimental frustrated / really / very lost /gone despair / die /pass grief upset yell /shout anger /temper cheat / lie irritated /annoyed abuse / bully insult unfair / so /quite mistake / errorrude / interrupt / ignore offensive disrespect / betrayed / run /escape severe /extreme scream / frighten fear / sacre danger / riskpanic unknown / accident murder / crime cancer /illness night /midnightreally / very horror /terror threat / : Visualization of semantic anchors (top row) and anchor-graph patterns (bottom row) learned by SEAN-GNN for 6 (3 pairs of easily confused) emotion classes. Top row: 6 most relevant anchors, each annotated by 2closest words, for different emotions as visualized by tSNE (colored squares: class-relevant anchors; gray: lessrelevant). Bottom row: averaged anchor-graph patterns (K K adjacency matrix in (4)) for each emotion class.",
  ": Weighted F1 (%) on 4 most confusable sub-sets of Empathetic Dialogue compared with previouseffective methods and RoBERTabase. represents theimprovement of our model over the second-best": "for sentences in each emotion category.Theanchor-graph patterns show a clear difference evenamong emotion categories with only small differ-ence.This shows the discriminative power of theanchor-graph based sentence representations. reports comparative results on fourmost confusable subsets of Empathetic Dialogueselected by Suresh and Ong (2021) (see AppendixD for details). Our method outperforms state-of-the-art methods by 1.1%-1.6% in weighted F1.",
  "Limitations": "We only evaluated the performance of the compet-ing methods on datasets in the English, due to thelack of fine-grained emotion classification datasetsin languages other than English,which potentiallyintroduced language and cultural biases. Moreover,the risk of reinforcing existing data biases and theconsideration of model fairness across differentdemographic groups were not addressed. Nurudin Alvarez-Gonzalez, Andreas Kaltenbrunner,and Vicen Gmez. 2021. Uncovering the limitsof text-based emotion detection. In Findings of theAssociation for Computational Linguistics: EMNLP2021, pages 25602583.",
  "Learning topic models going beyond svd. In 2012IEEE 53rd Annual Symposium on Foundations ofComputer Science, pages 110": "Chih Yao Chen, Tun Min Hung, Yi-Li Hsu, and Lun-Wei Ku. 2023. Label-aware hyperbolic embeddingsfor fine-grained emotion classification. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1094710958. Hyunjin Choi,Judong Kim,Seongho Joe,andYoungjune Gwon. 2021.Evaluation of bert andalbert sentence embedding performance on down-stream nlp tasks. In 2020 25th International con-ference on pattern recognition (ICPR), pages 54825487. IEEE. Dorottya Demszky, Dana Movshovitz-Attias, JeongwooKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.2020. Goemotions: A dataset of fine-grained emo-tions. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages40404054.",
  "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-ton. 2012. Imagenet classification with deep convo-lutional neural networks. Advances in neural infor-mation processing systems, 25": "Qianchu Liu, Diana McCarthy, and Anna Korhonen.2020.Towards better context-aware lexical se-mantics: Adjusting contextualized representationsthrough static anchors. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 40664075. Wei Liu, Junfeng He, and Shih-Fu Chang. 2010. Largegraph construction for scalable semi-supervisedlearning. In Proceedings of the 27th internationalconference on machine learning (ICML-10), pages679686. Citeseer. Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang,and Sophia Ananiadou. 2024. Emollms: A seriesof emotional large language models and annotationtools for comprehensive affective analysis. In Pro-ceedings of the 30th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pages 54875496. Ying Luo and Hai Zhao. 2020. Bipartite flat-graphnetwork for nested named entity recognition.InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 64086418. Saif M. Mohammad and Felipe Bravo-Marquez. 2017.WASSA-2017 shared task on emotion intensity. InProceedings of the Workshop on Computational Ap-proaches to Subjectivity, Sentiment and Social MediaAnalysis (WASSA), Copenhagen, Denmark.",
  "Gargi Singh, Dhanajit Brahma, Piyush Rai, andAshutosh Modi. 2023. Text-based fine-grained emo-tion prediction. IEEE Transactions on Affective Com-puting": "Tiberiu Sosea and Cornelia Caragea. 2020. Cancer-Emo: A dataset for fine-grained emotion detection.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 88928904, Online. Association for Computa-tional Linguistics. Tiberiu Sosea and Cornelia Caragea. 2021. emlm: anew pre-training objective for emotion related tasks.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 2: Short Papers), pages286293.",
  "Liang Yao, Chengsheng Mao, and Yuan Luo. 2019.Graph convolutional networks for text classification.In Proceedings of the AAAI conference on artificialintelligence, volume 33, pages 73707377": "Wenbiao Yin and Lin Shang. 2022. Efficient nearestneighbor emotion classification with bert-whitening.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages47384745. Zhitao Ying, Jiaxuan You, Christopher Morris, XiangRen, Will Hamilton, and Jure Leskovec. 2018. Hi-erarchical graph representation learning with differ-entiable pooling. Advances in neural informationprocessing systems, 31.",
  "t=1p(i)a (t) exp (|t s|)": "To prove this, we can simply swap the two sum-mation indices, s and t; due to the exchangabil-ity of the two indices and that exp (|s t|) =exp (|t s|), we can easily see the equivalence.Note that the computation in (3) can also bewritten in quadratic terms W(i) = (P(i))CP(i) considering that all the numbers are non-negative.This is computationally very efficient because allthe pairwise anchor relations can be computed withtwo matrix multiplications. However, this may notbe applicable to the computation of (4) because we have to explicitly compute the hadamard matrixK(i)ab C. This also makes our computation (of theinformation projector) very different from othermethods in the literature such as the hierarchicalpooling (Ying et al., 2018, etc.).",
  "A.2Identifying Emotion-Relevant Anchors": "When learning the semantic anchors {zks}, theyare not specifically tied to emotion classes but in-stead learned globally. After obtaining the anchors,however, we can associate each anchor to all theemotion classes so as to make post-hoc analysis.Suppose we have m sentences, each with thefeature H(i) RKd as learned by SEAN-GNN.Each sentence is also linked to a label vectory(i) R1L, with L the number of emotions.Then we can perform association analysis as fol-lows. We flatten each H(i) to a Kd-dimensionalvector, and put this vector all all the m sentencestogether as an m Kd matrix; we also put thelabel vectors together and form a m L matrix.We can then compute the correlation between thesetwo matrices and obtain an Kd L associationmatrix. We compute the absolute value of this ma-trix and compress it to a K L matrix by summingup those rows that belong to the same anchor. Thismatrix then tells the relevance between each anchorand each emotion class.",
  "A.3List of Anchors for Some Emotions": "In , we report a longer list of the anchorsthat are associated with each emotion class, bychoosing top 6 anchors for each class, (three clos-est words to each anchor for annotation), and 10different emotion classes appearing in EmpatheticDialogue. In the following, we denote each anchoras (w1, w2, w3), the three words with the closestembeddings to this anchor.",
  "BDetails on Datasets and Pre-processing": "(1) Empathetic Dialogue (Rashkin et al., 2019)consists of dialogues between a speaker and a lis-tener with 32 single emotion label. For fair compar-ison with the previous model (Chen et al., 2023),we only utilize the first turn of the dialogue. Thetraining/validation/test split of the dataset is 19,533/ 2,770 / 2,547, respectively.(2) GoEmotion (Demszky et al., 2020) is a datasetof Reddit comments where each sample is anno-tated with one or more labels from 27 emotions andneutral. Following Chen et al. (2023), we exclude",
  ": List of top-6 most relevant semantic anchors to10 emotion classes; each anchor is annotated by threewords whose embeddings are closest to it": "samples with multiple labels and the neutral label.The training/validation/test split of the remainingdataset is 23,485 / 2,956 / 2,984.(3) CancerEmo (Sosea and Caragea, 2020) com-poses of 8500 sentences sampled from an onlinecancer survivors network and label them with 8eight Plutchik basic emotions (Plutchik, 1980).(4) ISEAR (Scherer and Wallbott, 1994) includespersonal reports of emotional experiences fromdiverse cultural backgrounds. This collection com-prises 7000 sentences, which are categorized intoseven distinct emotions. The train/validation/testsplit of the dataset is 4,599 / 1,533 / 1,534.(5) GoEmotion-EK (Ekman et al., 1999) anno-tates data originally constructed by (Demszky et al.,2020) into Ekmans 6 basic emotions. FollowingYin and Shang (2022), sentences with multi la-bels and the neutral label are removed. The train-ing/validation/test split of the remaining dataset is23,485 / 2,956 / 2,984.(6) EmoInt (Mohammad and Bravo-Marquez,2017) comprises tweets of 4 emotion classes. Thetrain/validation/test split of this dataset is 3,612 /346 / 3,141.",
  "+ 3.4%+ 3.5%+ 3.1%+ 2.6%+ 1.7%+ 2.1%+ 1.2%+ 0.8%": ": Classification results (in %) using BERTbase as backbone for all methods, with weighted F1 and accuracyfor fine-grained task and Macro F1 for coarse-grained task. The best/second-best results highlighted in bold/underline. CE, IS, EK, EM stands for CancerEMO, ISEAR, GoEmotion-EK, EmoInt; numerals are the number ofclasses. represents the improvement of our model over the second-best.",
  "C.2Parameter settings of baseline models": "For baseline (1-6), we uniformly set the batch sizeto 64, the learning rate to 2e-5, use AdamW as theoptimizer, and set the weight decay to 0.01.For baseline (7-12), We select parameters fromthe following range and determine their valuesbased on performance on the validation set. Theseparameter candidates have subsumed their recom-mended parameters (if reported in their papers).The batch size is chosen from the set {4, 8, 16, 32,64}, the learning rate from {1e-5, 2e-5, 1e-4, 1e-3,1e-2}, the weight decay from {1e-5, 1e-4, 1e-3,1e-2, 1e-1, 0}, and the optimizer from Adam andAdamW.",
  "DDetails on 4 confusable subsets of ED": "The 4 subsets of Empathetic Dialogue are selectedby Suresh and Ong (2021), comprising the mostchallenging subsets identified after evaluating allpossible combinations of four labels. These sub-sets include: a: {Anxious, Apprehensive, Afraid,Terrified}, b: {Devastated, Nostalgic, Sad, Senti-mental}, c: {Angry, Ashamed, Furious, Guilty},and d: {Anticipating, Excited, Hopeful, Guilty}from the Empathetic Dialogue datasets.",
  "EComparisons with LLMs on FEC tasks": "Given the widespread application and promisingoutcomes of large language models, we further in-clude GPT-4o and Llama3-8b, two highly popularand competitive LLMs in new comparisons on 2largest fine-grained emotion classification datasetsin the paper: Empathetic Dialogue and GoEmo-tions, using the popular experimental settings asLiu et al. (2024) and the prompt template used byGao et al. (2023).Experimental results are shown in ,where ZS, FS denotes zero-shot and few-shot; ED,GE represents Empathetic Dialogue and GoEmo-tions respectively. The prompt template used forGoEmotions data is shown in .",
  ": Comparisons with GPT-4o and Llama3-8b onGE and ED datasets. The best results highlighted inbold": "As shown in , we can see that the twoLLMs perform less satisfactorily in zero / few-shot experiments on these two difficult fine-grainedemotion classification tasks. In fact, similar obser-vations were also made by other researchers (Liuet al., 2024; Kocon et al., 2023; Zhang et al., 2023).Indeed, why powerful LLMs do not excel in fine-grained emotion classification remains open andcould be related to many factors: processing andunderstanding context correctly and extracting fine-grained structured sentiment (Kocon et al., 2023),potential loss of structured emotional detail in thesentence (Liu et al., 2024; Zhang et al., 2023), etc."
}