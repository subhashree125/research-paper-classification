{
  "Abstract": "Hate speech is a complex and subjective phe-nomenon. In this paper, we present a dataset(GAZE4HATE) that provides gaze data col-lected in a hate speech annotation experiment.We study whether the gaze of an annotator pro-vides predictors of their subjective hatefulnessrating, and how gaze features can improve HateSpeech Detection (HSD). We conduct experi-ments on statistical modeling of subjective hateratings and gaze and analyze to what extentrationales derived from hate speech models cor-respond to human gaze and explanations in ourdata. Finally, we introduce MEANION, a firstgaze-integrated HSD model. Our experimentsshow that particular gaze features like dwelltime or fixation counts systematically corre-late with annotators subjective hate rating, andimprove predictions of text-only hate speechmodels.",
  "Introduction": "Hate speech is a real threat that harms individu-als, groups, and societies in a profound way. Eventhough research in NLP has developed many dif-ferent datasets and models for HSD (Poletto et al.,2021), the accurate modeling of hate speech is farfrom being solved (Ocampo et al., 2023; Rttgeret al., 2021). One of the key challenges in thisarea is that the definition and annotation of hatespeech are highly complex and subjective, depend-ing on the topic and domain of hate as well ason the individual annotators backgrounds and bi-ases (Waseem and Hovy, 2016; Abercrombie et al.,2023; ElSherief et al., 2018; Kovcs et al., 2021).This combines with the fact that state-of-the-artHSD models are typically designed as black-boxneural models that are well-known to pick up super-ficial, dataset-dependent patterns rather than learn-ing a generalizable model of the underlying task.Therefore, it is still an open question of how tohandle subjective variation in human annotationsand detection of hate speech.",
  ": Heatmaps for a human rationale, gaze fea-ture and model rationale for a hateful sentence fromGAZE4HATE": "Thispapercontributesanewdataset(GAZE4HATE) that provides gaze and anno-tations from hate speech annotators, illustratedin .We recorded the eye movementsof annotators while they read statements, whichwere carefully controlled and constructed. Thiswas followed by the annotation of hatefulness.Annotators gaze provides us with an extremelyrich signal of the subjective cognitive processesinvolved in human hate speech evaluation whilereading.In this paper, we explore whethersubjective hatefulness rating can be predicted bythe gaze of an annotator, and whether gaze featurescan be used to evaluate and improve HSD models. Generally, the NLP community has recentlystarted to leverage eye-tracking data as a means ofanalyzing the internal mechanisms in transformerlanguage models as elaborated on in .1.To the best of our knowledge, however, there is noavailable dataset of human reading of hate speech.Other work along these lines has adopted so-calledrationale annotations, where annotators mark textspans that they consider indicative of their labelingdecisions (e.g. DeYoung et al. (2020); Mathew et al.(2021)). These rationales can be used to measurethe plausibility and explainability of model deci-sions, by testing whether model-internal weights and gradients correlate with or even predict thesehuman rationales (Atanasova et al., 2020). Yet, todate, it is unclear how rational annotations com-pare to gaze signals recorded during plain read-ing for the task of hate speech classification. OurGAZE4HATE data closes this gap, as our annotatorsdid not only rate texts for hatefulness but also anno-tated token-level rationales for their ratings. shows an example that illustrates human gaze andrationales aligned with a models rationale.Our analyses and experiments center around thefollowing research questions:",
  "RQ3 Are gaze features useful for enriching LMsfor HSD?": "We address the first question by conducting sta-tistical modeling on our collected eye-tracking andannotation data (). To answer the secondquestion, we evaluate a range of existing HSD mod-els on our data, comparing models and humansrationales to human gaze (). presents the MEANION model, which integratestext-based HSD with gaze features. In sum, ourexperiments show that particular gaze features likedwell time or fixation counts systematically differwith respect to annotators subjective hate ratings.Models rationales, however, correlate more withexplicit, annotated rationales than with annotatorgaze. Finally, in some settings, adding gaze fea-tures improves predictions of text-only hate speechmodels more than human rationales do.",
  "Eyetracking Data in NLP": "In work on testing the cognitive plausibility ofattention-based transformer language models, hu-man gaze is a very relevant indicator of readerscognitive processes and a valuable source of evalu-ation data (Das et al., 2016; Malmaud et al., 2020;Sood et al., 2020; Hollenstein and Beinborn, 2021;Eberle et al., 2022; de Langis and Kang, 2023).Unfortunately, the collection of eyetracking data iscostly and existing task-specific datasets are smalland scarce (de Langis and Kang, 2023). Our workcontributes to enriching the landscape of availableNLP-tailored eyetracking datasets. Previous studies on using gaze to extend NLPmodels usually focus on a few high-level gaze fea-tures (Barrett et al., 2016; Long et al., 2019; Eberleet al., 2022), with some exceptions (Mishra et al.,2017; Hollenstein et al., 2019; Alacam et al., 2022).As one of the most commonly used group of gazefeatures in NLP, fixations measure the pause of theeye movement on an area of the visual field, andare strongly associated with visual intake (Rayner,1998; Kowler, 2011; Skaramagkas et al., 2021).However, reading hateful text also involves intenseemotions (e.g. feeling empathy, being the targetof the hate speech). Little NLP work has beendone on emotion-related eye movements such aspupil dilation, which is associated with emotionaland cognitive arousal (Bradley et al., 2008). Ourwork considers a range of gaze features and com-pares their predictive power for subjective hate rat-ings. Furthermore, gaze features are commonlypreprocessed in non-trivial ways, e.g. by aggregat-ing all token-level features or arranging them in atoken-based discretized sequence as in the above-mentioned studies. We adopt such a simple token-based preprocessing for our MEANION model, andleave exploration of more advanced architecturessuch as time series-based gaze transformers (Ala-cam et al., 2022) for future work.",
  "Explainability": "To assess whether models attend to relevant parts ofan input, various explanation and rationale extrac-tion methods have been developed, e.g., model sim-plification methods (Ribeiro et al., 2016), gradient-based techniques (Simonyan et al., 2014; Sun-dararajan et al., 2017), perturbation-based meth-ods (Zeiler and Fergus, 2013) and Shapley-basedmethods (Shapley, 1953). The work of Atanasovaet al. (2020) evaluates different methods for textclassification models, concluding that the gradient-based explanations perform best across tasks andmodel architectures. Yet, the best method highlydepends on the dataset/task, model, and diagnos-tic property used for evaluation. In this study, weevaluate a selection of explanation methods for hatespeech classification, which has not been attemptedbefore. We do so not only on human annotations ofsalient tokens (as e.g. Atanasova et al. (2020) did)but also on human gaze measurements.",
  "Since the advent of research on hate speech detec-tion (HSD), the reliable annotation of hate in texts": "has been recognized as a notorious issue (Waseem,2016; Schmidt and Wiegand, 2017). Still, HSD isoften modeled with text classifiers, trained and fine-tuned on ground-truth annotations and benchmarks(Davidson et al., 2017; Basile et al., 2019; Zampieriet al., 2019). Recent approaches and shared tasks,though, shifted the focus to specific domains ofhate such as sexism (Kirk et al., 2023) as well asexplainable HSD (Mathew et al., 2021; Pavlopou-los et al., 2022; ElSherief et al., 2021). Rttger et al.(2021) present the HateCheck benchmark, whichis composed of linguistically controlled functionaltests designed to systematically assess languageunderstanding in hate speech models. Davani et al.(2022) take some first steps in dealing with dis-agreements between annotators in HSD and com-pare the prediction of majority vote vs. individuallabels. Similarly, Wojatzki et al. (2018) comparehate speech annotations of female and male anno-tators on hateful statements about women.Furthermore, there is an emerging research thatexplores the contribution of injecting annotatorsdemographics and preferences along with the an-notated text (Kanclerz et al., 2022; Fleisig et al.,2023). The results of these studies indicate thatdemographic information is a successful predictorfor annotators ratings on the sentence-level hatespeech. Furthermore, Hoeken et al. (2024) showsthat annotators demographics are also useful forpredicting subjective annotations at the lexical leveli.e. predicting hateful words in context.Our collection of annotator gaze provides a newdirection for tackling the issues of explainabilityand subjectivity in an integrated fashion.",
  "Data and Sentence Selection": "To obtain a dataset for systematic analysis of hatespeech understanding in models, and of subjectivedifferences between annotators and their gaze, weopted for a carefully controlled set of constructeditems, similar to Rttger et al. (2021). As is com-mon in eyetracking studies in linguistics, we designour items as minimal pairs: we first collect a set of seed hateful statements. Within these statements,we manipulate specific tokens that change the hate-fulness of the statement and turn it into a neutralor even positive statement. Furthermore, we con-sider (i) items that express hate explicitly, throughdirect lexical cues, and (ii) items where the expres-sion of hate is implicit and results from the socialmeaning of the sentence as a whole. These condi-tions roughly correspond to the explicit vs. implicitderogation category in Rttger et al. (2021)s Hate-Check taxonomy. As an example, consider the hateful statementWomen can do nothing and are too stupid in . When women is replaced with minions, the state-ment is neutral towards women. When changingnothing and stupid the meaning of the statementeven turns positive. This example belongs to theexplicit condition in our design as it contains hate-ful lexical cues (e.g. stupid). The statement Womenbelong in the kitchen illustrates the implicit con-dition, as none of its words is hateful on its own.Analog to the explicit condition, minimal pairscan be constructed, i.e., when changing women topots, the statement is not hateful anymore. We conducted our in-lab experiments in Ger-many and based the construction of our annota-tion targets on the German FEMHATE (Wojatzkiet al., 2018) dataset that contains 400 statementsabout women. We have selected 20 sentences fromFEMHATE with respect to two criteria; (i) be-ing rated as hateful and (ii) allowing for minimalpair manipulation (for comparable hateful-neutral-positive triples). With minimal pair manipulations,we have created 20 positive statements and 20 neu-tral (w.r.t gender context) sentences. This subsetof 60 sentences constitutes our main manipulationinstances. To avoid participants becoming awareof these manipulations, we have introduced con-trol sentences (10 mean and 10 no-hate sentencesfrom the FEMHATE dataset, and 10 new sentencesthat express hate against men, see ). Intotal, we have collected eye movements on 90items, divided into 40 a priori hateful and 50 apriori non-hateful sentences. The selection fromthe FEMHATE dataset and controlled sentence cre-ations were conducted by one of the authors andthe stimuli set was finalized after the validation bytwo other NLP researchers. For the validation ofthe a priori-level annotations, we used the defini-tion inspired by Davidson et al. (2017) and the EU : GAZE4HATE provides annotated statements about women, divided into 3 conditions (explicit, implicit,control). Explicit and implicit examples figure in minimal pairs where words (in pink and lime ) in (a priori)hateful sentences are manipulated to obtain neutral and positive examples. Control examples are not manipulated.n = number of items in this category, P = number of participants recorded.",
  "Experimental Procedure for SubjectiveHate Speech Annotation": "Our study follows a within-subject design, i.e. allsubjects read and rate all items. Each trial consistsof two phases. In the first phase, we record annota-tors eye movements while they read the statements.In the second phase, we collect their explicit anno-tations. We ask participants to rate the statementshatefulness, to rate their confidence and to mark thewords in the statement that contribute to their ratingdecision. The order of sentences was randomizedfor each participant. Participants.43 university students (nativespeakers of German) participated in the experiment(32 female, 10 male, 1 non-binary, Mean age =23.5, SD = 5.3). They were paid or given a coursecredit to participate. The experiment took approxi-mately 40 minutes for each participant. Eyetracking Procedure.The stimuli were dis-played on an SR Eyelink 1000 Plus eye trackerintegrated into a 27 monitor with a resolution of2560 1440. We utilized a total of 94 sentences(including 4 familiarization trials). Each trial be-gan with a drift correction located to the left of thesentence onset location. Then followed the readingphase, in which the participants read the sentence",
  "at their own pace. We set a time limit of 20 sec-onds for the reading task, but the participants wereinstructed to read as quickly as possible": "Annotation Procedure.The instruction given tothe participants is detailed in Appendix A.1. Forcollecting subjective annotation, we intentionallydid not provide a strict hate speech definition to beable to get annotators interpretation of the state-ments closest to their personal stance.First, participants rated the hatefulness of thestatement in 1-to-7 Likert Scale (1:very positive,2:positive, 3:somehow positive, 4:neutral, 5:mean,6:hateful, 7:extremely hateful). Next, they ratedtheir confidence regarding their rating on a 5-Likertscale (1:not certain, 2:somewhat certain, 3:moder-ate, 4:certain, 5:very certain). Finally, they an-notated the rationale for the decision, by clickingwords in the statements that contributed most totheir rating. (top) illustrates the rationaleannotation.",
  "Overview": "GAZE4HATE provides gaze, hatefulness ratingsand rationales for 90 items and 43 participants eachsumming up to 3870 unique instances of subjectivehate ratings2. Our dataset is comparable in size toexisting eye-tracking datasets like, e.g. (de Langisand Kang, 2023). shows the average sub-jective hate ratings given by participants for a prioricategories. Some sentences were rated differentlythan their a priori labels (especially a priori pos-itive ones as neutral). The subjective ratings forsentences in other a priori categories also exhibitvariations except for the very hateful statements",
  ": Subjective hate ratings in GAZE4HATE w.r.t.annotators gender for the a priori labels": "(Appendix B.3). These mismatches between thea priori labels and our human ratings once againunderline the fact that subjectivity is one of themajor challenges in hate speech annotation. Yet,for this study, variation in the annotators ratingsis a feature rather than a bug as it allows us tostudy subjective hate speech annotations with thehelp of gaze features, which are highly participant-specific.For the following analysis, we groupsentence-based subjective hate ratings provided byusers into their hate speech labels (<=3:positive,4:neutral, >=5:hate). Train-Test Splits.Sentences from each a prioricategory were split into three groups (train, vali-dation and test) with a 70:10:20 ratio using 5-foldcross-validation. Each split has instances from eachparticipant, but not from the same sentence. Preprocessing Gaze Features.Eye movementsoften show participant-specific patterns and com-paring raw gaze features can be misleading. Wenormalized gaze features with min/max scaling foreach participant separately. The description of eachfeature and pre-processing steps are given in theAppendix A.3.",
  "Analysis of Annotators Gaze": "We start with testing whether the gaze parametersshow significant differences among the subjectivehate categories. We use Anova tests using the OLSlibrary in R on the continuous gaze features. On thecategorical gaze features, we utilized Chi-squaretests. Multiclass comparison is conducted amonghate, neutral and positively rated statements. Thebinary classification (similar to many existing hate speech classifiers) involves hate and non-hate cat-egories. The non-hate category consists of bothneutral and positive statements. For each gaze fea-ture, we checked whether there is a significant maineffect of subjective hate categories on the gaze fea-tures. presents F-scores and significancelevels of the above-mentioned statistical tests. Thefirst two columns in the table correspond to mea-surements on all tokens in the dataset, the last twocolumns on the right present the results conductedonly on the words selected as rationales.Six out of 13 features consistently show sig-nificant differences with high F-score values be-tween the subjective hate ratings for multiclass(hate, neutral, and positive) and for binary com-parisons (hate and no hate): FIXATION-COUNT,DWELL-TIME, MAX-FIX-PUPIL-SIZE, MIN-FIX- PUPIL-SIZE, AVERAGE-FIX-PUPIL-SIZE and FIRST-RUN-FIXATION-COUNT. Some features result inlow F-score values despite showing significant dif-ferences in terms of subjective hate rating. In thefollowing, we remove features that yield low F-scores or non-significant results.All features that are significant in the multiclasscondition are also significant in the binary one,but not the other way around. This indicates thatmerging neutral and positive categories has a nega-tive impact on the statistical difference. FIXATION-",
  "COUNT, DWELL-TIME and FIRST-RUN-FIXATION-": "COUNT are showing higher F-scores in the binarycomparison. Tukeys tests for pairwise compar-isons indicate that the differences in the fixationand dwell time originate from the difference be-tween the hate vs. neutral and hate vs. positiveconditions, while there is no difference betweenneutral and positive conditions. On the other hand,differences in the pupil size related parameters orig-inate from difference in neutral conditions to hateand positive conditions without showing a signif-icant difference between the latter two. This alsoconfirms the theory of pupil size being more sensi-tive to the magnitude of the emotion rather than itspolarity (Bradley et al., 2008).",
  "We tested five off-the-shelf models from Hugging-Face, which we named for reference in the remain-der of this paper deepset3, ortiz4, aluru5, rott6": "and ml67, respectively. These models are eitherGerman (G) or multilingual (M) BERT-based mod-els finetuned on one or more HSD datasets. Ratherthan aiming to outperform these models on general-purpose hate speech classification, we selectedthem as candidates to build upon our multimodalmodels. A more detailed overview of the models isgiven in and in Appendix C.1.Based on the performance results of the off-the-shelf models on our dataset (.2), we tookthe best-performing model for further finetuning.",
  "Classification results": "We evaluate all models regarding the subjectivehate ratings of all individual participants. Bothhuman and model output labels are converted to abinary classification scheme (details in inAppendix C.3). It must be emphasized that our taskis not to detect a majority-class annotation label.Instead, we aim to detect whether a sentence isperceived as hate by an individual.The F1-scores results are presented in .rott shows the best performance on detectingHATE sentences (F1 on HATE of 0.59), proba-bly due to the fact that this model is the only onethat has deliberately been trained to detect sexisthate speech. Fine-tuning this model further on theHateCheck dataset, resulted in a significant perfor-mance increase (the rott-hc model shows a macroavg. F1 of 0.68).",
  "Model rationales": "Model rationales for the best performing model (i.e.rott-hc) were generated using Captum (Kokhlikyanet al., 2020), an open source library built on Py-Torch. Based on Atanasova et al. (2020), we se-lected three methods that showed the best resultsfor Transformer-based models on a sentiment clas-sification task: (1) InputXGradient (2 aggregated),(2) Saliency (2 aggregated) and (3) Shapley value(sampling) 9.For each sentence, we extract model rationalesfor both classes, i.e. a rationale for classifying asentence as HATE and a rationale for classifyingthat same sentence as NO HATE. The extractedrationales are then converted from sub-word level(the output level that is inherent to BERT-basedmodels) to word level (aligning with the humanrationales), by averaging over multiple sub-wordvalues that constitute a single word.",
  ": Mean correlation (Pearsons r) between modelrationales, human rationales and gaze features": "For each sentence and annotator, we compare thesubjective hate rating (h), human rationale or a gazefeature (f) with a model rationale (r) with respectto class c, where c = r. We aggregate correlationvalues, each calculated as Pearsons r correlationmetric between f and r, over all sentences andannotators by taking the mean. reports mean correlation values of thehuman rationales and gaze features with the modelrationales extracted with different methods (detailsin in Appendix C.4). The six gaze featuresthat showed a significant effect on subjective hateratings () are selected for this analysis. Forall human rationales and gaze features, InputXGra-dient and Saliency rationales show substantiallyhigher correlation than Shapley Value rationales.Additionally, InputXGradient rationales, althoughless substantial, consistently show higher agree-ment than Saliency rationales. The variation inagreement among the different gaze features andhuman rationale show the same pattern for all threerationale methods. Human rationales correlate thehighest with model rationales. Among the gaze fea-tures, three features, i.e. DWELL-TIME FIXATION-",
  "rationales as bag-of-words (bow) vector ().We trained MLP classifiers using the scikit-learnlibrary10 on multimodal sentence representations(see Appendix D.3 for the training details)": "As changes in eye movement patterns are ratherlocal (e.g. fixation duration increases if the to-ken is unexpected), gaze features for some tokensmight be more informative than others for the clas-sification, and averaging over tokens might lose asignificant amount of signal. Therefore, we keptthe values of each feature for each token in the rep-resentation. We first add text features. We use Ger-man BERT-base (Chan et al., 2020) and (the fine-tuned) rott-hc model, which is the best model fromthe previous experiments.We also investigate twolarger decoder-only LLMs. We selected quantized(legacy) models from the German EM family11,namely em-LLaMA212 and em-Mistral13. The sen-tence embeddings are extracted via the LLaMA.cpptool14. We give the sentence as input to an (L)LM andextract the CLS token embeddings (dim=768 or4096). Depending on the testing configuration, weadd either gaze features (G) or rationales (R) orboth, to the sentence embeddings (E). For eachgaze feature, we create a feature vector fi that con-tains a series of token values for that feature asshown in padded to the maximum tokenlength of the sentences in GAZE4HATE (t=14). Therationales selected in each instance added as bag-of-words vector calculated using the COUNTVEC-",
  "Results": "summarizes the performance of variousfeature combinations on predicting subjective hate(binary classification as hate versus no-hate). Wereport macro-F1 and F1-scores for both hate andno-hate classes. The first row corresponds to theperformance of the model trained on only CLSembeddings (E). CLS&Gaze (EG) row providesthe highest score obtained with the inclusion of agaze feature one at a time. The third row belongs tothe CLS&Rationale (ER) model (no gaze feature).The next variation includes rationales added to theEG Model (EGR). Finally, the last two variationsinclude all gaze features (Plus). The contribution ofeach individual feature is presented in Appendix 7.For the subjective HSD, the finetuned MEANIONmodels predominantly outperform other MEANIONmodels. The injection of gaze features increasesperformance: .03 F1-score improvement using theBERT-base, .06 using the rott-hc, .02 with em-LLaMA2, and .03 using em-Mistral. The rationalescontribute more to the BERT-base MEANION (.09),slightly improve the performance of the MEANIONswith the finetuned (.03) and em-LLaMA2 mod-els (.02), and it drops the performance of the em-Mistral (.04). Except for the BERT-base model,they even hurt the performance up to .07 whencombined with gaze features. It should also behighlighted that integrating gaze and rationale fea-tures to BERT-base MEANION brings the perfor-mance closer to the text-only rott-hc MEANION.The results highlight that gaze features provide sub-stantial complementary information for subjectiveHSD and produce similar effects to fine-tuning onhate speech data.For E-only models, MEANIONs with only theem-LLaMA2 and em-Mistral embeddings (withoutfine-tuning) indicate higher performance comparedto the BERT-base MEANION. The contribution ofgaze and rationales to em-LLaMA2 embeddingsseems to be at the similar level. Furthermore, em- Mistral plus gaze embeddings are the best amongthe em-Mistral variations, and these results are sig-nificantly better than em-LLaMA2 performancesand BERT-base models. The results demonstratethat EG models outperform all other variations.These also further confirm our conclusion that gazefeatures provide complementary information forsubjective HSD, which is not represented in smalleror large LLMs.In conclusion, MEANION with the finetunedBERT, especially the gaze-integrated one, outper-forms all other variations. E-only em-LLaMA2and BERT-base models perform on a similarlevel. Among these variations, E-only em-Mistralachieves higher macro-F1, yet the finetuned (rott-hc) ones show better F1-score for the hate class.The contribution of eye movements on (L)LM onlymodels is consistently observed and statisticallyproven with our further pairwise model compar-isons using the McNemars test (see Appendix Fig-ure 11).",
  "Discussion": "Based on the above described experiments we re-visit our research questions.RQ 1: Do gaze features provide robust predic-tors for subjective hate speech annotations? Yes.According to the analysis of annotators gaze pat-terns, 6 out of 13 gaze features differ with respectto the subjective hate categories.RQ 2: How do gaze features correlate withhuman and model rationales?InputXGradientmethod seems to be more aligned with the fixation-based gaze and human rationales, which makes itmore suitable explanation method for subjectivehate ratings. But the pupil size related parametersare not correlated with model rationales, this mightmean that the signal carried by pupil size might beone of the missing components in the HSD models.More systematic analysis on the individual tokenlevel among the systematically manipulated con- ditions, which is beyond the scope of this paper,might provide valuable insight for future directions.RQ 3: Are gaze features useful for enrichingLMs for HSD? Yes. For a MEANION model all sixfeatures as well as the human rationale improve per-formance (compared to using embeddings alone).A further question arises from this conclusion: Dofeatures that correlate badly with model rationales(i.e. carrying complementary information) improvethe performance of a model enriched with thesefeatures? plots the relationship betweensubjective hate rating effects, correlation with In-putXGradient rationales, and error reduction in",
  "Conclusion": "We introduce a rich dataset of human readings ofhate speech. Our GAZE4HATE dataset is enrichedwith gaze features and subjective hatefulness rat-ings collected from 43 participants on 90 sentences(3870 unique subjective annotation instances). Wecompare subjective human hate ratings, humangaze and human rationales with hate speech mod-els rationales. By doing so, we also experimentwith various model explanation methods and com-pare their performance in aligning with human be-haviour. The human attention values (representedwith a set of gaze features and rationales) are ahighly valuable source not only for evaluating themodels, but also for training them with cognitivelyguided attention mechanisms (Ding et al., 2022;Long et al., 2019; Hollenstein et al., 2019). In ad-",
  "Acknowledgements": "The authors acknowledge financial support by theproject SAIL: SustAInable Life-cycle of Intelli-gent Socio-Technical Systems (Grant ID NW21-059A), which is funded by the program Netzw-erke 2021 of the Ministry of Culture and Scienceof the State of Northrhine Westphalia, Germany.Additionally, we would like to thank ElisabethTiemann and Maria Garcia-Abadillo Velasco fortheir valuable contribution to the annotation anddata collection phases.",
  "Limitations": "To evaluate the individual effect of the human gazeand rationale, we implement a basic solution with-out complex training schemes or multimodal fusiontechniques. Our results encourage pursuing moresophisticated implementation for modeling the hu-man gaze for classifying subjective hate speech.Because of space constraints, we could not elabo-rate on the differences between linguistic manipula-tions, which can help explain the relations betweenhuman gaze, human rationales, and model ratio-nales.There are linguistic or even non-linguistic fac-tors like (word length, word frequency, expecta-tions etc.) in our experimental set-up that influencecognitive processes. We attempt to minimize theserisks with the careful selection of minimal pairs,the random ordering of the sentences, dealing withnull values etc.It should be noted that the decoder-only modelsare trained on different objectives than BERT-basedmodels. There is a significant amount of ongoingresearch on how sentence or token embeddingsshould be extracted or how they could be inter-preted. In our paper, we do not aim to addressthese issues.Due to the controlled data collection procedureto explore the statistical robustness of differenttypes of gaze features for subjective hate speech de-tection, the experimental setup may not fully reflectreal-world scenarios of hate speech detection. Weknow that the participant pool lacks diversity, pri-marily consisting of university students. This mightraise concerns about ecological validity. Despite this limited diversity, our results indicate subjectivevariation, especially concerning specific statements,as could be seen in and in Ap-pendix B.3. Even in the same apriori category, weobserve variation in terms of averaged hatefulnessscore. Besides, the deviation for each sentence alsovaries. To address this limitations, future work willaddress extending the diversity in the participantpool (different backgrounds, cultures, languages,ages etc) and the target groups addressed in thedataset.",
  "Ethics Statement": "All recordings have been made after the signedconsent of the annotators. Participants identitiesare anonymized using pseudo-participant ID. Theshared data do not contain any cues to reveal theiridentities. The dataset contains hateful statementsabout women and men, which do not reflect theopinion of any of the authors.Hate speech is widespread in social media andcauses a lot of harm to individuals, groups, andsocieties. Therefore, we consider social media as apossible application area, where models fine-tunedwith gaze information can be used for individual-ized content moderation. Yet, our research doesnot imply that individual gaze information needsto be shared with/evaluated by social media com-panies. Eye-tracking technology, already part ofmany virtual headsets (HTC VIVE16, Apple Vi-sion17, etc.), seems to be entering our daily livesthrough our phones and laptops (e.g., Rathnayakeet al. (2023); Brousseau et al. (2020)). From an ap-plication point-of-view, incorporating users gazeinto phone applications via offline applications orthrough federated learning (by deploying a trainedmodel) that can be integrated into social media ormessaging APIs might take the privacy concernsinto account. Gavin Abercrombie, Dirk Hovy, and Vinodkumar Prab-hakaran. 2023. Temporal and second language in-fluence on intra-annotator agreement and stability inhate speech labelling. In Proceedings of the 17thLinguistic Annotation Workshop (LAW-XVII), pages96103, Toronto, Canada. Association for Computa-tional Linguistics. zge Alacam, Eugen Ruppert, Ganeshan Malhotra,Chris Biemann, and Sina Zarrie. 2022. Modelingreferential gaze in task-oriented settings of varyingreferential complexity. In Findings of the Associa-tion for Computational Linguistics: AACL-IJCNLP2022, pages 197210, Online only. Association forComputational Linguistics.",
  "Sai Saketh Aluru, Binny Mathew, Punyajoy Saha, andAnimesh Mukherjee. 2020.Deep learning mod-els for multilingual hate speech detection. CoRR,abs/2004.06465": "Dennis Assenmacher, Marco Niemann, Kilian Mller,Moritz Seiler, Dennis Riehle, Heike Trautmann,and Heike Trautmann. 2021. Rp-mod & rp-crowd:Moderator- and crowd-annotated german news com-ment datasets. In Proceedings of the Neural Infor-mation Processing Systems Track on Datasets andBenchmarks, volume 1. Curran. Pepa Atanasova, Jakob Grue Simonsen, Christina Li-oma, and Isabelle Augenstein. 2020. A diagnosticstudy of explainability techniques for text classifi-cation. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 32563274, Online. Association forComputational Linguistics. Maria Barrett, Joachim Bingel, Frank Keller, and An-ders Sgaard. 2016.Weakly supervised part-of-speech tagging using eye-tracking data. In Proceed-ings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 579584. Valerio Basile, Cristina Bosco, Elisabetta Fersini,Debora Nozza, Viviana Patti, Francisco ManuelRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.2019. SemEval-2019 task 5: Multilingual detectionof hate speech against immigrants and women inTwitter. In Proceedings of the 13th InternationalWorkshop on Semantic Evaluation, pages 5463, Min-neapolis, Minnesota, USA. Association for Compu-tational Linguistics.",
  "Branden Chan, Stefan Schweter, and Timo Mller. 2020": "Germans next language model. In Proceedings ofthe 28th International Conference on ComputationalLinguistics, pages 67886796, Barcelona, Spain (On-line). International Committee on Computational Lin-guistics. Abhishek Das, Harsh Agrawal, Larry Zitnick, DeviParikh, and Dhruv Batra. 2016. Human attentionin visual question answering: Do humans and deepnetworks look at the same regions? In Proceedingsof the 2016 Conference on Empirical Methods in Nat-ural Language Processing, pages 932937, Austin,Texas. Association for Computational Linguistics. Aida Mostafazadeh Davani, Mark Daz, and Vinodku-mar Prabhakaran. 2022. Dealing with disagreements:Looking beyond the majority vote in subjective an-notations. Transactions of the Association for Com-putational Linguistics, 10:92110.",
  "Thomas Davidson, Dana Warmsley, Michael W. Macy,and Ingmar Weber. 2017. Automated hate speechdetection and the problem of offensive language. InInternational Conference on Web and Social Media": "Karin de Langis and Dongyeop Kang. 2023. A com-parative study on textual saliency of styles from eyetracking, annotations, and language models. In Pro-ceedings of the 27th Conference on ComputationalNatural Language Learning (CoNLL), pages 108121, Singapore. Association for Computational Lin-guistics. Christoph Demus, Jonas Pitz, Mina Schtz, NadineProbol, Melanie Siegel, and Dirk Labudde. 2022.Detox: A comprehensive dataset for German offen-sive language and conversation analysis. In Proceed-ings of the Sixth Workshop on Online Abuse andHarms (WOAH), pages 143153, Seattle, Washington(Hybrid). Association for Computational Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,Eric Lehman, Caiming Xiong, Richard Socher, andByron C. Wallace. 2020. ERASER: A benchmark toevaluate rationalized NLP models. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 44434458, Online.Association for Computational Linguistics. Xiao Ding, Bowen Chen, Li Du, Bing Qin, and TingLiu. 2022. Cogbert: Cognition-guided pre-trainedlanguage models. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 32103225. Oliver Eberle, Stephanie Brandl, Jonas Pilot, and An-ders Sgaard. 2022. Do transformer models showsimilar attention patterns to task-specific humangaze?In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 42954309, Dublin,Ireland. Association for Computational Linguistics. Mai ElSherief, Vivek Kulkarni, Dana Nguyen, WilliamYang Wang, and Elizabeth Belding. 2018. Hate lingo:A target-based linguistic analysis of hate speech insocial media. Proceedings of the International AAAIConference on Web and Social Media, 12(1).",
  "Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish-navi Anupindi, Jordyn Seybolt, Munmun De Choud-hury, and Diyi Yang. 2021. Latent hatred: A bench-": "mark for understanding implicit hate speech. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 345363,Online and Punta Cana, Dominican Republic. Asso-ciation for Computational Linguistics. Eve Fleisig, Rediet Abebe, and Dan Klein. 2023. Whenthe majority is wrong: Modeling annotator disagree-ment for subjective tasks. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 67156726, Singapore. As-sociation for Computational Linguistics. Sanne Hoeken, Sina Zarriess, and \"Ozge Alacam. 2024.Hateful word in context classification. In Proceed-ings of the 2024 Conference on Empirical Methods inNatural Language Processing, Miami, Florida, USA.Association for Computational Linguistics.",
  "Nora Hollenstein, Maria Barrett, Marius Troendle,Francesco Bigiolli, Nicolas Langer, and Ce Zhang.2019. Advancing nlp with cognitive language pro-cessing signals. arXiv preprint arXiv:1904.02682": "Nora Hollenstein and Lisa Beinborn. 2021. Relativeimportance in sentence processing. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 2: Short Papers), pages 141150, Online.Association for Computational Linguistics. Kamil Kanclerz, Marcin Gruza, Konrad Karanowski,Julita Bielaniewicz, Piotr Milkowski, Jan Kocon, andPrzemyslaw Kazienko. 2022. What if ground truthis subjective? personalized deep neural hate speechdetection. In Proceedings of the 1st Workshop on Per-spectivist Approaches to NLP @LREC2022, pages3745, Marseille, France. European Language Re-sources Association. Hannah Kirk, Wenjie Yin, Bertie Vidgen, and PaulRttger. 2023. SemEval-2023 task 10: Explainabledetection of online sexism. In Proceedings of the17th International Workshop on Semantic Evaluation(SemEval-2023), pages 21932210, Toronto, Canada.Association for Computational Linguistics. Narine Kokhlikyan, Vivek Miglani, Miguel Martin,Edward Wang, Bilal Alsallakh, Jonathan Reynolds,Alexander Melnikov, Natalia Kliushkina, CarlosAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.Captum: A unified and generic model interpretabilitylibrary for pytorch. CoRR, abs/2009.07896.",
  "Eileen Kowler. 2011. Eye movements: The past 25years. Vision research, 51(13):14571483": "Yunfei Long, Rong Xiang, Qin Lu, Chu-Ren Huang, andMinglei Li. 2019. Improving attention model basedon cognition grounded data for sentiment analysis.IEEE transactions on affective computing, 12(4):900912. Jonathan Malmaud, Roger Levy, and Yevgeni Berzak.2020. Bridging information-seeking human gaze andmachine reading comprehension. In Proceedings ofthe 24th Conference on Computational Natural Lan-guage Learning, pages 142152, Online. Associationfor Computational Linguistics. Thomas Mandl, Sandip Modha, Prasenjit Majumder,Daksh Patel, Mohana Dave, Chintak Mandalia, andAditya Patel. 2019. Overview of the HASOC track atFIRE 2019: Hate speech and offensive content iden-tification in indo-european languages. In FIRE 19:Forum for Information Retrieval Evaluation, Kolkata,India, December, 2019, pages 1417. ACM. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,Chris Biemann, Pawan Goyal, and Animesh Mukher-jee. 2021.Hatexplain: A benchmark dataset forexplainable hate speech detection.Proceedingsof the AAAI Conference on Artificial Intelligence,35(17):1486714875. Abhijit Mishra, Kuntal Dey, and Pushpak Bhattacharyya.2017. Learning cognitive features from gaze data forsentiment and sarcasm classification using convo-lutional neural network. In Proceedings of the 55thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 377387. Nicols Benjamn Ocampo, Ekaterina Sviridova, ElenaCabrio, and Serena Villata. 2023. An in-depth analy-sis of implicit and subtle hate speech messages. InProceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 19972013, Dubrovnik, Croatia. As-sociation for Computational Linguistics. John Pavlopoulos, Leo Laugier, Alexandros Xenos, Jef-frey Sorensen, and Ion Androutsopoulos. 2022. Fromthe detection of toxic spans in online discussions tothe analysis of toxic-to-civil transfer. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 37213734, Dublin, Ireland. Association forComputational Linguistics. Fabio Poletto, Valerio Basile, Manuela Sanguinetti,Cristina Bosco, and Viviana Patti. 2021. Resourcesand benchmark corpora for hate speech detection: asystematic review. Lang Resources & Evaluation,55:477523. Rasanjalee Rathnayake, Nimantha Madhushan, Ash-mini Jeeva, Dhanushika Darshani, Akila Subasinghe,Bhagya Nathali Silva, Lakshitha Wijesingha, andUdaya Wijenayake. 2023. Current trends in humanpupil localization: A review. IEEE Access, 11.",
  "on Knowledge Discovery and Data Mining, KDD 16,page 11351144, New York, NY, USA. Associationfor Computing Machinery": "Julian Risch, Anke Stoll, Lena Wilms, and MichaelWiegand, editors. 2021.Proceedings of the Ger-mEval 2021 Shared Task on the Identification of Toxic,Engaging, and Fact-Claiming Comments. Associa-tion for Computational Linguistics, Duesseldorf, Ger-many. Bjrn Ross, Michael Rist, Guillermo Carbonell, Ben-jamin Cabrera, Nils Kurowsky, and Michael Wojatzki.2017. Measuring the reliability of hate speech an-notations: The case of the european refugee crisis.CoRR, abs/1701.08118. Paul Rttger, Bertie Vidgen, Dong Nguyen, ZeerakWaseem, Helen Margetts, and Janet Pierrehumbert.2021. HateCheck: Functional tests for hate speechdetection models. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 4158, Online. Association forComputational Linguistics. Anna Schmidt and Michael Wiegand. 2017. A surveyon hate speech detection using natural language pro-cessing. In Proceedings of the Fifth InternationalWorkshop on Natural Language Processing for So-cial Media, pages 110, Valencia, Spain. Associationfor Computational Linguistics.",
  "Lloyd S Shapley. 1953. A value for n-person games": "Karen Simonyan, Andrea Vedaldi, and Andrew Zis-serman. 2014. Deep inside convolutional networks:Visualising image classification models and saliencymaps. In Workshop at International Conference onLearning Representations. Vasileios Skaramagkas, Giorgos Giannakakis, Em-manouil Ktistakis, Dimitris Manousos, IoannisKaratzanis, Nikolaos S Tachos, Evanthia Tripoliti,Kostas Marias, Dimitrios I Fotiadis, and ManolisTsiknakis. 2021. Review of eye tracking metrics in-volved in emotional and cognitive processes. IEEEReviews in Biomedical Engineering, 16:260277. Ekta Sood, Simon Tannert, Diego Frassinelli, AndreasBulling, and Ngoc Thang Vu. 2020. Interpretingattention models with human visual attention in ma-chine reading comprehension. In Proceedings ofthe 24th Conference on Computational Natural Lan-guage Learning, pages 1225, Online. Associationfor Computational Linguistics.",
  "Zeerak Waseem and Dirk Hovy. 2016. Hateful symbolsor hateful people? predictive features for hate speechdetection on twitter. In Proceedings of the NAACLstudent research workshop, pages 8893": "Michael Wiegand, Melanie Siegel, and Josef Ruppen-hofer. 2019. Overview of the germeval 2018 sharedtask on the identification of offensive language. Pro-ceedings of GermEval 2018, 14th Conference onNatural Language Processing (KONVENS 2018), Vi-enna, Austria September 21, 2018, pages 1 10.Austrian Academy of Sciences, Vienna, Austria. Michael Wojatzki, Tobias Horsmann, Darina Gold, andTorsten Zesch. 2018. Do women perceive hate dif-ferently: Examining the relationship between hatespeech, gender, and agreement judgments. In Pro-ceedings of the 14th Conference on Natural Lan-guage Processing (KONVENS 2018). Marcos Zampieri, Shervin Malmasi, Preslav Nakov,Sara Rosenthal, Noura Farra, and Ritesh Kumar.2019. SemEval-2019 task 6: Identifying and cat-egorizing offensive language in social media (Of-fensEval). In Proceedings of the 13th InternationalWorkshop on Semantic Evaluation, pages 7586, Min-neapolis, Minnesota, USA. Association for Compu-tational Linguistics.",
  "A.1Instructions for the Annotators": "The experimental instructions were given in writ-ten format in German. After the instructions, theparticipants completed 4 familiarization trials. Be-fore starting with the main experiments, we makesure that they do not have any further questionsregarding the task. The following text correspondsto the translated instructions: During this experimental session, you will be pre-sented with 90 sentences. While some sentenceshave highly positive sentiments, some of them arehateful. There are also sentences that are neitherpositive nor hateful. For the current study, wedefine hate speech as expressions that carry a verynegative stance (in terms of their intent). Pleasealways keep this definition in mind and annotatethe sentences carefully. One trial consists of (i)reading a sentence, (ii) evaluating its hatefulness,(iii) evaluating your confidence in this decision,and finally, (iv) highlighting the parts of the sen-tence that contribute to its hateful meaning (ifany).",
  "Step-3: You will be asked to evaluate your cer-tainty/confidence while giving this score": "Step-4: In this final step, each word in the sen-tence is shown in a bounding box. Please click onthe words that contribute to your decision. Youcan have multiple selections. The boxes will behighlighted when you click them or hover themwith your mouse during a press. To unselect a boxor a series of boxes, you can click on them again.Feel free to try the annotation tool out during thefamiliarization period.",
  "SKIP (categorical): An interest area is con-sidered skipped (i.e., SKIP = 1) if no fixationoccurred in first-pass reading": "In addition to the participant-specific gaze nor-malization, the data needs to be preprocessed con-cerning missing values, which are not uncommonin gaze data. For example, if a participant skipsa word during reading or a blink is detected, therespective data point is null. If all token values fora gaze feature are null, the trial is removed from thedataset, otherwise, null values are replaced with ei-ther zero (if it is skipped) or the average (if a blinkis detected).",
  "B.1A Closer look at the manipulated tokensand rationales": "A Chi-square test has been conducted to see thedifference on rationale selections among subjectivehate categories. It revealed a significant main effect(2(1) = 110.49, p < .001). shows the distribution of rationales,manipulated words and other tokens in the entiredataset. Since manipulated tokens occur only in theminimal pair conditions (see 3), their frequency isoverall lower compared to rationales and other to-kens. The ratio of rationales to all tokens is similaramong the subjective hate categories (hate: 32.9%),neutral: 29.1%, positive: 33.49). On the other hand,the ratio of the tokens that are both manipulated andselected is higher in hate category (13.0%) com-pared to neutral (8.13%) and positive categories(8.33%). A detailed look on the interaction be-tween these two token types are beyond the scopeof this paper, here we will provide a glimpse of abigger analysis.Manipulated words (parts of minimal word pairs)are the markers that change the hatefulness of the statement. As an example, for the following sen-tences, Women belong in the kitchen and Potsbelong in the kitchen, women and pots arethe minimal pairs, which are manipulated. For theformer case, this manipulated token is selected asrationale, in the latter, not. Since (i) the annotators consistently selectedmore words in their rationales than only the wordwe manipulated, and (ii) they select rationales forthe positive statements too, the selection of a wordfor a rationale is not always an indication of hate,but also of general importance for the annotationdecision. We conducted further Anova tests to checkwhether the gaze features differ on words being ma-nipulated and /or selected for the rationale from theminimal pair conditions. shows statisticalsignificance levels of the Anova tests in multiclassand binary comparisons. The gaze measurementson the rationales differ among the subjective hatecategories. But when it comes to tokens which aremanipulated but not selected (e.g. pots as in theexample above), while fixation-based parametersstill show significance difference, only pupil sizerelated parameters do not differ, this might tell thatpupil size parameters might be more sensitive atthe token level while fixation-based parameters aremore in line with the overall sentence stance.",
  "B.3Subjective Hatefulness Ratings persentence and condition": "The following illustrates the variation forindividual sentences with the a priori labels (x-axis)and their average subjective hate ratings (y-axis),with the confidence intervals representing the vari-ation in the ratings.We initially created minimal triples (hate, neu-tral, positive apriori). These are aligned in Fig-ure 8. For example, the first bars in each subfigureare variations of the first hateful sentence (1:hate,52:neutral, 73:positive), and the second bars in eachsubfigure belong to the same minimal pair condi-tions (2, 53, 74) and go like that. The error barsindicate a general agreement between the annota-tors.These graphs show that there are several state-ments which were rated differently and the rangeof the confidence intervals vary substantially.In the high hate against women (c1) condi-tion (a), they are all evaluated as hateful,while the sentences [3: Frauen haben im statistis-chen Mittel ein kleineres Allgemeinwissen.] and[15:Eine klassische Rollenverteilung in der Ehe istzu bevorzugen.] are at the lower band.In the neutral (in terms of offensiveness to-wards gender) statements (b), they areoverall around the middle band. The sentences[59: Minions knnen nichts und sind zu dumm.]and [61:Die #keinurlaub-Bewegung ist hyster-isch.] are evaluated as hateful on average, while[64: Wissenschaftler sind berwiegend angetriebendurch Neugierde.]and [68:Gleichstellung der",
  ": Subjective hate ratings per experimental con-dition and stimulus (MC: Manipulated conditions)": "Geschlechter findet zunehmend hufiger statt.] aspositive.In the positive towards women condition (Fig-ure 8c), the evaluation shows mixed, but generallyneutral or positive ratings with the exceptions [78:Frauen sind Mnnern im Erinnerungsvermgenberlegen.] [83: Frauen sollten nicht zu Hausebleiben und sich um ihre Karriere kmmern].On the other hand, other conditions which areincluded as control conditions also display interest-ing tendencies. (a) is directly taken fromthe subset of FEMHATE dataset, namely \"mediumhate against women\". Our participants mostly con-sider these statements in either mean or neutral",
  "C.1Details of Huggingface Models": "DeepsetDeepset Model is finetuned on Ger-mEval18 (coarse and fine) (Wiegand et al., 2019),collected from Twitter data. GermEval18(Coarse)requires a system to classify a tweet into one oftwo classes: OFFENSE if the tweet contains someform of offensive language , and OTHER if it doesnot.For this dataset, similar to our study, thetarget groups are not explicitly mentioned in thehate speech definition. The author uses the follow- ing definition: In the case of PROFANITY, pro-fane words are used. However, the tweet does notwant to insult anyone. In the case of INSULT, un-like PROFANITY,the tweet clearly wants to offendsomeone. In the case of ABUSE, the tweet does notjust insult a person but represents the stronger formof abusive language ascribing a social identity toa person that is judged negatively by a (perceived)majority of society. All these categories weretreated in one category in GermEval18 (Coarse)dataset. This model that makes binary classifica-tion on broader terms of hate speech aligns with ourcontent as well, yet the inclusion/ratio of gender-related hate in the training data is not known. OrtizThe model Ortiz is a fine-tuned version ofbert-base-german-cased using the HASOC dataset(Mandl et al., 2019) to detect hate speech, specifi-cally in the German language. It has binary class ashate versus no hate, which aligns with our binaryclassification. Hate speech is defined as Describ-ing negative attributes or deficiencies to groups ofindividuals because they are members of a group(e.g. all poor people are stupid). Hateful commenttoward groups because of race, political opinion,sexual orientation, gender,social status, health con-dition or similar. Although gender is not directlymentioned as target group in the hate speech def-inition, the definition itself looks inclusive. Theinclusion/ratio of gender-related hate in the train-ing data is also not known.",
  "ALURUHate-Speech-CNERG (Aluru et al.,": "2020), another well-known hate speech model, isfine-tuned on the multilingual BERT model. Theyuse two labels, hate speech and normal, and dis-card other labels like (offensive, profanity, abusive,insult, etc.). For German, the model is trained on(Ross et al., 2017; Bretschneider and Peters, 2017)datasets. Both German datasets carry hate speechagainst foreigners. As definition, Ross et al. (2017)dataset uses the Twitter rule as You may not pro-mote violence against or directly attack or threatenother people on the basis of race,ethnicity, nationalorigin, sexual orientation, gender, gender identity,religious affiliation, age, disability,or disease. Wealso do not allow accounts whose primary purposeis inciting harm towards others onthe basis of thesecategories. The Bretschneider and Peters (2017)dataset contains sentences against the governmentrepresented by political parties and politicians, thepress and media, other identifiable targets, and un-known targets. Yet, gender-related hate speech is",
  "Forward_reg_count0.5880.5700.6290.6040.6810.6680.6320.5550.5900.5480.5600.5040.6440.5660.5970.571": "still part of the training data represented in otherlanguages. This dataset is different in terms of datacollection; they use seed words to scrap data fromFacebook; and the collected data has been anno-tated by two experts as slightly offensive to offen-sive, explicit to substantial offensive statementsand none of these conditions. To conclude, thismodel is trained on datasets with different annota-tion styles and labels contributing to its diversity. Rott: It is a fine-tuned model on three datasets:RP (Assenmacher et al., 2021) and DeTox (Demuset al., 2022). The details of the third dataset, whichis the Twitter dataset (Glasenbach, 2022) are unfor-tunately missing in the huggingface model card. Itperforms a multi-class classification of hate speech.The classes are No Hate Speech, Other Hate Speech(Threat, Insult, Profanity), Political Hate Speech,Racist Hate Speech and Sexist Hate Speech. For theAssenmacher et al. (2021) dataset, the definitionsvary with respect to the type of hate/abusive speechas follows: (i) Attacks on people based on theirgender (identity), often with a focus on women,(ii) Attacks on people based on their origin, ethnic-ity, nation , (iii) Announcements of the violationof the physical integrity of the victim, (iv) Deni-grating, insolvent, or contemptuous statements, (v)Usage of sexually explicit and inappropriate lan-guage, (vi) Organisational content, such as requestson why specific posts have been blocked and finally(vii) Comments advertising unrelated services orproducts. This dataset does not always includetargets in their definition as well. On the otherhand, another dataset used in the finetuning of Rott,DETOX has a stricter definition scheme. It distin-guishes between toxic comments and hate speech.Toxicity indicates the potential of a comment topoison a conversation. The more it encouragesaggressive responses or triggers other participantsto leave the conversation, the more toxic the com-ment is. On the other hand, hate speech is defined as any form of expression that attacks or dispar-ages persons or groups by characteristics attributedto the groups. Discriminatory statements can beaimed at, for example, political attitudes, religiousaffiliation, or sexual identity of the victims. Wesubsumed the predictions on our dataset into twoas no hate speech versus others (as hate). ml6: German DistilBERT model fine-tuned ona combination of five German datasets containingtoxicity, profanity, offensive, or hate speech. Alllabels were subsumed to either toxic or non-toxic.(i) GermEval18 (labels: abuse, profanity, toxic-ity). (ii) GermEval21 (Labels: toxic or not). Thetoxic comments contain Screaming - Implyingvolume by using all-caps at least twice, Vulgarlanguage Use of obscene, foul or boorish lan-guage, Insults Swear words and derogatorystatements, Sarcasm -Ruthless, biting mockeryand Discrimination Disparaging remarks aboutentire groups with sweeping condemnation, Dis-crediting Attempt to undermine the credibilityof persons, groups or ideas, or deny their trust-worthiness and finally Accusation of lying In-sinuation that ideas, plans,actions or policies aredishonest, subterfuge and misleading. The thirddataset is Ross et al. (2017) dataset as mentionedabove. The fourth one is Bretschneider and Pe-ters (2017) as mentioned above. The final one isthe HASOC 2019 (listed above). This dataset alsoaligns with our binary classification on a wide spec-trum. Yet the inclusion/ratio of gender-related hatein the training data is also not known.To sum up, in the fine-tuning of these existinghuggingface models, their authors seem to embracea variety in hate speech definitions and class labels.The wide range of the spectrum (offensive, abusive,toxic, etc.) utilized in the selected datasets for fine-tuning them also aligns with our wide spectrum.Furthermore, Rott is explicitly fine-tuned on sex-ism; this also explains its out-of-the-box best per- formance. Therefore, we continue with this modelfor further fine-tuning on the HateCheck Datasetand use the Hate-check further fine-tuned versionwith multimodal integration. The base models areintegrated into our model in a plug-and-play fash-ion, which makes the extension to include othermodels straightforward.",
  "C.2Finetuning Details of rott-hc": "We finetuned the rott model (see ) on theGerman HateCheck corpus18 (Rttger et al., 2021).For finetuning, we used 80% for training and 20%as development set (for evaluation over differentepochs). We finetuned the model for 3 epochs witha batch size of 8, running just on a Macbook ProsCPU. Other details: implementation with pytorchand transformers libraries, AdamW optimizer fortraining with learning rate of 5e-5 (and all otherdefault hyperfeatures), applying linear schedulerwith 0 warmup steps.",
  ": Accuracy scores for all model variations": "base_E base_EG base_ER base_EGR finetuned_E finetuned_EG finetuned_ER finetuned_EGR llama_E llama_EG llama_ER llama_EGR mistral_E mistral_EG mistral_ER mistral_EGR base_E base_EG base_ER base_EGR finetuned_E finetuned_EG finetuned_ER finetuned_EGR llama_E llama_EG llama_ER llama_EGR mistral_E mistral_EG mistral_ER mistral_EGR : Pairwise Model Comparisons using McNe-mars Statistics (only significant differences are visu-alized, the color denotes the chi-squared value. Thedarker value means higher Chi-squared value, meaninga bigger significant difference.)",
  "D.1Position-based and BOW RationaleRepresentation": "illustrates the effect of different ratio-nale representations combined with various LMand gaze embeddings on the HSD classification.Asseen from the graph, for the BERT-based models,adding rationales as bag-of-words representationresults in higher performance, while for LLMs,we observe the opposite trend, this might indicatethat semantic information regarding those words se-lected as rationales were already represented by theCLS embedding, highlighting the position of therationales in combination with gaze informationbring forth more complementary information.D.2Implicit versus Explicit Hate Speech",
  ": Model performance w.r.t. linguistic types": "that for the instances rated as hateful, the modelsperform better on the sentences where hatefulnessis based on lexical cues (F1-score of 0.68 for rott-hc) rather than on implicit knowledge (F1-score of0.65 for rott-hc). For the instances rated as non-hateful, it seems to be the other way around (F1-score of 0.76 for implicit, 0.63 for explicit cues).We further plotted the accuracy scores in Fig-ure 13 (i) to understand the models capabilitiesto detect explicit and implicit hate speech and (ii)to explore the effect of gaze and rationales on thisdistinction. Among the base models (BERT, em-LLaMA2 and em-Mistral), the performance dif-ference between hate (red lines) and no-hate (bluelines) classes with BERT and Mistral-based modelsare pretty clear. Overall patterns indicates the im-plicit no hate is the easier to classify, while implicithate is the most challenging case as expected.",
  "For each LLM model and feature configuration, weconducted grid search using sklearn. Later, eachconfiguration is trained with its best hyperparame-ters (": "parameter_space = { h i d d e n _ l a y e r _ s i z e s :[ ( 6 4 ,32) ,(128 ,64) ,(128 ,64 ,32) ,(256 ,100) ,(256 ,100 ,3 2 ) ] , a c t i v a t i o n :[ tanh , relu ] , solver :[ sgd ,adam ] , alpha :[0.0001 ,0.0005 ,0.001 ,0.005 ,0 . 0 1 ] ,#, l e a r n i n g _ r a t e :[ constant , adaptive ] ,"
}