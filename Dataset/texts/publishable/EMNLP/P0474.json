{
  "Abstract": "Large language models (LLMs) have shownpromisingabilitiesascost-effectiveandreference-free evaluators for assessing lan-guage generation quality. In particular, pair-wise LLM evaluators, which compare two gen-erated texts and determine the preferred one,have been employed in a wide range of appli-cations. However, LLMs exhibit preference bi-ases and worrying sensitivity to prompt designs.In this work, we first reveal that the predictivepreference of LLMs can be highly brittle andskewed, even with semantically equivalent in-structions. We find that fairer predictive pref-erences from LLMs consistently lead to judg-ments that are better aligned with humans. Mo-tivated by this phenomenon, we propose an au-tomatic Zero-shot Evaluation-oriented PromptOptimization framework, ZEPO, which aimsto produce fairer preference decisions and im-prove the alignment of LLM evaluators with hu-man judgments. To this end, we propose a zero-shot learning objective based on the preferencedecision fairness. ZEPO demonstrates sub-stantial performance improvements over state-of-the-art LLM evaluators, without requiringlabeled data, on representative meta-evaluationbenchmarks. Our findings underscore the criti-cal correlation between preference fairness andhuman alignment, positioning ZEPO as an effi-cient prompt optimizer for bridging the gap be-tween LLM evaluators and human judgments.",
  "Evaluator": "Which summary candidate has better coherence?If the candidate A is better, please return 'A'. If thecandidate B is better, please return 'B'. Which one exhibits better coherence? Return'A' for the rst summary or 'B' for the second.Only provide the letter of your choice. : Illustration of the ZEPO pipeline. Given amanual prompt, the distribution of LLM preferences canbe biased towards a certain class. ZEPO optimizes theprompt on a zero-shot fairness learning objective untilthe balance is achieved in the distribution. Owing to the remarkable in-context learning ca-pabilities of LLMs (Brown et al., 2020), promptingtechniques further enable versatile use of LLM eval-uators with user-defined evaluation criteria, wherepairwise-preference-based evaluators have so fardemonstrated superior human alignment to directscoring (Liusie et al., 2024; Liu et al., 2024b). However, LLMs have been known to exhibitpreference bias (Wang et al., 2023), a priori propen-sity to predict certain classes over others unfairly,and display strong sensitivity to the actual promptsdescribing evaluation criteria (Zhou et al., 2023a;Sclar et al., 2024). The preference bias is argued tobe largely due to various factors that result in a labeldistribution shift, such as position bias (Zheng et al.,2024b), verbosity bias (Saito et al., 2023), and con-textual bias (Zhou et al., 2024a), where LLMs un-fairly favor later and longer answers, or even followrepetitive answers in their demonstrations. We arethus motivated to explore the impact of preferencebiases on human alignment in the novel context ofLLM evaluators. We start by conducting a system-atic study examining the sensitivity of LLM evalu-ators to the provided instructions. By paraphrasing mistralllama 0.1 0.2 0.3 0.4 Spearman correlation 0.20.40.6 Decision rate 0.0 0.1 0.2 0.3 0.4 mistral llama fair preference",
  ": LLM evaluators show strong sensitivity to in-structions and fairer preference leads to better human-aligned LLM judgments. Sensitivity and evaluation per-formance studies on preference fairness": "from a set of instructions, we find that the pair-wise preference of LLMs largely varies even withsemantically equivalent instructions, and differentinstructions exert different degrees of preferencebiases. Noticeably, we observe that fairer prefer-ences consistently lead to better human-alignedjudgments. Motivated by this empirical finding, wethen propose an automatic Zero-shot Evaluation-oriented Prompt Optimization (ZEPO) frameworkfor steering LLM evaluators towards better agree-ments with humans; see . We design a newzero-shot fairness objective function by measuringthe absolute difference between a uniform priordistribution and the model preference distribution.ZEPO, without any labeled data, shows substan-tial performance gains over state-of-the-art LLMevaluators with manually designed instructions onmeta-evaluation benchmarks.In sum, we provide the following contributions.1) We present a systematic analysis that reveals thestrong sensitivity of LLM evaluators to instructions.Importantly, we find that fairer preferences elicitbetter human-aligned LLM judgments. 2) We in-troduce a Zero-shot Evaluation-oriented PromptOptimization framework (ZEPO) for automaticallyoptimizing LLM evaluators toward better humanagreements without any labeled data. 3) We demon-strate that ZEPO efficiently discovers the fairestinstruction for LLM evaluators, delivering substan-tial gains in evaluation over representative tasks.",
  "Related Work": "LLMs as Evaluators. LLMs have been widelyused to evaluate natural language generation tasks(Zhong et al., 2022; Chiang and Lee, 2023), en-abling automatic and reference-free evaluations(Liu et al., 2023; Fu et al., 2023; Chen et al.,2023b; Dong et al., 2024). Recent studies show thatLLM evaluators can serve as effective pairwise text rankers (Qin et al., 2023), where pairwise compar-isons lead to better human-aligned judgments thanLikert-score evaluations (Liusie et al., 2024; Liuet al., 2024b). Yet, there is still a prominent gap be-tween LLM evaluators and human agreement (Shenet al., 2023). LLM evaluators are yet sensitive toexemplars (Wang et al., 2023) and exhibit unfairpredictions due to position bias, verbosity bias, andself-preferences (Zheng et al., 2024b; Pezeshkpourand Hruschka, 2023; Panickssery et al., 2024; Liuet al., 2024a). Calibration methods have been pro-posed to alleviate biases (Li et al., 2023b,a; Zhouet al., 2024a), but are yet insufficient for addressingall aforementioned biases. In this work, we showthat instructions exert large impacts on LLM eval-uators, and searching for instructions with fairerpreferences is a necessary and critical componentin LLM-based evaluators. Automatic Prompt Optimization. Unlike softprompt tuning that requires white box access tomodel parameters (Lester et al., 2021; Zhou et al.,2024b), hard prompt tuning directly searches fordiscrete prompts that are portable and black box(Deng et al., 2022; Zhou et al., 2023a). Recentprompt optimization work further leverages LLMsas optimizers to generate more human interpretableprompts (Zhou et al., 2023b; Yang et al., 2024).Much effort has been devoted to more advancedsearch algorithms (Pryzant et al., 2023; Guo et al.,2024; Khattab et al., 2024; Wan et al., 2024; Liuet al., 2024c) but they heavily rely on labeled data.Instead, zero-shot prompt optimization is a ratherunderexplored research area, and previous work ismostly limited to entropy-based exemplar selection(Lu et al., 2022) or relies on model-synthesizeddata (Chen et al., 2023a). We explore the extreme,zero-shot learning setup and leverage LLMs self-predictive distribution to optimize toward fairerpreferences. As we will show, our fairness objec-tive shows the best correlation and outweighs otherzero-shot metrics for LLM evaluators in .",
  "Fairer Preferences Elicit ImprovedHuman-Aligned Judgments": "Prompt Sensitivity and Bias. We start by ana-lyzing the sensitivity of LLM evaluators to vari-ations in instructions.Formally, given somesource text and corresponding response candidatesas an input query xi, we have the predicted la-bel yi as the model preference.Evaluation in-struction I is formulated with the input query xi in a prompt template to form a complete con-text C(xi, I) = Template(xi, I) for evaluation.LLM evaluators then make predictions by yi =arg maxyY p(y|Ci), where the verbalizer Y de-fines the set of preferences (i.e., A or B for pair-wise preferences). To inspect prompt sensitivity,we leverage GPT-3.5 (OpenAI, 2023) to gener-ate a set of semantically equivalent instructionsI = {I1, ..., IM} by paraphrasing from an ini-tial instruction I1. In , we observe a severefluctuation in human agreement scores by prompt-ing Llama-3 8B (Touvron et al., 2023) model withCImI(x, Im). This reflects a high prompt sensitiv-ity and poor robustness of standard LLM evaluators.The observation aligns with previous research inposition biases (Zhao et al., 2021), and LLMs aresensitive to orders and formats of provided exem-plars (Lu et al., 2022; Sclar et al., 2024). Preference Fairness and Human Alignment.Following the previous finding, we hypothesizethat the prompt sensitivity is mainly due to thepreference bias incurred by spurious correlationsfrom the instructions I. We proceed to visualizethe human agreement regarding preference dis-tribution pI by different instructions I across theentire query set {x1, ..., xN}, measured by pI,A =1NNi=1 I (p(yi = A|xi, I) > p(yi = B|xi, I)),where I() is an indicator function that counts thenumber of predictions that candidate A is preferredto B in pairwise evaluations. In , we showthat the patterns are nearly perfectly fitted to aquadratic regression function, where the highesthuman agreement point is close to pI = 0.5,and instructions with more skewed decisiondistributionsalwaysdegradetheevaluationalignment. Therefore, pI is a good indicator thatconnects decision fairness with human judgments,and instructions with fairer decision preferencescan lead to better human-aligned LLM judgments.",
  "ZEPO: Zero-Shot PromptOptimization with Fairer Preferences": "Zero-Shot Fairness Learning. Motivated by thesefindings, we now propose to automatically optimizethe evaluation prompts for LLM evaluators towardfairer preferences, thereby achieving better humanalignments. Importantly, the source preference dis-tribution for an unbiased pairwise evaluator shouldnaturally be uniform pS = 1/|Y| (by the law oflarge numbers) given a sufficient number of ran-domly sampled pairwise candidates. Consequently,",
  "JJj=1 |pS pI,yj|in an unsupervised set of data D by measuring theabsolute difference between the source prior andpreference distribution": "Automatic Prompt Optimization.In contrastwith previous prompt optimization methods thatheavily rely on labeled data, we propose ZEPO, anautomatic Zero-shot Evaluation-oriented PromptOptimization framework.It is a more naturalsetup for reference-free LLM evaluations wherehuman scores are usually unavailable in advance.ZEPO optimizes the evaluation prompts by max-imizing the zero-shot fairness metric, such thatI = arg maxII fairxiD(I). We integrate anLLM paraphraser with a greedy search algorithmto update the instruction I iteratively, where thedetailed ZEPO algorithm is shown in Algorithm 1.We refer to Appendix A for more details on imple-menting ZEPO. It is worth noting that debiasingand calibration (Zheng et al., 2024a; Zhou et al.,2024a) methods can also control LLM evaluatorsfor fairer preferences. We show in thatZEPO is a meta-method orthogonal to existingdebiasing approaches and leads to further improve-ments. In addition, we report the initial (seed)prompt and ZEPO-optimized prompt with corre-sponding fairness scores in and 6.",
  "ZEPO0.57+8%0.54+3%0.55+9%0.56+11%0.40+16%0.25+13%0.30+0%0.39+18%0.45+10%": ": Spearman correlations on Mistral 7B and Llama-3 8B. We evaluate preference-based evaluators anddirect-scoring evaluators in terms of Coherence (COH), Relevancy (REL), Informativeness (INF), Fluency (FLU),and Consistency (CON). We highlight the % improvement/degradation of ZEPO over Pairwise in +green/-red. (Grusky et al., 2018) and SummEval (Fabbri et al.,2021), and one dialog task: TopicalChat (Mehriand Eskenazi, 2020) (see Appendix A for furtherdetails). We examine ZEPO with state-of-the-artopen-source LLMs, Mistral 7B (Jiang et al., 2023)and Llama-3 8B (Touvron et al., 2023). Baselines.We provide baseline scores forreference-free evaluators in the zero-shot setup,includingBERTScore(Zhangetal.,2020),GPTScore (Fu et al., 2023), and G-Eval (Liu et al.,2023). ZEPO is applicable to state-of-the-art pair-wise ranking evaluators, and we report experimen-tal results from Pairwise (Liu et al., 2024b) as themain baseline and provide direct scoring evaluationresults named Scoring and G-Eval for reference. Main Results. We present ZEPO on representa-tive meta-evaluation benchmarks in . No-tably, ZEPO yields substantial gains in alignmentwith human judgments over almost all aspects onthe Pairwise baseline: 17% and 10% on averageon Mistral 7B and Llama-3 8B, respectively. Itshows that manually designed evaluation criteriaand instructions (without prompt optimization) canexpose strong preference bias with LLM evaluators.By conducting ZEPO on Pairwise in a zero-shotsetup, the performance of pairwise evaluators canbe largely recovered, outperforming fine-calibrateddirect scoring and the G-Eval baselines. Further-more, we notice that weaker models, e.g. Mistral7B, can exhibit more catastrophic evaluations, suf-fering from preference biases (e.g., on COH andCON aspects in SummEval), whereas Llama-3 8Bgenerates relatively more robust evaluations. In 0.40.20.0",
  "Correlation": "Spearman: 0.75p-value < 0.001 0.500.25 Context-free Conf. 0.1 0.2 0.3 0.4Spearman: 0.36p-value > 0.1 : Fairness shows the strongest correlationwith LLM evaluation performance. Correlation stud-ies of zero-shot learning objectives and LLM evaluationperformance. The growth of the x-axis indicates bet-ter/stronger fairness, confidence (conf.), and calibration. both cases, ZEPO constantly mitigates the prefer-ence bias and better aligns LLM evaluators. Over-all, the results indicate that ZEPO is a label-freeand efficient prompt optimizer for effectively align-ing LLM evaluators with human judgments. Zero-shot Learning Objectives. We provide an in-depth analysis of the effectiveness of our proposedFairness metric in comparison to other zero-shotobjective functions as visualized in . Weinclude model confidence, a commonly used zero-shot metric in exemplar selection (Lu et al., 2022;Wan et al., 2023a,b), measured as the negative ofentropy. Calibration-based approaches have beeneffective in mitigating position biases (Zhao et al., PairwiseDebiasing 0.1 0.2 0.3 0.4 Spearman correlation 0.4950.5000.5050.510 Decision rate 0.25 0.30 0.35 0.40 0.45",
  "Debiasing": "fair preference : ZEPO is orthogonal to debiasing approachesand brings further improved LLM judgments. Sensitiv-ity and evaluation performance studies on preferencefairness before and after applying permutation debiasingon the COH aspect in SummEval from Llama-3 8B. 2021; Wang et al., 2023). We adopt a zero-shot cal-ibration metric from Batch Calibration (Zhou et al.,2024a) and context-free confidence as another met-ric from Fair-Prompting (Ma et al., 2023), whereoverconfidence is argued to result in unfairness.First, Fairness shows the largest Spearman cor-relation with LLM evaluation performance, guar-anteeing its effectiveness with ZEPO. Followingfairness, Calibration is more weakly correlated,whereas Confidence metrics fail to serve as goodobjectives for ZEPO, with poorer correlations. Complementarity with Debiasing. We furtherextend our study of ZEPO, focusing on its orthogo-nality/complementarity with debiasing approaches.We implement the permutation debiasing methodwhich averages the probability for different order-s/positions of the same candidates, also termedBalanced Position Calibration (Wang et al., 2023). shows that the Debias method first improvesthe lower bar of the evaluation performance ofLLMs. Secondly, when we inspect the preferencedistribution after applying Debias, we observe afairer preference distribution where the decisionrates become much closer to 0.5. However, LLMevaluators are still sensitive to semantically equiv-alent instructions even after debiasing, where thejudgment alignment varies substantially from 0.26to 0.43. In addition, we observe a similar quadraticcurve in the second plot, indicating that our previ-ous findings still hold: fairer preferences lead toimproved human-aligned LLM judgments. Following this observation, we conduct addi-tional experiments on ZEPO with and without per-mutation debiasing. shows that furthergains can be achieved by integrating debiasingmethods with prompt optimization. Therefore, weconclude that ZEPO is a meta-method on zero-",
  "Conclusion": "We first analyzed the relationship between pref-erence fairness and human alignment; it revealedthat LLM evaluators produce highly skewed prefer-ence distributions even with semantically equiva-lent instructions. We further showed that fairer pref-erences can yield improved human-aligned LLMjudgments. Based on this insight, we proposed azero-shot prompt optimization framework with afairness-aware zero-shot proxy. It substantially im-proves alignments of pairwise LLM evaluators withhumans, without any labeled data, and serves as ameta-method orthogonal to debiasing approaches.",
  "Limitations": "First, ZEPO is a zero-shot method that learns thezero-shot fairness metric from unlabeled data. Itstill requires a sufficient number of random unla-beled samples for pairwise evaluations to obtain agood estimation of preference distribution for fair-ness. We argue that such a data requirement is mild,as in the evaluation setup, the bottleneck lies inhuman-annotated labels, not unlabeled inputs. Sec-ond, ZEPO is primarily designed for preference-based evaluators, and we have widely examinedthe effectiveness of ZEPO in pairwise evaluations.Though pairwise evaluation appears to be the cur-rent leading standard, it is possible that future ad-vances in LLM evaluators can achieve more effi-cient evaluation-by-ranking in multi-choice ques-tion formats with more than two classes, whichhave not been included in our current study. How- ever, in principle, the proposed zero-shot fairnessobjective is a general learning metric scalable toany number of classes based on its uniform prior.Lastly, ZEPO only integrates a basic LLM opti-mizer in exploring instruction candidates at a para-graph level with a greedy search algorithm. How-ever, ZEPO is a meta-framework also orthogonalto LLM optimizers with more advanced search al-gorithms, and this synergy warrants further investi-gation in future work. ZEPO serves as a first steptowards LLM evaluation with fairer preferencesand is easy to extend with more exploitation-drivenLLM optimizers in alternative search spaces.",
  "Acknowledgements": "The work has been supported by the UK Researchand Innovation (UKRI) Frontier Research GrantEP/Y031350/1 (the UK governments funding guar-antee for ERC Advanced Grants) awarded to AnnaKorhonen at the University of Cambridge. Thework has also been supported in part by a Royal So-ciety University Research Fellowship (no 221137;2022-) awarded to Ivan Vulic, and by the UK EP-SRC grant EP/T02450X/1. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023a. Gemini: A familyof highly capable multimodal models. arXiv preprintarXiv:2312.11805. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023b. Palm 2 technical report. arXivpreprint arXiv:2305.10403.",
  "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,": "Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen,and Hsin-Hsi Chen. 2023a. Self-ICL: Zero-shot in-context learning with self-generated demonstrations.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1565115662, Singapore. Association for Computa-tional Linguistics. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, andRuifeng Xu. 2023b. Exploring the use of large lan-guage models for reference-free text quality evalua-tion: An empirical study. In Findings of the Associa-tion for Computational Linguistics: IJCNLP-AACL2023 (Findings), pages 361374, Nusa Dua, Bali.Association for Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2023. Can largelanguage models be an alternative to human evalua-tions? In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1560715631, Toronto,Canada. Association for Computational Linguistics. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-han Wang, Han Guo, Tianmin Shu, Meng Song, EricXing, and Zhiting Hu. 2022. RLPrompt: Optimizingdiscrete text prompts with reinforcement learning.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages33693391, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "Max Grusky, Mor Naaman, and Yoav Artzi. 2018": "Newsroom: A dataset of 1.3 million summaries withdiverse extractive strategies. In Proceedings of the2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long Pa-pers), pages 708719, New Orleans, Louisiana. As-sociation for Computational Linguistics. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, KaitaoSong, Xu Tan, Guoqing Liu, Jiang Bian, and YujiuYang. 2024. Connecting large language models withevolutionary algorithms yields powerful prompt opti-mizers. In The Twelfth International Conference onLearning Representations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023.Mistral 7b.arXivpreprint arXiv:2310.06825. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,Zhiyuan Zhang, Keshav Santhanam, Sri Vard-hamanan A, Saiful Haq, Ashutosh Sharma, Thomas T.Joshi, Hanna Moazam, Heather Miller, Matei Za-haria, and Christopher Potts. 2024. DSPy: Com-piling declarative language model calls into state-of-the-art pipelines. In The Twelfth InternationalConference on Learning Representations.",
  "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021": "The power of scale for parameter-efficient prompttuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 30453059, Online and Punta Cana, Domini-can Republic. Association for Computational Lin-guistics. Chengzu Li, Han Zhou, Goran Glava, Anna Ko-rhonen, and Ivan Vulic. 2023a.On task perfor-mance and model calibration with supervised andself-ensembled in-context learning. arXiv preprintarXiv:2312.13772. Zongjie Li, Chaozheng Wang, Pingchuan Ma, DaoyuanWu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023b.Split and merge: Aligning position biases in largelanguage model based evaluators. arXiv preprintarXiv:2310.01432.",
  "Yinhong Liu, Zhijiang Guo, Tianya Liang, EhsanShareghi, Ivan Vulic, and Nigel Collier. 2024a. Mea-suring, evaluating and improving logical consistencyin large language models": "Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi,Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024b.Aligning with human judgement: The role of pair-wise preference in large language model evaluators.arXiv preprint arXiv:2403.16950. Yuxuan Liu, Tianchi Yang, Shaohan Huang, ZihanZhang, Haizhen Huang, Furu Wei, Weiwei Deng,Feng Sun, and Qi Zhang. 2024c. Calibrating LLM-based evaluator. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 26382656, Torino, Italia.ELRA and ICCL.",
  "Adian Liusie, Potsawee Manakul, and Mark Gales. 2024": "LLM comparative assessment: Zero-shot NLG eval-uation through pairwise comparisons using large lan-guage models. In Proceedings of the 18th Confer-ence of the European Chapter of the Association forComputational Linguistics (Volume 1: Long Papers),pages 139151, St. Julians, Malta. Association forComputational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages80868098, Dublin, Ireland. Association for Compu-tational Linguistics. Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,Qinghua Hu, and Bingzhe Wu. 2023.Fairness-guided few-shot prompting for large language mod-els. In Advances in Neural Information ProcessingSystems 36: Annual Conference on Neural Informa-tion Processing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023. Shikib Mehri and Maxine Eskenazi. 2020. USR: Anunsupervised and reference free evaluation metricfor dialog generation. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 681707, Online. Association forComputational Linguistics.",
  "Keita Saito, Akifumi Wachi, Koki Wataoka, and YouheiAkimoto. 2023.Verbosity bias in preference la-beling by large language models.arXiv preprintarXiv:2310.10076": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and AlaneSuhr. 2024. Quantifying language models sensitiv-ity to spurious features in prompt design or: How ilearned to start worrying about prompt formatting.In The Twelfth International Conference on LearningRepresentations. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, YangYou, and Lidong Bing. 2023. Large language mod-els are not yet human-level evaluators for abstrac-tive summarization. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages42154233, Singapore. Association for Computa-tional Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik,and Tomas Pfister. 2023a. Better zero-shot reasoningwith self-adaptive prompting. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 34933514, Toronto, Canada. Association forComputational Linguistics.",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-uating text generation with bert. In InternationalConference on Learning Representations": "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021. Calibrate before use: Improv-ing few-shot performance of language models. InProceedings of the 38th International Conferenceon Machine Learning, ICML 2021, 18-24 July 2021,Virtual Event, pages 1269712706. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, andMinlie Huang. 2024a. Large language models arenot robust multiple choice selectors. In The TwelfthInternational Conference on Learning Representa-tions. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024b.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, YizhuJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, andJiawei Han. 2022.Towards a unified multi-dimensional evaluator for text generation. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu,Jilin Chen, Katherine A Heller, and Subhrajit Roy.2024a. Batch calibration: Rethinking calibrationfor in-context learning and prompt engineering. InThe Twelfth International Conference on LearningRepresentations. Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korho-nen. 2023a. Survival of the most influential prompts:Efficient black-box prompt search via clustering andpruning. In Findings of the Association for Com-putational Linguistics: EMNLP 2023, pages 1306413077, Singapore. Association for ComputationalLinguistics. Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Ko-rhonen. 2024b. AutoPEFT: Automatic ConfigurationSearch for Parameter-Efficient Fine-Tuning. Trans-actions of the Association for Computational Linguis-tics, 12:525542. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2023b. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations.",
  "AImplementation Details": "ZEPO. In this section, we include implementationdetails to enable the reproducibility of our work.Regarding the template and prompt across all theexperiments reported, we use the prompt templatefrom . ZEPO evaluation results are con-ducted on top of the state-of-the-art pairwise evalu-ator, PairS (Liu et al., 2024b), which leverages pair-wise comparisons between randomly sampled pairsand aggregates them into a ranked sequence with asorting-based search algorithm. We use GPT-3.5-turbo as the LLM optimizer with a temperature of0.9, which is instructed to generate diverse and cre-ative paraphrasing of the initial instruction. Follow-ing that, we implement Mistral-7B-Instruct-v0.1and Meta-Llama-3-8B-Instruct as our main LLMevaluators. In practice, we set 5 epochs with a pop-ulation size S of 5 that sufficiently converges to thefairest instruction. For |D|, we use 2,400 pairwisesampling (10 data points) per instruction for Sum-mEval, 840 (20 data points) for News Room, and1,200 (60 data points) for TopicalChat based on thenumber of candidates per data point. ZEPO servesas a first step towards fairer LLM evaluations, andwe defer investigations on ZEPO with tighter, moresampling-efficient constraints to future work. Zero-Shot Learning Objectives. Entropy is acommonly used zero-shot metric: j pj log pj.In , we use entropy as a confidence measure-ment for LLM evaluators and treat Confidence =j pj log pj in the negative of entropy averagedacross D. However, in the context of LLM evalua-tions, overconfidence may further misalign LLMevaluators with human judgments. Context-freeconfidence is computed with the same formulation",
  "Prompt templates for LLM Optimizer to generatenew instruction candidates": "Paraphrasethef o l l o w i n gi n s t r u c t i o nf o rap a i r w i s ecomparisont a s k .Do notchangethekeyword\" [ASPECT ] \" .Be d i v e r s eandc r e a t i v einp a r a p h r a s i n g .Returnthei n s t r u c t i o nonly .",
  ": Prompt template for pairwise comparisons andthe LLM optimizer to generate paraphrased instructions": "above but with a content-free input CI([N/A], I)adopted from the contextual calibration (Zhao et al.,2021). Context-free confidence is introduced inFair-Prompting (Ma et al., 2023), where the mainidea is to select exemplars with the lowest confi-dence with respect to a content-free input, suchthat the prediction for classes is more balancedwith the prompt template alone. In addition, weadopted a zero-shot calibration metric from BatchCalibration (Zhou et al., 2024a): Calibration =| 1",
  "N(log pA log pB)|, which measures the ab-solute distance in the marginalized logits betweentwo classes.It indicates a uniform prior in the logit space,": "and a better-calibrated model can generate fairerpredictions in terms of their scores. In contrastwith calibration, our fairness metric is based on auniform prior in the preference (decision) distri-bution and demonstrates the strongest correlationwith LLM evaluation performance. Pointwise Baselines. We implement two pointwiseevaluator baselines: direct Scoring and G-Eval.For both cases, the LLM evaluators are tasked withrating a specific aspect of the output candidate us-ing an integer score on the Likert scale (Likert,1932). In the Scoring approach, the evaluators as-sign a single score with the highest predictive prob-ability to each output candidate. For the G-Evalbaseline, the final score is calculated by taking theweighted average of the scores across all five scoretokens. We use the same prompt templates andevaluation criteria from previous work (Liu et al.,2024c), which have been calibrated and deliver ro-bust evaluations. As indicated in the main paper,ZEPO shows improved evaluation results in gen-eral over the aforementioned calibrated baselines.",
  "AspectInstruction PromptFairness": "RELInitial Prompt: Evaluate and compare the relevance of the twosummary candidates for the given source text.A summary isrelevant if it captures the main points from the article, withoutleaving out any crucial details or adding any unnecessary orinaccurate ones. A summary is more relevant if it uses the same orsimilar terms and expressions as the article.A summary is lessrelevant if it omits some of the key facts from the article, or if itintroduces irrelevant information that is not supported by the article.Which summary candidate has better relevance? If the candidateA is better, please return A. If the candidate B is better, pleasereturn B. You must return the choice only. ZEPO-Optimized Prompt: Assess the relevance of the two sum-maries presented for the text and pick the one that closely matchesthe main points of the article using similar language. Select A forcandidate A or B for candidate B. Display your selection.",
  "Optimized: -0.007": "FLUInitial Prompt: Evaluate and compare the fluency of the twosummary candidates for the given source text. Which summarycandidate has better fluency? If the candidate A is better, pleasereturn A. If the candidate B is better, please return B. You mustreturn the choice only. ZEPO-Optimized Prompt: Evaluate the smoothness of each sum-mary choice using the given text. Decide which summary showcasesbetter fluency. Choose A for candidate A or B for candidate B.Please only submit your chosen option.",
  "Optimized: -0.018": "CONInitial Prompt: Evaluate and compare the consistency of the twosummary candidates for the given source text.A summary isconsistent with the article if it faithfully reflects the main points,facts, and tone of the article.A summary is inconsistent if itintroduces any errors, contradictions, or distortions of the originalarticle. Which summary candidate has better consistency? If thecandidate A is better, please return A. If the candidate B is better,please return B. You must return the choice only. ZEPO-Optimized Prompt: Evaluate the consistency of two differentways of summarizing the given text. Find the summary that bestcaptures the main ideas, details, and tone of the original text. Noteany mistakes or differences in the summaries. Choose either Afor option A or B for option B as the superior choice. Share yourselected option.",
  "Optimized: -0.0003": "INFInitial Prompt: Evaluate and compare the informativeness of thetwo summary candidates for the given source text. Evaluate howeach summary converts their input text to natural language text,without omitting, adding, or distorting any facts. Which summarycandidate has better informativeness? If the candidate A is better,please return A. If the candidate B is better, please return B.You must return the choice only. ZEPO-Optimized Prompt: Assess and contrast the informativenessof two summaries based on the provided source material. Examinehow accurately each summary reflects the original content. Deter-mine which summary is more informative by selecting either A orB. Only indicate your choice."
}