{
  "Abstract": "Proprietary LMs such as GPT-4 are often em-ployed to assess the quality of responses fromvarious LMs. However, concerns includingtransparency, controllability, and affordabilitystrongly motivate the development of open-source LMs specialized in evaluations. On theother hand, existing open evaluator LMs ex-hibit critical shortcomings: 1) they issue scoresthat significantly diverge from those assignedby humans, and 2) they lack the flexibility toperform both direct assessment and pairwiseranking, the two most prevalent forms of as-sessment. Additionally, they often do not pos-sess the ability to evaluate based on customevaluation criteria, focusing instead on gen-eral attributes like helpfulness and harmless-ness. To address these issues, we introducePrometheus 2. Prometheus 2 is more powerfulthan its predecessor, and closely mirrors hu-man and GPT-4 judgements. Moreover, it iscapable of processing both direct assessmentand pair-wise ranking formats grouped with auser-defined evaluation criteria. On four directassessment benchmarks and four pairwise rank-ing benchmarks, PROMETHEUS 2 scores thehighest correlation and agreement with humansand proprietary LM judges among all testedopen evaluator LMs. Our models, code, anddata are all publicly available. 1",
  "Low Correlation": ": Weak evaluators (e.g., Llama-2-Chat-70B,Prometheus, and GPT-3.5-Turbo) achieve low scoringcorrelation with strong evaluators (e.g., Humans, GPT-4,and Claude-3-Opus). On the other hand, scores providedby strong evaluators highly correlate with each other. Gao et al., 2024). In this paradigm, LMs are ei-ther prompted to output a scalar indicator of qual-ity (denoted as direct assessment) (Zheng et al.,2023; Liu et al., 2023b; Ye et al., 2023; Kim et al.,2023) or to determine which of two outputs are pre-ferred (denoted as pairwise ranking) (Wang et al.,2023b; Li et al., 2023b; Lambert et al., 2024). Priorworks employing proprietary LMs as evaluatorshave demonstrated not only high correlations withhuman evaluations but also increased speed andcost-effectiveness (Zheng et al., 2023; Liu et al.,2023b; Dubois et al., 2023; Ye et al., 2023).However, relying on proprietary LMs for evalua-tion poses significant challenges. The lack of trans-parency about their training data compromises bothfairness and reproducibility, making it problematicto use them in evaluation pipelines. Additionally,concerns regarding controllability and affordabilityalso persist (Kim et al., 2023). To address theseissues, recent works have focused on developingevaluator LMs that are open-access, transparent,and controllable (Kim et al., 2023; Wang et al.,2023a,b; Li et al., 2023a; Zhu et al., 2023; Jianget al., 2023b,c; Lee et al., 2024). Yet, these modelsoften yield scoring decisions that do not correlatewell enough with human judgments or those made by proprietary LMs, failing to effectively simu-late them. Moreover, open evaluator LMs are notflexible since they are typically trained only to per-form either direct assessment or pairwise rankingand assess based on general public preferences likehelpfulness and harmlessness, limiting their abilityto handle diverse real-life scenarios.To close the gap with proprietary LMs, we in-vestigate unifying the two model-based evaluationparadigms - direct assessment and pairwise ranking- to train a robust unified evaluator LM. We proposea recipe based on merging the weights of two eval-uator LMs trained separately on direct assessmentand pairwise ranking formats. Our key empiricalobservation is that weight merging can yield anevaluator LM that not only works in both formats,but also outperforms evaluator LMs that are jointlytrained or only trained on a single format.To demonstrate our approach, we develop thePREFERENCE COLLECTION, a new fine-grainedpairwise ranking feedback dataset that builds onthe FEEDBACK COLLECTION (Kim et al., 2023),which is a direct assessment feedback dataset. Wechoose Mistral-7B (Jiang et al., 2023a) and Mixtral-8x7B (Jiang et al., 2024) as our base models, andmerge the weights of evaluator LMs separatelytrained on the FEEDBACK COLLECTION and thePREFERENCE COLLECTION to obtain our resultingmodels, PROMETHEUS 2 (7B & 8x7B).On four direct assessment benchmarks (VicunaBench, MT Bench, FLASK, Feedback Bench), thePROMETHEUS 2 models demonstrate the highestcorrelation with both human evaluators and pro-prietary LM-based judges compared to existingopen evaluator LMs, with the Pearson correla-tion surpassing other baselines by 0.2 units acrossall datasets. Similarly, on four pairwise rankingbenchmarks (HHH Alignment, MT Bench HumanJudgment, Auto-J Eval, Preference Bench), thePROMETHEUS 2 models show the highest agree-ment with human evaluators among all the openevaluator LMs we tested, reducing the performancegap with GPT-4 in half.Our contributions are summarized as follows: We introduce PROMETHEUS 2 (7B & 8x7B),state-of-the-art open evaluator LMs that scorehigh correlations with both human evaluatorsand proprietary LM-based judges on both di-rect assessment and pairwise ranking.",
  "Language Model-based Evaluation": "To assess the generation capabilities of LMs, priorworks such as the GEM benchmark (Gehrmannet al., 2021, 2022) employed ROUGE (Lin,2004),BLEU (Papineni et al., 2002),andBERTScore (Zhang et al., 2019) as their metrics,which measure the lexical or semantic similaritybetween a reference answer and a response. How-ever, these conventional metrics are prone to falsenegatives because they are not expressive enoughto recognize responses that are of good quality butdiffer from the reference answer (Schluter, 2017;Freitag et al., 2020; Hanna and Bojar, 2021).Recently, employing language models as a judgehas gained attention as a promising paradigm tomimic the depth and granularity that human evalu-ation offers (Zheng et al., 2023; Liu et al., 2023b;Li et al., 2023b; Chan et al., 2023; Ye et al., 2023).To reduce the over-reliance on proprietary LMs,follow-up works suggest training language modelsspecialized in evaluations (Cui et al., 2023; Kimet al., 2023; Jiang et al., 2023b,c; Li et al., 2023a;Lee et al., 2024). Yet, open evaluator LMs donot possess the flexibility to function in differentevaluation schemes and show weak evaluation per-formance compared to proprietary LMs. We aimto bridge this gap by introducing PROMETHEUS 2.",
  "Weight Merging": "Prior works have demonstrated that weight merg-ing can enhance performance across various do-mains, including language modeling (Li et al.,2022; Matena and Raffel, 2022; Ilharco et al.,2022; Don-Yehiya et al., 2022; Gururangan et al.,2023; Yadav et al., 2024; Sukhbaatar et al., 2024),instruction-tuning (Jang et al., 2023b; Yu et al.,2023), and aligning to user preferences (Jang et al.,2023a; Rame et al., 2024; Wang et al., 2024). Inour work, we specifically focus on enhancing theevaluation capabilities of open evaluator LMs. Bymerging models trained on different assessment for-matsspecifically, direct assessment and pairwise",
  "Methodology": "We propose a new recipe for training a unifiedevaluator LM based on merging the weights ofmodels trained for direct assessment and pairwiseranking. We begin with background on direct as-sessment and pairwise ranking for evaluator LMs(.1, 3.2), followed by the construction pro-cess of our training data (.3). Finally, wepresent our methods to train state-of-the-art evalua-tor LMs, Prometheus 2 models (.4).",
  "Direct Assessment": "+ The response effectively uses simple and accessible language to explain containerization and Docker, which is great for beginners. The analogy of putting things in a box is particularly helpful as it visually illustrates the concept of [...] However, the response could be improved by briefly mentioning why containerization is significant, such as its benefits in ensuring that software runs consistently across different computing environments. It loses a point for not fully addressing the significance of containerization in the broader context of software development, which could provide valuable insight for the reader.",
  "Pairwise Ranking": "+ Both responses attempt to convey the fundamental concept of containerization, but with varying degrees of clarity and technical detail. Response A approaches the concept by using the metaphor of 'putting things in a box,' which, while easy to understand, lacks precision and industry-specific [...] On the other hand, Response B employs technical jargon more effectively, such as 'packaging,' 'configuration files,' 'libraries,' and 'dependencies.' It can be concluded that Response B is better than Response A. AB B",
  ": Statistics of our training datasets, the FEED-BACK COLLECTION and the PREFERENCE COLLEC-": "TION. Note that the 1K evaluation criteria, 20K instruc-tions, and 20K reference answers are shared among thetwo datasets. Both datasets have an equal number ofscoring decisions (A or B; 100K each & 1-5; 20Keach) to prevent unintended biases after training. custom criterion, we add the evaluation criteria eas input to the evaluator LM (Ye et al., 2023; Kimet al., 2023). To the best of our knowledge, we arethe first to study such fine-grained evaluation inpairwise ranking settings. This is expressed as:",
  "The Preference Collection": "Popular pairwise ranking datasets such as HH-RLHF (Bai et al., 2022) or Ultra Feedback (Cuiet al., 2023) do not include an evaluation criterione and a verbal feedback vrm,rn. To train an evalu-ator LM that could assess based on such criteria,we construct the PREFERENCE COLLECTION, in-cluding 1K evaluation criteria. We apply two mod-ifications to the FEEDBACK COLLECTION. First,since the FEEDBACK COLLECTION includes fiveresponses for each instruction, each correspondingto a scoring decision between 1 and 5, we pair twoout of the five responses, resulting in a total of tencombinations per instruction. Using the existingscoring decisions for each response, we determinewhich response is better and assign a new scoringdecision for that pair (i.e., Response A is better orResponse B is better). Second, to generate newverbal feedback vrm,rn for each pair of responses,we prompt GPT-4-1106 to identify the commonali-ties and differences between the two responses. Does the response accurately employ specific industry terminologies and jargon?",
  "Instruction": "What is the meaning and significance of 'Containerization' in software development, and what role does Docker play in it? Containerization in software is similar to putting items in a box. It involves packaging your software and all its components into a container. Docker is a tool that facilitates this process. It helps to encapsulate the software into containers and simplifies their use. [...] Containerization in software development refers to the process of packaging up an application along with all its related configuration files, libraries, and dependencies required to run, into a standalone unit or a container. [...] Response B",
  "Evaluation Criteria": "A : Comparison of direct assessment and pairwise ranking. Both responses could be considered decent underthe umbrella of helpfulness. However, the scoring decision might change based on a specific evaluation criterion. The statistics of the resulting dataset are listed in along with the FEEDBACK COLLECTION.We explain about our quality verification processof the PREFERENCE COLLECTION in Appendix A.Also, we include the prompts we use for the aug-mentation process in Appendix H.",
  "Training Methods & Baselines": "PromptingPrompting involves querying an LMto make judgments in a specified evaluation for-mat without training. We employ Llama-2-Chat-7,13,70B (Touvron et al., 2023); Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a); and Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024) as our base-lines. Its worth noting that models not explicitlytrained on feedback data often fail to generate re-sponses in the required format, making it extremelydifficult to parse scoring decisions. Although it isimpractical for regular use, we make a fair compari-son by infinitely looping until scores can be parsed.Also, we include proprietary LMs such as GPT-3.5-Turbo-0613; GPT-4-1106; and Claude-3-Opus. Single-Format TrainingSingle-Format traininginvolves training a base model on either on adirect assessment feedback dataset Dd or a pair-wise ranking feedback dataset Dp.For single-format trained evaluator LMs, we test Prometheus-7,13B (Kim et al., 2023) (direct assessment);UltraRM-13B (Cui et al., 2023) (pairwise rank-ing); and PairRM-0.4B (Jiang et al., 2023c) (pair-",
  "wise ranking). In addition, we also report the per-formances of single-format training Mistral-7B-Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1 oneither direct assessment or pairwise ranking": "Joint TrainingJoint training involves training abase model on both a direct assessment feedbackdataset Dd and a pairwise ranking feedback datasetDp. This enables the resulting evaluator LM tofunction across both evaluation formats. For jointlytrained evaluator LMs, we test Auto-J (Li et al.,2023a). In addition, we report the performancesof jointly training Mistral-7B and Mixtral-8x7B onboth direct assessment and pairwise ranking. Weight MergingWeight Merging involves train-ing two models, d and p, separately on a directassessment feedback dataset Dd and a pairwiseranking feedback dataset Dp. Then, the final eval-uator LM final is obtained by merging d and p.For example, linear merging is as follows:",
  "final = d + (1 ) p(3)": "In addition to linear merging, we test 5 additionalvariants, namely Task Arithmetic merging (Ilharcoet al., 2022), TIES merging (Yadav et al., 2024),DARE-TIES and DARE-Linear merging (Yu et al.,2023), and SLERP merging (Goddard et al., 2024).We include an explanation of these merging meth-ods and ablation experiment results of the perfor-mance differences in Appendix G. Among them,",
  "The statistics of all the benchmarks are in .The four direct assessment benchmarks are:": "Vicuna Bench (Chiang et al., 2023): A single-turn chat benchmark that includes 80 testprompts, 80 hand-crafted score rubrics fromKim et al. (2023), and 320 responses obtainedby WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. MT Bench (Zheng et al., 2023): A multi-turn chat benchmark that consists of 80 testprompts, 80 hand-crafted score rubrics fromKim et al. (2023), and 320 responses obtainedby WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. FLASK (Ye et al., 2023): A fine-grainedevaluation benchmark comprised of 200 testprompts, 12 score rubrics, and 2000 responsesacquired from Alpaca-7B, Vicuna-13B, Bard,GPT-3.5-Turbo-0613. In addition to scoresfrom proprietary LMs, this benchmark alsoincludes scores marked by human evaluators.",
  "): A benchmark that shares the same 80prompts as MT-Bench. In addition, it provides3,360 response pairs (graded as win, tie, orlose) judged by human evaluators": "Auto-J Eval (Li et al., 2023a): A benchmarkconsisted of 58 prompts and 1,392 responsepairs (graded as win, tie, or lose) judgedby human evaluators. This benchmark is usedas the in-domain test set of Auto-J. Preference Bench: Our in-domain test set forthe PROMETHEUS models. Similar to how thePREFERENCE COLLECTION was made withthe FEEDBACK COLLECTION, we adjust theFEEDBACK BENCH and pair two out of thefive responses, resulting in a test set with 200prompts, 2,000 response pairs (graded as winor lose), and 200 evaluation criteria. In direct assessment, we conduct reference-based evaluations by appending the reference an-swer as the input. We use Pearson, Spearman, andKendall-Tau as performance metrics to measurescoring correlations against reference evaluators.Moreover, we include the results of the reference-free direct assessment evaluation in Appendix F.",
  "In pairwise ranking, we conduct reference-freeevaluations. Based on judgments assigned by hu-mans, we use accuracy as our metric to measureagreement between evaluator LMs and humans": "Also, the MT Bench Human Judgment and Auto-J test set includes a tie option assessed by humanevaluators. We evaluate in two ways: by excludingall tie options for pairwise ranking (denoted asw/o tie), or by using direct assessment where re-sponses scored as ties are grouped, and pairwiserankings are applied to the remaining responseswith differing scores (denoted as w/ tie).",
  "Help.Harm.Hon.OtherTotal Avg.w/ TIEw/o TIEw/ TIEw/o TIEInstance-wise Criteria": "LLAMA2-CHAT 7B55.9362.0749.1862.7957.0146.6850.3945.7645.7358.60LLAMA2-CHAT 13B71.1977.5960.6662.7968.3351.2249.6147.8443.2863.00LLAMA2-CHAT 70B62.7181.0365.5765.1268.7855.1460.8853.3850.6464.70MISTRAL-INSTRUCT-7B59.3268.9763.9381.4067.4253.8163.8253.8860.9479.40MIXTRAL-INSTRUCT-8X7B83.0587.9367.2169.7777.3851.8571.4253.8173.5084.00PAIR RM (0.4B)84.7584.4880.3390.7084.62-59.00-59.0581.80ULTRA RM (13B)86.4479.3181.9788.3783.71-56.00-59.8586.97AUTO-J (13B)77.9779.3170.4974.4275.5742.5669.1243.4676.6481.35PROMETHEUS-2-7B72.7879.3177.0576.7474.6650.4570.7854.9675.0793.25PROMETHEUS-2-8X7B84.7596.5581.9776.7485.5255.0771.9658.4179.9890.65 GPT-3.5-TURBO-061377.9781.0377.0567.4476.4754.6569.4145.9872.1375.05GPT-4-1106-PREVIEW89.8396.5591.8083.7290.9560.3879.9052.8083.1285.50CLAUDE-3-OPUS91.53100.0091.8095.3594.5755.3577.6560.7082.9289.85 : Pairwise Ranking Results Accuracy on human preference datasets. The best comparable accuracies are bolded andsecond best underlined except proprietary LMs. Note that HHH Alignment is an in-domain test set for PairRM, Auto-J Eval isan in-domain test set for Auto-J, and the Preference Bench is an in-domain test set for Prometheus-2 models.",
  "Direct Assessment Results": "The direct assessment results are shown in .The scoring decisions of PROMETHEUS 2 models(7B & 8x7B), GPT-4-1106, Claude-3-Opus, andhuman evaluators all strongly correlate with eachother, yielding Pearson correlations higher than 0.5regardless of the reference evaluator and bench-mark. On the other hand, base LMs, single-formattrained LMs, and jointly trained LMs show lowercorrelations, mostly falling below 0.5. Notably, PROMETHEUS 2 models outperformPrometheus and Auto-J by at least 0.2 unitsacross benchmarks in their correlation with pro-prietary LMs. Moreover, on the FLASK bench-mark, while the correlation between humans andGPT-4 is 0.679, the highest correlation previouslyachieved by Prometheus-13B with humans was0.449. PROMETHEUS-2-8X7B achieves a correla-tion of 0.555 with humans, halving the gap.",
  "Pairwise Ranking Results": "The pairwise ranking results are shown in .We exclude the results of Pair RM and Ultra RMon w/ Tie settings since they could not process it.On all of the 4 benchmarks, the PROMETHEUS2 models achieve the highest scores, showing thatthey could effectively simulate human judgments.Notably, while HHH Alignment is an in-domaintest set for Pair RM, and Auto-J Eval is for Auto- J, PROMETHEUS-2-8X7B achieves higher scores.This shows that training a large LM (i.e., Mixtral-8x7B) with feedback data could be an effectivestrategy to obtain a robust evaluator LM that couldgeneralize beyond its training data. Moreover, thePROMETHEUS 2 models at least halve the perfor-mance gap with proprietary LMs compared to ex-isting evaluator LMs on out-of-domain test sets.",
  "Weight Merging vs Joint Training": "compares the performance of evaluatorLMs trained via weight merging and joint training.Alongside this, we also add and compare the resultsof prompting and single-format training.Surprisingly, evaluator LMs trained via jointtraining often show lower performance comparedto those trained only in single-format, which indi-cates negative task transfer. Specifically, evaluatorLMs trained only on direct assessment formats ob-tain higher correlations compared to their jointlytrained counterparts across different model scales.Similarly, evaluator LMs trained solely on pairwiseranking formats achieve higher average accuracycompared to those trained on multiple tasks, partic-ularly when using Mixtral-8x7B as the base model.On the other hand, evaluator LMs trained viaweight merging show superior performance notonly compared to jointly trained evaluator LMsbut also single-format trained evaluator LMs, in-dicating positive task transfer. Also, while bothbenefit each other, merging the pairwise rankingevaluator LM weights improves direct assessmentperformance more significantly than the reverse.",
  "While we empirically find that weight merging iseffective, the underlying reason remains unclear. A": "natural assumption is that this effectiveness resultsfrom the ensembling effect of combining multiplemodels. To test this hypothesis, we conduct an abla-tion experiment where we train multiple evaluatorLMs on different random seeds and merge them.Specifically, we merge two evaluator LMs trainedon direct assessment formats (denoted as DirectAssessment & Direct Assessment) and two evalu-ator LMs trained on pairwise ranking formats (de-noted as Pairwise Ranking & Pairwise Ranking).We use Mistral-7B-Instruct as our base model. The results are presented in . Across mul-tiple benchmarks, merging evaluator LMs trainedon the same evaluation format does not enhanceevaluation performance. Specifically, merging twoevaluator LMs trained on the same evaluation for-matwhether direct assessment or pairwise rank-ingnegatively impacts performance on averagefor both direct assessment and pairwise rankingbenchmarks. In contrast, merging two evaluatorLMs, each trained on direct assessment and pair-wise ranking formats, results in superior perfor-mance compared to the other settings. This in-dicates that the beneficial task transfer in weightmerging arises from integrating different evaluationformats, not ensembling multiple models.",
  "Quantifying Positive Transfer acrossEvaluation Formats": "To explore how training on direct assessment feed-back data influences pairwise ranking accuracy andvice versa, we experiment by adjusting the valueduring linear merging. We evaluate the averageperformance using all eight benchmarks in our ex-periments. To illustrate the average performance(colored in black), we adjust the scale by multiply-ing the Pearson correlations from direct assessment,which originally range from 0 to 1, by 100 beforeaveraging them with the pairwise ranking accuracy. The results are shown in . For directassessment benchmarks, evaluator LMs obtain theoptimal performance when is set to 0.5. Thisindirectly indicates that both pairwise ranking anddirect assessment feedback data contribute equally.On the other hand, for pairwise ranking bench-marks, the performance is optimal when is set to0.3. This also implies that while both benefit eachother, training on pairwise ranking improves directassessment performance more than the reverse.",
  "DIRECT ASSESSMENT & PAIRWISE RANKING0.6660.5480.6590.62474.6670.7875.0773.50": ": Unifying Formats vs Ensembling Pearson correlations with GPT-4-1106 (Vicuna Bench, MT Bench, FLASK) andagreement with human evaluators (HHH Alignment, MT Bench Human Judgment, Auto-J Eval). Merging models trained withthe same evaluation formats (ensembling) underperforms merging models trained with different formats (unifying formats). Direct Assessment CorrelationPairwise Ranking AccuracyAverage Performance (Direct Assessment : Pairwise Ranking) Merging Ratio",
  "We introduce PROMETHEUS 2, an open-source LMspecialized in evaluating other responses. Unlikeexisting open evaluator LMs that cannot effectivelyprocess both direct assessment and pairwise rank-": "ingthe two most prevalent evaluation schemesthe PROMETHEUS 2 models demonstrate superiorperformance on both schemes, significantly narrow-ing the gap with proprietary LM-based evaluations.To train the PROMETHEUS 2 models, we developthe PREFERENCE COLLECTION, the first pairwise ranking dataset that includes over 1,000 instance-wise evaluation criteria beyond basic qualities suchas helpfulness and harmlessness. Notably, we findthat merging evaluator LMs trained on either directassessment or pairwise ranking formats can leadto a unified evaluator LM with strong performance.We hope that our work encourages more researchon using open-source LMs as evaluators.",
  "Acknowledgements": "We thank the KAIST AI LKLab members for help-ful discussions. This work was partly supportedby LG AI Research grant (Self-improving logicalreasoning capabilities of LLMs, 2024, 50%) andthe Institute of Information & CommunicationsTechnology Planning & Evaluation(IITP) grantfunded by the Korea government(MSIT) (RS-2024-00397966, Development of a Cybersecurity Spe-cialized RAG-based sLLM Model for SuppressingGen-AI Malfunctions and Construction of a Pub-licly Demonstration Platform, 50%).",
  "Limitations": "Evaluation is fundamentally a very multi-facetedtask. In this paper, we used an indirect method toassess the evaluation capability of evaluator LMsby measuring if they perform evaluations similar tohuman evaluators or proprietary LMs, such as GPT-4-1106 and Claude-3-Opus. However, this maynot necessarily be the best approach. Future workcould explore meta-evaluation pipelines that reeval-uate the results of evaluator LMs or methodologiesthat allow humans to efficiently review evaluationresults. Also note that it is crucial to use model-based evaluations in conjunction with human eval-uation instead of solely relying on it.Additionally, the degree to which evaluator LMscan generalize was based on an analysis by Kimet al. (2023), which checked for overlap betweenthe data used to train the evaluator LMs and thedata used to evaluate them. This study extended theevaluation to eight different datasets with humanjudgments to check the generalization capabilityof evaluation under various circumstances. How-ever, this may not be sufficient. One of the majorchallenges in evaluating evaluator LMs is obtain-ing the evaluation results (e.g., human judgment).Automating evaluations with LMs could greatlybenefit many areas of NLP research, hence the roleof future work in creating feedback benchmarksthat include human judgment or data for training evaluator LMs is crucial.One downside of the PROMETHEUS 2 is that itoperates only on a 1-5 point Likert scale for abso-lute evaluation or a comparative evaluation styleof A is better & B is better. Depending on theuse case, people may need a 1-10 point absoluteevaluation, a ranking method for five responses atonce, or a checklist-based evaluation not covered inthe paper. While proprietary LMs can flexibly con-duct evaluations in any format if a well-describedprompt is devised, open-source LMs cannot pro-duce good evaluation results without training, andconversely, if trained in one or two formats, theylose the flexibility to conduct different evaluations.Future work could examine whether evaluator LMstrained in each format, as done in this paper, canhandle evaluations for added formats well whenweight merging is employed.Lastly, the paper presents an evaluation modelthat can handle both absolute and comparativeevaluation formats well through weight mergingbased on empirical experiments. However, funda-mentally explaining why weight merging workswell remains a challenging task. To address this, indirectly analyzes the effectiveness ofweight merging by comparing it with joint training,demonstrating that the improvement in evaluationperformance is not due to model ensembling, andshowing that the impact of comparative evaluationon absolute evaluation is greater than the reverse.Our best current interpretation is that \"absolute andcomparative evaluations are not completely differ-ent tasks, so weight merging could handle bothwithout degeneration, and conversely, because theyare not too similar, weight merging performed bet-ter than joint training.\" Future work could theoreti-cally analyze this or further explore whether weightmerging can effectively work in fields other thanLLM evaluation. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, Nelson El-hage, Zac Hatfield-Dodds, Danny Hernandez, Jack-son Kernion, Kamal Ndousse, Catherine Olsson,Dario Amodei, Tom Brown, Jack Clark, Sam Mc-Candlish, Chris Olah, and Jared Kaplan. 2021. Ageneral language assistant as a laboratory for align-ment.",
  "Stanislav Fort, Deep Ganguli, Tom Henighan, et al.2022. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXivpreprint arXiv:2204.05862": "Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,Wei Xue, Shanghang Zhang, Jie Fu, and ZhiyuanLiu. 2023. Chateval: Towards better llm-based eval-uators through multi-agent debate. arXiv preprintarXiv:2308.07201. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, andMaosong Sun. 2023. Ultrafeedback: Boosting lan-guage models with high-quality feedback.arXivpreprint arXiv:2310.01377.",
  "Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and XiaojunWan. 2024. Llm-based nlg evaluation: Current statusand challenges. arXiv preprint arXiv:2402.01383": "Sebastian Gehrmann, Tosin Adewumi, Karmanya Ag-garwal, Pawan Sasanka Ammanamanchi, AremuAnuoluwapo, Antoine Bosselut, Khyathi RaghaviChandu, Miruna Clinciu, Dipanjan Das, Kaustubh DDhole, et al. 2021. The gem benchmark: Natural lan-guage generation, its evaluation and metrics. arXivpreprint arXiv:2102.01672. Sebastian Gehrmann, Abhik Bhattacharjee, AbinayaMahendiran, Alex Wang, Alexandros Papangelis,Aman Madaan, Angelina McMillan-Major, AnnaShvets, Ashish Upadhyay, Bingsheng Yao, et al. 2022.Gemv2: Multilingual nlg benchmarking in a singleline of code. arXiv preprint arXiv:2206.11249. Charles Goddard, Shamane Siriwardhana, MalikehEhghaghi, Luke Meyers, Vlad Karpukhin, BrianBenedict, Mark McQuade, and Jacob Solawetz. 2024.Arcees mergekit: A toolkit for merging large lan-guage models. arXiv preprint arXiv:2403.13257. Suchin Gururangan, Margaret Li, Mike Lewis, Wei-jia Shi, Tim Althoff, Noah A Smith, and LukeZettlemoyer. 2023. Scaling expert language modelswith unsupervised domain discovery. arXiv preprintarXiv:2303.14177.",
  "Michael Hanna and Ondrej Bojar. 2021. A fine-grainedanalysis of bertscore. In Proceedings of the SixthConference on Machine Translation, pages 507517": "Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Suchin Gururangan, Ludwig Schmidt, Han-naneh Hajishirzi, and Ali Farhadi. 2022.Edit-ing models with task arithmetic.arXiv preprintarXiv:2212.04089. Joel Jang, Seungone Kim, Bill Yuchen Lin, YizhongWang, Jack Hessel, Luke Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.2023a. Personalized soups: Personalized large lan-guage model alignment via post-hoc parameter merg-ing. arXiv preprint arXiv:2310.11564. Joel Jang, Seungone Kim, Seonghyeon Ye, DoyoungKim, Lajanugen Logeswaran, Moontae Lee, Kyung-jae Lee, and Minjoon Seo. 2023b. Exploring thebenefits of training expert language models over in-struction tuning. arXiv preprint arXiv:2302.03202. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023a. Mistral7b. arXiv preprint arXiv:2310.06825. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088.",
  "Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023c.Llm-blender: Ensembling large language modelswith pairwise ranking and generative fusion. arXivpreprint arXiv:2306.02561": "Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,Shayne Longpre,Hwaran Lee,Sangdoo Yun,Seongjin Shin, Sungdong Kim, James Thorne, et al.2023. Prometheus: Inducing fine-grained evalua-tion capability in language models. arXiv preprintarXiv:2310.08491. Seungone Kim, Juyoung Suk, Ji Yong Cho, ShayneLongpre, Chaeeun Kim, Dongkeun Yoon, GuijinSon, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et al.2024. The biggen bench: A principled benchmarkfor fine-grained evaluation of language models withlanguage models. arXiv preprint arXiv:2406.05761. Nathan Lambert, Valentina Pyatkin, Jacob Morrison,LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,et al. 2024.Rewardbench:Evaluating rewardmodels for language modeling.arXiv preprintarXiv:2403.13787.",
  "soups: towards pareto-optimal alignment by inter-polating weights fine-tuned on diverse rewards. Ad-vances in Neural Information Processing Systems,36": "Natalie Schluter. 2017. The limits of automatic sum-marisation according to rouge. In Proceedings of the15th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 4145.Association for Computational Linguistics. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma,Hu Xu, Xi Victoria Lin, Baptiste Rozire, Ja-cob Kahn, Daniel Li, Wen-tau Yih, Jason We-ston, et al. 2024. Branch-train-mix: Mixing expertllms into a mixture-of-experts llm. arXiv preprintarXiv:2403.07816. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang,Shizhe Diao, Shuang Qiu, Han Zhao, and TongZhang. 2024.Arithmetic control of llms for di-verse user preferences: Directional preference align-ment with multi-objective rewards. arXiv preprintarXiv:2402.18571. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, DamaiDai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.2023a. Math-shepherd: A label-free step-by-stepverifier for llms in mathematical reasoning. arXivpreprint arXiv:2312.08935. Yidong Wang, Zhuohao Yu, Zhengran Zeng, LinyiYang, Cunxiang Wang, Hao Chen, Chaoya Jiang,Rui Xie, Jindong Wang, Xing Xie, et al. 2023b.Pandalm: An automatic evaluation benchmark forllm instruction tuning optimization. arXiv preprintarXiv:2306.05087.",
  "To ensure the quality of the PREFERENCE COL-": "LECTION, particularly the generated verbal feed-back vrm,rn, we employ five annotators with back-grounds in natural language processing. The an-notation study was designed and administered inaccordance with [Affiliation X]s ethical guidelines.Crowd workers were informed of the potential risks of participation and researcher contact informationbefore hand in the study consent form. The hourlywage and expected study time were informed in theProlific platform. We compensated workers 9 GBPper hour. 3 were from USA and 2 were from Asiandemographics.We randomly sample 200 instances with differ-ent instructions and conduct a three-part verifica-tion process. First, we assess the coherence ofvrm,rn with the scoring decision (i.e., A is betteror B is better). Second, we evaluate the suit-ability of vrm,rn against the evaluation criteria e.Lastly, to determine the criticality of the feedback,we compare the newly generated vrm,rn with a con-catenation of vrm and vrn. This aims to determineif vrm,rn effectively leverages the mutual informa-tion between rm and rn. Annotators then vote onwhether vrm,rn or the concatenation of rm and rnis more critical. The results are shown in .Note that the Preference Collection only includesEnglish instances.",
  "BTraining and Inference Details": "The configurations we used for prompting and train-ing evaluator LMs are shown in , 9, 10.For Auto-J, PairRM and UltraRM, we utilize theirprompt template, inference hyperparameter men-tioned in the model cards or github repositories inorder to ensure the configuration is optimal for afair performance comparison. For proprietary LMs,PROMETHEUS 1, and PROMETHEUS 2 models, weuse the same prompt template and evaluation con-figurations.For both training and inference, we utilized eight40GB NVIDIA A100 GPUs. Training required ap-proximately 800 GPU hours, using the implemen-tation from the Alignment Handbook repository2.For inference, we used the vllm framework3.The results from Direct Assessment are aver-aged after three multiple runs, and pairwise grad-ing is conducted in a single run. Instead of usingerror bars, we report the consistency in assessmentformats, Krippendorffs alpha for consistency indirect assessment, and transitivity statistics for con-sistency in pairwise ranking.",
  "DLicense": "Our models are released under the Apache 2.0 li-cense. The Preference Collection dataset is sub-ject to OpenAIs Terms of Use for generated data.The model could be used for commercial purposeswhile the dataset is intended for research purposes.We used perspective API to ensure that the train-ing data or evaluation datasets do not include PII-included instances.",
  "EConsistency of Evaluator LMs": "In addition to obtaining high correlation and accu-racy, achieving high consistency is another impor-tant aspect for evaluator LMs. We first test if evalu-ator LMs could give consistent scoring decisions indirect assessment formats. We inferencing multipletimes with non-deterministic decoding (e.g., usingtemperature 1.0). Following the experimental de-sign from Ye et al. (2023), we choose to inference3 times and report the Krippendorffs alpha value.As shown in , the results indicate that train-ing on feedback data only slightly improves consis-tency. On the other hand, we find that the LMs witha large number of parameters achieve high consis-tency. This indicates the importance of selecting alarge LM as the base model when training an evalu-ator LM. Notably, PROMETHEUS-2-8X7B obtainsthe highest correlation among open evaluator LMs.Moreover, to evaluate consistency in pairwiseranking settings (), we measure transitivity(i.e., a higher score for response B over A, andfor C over B, results in a higher score for C overA). As shown in , the PROMETHEUS 2 models achieve performances on par with GPT-4,showing that they could provide robust judgmentsin pairwise ranking schemes.Lastly, we conduct an experiment to test if eval-uator LMs could achieve consistent scores acrossdifferent evaluation formats. To do this, we usepairwise ranking benchmarks and measure the per-formance differences when prompted with directassessment formats and pairwise ranking formats.Specifically, following Kim et al. (2023), to pro-cess pairwise ranking datasets in a direct assess-ment scheme, we evaluate each response separatelyand compare the scoring decisions. We mark it ascorrect if the evaluator LM provides a higher scorefor the human-chosen response over the rejectedone. As shown in (on the previous page),the results show that PROMETHEUS 2 models showlower performance differences across evaluationformats, indicating their robustness.",
  "FReference-free Evaluation in DirectAssessment Formats": "In this section, we assess the impact of excluding areference answer in evaluations conducted using di-rect assessment formats. The results are presentedin (on the previous page). For this experi-ment, we employ FLASK (Ye et al., 2023) whichincludes human judgments and additionally theBiGGen Bench (Kim et al., 2024). The BiGGenBench is a generation benchmark which includesa evaluation criteria tailored to each instance andprovides 2840 human judgments (excluding themultilingual tasks) in direct assessment formats.Across both benchmarks and different evalua-tor LM variants, the correlation with humans di-minishes when the reference answer is discarded.Even for GPT-4-1106, there is a significant perfor-mance degradation (0.045, 0.063). This suggeststhat including a reference answer is crucial for con-ducting effective evaluations with LMs. Interest-ingly, PROMETHEUS-2-7B achieves better perfor-mance in a reference-free setting (0.403, 0.425)than Mistral-7B-Instruct-v0.2 (0.310, 0.374). Sim-ilar trends are observed for PROMETHEUS-2-8X7B (0.424, 0.411) and Mixtral-8x7B-Instruct-v0.1 (0.322, 0.386). This implies that one effectof training an evaluator LM with a reference an-swer included is to induce the ability to groundjudgments to the given reference answer.",
  "(1 ) (p init)(5)": "where init is the weight of the base model.However, we empirically find that the result-ing evaluator LM final often does not gener-ate valid scoring decisions (e.g., generating aninteger during pairwise ranking). TIES merging (Yadav et al., 2024), whilesimilar to Task Arithmetic merging, adds (1) aTrim operation to remove redundant weightsin the task vector d init and p initand (2) Elect and Disjoint operations toresolve disagreement (i.e., opposite directedweights) between d init and p init. DARE merging (Yu et al., 2023), while alsosimilar to Task Arithmetic and TIES merging,performs a Random Drop and Re-scaleoperations in the task vector d init andp init to remove redundant weights. Wefind that DARE merging work best whenwe choose Mixtral-8x7B as our base model.DARE-linear merging is what was originallyproposed by Yu et al. (2023). In DARE-TIESmerging, the Elect operation from Yadavet al. (2024) is additionally added after theRe-scale operation.",
  "We conduct our experiments based on the imple-mentation from MergeKit (Goddard et al., 2024). 4": "In (on the previous page), we mea-sure the performance of evaluator LMs employingdifferent merging methods. In direct assessmentbenchmarks, DARE-Linear achieves the best per-formance, followed by DARE-TIES and Linearmerging. In pairwise ranking benchmarks, TaskArithmetics achieves the best performance, withonly a minimal difference compared to other meth-ods.On average, DARE-Linear performs best.Based on these results, we have trained Prometheus-2-7B with DARE-Linear merging. We also optedto train Prometheus-2-8x7B using DARE-Linearmerging. Although the optimal merging methodmight differ, we have not conducted additional ex-periments due to computational limitations. Futurework could explore whether these findings holdtrue.",
  "Prompt for Generating Verbal Feedbackin Pairwise Ranking": "###Task Description:An instruction (might include an Input in-side it), two responses to evaluate (denotedas Response A and Response B), a refer-ence answer, and a score rubric representinga evaluation criteria are given.1. Write a detailed feedback explaining why{sub_str}, focusing strictly on the aspectshighlighted in the evaluation criteria.2. While writing the feedback, make com-parisons between Response A, Response B,and Reference Answer. Instead of examin-ing Response A and Response B separately,go straight to the point and mention aboutthe commonalities and differences betweenthem.3. While writing the feedback, do not startby mentioning {sub_str} in the first sen-tence. Instead, try to write a reasoning pro-cess that delves into the commonalities anddifferences of the two responses and men-tion {sub_str} at the last part of your justifi-cation.4. Within the feedback, do not explicitlymention about the reference answer. For in-stance, do not use phrases like \"Comparedto the reference answer\". Assume that youinherently know the reference answer whichcould be used to determine details that arenot present in both responses under assess-ment.5. Please do not generate any other opening,closing, and explanations. Just write thefeedback.6. Within the feedback, generate a stringphrase \"[END]\" after you are finished.###Instruction: {instruction}###Response A: {response_A}###Response B: {response_B}###Reference Answer: {reference_answer}###Score Rubric: {criteria}###Feedback:",
  "Direct Assessment Prompt Template": "###Task Description:An instruction (might include an Input in-side it), a response to evaluate, and a scorerubric representing a evaluation criteria aregiven.1. Write a detailed feedback that assess thequality of the response strictly based on thegiven score rubric, not evaluating in general.2. After writing a feedback, write a scorethat is an integer between 1 and 5. Youshould refer to the score rubric.3. The output format should look as follows:\"Feedback: (write a feedback for criteria)[RESULT] (an integer number between 1and 5)\"4. Please do not generate any other opening,closing, and explanations.###The instruction to evaluate:{orig_instruction}###Response to evaluate:{orig_response}###Score Rubrics:{score_rubric}###Feedback:",
  "Pairwise Ranking Prompt Template": "###Task Description:An instruction (might include an Input in-side it), a response to evaluate, and a scorerubric representing a evaluation criteria aregiven.1. Write a detailed feedback that assessthe quality of two responses strictly basedon the given score rubric, not evaluating ingeneral.2. After writing a feedback, choose a bet-ter response between Response A and Re-sponse B. You should refer to the scorerubric.3. The output format should look as follows:\"Feedback: (write a feedback for criteria)[RESULT] (A or B)\"4. Please do not generate any other opening,closing, and explanations.###Instruction:{orig_instruction}###Response A:{response_A}###Response B:{response_B}###Score Rubric:{score_rubric}###Feedback:"
}