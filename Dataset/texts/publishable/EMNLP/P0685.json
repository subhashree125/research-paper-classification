{
  "Abstract": "We study LMs pretrained sequentially on twolanguages (L2LMs) for modeling nonnativesentence processing. In particular, we pretrainGPT2 on 6 different first languages (L1s), fol-lowed by English as the second language (L2).We examine the effect of the choice of pre-training L1 on the models ability to predicthuman reading times, evaluating on Englishreaders from a range of L1 backgrounds. Ex-perimental results show that, while all of theLMs word surprisals improve prediction of L2reading times, especially for human L1s dis-tant from English, there is no reliable effectof the choice of L2LMs L1. We also eval-uate the learning trajectory of a monolingualEnglish LM: for predicting L2 as opposed toL1 reading, it peaks much earlier and immedi-ately falls off, possibly mirroring the differencein proficiency between the native and nonna-tive populations. Lastly, we provide examplesof L2LMs surprisals, which could potentiallygenerate hypotheses about human L2 reading.",
  "Introduction": "It has been widely shown that ones first language(L1) affects both second language (L2) production(e.g., Murakami and Alexopoulou, 2016) and pro-cessing (e.g., Clahsen and Felser, 2006), a phe-nomenon called L1 transfer.In computationallinguistics, most studies of LMs as models of ac-quisition (e.g., Huebner et al., 2021) have consid-ered monolingual settings. To date, Yadavalli et al.(2023) and Oba et al. (2023) seem to be among therare exceptions that have investigated LMs as mod-els of second language acquisition (SLA), whichwe refer to as L2 Language Models (L2LMs). Bothof these studies establish that the typological dis-tance between the models first language and itssecond language (English) correlates with its En-glish performance as measured by a morphosyn-tactic benchmark. However, these L2LMs are yetto be tested against human L2 speakers. As one way of testing if the L1 of L2LMs affects L2LMsperformance on English in a humanlike manner,we study their sentence processing.Sentence processing is widely used to study(L)LMs cognitive plausibility (e.g., Oh et al.,2022; Oh and Schuler, 2022, 2023a; Kuribayashiet al., 2021, 2022; Wilcox et al., 2023, interalia). By comparing the LM surprisal (Hale, 2001;Levy, 2008) to human reading time, we determinewhether humans and models process given textsin a similar manner. Studies in this area have in-vestigated the impact of data size, model size, andmodel architecture, among other variables (see 2).In this study, we train autoregressive L2LMsfrom scratch to investigate the following ques-tions (3): (1) Does the L1 effect on L2LMs L2grammaticality discrimination, previously demon-strated for encoder-only models, extend to decoder-only models, namely GPT2? We hypothesize that,as in previous studies, the L2LMs trained withL1s closer to the L2 (English) will perform better.(2) Do L2LM surprisals predict reading time of hu-man English speakers of different L1 backgrounds?We hypothesize that L2LM best predicts L2 read-ing time when the L2LM and human L1s match.Our findings (4) are, in brief: 1. The L1 chosen for pretraining does impactthe L2LMs English perplexity and perfor-mance on the morphosyntactic benchmark(BLiMP), with closer-to-English languagesgenerally helping more, echoing prior find-ings about encoder-only models. 2. Contrary to our hypothesis, matching L1s be-tween L2LMs and humans has little effect onthe accuracy of models human reading timepredictions, which are largely dominated bythe main effect of human L1 alone.",
  "LMs and (Second) Language Acquisition": "With the ever-growing training data and modelsizes of modern large language models (LLMs),their cognitive plausibility has been garnering at-tention. Huebner et al. (2021) is among the ear-lier attempts to train a more cognitively plausibleLM, and they show that BabyBERTa, a downsizedRoBERTa (Liu et al., 2019b) trained on a substan-tially smaller amount of data, achieves competitiveperformances on various linguistic tasks. Warstadtand Bowman (2024) point out that many LLMsare trained on data that are orders of magnitudelarger than the realistic input humans are exposedto. A shared task called the BabyLM Challenge(Warstadt et al., 2023; Choshen et al., 2024) pro-motes evaluation of models trained with data quan-tities on par with child exposure.Another factor that plays an important role inunderstanding the language acquisition of LMs isinductive bias. For example, McCoy et al. (2020a)investigated the effect of various architectural fac-tors (e.g., choice of recurrent unit, attention type,and explicit tree structure in the model) on theway LMs process ambiguous input, and found that,among the various factors they studied, the pres-ence of an explicit tree structure in the encoder anddecoder was the only factor that consistently led toLMs preference for hierarchical generalization.Other works study inductive biases as a trainableset of parameters (e.g., McCoy et al., 2020b). Ofparticular relevance to this study is the work byPapadimitriou and Jurafsky (2020), where they pro-pose a method called TILT (test of inductive biasvia language transfer). Training LMs on variousfirst languages, including music scores, artifi-cial and natural languages, and then on the secondlanguage (Spanish), they find that all of them im-proved learning of the second language, with natu-ral language pretraining showing the best result. Yadavalli et al. (2023) used TILT to test positiveand negative language transfer by comparing howpretraining an LM on various L1s affects the per-formance on L2 (English). They find that the effectof the L1 training on L2 performance largely corre-lated with the L1s linguistic distance from L2, with",
  "Codeavailableat under a Creative Commons li-cense": "the pretraining on English exhibiting the best per-formance on English, followed by German, French,Polish, Japanese, and Indonesian, in descending or-der. Oba et al. (2023) report similar results, wherethe order was German, French, Japanese, and Rus-sian. Both of these studies rely on BLiMP for eval-uation, comparing the experimental results againsttypological/linguistic distance between L1s and L2.While this experimental setup is reasonable andhas its pros (e.g., no noise from human data), ourstudy focuses on adding human performance as areference to study L2LMs and the inductive biasesadded through the L1 pretraining. For this pur-pose, we use sentence processing as the primaryevaluation, which we now turn to.",
  "Sentence Processing": "Levy (2008) posits that any realistic theory of hu-man sentence comprehension must account forprocessing difficulty. He proposes the resource-allocation account of sentence processing, whichmaintains that the processing difficulty correspondsto the amount of resource reallocation needed. Thisaccount is found to be the equivalence of surprisaltheory (Hale, 2001), a probabilistic account of cog-nitive effort. Both Hale (2001) and Levy (2008)provide empirical support for the surprisal theoryusing probabilistic parsers, showing that the sur-prisal (negative log-probability) of a given wordpredicts human processing phenomena.More recently, decoder-only left-to-right incre-mental processing models such as GPT2 (alongsidesimpler LMs such as n-gram models and LSTMs)have become a standard testbed for the aforemen-tioned hypothesis. Specifically, surprisal theorycan be tested by measuring how well LMs con-ditional output probabilities predict human behav-ioral data (often referred to as psychometric predic-tive power, or ppp; e.g., Wilcox et al., 2020; Kurib-ayashi et al., 2022), such as self-paced readingtime data, eye-tracking data, and brain activity data.This line of literature corroborates the surprisal the-ory, showing that the model quality (as measuredin perplexity) correlates with the predictive powerof human reading time (quality-power hypothe-sis); in other words, the lower the perplexity, thehigher the predictive power (Goodkind and Bick-nell, 2018; Wilcox et al., 2020), and that this trendholds crosslinguistically (Wilcox et al., 2023).However, exceptions have been pointed out: thetrend has not been found in Japanese (Kuribayashiet al., 2021), and similarly for English, smaller",
  ": Training setup for L2LMs. The model is first pretrained on a given L1. We then freeze all the layersexcept for the embedding and output layers, and then continue pretraining on the L2 (English)": "LMs were more predictive of human reading timethan larger LMs (Oh and Schuler, 2023b). Ohand Schuler (2023a) explains this discrepancy byshowing that GPT-family LMs are most predictiveof human reading time after seeing around 2B4Bword tokens and their predictive power plateaus ordecreases beyond that point.Given that the surprisal theory has been empiri-cally supported and can be used to test humanlike-ness of LMs (e.g., Oh and Schuler, 2023a), andthat sentence processing is susceptible to L1 trans-fer (e.g., Clahsen and Felser, 2006), it providesan apposite evaluation for L2LMs. In light of allthese, we investigate L2LMs predictive power ofL2 English reading time, in addition to their mor-phosyntactic abilities.",
  "Training": "We adopt the TILT method from Papadimitriouand Jurafsky (2020), as well as its implementa-tion with transformer-based LMs from Yadavalliet al. (2023). illustrates the approach.We first train a GPT2-based LM on a given L1from scratch, freeze all the transformer blocks (i.e.,only the embedding layers and the LM head aretrainable), and then keep pretraining on the L2(English).2 The idea is that the high-level abstractlinguistic knowledge has been shown to be stored inthe intermediate layers (see Rogers et al., 2020 for",
  "Some refer to this as a finetuning phase; however, be-cause the training objective is the same, we refer to this phaseas the second pretraining phase, or L2 learning phase": "a comprehensive review), and that freezing thoseparameters will force the model to acquire a newlanguage primarily by learning new words (in theembedding layer) while relying on the L1 grammar(in the frozen decoder blocks).All of the models were downsized for computa-tional efficiency. Oh and Schuler (2023a) find thattransformer-based LMs as small as 2 layers with 3attention heads and an embedding size of 192 arecompetitive with or even better than larger modelson human reading time prediction. We thereforeadopt this model configuration.",
  "L1 Training": "We use the CC100 corpus (Conneau et al. 2020;Wenzek et al. 2020), a multilingual common webcrawl corpus that covers a set of 100 typologicallydiverse languages, available under Common Crawlterm of use.3 Since our goal is to measure the rela-tionship between LM surprisal and human L2 read-ing times, the LMs L1s were chosen based on theL1s of the human participants of the CELER cor-pus (Berzak et al., 2022, see 3.4.3): Arabic, Chi-nese (simplified), English, Japanese, Portuguese,and Spanish. We sampled a training set of 100Mtokens from each of the L1 subcorpora in CC100.For each L1, we first train a tokenizer. We useSentencePieceBPETokenizer from the HuggingFace library (Wolf et al., 2020), available underan Apache License. The SentencePiece algorithm(Kudo and Richardson, 2018) works well withlanguages not separated by white spaces, and itslanguage-agnostic nature fits our purpose to keepthe conditions as close as possible to each otheracross different L1s. We train each tokenizer on5M sentences (500MB of data). We set the vocab-ulary size to 20K, based on the estimate of humanL1 vocabulary size that ranges from 17K to 20K distinct words (Nation, 2006; Goulden et al., 1990;Zechmeister et al., 1995).Once the tokenizer is trained, we then train themodel on 4B words, saving the model at every400M words. We focus our analyses on the first(400M words) and the last (4B words) checkpoints.We consider the 400M variant to be cognitivelyplausible, based on the estimate that the numberof word tokens a person is annually exposed toamounts to 11M words (Hart and Risley, 1995).The 4B variant is by no means comparable to hu-mans in terms of the amount of input; however, thisvariant is expected to be most predictive of humanreading time based on Oh and Schuler (2023a),who find that LMs predictive power of humanreading time peaks after seeing about 2B tokens(and 4B tokens for smaller models). For both con-ditions, we used an effective batch size of 64 andcontext length of 256. This resulted in 24K and244K training steps for each condition, respec-tively. The training took 10 hours for each L1 ona single RTX A6000 GPU with 48GB vRAM. Forthe subsequent L2 training, we reuse each of theseL1LMs as the starting checkpoint.",
  "L2 Training": "Once the L1 training phase is complete, we freezeall of the LMs parameters except for the embed-ding layer and the LM head. By doing so, weallow the model to acquire a new set of vocabu-lary items (in the L2, namely English), as well asto adjust the classification space (we are changingboth the language and the size of the classificationspace; VL1 > VL2). For training data, we use Sim-ple English Wikipedia,4 available under a CreativeCommons license. The preprocessed version isavailable under the Hugging Face library.5 We train the model on 30M words and save acheckpoint at every 3M words, focusing on thefirst (3M words) and last (30M words) checkpoints.The 3M variant is motivated by Nation (2014) andMason and Krashen (2014), where they found thataround 1M and 3M words were necessary to see the5,000 most frequent word families and 9,000 mostfrequent word families, respectively, for at least 12times. The 30M variant is to mirror the L1 trainingphase: we simply exposed the model to 10 timesmore word tokens than the cognitively plausibleexposure condition (3M words), amounting to 30M word tokens. We used the same training setup, withan effective batch size and context length of 64 and256, respectively. This resulted in 183 and 1,831training steps for the two conditions, respectively.6 With 6 L1s, 1 L2, and 2 configurations for eachof the two training phases, we obtain 6122 =24 L2LMs. These variants establish the baselinefor the initial comparisons, although we train addi-tional models and investigate intermediate check-points as follow-ups, as described in later sections.The differences in the results can be fully attributedto the inductive biases of the L2 LMs, since every-thing else was held constant.",
  "Evaluation": "3.4.1PerplexityWe report the perplexity of each L2LMs, mainly forthe purpose of (1) ensuring that the TILT trainingis properly working as expected (L2 perplexity isexpected to go down) and (2) testing the quality-power hypothesis in L2 sentence processing. Weobtain the perplexity on a held-out validation set ofSimple English Wikipedia of 3M tokens, or 10%in size of the training set, using the sliding windowstrategy. Because each L2LM has a context lengthof 256, we set the sliding window size to 128, witheach token going through the forward pass twice. 3.4.2MorphosyntaxWe also evaluate L2LMs on morphosyntacticknowledge, namely the Benchmark of LinguisticMinimal Pairs (BLiMP; Warstadt et al., 2020). Asdiscussed earlier, this is mainly because the preced-ing studies (Yadavalli et al., 2023; Oba et al., 2023)used this benchmark as the main evaluation, andwe aim to test whether (1) the results in the litera-ture can be replicated with a decoder-only model(GPT2), and (2) the L2LM is properly infused withan inductive bias based on the L1 training. 3.4.3Reading TimeLMs are considered more humanlike in termsof sentence processing if their per-word surprisalestimates are more predictive of the per-word hu-man reading times. We need 3 key ingredients totest this: (i) a left-to-right incremental processingmodel, (ii) per-word surprisal estimates obtainedfrom such a model, and (iii) human reading timedata. For (i), we use the GPT2-based L2LM de-scribed earlier in this paper. For (ii), based on pre-vious work (e.g., Kuribayashi et al., 2021; Oh and",
  "Swi = logP(wi www<i).(1)": "This represents how surprised the model is giventhe word of interest and the preceding context. TheL2LMs predictive power of human reading behav-iors is then measured by how much improvementwe see in a linear regression models fit for humanreading time data when the surprisal is added tothe baseline model. This is operationalized as thedifference in the model fit between the 2 regressionmodels (delta log-likelihood; LL):",
  "LL = LLbl+S LLbl,(2)": "where LLbl+S and LLbl are log-lilekihood (measureof model fit) of the baseline model with and with-out surprisal estimates, respectively. The intuitionbehind this operationalization is as follows: The fitof the first regression model (LLbl) represents howwell human reading time can be predicted withoutan LM, whereas the fit of the second regressionmodel (LLbl+S) represents how well human read-ing time can be predicted with an LM. By takingthe difference between these two, we can measurehow much improvement the addition of LM sur-prisals makes on the fit on human reading time.This difference in the model fit, or LL, is the op-erationalization of the LMs predictive power ofhuman reading time. Following the previous stud-ies, we include the word length and position asfixed effects, and subject ID as a random effect inbaseline features (bl) to predict gaze duration. Wereport the per-word average LL.For (iii), we use CELER (Berzak et al., 2022), acorpus of English reading times collected from atotal of 365 L1 and L2 English speakers. For the L2speakers, L1s include Arabic, Japanese, MandarinChinese, Portuguese, and Spanish, as discussedearlier. We hypothesize that the surprisals obtainedfrom L2LM trained on the same L1 as the humanlearners will produce the greatest LL.",
  ": L2LMs perplexities on the L2 validation set.The color indicates the L1 of the L2LM as shown in thelegend. Dotted and solid lines represent the L1 trainingamount of 400M and 4B tokens, respectively": "tokens on 6 L1s, throughout the L2 training phase.The color of each bar corresponds to different L1sof L2LMs as shown in the legend (ordered in de-scending order based on the linguistic distancefrom English; Littell et al., 2017). As expected,regardless of the L1 and L1 training amount, theperplexity decreases monotonically throughout theL2 training.More importantly, the results support Papadim-itriou and Jurafskys (2020) finding that L1s typo-logically closer to the L2 result in lower perplexi-ties. The expected order was (from lower to higherin perplexity) English, Spanish, Portuguese, Ara-bic, Chinese, and Japanese, and the observed orderwas identical except for the flipped order betweenSpanish and Portuguese. In addition, this order wasonly observed when the L2LMs were sufficientlytrained on the L1 (4B tokens), and not when theywere trained less (400M tokens).",
  "Morphosyntax": "summarizes each L2LMs performance onthe BLiMP dataset. Each of the 4 blocks of 6 barscorresponds to one of the 4 possible combinationsof L1 and L2 training configurations. For example,400M3M means that all of the 6 L2LMs in thatblock were trained on their respective L1 for 400Mtokens, and then on L2 (English) for 3M tokens. Afew observations were made.",
  ": The difference in BLiMP accuracy scoreswhen L2 training amount is varied (left), and when L1training amount is varied (right)": "First, with enough exposure to L2, it appearsthat L2LMs performance on BLiMP negativelycorrelates with the corresponding L1s typologi-cal distance from English, largely replicating theresults from Yadavalli et al. (2023) and Oba et al.(2023). That is, models in both 400M30M and4B30M (but not *3M) configurations tend toperform better on BLiMP when their L1 is typolog-ically closer to English.Second, more L2 training always led to betterBLiMP performance. As shown in the left half of, although the degree of improvement variedbased on the L1, without exception, L2LMs per-formance improves with more L2 training when theL1 training amount is held constant. That is to say,the performance improved from the 400M3M tothe 400M30M setting, and from the 4B3M tothe 4B30M setting.Third, the effect of the amount of L1 trainingon BLiMP performance varied by L1, as shown inthe right half in . This is in stark contrast",
  "Baseline LL": ": LL of each L2LM when adding its surprisalestimates to the baseline linear regression model (top),and the corresponding log-likelihood of the baselinelinear regression model for each human L1 (bottom).Each L2LMs L1 is indicated by the color of the bar, andthe L1 of the human participants of the CELER corpusis indicated on the x-axis. Error bars represent 95%confidence intervals, obtained from fitting regressionmodels on bootstrapped samples 1,000 times. with the second observation that more L2 traininginvariably led to higher BLiMP performance. Moreconcretely, the L2LMs performance improves forsome L1s and degrades for others, when trained onmore L1 data with the L2 training amount held con-stant. Specifically, more L1 training on Japanesealways led to better BLiMP performance, whereasmore L1 training on Chinese always led to poorerBLiMP performance. More L1 training on Englishand Portuguese led to better BLiMP performancewhen L2 training was sufficient (30M) but to poorerBLiMP performance when L2 training was limited(3M), and the opposite was true about Arabic L1training. L1 training on Spanish had virtually noeffect on BLiMP performance, which may explainthe anomaly we observe in Spanish L2LM in Fig-ure 3. As we saw in 4.1, that each L1 differentlyaffects the outcome of identical L2 training, con-firming the idea that the L1 training infuses themodel with different inductive biases (Papadim-itriou and Jurafsky, 2020).",
  "Effect of L1 on LL": "summarizes the per-word average LL ofeach of the L2LM (indicated by the color of the bar)when its surprisals are added to the baseline regres-sion model to predict the reading time of humanparticipants of the CELER corpus (whose L1 isindicated on the x-axis). The shaded bars representthe LLs that were expected be the highest withinthe same block (i.e. within the 6 regression modelsthat predict the reading time of human participantsof the same L1), based on the hypothesis that L2humans and L2LMs behave similarly if they sharethe same L1. shows that this is not thecase: within-block differences are small, and thepredicted pattern is not consistently supported.Rather, differences across blocks are more pro-nounced. Two-way analysis of variance (ANOVA)reveals that the main effect for human L1 (F =628.64, df = 5, p < .001) is much stronger than thatof LM L1 (F = 16.67, df = 5, p < .001), althoughboth are statistically significant. In other words, re-gardless of the L1 of the L2LM, adding the L2LMsurprisals leads to the greatest improvement in theregression models reading time predictions whenthe human L1 is Japanese, followed by Arabic,Chinese, Spanish, Portuguese, and English, in de-scending order. We suspect that this is due to thefit of the baseline models, which rely on the wordlength and the word position alone. In the bottomof , we plot the LL of the baseline regres-sion model for each human L1. Indeed, there isa strong correlation between the mean LL of 6L2LMs for a given human L1, and the baseline LLfor the same human L1 (r = .92, p < .01), mean-ing that, the higher the LL of the baseline modelis (the more predictable human reading time is,based on the baseline variables i.e. word length and position), the lower the LL is (the less theimprovement from adding L2LM surprisals).Additionally, it is also worth noting that the or-der of baseline LL roughly follows the linguisticdistance between L1s and English (see 4.2). Thelinguistic distances are in the following order: En-glish (trivially), Spanish, Portuguese, Arabic, Chi-nese, and Japanese; while the baseline LL is in thefollowing order: English, Spanish, Chinese, Por-tuguese, Arabic, and Japanese. This suggests that,given an L2 English reader, the more distant theirL1 is from English, the less predictable their En-glish reading behavior is solely based on wordlength and word position. This also implies that,between L1 speakers and L2 speakers, and amongL2 speakers of different L1s, different strategiesare employed for online English processing, whichis also reported in applied psycholinguistics (e.g.Clahsen and Felser, 2006). 4.3.2Effect of L1 and L2 Training on LLIn this section, we provide analyses of the devel-opmental trajectory of each L2LMs reading timepredictions. We first show the equivalent plot of amonolingual English LM to (1) show that our meth-ods replicate the previously reported observationson the relationship between the predictive powerand training amount when tested on L1 Englishreading time, and (2) determine whether a similarobservation can be made about L2 reading time. summarizes how the predictions for hu-man L1 English reading times change throughoutthe pretraining process. The LM is trained on 4Btokens from scratch as described in 3.2, and eachof the 6 plots differs from each other only in theL1 of the humans whose reading times the LMis predicting. Notably, on the one hand, for En-glish speakers reading time, LL peaks at around2.4B tokens and plateaus for the most part after-",
  "EnglishSpanishPortugueseArabicChineseJapanese": ": The relation between L2LMs LL (y-axis) and L2 perplexity (x-axis) at every 3M tokens during the L2training phase. Each line represents an L2LM trained on the L1 of the corresponding color for 400M tokens (top)and 4B tokens (bottom), respectively. The shaded region around each line represents the 95% confidence interval.Human L1s are indicated on the x-axis. ward. This is congruent with previously reportedresults that LMs predictive power peaks after 2B4B tokens (Oh and Schuler, 2023a), confirming(1). On the other hand, when a monolingualEnglish LM predicts L2 English reading time,LL peaks at around 800M1.2B and plummetsbeyond that point. One potential account for thistendency is that L2 English speakers proficiency iscomparable to LMs trained on 800M1.2B words,while LMs reach nativelike proficiency at around2B4B words. plots the LLs and L2 perplexities col-ored by LM L1 for each human L1, trained on400M (top) and 4B (bottom) L1 tokens (see Fig-ure 9 in Appendix C for a plot similar to but for L2LMs). Importantly, when L2LMs aretrained on 4B L1 tokens (bottom half of the figure),regardless of LM L1 or human L1, L2 perplexityand LL are positively correlated, meaning thatthe higher the LM quality (lower the perplexity) is,the less they are predictive of L2 human readingtime. This is in contrast with the quality-powerhypothesis (Wilcox et al., 2023), where they find apositive correlation between LM quality and LMpsychometric predictive power. However, Oh andSchuler (2023a) show that 2B tokens is the tippingpoint where the quality-power correlation changesfrom positive to negative. Given that all of theL2LMs in the bottom half of the figure are trainedon 4B L1 tokens, they may be at the phase wherequality-power correlation is negative.",
  "It is important to note, however, that the L2 train-": "ing amount is on the order of 3M to 30M tokens,which is far from the aforementioned tipping point(2B tokens). Taken together with the observationthat the choice of LM L1 had little effect on theL2LMs L2 reading time predictions (see 4.3.2), itappears that, regardless of the choice of L1, whenan LM is trained on some L1 for a sufficient amount(say, 2B tokens), it reaches the maximum predic-tive power for human sentence processing, evenwhen the target language (on which sentence pro-cessing is measured) is different from the languagethe LM is trained on. Considering the small model(192-2-3 architecture) and training data (30M to-kens) sizes, there may be an alternative accountfor the inverse quality-power relation besides whathas been proposed to date, such as larger modelsize allowing for the memorization of training data(Oh and Schuler, 2023b), and the learning of infre-quent words later in pretraining (Oh et al., 2024).Clearly, these hypotheses have been proposed withrespect to monolingual English LMs, and the ob-servations we made on the L2LMs trained withthe TILT-based (Papadimitriou and Jurafsky, 2020)unique pretraining setup may not be straightfor-wardly applicable.",
  "Qualitative Examples": "Since human data are noisy, it is not surprising thatL2LMs behaviors are not in alignment with thoseof human L2 speakers. Needless to say, multiplehypotheses can be given to explain these results,with the obvious one being that the L2LMs intro- duced in this study are simply not good models ofhuman L2 speakers. However, in this section, weshow a sample sentence from the CELER corpuswhere L2LMs predictions were maximally differ-ent from each other (i.e. their surprisals were mostdivergent based on the L1 they were trained on).With these, we aim to show that these L2LMs doexhibit some behaviors that could potentially helpus generate hypotheses about human behaviors.In , we see that each L2LM shows a sim-ilar level of surprisal until faced with the word occu-pied. Interestingly, L2LMs first trained on Spanishand Portuguese are more than 1.5 times more sur-prised than the other three L2LMs (first trainedon Arabic, Chinese, and Japanese, respectively).To reiterate, their L2 English training and Englishtokenizers are identical. The word occupied wastokenized into [oc, c, up, ied], suggestingthat the model recognizes its part of speech (pastor past participle of a verb based on the rightmostsuffix). We speculate that this is because Arabic,Portuguese, and Spanish place relative clauses af-ter noun phrases while Chinese and Japanese lan-guages place them before. Therefore, because thedecoder blocks were frozen during the L2 trainingphase, the model may be more likely to have astrong expectation for a determiner or a noun aftera preposition for the latter group.This speculation is based on the idea that theTILT method allows for the preservation of L1structural information in the frozen middle layerslearned during the L1 pretraining phase (Papadim-itriou and Jurafsky, 2020). It has been widely ob-served in the BERTology literature that structuralinformation (including POS information, which iscritical to an expectation of given word class ashypothesized above) is encoded in middle layers ofthe transformer architecture has been widely shown(e.g., Tenney et al., 2019; Liu et al., 2019a, interalia). Aoyama and Schneider (2022) also corrob-orate this hypothesis directly through a languagemodeling task, showing that the model learns topredict a word with correct (i.e., same as the tar-get) POS most actively in middle layers.Given this literature, we suspect that our TILT-based L2LMs have L2 vocabulary and L1 structuralknowledge, potentially resulting in the observedpreference in certain word orders reflective of L1structure. However, it is important to note thatthis hypothesis calls into question why the ArabicL2LM patterns with Chinese and Japanese (see in Appendix B for a similar grouping",
  "Conclusion": "In this study of L2LMs, we trained GPT2styledecoder-only LMs on 6 L1s (Arabic, Chinese, En-glish, Japanese, Portuguese, and Spanish) and thenon a common L2 (English), freezing the decoderblocks the L2 training phase, with all of the vari-ables including data size and hyperparameters heldconstant. We replicate findings from previous lit-erature that the linguistic distance between the L1and L2 negatively correlates with the LMs perfor-mance on the L2 (as measured in BLiMP). Ournovel findings include that a monolingual EnglishLM is most predictive of L1 English reading timeat around 2.4B word tokens, and of L2 Englishreading time at around 800M1.2B word tokens;that L2LMs sentence processing was not shownto correlate with that of human L2 speakers of En-glish; and that the overall predictability of humanL2 speakers sentence processing largely dependedon their L1. We also showed that, despite the lackof overall correlation between L2LMs surprisalsand L2 English speakers reading times, qualita-tive examples could be conducive to generatinghypotheses of L2 reading behaviors.",
  "Limitations": "First, this work relied solely on the TILT method(Papadimitriou and Jurafsky, 2020) to simulate theL2 learning process; of course, it is not realisticto assume that all the neurons are frozen in hu-man brain once an L1 is acquired, and testing othermethods of simulating human L2 (and L1) acqui-sition with LMs is an important avenue for future research. More broadly, simulating language learn-ing with text inputs alone is both unrealistic and in-adequate, and incorporating other input modalities,such as vision, remains an important and excitingdirection.Second, since an extensive comparison of LMsof different sizes was not feasible, we only trainedL2LMs using the 192-2-3 architecture (hidden sizeof 192, 2 layers, and 3 attention heads), as dis-cussed in 3.1. Although the efficacy of small LMshas been widely shown (e.g., Huebner et al., 2021)and although this particular architecture was re-ported as a variant that had a predictive power ofhuman reading time similar to or even better thanlarger variants (Oh and Schuler, 2023a), it remainspossible that testing L2LMs of larger sizes wouldyield new insights. In a similar vein, we only testedGPT2-based LMs in this study, and results mayvary for other decoder-only models.Third, human reading time data are scarce, letalone human L2 reading time data. The data usedin this study, CELER (Berzak et al., 2022), is arare exception; however, the total of 365 partici-pants means that each of the 6 L1s is representedby 60 participants. While this is an impressivenumber given the cost of collecting such behavioraldata, we acknowledge that findings based on theseparticipants may not generalize to broader speakerpopulations.",
  "Ethics Statement": "We only studied L2 English and 6 L1s (Arabic,Chinese, English, Japanese, Portuguese, Spanish),all of which are well-resourced languages. Thisis because we needed to match the L1s availablein the CELER corpus (Berzak et al., 2022), andnot because particular languages are more im-portant than others. We acknowledge that it isimportant to study additional first and second lan-guages, and expanding the availability of datasetsin other languages remains an important goal of fu-ture research. In addition, although certain L2LMsseemed to perform better than other L2LMs, thisis not to be taken as an indication of superiority ofa certain L1 population in mastering English as asecond language.Lastly, this work involved training multiple deeplearning models, which is energy-intensive andcould contribute to carbon emissions. However,as already described in 3, we minimize the num-ber of models by first training 6 monolingual mod-",
  "Acknowledgements": "We thank Ethan Wilcox and anonymous reviewersfor their valuable feedback and insightful discus-sions. We also thank the organizers and participantsof Mid-Atlantic Student Colloquium on Speech,Language and Learning (MASC-SLL 2024) foran opportunity to present this work and receivevaluable feedback at an earlier stage of this work.The research was supported in part by NSF awardIIS-2144881 (Nathan Schneider, PI). Tatsuya Aoyama and Nathan Schneider. 2022. Probe-less probing of BERTs layer-wise linguistic knowl-edge with masked word prediction. In Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies: Student ResearchWorkshop, pages 195201, Hybrid: Seattle, Washing-ton + Online. Association for Computational Linguis-tics.",
  "Harald Clahsen and Claudia Felser. 2006. Continuityand shallow structures in language processing. Ap-plied Psycholinguistics, 27(1):107126": "Thomas Hikaru Clark, Clara Meister, Tiago Pimentel,Michael Hahn, Ryan Cotterell, Richard Futrell, andRoger Levy. 2023. A Cross-Linguistic Pressure forUniform Information Density in Word Order. Trans-actions of the Association for Computational Linguis-tics, 11:10481065. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Adam Goodkind and Klinton Bicknell. 2018. Predic-tive power of word surprisal for reading times is alinear function of language model quality. In Pro-ceedings of the 8th workshop on cognitive modelingand computational linguistics (CMCL 2018), pages1018.",
  "Betty Hart and Todd R Risley. 1995. Meaningful differ-ences in the everyday experience of young Americanchildren. Paul H Brookes Publishing": "Philip A. Huebner, Elior Sulem, Fisher Cynthia, andDan Roth. 2021. BabyBERTa: Learning more gram-mar with small-scale child-directed language. In Pro-ceedings of the 25th Conference on ComputationalNatural Language Learning, pages 624646, Online.Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics. Tatsuki Kuribayashi, Yohei Oseki, Ana Brassard, andKentaro Inui. 2022. Context limitations make neurallanguage models more human-like. In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1042110436,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, RyoYoshida, Masayuki Asahara, and Kentaro Inui. 2021.Lower perplexity is not always human-like. In Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics and the 11th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 52035217,Online. Association for Computational Linguistics.",
  "Nelson F. Liu, Matt Gardner, Yonatan Belinkov,Matthew E. Peters, and Noah A. Smith. 2019a. Lin-guistic knowledge and transferability of contextual": "representations. In Proceedings of the 2019 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long and Short Pa-pers), pages 10731094, Minneapolis, Minnesota.Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019b.RoBERTa: A robustly optimized BERT pretrainingapproach. arXiv preprint arXiv:1907.11692. Beniko Mason and Stephen Krashen. 2014. Can secondlanguage acquirers reach high levels of proficiencythrough self-selected reading? an attempt to con-firm nations (2014) results. International Journal ofForeign Language Teaching, 10(2):1019. R. Thomas McCoy, Robert Frank, and Tal Linzen.2020a.Does syntax need to grow on trees?sources of hierarchical inductive bias in sequence-to-sequence networks. Transactions of the Associationfor Computational Linguistics, 8:125140.",
  "Byung-Doh Oh, Shisen Yue, and William Schuler. 2024": "Frequency explains the inverse correlation of largelanguage models size, training data amount, andsurprisals fit to reading times. In Proceedings ofthe 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 26442663, St. Julians, Malta.Association for Computational Linguistics. Isabel Papadimitriou and Dan Jurafsky. 2020. Learn-ing Music Helps You Read: Using transfer to studylinguistic structure in language models. In Proceed-ings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages68296839, Online. Association for ComputationalLinguistics.",
  "Alex Warstadt and Samuel R. Bowman. 2024. Whatartificial neural networks can tell us about human lan-guage acquisition. arXiv preprint arXiv:2208.07998": "Alex Warstadt, Aaron Mueller, Leshem Choshen, EthanWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-quera, Bhargavi Paranjabe, Adina Williams, TalLinzen, and Ryan Cotterell. 2023. Findings of theBabyLM challenge: Sample-efficient pretraining ondevelopmentally plausible corpora. In Proceedingsof the BabyLM Challenge at the 27th Conference onComputational Natural Language Learning, pages134, Singapore. Association for Computational Lin-guistics. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.Bowman. 2020. BLiMP: The benchmark of linguis-tic minimal pairs for English. Transactions of theAssociation for Computational Linguistics, 8:377392. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-neau, Vishrav Chaudhary, Francisco Guzmn, Ar-mand Joulin, and Edouard Grave. 2020.CCNet:Extracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-guage Resources and Evaluation Conference, pages40034012, Marseille, France. European LanguageResources Association. Ethan Wilcox, Clara Meister, Ryan Cotterell, and TiagoPimentel. 2023. Language model quality correlateswith psychometric predictive power in multiple lan-guages. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 75037511, Singapore. Association for Com-putational Linguistics. Ethan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, PengQian, and Roger P. Levy. 2020. On the predictivepower of neural language models for human real-time comprehension behavior. In Proceedings ofthe 42nd Annual Meeting of the Cognitive ScienceSociety, page 17071713. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics. Aditya Yadavalli, Alekhya Yadavalli, and Vera Tobin.2023. SLABERT talk pretty one day: Modelingsecond language acquisition with BERT. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1176311777, Toronto, Canada. Associationfor Computational Linguistics.",
  ": Per-word surprisals of a sample sentencefrom the CELER corpus. L2LMs maximally differ fromeach other at the 7th word chairman": "plots the per-word suprisal obtainedfrom each of the 6 L2LMs. Here again, Chineseand Japanese L2LMs pattern together, showinglower surprisals compared to other L2LMs. This isperhaps due to the fact that Chinese and Japaneseare the only 2 languages among the 6 that do nothave the article system, and that an article-less sin-gular noun is less surprising.",
  "summarizes each L2LMs predictivepower of each L1 groups L2 English reading time": "at every 3M tokens they saw in the L2 trainingphase. Because L2 perplexity was monotonicallydecreasing in relation to L2 training amount (see), looks similar in shape to Fig-ure 9, which summarized the trajectory of LL asa function of L2 perplexity. We find a few generalpatterns.First, comparing the L2LMs trained on 400Mwords in the L1 (top) and those trained on 4B wordsin the L1 (top), even though the L2 input and sizeare identical, the development of the predictivepower seems to be different. When L2LMs areexposed to 4B words during the L1 training phase,with very few exceptions, L2 exposure does notimprove L2LMs predictive power, as observed inthe almost monotonically decreasing trend in all 6plots in the bottom half of .Second, when L2LMs are exposed to 400Mwords during the L1 training, L2 exposure some-times improves the predictive power, for some com-binations of L2LMs L1s and human L1s (e.g., theSpanish L2LM predicting L2 English reading timeof Spanish, Portuguese, and Chinese speakers).Lastly, it is worth noting that this overall down-ward trend in the development of predictive powerstands in sharp contrast to the monotonically up-ward trend in BLiMP performance (left half of; see 4.2), where L2 exposure always ledto higher BLiMP performance, regardless of thenumber of words L2LMs have seen during the L1training phase (400M and 4B). This suggests that,with the decoder blocks frozen and only embeddingand output layers left trainable, L2LMs adapted tothe English inputs during the L2 training phase bylearning to distinguish likely and unlikely (gram-matical and ungrammatical) sequences of inputs,using novel strategies that seem to deviate fromboth humans and regular LMs."
}